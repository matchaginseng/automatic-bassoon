Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100000
[GPU_0] Set GPU power limit to 100W.
[GPU_0] Set GPU power limit to 100W.
Launching Zeus monitor 0...
Training Epoch: 0 [1024/50176]	Loss: 4.6235
Training Epoch: 0 [2048/50176]	Loss: 4.7067
Training Epoch: 0 [3072/50176]	Loss: 4.6699
Training Epoch: 0 [4096/50176]	Loss: 4.6377
Training Epoch: 0 [5120/50176]	Loss: 4.6338
Training Epoch: 0 [6144/50176]	Loss: 4.6272
Training Epoch: 0 [7168/50176]	Loss: 4.6394
Training Epoch: 0 [8192/50176]	Loss: 4.5763
Training Epoch: 0 [9216/50176]	Loss: 4.5694
Training Epoch: 0 [10240/50176]	Loss: 4.5937
Training Epoch: 0 [11264/50176]	Loss: 4.5209
Training Epoch: 0 [12288/50176]	Loss: 4.5596
Training Epoch: 0 [13312/50176]	Loss: 4.5756
Training Epoch: 0 [14336/50176]	Loss: 4.5104
Training Epoch: 0 [15360/50176]	Loss: 4.4768
Training Epoch: 0 [16384/50176]	Loss: 4.4852
Training Epoch: 0 [17408/50176]	Loss: 4.4252
Training Epoch: 0 [18432/50176]	Loss: 4.4233
Training Epoch: 0 [19456/50176]	Loss: 4.3208
Training Epoch: 0 [20480/50176]	Loss: 4.3561
Training Epoch: 0 [21504/50176]	Loss: 4.4750
Training Epoch: 0 [22528/50176]	Loss: 4.5259
Training Epoch: 0 [23552/50176]	Loss: 4.5040
Training Epoch: 0 [24576/50176]	Loss: 4.3680
Training Epoch: 0 [25600/50176]	Loss: 4.3164
Training Epoch: 0 [26624/50176]	Loss: 4.3399
Training Epoch: 0 [27648/50176]	Loss: 4.3060
Training Epoch: 0 [28672/50176]	Loss: 4.2271
Training Epoch: 0 [29696/50176]	Loss: 4.3024
Training Epoch: 0 [30720/50176]	Loss: 4.2823
Training Epoch: 0 [31744/50176]	Loss: 4.2910
Training Epoch: 0 [32768/50176]	Loss: 4.2473
Training Epoch: 0 [33792/50176]	Loss: 4.3139
Training Epoch: 0 [34816/50176]	Loss: 4.2500
Training Epoch: 0 [35840/50176]	Loss: 4.2375
Training Epoch: 0 [36864/50176]	Loss: 4.2016
Training Epoch: 0 [37888/50176]	Loss: 4.2164
Training Epoch: 0 [38912/50176]	Loss: 4.2387
Training Epoch: 0 [39936/50176]	Loss: 4.3132
Training Epoch: 0 [40960/50176]	Loss: 4.1755
Training Epoch: 0 [41984/50176]	Loss: 4.1089
Training Epoch: 0 [43008/50176]	Loss: 4.1819
Training Epoch: 0 [44032/50176]	Loss: 4.1196
Training Epoch: 0 [45056/50176]	Loss: 4.2181
Training Epoch: 0 [46080/50176]	Loss: 4.1841
Training Epoch: 0 [47104/50176]	Loss: 4.1008
Training Epoch: 0 [48128/50176]	Loss: 4.0452
Training Epoch: 0 [49152/50176]	Loss: 4.0764
Training Epoch: 0 [50176/50176]	Loss: 4.0476
epoch 1 train time consumed: 91.91s
epoch 1 eval time consumed: 4.49s
Validation Epoch: 0, Average loss: 0.0044, Accuracy: 0.0484
Training Epoch: 1 [1024/50176]	Loss: 4.0564
Training Epoch: 1 [2048/50176]	Loss: 4.0084
Training Epoch: 1 [3072/50176]	Loss: 4.0631
Training Epoch: 1 [4096/50176]	Loss: 3.9780
Training Epoch: 1 [5120/50176]	Loss: 3.9344
Training Epoch: 1 [6144/50176]	Loss: 4.0008
Training Epoch: 1 [7168/50176]	Loss: 4.0050
Training Epoch: 1 [8192/50176]	Loss: 4.0388
Training Epoch: 1 [9216/50176]	Loss: 4.1143
Training Epoch: 1 [10240/50176]	Loss: 3.9878
Training Epoch: 1 [11264/50176]	Loss: 3.9779
Training Epoch: 1 [12288/50176]	Loss: 3.9108
Training Epoch: 1 [13312/50176]	Loss: 3.8882
Training Epoch: 1 [14336/50176]	Loss: 4.0384
Training Epoch: 1 [15360/50176]	Loss: 3.9191
Training Epoch: 1 [16384/50176]	Loss: 3.9427
Training Epoch: 1 [17408/50176]	Loss: 3.9770
Training Epoch: 1 [18432/50176]	Loss: 3.8256
Training Epoch: 1 [19456/50176]	Loss: 3.9539
Training Epoch: 1 [20480/50176]	Loss: 3.8557
Training Epoch: 1 [21504/50176]	Loss: 3.9375
Training Epoch: 1 [22528/50176]	Loss: 3.8948
Training Epoch: 1 [23552/50176]	Loss: 3.9287
Training Epoch: 1 [24576/50176]	Loss: 3.9181
Training Epoch: 1 [25600/50176]	Loss: 3.8621
Training Epoch: 1 [26624/50176]	Loss: 3.8675
Training Epoch: 1 [27648/50176]	Loss: 3.8008
Training Epoch: 1 [28672/50176]	Loss: 3.7728
Training Epoch: 1 [29696/50176]	Loss: 3.8262
Training Epoch: 1 [30720/50176]	Loss: 3.8318
Training Epoch: 1 [31744/50176]	Loss: 3.9016
Training Epoch: 1 [32768/50176]	Loss: 3.8514
Training Epoch: 1 [33792/50176]	Loss: 3.8886
Training Epoch: 1 [34816/50176]	Loss: 3.8805
Training Epoch: 1 [35840/50176]	Loss: 3.8164
Training Epoch: 1 [36864/50176]	Loss: 3.7514
Training Epoch: 1 [37888/50176]	Loss: 3.7549
Training Epoch: 1 [38912/50176]	Loss: 3.7823
Training Epoch: 1 [39936/50176]	Loss: 3.7541
Training Epoch: 1 [40960/50176]	Loss: 3.7795
Training Epoch: 1 [41984/50176]	Loss: 3.7687
Training Epoch: 1 [43008/50176]	Loss: 3.9210
Training Epoch: 1 [44032/50176]	Loss: 3.7194
Training Epoch: 1 [45056/50176]	Loss: 3.7606
Training Epoch: 1 [46080/50176]	Loss: 3.7226
Training Epoch: 1 [47104/50176]	Loss: 3.7837
Training Epoch: 1 [48128/50176]	Loss: 3.9070
Training Epoch: 1 [49152/50176]	Loss: 3.7554
Training Epoch: 1 [50176/50176]	Loss: 3.7302
epoch 2 train time consumed: 91.26s
epoch 2 eval time consumed: 4.52s
Validation Epoch: 1, Average loss: 0.0037, Accuracy: 0.1121
Training Epoch: 2 [1024/50176]	Loss: 3.7246
Training Epoch: 2 [2048/50176]	Loss: 3.7482
Training Epoch: 2 [3072/50176]	Loss: 3.6546
Training Epoch: 2 [4096/50176]	Loss: 3.6995
Training Epoch: 2 [5120/50176]	Loss: 3.6881
Training Epoch: 2 [6144/50176]	Loss: 3.7304
Training Epoch: 2 [7168/50176]	Loss: 3.7931
Training Epoch: 2 [8192/50176]	Loss: 3.8411
Training Epoch: 2 [9216/50176]	Loss: 3.6393
Training Epoch: 2 [10240/50176]	Loss: 3.5911
Training Epoch: 2 [11264/50176]	Loss: 3.6046
Training Epoch: 2 [12288/50176]	Loss: 3.6418
Training Epoch: 2 [13312/50176]	Loss: 3.6245
Training Epoch: 2 [14336/50176]	Loss: 3.6523
Training Epoch: 2 [15360/50176]	Loss: 3.6311
Training Epoch: 2 [16384/50176]	Loss: 3.6885
Training Epoch: 2 [17408/50176]	Loss: 3.6419
Training Epoch: 2 [18432/50176]	Loss: 3.5901
Training Epoch: 2 [19456/50176]	Loss: 3.6659
Training Epoch: 2 [20480/50176]	Loss: 3.5848
Training Epoch: 2 [21504/50176]	Loss: 3.6585
Training Epoch: 2 [22528/50176]	Loss: 3.5842
Training Epoch: 2 [23552/50176]	Loss: 3.5687
Training Epoch: 2 [24576/50176]	Loss: 3.6350
Training Epoch: 2 [25600/50176]	Loss: 3.5333
Training Epoch: 2 [26624/50176]	Loss: 3.5784
Training Epoch: 2 [27648/50176]	Loss: 3.5738
Training Epoch: 2 [28672/50176]	Loss: 3.5948
Training Epoch: 2 [29696/50176]	Loss: 3.6068
Training Epoch: 2 [30720/50176]	Loss: 3.6469
Training Epoch: 2 [31744/50176]	Loss: 3.6973
Training Epoch: 2 [32768/50176]	Loss: 3.7810
Training Epoch: 2 [33792/50176]	Loss: 3.7333
Training Epoch: 2 [34816/50176]	Loss: 3.6225
Training Epoch: 2 [35840/50176]	Loss: 3.5428
Training Epoch: 2 [36864/50176]	Loss: 3.5827
Training Epoch: 2 [37888/50176]	Loss: 3.6228
Training Epoch: 2 [38912/50176]	Loss: 3.5534
Training Epoch: 2 [39936/50176]	Loss: 3.5212
Training Epoch: 2 [40960/50176]	Loss: 3.5581
Training Epoch: 2 [41984/50176]	Loss: 3.5217
Training Epoch: 2 [43008/50176]	Loss: 3.5431
Training Epoch: 2 [44032/50176]	Loss: 3.5029
Training Epoch: 2 [45056/50176]	Loss: 3.5723
Training Epoch: 2 [46080/50176]	Loss: 3.5696
Training Epoch: 2 [47104/50176]	Loss: 3.5060
Training Epoch: 2 [48128/50176]	Loss: 3.5751
Training Epoch: 2 [49152/50176]	Loss: 3.5902
Training Epoch: 2 [50176/50176]	Loss: 3.4494
epoch 3 train time consumed: 91.19s
epoch 3 eval time consumed: 4.50s
Validation Epoch: 2, Average loss: 0.0035, Accuracy: 0.1387
Training Epoch: 3 [1024/50176]	Loss: 3.4441
Training Epoch: 3 [2048/50176]	Loss: 3.4683
Training Epoch: 3 [3072/50176]	Loss: 3.5978
Training Epoch: 3 [4096/50176]	Loss: 3.6223
Training Epoch: 3 [5120/50176]	Loss: 3.5373
Training Epoch: 3 [6144/50176]	Loss: 3.4637
Training Epoch: 3 [7168/50176]	Loss: 3.4514
Training Epoch: 3 [8192/50176]	Loss: 3.4365
Training Epoch: 3 [9216/50176]	Loss: 3.3982
Training Epoch: 3 [10240/50176]	Loss: 3.4296
Training Epoch: 3 [11264/50176]	Loss: 3.4866
Training Epoch: 3 [12288/50176]	Loss: 3.3915
Training Epoch: 3 [13312/50176]	Loss: 3.3997
Training Epoch: 3 [14336/50176]	Loss: 3.3987
Training Epoch: 3 [15360/50176]	Loss: 3.3967
Training Epoch: 3 [16384/50176]	Loss: 3.4979
Training Epoch: 3 [17408/50176]	Loss: 3.4452
Training Epoch: 3 [18432/50176]	Loss: 3.4306
Training Epoch: 3 [19456/50176]	Loss: 3.4114
Training Epoch: 3 [20480/50176]	Loss: 3.3422
Training Epoch: 3 [21504/50176]	Loss: 3.3664
Training Epoch: 3 [22528/50176]	Loss: 3.3316
Training Epoch: 3 [23552/50176]	Loss: 3.4031
Training Epoch: 3 [24576/50176]	Loss: 3.4391
Training Epoch: 3 [25600/50176]	Loss: 3.2770
Training Epoch: 3 [26624/50176]	Loss: 3.3714
Training Epoch: 3 [27648/50176]	Loss: 3.4076
Training Epoch: 3 [28672/50176]	Loss: 3.3701
Training Epoch: 3 [29696/50176]	Loss: 3.3196
Training Epoch: 3 [30720/50176]	Loss: 3.3359
Training Epoch: 3 [31744/50176]	Loss: 3.3711
Training Epoch: 3 [32768/50176]	Loss: 3.3018
Training Epoch: 3 [33792/50176]	Loss: 3.2119
Training Epoch: 3 [34816/50176]	Loss: 3.3270
Training Epoch: 3 [35840/50176]	Loss: 3.3394
Training Epoch: 3 [36864/50176]	Loss: 3.3432
Training Epoch: 3 [37888/50176]	Loss: 3.3537
Training Epoch: 3 [38912/50176]	Loss: 3.4106
Training Epoch: 3 [39936/50176]	Loss: 3.3567
Training Epoch: 3 [40960/50176]	Loss: 3.2911
Training Epoch: 3 [41984/50176]	Loss: 3.3671
Training Epoch: 3 [43008/50176]	Loss: 3.2843
Training Epoch: 3 [44032/50176]	Loss: 3.3208
Training Epoch: 3 [45056/50176]	Loss: 3.2257
Training Epoch: 3 [46080/50176]	Loss: 3.3228
Training Epoch: 3 [47104/50176]	Loss: 3.3006
Training Epoch: 3 [48128/50176]	Loss: 3.5164
Training Epoch: 3 [49152/50176]	Loss: 3.2959
Training Epoch: 3 [50176/50176]	Loss: 3.3655
epoch 4 train time consumed: 91.31s
epoch 4 eval time consumed: 4.53s
Validation Epoch: 3, Average loss: 0.0035, Accuracy: 0.1512
Stopped Zeus monitor 0.
