Files already downloaded and verified
Files already downloaded and verified
Job(cifar100,shufflenetv2,adam,0.6,bs1024~100)
[Training Loop] Testing batch sizes: [128, 256, 512, 1024]
[Training Loop] Testing power limits: [175, 150, 125, 100]
[Training Loop] Testing learning rates: [0.001, 0.005, 0.01]
[Training Loop] Testing dropout rates: [0.0, 0.25, 0.5]
[Training Loop] Reprofiling at accuracy thresholds [0.5, 0.4, 0.3]
[Training Loop] Model's accuracy 0.0 surpasses threshold 0.0! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
Launching Zeus monitor 0...
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6853
Profiling... [256/50048]	Loss: 4.6655
Profiling... [384/50048]	Loss: 4.7404
Profiling... [512/50048]	Loss: 4.7566
Profiling... [640/50048]	Loss: 4.6665
Profiling... [768/50048]	Loss: 4.7975
Profiling... [896/50048]	Loss: 4.7309
Profiling... [1024/50048]	Loss: 4.7454
Profiling... [1152/50048]	Loss: 4.8439
Profiling... [1280/50048]	Loss: 4.7034
Profiling... [1408/50048]	Loss: 4.6447
Profiling... [1536/50048]	Loss: 4.6446
Profiling... [1664/50048]	Loss: 4.6627
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 0, Average loss: 0.0363, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 146.4265549367964,
                        "time": 2.176039729999502,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38507199.06207119
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6101
Profiling... [256/50048]	Loss: 4.7474
Profiling... [384/50048]	Loss: 4.7553
Profiling... [512/50048]	Loss: 4.6768
Profiling... [640/50048]	Loss: 4.5649
Profiling... [768/50048]	Loss: 4.6306
Profiling... [896/50048]	Loss: 4.6742
Profiling... [1024/50048]	Loss: 4.7666
Profiling... [1152/50048]	Loss: 4.6931
Profiling... [1280/50048]	Loss: 4.6969
Profiling... [1408/50048]	Loss: 4.6135
Profiling... [1536/50048]	Loss: 4.6456
Profiling... [1664/50048]	Loss: 4.7096
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 138.18687916609076,
                        "time": 2.175600771000063,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38499431.24361711
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6659
Profiling... [256/50048]	Loss: 4.6737
Profiling... [384/50048]	Loss: 4.7007
Profiling... [512/50048]	Loss: 4.7324
Profiling... [640/50048]	Loss: 4.7135
Profiling... [768/50048]	Loss: 4.7212
Profiling... [896/50048]	Loss: 4.6650
Profiling... [1024/50048]	Loss: 4.6751
Profiling... [1152/50048]	Loss: 4.6159
Profiling... [1280/50048]	Loss: 4.6892
Profiling... [1408/50048]	Loss: 4.5405
Profiling... [1536/50048]	Loss: 4.6888
Profiling... [1664/50048]	Loss: 4.6541
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 135.30381282793604,
                        "time": 2.423169575999964,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 42880408.81689536
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6398
Profiling... [256/50048]	Loss: 4.6649
Profiling... [384/50048]	Loss: 4.6746
Profiling... [512/50048]	Loss: 4.8081
Profiling... [640/50048]	Loss: 4.7438
Profiling... [768/50048]	Loss: 4.7285
Profiling... [896/50048]	Loss: 4.6471
Profiling... [1024/50048]	Loss: 4.6304
Profiling... [1152/50048]	Loss: 4.6487
Profiling... [1280/50048]	Loss: 4.7078
Profiling... [1408/50048]	Loss: 4.6151
Profiling... [1536/50048]	Loss: 4.7116
Profiling... [1664/50048]	Loss: 4.5790
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 124.28605342334296,
                        "time": 5.456548928999837,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 96559089.8475811
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6177
Profiling... [256/50048]	Loss: 4.6680
Profiling... [384/50048]	Loss: 4.7286
Profiling... [512/50048]	Loss: 4.7813
Profiling... [640/50048]	Loss: 4.7008
Profiling... [768/50048]	Loss: 4.8404
Profiling... [896/50048]	Loss: 4.5339
Profiling... [1024/50048]	Loss: 4.7588
Profiling... [1152/50048]	Loss: 4.6269
Profiling... [1280/50048]	Loss: 4.7162
Profiling... [1408/50048]	Loss: 4.7419
Profiling... [1536/50048]	Loss: 4.6931
Profiling... [1664/50048]	Loss: 4.5950
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.18922582978836,
                        "time": 2.1644182520003596,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38301545.38739836
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6676
Profiling... [256/50048]	Loss: 4.5966
Profiling... [384/50048]	Loss: 4.7581
Profiling... [512/50048]	Loss: 4.7082
Profiling... [640/50048]	Loss: 4.7795
Profiling... [768/50048]	Loss: 4.6853
Profiling... [896/50048]	Loss: 4.6500
Profiling... [1024/50048]	Loss: 4.6571
Profiling... [1152/50048]	Loss: 4.6130
Profiling... [1280/50048]	Loss: 4.5678
Profiling... [1408/50048]	Loss: 4.7688
Profiling... [1536/50048]	Loss: 4.7308
Profiling... [1664/50048]	Loss: 4.7055
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.1316103239251,
                        "time": 2.179408545000115,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38566813.612322025
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6519
Profiling... [256/50048]	Loss: 4.6708
Profiling... [384/50048]	Loss: 4.5961
Profiling... [512/50048]	Loss: 4.6499
Profiling... [640/50048]	Loss: 4.7619
Profiling... [768/50048]	Loss: 4.7875
Profiling... [896/50048]	Loss: 4.6625
Profiling... [1024/50048]	Loss: 4.7796
Profiling... [1152/50048]	Loss: 4.8313
Profiling... [1280/50048]	Loss: 4.6706
Profiling... [1408/50048]	Loss: 4.7105
Profiling... [1536/50048]	Loss: 4.6458
Profiling... [1664/50048]	Loss: 4.6247
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.4717245029518,
                        "time": 2.442172912999922,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 43216691.86844662
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.5998
Profiling... [256/50048]	Loss: 4.7629
Profiling... [384/50048]	Loss: 4.6868
Profiling... [512/50048]	Loss: 4.6852
Profiling... [640/50048]	Loss: 4.6775
Profiling... [768/50048]	Loss: 4.7531
Profiling... [896/50048]	Loss: 4.7065
Profiling... [1024/50048]	Loss: 4.7347
Profiling... [1152/50048]	Loss: 4.7037
Profiling... [1280/50048]	Loss: 4.7346
Profiling... [1408/50048]	Loss: 4.6280
Profiling... [1536/50048]	Loss: 4.6935
Profiling... [1664/50048]	Loss: 4.6536
Profile done
epoch 1 train time consumed: 7.09s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.9774038999599,
                        "time": 5.414077661999727,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 95807518.30674715
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6093
Profiling... [256/50048]	Loss: 4.6841
Profiling... [384/50048]	Loss: 4.6441
Profiling... [512/50048]	Loss: 4.7636
Profiling... [640/50048]	Loss: 4.7383
Profiling... [768/50048]	Loss: 4.7759
Profiling... [896/50048]	Loss: 4.7079
Profiling... [1024/50048]	Loss: 4.7244
Profiling... [1152/50048]	Loss: 4.5962
Profiling... [1280/50048]	Loss: 4.6383
Profiling... [1408/50048]	Loss: 4.7284
Profiling... [1536/50048]	Loss: 4.6694
Profiling... [1664/50048]	Loss: 4.6315
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.56778966742536,
                        "time": 2.172703781000564,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38448166.108585976
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6463
Profiling... [256/50048]	Loss: 4.6488
Profiling... [384/50048]	Loss: 4.7286
Profiling... [512/50048]	Loss: 4.6964
Profiling... [640/50048]	Loss: 4.8281
Profiling... [768/50048]	Loss: 4.7059
Profiling... [896/50048]	Loss: 4.6046
Profiling... [1024/50048]	Loss: 4.6688
Profiling... [1152/50048]	Loss: 4.6288
Profiling... [1280/50048]	Loss: 4.6340
Profiling... [1408/50048]	Loss: 4.6123
Profiling... [1536/50048]	Loss: 4.6683
Profiling... [1664/50048]	Loss: 4.6701
Profile done
epoch 1 train time consumed: 3.16s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.28605554417936,
                        "time": 2.1890997299997252,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38738308.822075136
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6009
Profiling... [256/50048]	Loss: 4.6334
Profiling... [384/50048]	Loss: 4.8067
Profiling... [512/50048]	Loss: 4.7277
Profiling... [640/50048]	Loss: 4.7496
Profiling... [768/50048]	Loss: 4.7210
Profiling... [896/50048]	Loss: 4.6509
Profiling... [1024/50048]	Loss: 4.6853
Profiling... [1152/50048]	Loss: 4.5616
Profiling... [1280/50048]	Loss: 4.6561
Profiling... [1408/50048]	Loss: 4.7041
Profiling... [1536/50048]	Loss: 4.6284
Profiling... [1664/50048]	Loss: 4.5759
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 0, Average loss: 0.0363, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.25954747669226,
                        "time": 2.4417502109999987,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 43209211.73385598
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6436
Profiling... [256/50048]	Loss: 4.6503
Profiling... [384/50048]	Loss: 4.6531
Profiling... [512/50048]	Loss: 4.7555
Profiling... [640/50048]	Loss: 4.7464
Profiling... [768/50048]	Loss: 4.6813
Profiling... [896/50048]	Loss: 4.7378
Profiling... [1024/50048]	Loss: 4.6756
Profiling... [1152/50048]	Loss: 4.6489
Profiling... [1280/50048]	Loss: 4.5697
Profiling... [1408/50048]	Loss: 4.7382
Profiling... [1536/50048]	Loss: 4.6389
Profiling... [1664/50048]	Loss: 4.6840
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.93366290959935,
                        "time": 5.459381290999772,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 96609211.32553196
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6119
Profiling... [256/50048]	Loss: 5.3925
Profiling... [384/50048]	Loss: 6.1695
Profiling... [512/50048]	Loss: 5.6178
Profiling... [640/50048]	Loss: 5.2081
Profiling... [768/50048]	Loss: 5.0484
Profiling... [896/50048]	Loss: 4.7319
Profiling... [1024/50048]	Loss: 5.0785
Profiling... [1152/50048]	Loss: 4.6024
Profiling... [1280/50048]	Loss: 4.7839
Profiling... [1408/50048]	Loss: 4.7835
Profiling... [1536/50048]	Loss: 4.6051
Profiling... [1664/50048]	Loss: 5.0872
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0375, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.48997643969395,
                        "time": 2.1787256999996316,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38554729.98719348
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6225
Profiling... [256/50048]	Loss: 5.6150
Profiling... [384/50048]	Loss: 5.8944
Profiling... [512/50048]	Loss: 5.2362
Profiling... [640/50048]	Loss: 5.3351
Profiling... [768/50048]	Loss: 5.2934
Profiling... [896/50048]	Loss: 5.0533
Profiling... [1024/50048]	Loss: 4.7729
Profiling... [1152/50048]	Loss: 4.6295
Profiling... [1280/50048]	Loss: 4.9437
Profiling... [1408/50048]	Loss: 4.8125
Profiling... [1536/50048]	Loss: 4.9332
Profiling... [1664/50048]	Loss: 4.7653
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0371, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.67194302382114,
                        "time": 2.1905995170000097,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38764849.052832164
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6006
Profiling... [256/50048]	Loss: 5.5730
Profiling... [384/50048]	Loss: 6.1388
Profiling... [512/50048]	Loss: 5.6493
Profiling... [640/50048]	Loss: 5.1809
Profiling... [768/50048]	Loss: 4.8390
Profiling... [896/50048]	Loss: 4.9559
Profiling... [1024/50048]	Loss: 4.7114
Profiling... [1152/50048]	Loss: 4.6528
Profiling... [1280/50048]	Loss: 4.6511
Profiling... [1408/50048]	Loss: 5.0656
Profiling... [1536/50048]	Loss: 4.5934
Profiling... [1664/50048]	Loss: 4.7460
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 0, Average loss: 0.0375, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.41009357217827,
                        "time": 2.45947088200046,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 43522796.727880135
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6449
Profiling... [256/50048]	Loss: 5.7302
Profiling... [384/50048]	Loss: 5.9665
Profiling... [512/50048]	Loss: 5.4185
Profiling... [640/50048]	Loss: 5.3093
Profiling... [768/50048]	Loss: 4.8918
Profiling... [896/50048]	Loss: 4.7595
Profiling... [1024/50048]	Loss: 4.8502
Profiling... [1152/50048]	Loss: 4.7730
Profiling... [1280/50048]	Loss: 4.8958
Profiling... [1408/50048]	Loss: 4.8283
Profiling... [1536/50048]	Loss: 4.6329
Profiling... [1664/50048]	Loss: 4.7525
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 0, Average loss: 0.0374, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.48440455099153,
                        "time": 5.431934436999654,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 96123511.79714587
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6233
Profiling... [256/50048]	Loss: 5.7957
Profiling... [384/50048]	Loss: 5.9327
Profiling... [512/50048]	Loss: 5.2112
Profiling... [640/50048]	Loss: 5.1239
Profiling... [768/50048]	Loss: 5.1254
Profiling... [896/50048]	Loss: 4.8425
Profiling... [1024/50048]	Loss: 5.0319
Profiling... [1152/50048]	Loss: 4.8956
Profiling... [1280/50048]	Loss: 4.8104
Profiling... [1408/50048]	Loss: 4.8805
Profiling... [1536/50048]	Loss: 4.7973
Profiling... [1664/50048]	Loss: 4.8231
Profile done
epoch 1 train time consumed: 3.16s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.39720400043647,
                        "time": 2.174539421999725,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38480649.611707136
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6056
Profiling... [256/50048]	Loss: 5.8001
Profiling... [384/50048]	Loss: 6.1692
Profiling... [512/50048]	Loss: 5.3148
Profiling... [640/50048]	Loss: 4.9979
Profiling... [768/50048]	Loss: 4.6595
Profiling... [896/50048]	Loss: 4.6929
Profiling... [1024/50048]	Loss: 4.8880
Profiling... [1152/50048]	Loss: 4.6687
Profiling... [1280/50048]	Loss: 4.7543
Profiling... [1408/50048]	Loss: 4.6366
Profiling... [1536/50048]	Loss: 4.6674
Profiling... [1664/50048]	Loss: 4.9363
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0368, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.30257215099085,
                        "time": 2.1818663260000903,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38610306.504897594
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6278
Profiling... [256/50048]	Loss: 5.6356
Profiling... [384/50048]	Loss: 5.8658
Profiling... [512/50048]	Loss: 5.2656
Profiling... [640/50048]	Loss: 5.1320
Profiling... [768/50048]	Loss: 4.9520
Profiling... [896/50048]	Loss: 4.9964
Profiling... [1024/50048]	Loss: 4.7526
Profiling... [1152/50048]	Loss: 5.1603
Profiling... [1280/50048]	Loss: 4.6323
Profiling... [1408/50048]	Loss: 4.7415
Profiling... [1536/50048]	Loss: 4.6845
Profiling... [1664/50048]	Loss: 4.6998
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 0, Average loss: 0.0371, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.87304056308481,
                        "time": 2.449343487999613,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 43343582.36364114
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6341
Profiling... [256/50048]	Loss: 5.7612
Profiling... [384/50048]	Loss: 5.8227
Profiling... [512/50048]	Loss: 5.3075
Profiling... [640/50048]	Loss: 5.1545
Profiling... [768/50048]	Loss: 5.1283
Profiling... [896/50048]	Loss: 5.0937
Profiling... [1024/50048]	Loss: 4.9212
Profiling... [1152/50048]	Loss: 4.7452
Profiling... [1280/50048]	Loss: 4.6358
Profiling... [1408/50048]	Loss: 4.6235
Profiling... [1536/50048]	Loss: 4.8263
Profiling... [1664/50048]	Loss: 4.8707
Profile done
epoch 1 train time consumed: 7.14s
Validation Epoch: 0, Average loss: 0.0368, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.12398792540571,
                        "time": 5.456971628000247,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 96566569.92909236
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6573
Profiling... [256/50048]	Loss: 5.7586
Profiling... [384/50048]	Loss: 5.8868
Profiling... [512/50048]	Loss: 5.4076
Profiling... [640/50048]	Loss: 5.1066
Profiling... [768/50048]	Loss: 4.7249
Profiling... [896/50048]	Loss: 4.7734
Profiling... [1024/50048]	Loss: 4.8608
Profiling... [1152/50048]	Loss: 5.2945
Profiling... [1280/50048]	Loss: 4.6477
Profiling... [1408/50048]	Loss: 4.9282
Profiling... [1536/50048]	Loss: 4.5964
Profiling... [1664/50048]	Loss: 4.7711
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.27742603428287,
                        "time": 2.1719612229999257,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38435025.80220669
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6401
Profiling... [256/50048]	Loss: 5.7300
Profiling... [384/50048]	Loss: 5.6173
Profiling... [512/50048]	Loss: 5.5054
Profiling... [640/50048]	Loss: 5.3037
Profiling... [768/50048]	Loss: 5.3969
Profiling... [896/50048]	Loss: 5.1742
Profiling... [1024/50048]	Loss: 4.8683
Profiling... [1152/50048]	Loss: 4.7727
Profiling... [1280/50048]	Loss: 4.8634
Profiling... [1408/50048]	Loss: 4.6049
Profiling... [1536/50048]	Loss: 4.7047
Profiling... [1664/50048]	Loss: 4.7176
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.0369, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.02514437358082,
                        "time": 2.1922602869999537,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38794238.03875118
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6095
Profiling... [256/50048]	Loss: 5.7246
Profiling... [384/50048]	Loss: 5.8771
Profiling... [512/50048]	Loss: 5.6631
Profiling... [640/50048]	Loss: 5.0935
Profiling... [768/50048]	Loss: 5.1282
Profiling... [896/50048]	Loss: 5.0009
Profiling... [1024/50048]	Loss: 4.9437
Profiling... [1152/50048]	Loss: 4.7977
Profiling... [1280/50048]	Loss: 4.7618
Profiling... [1408/50048]	Loss: 4.6243
Profiling... [1536/50048]	Loss: 4.6379
Profiling... [1664/50048]	Loss: 4.6116
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 0, Average loss: 0.0371, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.50452217773633,
                        "time": 2.4415734410004006,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 43206083.61194308
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6457
Profiling... [256/50048]	Loss: 5.4560
Profiling... [384/50048]	Loss: 5.9345
Profiling... [512/50048]	Loss: 5.5397
Profiling... [640/50048]	Loss: 5.6067
Profiling... [768/50048]	Loss: 5.2670
Profiling... [896/50048]	Loss: 4.7917
Profiling... [1024/50048]	Loss: 4.8010
Profiling... [1152/50048]	Loss: 4.9752
Profiling... [1280/50048]	Loss: 4.8443
Profiling... [1408/50048]	Loss: 4.7043
Profiling... [1536/50048]	Loss: 4.7839
Profiling... [1664/50048]	Loss: 4.8945
Profile done
epoch 1 train time consumed: 7.14s
Validation Epoch: 0, Average loss: 0.0375, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.94939129942043,
                        "time": 5.436915350000163,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 96211654.03360288
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6136
Profiling... [256/50048]	Loss: 7.7433
Profiling... [384/50048]	Loss: 6.4860
Profiling... [512/50048]	Loss: 5.6553
Profiling... [640/50048]	Loss: 5.0162
Profiling... [768/50048]	Loss: 4.6067
Profiling... [896/50048]	Loss: 4.6479
Profiling... [1024/50048]	Loss: 4.7970
Profiling... [1152/50048]	Loss: 5.0743
Profiling... [1280/50048]	Loss: 4.9528
Profiling... [1408/50048]	Loss: 4.6622
Profiling... [1536/50048]	Loss: 5.0792
Profiling... [1664/50048]	Loss: 4.7644
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0095
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.27291959411434,
                        "time": 2.1618874229998255,
                        "accuracy": 0.00949367088607595,
                        "total_cost": 39850791.49729678
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6106
Profiling... [256/50048]	Loss: 8.2122
Profiling... [384/50048]	Loss: 5.6967
Profiling... [512/50048]	Loss: 6.4194
Profiling... [640/50048]	Loss: 5.7913
Profiling... [768/50048]	Loss: 4.7470
Profiling... [896/50048]	Loss: 5.2012
Profiling... [1024/50048]	Loss: 6.7618
Profiling... [1152/50048]	Loss: 4.8320
Profiling... [1280/50048]	Loss: 4.8464
Profiling... [1408/50048]	Loss: 4.6729
Profiling... [1536/50048]	Loss: 4.8435
Profiling... [1664/50048]	Loss: 4.9912
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.0599, Accuracy: 0.0118
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.89657009015647,
                        "time": 2.1980633120001585,
                        "accuracy": 0.011768196202531646,
                        "total_cost": 32686494.42786118
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6424
Profiling... [256/50048]	Loss: 7.9352
Profiling... [384/50048]	Loss: 6.9089
Profiling... [512/50048]	Loss: 6.1459
Profiling... [640/50048]	Loss: 5.2179
Profiling... [768/50048]	Loss: 4.9237
Profiling... [896/50048]	Loss: 5.4914
Profiling... [1024/50048]	Loss: 5.0733
Profiling... [1152/50048]	Loss: 4.6194
Profiling... [1280/50048]	Loss: 4.7510
Profiling... [1408/50048]	Loss: 5.1209
Profiling... [1536/50048]	Loss: 4.5961
Profiling... [1664/50048]	Loss: 4.7888
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.32299493253693,
                        "time": 2.466775023000082,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 43652050.80700945
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6372
Profiling... [256/50048]	Loss: 8.1474
Profiling... [384/50048]	Loss: 6.1781
Profiling... [512/50048]	Loss: 5.8730
Profiling... [640/50048]	Loss: 5.4135
Profiling... [768/50048]	Loss: 5.4700
Profiling... [896/50048]	Loss: 4.6439
Profiling... [1024/50048]	Loss: 4.9181
Profiling... [1152/50048]	Loss: 5.0779
Profiling... [1280/50048]	Loss: 4.6735
Profiling... [1408/50048]	Loss: 5.0937
Profiling... [1536/50048]	Loss: 4.8751
Profiling... [1664/50048]	Loss: 4.8153
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.85655240545272,
                        "time": 5.033857641000395,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 89079144.81514299
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6469
Profiling... [256/50048]	Loss: 7.6175
Profiling... [384/50048]	Loss: 6.7533
Profiling... [512/50048]	Loss: 5.8641
Profiling... [640/50048]	Loss: 5.3137
Profiling... [768/50048]	Loss: 5.8481
Profiling... [896/50048]	Loss: 5.5914
Profiling... [1024/50048]	Loss: 5.4159
Profiling... [1152/50048]	Loss: 4.9053
Profiling... [1280/50048]	Loss: 5.1546
Profiling... [1408/50048]	Loss: 4.8844
Profiling... [1536/50048]	Loss: 4.9105
Profiling... [1664/50048]	Loss: 4.7143
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.29023307502703,
                        "time": 2.180206867000379,
                        "accuracy": 0.009790348101265823,
                        "total_cost": 38970647.19034213
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6300
Profiling... [256/50048]	Loss: 7.9507
Profiling... [384/50048]	Loss: 7.8121
Profiling... [512/50048]	Loss: 6.4873
Profiling... [640/50048]	Loss: 5.5615
Profiling... [768/50048]	Loss: 4.7891
Profiling... [896/50048]	Loss: 5.1552
Profiling... [1024/50048]	Loss: 5.0093
Profiling... [1152/50048]	Loss: 4.9451
Profiling... [1280/50048]	Loss: 4.9318
Profiling... [1408/50048]	Loss: 4.8378
Profiling... [1536/50048]	Loss: 4.7643
Profiling... [1664/50048]	Loss: 4.5978
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.8602203476403,
                        "time": 2.1916873549998854,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38784099.43407797
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.5990
Profiling... [256/50048]	Loss: 7.7206
Profiling... [384/50048]	Loss: 7.7300
Profiling... [512/50048]	Loss: 5.9678
Profiling... [640/50048]	Loss: 5.4559
Profiling... [768/50048]	Loss: 5.5543
Profiling... [896/50048]	Loss: 4.9356
Profiling... [1024/50048]	Loss: 4.8601
Profiling... [1152/50048]	Loss: 4.7320
Profiling... [1280/50048]	Loss: 5.3216
Profiling... [1408/50048]	Loss: 4.7331
Profiling... [1536/50048]	Loss: 4.5926
Profiling... [1664/50048]	Loss: 4.8573
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 0, Average loss: 0.1486, Accuracy: 0.0156
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.2281441623422,
                        "time": 2.4752045400000497,
                        "accuracy": 0.015625,
                        "total_cost": 27722290.848000556
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6458
Profiling... [256/50048]	Loss: 7.9720
Profiling... [384/50048]	Loss: 5.7319
Profiling... [512/50048]	Loss: 6.2386
Profiling... [640/50048]	Loss: 5.0514
Profiling... [768/50048]	Loss: 5.3191
Profiling... [896/50048]	Loss: 5.6667
Profiling... [1024/50048]	Loss: 6.5562
Profiling... [1152/50048]	Loss: 6.4206
Profiling... [1280/50048]	Loss: 5.3863
Profiling... [1408/50048]	Loss: 4.6608
Profiling... [1536/50048]	Loss: 4.5431
Profiling... [1664/50048]	Loss: 5.0359
Profile done
epoch 1 train time consumed: 6.82s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.83499213257583,
                        "time": 5.093844191999779,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 90140666.82162808
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6114
Profiling... [256/50048]	Loss: 8.1043
Profiling... [384/50048]	Loss: 5.8701
Profiling... [512/50048]	Loss: 7.6112
Profiling... [640/50048]	Loss: 5.6783
Profiling... [768/50048]	Loss: 6.1084
Profiling... [896/50048]	Loss: 5.7480
Profiling... [1024/50048]	Loss: 5.3011
Profiling... [1152/50048]	Loss: 5.1873
Profiling... [1280/50048]	Loss: 4.7309
Profiling... [1408/50048]	Loss: 4.8681
Profiling... [1536/50048]	Loss: 4.9700
Profiling... [1664/50048]	Loss: 5.4143
Profile done
epoch 1 train time consumed: 3.16s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.3394870783905,
                        "time": 2.183398278999448,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38637415.94517423
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6374
Profiling... [256/50048]	Loss: 7.8254
Profiling... [384/50048]	Loss: 7.1771
Profiling... [512/50048]	Loss: 5.1852
Profiling... [640/50048]	Loss: 4.7487
Profiling... [768/50048]	Loss: 4.7105
Profiling... [896/50048]	Loss: 4.8990
Profiling... [1024/50048]	Loss: 5.5423
Profiling... [1152/50048]	Loss: 4.8963
Profiling... [1280/50048]	Loss: 4.9352
Profiling... [1408/50048]	Loss: 4.9205
Profiling... [1536/50048]	Loss: 5.2918
Profiling... [1664/50048]	Loss: 5.1430
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.0443, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.83823922506251,
                        "time": 2.195138254000085,
                        "accuracy": 0.009790348101265823,
                        "total_cost": 39237541.9624096
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6327
Profiling... [256/50048]	Loss: 7.4538
Profiling... [384/50048]	Loss: 6.7433
Profiling... [512/50048]	Loss: 5.7958
Profiling... [640/50048]	Loss: 5.4382
Profiling... [768/50048]	Loss: 5.6643
Profiling... [896/50048]	Loss: 6.3489
Profiling... [1024/50048]	Loss: 4.8748
Profiling... [1152/50048]	Loss: 4.8700
Profiling... [1280/50048]	Loss: 4.7864
Profiling... [1408/50048]	Loss: 5.0503
Profiling... [1536/50048]	Loss: 4.6463
Profiling... [1664/50048]	Loss: 5.5574
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 0, Average loss: 0.0373, Accuracy: 0.0114
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.15651288526563,
                        "time": 2.459649872999762,
                        "accuracy": 0.011372626582278481,
                        "total_cost": 37848664.48052503
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6294
Profiling... [256/50048]	Loss: 7.2980
Profiling... [384/50048]	Loss: 6.5937
Profiling... [512/50048]	Loss: 5.1977
Profiling... [640/50048]	Loss: 4.8998
Profiling... [768/50048]	Loss: 4.7420
Profiling... [896/50048]	Loss: 4.6703
Profiling... [1024/50048]	Loss: 5.3485
Profiling... [1152/50048]	Loss: 4.7647
Profiling... [1280/50048]	Loss: 4.8370
Profiling... [1408/50048]	Loss: 4.7203
Profiling... [1536/50048]	Loss: 4.6483
Profiling... [1664/50048]	Loss: 4.5432
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 0, Average loss: 0.1202, Accuracy: 0.0174
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.81432101963655,
                        "time": 5.07137391499964,
                        "accuracy": 0.01740506329113924,
                        "total_cost": 50990359.54536002
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6329
Profiling... [512/50176]	Loss: 4.6560
Profiling... [768/50176]	Loss: 4.6773
Profiling... [1024/50176]	Loss: 4.6792
Profiling... [1280/50176]	Loss: 4.7974
Profiling... [1536/50176]	Loss: 4.7265
Profiling... [1792/50176]	Loss: 4.6984
Profiling... [2048/50176]	Loss: 4.6042
Profiling... [2304/50176]	Loss: 4.5769
Profiling... [2560/50176]	Loss: 4.5743
Profiling... [2816/50176]	Loss: 4.6086
Profiling... [3072/50176]	Loss: 4.6180
Profiling... [3328/50176]	Loss: 4.5894
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.45127448601698,
                        "time": 2.4426782330001515,
                        "accuracy": 0.009765625,
                        "total_cost": 43772793.93536271
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6143
Profiling... [512/50176]	Loss: 4.6377
Profiling... [768/50176]	Loss: 4.7589
Profiling... [1024/50176]	Loss: 4.6695
Profiling... [1280/50176]	Loss: 4.6580
Profiling... [1536/50176]	Loss: 4.6031
Profiling... [1792/50176]	Loss: 4.6820
Profiling... [2048/50176]	Loss: 4.7277
Profiling... [2304/50176]	Loss: 4.6593
Profiling... [2560/50176]	Loss: 4.5393
Profiling... [2816/50176]	Loss: 4.6231
Profiling... [3072/50176]	Loss: 4.6256
Profiling... [3328/50176]	Loss: 4.6829
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.9385688387464,
                        "time": 2.486603997000202,
                        "accuracy": 0.009765625,
                        "total_cost": 44559943.62624362
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6178
Profiling... [512/50176]	Loss: 4.6691
Profiling... [768/50176]	Loss: 4.7007
Profiling... [1024/50176]	Loss: 4.6207
Profiling... [1280/50176]	Loss: 4.6692
Profiling... [1536/50176]	Loss: 4.6332
Profiling... [1792/50176]	Loss: 4.6341
Profiling... [2048/50176]	Loss: 4.5610
Profiling... [2304/50176]	Loss: 4.5775
Profiling... [2560/50176]	Loss: 4.5834
Profiling... [2816/50176]	Loss: 4.6075
Profiling... [3072/50176]	Loss: 4.6092
Profiling... [3328/50176]	Loss: 4.5268
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.1797053631947,
                        "time": 2.8325679669997044,
                        "accuracy": 0.009765625,
                        "total_cost": 50759617.9686347
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6313
Profiling... [512/50176]	Loss: 4.6221
Profiling... [768/50176]	Loss: 4.6407
Profiling... [1024/50176]	Loss: 4.6651
Profiling... [1280/50176]	Loss: 4.6305
Profiling... [1536/50176]	Loss: 4.7638
Profiling... [1792/50176]	Loss: 4.6426
Profiling... [2048/50176]	Loss: 4.6349
Profiling... [2304/50176]	Loss: 4.6181
Profiling... [2560/50176]	Loss: 4.5514
Profiling... [2816/50176]	Loss: 4.6088
Profiling... [3072/50176]	Loss: 4.6107
Profiling... [3328/50176]	Loss: 4.5703
Profile done
epoch 1 train time consumed: 9.69s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.75363616903485,
                        "time": 7.314462533000551,
                        "accuracy": 0.009765625,
                        "total_cost": 131075168.59136987
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6285
Profiling... [512/50176]	Loss: 4.6565
Profiling... [768/50176]	Loss: 4.6706
Profiling... [1024/50176]	Loss: 4.7123
Profiling... [1280/50176]	Loss: 4.6712
Profiling... [1536/50176]	Loss: 4.7076
Profiling... [1792/50176]	Loss: 4.6790
Profiling... [2048/50176]	Loss: 4.6227
Profiling... [2304/50176]	Loss: 4.6376
Profiling... [2560/50176]	Loss: 4.6143
Profiling... [2816/50176]	Loss: 4.5721
Profiling... [3072/50176]	Loss: 4.6555
Profiling... [3328/50176]	Loss: 4.5538
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.52968727904361,
                        "time": 2.4443279950000942,
                        "accuracy": 0.009765625,
                        "total_cost": 43802357.670401685
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6378
Profiling... [512/50176]	Loss: 4.6656
Profiling... [768/50176]	Loss: 4.6665
Profiling... [1024/50176]	Loss: 4.6285
Profiling... [1280/50176]	Loss: 4.6757
Profiling... [1536/50176]	Loss: 4.6499
Profiling... [1792/50176]	Loss: 4.6940
Profiling... [2048/50176]	Loss: 4.6183
Profiling... [2304/50176]	Loss: 4.6047
Profiling... [2560/50176]	Loss: 4.6401
Profiling... [2816/50176]	Loss: 4.5778
Profiling... [3072/50176]	Loss: 4.5500
Profiling... [3328/50176]	Loss: 4.5515
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.92926670366376,
                        "time": 2.496387756000331,
                        "accuracy": 0.009765625,
                        "total_cost": 44735268.587525934
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6036
Profiling... [512/50176]	Loss: 4.6410
Profiling... [768/50176]	Loss: 4.6432
Profiling... [1024/50176]	Loss: 4.6149
Profiling... [1280/50176]	Loss: 4.6461
Profiling... [1536/50176]	Loss: 4.6238
Profiling... [1792/50176]	Loss: 4.5911
Profiling... [2048/50176]	Loss: 4.6022
Profiling... [2304/50176]	Loss: 4.6351
Profiling... [2560/50176]	Loss: 4.6383
Profiling... [2816/50176]	Loss: 4.6654
Profiling... [3072/50176]	Loss: 4.5848
Profiling... [3328/50176]	Loss: 4.5989
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.15553741217197,
                        "time": 2.8382641459993465,
                        "accuracy": 0.009765625,
                        "total_cost": 50861693.49630829
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6131
Profiling... [512/50176]	Loss: 4.6189
Profiling... [768/50176]	Loss: 4.7099
Profiling... [1024/50176]	Loss: 4.7482
Profiling... [1280/50176]	Loss: 4.6912
Profiling... [1536/50176]	Loss: 4.6952
Profiling... [1792/50176]	Loss: 4.6318
Profiling... [2048/50176]	Loss: 4.6158
Profiling... [2304/50176]	Loss: 4.5892
Profiling... [2560/50176]	Loss: 4.5431
Profiling... [2816/50176]	Loss: 4.5514
Profiling... [3072/50176]	Loss: 4.6279
Profiling... [3328/50176]	Loss: 4.5349
Profile done
epoch 1 train time consumed: 9.39s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.80740286345468,
                        "time": 7.044992845000706,
                        "accuracy": 0.009765625,
                        "total_cost": 126246271.78241265
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6326
Profiling... [512/50176]	Loss: 4.6455
Profiling... [768/50176]	Loss: 4.6865
Profiling... [1024/50176]	Loss: 4.7208
Profiling... [1280/50176]	Loss: 4.7058
Profiling... [1536/50176]	Loss: 4.6833
Profiling... [1792/50176]	Loss: 4.6650
Profiling... [2048/50176]	Loss: 4.5974
Profiling... [2304/50176]	Loss: 4.6271
Profiling... [2560/50176]	Loss: 4.7094
Profiling... [2816/50176]	Loss: 4.6089
Profiling... [3072/50176]	Loss: 4.5954
Profiling... [3328/50176]	Loss: 4.5647
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.58297833394947,
                        "time": 2.41443737300051,
                        "accuracy": 0.009765625,
                        "total_cost": 43266717.72416914
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6356
Profiling... [512/50176]	Loss: 4.6716
Profiling... [768/50176]	Loss: 4.7140
Profiling... [1024/50176]	Loss: 4.6597
Profiling... [1280/50176]	Loss: 4.6879
Profiling... [1536/50176]	Loss: 4.6218
Profiling... [1792/50176]	Loss: 4.5901
Profiling... [2048/50176]	Loss: 4.5782
Profiling... [2304/50176]	Loss: 4.5888
Profiling... [2560/50176]	Loss: 4.5698
Profiling... [2816/50176]	Loss: 4.6375
Profiling... [3072/50176]	Loss: 4.6127
Profiling... [3328/50176]	Loss: 4.5847
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.96945888964127,
                        "time": 2.4748052969998753,
                        "accuracy": 0.009765625,
                        "total_cost": 44348510.92223777
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6567
Profiling... [512/50176]	Loss: 4.6536
Profiling... [768/50176]	Loss: 4.7162
Profiling... [1024/50176]	Loss: 4.7302
Profiling... [1280/50176]	Loss: 4.6535
Profiling... [1536/50176]	Loss: 4.6729
Profiling... [1792/50176]	Loss: 4.6335
Profiling... [2048/50176]	Loss: 4.5458
Profiling... [2304/50176]	Loss: 4.6575
Profiling... [2560/50176]	Loss: 4.6630
Profiling... [2816/50176]	Loss: 4.5952
Profiling... [3072/50176]	Loss: 4.5451
Profiling... [3328/50176]	Loss: 4.5850
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.16094016612251,
                        "time": 2.8186292079999475,
                        "accuracy": 0.009765625,
                        "total_cost": 50509835.407359056
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6215
Profiling... [512/50176]	Loss: 4.6978
Profiling... [768/50176]	Loss: 4.6961
Profiling... [1024/50176]	Loss: 4.6690
Profiling... [1280/50176]	Loss: 4.7141
Profiling... [1536/50176]	Loss: 4.7187
Profiling... [1792/50176]	Loss: 4.7309
Profiling... [2048/50176]	Loss: 4.6508
Profiling... [2304/50176]	Loss: 4.6586
Profiling... [2560/50176]	Loss: 4.6423
Profiling... [2816/50176]	Loss: 4.6074
Profiling... [3072/50176]	Loss: 4.6006
Profiling... [3328/50176]	Loss: 4.5231
Profile done
epoch 1 train time consumed: 9.42s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.83105260557936,
                        "time": 7.090394989999368,
                        "accuracy": 0.009765625,
                        "total_cost": 127059878.22078867
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6120
Profiling... [512/50176]	Loss: 5.5841
Profiling... [768/50176]	Loss: 5.4201
Profiling... [1024/50176]	Loss: 5.3086
Profiling... [1280/50176]	Loss: 4.9701
Profiling... [1536/50176]	Loss: 4.7021
Profiling... [1792/50176]	Loss: 4.7885
Profiling... [2048/50176]	Loss: 4.7637
Profiling... [2304/50176]	Loss: 4.7347
Profiling... [2560/50176]	Loss: 4.6988
Profiling... [2816/50176]	Loss: 4.6114
Profiling... [3072/50176]	Loss: 4.5759
Profiling... [3328/50176]	Loss: 4.6098
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0188, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.62211697050299,
                        "time": 2.43300089999957,
                        "accuracy": 0.009765625,
                        "total_cost": 43599376.12799229
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6510
Profiling... [512/50176]	Loss: 5.7619
Profiling... [768/50176]	Loss: 5.5407
Profiling... [1024/50176]	Loss: 5.1134
Profiling... [1280/50176]	Loss: 4.8861
Profiling... [1536/50176]	Loss: 4.9212
Profiling... [1792/50176]	Loss: 4.8931
Profiling... [2048/50176]	Loss: 4.8065
Profiling... [2304/50176]	Loss: 4.7206
Profiling... [2560/50176]	Loss: 4.5663
Profiling... [2816/50176]	Loss: 4.5395
Profiling... [3072/50176]	Loss: 4.5735
Profiling... [3328/50176]	Loss: 4.6076
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 0, Average loss: 0.0191, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.98336815242948,
                        "time": 2.4801853620001566,
                        "accuracy": 0.009765625,
                        "total_cost": 44444921.68704281
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6429
Profiling... [512/50176]	Loss: 5.7464
Profiling... [768/50176]	Loss: 5.5895
Profiling... [1024/50176]	Loss: 5.1319
Profiling... [1280/50176]	Loss: 4.8391
Profiling... [1536/50176]	Loss: 4.8400
Profiling... [1792/50176]	Loss: 4.6636
Profiling... [2048/50176]	Loss: 4.7656
Profiling... [2304/50176]	Loss: 4.6874
Profiling... [2560/50176]	Loss: 4.6704
Profiling... [2816/50176]	Loss: 4.9211
Profiling... [3072/50176]	Loss: 4.9225
Profiling... [3328/50176]	Loss: 4.7708
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.16263920124773,
                        "time": 2.84486159700009,
                        "accuracy": 0.009765625,
                        "total_cost": 50979919.81824161
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6549
Profiling... [512/50176]	Loss: 5.4927
Profiling... [768/50176]	Loss: 5.4890
Profiling... [1024/50176]	Loss: 5.0576
Profiling... [1280/50176]	Loss: 4.8449
Profiling... [1536/50176]	Loss: 4.9833
Profiling... [1792/50176]	Loss: 4.7960
Profiling... [2048/50176]	Loss: 4.6132
Profiling... [2304/50176]	Loss: 4.7293
Profiling... [2560/50176]	Loss: 4.7323
Profiling... [2816/50176]	Loss: 4.5767
Profiling... [3072/50176]	Loss: 4.6381
Profiling... [3328/50176]	Loss: 4.8641
Profile done
epoch 1 train time consumed: 9.58s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.82916178049312,
                        "time": 7.224483422999583,
                        "accuracy": 0.009765625,
                        "total_cost": 129462742.94015253
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6238
Profiling... [512/50176]	Loss: 5.8794
Profiling... [768/50176]	Loss: 5.5185
Profiling... [1024/50176]	Loss: 5.1348
Profiling... [1280/50176]	Loss: 4.9565
Profiling... [1536/50176]	Loss: 4.7724
Profiling... [1792/50176]	Loss: 4.8788
Profiling... [2048/50176]	Loss: 4.6456
Profiling... [2304/50176]	Loss: 4.6917
Profiling... [2560/50176]	Loss: 4.7605
Profiling... [2816/50176]	Loss: 4.6435
Profiling... [3072/50176]	Loss: 4.6737
Profiling... [3328/50176]	Loss: 4.5517
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 0, Average loss: 0.0190, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.65436948588545,
                        "time": 2.498466209999606,
                        "accuracy": 0.009765625,
                        "total_cost": 44772514.483192936
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6088
Profiling... [512/50176]	Loss: 5.7832
Profiling... [768/50176]	Loss: 5.0918
Profiling... [1024/50176]	Loss: 4.9213
Profiling... [1280/50176]	Loss: 4.7643
Profiling... [1536/50176]	Loss: 4.8478
Profiling... [1792/50176]	Loss: 4.9504
Profiling... [2048/50176]	Loss: 4.7348
Profiling... [2304/50176]	Loss: 4.7411
Profiling... [2560/50176]	Loss: 4.7462
Profiling... [2816/50176]	Loss: 4.6381
Profiling... [3072/50176]	Loss: 4.6715
Profiling... [3328/50176]	Loss: 4.7036
Profile done
epoch 1 train time consumed: 3.84s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.96541945485635,
                        "time": 2.5889715889998115,
                        "accuracy": 0.009765625,
                        "total_cost": 46394370.874876626
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6377
Profiling... [512/50176]	Loss: 5.7076
Profiling... [768/50176]	Loss: 5.7084
Profiling... [1024/50176]	Loss: 5.0637
Profiling... [1280/50176]	Loss: 4.9517
Profiling... [1536/50176]	Loss: 4.9636
Profiling... [1792/50176]	Loss: 5.1459
Profiling... [2048/50176]	Loss: 4.9845
Profiling... [2304/50176]	Loss: 4.8612
Profiling... [2560/50176]	Loss: 4.7164
Profiling... [2816/50176]	Loss: 4.6849
Profiling... [3072/50176]	Loss: 4.6295
Profiling... [3328/50176]	Loss: 4.5759
Profile done
epoch 1 train time consumed: 4.09s
Validation Epoch: 0, Average loss: 0.0195, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.14537124286058,
                        "time": 2.88136800600023,
                        "accuracy": 0.009765625,
                        "total_cost": 51634114.667524114
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6408
Profiling... [512/50176]	Loss: 5.5484
Profiling... [768/50176]	Loss: 5.3338
Profiling... [1024/50176]	Loss: 5.1133
Profiling... [1280/50176]	Loss: 4.8605
Profiling... [1536/50176]	Loss: 4.6607
Profiling... [1792/50176]	Loss: 4.6852
Profiling... [2048/50176]	Loss: 4.8897
Profiling... [2304/50176]	Loss: 4.7312
Profiling... [2560/50176]	Loss: 4.5742
Profiling... [2816/50176]	Loss: 4.7307
Profiling... [3072/50176]	Loss: 4.6137
Profiling... [3328/50176]	Loss: 4.5673
Profile done
epoch 1 train time consumed: 9.97s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.79870157169275,
                        "time": 7.272021937000318,
                        "accuracy": 0.009765625,
                        "total_cost": 130314633.11104569
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.5895
Profiling... [512/50176]	Loss: 5.8433
Profiling... [768/50176]	Loss: 5.5522
Profiling... [1024/50176]	Loss: 5.1884
Profiling... [1280/50176]	Loss: 4.8197
Profiling... [1536/50176]	Loss: 4.8385
Profiling... [1792/50176]	Loss: 4.8407
Profiling... [2048/50176]	Loss: 4.8380
Profiling... [2304/50176]	Loss: 4.7428
Profiling... [2560/50176]	Loss: 4.7149
Profiling... [2816/50176]	Loss: 4.6511
Profiling... [3072/50176]	Loss: 4.7548
Profiling... [3328/50176]	Loss: 4.6933
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.64309233389129,
                        "time": 2.496653215999686,
                        "accuracy": 0.009765625,
                        "total_cost": 44740025.63071437
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6289
Profiling... [512/50176]	Loss: 5.5535
Profiling... [768/50176]	Loss: 5.5232
Profiling... [1024/50176]	Loss: 5.1720
Profiling... [1280/50176]	Loss: 4.7904
Profiling... [1536/50176]	Loss: 4.8408
Profiling... [1792/50176]	Loss: 4.8386
Profiling... [2048/50176]	Loss: 4.7330
Profiling... [2304/50176]	Loss: 4.7396
Profiling... [2560/50176]	Loss: 4.6862
Profiling... [2816/50176]	Loss: 4.6417
Profiling... [3072/50176]	Loss: 4.7387
Profiling... [3328/50176]	Loss: 4.5456
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 0, Average loss: 0.0188, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.92988417089664,
                        "time": 2.572084112999619,
                        "accuracy": 0.009765625,
                        "total_cost": 46091747.30495317
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6284
Profiling... [512/50176]	Loss: 5.5363
Profiling... [768/50176]	Loss: 5.4337
Profiling... [1024/50176]	Loss: 5.3053
Profiling... [1280/50176]	Loss: 4.9783
Profiling... [1536/50176]	Loss: 4.9651
Profiling... [1792/50176]	Loss: 4.7824
Profiling... [2048/50176]	Loss: 4.6953
Profiling... [2304/50176]	Loss: 4.8162
Profiling... [2560/50176]	Loss: 4.7312
Profiling... [2816/50176]	Loss: 4.6360
Profiling... [3072/50176]	Loss: 4.6061
Profiling... [3328/50176]	Loss: 4.5827
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.09202597304161,
                        "time": 2.8655838469994706,
                        "accuracy": 0.009765625,
                        "total_cost": 51351262.538230516
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6507
Profiling... [512/50176]	Loss: 5.4658
Profiling... [768/50176]	Loss: 5.5341
Profiling... [1024/50176]	Loss: 5.2590
Profiling... [1280/50176]	Loss: 4.9855
Profiling... [1536/50176]	Loss: 4.9824
Profiling... [1792/50176]	Loss: 4.7253
Profiling... [2048/50176]	Loss: 4.7052
Profiling... [2304/50176]	Loss: 4.6892
Profiling... [2560/50176]	Loss: 4.8231
Profiling... [2816/50176]	Loss: 4.6777
Profiling... [3072/50176]	Loss: 4.5713
Profiling... [3328/50176]	Loss: 4.7033
Profile done
epoch 1 train time consumed: 9.75s
Validation Epoch: 0, Average loss: 0.0189, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.78113107612543,
                        "time": 7.290572289000011,
                        "accuracy": 0.009765625,
                        "total_cost": 130647055.4188802
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6023
Profiling... [512/50176]	Loss: 7.6022
Profiling... [768/50176]	Loss: 7.0448
Profiling... [1024/50176]	Loss: 6.4183
Profiling... [1280/50176]	Loss: 5.4730
Profiling... [1536/50176]	Loss: 5.0039
Profiling... [1792/50176]	Loss: 4.9236
Profiling... [2048/50176]	Loss: 4.7086
Profiling... [2304/50176]	Loss: 4.7640
Profiling... [2560/50176]	Loss: 4.7507
Profiling... [2816/50176]	Loss: 4.6982
Profiling... [3072/50176]	Loss: 4.7959
Profiling... [3328/50176]	Loss: 4.8099
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.61657416340348,
                        "time": 2.504422419000548,
                        "accuracy": 0.009765625,
                        "total_cost": 44879249.74848982
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6379
Profiling... [512/50176]	Loss: 7.1910
Profiling... [768/50176]	Loss: 6.4432
Profiling... [1024/50176]	Loss: 5.4508
Profiling... [1280/50176]	Loss: 5.0725
Profiling... [1536/50176]	Loss: 5.5383
Profiling... [1792/50176]	Loss: 4.8498
Profiling... [2048/50176]	Loss: 4.6156
Profiling... [2304/50176]	Loss: 4.6003
Profiling... [2560/50176]	Loss: 5.0847
Profiling... [2816/50176]	Loss: 4.8670
Profiling... [3072/50176]	Loss: 5.0724
Profiling... [3328/50176]	Loss: 5.0946
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0095
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.88910084376741,
                        "time": 2.4848843690006106,
                        "accuracy": 0.00947265625,
                        "total_cost": 45906317.41493912
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6417
Profiling... [512/50176]	Loss: 7.1371
Profiling... [768/50176]	Loss: 6.6722
Profiling... [1024/50176]	Loss: 5.1870
Profiling... [1280/50176]	Loss: 5.2821
Profiling... [1536/50176]	Loss: 4.9032
Profiling... [1792/50176]	Loss: 4.9276
Profiling... [2048/50176]	Loss: 5.2716
Profiling... [2304/50176]	Loss: 4.7741
Profiling... [2560/50176]	Loss: 4.6827
Profiling... [2816/50176]	Loss: 4.8558
Profiling... [3072/50176]	Loss: 4.5654
Profiling... [3328/50176]	Loss: 4.7233
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0102
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.04134400061506,
                        "time": 2.830791321000106,
                        "accuracy": 0.01015625,
                        "total_cost": 48776711.99261721
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.5875
Profiling... [512/50176]	Loss: 7.8887
Profiling... [768/50176]	Loss: 5.7858
Profiling... [1024/50176]	Loss: 4.9871
Profiling... [1280/50176]	Loss: 4.8704
Profiling... [1536/50176]	Loss: 4.7189
Profiling... [1792/50176]	Loss: 4.8969
Profiling... [2048/50176]	Loss: 4.6366
Profiling... [2304/50176]	Loss: 4.6944
Profiling... [2560/50176]	Loss: 4.6474
Profiling... [2816/50176]	Loss: 4.6380
Profiling... [3072/50176]	Loss: 4.6616
Profiling... [3328/50176]	Loss: 4.7257
Profile done
epoch 1 train time consumed: 9.67s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0110
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.77757665713708,
                        "time": 7.1999384050004664,
                        "accuracy": 0.01103515625,
                        "total_cost": 114179554.17487465
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6192
Profiling... [512/50176]	Loss: 7.1976
Profiling... [768/50176]	Loss: 6.1969
Profiling... [1024/50176]	Loss: 5.7981
Profiling... [1280/50176]	Loss: 5.1719
Profiling... [1536/50176]	Loss: 4.6873
Profiling... [1792/50176]	Loss: 4.7332
Profiling... [2048/50176]	Loss: 4.6752
Profiling... [2304/50176]	Loss: 4.6499
Profiling... [2560/50176]	Loss: 4.8726
Profiling... [2816/50176]	Loss: 4.7486
Profiling... [3072/50176]	Loss: 4.6156
Profiling... [3328/50176]	Loss: 4.8456
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0096
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.63933165745358,
                        "time": 2.4144660640004076,
                        "accuracy": 0.0095703125,
                        "total_cost": 44150236.59886459
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6555
Profiling... [512/50176]	Loss: 7.2071
Profiling... [768/50176]	Loss: 6.0347
Profiling... [1024/50176]	Loss: 5.0597
Profiling... [1280/50176]	Loss: 5.1609
Profiling... [1536/50176]	Loss: 4.9424
Profiling... [1792/50176]	Loss: 4.6475
Profiling... [2048/50176]	Loss: 4.8201
Profiling... [2304/50176]	Loss: 4.7778
Profiling... [2560/50176]	Loss: 4.7911
Profiling... [2816/50176]	Loss: 4.7457
Profiling... [3072/50176]	Loss: 4.5736
Profiling... [3328/50176]	Loss: 4.8182
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 0, Average loss: 0.0198, Accuracy: 0.0116
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.89387657701678,
                        "time": 2.465972478999902,
                        "accuracy": 0.01162109375,
                        "total_cost": 37134644.389645584
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6155
Profiling... [512/50176]	Loss: 7.7011
Profiling... [768/50176]	Loss: 5.9550
Profiling... [1024/50176]	Loss: 4.9736
Profiling... [1280/50176]	Loss: 4.8139
Profiling... [1536/50176]	Loss: 4.7890
Profiling... [1792/50176]	Loss: 4.7573
Profiling... [2048/50176]	Loss: 4.7781
Profiling... [2304/50176]	Loss: 4.7801
Profiling... [2560/50176]	Loss: 4.8711
Profiling... [2816/50176]	Loss: 4.7645
Profiling... [3072/50176]	Loss: 4.5829
Profiling... [3328/50176]	Loss: 4.6098
Profile done
epoch 1 train time consumed: 4.04s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0104
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.03492346826006,
                        "time": 2.8338534519998575,
                        "accuracy": 0.01044921875,
                        "total_cost": 47460424.16807239
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6444
Profiling... [512/50176]	Loss: 7.7370
Profiling... [768/50176]	Loss: 6.4828
Profiling... [1024/50176]	Loss: 4.8680
Profiling... [1280/50176]	Loss: 5.0732
Profiling... [1536/50176]	Loss: 5.0190
Profiling... [1792/50176]	Loss: 4.9525
Profiling... [2048/50176]	Loss: 4.7325
Profiling... [2304/50176]	Loss: 4.7440
Profiling... [2560/50176]	Loss: 4.6434
Profiling... [2816/50176]	Loss: 4.7555
Profiling... [3072/50176]	Loss: 4.7359
Profiling... [3328/50176]	Loss: 4.8963
Profile done
epoch 1 train time consumed: 9.71s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.77083923380304,
                        "time": 6.985229738000271,
                        "accuracy": 0.009765625,
                        "total_cost": 125175316.90496485
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6308
Profiling... [512/50176]	Loss: 7.4917
Profiling... [768/50176]	Loss: 6.5707
Profiling... [1024/50176]	Loss: 5.3914
Profiling... [1280/50176]	Loss: 5.0558
Profiling... [1536/50176]	Loss: 4.8196
Profiling... [1792/50176]	Loss: 4.6822
Profiling... [2048/50176]	Loss: 4.8028
Profiling... [2304/50176]	Loss: 4.6753
Profiling... [2560/50176]	Loss: 4.7189
Profiling... [2816/50176]	Loss: 4.6261
Profiling... [3072/50176]	Loss: 4.5919
Profiling... [3328/50176]	Loss: 4.6784
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 0, Average loss: 0.1421, Accuracy: 0.0120
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.66278951070481,
                        "time": 2.4117250350000177,
                        "accuracy": 0.01201171875,
                        "total_cost": 35136676.932683185
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6075
Profiling... [512/50176]	Loss: 7.2687
Profiling... [768/50176]	Loss: 6.0475
Profiling... [1024/50176]	Loss: 5.1719
Profiling... [1280/50176]	Loss: 4.7192
Profiling... [1536/50176]	Loss: 5.6533
Profiling... [1792/50176]	Loss: 6.1222
Profiling... [2048/50176]	Loss: 4.7117
Profiling... [2304/50176]	Loss: 4.9509
Profiling... [2560/50176]	Loss: 4.9765
Profiling... [2816/50176]	Loss: 4.9421
Profiling... [3072/50176]	Loss: 4.7913
Profiling... [3328/50176]	Loss: 4.6582
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.90155938211865,
                        "time": 2.497982822999802,
                        "accuracy": 0.009765625,
                        "total_cost": 44763852.18815645
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6238
Profiling... [512/50176]	Loss: 7.6626
Profiling... [768/50176]	Loss: 6.6583
Profiling... [1024/50176]	Loss: 5.4418
Profiling... [1280/50176]	Loss: 4.8547
Profiling... [1536/50176]	Loss: 4.8899
Profiling... [1792/50176]	Loss: 4.8593
Profiling... [2048/50176]	Loss: 4.6836
Profiling... [2304/50176]	Loss: 4.6842
Profiling... [2560/50176]	Loss: 4.6887
Profiling... [2816/50176]	Loss: 4.7184
Profiling... [3072/50176]	Loss: 4.6037
Profiling... [3328/50176]	Loss: 4.7105
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 0, Average loss: 0.0181, Accuracy: 0.0104
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.02107741035222,
                        "time": 2.848888352999893,
                        "accuracy": 0.01044921875,
                        "total_cost": 47712223.63154961
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6282
Profiling... [512/50176]	Loss: 7.3693
Profiling... [768/50176]	Loss: 6.4645
Profiling... [1024/50176]	Loss: 5.4372
Profiling... [1280/50176]	Loss: 4.6475
Profiling... [1536/50176]	Loss: 5.1032
Profiling... [1792/50176]	Loss: 4.8969
Profiling... [2048/50176]	Loss: 4.6999
Profiling... [2304/50176]	Loss: 4.7978
Profiling... [2560/50176]	Loss: 4.6831
Profiling... [2816/50176]	Loss: 5.3007
Profiling... [3072/50176]	Loss: 4.7357
Profiling... [3328/50176]	Loss: 4.6299
Profile done
epoch 1 train time consumed: 9.77s
Validation Epoch: 0, Average loss: 0.0883, Accuracy: 0.0165
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.7810424840091,
                        "time": 7.100716373999603,
                        "accuracy": 0.01650390625,
                        "total_cost": 75292803.20832716
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6361
Profiling... [1024/50176]	Loss: 4.6344
Profiling... [1536/50176]	Loss: 4.6439
Profiling... [2048/50176]	Loss: 4.6576
Profiling... [2560/50176]	Loss: 4.6144
Profiling... [3072/50176]	Loss: 4.6491
Profiling... [3584/50176]	Loss: 4.5567
Profiling... [4096/50176]	Loss: 4.5861
Profiling... [4608/50176]	Loss: 4.5442
Profiling... [5120/50176]	Loss: 4.5673
Profiling... [5632/50176]	Loss: 4.5364
Profiling... [6144/50176]	Loss: 4.5784
Profiling... [6656/50176]	Loss: 4.5165
Profile done
epoch 1 train time consumed: 6.81s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.80308173715414,
                        "time": 4.608421178999379,
                        "accuracy": 0.009765625,
                        "total_cost": 82582907.52766886
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6378
Profiling... [1024/50176]	Loss: 4.6488
Profiling... [1536/50176]	Loss: 4.6471
Profiling... [2048/50176]	Loss: 4.6280
Profiling... [2560/50176]	Loss: 4.6078
Profiling... [3072/50176]	Loss: 4.5863
Profiling... [3584/50176]	Loss: 4.5434
Profiling... [4096/50176]	Loss: 4.5470
Profiling... [4608/50176]	Loss: 4.5390
Profiling... [5120/50176]	Loss: 4.5821
Profiling... [5632/50176]	Loss: 4.6204
Profiling... [6144/50176]	Loss: 4.5323
Profiling... [6656/50176]	Loss: 4.4754
Profile done
epoch 1 train time consumed: 6.70s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.14705589409856,
                        "time": 4.723592151999583,
                        "accuracy": 0.009765625,
                        "total_cost": 84646771.36383252
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6364
Profiling... [1024/50176]	Loss: 4.6588
Profiling... [1536/50176]	Loss: 4.6103
Profiling... [2048/50176]	Loss: 4.6211
Profiling... [2560/50176]	Loss: 4.5995
Profiling... [3072/50176]	Loss: 4.6195
Profiling... [3584/50176]	Loss: 4.6184
Profiling... [4096/50176]	Loss: 4.5617
Profiling... [4608/50176]	Loss: 4.5788
Profiling... [5120/50176]	Loss: 4.5299
Profiling... [5632/50176]	Loss: 4.5808
Profiling... [6144/50176]	Loss: 4.5559
Profiling... [6656/50176]	Loss: 4.5339
Profile done
epoch 1 train time consumed: 7.78s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.25900169928394,
                        "time": 5.456719666999561,
                        "accuracy": 0.009765625,
                        "total_cost": 97784416.43263213
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6310
Profiling... [1024/50176]	Loss: 4.6511
Profiling... [1536/50176]	Loss: 4.6333
Profiling... [2048/50176]	Loss: 4.6485
Profiling... [2560/50176]	Loss: 4.6009
Profiling... [3072/50176]	Loss: 4.6185
Profiling... [3584/50176]	Loss: 4.5849
Profiling... [4096/50176]	Loss: 4.5666
Profiling... [4608/50176]	Loss: 4.6205
Profiling... [5120/50176]	Loss: 4.5669
Profiling... [5632/50176]	Loss: 4.5378
Profiling... [6144/50176]	Loss: 4.5234
Profiling... [6656/50176]	Loss: 4.4897
Profile done
epoch 1 train time consumed: 17.43s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 117.8562997819756,
                        "time": 13.01719709200006,
                        "accuracy": 0.009765625,
                        "total_cost": 233268171.88864106
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6302
Profiling... [1024/50176]	Loss: 4.6558
Profiling... [1536/50176]	Loss: 4.6562
Profiling... [2048/50176]	Loss: 4.6328
Profiling... [2560/50176]	Loss: 4.6472
Profiling... [3072/50176]	Loss: 4.5987
Profiling... [3584/50176]	Loss: 4.5811
Profiling... [4096/50176]	Loss: 4.5623
Profiling... [4608/50176]	Loss: 4.5612
Profiling... [5120/50176]	Loss: 4.5836
Profiling... [5632/50176]	Loss: 4.5813
Profiling... [6144/50176]	Loss: 4.5533
Profiling... [6656/50176]	Loss: 4.5570
Profile done
epoch 1 train time consumed: 6.77s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.91334533453066,
                        "time": 4.600859558000593,
                        "accuracy": 0.009765625,
                        "total_cost": 82447403.27937064
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6555
Profiling... [1024/50176]	Loss: 4.6312
Profiling... [1536/50176]	Loss: 4.6580
Profiling... [2048/50176]	Loss: 4.6136
Profiling... [2560/50176]	Loss: 4.6094
Profiling... [3072/50176]	Loss: 4.5845
Profiling... [3584/50176]	Loss: 4.6051
Profiling... [4096/50176]	Loss: 4.5745
Profiling... [4608/50176]	Loss: 4.5692
Profiling... [5120/50176]	Loss: 4.6038
Profiling... [5632/50176]	Loss: 4.5259
Profiling... [6144/50176]	Loss: 4.5334
Profiling... [6656/50176]	Loss: 4.4919
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.2168094451241,
                        "time": 4.733581999999842,
                        "accuracy": 0.009765625,
                        "total_cost": 84825789.43999717
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6246
Profiling... [1024/50176]	Loss: 4.6344
Profiling... [1536/50176]	Loss: 4.6125
Profiling... [2048/50176]	Loss: 4.6726
Profiling... [2560/50176]	Loss: 4.5893
Profiling... [3072/50176]	Loss: 4.5952
Profiling... [3584/50176]	Loss: 4.6216
Profiling... [4096/50176]	Loss: 4.5496
Profiling... [4608/50176]	Loss: 4.5473
Profiling... [5120/50176]	Loss: 4.5511
Profiling... [5632/50176]	Loss: 4.5668
Profiling... [6144/50176]	Loss: 4.5655
Profiling... [6656/50176]	Loss: 4.4885
Profile done
epoch 1 train time consumed: 7.57s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.32784616152941,
                        "time": 5.445568070999798,
                        "accuracy": 0.009765625,
                        "total_cost": 97584579.83231637
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6417
Profiling... [1024/50176]	Loss: 4.6196
Profiling... [1536/50176]	Loss: 4.6468
Profiling... [2048/50176]	Loss: 4.5832
Profiling... [2560/50176]	Loss: 4.7078
Profiling... [3072/50176]	Loss: 4.6445
Profiling... [3584/50176]	Loss: 4.5806
Profiling... [4096/50176]	Loss: 4.5697
Profiling... [4608/50176]	Loss: 4.5442
Profiling... [5120/50176]	Loss: 4.5705
Profiling... [5632/50176]	Loss: 4.5601
Profiling... [6144/50176]	Loss: 4.5532
Profiling... [6656/50176]	Loss: 4.5810
Profile done
epoch 1 train time consumed: 17.65s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.9153880671561,
                        "time": 12.996511235999606,
                        "accuracy": 0.009765625,
                        "total_cost": 232897481.34911293
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6327
Profiling... [1024/50176]	Loss: 4.6237
Profiling... [1536/50176]	Loss: 4.6289
Profiling... [2048/50176]	Loss: 4.5974
Profiling... [2560/50176]	Loss: 4.6074
Profiling... [3072/50176]	Loss: 4.5919
Profiling... [3584/50176]	Loss: 4.5903
Profiling... [4096/50176]	Loss: 4.5843
Profiling... [4608/50176]	Loss: 4.5338
Profiling... [5120/50176]	Loss: 4.5364
Profiling... [5632/50176]	Loss: 4.5766
Profiling... [6144/50176]	Loss: 4.5278
Profiling... [6656/50176]	Loss: 4.4938
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.97383827267151,
                        "time": 4.563656911999715,
                        "accuracy": 0.009765625,
                        "total_cost": 81780731.86303489
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6168
Profiling... [1024/50176]	Loss: 4.6603
Profiling... [1536/50176]	Loss: 4.6383
Profiling... [2048/50176]	Loss: 4.6094
Profiling... [2560/50176]	Loss: 4.5946
Profiling... [3072/50176]	Loss: 4.5859
Profiling... [3584/50176]	Loss: 4.6054
Profiling... [4096/50176]	Loss: 4.5615
Profiling... [4608/50176]	Loss: 4.5543
Profiling... [5120/50176]	Loss: 4.5277
Profiling... [5632/50176]	Loss: 4.5167
Profiling... [6144/50176]	Loss: 4.5495
Profiling... [6656/50176]	Loss: 4.4846
Profile done
epoch 1 train time consumed: 6.73s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.25014669841735,
                        "time": 4.710066052999537,
                        "accuracy": 0.009765625,
                        "total_cost": 84404383.66975169
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6349
Profiling... [1024/50176]	Loss: 4.6520
Profiling... [1536/50176]	Loss: 4.6610
Profiling... [2048/50176]	Loss: 4.5996
Profiling... [2560/50176]	Loss: 4.6150
Profiling... [3072/50176]	Loss: 4.5808
Profiling... [3584/50176]	Loss: 4.5886
Profiling... [4096/50176]	Loss: 4.5708
Profiling... [4608/50176]	Loss: 4.5771
Profiling... [5120/50176]	Loss: 4.5568
Profiling... [5632/50176]	Loss: 4.5462
Profiling... [6144/50176]	Loss: 4.5634
Profiling... [6656/50176]	Loss: 4.5484
Profile done
epoch 1 train time consumed: 7.71s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.34299320003181,
                        "time": 5.438603880999835,
                        "accuracy": 0.009765625,
                        "total_cost": 97459781.54751705
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6235
Profiling... [1024/50176]	Loss: 4.6573
Profiling... [1536/50176]	Loss: 4.6385
Profiling... [2048/50176]	Loss: 4.6642
Profiling... [2560/50176]	Loss: 4.6132
Profiling... [3072/50176]	Loss: 4.5975
Profiling... [3584/50176]	Loss: 4.5892
Profiling... [4096/50176]	Loss: 4.5383
Profiling... [4608/50176]	Loss: 4.5113
Profiling... [5120/50176]	Loss: 4.5149
Profiling... [5632/50176]	Loss: 4.5243
Profiling... [6144/50176]	Loss: 4.5367
Profiling... [6656/50176]	Loss: 4.5166
Profile done
epoch 1 train time consumed: 18.42s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.9579294169391,
                        "time": 13.870207988999937,
                        "accuracy": 0.009765625,
                        "total_cost": 248554127.16287884
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6375
Profiling... [1024/50176]	Loss: 5.6339
Profiling... [1536/50176]	Loss: 5.1546
Profiling... [2048/50176]	Loss: 5.1280
Profiling... [2560/50176]	Loss: 4.9288
Profiling... [3072/50176]	Loss: 4.8412
Profiling... [3584/50176]	Loss: 4.7079
Profiling... [4096/50176]	Loss: 4.7511
Profiling... [4608/50176]	Loss: 4.6911
Profiling... [5120/50176]	Loss: 4.5556
Profiling... [5632/50176]	Loss: 4.5461
Profiling... [6144/50176]	Loss: 4.5525
Profiling... [6656/50176]	Loss: 4.5444
Profile done
epoch 1 train time consumed: 6.58s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.00465658514132,
                        "time": 4.602170758999819,
                        "accuracy": 0.009765625,
                        "total_cost": 82470900.00127676
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6508
Profiling... [1024/50176]	Loss: 5.5898
Profiling... [1536/50176]	Loss: 5.3586
Profiling... [2048/50176]	Loss: 4.8795
Profiling... [2560/50176]	Loss: 4.8303
Profiling... [3072/50176]	Loss: 4.7620
Profiling... [3584/50176]	Loss: 4.7809
Profiling... [4096/50176]	Loss: 4.7127
Profiling... [4608/50176]	Loss: 4.6960
Profiling... [5120/50176]	Loss: 4.6038
Profiling... [5632/50176]	Loss: 4.6001
Profiling... [6144/50176]	Loss: 4.5634
Profiling... [6656/50176]	Loss: 4.5591
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.26294342763586,
                        "time": 4.720993248999548,
                        "accuracy": 0.009765625,
                        "total_cost": 84600199.0220719
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6305
Profiling... [1024/50176]	Loss: 5.4392
Profiling... [1536/50176]	Loss: 5.2471
Profiling... [2048/50176]	Loss: 4.8401
Profiling... [2560/50176]	Loss: 4.7243
Profiling... [3072/50176]	Loss: 4.7658
Profiling... [3584/50176]	Loss: 4.7484
Profiling... [4096/50176]	Loss: 4.5696
Profiling... [4608/50176]	Loss: 4.6325
Profiling... [5120/50176]	Loss: 4.6050
Profiling... [5632/50176]	Loss: 4.5517
Profiling... [6144/50176]	Loss: 4.6147
Profiling... [6656/50176]	Loss: 4.6083
Profile done
epoch 1 train time consumed: 7.58s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0097
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.35552141955087,
                        "time": 5.440961993999736,
                        "accuracy": 0.00966796875,
                        "total_cost": 98486908.01260127
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6212
Profiling... [1024/50176]	Loss: 5.7610
Profiling... [1536/50176]	Loss: 5.3381
Profiling... [2048/50176]	Loss: 4.8782
Profiling... [2560/50176]	Loss: 4.7667
Profiling... [3072/50176]	Loss: 4.7348
Profiling... [3584/50176]	Loss: 4.8549
Profiling... [4096/50176]	Loss: 4.7293
Profiling... [4608/50176]	Loss: 4.7287
Profiling... [5120/50176]	Loss: 4.5631
Profiling... [5632/50176]	Loss: 4.8339
Profiling... [6144/50176]	Loss: 4.6795
Profiling... [6656/50176]	Loss: 4.5984
Profile done
epoch 1 train time consumed: 17.58s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.0134924101533,
                        "time": 13.00355804100036,
                        "accuracy": 0.009765625,
                        "total_cost": 233023760.09472647
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6240
Profiling... [1024/50176]	Loss: 5.5774
Profiling... [1536/50176]	Loss: 5.2762
Profiling... [2048/50176]	Loss: 4.8775
Profiling... [2560/50176]	Loss: 4.8104
Profiling... [3072/50176]	Loss: 4.6904
Profiling... [3584/50176]	Loss: 4.8290
Profiling... [4096/50176]	Loss: 4.6778
Profiling... [4608/50176]	Loss: 4.6094
Profiling... [5120/50176]	Loss: 4.6583
Profiling... [5632/50176]	Loss: 4.5481
Profiling... [6144/50176]	Loss: 4.6195
Profiling... [6656/50176]	Loss: 4.6236
Profile done
epoch 1 train time consumed: 6.83s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.04848305711074,
                        "time": 4.592165047999515,
                        "accuracy": 0.009765625,
                        "total_cost": 82291597.66015132
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6300
Profiling... [1024/50176]	Loss: 5.4368
Profiling... [1536/50176]	Loss: 5.3610
Profiling... [2048/50176]	Loss: 4.8460
Profiling... [2560/50176]	Loss: 4.7102
Profiling... [3072/50176]	Loss: 4.6952
Profiling... [3584/50176]	Loss: 4.8620
Profiling... [4096/50176]	Loss: 4.6541
Profiling... [4608/50176]	Loss: 4.6146
Profiling... [5120/50176]	Loss: 4.6807
Profiling... [5632/50176]	Loss: 4.5865
Profiling... [6144/50176]	Loss: 4.5693
Profiling... [6656/50176]	Loss: 4.6059
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.3084964114824,
                        "time": 4.736365548000322,
                        "accuracy": 0.009765625,
                        "total_cost": 84875670.62016577
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6399
Profiling... [1024/50176]	Loss: 5.5778
Profiling... [1536/50176]	Loss: 5.3148
Profiling... [2048/50176]	Loss: 4.9263
Profiling... [2560/50176]	Loss: 4.9102
Profiling... [3072/50176]	Loss: 4.9244
Profiling... [3584/50176]	Loss: 4.6654
Profiling... [4096/50176]	Loss: 4.6639
Profiling... [4608/50176]	Loss: 4.6988
Profiling... [5120/50176]	Loss: 4.6095
Profiling... [5632/50176]	Loss: 4.5878
Profiling... [6144/50176]	Loss: 4.5690
Profiling... [6656/50176]	Loss: 4.5672
Profile done
epoch 1 train time consumed: 7.67s
Validation Epoch: 0, Average loss: 0.0095, Accuracy: 0.0126
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.39196857964178,
                        "time": 5.467322168999999,
                        "accuracy": 0.01259765625,
                        "total_cost": 75949157.57246509
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6321
Profiling... [1024/50176]	Loss: 5.4719
Profiling... [1536/50176]	Loss: 5.1841
Profiling... [2048/50176]	Loss: 4.9350
Profiling... [2560/50176]	Loss: 4.7612
Profiling... [3072/50176]	Loss: 4.6111
Profiling... [3584/50176]	Loss: 4.7162
Profiling... [4096/50176]	Loss: 4.7807
Profiling... [4608/50176]	Loss: 4.6443
Profiling... [5120/50176]	Loss: 4.5878
Profiling... [5632/50176]	Loss: 4.6463
Profiling... [6144/50176]	Loss: 4.5933
Profiling... [6656/50176]	Loss: 4.6248
Profile done
epoch 1 train time consumed: 17.54s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.07339549683847,
                        "time": 13.007719934000306,
                        "accuracy": 0.009765625,
                        "total_cost": 233098341.21728548
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6566
Profiling... [1024/50176]	Loss: 5.4350
Profiling... [1536/50176]	Loss: 5.3181
Profiling... [2048/50176]	Loss: 4.8127
Profiling... [2560/50176]	Loss: 4.8639
Profiling... [3072/50176]	Loss: 4.7827
Profiling... [3584/50176]	Loss: 4.7017
Profiling... [4096/50176]	Loss: 4.6801
Profiling... [4608/50176]	Loss: 4.6725
Profiling... [5120/50176]	Loss: 4.7328
Profiling... [5632/50176]	Loss: 4.6625
Profiling... [6144/50176]	Loss: 4.6391
Profiling... [6656/50176]	Loss: 4.5433
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.1226337307982,
                        "time": 4.61256273699928,
                        "accuracy": 0.009765625,
                        "total_cost": 82657124.2470271
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6358
Profiling... [1024/50176]	Loss: 5.5146
Profiling... [1536/50176]	Loss: 5.1816
Profiling... [2048/50176]	Loss: 4.7656
Profiling... [2560/50176]	Loss: 4.8769
Profiling... [3072/50176]	Loss: 4.8478
Profiling... [3584/50176]	Loss: 4.8271
Profiling... [4096/50176]	Loss: 4.6738
Profiling... [4608/50176]	Loss: 4.6831
Profiling... [5120/50176]	Loss: 4.5861
Profiling... [5632/50176]	Loss: 4.6909
Profiling... [6144/50176]	Loss: 4.5862
Profiling... [6656/50176]	Loss: 4.5709
Profile done
epoch 1 train time consumed: 6.71s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.3588156559577,
                        "time": 4.720418548999987,
                        "accuracy": 0.009765625,
                        "total_cost": 84589900.39807978
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6415
Profiling... [1024/50176]	Loss: 5.6333
Profiling... [1536/50176]	Loss: 5.2960
Profiling... [2048/50176]	Loss: 4.9466
Profiling... [2560/50176]	Loss: 4.9402
Profiling... [3072/50176]	Loss: 4.9277
Profiling... [3584/50176]	Loss: 4.7875
Profiling... [4096/50176]	Loss: 4.7660
Profiling... [4608/50176]	Loss: 4.6108
Profiling... [5120/50176]	Loss: 4.6126
Profiling... [5632/50176]	Loss: 4.6057
Profiling... [6144/50176]	Loss: 4.5749
Profiling... [6656/50176]	Loss: 4.5336
Profile done
epoch 1 train time consumed: 7.60s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.43690901387349,
                        "time": 5.427978530000473,
                        "accuracy": 0.009765625,
                        "total_cost": 97269375.25760849
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6364
Profiling... [1024/50176]	Loss: 5.7709
Profiling... [1536/50176]	Loss: 5.3026
Profiling... [2048/50176]	Loss: 4.8490
Profiling... [2560/50176]	Loss: 4.7272
Profiling... [3072/50176]	Loss: 4.6625
Profiling... [3584/50176]	Loss: 4.5800
Profiling... [4096/50176]	Loss: 4.5780
Profiling... [4608/50176]	Loss: 4.5883
Profiling... [5120/50176]	Loss: 4.7130
Profiling... [5632/50176]	Loss: 4.6877
Profiling... [6144/50176]	Loss: 4.5321
Profiling... [6656/50176]	Loss: 4.5104
Profile done
epoch 1 train time consumed: 17.56s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.14107257911375,
                        "time": 13.009753551999893,
                        "accuracy": 0.009765625,
                        "total_cost": 233134783.65183806
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6387
Profiling... [1024/50176]	Loss: 7.5351
Profiling... [1536/50176]	Loss: 6.0753
Profiling... [2048/50176]	Loss: 5.1029
Profiling... [2560/50176]	Loss: 5.1243
Profiling... [3072/50176]	Loss: 4.6709
Profiling... [3584/50176]	Loss: 4.6815
Profiling... [4096/50176]	Loss: 4.6133
Profiling... [4608/50176]	Loss: 4.6163
Profiling... [5120/50176]	Loss: 4.7240
Profiling... [5632/50176]	Loss: 4.7039
Profiling... [6144/50176]	Loss: 4.6963
Profiling... [6656/50176]	Loss: 4.6247
Profile done
epoch 1 train time consumed: 6.55s
Validation Epoch: 0, Average loss: 0.0302, Accuracy: 0.0185
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.17740567034956,
                        "time": 4.583581527000206,
                        "accuracy": 0.01845703125,
                        "total_cost": 43459143.36711307
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6345
Profiling... [1024/50176]	Loss: 7.2368
Profiling... [1536/50176]	Loss: 5.6598
Profiling... [2048/50176]	Loss: 5.2963
Profiling... [2560/50176]	Loss: 5.0576
Profiling... [3072/50176]	Loss: 4.9361
Profiling... [3584/50176]	Loss: 4.7495
Profiling... [4096/50176]	Loss: 4.7501
Profiling... [4608/50176]	Loss: 4.6689
Profiling... [5120/50176]	Loss: 4.5874
Profiling... [5632/50176]	Loss: 4.7252
Profiling... [6144/50176]	Loss: 4.7032
Profiling... [6656/50176]	Loss: 4.7462
Profile done
epoch 1 train time consumed: 6.74s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0097
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.40381668663566,
                        "time": 4.710162562000733,
                        "accuracy": 0.00966796875,
                        "total_cost": 85258700.11217488
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6401
Profiling... [1024/50176]	Loss: 7.5415
Profiling... [1536/50176]	Loss: 5.7063
Profiling... [2048/50176]	Loss: 5.3793
Profiling... [2560/50176]	Loss: 5.0931
Profiling... [3072/50176]	Loss: 4.7914
Profiling... [3584/50176]	Loss: 4.7596
Profiling... [4096/50176]	Loss: 5.3955
Profiling... [4608/50176]	Loss: 5.0294
Profiling... [5120/50176]	Loss: 4.6745
Profiling... [5632/50176]	Loss: 4.8886
Profiling... [6144/50176]	Loss: 4.7697
Profiling... [6656/50176]	Loss: 4.6547
Profile done
epoch 1 train time consumed: 7.70s
Validation Epoch: 0, Average loss: 0.0096, Accuracy: 0.0071
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.48059609172095,
                        "time": 5.454361981000147,
                        "accuracy": 0.00712890625,
                        "total_cost": 133893379.04044197
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6458
Profiling... [1024/50176]	Loss: 7.5027
Profiling... [1536/50176]	Loss: 6.0768
Profiling... [2048/50176]	Loss: 5.0984
Profiling... [2560/50176]	Loss: 4.9760
Profiling... [3072/50176]	Loss: 4.7655
Profiling... [3584/50176]	Loss: 4.7303
Profiling... [4096/50176]	Loss: 4.8435
Profiling... [4608/50176]	Loss: 4.8413
Profiling... [5120/50176]	Loss: 4.8219
Profiling... [5632/50176]	Loss: 4.6737
Profiling... [6144/50176]	Loss: 4.7544
Profiling... [6656/50176]	Loss: 4.7402
Profile done
epoch 1 train time consumed: 17.42s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.19234976223353,
                        "time": 13.018433395000102,
                        "accuracy": 0.009765625,
                        "total_cost": 233290326.43840185
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6503
Profiling... [1024/50176]	Loss: 7.1992
Profiling... [1536/50176]	Loss: 5.7714
Profiling... [2048/50176]	Loss: 5.0558
Profiling... [2560/50176]	Loss: 5.1457
Profiling... [3072/50176]	Loss: 4.8571
Profiling... [3584/50176]	Loss: 4.8379
Profiling... [4096/50176]	Loss: 4.8588
Profiling... [4608/50176]	Loss: 4.8173
Profiling... [5120/50176]	Loss: 4.7723
Profiling... [5632/50176]	Loss: 4.8876
Profiling... [6144/50176]	Loss: 4.6408
Profiling... [6656/50176]	Loss: 4.6952
Profile done
epoch 1 train time consumed: 6.74s
Validation Epoch: 0, Average loss: 0.0859, Accuracy: 0.0111
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.23159666477252,
                        "time": 4.587503859999742,
                        "accuracy": 0.0111328125,
                        "total_cost": 72112341.37824157
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6509
Profiling... [1024/50176]	Loss: 7.3659
Profiling... [1536/50176]	Loss: 5.7206
Profiling... [2048/50176]	Loss: 5.2874
Profiling... [2560/50176]	Loss: 4.9049
Profiling... [3072/50176]	Loss: 4.9118
Profiling... [3584/50176]	Loss: 4.7135
Profiling... [4096/50176]	Loss: 4.8923
Profiling... [4608/50176]	Loss: 4.9924
Profiling... [5120/50176]	Loss: 4.8190
Profiling... [5632/50176]	Loss: 4.9318
Profiling... [6144/50176]	Loss: 4.7294
Profiling... [6656/50176]	Loss: 4.6948
Profile done
epoch 1 train time consumed: 6.74s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.44436861774813,
                        "time": 4.719514516000345,
                        "accuracy": 0.009765625,
                        "total_cost": 84573700.12672618
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6253
Profiling... [1024/50176]	Loss: 7.3393
Profiling... [1536/50176]	Loss: 5.7550
Profiling... [2048/50176]	Loss: 5.2739
Profiling... [2560/50176]	Loss: 4.8859
Profiling... [3072/50176]	Loss: 4.7399
Profiling... [3584/50176]	Loss: 4.8277
Profiling... [4096/50176]	Loss: 5.0013
Profiling... [4608/50176]	Loss: 4.7620
Profiling... [5120/50176]	Loss: 4.5774
Profiling... [5632/50176]	Loss: 4.5919
Profiling... [6144/50176]	Loss: 4.6145
Profiling... [6656/50176]	Loss: 4.8526
Profile done
epoch 1 train time consumed: 7.80s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.51058052824996,
                        "time": 5.441007718000037,
                        "accuracy": 0.009765625,
                        "total_cost": 97502858.30656067
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6248
Profiling... [1024/50176]	Loss: 7.5142
Profiling... [1536/50176]	Loss: 6.1596
Profiling... [2048/50176]	Loss: 5.1611
Profiling... [2560/50176]	Loss: 4.8820
Profiling... [3072/50176]	Loss: 4.8530
Profiling... [3584/50176]	Loss: 4.6995
Profiling... [4096/50176]	Loss: 4.7670
Profiling... [4608/50176]	Loss: 4.7395
Profiling... [5120/50176]	Loss: 4.8189
Profiling... [5632/50176]	Loss: 4.6359
Profiling... [6144/50176]	Loss: 4.6933
Profiling... [6656/50176]	Loss: 4.7206
Profile done
epoch 1 train time consumed: 17.52s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0162
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.2347847686945,
                        "time": 13.000652243999866,
                        "accuracy": 0.0162109375,
                        "total_cost": 140344390.48944432
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6469
Profiling... [1024/50176]	Loss: 7.5616
Profiling... [1536/50176]	Loss: 6.0229
Profiling... [2048/50176]	Loss: 5.0334
Profiling... [2560/50176]	Loss: 4.7580
Profiling... [3072/50176]	Loss: 4.5817
Profiling... [3584/50176]	Loss: 4.8186
Profiling... [4096/50176]	Loss: 4.6585
Profiling... [4608/50176]	Loss: 4.6742
Profiling... [5120/50176]	Loss: 4.7208
Profiling... [5632/50176]	Loss: 4.6645
Profiling... [6144/50176]	Loss: 4.6251
Profiling... [6656/50176]	Loss: 4.5947
Profile done
epoch 1 train time consumed: 6.69s
Validation Epoch: 0, Average loss: 0.0139, Accuracy: 0.0158
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.275580678773,
                        "time": 4.598939094999878,
                        "accuracy": 0.0158203125,
                        "total_cost": 50872215.17431963
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6456
Profiling... [1024/50176]	Loss: 7.2289
Profiling... [1536/50176]	Loss: 6.2036
Profiling... [2048/50176]	Loss: 5.3603
Profiling... [2560/50176]	Loss: 4.9366
Profiling... [3072/50176]	Loss: 4.8546
Profiling... [3584/50176]	Loss: 4.9745
Profiling... [4096/50176]	Loss: 4.9558
Profiling... [4608/50176]	Loss: 4.9938
Profiling... [5120/50176]	Loss: 4.8479
Profiling... [5632/50176]	Loss: 4.8003
Profiling... [6144/50176]	Loss: 4.6866
Profiling... [6656/50176]	Loss: 4.7056
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.46816654943225,
                        "time": 4.728728206000596,
                        "accuracy": 0.009765625,
                        "total_cost": 84738809.45153068
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6312
Profiling... [1024/50176]	Loss: 7.2700
Profiling... [1536/50176]	Loss: 6.0396
Profiling... [2048/50176]	Loss: 5.2341
Profiling... [2560/50176]	Loss: 5.1783
Profiling... [3072/50176]	Loss: 4.8646
Profiling... [3584/50176]	Loss: 4.8521
Profiling... [4096/50176]	Loss: 4.8638
Profiling... [4608/50176]	Loss: 4.6613
Profiling... [5120/50176]	Loss: 4.7766
Profiling... [5632/50176]	Loss: 4.7239
Profiling... [6144/50176]	Loss: 4.7565
Profiling... [6656/50176]	Loss: 4.8090
Profile done
epoch 1 train time consumed: 7.64s
Validation Epoch: 0, Average loss: 0.0096, Accuracy: 0.0096
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.53314821690897,
                        "time": 5.440273232999971,
                        "accuracy": 0.0095703125,
                        "total_cost": 99479281.9748566
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6188
Profiling... [1024/50176]	Loss: 7.2086
Profiling... [1536/50176]	Loss: 6.3937
Profiling... [2048/50176]	Loss: 5.4584
Profiling... [2560/50176]	Loss: 4.7446
Profiling... [3072/50176]	Loss: 5.2078
Profiling... [3584/50176]	Loss: 4.7384
Profiling... [4096/50176]	Loss: 4.6421
Profiling... [4608/50176]	Loss: 4.8042
Profiling... [5120/50176]	Loss: 4.8202
Profiling... [5632/50176]	Loss: 4.6666
Profiling... [6144/50176]	Loss: 4.7512
Profiling... [6656/50176]	Loss: 4.6269
Profile done
epoch 1 train time consumed: 17.49s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.28704554814396,
                        "time": 13.05106530500052,
                        "accuracy": 0.009765625,
                        "total_cost": 233875090.26560932
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6269
Profiling... [2048/50176]	Loss: 4.6278
Profiling... [3072/50176]	Loss: 4.6145
Profiling... [4096/50176]	Loss: 4.5902
Profiling... [5120/50176]	Loss: 4.5881
Profiling... [6144/50176]	Loss: 4.5729
Profiling... [7168/50176]	Loss: 4.5376
Profiling... [8192/50176]	Loss: 4.5282
Profiling... [9216/50176]	Loss: 4.5212
Profiling... [10240/50176]	Loss: 4.5138
Profiling... [11264/50176]	Loss: 4.5259
Profiling... [12288/50176]	Loss: 4.4833
Profiling... [13312/50176]	Loss: 4.4692
Profile done
epoch 1 train time consumed: 12.85s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.44513511387116,
                        "time": 8.97465013800047,
                        "accuracy": 0.009765625,
                        "total_cost": 160825730.47296843
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6309
Profiling... [2048/50176]	Loss: 4.6450
Profiling... [3072/50176]	Loss: 4.6477
Profiling... [4096/50176]	Loss: 4.5864
Profiling... [5120/50176]	Loss: 4.5694
Profiling... [6144/50176]	Loss: 4.5461
Profiling... [7168/50176]	Loss: 4.5779
Profiling... [8192/50176]	Loss: 4.5544
Profiling... [9216/50176]	Loss: 4.5351
Profiling... [10240/50176]	Loss: 4.4916
Profiling... [11264/50176]	Loss: 4.4948
Profiling... [12288/50176]	Loss: 4.4919
Profiling... [13312/50176]	Loss: 4.4584
Profile done
epoch 1 train time consumed: 13.14s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.73855530751874,
                        "time": 9.273021524000796,
                        "accuracy": 0.009765625,
                        "total_cost": 166172545.71009427
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6263
Profiling... [2048/50176]	Loss: 4.6338
Profiling... [3072/50176]	Loss: 4.6248
Profiling... [4096/50176]	Loss: 4.5943
Profiling... [5120/50176]	Loss: 4.5854
Profiling... [6144/50176]	Loss: 4.5828
Profiling... [7168/50176]	Loss: 4.5588
Profiling... [8192/50176]	Loss: 4.5324
Profiling... [9216/50176]	Loss: 4.5275
Profiling... [10240/50176]	Loss: 4.5230
Profiling... [11264/50176]	Loss: 4.4841
Profiling... [12288/50176]	Loss: 4.4780
Profiling... [13312/50176]	Loss: 4.4875
Profile done
epoch 1 train time consumed: 15.00s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.79999935673433,
                        "time": 10.766842345999976,
                        "accuracy": 0.009765625,
                        "total_cost": 192941814.84031957
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6292
Profiling... [2048/50176]	Loss: 4.6250
Profiling... [3072/50176]	Loss: 4.6255
Profiling... [4096/50176]	Loss: 4.6056
Profiling... [5120/50176]	Loss: 4.5963
Profiling... [6144/50176]	Loss: 4.5486
Profiling... [7168/50176]	Loss: 4.5557
Profiling... [8192/50176]	Loss: 4.5245
Profiling... [9216/50176]	Loss: 4.5272
Profiling... [10240/50176]	Loss: 4.5253
Profiling... [11264/50176]	Loss: 4.4862
Profiling... [12288/50176]	Loss: 4.4901
Profiling... [13312/50176]	Loss: 4.4848
Profile done
epoch 1 train time consumed: 37.13s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.30059431347553,
                        "time": 27.59896727600062,
                        "accuracy": 0.009765625,
                        "total_cost": 494573493.5859311
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6399
Profiling... [2048/50176]	Loss: 4.6208
Profiling... [3072/50176]	Loss: 4.6099
Profiling... [4096/50176]	Loss: 4.6080
Profiling... [5120/50176]	Loss: 4.5885
Profiling... [6144/50176]	Loss: 4.5504
Profiling... [7168/50176]	Loss: 4.5264
Profiling... [8192/50176]	Loss: 4.5475
Profiling... [9216/50176]	Loss: 4.5070
Profiling... [10240/50176]	Loss: 4.5424
Profiling... [11264/50176]	Loss: 4.5349
Profiling... [12288/50176]	Loss: 4.5117
Profiling... [13312/50176]	Loss: 4.4967
Profile done
epoch 1 train time consumed: 12.83s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.4894879427832,
                        "time": 8.998852822000117,
                        "accuracy": 0.009765625,
                        "total_cost": 161259442.5702421
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6362
Profiling... [2048/50176]	Loss: 4.6229
Profiling... [3072/50176]	Loss: 4.6296
Profiling... [4096/50176]	Loss: 4.5906
Profiling... [5120/50176]	Loss: 4.5435
Profiling... [6144/50176]	Loss: 4.5341
Profiling... [7168/50176]	Loss: 4.5432
Profiling... [8192/50176]	Loss: 4.5430
Profiling... [9216/50176]	Loss: 4.5075
Profiling... [10240/50176]	Loss: 4.4911
Profiling... [11264/50176]	Loss: 4.4934
Profiling... [12288/50176]	Loss: 4.4803
Profiling... [13312/50176]	Loss: 4.4732
Profile done
epoch 1 train time consumed: 13.15s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.753450724563,
                        "time": 9.331680839999535,
                        "accuracy": 0.009765625,
                        "total_cost": 167223720.65279168
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6333
Profiling... [2048/50176]	Loss: 4.6200
Profiling... [3072/50176]	Loss: 4.6170
Profiling... [4096/50176]	Loss: 4.6130
Profiling... [5120/50176]	Loss: 4.5692
Profiling... [6144/50176]	Loss: 4.5506
Profiling... [7168/50176]	Loss: 4.5365
Profiling... [8192/50176]	Loss: 4.5290
Profiling... [9216/50176]	Loss: 4.5399
Profiling... [10240/50176]	Loss: 4.5286
Profiling... [11264/50176]	Loss: 4.5121
Profiling... [12288/50176]	Loss: 4.5070
Profiling... [13312/50176]	Loss: 4.4711
Profile done
epoch 1 train time consumed: 15.07s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.81534940532242,
                        "time": 10.798230254000373,
                        "accuracy": 0.009765625,
                        "total_cost": 193504286.1516867
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6370
Profiling... [2048/50176]	Loss: 4.6132
Profiling... [3072/50176]	Loss: 4.6026
Profiling... [4096/50176]	Loss: 4.6252
Profiling... [5120/50176]	Loss: 4.5835
Profiling... [6144/50176]	Loss: 4.5622
Profiling... [7168/50176]	Loss: 4.5538
Profiling... [8192/50176]	Loss: 4.5458
Profiling... [9216/50176]	Loss: 4.5437
Profiling... [10240/50176]	Loss: 4.5327
Profiling... [11264/50176]	Loss: 4.5265
Profiling... [12288/50176]	Loss: 4.5177
Profiling... [13312/50176]	Loss: 4.4444
Profile done
epoch 1 train time consumed: 37.24s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.33202621418782,
                        "time": 27.64366526499998,
                        "accuracy": 0.009765625,
                        "total_cost": 495374481.5487997
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6362
Profiling... [2048/50176]	Loss: 4.6264
Profiling... [3072/50176]	Loss: 4.5931
Profiling... [4096/50176]	Loss: 4.6142
Profiling... [5120/50176]	Loss: 4.5930
Profiling... [6144/50176]	Loss: 4.5522
Profiling... [7168/50176]	Loss: 4.5677
Profiling... [8192/50176]	Loss: 4.5542
Profiling... [9216/50176]	Loss: 4.5366
Profiling... [10240/50176]	Loss: 4.5471
Profiling... [11264/50176]	Loss: 4.5047
Profiling... [12288/50176]	Loss: 4.4937
Profiling... [13312/50176]	Loss: 4.4983
Profile done
epoch 1 train time consumed: 12.75s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.51139666822442,
                        "time": 8.99970667999969,
                        "accuracy": 0.009765625,
                        "total_cost": 161274743.70559445
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6273
Profiling... [2048/50176]	Loss: 4.6165
Profiling... [3072/50176]	Loss: 4.6240
Profiling... [4096/50176]	Loss: 4.5916
Profiling... [5120/50176]	Loss: 4.5943
Profiling... [6144/50176]	Loss: 4.5455
Profiling... [7168/50176]	Loss: 4.5641
Profiling... [8192/50176]	Loss: 4.5156
Profiling... [9216/50176]	Loss: 4.5223
Profiling... [10240/50176]	Loss: 4.5364
Profiling... [11264/50176]	Loss: 4.5102
Profiling... [12288/50176]	Loss: 4.5025
Profiling... [13312/50176]	Loss: 4.4710
Profile done
epoch 1 train time consumed: 13.26s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.76515883182606,
                        "time": 9.282307787000718,
                        "accuracy": 0.009765625,
                        "total_cost": 166338955.54305285
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6337
Profiling... [2048/50176]	Loss: 4.6408
Profiling... [3072/50176]	Loss: 4.6113
Profiling... [4096/50176]	Loss: 4.5968
Profiling... [5120/50176]	Loss: 4.5788
Profiling... [6144/50176]	Loss: 4.5379
Profiling... [7168/50176]	Loss: 4.5612
Profiling... [8192/50176]	Loss: 4.5429
Profiling... [9216/50176]	Loss: 4.5190
Profiling... [10240/50176]	Loss: 4.5425
Profiling... [11264/50176]	Loss: 4.5043
Profiling... [12288/50176]	Loss: 4.4887
Profiling... [13312/50176]	Loss: 4.4708
Profile done
epoch 1 train time consumed: 14.92s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.81920655450243,
                        "time": 10.764638317999925,
                        "accuracy": 0.009765625,
                        "total_cost": 192902318.65855867
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6348
Profiling... [2048/50176]	Loss: 4.6092
Profiling... [3072/50176]	Loss: 4.6209
Profiling... [4096/50176]	Loss: 4.6075
Profiling... [5120/50176]	Loss: 4.5756
Profiling... [6144/50176]	Loss: 4.5530
Profiling... [7168/50176]	Loss: 4.5582
Profiling... [8192/50176]	Loss: 4.5283
Profiling... [9216/50176]	Loss: 4.5287
Profiling... [10240/50176]	Loss: 4.5537
Profiling... [11264/50176]	Loss: 4.4930
Profiling... [12288/50176]	Loss: 4.5157
Profiling... [13312/50176]	Loss: 4.4629
Profile done
epoch 1 train time consumed: 38.47s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.31153391318728,
                        "time": 28.474552998000036,
                        "accuracy": 0.009765625,
                        "total_cost": 510263989.7241607
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6371
Profiling... [2048/50176]	Loss: 5.5249
Profiling... [3072/50176]	Loss: 5.2002
Profiling... [4096/50176]	Loss: 4.7341
Profiling... [5120/50176]	Loss: 4.6767
Profiling... [6144/50176]	Loss: 4.6022
Profiling... [7168/50176]	Loss: 4.7027
Profiling... [8192/50176]	Loss: 4.6412
Profiling... [9216/50176]	Loss: 4.6025
Profiling... [10240/50176]	Loss: 4.5917
Profiling... [11264/50176]	Loss: 4.5756
Profiling... [12288/50176]	Loss: 4.5802
Profiling... [13312/50176]	Loss: 4.5703
Profile done
epoch 1 train time consumed: 12.78s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.48840781354994,
                        "time": 8.976696547999381,
                        "accuracy": 0.009765625,
                        "total_cost": 160862402.14014894
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6344
Profiling... [2048/50176]	Loss: 5.6068
Profiling... [3072/50176]	Loss: 5.1897
Profiling... [4096/50176]	Loss: 4.7205
Profiling... [5120/50176]	Loss: 4.7037
Profiling... [6144/50176]	Loss: 4.7365
Profiling... [7168/50176]	Loss: 4.6666
Profiling... [8192/50176]	Loss: 4.6202
Profiling... [9216/50176]	Loss: 4.6719
Profiling... [10240/50176]	Loss: 4.5871
Profiling... [11264/50176]	Loss: 4.5248
Profiling... [12288/50176]	Loss: 4.4819
Profiling... [13312/50176]	Loss: 4.4376
Profile done
epoch 1 train time consumed: 13.08s
Validation Epoch: 0, Average loss: 0.0048, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.72345162753696,
                        "time": 9.281474731999879,
                        "accuracy": 0.009765625,
                        "total_cost": 166324027.19743782
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6379
Profiling... [2048/50176]	Loss: 5.5141
Profiling... [3072/50176]	Loss: 5.2178
Profiling... [4096/50176]	Loss: 4.7555
Profiling... [5120/50176]	Loss: 4.7189
Profiling... [6144/50176]	Loss: 4.6575
Profiling... [7168/50176]	Loss: 4.7215
Profiling... [8192/50176]	Loss: 4.6824
Profiling... [9216/50176]	Loss: 5.0237
Profiling... [10240/50176]	Loss: 4.7221
Profiling... [11264/50176]	Loss: 4.6589
Profiling... [12288/50176]	Loss: 4.6040
Profiling... [13312/50176]	Loss: 4.5526
Profile done
epoch 1 train time consumed: 14.96s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.77400460338866,
                        "time": 10.765876492999269,
                        "accuracy": 0.009765625,
                        "total_cost": 192924506.75454688
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6436
Profiling... [2048/50176]	Loss: 5.4620
Profiling... [3072/50176]	Loss: 5.1435
Profiling... [4096/50176]	Loss: 4.7927
Profiling... [5120/50176]	Loss: 4.7815
Profiling... [6144/50176]	Loss: 4.7658
Profiling... [7168/50176]	Loss: 4.6654
Profiling... [8192/50176]	Loss: 4.6746
Profiling... [9216/50176]	Loss: 4.6294
Profiling... [10240/50176]	Loss: 4.5724
Profiling... [11264/50176]	Loss: 4.5964
Profiling... [12288/50176]	Loss: 4.5511
Profiling... [13312/50176]	Loss: 4.5631
Profile done
epoch 1 train time consumed: 37.19s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.34822015636003,
                        "time": 27.641308164999828,
                        "accuracy": 0.0099609375,
                        "total_cost": 485619845.4086244
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6415
Profiling... [2048/50176]	Loss: 5.5348
Profiling... [3072/50176]	Loss: 5.0207
Profiling... [4096/50176]	Loss: 4.7482
Profiling... [5120/50176]	Loss: 4.8442
Profiling... [6144/50176]	Loss: 4.7153
Profiling... [7168/50176]	Loss: 4.7033
Profiling... [8192/50176]	Loss: 4.6484
Profiling... [9216/50176]	Loss: 4.5976
Profiling... [10240/50176]	Loss: 4.5615
Profiling... [11264/50176]	Loss: 4.4808
Profiling... [12288/50176]	Loss: 4.4960
Profiling... [13312/50176]	Loss: 4.4650
Profile done
epoch 1 train time consumed: 12.90s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.51298907449512,
                        "time": 9.110895511999843,
                        "accuracy": 0.009765625,
                        "total_cost": 163267247.57503718
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6413
Profiling... [2048/50176]	Loss: 5.4520
Profiling... [3072/50176]	Loss: 5.1080
Profiling... [4096/50176]	Loss: 4.7440
Profiling... [5120/50176]	Loss: 4.6643
Profiling... [6144/50176]	Loss: 4.6876
Profiling... [7168/50176]	Loss: 4.6462
Profiling... [8192/50176]	Loss: 4.6028
Profiling... [9216/50176]	Loss: 4.6082
Profiling... [10240/50176]	Loss: 4.6550
Profiling... [11264/50176]	Loss: 4.5392
Profiling... [12288/50176]	Loss: 4.5146
Profiling... [13312/50176]	Loss: 4.4633
Profile done
epoch 1 train time consumed: 13.34s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.74118001046243,
                        "time": 9.418851755000105,
                        "accuracy": 0.009765625,
                        "total_cost": 168785823.4496019
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6268
Profiling... [2048/50176]	Loss: 5.5117
Profiling... [3072/50176]	Loss: 5.1413
Profiling... [4096/50176]	Loss: 4.8470
Profiling... [5120/50176]	Loss: 4.7449
Profiling... [6144/50176]	Loss: 4.5813
Profiling... [7168/50176]	Loss: 4.7242
Profiling... [8192/50176]	Loss: 4.6522
Profiling... [9216/50176]	Loss: 4.6737
Profiling... [10240/50176]	Loss: 4.6444
Profiling... [11264/50176]	Loss: 4.5859
Profiling... [12288/50176]	Loss: 4.5503
Profiling... [13312/50176]	Loss: 4.5784
Profile done
epoch 1 train time consumed: 15.10s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.79259739778509,
                        "time": 10.874407356999654,
                        "accuracy": 0.009765625,
                        "total_cost": 194869379.83743382
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6355
Profiling... [2048/50176]	Loss: 5.5506
Profiling... [3072/50176]	Loss: 4.9520
Profiling... [4096/50176]	Loss: 4.8685
Profiling... [5120/50176]	Loss: 4.9848
Profiling... [6144/50176]	Loss: 4.8811
Profiling... [7168/50176]	Loss: 4.7631
Profiling... [8192/50176]	Loss: 4.6731
Profiling... [9216/50176]	Loss: 4.7201
Profiling... [10240/50176]	Loss: 4.6350
Profiling... [11264/50176]	Loss: 4.6011
Profiling... [12288/50176]	Loss: 4.5635
Profiling... [13312/50176]	Loss: 4.5962
Profile done
epoch 1 train time consumed: 37.41s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.40387684992038,
                        "time": 27.749774146000163,
                        "accuracy": 0.009765625,
                        "total_cost": 497275952.6963229
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6220
Profiling... [2048/50176]	Loss: 5.5372
Profiling... [3072/50176]	Loss: 4.8702
Profiling... [4096/50176]	Loss: 4.8944
Profiling... [5120/50176]	Loss: 4.8569
Profiling... [6144/50176]	Loss: 4.7935
Profiling... [7168/50176]	Loss: 4.7566
Profiling... [8192/50176]	Loss: 4.7144
Profiling... [9216/50176]	Loss: 4.6095
Profiling... [10240/50176]	Loss: 4.6865
Profiling... [11264/50176]	Loss: 4.5832
Profiling... [12288/50176]	Loss: 4.5690
Profiling... [13312/50176]	Loss: 4.5602
Profile done
epoch 1 train time consumed: 12.71s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.55243643743287,
                        "time": 8.947814169000594,
                        "accuracy": 0.009765625,
                        "total_cost": 160344829.90849066
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6234
Profiling... [2048/50176]	Loss: 5.4809
Profiling... [3072/50176]	Loss: 5.1574
Profiling... [4096/50176]	Loss: 4.8443
Profiling... [5120/50176]	Loss: 4.7209
Profiling... [6144/50176]	Loss: 4.6726
Profiling... [7168/50176]	Loss: 4.6626
Profiling... [8192/50176]	Loss: 4.6616
Profiling... [9216/50176]	Loss: 4.6286
Profiling... [10240/50176]	Loss: 4.5937
Profiling... [11264/50176]	Loss: 4.4953
Profiling... [12288/50176]	Loss: 4.4755
Profiling... [13312/50176]	Loss: 4.4426
Profile done
epoch 1 train time consumed: 13.05s
Validation Epoch: 0, Average loss: 0.0048, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.76129167321764,
                        "time": 9.262492224000198,
                        "accuracy": 0.009765625,
                        "total_cost": 165983860.65408355
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6452
Profiling... [2048/50176]	Loss: 5.5341
Profiling... [3072/50176]	Loss: 5.1291
Profiling... [4096/50176]	Loss: 4.7632
Profiling... [5120/50176]	Loss: 4.7464
Profiling... [6144/50176]	Loss: 4.7191
Profiling... [7168/50176]	Loss: 4.6330
Profiling... [8192/50176]	Loss: 4.6419
Profiling... [9216/50176]	Loss: 4.6743
Profiling... [10240/50176]	Loss: 4.5832
Profiling... [11264/50176]	Loss: 4.5345
Profiling... [12288/50176]	Loss: 4.5456
Profiling... [13312/50176]	Loss: 4.5362
Profile done
epoch 1 train time consumed: 15.02s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.80501321287161,
                        "time": 10.712816997000118,
                        "accuracy": 0.009765625,
                        "total_cost": 191973680.58624214
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6403
Profiling... [2048/50176]	Loss: 5.5982
Profiling... [3072/50176]	Loss: 5.0936
Profiling... [4096/50176]	Loss: 4.7886
Profiling... [5120/50176]	Loss: 4.8733
Profiling... [6144/50176]	Loss: 4.6620
Profiling... [7168/50176]	Loss: 4.6578
Profiling... [8192/50176]	Loss: 4.6402
Profiling... [9216/50176]	Loss: 4.6421
Profiling... [10240/50176]	Loss: 4.6087
Profiling... [11264/50176]	Loss: 4.5438
Profiling... [12288/50176]	Loss: 4.5064
Profiling... [13312/50176]	Loss: 4.5844
Profile done
epoch 1 train time consumed: 37.64s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.40112379234215,
                        "time": 28.12449073300013,
                        "accuracy": 0.009765625,
                        "total_cost": 503990873.93536234
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6289
Profiling... [2048/50176]	Loss: 6.9695
Profiling... [3072/50176]	Loss: 5.8313
Profiling... [4096/50176]	Loss: 5.2244
Profiling... [5120/50176]	Loss: 5.0522
Profiling... [6144/50176]	Loss: 4.8772
Profiling... [7168/50176]	Loss: 5.1689
Profiling... [8192/50176]	Loss: 4.7786
Profiling... [9216/50176]	Loss: 4.7569
Profiling... [10240/50176]	Loss: 4.7041
Profiling... [11264/50176]	Loss: 4.6607
Profiling... [12288/50176]	Loss: 4.6602
Profiling... [13312/50176]	Loss: 4.7138
Profile done
epoch 1 train time consumed: 12.86s
Validation Epoch: 0, Average loss: 0.0531, Accuracy: 0.0121
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.53645850323245,
                        "time": 8.933282560999942,
                        "accuracy": 0.012109375,
                        "total_cost": 129100341.52670883
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6203
Profiling... [2048/50176]	Loss: 7.2427
Profiling... [3072/50176]	Loss: 5.3852
Profiling... [4096/50176]	Loss: 4.8412
Profiling... [5120/50176]	Loss: 4.8388
Profiling... [6144/50176]	Loss: 4.9091
Profiling... [7168/50176]	Loss: 4.6434
Profiling... [8192/50176]	Loss: 4.7640
Profiling... [9216/50176]	Loss: 4.6098
Profiling... [10240/50176]	Loss: 4.8115
Profiling... [11264/50176]	Loss: 4.7830
Profiling... [12288/50176]	Loss: 4.6649
Profiling... [13312/50176]	Loss: 4.7634
Profile done
epoch 1 train time consumed: 13.12s
Validation Epoch: 0, Average loss: 0.0069, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.73154121588736,
                        "time": 9.250495922000482,
                        "accuracy": 0.0099609375,
                        "total_cost": 162518516.59043986
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6314
Profiling... [2048/50176]	Loss: 7.1993
Profiling... [3072/50176]	Loss: 5.6751
Profiling... [4096/50176]	Loss: 5.1646
Profiling... [5120/50176]	Loss: 4.9498
Profiling... [6144/50176]	Loss: 4.8013
Profiling... [7168/50176]	Loss: 5.0088
Profiling... [8192/50176]	Loss: 4.6448
Profiling... [9216/50176]	Loss: 4.6493
Profiling... [10240/50176]	Loss: 4.7594
Profiling... [11264/50176]	Loss: 4.6995
Profiling... [12288/50176]	Loss: 4.7587
Profiling... [13312/50176]	Loss: 4.6648
Profile done
epoch 1 train time consumed: 15.01s
Validation Epoch: 0, Average loss: 0.0696, Accuracy: 0.0121
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.77219357683128,
                        "time": 10.730123008999726,
                        "accuracy": 0.012109375,
                        "total_cost": 155067584.13006055
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6430
Profiling... [2048/50176]	Loss: 6.9033
Profiling... [3072/50176]	Loss: 5.4015
Profiling... [4096/50176]	Loss: 5.0306
Profiling... [5120/50176]	Loss: 4.8611
Profiling... [6144/50176]	Loss: 4.6725
Profiling... [7168/50176]	Loss: 4.7914
Profiling... [8192/50176]	Loss: 4.7944
Profiling... [9216/50176]	Loss: 4.9781
Profiling... [10240/50176]	Loss: 5.0129
Profiling... [11264/50176]	Loss: 4.8590
Profiling... [12288/50176]	Loss: 4.7819
Profiling... [13312/50176]	Loss: 4.7600
Profile done
epoch 1 train time consumed: 37.96s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.3979644197484,
                        "time": 28.30660501500006,
                        "accuracy": 0.009765625,
                        "total_cost": 507254361.868801
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6254
Profiling... [2048/50176]	Loss: 6.8364
Profiling... [3072/50176]	Loss: 5.7214
Profiling... [4096/50176]	Loss: 5.0063
Profiling... [5120/50176]	Loss: 4.8111
Profiling... [6144/50176]	Loss: 4.7105
Profiling... [7168/50176]	Loss: 4.7202
Profiling... [8192/50176]	Loss: 5.2614
Profiling... [9216/50176]	Loss: 4.7480
Profiling... [10240/50176]	Loss: 4.9010
Profiling... [11264/50176]	Loss: 4.7514
Profiling... [12288/50176]	Loss: 4.7157
Profiling... [13312/50176]	Loss: 4.7395
Profile done
epoch 1 train time consumed: 12.77s
Validation Epoch: 0, Average loss: 0.0436, Accuracy: 0.0184
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.52215082723652,
                        "time": 8.941676090000328,
                        "accuracy": 0.018359375,
                        "total_cost": 85231295.49617334
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6318
Profiling... [2048/50176]	Loss: 6.9773
Profiling... [3072/50176]	Loss: 5.6401
Profiling... [4096/50176]	Loss: 4.9212
Profiling... [5120/50176]	Loss: 4.8567
Profiling... [6144/50176]	Loss: 4.7743
Profiling... [7168/50176]	Loss: 4.8340
Profiling... [8192/50176]	Loss: 4.9898
Profiling... [9216/50176]	Loss: 4.9383
Profiling... [10240/50176]	Loss: 4.8311
Profiling... [11264/50176]	Loss: 4.8106
Profiling... [12288/50176]	Loss: 4.8277
Profiling... [13312/50176]	Loss: 4.7605
Profile done
epoch 1 train time consumed: 13.04s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0119
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.71194213374841,
                        "time": 9.262766886999998,
                        "accuracy": 0.0119140625,
                        "total_cost": 136056379.1926557
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6351
Profiling... [2048/50176]	Loss: 6.8365
Profiling... [3072/50176]	Loss: 5.9636
Profiling... [4096/50176]	Loss: 5.3685
Profiling... [5120/50176]	Loss: 4.9335
Profiling... [6144/50176]	Loss: 4.7054
Profiling... [7168/50176]	Loss: 4.5945
Profiling... [8192/50176]	Loss: 4.8053
Profiling... [9216/50176]	Loss: 4.6825
Profiling... [10240/50176]	Loss: 4.7160
Profiling... [11264/50176]	Loss: 4.9596
Profiling... [12288/50176]	Loss: 4.6629
Profiling... [13312/50176]	Loss: 4.8564
Profile done
epoch 1 train time consumed: 15.00s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.74529544771609,
                        "time": 10.729927223000232,
                        "accuracy": 0.00986328125,
                        "total_cost": 190376530.5308556
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6340
Profiling... [2048/50176]	Loss: 7.1828
Profiling... [3072/50176]	Loss: 5.2534
Profiling... [4096/50176]	Loss: 5.0031
Profiling... [5120/50176]	Loss: 4.9911
Profiling... [6144/50176]	Loss: 4.7581
Profiling... [7168/50176]	Loss: 4.6902
Profiling... [8192/50176]	Loss: 5.4007
Profiling... [9216/50176]	Loss: 4.8030
Profiling... [10240/50176]	Loss: 5.7073
Profiling... [11264/50176]	Loss: 4.6466
Profiling... [12288/50176]	Loss: 4.6654
Profiling... [13312/50176]	Loss: 4.7172
Profile done
epoch 1 train time consumed: 37.45s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.40324992747257,
                        "time": 27.907184071999836,
                        "accuracy": 0.009765625,
                        "total_cost": 500096738.57023704
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6314
Profiling... [2048/50176]	Loss: 7.2660
Profiling... [3072/50176]	Loss: 5.2933
Profiling... [4096/50176]	Loss: 4.9418
Profiling... [5120/50176]	Loss: 4.7227
Profiling... [6144/50176]	Loss: 4.7549
Profiling... [7168/50176]	Loss: 4.8243
Profiling... [8192/50176]	Loss: 4.5913
Profiling... [9216/50176]	Loss: 4.7773
Profiling... [10240/50176]	Loss: 4.7300
Profiling... [11264/50176]	Loss: 4.5988
Profiling... [12288/50176]	Loss: 4.6214
Profiling... [13312/50176]	Loss: 4.6757
Profile done
epoch 1 train time consumed: 12.67s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.52205178090392,
                        "time": 8.934603386000163,
                        "accuracy": 0.009765625,
                        "total_cost": 160108092.6771229
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6381
Profiling... [2048/50176]	Loss: 6.8999
Profiling... [3072/50176]	Loss: 5.3088
Profiling... [4096/50176]	Loss: 4.8219
Profiling... [5120/50176]	Loss: 4.7639
Profiling... [6144/50176]	Loss: 5.5478
Profiling... [7168/50176]	Loss: 4.6505
Profiling... [8192/50176]	Loss: 4.6630
Profiling... [9216/50176]	Loss: 4.5898
Profiling... [10240/50176]	Loss: 4.7462
Profiling... [11264/50176]	Loss: 4.6477
Profiling... [12288/50176]	Loss: 4.6859
Profiling... [13312/50176]	Loss: 4.8297
Profile done
epoch 1 train time consumed: 13.36s
Validation Epoch: 0, Average loss: 0.0073, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.69397743045822,
                        "time": 9.26730415899965,
                        "accuracy": 0.009765625,
                        "total_cost": 166070090.52927372
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6377
Profiling... [2048/50176]	Loss: 7.2476
Profiling... [3072/50176]	Loss: 5.6324
Profiling... [4096/50176]	Loss: 4.9089
Profiling... [5120/50176]	Loss: 4.8308
Profiling... [6144/50176]	Loss: 4.8484
Profiling... [7168/50176]	Loss: 4.9478
Profiling... [8192/50176]	Loss: 4.7284
Profiling... [9216/50176]	Loss: 4.7010
Profiling... [10240/50176]	Loss: 4.5955
Profiling... [11264/50176]	Loss: 4.6125
Profiling... [12288/50176]	Loss: 4.6208
Profiling... [13312/50176]	Loss: 4.7293
Profile done
epoch 1 train time consumed: 14.99s
Validation Epoch: 0, Average loss: 0.0061, Accuracy: 0.0105
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.73048266619814,
                        "time": 10.73266131499986,
                        "accuracy": 0.010546875,
                        "total_cost": 178082676.6340717
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6276
Profiling... [2048/50176]	Loss: 7.2084
Profiling... [3072/50176]	Loss: 5.5181
Profiling... [4096/50176]	Loss: 5.1751
Profiling... [5120/50176]	Loss: 4.8453
Profiling... [6144/50176]	Loss: 4.7761
Profiling... [7168/50176]	Loss: 4.6258
Profiling... [8192/50176]	Loss: 4.7712
Profiling... [9216/50176]	Loss: 4.8369
Profiling... [10240/50176]	Loss: 4.8362
Profiling... [11264/50176]	Loss: 4.6381
Profiling... [12288/50176]	Loss: 4.8238
Profiling... [13312/50176]	Loss: 4.6796
Profile done
epoch 1 train time consumed: 38.06s
Validation Epoch: 0, Average loss: 0.0076, Accuracy: 0.0146
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.39343585020676,
                        "time": 28.45647651299987,
                        "accuracy": 0.0146484375,
                        "total_cost": 339960039.4086384
                    },
                    
[Training Loop] The optimal parameters are lr: 0.01 dr: 0.25 pl: 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[GPU_0] Set GPU power limit to 125W.
[GPU_0] Set GPU power limit to 125W.
Training Epoch: 0 [128/50048]	Loss: 4.6451
Training Epoch: 0 [256/50048]	Loss: 7.9294
Training Epoch: 0 [384/50048]	Loss: 6.3578
Training Epoch: 0 [512/50048]	Loss: 5.8960
Training Epoch: 0 [640/50048]	Loss: 6.3590
Training Epoch: 0 [768/50048]	Loss: 4.9382
Training Epoch: 0 [896/50048]	Loss: 4.7085
Training Epoch: 0 [1024/50048]	Loss: 4.9745
Training Epoch: 0 [1152/50048]	Loss: 5.6901
Training Epoch: 0 [1280/50048]	Loss: 5.0531
Training Epoch: 0 [1408/50048]	Loss: 4.6226
Training Epoch: 0 [1536/50048]	Loss: 4.8148
Training Epoch: 0 [1664/50048]	Loss: 5.0294
Training Epoch: 0 [1792/50048]	Loss: 4.8349
Training Epoch: 0 [1920/50048]	Loss: 4.8840
Training Epoch: 0 [2048/50048]	Loss: 4.6530
Training Epoch: 0 [2176/50048]	Loss: 4.7721
Training Epoch: 0 [2304/50048]	Loss: 4.6970
Training Epoch: 0 [2432/50048]	Loss: 4.6264
Training Epoch: 0 [2560/50048]	Loss: 4.6370
Training Epoch: 0 [2688/50048]	Loss: 4.6115
Training Epoch: 0 [2816/50048]	Loss: 4.6113
Training Epoch: 0 [2944/50048]	Loss: 4.6056
Training Epoch: 0 [3072/50048]	Loss: 4.5965
Training Epoch: 0 [3200/50048]	Loss: 4.5938
Training Epoch: 0 [3328/50048]	Loss: 4.6075
Training Epoch: 0 [3456/50048]	Loss: 4.6135
Training Epoch: 0 [3584/50048]	Loss: 4.5994
Training Epoch: 0 [3712/50048]	Loss: 4.5948
Training Epoch: 0 [3840/50048]	Loss: 4.5844
Training Epoch: 0 [3968/50048]	Loss: 4.5825
Training Epoch: 0 [4096/50048]	Loss: 4.5884
Training Epoch: 0 [4224/50048]	Loss: 4.5790
Training Epoch: 0 [4352/50048]	Loss: 4.5694
Training Epoch: 0 [4480/50048]	Loss: 4.5836
Training Epoch: 0 [4608/50048]	Loss: 4.5749
Training Epoch: 0 [4736/50048]	Loss: 4.5916
Training Epoch: 0 [4864/50048]	Loss: 4.5838
Training Epoch: 0 [4992/50048]	Loss: 4.5581
Training Epoch: 0 [5120/50048]	Loss: 4.5727
Training Epoch: 0 [5248/50048]	Loss: 4.5731
Training Epoch: 0 [5376/50048]	Loss: 4.5640
Training Epoch: 0 [5504/50048]	Loss: 4.5777
Training Epoch: 0 [5632/50048]	Loss: 4.5697
Training Epoch: 0 [5760/50048]	Loss: 4.5649
Training Epoch: 0 [5888/50048]	Loss: 4.5829
Training Epoch: 0 [6016/50048]	Loss: 4.5814
Training Epoch: 0 [6144/50048]	Loss: 4.5618
Training Epoch: 0 [6272/50048]	Loss: 4.5223
Training Epoch: 0 [6400/50048]	Loss: 4.5530
Training Epoch: 0 [6528/50048]	Loss: 4.5244
Training Epoch: 0 [6656/50048]	Loss: 4.5516
Training Epoch: 0 [6784/50048]	Loss: 4.5090
Training Epoch: 0 [6912/50048]	Loss: 4.4654
Training Epoch: 0 [7040/50048]	Loss: 4.5431
Training Epoch: 0 [7168/50048]	Loss: 4.4967
Training Epoch: 0 [7296/50048]	Loss: 4.5177
Training Epoch: 0 [7424/50048]	Loss: 4.5765
Training Epoch: 0 [7552/50048]	Loss: 4.5604
Training Epoch: 0 [7680/50048]	Loss: 4.4631
Training Epoch: 0 [7808/50048]	Loss: 4.4413
Training Epoch: 0 [7936/50048]	Loss: 4.4585
Training Epoch: 0 [8064/50048]	Loss: 4.5066
Training Epoch: 0 [8192/50048]	Loss: 4.4719
Training Epoch: 0 [8320/50048]	Loss: 4.4337
Training Epoch: 0 [8448/50048]	Loss: 4.5073
Training Epoch: 0 [8576/50048]	Loss: 4.4214
Training Epoch: 0 [8704/50048]	Loss: 4.4545
Training Epoch: 0 [8832/50048]	Loss: 4.5026
Training Epoch: 0 [8960/50048]	Loss: 4.4255
Training Epoch: 0 [9088/50048]	Loss: 4.5291
Training Epoch: 0 [9216/50048]	Loss: 4.4353
Training Epoch: 0 [9344/50048]	Loss: 4.5332
Training Epoch: 0 [9472/50048]	Loss: 4.4413
Training Epoch: 0 [9600/50048]	Loss: 4.4477
Training Epoch: 0 [9728/50048]	Loss: 4.3717
Training Epoch: 0 [9856/50048]	Loss: 4.4210
Training Epoch: 0 [9984/50048]	Loss: 4.4294
Training Epoch: 0 [10112/50048]	Loss: 4.5007
Training Epoch: 0 [10240/50048]	Loss: 4.4261
Training Epoch: 0 [10368/50048]	Loss: 4.4152
Training Epoch: 0 [10496/50048]	Loss: 4.3140
Training Epoch: 0 [10624/50048]	Loss: 4.3604
Training Epoch: 0 [10752/50048]	Loss: 4.4016
Training Epoch: 0 [10880/50048]	Loss: 4.4640
Training Epoch: 0 [11008/50048]	Loss: 4.3899
Training Epoch: 0 [11136/50048]	Loss: 4.3846
Training Epoch: 0 [11264/50048]	Loss: 4.5669
Training Epoch: 0 [11392/50048]	Loss: 4.4329
Training Epoch: 0 [11520/50048]	Loss: 4.3991
Training Epoch: 0 [11648/50048]	Loss: 4.3434
Training Epoch: 0 [11776/50048]	Loss: 4.3258
Training Epoch: 0 [11904/50048]	Loss: 4.3640
Training Epoch: 0 [12032/50048]	Loss: 4.3306
Training Epoch: 0 [12160/50048]	Loss: 4.4170
Training Epoch: 0 [12288/50048]	Loss: 4.1956
Training Epoch: 0 [12416/50048]	Loss: 4.2567
Training Epoch: 0 [12544/50048]	Loss: 4.3588
Training Epoch: 0 [12672/50048]	Loss: 4.3764
Training Epoch: 0 [12800/50048]	Loss: 4.2240
Training Epoch: 0 [12928/50048]	Loss: 4.3355
Training Epoch: 0 [13056/50048]	Loss: 4.2981
Training Epoch: 0 [13184/50048]	Loss: 4.4066
Training Epoch: 0 [13312/50048]	Loss: 4.3480
Training Epoch: 0 [13440/50048]	Loss: 4.1580
Training Epoch: 0 [13568/50048]	Loss: 4.3239
Training Epoch: 0 [13696/50048]	Loss: 4.3159
Training Epoch: 0 [13824/50048]	Loss: 4.4089
Training Epoch: 0 [13952/50048]	Loss: 4.3478
Training Epoch: 0 [14080/50048]	Loss: 4.2340
Training Epoch: 0 [14208/50048]	Loss: 4.2086
Training Epoch: 0 [14336/50048]	Loss: 4.2567
Training Epoch: 0 [14464/50048]	Loss: 4.3390
Training Epoch: 0 [14592/50048]	Loss: 4.4135
Training Epoch: 0 [14720/50048]	Loss: 4.2649
Training Epoch: 0 [14848/50048]	Loss: 4.1886
Training Epoch: 0 [14976/50048]	Loss: 4.2515
Training Epoch: 0 [15104/50048]	Loss: 4.2940
Training Epoch: 0 [15232/50048]	Loss: 4.3784
Training Epoch: 0 [15360/50048]	Loss: 4.3118
Training Epoch: 0 [15488/50048]	Loss: 4.2349
Training Epoch: 0 [15616/50048]	Loss: 4.2474
Training Epoch: 0 [15744/50048]	Loss: 4.2876
Training Epoch: 0 [15872/50048]	Loss: 4.1947
Training Epoch: 0 [16000/50048]	Loss: 4.3344
Training Epoch: 0 [16128/50048]	Loss: 4.2886
Training Epoch: 0 [16256/50048]	Loss: 4.2170
Training Epoch: 0 [16384/50048]	Loss: 4.2654
Training Epoch: 0 [16512/50048]	Loss: 4.2234
Training Epoch: 0 [16640/50048]	Loss: 4.0822
Training Epoch: 0 [16768/50048]	Loss: 4.2941
Training Epoch: 0 [16896/50048]	Loss: 4.1018
Training Epoch: 0 [17024/50048]	Loss: 4.2398
Training Epoch: 0 [17152/50048]	Loss: 4.0279
Training Epoch: 0 [17280/50048]	Loss: 4.1383
Training Epoch: 0 [17408/50048]	Loss: 4.1838
Training Epoch: 0 [17536/50048]	Loss: 4.1896
Training Epoch: 0 [17664/50048]	Loss: 4.1395
Training Epoch: 0 [17792/50048]	Loss: 4.1530
Training Epoch: 0 [17920/50048]	Loss: 4.3058
Training Epoch: 0 [18048/50048]	Loss: 4.1578
Training Epoch: 0 [18176/50048]	Loss: 4.1970
Training Epoch: 0 [18304/50048]	Loss: 4.1801
Training Epoch: 0 [18432/50048]	Loss: 4.3004
Training Epoch: 0 [18560/50048]	Loss: 4.1882
Training Epoch: 0 [18688/50048]	Loss: 4.2457
Training Epoch: 0 [18816/50048]	Loss: 4.1918
Training Epoch: 0 [18944/50048]	Loss: 4.1164
Training Epoch: 0 [19072/50048]	Loss: 4.2798
Training Epoch: 0 [19200/50048]	Loss: 4.2530
Training Epoch: 0 [19328/50048]	Loss: 4.1626
Training Epoch: 0 [19456/50048]	Loss: 4.1732
Training Epoch: 0 [19584/50048]	Loss: 4.1908
Training Epoch: 0 [19712/50048]	Loss: 4.1433
Training Epoch: 0 [19840/50048]	Loss: 4.1465
Training Epoch: 0 [19968/50048]	Loss: 4.1661
Training Epoch: 0 [20096/50048]	Loss: 4.1742
Training Epoch: 0 [20224/50048]	Loss: 4.2482
Training Epoch: 0 [20352/50048]	Loss: 4.3465
Training Epoch: 0 [20480/50048]	Loss: 4.1335
Training Epoch: 0 [20608/50048]	Loss: 4.3381
Training Epoch: 0 [20736/50048]	Loss: 4.0706
Training Epoch: 0 [20864/50048]	Loss: 4.1212
Training Epoch: 0 [20992/50048]	Loss: 4.1890
Training Epoch: 0 [21120/50048]	Loss: 4.1642
Training Epoch: 0 [21248/50048]	Loss: 4.2320
Training Epoch: 0 [21376/50048]	Loss: 4.1931
Training Epoch: 0 [21504/50048]	Loss: 4.2305
Training Epoch: 0 [21632/50048]	Loss: 4.1822
Training Epoch: 0 [21760/50048]	Loss: 4.2360
Training Epoch: 0 [21888/50048]	Loss: 4.2130
Training Epoch: 0 [22016/50048]	Loss: 4.3515
Training Epoch: 0 [22144/50048]	Loss: 4.2340
Training Epoch: 0 [22272/50048]	Loss: 4.1454
Training Epoch: 0 [22400/50048]	Loss: 4.1417
Training Epoch: 0 [22528/50048]	Loss: 4.1516
Training Epoch: 0 [22656/50048]	Loss: 4.1156
Training Epoch: 0 [22784/50048]	Loss: 3.9832
Training Epoch: 0 [22912/50048]	Loss: 4.1716
Training Epoch: 0 [23040/50048]	Loss: 4.1535
Training Epoch: 0 [23168/50048]	Loss: 4.2955
Training Epoch: 0 [23296/50048]	Loss: 4.1627
Training Epoch: 0 [23424/50048]	Loss: 4.1159
Training Epoch: 0 [23552/50048]	Loss: 4.1024
Training Epoch: 0 [23680/50048]	Loss: 4.1367
Training Epoch: 0 [23808/50048]	Loss: 4.1097
Training Epoch: 0 [23936/50048]	Loss: 4.1512
Training Epoch: 0 [24064/50048]	Loss: 4.1941
Training Epoch: 0 [24192/50048]	Loss: 4.2055
Training Epoch: 0 [24320/50048]	Loss: 3.9635
Training Epoch: 0 [24448/50048]	Loss: 4.2486
Training Epoch: 0 [24576/50048]	Loss: 4.1797
Training Epoch: 0 [24704/50048]	Loss: 3.9623
Training Epoch: 0 [24832/50048]	Loss: 4.0958
Training Epoch: 0 [24960/50048]	Loss: 3.9766
Training Epoch: 0 [25088/50048]	Loss: 4.1665
Training Epoch: 0 [25216/50048]	Loss: 4.2692
Training Epoch: 0 [25344/50048]	Loss: 4.1305
Training Epoch: 0 [25472/50048]	Loss: 3.9787
Training Epoch: 0 [25600/50048]	Loss: 4.1670
Training Epoch: 0 [25728/50048]	Loss: 4.3252
Training Epoch: 0 [25856/50048]	Loss: 4.1153
Training Epoch: 0 [25984/50048]	Loss: 4.0737
Training Epoch: 0 [26112/50048]	Loss: 4.1145
Training Epoch: 0 [26240/50048]	Loss: 4.1378
Training Epoch: 0 [26368/50048]	Loss: 4.0665
Training Epoch: 0 [26496/50048]	Loss: 3.9812
Training Epoch: 0 [26624/50048]	Loss: 4.0020
Training Epoch: 0 [26752/50048]	Loss: 4.0567
Training Epoch: 0 [26880/50048]	Loss: 4.1146
Training Epoch: 0 [27008/50048]	Loss: 4.1430
Training Epoch: 0 [27136/50048]	Loss: 4.1198
Training Epoch: 0 [27264/50048]	Loss: 4.1499
Training Epoch: 0 [27392/50048]	Loss: 4.1096
Training Epoch: 0 [27520/50048]	Loss: 4.0309
Training Epoch: 0 [27648/50048]	Loss: 4.0509
Training Epoch: 0 [27776/50048]	Loss: 4.1773
Training Epoch: 0 [27904/50048]	Loss: 4.1029
Training Epoch: 0 [28032/50048]	Loss: 4.1529
Training Epoch: 0 [28160/50048]	Loss: 4.1893
Training Epoch: 0 [28288/50048]	Loss: 4.0916
Training Epoch: 0 [28416/50048]	Loss: 3.9641
Training Epoch: 0 [28544/50048]	Loss: 4.0122
Training Epoch: 0 [28672/50048]	Loss: 4.1806
Training Epoch: 0 [28800/50048]	Loss: 4.0339
Training Epoch: 0 [28928/50048]	Loss: 3.8352
Training Epoch: 0 [29056/50048]	Loss: 4.2169
Training Epoch: 0 [29184/50048]	Loss: 4.0954
Training Epoch: 0 [29312/50048]	Loss: 4.1086
Training Epoch: 0 [29440/50048]	Loss: 4.0025
Training Epoch: 0 [29568/50048]	Loss: 4.0909
Training Epoch: 0 [29696/50048]	Loss: 4.2313
Training Epoch: 0 [29824/50048]	Loss: 4.1689
Training Epoch: 0 [29952/50048]	Loss: 4.1471
Training Epoch: 0 [30080/50048]	Loss: 4.0896
Training Epoch: 0 [30208/50048]	Loss: 3.9909
Training Epoch: 0 [30336/50048]	Loss: 3.9875
Training Epoch: 0 [30464/50048]	Loss: 3.9506
Training Epoch: 0 [30592/50048]	Loss: 4.0556
Training Epoch: 0 [30720/50048]	Loss: 4.0936
Training Epoch: 0 [30848/50048]	Loss: 4.0635
Training Epoch: 0 [30976/50048]	Loss: 4.0108
Training Epoch: 0 [31104/50048]	Loss: 4.1304
Training Epoch: 0 [31232/50048]	Loss: 4.0106
Training Epoch: 0 [31360/50048]	Loss: 3.8674
Training Epoch: 0 [31488/50048]	Loss: 4.2710
Training Epoch: 0 [31616/50048]	Loss: 4.0650
Training Epoch: 0 [31744/50048]	Loss: 4.1151
Training Epoch: 0 [31872/50048]	Loss: 4.2954
Training Epoch: 0 [32000/50048]	Loss: 4.2711
Training Epoch: 0 [32128/50048]	Loss: 3.9926
Training Epoch: 0 [32256/50048]	Loss: 3.9256
Training Epoch: 0 [32384/50048]	Loss: 4.0972
Training Epoch: 0 [32512/50048]	Loss: 4.0908
Training Epoch: 0 [32640/50048]	Loss: 3.8870
Training Epoch: 0 [32768/50048]	Loss: 4.0776
Training Epoch: 0 [32896/50048]	Loss: 3.8505
Training Epoch: 0 [33024/50048]	Loss: 4.0562
Training Epoch: 0 [33152/50048]	Loss: 4.0347
Training Epoch: 0 [33280/50048]	Loss: 4.1842
Training Epoch: 0 [33408/50048]	Loss: 3.9936
Training Epoch: 0 [33536/50048]	Loss: 4.0582
Training Epoch: 0 [33664/50048]	Loss: 3.8900
Training Epoch: 0 [33792/50048]	Loss: 4.0696
Training Epoch: 0 [33920/50048]	Loss: 4.1399
Training Epoch: 0 [34048/50048]	Loss: 3.8388
Training Epoch: 0 [34176/50048]	Loss: 4.0389
Training Epoch: 0 [34304/50048]	Loss: 4.1019
Training Epoch: 0 [34432/50048]	Loss: 3.9228
Training Epoch: 0 [34560/50048]	Loss: 4.0556
Training Epoch: 0 [34688/50048]	Loss: 3.9235
Training Epoch: 0 [34816/50048]	Loss: 4.1362
Training Epoch: 0 [34944/50048]	Loss: 3.9878
Training Epoch: 0 [35072/50048]	Loss: 3.9169
Training Epoch: 0 [35200/50048]	Loss: 4.0655
Training Epoch: 0 [35328/50048]	Loss: 4.0044
Training Epoch: 0 [35456/50048]	Loss: 3.8690
Training Epoch: 0 [35584/50048]	Loss: 4.0614
Training Epoch: 0 [35712/50048]	Loss: 3.7704
Training Epoch: 0 [35840/50048]	Loss: 3.7510
Training Epoch: 0 [35968/50048]	Loss: 4.0053
Training Epoch: 0 [36096/50048]	Loss: 4.1012
Training Epoch: 0 [36224/50048]	Loss: 3.9948
Training Epoch: 0 [36352/50048]	Loss: 4.0091
Training Epoch: 0 [36480/50048]	Loss: 3.8309
Training Epoch: 0 [36608/50048]	Loss: 3.9384
Training Epoch: 0 [36736/50048]	Loss: 3.9417
Training Epoch: 0 [36864/50048]	Loss: 4.0915
Training Epoch: 0 [36992/50048]	Loss: 3.8976
Training Epoch: 0 [37120/50048]	Loss: 4.1048
Training Epoch: 0 [37248/50048]	Loss: 4.0543
Training Epoch: 0 [37376/50048]	Loss: 3.9417
Training Epoch: 0 [37504/50048]	Loss: 3.9266
Training Epoch: 0 [37632/50048]	Loss: 4.0033
Training Epoch: 0 [37760/50048]	Loss: 4.0165
Training Epoch: 0 [37888/50048]	Loss: 4.0180
Training Epoch: 0 [38016/50048]	Loss: 3.8259
Training Epoch: 0 [38144/50048]	Loss: 3.9709
Training Epoch: 0 [38272/50048]	Loss: 3.8810
Training Epoch: 0 [38400/50048]	Loss: 3.9296
Training Epoch: 0 [38528/50048]	Loss: 3.8898
Training Epoch: 0 [38656/50048]	Loss: 3.9549
Training Epoch: 0 [38784/50048]	Loss: 4.1277
Training Epoch: 0 [38912/50048]	Loss: 4.0610
Training Epoch: 0 [39040/50048]	Loss: 3.9935
Training Epoch: 0 [39168/50048]	Loss: 3.8820
Training Epoch: 0 [39296/50048]	Loss: 4.1192
Training Epoch: 0 [39424/50048]	Loss: 3.8882
Training Epoch: 0 [39552/50048]	Loss: 3.9771
Training Epoch: 0 [39680/50048]	Loss: 4.0590
Training Epoch: 0 [39808/50048]	Loss: 4.0907
Training Epoch: 0 [39936/50048]	Loss: 3.8804
Training Epoch: 0 [40064/50048]	Loss: 3.8962
Training Epoch: 0 [40192/50048]	Loss: 3.8276
Training Epoch: 0 [40320/50048]	Loss: 3.9878
Training Epoch: 0 [40448/50048]	Loss: 3.9668
Training Epoch: 0 [40576/50048]	Loss: 3.9032
Training Epoch: 0 [40704/50048]	Loss: 3.9535
Training Epoch: 0 [40832/50048]	Loss: 3.9340
Training Epoch: 0 [40960/50048]	Loss: 3.9011
Training Epoch: 0 [41088/50048]	Loss: 4.1224
Training Epoch: 0 [41216/50048]	Loss: 3.8684
Training Epoch: 0 [41344/50048]	Loss: 3.8984
Training Epoch: 0 [41472/50048]	Loss: 3.9947
Training Epoch: 0 [41600/50048]	Loss: 3.8330
Training Epoch: 0 [41728/50048]	Loss: 3.8447
Training Epoch: 0 [41856/50048]	Loss: 4.0249
Training Epoch: 0 [41984/50048]	Loss: 3.7900
Training Epoch: 0 [42112/50048]	Loss: 3.9850
Training Epoch: 0 [42240/50048]	Loss: 4.0912
Training Epoch: 0 [42368/50048]	Loss: 4.0892
Training Epoch: 0 [42496/50048]	Loss: 4.0675
Training Epoch: 0 [42624/50048]	Loss: 3.9906
Training Epoch: 0 [42752/50048]	Loss: 3.8015
Training Epoch: 0 [42880/50048]	Loss: 3.9620
Training Epoch: 0 [43008/50048]	Loss: 4.0341
Training Epoch: 0 [43136/50048]	Loss: 3.9042
Training Epoch: 0 [43264/50048]	Loss: 3.9910
Training Epoch: 0 [43392/50048]	Loss: 3.9409
Training Epoch: 0 [43520/50048]	Loss: 3.8772
Training Epoch: 0 [43648/50048]	Loss: 3.7803
Training Epoch: 0 [43776/50048]	Loss: 3.9306
Training Epoch: 0 [43904/50048]	Loss: 3.8407
Training Epoch: 0 [44032/50048]	Loss: 3.9259
Training Epoch: 0 [44160/50048]	Loss: 3.7785
Training Epoch: 0 [44288/50048]	Loss: 3.6652
Training Epoch: 0 [44416/50048]	Loss: 3.7661
Training Epoch: 0 [44544/50048]	Loss: 3.7353
Training Epoch: 0 [44672/50048]	Loss: 4.0969
Training Epoch: 0 [44800/50048]	Loss: 3.7698
Training Epoch: 0 [44928/50048]	Loss: 3.9580
Training Epoch: 0 [45056/50048]	Loss: 3.8414
Training Epoch: 0 [45184/50048]	Loss: 3.8009
Training Epoch: 0 [45312/50048]	Loss: 3.8972
Training Epoch: 0 [45440/50048]	Loss: 3.8768
Training Epoch: 0 [45568/50048]	Loss: 4.0052
Training Epoch: 0 [45696/50048]	Loss: 3.8298
Training Epoch: 0 [45824/50048]	Loss: 3.8411
Training Epoch: 0 [45952/50048]	Loss: 3.9218
Training Epoch: 0 [46080/50048]	Loss: 3.9204
Training Epoch: 0 [46208/50048]	Loss: 3.9294
Training Epoch: 0 [46336/50048]	Loss: 3.8177
Training Epoch: 0 [46464/50048]	Loss: 4.0086
Training Epoch: 0 [46592/50048]	Loss: 3.7469
Training Epoch: 0 [46720/50048]	Loss: 3.8515
Training Epoch: 0 [46848/50048]	Loss: 4.0259
Training Epoch: 0 [46976/50048]	Loss: 3.9140
Training Epoch: 0 [47104/50048]	Loss: 3.7664
Training Epoch: 0 [47232/50048]	Loss: 3.7707
Training Epoch: 0 [47360/50048]	Loss: 3.9239
Training Epoch: 0 [47488/50048]	Loss: 3.8511
Training Epoch: 0 [47616/50048]	Loss: 3.9406
Training Epoch: 0 [47744/50048]	Loss: 3.8872
Training Epoch: 0 [47872/50048]	Loss: 3.8929
Training Epoch: 0 [48000/50048]	Loss: 3.8901
Training Epoch: 0 [48128/50048]	Loss: 3.7286
Training Epoch: 0 [48256/50048]	Loss: 3.7984
Training Epoch: 0 [48384/50048]	Loss: 3.9143
Training Epoch: 0 [48512/50048]	Loss: 4.0405
Training Epoch: 0 [48640/50048]	Loss: 3.7865
Training Epoch: 0 [48768/50048]	Loss: 3.6593
Training Epoch: 0 [48896/50048]	Loss: 3.9656
Training Epoch: 0 [49024/50048]	Loss: 3.8142
Training Epoch: 0 [49152/50048]	Loss: 4.0377
Training Epoch: 0 [49280/50048]	Loss: 3.8407
Training Epoch: 0 [49408/50048]	Loss: 3.9766
Training Epoch: 0 [49536/50048]	Loss: 3.7326
Training Epoch: 0 [49664/50048]	Loss: 3.8309
Training Epoch: 0 [49792/50048]	Loss: 3.8071
Training Epoch: 0 [49920/50048]	Loss: 3.8503
Training Epoch: 0 [50048/50048]	Loss: 3.9376
Validation Epoch: 0, Average loss: 0.0317, Accuracy: 0.0765
Training Epoch: 1 [128/50048]	Loss: 3.9800
Training Epoch: 1 [256/50048]	Loss: 3.9273
Training Epoch: 1 [384/50048]	Loss: 3.8208
Training Epoch: 1 [512/50048]	Loss: 3.7158
Training Epoch: 1 [640/50048]	Loss: 3.6358
Training Epoch: 1 [768/50048]	Loss: 3.8727
Training Epoch: 1 [896/50048]	Loss: 3.8326
Training Epoch: 1 [1024/50048]	Loss: 3.7971
Training Epoch: 1 [1152/50048]	Loss: 3.7003
Training Epoch: 1 [1280/50048]	Loss: 3.7827
Training Epoch: 1 [1408/50048]	Loss: 3.9364
Training Epoch: 1 [1536/50048]	Loss: 3.8890
Training Epoch: 1 [1664/50048]	Loss: 3.6807
Training Epoch: 1 [1792/50048]	Loss: 3.6865
Training Epoch: 1 [1920/50048]	Loss: 3.6649
Training Epoch: 1 [2048/50048]	Loss: 3.8696
Training Epoch: 1 [2176/50048]	Loss: 3.8227
Training Epoch: 1 [2304/50048]	Loss: 3.8091
Training Epoch: 1 [2432/50048]	Loss: 3.8185
Training Epoch: 1 [2560/50048]	Loss: 3.8041
Training Epoch: 1 [2688/50048]	Loss: 4.0961
Training Epoch: 1 [2816/50048]	Loss: 3.6111
Training Epoch: 1 [2944/50048]	Loss: 3.7522
Training Epoch: 1 [3072/50048]	Loss: 3.7584
Training Epoch: 1 [3200/50048]	Loss: 3.7024
Training Epoch: 1 [3328/50048]	Loss: 3.7464
Training Epoch: 1 [3456/50048]	Loss: 3.6270
Training Epoch: 1 [3584/50048]	Loss: 3.7975
Training Epoch: 1 [3712/50048]	Loss: 3.7651
Training Epoch: 1 [3840/50048]	Loss: 3.6944
Training Epoch: 1 [3968/50048]	Loss: 3.7298
Training Epoch: 1 [4096/50048]	Loss: 4.0042
Training Epoch: 1 [4224/50048]	Loss: 3.7249
Training Epoch: 1 [4352/50048]	Loss: 3.9533
Training Epoch: 1 [4480/50048]	Loss: 3.6847
Training Epoch: 1 [4608/50048]	Loss: 3.7573
Training Epoch: 1 [4736/50048]	Loss: 3.8309
Training Epoch: 1 [4864/50048]	Loss: 3.8625
Training Epoch: 1 [4992/50048]	Loss: 3.7185
Training Epoch: 1 [5120/50048]	Loss: 3.8103
Training Epoch: 1 [5248/50048]	Loss: 3.7118
Training Epoch: 1 [5376/50048]	Loss: 3.8312
Training Epoch: 1 [5504/50048]	Loss: 3.7667
Training Epoch: 1 [5632/50048]	Loss: 3.7832
Training Epoch: 1 [5760/50048]	Loss: 3.8400
Training Epoch: 1 [5888/50048]	Loss: 3.6712
Training Epoch: 1 [6016/50048]	Loss: 3.9303
Training Epoch: 1 [6144/50048]	Loss: 3.7215
Training Epoch: 1 [6272/50048]	Loss: 3.8172
Training Epoch: 1 [6400/50048]	Loss: 3.8566
Training Epoch: 1 [6528/50048]	Loss: 3.6854
Training Epoch: 1 [6656/50048]	Loss: 3.9082
Training Epoch: 1 [6784/50048]	Loss: 3.7445
Training Epoch: 1 [6912/50048]	Loss: 3.7275
Training Epoch: 1 [7040/50048]	Loss: 3.7887
Training Epoch: 1 [7168/50048]	Loss: 4.0006
Training Epoch: 1 [7296/50048]	Loss: 3.7147
Training Epoch: 1 [7424/50048]	Loss: 3.6958
Training Epoch: 1 [7552/50048]	Loss: 3.7589
Training Epoch: 1 [7680/50048]	Loss: 3.7298
Training Epoch: 1 [7808/50048]	Loss: 3.7048
Training Epoch: 1 [7936/50048]	Loss: 3.6552
Training Epoch: 1 [8064/50048]	Loss: 3.7533
Training Epoch: 1 [8192/50048]	Loss: 3.6367
Training Epoch: 1 [8320/50048]	Loss: 3.6957
Training Epoch: 1 [8448/50048]	Loss: 3.6715
Training Epoch: 1 [8576/50048]	Loss: 3.5986
Training Epoch: 1 [8704/50048]	Loss: 3.4193
Training Epoch: 1 [8832/50048]	Loss: 3.6903
Training Epoch: 1 [8960/50048]	Loss: 3.9179
Training Epoch: 1 [9088/50048]	Loss: 3.8802
Training Epoch: 1 [9216/50048]	Loss: 3.8803
Training Epoch: 1 [9344/50048]	Loss: 3.7675
Training Epoch: 1 [9472/50048]	Loss: 3.8540
Training Epoch: 1 [9600/50048]	Loss: 3.7608
Training Epoch: 1 [9728/50048]	Loss: 3.8478
Training Epoch: 1 [9856/50048]	Loss: 3.7074
Training Epoch: 1 [9984/50048]	Loss: 3.6125
Training Epoch: 1 [10112/50048]	Loss: 3.6220
Training Epoch: 1 [10240/50048]	Loss: 3.7709
Training Epoch: 1 [10368/50048]	Loss: 3.6380
Training Epoch: 1 [10496/50048]	Loss: 3.7356
Training Epoch: 1 [10624/50048]	Loss: 3.5747
Training Epoch: 1 [10752/50048]	Loss: 3.7145
Training Epoch: 1 [10880/50048]	Loss: 3.7287
Training Epoch: 1 [11008/50048]	Loss: 3.6620
Training Epoch: 1 [11136/50048]	Loss: 3.6121
Training Epoch: 1 [11264/50048]	Loss: 3.5567
Training Epoch: 1 [11392/50048]	Loss: 3.6709
Training Epoch: 1 [11520/50048]	Loss: 3.6191
Training Epoch: 1 [11648/50048]	Loss: 3.7860
Training Epoch: 1 [11776/50048]	Loss: 3.9169
Training Epoch: 1 [11904/50048]	Loss: 3.6820
Training Epoch: 1 [12032/50048]	Loss: 3.6106
Training Epoch: 1 [12160/50048]	Loss: 3.5867
Training Epoch: 1 [12288/50048]	Loss: 3.6319
Training Epoch: 1 [12416/50048]	Loss: 3.6534
Training Epoch: 1 [12544/50048]	Loss: 3.6161
Training Epoch: 1 [12672/50048]	Loss: 3.7953
Training Epoch: 1 [12800/50048]	Loss: 3.6035
Training Epoch: 1 [12928/50048]	Loss: 3.7603
Training Epoch: 1 [13056/50048]	Loss: 3.6646
Training Epoch: 1 [13184/50048]	Loss: 3.7775
Training Epoch: 1 [13312/50048]	Loss: 3.8322
Training Epoch: 1 [13440/50048]	Loss: 3.7534
Training Epoch: 1 [13568/50048]	Loss: 3.8193
Training Epoch: 1 [13696/50048]	Loss: 3.6812
Training Epoch: 1 [13824/50048]	Loss: 3.6133
Training Epoch: 1 [13952/50048]	Loss: 3.7394
Training Epoch: 1 [14080/50048]	Loss: 3.5823
Training Epoch: 1 [14208/50048]	Loss: 3.7376
Training Epoch: 1 [14336/50048]	Loss: 3.7498
Training Epoch: 1 [14464/50048]	Loss: 3.6056
Training Epoch: 1 [14592/50048]	Loss: 3.4743
Training Epoch: 1 [14720/50048]	Loss: 3.5312
Training Epoch: 1 [14848/50048]	Loss: 3.6463
Training Epoch: 1 [14976/50048]	Loss: 3.7024
Training Epoch: 1 [15104/50048]	Loss: 3.6257
Training Epoch: 1 [15232/50048]	Loss: 3.7072
Training Epoch: 1 [15360/50048]	Loss: 3.8901
Training Epoch: 1 [15488/50048]	Loss: 3.7964
Training Epoch: 1 [15616/50048]	Loss: 3.6937
Training Epoch: 1 [15744/50048]	Loss: 3.7145
Training Epoch: 1 [15872/50048]	Loss: 3.8906
Training Epoch: 1 [16000/50048]	Loss: 3.8839
Training Epoch: 1 [16128/50048]	Loss: 3.7505
Training Epoch: 1 [16256/50048]	Loss: 3.5390
Training Epoch: 1 [16384/50048]	Loss: 3.6106
Training Epoch: 1 [16512/50048]	Loss: 3.7867
Training Epoch: 1 [16640/50048]	Loss: 3.9424
Training Epoch: 1 [16768/50048]	Loss: 3.6387
Training Epoch: 1 [16896/50048]	Loss: 3.6778
Training Epoch: 1 [17024/50048]	Loss: 3.5916
Training Epoch: 1 [17152/50048]	Loss: 3.8386
Training Epoch: 1 [17280/50048]	Loss: 3.8921
Training Epoch: 1 [17408/50048]	Loss: 3.8529
Training Epoch: 1 [17536/50048]	Loss: 3.7335
Training Epoch: 1 [17664/50048]	Loss: 3.6546
Training Epoch: 1 [17792/50048]	Loss: 3.7228
Training Epoch: 1 [17920/50048]	Loss: 3.7421
Training Epoch: 1 [18048/50048]	Loss: 3.7030
Training Epoch: 1 [18176/50048]	Loss: 3.5943
Training Epoch: 1 [18304/50048]	Loss: 3.6827
Training Epoch: 1 [18432/50048]	Loss: 3.7588
Training Epoch: 1 [18560/50048]	Loss: 3.6532
Training Epoch: 1 [18688/50048]	Loss: 3.6335
Training Epoch: 1 [18816/50048]	Loss: 3.6379
Training Epoch: 1 [18944/50048]	Loss: 3.6267
Training Epoch: 1 [19072/50048]	Loss: 3.8134
Training Epoch: 1 [19200/50048]	Loss: 3.6345
Training Epoch: 1 [19328/50048]	Loss: 3.5491
Training Epoch: 1 [19456/50048]	Loss: 3.6355
Training Epoch: 1 [19584/50048]	Loss: 3.8277
Training Epoch: 1 [19712/50048]	Loss: 3.7140
Training Epoch: 1 [19840/50048]	Loss: 3.5524
Training Epoch: 1 [19968/50048]	Loss: 3.6114
Training Epoch: 1 [20096/50048]	Loss: 3.6749
Training Epoch: 1 [20224/50048]	Loss: 3.7630
Training Epoch: 1 [20352/50048]	Loss: 3.4831
Training Epoch: 1 [20480/50048]	Loss: 3.6179
Training Epoch: 1 [20608/50048]	Loss: 3.8639
Training Epoch: 1 [20736/50048]	Loss: 3.6090
Training Epoch: 1 [20864/50048]	Loss: 3.5291
Training Epoch: 1 [20992/50048]	Loss: 3.7786
Training Epoch: 1 [21120/50048]	Loss: 3.5754
Training Epoch: 1 [21248/50048]	Loss: 3.6634
Training Epoch: 1 [21376/50048]	Loss: 3.4943
Training Epoch: 1 [21504/50048]	Loss: 3.7783
Training Epoch: 1 [21632/50048]	Loss: 3.6570
Training Epoch: 1 [21760/50048]	Loss: 3.7631
Training Epoch: 1 [21888/50048]	Loss: 3.6375
Training Epoch: 1 [22016/50048]	Loss: 3.5903
Training Epoch: 1 [22144/50048]	Loss: 3.7649
Training Epoch: 1 [22272/50048]	Loss: 3.7177
Training Epoch: 1 [22400/50048]	Loss: 3.7109
Training Epoch: 1 [22528/50048]	Loss: 4.0124
Training Epoch: 1 [22656/50048]	Loss: 3.5481
Training Epoch: 1 [22784/50048]	Loss: 3.8485
Training Epoch: 1 [22912/50048]	Loss: 3.8882
Training Epoch: 1 [23040/50048]	Loss: 3.6424
Training Epoch: 1 [23168/50048]	Loss: 3.6460
Training Epoch: 1 [23296/50048]	Loss: 3.5951
Training Epoch: 1 [23424/50048]	Loss: 3.7523
Training Epoch: 1 [23552/50048]	Loss: 3.6319
Training Epoch: 1 [23680/50048]	Loss: 3.7713
Training Epoch: 1 [23808/50048]	Loss: 3.8101
Training Epoch: 1 [23936/50048]	Loss: 3.4078
Training Epoch: 1 [24064/50048]	Loss: 3.4480
Training Epoch: 1 [24192/50048]	Loss: 3.9823
Training Epoch: 1 [24320/50048]	Loss: 3.5822
Training Epoch: 1 [24448/50048]	Loss: 3.5295
Training Epoch: 1 [24576/50048]	Loss: 3.2991
Training Epoch: 1 [24704/50048]	Loss: 3.5451
Training Epoch: 1 [24832/50048]	Loss: 3.4161
Training Epoch: 1 [24960/50048]	Loss: 3.4984
Training Epoch: 1 [25088/50048]	Loss: 3.7965
Training Epoch: 1 [25216/50048]	Loss: 3.3717
Training Epoch: 1 [25344/50048]	Loss: 3.6222
Training Epoch: 1 [25472/50048]	Loss: 3.7432
Training Epoch: 1 [25600/50048]	Loss: 3.6653
Training Epoch: 1 [25728/50048]	Loss: 3.6822
Training Epoch: 1 [25856/50048]	Loss: 3.4226
Training Epoch: 1 [25984/50048]	Loss: 3.5928
Training Epoch: 1 [26112/50048]	Loss: 3.5270
Training Epoch: 1 [26240/50048]	Loss: 3.5428
Training Epoch: 1 [26368/50048]	Loss: 3.6979
Training Epoch: 1 [26496/50048]	Loss: 3.7108
Training Epoch: 1 [26624/50048]	Loss: 3.7458
Training Epoch: 1 [26752/50048]	Loss: 3.5918
Training Epoch: 1 [26880/50048]	Loss: 3.6680
Training Epoch: 1 [27008/50048]	Loss: 3.6200
Training Epoch: 1 [27136/50048]	Loss: 3.5319
Training Epoch: 1 [27264/50048]	Loss: 3.6902
Training Epoch: 1 [27392/50048]	Loss: 3.5878
Training Epoch: 1 [27520/50048]	Loss: 3.6648
Training Epoch: 1 [27648/50048]	Loss: 3.5883
Training Epoch: 1 [27776/50048]	Loss: 3.6477
Training Epoch: 1 [27904/50048]	Loss: 3.5004
Training Epoch: 1 [28032/50048]	Loss: 3.4258
Training Epoch: 1 [28160/50048]	Loss: 3.6286
Training Epoch: 1 [28288/50048]	Loss: 3.4051
Training Epoch: 1 [28416/50048]	Loss: 3.3864
Training Epoch: 1 [28544/50048]	Loss: 3.5437
Training Epoch: 1 [28672/50048]	Loss: 3.5959
Training Epoch: 1 [28800/50048]	Loss: 3.8396
Training Epoch: 1 [28928/50048]	Loss: 3.6788
Training Epoch: 1 [29056/50048]	Loss: 3.4140
Training Epoch: 1 [29184/50048]	Loss: 3.6512
Training Epoch: 1 [29312/50048]	Loss: 3.6724
Training Epoch: 1 [29440/50048]	Loss: 3.6862
Training Epoch: 1 [29568/50048]	Loss: 3.6281
Training Epoch: 1 [29696/50048]	Loss: 3.7826
Training Epoch: 1 [29824/50048]	Loss: 3.6220
Training Epoch: 1 [29952/50048]	Loss: 3.7834
Training Epoch: 1 [30080/50048]	Loss: 3.5875
Training Epoch: 1 [30208/50048]	Loss: 3.8319
Training Epoch: 1 [30336/50048]	Loss: 3.8027
Training Epoch: 1 [30464/50048]	Loss: 3.5809
Training Epoch: 1 [30592/50048]	Loss: 3.6397
Training Epoch: 1 [30720/50048]	Loss: 3.6119
Training Epoch: 1 [30848/50048]	Loss: 3.7358
Training Epoch: 1 [30976/50048]	Loss: 3.4770
Training Epoch: 1 [31104/50048]	Loss: 3.6028
Training Epoch: 1 [31232/50048]	Loss: 3.5730
Training Epoch: 1 [31360/50048]	Loss: 3.4191
Training Epoch: 1 [31488/50048]	Loss: 3.6693
Training Epoch: 1 [31616/50048]	Loss: 3.5703
Training Epoch: 1 [31744/50048]	Loss: 3.3235
Training Epoch: 1 [31872/50048]	Loss: 3.4358
Training Epoch: 1 [32000/50048]	Loss: 3.5980
Training Epoch: 1 [32128/50048]	Loss: 3.4407
Training Epoch: 1 [32256/50048]	Loss: 3.5498
Training Epoch: 1 [32384/50048]	Loss: 3.6042
Training Epoch: 1 [32512/50048]	Loss: 3.5111
Training Epoch: 1 [32640/50048]	Loss: 3.6003
Training Epoch: 1 [32768/50048]	Loss: 3.3926
Training Epoch: 1 [32896/50048]	Loss: 3.4658
Training Epoch: 1 [33024/50048]	Loss: 3.4832
Training Epoch: 1 [33152/50048]	Loss: 3.5157
Training Epoch: 1 [33280/50048]	Loss: 3.6197
Training Epoch: 1 [33408/50048]	Loss: 3.3928
Training Epoch: 1 [33536/50048]	Loss: 3.4143
Training Epoch: 1 [33664/50048]	Loss: 3.8242
Training Epoch: 1 [33792/50048]	Loss: 3.5756
Training Epoch: 1 [33920/50048]	Loss: 3.8770
Training Epoch: 1 [34048/50048]	Loss: 3.5150
Training Epoch: 1 [34176/50048]	Loss: 3.4645
Training Epoch: 1 [34304/50048]	Loss: 3.6336
Training Epoch: 1 [34432/50048]	Loss: 3.4686
Training Epoch: 1 [34560/50048]	Loss: 3.4591
Training Epoch: 1 [34688/50048]	Loss: 3.5539
Training Epoch: 1 [34816/50048]	Loss: 3.5892
Training Epoch: 1 [34944/50048]	Loss: 3.3991
Training Epoch: 1 [35072/50048]	Loss: 3.3305
Training Epoch: 1 [35200/50048]	Loss: 3.8261
Training Epoch: 1 [35328/50048]	Loss: 3.6095
Training Epoch: 1 [35456/50048]	Loss: 3.4804
Training Epoch: 1 [35584/50048]	Loss: 3.6676
Training Epoch: 1 [35712/50048]	Loss: 3.3719
Training Epoch: 1 [35840/50048]	Loss: 3.4956
Training Epoch: 1 [35968/50048]	Loss: 3.5136
Training Epoch: 1 [36096/50048]	Loss: 3.5689
Training Epoch: 1 [36224/50048]	Loss: 3.5380
Training Epoch: 1 [36352/50048]	Loss: 3.2032
Training Epoch: 1 [36480/50048]	Loss: 3.4489
Training Epoch: 1 [36608/50048]	Loss: 3.4663
Training Epoch: 1 [36736/50048]	Loss: 3.4061
Training Epoch: 1 [36864/50048]	Loss: 3.6105
Training Epoch: 1 [36992/50048]	Loss: 3.8678
Training Epoch: 1 [37120/50048]	Loss: 3.6914
Training Epoch: 1 [37248/50048]	Loss: 3.7013
Training Epoch: 1 [37376/50048]	Loss: 3.5264
Training Epoch: 1 [37504/50048]	Loss: 3.4877
Training Epoch: 1 [37632/50048]	Loss: 3.6275
Training Epoch: 1 [37760/50048]	Loss: 3.5049
Training Epoch: 1 [37888/50048]	Loss: 3.4531
Training Epoch: 1 [38016/50048]	Loss: 3.5917
Training Epoch: 1 [38144/50048]	Loss: 3.5511
Training Epoch: 1 [38272/50048]	Loss: 3.4682
Training Epoch: 1 [38400/50048]	Loss: 3.3480
Training Epoch: 1 [38528/50048]	Loss: 3.5923
Training Epoch: 1 [38656/50048]	Loss: 3.3127
Training Epoch: 1 [38784/50048]	Loss: 3.2981
Training Epoch: 1 [38912/50048]	Loss: 3.4240
Training Epoch: 1 [39040/50048]	Loss: 3.6270
Training Epoch: 1 [39168/50048]	Loss: 3.5842
Training Epoch: 1 [39296/50048]	Loss: 3.2237
Training Epoch: 1 [39424/50048]	Loss: 3.5698
Training Epoch: 1 [39552/50048]	Loss: 3.5926
Training Epoch: 1 [39680/50048]	Loss: 3.8689
Training Epoch: 1 [39808/50048]	Loss: 3.2743
Training Epoch: 1 [39936/50048]	Loss: 3.6205
Training Epoch: 1 [40064/50048]	Loss: 3.5054
Training Epoch: 1 [40192/50048]	Loss: 3.5057
Training Epoch: 1 [40320/50048]	Loss: 3.5136
Training Epoch: 1 [40448/50048]	Loss: 3.2492
Training Epoch: 1 [40576/50048]	Loss: 3.5105
Training Epoch: 1 [40704/50048]	Loss: 3.4216
Training Epoch: 1 [40832/50048]	Loss: 3.5180
Training Epoch: 1 [40960/50048]	Loss: 3.4522
Training Epoch: 1 [41088/50048]	Loss: 3.5770
Training Epoch: 1 [41216/50048]	Loss: 3.4179
Training Epoch: 1 [41344/50048]	Loss: 3.4495
Training Epoch: 1 [41472/50048]	Loss: 3.3970
Training Epoch: 1 [41600/50048]	Loss: 3.6075
Training Epoch: 1 [41728/50048]	Loss: 3.4070
Training Epoch: 1 [41856/50048]	Loss: 3.4160
Training Epoch: 1 [41984/50048]	Loss: 3.4678
Training Epoch: 1 [42112/50048]	Loss: 3.5455
Training Epoch: 1 [42240/50048]	Loss: 3.7333
Training Epoch: 1 [42368/50048]	Loss: 3.7446
Training Epoch: 1 [42496/50048]	Loss: 3.5134
Training Epoch: 1 [42624/50048]	Loss: 3.4221
Training Epoch: 1 [42752/50048]	Loss: 3.4102
Training Epoch: 1 [42880/50048]	Loss: 3.4626
Training Epoch: 1 [43008/50048]	Loss: 3.2292
Training Epoch: 1 [43136/50048]	Loss: 3.3180
Training Epoch: 1 [43264/50048]	Loss: 3.2912
Training Epoch: 1 [43392/50048]	Loss: 3.5013
Training Epoch: 1 [43520/50048]	Loss: 3.6556
Training Epoch: 1 [43648/50048]	Loss: 3.5780
Training Epoch: 1 [43776/50048]	Loss: 3.4104
Training Epoch: 1 [43904/50048]	Loss: 3.4380
Training Epoch: 1 [44032/50048]	Loss: 3.4121
Training Epoch: 1 [44160/50048]	Loss: 3.4055
Training Epoch: 1 [44288/50048]	Loss: 3.2837
Training Epoch: 1 [44416/50048]	Loss: 3.5865
Training Epoch: 1 [44544/50048]	Loss: 3.4836
Training Epoch: 1 [44672/50048]	Loss: 3.3874
Training Epoch: 1 [44800/50048]	Loss: 3.5703
Training Epoch: 1 [44928/50048]	Loss: 3.4484
Training Epoch: 1 [45056/50048]	Loss: 3.4591
Training Epoch: 1 [45184/50048]	Loss: 3.5139
Training Epoch: 1 [45312/50048]	Loss: 3.4332
Training Epoch: 1 [45440/50048]	Loss: 3.4999
Training Epoch: 1 [45568/50048]	Loss: 3.5220
Training Epoch: 1 [45696/50048]	Loss: 3.4226
Training Epoch: 1 [45824/50048]	Loss: 3.3374
Training Epoch: 1 [45952/50048]	Loss: 3.2225
Training Epoch: 1 [46080/50048]	Loss: 3.5689
Training Epoch: 1 [46208/50048]	Loss: 3.6658
Training Epoch: 1 [46336/50048]	Loss: 3.5702
Training Epoch: 1 [46464/50048]	Loss: 3.4200
Training Epoch: 1 [46592/50048]	Loss: 3.5428
Training Epoch: 1 [46720/50048]	Loss: 3.4473
Training Epoch: 1 [46848/50048]	Loss: 3.5275
Training Epoch: 1 [46976/50048]	Loss: 3.3842
Training Epoch: 1 [47104/50048]	Loss: 3.4230
Training Epoch: 1 [47232/50048]	Loss: 3.6111
Training Epoch: 1 [47360/50048]	Loss: 3.4089
Training Epoch: 1 [47488/50048]	Loss: 3.4403
Training Epoch: 1 [47616/50048]	Loss: 3.4592
Training Epoch: 1 [47744/50048]	Loss: 3.5697
Training Epoch: 1 [47872/50048]	Loss: 3.6041
Training Epoch: 1 [48000/50048]	Loss: 3.5977
Training Epoch: 1 [48128/50048]	Loss: 3.2204
Training Epoch: 1 [48256/50048]	Loss: 3.5720
Training Epoch: 1 [48384/50048]	Loss: 3.4614
Training Epoch: 1 [48512/50048]	Loss: 3.4871
Training Epoch: 1 [48640/50048]	Loss: 3.6502
Training Epoch: 1 [48768/50048]	Loss: 3.3819
Training Epoch: 1 [48896/50048]	Loss: 3.4619
Training Epoch: 1 [49024/50048]	Loss: 3.4825
Training Epoch: 1 [49152/50048]	Loss: 3.3564
Training Epoch: 1 [49280/50048]	Loss: 3.5207
Training Epoch: 1 [49408/50048]	Loss: 3.4647
Training Epoch: 1 [49536/50048]	Loss: 3.6604
Training Epoch: 1 [49664/50048]	Loss: 3.3769
Training Epoch: 1 [49792/50048]	Loss: 3.4034
Training Epoch: 1 [49920/50048]	Loss: 3.3688
Training Epoch: 1 [50048/50048]	Loss: 3.2645
Validation Epoch: 1, Average loss: 0.0277, Accuracy: 0.1645
Training Epoch: 2 [128/50048]	Loss: 3.4188
Training Epoch: 2 [256/50048]	Loss: 3.2232
Training Epoch: 2 [384/50048]	Loss: 3.2892
Training Epoch: 2 [512/50048]	Loss: 3.4602
Training Epoch: 2 [640/50048]	Loss: 3.3661
Training Epoch: 2 [768/50048]	Loss: 3.5343
Training Epoch: 2 [896/50048]	Loss: 3.3310
Training Epoch: 2 [1024/50048]	Loss: 3.4263
Training Epoch: 2 [1152/50048]	Loss: 3.4696
Training Epoch: 2 [1280/50048]	Loss: 3.3944
Training Epoch: 2 [1408/50048]	Loss: 3.4748
Training Epoch: 2 [1536/50048]	Loss: 3.3478
Training Epoch: 2 [1664/50048]	Loss: 3.5682
Training Epoch: 2 [1792/50048]	Loss: 3.4762
Training Epoch: 2 [1920/50048]	Loss: 3.3113
Training Epoch: 2 [2048/50048]	Loss: 3.1015
Training Epoch: 2 [2176/50048]	Loss: 3.2183
Training Epoch: 2 [2304/50048]	Loss: 3.3477
Training Epoch: 2 [2432/50048]	Loss: 3.4733
Training Epoch: 2 [2560/50048]	Loss: 3.5995
Training Epoch: 2 [2688/50048]	Loss: 3.1860
Training Epoch: 2 [2816/50048]	Loss: 3.4179
Training Epoch: 2 [2944/50048]	Loss: 3.3111
Training Epoch: 2 [3072/50048]	Loss: 3.2467
Training Epoch: 2 [3200/50048]	Loss: 3.2356
Training Epoch: 2 [3328/50048]	Loss: 3.3574
Training Epoch: 2 [3456/50048]	Loss: 3.2589
Training Epoch: 2 [3584/50048]	Loss: 3.2655
Training Epoch: 2 [3712/50048]	Loss: 3.4388
Training Epoch: 2 [3840/50048]	Loss: 3.2075
Training Epoch: 2 [3968/50048]	Loss: 3.4221
Training Epoch: 2 [4096/50048]	Loss: 3.0950
Training Epoch: 2 [4224/50048]	Loss: 3.4918
Training Epoch: 2 [4352/50048]	Loss: 3.3325
Training Epoch: 2 [4480/50048]	Loss: 3.3509
Training Epoch: 2 [4608/50048]	Loss: 3.3041
Training Epoch: 2 [4736/50048]	Loss: 3.4137
Training Epoch: 2 [4864/50048]	Loss: 3.3740
Training Epoch: 2 [4992/50048]	Loss: 3.4557
Training Epoch: 2 [5120/50048]	Loss: 3.5133
Training Epoch: 2 [5248/50048]	Loss: 3.0313
Training Epoch: 2 [5376/50048]	Loss: 3.3161
Training Epoch: 2 [5504/50048]	Loss: 3.5415
Training Epoch: 2 [5632/50048]	Loss: 3.5971
Training Epoch: 2 [5760/50048]	Loss: 3.2926
Training Epoch: 2 [5888/50048]	Loss: 3.2604
Training Epoch: 2 [6016/50048]	Loss: 3.3957
Training Epoch: 2 [6144/50048]	Loss: 3.2865
Training Epoch: 2 [6272/50048]	Loss: 3.5094
Training Epoch: 2 [6400/50048]	Loss: 3.5218
Training Epoch: 2 [6528/50048]	Loss: 3.2182
Training Epoch: 2 [6656/50048]	Loss: 3.4655
Training Epoch: 2 [6784/50048]	Loss: 3.4632
Training Epoch: 2 [6912/50048]	Loss: 3.4242
Training Epoch: 2 [7040/50048]	Loss: 3.2557
Training Epoch: 2 [7168/50048]	Loss: 3.4712
Training Epoch: 2 [7296/50048]	Loss: 3.3119
Training Epoch: 2 [7424/50048]	Loss: 3.3138
Training Epoch: 2 [7552/50048]	Loss: 3.4346
Training Epoch: 2 [7680/50048]	Loss: 3.1030
Training Epoch: 2 [7808/50048]	Loss: 3.3990
Training Epoch: 2 [7936/50048]	Loss: 3.4727
Training Epoch: 2 [8064/50048]	Loss: 3.2229
Training Epoch: 2 [8192/50048]	Loss: 3.0935
Training Epoch: 2 [8320/50048]	Loss: 3.3004
Training Epoch: 2 [8448/50048]	Loss: 3.1360
Training Epoch: 2 [8576/50048]	Loss: 3.2203
Training Epoch: 2 [8704/50048]	Loss: 3.3257
Training Epoch: 2 [8832/50048]	Loss: 3.2790
Training Epoch: 2 [8960/50048]	Loss: 3.3307
Training Epoch: 2 [9088/50048]	Loss: 3.3807
Training Epoch: 2 [9216/50048]	Loss: 3.2071
Training Epoch: 2 [9344/50048]	Loss: 3.2471
Training Epoch: 2 [9472/50048]	Loss: 3.3607
Training Epoch: 2 [9600/50048]	Loss: 3.4551
Training Epoch: 2 [9728/50048]	Loss: 3.5140
Training Epoch: 2 [9856/50048]	Loss: 3.4584
Training Epoch: 2 [9984/50048]	Loss: 3.1137
Training Epoch: 2 [10112/50048]	Loss: 3.0479
Training Epoch: 2 [10240/50048]	Loss: 3.1909
Training Epoch: 2 [10368/50048]	Loss: 3.2131
Training Epoch: 2 [10496/50048]	Loss: 3.3201
Training Epoch: 2 [10624/50048]	Loss: 3.3802
Training Epoch: 2 [10752/50048]	Loss: 3.2565
Training Epoch: 2 [10880/50048]	Loss: 3.3535
Training Epoch: 2 [11008/50048]	Loss: 3.3095
Training Epoch: 2 [11136/50048]	Loss: 3.2343
Training Epoch: 2 [11264/50048]	Loss: 3.3237
Training Epoch: 2 [11392/50048]	Loss: 3.2699
Training Epoch: 2 [11520/50048]	Loss: 3.4604
Training Epoch: 2 [11648/50048]	Loss: 3.2690
Training Epoch: 2 [11776/50048]	Loss: 3.5515
Training Epoch: 2 [11904/50048]	Loss: 3.1936
Training Epoch: 2 [12032/50048]	Loss: 3.4411
Training Epoch: 2 [12160/50048]	Loss: 3.4228
Training Epoch: 2 [12288/50048]	Loss: 3.4896
Training Epoch: 2 [12416/50048]	Loss: 3.1976
Training Epoch: 2 [12544/50048]	Loss: 3.2957
Training Epoch: 2 [12672/50048]	Loss: 3.5999
Training Epoch: 2 [12800/50048]	Loss: 3.1528
Training Epoch: 2 [12928/50048]	Loss: 3.2530
Training Epoch: 2 [13056/50048]	Loss: 3.1743
Training Epoch: 2 [13184/50048]	Loss: 3.2356
Training Epoch: 2 [13312/50048]	Loss: 3.4372
Training Epoch: 2 [13440/50048]	Loss: 3.3857
Training Epoch: 2 [13568/50048]	Loss: 3.4107
Training Epoch: 2 [13696/50048]	Loss: 3.3247
Training Epoch: 2 [13824/50048]	Loss: 3.2886
Training Epoch: 2 [13952/50048]	Loss: 3.3862
Training Epoch: 2 [14080/50048]	Loss: 3.1057
Training Epoch: 2 [14208/50048]	Loss: 3.0321
Training Epoch: 2 [14336/50048]	Loss: 3.0510
Training Epoch: 2 [14464/50048]	Loss: 3.1840
Training Epoch: 2 [14592/50048]	Loss: 3.0446
Training Epoch: 2 [14720/50048]	Loss: 3.1956
Training Epoch: 2 [14848/50048]	Loss: 3.3764
Training Epoch: 2 [14976/50048]	Loss: 3.2779
Training Epoch: 2 [15104/50048]	Loss: 3.1236
Training Epoch: 2 [15232/50048]	Loss: 3.2985
Training Epoch: 2 [15360/50048]	Loss: 3.2403
Training Epoch: 2 [15488/50048]	Loss: 3.3318
Training Epoch: 2 [15616/50048]	Loss: 3.3271
Training Epoch: 2 [15744/50048]	Loss: 3.1489
Training Epoch: 2 [15872/50048]	Loss: 3.1526
Training Epoch: 2 [16000/50048]	Loss: 3.2588
Training Epoch: 2 [16128/50048]	Loss: 3.3012
Training Epoch: 2 [16256/50048]	Loss: 3.2178
Training Epoch: 2 [16384/50048]	Loss: 3.0432
Training Epoch: 2 [16512/50048]	Loss: 3.1598
Training Epoch: 2 [16640/50048]	Loss: 3.1955
Training Epoch: 2 [16768/50048]	Loss: 3.0403
Training Epoch: 2 [16896/50048]	Loss: 3.1565
Training Epoch: 2 [17024/50048]	Loss: 3.1823
Training Epoch: 2 [17152/50048]	Loss: 3.1684
Training Epoch: 2 [17280/50048]	Loss: 2.9696
Training Epoch: 2 [17408/50048]	Loss: 2.8726
Training Epoch: 2 [17536/50048]	Loss: 3.3268
Training Epoch: 2 [17664/50048]	Loss: 3.1019
Training Epoch: 2 [17792/50048]	Loss: 3.1582
Training Epoch: 2 [17920/50048]	Loss: 3.2033
Training Epoch: 2 [18048/50048]	Loss: 3.1219
Training Epoch: 2 [18176/50048]	Loss: 3.1043
Training Epoch: 2 [18304/50048]	Loss: 3.4103
Training Epoch: 2 [18432/50048]	Loss: 3.2009
Training Epoch: 2 [18560/50048]	Loss: 3.3569
Training Epoch: 2 [18688/50048]	Loss: 3.1409
Training Epoch: 2 [18816/50048]	Loss: 2.9430
Training Epoch: 2 [18944/50048]	Loss: 3.1158
Training Epoch: 2 [19072/50048]	Loss: 3.2522
Training Epoch: 2 [19200/50048]	Loss: 3.1499
Training Epoch: 2 [19328/50048]	Loss: 3.0303
Training Epoch: 2 [19456/50048]	Loss: 3.1335
Training Epoch: 2 [19584/50048]	Loss: 3.0541
Training Epoch: 2 [19712/50048]	Loss: 3.4635
Training Epoch: 2 [19840/50048]	Loss: 3.0875
Training Epoch: 2 [19968/50048]	Loss: 3.3571
Training Epoch: 2 [20096/50048]	Loss: 3.2916
Training Epoch: 2 [20224/50048]	Loss: 3.2769
Training Epoch: 2 [20352/50048]	Loss: 3.0957
Training Epoch: 2 [20480/50048]	Loss: 3.4823
Training Epoch: 2 [20608/50048]	Loss: 3.2700
Training Epoch: 2 [20736/50048]	Loss: 3.0141
Training Epoch: 2 [20864/50048]	Loss: 2.9604
Training Epoch: 2 [20992/50048]	Loss: 2.8920
Training Epoch: 2 [21120/50048]	Loss: 3.1976
Training Epoch: 2 [21248/50048]	Loss: 2.8259
Training Epoch: 2 [21376/50048]	Loss: 3.4625
Training Epoch: 2 [21504/50048]	Loss: 2.9690
Training Epoch: 2 [21632/50048]	Loss: 2.9700
Training Epoch: 2 [21760/50048]	Loss: 3.1170
Training Epoch: 2 [21888/50048]	Loss: 3.2349
Training Epoch: 2 [22016/50048]	Loss: 3.1362
Training Epoch: 2 [22144/50048]	Loss: 3.2866
Training Epoch: 2 [22272/50048]	Loss: 3.1789
Training Epoch: 2 [22400/50048]	Loss: 3.2069
Training Epoch: 2 [22528/50048]	Loss: 3.0290
Training Epoch: 2 [22656/50048]	Loss: 3.1645
Training Epoch: 2 [22784/50048]	Loss: 3.0074
Training Epoch: 2 [22912/50048]	Loss: 3.3981
Training Epoch: 2 [23040/50048]	Loss: 3.0425
Training Epoch: 2 [23168/50048]	Loss: 3.2561
Training Epoch: 2 [23296/50048]	Loss: 3.4175
Training Epoch: 2 [23424/50048]	Loss: 3.4392
Training Epoch: 2 [23552/50048]	Loss: 3.1117
Training Epoch: 2 [23680/50048]	Loss: 3.1464
Training Epoch: 2 [23808/50048]	Loss: 3.2968
Training Epoch: 2 [23936/50048]	Loss: 3.0107
Training Epoch: 2 [24064/50048]	Loss: 3.1950
Training Epoch: 2 [24192/50048]	Loss: 3.1425
Training Epoch: 2 [24320/50048]	Loss: 3.3002
Training Epoch: 2 [24448/50048]	Loss: 3.1205
Training Epoch: 2 [24576/50048]	Loss: 3.0759
Training Epoch: 2 [24704/50048]	Loss: 3.2127
Training Epoch: 2 [24832/50048]	Loss: 3.2867
Training Epoch: 2 [24960/50048]	Loss: 3.3491
Training Epoch: 2 [25088/50048]	Loss: 3.3425
Training Epoch: 2 [25216/50048]	Loss: 3.1591
Training Epoch: 2 [25344/50048]	Loss: 2.9835
Training Epoch: 2 [25472/50048]	Loss: 2.9718
Training Epoch: 2 [25600/50048]	Loss: 2.9382
Training Epoch: 2 [25728/50048]	Loss: 2.8698
Training Epoch: 2 [25856/50048]	Loss: 3.0543
Training Epoch: 2 [25984/50048]	Loss: 3.1216
Training Epoch: 2 [26112/50048]	Loss: 3.2446
Training Epoch: 2 [26240/50048]	Loss: 3.1978
Training Epoch: 2 [26368/50048]	Loss: 3.2271
Training Epoch: 2 [26496/50048]	Loss: 3.0300
Training Epoch: 2 [26624/50048]	Loss: 3.0426
Training Epoch: 2 [26752/50048]	Loss: 2.9929
Training Epoch: 2 [26880/50048]	Loss: 3.1409
Training Epoch: 2 [27008/50048]	Loss: 3.0528
Training Epoch: 2 [27136/50048]	Loss: 3.0061
Training Epoch: 2 [27264/50048]	Loss: 3.1524
Training Epoch: 2 [27392/50048]	Loss: 2.9122
Training Epoch: 2 [27520/50048]	Loss: 2.8365
Training Epoch: 2 [27648/50048]	Loss: 2.9462
Training Epoch: 2 [27776/50048]	Loss: 3.2809
Training Epoch: 2 [27904/50048]	Loss: 3.2222
Training Epoch: 2 [28032/50048]	Loss: 3.0742
Training Epoch: 2 [28160/50048]	Loss: 2.9831
Training Epoch: 2 [28288/50048]	Loss: 2.9806
Training Epoch: 2 [28416/50048]	Loss: 3.1081
Training Epoch: 2 [28544/50048]	Loss: 3.2784
Training Epoch: 2 [28672/50048]	Loss: 2.9734
Training Epoch: 2 [28800/50048]	Loss: 3.2238
Training Epoch: 2 [28928/50048]	Loss: 3.1009
Training Epoch: 2 [29056/50048]	Loss: 3.3113
Training Epoch: 2 [29184/50048]	Loss: 2.9414
Training Epoch: 2 [29312/50048]	Loss: 2.9308
Training Epoch: 2 [29440/50048]	Loss: 3.1540
Training Epoch: 2 [29568/50048]	Loss: 3.2200
Training Epoch: 2 [29696/50048]	Loss: 3.1042
Training Epoch: 2 [29824/50048]	Loss: 2.9447
Training Epoch: 2 [29952/50048]	Loss: 3.1560
Training Epoch: 2 [30080/50048]	Loss: 3.1232
Training Epoch: 2 [30208/50048]	Loss: 3.1966
Training Epoch: 2 [30336/50048]	Loss: 3.0784
Training Epoch: 2 [30464/50048]	Loss: 3.1501
Training Epoch: 2 [30592/50048]	Loss: 3.0097
Training Epoch: 2 [30720/50048]	Loss: 3.2794
Training Epoch: 2 [30848/50048]	Loss: 3.2339
Training Epoch: 2 [30976/50048]	Loss: 3.4381
Training Epoch: 2 [31104/50048]	Loss: 3.0772
Training Epoch: 2 [31232/50048]	Loss: 3.0935
Training Epoch: 2 [31360/50048]	Loss: 2.9395
Training Epoch: 2 [31488/50048]	Loss: 3.2001
Training Epoch: 2 [31616/50048]	Loss: 3.2076
Training Epoch: 2 [31744/50048]	Loss: 3.1648
Training Epoch: 2 [31872/50048]	Loss: 2.9691
Training Epoch: 2 [32000/50048]	Loss: 3.2117
Training Epoch: 2 [32128/50048]	Loss: 2.9560
Training Epoch: 2 [32256/50048]	Loss: 3.2620
Training Epoch: 2 [32384/50048]	Loss: 3.2226
Training Epoch: 2 [32512/50048]	Loss: 3.3524
Training Epoch: 2 [32640/50048]	Loss: 3.0620
Training Epoch: 2 [32768/50048]	Loss: 3.2725
Training Epoch: 2 [32896/50048]	Loss: 3.0770
Training Epoch: 2 [33024/50048]	Loss: 3.1770
Training Epoch: 2 [33152/50048]	Loss: 3.1638
Training Epoch: 2 [33280/50048]	Loss: 3.0977
Training Epoch: 2 [33408/50048]	Loss: 2.8606
Training Epoch: 2 [33536/50048]	Loss: 2.9995
Training Epoch: 2 [33664/50048]	Loss: 2.9188
Training Epoch: 2 [33792/50048]	Loss: 2.8370
Training Epoch: 2 [33920/50048]	Loss: 3.1885
Training Epoch: 2 [34048/50048]	Loss: 2.9687
Training Epoch: 2 [34176/50048]	Loss: 2.9289
Training Epoch: 2 [34304/50048]	Loss: 2.9754
Training Epoch: 2 [34432/50048]	Loss: 3.0045
Training Epoch: 2 [34560/50048]	Loss: 2.9994
Training Epoch: 2 [34688/50048]	Loss: 2.9859
Training Epoch: 2 [34816/50048]	Loss: 2.9579
Training Epoch: 2 [34944/50048]	Loss: 2.9560
Training Epoch: 2 [35072/50048]	Loss: 3.2181
Training Epoch: 2 [35200/50048]	Loss: 3.1198
Training Epoch: 2 [35328/50048]	Loss: 2.9764
Training Epoch: 2 [35456/50048]	Loss: 2.9441
Training Epoch: 2 [35584/50048]	Loss: 3.0375
Training Epoch: 2 [35712/50048]	Loss: 2.9972
Training Epoch: 2 [35840/50048]	Loss: 3.1378
Training Epoch: 2 [35968/50048]	Loss: 2.8101
Training Epoch: 2 [36096/50048]	Loss: 3.0285
Training Epoch: 2 [36224/50048]	Loss: 3.2451
Training Epoch: 2 [36352/50048]	Loss: 3.4669
Training Epoch: 2 [36480/50048]	Loss: 2.9753
Training Epoch: 2 [36608/50048]	Loss: 3.1213
Training Epoch: 2 [36736/50048]	Loss: 3.1117
Training Epoch: 2 [36864/50048]	Loss: 2.7740
Training Epoch: 2 [36992/50048]	Loss: 3.1458
Training Epoch: 2 [37120/50048]	Loss: 3.1795
Training Epoch: 2 [37248/50048]	Loss: 3.0109
Training Epoch: 2 [37376/50048]	Loss: 3.1327
Training Epoch: 2 [37504/50048]	Loss: 3.0911
Training Epoch: 2 [37632/50048]	Loss: 2.8525
Training Epoch: 2 [37760/50048]	Loss: 3.1446
Training Epoch: 2 [37888/50048]	Loss: 3.2451
Training Epoch: 2 [38016/50048]	Loss: 2.9538
Training Epoch: 2 [38144/50048]	Loss: 3.3444
Training Epoch: 2 [38272/50048]	Loss: 3.1547
Training Epoch: 2 [38400/50048]	Loss: 2.9298
Training Epoch: 2 [38528/50048]	Loss: 2.8509
Training Epoch: 2 [38656/50048]	Loss: 3.1836
Training Epoch: 2 [38784/50048]	Loss: 3.0275
Training Epoch: 2 [38912/50048]	Loss: 3.0441
Training Epoch: 2 [39040/50048]	Loss: 2.8524
Training Epoch: 2 [39168/50048]	Loss: 2.9897
Training Epoch: 2 [39296/50048]	Loss: 3.2359
Training Epoch: 2 [39424/50048]	Loss: 3.0675
Training Epoch: 2 [39552/50048]	Loss: 3.0294
Training Epoch: 2 [39680/50048]	Loss: 3.0257
Training Epoch: 2 [39808/50048]	Loss: 2.8231
Training Epoch: 2 [39936/50048]	Loss: 2.8936
Training Epoch: 2 [40064/50048]	Loss: 3.0533
Training Epoch: 2 [40192/50048]	Loss: 3.1627
Training Epoch: 2 [40320/50048]	Loss: 3.0389
Training Epoch: 2 [40448/50048]	Loss: 3.0733
Training Epoch: 2 [40576/50048]	Loss: 3.3807
Training Epoch: 2 [40704/50048]	Loss: 3.2539
Training Epoch: 2 [40832/50048]	Loss: 3.0052
Training Epoch: 2 [40960/50048]	Loss: 3.0700
Training Epoch: 2 [41088/50048]	Loss: 3.0885
Training Epoch: 2 [41216/50048]	Loss: 3.1076
Training Epoch: 2 [41344/50048]	Loss: 3.2190
Training Epoch: 2 [41472/50048]	Loss: 2.9946
Training Epoch: 2 [41600/50048]	Loss: 3.0622
Training Epoch: 2 [41728/50048]	Loss: 2.9230
Training Epoch: 2 [41856/50048]	Loss: 2.7598
Training Epoch: 2 [41984/50048]	Loss: 3.0327
Training Epoch: 2 [42112/50048]	Loss: 3.1446
Training Epoch: 2 [42240/50048]	Loss: 3.0373
Training Epoch: 2 [42368/50048]	Loss: 2.9955
Training Epoch: 2 [42496/50048]	Loss: 2.9992
Training Epoch: 2 [42624/50048]	Loss: 2.7454
Training Epoch: 2 [42752/50048]	Loss: 2.9144
Training Epoch: 2 [42880/50048]	Loss: 2.9700
Training Epoch: 2 [43008/50048]	Loss: 2.9748
Training Epoch: 2 [43136/50048]	Loss: 2.8051
Training Epoch: 2 [43264/50048]	Loss: 3.1161
Training Epoch: 2 [43392/50048]	Loss: 2.8393
Training Epoch: 2 [43520/50048]	Loss: 3.1764
Training Epoch: 2 [43648/50048]	Loss: 2.7915
Training Epoch: 2 [43776/50048]	Loss: 3.1284
Training Epoch: 2 [43904/50048]	Loss: 2.9051
Training Epoch: 2 [44032/50048]	Loss: 3.0672
Training Epoch: 2 [44160/50048]	Loss: 3.2290
Training Epoch: 2 [44288/50048]	Loss: 3.0889
Training Epoch: 2 [44416/50048]	Loss: 3.2614
Training Epoch: 2 [44544/50048]	Loss: 3.0556
Training Epoch: 2 [44672/50048]	Loss: 2.6874
Training Epoch: 2 [44800/50048]	Loss: 2.9181
Training Epoch: 2 [44928/50048]	Loss: 2.8292
Training Epoch: 2 [45056/50048]	Loss: 2.8058
Training Epoch: 2 [45184/50048]	Loss: 2.9379
Training Epoch: 2 [45312/50048]	Loss: 3.0654
Training Epoch: 2 [45440/50048]	Loss: 2.8612
Training Epoch: 2 [45568/50048]	Loss: 2.8652
Training Epoch: 2 [45696/50048]	Loss: 2.8342
Training Epoch: 2 [45824/50048]	Loss: 2.9650
Training Epoch: 2 [45952/50048]	Loss: 2.7792
Training Epoch: 2 [46080/50048]	Loss: 3.1867
Training Epoch: 2 [46208/50048]	Loss: 3.0564
Training Epoch: 2 [46336/50048]	Loss: 3.2445
Training Epoch: 2 [46464/50048]	Loss: 3.1282
Training Epoch: 2 [46592/50048]	Loss: 2.8229
Training Epoch: 2 [46720/50048]	Loss: 2.8216
Training Epoch: 2 [46848/50048]	Loss: 3.1578
Training Epoch: 2 [46976/50048]	Loss: 3.0855
Training Epoch: 2 [47104/50048]	Loss: 3.0962
Training Epoch: 2 [47232/50048]	Loss: 3.0790
Training Epoch: 2 [47360/50048]	Loss: 2.9402
Training Epoch: 2 [47488/50048]	Loss: 2.9106
Training Epoch: 2 [47616/50048]	Loss: 2.8707
Training Epoch: 2 [47744/50048]	Loss: 2.9153
Training Epoch: 2 [47872/50048]	Loss: 2.7455
Training Epoch: 2 [48000/50048]	Loss: 2.9554
Training Epoch: 2 [48128/50048]	Loss: 2.8805
Training Epoch: 2 [48256/50048]	Loss: 3.0280
Training Epoch: 2 [48384/50048]	Loss: 2.9071
Training Epoch: 2 [48512/50048]	Loss: 2.9725
Training Epoch: 2 [48640/50048]	Loss: 2.7748
Training Epoch: 2 [48768/50048]	Loss: 3.1162
Training Epoch: 2 [48896/50048]	Loss: 3.0124
Training Epoch: 2 [49024/50048]	Loss: 2.7495
Training Epoch: 2 [49152/50048]	Loss: 2.5236
Training Epoch: 2 [49280/50048]	Loss: 2.9343
Training Epoch: 2 [49408/50048]	Loss: 2.9908
Training Epoch: 2 [49536/50048]	Loss: 3.0434
Training Epoch: 2 [49664/50048]	Loss: 2.9778
Training Epoch: 2 [49792/50048]	Loss: 3.1251
Training Epoch: 2 [49920/50048]	Loss: 2.9938
Training Epoch: 2 [50048/50048]	Loss: 2.8638
Validation Epoch: 2, Average loss: 0.0240, Accuracy: 0.2401
Training Epoch: 3 [128/50048]	Loss: 3.0149
Training Epoch: 3 [256/50048]	Loss: 2.8093
Training Epoch: 3 [384/50048]	Loss: 2.7625
Training Epoch: 3 [512/50048]	Loss: 2.9911
Training Epoch: 3 [640/50048]	Loss: 2.8975
Training Epoch: 3 [768/50048]	Loss: 2.9329
Training Epoch: 3 [896/50048]	Loss: 2.8245
Training Epoch: 3 [1024/50048]	Loss: 2.9547
Training Epoch: 3 [1152/50048]	Loss: 2.8984
Training Epoch: 3 [1280/50048]	Loss: 3.1355
Training Epoch: 3 [1408/50048]	Loss: 2.8255
Training Epoch: 3 [1536/50048]	Loss: 2.7875
Training Epoch: 3 [1664/50048]	Loss: 2.9689
Training Epoch: 3 [1792/50048]	Loss: 2.9302
Training Epoch: 3 [1920/50048]	Loss: 2.8869
Training Epoch: 3 [2048/50048]	Loss: 2.8593
Training Epoch: 3 [2176/50048]	Loss: 3.0571
Training Epoch: 3 [2304/50048]	Loss: 2.9358
Training Epoch: 3 [2432/50048]	Loss: 2.8764
Training Epoch: 3 [2560/50048]	Loss: 3.2672
Training Epoch: 3 [2688/50048]	Loss: 2.7105
Training Epoch: 3 [2816/50048]	Loss: 2.8901
Training Epoch: 3 [2944/50048]	Loss: 2.7231
Training Epoch: 3 [3072/50048]	Loss: 2.7091
Training Epoch: 3 [3200/50048]	Loss: 3.2579
Training Epoch: 3 [3328/50048]	Loss: 2.6695
Training Epoch: 3 [3456/50048]	Loss: 3.2001
Training Epoch: 3 [3584/50048]	Loss: 2.8357
Training Epoch: 3 [3712/50048]	Loss: 2.8251
Training Epoch: 3 [3840/50048]	Loss: 2.9694
Training Epoch: 3 [3968/50048]	Loss: 2.7562
Training Epoch: 3 [4096/50048]	Loss: 3.2368
Training Epoch: 3 [4224/50048]	Loss: 2.8382
Training Epoch: 3 [4352/50048]	Loss: 2.9825
Training Epoch: 3 [4480/50048]	Loss: 2.8605
Training Epoch: 3 [4608/50048]	Loss: 2.9057
Training Epoch: 3 [4736/50048]	Loss: 2.8584
Training Epoch: 3 [4864/50048]	Loss: 2.9573
Training Epoch: 3 [4992/50048]	Loss: 3.0087
Training Epoch: 3 [5120/50048]	Loss: 2.7115
Training Epoch: 3 [5248/50048]	Loss: 2.8839
Training Epoch: 3 [5376/50048]	Loss: 2.9406
Training Epoch: 3 [5504/50048]	Loss: 2.8514
Training Epoch: 3 [5632/50048]	Loss: 3.3228
Training Epoch: 3 [5760/50048]	Loss: 2.6101
Training Epoch: 3 [5888/50048]	Loss: 2.8765
Training Epoch: 3 [6016/50048]	Loss: 3.0239
Training Epoch: 3 [6144/50048]	Loss: 2.8475
Training Epoch: 3 [6272/50048]	Loss: 2.7496
Training Epoch: 3 [6400/50048]	Loss: 2.7441
Training Epoch: 3 [6528/50048]	Loss: 3.0234
Training Epoch: 3 [6656/50048]	Loss: 2.7021
Training Epoch: 3 [6784/50048]	Loss: 2.9466
Training Epoch: 3 [6912/50048]	Loss: 3.1120
Training Epoch: 3 [7040/50048]	Loss: 2.9448
Training Epoch: 3 [7168/50048]	Loss: 3.1754
Training Epoch: 3 [7296/50048]	Loss: 2.9530
Training Epoch: 3 [7424/50048]	Loss: 2.8647
Training Epoch: 3 [7552/50048]	Loss: 2.6821
Training Epoch: 3 [7680/50048]	Loss: 2.8275
Training Epoch: 3 [7808/50048]	Loss: 2.7781
Training Epoch: 3 [7936/50048]	Loss: 2.8550
Training Epoch: 3 [8064/50048]	Loss: 3.1130
Training Epoch: 3 [8192/50048]	Loss: 2.7724
Training Epoch: 3 [8320/50048]	Loss: 2.7742
Training Epoch: 3 [8448/50048]	Loss: 2.7243
Training Epoch: 3 [8576/50048]	Loss: 3.0222
Training Epoch: 3 [8704/50048]	Loss: 2.8326
Training Epoch: 3 [8832/50048]	Loss: 2.8693
Training Epoch: 3 [8960/50048]	Loss: 2.4780
Training Epoch: 3 [9088/50048]	Loss: 2.7418
Training Epoch: 3 [9216/50048]	Loss: 2.7820
Training Epoch: 3 [9344/50048]	Loss: 2.8540
Training Epoch: 3 [9472/50048]	Loss: 2.9539
Training Epoch: 3 [9600/50048]	Loss: 2.8258
Training Epoch: 3 [9728/50048]	Loss: 2.8352
Training Epoch: 3 [9856/50048]	Loss: 3.0088
Training Epoch: 3 [9984/50048]	Loss: 2.8268
Training Epoch: 3 [10112/50048]	Loss: 2.8600
Training Epoch: 3 [10240/50048]	Loss: 2.7081
Training Epoch: 3 [10368/50048]	Loss: 2.7425
Training Epoch: 3 [10496/50048]	Loss: 2.9112
Training Epoch: 3 [10624/50048]	Loss: 2.9041
Training Epoch: 3 [10752/50048]	Loss: 2.9662
Training Epoch: 3 [10880/50048]	Loss: 2.6345
Training Epoch: 3 [11008/50048]	Loss: 2.5906
Training Epoch: 3 [11136/50048]	Loss: 2.9004
Training Epoch: 3 [11264/50048]	Loss: 2.8472
Training Epoch: 3 [11392/50048]	Loss: 2.8532
Training Epoch: 3 [11520/50048]	Loss: 2.9776
Training Epoch: 3 [11648/50048]	Loss: 2.7520
Training Epoch: 3 [11776/50048]	Loss: 2.8642
Training Epoch: 3 [11904/50048]	Loss: 2.7707
Training Epoch: 3 [12032/50048]	Loss: 2.8097
Training Epoch: 3 [12160/50048]	Loss: 2.9403
Training Epoch: 3 [12288/50048]	Loss: 3.0069
Training Epoch: 3 [12416/50048]	Loss: 3.0210
Training Epoch: 3 [12544/50048]	Loss: 3.1182
Training Epoch: 3 [12672/50048]	Loss: 2.9362
Training Epoch: 3 [12800/50048]	Loss: 2.5609
Training Epoch: 3 [12928/50048]	Loss: 2.6913
Training Epoch: 3 [13056/50048]	Loss: 2.8811
Training Epoch: 3 [13184/50048]	Loss: 2.8397
Training Epoch: 3 [13312/50048]	Loss: 2.8948
Training Epoch: 3 [13440/50048]	Loss: 2.7781
Training Epoch: 3 [13568/50048]	Loss: 2.6697
Training Epoch: 3 [13696/50048]	Loss: 2.6990
Training Epoch: 3 [13824/50048]	Loss: 2.9231
Training Epoch: 3 [13952/50048]	Loss: 2.6585
Training Epoch: 3 [14080/50048]	Loss: 2.8651
Training Epoch: 3 [14208/50048]	Loss: 2.5615
Training Epoch: 3 [14336/50048]	Loss: 2.7752
Training Epoch: 3 [14464/50048]	Loss: 2.6431
Training Epoch: 3 [14592/50048]	Loss: 2.8566
Training Epoch: 3 [14720/50048]	Loss: 2.8411
Training Epoch: 3 [14848/50048]	Loss: 2.9211
Training Epoch: 3 [14976/50048]	Loss: 2.4188
Training Epoch: 3 [15104/50048]	Loss: 2.7960
Training Epoch: 3 [15232/50048]	Loss: 2.9358
Training Epoch: 3 [15360/50048]	Loss: 2.5698
Training Epoch: 3 [15488/50048]	Loss: 2.8257
Training Epoch: 3 [15616/50048]	Loss: 2.7828
Training Epoch: 3 [15744/50048]	Loss: 3.0076
Training Epoch: 3 [15872/50048]	Loss: 2.7481
Training Epoch: 3 [16000/50048]	Loss: 2.7601
Training Epoch: 3 [16128/50048]	Loss: 2.7662
Training Epoch: 3 [16256/50048]	Loss: 2.6119
Training Epoch: 3 [16384/50048]	Loss: 2.7048
Training Epoch: 3 [16512/50048]	Loss: 2.4818
Training Epoch: 3 [16640/50048]	Loss: 2.8128
Training Epoch: 3 [16768/50048]	Loss: 2.7870
Training Epoch: 3 [16896/50048]	Loss: 2.9301
Training Epoch: 3 [17024/50048]	Loss: 2.7314
Training Epoch: 3 [17152/50048]	Loss: 2.7684
Training Epoch: 3 [17280/50048]	Loss: 2.6680
Training Epoch: 3 [17408/50048]	Loss: 3.1417
Training Epoch: 3 [17536/50048]	Loss: 2.7040
Training Epoch: 3 [17664/50048]	Loss: 2.9013
Training Epoch: 3 [17792/50048]	Loss: 2.6156
Training Epoch: 3 [17920/50048]	Loss: 2.5729
Training Epoch: 3 [18048/50048]	Loss: 2.7793
Training Epoch: 3 [18176/50048]	Loss: 2.6393
Training Epoch: 3 [18304/50048]	Loss: 2.8142
Training Epoch: 3 [18432/50048]	Loss: 2.7937
Training Epoch: 3 [18560/50048]	Loss: 2.7453
Training Epoch: 3 [18688/50048]	Loss: 2.8397
Training Epoch: 3 [18816/50048]	Loss: 2.9229
Training Epoch: 3 [18944/50048]	Loss: 2.9533
Training Epoch: 3 [19072/50048]	Loss: 2.9911
Training Epoch: 3 [19200/50048]	Loss: 2.9186
Training Epoch: 3 [19328/50048]	Loss: 2.6206
Training Epoch: 3 [19456/50048]	Loss: 2.7333
Training Epoch: 3 [19584/50048]	Loss: 2.9766
Training Epoch: 3 [19712/50048]	Loss: 2.7567
Training Epoch: 3 [19840/50048]	Loss: 2.9957
Training Epoch: 3 [19968/50048]	Loss: 2.8122
Training Epoch: 3 [20096/50048]	Loss: 2.7830
Training Epoch: 3 [20224/50048]	Loss: 2.9094
Training Epoch: 3 [20352/50048]	Loss: 2.8167
Training Epoch: 3 [20480/50048]	Loss: 3.1506
Training Epoch: 3 [20608/50048]	Loss: 2.6417
Training Epoch: 3 [20736/50048]	Loss: 2.7749
Training Epoch: 3 [20864/50048]	Loss: 2.9726
Training Epoch: 3 [20992/50048]	Loss: 2.7785
Training Epoch: 3 [21120/50048]	Loss: 2.8019
Training Epoch: 3 [21248/50048]	Loss: 2.8408
Training Epoch: 3 [21376/50048]	Loss: 2.7530
Training Epoch: 3 [21504/50048]	Loss: 2.6483
Training Epoch: 3 [21632/50048]	Loss: 2.6609
Training Epoch: 3 [21760/50048]	Loss: 2.7845
Training Epoch: 3 [21888/50048]	Loss: 2.9171
Training Epoch: 3 [22016/50048]	Loss: 2.7419
Training Epoch: 3 [22144/50048]	Loss: 2.7482
Training Epoch: 3 [22272/50048]	Loss: 2.6198
Training Epoch: 3 [22400/50048]	Loss: 2.7875
Training Epoch: 3 [22528/50048]	Loss: 2.8101
Training Epoch: 3 [22656/50048]	Loss: 2.6803
Training Epoch: 3 [22784/50048]	Loss: 2.7208
Training Epoch: 3 [22912/50048]	Loss: 2.5717
Training Epoch: 3 [23040/50048]	Loss: 2.5767
Training Epoch: 3 [23168/50048]	Loss: 2.8262
Training Epoch: 3 [23296/50048]	Loss: 2.8562
Training Epoch: 3 [23424/50048]	Loss: 2.5903
Training Epoch: 3 [23552/50048]	Loss: 2.4842
Training Epoch: 3 [23680/50048]	Loss: 2.9060
Training Epoch: 3 [23808/50048]	Loss: 2.6297
Training Epoch: 3 [23936/50048]	Loss: 2.6953
Training Epoch: 3 [24064/50048]	Loss: 2.7974
Training Epoch: 3 [24192/50048]	Loss: 2.8135
Training Epoch: 3 [24320/50048]	Loss: 2.7067
Training Epoch: 3 [24448/50048]	Loss: 2.6104
Training Epoch: 3 [24576/50048]	Loss: 2.8213
Training Epoch: 3 [24704/50048]	Loss: 2.7356
Training Epoch: 3 [24832/50048]	Loss: 2.7195
Training Epoch: 3 [24960/50048]	Loss: 2.8847
Training Epoch: 3 [25088/50048]	Loss: 2.9563
Training Epoch: 3 [25216/50048]	Loss: 3.0610
Training Epoch: 3 [25344/50048]	Loss: 2.8830
Training Epoch: 3 [25472/50048]	Loss: 2.6561
Training Epoch: 3 [25600/50048]	Loss: 2.9570
Training Epoch: 3 [25728/50048]	Loss: 2.6274
Training Epoch: 3 [25856/50048]	Loss: 2.7113
Training Epoch: 3 [25984/50048]	Loss: 2.8630
Training Epoch: 3 [26112/50048]	Loss: 2.7010
Training Epoch: 3 [26240/50048]	Loss: 2.9855
Training Epoch: 3 [26368/50048]	Loss: 2.9326
Training Epoch: 3 [26496/50048]	Loss: 2.8290
Training Epoch: 3 [26624/50048]	Loss: 3.0095
Training Epoch: 3 [26752/50048]	Loss: 2.9183
Training Epoch: 3 [26880/50048]	Loss: 2.7128
Training Epoch: 3 [27008/50048]	Loss: 2.8213
Training Epoch: 3 [27136/50048]	Loss: 2.9613
Training Epoch: 3 [27264/50048]	Loss: 2.6684
Training Epoch: 3 [27392/50048]	Loss: 2.8447
Training Epoch: 3 [27520/50048]	Loss: 2.9097
Training Epoch: 3 [27648/50048]	Loss: 2.7374
Training Epoch: 3 [27776/50048]	Loss: 2.9723
Training Epoch: 3 [27904/50048]	Loss: 2.8605
Training Epoch: 3 [28032/50048]	Loss: 2.5678
Training Epoch: 3 [28160/50048]	Loss: 2.7816
Training Epoch: 3 [28288/50048]	Loss: 2.6905
Training Epoch: 3 [28416/50048]	Loss: 2.5143
Training Epoch: 3 [28544/50048]	Loss: 2.7948
Training Epoch: 3 [28672/50048]	Loss: 2.5584
Training Epoch: 3 [28800/50048]	Loss: 2.6387
Training Epoch: 3 [28928/50048]	Loss: 2.7618
Training Epoch: 3 [29056/50048]	Loss: 2.9875
Training Epoch: 3 [29184/50048]	Loss: 2.8579
Training Epoch: 3 [29312/50048]	Loss: 2.6424
Training Epoch: 3 [29440/50048]	Loss: 2.5871
Training Epoch: 3 [29568/50048]	Loss: 2.6616
Training Epoch: 3 [29696/50048]	Loss: 2.5227
Training Epoch: 3 [29824/50048]	Loss: 2.7171
Training Epoch: 3 [29952/50048]	Loss: 2.9418
Training Epoch: 3 [30080/50048]	Loss: 2.7919
Training Epoch: 3 [30208/50048]	Loss: 2.8351
Training Epoch: 3 [30336/50048]	Loss: 2.7813
Training Epoch: 3 [30464/50048]	Loss: 2.6794
Training Epoch: 3 [30592/50048]	Loss: 2.8095
Training Epoch: 3 [30720/50048]	Loss: 2.6241
Training Epoch: 3 [30848/50048]	Loss: 2.9022
Training Epoch: 3 [30976/50048]	Loss: 2.7653
Training Epoch: 3 [31104/50048]	Loss: 2.8987
Training Epoch: 3 [31232/50048]	Loss: 2.6416
Training Epoch: 3 [31360/50048]	Loss: 2.8454
Training Epoch: 3 [31488/50048]	Loss: 2.5772
Training Epoch: 3 [31616/50048]	Loss: 2.8722
Training Epoch: 3 [31744/50048]	Loss: 2.5503
Training Epoch: 3 [31872/50048]	Loss: 2.8043
Training Epoch: 3 [32000/50048]	Loss: 2.5405
Training Epoch: 3 [32128/50048]	Loss: 3.0359
Training Epoch: 3 [32256/50048]	Loss: 2.7946
Training Epoch: 3 [32384/50048]	Loss: 2.8436
Training Epoch: 3 [32512/50048]	Loss: 2.5495
Training Epoch: 3 [32640/50048]	Loss: 2.6828
Training Epoch: 3 [32768/50048]	Loss: 2.6476
Training Epoch: 3 [32896/50048]	Loss: 2.3858
Training Epoch: 3 [33024/50048]	Loss: 2.5400
Training Epoch: 3 [33152/50048]	Loss: 2.6544
Training Epoch: 3 [33280/50048]	Loss: 2.7174
Training Epoch: 3 [33408/50048]	Loss: 2.5521
Training Epoch: 3 [33536/50048]	Loss: 2.7310
Training Epoch: 3 [33664/50048]	Loss: 2.6953
Training Epoch: 3 [33792/50048]	Loss: 2.6151
Training Epoch: 3 [33920/50048]	Loss: 2.5698
Training Epoch: 3 [34048/50048]	Loss: 2.5772
Training Epoch: 3 [34176/50048]	Loss: 2.7029
Training Epoch: 3 [34304/50048]	Loss: 2.5717
Training Epoch: 3 [34432/50048]	Loss: 2.5498
Training Epoch: 3 [34560/50048]	Loss: 2.7431
Training Epoch: 3 [34688/50048]	Loss: 2.9667
Training Epoch: 3 [34816/50048]	Loss: 2.7511
Training Epoch: 3 [34944/50048]	Loss: 2.5480
Training Epoch: 3 [35072/50048]	Loss: 2.6966
Training Epoch: 3 [35200/50048]	Loss: 2.7404
Training Epoch: 3 [35328/50048]	Loss: 2.7227
Training Epoch: 3 [35456/50048]	Loss: 2.6397
Training Epoch: 3 [35584/50048]	Loss: 2.5096
Training Epoch: 3 [35712/50048]	Loss: 2.8825
Training Epoch: 3 [35840/50048]	Loss: 2.4829
Training Epoch: 3 [35968/50048]	Loss: 2.6112
Training Epoch: 3 [36096/50048]	Loss: 2.9747
Training Epoch: 3 [36224/50048]	Loss: 2.7757
Training Epoch: 3 [36352/50048]	Loss: 2.6112
Training Epoch: 3 [36480/50048]	Loss: 2.8151
Training Epoch: 3 [36608/50048]	Loss: 2.5880
Training Epoch: 3 [36736/50048]	Loss: 2.5187
Training Epoch: 3 [36864/50048]	Loss: 2.7689
Training Epoch: 3 [36992/50048]	Loss: 2.5600
Training Epoch: 3 [37120/50048]	Loss: 2.8536
Training Epoch: 3 [37248/50048]	Loss: 2.6885
Training Epoch: 3 [37376/50048]	Loss: 2.5807
Training Epoch: 3 [37504/50048]	Loss: 2.4723
Training Epoch: 3 [37632/50048]	Loss: 2.5998
Training Epoch: 3 [37760/50048]	Loss: 2.5671
Training Epoch: 3 [37888/50048]	Loss: 2.5465
Training Epoch: 3 [38016/50048]	Loss: 2.7220
Training Epoch: 3 [38144/50048]	Loss: 2.7717
Training Epoch: 3 [38272/50048]	Loss: 2.9315
Training Epoch: 3 [38400/50048]	Loss: 2.5963
Training Epoch: 3 [38528/50048]	Loss: 2.4696
Training Epoch: 3 [38656/50048]	Loss: 2.7086
Training Epoch: 3 [38784/50048]	Loss: 2.7012
Training Epoch: 3 [38912/50048]	Loss: 2.8865
Training Epoch: 3 [39040/50048]	Loss: 2.9049
Training Epoch: 3 [39168/50048]	Loss: 2.8296
Training Epoch: 3 [39296/50048]	Loss: 2.5200
Training Epoch: 3 [39424/50048]	Loss: 2.7550
Training Epoch: 3 [39552/50048]	Loss: 2.7194
Training Epoch: 3 [39680/50048]	Loss: 2.9048
Training Epoch: 3 [39808/50048]	Loss: 2.7427
Training Epoch: 3 [39936/50048]	Loss: 2.6510
Training Epoch: 3 [40064/50048]	Loss: 2.6491
Training Epoch: 3 [40192/50048]	Loss: 2.6853
Training Epoch: 3 [40320/50048]	Loss: 2.5876
Training Epoch: 3 [40448/50048]	Loss: 2.7066
Training Epoch: 3 [40576/50048]	Loss: 2.6563
Training Epoch: 3 [40704/50048]	Loss: 2.5358
Training Epoch: 3 [40832/50048]	Loss: 2.6905
Training Epoch: 3 [40960/50048]	Loss: 2.8190
Training Epoch: 3 [41088/50048]	Loss: 2.8202
Training Epoch: 3 [41216/50048]	Loss: 2.5434
Training Epoch: 3 [41344/50048]	Loss: 2.8724
Training Epoch: 3 [41472/50048]	Loss: 2.7705
Training Epoch: 3 [41600/50048]	Loss: 2.6900
Training Epoch: 3 [41728/50048]	Loss: 2.7814
Training Epoch: 3 [41856/50048]	Loss: 2.7452
Training Epoch: 3 [41984/50048]	Loss: 2.6385
Training Epoch: 3 [42112/50048]	Loss: 2.7591
Training Epoch: 3 [42240/50048]	Loss: 2.6003
Training Epoch: 3 [42368/50048]	Loss: 2.7376
Training Epoch: 3 [42496/50048]	Loss: 2.9913
Training Epoch: 3 [42624/50048]	Loss: 2.9387
Training Epoch: 3 [42752/50048]	Loss: 2.7749
Training Epoch: 3 [42880/50048]	Loss: 2.6407
Training Epoch: 3 [43008/50048]	Loss: 2.7817
Training Epoch: 3 [43136/50048]	Loss: 2.6365
Training Epoch: 3 [43264/50048]	Loss: 2.6575
Training Epoch: 3 [43392/50048]	Loss: 2.6838
Training Epoch: 3 [43520/50048]	Loss: 2.6391
Training Epoch: 3 [43648/50048]	Loss: 2.6482
Training Epoch: 3 [43776/50048]	Loss: 2.8769
Training Epoch: 3 [43904/50048]	Loss: 2.6541
Training Epoch: 3 [44032/50048]	Loss: 2.6259
Training Epoch: 3 [44160/50048]	Loss: 2.5332
Training Epoch: 3 [44288/50048]	Loss: 2.4477
Training Epoch: 3 [44416/50048]	Loss: 2.7682
Training Epoch: 3 [44544/50048]	Loss: 2.7548
Training Epoch: 3 [44672/50048]	Loss: 2.7920
Training Epoch: 3 [44800/50048]	Loss: 2.8337
Training Epoch: 3 [44928/50048]	Loss: 2.6051
Training Epoch: 3 [45056/50048]	Loss: 2.8093
Training Epoch: 3 [45184/50048]	Loss: 2.4308
Training Epoch: 3 [45312/50048]	Loss: 2.5897
Training Epoch: 3 [45440/50048]	Loss: 2.2074
Training Epoch: 3 [45568/50048]	Loss: 2.5383
Training Epoch: 3 [45696/50048]	Loss: 2.5880
Training Epoch: 3 [45824/50048]	Loss: 2.8326
Training Epoch: 3 [45952/50048]	Loss: 2.3229
Training Epoch: 3 [46080/50048]	Loss: 2.4837
Training Epoch: 3 [46208/50048]	Loss: 2.6765
Training Epoch: 3 [46336/50048]	Loss: 2.5477
Training Epoch: 3 [46464/50048]	Loss: 2.5618
Training Epoch: 3 [46592/50048]	Loss: 2.6908
Training Epoch: 3 [46720/50048]	Loss: 2.7732
Training Epoch: 3 [46848/50048]	Loss: 2.6325
Training Epoch: 3 [46976/50048]	Loss: 2.4546
Training Epoch: 3 [47104/50048]	Loss: 2.7521
Training Epoch: 3 [47232/50048]	Loss: 2.4196
Training Epoch: 3 [47360/50048]	Loss: 2.7078
Training Epoch: 3 [47488/50048]	Loss: 2.6559
Training Epoch: 3 [47616/50048]	Loss: 2.4118
Training Epoch: 3 [47744/50048]	Loss: 2.6931
Training Epoch: 3 [47872/50048]	Loss: 2.4962
Training Epoch: 3 [48000/50048]	Loss: 2.6044
Training Epoch: 3 [48128/50048]	Loss: 2.5074
Training Epoch: 3 [48256/50048]	Loss: 2.7072
Training Epoch: 3 [48384/50048]	Loss: 2.6096
Training Epoch: 3 [48512/50048]	Loss: 2.6190
Training Epoch: 3 [48640/50048]	Loss: 2.8272
Training Epoch: 3 [48768/50048]	Loss: 2.8881
Training Epoch: 3 [48896/50048]	Loss: 2.7403
Training Epoch: 3 [49024/50048]	Loss: 2.7687
Training Epoch: 3 [49152/50048]	Loss: 2.7303
Training Epoch: 3 [49280/50048]	Loss: 2.7742
Training Epoch: 3 [49408/50048]	Loss: 2.8903
Training Epoch: 3 [49536/50048]	Loss: 2.6373
Training Epoch: 3 [49664/50048]	Loss: 2.3704
Training Epoch: 3 [49792/50048]	Loss: 2.9172
Training Epoch: 3 [49920/50048]	Loss: 2.7272
Training Epoch: 3 [50048/50048]	Loss: 2.5616
Validation Epoch: 3, Average loss: 0.0220, Accuracy: 0.2945
Training Epoch: 4 [128/50048]	Loss: 2.4264
Training Epoch: 4 [256/50048]	Loss: 2.7055
Training Epoch: 4 [384/50048]	Loss: 2.7065
Training Epoch: 4 [512/50048]	Loss: 2.5176
Training Epoch: 4 [640/50048]	Loss: 2.4591
Training Epoch: 4 [768/50048]	Loss: 2.6028
Training Epoch: 4 [896/50048]	Loss: 2.4467
Training Epoch: 4 [1024/50048]	Loss: 2.5560
Training Epoch: 4 [1152/50048]	Loss: 2.4782
Training Epoch: 4 [1280/50048]	Loss: 2.3792
Training Epoch: 4 [1408/50048]	Loss: 2.6755
Training Epoch: 4 [1536/50048]	Loss: 2.3321
Training Epoch: 4 [1664/50048]	Loss: 2.6039
Training Epoch: 4 [1792/50048]	Loss: 2.5686
Training Epoch: 4 [1920/50048]	Loss: 2.4456
Training Epoch: 4 [2048/50048]	Loss: 2.6117
Training Epoch: 4 [2176/50048]	Loss: 2.4524
Training Epoch: 4 [2304/50048]	Loss: 2.3772
Training Epoch: 4 [2432/50048]	Loss: 2.6605
Training Epoch: 4 [2560/50048]	Loss: 2.5337
Training Epoch: 4 [2688/50048]	Loss: 2.5291
Training Epoch: 4 [2816/50048]	Loss: 2.4466
Training Epoch: 4 [2944/50048]	Loss: 2.5642
Training Epoch: 4 [3072/50048]	Loss: 2.6670
Training Epoch: 4 [3200/50048]	Loss: 2.5077
Training Epoch: 4 [3328/50048]	Loss: 2.6044
Training Epoch: 4 [3456/50048]	Loss: 2.7843
Training Epoch: 4 [3584/50048]	Loss: 2.6859
Training Epoch: 4 [3712/50048]	Loss: 2.3522
Training Epoch: 4 [3840/50048]	Loss: 2.5853
Training Epoch: 4 [3968/50048]	Loss: 2.7655
Training Epoch: 4 [4096/50048]	Loss: 2.8165
Training Epoch: 4 [4224/50048]	Loss: 2.3509
Training Epoch: 4 [4352/50048]	Loss: 2.3591
Training Epoch: 4 [4480/50048]	Loss: 2.6788
Training Epoch: 4 [4608/50048]	Loss: 2.6108
Training Epoch: 4 [4736/50048]	Loss: 2.6110
Training Epoch: 4 [4864/50048]	Loss: 2.7495
Training Epoch: 4 [4992/50048]	Loss: 2.6529
Training Epoch: 4 [5120/50048]	Loss: 2.5417
Training Epoch: 4 [5248/50048]	Loss: 2.3998
Training Epoch: 4 [5376/50048]	Loss: 2.5667
Training Epoch: 4 [5504/50048]	Loss: 2.3519
Training Epoch: 4 [5632/50048]	Loss: 2.4783
Training Epoch: 4 [5760/50048]	Loss: 2.5091
Training Epoch: 4 [5888/50048]	Loss: 2.3867
Training Epoch: 4 [6016/50048]	Loss: 2.3831
Training Epoch: 4 [6144/50048]	Loss: 2.3749
Training Epoch: 4 [6272/50048]	Loss: 2.5216
Training Epoch: 4 [6400/50048]	Loss: 2.5148
Training Epoch: 4 [6528/50048]	Loss: 2.5737
Training Epoch: 4 [6656/50048]	Loss: 2.5119
Training Epoch: 4 [6784/50048]	Loss: 2.5701
Training Epoch: 4 [6912/50048]	Loss: 2.4844
Training Epoch: 4 [7040/50048]	Loss: 2.2937
Training Epoch: 4 [7168/50048]	Loss: 2.7101
Training Epoch: 4 [7296/50048]	Loss: 2.4325
Training Epoch: 4 [7424/50048]	Loss: 2.5687
Training Epoch: 4 [7552/50048]	Loss: 2.5700
Training Epoch: 4 [7680/50048]	Loss: 2.4117
Training Epoch: 4 [7808/50048]	Loss: 2.1866
Training Epoch: 4 [7936/50048]	Loss: 2.9205
Training Epoch: 4 [8064/50048]	Loss: 2.6198
Training Epoch: 4 [8192/50048]	Loss: 2.5962
Training Epoch: 4 [8320/50048]	Loss: 2.5681
Training Epoch: 4 [8448/50048]	Loss: 2.3509
Training Epoch: 4 [8576/50048]	Loss: 2.3921
Training Epoch: 4 [8704/50048]	Loss: 2.3762
Training Epoch: 4 [8832/50048]	Loss: 2.6868
Training Epoch: 4 [8960/50048]	Loss: 2.2897
Training Epoch: 4 [9088/50048]	Loss: 2.5684
Training Epoch: 4 [9216/50048]	Loss: 2.4676
Training Epoch: 4 [9344/50048]	Loss: 2.6341
Training Epoch: 4 [9472/50048]	Loss: 2.6883
Training Epoch: 4 [9600/50048]	Loss: 2.5380
Training Epoch: 4 [9728/50048]	Loss: 2.3263
Training Epoch: 4 [9856/50048]	Loss: 2.5634
Training Epoch: 4 [9984/50048]	Loss: 2.6471
Training Epoch: 4 [10112/50048]	Loss: 2.5003
Training Epoch: 4 [10240/50048]	Loss: 2.6646
Training Epoch: 4 [10368/50048]	Loss: 2.8320
Training Epoch: 4 [10496/50048]	Loss: 2.7874
Training Epoch: 4 [10624/50048]	Loss: 2.3777
Training Epoch: 4 [10752/50048]	Loss: 2.4399
Training Epoch: 4 [10880/50048]	Loss: 2.4532
Training Epoch: 4 [11008/50048]	Loss: 2.5469
Training Epoch: 4 [11136/50048]	Loss: 2.4437
Training Epoch: 4 [11264/50048]	Loss: 2.5753
Training Epoch: 4 [11392/50048]	Loss: 2.5953
Training Epoch: 4 [11520/50048]	Loss: 2.4538
Training Epoch: 4 [11648/50048]	Loss: 2.4318
Training Epoch: 4 [11776/50048]	Loss: 2.4820
Training Epoch: 4 [11904/50048]	Loss: 2.2125
Training Epoch: 4 [12032/50048]	Loss: 2.2390
Training Epoch: 4 [12160/50048]	Loss: 2.7168
Training Epoch: 4 [12288/50048]	Loss: 2.5125
Training Epoch: 4 [12416/50048]	Loss: 2.2382
Training Epoch: 4 [12544/50048]	Loss: 2.3748
Training Epoch: 4 [12672/50048]	Loss: 2.5298
Training Epoch: 4 [12800/50048]	Loss: 2.4224
Training Epoch: 4 [12928/50048]	Loss: 2.5879
Training Epoch: 4 [13056/50048]	Loss: 2.4549
Training Epoch: 4 [13184/50048]	Loss: 2.3317
Training Epoch: 4 [13312/50048]	Loss: 2.2343
Training Epoch: 4 [13440/50048]	Loss: 2.5321
Training Epoch: 4 [13568/50048]	Loss: 2.6246
Training Epoch: 4 [13696/50048]	Loss: 2.6174
Training Epoch: 4 [13824/50048]	Loss: 2.6166
Training Epoch: 4 [13952/50048]	Loss: 2.6462
Training Epoch: 4 [14080/50048]	Loss: 2.7506
Training Epoch: 4 [14208/50048]	Loss: 2.6305
Training Epoch: 4 [14336/50048]	Loss: 2.5866
Training Epoch: 4 [14464/50048]	Loss: 2.4167
Training Epoch: 4 [14592/50048]	Loss: 2.6372
Training Epoch: 4 [14720/50048]	Loss: 2.5113
Training Epoch: 4 [14848/50048]	Loss: 2.5389
Training Epoch: 4 [14976/50048]	Loss: 2.2463
Training Epoch: 4 [15104/50048]	Loss: 2.4909
Training Epoch: 4 [15232/50048]	Loss: 2.2634
Training Epoch: 4 [15360/50048]	Loss: 2.3032
Training Epoch: 4 [15488/50048]	Loss: 2.2987
Training Epoch: 4 [15616/50048]	Loss: 2.4355
Training Epoch: 4 [15744/50048]	Loss: 2.6268
Training Epoch: 4 [15872/50048]	Loss: 2.3864
Training Epoch: 4 [16000/50048]	Loss: 2.5795
Training Epoch: 4 [16128/50048]	Loss: 2.5445
Training Epoch: 4 [16256/50048]	Loss: 2.2868
Training Epoch: 4 [16384/50048]	Loss: 2.5091
Training Epoch: 4 [16512/50048]	Loss: 2.6315
Training Epoch: 4 [16640/50048]	Loss: 2.6001
Training Epoch: 4 [16768/50048]	Loss: 2.4984
Training Epoch: 4 [16896/50048]	Loss: 2.7032
Training Epoch: 4 [17024/50048]	Loss: 2.6582
Training Epoch: 4 [17152/50048]	Loss: 2.6698
Training Epoch: 4 [17280/50048]	Loss: 2.4605
Training Epoch: 4 [17408/50048]	Loss: 2.4455
Training Epoch: 4 [17536/50048]	Loss: 2.4330
Training Epoch: 4 [17664/50048]	Loss: 2.4291
Training Epoch: 4 [17792/50048]	Loss: 2.6203
Training Epoch: 4 [17920/50048]	Loss: 2.7083
Training Epoch: 4 [18048/50048]	Loss: 2.6724
Training Epoch: 4 [18176/50048]	Loss: 2.6763
Training Epoch: 4 [18304/50048]	Loss: 2.4299
Training Epoch: 4 [18432/50048]	Loss: 2.4619
Training Epoch: 4 [18560/50048]	Loss: 2.6383
Training Epoch: 4 [18688/50048]	Loss: 2.8103
Training Epoch: 4 [18816/50048]	Loss: 2.3559
Training Epoch: 4 [18944/50048]	Loss: 2.4295
Training Epoch: 4 [19072/50048]	Loss: 2.7018
Training Epoch: 4 [19200/50048]	Loss: 2.7020
Training Epoch: 4 [19328/50048]	Loss: 2.5446
Training Epoch: 4 [19456/50048]	Loss: 2.5254
Training Epoch: 4 [19584/50048]	Loss: 2.5439
Training Epoch: 4 [19712/50048]	Loss: 2.2635
Training Epoch: 4 [19840/50048]	Loss: 2.5197
Training Epoch: 4 [19968/50048]	Loss: 2.3191
Training Epoch: 4 [20096/50048]	Loss: 2.7035
Training Epoch: 4 [20224/50048]	Loss: 2.5421
Training Epoch: 4 [20352/50048]	Loss: 2.3949
Training Epoch: 4 [20480/50048]	Loss: 2.5915
Training Epoch: 4 [20608/50048]	Loss: 2.3981
Training Epoch: 4 [20736/50048]	Loss: 2.5170
Training Epoch: 4 [20864/50048]	Loss: 2.1493
Training Epoch: 4 [20992/50048]	Loss: 2.5640
Training Epoch: 4 [21120/50048]	Loss: 2.7546
Training Epoch: 4 [21248/50048]	Loss: 2.5912
Training Epoch: 4 [21376/50048]	Loss: 2.8840
Training Epoch: 4 [21504/50048]	Loss: 2.6979
Training Epoch: 4 [21632/50048]	Loss: 2.7401
Training Epoch: 4 [21760/50048]	Loss: 2.4899
Training Epoch: 4 [21888/50048]	Loss: 2.4287
Training Epoch: 4 [22016/50048]	Loss: 2.4164
Training Epoch: 4 [22144/50048]	Loss: 2.7665
Training Epoch: 4 [22272/50048]	Loss: 2.4481
Training Epoch: 4 [22400/50048]	Loss: 2.2980
Training Epoch: 4 [22528/50048]	Loss: 2.3599
Training Epoch: 4 [22656/50048]	Loss: 2.3676
Training Epoch: 4 [22784/50048]	Loss: 2.3529
Training Epoch: 4 [22912/50048]	Loss: 2.2897
Training Epoch: 4 [23040/50048]	Loss: 2.3238
Training Epoch: 4 [23168/50048]	Loss: 2.2113
Training Epoch: 4 [23296/50048]	Loss: 2.6584
Training Epoch: 4 [23424/50048]	Loss: 2.6430
Training Epoch: 4 [23552/50048]	Loss: 2.1151
Training Epoch: 4 [23680/50048]	Loss: 2.6059
Training Epoch: 4 [23808/50048]	Loss: 2.3780
Training Epoch: 4 [23936/50048]	Loss: 2.5556
Training Epoch: 4 [24064/50048]	Loss: 2.3919
Training Epoch: 4 [24192/50048]	Loss: 2.4500
Training Epoch: 4 [24320/50048]	Loss: 2.3667
Training Epoch: 4 [24448/50048]	Loss: 2.4319
Training Epoch: 4 [24576/50048]	Loss: 2.5206
Training Epoch: 4 [24704/50048]	Loss: 2.6795
Training Epoch: 4 [24832/50048]	Loss: 2.5073
Training Epoch: 4 [24960/50048]	Loss: 2.2376
Training Epoch: 4 [25088/50048]	Loss: 2.6544
Training Epoch: 4 [25216/50048]	Loss: 2.3130
Training Epoch: 4 [25344/50048]	Loss: 2.7133
Training Epoch: 4 [25472/50048]	Loss: 2.6075
Training Epoch: 4 [25600/50048]	Loss: 2.4698
Training Epoch: 4 [25728/50048]	Loss: 2.3977
Training Epoch: 4 [25856/50048]	Loss: 2.6743
Training Epoch: 4 [25984/50048]	Loss: 2.6679
Training Epoch: 4 [26112/50048]	Loss: 2.3724
Training Epoch: 4 [26240/50048]	Loss: 2.1816
Training Epoch: 4 [26368/50048]	Loss: 2.3117
Training Epoch: 4 [26496/50048]	Loss: 2.6385
Training Epoch: 4 [26624/50048]	Loss: 2.4612
Training Epoch: 4 [26752/50048]	Loss: 2.5933
Training Epoch: 4 [26880/50048]	Loss: 2.6246
Training Epoch: 4 [27008/50048]	Loss: 2.5239
Training Epoch: 4 [27136/50048]	Loss: 2.6097
Training Epoch: 4 [27264/50048]	Loss: 2.7463
Training Epoch: 4 [27392/50048]	Loss: 2.4365
Training Epoch: 4 [27520/50048]	Loss: 2.5109
Training Epoch: 4 [27648/50048]	Loss: 2.6752
Training Epoch: 4 [27776/50048]	Loss: 2.4891
Training Epoch: 4 [27904/50048]	Loss: 2.3827
Training Epoch: 4 [28032/50048]	Loss: 2.3068
Training Epoch: 4 [28160/50048]	Loss: 2.5850
Training Epoch: 4 [28288/50048]	Loss: 2.5718
Training Epoch: 4 [28416/50048]	Loss: 2.6323
Training Epoch: 4 [28544/50048]	Loss: 2.4533
Training Epoch: 4 [28672/50048]	Loss: 2.4495
Training Epoch: 4 [28800/50048]	Loss: 2.2567
Training Epoch: 4 [28928/50048]	Loss: 2.4645
Training Epoch: 4 [29056/50048]	Loss: 2.4681
Training Epoch: 4 [29184/50048]	Loss: 2.6950
Training Epoch: 4 [29312/50048]	Loss: 2.5425
Training Epoch: 4 [29440/50048]	Loss: 2.3917
Training Epoch: 4 [29568/50048]	Loss: 2.1626
Training Epoch: 4 [29696/50048]	Loss: 2.3960
Training Epoch: 4 [29824/50048]	Loss: 2.4120
Training Epoch: 4 [29952/50048]	Loss: 2.6655
Training Epoch: 4 [30080/50048]	Loss: 2.5667
Training Epoch: 4 [30208/50048]	Loss: 2.4879
Training Epoch: 4 [30336/50048]	Loss: 2.3186
Training Epoch: 4 [30464/50048]	Loss: 2.5522
Training Epoch: 4 [30592/50048]	Loss: 2.2744
Training Epoch: 4 [30720/50048]	Loss: 2.2297
Training Epoch: 4 [30848/50048]	Loss: 2.4538
Training Epoch: 4 [30976/50048]	Loss: 2.5345
Training Epoch: 4 [31104/50048]	Loss: 2.2744
Training Epoch: 4 [31232/50048]	Loss: 2.6776
Training Epoch: 4 [31360/50048]	Loss: 2.3991
Training Epoch: 4 [31488/50048]	Loss: 2.6022
Training Epoch: 4 [31616/50048]	Loss: 2.4131
Training Epoch: 4 [31744/50048]	Loss: 2.3356
Training Epoch: 4 [31872/50048]	Loss: 2.3214
Training Epoch: 4 [32000/50048]	Loss: 2.5878
Training Epoch: 4 [32128/50048]	Loss: 2.5117
Training Epoch: 4 [32256/50048]	Loss: 2.6207
Training Epoch: 4 [32384/50048]	Loss: 2.5323
Training Epoch: 4 [32512/50048]	Loss: 2.4662
Training Epoch: 4 [32640/50048]	Loss: 2.6119
Training Epoch: 4 [32768/50048]	Loss: 2.2444
Training Epoch: 4 [32896/50048]	Loss: 2.2347
Training Epoch: 4 [33024/50048]	Loss: 2.5565
Training Epoch: 4 [33152/50048]	Loss: 2.6621
Training Epoch: 4 [33280/50048]	Loss: 2.4567
Training Epoch: 4 [33408/50048]	Loss: 2.4591
Training Epoch: 4 [33536/50048]	Loss: 2.5108
Training Epoch: 4 [33664/50048]	Loss: 2.5400
Training Epoch: 4 [33792/50048]	Loss: 2.3360
Training Epoch: 4 [33920/50048]	Loss: 2.4361
Training Epoch: 4 [34048/50048]	Loss: 2.7485
Training Epoch: 4 [34176/50048]	Loss: 2.4790
Training Epoch: 4 [34304/50048]	Loss: 2.5321
Training Epoch: 4 [34432/50048]	Loss: 2.2383
Training Epoch: 4 [34560/50048]	Loss: 2.2428
Training Epoch: 4 [34688/50048]	Loss: 2.2660
Training Epoch: 4 [34816/50048]	Loss: 2.3348
Training Epoch: 4 [34944/50048]	Loss: 2.5840
Training Epoch: 4 [35072/50048]	Loss: 2.3847
Training Epoch: 4 [35200/50048]	Loss: 2.8856
Training Epoch: 4 [35328/50048]	Loss: 2.5136
Training Epoch: 4 [35456/50048]	Loss: 2.1405
Training Epoch: 4 [35584/50048]	Loss: 2.3162
Training Epoch: 4 [35712/50048]	Loss: 2.4407
Training Epoch: 4 [35840/50048]	Loss: 2.2421
Training Epoch: 4 [35968/50048]	Loss: 2.5312
Training Epoch: 4 [36096/50048]	Loss: 2.3493
Training Epoch: 4 [36224/50048]	Loss: 2.3132
Training Epoch: 4 [36352/50048]	Loss: 2.4543
Training Epoch: 4 [36480/50048]	Loss: 2.4951
Training Epoch: 4 [36608/50048]	Loss: 2.6754
Training Epoch: 4 [36736/50048]	Loss: 2.4884
Training Epoch: 4 [36864/50048]	Loss: 2.2830
Training Epoch: 4 [36992/50048]	Loss: 2.2654
Training Epoch: 4 [37120/50048]	Loss: 2.3599
Training Epoch: 4 [37248/50048]	Loss: 2.3022
Training Epoch: 4 [37376/50048]	Loss: 2.2453
Training Epoch: 4 [37504/50048]	Loss: 2.3451
Training Epoch: 4 [37632/50048]	Loss: 2.4764
Training Epoch: 4 [37760/50048]	Loss: 2.4364
Training Epoch: 4 [37888/50048]	Loss: 2.7830
Training Epoch: 4 [38016/50048]	Loss: 2.7033
Training Epoch: 4 [38144/50048]	Loss: 2.3770
Training Epoch: 4 [38272/50048]	Loss: 2.4708
Training Epoch: 4 [38400/50048]	Loss: 2.7209
Training Epoch: 4 [38528/50048]	Loss: 2.6736
Training Epoch: 4 [38656/50048]	Loss: 2.4039
Training Epoch: 4 [38784/50048]	Loss: 2.3623
Training Epoch: 4 [38912/50048]	Loss: 2.3446
Training Epoch: 4 [39040/50048]	Loss: 2.3787
Training Epoch: 4 [39168/50048]	Loss: 2.3769
Training Epoch: 4 [39296/50048]	Loss: 2.1361
Training Epoch: 4 [39424/50048]	Loss: 2.7160
Training Epoch: 4 [39552/50048]	Loss: 2.6394
Training Epoch: 4 [39680/50048]	Loss: 2.4894
Training Epoch: 4 [39808/50048]	Loss: 2.4262
Training Epoch: 4 [39936/50048]	Loss: 2.3614
Training Epoch: 4 [40064/50048]	Loss: 2.4802
Training Epoch: 4 [40192/50048]	Loss: 2.3052
Training Epoch: 4 [40320/50048]	Loss: 2.5137
Training Epoch: 4 [40448/50048]	Loss: 2.6674
Training Epoch: 4 [40576/50048]	Loss: 2.2465
Training Epoch: 4 [40704/50048]	Loss: 2.4174
Training Epoch: 4 [40832/50048]	Loss: 2.4128
Training Epoch: 4 [40960/50048]	Loss: 2.1299
Training Epoch: 4 [41088/50048]	Loss: 2.4735
Training Epoch: 4 [41216/50048]	Loss: 2.4383
Training Epoch: 4 [41344/50048]	Loss: 2.4331
Training Epoch: 4 [41472/50048]	Loss: 2.3327
Training Epoch: 4 [41600/50048]	Loss: 2.3896
Training Epoch: 4 [41728/50048]	Loss: 2.5182
Training Epoch: 4 [41856/50048]	Loss: 2.3793
Training Epoch: 4 [41984/50048]	Loss: 2.3995
Training Epoch: 4 [42112/50048]	Loss: 2.2938
Training Epoch: 4 [42240/50048]	Loss: 2.4796
Training Epoch: 4 [42368/50048]	Loss: 2.5475
Training Epoch: 4 [42496/50048]	Loss: 2.7172
Training Epoch: 4 [42624/50048]	Loss: 2.3947
Training Epoch: 4 [42752/50048]	Loss: 2.4191
Training Epoch: 4 [42880/50048]	Loss: 2.4431
Training Epoch: 4 [43008/50048]	Loss: 2.4080
Training Epoch: 4 [43136/50048]	Loss: 2.2884
Training Epoch: 4 [43264/50048]	Loss: 2.5398
Training Epoch: 4 [43392/50048]	Loss: 2.4675
Training Epoch: 4 [43520/50048]	Loss: 2.6222
Training Epoch: 4 [43648/50048]	Loss: 2.3420
Training Epoch: 4 [43776/50048]	Loss: 2.3734
Training Epoch: 4 [43904/50048]	Loss: 2.6304
Training Epoch: 4 [44032/50048]	Loss: 2.3610
Training Epoch: 4 [44160/50048]	Loss: 2.5012
Training Epoch: 4 [44288/50048]	Loss: 2.6111
Training Epoch: 4 [44416/50048]	Loss: 2.6388
Training Epoch: 4 [44544/50048]	Loss: 2.5412
Training Epoch: 4 [44672/50048]	Loss: 2.0846
Training Epoch: 4 [44800/50048]	Loss: 2.7543
Training Epoch: 4 [44928/50048]	Loss: 2.4327
Training Epoch: 4 [45056/50048]	Loss: 2.3477
Training Epoch: 4 [45184/50048]	Loss: 2.3527
Training Epoch: 4 [45312/50048]	Loss: 2.5102
Training Epoch: 4 [45440/50048]	Loss: 2.6644
Training Epoch: 4 [45568/50048]	Loss: 2.1946
Training Epoch: 4 [45696/50048]	Loss: 2.4154
Training Epoch: 4 [45824/50048]	Loss: 2.4201
Training Epoch: 4 [45952/50048]	Loss: 2.3007
Training Epoch: 4 [46080/50048]	Loss: 2.2913
Training Epoch: 4 [46208/50048]	Loss: 2.5017
Training Epoch: 4 [46336/50048]	Loss: 2.2973
Training Epoch: 4 [46464/50048]	Loss: 2.3788
Training Epoch: 4 [46592/50048]	Loss: 2.4201
Training Epoch: 4 [46720/50048]	Loss: 2.2675
Training Epoch: 4 [46848/50048]	Loss: 2.1956
Training Epoch: 4 [46976/50048]	Loss: 2.4030
Training Epoch: 4 [47104/50048]	Loss: 2.3604
Training Epoch: 4 [47232/50048]	Loss: 2.3333
Training Epoch: 4 [47360/50048]	Loss: 2.4852
Training Epoch: 4 [47488/50048]	Loss: 2.6515
Training Epoch: 4 [47616/50048]	Loss: 2.4328
Training Epoch: 4 [47744/50048]	Loss: 2.4963
Training Epoch: 4 [47872/50048]	Loss: 2.5478
Training Epoch: 4 [48000/50048]	Loss: 2.3876
Training Epoch: 4 [48128/50048]	Loss: 2.3720
Training Epoch: 4 [48256/50048]	Loss: 2.4316
Training Epoch: 4 [48384/50048]	Loss: 2.3010
Training Epoch: 4 [48512/50048]	Loss: 2.4178
Training Epoch: 4 [48640/50048]	Loss: 2.1010
Training Epoch: 4 [48768/50048]	Loss: 2.2922
Training Epoch: 4 [48896/50048]	Loss: 2.5727
Training Epoch: 4 [49024/50048]	Loss: 2.4114
Training Epoch: 4 [49152/50048]	Loss: 2.1915
Training Epoch: 4 [49280/50048]	Loss: 2.3571
Training Epoch: 4 [49408/50048]	Loss: 2.5060
Training Epoch: 4 [49536/50048]	Loss: 2.3145
Training Epoch: 4 [49664/50048]	Loss: 2.5875
Training Epoch: 4 [49792/50048]	Loss: 2.4517
Training Epoch: 4 [49920/50048]	Loss: 2.2293
Training Epoch: 4 [50048/50048]	Loss: 2.3459
Validation Epoch: 4, Average loss: 0.0286, Accuracy: 0.2158
Training Epoch: 5 [128/50048]	Loss: 2.0290
Training Epoch: 5 [256/50048]	Loss: 2.2565
Training Epoch: 5 [384/50048]	Loss: 2.5468
Training Epoch: 5 [512/50048]	Loss: 2.3326
Training Epoch: 5 [640/50048]	Loss: 2.2268
Training Epoch: 5 [768/50048]	Loss: 2.4112
Training Epoch: 5 [896/50048]	Loss: 2.2849
Training Epoch: 5 [1024/50048]	Loss: 2.2442
Training Epoch: 5 [1152/50048]	Loss: 2.0189
Training Epoch: 5 [1280/50048]	Loss: 2.2283
Training Epoch: 5 [1408/50048]	Loss: 2.1366
Training Epoch: 5 [1536/50048]	Loss: 2.5874
Training Epoch: 5 [1664/50048]	Loss: 2.0758
Training Epoch: 5 [1792/50048]	Loss: 2.5907
Training Epoch: 5 [1920/50048]	Loss: 2.2140
Training Epoch: 5 [2048/50048]	Loss: 2.2045
Training Epoch: 5 [2176/50048]	Loss: 2.2216
Training Epoch: 5 [2304/50048]	Loss: 2.1500
Training Epoch: 5 [2432/50048]	Loss: 2.4521
Training Epoch: 5 [2560/50048]	Loss: 2.3595
Training Epoch: 5 [2688/50048]	Loss: 2.2790
Training Epoch: 5 [2816/50048]	Loss: 2.2946
Training Epoch: 5 [2944/50048]	Loss: 2.4888
Training Epoch: 5 [3072/50048]	Loss: 2.2755
Training Epoch: 5 [3200/50048]	Loss: 2.1677
Training Epoch: 5 [3328/50048]	Loss: 2.3260
Training Epoch: 5 [3456/50048]	Loss: 2.2365
Training Epoch: 5 [3584/50048]	Loss: 2.1952
Training Epoch: 5 [3712/50048]	Loss: 2.3407
Training Epoch: 5 [3840/50048]	Loss: 2.0905
Training Epoch: 5 [3968/50048]	Loss: 2.0222
Training Epoch: 5 [4096/50048]	Loss: 2.5009
Training Epoch: 5 [4224/50048]	Loss: 2.2365
Training Epoch: 5 [4352/50048]	Loss: 2.0987
Training Epoch: 5 [4480/50048]	Loss: 2.4577
Training Epoch: 5 [4608/50048]	Loss: 2.3473
Training Epoch: 5 [4736/50048]	Loss: 2.2532
Training Epoch: 5 [4864/50048]	Loss: 2.1318
Training Epoch: 5 [4992/50048]	Loss: 2.3970
Training Epoch: 5 [5120/50048]	Loss: 2.4970
Training Epoch: 5 [5248/50048]	Loss: 2.3529
Training Epoch: 5 [5376/50048]	Loss: 2.2461
Training Epoch: 5 [5504/50048]	Loss: 2.3606
Training Epoch: 5 [5632/50048]	Loss: 2.2526
Training Epoch: 5 [5760/50048]	Loss: 2.1442
Training Epoch: 5 [5888/50048]	Loss: 2.3239
Training Epoch: 5 [6016/50048]	Loss: 2.2453
Training Epoch: 5 [6144/50048]	Loss: 2.5794
Training Epoch: 5 [6272/50048]	Loss: 2.3769
Training Epoch: 5 [6400/50048]	Loss: 2.4915
Training Epoch: 5 [6528/50048]	Loss: 2.1245
Training Epoch: 5 [6656/50048]	Loss: 2.4518
Training Epoch: 5 [6784/50048]	Loss: 2.3403
Training Epoch: 5 [6912/50048]	Loss: 2.1581
Training Epoch: 5 [7040/50048]	Loss: 2.3360
Training Epoch: 5 [7168/50048]	Loss: 1.9782
Training Epoch: 5 [7296/50048]	Loss: 2.2111
Training Epoch: 5 [7424/50048]	Loss: 2.4834
Training Epoch: 5 [7552/50048]	Loss: 2.3537
Training Epoch: 5 [7680/50048]	Loss: 1.9479
Training Epoch: 5 [7808/50048]	Loss: 2.2341
Training Epoch: 5 [7936/50048]	Loss: 2.4638
Training Epoch: 5 [8064/50048]	Loss: 2.4819
Training Epoch: 5 [8192/50048]	Loss: 2.3469
Training Epoch: 5 [8320/50048]	Loss: 2.5920
Training Epoch: 5 [8448/50048]	Loss: 2.2379
Training Epoch: 5 [8576/50048]	Loss: 2.3657
Training Epoch: 5 [8704/50048]	Loss: 2.5695
Training Epoch: 5 [8832/50048]	Loss: 2.2431
Training Epoch: 5 [8960/50048]	Loss: 2.1841
Training Epoch: 5 [9088/50048]	Loss: 2.4747
Training Epoch: 5 [9216/50048]	Loss: 2.6273
Training Epoch: 5 [9344/50048]	Loss: 2.2319
Training Epoch: 5 [9472/50048]	Loss: 2.2558
Training Epoch: 5 [9600/50048]	Loss: 2.2673
Training Epoch: 5 [9728/50048]	Loss: 2.1999
Training Epoch: 5 [9856/50048]	Loss: 2.1877
Training Epoch: 5 [9984/50048]	Loss: 2.1967
Training Epoch: 5 [10112/50048]	Loss: 2.3264
Training Epoch: 5 [10240/50048]	Loss: 2.4222
Training Epoch: 5 [10368/50048]	Loss: 2.2667
Training Epoch: 5 [10496/50048]	Loss: 2.2056
Training Epoch: 5 [10624/50048]	Loss: 2.0986
Training Epoch: 5 [10752/50048]	Loss: 2.3295
Training Epoch: 5 [10880/50048]	Loss: 2.4567
Training Epoch: 5 [11008/50048]	Loss: 2.2547
Training Epoch: 5 [11136/50048]	Loss: 2.3445
Training Epoch: 5 [11264/50048]	Loss: 2.0805
Training Epoch: 5 [11392/50048]	Loss: 2.4371
Training Epoch: 5 [11520/50048]	Loss: 2.1927
Training Epoch: 5 [11648/50048]	Loss: 1.9657
Training Epoch: 5 [11776/50048]	Loss: 2.4682
Training Epoch: 5 [11904/50048]	Loss: 2.3884
Training Epoch: 5 [12032/50048]	Loss: 2.1946
Training Epoch: 5 [12160/50048]	Loss: 2.4697
Training Epoch: 5 [12288/50048]	Loss: 2.2187
Training Epoch: 5 [12416/50048]	Loss: 2.3718
Training Epoch: 5 [12544/50048]	Loss: 2.5039
Training Epoch: 5 [12672/50048]	Loss: 2.3927
Training Epoch: 5 [12800/50048]	Loss: 2.1416
Training Epoch: 5 [12928/50048]	Loss: 2.5944
Training Epoch: 5 [13056/50048]	Loss: 2.4162
Training Epoch: 5 [13184/50048]	Loss: 2.5487
Training Epoch: 5 [13312/50048]	Loss: 2.5081
Training Epoch: 5 [13440/50048]	Loss: 2.2407
Training Epoch: 5 [13568/50048]	Loss: 2.4649
Training Epoch: 5 [13696/50048]	Loss: 2.2010
Training Epoch: 5 [13824/50048]	Loss: 2.1798
Training Epoch: 5 [13952/50048]	Loss: 2.0918
Training Epoch: 5 [14080/50048]	Loss: 2.6580
Training Epoch: 5 [14208/50048]	Loss: 2.2548
Training Epoch: 5 [14336/50048]	Loss: 2.2761
Training Epoch: 5 [14464/50048]	Loss: 2.3362
Training Epoch: 5 [14592/50048]	Loss: 2.1373
Training Epoch: 5 [14720/50048]	Loss: 2.1626
Training Epoch: 5 [14848/50048]	Loss: 2.1689
Training Epoch: 5 [14976/50048]	Loss: 2.3372
Training Epoch: 5 [15104/50048]	Loss: 2.2259
Training Epoch: 5 [15232/50048]	Loss: 2.0553
Training Epoch: 5 [15360/50048]	Loss: 2.0192
Training Epoch: 5 [15488/50048]	Loss: 2.1619
Training Epoch: 5 [15616/50048]	Loss: 2.4477
Training Epoch: 5 [15744/50048]	Loss: 2.3767
Training Epoch: 5 [15872/50048]	Loss: 2.1537
Training Epoch: 5 [16000/50048]	Loss: 2.5366
Training Epoch: 5 [16128/50048]	Loss: 2.3031
Training Epoch: 5 [16256/50048]	Loss: 2.3077
Training Epoch: 5 [16384/50048]	Loss: 2.3453
Training Epoch: 5 [16512/50048]	Loss: 2.0198
Training Epoch: 5 [16640/50048]	Loss: 2.3589
Training Epoch: 5 [16768/50048]	Loss: 2.3313
Training Epoch: 5 [16896/50048]	Loss: 2.1942
Training Epoch: 5 [17024/50048]	Loss: 2.7540
Training Epoch: 5 [17152/50048]	Loss: 2.4199
Training Epoch: 5 [17280/50048]	Loss: 2.5199
Training Epoch: 5 [17408/50048]	Loss: 2.4256
Training Epoch: 5 [17536/50048]	Loss: 2.4221
Training Epoch: 5 [17664/50048]	Loss: 1.9986
Training Epoch: 5 [17792/50048]	Loss: 2.4997
Training Epoch: 5 [17920/50048]	Loss: 2.1870
Training Epoch: 5 [18048/50048]	Loss: 2.1676
Training Epoch: 5 [18176/50048]	Loss: 2.3012
Training Epoch: 5 [18304/50048]	Loss: 2.1166
Training Epoch: 5 [18432/50048]	Loss: 1.9051
Training Epoch: 5 [18560/50048]	Loss: 2.1016
Training Epoch: 5 [18688/50048]	Loss: 2.5639
Training Epoch: 5 [18816/50048]	Loss: 2.4394
Training Epoch: 5 [18944/50048]	Loss: 2.2542
Training Epoch: 5 [19072/50048]	Loss: 2.1346
Training Epoch: 5 [19200/50048]	Loss: 2.1096
Training Epoch: 5 [19328/50048]	Loss: 2.5074
Training Epoch: 5 [19456/50048]	Loss: 2.3294
Training Epoch: 5 [19584/50048]	Loss: 2.3037
Training Epoch: 5 [19712/50048]	Loss: 2.3144
Training Epoch: 5 [19840/50048]	Loss: 2.1043
Training Epoch: 5 [19968/50048]	Loss: 2.3036
Training Epoch: 5 [20096/50048]	Loss: 2.2086
Training Epoch: 5 [20224/50048]	Loss: 2.5388
Training Epoch: 5 [20352/50048]	Loss: 2.4757
Training Epoch: 5 [20480/50048]	Loss: 2.1653
Training Epoch: 5 [20608/50048]	Loss: 2.1057
Training Epoch: 5 [20736/50048]	Loss: 2.1851
Training Epoch: 5 [20864/50048]	Loss: 2.3625
Training Epoch: 5 [20992/50048]	Loss: 2.1680
Training Epoch: 5 [21120/50048]	Loss: 2.4595
Training Epoch: 5 [21248/50048]	Loss: 2.4283
Training Epoch: 5 [21376/50048]	Loss: 2.2234
Training Epoch: 5 [21504/50048]	Loss: 2.4877
Training Epoch: 5 [21632/50048]	Loss: 2.2120
Training Epoch: 5 [21760/50048]	Loss: 2.5943
Training Epoch: 5 [21888/50048]	Loss: 2.2472
Training Epoch: 5 [22016/50048]	Loss: 2.4983
Training Epoch: 5 [22144/50048]	Loss: 2.2848
Training Epoch: 5 [22272/50048]	Loss: 2.0713
Training Epoch: 5 [22400/50048]	Loss: 2.3144
Training Epoch: 5 [22528/50048]	Loss: 2.1063
Training Epoch: 5 [22656/50048]	Loss: 2.3217
Training Epoch: 5 [22784/50048]	Loss: 2.2440
Training Epoch: 5 [22912/50048]	Loss: 2.2505
Training Epoch: 5 [23040/50048]	Loss: 2.5063
Training Epoch: 5 [23168/50048]	Loss: 2.2538
Training Epoch: 5 [23296/50048]	Loss: 2.6034
Training Epoch: 5 [23424/50048]	Loss: 2.1522
Training Epoch: 5 [23552/50048]	Loss: 2.2267
Training Epoch: 5 [23680/50048]	Loss: 2.4546
Training Epoch: 5 [23808/50048]	Loss: 2.7111
Training Epoch: 5 [23936/50048]	Loss: 2.0870
Training Epoch: 5 [24064/50048]	Loss: 2.3595
Training Epoch: 5 [24192/50048]	Loss: 2.0608
Training Epoch: 5 [24320/50048]	Loss: 2.2687
Training Epoch: 5 [24448/50048]	Loss: 2.0637
Training Epoch: 5 [24576/50048]	Loss: 2.3042
Training Epoch: 5 [24704/50048]	Loss: 2.2164
Training Epoch: 5 [24832/50048]	Loss: 2.3303
Training Epoch: 5 [24960/50048]	Loss: 2.1582
Training Epoch: 5 [25088/50048]	Loss: 2.2328
Training Epoch: 5 [25216/50048]	Loss: 2.1356
Training Epoch: 5 [25344/50048]	Loss: 2.3971
Training Epoch: 5 [25472/50048]	Loss: 2.0960
Training Epoch: 5 [25600/50048]	Loss: 2.2695
Training Epoch: 5 [25728/50048]	Loss: 2.5018
Training Epoch: 5 [25856/50048]	Loss: 2.4561
Training Epoch: 5 [25984/50048]	Loss: 2.3466
Training Epoch: 5 [26112/50048]	Loss: 2.3854
Training Epoch: 5 [26240/50048]	Loss: 2.3530
Training Epoch: 5 [26368/50048]	Loss: 2.3024
Training Epoch: 5 [26496/50048]	Loss: 2.2235
Training Epoch: 5 [26624/50048]	Loss: 2.0610
Training Epoch: 5 [26752/50048]	Loss: 2.2371
Training Epoch: 5 [26880/50048]	Loss: 2.2771
Training Epoch: 5 [27008/50048]	Loss: 2.0038
Training Epoch: 5 [27136/50048]	Loss: 2.2663
Training Epoch: 5 [27264/50048]	Loss: 2.1868
Training Epoch: 5 [27392/50048]	Loss: 2.3555
Training Epoch: 5 [27520/50048]	Loss: 2.2817
Training Epoch: 5 [27648/50048]	Loss: 2.4231
Training Epoch: 5 [27776/50048]	Loss: 2.5777
Training Epoch: 5 [27904/50048]	Loss: 2.4751
Training Epoch: 5 [28032/50048]	Loss: 2.3117
Training Epoch: 5 [28160/50048]	Loss: 2.5560
Training Epoch: 5 [28288/50048]	Loss: 2.1136
Training Epoch: 5 [28416/50048]	Loss: 2.1953
Training Epoch: 5 [28544/50048]	Loss: 2.1522
Training Epoch: 5 [28672/50048]	Loss: 2.1947
Training Epoch: 5 [28800/50048]	Loss: 2.1417
Training Epoch: 5 [28928/50048]	Loss: 2.1586
Training Epoch: 5 [29056/50048]	Loss: 2.3933
Training Epoch: 5 [29184/50048]	Loss: 2.3832
Training Epoch: 5 [29312/50048]	Loss: 2.4236
Training Epoch: 5 [29440/50048]	Loss: 2.2677
Training Epoch: 5 [29568/50048]	Loss: 2.0535
Training Epoch: 5 [29696/50048]	Loss: 2.2137
Training Epoch: 5 [29824/50048]	Loss: 2.4134
Training Epoch: 5 [29952/50048]	Loss: 2.1399
Training Epoch: 5 [30080/50048]	Loss: 2.2789
Training Epoch: 5 [30208/50048]	Loss: 2.2927
Training Epoch: 5 [30336/50048]	Loss: 2.3681
Training Epoch: 5 [30464/50048]	Loss: 2.4017
Training Epoch: 5 [30592/50048]	Loss: 2.4920
Training Epoch: 5 [30720/50048]	Loss: 2.2591
Training Epoch: 5 [30848/50048]	Loss: 2.2539
Training Epoch: 5 [30976/50048]	Loss: 2.3461
Training Epoch: 5 [31104/50048]	Loss: 2.2627
Training Epoch: 5 [31232/50048]	Loss: 2.3324
Training Epoch: 5 [31360/50048]	Loss: 2.1970
Training Epoch: 5 [31488/50048]	Loss: 2.0312
Training Epoch: 5 [31616/50048]	Loss: 1.9493
Training Epoch: 5 [31744/50048]	Loss: 2.4427
Training Epoch: 5 [31872/50048]	Loss: 2.2525
Training Epoch: 5 [32000/50048]	Loss: 2.2541
Training Epoch: 5 [32128/50048]	Loss: 2.3618
Training Epoch: 5 [32256/50048]	Loss: 2.2989
Training Epoch: 5 [32384/50048]	Loss: 2.3520
Training Epoch: 5 [32512/50048]	Loss: 2.1195
Training Epoch: 5 [32640/50048]	Loss: 2.3132
Training Epoch: 5 [32768/50048]	Loss: 2.2438
Training Epoch: 5 [32896/50048]	Loss: 2.1595
Training Epoch: 5 [33024/50048]	Loss: 2.4041
Training Epoch: 5 [33152/50048]	Loss: 2.2364
Training Epoch: 5 [33280/50048]	Loss: 2.4887
Training Epoch: 5 [33408/50048]	Loss: 2.2915
Training Epoch: 5 [33536/50048]	Loss: 2.2364
Training Epoch: 5 [33664/50048]	Loss: 2.4680
Training Epoch: 5 [33792/50048]	Loss: 2.4697
Training Epoch: 5 [33920/50048]	Loss: 2.3772
Training Epoch: 5 [34048/50048]	Loss: 2.4140
Training Epoch: 5 [34176/50048]	Loss: 2.1442
Training Epoch: 5 [34304/50048]	Loss: 2.2059
Training Epoch: 5 [34432/50048]	Loss: 2.4477
Training Epoch: 5 [34560/50048]	Loss: 2.0650
Training Epoch: 5 [34688/50048]	Loss: 2.2141
Training Epoch: 5 [34816/50048]	Loss: 2.1655
Training Epoch: 5 [34944/50048]	Loss: 2.3230
Training Epoch: 5 [35072/50048]	Loss: 2.1042
Training Epoch: 5 [35200/50048]	Loss: 2.2963
Training Epoch: 5 [35328/50048]	Loss: 2.4993
Training Epoch: 5 [35456/50048]	Loss: 2.4011
Training Epoch: 5 [35584/50048]	Loss: 2.2603
Training Epoch: 5 [35712/50048]	Loss: 2.5992
Training Epoch: 5 [35840/50048]	Loss: 2.3354
Training Epoch: 5 [35968/50048]	Loss: 2.1446
Training Epoch: 5 [36096/50048]	Loss: 2.0484
Training Epoch: 5 [36224/50048]	Loss: 2.3832
Training Epoch: 5 [36352/50048]	Loss: 2.3120
Training Epoch: 5 [36480/50048]	Loss: 2.2886
Training Epoch: 5 [36608/50048]	Loss: 1.9701
Training Epoch: 5 [36736/50048]	Loss: 2.1119
Training Epoch: 5 [36864/50048]	Loss: 2.2427
Training Epoch: 5 [36992/50048]	Loss: 2.1521
Training Epoch: 5 [37120/50048]	Loss: 2.1836
Training Epoch: 5 [37248/50048]	Loss: 2.0077
Training Epoch: 5 [37376/50048]	Loss: 2.0886
Training Epoch: 5 [37504/50048]	Loss: 2.4083
Training Epoch: 5 [37632/50048]	Loss: 2.4733
Training Epoch: 5 [37760/50048]	Loss: 2.1678
Training Epoch: 5 [37888/50048]	Loss: 2.1722
Training Epoch: 5 [38016/50048]	Loss: 2.3322
Training Epoch: 5 [38144/50048]	Loss: 2.1466
Training Epoch: 5 [38272/50048]	Loss: 2.2268
Training Epoch: 5 [38400/50048]	Loss: 2.3239
Training Epoch: 5 [38528/50048]	Loss: 2.0711
Training Epoch: 5 [38656/50048]	Loss: 2.1018
Training Epoch: 5 [38784/50048]	Loss: 2.2146
Training Epoch: 5 [38912/50048]	Loss: 2.0485
Training Epoch: 5 [39040/50048]	Loss: 2.0263
Training Epoch: 5 [39168/50048]	Loss: 2.0240
Training Epoch: 5 [39296/50048]	Loss: 2.2557
Training Epoch: 5 [39424/50048]	Loss: 2.1366
Training Epoch: 5 [39552/50048]	Loss: 2.3609
Training Epoch: 5 [39680/50048]	Loss: 1.9190
Training Epoch: 5 [39808/50048]	Loss: 2.2674
Training Epoch: 5 [39936/50048]	Loss: 2.1659
Training Epoch: 5 [40064/50048]	Loss: 2.2810
Training Epoch: 5 [40192/50048]	Loss: 2.4796
Training Epoch: 5 [40320/50048]	Loss: 2.1602
Training Epoch: 5 [40448/50048]	Loss: 2.2990
Training Epoch: 5 [40576/50048]	Loss: 2.2186
Training Epoch: 5 [40704/50048]	Loss: 2.3545
Training Epoch: 5 [40832/50048]	Loss: 2.3971
Training Epoch: 5 [40960/50048]	Loss: 2.1799
Training Epoch: 5 [41088/50048]	Loss: 2.1848
Training Epoch: 5 [41216/50048]	Loss: 2.1627
Training Epoch: 5 [41344/50048]	Loss: 2.2635
Training Epoch: 5 [41472/50048]	Loss: 2.0145
Training Epoch: 5 [41600/50048]	Loss: 2.1183
Training Epoch: 5 [41728/50048]	Loss: 2.2473
Training Epoch: 5 [41856/50048]	Loss: 2.0006
Training Epoch: 5 [41984/50048]	Loss: 2.0757
Training Epoch: 5 [42112/50048]	Loss: 2.3095
Training Epoch: 5 [42240/50048]	Loss: 2.3604
Training Epoch: 5 [42368/50048]	Loss: 2.2700
Training Epoch: 5 [42496/50048]	Loss: 2.1429
Training Epoch: 5 [42624/50048]	Loss: 2.0606
Training Epoch: 5 [42752/50048]	Loss: 2.0047
Training Epoch: 5 [42880/50048]	Loss: 2.0638
Training Epoch: 5 [43008/50048]	Loss: 2.3767
Training Epoch: 5 [43136/50048]	Loss: 2.1273
Training Epoch: 5 [43264/50048]	Loss: 2.1665
Training Epoch: 5 [43392/50048]	Loss: 2.3643
Training Epoch: 5 [43520/50048]	Loss: 2.0278
Training Epoch: 5 [43648/50048]	Loss: 2.1907
Training Epoch: 5 [43776/50048]	Loss: 2.3972
Training Epoch: 5 [43904/50048]	Loss: 2.1913
Training Epoch: 5 [44032/50048]	Loss: 2.2901
Training Epoch: 5 [44160/50048]	Loss: 2.2423
Training Epoch: 5 [44288/50048]	Loss: 2.1681
Training Epoch: 5 [44416/50048]	Loss: 2.1888
Training Epoch: 5 [44544/50048]	Loss: 2.3240
Training Epoch: 5 [44672/50048]	Loss: 2.1185
Training Epoch: 5 [44800/50048]	Loss: 2.2814
Training Epoch: 5 [44928/50048]	Loss: 2.3618
Training Epoch: 5 [45056/50048]	Loss: 2.3239
Training Epoch: 5 [45184/50048]	Loss: 2.4637
Training Epoch: 5 [45312/50048]	Loss: 2.1142
Training Epoch: 5 [45440/50048]	Loss: 2.3015
Training Epoch: 5 [45568/50048]	Loss: 2.0846
Training Epoch: 5 [45696/50048]	Loss: 2.3313
Training Epoch: 5 [45824/50048]	Loss: 2.0549
Training Epoch: 5 [45952/50048]	Loss: 1.8524
Training Epoch: 5 [46080/50048]	Loss: 2.2016
Training Epoch: 5 [46208/50048]	Loss: 2.3952
Training Epoch: 5 [46336/50048]	Loss: 2.3686
Training Epoch: 5 [46464/50048]	Loss: 2.2065
Training Epoch: 5 [46592/50048]	Loss: 2.0165
Training Epoch: 5 [46720/50048]	Loss: 2.1322
Training Epoch: 5 [46848/50048]	Loss: 2.1666
Training Epoch: 5 [46976/50048]	Loss: 2.3298
Training Epoch: 5 [47104/50048]	Loss: 2.2481
Training Epoch: 5 [47232/50048]	Loss: 2.2276
Training Epoch: 5 [47360/50048]	Loss: 2.2394
Training Epoch: 5 [47488/50048]	Loss: 2.2985
Training Epoch: 5 [47616/50048]	Loss: 2.5766
Training Epoch: 5 [47744/50048]	Loss: 2.1453
Training Epoch: 5 [47872/50048]	Loss: 2.1183
Training Epoch: 5 [48000/50048]	Loss: 2.6410
Training Epoch: 5 [48128/50048]	Loss: 2.3053
Training Epoch: 5 [48256/50048]	Loss: 2.3803
Training Epoch: 5 [48384/50048]	Loss: 2.3976
Training Epoch: 5 [48512/50048]	Loss: 2.1495
Training Epoch: 5 [48640/50048]	Loss: 2.1619
Training Epoch: 5 [48768/50048]	Loss: 2.0233
Training Epoch: 5 [48896/50048]	Loss: 2.2951
Training Epoch: 5 [49024/50048]	Loss: 2.2344
Training Epoch: 5 [49152/50048]	Loss: 2.1041
Training Epoch: 5 [49280/50048]	Loss: 2.2715
Training Epoch: 5 [49408/50048]	Loss: 2.0868
Training Epoch: 5 [49536/50048]	Loss: 2.3502
Training Epoch: 5 [49664/50048]	Loss: 2.2526
Training Epoch: 5 [49792/50048]	Loss: 2.1831
Training Epoch: 5 [49920/50048]	Loss: 2.0948
Training Epoch: 5 [50048/50048]	Loss: 2.3144
Validation Epoch: 5, Average loss: 0.0506, Accuracy: 0.0728
Training Epoch: 6 [128/50048]	Loss: 2.0606
Training Epoch: 6 [256/50048]	Loss: 1.9974
Training Epoch: 6 [384/50048]	Loss: 2.1590
Training Epoch: 6 [512/50048]	Loss: 2.2064
Training Epoch: 6 [640/50048]	Loss: 1.9956
Training Epoch: 6 [768/50048]	Loss: 2.1713
Training Epoch: 6 [896/50048]	Loss: 2.0283
Training Epoch: 6 [1024/50048]	Loss: 1.9329
Training Epoch: 6 [1152/50048]	Loss: 2.1962
Training Epoch: 6 [1280/50048]	Loss: 2.1959
Training Epoch: 6 [1408/50048]	Loss: 1.9882
Training Epoch: 6 [1536/50048]	Loss: 2.1963
Training Epoch: 6 [1664/50048]	Loss: 2.1962
Training Epoch: 6 [1792/50048]	Loss: 2.5322
Training Epoch: 6 [1920/50048]	Loss: 2.0456
Training Epoch: 6 [2048/50048]	Loss: 2.1758
Training Epoch: 6 [2176/50048]	Loss: 2.0691
Training Epoch: 6 [2304/50048]	Loss: 2.1510
Training Epoch: 6 [2432/50048]	Loss: 1.8553
Training Epoch: 6 [2560/50048]	Loss: 2.2366
Training Epoch: 6 [2688/50048]	Loss: 1.9184
Training Epoch: 6 [2816/50048]	Loss: 2.3119
Training Epoch: 6 [2944/50048]	Loss: 2.2655
Training Epoch: 6 [3072/50048]	Loss: 2.2343
Training Epoch: 6 [3200/50048]	Loss: 2.0875
Training Epoch: 6 [3328/50048]	Loss: 2.3533
Training Epoch: 6 [3456/50048]	Loss: 1.9513
Training Epoch: 6 [3584/50048]	Loss: 2.1878
Training Epoch: 6 [3712/50048]	Loss: 2.1017
Training Epoch: 6 [3840/50048]	Loss: 2.1929
Training Epoch: 6 [3968/50048]	Loss: 1.9172
Training Epoch: 6 [4096/50048]	Loss: 2.0024
Training Epoch: 6 [4224/50048]	Loss: 2.2379
Training Epoch: 6 [4352/50048]	Loss: 2.0607
Training Epoch: 6 [4480/50048]	Loss: 2.1983
Training Epoch: 6 [4608/50048]	Loss: 2.0721
Training Epoch: 6 [4736/50048]	Loss: 2.2892
Training Epoch: 6 [4864/50048]	Loss: 2.1759
Training Epoch: 6 [4992/50048]	Loss: 2.2507
Training Epoch: 6 [5120/50048]	Loss: 2.0528
Training Epoch: 6 [5248/50048]	Loss: 2.2106
Training Epoch: 6 [5376/50048]	Loss: 2.2139
Training Epoch: 6 [5504/50048]	Loss: 2.1347
Training Epoch: 6 [5632/50048]	Loss: 2.1730
Training Epoch: 6 [5760/50048]	Loss: 2.2112
Training Epoch: 6 [5888/50048]	Loss: 2.1831
Training Epoch: 6 [6016/50048]	Loss: 2.2368
Training Epoch: 6 [6144/50048]	Loss: 2.1507
Training Epoch: 6 [6272/50048]	Loss: 2.4641
Training Epoch: 6 [6400/50048]	Loss: 2.1516
Training Epoch: 6 [6528/50048]	Loss: 2.1198
Training Epoch: 6 [6656/50048]	Loss: 2.1386
Training Epoch: 6 [6784/50048]	Loss: 2.0281
Training Epoch: 6 [6912/50048]	Loss: 2.0999
Training Epoch: 6 [7040/50048]	Loss: 2.0991
Training Epoch: 6 [7168/50048]	Loss: 1.9268
Training Epoch: 6 [7296/50048]	Loss: 2.1496
Training Epoch: 6 [7424/50048]	Loss: 1.8546
Training Epoch: 6 [7552/50048]	Loss: 2.1807
Training Epoch: 6 [7680/50048]	Loss: 2.0791
Training Epoch: 6 [7808/50048]	Loss: 1.9664
Training Epoch: 6 [7936/50048]	Loss: 2.1509
Training Epoch: 6 [8064/50048]	Loss: 2.0532
Training Epoch: 6 [8192/50048]	Loss: 2.0809
Training Epoch: 6 [8320/50048]	Loss: 2.0463
Training Epoch: 6 [8448/50048]	Loss: 2.1663
Training Epoch: 6 [8576/50048]	Loss: 1.7756
Training Epoch: 6 [8704/50048]	Loss: 2.1024
Training Epoch: 6 [8832/50048]	Loss: 2.1897
Training Epoch: 6 [8960/50048]	Loss: 2.2803
Training Epoch: 6 [9088/50048]	Loss: 2.0998
Training Epoch: 6 [9216/50048]	Loss: 2.0723
Training Epoch: 6 [9344/50048]	Loss: 2.1492
Training Epoch: 6 [9472/50048]	Loss: 2.0876
Training Epoch: 6 [9600/50048]	Loss: 1.9289
Training Epoch: 6 [9728/50048]	Loss: 1.9499
Training Epoch: 6 [9856/50048]	Loss: 2.2795
Training Epoch: 6 [9984/50048]	Loss: 2.1878
Training Epoch: 6 [10112/50048]	Loss: 2.0231
Training Epoch: 6 [10240/50048]	Loss: 2.2823
Training Epoch: 6 [10368/50048]	Loss: 1.9436
Training Epoch: 6 [10496/50048]	Loss: 2.3638
Training Epoch: 6 [10624/50048]	Loss: 2.1731
Training Epoch: 6 [10752/50048]	Loss: 1.9519
Training Epoch: 6 [10880/50048]	Loss: 2.0295
Training Epoch: 6 [11008/50048]	Loss: 1.9480
Training Epoch: 6 [11136/50048]	Loss: 2.2390
Training Epoch: 6 [11264/50048]	Loss: 2.3768
Training Epoch: 6 [11392/50048]	Loss: 2.3723
Training Epoch: 6 [11520/50048]	Loss: 2.1486
Training Epoch: 6 [11648/50048]	Loss: 2.2336
Training Epoch: 6 [11776/50048]	Loss: 1.9821
Training Epoch: 6 [11904/50048]	Loss: 1.9829
Training Epoch: 6 [12032/50048]	Loss: 2.0659
Training Epoch: 6 [12160/50048]	Loss: 2.1986
Training Epoch: 6 [12288/50048]	Loss: 2.2906
Training Epoch: 6 [12416/50048]	Loss: 2.1842
Training Epoch: 6 [12544/50048]	Loss: 2.2407
Training Epoch: 6 [12672/50048]	Loss: 2.1129
Training Epoch: 6 [12800/50048]	Loss: 2.2136
Training Epoch: 6 [12928/50048]	Loss: 1.9298
Training Epoch: 6 [13056/50048]	Loss: 2.1897
Training Epoch: 6 [13184/50048]	Loss: 2.1996
Training Epoch: 6 [13312/50048]	Loss: 2.1936
Training Epoch: 6 [13440/50048]	Loss: 2.4160
Training Epoch: 6 [13568/50048]	Loss: 2.1947
Training Epoch: 6 [13696/50048]	Loss: 1.9928
Training Epoch: 6 [13824/50048]	Loss: 2.2935
Training Epoch: 6 [13952/50048]	Loss: 2.2525
Training Epoch: 6 [14080/50048]	Loss: 2.2338
Training Epoch: 6 [14208/50048]	Loss: 2.1064
Training Epoch: 6 [14336/50048]	Loss: 1.9267
Training Epoch: 6 [14464/50048]	Loss: 2.1210
Training Epoch: 6 [14592/50048]	Loss: 1.8814
Training Epoch: 6 [14720/50048]	Loss: 2.1561
Training Epoch: 6 [14848/50048]	Loss: 2.0705
Training Epoch: 6 [14976/50048]	Loss: 2.3527
Training Epoch: 6 [15104/50048]	Loss: 2.3487
Training Epoch: 6 [15232/50048]	Loss: 1.9269
Training Epoch: 6 [15360/50048]	Loss: 2.0025
Training Epoch: 6 [15488/50048]	Loss: 2.2944
Training Epoch: 6 [15616/50048]	Loss: 2.1547
Training Epoch: 6 [15744/50048]	Loss: 1.9903
Training Epoch: 6 [15872/50048]	Loss: 1.9453
Training Epoch: 6 [16000/50048]	Loss: 2.1546
Training Epoch: 6 [16128/50048]	Loss: 1.8123
Training Epoch: 6 [16256/50048]	Loss: 2.3721
Training Epoch: 6 [16384/50048]	Loss: 2.4420
Training Epoch: 6 [16512/50048]	Loss: 1.8927
Training Epoch: 6 [16640/50048]	Loss: 2.0327
Training Epoch: 6 [16768/50048]	Loss: 1.8941
Training Epoch: 6 [16896/50048]	Loss: 2.0579
Training Epoch: 6 [17024/50048]	Loss: 1.9674
Training Epoch: 6 [17152/50048]	Loss: 2.0710
Training Epoch: 6 [17280/50048]	Loss: 1.9723
Training Epoch: 6 [17408/50048]	Loss: 2.1975
Training Epoch: 6 [17536/50048]	Loss: 2.1276
Training Epoch: 6 [17664/50048]	Loss: 2.0124
Training Epoch: 6 [17792/50048]	Loss: 2.2089
Training Epoch: 6 [17920/50048]	Loss: 2.0332
Training Epoch: 6 [18048/50048]	Loss: 2.1103
Training Epoch: 6 [18176/50048]	Loss: 2.1722
Training Epoch: 6 [18304/50048]	Loss: 2.0583
Training Epoch: 6 [18432/50048]	Loss: 2.2134
Training Epoch: 6 [18560/50048]	Loss: 2.2327
Training Epoch: 6 [18688/50048]	Loss: 2.1628
Training Epoch: 6 [18816/50048]	Loss: 2.3195
Training Epoch: 6 [18944/50048]	Loss: 2.1880
Training Epoch: 6 [19072/50048]	Loss: 2.1046
Training Epoch: 6 [19200/50048]	Loss: 2.2776
Training Epoch: 6 [19328/50048]	Loss: 2.1540
Training Epoch: 6 [19456/50048]	Loss: 2.3409
Training Epoch: 6 [19584/50048]	Loss: 2.0663
Training Epoch: 6 [19712/50048]	Loss: 2.3096
Training Epoch: 6 [19840/50048]	Loss: 2.0847
Training Epoch: 6 [19968/50048]	Loss: 2.3266
Training Epoch: 6 [20096/50048]	Loss: 2.2675
Training Epoch: 6 [20224/50048]	Loss: 2.2343
Training Epoch: 6 [20352/50048]	Loss: 2.2249
Training Epoch: 6 [20480/50048]	Loss: 1.9593
Training Epoch: 6 [20608/50048]	Loss: 1.8863
Training Epoch: 6 [20736/50048]	Loss: 2.0782
Training Epoch: 6 [20864/50048]	Loss: 1.8590
Training Epoch: 6 [20992/50048]	Loss: 2.1015
Training Epoch: 6 [21120/50048]	Loss: 2.2940
Training Epoch: 6 [21248/50048]	Loss: 2.2528
Training Epoch: 6 [21376/50048]	Loss: 2.1569
Training Epoch: 6 [21504/50048]	Loss: 1.9432
Training Epoch: 6 [21632/50048]	Loss: 2.3101
Training Epoch: 6 [21760/50048]	Loss: 2.1091
Training Epoch: 6 [21888/50048]	Loss: 2.0888
Training Epoch: 6 [22016/50048]	Loss: 2.2221
Training Epoch: 6 [22144/50048]	Loss: 2.2310
Training Epoch: 6 [22272/50048]	Loss: 2.0650
Training Epoch: 6 [22400/50048]	Loss: 2.1122
Training Epoch: 6 [22528/50048]	Loss: 2.1908
Training Epoch: 6 [22656/50048]	Loss: 2.2831
Training Epoch: 6 [22784/50048]	Loss: 2.0440
Training Epoch: 6 [22912/50048]	Loss: 2.1779
Training Epoch: 6 [23040/50048]	Loss: 1.9851
Training Epoch: 6 [23168/50048]	Loss: 2.0238
Training Epoch: 6 [23296/50048]	Loss: 1.8338
Training Epoch: 6 [23424/50048]	Loss: 2.2034
Training Epoch: 6 [23552/50048]	Loss: 2.0057
Training Epoch: 6 [23680/50048]	Loss: 2.1171
Training Epoch: 6 [23808/50048]	Loss: 2.2928
Training Epoch: 6 [23936/50048]	Loss: 1.9091
Training Epoch: 6 [24064/50048]	Loss: 2.0416
Training Epoch: 6 [24192/50048]	Loss: 1.9967
Training Epoch: 6 [24320/50048]	Loss: 2.1294
Training Epoch: 6 [24448/50048]	Loss: 2.3339
Training Epoch: 6 [24576/50048]	Loss: 2.4977
Training Epoch: 6 [24704/50048]	Loss: 2.1387
Training Epoch: 6 [24832/50048]	Loss: 2.1939
Training Epoch: 6 [24960/50048]	Loss: 2.0555
Training Epoch: 6 [25088/50048]	Loss: 2.0738
Training Epoch: 6 [25216/50048]	Loss: 1.9786
Training Epoch: 6 [25344/50048]	Loss: 1.9677
Training Epoch: 6 [25472/50048]	Loss: 2.3186
Training Epoch: 6 [25600/50048]	Loss: 2.0289
Training Epoch: 6 [25728/50048]	Loss: 1.8754
Training Epoch: 6 [25856/50048]	Loss: 2.3205
Training Epoch: 6 [25984/50048]	Loss: 2.1117
Training Epoch: 6 [26112/50048]	Loss: 2.0435
Training Epoch: 6 [26240/50048]	Loss: 2.1830
Training Epoch: 6 [26368/50048]	Loss: 2.2032
Training Epoch: 6 [26496/50048]	Loss: 1.9180
Training Epoch: 6 [26624/50048]	Loss: 2.3729
Training Epoch: 6 [26752/50048]	Loss: 2.1676
Training Epoch: 6 [26880/50048]	Loss: 2.2013
Training Epoch: 6 [27008/50048]	Loss: 1.9101
Training Epoch: 6 [27136/50048]	Loss: 2.1982
Training Epoch: 6 [27264/50048]	Loss: 1.9241
Training Epoch: 6 [27392/50048]	Loss: 2.0143
Training Epoch: 6 [27520/50048]	Loss: 2.2132
Training Epoch: 6 [27648/50048]	Loss: 2.0103
Training Epoch: 6 [27776/50048]	Loss: 2.0611
Training Epoch: 6 [27904/50048]	Loss: 2.4686
Training Epoch: 6 [28032/50048]	Loss: 1.8976
Training Epoch: 6 [28160/50048]	Loss: 1.9016
Training Epoch: 6 [28288/50048]	Loss: 2.4042
Training Epoch: 6 [28416/50048]	Loss: 1.9701
Training Epoch: 6 [28544/50048]	Loss: 1.9182
Training Epoch: 6 [28672/50048]	Loss: 2.1143
Training Epoch: 6 [28800/50048]	Loss: 2.0913
Training Epoch: 6 [28928/50048]	Loss: 2.1778
Training Epoch: 6 [29056/50048]	Loss: 1.9285
Training Epoch: 6 [29184/50048]	Loss: 2.2123
Training Epoch: 6 [29312/50048]	Loss: 2.2726
Training Epoch: 6 [29440/50048]	Loss: 2.1507
Training Epoch: 6 [29568/50048]	Loss: 2.4234
Training Epoch: 6 [29696/50048]	Loss: 2.2774
Training Epoch: 6 [29824/50048]	Loss: 1.9445
Training Epoch: 6 [29952/50048]	Loss: 2.4023
Training Epoch: 6 [30080/50048]	Loss: 2.3082
Training Epoch: 6 [30208/50048]	Loss: 1.8695
Training Epoch: 6 [30336/50048]	Loss: 2.1625
Training Epoch: 6 [30464/50048]	Loss: 2.1668
Training Epoch: 6 [30592/50048]	Loss: 2.0760
Training Epoch: 6 [30720/50048]	Loss: 2.3608
Training Epoch: 6 [30848/50048]	Loss: 1.8742
Training Epoch: 6 [30976/50048]	Loss: 2.0832
Training Epoch: 6 [31104/50048]	Loss: 2.0920
Training Epoch: 6 [31232/50048]	Loss: 2.2428
Training Epoch: 6 [31360/50048]	Loss: 2.0080
Training Epoch: 6 [31488/50048]	Loss: 2.1189
Training Epoch: 6 [31616/50048]	Loss: 2.1830
Training Epoch: 6 [31744/50048]	Loss: 1.9024
Training Epoch: 6 [31872/50048]	Loss: 2.2469
Training Epoch: 6 [32000/50048]	Loss: 2.2203
Training Epoch: 6 [32128/50048]	Loss: 2.0054
Training Epoch: 6 [32256/50048]	Loss: 1.9275
Training Epoch: 6 [32384/50048]	Loss: 2.1059
Training Epoch: 6 [32512/50048]	Loss: 2.3665
Training Epoch: 6 [32640/50048]	Loss: 1.9516
Training Epoch: 6 [32768/50048]	Loss: 2.0395
Training Epoch: 6 [32896/50048]	Loss: 2.0735
Training Epoch: 6 [33024/50048]	Loss: 2.2041
Training Epoch: 6 [33152/50048]	Loss: 2.2690
Training Epoch: 6 [33280/50048]	Loss: 2.1199
Training Epoch: 6 [33408/50048]	Loss: 2.1791
Training Epoch: 6 [33536/50048]	Loss: 2.1599
Training Epoch: 6 [33664/50048]	Loss: 2.0604
Training Epoch: 6 [33792/50048]	Loss: 1.8588
Training Epoch: 6 [33920/50048]	Loss: 2.1092
Training Epoch: 6 [34048/50048]	Loss: 2.1600
Training Epoch: 6 [34176/50048]	Loss: 2.6131
Training Epoch: 6 [34304/50048]	Loss: 2.0386
Training Epoch: 6 [34432/50048]	Loss: 2.1189
Training Epoch: 6 [34560/50048]	Loss: 2.1668
Training Epoch: 6 [34688/50048]	Loss: 2.3381
Training Epoch: 6 [34816/50048]	Loss: 1.9157
Training Epoch: 6 [34944/50048]	Loss: 1.9688
Training Epoch: 6 [35072/50048]	Loss: 2.1083
Training Epoch: 6 [35200/50048]	Loss: 2.5025
Training Epoch: 6 [35328/50048]	Loss: 2.1308
Training Epoch: 6 [35456/50048]	Loss: 2.1335
Training Epoch: 6 [35584/50048]	Loss: 2.1411
Training Epoch: 6 [35712/50048]	Loss: 2.1394
Training Epoch: 6 [35840/50048]	Loss: 1.8845
Training Epoch: 6 [35968/50048]	Loss: 1.9836
Training Epoch: 6 [36096/50048]	Loss: 2.4344
Training Epoch: 6 [36224/50048]	Loss: 2.0286
Training Epoch: 6 [36352/50048]	Loss: 1.9980
Training Epoch: 6 [36480/50048]	Loss: 2.0756
Training Epoch: 6 [36608/50048]	Loss: 2.1943
Training Epoch: 6 [36736/50048]	Loss: 2.4018
Training Epoch: 6 [36864/50048]	Loss: 2.0120
Training Epoch: 6 [36992/50048]	Loss: 2.0128
Training Epoch: 6 [37120/50048]	Loss: 2.4176
Training Epoch: 6 [37248/50048]	Loss: 2.0602
Training Epoch: 6 [37376/50048]	Loss: 1.7632
Training Epoch: 6 [37504/50048]	Loss: 1.8758
Training Epoch: 6 [37632/50048]	Loss: 2.2310
Training Epoch: 6 [37760/50048]	Loss: 1.9902
Training Epoch: 6 [37888/50048]	Loss: 2.1681
Training Epoch: 6 [38016/50048]	Loss: 2.4691
Training Epoch: 6 [38144/50048]	Loss: 1.9512
Training Epoch: 6 [38272/50048]	Loss: 2.2496
Training Epoch: 6 [38400/50048]	Loss: 2.3322
Training Epoch: 6 [38528/50048]	Loss: 2.2037
Training Epoch: 6 [38656/50048]	Loss: 1.8497
Training Epoch: 6 [38784/50048]	Loss: 1.9916
Training Epoch: 6 [38912/50048]	Loss: 2.3939
Training Epoch: 6 [39040/50048]	Loss: 2.0972
Training Epoch: 6 [39168/50048]	Loss: 2.3088
Training Epoch: 6 [39296/50048]	Loss: 2.1447
Training Epoch: 6 [39424/50048]	Loss: 2.2369
Training Epoch: 6 [39552/50048]	Loss: 2.3110
Training Epoch: 6 [39680/50048]	Loss: 2.2545
Training Epoch: 6 [39808/50048]	Loss: 1.9536
Training Epoch: 6 [39936/50048]	Loss: 2.4182
Training Epoch: 6 [40064/50048]	Loss: 2.2322
Training Epoch: 6 [40192/50048]	Loss: 2.0648
Training Epoch: 6 [40320/50048]	Loss: 1.9863
Training Epoch: 6 [40448/50048]	Loss: 2.1493
Training Epoch: 6 [40576/50048]	Loss: 2.0096
Training Epoch: 6 [40704/50048]	Loss: 2.0292
Training Epoch: 6 [40832/50048]	Loss: 2.0851
Training Epoch: 6 [40960/50048]	Loss: 1.9182
Training Epoch: 6 [41088/50048]	Loss: 1.9933
Training Epoch: 6 [41216/50048]	Loss: 2.0028
Training Epoch: 6 [41344/50048]	Loss: 2.1918
Training Epoch: 6 [41472/50048]	Loss: 1.8450
Training Epoch: 6 [41600/50048]	Loss: 1.6925
Training Epoch: 6 [41728/50048]	Loss: 2.0103
Training Epoch: 6 [41856/50048]	Loss: 1.7683
Training Epoch: 6 [41984/50048]	Loss: 2.3017
Training Epoch: 6 [42112/50048]	Loss: 2.0452
Training Epoch: 6 [42240/50048]	Loss: 2.0350
Training Epoch: 6 [42368/50048]	Loss: 2.2115
Training Epoch: 6 [42496/50048]	Loss: 2.0265
Training Epoch: 6 [42624/50048]	Loss: 2.1138
Training Epoch: 6 [42752/50048]	Loss: 1.9329
Training Epoch: 6 [42880/50048]	Loss: 2.0399
Training Epoch: 6 [43008/50048]	Loss: 2.1955
Training Epoch: 6 [43136/50048]	Loss: 1.8303
Training Epoch: 6 [43264/50048]	Loss: 1.8607
Training Epoch: 6 [43392/50048]	Loss: 2.1682
Training Epoch: 6 [43520/50048]	Loss: 1.8682
Training Epoch: 6 [43648/50048]	Loss: 2.2593
Training Epoch: 6 [43776/50048]	Loss: 1.9538
Training Epoch: 6 [43904/50048]	Loss: 1.9799
Training Epoch: 6 [44032/50048]	Loss: 2.1373
Training Epoch: 6 [44160/50048]	Loss: 2.1722
Training Epoch: 6 [44288/50048]	Loss: 2.1091
Training Epoch: 6 [44416/50048]	Loss: 2.0515
Training Epoch: 6 [44544/50048]	Loss: 2.0861
Training Epoch: 6 [44672/50048]	Loss: 2.0025
Training Epoch: 6 [44800/50048]	Loss: 2.0232
Training Epoch: 6 [44928/50048]	Loss: 2.3966
Training Epoch: 6 [45056/50048]	Loss: 1.9341
Training Epoch: 6 [45184/50048]	Loss: 2.0335
Training Epoch: 6 [45312/50048]	Loss: 2.1666
Training Epoch: 6 [45440/50048]	Loss: 2.2889
Training Epoch: 6 [45568/50048]	Loss: 2.0703
Training Epoch: 6 [45696/50048]	Loss: 1.9987
Training Epoch: 6 [45824/50048]	Loss: 2.0700
Training Epoch: 6 [45952/50048]	Loss: 1.9871
Training Epoch: 6 [46080/50048]	Loss: 1.9868
Training Epoch: 6 [46208/50048]	Loss: 2.0601
Training Epoch: 6 [46336/50048]	Loss: 1.9438
Training Epoch: 6 [46464/50048]	Loss: 2.0839
Training Epoch: 6 [46592/50048]	Loss: 2.0475
Training Epoch: 6 [46720/50048]	Loss: 2.3393
Training Epoch: 6 [46848/50048]	Loss: 2.1771
Training Epoch: 6 [46976/50048]	Loss: 1.9012
Training Epoch: 6 [47104/50048]	Loss: 1.7156
Training Epoch: 6 [47232/50048]	Loss: 2.0711
Training Epoch: 6 [47360/50048]	Loss: 2.0827
Training Epoch: 6 [47488/50048]	Loss: 2.1505
Training Epoch: 6 [47616/50048]	Loss: 2.1707
Training Epoch: 6 [47744/50048]	Loss: 2.0108
Training Epoch: 6 [47872/50048]	Loss: 2.0294
Training Epoch: 6 [48000/50048]	Loss: 1.9839
Training Epoch: 6 [48128/50048]	Loss: 1.9481
Training Epoch: 6 [48256/50048]	Loss: 1.9193
Training Epoch: 6 [48384/50048]	Loss: 2.1870
Training Epoch: 6 [48512/50048]	Loss: 1.9956
Training Epoch: 6 [48640/50048]	Loss: 2.1003
Training Epoch: 6 [48768/50048]	Loss: 1.8054
Training Epoch: 6 [48896/50048]	Loss: 1.9198
Training Epoch: 6 [49024/50048]	Loss: 2.0824
Training Epoch: 6 [49152/50048]	Loss: 1.8948
Training Epoch: 6 [49280/50048]	Loss: 2.0902
Training Epoch: 6 [49408/50048]	Loss: 2.1127
Training Epoch: 6 [49536/50048]	Loss: 2.0460
Training Epoch: 6 [49664/50048]	Loss: 1.9681
Training Epoch: 6 [49792/50048]	Loss: 2.2256
Training Epoch: 6 [49920/50048]	Loss: 2.0151
Training Epoch: 6 [50048/50048]	Loss: 2.2399
Validation Epoch: 6, Average loss: 0.0178, Accuracy: 0.4115
[Training Loop] Model's accuracy 0.41149129746835444 surpasses threshold 0.3! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0278
Profiling... [256/50048]	Loss: 2.0173
Profiling... [384/50048]	Loss: 1.9487
Profiling... [512/50048]	Loss: 1.7916
Profiling... [640/50048]	Loss: 2.0140
Profiling... [768/50048]	Loss: 1.9616
Profiling... [896/50048]	Loss: 1.8734
Profiling... [1024/50048]	Loss: 1.9866
Profiling... [1152/50048]	Loss: 1.8298
Profiling... [1280/50048]	Loss: 1.8430
Profiling... [1408/50048]	Loss: 2.1171
Profiling... [1536/50048]	Loss: 2.0040
Profiling... [1664/50048]	Loss: 1.9144
Profile done
epoch 1 train time consumed: 3.43s
Validation Epoch: 7, Average loss: 0.0152, Accuracy: 0.4719
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.46966109542538,
                        "time": 2.1991652719998456,
                        "accuracy": 0.47191455696202533,
                        "total_cost": 815516.1075714432
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1311
Profiling... [256/50048]	Loss: 1.8156
Profiling... [384/50048]	Loss: 1.5892
Profiling... [512/50048]	Loss: 1.8202
Profiling... [640/50048]	Loss: 1.8057
Profiling... [768/50048]	Loss: 1.9249
Profiling... [896/50048]	Loss: 2.1289
Profiling... [1024/50048]	Loss: 2.0043
Profiling... [1152/50048]	Loss: 1.8837
Profiling... [1280/50048]	Loss: 2.1158
Profiling... [1408/50048]	Loss: 1.8291
Profiling... [1536/50048]	Loss: 2.1763
Profiling... [1664/50048]	Loss: 1.7799
Profile done
epoch 1 train time consumed: 3.34s
Validation Epoch: 7, Average loss: 0.0149, Accuracy: 0.4773
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.52077756054577,
                        "time": 2.190295803000481,
                        "accuracy": 0.47725474683544306,
                        "total_cost": 803138.7179837653
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9093
Profiling... [256/50048]	Loss: 2.1076
Profiling... [384/50048]	Loss: 1.8386
Profiling... [512/50048]	Loss: 1.8822
Profiling... [640/50048]	Loss: 2.0244
Profiling... [768/50048]	Loss: 1.9278
Profiling... [896/50048]	Loss: 2.0194
Profiling... [1024/50048]	Loss: 1.7121
Profiling... [1152/50048]	Loss: 2.0947
Profiling... [1280/50048]	Loss: 1.6731
Profiling... [1408/50048]	Loss: 1.9403
Profiling... [1536/50048]	Loss: 2.0448
Profiling... [1664/50048]	Loss: 1.7593
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 7, Average loss: 0.0152, Accuracy: 0.4731
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.5510867968438,
                        "time": 2.4554719339994335,
                        "accuracy": 0.47310126582278483,
                        "total_cost": 908278.2471583189
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8598
Profiling... [256/50048]	Loss: 1.9908
Profiling... [384/50048]	Loss: 2.1876
Profiling... [512/50048]	Loss: 1.7388
Profiling... [640/50048]	Loss: 2.1988
Profiling... [768/50048]	Loss: 1.8518
Profiling... [896/50048]	Loss: 1.8550
Profiling... [1024/50048]	Loss: 1.8529
Profiling... [1152/50048]	Loss: 2.0681
Profiling... [1280/50048]	Loss: 2.1443
Profiling... [1408/50048]	Loss: 1.6706
Profiling... [1536/50048]	Loss: 1.7479
Profiling... [1664/50048]	Loss: 2.1426
Profile done
epoch 1 train time consumed: 7.29s
Validation Epoch: 7, Average loss: 0.0149, Accuracy: 0.4767
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.50087325080867,
                        "time": 5.332620886999393,
                        "accuracy": 0.4766613924050633,
                        "total_cost": 1957802.0584303166
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9939
Profiling... [256/50048]	Loss: 1.9284
Profiling... [384/50048]	Loss: 2.3173
Profiling... [512/50048]	Loss: 2.1667
Profiling... [640/50048]	Loss: 1.8806
Profiling... [768/50048]	Loss: 2.1073
Profiling... [896/50048]	Loss: 2.0107
Profiling... [1024/50048]	Loss: 1.6798
Profiling... [1152/50048]	Loss: 1.8276
Profiling... [1280/50048]	Loss: 1.9717
Profiling... [1408/50048]	Loss: 1.9291
Profiling... [1536/50048]	Loss: 1.9598
Profiling... [1664/50048]	Loss: 1.5234
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 7, Average loss: 0.0150, Accuracy: 0.4750
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.4387133998947,
                        "time": 2.1887690680005107,
                        "accuracy": 0.47498022151898733,
                        "total_cost": 806422.1825387683
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8998
Profiling... [256/50048]	Loss: 2.0874
Profiling... [384/50048]	Loss: 2.0285
Profiling... [512/50048]	Loss: 2.0963
Profiling... [640/50048]	Loss: 2.0560
Profiling... [768/50048]	Loss: 1.9156
Profiling... [896/50048]	Loss: 1.8853
Profiling... [1024/50048]	Loss: 2.0593
Profiling... [1152/50048]	Loss: 1.8522
Profiling... [1280/50048]	Loss: 1.7925
Profiling... [1408/50048]	Loss: 1.9296
Profiling... [1536/50048]	Loss: 2.0008
Profiling... [1664/50048]	Loss: 2.0822
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 7, Average loss: 0.0149, Accuracy: 0.4787
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.48848395186154,
                        "time": 2.1915172770004574,
                        "accuracy": 0.4787381329113924,
                        "total_cost": 801096.6687419974
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0734
Profiling... [256/50048]	Loss: 1.9464
Profiling... [384/50048]	Loss: 1.8003
Profiling... [512/50048]	Loss: 2.0601
Profiling... [640/50048]	Loss: 2.0225
Profiling... [768/50048]	Loss: 1.8772
Profiling... [896/50048]	Loss: 1.8692
Profiling... [1024/50048]	Loss: 1.9267
Profiling... [1152/50048]	Loss: 1.7099
Profiling... [1280/50048]	Loss: 1.8338
Profiling... [1408/50048]	Loss: 1.8494
Profiling... [1536/50048]	Loss: 2.1116
Profiling... [1664/50048]	Loss: 1.8558
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 7, Average loss: 0.0151, Accuracy: 0.4776
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.5145860339608,
                        "time": 2.43395135999981,
                        "accuracy": 0.4775514240506329,
                        "total_cost": 891928.0030349272
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0181
Profiling... [256/50048]	Loss: 1.9652
Profiling... [384/50048]	Loss: 1.7720
Profiling... [512/50048]	Loss: 1.9361
Profiling... [640/50048]	Loss: 2.0812
Profiling... [768/50048]	Loss: 1.8370
Profiling... [896/50048]	Loss: 1.9205
Profiling... [1024/50048]	Loss: 1.7101
Profiling... [1152/50048]	Loss: 2.2034
Profiling... [1280/50048]	Loss: 1.9076
Profiling... [1408/50048]	Loss: 1.6875
Profiling... [1536/50048]	Loss: 2.0634
Profiling... [1664/50048]	Loss: 1.8284
Profile done
epoch 1 train time consumed: 7.33s
Validation Epoch: 7, Average loss: 0.0150, Accuracy: 0.4777
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.4629339122364,
                        "time": 5.432910309000363,
                        "accuracy": 0.4776503164556962,
                        "total_cost": 1990492.3566888284
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7368
Profiling... [256/50048]	Loss: 1.8022
Profiling... [384/50048]	Loss: 1.7754
Profiling... [512/50048]	Loss: 2.0536
Profiling... [640/50048]	Loss: 1.6408
Profiling... [768/50048]	Loss: 1.8329
Profiling... [896/50048]	Loss: 1.8805
Profiling... [1024/50048]	Loss: 1.8465
Profiling... [1152/50048]	Loss: 2.1293
Profiling... [1280/50048]	Loss: 2.0694
Profiling... [1408/50048]	Loss: 1.8063
Profiling... [1536/50048]	Loss: 1.8658
Profiling... [1664/50048]	Loss: 1.8282
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 7, Average loss: 0.0151, Accuracy: 0.4747
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.39948946867673,
                        "time": 2.1794159720002426,
                        "accuracy": 0.47468354430379744,
                        "total_cost": 803478.0216774229
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8358
Profiling... [256/50048]	Loss: 1.9187
Profiling... [384/50048]	Loss: 2.0843
Profiling... [512/50048]	Loss: 1.9804
Profiling... [640/50048]	Loss: 1.9956
Profiling... [768/50048]	Loss: 2.1855
Profiling... [896/50048]	Loss: 1.8611
Profiling... [1024/50048]	Loss: 1.8507
Profiling... [1152/50048]	Loss: 1.7507
Profiling... [1280/50048]	Loss: 1.9820
Profiling... [1408/50048]	Loss: 1.9025
Profiling... [1536/50048]	Loss: 2.0036
Profiling... [1664/50048]	Loss: 2.0298
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 7, Average loss: 0.0150, Accuracy: 0.4782
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.44315247038297,
                        "time": 2.1833702320000157,
                        "accuracy": 0.47824367088607594,
                        "total_cost": 798943.747424985
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9510
Profiling... [256/50048]	Loss: 1.8015
Profiling... [384/50048]	Loss: 2.0197
Profiling... [512/50048]	Loss: 1.9158
Profiling... [640/50048]	Loss: 1.8962
Profiling... [768/50048]	Loss: 2.2167
Profiling... [896/50048]	Loss: 1.8451
Profiling... [1024/50048]	Loss: 2.2318
Profiling... [1152/50048]	Loss: 1.8450
Profiling... [1280/50048]	Loss: 2.1775
Profiling... [1408/50048]	Loss: 1.6983
Profiling... [1536/50048]	Loss: 1.7587
Profiling... [1664/50048]	Loss: 1.6838
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 7, Average loss: 0.0151, Accuracy: 0.4799
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.46915548416169,
                        "time": 2.445085941999423,
                        "accuracy": 0.4799248417721519,
                        "total_cost": 891577.1858566204
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0167
Profiling... [256/50048]	Loss: 2.1816
Profiling... [384/50048]	Loss: 2.0962
Profiling... [512/50048]	Loss: 1.9248
Profiling... [640/50048]	Loss: 1.7572
Profiling... [768/50048]	Loss: 2.0156
Profiling... [896/50048]	Loss: 1.9156
Profiling... [1024/50048]	Loss: 2.2048
Profiling... [1152/50048]	Loss: 2.0068
Profiling... [1280/50048]	Loss: 2.1940
Profiling... [1408/50048]	Loss: 1.9316
Profiling... [1536/50048]	Loss: 1.8591
Profiling... [1664/50048]	Loss: 2.0275
Profile done
epoch 1 train time consumed: 7.36s
Validation Epoch: 7, Average loss: 0.0149, Accuracy: 0.4774
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.42080445184264,
                        "time": 5.436692375999883,
                        "accuracy": 0.47735363924050633,
                        "total_cost": 1993115.9785724864
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7643
Profiling... [256/50048]	Loss: 1.7247
Profiling... [384/50048]	Loss: 1.9131
Profiling... [512/50048]	Loss: 1.9798
Profiling... [640/50048]	Loss: 1.8333
Profiling... [768/50048]	Loss: 2.1020
Profiling... [896/50048]	Loss: 1.9840
Profiling... [1024/50048]	Loss: 1.8824
Profiling... [1152/50048]	Loss: 2.0892
Profiling... [1280/50048]	Loss: 1.9403
Profiling... [1408/50048]	Loss: 1.8135
Profiling... [1536/50048]	Loss: 1.6468
Profiling... [1664/50048]	Loss: 1.9471
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 7, Average loss: 0.0169, Accuracy: 0.4269
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.35768726335942,
                        "time": 2.1746681530003116,
                        "accuracy": 0.42691851265822783,
                        "total_cost": 891427.5569954487
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9766
Profiling... [256/50048]	Loss: 1.7997
Profiling... [384/50048]	Loss: 2.0516
Profiling... [512/50048]	Loss: 1.6735
Profiling... [640/50048]	Loss: 1.9958
Profiling... [768/50048]	Loss: 1.9630
Profiling... [896/50048]	Loss: 2.0057
Profiling... [1024/50048]	Loss: 1.9552
Profiling... [1152/50048]	Loss: 2.0700
Profiling... [1280/50048]	Loss: 1.9649
Profiling... [1408/50048]	Loss: 1.8765
Profiling... [1536/50048]	Loss: 2.0323
Profiling... [1664/50048]	Loss: 2.2065
Profile done
epoch 1 train time consumed: 3.37s
Validation Epoch: 7, Average loss: 0.0162, Accuracy: 0.4422
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.399861722885,
                        "time": 2.184951173000627,
                        "accuracy": 0.442246835443038,
                        "total_cost": 864599.64126608
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9029
Profiling... [256/50048]	Loss: 1.8540
Profiling... [384/50048]	Loss: 1.9993
Profiling... [512/50048]	Loss: 2.1184
Profiling... [640/50048]	Loss: 2.1560
Profiling... [768/50048]	Loss: 1.7808
Profiling... [896/50048]	Loss: 1.9058
Profiling... [1024/50048]	Loss: 1.6907
Profiling... [1152/50048]	Loss: 1.9569
Profiling... [1280/50048]	Loss: 1.9613
Profiling... [1408/50048]	Loss: 1.8607
Profiling... [1536/50048]	Loss: 1.8348
Profiling... [1664/50048]	Loss: 2.1853
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 7, Average loss: 0.0230, Accuracy: 0.3016
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.42809512062549,
                        "time": 2.4392268809997404,
                        "accuracy": 0.301621835443038,
                        "total_cost": 1415231.4388908655
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.1318
Profiling... [256/50048]	Loss: 1.8520
Profiling... [384/50048]	Loss: 2.0331
Profiling... [512/50048]	Loss: 1.9682
Profiling... [640/50048]	Loss: 2.1071
Profiling... [768/50048]	Loss: 2.0013
Profiling... [896/50048]	Loss: 1.9342
Profiling... [1024/50048]	Loss: 1.8497
Profiling... [1152/50048]	Loss: 1.9107
Profiling... [1280/50048]	Loss: 1.9916
Profiling... [1408/50048]	Loss: 1.7179
Profiling... [1536/50048]	Loss: 1.7397
Profiling... [1664/50048]	Loss: 1.9005
Profile done
epoch 1 train time consumed: 7.45s
Validation Epoch: 7, Average loss: 0.0159, Accuracy: 0.4498
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.38022994971055,
                        "time": 5.434526152000217,
                        "accuracy": 0.4497626582278481,
                        "total_cost": 2114542.1017105505
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9314
Profiling... [256/50048]	Loss: 2.1234
Profiling... [384/50048]	Loss: 1.8108
Profiling... [512/50048]	Loss: 1.9199
Profiling... [640/50048]	Loss: 1.9337
Profiling... [768/50048]	Loss: 1.8110
Profiling... [896/50048]	Loss: 1.7837
Profiling... [1024/50048]	Loss: 1.7378
Profiling... [1152/50048]	Loss: 1.8893
Profiling... [1280/50048]	Loss: 1.8804
Profiling... [1408/50048]	Loss: 2.1270
Profiling... [1536/50048]	Loss: 2.2447
Profiling... [1664/50048]	Loss: 2.1857
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 7, Average loss: 0.0176, Accuracy: 0.4253
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.31908867780788,
                        "time": 2.184724585999902,
                        "accuracy": 0.42533623417721517,
                        "total_cost": 898881.3362904969
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9319
Profiling... [256/50048]	Loss: 2.1475
Profiling... [384/50048]	Loss: 1.6135
Profiling... [512/50048]	Loss: 1.9691
Profiling... [640/50048]	Loss: 1.9840
Profiling... [768/50048]	Loss: 1.7349
Profiling... [896/50048]	Loss: 1.8615
Profiling... [1024/50048]	Loss: 1.8812
Profiling... [1152/50048]	Loss: 1.9257
Profiling... [1280/50048]	Loss: 1.8976
Profiling... [1408/50048]	Loss: 1.9826
Profiling... [1536/50048]	Loss: 2.1030
Profiling... [1664/50048]	Loss: 1.9880
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 7, Average loss: 0.0178, Accuracy: 0.4052
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.3621031668608,
                        "time": 2.1944838330000493,
                        "accuracy": 0.4051621835443038,
                        "total_cost": 947854.1837629698
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6557
Profiling... [256/50048]	Loss: 1.9571
Profiling... [384/50048]	Loss: 1.9136
Profiling... [512/50048]	Loss: 2.1712
Profiling... [640/50048]	Loss: 1.9280
Profiling... [768/50048]	Loss: 1.9711
Profiling... [896/50048]	Loss: 2.1140
Profiling... [1024/50048]	Loss: 2.1297
Profiling... [1152/50048]	Loss: 1.9403
Profiling... [1280/50048]	Loss: 1.7603
Profiling... [1408/50048]	Loss: 1.8589
Profiling... [1536/50048]	Loss: 1.8995
Profiling... [1664/50048]	Loss: 1.8645
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 7, Average loss: 0.0163, Accuracy: 0.4392
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.387623609016,
                        "time": 2.4348261840004852,
                        "accuracy": 0.43918117088607594,
                        "total_cost": 970202.3002042916
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.2221
Profiling... [256/50048]	Loss: 1.6379
Profiling... [384/50048]	Loss: 1.7138
Profiling... [512/50048]	Loss: 2.3358
Profiling... [640/50048]	Loss: 2.0198
Profiling... [768/50048]	Loss: 1.9278
Profiling... [896/50048]	Loss: 1.9229
Profiling... [1024/50048]	Loss: 1.6180
Profiling... [1152/50048]	Loss: 2.2017
Profiling... [1280/50048]	Loss: 2.1405
Profiling... [1408/50048]	Loss: 1.9636
Profiling... [1536/50048]	Loss: 1.7890
Profiling... [1664/50048]	Loss: 1.9806
Profile done
epoch 1 train time consumed: 6.35s
Validation Epoch: 7, Average loss: 0.0199, Accuracy: 0.3510
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.34517340621072,
                        "time": 4.508298656999614,
                        "accuracy": 0.3509691455696203,
                        "total_cost": 2247924.8530364935
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8754
Profiling... [256/50048]	Loss: 1.9304
Profiling... [384/50048]	Loss: 1.6546
Profiling... [512/50048]	Loss: 1.9070
Profiling... [640/50048]	Loss: 1.7997
Profiling... [768/50048]	Loss: 1.8725
Profiling... [896/50048]	Loss: 2.0128
Profiling... [1024/50048]	Loss: 1.9960
Profiling... [1152/50048]	Loss: 1.6975
Profiling... [1280/50048]	Loss: 1.9203
Profiling... [1408/50048]	Loss: 1.8257
Profiling... [1536/50048]	Loss: 1.9784
Profiling... [1664/50048]	Loss: 2.1742
Profile done
epoch 1 train time consumed: 3.34s
Validation Epoch: 7, Average loss: 0.0159, Accuracy: 0.4466
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.28585106002195,
                        "time": 2.1966240860001562,
                        "accuracy": 0.4465981012658228,
                        "total_cost": 860749.7747090072
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1311
Profiling... [256/50048]	Loss: 1.9406
Profiling... [384/50048]	Loss: 1.9649
Profiling... [512/50048]	Loss: 2.1501
Profiling... [640/50048]	Loss: 1.7891
Profiling... [768/50048]	Loss: 1.6923
Profiling... [896/50048]	Loss: 1.7963
Profiling... [1024/50048]	Loss: 1.8215
Profiling... [1152/50048]	Loss: 2.0551
Profiling... [1280/50048]	Loss: 2.0084
Profiling... [1408/50048]	Loss: 1.7906
Profiling... [1536/50048]	Loss: 1.7962
Profiling... [1664/50048]	Loss: 1.8429
Profile done
epoch 1 train time consumed: 3.40s
Validation Epoch: 7, Average loss: 0.0176, Accuracy: 0.4194
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.33101714073572,
                        "time": 2.1860313010001846,
                        "accuracy": 0.4194026898734177,
                        "total_cost": 912143.5959089665
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9425
Profiling... [256/50048]	Loss: 1.6688
Profiling... [384/50048]	Loss: 1.8645
Profiling... [512/50048]	Loss: 2.1124
Profiling... [640/50048]	Loss: 2.1064
Profiling... [768/50048]	Loss: 1.9523
Profiling... [896/50048]	Loss: 1.8312
Profiling... [1024/50048]	Loss: 2.0873
Profiling... [1152/50048]	Loss: 2.1901
Profiling... [1280/50048]	Loss: 2.0521
Profiling... [1408/50048]	Loss: 1.9744
Profiling... [1536/50048]	Loss: 2.1019
Profiling... [1664/50048]	Loss: 1.9185
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 7, Average loss: 0.0167, Accuracy: 0.4349
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.35439177956097,
                        "time": 2.4397836970001663,
                        "accuracy": 0.43492879746835444,
                        "total_cost": 981682.8627129364
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7289
Profiling... [256/50048]	Loss: 1.7414
Profiling... [384/50048]	Loss: 2.0226
Profiling... [512/50048]	Loss: 1.8595
Profiling... [640/50048]	Loss: 1.8034
Profiling... [768/50048]	Loss: 1.9333
Profiling... [896/50048]	Loss: 2.0726
Profiling... [1024/50048]	Loss: 2.0619
Profiling... [1152/50048]	Loss: 2.0205
Profiling... [1280/50048]	Loss: 1.5882
Profiling... [1408/50048]	Loss: 1.8461
Profiling... [1536/50048]	Loss: 1.9742
Profiling... [1664/50048]	Loss: 2.0224
Profile done
epoch 1 train time consumed: 7.28s
Validation Epoch: 7, Average loss: 0.0164, Accuracy: 0.4321
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.3079524533517,
                        "time": 5.40000486199915,
                        "accuracy": 0.432060917721519,
                        "total_cost": 2187193.546302059
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8118
Profiling... [256/50048]	Loss: 1.6604
Profiling... [384/50048]	Loss: 2.0676
Profiling... [512/50048]	Loss: 1.8897
Profiling... [640/50048]	Loss: 2.0757
Profiling... [768/50048]	Loss: 2.3180
Profiling... [896/50048]	Loss: 2.4456
Profiling... [1024/50048]	Loss: 1.9996
Profiling... [1152/50048]	Loss: 2.1330
Profiling... [1280/50048]	Loss: 1.7958
Profiling... [1408/50048]	Loss: 2.0508
Profiling... [1536/50048]	Loss: 2.1230
Profiling... [1664/50048]	Loss: 2.2313
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 7, Average loss: 0.0257, Accuracy: 0.2524
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.24559025744429,
                        "time": 2.1773235279997607,
                        "accuracy": 0.252373417721519,
                        "total_cost": 1509792.9918293012
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.0542
Profiling... [256/50048]	Loss: 2.3337
Profiling... [384/50048]	Loss: 2.1485
Profiling... [512/50048]	Loss: 1.8852
Profiling... [640/50048]	Loss: 1.9556
Profiling... [768/50048]	Loss: 2.3593
Profiling... [896/50048]	Loss: 2.0389
Profiling... [1024/50048]	Loss: 1.8744
Profiling... [1152/50048]	Loss: 2.2124
Profiling... [1280/50048]	Loss: 2.0682
Profiling... [1408/50048]	Loss: 2.0665
Profiling... [1536/50048]	Loss: 2.0242
Profiling... [1664/50048]	Loss: 2.1680
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 7, Average loss: 0.0213, Accuracy: 0.3367
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.28546414873733,
                        "time": 2.173026782999841,
                        "accuracy": 0.33672863924050633,
                        "total_cost": 1129335.7401458204
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.2292
Profiling... [256/50048]	Loss: 1.7978
Profiling... [384/50048]	Loss: 2.4809
Profiling... [512/50048]	Loss: 2.1969
Profiling... [640/50048]	Loss: 2.2755
Profiling... [768/50048]	Loss: 2.2579
Profiling... [896/50048]	Loss: 2.1614
Profiling... [1024/50048]	Loss: 2.3264
Profiling... [1152/50048]	Loss: 2.2877
Profiling... [1280/50048]	Loss: 1.7852
Profiling... [1408/50048]	Loss: 2.2245
Profiling... [1536/50048]	Loss: 2.1283
Profiling... [1664/50048]	Loss: 2.0781
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 7, Average loss: 0.0215, Accuracy: 0.3245
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.31100045085455,
                        "time": 2.4247952579999037,
                        "accuracy": 0.3244659810126582,
                        "total_cost": 1307807.8904470068
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0084
Profiling... [256/50048]	Loss: 2.0996
Profiling... [384/50048]	Loss: 2.0388
Profiling... [512/50048]	Loss: 1.9834
Profiling... [640/50048]	Loss: 2.2903
Profiling... [768/50048]	Loss: 2.1307
Profiling... [896/50048]	Loss: 1.9855
Profiling... [1024/50048]	Loss: 2.1808
Profiling... [1152/50048]	Loss: 2.0297
Profiling... [1280/50048]	Loss: 2.3439
Profiling... [1408/50048]	Loss: 2.0646
Profiling... [1536/50048]	Loss: 2.2792
Profiling... [1664/50048]	Loss: 2.0756
Profile done
epoch 1 train time consumed: 7.10s
Validation Epoch: 7, Average loss: 0.0298, Accuracy: 0.2227
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.2652888063333,
                        "time": 5.06252707800013,
                        "accuracy": 0.22270569620253164,
                        "total_cost": 3978085.2207944184
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0402
Profiling... [256/50048]	Loss: 2.1937
Profiling... [384/50048]	Loss: 2.1068
Profiling... [512/50048]	Loss: 1.9812
Profiling... [640/50048]	Loss: 2.0991
Profiling... [768/50048]	Loss: 1.9904
Profiling... [896/50048]	Loss: 2.1931
Profiling... [1024/50048]	Loss: 1.9733
Profiling... [1152/50048]	Loss: 2.3080
Profiling... [1280/50048]	Loss: 2.2541
Profiling... [1408/50048]	Loss: 2.0597
Profiling... [1536/50048]	Loss: 2.3830
Profiling... [1664/50048]	Loss: 1.7697
Profile done
epoch 1 train time consumed: 3.41s
Validation Epoch: 7, Average loss: 0.0252, Accuracy: 0.2813
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.20941302313263,
                        "time": 2.175558328999614,
                        "accuracy": 0.2813488924050633,
                        "total_cost": 1353204.9275914647
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9777
Profiling... [256/50048]	Loss: 1.8983
Profiling... [384/50048]	Loss: 2.1962
Profiling... [512/50048]	Loss: 2.1318
Profiling... [640/50048]	Loss: 1.9199
Profiling... [768/50048]	Loss: 2.2768
Profiling... [896/50048]	Loss: 2.2133
Profiling... [1024/50048]	Loss: 2.1743
Profiling... [1152/50048]	Loss: 2.0558
Profiling... [1280/50048]	Loss: 1.7065
Profiling... [1408/50048]	Loss: 1.8216
Profiling... [1536/50048]	Loss: 2.1249
Profiling... [1664/50048]	Loss: 2.1149
Profile done
epoch 1 train time consumed: 3.41s
Validation Epoch: 7, Average loss: 0.0213, Accuracy: 0.3228
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.24934557439028,
                        "time": 2.184039981000751,
                        "accuracy": 0.3227848101265823,
                        "total_cost": 1184092.2642092307
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0455
Profiling... [256/50048]	Loss: 2.0113
Profiling... [384/50048]	Loss: 2.0784
Profiling... [512/50048]	Loss: 2.0014
Profiling... [640/50048]	Loss: 1.9253
Profiling... [768/50048]	Loss: 2.2938
Profiling... [896/50048]	Loss: 2.1064
Profiling... [1024/50048]	Loss: 1.8558
Profiling... [1152/50048]	Loss: 2.1337
Profiling... [1280/50048]	Loss: 1.9643
Profiling... [1408/50048]	Loss: 2.2434
Profiling... [1536/50048]	Loss: 2.1080
Profiling... [1664/50048]	Loss: 2.1630
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 7, Average loss: 0.0338, Accuracy: 0.1445
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.2743273619716,
                        "time": 2.433619606999855,
                        "accuracy": 0.14448180379746836,
                        "total_cost": 2947661.366561905
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5070
Profiling... [256/50048]	Loss: 1.8161
Profiling... [384/50048]	Loss: 2.0881
Profiling... [512/50048]	Loss: 2.0281
Profiling... [640/50048]	Loss: 1.9373
Profiling... [768/50048]	Loss: 2.0604
Profiling... [896/50048]	Loss: 2.1847
Profiling... [1024/50048]	Loss: 2.3892
Profiling... [1152/50048]	Loss: 2.3806
Profiling... [1280/50048]	Loss: 1.8925
Profiling... [1408/50048]	Loss: 1.9606
Profiling... [1536/50048]	Loss: 2.1339
Profiling... [1664/50048]	Loss: 2.1426
Profile done
epoch 1 train time consumed: 7.02s
Validation Epoch: 7, Average loss: 0.0218, Accuracy: 0.3103
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.23089366244514,
                        "time": 5.0881107089999205,
                        "accuracy": 0.3103243670886076,
                        "total_cost": 2869318.26343093
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9821
Profiling... [256/50048]	Loss: 2.0482
Profiling... [384/50048]	Loss: 1.9453
Profiling... [512/50048]	Loss: 2.1487
Profiling... [640/50048]	Loss: 2.1732
Profiling... [768/50048]	Loss: 2.0305
Profiling... [896/50048]	Loss: 2.0093
Profiling... [1024/50048]	Loss: 2.2594
Profiling... [1152/50048]	Loss: 1.9722
Profiling... [1280/50048]	Loss: 2.1715
Profiling... [1408/50048]	Loss: 2.1180
Profiling... [1536/50048]	Loss: 2.2994
Profiling... [1664/50048]	Loss: 2.2081
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 7, Average loss: 0.0378, Accuracy: 0.1281
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.17619623398419,
                        "time": 2.172334754000076,
                        "accuracy": 0.12806566455696203,
                        "total_cost": 2968466.0854660496
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6700
Profiling... [256/50048]	Loss: 2.0269
Profiling... [384/50048]	Loss: 1.9534
Profiling... [512/50048]	Loss: 1.9264
Profiling... [640/50048]	Loss: 1.9772
Profiling... [768/50048]	Loss: 2.2395
Profiling... [896/50048]	Loss: 1.9955
Profiling... [1024/50048]	Loss: 2.0493
Profiling... [1152/50048]	Loss: 2.0612
Profiling... [1280/50048]	Loss: 2.0258
Profiling... [1408/50048]	Loss: 1.8881
Profiling... [1536/50048]	Loss: 2.3545
Profiling... [1664/50048]	Loss: 1.7788
Profile done
epoch 1 train time consumed: 3.34s
Validation Epoch: 7, Average loss: 0.0267, Accuracy: 0.2156
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.21619563301789,
                        "time": 2.1925376730005155,
                        "accuracy": 0.2155854430379747,
                        "total_cost": 1779777.3697897762
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8108
Profiling... [256/50048]	Loss: 1.8549
Profiling... [384/50048]	Loss: 1.7879
Profiling... [512/50048]	Loss: 1.9144
Profiling... [640/50048]	Loss: 1.9609
Profiling... [768/50048]	Loss: 2.3457
Profiling... [896/50048]	Loss: 2.0997
Profiling... [1024/50048]	Loss: 2.3844
Profiling... [1152/50048]	Loss: 2.0915
Profiling... [1280/50048]	Loss: 1.9395
Profiling... [1408/50048]	Loss: 2.2407
Profiling... [1536/50048]	Loss: 1.9436
Profiling... [1664/50048]	Loss: 1.9291
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 7, Average loss: 0.0203, Accuracy: 0.3433
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.24267182479058,
                        "time": 2.4456382050002503,
                        "accuracy": 0.34325553797468356,
                        "total_cost": 1246845.6835403177
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9468
Profiling... [256/50048]	Loss: 1.7159
Profiling... [384/50048]	Loss: 2.1406
Profiling... [512/50048]	Loss: 2.1804
Profiling... [640/50048]	Loss: 1.9929
Profiling... [768/50048]	Loss: 2.1695
Profiling... [896/50048]	Loss: 2.1773
Profiling... [1024/50048]	Loss: 2.0581
Profiling... [1152/50048]	Loss: 2.2424
Profiling... [1280/50048]	Loss: 1.9207
Profiling... [1408/50048]	Loss: 1.8597
Profiling... [1536/50048]	Loss: 2.1648
Profiling... [1664/50048]	Loss: 2.3136
Profile done
epoch 1 train time consumed: 7.02s
Validation Epoch: 7, Average loss: 0.0234, Accuracy: 0.3120
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.1994328525321,
                        "time": 5.073523088000002,
                        "accuracy": 0.31200553797468356,
                        "total_cost": 2845675.580514993
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8842
Profiling... [512/50176]	Loss: 1.9171
Profiling... [768/50176]	Loss: 2.0298
Profiling... [1024/50176]	Loss: 1.8540
Profiling... [1280/50176]	Loss: 1.9427
Profiling... [1536/50176]	Loss: 1.7571
Profiling... [1792/50176]	Loss: 1.7056
Profiling... [2048/50176]	Loss: 1.9490
Profiling... [2304/50176]	Loss: 1.8284
Profiling... [2560/50176]	Loss: 1.8115
Profiling... [2816/50176]	Loss: 1.8843
Profiling... [3072/50176]	Loss: 1.7071
Profiling... [3328/50176]	Loss: 1.9809
Profile done
epoch 1 train time consumed: 4.12s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4749
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.15721249309813,
                        "time": 2.428856557000472,
                        "accuracy": 0.47490234375,
                        "total_cost": 895025.8996802068
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0024
Profiling... [512/50176]	Loss: 1.9782
Profiling... [768/50176]	Loss: 1.8474
Profiling... [1024/50176]	Loss: 1.9292
Profiling... [1280/50176]	Loss: 1.9219
Profiling... [1536/50176]	Loss: 1.8412
Profiling... [1792/50176]	Loss: 1.8549
Profiling... [2048/50176]	Loss: 2.0683
Profiling... [2304/50176]	Loss: 1.8493
Profiling... [2560/50176]	Loss: 1.7521
Profiling... [2816/50176]	Loss: 1.8317
Profiling... [3072/50176]	Loss: 1.9843
Profiling... [3328/50176]	Loss: 1.9506
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 7, Average loss: 0.0075, Accuracy: 0.4723
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.20338809952042,
                        "time": 2.475538802000301,
                        "accuracy": 0.472265625,
                        "total_cost": 917321.2434211207
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0273
Profiling... [512/50176]	Loss: 1.8177
Profiling... [768/50176]	Loss: 1.7603
Profiling... [1024/50176]	Loss: 1.8333
Profiling... [1280/50176]	Loss: 1.8251
Profiling... [1536/50176]	Loss: 1.8405
Profiling... [1792/50176]	Loss: 1.9591
Profiling... [2048/50176]	Loss: 1.8256
Profiling... [2304/50176]	Loss: 1.9455
Profiling... [2560/50176]	Loss: 1.7531
Profiling... [2816/50176]	Loss: 1.8595
Profiling... [3072/50176]	Loss: 1.9581
Profiling... [3328/50176]	Loss: 1.9725
Profile done
epoch 1 train time consumed: 4.25s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4765
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.22768250764722,
                        "time": 2.809810061999997,
                        "accuracy": 0.47646484375,
                        "total_cost": 1032010.5823127679
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1123
Profiling... [512/50176]	Loss: 1.8534
Profiling... [768/50176]	Loss: 1.8508
Profiling... [1024/50176]	Loss: 1.9925
Profiling... [1280/50176]	Loss: 1.9900
Profiling... [1536/50176]	Loss: 1.9013
Profiling... [1792/50176]	Loss: 1.8334
Profiling... [2048/50176]	Loss: 1.9004
Profiling... [2304/50176]	Loss: 1.7714
Profiling... [2560/50176]	Loss: 1.6986
Profiling... [2816/50176]	Loss: 1.9630
Profiling... [3072/50176]	Loss: 1.7518
Profiling... [3328/50176]	Loss: 1.8126
Profile done
epoch 1 train time consumed: 9.71s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4751
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.16491059998532,
                        "time": 6.811350616000709,
                        "accuracy": 0.47509765625,
                        "total_cost": 2508929.14776429
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0016
Profiling... [512/50176]	Loss: 2.0578
Profiling... [768/50176]	Loss: 2.0461
Profiling... [1024/50176]	Loss: 1.8162
Profiling... [1280/50176]	Loss: 2.0687
Profiling... [1536/50176]	Loss: 1.9756
Profiling... [1792/50176]	Loss: 1.9691
Profiling... [2048/50176]	Loss: 1.9404
Profiling... [2304/50176]	Loss: 2.0544
Profiling... [2560/50176]	Loss: 1.8326
Profiling... [2816/50176]	Loss: 1.8028
Profiling... [3072/50176]	Loss: 1.7984
Profiling... [3328/50176]	Loss: 1.9754
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 7, Average loss: 0.0073, Accuracy: 0.4823
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.13084903821375,
                        "time": 2.4268057899998894,
                        "accuracy": 0.48232421875,
                        "total_cost": 880509.4099371942
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9026
Profiling... [512/50176]	Loss: 1.8678
Profiling... [768/50176]	Loss: 1.8859
Profiling... [1024/50176]	Loss: 1.7906
Profiling... [1280/50176]	Loss: 1.9381
Profiling... [1536/50176]	Loss: 1.9772
Profiling... [1792/50176]	Loss: 1.8153
Profiling... [2048/50176]	Loss: 1.9185
Profiling... [2304/50176]	Loss: 1.8857
Profiling... [2560/50176]	Loss: 1.9858
Profiling... [2816/50176]	Loss: 1.8427
Profiling... [3072/50176]	Loss: 1.9073
Profiling... [3328/50176]	Loss: 1.8221
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 7, Average loss: 0.0075, Accuracy: 0.4691
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.17705356270605,
                        "time": 2.4760886949998167,
                        "accuracy": 0.469140625,
                        "total_cost": 923636.7488425628
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0909
Profiling... [512/50176]	Loss: 1.8801
Profiling... [768/50176]	Loss: 1.9675
Profiling... [1024/50176]	Loss: 1.8663
Profiling... [1280/50176]	Loss: 1.7306
Profiling... [1536/50176]	Loss: 1.9249
Profiling... [1792/50176]	Loss: 1.7725
Profiling... [2048/50176]	Loss: 1.8962
Profiling... [2304/50176]	Loss: 1.8243
Profiling... [2560/50176]	Loss: 1.7465
Profiling... [2816/50176]	Loss: 1.8363
Profiling... [3072/50176]	Loss: 1.6747
Profiling... [3328/50176]	Loss: 1.8750
Profile done
epoch 1 train time consumed: 4.23s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4756
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.20188602285762,
                        "time": 2.823882495000362,
                        "accuracy": 0.4755859375,
                        "total_cost": 1039095.9817331927
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8619
Profiling... [512/50176]	Loss: 1.8951
Profiling... [768/50176]	Loss: 1.9952
Profiling... [1024/50176]	Loss: 1.9179
Profiling... [1280/50176]	Loss: 1.8092
Profiling... [1536/50176]	Loss: 1.9422
Profiling... [1792/50176]	Loss: 1.9612
Profiling... [2048/50176]	Loss: 1.8618
Profiling... [2304/50176]	Loss: 2.0187
Profiling... [2560/50176]	Loss: 1.7664
Profiling... [2816/50176]	Loss: 2.0420
Profiling... [3072/50176]	Loss: 1.9371
Profiling... [3328/50176]	Loss: 1.9009
Profile done
epoch 1 train time consumed: 9.86s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4758
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.1433593119889,
                        "time": 7.173401160999674,
                        "accuracy": 0.47578125,
                        "total_cost": 2638492.3810573514
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0849
Profiling... [512/50176]	Loss: 1.9740
Profiling... [768/50176]	Loss: 1.7313
Profiling... [1024/50176]	Loss: 1.9411
Profiling... [1280/50176]	Loss: 1.8931
Profiling... [1536/50176]	Loss: 2.0055
Profiling... [1792/50176]	Loss: 1.8041
Profiling... [2048/50176]	Loss: 1.9645
Profiling... [2304/50176]	Loss: 1.9053
Profiling... [2560/50176]	Loss: 2.0202
Profiling... [2816/50176]	Loss: 1.6837
Profiling... [3072/50176]	Loss: 1.8733
Profiling... [3328/50176]	Loss: 1.7965
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4755
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.11053343179395,
                        "time": 2.4091942790000758,
                        "accuracy": 0.47548828125,
                        "total_cost": 886686.4136307529
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8189
Profiling... [512/50176]	Loss: 2.1322
Profiling... [768/50176]	Loss: 1.8553
Profiling... [1024/50176]	Loss: 2.0361
Profiling... [1280/50176]	Loss: 1.8705
Profiling... [1536/50176]	Loss: 2.0475
Profiling... [1792/50176]	Loss: 2.0865
Profiling... [2048/50176]	Loss: 1.9216
Profiling... [2304/50176]	Loss: 1.8113
Profiling... [2560/50176]	Loss: 1.9372
Profiling... [2816/50176]	Loss: 1.7439
Profiling... [3072/50176]	Loss: 2.0020
Profiling... [3328/50176]	Loss: 1.9183
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4707
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.15874760734147,
                        "time": 2.478001301000404,
                        "accuracy": 0.470703125,
                        "total_cost": 921281.8114922664
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8771
Profiling... [512/50176]	Loss: 1.8152
Profiling... [768/50176]	Loss: 2.0555
Profiling... [1024/50176]	Loss: 1.8728
Profiling... [1280/50176]	Loss: 2.0088
Profiling... [1536/50176]	Loss: 1.8219
Profiling... [1792/50176]	Loss: 1.8152
Profiling... [2048/50176]	Loss: 1.8323
Profiling... [2304/50176]	Loss: 1.7384
Profiling... [2560/50176]	Loss: 1.8829
Profiling... [2816/50176]	Loss: 1.8869
Profiling... [3072/50176]	Loss: 1.8676
Profiling... [3328/50176]	Loss: 2.0023
Profile done
epoch 1 train time consumed: 4.24s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4780
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.18083280431946,
                        "time": 2.8100648620002175,
                        "accuracy": 0.47802734375,
                        "total_cost": 1028730.5889079447
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7543
Profiling... [512/50176]	Loss: 1.9005
Profiling... [768/50176]	Loss: 2.1173
Profiling... [1024/50176]	Loss: 1.8040
Profiling... [1280/50176]	Loss: 1.7854
Profiling... [1536/50176]	Loss: 1.6765
Profiling... [1792/50176]	Loss: 2.0042
Profiling... [2048/50176]	Loss: 1.9288
Profiling... [2304/50176]	Loss: 1.8385
Profiling... [2560/50176]	Loss: 1.9578
Profiling... [2816/50176]	Loss: 1.9294
Profiling... [3072/50176]	Loss: 1.8964
Profiling... [3328/50176]	Loss: 1.8814
Profile done
epoch 1 train time consumed: 9.72s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4782
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.12449096930361,
                        "time": 7.135414956999739,
                        "accuracy": 0.47822265625,
                        "total_cost": 2611121.830292737
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9377
Profiling... [512/50176]	Loss: 1.9562
Profiling... [768/50176]	Loss: 1.8809
Profiling... [1024/50176]	Loss: 1.9237
Profiling... [1280/50176]	Loss: 1.9395
Profiling... [1536/50176]	Loss: 1.8299
Profiling... [1792/50176]	Loss: 1.7211
Profiling... [2048/50176]	Loss: 1.8155
Profiling... [2304/50176]	Loss: 1.9024
Profiling... [2560/50176]	Loss: 1.8360
Profiling... [2816/50176]	Loss: 1.9342
Profiling... [3072/50176]	Loss: 1.9195
Profiling... [3328/50176]	Loss: 1.9697
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 7, Average loss: 0.0107, Accuracy: 0.3254
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.09213186042918,
                        "time": 2.4363209470002403,
                        "accuracy": 0.325390625,
                        "total_cost": 1310290.2572102132
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7752
Profiling... [512/50176]	Loss: 1.9063
Profiling... [768/50176]	Loss: 1.9656
Profiling... [1024/50176]	Loss: 1.9062
Profiling... [1280/50176]	Loss: 1.9860
Profiling... [1536/50176]	Loss: 1.9250
Profiling... [1792/50176]	Loss: 1.8765
Profiling... [2048/50176]	Loss: 2.1003
Profiling... [2304/50176]	Loss: 1.7214
Profiling... [2560/50176]	Loss: 1.8083
Profiling... [2816/50176]	Loss: 1.8925
Profiling... [3072/50176]	Loss: 1.9413
Profiling... [3328/50176]	Loss: 1.6442
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 7, Average loss: 0.0081, Accuracy: 0.4407
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.1379742894948,
                        "time": 2.471107610999752,
                        "accuracy": 0.44072265625,
                        "total_cost": 981215.3421031588
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9364
Profiling... [512/50176]	Loss: 1.8590
Profiling... [768/50176]	Loss: 1.8309
Profiling... [1024/50176]	Loss: 1.8511
Profiling... [1280/50176]	Loss: 1.8321
Profiling... [1536/50176]	Loss: 1.8111
Profiling... [1792/50176]	Loss: 2.1118
Profiling... [2048/50176]	Loss: 1.8804
Profiling... [2304/50176]	Loss: 1.9059
Profiling... [2560/50176]	Loss: 1.8185
Profiling... [2816/50176]	Loss: 2.1680
Profiling... [3072/50176]	Loss: 1.6997
Profiling... [3328/50176]	Loss: 1.7918
Profile done
epoch 1 train time consumed: 4.26s
Validation Epoch: 7, Average loss: 0.0077, Accuracy: 0.4517
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.16007129386867,
                        "time": 2.832553791999999,
                        "accuracy": 0.45166015625,
                        "total_cost": 1097499.7611381616
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8891
Profiling... [512/50176]	Loss: 1.8894
Profiling... [768/50176]	Loss: 1.8945
Profiling... [1024/50176]	Loss: 2.1158
Profiling... [1280/50176]	Loss: 2.1198
Profiling... [1536/50176]	Loss: 1.7589
Profiling... [1792/50176]	Loss: 2.0159
Profiling... [2048/50176]	Loss: 1.9469
Profiling... [2304/50176]	Loss: 1.9720
Profiling... [2560/50176]	Loss: 1.9484
Profiling... [2816/50176]	Loss: 1.9024
Profiling... [3072/50176]	Loss: 1.8306
Profiling... [3328/50176]	Loss: 1.8503
Profile done
epoch 1 train time consumed: 9.69s
Validation Epoch: 7, Average loss: 0.0086, Accuracy: 0.4188
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.10619288647463,
                        "time": 7.169504398000754,
                        "accuracy": 0.41884765625,
                        "total_cost": 2995512.2129208096
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9810
Profiling... [512/50176]	Loss: 2.0562
Profiling... [768/50176]	Loss: 1.7233
Profiling... [1024/50176]	Loss: 1.6039
Profiling... [1280/50176]	Loss: 1.8872
Profiling... [1536/50176]	Loss: 1.7340
Profiling... [1792/50176]	Loss: 1.8855
Profiling... [2048/50176]	Loss: 1.9686
Profiling... [2304/50176]	Loss: 2.0155
Profiling... [2560/50176]	Loss: 1.8224
Profiling... [2816/50176]	Loss: 1.9967
Profiling... [3072/50176]	Loss: 1.8283
Profiling... [3328/50176]	Loss: 1.9758
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 7, Average loss: 0.0086, Accuracy: 0.4203
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.07205251471885,
                        "time": 2.4226963780001824,
                        "accuracy": 0.4203125,
                        "total_cost": 1008706.2986469162
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9299
Profiling... [512/50176]	Loss: 1.7086
Profiling... [768/50176]	Loss: 1.8464
Profiling... [1024/50176]	Loss: 1.8264
Profiling... [1280/50176]	Loss: 1.9338
Profiling... [1536/50176]	Loss: 1.8823
Profiling... [1792/50176]	Loss: 2.0618
Profiling... [2048/50176]	Loss: 1.9087
Profiling... [2304/50176]	Loss: 1.7890
Profiling... [2560/50176]	Loss: 1.8835
Profiling... [2816/50176]	Loss: 1.7674
Profiling... [3072/50176]	Loss: 1.7806
Profiling... [3328/50176]	Loss: 1.9802
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 7, Average loss: 0.0081, Accuracy: 0.4394
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.11540205845904,
                        "time": 2.473452791999989,
                        "accuracy": 0.43935546875,
                        "total_cost": 985202.8013478508
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8971
Profiling... [512/50176]	Loss: 2.0536
Profiling... [768/50176]	Loss: 1.9403
Profiling... [1024/50176]	Loss: 1.8110
Profiling... [1280/50176]	Loss: 1.7723
Profiling... [1536/50176]	Loss: 1.8287
Profiling... [1792/50176]	Loss: 1.9189
Profiling... [2048/50176]	Loss: 1.7615
Profiling... [2304/50176]	Loss: 1.9035
Profiling... [2560/50176]	Loss: 1.8518
Profiling... [2816/50176]	Loss: 1.7278
Profiling... [3072/50176]	Loss: 1.7225
Profiling... [3328/50176]	Loss: 1.9571
Profile done
epoch 1 train time consumed: 4.23s
Validation Epoch: 7, Average loss: 0.0082, Accuracy: 0.4406
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.14006411362386,
                        "time": 2.8245995030001723,
                        "accuracy": 0.440625,
                        "total_cost": 1121826.7529646074
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7731
Profiling... [512/50176]	Loss: 1.6722
Profiling... [768/50176]	Loss: 2.0292
Profiling... [1024/50176]	Loss: 1.8936
Profiling... [1280/50176]	Loss: 1.7507
Profiling... [1536/50176]	Loss: 1.9030
Profiling... [1792/50176]	Loss: 1.8004
Profiling... [2048/50176]	Loss: 1.8787
Profiling... [2304/50176]	Loss: 1.5815
Profiling... [2560/50176]	Loss: 1.7343
Profiling... [2816/50176]	Loss: 1.7603
Profiling... [3072/50176]	Loss: 2.0485
Profiling... [3328/50176]	Loss: 1.5862
Profile done
epoch 1 train time consumed: 9.72s
Validation Epoch: 7, Average loss: 0.0081, Accuracy: 0.4428
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.08728776267905,
                        "time": 7.1262575990003825,
                        "accuracy": 0.4427734375,
                        "total_cost": 2816553.5106768166
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8786
Profiling... [512/50176]	Loss: 2.0427
Profiling... [768/50176]	Loss: 1.7922
Profiling... [1024/50176]	Loss: 1.7579
Profiling... [1280/50176]	Loss: 1.7753
Profiling... [1536/50176]	Loss: 1.9786
Profiling... [1792/50176]	Loss: 1.8084
Profiling... [2048/50176]	Loss: 1.9239
Profiling... [2304/50176]	Loss: 1.9515
Profiling... [2560/50176]	Loss: 1.8563
Profiling... [2816/50176]	Loss: 1.8208
Profiling... [3072/50176]	Loss: 1.8948
Profiling... [3328/50176]	Loss: 1.8086
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 7, Average loss: 0.0082, Accuracy: 0.4331
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.0543659780294,
                        "time": 2.4176444159984385,
                        "accuracy": 0.43310546875,
                        "total_cost": 976870.0774451414
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8410
Profiling... [512/50176]	Loss: 1.8678
Profiling... [768/50176]	Loss: 2.0407
Profiling... [1024/50176]	Loss: 2.0517
Profiling... [1280/50176]	Loss: 1.8103
Profiling... [1536/50176]	Loss: 1.9209
Profiling... [1792/50176]	Loss: 1.8809
Profiling... [2048/50176]	Loss: 1.8394
Profiling... [2304/50176]	Loss: 1.7283
Profiling... [2560/50176]	Loss: 1.9221
Profiling... [2816/50176]	Loss: 1.9550
Profiling... [3072/50176]	Loss: 1.9338
Profiling... [3328/50176]	Loss: 1.9749
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 7, Average loss: 0.0081, Accuracy: 0.4414
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.0987897760792,
                        "time": 2.481252024001151,
                        "accuracy": 0.44140625,
                        "total_cost": 983717.6165951465
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8254
Profiling... [512/50176]	Loss: 1.8956
Profiling... [768/50176]	Loss: 1.8279
Profiling... [1024/50176]	Loss: 2.1471
Profiling... [1280/50176]	Loss: 2.0018
Profiling... [1536/50176]	Loss: 1.6988
Profiling... [1792/50176]	Loss: 1.8288
Profiling... [2048/50176]	Loss: 1.9933
Profiling... [2304/50176]	Loss: 1.9818
Profiling... [2560/50176]	Loss: 2.0430
Profiling... [2816/50176]	Loss: 1.8459
Profiling... [3072/50176]	Loss: 1.7999
Profiling... [3328/50176]	Loss: 1.9130
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 7, Average loss: 0.0082, Accuracy: 0.4281
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.11937115047681,
                        "time": 2.8179446249996545,
                        "accuracy": 0.428125,
                        "total_cost": 1151860.5766421945
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0121
Profiling... [512/50176]	Loss: 1.9459
Profiling... [768/50176]	Loss: 1.8581
Profiling... [1024/50176]	Loss: 1.9600
Profiling... [1280/50176]	Loss: 2.1190
Profiling... [1536/50176]	Loss: 1.9038
Profiling... [1792/50176]	Loss: 2.0669
Profiling... [2048/50176]	Loss: 2.0188
Profiling... [2304/50176]	Loss: 1.7402
Profiling... [2560/50176]	Loss: 1.8787
Profiling... [2816/50176]	Loss: 2.0239
Profiling... [3072/50176]	Loss: 1.9573
Profiling... [3328/50176]	Loss: 1.7730
Profile done
epoch 1 train time consumed: 9.68s
Validation Epoch: 7, Average loss: 0.0079, Accuracy: 0.4491
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.06418663553785,
                        "time": 7.11288985399915,
                        "accuracy": 0.44912109375,
                        "total_cost": 2771536.99029495
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8721
Profiling... [512/50176]	Loss: 1.8892
Profiling... [768/50176]	Loss: 2.0548
Profiling... [1024/50176]	Loss: 1.7701
Profiling... [1280/50176]	Loss: 1.8506
Profiling... [1536/50176]	Loss: 1.8899
Profiling... [1792/50176]	Loss: 2.0489
Profiling... [2048/50176]	Loss: 1.9770
Profiling... [2304/50176]	Loss: 1.9113
Profiling... [2560/50176]	Loss: 1.8970
Profiling... [2816/50176]	Loss: 1.6729
Profiling... [3072/50176]	Loss: 1.8367
Profiling... [3328/50176]	Loss: 2.1112
Profile done
epoch 1 train time consumed: 3.81s
Validation Epoch: 7, Average loss: 0.0116, Accuracy: 0.2764
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.030243591351,
                        "time": 2.412233959999867,
                        "accuracy": 0.2763671875,
                        "total_cost": 1527464.048169527
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7559
Profiling... [512/50176]	Loss: 2.0839
Profiling... [768/50176]	Loss: 2.0561
Profiling... [1024/50176]	Loss: 2.1561
Profiling... [1280/50176]	Loss: 2.0502
Profiling... [1536/50176]	Loss: 1.9110
Profiling... [1792/50176]	Loss: 2.0787
Profiling... [2048/50176]	Loss: 1.8484
Profiling... [2304/50176]	Loss: 2.0045
Profiling... [2560/50176]	Loss: 1.9627
Profiling... [2816/50176]	Loss: 2.1851
Profiling... [3072/50176]	Loss: 1.9834
Profiling... [3328/50176]	Loss: 1.7842
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 7, Average loss: 0.0173, Accuracy: 0.1133
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.07101214812165,
                        "time": 2.4986859889995685,
                        "accuracy": 0.11328125,
                        "total_cost": 3860039.045075196
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0089
Profiling... [512/50176]	Loss: 1.8886
Profiling... [768/50176]	Loss: 1.9706
Profiling... [1024/50176]	Loss: 1.9363
Profiling... [1280/50176]	Loss: 2.0623
Profiling... [1536/50176]	Loss: 2.0340
Profiling... [1792/50176]	Loss: 1.8493
Profiling... [2048/50176]	Loss: 1.8015
Profiling... [2304/50176]	Loss: 2.1397
Profiling... [2560/50176]	Loss: 2.0401
Profiling... [2816/50176]	Loss: 1.9154
Profiling... [3072/50176]	Loss: 2.0541
Profiling... [3328/50176]	Loss: 1.9698
Profile done
epoch 1 train time consumed: 4.26s
Validation Epoch: 7, Average loss: 0.0109, Accuracy: 0.3007
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.09201273092569,
                        "time": 2.8278027979995386,
                        "accuracy": 0.30068359375,
                        "total_cost": 1645801.43358726
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8762
Profiling... [512/50176]	Loss: 1.9230
Profiling... [768/50176]	Loss: 1.8701
Profiling... [1024/50176]	Loss: 2.2135
Profiling... [1280/50176]	Loss: 2.1060
Profiling... [1536/50176]	Loss: 2.0101
Profiling... [1792/50176]	Loss: 2.0778
Profiling... [2048/50176]	Loss: 1.9531
Profiling... [2304/50176]	Loss: 1.9983
Profiling... [2560/50176]	Loss: 2.2415
Profiling... [2816/50176]	Loss: 1.7996
Profiling... [3072/50176]	Loss: 1.9848
Profiling... [3328/50176]	Loss: 1.8707
Profile done
epoch 1 train time consumed: 9.96s
Validation Epoch: 7, Average loss: 0.0097, Accuracy: 0.3585
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.0337423938319,
                        "time": 7.158097264000389,
                        "accuracy": 0.35849609375,
                        "total_cost": 3494227.811792072
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8855
Profiling... [512/50176]	Loss: 1.8914
Profiling... [768/50176]	Loss: 1.9495
Profiling... [1024/50176]	Loss: 1.8398
Profiling... [1280/50176]	Loss: 1.9882
Profiling... [1536/50176]	Loss: 1.9742
Profiling... [1792/50176]	Loss: 1.9798
Profiling... [2048/50176]	Loss: 1.6981
Profiling... [2304/50176]	Loss: 1.8015
Profiling... [2560/50176]	Loss: 1.8251
Profiling... [2816/50176]	Loss: 1.8983
Profiling... [3072/50176]	Loss: 2.1932
Profiling... [3328/50176]	Loss: 1.9210
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 7, Average loss: 0.0110, Accuracy: 0.3298
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.00575152502445,
                        "time": 2.4410292699994898,
                        "accuracy": 0.32978515625,
                        "total_cost": 1295328.531785338
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9480
Profiling... [512/50176]	Loss: 1.8814
Profiling... [768/50176]	Loss: 1.8634
Profiling... [1024/50176]	Loss: 1.9820
Profiling... [1280/50176]	Loss: 1.9189
Profiling... [1536/50176]	Loss: 2.0466
Profiling... [1792/50176]	Loss: 2.0571
Profiling... [2048/50176]	Loss: 1.7907
Profiling... [2304/50176]	Loss: 1.8519
Profiling... [2560/50176]	Loss: 2.1789
Profiling... [2816/50176]	Loss: 2.0391
Profiling... [3072/50176]	Loss: 2.0818
Profiling... [3328/50176]	Loss: 1.9971
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 7, Average loss: 0.0151, Accuracy: 0.1869
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.04574485969239,
                        "time": 2.4759960049996153,
                        "accuracy": 0.1869140625,
                        "total_cost": 2318173.8980978634
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0033
Profiling... [512/50176]	Loss: 2.0479
Profiling... [768/50176]	Loss: 1.9932
Profiling... [1024/50176]	Loss: 2.0558
Profiling... [1280/50176]	Loss: 2.2212
Profiling... [1536/50176]	Loss: 2.1843
Profiling... [1792/50176]	Loss: 2.2158
Profiling... [2048/50176]	Loss: 2.1344
Profiling... [2304/50176]	Loss: 1.8950
Profiling... [2560/50176]	Loss: 1.7315
Profiling... [2816/50176]	Loss: 1.9723
Profiling... [3072/50176]	Loss: 1.8948
Profiling... [3328/50176]	Loss: 1.9637
Profile done
epoch 1 train time consumed: 4.22s
Validation Epoch: 7, Average loss: 0.0121, Accuracy: 0.2641
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.0677435428092,
                        "time": 2.819134137000219,
                        "accuracy": 0.2640625,
                        "total_cost": 1868301.9132782521
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9799
Profiling... [512/50176]	Loss: 1.7879
Profiling... [768/50176]	Loss: 1.9364
Profiling... [1024/50176]	Loss: 1.9114
Profiling... [1280/50176]	Loss: 1.9665
Profiling... [1536/50176]	Loss: 2.0480
Profiling... [1792/50176]	Loss: 1.8806
Profiling... [2048/50176]	Loss: 2.1089
Profiling... [2304/50176]	Loss: 2.1505
Profiling... [2560/50176]	Loss: 1.9925
Profiling... [2816/50176]	Loss: 2.0485
Profiling... [3072/50176]	Loss: 1.9166
Profiling... [3328/50176]	Loss: 2.1418
Profile done
epoch 1 train time consumed: 10.04s
Validation Epoch: 7, Average loss: 0.0150, Accuracy: 0.2263
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.01066127592097,
                        "time": 7.394697935000295,
                        "accuracy": 0.22626953125,
                        "total_cost": 5719162.149124095
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9469
Profiling... [512/50176]	Loss: 1.7809
Profiling... [768/50176]	Loss: 2.0521
Profiling... [1024/50176]	Loss: 2.0017
Profiling... [1280/50176]	Loss: 2.0718
Profiling... [1536/50176]	Loss: 2.0479
Profiling... [1792/50176]	Loss: 2.0036
Profiling... [2048/50176]	Loss: 2.0569
Profiling... [2304/50176]	Loss: 2.0528
Profiling... [2560/50176]	Loss: 1.8862
Profiling... [2816/50176]	Loss: 1.9219
Profiling... [3072/50176]	Loss: 1.8704
Profiling... [3328/50176]	Loss: 1.7840
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 7, Average loss: 0.0094, Accuracy: 0.3870
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.97974722065305,
                        "time": 2.404339589000301,
                        "accuracy": 0.38701171875,
                        "total_cost": 1087200.742742503
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0129
Profiling... [512/50176]	Loss: 1.8550
Profiling... [768/50176]	Loss: 2.1714
Profiling... [1024/50176]	Loss: 2.2168
Profiling... [1280/50176]	Loss: 1.8565
Profiling... [1536/50176]	Loss: 2.2160
Profiling... [1792/50176]	Loss: 2.1514
Profiling... [2048/50176]	Loss: 1.9977
Profiling... [2304/50176]	Loss: 1.8634
Profiling... [2560/50176]	Loss: 1.7428
Profiling... [2816/50176]	Loss: 1.9761
Profiling... [3072/50176]	Loss: 1.8529
Profiling... [3328/50176]	Loss: 1.8842
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 7, Average loss: 0.0120, Accuracy: 0.2827
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.02446914782102,
                        "time": 2.457000053000229,
                        "accuracy": 0.28271484375,
                        "total_cost": 1520878.789283734
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8058
Profiling... [512/50176]	Loss: 2.0271
Profiling... [768/50176]	Loss: 1.9116
Profiling... [1024/50176]	Loss: 1.9052
Profiling... [1280/50176]	Loss: 2.2218
Profiling... [1536/50176]	Loss: 1.8908
Profiling... [1792/50176]	Loss: 2.1113
Profiling... [2048/50176]	Loss: 1.9177
Profiling... [2304/50176]	Loss: 1.9199
Profiling... [2560/50176]	Loss: 2.0002
Profiling... [2816/50176]	Loss: 2.1246
Profiling... [3072/50176]	Loss: 1.9892
Profiling... [3328/50176]	Loss: 2.0246
Profile done
epoch 1 train time consumed: 4.21s
Validation Epoch: 7, Average loss: 0.0102, Accuracy: 0.3600
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.04484952150767,
                        "time": 2.807712977999472,
                        "accuracy": 0.3599609375,
                        "total_cost": 1365008.5883274698
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.2189
Profiling... [512/50176]	Loss: 2.1235
Profiling... [768/50176]	Loss: 1.9533
Profiling... [1024/50176]	Loss: 2.0474
Profiling... [1280/50176]	Loss: 1.9003
Profiling... [1536/50176]	Loss: 1.8788
Profiling... [1792/50176]	Loss: 2.1215
Profiling... [2048/50176]	Loss: 1.7599
Profiling... [2304/50176]	Loss: 2.0484
Profiling... [2560/50176]	Loss: 2.0743
Profiling... [2816/50176]	Loss: 2.1478
Profiling... [3072/50176]	Loss: 1.9301
Profiling... [3328/50176]	Loss: 2.1602
Profile done
epoch 1 train time consumed: 10.02s
Validation Epoch: 7, Average loss: 0.0124, Accuracy: 0.3097
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.98985191724816,
                        "time": 7.086773442999402,
                        "accuracy": 0.30966796875,
                        "total_cost": 4004887.420326372
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0408
Profiling... [1024/50176]	Loss: 1.7984
Profiling... [1536/50176]	Loss: 1.8528
Profiling... [2048/50176]	Loss: 1.8847
Profiling... [2560/50176]	Loss: 1.8099
Profiling... [3072/50176]	Loss: 1.8628
Profiling... [3584/50176]	Loss: 1.8148
Profiling... [4096/50176]	Loss: 1.9065
Profiling... [4608/50176]	Loss: 1.8522
Profiling... [5120/50176]	Loss: 1.9213
Profiling... [5632/50176]	Loss: 1.8607
Profiling... [6144/50176]	Loss: 1.8245
Profiling... [6656/50176]	Loss: 1.9482
Profile done
epoch 1 train time consumed: 6.74s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4779
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.98381774092825,
                        "time": 4.60345428599976,
                        "accuracy": 0.4779296875,
                        "total_cost": 1685613.0119557765
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8255
Profiling... [1024/50176]	Loss: 1.8491
Profiling... [1536/50176]	Loss: 1.7792
Profiling... [2048/50176]	Loss: 1.9112
Profiling... [2560/50176]	Loss: 1.8987
Profiling... [3072/50176]	Loss: 1.8897
Profiling... [3584/50176]	Loss: 1.6832
Profiling... [4096/50176]	Loss: 1.8045
Profiling... [4608/50176]	Loss: 1.9219
Profiling... [5120/50176]	Loss: 1.9298
Profiling... [5632/50176]	Loss: 1.7986
Profiling... [6144/50176]	Loss: 1.8132
Profiling... [6656/50176]	Loss: 1.8301
Profile done
epoch 1 train time consumed: 7.08s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4790
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.0437378780087,
                        "time": 4.709907385999031,
                        "accuracy": 0.47900390625,
                        "total_cost": 1720724.5740489836
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0048
Profiling... [1024/50176]	Loss: 1.9083
Profiling... [1536/50176]	Loss: 1.8461
Profiling... [2048/50176]	Loss: 1.8130
Profiling... [2560/50176]	Loss: 1.8184
Profiling... [3072/50176]	Loss: 1.8508
Profiling... [3584/50176]	Loss: 1.8022
Profiling... [4096/50176]	Loss: 1.8223
Profiling... [4608/50176]	Loss: 1.8779
Profiling... [5120/50176]	Loss: 1.8646
Profiling... [5632/50176]	Loss: 1.8050
Profiling... [6144/50176]	Loss: 1.8884
Profiling... [6656/50176]	Loss: 1.8578
Profile done
epoch 1 train time consumed: 7.93s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4829
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.06397741865476,
                        "time": 5.429532120000658,
                        "accuracy": 0.48291015625,
                        "total_cost": 1967587.777359187
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9895
Profiling... [1024/50176]	Loss: 1.9366
Profiling... [1536/50176]	Loss: 1.9382
Profiling... [2048/50176]	Loss: 1.9027
Profiling... [2560/50176]	Loss: 1.9264
Profiling... [3072/50176]	Loss: 1.9223
Profiling... [3584/50176]	Loss: 1.9324
Profiling... [4096/50176]	Loss: 1.9005
Profiling... [4608/50176]	Loss: 1.9133
Profiling... [5120/50176]	Loss: 1.8281
Profiling... [5632/50176]	Loss: 1.9236
Profiling... [6144/50176]	Loss: 1.9392
Profiling... [6656/50176]	Loss: 1.8200
Profile done
epoch 1 train time consumed: 17.73s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4803
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.96694885701775,
                        "time": 13.08161366299828,
                        "accuracy": 0.4802734375,
                        "total_cost": 4766622.953251915
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0336
Profiling... [1024/50176]	Loss: 2.0519
Profiling... [1536/50176]	Loss: 1.8960
Profiling... [2048/50176]	Loss: 1.8996
Profiling... [2560/50176]	Loss: 1.9989
Profiling... [3072/50176]	Loss: 1.8871
Profiling... [3584/50176]	Loss: 1.9129
Profiling... [4096/50176]	Loss: 1.8159
Profiling... [4608/50176]	Loss: 1.8962
Profiling... [5120/50176]	Loss: 1.9479
Profiling... [5632/50176]	Loss: 1.9169
Profiling... [6144/50176]	Loss: 1.8342
Profiling... [6656/50176]	Loss: 1.8557
Profile done
epoch 1 train time consumed: 7.02s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4760
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.97370812144622,
                        "time": 4.751411156001268,
                        "accuracy": 0.4759765625,
                        "total_cost": 1746928.352801451
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0212
Profiling... [1024/50176]	Loss: 1.9498
Profiling... [1536/50176]	Loss: 1.8908
Profiling... [2048/50176]	Loss: 1.9042
Profiling... [2560/50176]	Loss: 1.8859
Profiling... [3072/50176]	Loss: 1.9205
Profiling... [3584/50176]	Loss: 1.7590
Profiling... [4096/50176]	Loss: 1.8846
Profiling... [4608/50176]	Loss: 1.9128
Profiling... [5120/50176]	Loss: 1.7648
Profiling... [5632/50176]	Loss: 1.7832
Profiling... [6144/50176]	Loss: 1.7475
Profiling... [6656/50176]	Loss: 1.8587
Profile done
epoch 1 train time consumed: 7.20s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4825
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.03410137361007,
                        "time": 4.847419047000585,
                        "accuracy": 0.48251953125,
                        "total_cost": 1758060.0955727682
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7872
Profiling... [1024/50176]	Loss: 1.8530
Profiling... [1536/50176]	Loss: 1.8324
Profiling... [2048/50176]	Loss: 1.9678
Profiling... [2560/50176]	Loss: 1.8269
Profiling... [3072/50176]	Loss: 1.8240
Profiling... [3584/50176]	Loss: 1.9623
Profiling... [4096/50176]	Loss: 1.9356
Profiling... [4608/50176]	Loss: 1.7753
Profiling... [5120/50176]	Loss: 1.9012
Profiling... [5632/50176]	Loss: 1.9962
Profiling... [6144/50176]	Loss: 1.9099
Profiling... [6656/50176]	Loss: 1.8822
Profile done
epoch 1 train time consumed: 7.84s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4791
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.0537644686805,
                        "time": 5.511181207999471,
                        "accuracy": 0.4791015625,
                        "total_cost": 2013052.7363911641
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.8471
Profiling... [1024/50176]	Loss: 1.9733
Profiling... [1536/50176]	Loss: 1.9338
Profiling... [2048/50176]	Loss: 1.9455
Profiling... [2560/50176]	Loss: 1.9645
Profiling... [3072/50176]	Loss: 1.8188
Profiling... [3584/50176]	Loss: 1.8048
Profiling... [4096/50176]	Loss: 1.8914
Profiling... [4608/50176]	Loss: 1.8701
Profiling... [5120/50176]	Loss: 1.8739
Profiling... [5632/50176]	Loss: 1.7975
Profiling... [6144/50176]	Loss: 1.8403
Profiling... [6656/50176]	Loss: 1.8063
Profile done
epoch 1 train time consumed: 18.76s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4812
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.9523240708767,
                        "time": 14.042543747000309,
                        "accuracy": 0.48115234375,
                        "total_cost": 5107415.951821505
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8924
Profiling... [1024/50176]	Loss: 1.9393
Profiling... [1536/50176]	Loss: 1.7963
Profiling... [2048/50176]	Loss: 1.8115
Profiling... [2560/50176]	Loss: 1.9161
Profiling... [3072/50176]	Loss: 1.7660
Profiling... [3584/50176]	Loss: 1.8845
Profiling... [4096/50176]	Loss: 1.8235
Profiling... [4608/50176]	Loss: 1.7835
Profiling... [5120/50176]	Loss: 1.7548
Profiling... [5632/50176]	Loss: 1.7902
Profiling... [6144/50176]	Loss: 1.7244
Profiling... [6656/50176]	Loss: 1.9757
Profile done
epoch 1 train time consumed: 6.98s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4795
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.95695749601342,
                        "time": 4.723469475000456,
                        "accuracy": 0.4794921875,
                        "total_cost": 1723922.0568637105
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9299
Profiling... [1024/50176]	Loss: 1.8735
Profiling... [1536/50176]	Loss: 1.8830
Profiling... [2048/50176]	Loss: 1.8681
Profiling... [2560/50176]	Loss: 2.0032
Profiling... [3072/50176]	Loss: 1.7933
Profiling... [3584/50176]	Loss: 1.8676
Profiling... [4096/50176]	Loss: 1.8317
Profiling... [4608/50176]	Loss: 1.8742
Profiling... [5120/50176]	Loss: 1.8528
Profiling... [5632/50176]	Loss: 1.9113
Profiling... [6144/50176]	Loss: 1.8019
Profiling... [6656/50176]	Loss: 1.8111
Profile done
epoch 1 train time consumed: 7.09s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4809
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.0170752823647,
                        "time": 4.828013518999796,
                        "accuracy": 0.480859375,
                        "total_cost": 1757067.4707651576
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8361
Profiling... [1024/50176]	Loss: 1.8757
Profiling... [1536/50176]	Loss: 1.9736
Profiling... [2048/50176]	Loss: 1.8481
Profiling... [2560/50176]	Loss: 1.8952
Profiling... [3072/50176]	Loss: 1.7721
Profiling... [3584/50176]	Loss: 1.8250
Profiling... [4096/50176]	Loss: 1.8037
Profiling... [4608/50176]	Loss: 1.7672
Profiling... [5120/50176]	Loss: 1.9377
Profiling... [5632/50176]	Loss: 1.7938
Profiling... [6144/50176]	Loss: 1.8201
Profiling... [6656/50176]	Loss: 1.8456
Profile done
epoch 1 train time consumed: 7.96s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4787
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.03505271462222,
                        "time": 5.499294895000276,
                        "accuracy": 0.4787109375,
                        "total_cost": 2010350.1533742337
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.8962
Profiling... [1024/50176]	Loss: 1.9251
Profiling... [1536/50176]	Loss: 1.8638
Profiling... [2048/50176]	Loss: 1.9155
Profiling... [2560/50176]	Loss: 1.9055
Profiling... [3072/50176]	Loss: 1.9924
Profiling... [3584/50176]	Loss: 1.9699
Profiling... [4096/50176]	Loss: 1.8738
Profiling... [4608/50176]	Loss: 1.8415
Profiling... [5120/50176]	Loss: 1.8764
Profiling... [5632/50176]	Loss: 1.7979
Profiling... [6144/50176]	Loss: 1.8904
Profiling... [6656/50176]	Loss: 1.8793
Profile done
epoch 1 train time consumed: 17.74s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4757
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.94084067865587,
                        "time": 13.04201526900033,
                        "accuracy": 0.47568359375,
                        "total_cost": 4798047.908447668
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9852
Profiling... [1024/50176]	Loss: 1.7818
Profiling... [1536/50176]	Loss: 1.9976
Profiling... [2048/50176]	Loss: 1.7999
Profiling... [2560/50176]	Loss: 1.8466
Profiling... [3072/50176]	Loss: 1.8978
Profiling... [3584/50176]	Loss: 1.9464
Profiling... [4096/50176]	Loss: 1.8190
Profiling... [4608/50176]	Loss: 1.8267
Profiling... [5120/50176]	Loss: 1.9268
Profiling... [5632/50176]	Loss: 1.7935
Profiling... [6144/50176]	Loss: 1.9749
Profiling... [6656/50176]	Loss: 1.8521
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4518
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.94339776606935,
                        "time": 4.598787425999035,
                        "accuracy": 0.4517578125,
                        "total_cost": 1781458.5100281604
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8961
Profiling... [1024/50176]	Loss: 1.8334
Profiling... [1536/50176]	Loss: 1.8671
Profiling... [2048/50176]	Loss: 1.8606
Profiling... [2560/50176]	Loss: 1.7493
Profiling... [3072/50176]	Loss: 2.1151
Profiling... [3584/50176]	Loss: 1.8538
Profiling... [4096/50176]	Loss: 1.8818
Profiling... [4608/50176]	Loss: 1.7284
Profiling... [5120/50176]	Loss: 1.7536
Profiling... [5632/50176]	Loss: 1.8227
Profiling... [6144/50176]	Loss: 1.8226
Profiling... [6656/50176]	Loss: 1.8501
Profile done
epoch 1 train time consumed: 7.01s
Validation Epoch: 7, Average loss: 0.0042, Accuracy: 0.4339
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.0069489561728,
                        "time": 4.71867516400016,
                        "accuracy": 0.43388671875,
                        "total_cost": 1903188.362342626
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9689
Profiling... [1024/50176]	Loss: 1.8889
Profiling... [1536/50176]	Loss: 1.9104
Profiling... [2048/50176]	Loss: 1.8108
Profiling... [2560/50176]	Loss: 1.8797
Profiling... [3072/50176]	Loss: 1.7672
Profiling... [3584/50176]	Loss: 1.8679
Profiling... [4096/50176]	Loss: 1.9605
Profiling... [4608/50176]	Loss: 1.7022
Profiling... [5120/50176]	Loss: 1.8488
Profiling... [5632/50176]	Loss: 1.7723
Profiling... [6144/50176]	Loss: 1.8167
Profiling... [6656/50176]	Loss: 1.7364
Profile done
epoch 1 train time consumed: 7.79s
Validation Epoch: 7, Average loss: 0.0045, Accuracy: 0.4168
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.02338213565535,
                        "time": 5.406001532999653,
                        "accuracy": 0.416796875,
                        "total_cost": 2269811.3278199104
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0318
Profiling... [1024/50176]	Loss: 1.9878
Profiling... [1536/50176]	Loss: 1.9127
Profiling... [2048/50176]	Loss: 1.8332
Profiling... [2560/50176]	Loss: 1.7777
Profiling... [3072/50176]	Loss: 1.9546
Profiling... [3584/50176]	Loss: 1.9797
Profiling... [4096/50176]	Loss: 1.8376
Profiling... [4608/50176]	Loss: 1.8087
Profiling... [5120/50176]	Loss: 1.7094
Profiling... [5632/50176]	Loss: 1.8975
Profiling... [6144/50176]	Loss: 1.9045
Profiling... [6656/50176]	Loss: 1.7731
Profile done
epoch 1 train time consumed: 17.77s
Validation Epoch: 7, Average loss: 0.0046, Accuracy: 0.3897
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.93179132810168,
                        "time": 13.040296273000422,
                        "accuracy": 0.38974609375,
                        "total_cost": 5855226.991034015
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9502
Profiling... [1024/50176]	Loss: 1.8435
Profiling... [1536/50176]	Loss: 2.0261
Profiling... [2048/50176]	Loss: 1.9044
Profiling... [2560/50176]	Loss: 1.9998
Profiling... [3072/50176]	Loss: 1.9138
Profiling... [3584/50176]	Loss: 2.0084
Profiling... [4096/50176]	Loss: 1.8509
Profiling... [4608/50176]	Loss: 1.8646
Profiling... [5120/50176]	Loss: 1.8733
Profiling... [5632/50176]	Loss: 1.6604
Profiling... [6144/50176]	Loss: 1.6602
Profiling... [6656/50176]	Loss: 1.8415
Profile done
epoch 1 train time consumed: 6.81s
Validation Epoch: 7, Average loss: 0.0051, Accuracy: 0.3631
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.93543355785786,
                        "time": 4.602501665998716,
                        "accuracy": 0.3630859375,
                        "total_cost": 2218311.7228267076
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8994
Profiling... [1024/50176]	Loss: 1.9050
Profiling... [1536/50176]	Loss: 1.8849
Profiling... [2048/50176]	Loss: 1.7848
Profiling... [2560/50176]	Loss: 1.8518
Profiling... [3072/50176]	Loss: 1.8339
Profiling... [3584/50176]	Loss: 1.8775
Profiling... [4096/50176]	Loss: 1.9162
Profiling... [4608/50176]	Loss: 1.9087
Profiling... [5120/50176]	Loss: 1.7773
Profiling... [5632/50176]	Loss: 1.9529
Profiling... [6144/50176]	Loss: 1.7992
Profiling... [6656/50176]	Loss: 1.8805
Profile done
epoch 1 train time consumed: 7.04s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4480
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.99330897906874,
                        "time": 4.7081809299997985,
                        "accuracy": 0.448046875,
                        "total_cost": 1838940.764289372
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9191
Profiling... [1024/50176]	Loss: 1.9649
Profiling... [1536/50176]	Loss: 1.7965
Profiling... [2048/50176]	Loss: 1.6252
Profiling... [2560/50176]	Loss: 1.7643
Profiling... [3072/50176]	Loss: 1.8993
Profiling... [3584/50176]	Loss: 1.9155
Profiling... [4096/50176]	Loss: 1.7510
Profiling... [4608/50176]	Loss: 2.0154
Profiling... [5120/50176]	Loss: 1.8550
Profiling... [5632/50176]	Loss: 1.8311
Profiling... [6144/50176]	Loss: 1.8277
Profiling... [6656/50176]	Loss: 1.9167
Profile done
epoch 1 train time consumed: 7.81s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4504
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.00875419808533,
                        "time": 5.390580588000375,
                        "accuracy": 0.450390625,
                        "total_cost": 2094518.736707865
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9355
Profiling... [1024/50176]	Loss: 1.9206
Profiling... [1536/50176]	Loss: 1.8805
Profiling... [2048/50176]	Loss: 1.8687
Profiling... [2560/50176]	Loss: 1.8124
Profiling... [3072/50176]	Loss: 1.8439
Profiling... [3584/50176]	Loss: 1.8559
Profiling... [4096/50176]	Loss: 1.7502
Profiling... [4608/50176]	Loss: 1.8475
Profiling... [5120/50176]	Loss: 1.8893
Profiling... [5632/50176]	Loss: 1.9137
Profiling... [6144/50176]	Loss: 1.9493
Profiling... [6656/50176]	Loss: 1.8686
Profile done
epoch 1 train time consumed: 17.67s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4453
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.91839758987476,
                        "time": 13.04165459700016,
                        "accuracy": 0.4453125,
                        "total_cost": 5125141.455663221
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0243
Profiling... [1024/50176]	Loss: 1.7875
Profiling... [1536/50176]	Loss: 1.9254
Profiling... [2048/50176]	Loss: 1.9557
Profiling... [2560/50176]	Loss: 1.8364
Profiling... [3072/50176]	Loss: 1.8317
Profiling... [3584/50176]	Loss: 1.8188
Profiling... [4096/50176]	Loss: 1.8547
Profiling... [4608/50176]	Loss: 1.8413
Profiling... [5120/50176]	Loss: 1.7953
Profiling... [5632/50176]	Loss: 1.9369
Profiling... [6144/50176]	Loss: 1.8462
Profiling... [6656/50176]	Loss: 1.7898
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 7, Average loss: 0.0038, Accuracy: 0.4632
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.9235031844071,
                        "time": 4.613012493999122,
                        "accuracy": 0.46318359375,
                        "total_cost": 1742888.1276083547
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9425
Profiling... [1024/50176]	Loss: 1.9008
Profiling... [1536/50176]	Loss: 2.0523
Profiling... [2048/50176]	Loss: 1.8783
Profiling... [2560/50176]	Loss: 1.8462
Profiling... [3072/50176]	Loss: 1.9532
Profiling... [3584/50176]	Loss: 1.7017
Profiling... [4096/50176]	Loss: 1.7753
Profiling... [4608/50176]	Loss: 1.7584
Profiling... [5120/50176]	Loss: 1.8306
Profiling... [5632/50176]	Loss: 1.9009
Profiling... [6144/50176]	Loss: 1.8123
Profiling... [6656/50176]	Loss: 1.8575
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4572
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.98476569383472,
                        "time": 4.7181060559996695,
                        "accuracy": 0.4572265625,
                        "total_cost": 1805819.3191694592
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7940
Profiling... [1024/50176]	Loss: 1.9002
Profiling... [1536/50176]	Loss: 1.8259
Profiling... [2048/50176]	Loss: 1.9351
Profiling... [2560/50176]	Loss: 1.9725
Profiling... [3072/50176]	Loss: 1.9096
Profiling... [3584/50176]	Loss: 1.8571
Profiling... [4096/50176]	Loss: 1.7770
Profiling... [4608/50176]	Loss: 1.8282
Profiling... [5120/50176]	Loss: 1.9226
Profiling... [5632/50176]	Loss: 1.8290
Profiling... [6144/50176]	Loss: 1.8073
Profiling... [6656/50176]	Loss: 1.7744
Profile done
epoch 1 train time consumed: 7.85s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4474
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.99967237292881,
                        "time": 5.433788838001419,
                        "accuracy": 0.44736328125,
                        "total_cost": 2125594.760466829
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9461
Profiling... [1024/50176]	Loss: 1.8139
Profiling... [1536/50176]	Loss: 1.8007
Profiling... [2048/50176]	Loss: 1.8448
Profiling... [2560/50176]	Loss: 1.8811
Profiling... [3072/50176]	Loss: 1.9134
Profiling... [3584/50176]	Loss: 1.8843
Profiling... [4096/50176]	Loss: 1.9197
Profiling... [4608/50176]	Loss: 1.8693
Profiling... [5120/50176]	Loss: 1.7844
Profiling... [5632/50176]	Loss: 1.8424
Profiling... [6144/50176]	Loss: 1.8885
Profiling... [6656/50176]	Loss: 1.7866
Profile done
epoch 1 train time consumed: 18.65s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4560
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.90337548854532,
                        "time": 13.898523030000433,
                        "accuracy": 0.45595703125,
                        "total_cost": 5334365.6606898215
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0096
Profiling... [1024/50176]	Loss: 1.8664
Profiling... [1536/50176]	Loss: 1.8157
Profiling... [2048/50176]	Loss: 1.9335
Profiling... [2560/50176]	Loss: 2.0159
Profiling... [3072/50176]	Loss: 1.8485
Profiling... [3584/50176]	Loss: 1.8687
Profiling... [4096/50176]	Loss: 1.9100
Profiling... [4608/50176]	Loss: 2.0463
Profiling... [5120/50176]	Loss: 2.0171
Profiling... [5632/50176]	Loss: 1.9029
Profiling... [6144/50176]	Loss: 1.9378
Profiling... [6656/50176]	Loss: 1.9842
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 7, Average loss: 0.0061, Accuracy: 0.2887
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.90779716985188,
                        "time": 4.618707323999843,
                        "accuracy": 0.288671875,
                        "total_cost": 2799974.128757685
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0059
Profiling... [1024/50176]	Loss: 1.9489
Profiling... [1536/50176]	Loss: 1.9467
Profiling... [2048/50176]	Loss: 1.8473
Profiling... [2560/50176]	Loss: 2.1065
Profiling... [3072/50176]	Loss: 1.8765
Profiling... [3584/50176]	Loss: 1.9592
Profiling... [4096/50176]	Loss: 1.8651
Profiling... [4608/50176]	Loss: 1.9439
Profiling... [5120/50176]	Loss: 1.8812
Profiling... [5632/50176]	Loss: 1.8757
Profiling... [6144/50176]	Loss: 1.8557
Profiling... [6656/50176]	Loss: 1.7652
Profile done
epoch 1 train time consumed: 6.98s
Validation Epoch: 7, Average loss: 0.0044, Accuracy: 0.4092
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.96398464403629,
                        "time": 4.722697601000618,
                        "accuracy": 0.4091796875,
                        "total_cost": 2019826.7544136294
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9232
Profiling... [1024/50176]	Loss: 1.8894
Profiling... [1536/50176]	Loss: 1.9714
Profiling... [2048/50176]	Loss: 1.9222
Profiling... [2560/50176]	Loss: 1.8760
Profiling... [3072/50176]	Loss: 2.0169
Profiling... [3584/50176]	Loss: 2.0324
Profiling... [4096/50176]	Loss: 1.8950
Profiling... [4608/50176]	Loss: 1.8750
Profiling... [5120/50176]	Loss: 2.0274
Profiling... [5632/50176]	Loss: 1.8040
Profiling... [6144/50176]	Loss: 1.8949
Profiling... [6656/50176]	Loss: 1.8777
Profile done
epoch 1 train time consumed: 7.91s
Validation Epoch: 7, Average loss: 0.0058, Accuracy: 0.3187
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.97952632418986,
                        "time": 5.460575202001564,
                        "accuracy": 0.31875,
                        "total_cost": 2997962.856000859
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9418
Profiling... [1024/50176]	Loss: 1.8717
Profiling... [1536/50176]	Loss: 1.9377
Profiling... [2048/50176]	Loss: 2.0785
Profiling... [2560/50176]	Loss: 1.9759
Profiling... [3072/50176]	Loss: 1.9693
Profiling... [3584/50176]	Loss: 1.9259
Profiling... [4096/50176]	Loss: 1.7536
Profiling... [4608/50176]	Loss: 2.0948
Profiling... [5120/50176]	Loss: 1.8651
Profiling... [5632/50176]	Loss: 1.7976
Profiling... [6144/50176]	Loss: 1.9208
Profiling... [6656/50176]	Loss: 1.9051
Profile done
epoch 1 train time consumed: 17.69s
Validation Epoch: 7, Average loss: 0.0050, Accuracy: 0.3657
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.8968504528499,
                        "time": 13.022455342001194,
                        "accuracy": 0.36572265625,
                        "total_cost": 6231305.733742627
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9183
Profiling... [1024/50176]	Loss: 1.8334
Profiling... [1536/50176]	Loss: 1.8911
Profiling... [2048/50176]	Loss: 1.9754
Profiling... [2560/50176]	Loss: 1.9670
Profiling... [3072/50176]	Loss: 1.8518
Profiling... [3584/50176]	Loss: 1.8359
Profiling... [4096/50176]	Loss: 1.8099
Profiling... [4608/50176]	Loss: 1.9187
Profiling... [5120/50176]	Loss: 1.8247
Profiling... [5632/50176]	Loss: 1.8627
Profiling... [6144/50176]	Loss: 1.8759
Profiling... [6656/50176]	Loss: 1.8765
Profile done
epoch 1 train time consumed: 7.09s
Validation Epoch: 7, Average loss: 0.0063, Accuracy: 0.2910
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.90341254016731,
                        "time": 4.612184432000504,
                        "accuracy": 0.291015625,
                        "total_cost": 2773501.5107868803
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7940
Profiling... [1024/50176]	Loss: 1.8464
Profiling... [1536/50176]	Loss: 1.9075
Profiling... [2048/50176]	Loss: 1.9840
Profiling... [2560/50176]	Loss: 1.8004
Profiling... [3072/50176]	Loss: 1.9310
Profiling... [3584/50176]	Loss: 1.9898
Profiling... [4096/50176]	Loss: 1.8233
Profiling... [4608/50176]	Loss: 1.9928
Profiling... [5120/50176]	Loss: 1.9905
Profiling... [5632/50176]	Loss: 1.8748
Profiling... [6144/50176]	Loss: 1.9355
Profiling... [6656/50176]	Loss: 1.8845
Profile done
epoch 1 train time consumed: 6.91s
Validation Epoch: 7, Average loss: 0.0061, Accuracy: 0.2959
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.9581443625457,
                        "time": 4.706455621999339,
                        "accuracy": 0.2958984375,
                        "total_cost": 2783487.945420071
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8190
Profiling... [1024/50176]	Loss: 1.9957
Profiling... [1536/50176]	Loss: 1.8117
Profiling... [2048/50176]	Loss: 1.9577
Profiling... [2560/50176]	Loss: 1.9422
Profiling... [3072/50176]	Loss: 1.9782
Profiling... [3584/50176]	Loss: 1.9274
Profiling... [4096/50176]	Loss: 1.9937
Profiling... [4608/50176]	Loss: 1.9328
Profiling... [5120/50176]	Loss: 1.8753
Profiling... [5632/50176]	Loss: 1.7623
Profiling... [6144/50176]	Loss: 1.9010
Profiling... [6656/50176]	Loss: 1.7936
Profile done
epoch 1 train time consumed: 7.87s
Validation Epoch: 7, Average loss: 0.0050, Accuracy: 0.3486
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.97361764063558,
                        "time": 5.4002052239993645,
                        "accuracy": 0.3486328125,
                        "total_cost": 2710691.249693799
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9240
Profiling... [1024/50176]	Loss: 1.9191
Profiling... [1536/50176]	Loss: 1.9995
Profiling... [2048/50176]	Loss: 1.8991
Profiling... [2560/50176]	Loss: 2.0240
Profiling... [3072/50176]	Loss: 1.9459
Profiling... [3584/50176]	Loss: 1.8242
Profiling... [4096/50176]	Loss: 2.0206
Profiling... [4608/50176]	Loss: 1.9118
Profiling... [5120/50176]	Loss: 1.8655
Profiling... [5632/50176]	Loss: 1.9262
Profiling... [6144/50176]	Loss: 1.8960
Profiling... [6656/50176]	Loss: 1.7887
Profile done
epoch 1 train time consumed: 17.77s
Validation Epoch: 7, Average loss: 0.0056, Accuracy: 0.3144
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.88739107217145,
                        "time": 13.036949584999093,
                        "accuracy": 0.31435546875,
                        "total_cost": 7257599.768971225
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0664
Profiling... [1024/50176]	Loss: 1.9034
Profiling... [1536/50176]	Loss: 2.0525
Profiling... [2048/50176]	Loss: 1.8935
Profiling... [2560/50176]	Loss: 1.8855
Profiling... [3072/50176]	Loss: 2.0582
Profiling... [3584/50176]	Loss: 2.0518
Profiling... [4096/50176]	Loss: 1.8893
Profiling... [4608/50176]	Loss: 1.9257
Profiling... [5120/50176]	Loss: 1.8654
Profiling... [5632/50176]	Loss: 1.9451
Profiling... [6144/50176]	Loss: 1.9389
Profiling... [6656/50176]	Loss: 1.8627
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 7, Average loss: 0.0059, Accuracy: 0.3003
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.89295609320648,
                        "time": 4.620852746998935,
                        "accuracy": 0.30029296875,
                        "total_cost": 2692867.6821535258
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7891
Profiling... [1024/50176]	Loss: 1.8254
Profiling... [1536/50176]	Loss: 2.0215
Profiling... [2048/50176]	Loss: 2.0259
Profiling... [2560/50176]	Loss: 1.9727
Profiling... [3072/50176]	Loss: 1.9397
Profiling... [3584/50176]	Loss: 1.8488
Profiling... [4096/50176]	Loss: 1.9971
Profiling... [4608/50176]	Loss: 2.0083
Profiling... [5120/50176]	Loss: 1.9101
Profiling... [5632/50176]	Loss: 1.9649
Profiling... [6144/50176]	Loss: 1.8557
Profiling... [6656/50176]	Loss: 2.0062
Profile done
epoch 1 train time consumed: 7.05s
Validation Epoch: 7, Average loss: 0.0050, Accuracy: 0.3525
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.95123312013547,
                        "time": 4.7162300259988115,
                        "accuracy": 0.3525390625,
                        "total_cost": 2341131.359166169
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8600
Profiling... [1024/50176]	Loss: 1.9428
Profiling... [1536/50176]	Loss: 1.9432
Profiling... [2048/50176]	Loss: 1.8858
Profiling... [2560/50176]	Loss: 1.9406
Profiling... [3072/50176]	Loss: 2.0165
Profiling... [3584/50176]	Loss: 1.8988
Profiling... [4096/50176]	Loss: 1.9181
Profiling... [4608/50176]	Loss: 1.9967
Profiling... [5120/50176]	Loss: 1.8555
Profiling... [5632/50176]	Loss: 1.8280
Profiling... [6144/50176]	Loss: 1.8422
Profiling... [6656/50176]	Loss: 1.9113
Profile done
epoch 1 train time consumed: 7.84s
Validation Epoch: 7, Average loss: 0.0054, Accuracy: 0.3456
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.96649843372566,
                        "time": 5.414235637999809,
                        "accuracy": 0.34560546875,
                        "total_cost": 2741540.057444379
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.8402
Profiling... [1024/50176]	Loss: 1.8541
Profiling... [1536/50176]	Loss: 1.9522
Profiling... [2048/50176]	Loss: 1.9006
Profiling... [2560/50176]	Loss: 1.9754
Profiling... [3072/50176]	Loss: 1.9341
Profiling... [3584/50176]	Loss: 1.8117
Profiling... [4096/50176]	Loss: 1.9441
Profiling... [4608/50176]	Loss: 1.9588
Profiling... [5120/50176]	Loss: 2.0166
Profiling... [5632/50176]	Loss: 1.9991
Profiling... [6144/50176]	Loss: 1.8182
Profiling... [6656/50176]	Loss: 1.8573
Profile done
epoch 1 train time consumed: 17.73s
Validation Epoch: 7, Average loss: 0.0058, Accuracy: 0.3032
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.88297500269071,
                        "time": 13.023219808999784,
                        "accuracy": 0.30322265625,
                        "total_cost": 7516138.453374432
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9818
Profiling... [2048/50176]	Loss: 1.9737
Profiling... [3072/50176]	Loss: 1.8391
Profiling... [4096/50176]	Loss: 1.9151
Profiling... [5120/50176]	Loss: 1.8035
Profiling... [6144/50176]	Loss: 1.8751
Profiling... [7168/50176]	Loss: 1.8512
Profiling... [8192/50176]	Loss: 1.8627
Profiling... [9216/50176]	Loss: 1.7981
Profiling... [10240/50176]	Loss: 1.8321
Profiling... [11264/50176]	Loss: 1.7903
Profiling... [12288/50176]	Loss: 1.6912
Profiling... [13312/50176]	Loss: 1.8462
Profile done
epoch 1 train time consumed: 12.95s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4826
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.92745537280572,
                        "time": 8.972444512999573,
                        "accuracy": 0.4826171875,
                        "total_cost": 3253464.2993312897
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8863
Profiling... [2048/50176]	Loss: 1.9151
Profiling... [3072/50176]	Loss: 1.7969
Profiling... [4096/50176]	Loss: 1.8764
Profiling... [5120/50176]	Loss: 1.9084
Profiling... [6144/50176]	Loss: 1.8309
Profiling... [7168/50176]	Loss: 1.8909
Profiling... [8192/50176]	Loss: 1.9270
Profiling... [9216/50176]	Loss: 1.9128
Profiling... [10240/50176]	Loss: 1.7597
Profiling... [11264/50176]	Loss: 1.8622
Profiling... [12288/50176]	Loss: 1.7927
Profiling... [13312/50176]	Loss: 1.7848
Profile done
epoch 1 train time consumed: 13.27s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4852
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.01530013691603,
                        "time": 9.252751037000053,
                        "accuracy": 0.48515625,
                        "total_cost": 3337546.2677745763
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9091
Profiling... [2048/50176]	Loss: 1.9905
Profiling... [3072/50176]	Loss: 1.9149
Profiling... [4096/50176]	Loss: 1.8381
Profiling... [5120/50176]	Loss: 1.8436
Profiling... [6144/50176]	Loss: 1.7998
Profiling... [7168/50176]	Loss: 1.8805
Profiling... [8192/50176]	Loss: 1.9276
Profiling... [9216/50176]	Loss: 1.8563
Profiling... [10240/50176]	Loss: 1.8124
Profiling... [11264/50176]	Loss: 1.8945
Profiling... [12288/50176]	Loss: 1.8937
Profiling... [13312/50176]	Loss: 1.8207
Profile done
epoch 1 train time consumed: 15.16s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4828
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.0327134881909,
                        "time": 10.711198315999354,
                        "accuracy": 0.4828125,
                        "total_cost": 3882376.088647015
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0142
Profiling... [2048/50176]	Loss: 1.8848
Profiling... [3072/50176]	Loss: 1.8127
Profiling... [4096/50176]	Loss: 1.9111
Profiling... [5120/50176]	Loss: 1.8849
Profiling... [6144/50176]	Loss: 1.9061
Profiling... [7168/50176]	Loss: 1.9364
Profiling... [8192/50176]	Loss: 1.8446
Profiling... [9216/50176]	Loss: 1.8013
Profiling... [10240/50176]	Loss: 1.9320
Profiling... [11264/50176]	Loss: 1.8510
Profiling... [12288/50176]	Loss: 1.8012
Profiling... [13312/50176]	Loss: 1.7021
Profile done
epoch 1 train time consumed: 37.44s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4758
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.86212096810021,
                        "time": 27.680607075000808,
                        "accuracy": 0.47578125,
                        "total_cost": 10181372.717241677
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8810
Profiling... [2048/50176]	Loss: 1.8914
Profiling... [3072/50176]	Loss: 1.8430
Profiling... [4096/50176]	Loss: 1.9372
Profiling... [5120/50176]	Loss: 1.8791
Profiling... [6144/50176]	Loss: 1.8644
Profiling... [7168/50176]	Loss: 1.8069
Profiling... [8192/50176]	Loss: 1.8699
Profiling... [9216/50176]	Loss: 1.9313
Profiling... [10240/50176]	Loss: 1.8727
Profiling... [11264/50176]	Loss: 1.8133
Profiling... [12288/50176]	Loss: 1.8140
Profiling... [13312/50176]	Loss: 1.8615
Profile done
epoch 1 train time consumed: 12.90s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4831
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.92358267404397,
                        "time": 8.923064258999148,
                        "accuracy": 0.48310546875,
                        "total_cost": 3232288.4884023597
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9347
Profiling... [2048/50176]	Loss: 1.9966
Profiling... [3072/50176]	Loss: 1.8636
Profiling... [4096/50176]	Loss: 1.7894
Profiling... [5120/50176]	Loss: 1.9151
Profiling... [6144/50176]	Loss: 1.8670
Profiling... [7168/50176]	Loss: 1.7859
Profiling... [8192/50176]	Loss: 1.8359
Profiling... [9216/50176]	Loss: 1.9459
Profiling... [10240/50176]	Loss: 1.8996
Profiling... [11264/50176]	Loss: 1.8054
Profiling... [12288/50176]	Loss: 1.8091
Profiling... [13312/50176]	Loss: 1.8049
Profile done
epoch 1 train time consumed: 13.30s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4832
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.00713689307312,
                        "time": 9.24508207100007,
                        "accuracy": 0.483203125,
                        "total_cost": 3348259.3110816744
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0143
Profiling... [2048/50176]	Loss: 1.9388
Profiling... [3072/50176]	Loss: 1.8916
Profiling... [4096/50176]	Loss: 1.8903
Profiling... [5120/50176]	Loss: 1.9733
Profiling... [6144/50176]	Loss: 1.8687
Profiling... [7168/50176]	Loss: 1.8353
Profiling... [8192/50176]	Loss: 1.8671
Profiling... [9216/50176]	Loss: 1.8776
Profiling... [10240/50176]	Loss: 1.8944
Profiling... [11264/50176]	Loss: 1.8895
Profiling... [12288/50176]	Loss: 1.8563
Profiling... [13312/50176]	Loss: 1.8567
Profile done
epoch 1 train time consumed: 15.15s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4860
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.02180017819747,
                        "time": 10.725971149000543,
                        "accuracy": 0.48603515625,
                        "total_cost": 3861953.0438032895
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9007
Profiling... [2048/50176]	Loss: 1.9190
Profiling... [3072/50176]	Loss: 1.8426
Profiling... [4096/50176]	Loss: 1.8006
Profiling... [5120/50176]	Loss: 1.9212
Profiling... [6144/50176]	Loss: 1.8354
Profiling... [7168/50176]	Loss: 1.8270
Profiling... [8192/50176]	Loss: 1.8069
Profiling... [9216/50176]	Loss: 1.8788
Profiling... [10240/50176]	Loss: 1.7873
Profiling... [11264/50176]	Loss: 1.8755
Profiling... [12288/50176]	Loss: 1.8330
Profiling... [13312/50176]	Loss: 1.8911
Profile done
epoch 1 train time consumed: 37.68s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4799
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.85622820007549,
                        "time": 28.023657475001528,
                        "accuracy": 0.4798828125,
                        "total_cost": 10219453.438177196
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8690
Profiling... [2048/50176]	Loss: 1.9034
Profiling... [3072/50176]	Loss: 1.8414
Profiling... [4096/50176]	Loss: 1.8824
Profiling... [5120/50176]	Loss: 1.8687
Profiling... [6144/50176]	Loss: 1.9303
Profiling... [7168/50176]	Loss: 1.8502
Profiling... [8192/50176]	Loss: 1.8440
Profiling... [9216/50176]	Loss: 1.8385
Profiling... [10240/50176]	Loss: 1.8851
Profiling... [11264/50176]	Loss: 1.8376
Profiling... [12288/50176]	Loss: 1.7912
Profiling... [13312/50176]	Loss: 1.8277
Profile done
epoch 1 train time consumed: 12.84s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4823
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.91221508241199,
                        "time": 8.956087286998809,
                        "accuracy": 0.48232421875,
                        "total_cost": 3249505.6526223663
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9295
Profiling... [2048/50176]	Loss: 1.8496
Profiling... [3072/50176]	Loss: 1.8419
Profiling... [4096/50176]	Loss: 1.9028
Profiling... [5120/50176]	Loss: 1.8243
Profiling... [6144/50176]	Loss: 1.8994
Profiling... [7168/50176]	Loss: 1.8288
Profiling... [8192/50176]	Loss: 1.8434
Profiling... [9216/50176]	Loss: 1.8803
Profiling... [10240/50176]	Loss: 1.7749
Profiling... [11264/50176]	Loss: 1.7687
Profiling... [12288/50176]	Loss: 1.8391
Profiling... [13312/50176]	Loss: 1.8322
Profile done
epoch 1 train time consumed: 13.19s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4761
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.99284969222649,
                        "time": 9.246261583999512,
                        "accuracy": 0.47607421875,
                        "total_cost": 3398830.924826077
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9228
Profiling... [2048/50176]	Loss: 2.0144
Profiling... [3072/50176]	Loss: 1.9009
Profiling... [4096/50176]	Loss: 1.9375
Profiling... [5120/50176]	Loss: 1.8562
Profiling... [6144/50176]	Loss: 1.8275
Profiling... [7168/50176]	Loss: 1.8228
Profiling... [8192/50176]	Loss: 1.7658
Profiling... [9216/50176]	Loss: 1.7851
Profiling... [10240/50176]	Loss: 1.7803
Profiling... [11264/50176]	Loss: 1.8470
Profiling... [12288/50176]	Loss: 1.8750
Profiling... [13312/50176]	Loss: 1.8239
Profile done
epoch 1 train time consumed: 15.22s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4828
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.00855731411285,
                        "time": 10.706499922000148,
                        "accuracy": 0.4828125,
                        "total_cost": 3880673.110886785
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8507
Profiling... [2048/50176]	Loss: 1.9264
Profiling... [3072/50176]	Loss: 1.9310
Profiling... [4096/50176]	Loss: 1.9618
Profiling... [5120/50176]	Loss: 1.8912
Profiling... [6144/50176]	Loss: 1.8695
Profiling... [7168/50176]	Loss: 1.7571
Profiling... [8192/50176]	Loss: 1.8755
Profiling... [9216/50176]	Loss: 1.7384
Profiling... [10240/50176]	Loss: 1.8077
Profiling... [11264/50176]	Loss: 1.8299
Profiling... [12288/50176]	Loss: 1.7424
Profiling... [13312/50176]	Loss: 1.7930
Profile done
epoch 1 train time consumed: 37.22s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4798
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.85407709597962,
                        "time": 27.767776783000954,
                        "accuracy": 0.47978515625,
                        "total_cost": 10128201.912301589
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9990
Profiling... [2048/50176]	Loss: 1.8261
Profiling... [3072/50176]	Loss: 1.7484
Profiling... [4096/50176]	Loss: 1.7902
Profiling... [5120/50176]	Loss: 1.8235
Profiling... [6144/50176]	Loss: 1.8959
Profiling... [7168/50176]	Loss: 1.7850
Profiling... [8192/50176]	Loss: 1.8395
Profiling... [9216/50176]	Loss: 1.8236
Profiling... [10240/50176]	Loss: 1.8296
Profiling... [11264/50176]	Loss: 1.8118
Profiling... [12288/50176]	Loss: 1.8154
Profiling... [13312/50176]	Loss: 1.8660
Profile done
epoch 1 train time consumed: 12.87s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4228
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.90594257450299,
                        "time": 8.948586988000898,
                        "accuracy": 0.42275390625,
                        "total_cost": 3704289.185146133
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9583
Profiling... [2048/50176]	Loss: 1.8409
Profiling... [3072/50176]	Loss: 1.8814
Profiling... [4096/50176]	Loss: 1.8439
Profiling... [5120/50176]	Loss: 1.8548
Profiling... [6144/50176]	Loss: 1.8270
Profiling... [7168/50176]	Loss: 1.7497
Profiling... [8192/50176]	Loss: 1.7905
Profiling... [9216/50176]	Loss: 1.8322
Profiling... [10240/50176]	Loss: 1.7650
Profiling... [11264/50176]	Loss: 1.7495
Profiling... [12288/50176]	Loss: 1.7414
Profiling... [13312/50176]	Loss: 1.7435
Profile done
epoch 1 train time consumed: 13.27s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4208
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.98704865396276,
                        "time": 9.259621598999729,
                        "accuracy": 0.42080078125,
                        "total_cost": 3850833.582132169
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8649
Profiling... [2048/50176]	Loss: 1.8996
Profiling... [3072/50176]	Loss: 1.8412
Profiling... [4096/50176]	Loss: 1.8507
Profiling... [5120/50176]	Loss: 1.7993
Profiling... [6144/50176]	Loss: 1.8227
Profiling... [7168/50176]	Loss: 1.8678
Profiling... [8192/50176]	Loss: 1.6521
Profiling... [9216/50176]	Loss: 1.8146
Profiling... [10240/50176]	Loss: 1.8315
Profiling... [11264/50176]	Loss: 1.7975
Profiling... [12288/50176]	Loss: 1.7921
Profiling... [13312/50176]	Loss: 1.7078
Profile done
epoch 1 train time consumed: 15.09s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4396
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.00268741983382,
                        "time": 10.697079330000633,
                        "accuracy": 0.43955078125,
                        "total_cost": 4258868.286905385
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0106
Profiling... [2048/50176]	Loss: 1.8855
Profiling... [3072/50176]	Loss: 1.8570
Profiling... [4096/50176]	Loss: 1.7717
Profiling... [5120/50176]	Loss: 1.7800
Profiling... [6144/50176]	Loss: 1.7801
Profiling... [7168/50176]	Loss: 1.7215
Profiling... [8192/50176]	Loss: 1.8201
Profiling... [9216/50176]	Loss: 1.8448
Profiling... [10240/50176]	Loss: 1.7899
Profiling... [11264/50176]	Loss: 1.8855
Profiling... [12288/50176]	Loss: 1.6628
Profiling... [13312/50176]	Loss: 1.7845
Profile done
epoch 1 train time consumed: 38.24s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4665
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.83793389058337,
                        "time": 28.49805560599998,
                        "accuracy": 0.46650390625,
                        "total_cost": 10690499.40254385
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9683
Profiling... [2048/50176]	Loss: 1.8883
Profiling... [3072/50176]	Loss: 1.8198
Profiling... [4096/50176]	Loss: 1.7760
Profiling... [5120/50176]	Loss: 1.7626
Profiling... [6144/50176]	Loss: 1.8832
Profiling... [7168/50176]	Loss: 1.8254
Profiling... [8192/50176]	Loss: 1.8378
Profiling... [9216/50176]	Loss: 1.8175
Profiling... [10240/50176]	Loss: 1.8353
Profiling... [11264/50176]	Loss: 1.7545
Profiling... [12288/50176]	Loss: 1.8087
Profiling... [13312/50176]	Loss: 1.7529
Profile done
epoch 1 train time consumed: 13.01s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4525
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.89168410896457,
                        "time": 8.961289582999598,
                        "accuracy": 0.4525390625,
                        "total_cost": 3465392.950525524
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8465
Profiling... [2048/50176]	Loss: 1.9827
Profiling... [3072/50176]	Loss: 1.7861
Profiling... [4096/50176]	Loss: 1.8865
Profiling... [5120/50176]	Loss: 1.7809
Profiling... [6144/50176]	Loss: 1.7601
Profiling... [7168/50176]	Loss: 1.7906
Profiling... [8192/50176]	Loss: 1.7602
Profiling... [9216/50176]	Loss: 1.8839
Profiling... [10240/50176]	Loss: 1.7643
Profiling... [11264/50176]	Loss: 1.7452
Profiling... [12288/50176]	Loss: 1.7930
Profiling... [13312/50176]	Loss: 1.7281
Profile done
epoch 1 train time consumed: 13.45s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4589
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.96701895092843,
                        "time": 9.260831621999387,
                        "accuracy": 0.45888671875,
                        "total_cost": 3531689.77795763
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9614
Profiling... [2048/50176]	Loss: 1.8955
Profiling... [3072/50176]	Loss: 1.8565
Profiling... [4096/50176]	Loss: 1.8475
Profiling... [5120/50176]	Loss: 1.8020
Profiling... [6144/50176]	Loss: 1.8722
Profiling... [7168/50176]	Loss: 1.8660
Profiling... [8192/50176]	Loss: 1.7593
Profiling... [9216/50176]	Loss: 1.8569
Profiling... [10240/50176]	Loss: 1.7626
Profiling... [11264/50176]	Loss: 1.7654
Profiling... [12288/50176]	Loss: 1.7778
Profiling... [13312/50176]	Loss: 1.9173
Profile done
epoch 1 train time consumed: 15.14s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4446
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.98145238990975,
                        "time": 10.688071671000216,
                        "accuracy": 0.44462890625,
                        "total_cost": 4206682.28298537
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9398
Profiling... [2048/50176]	Loss: 1.8359
Profiling... [3072/50176]	Loss: 1.9007
Profiling... [4096/50176]	Loss: 1.8055
Profiling... [5120/50176]	Loss: 1.8247
Profiling... [6144/50176]	Loss: 1.8518
Profiling... [7168/50176]	Loss: 1.8379
Profiling... [8192/50176]	Loss: 1.8026
Profiling... [9216/50176]	Loss: 1.7964
Profiling... [10240/50176]	Loss: 1.7792
Profiling... [11264/50176]	Loss: 1.8253
Profiling... [12288/50176]	Loss: 1.7460
Profiling... [13312/50176]	Loss: 1.7850
Profile done
epoch 1 train time consumed: 38.03s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4036
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.81321918140422,
                        "time": 28.172343385000204,
                        "accuracy": 0.40361328125,
                        "total_cost": 12215059.120716274
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8780
Profiling... [2048/50176]	Loss: 1.8819
Profiling... [3072/50176]	Loss: 1.8466
Profiling... [4096/50176]	Loss: 1.7277
Profiling... [5120/50176]	Loss: 1.7649
Profiling... [6144/50176]	Loss: 1.8317
Profiling... [7168/50176]	Loss: 1.8793
Profiling... [8192/50176]	Loss: 1.8989
Profiling... [9216/50176]	Loss: 1.8426
Profiling... [10240/50176]	Loss: 1.8545
Profiling... [11264/50176]	Loss: 1.7653
Profiling... [12288/50176]	Loss: 1.8191
Profiling... [13312/50176]	Loss: 1.8358
Profile done
epoch 1 train time consumed: 12.95s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4339
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.86659258397965,
                        "time": 8.952389386000505,
                        "accuracy": 0.43388671875,
                        "total_cost": 3610776.9029288553
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0017
Profiling... [2048/50176]	Loss: 1.8620
Profiling... [3072/50176]	Loss: 1.8494
Profiling... [4096/50176]	Loss: 1.8714
Profiling... [5120/50176]	Loss: 1.8727
Profiling... [6144/50176]	Loss: 1.8588
Profiling... [7168/50176]	Loss: 1.8772
Profiling... [8192/50176]	Loss: 1.8149
Profiling... [9216/50176]	Loss: 1.7798
Profiling... [10240/50176]	Loss: 1.7907
Profiling... [11264/50176]	Loss: 1.7815
Profiling... [12288/50176]	Loss: 1.7968
Profiling... [13312/50176]	Loss: 1.8218
Profile done
epoch 1 train time consumed: 13.27s
Validation Epoch: 7, Average loss: 0.0024, Accuracy: 0.3946
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.94273496578106,
                        "time": 9.246132711999962,
                        "accuracy": 0.39462890625,
                        "total_cost": 4100239.9950269563
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9144
Profiling... [2048/50176]	Loss: 1.8664
Profiling... [3072/50176]	Loss: 1.8241
Profiling... [4096/50176]	Loss: 1.7893
Profiling... [5120/50176]	Loss: 1.8043
Profiling... [6144/50176]	Loss: 1.9117
Profiling... [7168/50176]	Loss: 1.8484
Profiling... [8192/50176]	Loss: 1.7735
Profiling... [9216/50176]	Loss: 1.8627
Profiling... [10240/50176]	Loss: 1.7687
Profiling... [11264/50176]	Loss: 1.8218
Profiling... [12288/50176]	Loss: 1.7886
Profiling... [13312/50176]	Loss: 1.7558
Profile done
epoch 1 train time consumed: 15.17s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4191
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.95632557394711,
                        "time": 10.6956799089985,
                        "accuracy": 0.419140625,
                        "total_cost": 4465670.642340473
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9224
Profiling... [2048/50176]	Loss: 1.9017
Profiling... [3072/50176]	Loss: 1.9164
Profiling... [4096/50176]	Loss: 1.8426
Profiling... [5120/50176]	Loss: 1.8362
Profiling... [6144/50176]	Loss: 1.8399
Profiling... [7168/50176]	Loss: 1.8697
Profiling... [8192/50176]	Loss: 1.7912
Profiling... [9216/50176]	Loss: 1.7837
Profiling... [10240/50176]	Loss: 1.7482
Profiling... [11264/50176]	Loss: 1.8825
Profiling... [12288/50176]	Loss: 1.9004
Profiling... [13312/50176]	Loss: 1.7766
Profile done
epoch 1 train time consumed: 38.25s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4476
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.80087409979066,
                        "time": 28.368292160999772,
                        "accuracy": 0.44755859375,
                        "total_cost": 11092293.160050532
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8712
Profiling... [2048/50176]	Loss: 1.8853
Profiling... [3072/50176]	Loss: 1.9223
Profiling... [4096/50176]	Loss: 1.9304
Profiling... [5120/50176]	Loss: 1.8780
Profiling... [6144/50176]	Loss: 1.9101
Profiling... [7168/50176]	Loss: 1.8423
Profiling... [8192/50176]	Loss: 1.8037
Profiling... [9216/50176]	Loss: 1.9404
Profiling... [10240/50176]	Loss: 1.8262
Profiling... [11264/50176]	Loss: 1.8660
Profiling... [12288/50176]	Loss: 1.8288
Profiling... [13312/50176]	Loss: 1.8451
Profile done
epoch 1 train time consumed: 12.97s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3522
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.84950449664099,
                        "time": 8.950463020000825,
                        "accuracy": 0.35224609375,
                        "total_cost": 4446695.240322007
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9236
Profiling... [2048/50176]	Loss: 1.8440
Profiling... [3072/50176]	Loss: 1.9184
Profiling... [4096/50176]	Loss: 1.8079
Profiling... [5120/50176]	Loss: 1.8953
Profiling... [6144/50176]	Loss: 1.9488
Profiling... [7168/50176]	Loss: 1.8435
Profiling... [8192/50176]	Loss: 1.8974
Profiling... [9216/50176]	Loss: 1.8041
Profiling... [10240/50176]	Loss: 1.8095
Profiling... [11264/50176]	Loss: 1.8760
Profiling... [12288/50176]	Loss: 1.8728
Profiling... [13312/50176]	Loss: 1.9377
Profile done
epoch 1 train time consumed: 13.32s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3421
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.92667257613917,
                        "time": 9.28112166199935,
                        "accuracy": 0.34208984375,
                        "total_cost": 4747864.692635694
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9003
Profiling... [2048/50176]	Loss: 1.8852
Profiling... [3072/50176]	Loss: 1.8521
Profiling... [4096/50176]	Loss: 1.8146
Profiling... [5120/50176]	Loss: 1.9163
Profiling... [6144/50176]	Loss: 1.8869
Profiling... [7168/50176]	Loss: 1.8447
Profiling... [8192/50176]	Loss: 1.8965
Profiling... [9216/50176]	Loss: 1.8136
Profiling... [10240/50176]	Loss: 1.8783
Profiling... [11264/50176]	Loss: 1.9143
Profiling... [12288/50176]	Loss: 1.9991
Profiling... [13312/50176]	Loss: 1.7863
Profile done
epoch 1 train time consumed: 15.12s
Validation Epoch: 7, Average loss: 0.0030, Accuracy: 0.2717
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.94325376678852,
                        "time": 10.693492974000037,
                        "accuracy": 0.2716796875,
                        "total_cost": 6888116.250685861
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8909
Profiling... [2048/50176]	Loss: 1.8179
Profiling... [3072/50176]	Loss: 1.8599
Profiling... [4096/50176]	Loss: 1.7876
Profiling... [5120/50176]	Loss: 1.8525
Profiling... [6144/50176]	Loss: 1.8508
Profiling... [7168/50176]	Loss: 1.8461
Profiling... [8192/50176]	Loss: 1.8611
Profiling... [9216/50176]	Loss: 1.8965
Profiling... [10240/50176]	Loss: 1.9028
Profiling... [11264/50176]	Loss: 1.8104
Profiling... [12288/50176]	Loss: 1.8254
Profiling... [13312/50176]	Loss: 1.8623
Profile done
epoch 1 train time consumed: 37.41s
Validation Epoch: 7, Average loss: 0.0029, Accuracy: 0.2926
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.79454700869621,
                        "time": 27.936053482000716,
                        "accuracy": 0.292578125,
                        "total_cost": 16709415.166804165
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9653
Profiling... [2048/50176]	Loss: 1.9242
Profiling... [3072/50176]	Loss: 1.9466
Profiling... [4096/50176]	Loss: 1.8377
Profiling... [5120/50176]	Loss: 1.8529
Profiling... [6144/50176]	Loss: 1.8544
Profiling... [7168/50176]	Loss: 1.7941
Profiling... [8192/50176]	Loss: 1.9106
Profiling... [9216/50176]	Loss: 1.8406
Profiling... [10240/50176]	Loss: 1.9450
Profiling... [11264/50176]	Loss: 1.7980
Profiling... [12288/50176]	Loss: 1.9225
Profiling... [13312/50176]	Loss: 1.8515
Profile done
epoch 1 train time consumed: 13.52s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.2609
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.83079718663387,
                        "time": 9.01768427800016,
                        "accuracy": 0.2609375,
                        "total_cost": 6047788.258299509
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8780
Profiling... [2048/50176]	Loss: 2.0001
Profiling... [3072/50176]	Loss: 1.8070
Profiling... [4096/50176]	Loss: 1.8637
Profiling... [5120/50176]	Loss: 1.8823
Profiling... [6144/50176]	Loss: 1.9401
Profiling... [7168/50176]	Loss: 1.8445
Profiling... [8192/50176]	Loss: 1.7820
Profiling... [9216/50176]	Loss: 1.8290
Profiling... [10240/50176]	Loss: 1.9292
Profiling... [11264/50176]	Loss: 1.8063
Profiling... [12288/50176]	Loss: 1.8595
Profiling... [13312/50176]	Loss: 1.7687
Profile done
epoch 1 train time consumed: 13.37s
Validation Epoch: 7, Average loss: 0.0024, Accuracy: 0.3677
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.90303201399296,
                        "time": 9.25321112399979,
                        "accuracy": 0.36767578125,
                        "total_cost": 4404184.418116235
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9485
Profiling... [2048/50176]	Loss: 1.8550
Profiling... [3072/50176]	Loss: 1.9203
Profiling... [4096/50176]	Loss: 1.8435
Profiling... [5120/50176]	Loss: 1.9468
Profiling... [6144/50176]	Loss: 1.8688
Profiling... [7168/50176]	Loss: 1.7878
Profiling... [8192/50176]	Loss: 1.7877
Profiling... [9216/50176]	Loss: 1.9132
Profiling... [10240/50176]	Loss: 1.8516
Profiling... [11264/50176]	Loss: 1.8575
Profiling... [12288/50176]	Loss: 1.7971
Profiling... [13312/50176]	Loss: 1.8107
Profile done
epoch 1 train time consumed: 15.09s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4034
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.91738961313699,
                        "time": 10.689436705000844,
                        "accuracy": 0.40341796875,
                        "total_cost": 4637005.706938153
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8315
Profiling... [2048/50176]	Loss: 1.9249
Profiling... [3072/50176]	Loss: 1.9057
Profiling... [4096/50176]	Loss: 1.8793
Profiling... [5120/50176]	Loss: 1.9048
Profiling... [6144/50176]	Loss: 1.9465
Profiling... [7168/50176]	Loss: 1.8372
Profiling... [8192/50176]	Loss: 1.8354
Profiling... [9216/50176]	Loss: 1.8567
Profiling... [10240/50176]	Loss: 1.7703
Profiling... [11264/50176]	Loss: 1.9127
Profiling... [12288/50176]	Loss: 1.8621
Profiling... [13312/50176]	Loss: 1.8141
Profile done
epoch 1 train time consumed: 37.79s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.2665
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.77175714089043,
                        "time": 28.036450182000408,
                        "accuracy": 0.26650390625,
                        "total_cost": 18410157.10008968
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9355
Profiling... [2048/50176]	Loss: 1.8792
Profiling... [3072/50176]	Loss: 1.9154
Profiling... [4096/50176]	Loss: 1.8896
Profiling... [5120/50176]	Loss: 1.7928
Profiling... [6144/50176]	Loss: 1.8647
Profiling... [7168/50176]	Loss: 1.8165
Profiling... [8192/50176]	Loss: 1.8549
Profiling... [9216/50176]	Loss: 1.7697
Profiling... [10240/50176]	Loss: 1.8428
Profiling... [11264/50176]	Loss: 1.9021
Profiling... [12288/50176]	Loss: 1.8310
Profiling... [13312/50176]	Loss: 1.7897
Profile done
epoch 1 train time consumed: 12.96s
Validation Epoch: 7, Average loss: 0.0043, Accuracy: 0.1044
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.82517567438555,
                        "time": 8.9468600600012,
                        "accuracy": 0.10439453125,
                        "total_cost": 14997916.957457576
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9135
Profiling... [2048/50176]	Loss: 1.8058
Profiling... [3072/50176]	Loss: 1.8525
Profiling... [4096/50176]	Loss: 1.9225
Profiling... [5120/50176]	Loss: 1.9255
Profiling... [6144/50176]	Loss: 1.7758
Profiling... [7168/50176]	Loss: 1.9035
Profiling... [8192/50176]	Loss: 1.8192
Profiling... [9216/50176]	Loss: 1.8601
Profiling... [10240/50176]	Loss: 1.7942
Profiling... [11264/50176]	Loss: 1.8565
Profiling... [12288/50176]	Loss: 1.7951
Profiling... [13312/50176]	Loss: 1.7728
Profile done
epoch 1 train time consumed: 13.39s
Validation Epoch: 7, Average loss: 0.0028, Accuracy: 0.2871
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.89574561709911,
                        "time": 9.257223419001093,
                        "accuracy": 0.287109375,
                        "total_cost": 5642498.083962571
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9733
Profiling... [2048/50176]	Loss: 2.0239
Profiling... [3072/50176]	Loss: 1.9131
Profiling... [4096/50176]	Loss: 1.7795
Profiling... [5120/50176]	Loss: 1.9394
Profiling... [6144/50176]	Loss: 1.8272
Profiling... [7168/50176]	Loss: 1.8034
Profiling... [8192/50176]	Loss: 1.8791
Profiling... [9216/50176]	Loss: 1.8331
Profiling... [10240/50176]	Loss: 1.8853
Profiling... [11264/50176]	Loss: 1.8661
Profiling... [12288/50176]	Loss: 1.7117
Profiling... [13312/50176]	Loss: 1.9001
Profile done
epoch 1 train time consumed: 15.19s
Validation Epoch: 7, Average loss: 0.0025, Accuracy: 0.3443
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.90817086746762,
                        "time": 10.70699026299917,
                        "accuracy": 0.3443359375,
                        "total_cost": 5441556.027026237
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9708
Profiling... [2048/50176]	Loss: 1.8468
Profiling... [3072/50176]	Loss: 1.8558
Profiling... [4096/50176]	Loss: 1.8389
Profiling... [5120/50176]	Loss: 1.9632
Profiling... [6144/50176]	Loss: 1.8343
Profiling... [7168/50176]	Loss: 1.8365
Profiling... [8192/50176]	Loss: 1.9197
Profiling... [9216/50176]	Loss: 1.7738
Profiling... [10240/50176]	Loss: 1.9162
Profiling... [11264/50176]	Loss: 1.8969
Profiling... [12288/50176]	Loss: 1.9035
Profiling... [13312/50176]	Loss: 1.7756
Profile done
epoch 1 train time consumed: 38.40s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3389
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.75978047725769,
                        "time": 28.50415826199969,
                        "accuracy": 0.3388671875,
                        "total_cost": 14720303.05634105
                    },
                    
[Training Loop] The optimal parameters are lr: 0.01 dr: 0.5 pl: 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[GPU_0] Set GPU power limit to 125W.
[GPU_0] Set GPU power limit to 125W.
[Training Loop] Model's accuracy 0.41149129746835444 surpasses threshold 0.4! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7959
Profiling... [256/50048]	Loss: 1.9273
Profiling... [384/50048]	Loss: 1.8393
Profiling... [512/50048]	Loss: 1.9997
Profiling... [640/50048]	Loss: 1.9372
Profiling... [768/50048]	Loss: 2.1497
Profiling... [896/50048]	Loss: 1.9405
Profiling... [1024/50048]	Loss: 1.8959
Profiling... [1152/50048]	Loss: 1.8810
Profiling... [1280/50048]	Loss: 1.9805
Profiling... [1408/50048]	Loss: 2.1535
Profiling... [1536/50048]	Loss: 1.7396
Profiling... [1664/50048]	Loss: 1.8169
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 7, Average loss: 0.0150, Accuracy: 0.4738
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.75557664961188,
                        "time": 2.169852257000457,
                        "accuracy": 0.47379351265822783,
                        "total_cost": 801454.9267351303
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.0834
Profiling... [256/50048]	Loss: 2.0530
Profiling... [384/50048]	Loss: 1.8486
Profiling... [512/50048]	Loss: 2.0456
Profiling... [640/50048]	Loss: 1.7991
Profiling... [768/50048]	Loss: 2.0162
Profiling... [896/50048]	Loss: 1.7630
Profiling... [1024/50048]	Loss: 1.9978
Profiling... [1152/50048]	Loss: 1.7439
Profiling... [1280/50048]	Loss: 1.9145
Profiling... [1408/50048]	Loss: 1.8474
Profiling... [1536/50048]	Loss: 1.7050
Profiling... [1664/50048]	Loss: 2.1357
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 7, Average loss: 0.0150, Accuracy: 0.4783
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.78072279171205,
                        "time": 2.1691276069996093,
                        "accuracy": 0.4783425632911392,
                        "total_cost": 793567.958103475
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9744
Profiling... [256/50048]	Loss: 2.0945
Profiling... [384/50048]	Loss: 1.8648
Profiling... [512/50048]	Loss: 1.8623
Profiling... [640/50048]	Loss: 1.9380
Profiling... [768/50048]	Loss: 2.1741
Profiling... [896/50048]	Loss: 2.1171
Profiling... [1024/50048]	Loss: 1.9427
Profiling... [1152/50048]	Loss: 1.7188
Profiling... [1280/50048]	Loss: 1.7996
Profiling... [1408/50048]	Loss: 1.9317
Profiling... [1536/50048]	Loss: 2.0272
Profiling... [1664/50048]	Loss: 1.8774
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 7, Average loss: 0.0150, Accuracy: 0.4791
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.79544769229297,
                        "time": 2.42868611799895,
                        "accuracy": 0.47913370253164556,
                        "total_cost": 887059.433314952
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7709
Profiling... [256/50048]	Loss: 1.9240
Profiling... [384/50048]	Loss: 1.9064
Profiling... [512/50048]	Loss: 1.8985
Profiling... [640/50048]	Loss: 2.0768
Profiling... [768/50048]	Loss: 1.9668
Profiling... [896/50048]	Loss: 2.0806
Profiling... [1024/50048]	Loss: 1.9614
Profiling... [1152/50048]	Loss: 1.9591
Profiling... [1280/50048]	Loss: 2.0128
Profiling... [1408/50048]	Loss: 1.9103
Profiling... [1536/50048]	Loss: 2.0301
Profiling... [1664/50048]	Loss: 2.0499
Profile done
epoch 1 train time consumed: 7.07s
Validation Epoch: 7, Average loss: 0.0151, Accuracy: 0.4766
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.76726262881999,
                        "time": 4.981326171000546,
                        "accuracy": 0.4765625,
                        "total_cost": 1829208.2988592172
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1469
Profiling... [256/50048]	Loss: 2.0374
Profiling... [384/50048]	Loss: 1.8665
Profiling... [512/50048]	Loss: 2.2345
Profiling... [640/50048]	Loss: 2.2314
Profiling... [768/50048]	Loss: 2.1735
Profiling... [896/50048]	Loss: 1.8612
Profiling... [1024/50048]	Loss: 1.9194
Profiling... [1152/50048]	Loss: 2.2367
Profiling... [1280/50048]	Loss: 2.1921
Profiling... [1408/50048]	Loss: 1.9743
Profiling... [1536/50048]	Loss: 1.9673
Profiling... [1664/50048]	Loss: 2.0198
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 7, Average loss: 0.0151, Accuracy: 0.4772
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.73176692053734,
                        "time": 2.1592659710004227,
                        "accuracy": 0.4771558544303797,
                        "total_cost": 791924.779747637
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1639
Profiling... [256/50048]	Loss: 2.2194
Profiling... [384/50048]	Loss: 1.9681
Profiling... [512/50048]	Loss: 1.7405
Profiling... [640/50048]	Loss: 1.9956
Profiling... [768/50048]	Loss: 1.9934
Profiling... [896/50048]	Loss: 2.0247
Profiling... [1024/50048]	Loss: 2.0701
Profiling... [1152/50048]	Loss: 1.9572
Profiling... [1280/50048]	Loss: 1.8684
Profiling... [1408/50048]	Loss: 1.7015
Profiling... [1536/50048]	Loss: 1.8687
Profiling... [1664/50048]	Loss: 1.7602
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 7, Average loss: 0.0151, Accuracy: 0.4758
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.75608880463959,
                        "time": 2.1625428719999036,
                        "accuracy": 0.47577136075949367,
                        "total_cost": 795434.6011829203
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1229
Profiling... [256/50048]	Loss: 1.8650
Profiling... [384/50048]	Loss: 1.8577
Profiling... [512/50048]	Loss: 1.9829
Profiling... [640/50048]	Loss: 2.1757
Profiling... [768/50048]	Loss: 2.0231
Profiling... [896/50048]	Loss: 2.0491
Profiling... [1024/50048]	Loss: 1.9228
Profiling... [1152/50048]	Loss: 2.0770
Profiling... [1280/50048]	Loss: 2.1117
Profiling... [1408/50048]	Loss: 2.0804
Profiling... [1536/50048]	Loss: 2.0700
Profiling... [1664/50048]	Loss: 1.8919
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 7, Average loss: 0.0152, Accuracy: 0.4709
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.77214093418526,
                        "time": 2.4147991680001724,
                        "accuracy": 0.4709256329113924,
                        "total_cost": 897360.0604143438
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9661
Profiling... [256/50048]	Loss: 1.8979
Profiling... [384/50048]	Loss: 2.3128
Profiling... [512/50048]	Loss: 2.1353
Profiling... [640/50048]	Loss: 1.9246
Profiling... [768/50048]	Loss: 1.9483
Profiling... [896/50048]	Loss: 2.0212
Profiling... [1024/50048]	Loss: 2.0980
Profiling... [1152/50048]	Loss: 2.0223
Profiling... [1280/50048]	Loss: 1.8610
Profiling... [1408/50048]	Loss: 2.3099
Profiling... [1536/50048]	Loss: 1.9515
Profiling... [1664/50048]	Loss: 2.0452
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 7, Average loss: 0.0152, Accuracy: 0.4734
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.74459734275001,
                        "time": 5.047578162000718,
                        "accuracy": 0.47339794303797467,
                        "total_cost": 1865927.3690153481
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1643
Profiling... [256/50048]	Loss: 1.8133
Profiling... [384/50048]	Loss: 1.7724
Profiling... [512/50048]	Loss: 1.8681
Profiling... [640/50048]	Loss: 2.0680
Profiling... [768/50048]	Loss: 1.9106
Profiling... [896/50048]	Loss: 1.7725
Profiling... [1024/50048]	Loss: 2.0082
Profiling... [1152/50048]	Loss: 2.0324
Profiling... [1280/50048]	Loss: 2.0795
Profiling... [1408/50048]	Loss: 1.9820
Profiling... [1536/50048]	Loss: 1.7894
Profiling... [1664/50048]	Loss: 1.8839
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 7, Average loss: 0.0152, Accuracy: 0.4709
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.71206393076444,
                        "time": 2.1635166380001465,
                        "accuracy": 0.4709256329113924,
                        "total_cost": 803981.319320676
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1337
Profiling... [256/50048]	Loss: 1.9679
Profiling... [384/50048]	Loss: 2.0442
Profiling... [512/50048]	Loss: 1.8342
Profiling... [640/50048]	Loss: 2.1853
Profiling... [768/50048]	Loss: 1.9682
Profiling... [896/50048]	Loss: 1.8439
Profiling... [1024/50048]	Loss: 2.0481
Profiling... [1152/50048]	Loss: 2.0448
Profiling... [1280/50048]	Loss: 2.0026
Profiling... [1408/50048]	Loss: 1.7600
Profiling... [1536/50048]	Loss: 1.7840
Profiling... [1664/50048]	Loss: 2.0277
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 7, Average loss: 0.0152, Accuracy: 0.4736
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.73994657141183,
                        "time": 2.161143115001323,
                        "accuracy": 0.4735957278481013,
                        "total_cost": 798571.4880572858
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6998
Profiling... [256/50048]	Loss: 1.9681
Profiling... [384/50048]	Loss: 1.8856
Profiling... [512/50048]	Loss: 1.9925
Profiling... [640/50048]	Loss: 1.9219
Profiling... [768/50048]	Loss: 1.9819
Profiling... [896/50048]	Loss: 2.0369
Profiling... [1024/50048]	Loss: 2.0756
Profiling... [1152/50048]	Loss: 1.9360
Profiling... [1280/50048]	Loss: 2.1324
Profiling... [1408/50048]	Loss: 1.8793
Profiling... [1536/50048]	Loss: 2.0038
Profiling... [1664/50048]	Loss: 2.0818
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 7, Average loss: 0.0151, Accuracy: 0.4797
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.75403059198001,
                        "time": 2.4303238350003085,
                        "accuracy": 0.47972705696202533,
                        "total_cost": 886559.690458987
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8776
Profiling... [256/50048]	Loss: 1.9350
Profiling... [384/50048]	Loss: 2.0032
Profiling... [512/50048]	Loss: 1.8750
Profiling... [640/50048]	Loss: 2.0629
Profiling... [768/50048]	Loss: 1.8495
Profiling... [896/50048]	Loss: 1.8683
Profiling... [1024/50048]	Loss: 1.8830
Profiling... [1152/50048]	Loss: 2.0146
Profiling... [1280/50048]	Loss: 2.0995
Profiling... [1408/50048]	Loss: 1.8057
Profiling... [1536/50048]	Loss: 1.9510
Profiling... [1664/50048]	Loss: 2.0215
Profile done
epoch 1 train time consumed: 7.50s
Validation Epoch: 7, Average loss: 0.0152, Accuracy: 0.4747
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.72639356428319,
                        "time": 5.419729617000485,
                        "accuracy": 0.47468354430379744,
                        "total_cost": 1998073.652134179
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1096
Profiling... [256/50048]	Loss: 2.1383
Profiling... [384/50048]	Loss: 1.9764
Profiling... [512/50048]	Loss: 1.9032
Profiling... [640/50048]	Loss: 1.9182
Profiling... [768/50048]	Loss: 2.0030
Profiling... [896/50048]	Loss: 1.8790
Profiling... [1024/50048]	Loss: 1.7358
Profiling... [1152/50048]	Loss: 2.0194
Profiling... [1280/50048]	Loss: 2.0925
Profiling... [1408/50048]	Loss: 2.1579
Profiling... [1536/50048]	Loss: 2.3093
Profiling... [1664/50048]	Loss: 1.9684
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 7, Average loss: 0.0172, Accuracy: 0.4302
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.69141368231726,
                        "time": 2.573542558999179,
                        "accuracy": 0.43018196202531644,
                        "total_cost": 1046928.9453804478
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9229
Profiling... [256/50048]	Loss: 1.7107
Profiling... [384/50048]	Loss: 2.2064
Profiling... [512/50048]	Loss: 2.0574
Profiling... [640/50048]	Loss: 2.0048
Profiling... [768/50048]	Loss: 2.0066
Profiling... [896/50048]	Loss: 1.9333
Profiling... [1024/50048]	Loss: 1.8918
Profiling... [1152/50048]	Loss: 1.8244
Profiling... [1280/50048]	Loss: 2.2371
Profiling... [1408/50048]	Loss: 1.9624
Profiling... [1536/50048]	Loss: 1.9669
Profiling... [1664/50048]	Loss: 1.9314
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 7, Average loss: 0.0201, Accuracy: 0.3466
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.71045890503939,
                        "time": 2.5405850400002237,
                        "accuracy": 0.34661787974683544,
                        "total_cost": 1282687.3856731516
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0221
Profiling... [256/50048]	Loss: 1.6065
Profiling... [384/50048]	Loss: 2.1148
Profiling... [512/50048]	Loss: 1.9896
Profiling... [640/50048]	Loss: 2.1236
Profiling... [768/50048]	Loss: 1.9087
Profiling... [896/50048]	Loss: 1.9677
Profiling... [1024/50048]	Loss: 1.9349
Profiling... [1152/50048]	Loss: 1.9458
Profiling... [1280/50048]	Loss: 2.0764
Profiling... [1408/50048]	Loss: 2.0083
Profiling... [1536/50048]	Loss: 1.9249
Profiling... [1664/50048]	Loss: 2.0007
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 7, Average loss: 0.0158, Accuracy: 0.4538
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.72231573082038,
                        "time": 2.6893943909999507,
                        "accuracy": 0.45381724683544306,
                        "total_cost": 1037078.29904413
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0203
Profiling... [256/50048]	Loss: 2.0861
Profiling... [384/50048]	Loss: 1.8397
Profiling... [512/50048]	Loss: 2.1920
Profiling... [640/50048]	Loss: 2.0665
Profiling... [768/50048]	Loss: 2.1264
Profiling... [896/50048]	Loss: 1.9294
Profiling... [1024/50048]	Loss: 2.2081
Profiling... [1152/50048]	Loss: 1.8323
Profiling... [1280/50048]	Loss: 2.0561
Profiling... [1408/50048]	Loss: 1.8790
Profiling... [1536/50048]	Loss: 1.8921
Profiling... [1664/50048]	Loss: 1.8863
Profile done
epoch 1 train time consumed: 6.64s
Validation Epoch: 7, Average loss: 0.0167, Accuracy: 0.4286
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.6960775838211,
                        "time": 4.548859568998523,
                        "accuracy": 0.4285996835443038,
                        "total_cost": 1857328.5402168406
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1068
Profiling... [256/50048]	Loss: 1.8595
Profiling... [384/50048]	Loss: 2.1136
Profiling... [512/50048]	Loss: 2.0409
Profiling... [640/50048]	Loss: 2.0999
Profiling... [768/50048]	Loss: 1.9250
Profiling... [896/50048]	Loss: 2.0624
Profiling... [1024/50048]	Loss: 2.1446
Profiling... [1152/50048]	Loss: 1.9544
Profiling... [1280/50048]	Loss: 1.9366
Profiling... [1408/50048]	Loss: 1.9322
Profiling... [1536/50048]	Loss: 1.7719
Profiling... [1664/50048]	Loss: 1.7816
Profile done
epoch 1 train time consumed: 4.21s
Validation Epoch: 7, Average loss: 0.0160, Accuracy: 0.4504
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.66020385968609,
                        "time": 2.5375354699990567,
                        "accuracy": 0.45035601265822783,
                        "total_cost": 986039.255096691
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9786
Profiling... [256/50048]	Loss: 1.9594
Profiling... [384/50048]	Loss: 1.8643
Profiling... [512/50048]	Loss: 1.8913
Profiling... [640/50048]	Loss: 2.2332
Profiling... [768/50048]	Loss: 2.0750
Profiling... [896/50048]	Loss: 2.0358
Profiling... [1024/50048]	Loss: 1.8587
Profiling... [1152/50048]	Loss: 1.9215
Profiling... [1280/50048]	Loss: 1.8999
Profiling... [1408/50048]	Loss: 2.0219
Profiling... [1536/50048]	Loss: 1.9812
Profiling... [1664/50048]	Loss: 2.1606
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 7, Average loss: 0.0170, Accuracy: 0.4258
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.67758900298973,
                        "time": 2.547556451001583,
                        "accuracy": 0.42583069620253167,
                        "total_cost": 1046947.4908714355
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9513
Profiling... [256/50048]	Loss: 2.0854
Profiling... [384/50048]	Loss: 1.8094
Profiling... [512/50048]	Loss: 2.0858
Profiling... [640/50048]	Loss: 2.0867
Profiling... [768/50048]	Loss: 1.9686
Profiling... [896/50048]	Loss: 2.0605
Profiling... [1024/50048]	Loss: 2.0125
Profiling... [1152/50048]	Loss: 1.9567
Profiling... [1280/50048]	Loss: 2.1256
Profiling... [1408/50048]	Loss: 2.0185
Profiling... [1536/50048]	Loss: 1.8212
Profiling... [1664/50048]	Loss: 2.0343
Profile done
epoch 1 train time consumed: 4.15s
Validation Epoch: 7, Average loss: 0.0168, Accuracy: 0.4381
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.68886716001732,
                        "time": 2.6893607290003274,
                        "accuracy": 0.4380933544303797,
                        "total_cost": 1074287.3015889346
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.1648
Profiling... [256/50048]	Loss: 1.9884
Profiling... [384/50048]	Loss: 2.1083
Profiling... [512/50048]	Loss: 2.0315
Profiling... [640/50048]	Loss: 1.7313
Profiling... [768/50048]	Loss: 1.8637
Profiling... [896/50048]	Loss: 1.9956
Profiling... [1024/50048]	Loss: 1.9947
Profiling... [1152/50048]	Loss: 2.0740
Profiling... [1280/50048]	Loss: 2.0386
Profiling... [1408/50048]	Loss: 1.9125
Profiling... [1536/50048]	Loss: 2.1787
Profiling... [1664/50048]	Loss: 1.8648
Profile done
epoch 1 train time consumed: 6.73s
Validation Epoch: 7, Average loss: 0.0180, Accuracy: 0.4133
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.66228243612973,
                        "time": 4.539923444999658,
                        "accuracy": 0.41327136075949367,
                        "total_cost": 1922433.244381765
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0479
Profiling... [256/50048]	Loss: 1.9986
Profiling... [384/50048]	Loss: 2.1365
Profiling... [512/50048]	Loss: 1.9538
Profiling... [640/50048]	Loss: 1.9249
Profiling... [768/50048]	Loss: 1.9682
Profiling... [896/50048]	Loss: 1.9722
Profiling... [1024/50048]	Loss: 2.2331
Profiling... [1152/50048]	Loss: 2.1123
Profiling... [1280/50048]	Loss: 1.9876
Profiling... [1408/50048]	Loss: 1.9408
Profiling... [1536/50048]	Loss: 1.9789
Profiling... [1664/50048]	Loss: 1.9222
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 7, Average loss: 0.0176, Accuracy: 0.4168
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.62539031066574,
                        "time": 2.5325216140008706,
                        "accuracy": 0.41683148734177217,
                        "total_cost": 1063238.4930334378
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1665
Profiling... [256/50048]	Loss: 1.9191
Profiling... [384/50048]	Loss: 2.0083
Profiling... [512/50048]	Loss: 2.2344
Profiling... [640/50048]	Loss: 1.7840
Profiling... [768/50048]	Loss: 1.9745
Profiling... [896/50048]	Loss: 2.0713
Profiling... [1024/50048]	Loss: 2.0952
Profiling... [1152/50048]	Loss: 1.9644
Profiling... [1280/50048]	Loss: 1.8941
Profiling... [1408/50048]	Loss: 2.3875
Profiling... [1536/50048]	Loss: 1.9850
Profiling... [1664/50048]	Loss: 2.0447
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 7, Average loss: 0.0186, Accuracy: 0.3954
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.64489614670205,
                        "time": 2.541390433001652,
                        "accuracy": 0.395371835443038,
                        "total_cost": 1124873.5643421018
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8938
Profiling... [256/50048]	Loss: 2.0830
Profiling... [384/50048]	Loss: 1.8686
Profiling... [512/50048]	Loss: 1.9512
Profiling... [640/50048]	Loss: 2.1013
Profiling... [768/50048]	Loss: 2.0196
Profiling... [896/50048]	Loss: 1.9540
Profiling... [1024/50048]	Loss: 1.9179
Profiling... [1152/50048]	Loss: 2.0118
Profiling... [1280/50048]	Loss: 1.7896
Profiling... [1408/50048]	Loss: 2.0551
Profiling... [1536/50048]	Loss: 1.9079
Profiling... [1664/50048]	Loss: 1.8827
Profile done
epoch 1 train time consumed: 4.11s
Validation Epoch: 7, Average loss: 0.0162, Accuracy: 0.4446
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.65721006464334,
                        "time": 2.701613303001068,
                        "accuracy": 0.44462025316455694,
                        "total_cost": 1063339.613209673
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7626
Profiling... [256/50048]	Loss: 2.0872
Profiling... [384/50048]	Loss: 1.9113
Profiling... [512/50048]	Loss: 1.9393
Profiling... [640/50048]	Loss: 2.0216
Profiling... [768/50048]	Loss: 2.1133
Profiling... [896/50048]	Loss: 2.0125
Profiling... [1024/50048]	Loss: 1.8940
Profiling... [1152/50048]	Loss: 2.3276
Profiling... [1280/50048]	Loss: 2.0846
Profiling... [1408/50048]	Loss: 2.1637
Profiling... [1536/50048]	Loss: 2.2118
Profiling... [1664/50048]	Loss: 1.8115
Profile done
epoch 1 train time consumed: 6.64s
Validation Epoch: 7, Average loss: 0.0176, Accuracy: 0.3993
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.63213558070002,
                        "time": 4.556706134999331,
                        "accuracy": 0.3993275316455696,
                        "total_cost": 1996916.091256765
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7819
Profiling... [256/50048]	Loss: 2.0037
Profiling... [384/50048]	Loss: 2.0321
Profiling... [512/50048]	Loss: 1.9524
Profiling... [640/50048]	Loss: 2.2149
Profiling... [768/50048]	Loss: 2.1778
Profiling... [896/50048]	Loss: 1.9687
Profiling... [1024/50048]	Loss: 2.5025
Profiling... [1152/50048]	Loss: 2.0329
Profiling... [1280/50048]	Loss: 2.1741
Profiling... [1408/50048]	Loss: 2.1509
Profiling... [1536/50048]	Loss: 2.1416
Profiling... [1664/50048]	Loss: 2.0812
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 7, Average loss: 0.0281, Accuracy: 0.2412
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.59826192385609,
                        "time": 2.1592997259995172,
                        "accuracy": 0.24119857594936708,
                        "total_cost": 1566665.3526563123
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7993
Profiling... [256/50048]	Loss: 2.0453
Profiling... [384/50048]	Loss: 1.8730
Profiling... [512/50048]	Loss: 2.2143
Profiling... [640/50048]	Loss: 2.3163
Profiling... [768/50048]	Loss: 2.1902
Profiling... [896/50048]	Loss: 2.5410
Profiling... [1024/50048]	Loss: 2.1786
Profiling... [1152/50048]	Loss: 1.8491
Profiling... [1280/50048]	Loss: 2.2948
Profiling... [1408/50048]	Loss: 2.0102
Profiling... [1536/50048]	Loss: 2.2571
Profiling... [1664/50048]	Loss: 1.8812
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 7, Average loss: 0.0202, Accuracy: 0.3635
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.62261228161115,
                        "time": 2.1752842020014214,
                        "accuracy": 0.3635284810126582,
                        "total_cost": 1047166.1925630347
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0750
Profiling... [256/50048]	Loss: 1.9195
Profiling... [384/50048]	Loss: 1.7934
Profiling... [512/50048]	Loss: 1.9650
Profiling... [640/50048]	Loss: 1.9814
Profiling... [768/50048]	Loss: 2.2268
Profiling... [896/50048]	Loss: 2.2523
Profiling... [1024/50048]	Loss: 1.9553
Profiling... [1152/50048]	Loss: 2.2832
Profiling... [1280/50048]	Loss: 2.0332
Profiling... [1408/50048]	Loss: 2.0257
Profiling... [1536/50048]	Loss: 2.3329
Profiling... [1664/50048]	Loss: 2.0698
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 7, Average loss: 0.0236, Accuracy: 0.3011
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.63775461967772,
                        "time": 2.439033312999527,
                        "accuracy": 0.3011273734177215,
                        "total_cost": 1417442.808106392
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9613
Profiling... [256/50048]	Loss: 2.2000
Profiling... [384/50048]	Loss: 2.1933
Profiling... [512/50048]	Loss: 2.1695
Profiling... [640/50048]	Loss: 2.5917
Profiling... [768/50048]	Loss: 2.3099
Profiling... [896/50048]	Loss: 2.0442
Profiling... [1024/50048]	Loss: 2.3000
Profiling... [1152/50048]	Loss: 2.0206
Profiling... [1280/50048]	Loss: 2.2538
Profiling... [1408/50048]	Loss: 2.2888
Profiling... [1536/50048]	Loss: 2.1449
Profiling... [1664/50048]	Loss: 2.1294
Profile done
epoch 1 train time consumed: 7.51s
Validation Epoch: 7, Average loss: 0.0210, Accuracy: 0.3421
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.61004834987688,
                        "time": 5.438691580000523,
                        "accuracy": 0.34206882911392406,
                        "total_cost": 2782396.247461383
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.2056
Profiling... [256/50048]	Loss: 1.8683
Profiling... [384/50048]	Loss: 1.9086
Profiling... [512/50048]	Loss: 2.2740
Profiling... [640/50048]	Loss: 2.1433
Profiling... [768/50048]	Loss: 2.4451
Profiling... [896/50048]	Loss: 2.2387
Profiling... [1024/50048]	Loss: 2.1995
Profiling... [1152/50048]	Loss: 2.1658
Profiling... [1280/50048]	Loss: 2.2669
Profiling... [1408/50048]	Loss: 2.2007
Profiling... [1536/50048]	Loss: 2.1868
Profiling... [1664/50048]	Loss: 2.1747
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 7, Average loss: 0.0204, Accuracy: 0.3500
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.57853790004616,
                        "time": 2.159607149000294,
                        "accuracy": 0.34998022151898733,
                        "total_cost": 1079864.5975899748
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.0458
Profiling... [256/50048]	Loss: 2.0725
Profiling... [384/50048]	Loss: 2.1024
Profiling... [512/50048]	Loss: 1.9169
Profiling... [640/50048]	Loss: 2.1846
Profiling... [768/50048]	Loss: 2.0389
Profiling... [896/50048]	Loss: 2.1543
Profiling... [1024/50048]	Loss: 1.9231
Profiling... [1152/50048]	Loss: 2.0787
Profiling... [1280/50048]	Loss: 2.0568
Profiling... [1408/50048]	Loss: 2.2362
Profiling... [1536/50048]	Loss: 2.2422
Profiling... [1664/50048]	Loss: 2.2989
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 7, Average loss: 0.0277, Accuracy: 0.2570
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.60354146039732,
                        "time": 2.166665861999718,
                        "accuracy": 0.25702136075949367,
                        "total_cost": 1475233.5165043096
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1190
Profiling... [256/50048]	Loss: 1.8924
Profiling... [384/50048]	Loss: 2.0987
Profiling... [512/50048]	Loss: 2.0837
Profiling... [640/50048]	Loss: 2.3364
Profiling... [768/50048]	Loss: 2.2124
Profiling... [896/50048]	Loss: 2.0161
Profiling... [1024/50048]	Loss: 2.2877
Profiling... [1152/50048]	Loss: 2.2490
Profiling... [1280/50048]	Loss: 2.4369
Profiling... [1408/50048]	Loss: 2.0115
Profiling... [1536/50048]	Loss: 2.0659
Profiling... [1664/50048]	Loss: 2.1149
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 7, Average loss: 0.0260, Accuracy: 0.2928
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.6169913226124,
                        "time": 2.4348147139990033,
                        "accuracy": 0.29282041139240506,
                        "total_cost": 1455132.76524574
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0257
Profiling... [256/50048]	Loss: 1.8325
Profiling... [384/50048]	Loss: 2.1268
Profiling... [512/50048]	Loss: 1.9037
Profiling... [640/50048]	Loss: 2.1464
Profiling... [768/50048]	Loss: 1.9255
Profiling... [896/50048]	Loss: 2.1155
Profiling... [1024/50048]	Loss: 2.1385
Profiling... [1152/50048]	Loss: 1.9658
Profiling... [1280/50048]	Loss: 2.0253
Profiling... [1408/50048]	Loss: 2.1121
Profiling... [1536/50048]	Loss: 2.0276
Profiling... [1664/50048]	Loss: 2.0388
Profile done
epoch 1 train time consumed: 7.45s
Validation Epoch: 7, Average loss: 0.0296, Accuracy: 0.2182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.59217825203326,
                        "time": 5.418863528999282,
                        "accuracy": 0.21815664556962025,
                        "total_cost": 4346881.641394891
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0748
Profiling... [256/50048]	Loss: 1.8137
Profiling... [384/50048]	Loss: 2.0944
Profiling... [512/50048]	Loss: 2.0454
Profiling... [640/50048]	Loss: 2.2645
Profiling... [768/50048]	Loss: 2.1009
Profiling... [896/50048]	Loss: 2.1602
Profiling... [1024/50048]	Loss: 2.0949
Profiling... [1152/50048]	Loss: 2.1129
Profiling... [1280/50048]	Loss: 2.3016
Profiling... [1408/50048]	Loss: 1.9343
Profiling... [1536/50048]	Loss: 1.8941
Profiling... [1664/50048]	Loss: 2.3296
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 7, Average loss: 0.0236, Accuracy: 0.2921
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.56063205488985,
                        "time": 2.155875265998475,
                        "accuracy": 0.292128164556962,
                        "total_cost": 1291481.6759346314
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8278
Profiling... [256/50048]	Loss: 2.1452
Profiling... [384/50048]	Loss: 2.1318
Profiling... [512/50048]	Loss: 2.1895
Profiling... [640/50048]	Loss: 2.1739
Profiling... [768/50048]	Loss: 2.0075
Profiling... [896/50048]	Loss: 1.9676
Profiling... [1024/50048]	Loss: 2.2166
Profiling... [1152/50048]	Loss: 2.2608
Profiling... [1280/50048]	Loss: 2.3434
Profiling... [1408/50048]	Loss: 2.5715
Profiling... [1536/50048]	Loss: 2.2885
Profiling... [1664/50048]	Loss: 2.0309
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 7, Average loss: 0.0236, Accuracy: 0.2978
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.58534178613914,
                        "time": 2.1724174520004453,
                        "accuracy": 0.2977650316455696,
                        "total_cost": 1276755.2052673493
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0272
Profiling... [256/50048]	Loss: 2.2551
Profiling... [384/50048]	Loss: 2.3005
Profiling... [512/50048]	Loss: 1.9580
Profiling... [640/50048]	Loss: 2.0257
Profiling... [768/50048]	Loss: 2.5249
Profiling... [896/50048]	Loss: 2.2759
Profiling... [1024/50048]	Loss: 2.1456
Profiling... [1152/50048]	Loss: 2.0780
Profiling... [1280/50048]	Loss: 2.2551
Profiling... [1408/50048]	Loss: 1.9688
Profiling... [1536/50048]	Loss: 2.1972
Profiling... [1664/50048]	Loss: 2.2212
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 7, Average loss: 0.0214, Accuracy: 0.3085
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.60063849096743,
                        "time": 2.4248389980002685,
                        "accuracy": 0.30854430379746833,
                        "total_cost": 1375318.9393786138
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0086
Profiling... [256/50048]	Loss: 2.1376
Profiling... [384/50048]	Loss: 1.9653
Profiling... [512/50048]	Loss: 2.0399
Profiling... [640/50048]	Loss: 2.1501
Profiling... [768/50048]	Loss: 2.1657
Profiling... [896/50048]	Loss: 2.1424
Profiling... [1024/50048]	Loss: 2.2874
Profiling... [1152/50048]	Loss: 1.9344
Profiling... [1280/50048]	Loss: 2.3663
Profiling... [1408/50048]	Loss: 2.0280
Profiling... [1536/50048]	Loss: 2.0807
Profiling... [1664/50048]	Loss: 2.3209
Profile done
epoch 1 train time consumed: 7.58s
Validation Epoch: 7, Average loss: 0.0235, Accuracy: 0.3132
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.57221391753451,
                        "time": 5.438070070000322,
                        "accuracy": 0.31319224683544306,
                        "total_cost": 3038588.189413504
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9530
Profiling... [512/50176]	Loss: 2.1349
Profiling... [768/50176]	Loss: 1.9831
Profiling... [1024/50176]	Loss: 1.9017
Profiling... [1280/50176]	Loss: 1.8764
Profiling... [1536/50176]	Loss: 1.8203
Profiling... [1792/50176]	Loss: 1.9743
Profiling... [2048/50176]	Loss: 1.9971
Profiling... [2304/50176]	Loss: 1.9330
Profiling... [2560/50176]	Loss: 1.9637
Profiling... [2816/50176]	Loss: 2.0242
Profiling... [3072/50176]	Loss: 1.9217
Profiling... [3328/50176]	Loss: 1.8491
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4758
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.54566370228967,
                        "time": 2.405997116000435,
                        "accuracy": 0.47578125,
                        "total_cost": 884964.4564599302
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1786
Profiling... [512/50176]	Loss: 1.9019
Profiling... [768/50176]	Loss: 2.0466
Profiling... [1024/50176]	Loss: 2.0474
Profiling... [1280/50176]	Loss: 1.9768
Profiling... [1536/50176]	Loss: 2.1064
Profiling... [1792/50176]	Loss: 2.0841
Profiling... [2048/50176]	Loss: 2.0844
Profiling... [2304/50176]	Loss: 1.9248
Profiling... [2560/50176]	Loss: 2.0747
Profiling... [2816/50176]	Loss: 1.9835
Profiling... [3072/50176]	Loss: 1.9450
Profiling... [3328/50176]	Loss: 1.9196
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4747
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.57250332012367,
                        "time": 2.4455138400007854,
                        "accuracy": 0.47470703125,
                        "total_cost": 901534.8284882549
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9528
Profiling... [512/50176]	Loss: 2.0689
Profiling... [768/50176]	Loss: 1.7706
Profiling... [1024/50176]	Loss: 2.1315
Profiling... [1280/50176]	Loss: 1.7199
Profiling... [1536/50176]	Loss: 1.9119
Profiling... [1792/50176]	Loss: 1.9260
Profiling... [2048/50176]	Loss: 1.9996
Profiling... [2304/50176]	Loss: 1.9530
Profiling... [2560/50176]	Loss: 1.9848
Profiling... [2816/50176]	Loss: 2.0203
Profiling... [3072/50176]	Loss: 2.1721
Profiling... [3328/50176]	Loss: 2.0042
Profile done
epoch 1 train time consumed: 4.36s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4775
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.58582844755688,
                        "time": 2.782704957000533,
                        "accuracy": 0.4775390625,
                        "total_cost": 1019756.0905817903
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1472
Profiling... [512/50176]	Loss: 2.1417
Profiling... [768/50176]	Loss: 1.9641
Profiling... [1024/50176]	Loss: 2.0126
Profiling... [1280/50176]	Loss: 2.1015
Profiling... [1536/50176]	Loss: 1.9637
Profiling... [1792/50176]	Loss: 1.9997
Profiling... [2048/50176]	Loss: 1.9216
Profiling... [2304/50176]	Loss: 1.9681
Profiling... [2560/50176]	Loss: 1.8606
Profiling... [2816/50176]	Loss: 2.0059
Profiling... [3072/50176]	Loss: 1.9801
Profiling... [3328/50176]	Loss: 1.9252
Profile done
epoch 1 train time consumed: 9.80s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4773
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.55112333239327,
                        "time": 7.033012788999258,
                        "accuracy": 0.47734375,
                        "total_cost": 2578387.6673254236
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0102
Profiling... [512/50176]	Loss: 1.9841
Profiling... [768/50176]	Loss: 1.9230
Profiling... [1024/50176]	Loss: 2.0303
Profiling... [1280/50176]	Loss: 1.8038
Profiling... [1536/50176]	Loss: 2.2963
Profiling... [1792/50176]	Loss: 1.9569
Profiling... [2048/50176]	Loss: 1.9201
Profiling... [2304/50176]	Loss: 2.0775
Profiling... [2560/50176]	Loss: 1.7803
Profiling... [2816/50176]	Loss: 2.0600
Profiling... [3072/50176]	Loss: 1.8922
Profiling... [3328/50176]	Loss: 1.8588
Profile done
epoch 1 train time consumed: 4.18s
Validation Epoch: 7, Average loss: 0.0075, Accuracy: 0.4752
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.53009939470019,
                        "time": 2.397616216001552,
                        "accuracy": 0.4751953125,
                        "total_cost": 882969.2271012702
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0156
Profiling... [512/50176]	Loss: 1.9614
Profiling... [768/50176]	Loss: 1.9202
Profiling... [1024/50176]	Loss: 1.9050
Profiling... [1280/50176]	Loss: 1.8629
Profiling... [1536/50176]	Loss: 1.9616
Profiling... [1792/50176]	Loss: 2.0450
Profiling... [2048/50176]	Loss: 1.9716
Profiling... [2304/50176]	Loss: 2.0415
Profiling... [2560/50176]	Loss: 1.8972
Profiling... [2816/50176]	Loss: 2.0065
Profiling... [3072/50176]	Loss: 2.0072
Profiling... [3328/50176]	Loss: 1.8011
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 7, Average loss: 0.0075, Accuracy: 0.4692
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.55673495695395,
                        "time": 2.4444367269989016,
                        "accuracy": 0.46923828125,
                        "total_cost": 911640.0863230035
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9519
Profiling... [512/50176]	Loss: 1.9599
Profiling... [768/50176]	Loss: 2.1696
Profiling... [1024/50176]	Loss: 2.0180
Profiling... [1280/50176]	Loss: 1.9125
Profiling... [1536/50176]	Loss: 2.0390
Profiling... [1792/50176]	Loss: 1.9007
Profiling... [2048/50176]	Loss: 1.9377
Profiling... [2304/50176]	Loss: 1.8451
Profiling... [2560/50176]	Loss: 1.8939
Profiling... [2816/50176]	Loss: 2.0641
Profiling... [3072/50176]	Loss: 2.0752
Profiling... [3328/50176]	Loss: 1.8203
Profile done
epoch 1 train time consumed: 4.26s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4792
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.57021218255444,
                        "time": 2.780150364000292,
                        "accuracy": 0.47919921875,
                        "total_cost": 1015290.2898488941
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9598
Profiling... [512/50176]	Loss: 1.9970
Profiling... [768/50176]	Loss: 1.9436
Profiling... [1024/50176]	Loss: 1.8233
Profiling... [1280/50176]	Loss: 2.0917
Profiling... [1536/50176]	Loss: 2.0297
Profiling... [1792/50176]	Loss: 1.9332
Profiling... [2048/50176]	Loss: 2.0349
Profiling... [2304/50176]	Loss: 1.9440
Profiling... [2560/50176]	Loss: 2.0289
Profiling... [2816/50176]	Loss: 1.9203
Profiling... [3072/50176]	Loss: 1.8978
Profiling... [3328/50176]	Loss: 1.9701
Profile done
epoch 1 train time consumed: 9.85s
Validation Epoch: 7, Average loss: 0.0075, Accuracy: 0.4726
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.53626683437493,
                        "time": 7.134549357000651,
                        "accuracy": 0.47255859375,
                        "total_cost": 2642098.046651202
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8620
Profiling... [512/50176]	Loss: 1.9707
Profiling... [768/50176]	Loss: 1.9430
Profiling... [1024/50176]	Loss: 2.1298
Profiling... [1280/50176]	Loss: 1.9568
Profiling... [1536/50176]	Loss: 2.1011
Profiling... [1792/50176]	Loss: 1.9637
Profiling... [2048/50176]	Loss: 2.0324
Profiling... [2304/50176]	Loss: 1.8914
Profiling... [2560/50176]	Loss: 1.9054
Profiling... [2816/50176]	Loss: 1.8837
Profiling... [3072/50176]	Loss: 1.8132
Profiling... [3328/50176]	Loss: 1.8367
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4780
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.5133180831015,
                        "time": 2.4074344860000565,
                        "accuracy": 0.47802734375,
                        "total_cost": 881332.5023313792
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8745
Profiling... [512/50176]	Loss: 2.1484
Profiling... [768/50176]	Loss: 1.9457
Profiling... [1024/50176]	Loss: 2.0856
Profiling... [1280/50176]	Loss: 2.1590
Profiling... [1536/50176]	Loss: 1.7139
Profiling... [1792/50176]	Loss: 2.2074
Profiling... [2048/50176]	Loss: 2.0184
Profiling... [2304/50176]	Loss: 1.9872
Profiling... [2560/50176]	Loss: 1.9948
Profiling... [2816/50176]	Loss: 2.1238
Profiling... [3072/50176]	Loss: 2.0333
Profiling... [3328/50176]	Loss: 1.8717
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4774
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.53900295083473,
                        "time": 2.4499050280010124,
                        "accuracy": 0.47744140625,
                        "total_cost": 897981.1434194753
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0142
Profiling... [512/50176]	Loss: 1.9758
Profiling... [768/50176]	Loss: 2.0190
Profiling... [1024/50176]	Loss: 1.8333
Profiling... [1280/50176]	Loss: 2.0969
Profiling... [1536/50176]	Loss: 1.8747
Profiling... [1792/50176]	Loss: 1.8324
Profiling... [2048/50176]	Loss: 1.8225
Profiling... [2304/50176]	Loss: 1.9408
Profiling... [2560/50176]	Loss: 1.8938
Profiling... [2816/50176]	Loss: 1.9385
Profiling... [3072/50176]	Loss: 1.9255
Profiling... [3328/50176]	Loss: 2.0115
Profile done
epoch 1 train time consumed: 4.26s
Validation Epoch: 7, Average loss: 0.0075, Accuracy: 0.4745
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.55152218469674,
                        "time": 2.7921014680014196,
                        "accuracy": 0.47451171875,
                        "total_cost": 1029727.4810986919
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0947
Profiling... [512/50176]	Loss: 2.1131
Profiling... [768/50176]	Loss: 1.8878
Profiling... [1024/50176]	Loss: 2.2412
Profiling... [1280/50176]	Loss: 2.0793
Profiling... [1536/50176]	Loss: 2.0013
Profiling... [1792/50176]	Loss: 1.9704
Profiling... [2048/50176]	Loss: 1.8690
Profiling... [2304/50176]	Loss: 1.9043
Profiling... [2560/50176]	Loss: 1.7224
Profiling... [2816/50176]	Loss: 1.9166
Profiling... [3072/50176]	Loss: 1.8097
Profiling... [3328/50176]	Loss: 1.7369
Profile done
epoch 1 train time consumed: 9.80s
Validation Epoch: 7, Average loss: 0.0075, Accuracy: 0.4748
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.51642675702418,
                        "time": 7.112228776999473,
                        "accuracy": 0.4748046875,
                        "total_cost": 2621372.679634524
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9682
Profiling... [512/50176]	Loss: 1.9109
Profiling... [768/50176]	Loss: 1.9096
Profiling... [1024/50176]	Loss: 2.1128
Profiling... [1280/50176]	Loss: 2.0585
Profiling... [1536/50176]	Loss: 1.9691
Profiling... [1792/50176]	Loss: 1.7994
Profiling... [2048/50176]	Loss: 1.9581
Profiling... [2304/50176]	Loss: 2.0079
Profiling... [2560/50176]	Loss: 1.9450
Profiling... [2816/50176]	Loss: 2.1199
Profiling... [3072/50176]	Loss: 1.8573
Profiling... [3328/50176]	Loss: 1.9610
Profile done
epoch 1 train time consumed: 3.81s
Validation Epoch: 7, Average loss: 0.0088, Accuracy: 0.4143
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.49630436353262,
                        "time": 2.398781993999364,
                        "accuracy": 0.4142578125,
                        "total_cost": 1013346.8489502264
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9536
Profiling... [512/50176]	Loss: 2.0357
Profiling... [768/50176]	Loss: 2.0550
Profiling... [1024/50176]	Loss: 1.8646
Profiling... [1280/50176]	Loss: 1.9003
Profiling... [1536/50176]	Loss: 1.9143
Profiling... [1792/50176]	Loss: 1.9419
Profiling... [2048/50176]	Loss: 1.9359
Profiling... [2304/50176]	Loss: 1.9537
Profiling... [2560/50176]	Loss: 1.9441
Profiling... [2816/50176]	Loss: 1.8207
Profiling... [3072/50176]	Loss: 1.9793
Profiling... [3328/50176]	Loss: 2.2347
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 7, Average loss: 0.0078, Accuracy: 0.4510
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.52105689039762,
                        "time": 2.4648937980000483,
                        "accuracy": 0.4509765625,
                        "total_cost": 956494.085321803
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9034
Profiling... [512/50176]	Loss: 1.8778
Profiling... [768/50176]	Loss: 2.0485
Profiling... [1024/50176]	Loss: 1.9142
Profiling... [1280/50176]	Loss: 1.8583
Profiling... [1536/50176]	Loss: 1.8786
Profiling... [1792/50176]	Loss: 2.0234
Profiling... [2048/50176]	Loss: 2.0132
Profiling... [2304/50176]	Loss: 1.9185
Profiling... [2560/50176]	Loss: 1.8386
Profiling... [2816/50176]	Loss: 2.0177
Profiling... [3072/50176]	Loss: 2.1300
Profiling... [3328/50176]	Loss: 1.7827
Profile done
epoch 1 train time consumed: 4.26s
Validation Epoch: 7, Average loss: 0.0085, Accuracy: 0.4254
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.5330111927358,
                        "time": 2.7832363939996867,
                        "accuracy": 0.425390625,
                        "total_cost": 1144986.138211074
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9874
Profiling... [512/50176]	Loss: 2.1978
Profiling... [768/50176]	Loss: 1.9401
Profiling... [1024/50176]	Loss: 1.8695
Profiling... [1280/50176]	Loss: 1.8795
Profiling... [1536/50176]	Loss: 1.8349
Profiling... [1792/50176]	Loss: 1.7556
Profiling... [2048/50176]	Loss: 1.9223
Profiling... [2304/50176]	Loss: 1.8880
Profiling... [2560/50176]	Loss: 2.1611
Profiling... [2816/50176]	Loss: 2.1388
Profiling... [3072/50176]	Loss: 1.9797
Profiling... [3328/50176]	Loss: 1.8314
Profile done
epoch 1 train time consumed: 9.91s
Validation Epoch: 7, Average loss: 0.0088, Accuracy: 0.4128
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.49775544781974,
                        "time": 7.150678121000965,
                        "accuracy": 0.41279296875,
                        "total_cost": 3031467.989787965
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9907
Profiling... [512/50176]	Loss: 2.0320
Profiling... [768/50176]	Loss: 1.9928
Profiling... [1024/50176]	Loss: 1.8780
Profiling... [1280/50176]	Loss: 1.9122
Profiling... [1536/50176]	Loss: 2.1413
Profiling... [1792/50176]	Loss: 1.9629
Profiling... [2048/50176]	Loss: 1.9528
Profiling... [2304/50176]	Loss: 1.9323
Profiling... [2560/50176]	Loss: 1.9760
Profiling... [2816/50176]	Loss: 2.0600
Profiling... [3072/50176]	Loss: 1.9633
Profiling... [3328/50176]	Loss: 1.9310
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 7, Average loss: 0.0094, Accuracy: 0.3800
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.47758213709388,
                        "time": 2.400438189999477,
                        "accuracy": 0.37998046875,
                        "total_cost": 1105521.7775582273
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9408
Profiling... [512/50176]	Loss: 1.8745
Profiling... [768/50176]	Loss: 2.1736
Profiling... [1024/50176]	Loss: 1.8817
Profiling... [1280/50176]	Loss: 1.9111
Profiling... [1536/50176]	Loss: 1.8417
Profiling... [1792/50176]	Loss: 2.0729
Profiling... [2048/50176]	Loss: 1.9564
Profiling... [2304/50176]	Loss: 1.8592
Profiling... [2560/50176]	Loss: 1.9804
Profiling... [2816/50176]	Loss: 1.9274
Profiling... [3072/50176]	Loss: 1.9295
Profiling... [3328/50176]	Loss: 2.0221
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 7, Average loss: 0.0086, Accuracy: 0.4295
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.50507457422125,
                        "time": 2.446355088000928,
                        "accuracy": 0.4294921875,
                        "total_cost": 996786.793473775
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0742
Profiling... [512/50176]	Loss: 1.9897
Profiling... [768/50176]	Loss: 1.8292
Profiling... [1024/50176]	Loss: 1.9594
Profiling... [1280/50176]	Loss: 2.1030
Profiling... [1536/50176]	Loss: 2.0722
Profiling... [1792/50176]	Loss: 2.0895
Profiling... [2048/50176]	Loss: 2.0211
Profiling... [2304/50176]	Loss: 1.9597
Profiling... [2560/50176]	Loss: 2.0567
Profiling... [2816/50176]	Loss: 2.0265
Profiling... [3072/50176]	Loss: 1.9381
Profiling... [3328/50176]	Loss: 2.1508
Profile done
epoch 1 train time consumed: 4.38s
Validation Epoch: 7, Average loss: 0.0077, Accuracy: 0.4588
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.51605341448192,
                        "time": 2.7890148070000578,
                        "accuracy": 0.4587890625,
                        "total_cost": 1063838.7684427637
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1082
Profiling... [512/50176]	Loss: 1.9638
Profiling... [768/50176]	Loss: 1.8309
Profiling... [1024/50176]	Loss: 1.9719
Profiling... [1280/50176]	Loss: 1.9073
Profiling... [1536/50176]	Loss: 1.9485
Profiling... [1792/50176]	Loss: 1.9349
Profiling... [2048/50176]	Loss: 2.1280
Profiling... [2304/50176]	Loss: 1.8802
Profiling... [2560/50176]	Loss: 2.0113
Profiling... [2816/50176]	Loss: 1.9756
Profiling... [3072/50176]	Loss: 1.8738
Profiling... [3328/50176]	Loss: 1.8370
Profile done
epoch 1 train time consumed: 9.84s
Validation Epoch: 7, Average loss: 0.0080, Accuracy: 0.4388
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.48255947837863,
                        "time": 7.122279803999845,
                        "accuracy": 0.43876953125,
                        "total_cost": 2840668.9091403787
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0297
Profiling... [512/50176]	Loss: 1.9456
Profiling... [768/50176]	Loss: 2.0946
Profiling... [1024/50176]	Loss: 2.0372
Profiling... [1280/50176]	Loss: 1.9389
Profiling... [1536/50176]	Loss: 2.0729
Profiling... [1792/50176]	Loss: 2.0673
Profiling... [2048/50176]	Loss: 1.8732
Profiling... [2304/50176]	Loss: 2.0584
Profiling... [2560/50176]	Loss: 1.9655
Profiling... [2816/50176]	Loss: 1.8841
Profiling... [3072/50176]	Loss: 1.9210
Profiling... [3328/50176]	Loss: 1.9040
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 7, Average loss: 0.0096, Accuracy: 0.3710
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.46190216565455,
                        "time": 2.4011052919995564,
                        "accuracy": 0.37099609375,
                        "total_cost": 1132608.761059017
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9201
Profiling... [512/50176]	Loss: 2.1895
Profiling... [768/50176]	Loss: 2.0152
Profiling... [1024/50176]	Loss: 1.8886
Profiling... [1280/50176]	Loss: 1.9976
Profiling... [1536/50176]	Loss: 2.0431
Profiling... [1792/50176]	Loss: 1.7789
Profiling... [2048/50176]	Loss: 1.8944
Profiling... [2304/50176]	Loss: 1.7937
Profiling... [2560/50176]	Loss: 1.9183
Profiling... [2816/50176]	Loss: 1.9002
Profiling... [3072/50176]	Loss: 1.9127
Profiling... [3328/50176]	Loss: 2.0522
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 7, Average loss: 0.0084, Accuracy: 0.4344
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.4866201555206,
                        "time": 2.456855803999133,
                        "accuracy": 0.434375,
                        "total_cost": 989812.4102442551
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1514
Profiling... [512/50176]	Loss: 1.9645
Profiling... [768/50176]	Loss: 2.0750
Profiling... [1024/50176]	Loss: 1.8779
Profiling... [1280/50176]	Loss: 1.9196
Profiling... [1536/50176]	Loss: 2.0021
Profiling... [1792/50176]	Loss: 1.9933
Profiling... [2048/50176]	Loss: 2.0206
Profiling... [2304/50176]	Loss: 2.0073
Profiling... [2560/50176]	Loss: 1.9333
Profiling... [2816/50176]	Loss: 2.1720
Profiling... [3072/50176]	Loss: 1.8844
Profiling... [3328/50176]	Loss: 1.8872
Profile done
epoch 1 train time consumed: 4.35s
Validation Epoch: 7, Average loss: 0.0083, Accuracy: 0.4217
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.49967186999838,
                        "time": 2.7919392549993063,
                        "accuracy": 0.4216796875,
                        "total_cost": 1158674.1882720604
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9041
Profiling... [512/50176]	Loss: 2.0663
Profiling... [768/50176]	Loss: 2.0852
Profiling... [1024/50176]	Loss: 2.0092
Profiling... [1280/50176]	Loss: 2.0916
Profiling... [1536/50176]	Loss: 2.0539
Profiling... [1792/50176]	Loss: 2.0324
Profiling... [2048/50176]	Loss: 1.8756
Profiling... [2304/50176]	Loss: 1.9707
Profiling... [2560/50176]	Loss: 2.0345
Profiling... [2816/50176]	Loss: 1.8121
Profiling... [3072/50176]	Loss: 2.0114
Profiling... [3328/50176]	Loss: 2.0033
Profile done
epoch 1 train time consumed: 9.85s
Validation Epoch: 7, Average loss: 0.0081, Accuracy: 0.4449
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.4645864168798,
                        "time": 7.136382238000806,
                        "accuracy": 0.444921875,
                        "total_cost": 2806935.2437439514
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.1768
Profiling... [512/50176]	Loss: 1.9619
Profiling... [768/50176]	Loss: 1.9531
Profiling... [1024/50176]	Loss: 2.0275
Profiling... [1280/50176]	Loss: 1.9383
Profiling... [1536/50176]	Loss: 2.0524
Profiling... [1792/50176]	Loss: 2.0362
Profiling... [2048/50176]	Loss: 2.1360
Profiling... [2304/50176]	Loss: 2.0920
Profiling... [2560/50176]	Loss: 1.9255
Profiling... [2816/50176]	Loss: 1.9897
Profiling... [3072/50176]	Loss: 2.0493
Profiling... [3328/50176]	Loss: 2.0207
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 7, Average loss: 0.0109, Accuracy: 0.3121
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.44417854328414,
                        "time": 2.398575618000905,
                        "accuracy": 0.312109375,
                        "total_cost": 1344883.4503934984
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1620
Profiling... [512/50176]	Loss: 1.8908
Profiling... [768/50176]	Loss: 2.0923
Profiling... [1024/50176]	Loss: 2.0499
Profiling... [1280/50176]	Loss: 2.2408
Profiling... [1536/50176]	Loss: 1.9393
Profiling... [1792/50176]	Loss: 1.9295
Profiling... [2048/50176]	Loss: 2.0624
Profiling... [2304/50176]	Loss: 2.2118
Profiling... [2560/50176]	Loss: 2.0557
Profiling... [2816/50176]	Loss: 2.1014
Profiling... [3072/50176]	Loss: 2.2043
Profiling... [3328/50176]	Loss: 2.1249
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 7, Average loss: 0.0095, Accuracy: 0.3745
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.47033474954709,
                        "time": 2.4472024610004155,
                        "accuracy": 0.37451171875,
                        "total_cost": 1143516.769260168
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7937
Profiling... [512/50176]	Loss: 2.0343
Profiling... [768/50176]	Loss: 2.0582
Profiling... [1024/50176]	Loss: 1.8445
Profiling... [1280/50176]	Loss: 2.1191
Profiling... [1536/50176]	Loss: 1.9516
Profiling... [1792/50176]	Loss: 2.1570
Profiling... [2048/50176]	Loss: 1.9956
Profiling... [2304/50176]	Loss: 2.0799
Profiling... [2560/50176]	Loss: 2.1343
Profiling... [2816/50176]	Loss: 2.1565
Profiling... [3072/50176]	Loss: 1.9274
Profiling... [3328/50176]	Loss: 1.9136
Profile done
epoch 1 train time consumed: 4.30s
Validation Epoch: 7, Average loss: 0.0098, Accuracy: 0.3731
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.4841059948058,
                        "time": 2.787972203999743,
                        "accuracy": 0.37314453125,
                        "total_cost": 1307523.2110880762
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0307
Profiling... [512/50176]	Loss: 2.0472
Profiling... [768/50176]	Loss: 1.7211
Profiling... [1024/50176]	Loss: 2.2607
Profiling... [1280/50176]	Loss: 2.1265
Profiling... [1536/50176]	Loss: 2.1494
Profiling... [1792/50176]	Loss: 2.3967
Profiling... [2048/50176]	Loss: 1.9019
Profiling... [2304/50176]	Loss: 2.0607
Profiling... [2560/50176]	Loss: 1.9082
Profiling... [2816/50176]	Loss: 2.1406
Profiling... [3072/50176]	Loss: 2.0985
Profiling... [3328/50176]	Loss: 2.0001
Profile done
epoch 1 train time consumed: 9.91s
Validation Epoch: 7, Average loss: 0.0129, Accuracy: 0.2250
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.44981633168928,
                        "time": 6.879673350000303,
                        "accuracy": 0.225,
                        "total_cost": 5350857.050000235
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9256
Profiling... [512/50176]	Loss: 2.1656
Profiling... [768/50176]	Loss: 2.1620
Profiling... [1024/50176]	Loss: 2.1226
Profiling... [1280/50176]	Loss: 1.9848
Profiling... [1536/50176]	Loss: 1.9561
Profiling... [1792/50176]	Loss: 2.0302
Profiling... [2048/50176]	Loss: 1.9944
Profiling... [2304/50176]	Loss: 2.2122
Profiling... [2560/50176]	Loss: 1.9497
Profiling... [2816/50176]	Loss: 2.0021
Profiling... [3072/50176]	Loss: 2.0998
Profiling... [3328/50176]	Loss: 1.9631
Profile done
epoch 1 train time consumed: 4.04s
Validation Epoch: 7, Average loss: 0.0098, Accuracy: 0.3484
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.43094846026679,
                        "time": 2.392734406999807,
                        "accuracy": 0.3484375,
                        "total_cost": 1201732.078851921
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8677
Profiling... [512/50176]	Loss: 2.0551
Profiling... [768/50176]	Loss: 2.0177
Profiling... [1024/50176]	Loss: 2.1324
Profiling... [1280/50176]	Loss: 1.9661
Profiling... [1536/50176]	Loss: 1.9965
Profiling... [1792/50176]	Loss: 2.2627
Profiling... [2048/50176]	Loss: 2.1192
Profiling... [2304/50176]	Loss: 2.0350
Profiling... [2560/50176]	Loss: 2.1018
Profiling... [2816/50176]	Loss: 1.9524
Profiling... [3072/50176]	Loss: 1.9763
Profiling... [3328/50176]	Loss: 1.9518
Profile done
epoch 1 train time consumed: 4.01s
Validation Epoch: 7, Average loss: 0.0162, Accuracy: 0.1765
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.45530654252775,
                        "time": 2.454811747000349,
                        "accuracy": 0.17646484375,
                        "total_cost": 2434434.2283478836
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0492
Profiling... [512/50176]	Loss: 2.2058
Profiling... [768/50176]	Loss: 2.1079
Profiling... [1024/50176]	Loss: 1.9098
Profiling... [1280/50176]	Loss: 2.2146
Profiling... [1536/50176]	Loss: 1.8570
Profiling... [1792/50176]	Loss: 2.0497
Profiling... [2048/50176]	Loss: 1.9031
Profiling... [2304/50176]	Loss: 1.9923
Profiling... [2560/50176]	Loss: 2.0250
Profiling... [2816/50176]	Loss: 2.0189
Profiling... [3072/50176]	Loss: 2.1043
Profiling... [3328/50176]	Loss: 1.9588
Profile done
epoch 1 train time consumed: 4.29s
Validation Epoch: 7, Average loss: 0.0142, Accuracy: 0.2493
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.46638468690469,
                        "time": 2.7769279059994005,
                        "accuracy": 0.24931640625,
                        "total_cost": 1949179.3214065512
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8092
Profiling... [512/50176]	Loss: 1.9337
Profiling... [768/50176]	Loss: 2.1906
Profiling... [1024/50176]	Loss: 2.1055
Profiling... [1280/50176]	Loss: 2.0173
Profiling... [1536/50176]	Loss: 2.0326
Profiling... [1792/50176]	Loss: 2.1394
Profiling... [2048/50176]	Loss: 1.9409
Profiling... [2304/50176]	Loss: 1.9988
Profiling... [2560/50176]	Loss: 2.0418
Profiling... [2816/50176]	Loss: 1.8897
Profiling... [3072/50176]	Loss: 1.9618
Profiling... [3328/50176]	Loss: 1.9496
Profile done
epoch 1 train time consumed: 9.78s
Validation Epoch: 7, Average loss: 0.0127, Accuracy: 0.2654
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.43297817071648,
                        "time": 7.00887155899909,
                        "accuracy": 0.2654296875,
                        "total_cost": 4621007.2971767355
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9973
Profiling... [512/50176]	Loss: 1.8906
Profiling... [768/50176]	Loss: 1.9335
Profiling... [1024/50176]	Loss: 2.0642
Profiling... [1280/50176]	Loss: 2.0844
Profiling... [1536/50176]	Loss: 2.1153
Profiling... [1792/50176]	Loss: 2.2026
Profiling... [2048/50176]	Loss: 1.9756
Profiling... [2304/50176]	Loss: 2.0368
Profiling... [2560/50176]	Loss: 2.2321
Profiling... [2816/50176]	Loss: 2.1754
Profiling... [3072/50176]	Loss: 2.1168
Profiling... [3328/50176]	Loss: 2.0126
Profile done
epoch 1 train time consumed: 4.01s
Validation Epoch: 7, Average loss: 0.0117, Accuracy: 0.2674
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.41093394906028,
                        "time": 2.4008018689983146,
                        "accuracy": 0.2673828125,
                        "total_cost": 1571306.4095124104
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9535
Profiling... [512/50176]	Loss: 1.7953
Profiling... [768/50176]	Loss: 1.9536
Profiling... [1024/50176]	Loss: 2.0939
Profiling... [1280/50176]	Loss: 2.2362
Profiling... [1536/50176]	Loss: 2.0525
Profiling... [1792/50176]	Loss: 2.1280
Profiling... [2048/50176]	Loss: 2.0098
Profiling... [2304/50176]	Loss: 1.9652
Profiling... [2560/50176]	Loss: 2.0025
Profiling... [2816/50176]	Loss: 2.0630
Profiling... [3072/50176]	Loss: 1.9763
Profiling... [3328/50176]	Loss: 2.0207
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 7, Average loss: 0.0113, Accuracy: 0.3158
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.43672713804658,
                        "time": 2.4491383689983195,
                        "accuracy": 0.3158203125,
                        "total_cost": 1357098.317020714
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1221
Profiling... [512/50176]	Loss: 1.7694
Profiling... [768/50176]	Loss: 1.8766
Profiling... [1024/50176]	Loss: 2.0724
Profiling... [1280/50176]	Loss: 2.0012
Profiling... [1536/50176]	Loss: 2.3178
Profiling... [1792/50176]	Loss: 2.0508
Profiling... [2048/50176]	Loss: 2.1427
Profiling... [2304/50176]	Loss: 2.0121
Profiling... [2560/50176]	Loss: 2.0990
Profiling... [2816/50176]	Loss: 2.1431
Profiling... [3072/50176]	Loss: 2.0570
Profiling... [3328/50176]	Loss: 1.9534
Profile done
epoch 1 train time consumed: 4.32s
Validation Epoch: 7, Average loss: 0.0117, Accuracy: 0.2848
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.4499674803146,
                        "time": 2.781865993998508,
                        "accuracy": 0.284765625,
                        "total_cost": 1709569.2253927733
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0345
Profiling... [512/50176]	Loss: 1.9388
Profiling... [768/50176]	Loss: 2.1650
Profiling... [1024/50176]	Loss: 2.0531
Profiling... [1280/50176]	Loss: 1.9046
Profiling... [1536/50176]	Loss: 2.0598
Profiling... [1792/50176]	Loss: 2.0534
Profiling... [2048/50176]	Loss: 1.9607
Profiling... [2304/50176]	Loss: 1.9707
Profiling... [2560/50176]	Loss: 2.1501
Profiling... [2816/50176]	Loss: 2.0427
Profiling... [3072/50176]	Loss: 2.1651
Profiling... [3328/50176]	Loss: 2.0606
Profile done
epoch 1 train time consumed: 9.82s
Validation Epoch: 7, Average loss: 0.0156, Accuracy: 0.1861
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.41511238706941,
                        "time": 6.957323798998914,
                        "accuracy": 0.1861328125,
                        "total_cost": 6541198.4511049595
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8745
Profiling... [1024/50176]	Loss: 1.9916
Profiling... [1536/50176]	Loss: 1.8304
Profiling... [2048/50176]	Loss: 1.8016
Profiling... [2560/50176]	Loss: 1.9770
Profiling... [3072/50176]	Loss: 1.9782
Profiling... [3584/50176]	Loss: 1.9457
Profiling... [4096/50176]	Loss: 2.0283
Profiling... [4608/50176]	Loss: 1.9385
Profiling... [5120/50176]	Loss: 1.8925
Profiling... [5632/50176]	Loss: 2.0722
Profiling... [6144/50176]	Loss: 1.9987
Profiling... [6656/50176]	Loss: 1.9770
Profile done
epoch 1 train time consumed: 7.13s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4752
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.41339785774582,
                        "time": 4.576783685999544,
                        "accuracy": 0.4751953125,
                        "total_cost": 1685490.416216848
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0075
Profiling... [1024/50176]	Loss: 1.9991
Profiling... [1536/50176]	Loss: 1.9000
Profiling... [2048/50176]	Loss: 1.8689
Profiling... [2560/50176]	Loss: 1.8444
Profiling... [3072/50176]	Loss: 2.0074
Profiling... [3584/50176]	Loss: 1.9568
Profiling... [4096/50176]	Loss: 1.8773
Profiling... [4608/50176]	Loss: 1.8657
Profiling... [5120/50176]	Loss: 1.9487
Profiling... [5632/50176]	Loss: 1.8464
Profiling... [6144/50176]	Loss: 1.8872
Profiling... [6656/50176]	Loss: 1.9552
Profile done
epoch 1 train time consumed: 7.06s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4809
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.45067732008052,
                        "time": 4.6817151560007915,
                        "accuracy": 0.480859375,
                        "total_cost": 1703824.8496249835
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9688
Profiling... [1024/50176]	Loss: 2.1133
Profiling... [1536/50176]	Loss: 1.9375
Profiling... [2048/50176]	Loss: 1.9774
Profiling... [2560/50176]	Loss: 2.0339
Profiling... [3072/50176]	Loss: 1.9417
Profiling... [3584/50176]	Loss: 1.9960
Profiling... [4096/50176]	Loss: 1.7832
Profiling... [4608/50176]	Loss: 1.8888
Profiling... [5120/50176]	Loss: 1.9019
Profiling... [5632/50176]	Loss: 1.8345
Profiling... [6144/50176]	Loss: 1.9292
Profiling... [6656/50176]	Loss: 1.8410
Profile done
epoch 1 train time consumed: 7.84s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4818
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.46244352493393,
                        "time": 5.372342208000191,
                        "accuracy": 0.4818359375,
                        "total_cost": 1951203.3313207019
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1667
Profiling... [1024/50176]	Loss: 1.8887
Profiling... [1536/50176]	Loss: 1.9133
Profiling... [2048/50176]	Loss: 1.9982
Profiling... [2560/50176]	Loss: 1.8979
Profiling... [3072/50176]	Loss: 1.9747
Profiling... [3584/50176]	Loss: 1.8497
Profiling... [4096/50176]	Loss: 1.8914
Profiling... [4608/50176]	Loss: 2.0030
Profiling... [5120/50176]	Loss: 2.0275
Profiling... [5632/50176]	Loss: 1.9446
Profiling... [6144/50176]	Loss: 1.8542
Profiling... [6656/50176]	Loss: 1.8504
Profile done
epoch 1 train time consumed: 17.88s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4814
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.40301302475434,
                        "time": 13.040296155999386,
                        "accuracy": 0.4814453125,
                        "total_cost": 4740002.172728377
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9184
Profiling... [1024/50176]	Loss: 1.9544
Profiling... [1536/50176]	Loss: 1.9567
Profiling... [2048/50176]	Loss: 1.9955
Profiling... [2560/50176]	Loss: 2.0326
Profiling... [3072/50176]	Loss: 1.8990
Profiling... [3584/50176]	Loss: 1.8892
Profiling... [4096/50176]	Loss: 1.8412
Profiling... [4608/50176]	Loss: 1.9515
Profiling... [5120/50176]	Loss: 1.9336
Profiling... [5632/50176]	Loss: 1.8150
Profiling... [6144/50176]	Loss: 2.0109
Profiling... [6656/50176]	Loss: 1.8865
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4754
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.40569013548014,
                        "time": 4.566372403000059,
                        "accuracy": 0.475390625,
                        "total_cost": 1680965.3545965708
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9135
Profiling... [1024/50176]	Loss: 2.0109
Profiling... [1536/50176]	Loss: 1.9528
Profiling... [2048/50176]	Loss: 2.0563
Profiling... [2560/50176]	Loss: 1.9651
Profiling... [3072/50176]	Loss: 1.8976
Profiling... [3584/50176]	Loss: 2.0358
Profiling... [4096/50176]	Loss: 1.9952
Profiling... [4608/50176]	Loss: 1.9536
Profiling... [5120/50176]	Loss: 1.9224
Profiling... [5632/50176]	Loss: 1.8185
Profiling... [6144/50176]	Loss: 1.9110
Profiling... [6656/50176]	Loss: 1.7815
Profile done
epoch 1 train time consumed: 7.00s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4793
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.4421593450151,
                        "time": 4.678948483000568,
                        "accuracy": 0.479296875,
                        "total_cost": 1708369.128267526
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8958
Profiling... [1024/50176]	Loss: 2.0841
Profiling... [1536/50176]	Loss: 1.9736
Profiling... [2048/50176]	Loss: 2.0774
Profiling... [2560/50176]	Loss: 1.9729
Profiling... [3072/50176]	Loss: 2.0441
Profiling... [3584/50176]	Loss: 1.7835
Profiling... [4096/50176]	Loss: 1.8759
Profiling... [4608/50176]	Loss: 1.9207
Profiling... [5120/50176]	Loss: 1.9540
Profiling... [5632/50176]	Loss: 1.9746
Profiling... [6144/50176]	Loss: 1.9527
Profiling... [6656/50176]	Loss: 1.8100
Profile done
epoch 1 train time consumed: 7.83s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4813
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.45363583666813,
                        "time": 5.352845310999328,
                        "accuracy": 0.48134765625,
                        "total_cost": 1946094.2985008715
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9395
Profiling... [1024/50176]	Loss: 2.0261
Profiling... [1536/50176]	Loss: 1.8662
Profiling... [2048/50176]	Loss: 2.0169
Profiling... [2560/50176]	Loss: 1.9610
Profiling... [3072/50176]	Loss: 1.9517
Profiling... [3584/50176]	Loss: 1.9218
Profiling... [4096/50176]	Loss: 1.8236
Profiling... [4608/50176]	Loss: 1.8970
Profiling... [5120/50176]	Loss: 1.8377
Profiling... [5632/50176]	Loss: 1.9641
Profiling... [6144/50176]	Loss: 1.9809
Profiling... [6656/50176]	Loss: 1.7693
Profile done
epoch 1 train time consumed: 17.81s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4830
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.398209285312,
                        "time": 13.025673338999695,
                        "accuracy": 0.4830078125,
                        "total_cost": 4719370.52638242
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8787
Profiling... [1024/50176]	Loss: 1.9109
Profiling... [1536/50176]	Loss: 1.9761
Profiling... [2048/50176]	Loss: 1.9752
Profiling... [2560/50176]	Loss: 1.9063
Profiling... [3072/50176]	Loss: 1.8905
Profiling... [3584/50176]	Loss: 1.8938
Profiling... [4096/50176]	Loss: 1.8390
Profiling... [4608/50176]	Loss: 1.9214
Profiling... [5120/50176]	Loss: 1.8619
Profiling... [5632/50176]	Loss: 2.0790
Profiling... [6144/50176]	Loss: 1.8521
Profiling... [6656/50176]	Loss: 1.9124
Profile done
epoch 1 train time consumed: 6.90s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4765
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.39967640926886,
                        "time": 4.574453618000916,
                        "accuracy": 0.47646484375,
                        "total_cost": 1680143.6530964628
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0080
Profiling... [1024/50176]	Loss: 1.9034
Profiling... [1536/50176]	Loss: 2.0163
Profiling... [2048/50176]	Loss: 2.0585
Profiling... [2560/50176]	Loss: 2.0030
Profiling... [3072/50176]	Loss: 1.8938
Profiling... [3584/50176]	Loss: 1.8438
Profiling... [4096/50176]	Loss: 2.0619
Profiling... [4608/50176]	Loss: 1.8493
Profiling... [5120/50176]	Loss: 1.9326
Profiling... [5632/50176]	Loss: 1.9092
Profiling... [6144/50176]	Loss: 1.8795
Profiling... [6656/50176]	Loss: 1.9526
Profile done
epoch 1 train time consumed: 7.02s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4797
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.43924912646739,
                        "time": 4.674183228999027,
                        "accuracy": 0.4796875,
                        "total_cost": 1705239.4841950848
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8310
Profiling... [1024/50176]	Loss: 2.0025
Profiling... [1536/50176]	Loss: 2.0163
Profiling... [2048/50176]	Loss: 2.0363
Profiling... [2560/50176]	Loss: 1.8962
Profiling... [3072/50176]	Loss: 1.8308
Profiling... [3584/50176]	Loss: 1.9042
Profiling... [4096/50176]	Loss: 1.9571
Profiling... [4608/50176]	Loss: 1.9945
Profiling... [5120/50176]	Loss: 1.8518
Profiling... [5632/50176]	Loss: 1.9849
Profiling... [6144/50176]	Loss: 1.9161
Profiling... [6656/50176]	Loss: 1.9471
Profile done
epoch 1 train time consumed: 7.89s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4774
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.45025242202908,
                        "time": 5.3457722019993525,
                        "accuracy": 0.47744140625,
                        "total_cost": 1959423.9693153694
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9451
Profiling... [1024/50176]	Loss: 2.0036
Profiling... [1536/50176]	Loss: 1.9879
Profiling... [2048/50176]	Loss: 1.9089
Profiling... [2560/50176]	Loss: 1.7977
Profiling... [3072/50176]	Loss: 1.9956
Profiling... [3584/50176]	Loss: 1.8074
Profiling... [4096/50176]	Loss: 1.8956
Profiling... [4608/50176]	Loss: 1.8688
Profiling... [5120/50176]	Loss: 2.0628
Profiling... [5632/50176]	Loss: 2.0039
Profiling... [6144/50176]	Loss: 2.0555
Profiling... [6656/50176]	Loss: 1.8468
Profile done
epoch 1 train time consumed: 17.87s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4774
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.39060631395998,
                        "time": 13.02116386599846,
                        "accuracy": 0.47744140625,
                        "total_cost": 4772739.956610603
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9872
Profiling... [1024/50176]	Loss: 1.8965
Profiling... [1536/50176]	Loss: 1.8553
Profiling... [2048/50176]	Loss: 1.9075
Profiling... [2560/50176]	Loss: 2.0851
Profiling... [3072/50176]	Loss: 1.8928
Profiling... [3584/50176]	Loss: 1.8238
Profiling... [4096/50176]	Loss: 1.7662
Profiling... [4608/50176]	Loss: 1.7845
Profiling... [5120/50176]	Loss: 1.9814
Profiling... [5632/50176]	Loss: 1.8272
Profiling... [6144/50176]	Loss: 1.9910
Profiling... [6656/50176]	Loss: 1.8582
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 7, Average loss: 0.0041, Accuracy: 0.4396
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.3926914008881,
                        "time": 4.562359272000322,
                        "accuracy": 0.4396484375,
                        "total_cost": 1816025.725327538
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9270
Profiling... [1024/50176]	Loss: 2.0196
Profiling... [1536/50176]	Loss: 1.9631
Profiling... [2048/50176]	Loss: 2.0796
Profiling... [2560/50176]	Loss: 1.8700
Profiling... [3072/50176]	Loss: 1.8659
Profiling... [3584/50176]	Loss: 1.8661
Profiling... [4096/50176]	Loss: 1.8266
Profiling... [4608/50176]	Loss: 1.7984
Profiling... [5120/50176]	Loss: 1.9048
Profiling... [5632/50176]	Loss: 1.8277
Profiling... [6144/50176]	Loss: 1.9916
Profiling... [6656/50176]	Loss: 1.9268
Profile done
epoch 1 train time consumed: 7.21s
Validation Epoch: 7, Average loss: 0.0056, Accuracy: 0.3194
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.42936258359325,
                        "time": 4.742436129999987,
                        "accuracy": 0.31943359375,
                        "total_cost": 2598118.479046156
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0381
Profiling... [1024/50176]	Loss: 1.9164
Profiling... [1536/50176]	Loss: 1.9582
Profiling... [2048/50176]	Loss: 1.7957
Profiling... [2560/50176]	Loss: 1.8963
Profiling... [3072/50176]	Loss: 1.8555
Profiling... [3584/50176]	Loss: 1.9421
Profiling... [4096/50176]	Loss: 1.8828
Profiling... [4608/50176]	Loss: 1.8465
Profiling... [5120/50176]	Loss: 2.0070
Profiling... [5632/50176]	Loss: 1.8372
Profiling... [6144/50176]	Loss: 1.9836
Profiling... [6656/50176]	Loss: 1.9744
Profile done
epoch 1 train time consumed: 7.90s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4447
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.43872175738363,
                        "time": 5.369183688000703,
                        "accuracy": 0.4447265625,
                        "total_cost": 2112774.960232161
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9369
Profiling... [1024/50176]	Loss: 2.0506
Profiling... [1536/50176]	Loss: 1.9490
Profiling... [2048/50176]	Loss: 1.8500
Profiling... [2560/50176]	Loss: 1.9518
Profiling... [3072/50176]	Loss: 2.0067
Profiling... [3584/50176]	Loss: 1.8803
Profiling... [4096/50176]	Loss: 1.9517
Profiling... [4608/50176]	Loss: 1.9499
Profiling... [5120/50176]	Loss: 1.8930
Profiling... [5632/50176]	Loss: 1.8289
Profiling... [6144/50176]	Loss: 1.9315
Profiling... [6656/50176]	Loss: 1.9208
Profile done
epoch 1 train time consumed: 17.89s
Validation Epoch: 7, Average loss: 0.0049, Accuracy: 0.3883
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.38193765083909,
                        "time": 13.048423552001623,
                        "accuracy": 0.38828125,
                        "total_cost": 5880979.629071153
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1224
Profiling... [1024/50176]	Loss: 1.9154
Profiling... [1536/50176]	Loss: 1.9579
Profiling... [2048/50176]	Loss: 1.9521
Profiling... [2560/50176]	Loss: 1.9707
Profiling... [3072/50176]	Loss: 1.9554
Profiling... [3584/50176]	Loss: 1.8412
Profiling... [4096/50176]	Loss: 1.9530
Profiling... [4608/50176]	Loss: 1.9121
Profiling... [5120/50176]	Loss: 1.9924
Profiling... [5632/50176]	Loss: 1.8955
Profiling... [6144/50176]	Loss: 1.9792
Profiling... [6656/50176]	Loss: 1.8848
Profile done
epoch 1 train time consumed: 7.04s
Validation Epoch: 7, Average loss: 0.0038, Accuracy: 0.4585
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.38350402575158,
                        "time": 4.562107736001053,
                        "accuracy": 0.45849609375,
                        "total_cost": 1741277.3296941188
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9649
Profiling... [1024/50176]	Loss: 1.8125
Profiling... [1536/50176]	Loss: 1.9448
Profiling... [2048/50176]	Loss: 1.9748
Profiling... [2560/50176]	Loss: 1.9529
Profiling... [3072/50176]	Loss: 1.9136
Profiling... [3584/50176]	Loss: 1.7646
Profiling... [4096/50176]	Loss: 1.9679
Profiling... [4608/50176]	Loss: 1.9282
Profiling... [5120/50176]	Loss: 1.8335
Profiling... [5632/50176]	Loss: 1.8026
Profiling... [6144/50176]	Loss: 1.7564
Profiling... [6656/50176]	Loss: 1.9353
Profile done
epoch 1 train time consumed: 7.09s
Validation Epoch: 7, Average loss: 0.0041, Accuracy: 0.4371
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.41940975806335,
                        "time": 4.691044979999788,
                        "accuracy": 0.437109375,
                        "total_cost": 1878094.8624127836
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0501
Profiling... [1024/50176]	Loss: 1.9872
Profiling... [1536/50176]	Loss: 1.8813
Profiling... [2048/50176]	Loss: 1.9831
Profiling... [2560/50176]	Loss: 1.9553
Profiling... [3072/50176]	Loss: 1.8149
Profiling... [3584/50176]	Loss: 1.9242
Profiling... [4096/50176]	Loss: 2.0362
Profiling... [4608/50176]	Loss: 1.7818
Profiling... [5120/50176]	Loss: 1.9651
Profiling... [5632/50176]	Loss: 2.0431
Profiling... [6144/50176]	Loss: 1.7887
Profiling... [6656/50176]	Loss: 1.8661
Profile done
epoch 1 train time consumed: 7.85s
Validation Epoch: 7, Average loss: 0.0043, Accuracy: 0.4233
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.43119707076298,
                        "time": 5.364434105998953,
                        "accuracy": 0.42333984375,
                        "total_cost": 2217546.9245559685
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9924
Profiling... [1024/50176]	Loss: 1.8580
Profiling... [1536/50176]	Loss: 1.8294
Profiling... [2048/50176]	Loss: 1.8662
Profiling... [2560/50176]	Loss: 1.9412
Profiling... [3072/50176]	Loss: 1.8818
Profiling... [3584/50176]	Loss: 1.9033
Profiling... [4096/50176]	Loss: 1.7924
Profiling... [4608/50176]	Loss: 1.8417
Profiling... [5120/50176]	Loss: 1.8850
Profiling... [5632/50176]	Loss: 1.9110
Profiling... [6144/50176]	Loss: 1.8592
Profiling... [6656/50176]	Loss: 1.7569
Profile done
epoch 1 train time consumed: 17.86s
Validation Epoch: 7, Average loss: 0.0044, Accuracy: 0.4073
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.37687921175765,
                        "time": 13.03431370600083,
                        "accuracy": 0.40732421875,
                        "total_cost": 5599973.66606413
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9774
Profiling... [1024/50176]	Loss: 2.0286
Profiling... [1536/50176]	Loss: 1.9140
Profiling... [2048/50176]	Loss: 1.9343
Profiling... [2560/50176]	Loss: 2.0162
Profiling... [3072/50176]	Loss: 1.8888
Profiling... [3584/50176]	Loss: 1.9737
Profiling... [4096/50176]	Loss: 1.9463
Profiling... [4608/50176]	Loss: 1.8429
Profiling... [5120/50176]	Loss: 1.9638
Profiling... [5632/50176]	Loss: 1.8731
Profiling... [6144/50176]	Loss: 1.9475
Profiling... [6656/50176]	Loss: 1.8860
Profile done
epoch 1 train time consumed: 6.98s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4503
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.38020788650579,
                        "time": 4.5698212110000895,
                        "accuracy": 0.45029296875,
                        "total_cost": 1775996.4454808414
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9994
Profiling... [1024/50176]	Loss: 1.9863
Profiling... [1536/50176]	Loss: 1.8381
Profiling... [2048/50176]	Loss: 2.0122
Profiling... [2560/50176]	Loss: 2.0686
Profiling... [3072/50176]	Loss: 1.9830
Profiling... [3584/50176]	Loss: 2.0304
Profiling... [4096/50176]	Loss: 2.0284
Profiling... [4608/50176]	Loss: 1.9323
Profiling... [5120/50176]	Loss: 1.7986
Profiling... [5632/50176]	Loss: 1.8912
Profiling... [6144/50176]	Loss: 1.7900
Profiling... [6656/50176]	Loss: 1.8321
Profile done
epoch 1 train time consumed: 7.08s
Validation Epoch: 7, Average loss: 0.0043, Accuracy: 0.4089
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.41701293163118,
                        "time": 4.6891963580001175,
                        "accuracy": 0.40888671875,
                        "total_cost": 2006935.7233188942
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9738
Profiling... [1024/50176]	Loss: 2.0092
Profiling... [1536/50176]	Loss: 1.8154
Profiling... [2048/50176]	Loss: 1.8011
Profiling... [2560/50176]	Loss: 1.9105
Profiling... [3072/50176]	Loss: 1.8671
Profiling... [3584/50176]	Loss: 1.9121
Profiling... [4096/50176]	Loss: 1.9702
Profiling... [4608/50176]	Loss: 1.9443
Profiling... [5120/50176]	Loss: 1.8614
Profiling... [5632/50176]	Loss: 1.8587
Profiling... [6144/50176]	Loss: 2.0902
Profiling... [6656/50176]	Loss: 1.8735
Profile done
epoch 1 train time consumed: 8.05s
Validation Epoch: 7, Average loss: 0.0038, Accuracy: 0.4711
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.42879667550932,
                        "time": 5.3985589089988935,
                        "accuracy": 0.47109375,
                        "total_cost": 2005434.8185999207
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0782
Profiling... [1024/50176]	Loss: 1.9855
Profiling... [1536/50176]	Loss: 2.0354
Profiling... [2048/50176]	Loss: 1.9428
Profiling... [2560/50176]	Loss: 1.8427
Profiling... [3072/50176]	Loss: 1.9025
Profiling... [3584/50176]	Loss: 1.8766
Profiling... [4096/50176]	Loss: 1.8979
Profiling... [4608/50176]	Loss: 1.9742
Profiling... [5120/50176]	Loss: 1.8884
Profiling... [5632/50176]	Loss: 1.9005
Profiling... [6144/50176]	Loss: 1.9112
Profiling... [6656/50176]	Loss: 1.8746
Profile done
epoch 1 train time consumed: 17.77s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4539
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.37520366530184,
                        "time": 13.010737935001089,
                        "accuracy": 0.45390625,
                        "total_cost": 5016188.119518492
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9066
Profiling... [1024/50176]	Loss: 1.8427
Profiling... [1536/50176]	Loss: 1.9194
Profiling... [2048/50176]	Loss: 1.9230
Profiling... [2560/50176]	Loss: 1.9351
Profiling... [3072/50176]	Loss: 2.1427
Profiling... [3584/50176]	Loss: 1.9467
Profiling... [4096/50176]	Loss: 2.0385
Profiling... [4608/50176]	Loss: 1.9841
Profiling... [5120/50176]	Loss: 1.9620
Profiling... [5632/50176]	Loss: 1.9882
Profiling... [6144/50176]	Loss: 2.0630
Profiling... [6656/50176]	Loss: 1.9549
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 7, Average loss: 0.0057, Accuracy: 0.3125
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.37891752037825,
                        "time": 4.563078847000725,
                        "accuracy": 0.3125,
                        "total_cost": 2555324.154320406
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0570
Profiling... [1024/50176]	Loss: 2.0499
Profiling... [1536/50176]	Loss: 2.1976
Profiling... [2048/50176]	Loss: 1.8392
Profiling... [2560/50176]	Loss: 1.9814
Profiling... [3072/50176]	Loss: 2.0613
Profiling... [3584/50176]	Loss: 1.9161
Profiling... [4096/50176]	Loss: 1.9858
Profiling... [4608/50176]	Loss: 1.9030
Profiling... [5120/50176]	Loss: 2.0269
Profiling... [5632/50176]	Loss: 1.9443
Profiling... [6144/50176]	Loss: 1.7643
Profiling... [6656/50176]	Loss: 1.9681
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 7, Average loss: 0.0056, Accuracy: 0.3320
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.41505059680605,
                        "time": 4.690021184000216,
                        "accuracy": 0.33203125,
                        "total_cost": 2471917.0475671724
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0079
Profiling... [1024/50176]	Loss: 1.9976
Profiling... [1536/50176]	Loss: 2.0532
Profiling... [2048/50176]	Loss: 1.9353
Profiling... [2560/50176]	Loss: 2.0901
Profiling... [3072/50176]	Loss: 1.9175
Profiling... [3584/50176]	Loss: 1.8129
Profiling... [4096/50176]	Loss: 1.9165
Profiling... [4608/50176]	Loss: 1.8795
Profiling... [5120/50176]	Loss: 2.0039
Profiling... [5632/50176]	Loss: 2.1087
Profiling... [6144/50176]	Loss: 1.8905
Profiling... [6656/50176]	Loss: 2.0322
Profile done
epoch 1 train time consumed: 7.92s
Validation Epoch: 7, Average loss: 0.0053, Accuracy: 0.3331
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.42675330393946,
                        "time": 5.365809735001676,
                        "accuracy": 0.33310546875,
                        "total_cost": 2818977.1460343017
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9694
Profiling... [1024/50176]	Loss: 1.9280
Profiling... [1536/50176]	Loss: 1.8859
Profiling... [2048/50176]	Loss: 1.9604
Profiling... [2560/50176]	Loss: 2.0252
Profiling... [3072/50176]	Loss: 2.0432
Profiling... [3584/50176]	Loss: 1.9743
Profiling... [4096/50176]	Loss: 1.9879
Profiling... [4608/50176]	Loss: 2.1345
Profiling... [5120/50176]	Loss: 1.9040
Profiling... [5632/50176]	Loss: 1.9385
Profiling... [6144/50176]	Loss: 1.9573
Profiling... [6656/50176]	Loss: 2.1624
Profile done
epoch 1 train time consumed: 17.89s
Validation Epoch: 7, Average loss: 0.0069, Accuracy: 0.2757
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.36855353124254,
                        "time": 13.012684693001574,
                        "accuracy": 0.27568359375,
                        "total_cost": 8260266.018370111
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0339
Profiling... [1024/50176]	Loss: 1.9312
Profiling... [1536/50176]	Loss: 1.9805
Profiling... [2048/50176]	Loss: 2.0179
Profiling... [2560/50176]	Loss: 1.8169
Profiling... [3072/50176]	Loss: 2.0167
Profiling... [3584/50176]	Loss: 1.9360
Profiling... [4096/50176]	Loss: 1.9060
Profiling... [4608/50176]	Loss: 1.7563
Profiling... [5120/50176]	Loss: 1.9585
Profiling... [5632/50176]	Loss: 1.8731
Profiling... [6144/50176]	Loss: 2.0003
Profiling... [6656/50176]	Loss: 1.9851
Profile done
epoch 1 train time consumed: 6.87s
Validation Epoch: 7, Average loss: 0.0088, Accuracy: 0.1183
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.37285085779008,
                        "time": 4.565887439999642,
                        "accuracy": 0.11826171875,
                        "total_cost": 6756457.714681552
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0412
Profiling... [1024/50176]	Loss: 1.8467
Profiling... [1536/50176]	Loss: 2.0006
Profiling... [2048/50176]	Loss: 1.8854
Profiling... [2560/50176]	Loss: 2.0071
Profiling... [3072/50176]	Loss: 1.9760
Profiling... [3584/50176]	Loss: 2.0173
Profiling... [4096/50176]	Loss: 1.8195
Profiling... [4608/50176]	Loss: 1.9831
Profiling... [5120/50176]	Loss: 1.8948
Profiling... [5632/50176]	Loss: 2.0202
Profiling... [6144/50176]	Loss: 1.9245
Profiling... [6656/50176]	Loss: 1.9793
Profile done
epoch 1 train time consumed: 7.03s
Validation Epoch: 7, Average loss: 0.0047, Accuracy: 0.3895
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.41004411772767,
                        "time": 4.701000267999916,
                        "accuracy": 0.389453125,
                        "total_cost": 2112385.275891637
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1157
Profiling... [1024/50176]	Loss: 1.9928
Profiling... [1536/50176]	Loss: 1.9621
Profiling... [2048/50176]	Loss: 1.9055
Profiling... [2560/50176]	Loss: 2.0148
Profiling... [3072/50176]	Loss: 1.9754
Profiling... [3584/50176]	Loss: 1.9608
Profiling... [4096/50176]	Loss: 2.0373
Profiling... [4608/50176]	Loss: 1.9153
Profiling... [5120/50176]	Loss: 1.8774
Profiling... [5632/50176]	Loss: 1.9401
Profiling... [6144/50176]	Loss: 1.9305
Profiling... [6656/50176]	Loss: 1.9640
Profile done
epoch 1 train time consumed: 7.97s
Validation Epoch: 7, Average loss: 0.0048, Accuracy: 0.3677
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.42085950568966,
                        "time": 5.391329850999682,
                        "accuracy": 0.36767578125,
                        "total_cost": 2566072.5346590783
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9858
Profiling... [1024/50176]	Loss: 1.9693
Profiling... [1536/50176]	Loss: 1.8940
Profiling... [2048/50176]	Loss: 2.0555
Profiling... [2560/50176]	Loss: 2.0596
Profiling... [3072/50176]	Loss: 1.9784
Profiling... [3584/50176]	Loss: 2.0131
Profiling... [4096/50176]	Loss: 2.0178
Profiling... [4608/50176]	Loss: 2.0345
Profiling... [5120/50176]	Loss: 1.9192
Profiling... [5632/50176]	Loss: 1.9305
Profiling... [6144/50176]	Loss: 1.9055
Profiling... [6656/50176]	Loss: 1.9947
Profile done
epoch 1 train time consumed: 17.94s
Validation Epoch: 7, Average loss: 0.0058, Accuracy: 0.2989
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.36635351431255,
                        "time": 13.09743292100029,
                        "accuracy": 0.29892578125,
                        "total_cost": 7667624.892006704
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0355
Profiling... [1024/50176]	Loss: 1.9676
Profiling... [1536/50176]	Loss: 2.0447
Profiling... [2048/50176]	Loss: 1.8570
Profiling... [2560/50176]	Loss: 2.0508
Profiling... [3072/50176]	Loss: 1.9424
Profiling... [3584/50176]	Loss: 1.9850
Profiling... [4096/50176]	Loss: 1.8443
Profiling... [4608/50176]	Loss: 1.9392
Profiling... [5120/50176]	Loss: 1.9421
Profiling... [5632/50176]	Loss: 1.9341
Profiling... [6144/50176]	Loss: 2.0401
Profiling... [6656/50176]	Loss: 2.0620
Profile done
epoch 1 train time consumed: 7.05s
Validation Epoch: 7, Average loss: 0.0135, Accuracy: 0.0964
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.36965205554709,
                        "time": 4.577435293000235,
                        "accuracy": 0.09638671875,
                        "total_cost": 8310804.503603264
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9465
Profiling... [1024/50176]	Loss: 2.0330
Profiling... [1536/50176]	Loss: 2.0258
Profiling... [2048/50176]	Loss: 2.0783
Profiling... [2560/50176]	Loss: 2.0053
Profiling... [3072/50176]	Loss: 2.0735
Profiling... [3584/50176]	Loss: 2.0663
Profiling... [4096/50176]	Loss: 1.9766
Profiling... [4608/50176]	Loss: 2.0390
Profiling... [5120/50176]	Loss: 2.1647
Profiling... [5632/50176]	Loss: 1.8901
Profiling... [6144/50176]	Loss: 2.0786
Profiling... [6656/50176]	Loss: 2.0427
Profile done
epoch 1 train time consumed: 7.15s
Validation Epoch: 7, Average loss: 0.0058, Accuracy: 0.2788
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.40561682452298,
                        "time": 4.709804851998342,
                        "accuracy": 0.27880859375,
                        "total_cost": 2956206.758242042
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1197
Profiling... [1024/50176]	Loss: 1.9713
Profiling... [1536/50176]	Loss: 1.9598
Profiling... [2048/50176]	Loss: 1.8889
Profiling... [2560/50176]	Loss: 2.0895
Profiling... [3072/50176]	Loss: 2.1309
Profiling... [3584/50176]	Loss: 1.9104
Profiling... [4096/50176]	Loss: 1.8682
Profiling... [4608/50176]	Loss: 2.1055
Profiling... [5120/50176]	Loss: 1.9317
Profiling... [5632/50176]	Loss: 2.1122
Profiling... [6144/50176]	Loss: 1.9724
Profiling... [6656/50176]	Loss: 1.9540
Profile done
epoch 1 train time consumed: 7.95s
Validation Epoch: 7, Average loss: 0.0068, Accuracy: 0.2705
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.41641874072832,
                        "time": 5.438433569999688,
                        "accuracy": 0.2705078125,
                        "total_cost": 3518293.4864402316
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9504
Profiling... [1024/50176]	Loss: 2.0261
Profiling... [1536/50176]	Loss: 1.8725
Profiling... [2048/50176]	Loss: 2.0412
Profiling... [2560/50176]	Loss: 2.0423
Profiling... [3072/50176]	Loss: 1.9439
Profiling... [3584/50176]	Loss: 2.1168
Profiling... [4096/50176]	Loss: 2.1212
Profiling... [4608/50176]	Loss: 2.0053
Profiling... [5120/50176]	Loss: 2.0101
Profiling... [5632/50176]	Loss: 1.9253
Profiling... [6144/50176]	Loss: 2.0588
Profiling... [6656/50176]	Loss: 1.9803
Profile done
epoch 1 train time consumed: 17.86s
Validation Epoch: 7, Average loss: 0.0048, Accuracy: 0.3645
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.36202497086701,
                        "time": 13.026275939000698,
                        "accuracy": 0.364453125,
                        "total_cost": 6254846.324407624
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9083
Profiling... [2048/50176]	Loss: 2.0331
Profiling... [3072/50176]	Loss: 2.0334
Profiling... [4096/50176]	Loss: 1.9520
Profiling... [5120/50176]	Loss: 1.9584
Profiling... [6144/50176]	Loss: 1.9692
Profiling... [7168/50176]	Loss: 1.8288
Profiling... [8192/50176]	Loss: 1.8793
Profiling... [9216/50176]	Loss: 2.0055
Profiling... [10240/50176]	Loss: 1.9336
Profiling... [11264/50176]	Loss: 1.7745
Profiling... [12288/50176]	Loss: 1.9196
Profiling... [13312/50176]	Loss: 1.9322
Profile done
epoch 1 train time consumed: 13.04s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4819
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.38949425230099,
                        "time": 8.969523260000642,
                        "accuracy": 0.48193359375,
                        "total_cost": 3257018.3752626446
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9523
Profiling... [2048/50176]	Loss: 1.9477
Profiling... [3072/50176]	Loss: 1.9828
Profiling... [4096/50176]	Loss: 1.8770
Profiling... [5120/50176]	Loss: 2.0618
Profiling... [6144/50176]	Loss: 1.7924
Profiling... [7168/50176]	Loss: 1.9829
Profiling... [8192/50176]	Loss: 1.8232
Profiling... [9216/50176]	Loss: 1.8140
Profiling... [10240/50176]	Loss: 1.9023
Profiling... [11264/50176]	Loss: 1.9148
Profiling... [12288/50176]	Loss: 1.7870
Profiling... [13312/50176]	Loss: 1.7974
Profile done
epoch 1 train time consumed: 13.40s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4861
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.44612856912728,
                        "time": 9.226364571000886,
                        "accuracy": 0.4861328125,
                        "total_cost": 3321342.9713205276
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0314
Profiling... [2048/50176]	Loss: 2.0180
Profiling... [3072/50176]	Loss: 1.9391
Profiling... [4096/50176]	Loss: 2.0087
Profiling... [5120/50176]	Loss: 1.9055
Profiling... [6144/50176]	Loss: 1.8517
Profiling... [7168/50176]	Loss: 1.9722
Profiling... [8192/50176]	Loss: 1.8631
Profiling... [9216/50176]	Loss: 2.0222
Profiling... [10240/50176]	Loss: 1.8956
Profiling... [11264/50176]	Loss: 1.9290
Profiling... [12288/50176]	Loss: 1.7827
Profiling... [13312/50176]	Loss: 1.9242
Profile done
epoch 1 train time consumed: 15.19s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4808
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.45754020320211,
                        "time": 10.697955589999765,
                        "accuracy": 0.48076171875,
                        "total_cost": 3894116.680333045
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9589
Profiling... [2048/50176]	Loss: 1.9409
Profiling... [3072/50176]	Loss: 2.0008
Profiling... [4096/50176]	Loss: 2.0133
Profiling... [5120/50176]	Loss: 1.9286
Profiling... [6144/50176]	Loss: 1.9484
Profiling... [7168/50176]	Loss: 1.8883
Profiling... [8192/50176]	Loss: 1.9494
Profiling... [9216/50176]	Loss: 1.9354
Profiling... [10240/50176]	Loss: 1.9154
Profiling... [11264/50176]	Loss: 1.9263
Profiling... [12288/50176]	Loss: 1.9523
Profiling... [13312/50176]	Loss: 1.8310
Profile done
epoch 1 train time consumed: 37.62s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4807
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.35206410619607,
                        "time": 28.07432195399997,
                        "accuracy": 0.4806640625,
                        "total_cost": 10221289.09824623
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9831
Profiling... [2048/50176]	Loss: 1.9675
Profiling... [3072/50176]	Loss: 2.0170
Profiling... [4096/50176]	Loss: 1.9121
Profiling... [5120/50176]	Loss: 1.8675
Profiling... [6144/50176]	Loss: 1.9105
Profiling... [7168/50176]	Loss: 1.8378
Profiling... [8192/50176]	Loss: 1.8803
Profiling... [9216/50176]	Loss: 1.9556
Profiling... [10240/50176]	Loss: 1.9715
Profiling... [11264/50176]	Loss: 1.8972
Profiling... [12288/50176]	Loss: 1.9719
Profiling... [13312/50176]	Loss: 1.9786
Profile done
epoch 1 train time consumed: 13.13s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4842
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.39125648441089,
                        "time": 8.960968400000638,
                        "accuracy": 0.4841796875,
                        "total_cost": 3238817.1385238287
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9275
Profiling... [2048/50176]	Loss: 1.9101
Profiling... [3072/50176]	Loss: 1.9496
Profiling... [4096/50176]	Loss: 1.8309
Profiling... [5120/50176]	Loss: 1.8588
Profiling... [6144/50176]	Loss: 1.9907
Profiling... [7168/50176]	Loss: 1.9298
Profiling... [8192/50176]	Loss: 1.9509
Profiling... [9216/50176]	Loss: 1.8907
Profiling... [10240/50176]	Loss: 1.8875
Profiling... [11264/50176]	Loss: 1.9056
Profiling... [12288/50176]	Loss: 1.8242
Profiling... [13312/50176]	Loss: 1.8817
Profile done
epoch 1 train time consumed: 13.38s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4806
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.44694128077089,
                        "time": 9.238602333000017,
                        "accuracy": 0.48056640625,
                        "total_cost": 3364270.5508506466
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9760
Profiling... [2048/50176]	Loss: 1.9255
Profiling... [3072/50176]	Loss: 1.9873
Profiling... [4096/50176]	Loss: 1.9717
Profiling... [5120/50176]	Loss: 1.9629
Profiling... [6144/50176]	Loss: 1.8839
Profiling... [7168/50176]	Loss: 1.8159
Profiling... [8192/50176]	Loss: 1.9197
Profiling... [9216/50176]	Loss: 1.9307
Profiling... [10240/50176]	Loss: 1.9367
Profiling... [11264/50176]	Loss: 1.8978
Profiling... [12288/50176]	Loss: 1.8585
Profiling... [13312/50176]	Loss: 2.0203
Profile done
epoch 1 train time consumed: 15.28s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4841
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.45909372053313,
                        "time": 10.766562958999202,
                        "accuracy": 0.48408203125,
                        "total_cost": 3892209.163309778
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9782
Profiling... [2048/50176]	Loss: 1.9713
Profiling... [3072/50176]	Loss: 1.9190
Profiling... [4096/50176]	Loss: 1.9423
Profiling... [5120/50176]	Loss: 1.9166
Profiling... [6144/50176]	Loss: 1.9850
Profiling... [7168/50176]	Loss: 1.9286
Profiling... [8192/50176]	Loss: 1.8851
Profiling... [9216/50176]	Loss: 1.7862
Profiling... [10240/50176]	Loss: 1.8968
Profiling... [11264/50176]	Loss: 1.8481
Profiling... [12288/50176]	Loss: 1.8827
Profiling... [13312/50176]	Loss: 1.8883
Profile done
epoch 1 train time consumed: 37.48s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4785
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.35526884228166,
                        "time": 27.739546143000553,
                        "accuracy": 0.478515625,
                        "total_cost": 10144748.303725917
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9066
Profiling... [2048/50176]	Loss: 2.0653
Profiling... [3072/50176]	Loss: 1.9650
Profiling... [4096/50176]	Loss: 1.9130
Profiling... [5120/50176]	Loss: 1.9744
Profiling... [6144/50176]	Loss: 1.9335
Profiling... [7168/50176]	Loss: 1.8738
Profiling... [8192/50176]	Loss: 1.9153
Profiling... [9216/50176]	Loss: 1.9059
Profiling... [10240/50176]	Loss: 1.8905
Profiling... [11264/50176]	Loss: 1.9257
Profiling... [12288/50176]	Loss: 1.9071
Profiling... [13312/50176]	Loss: 1.9373
Profile done
epoch 1 train time consumed: 12.98s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4796
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.39352598309186,
                        "time": 8.905094687999735,
                        "accuracy": 0.47958984375,
                        "total_cost": 3249425.713886281
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0193
Profiling... [2048/50176]	Loss: 1.9356
Profiling... [3072/50176]	Loss: 1.8841
Profiling... [4096/50176]	Loss: 1.9045
Profiling... [5120/50176]	Loss: 1.8160
Profiling... [6144/50176]	Loss: 2.0096
Profiling... [7168/50176]	Loss: 1.8167
Profiling... [8192/50176]	Loss: 1.8821
Profiling... [9216/50176]	Loss: 1.9397
Profiling... [10240/50176]	Loss: 1.9903
Profiling... [11264/50176]	Loss: 1.9410
Profiling... [12288/50176]	Loss: 1.9326
Profiling... [13312/50176]	Loss: 1.8142
Profile done
epoch 1 train time consumed: 13.36s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4812
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.44877244109827,
                        "time": 9.228527920000488,
                        "accuracy": 0.48115234375,
                        "total_cost": 3356509.444416658
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0337
Profiling... [2048/50176]	Loss: 1.8289
Profiling... [3072/50176]	Loss: 1.9154
Profiling... [4096/50176]	Loss: 1.9874
Profiling... [5120/50176]	Loss: 1.8725
Profiling... [6144/50176]	Loss: 1.9888
Profiling... [7168/50176]	Loss: 1.8335
Profiling... [8192/50176]	Loss: 1.9885
Profiling... [9216/50176]	Loss: 1.8310
Profiling... [10240/50176]	Loss: 1.8567
Profiling... [11264/50176]	Loss: 1.8902
Profiling... [12288/50176]	Loss: 1.9482
Profiling... [13312/50176]	Loss: 1.8709
Profile done
epoch 1 train time consumed: 15.30s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4800
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.45898075726313,
                        "time": 10.803055796999615,
                        "accuracy": 0.47998046875,
                        "total_cost": 3938774.36179518
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9579
Profiling... [2048/50176]	Loss: 1.9684
Profiling... [3072/50176]	Loss: 2.0135
Profiling... [4096/50176]	Loss: 2.0161
Profiling... [5120/50176]	Loss: 1.9287
Profiling... [6144/50176]	Loss: 2.0122
Profiling... [7168/50176]	Loss: 1.9709
Profiling... [8192/50176]	Loss: 1.9465
Profiling... [9216/50176]	Loss: 1.8958
Profiling... [10240/50176]	Loss: 1.9466
Profiling... [11264/50176]	Loss: 1.8868
Profiling... [12288/50176]	Loss: 1.8053
Profiling... [13312/50176]	Loss: 1.8640
Profile done
epoch 1 train time consumed: 37.81s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4793
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.35040972039228,
                        "time": 27.81052006200116,
                        "accuracy": 0.479296875,
                        "total_cost": 10154126.314406293
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9674
Profiling... [2048/50176]	Loss: 1.9648
Profiling... [3072/50176]	Loss: 1.9398
Profiling... [4096/50176]	Loss: 1.8152
Profiling... [5120/50176]	Loss: 1.9100
Profiling... [6144/50176]	Loss: 1.7922
Profiling... [7168/50176]	Loss: 1.8210
Profiling... [8192/50176]	Loss: 1.9199
Profiling... [9216/50176]	Loss: 1.8690
Profiling... [10240/50176]	Loss: 1.9062
Profiling... [11264/50176]	Loss: 1.8459
Profiling... [12288/50176]	Loss: 1.8497
Profiling... [13312/50176]	Loss: 1.8878
Profile done
epoch 1 train time consumed: 13.12s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4504
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.38964435330881,
                        "time": 9.034637390999706,
                        "accuracy": 0.450390625,
                        "total_cost": 3510422.854438741
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9674
Profiling... [2048/50176]	Loss: 1.9002
Profiling... [3072/50176]	Loss: 1.8598
Profiling... [4096/50176]	Loss: 1.9767
Profiling... [5120/50176]	Loss: 1.9053
Profiling... [6144/50176]	Loss: 1.8682
Profiling... [7168/50176]	Loss: 1.9871
Profiling... [8192/50176]	Loss: 1.8081
Profiling... [9216/50176]	Loss: 1.8622
Profiling... [10240/50176]	Loss: 1.9282
Profiling... [11264/50176]	Loss: 1.8375
Profiling... [12288/50176]	Loss: 1.8676
Profiling... [13312/50176]	Loss: 1.7381
Profile done
epoch 1 train time consumed: 13.46s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4287
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.44672508287036,
                        "time": 9.266082604000985,
                        "accuracy": 0.4287109375,
                        "total_cost": 3782419.1404031357
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9681
Profiling... [2048/50176]	Loss: 1.8765
Profiling... [3072/50176]	Loss: 1.8979
Profiling... [4096/50176]	Loss: 1.9256
Profiling... [5120/50176]	Loss: 1.8657
Profiling... [6144/50176]	Loss: 1.8878
Profiling... [7168/50176]	Loss: 1.9061
Profiling... [8192/50176]	Loss: 1.8725
Profiling... [9216/50176]	Loss: 1.9138
Profiling... [10240/50176]	Loss: 1.9084
Profiling... [11264/50176]	Loss: 1.9424
Profiling... [12288/50176]	Loss: 1.8440
Profiling... [13312/50176]	Loss: 1.8295
Profile done
epoch 1 train time consumed: 15.29s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4632
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.45790152443467,
                        "time": 10.720317617000546,
                        "accuracy": 0.46318359375,
                        "total_cost": 4050349.814392785
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0487
Profiling... [2048/50176]	Loss: 1.8594
Profiling... [3072/50176]	Loss: 1.8037
Profiling... [4096/50176]	Loss: 1.8661
Profiling... [5120/50176]	Loss: 1.8234
Profiling... [6144/50176]	Loss: 1.9050
Profiling... [7168/50176]	Loss: 1.8888
Profiling... [8192/50176]	Loss: 1.9246
Profiling... [9216/50176]	Loss: 1.8706
Profiling... [10240/50176]	Loss: 1.7357
Profiling... [11264/50176]	Loss: 1.8675
Profiling... [12288/50176]	Loss: 1.9200
Profiling... [13312/50176]	Loss: 1.7843
Profile done
epoch 1 train time consumed: 38.49s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4164
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.34956030198279,
                        "time": 28.476956783000787,
                        "accuracy": 0.41640625,
                        "total_cost": 11967801.724938419
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9835
Profiling... [2048/50176]	Loss: 1.9307
Profiling... [3072/50176]	Loss: 1.9911
Profiling... [4096/50176]	Loss: 1.9197
Profiling... [5120/50176]	Loss: 1.8602
Profiling... [6144/50176]	Loss: 1.7650
Profiling... [7168/50176]	Loss: 1.9079
Profiling... [8192/50176]	Loss: 1.8773
Profiling... [9216/50176]	Loss: 1.8473
Profiling... [10240/50176]	Loss: 1.8515
Profiling... [11264/50176]	Loss: 1.8271
Profiling... [12288/50176]	Loss: 1.8561
Profiling... [13312/50176]	Loss: 1.8416
Profile done
epoch 1 train time consumed: 13.44s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4444
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.38998991316983,
                        "time": 9.258530113000234,
                        "accuracy": 0.44443359375,
                        "total_cost": 3645635.2367603644
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0396
Profiling... [2048/50176]	Loss: 1.8921
Profiling... [3072/50176]	Loss: 1.9177
Profiling... [4096/50176]	Loss: 1.8180
Profiling... [5120/50176]	Loss: 1.9338
Profiling... [6144/50176]	Loss: 1.8168
Profiling... [7168/50176]	Loss: 1.8677
Profiling... [8192/50176]	Loss: 1.8800
Profiling... [9216/50176]	Loss: 1.8435
Profiling... [10240/50176]	Loss: 1.7821
Profiling... [11264/50176]	Loss: 1.8420
Profiling... [12288/50176]	Loss: 1.8402
Profiling... [13312/50176]	Loss: 1.8353
Profile done
epoch 1 train time consumed: 13.83s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4357
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.44568284860206,
                        "time": 9.513123589000315,
                        "accuracy": 0.4357421875,
                        "total_cost": 3820600.0608445914
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0165
Profiling... [2048/50176]	Loss: 1.9731
Profiling... [3072/50176]	Loss: 1.8872
Profiling... [4096/50176]	Loss: 1.8934
Profiling... [5120/50176]	Loss: 1.8925
Profiling... [6144/50176]	Loss: 1.7983
Profiling... [7168/50176]	Loss: 1.9360
Profiling... [8192/50176]	Loss: 1.9177
Profiling... [9216/50176]	Loss: 1.8553
Profiling... [10240/50176]	Loss: 1.9429
Profiling... [11264/50176]	Loss: 1.8343
Profiling... [12288/50176]	Loss: 1.9440
Profiling... [13312/50176]	Loss: 1.9052
Profile done
epoch 1 train time consumed: 15.53s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4580
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.45813729387977,
                        "time": 10.94240782900124,
                        "accuracy": 0.4580078125,
                        "total_cost": 4180979.7077974887
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0642
Profiling... [2048/50176]	Loss: 2.0690
Profiling... [3072/50176]	Loss: 1.8883
Profiling... [4096/50176]	Loss: 1.9441
Profiling... [5120/50176]	Loss: 1.9198
Profiling... [6144/50176]	Loss: 1.8808
Profiling... [7168/50176]	Loss: 1.9381
Profiling... [8192/50176]	Loss: 1.9365
Profiling... [9216/50176]	Loss: 1.8068
Profiling... [10240/50176]	Loss: 1.8798
Profiling... [11264/50176]	Loss: 1.8584
Profiling... [12288/50176]	Loss: 1.8902
Profiling... [13312/50176]	Loss: 1.8274
Profile done
epoch 1 train time consumed: 38.10s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4482
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.35325073738872,
                        "time": 27.99168132900013,
                        "accuracy": 0.4482421875,
                        "total_cost": 10928342.688794822
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9156
Profiling... [2048/50176]	Loss: 1.9389
Profiling... [3072/50176]	Loss: 1.9839
Profiling... [4096/50176]	Loss: 1.9677
Profiling... [5120/50176]	Loss: 1.9077
Profiling... [6144/50176]	Loss: 1.8787
Profiling... [7168/50176]	Loss: 1.8723
Profiling... [8192/50176]	Loss: 1.8670
Profiling... [9216/50176]	Loss: 1.8438
Profiling... [10240/50176]	Loss: 1.9229
Profiling... [11264/50176]	Loss: 1.9389
Profiling... [12288/50176]	Loss: 1.8863
Profiling... [13312/50176]	Loss: 1.8239
Profile done
epoch 1 train time consumed: 12.97s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4317
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.39345171265387,
                        "time": 8.912698329000705,
                        "accuracy": 0.43173828125,
                        "total_cost": 3612656.730506506
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9087
Profiling... [2048/50176]	Loss: 1.9175
Profiling... [3072/50176]	Loss: 1.9225
Profiling... [4096/50176]	Loss: 1.8988
Profiling... [5120/50176]	Loss: 1.9039
Profiling... [6144/50176]	Loss: 1.8462
Profiling... [7168/50176]	Loss: 1.8205
Profiling... [8192/50176]	Loss: 1.8912
Profiling... [9216/50176]	Loss: 1.8802
Profiling... [10240/50176]	Loss: 1.8846
Profiling... [11264/50176]	Loss: 1.8220
Profiling... [12288/50176]	Loss: 1.8569
Profiling... [13312/50176]	Loss: 1.8613
Profile done
epoch 1 train time consumed: 13.44s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4715
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.44797323268172,
                        "time": 9.22488543899999,
                        "accuracy": 0.471484375,
                        "total_cost": 3423983.9906147434
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9987
Profiling... [2048/50176]	Loss: 1.8866
Profiling... [3072/50176]	Loss: 1.9362
Profiling... [4096/50176]	Loss: 1.9781
Profiling... [5120/50176]	Loss: 1.9446
Profiling... [6144/50176]	Loss: 1.8437
Profiling... [7168/50176]	Loss: 1.8792
Profiling... [8192/50176]	Loss: 1.9949
Profiling... [9216/50176]	Loss: 1.8755
Profiling... [10240/50176]	Loss: 1.9347
Profiling... [11264/50176]	Loss: 1.8929
Profiling... [12288/50176]	Loss: 1.7834
Profiling... [13312/50176]	Loss: 1.7041
Profile done
epoch 1 train time consumed: 15.30s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4723
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.4585542294978,
                        "time": 10.72784756100009,
                        "accuracy": 0.472265625,
                        "total_cost": 3975248.7240099586
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9033
Profiling... [2048/50176]	Loss: 1.9302
Profiling... [3072/50176]	Loss: 2.0069
Profiling... [4096/50176]	Loss: 1.9473
Profiling... [5120/50176]	Loss: 1.8864
Profiling... [6144/50176]	Loss: 1.8039
Profiling... [7168/50176]	Loss: 1.8137
Profiling... [8192/50176]	Loss: 1.8522
Profiling... [9216/50176]	Loss: 1.8291
Profiling... [10240/50176]	Loss: 1.7477
Profiling... [11264/50176]	Loss: 1.8673
Profiling... [12288/50176]	Loss: 1.8070
Profiling... [13312/50176]	Loss: 1.8298
Profile done
epoch 1 train time consumed: 38.49s
Validation Epoch: 7, Average loss: 0.0029, Accuracy: 0.3043
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.35299196060956,
                        "time": 28.467656173999785,
                        "accuracy": 0.304296875,
                        "total_cost": 16371643.088513354
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9283
Profiling... [2048/50176]	Loss: 2.0190
Profiling... [3072/50176]	Loss: 1.9381
Profiling... [4096/50176]	Loss: 1.9765
Profiling... [5120/50176]	Loss: 1.9502
Profiling... [6144/50176]	Loss: 2.0115
Profiling... [7168/50176]	Loss: 1.9327
Profiling... [8192/50176]	Loss: 1.9029
Profiling... [9216/50176]	Loss: 1.9533
Profiling... [10240/50176]	Loss: 1.8995
Profiling... [11264/50176]	Loss: 1.8757
Profiling... [12288/50176]	Loss: 1.9386
Profiling... [13312/50176]	Loss: 1.9366
Profile done
epoch 1 train time consumed: 12.97s
Validation Epoch: 7, Average loss: 0.0025, Accuracy: 0.3561
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.38788555997188,
                        "time": 8.891548269999475,
                        "accuracy": 0.3560546875,
                        "total_cost": 4370174.026286083
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0069
Profiling... [2048/50176]	Loss: 1.9696
Profiling... [3072/50176]	Loss: 1.9934
Profiling... [4096/50176]	Loss: 1.8926
Profiling... [5120/50176]	Loss: 1.9506
Profiling... [6144/50176]	Loss: 1.9613
Profiling... [7168/50176]	Loss: 1.9710
Profiling... [8192/50176]	Loss: 2.0465
Profiling... [9216/50176]	Loss: 1.7667
Profiling... [10240/50176]	Loss: 1.9219
Profiling... [11264/50176]	Loss: 1.8666
Profiling... [12288/50176]	Loss: 1.8956
Profiling... [13312/50176]	Loss: 1.8949
Profile done
epoch 1 train time consumed: 13.35s
Validation Epoch: 7, Average loss: 0.0035, Accuracy: 0.2038
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.44005960610477,
                        "time": 9.15323533999981,
                        "accuracy": 0.20380859375,
                        "total_cost": 7859414.340814403
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9721
Profiling... [2048/50176]	Loss: 1.8756
Profiling... [3072/50176]	Loss: 1.9311
Profiling... [4096/50176]	Loss: 1.9379
Profiling... [5120/50176]	Loss: 1.9471
Profiling... [6144/50176]	Loss: 1.9359
Profiling... [7168/50176]	Loss: 1.7950
Profiling... [8192/50176]	Loss: 1.9664
Profiling... [9216/50176]	Loss: 1.8869
Profiling... [10240/50176]	Loss: 1.9710
Profiling... [11264/50176]	Loss: 1.9911
Profiling... [12288/50176]	Loss: 1.9925
Profiling... [13312/50176]	Loss: 1.8043
Profile done
epoch 1 train time consumed: 15.04s
Validation Epoch: 7, Average loss: 0.0034, Accuracy: 0.2687
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.45056307252653,
                        "time": 10.559458955998707,
                        "accuracy": 0.26875,
                        "total_cost": 6875926.762045669
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0079
Profiling... [2048/50176]	Loss: 1.9611
Profiling... [3072/50176]	Loss: 1.9505
Profiling... [4096/50176]	Loss: 1.9387
Profiling... [5120/50176]	Loss: 1.9258
Profiling... [6144/50176]	Loss: 1.8700
Profiling... [7168/50176]	Loss: 1.9077
Profiling... [8192/50176]	Loss: 1.8257
Profiling... [9216/50176]	Loss: 1.8934
Profiling... [10240/50176]	Loss: 1.8430
Profiling... [11264/50176]	Loss: 1.9412
Profiling... [12288/50176]	Loss: 1.8023
Profiling... [13312/50176]	Loss: 1.8701
Profile done
epoch 1 train time consumed: 37.71s
Validation Epoch: 7, Average loss: 0.0030, Accuracy: 0.2718
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.34550049312723,
                        "time": 27.683222478999596,
                        "accuracy": 0.27177734375,
                        "total_cost": 17825488.567145985
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0627
Profiling... [2048/50176]	Loss: 1.9547
Profiling... [3072/50176]	Loss: 1.9003
Profiling... [4096/50176]	Loss: 2.0015
Profiling... [5120/50176]	Loss: 1.8292
Profiling... [6144/50176]	Loss: 1.9498
Profiling... [7168/50176]	Loss: 1.8039
Profiling... [8192/50176]	Loss: 1.9323
Profiling... [9216/50176]	Loss: 1.8914
Profiling... [10240/50176]	Loss: 1.8089
Profiling... [11264/50176]	Loss: 1.9539
Profiling... [12288/50176]	Loss: 1.8890
Profiling... [13312/50176]	Loss: 1.9164
Profile done
epoch 1 train time consumed: 12.95s
Validation Epoch: 7, Average loss: 0.0038, Accuracy: 0.2282
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.38165066380918,
                        "time": 8.904725410000538,
                        "accuracy": 0.22822265625,
                        "total_cost": 6828099.244638839
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0104
Profiling... [2048/50176]	Loss: 1.9786
Profiling... [3072/50176]	Loss: 1.9125
Profiling... [4096/50176]	Loss: 2.0379
Profiling... [5120/50176]	Loss: 1.9607
Profiling... [6144/50176]	Loss: 1.9993
Profiling... [7168/50176]	Loss: 1.9212
Profiling... [8192/50176]	Loss: 1.9405
Profiling... [9216/50176]	Loss: 1.8599
Profiling... [10240/50176]	Loss: 1.8681
Profiling... [11264/50176]	Loss: 1.7857
Profiling... [12288/50176]	Loss: 1.9670
Profiling... [13312/50176]	Loss: 1.8505
Profile done
epoch 1 train time consumed: 13.47s
Validation Epoch: 7, Average loss: 0.0038, Accuracy: 0.2064
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.43229342939522,
                        "time": 9.183840538000368,
                        "accuracy": 0.2064453125,
                        "total_cost": 7784977.409695676
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9613
Profiling... [2048/50176]	Loss: 1.9865
Profiling... [3072/50176]	Loss: 1.9947
Profiling... [4096/50176]	Loss: 1.8863
Profiling... [5120/50176]	Loss: 1.9604
Profiling... [6144/50176]	Loss: 2.0259
Profiling... [7168/50176]	Loss: 1.8783
Profiling... [8192/50176]	Loss: 1.9541
Profiling... [9216/50176]	Loss: 2.0171
Profiling... [10240/50176]	Loss: 1.9243
Profiling... [11264/50176]	Loss: 1.8738
Profiling... [12288/50176]	Loss: 1.8566
Profiling... [13312/50176]	Loss: 1.8564
Profile done
epoch 1 train time consumed: 15.11s
Validation Epoch: 7, Average loss: 0.0024, Accuracy: 0.3765
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.44188681691551,
                        "time": 10.580410869999469,
                        "accuracy": 0.37646484375,
                        "total_cost": 4918312.912850596
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9916
Profiling... [2048/50176]	Loss: 2.0487
Profiling... [3072/50176]	Loss: 1.9648
Profiling... [4096/50176]	Loss: 1.9418
Profiling... [5120/50176]	Loss: 1.9965
Profiling... [6144/50176]	Loss: 1.7945
Profiling... [7168/50176]	Loss: 1.8312
Profiling... [8192/50176]	Loss: 1.8302
Profiling... [9216/50176]	Loss: 1.9203
Profiling... [10240/50176]	Loss: 1.7821
Profiling... [11264/50176]	Loss: 1.9068
Profiling... [12288/50176]	Loss: 1.8790
Profiling... [13312/50176]	Loss: 1.8914
Profile done
epoch 1 train time consumed: 37.70s
Validation Epoch: 7, Average loss: 0.0024, Accuracy: 0.3775
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.3383794535567,
                        "time": 27.631091570001445,
                        "accuracy": 0.3775390625,
                        "total_cost": 12807789.987957215
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0244
Profiling... [2048/50176]	Loss: 1.8705
Profiling... [3072/50176]	Loss: 1.8514
Profiling... [4096/50176]	Loss: 1.8670
Profiling... [5120/50176]	Loss: 1.8904
Profiling... [6144/50176]	Loss: 1.9903
Profiling... [7168/50176]	Loss: 1.9308
Profiling... [8192/50176]	Loss: 1.7782
Profiling... [9216/50176]	Loss: 1.8978
Profiling... [10240/50176]	Loss: 1.9045
Profiling... [11264/50176]	Loss: 1.8545
Profiling... [12288/50176]	Loss: 1.8232
Profiling... [13312/50176]	Loss: 1.8404
Profile done
epoch 1 train time consumed: 13.14s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4420
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.37596562142788,
                        "time": 8.912775909999255,
                        "accuracy": 0.4419921875,
                        "total_cost": 3528876.3656028863
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9601
Profiling... [2048/50176]	Loss: 1.9377
Profiling... [3072/50176]	Loss: 2.0011
Profiling... [4096/50176]	Loss: 1.9925
Profiling... [5120/50176]	Loss: 1.9518
Profiling... [6144/50176]	Loss: 2.0136
Profiling... [7168/50176]	Loss: 1.9645
Profiling... [8192/50176]	Loss: 1.9188
Profiling... [9216/50176]	Loss: 1.9338
Profiling... [10240/50176]	Loss: 1.9342
Profiling... [11264/50176]	Loss: 1.8888
Profiling... [12288/50176]	Loss: 1.8657
Profiling... [13312/50176]	Loss: 1.9022
Profile done
epoch 1 train time consumed: 13.36s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3127
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.42490935969374,
                        "time": 9.192978019000293,
                        "accuracy": 0.3126953125,
                        "total_cost": 5144852.158041389
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9889
Profiling... [2048/50176]	Loss: 1.8839
Profiling... [3072/50176]	Loss: 1.8744
Profiling... [4096/50176]	Loss: 1.9243
Profiling... [5120/50176]	Loss: 1.9431
Profiling... [6144/50176]	Loss: 1.9195
Profiling... [7168/50176]	Loss: 1.9384
Profiling... [8192/50176]	Loss: 1.8507
Profiling... [9216/50176]	Loss: 1.8494
Profiling... [10240/50176]	Loss: 1.9094
Profiling... [11264/50176]	Loss: 1.9935
Profiling... [12288/50176]	Loss: 1.9841
Profiling... [13312/50176]	Loss: 1.8927
Profile done
epoch 1 train time consumed: 15.15s
Validation Epoch: 7, Average loss: 0.0028, Accuracy: 0.3402
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.43490636476365,
                        "time": 10.58661137099989,
                        "accuracy": 0.340234375,
                        "total_cost": 5445237.536404077
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9754
Profiling... [2048/50176]	Loss: 1.9307
Profiling... [3072/50176]	Loss: 1.8864
Profiling... [4096/50176]	Loss: 1.9378
Profiling... [5120/50176]	Loss: 1.9335
Profiling... [6144/50176]	Loss: 1.9158
Profiling... [7168/50176]	Loss: 1.9535
Profiling... [8192/50176]	Loss: 1.9531
Profiling... [9216/50176]	Loss: 1.9076
Profiling... [10240/50176]	Loss: 1.7720
Profiling... [11264/50176]	Loss: 1.9695
Profiling... [12288/50176]	Loss: 1.8309
Profiling... [13312/50176]	Loss: 1.8712
Profile done
epoch 1 train time consumed: 37.64s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4056
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.33692749669206,
                        "time": 27.75565030299913,
                        "accuracy": 0.40556640625,
                        "total_cost": 11976432.781838296
                    },
                    
[Training Loop] The optimal parameters are lr: 0.005 dr: 0.0 pl: 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[GPU_0] Set GPU power limit to 125W.
[GPU_0] Set GPU power limit to 125W.
Training Epoch: 7 [128/50048]	Loss: 1.9141
Training Epoch: 7 [256/50048]	Loss: 1.8658
Training Epoch: 7 [384/50048]	Loss: 1.9636
Training Epoch: 7 [512/50048]	Loss: 1.7733
Training Epoch: 7 [640/50048]	Loss: 1.8732
Training Epoch: 7 [768/50048]	Loss: 1.8101
Training Epoch: 7 [896/50048]	Loss: 1.8599
Training Epoch: 7 [1024/50048]	Loss: 1.8248
Training Epoch: 7 [1152/50048]	Loss: 1.8501
Training Epoch: 7 [1280/50048]	Loss: 1.6866
Training Epoch: 7 [1408/50048]	Loss: 1.8284
Training Epoch: 7 [1536/50048]	Loss: 1.9140
Training Epoch: 7 [1664/50048]	Loss: 2.1422
Training Epoch: 7 [1792/50048]	Loss: 1.7763
Training Epoch: 7 [1920/50048]	Loss: 1.9320
Training Epoch: 7 [2048/50048]	Loss: 1.7942
Training Epoch: 7 [2176/50048]	Loss: 1.7669
Training Epoch: 7 [2304/50048]	Loss: 2.1148
Training Epoch: 7 [2432/50048]	Loss: 2.0610
Training Epoch: 7 [2560/50048]	Loss: 1.8271
Training Epoch: 7 [2688/50048]	Loss: 1.8360
Training Epoch: 7 [2816/50048]	Loss: 1.9601
Training Epoch: 7 [2944/50048]	Loss: 1.9164
Training Epoch: 7 [3072/50048]	Loss: 1.8201
Training Epoch: 7 [3200/50048]	Loss: 1.9038
Training Epoch: 7 [3328/50048]	Loss: 1.7568
Training Epoch: 7 [3456/50048]	Loss: 1.8079
Training Epoch: 7 [3584/50048]	Loss: 1.7873
Training Epoch: 7 [3712/50048]	Loss: 1.9115
Training Epoch: 7 [3840/50048]	Loss: 1.9136
Training Epoch: 7 [3968/50048]	Loss: 1.9970
Training Epoch: 7 [4096/50048]	Loss: 1.5942
Training Epoch: 7 [4224/50048]	Loss: 1.6856
Training Epoch: 7 [4352/50048]	Loss: 1.9314
Training Epoch: 7 [4480/50048]	Loss: 2.0709
Training Epoch: 7 [4608/50048]	Loss: 1.7882
Training Epoch: 7 [4736/50048]	Loss: 1.9269
Training Epoch: 7 [4864/50048]	Loss: 1.7881
Training Epoch: 7 [4992/50048]	Loss: 1.8242
Training Epoch: 7 [5120/50048]	Loss: 1.9506
Training Epoch: 7 [5248/50048]	Loss: 1.8051
Training Epoch: 7 [5376/50048]	Loss: 1.7472
Training Epoch: 7 [5504/50048]	Loss: 1.8417
Training Epoch: 7 [5632/50048]	Loss: 1.7530
Training Epoch: 7 [5760/50048]	Loss: 1.5406
Training Epoch: 7 [5888/50048]	Loss: 1.9979
Training Epoch: 7 [6016/50048]	Loss: 1.7294
Training Epoch: 7 [6144/50048]	Loss: 1.9363
Training Epoch: 7 [6272/50048]	Loss: 1.8365
Training Epoch: 7 [6400/50048]	Loss: 1.7275
Training Epoch: 7 [6528/50048]	Loss: 1.9351
Training Epoch: 7 [6656/50048]	Loss: 1.5556
Training Epoch: 7 [6784/50048]	Loss: 1.7001
Training Epoch: 7 [6912/50048]	Loss: 1.8078
Training Epoch: 7 [7040/50048]	Loss: 2.0463
Training Epoch: 7 [7168/50048]	Loss: 1.6614
Training Epoch: 7 [7296/50048]	Loss: 1.8269
Training Epoch: 7 [7424/50048]	Loss: 1.8494
Training Epoch: 7 [7552/50048]	Loss: 1.8519
Training Epoch: 7 [7680/50048]	Loss: 1.6799
Training Epoch: 7 [7808/50048]	Loss: 2.0281
Training Epoch: 7 [7936/50048]	Loss: 1.6491
Training Epoch: 7 [8064/50048]	Loss: 1.6823
Training Epoch: 7 [8192/50048]	Loss: 1.5906
Training Epoch: 7 [8320/50048]	Loss: 1.8467
Training Epoch: 7 [8448/50048]	Loss: 1.6612
Training Epoch: 7 [8576/50048]	Loss: 1.5931
Training Epoch: 7 [8704/50048]	Loss: 1.7333
Training Epoch: 7 [8832/50048]	Loss: 1.8747
Training Epoch: 7 [8960/50048]	Loss: 1.6866
Training Epoch: 7 [9088/50048]	Loss: 1.8189
Training Epoch: 7 [9216/50048]	Loss: 1.4679
Training Epoch: 7 [9344/50048]	Loss: 1.8015
Training Epoch: 7 [9472/50048]	Loss: 1.7241
Training Epoch: 7 [9600/50048]	Loss: 1.8879
Training Epoch: 7 [9728/50048]	Loss: 1.8286
Training Epoch: 7 [9856/50048]	Loss: 1.7247
Training Epoch: 7 [9984/50048]	Loss: 1.8684
Training Epoch: 7 [10112/50048]	Loss: 1.7615
Training Epoch: 7 [10240/50048]	Loss: 1.6830
Training Epoch: 7 [10368/50048]	Loss: 1.8475
Training Epoch: 7 [10496/50048]	Loss: 1.7286
Training Epoch: 7 [10624/50048]	Loss: 1.8909
Training Epoch: 7 [10752/50048]	Loss: 1.9222
Training Epoch: 7 [10880/50048]	Loss: 1.7547
Training Epoch: 7 [11008/50048]	Loss: 1.8252
Training Epoch: 7 [11136/50048]	Loss: 1.8903
Training Epoch: 7 [11264/50048]	Loss: 1.9996
Training Epoch: 7 [11392/50048]	Loss: 2.0054
Training Epoch: 7 [11520/50048]	Loss: 1.9482
Training Epoch: 7 [11648/50048]	Loss: 1.9401
Training Epoch: 7 [11776/50048]	Loss: 1.8355
Training Epoch: 7 [11904/50048]	Loss: 1.6755
Training Epoch: 7 [12032/50048]	Loss: 2.0115
Training Epoch: 7 [12160/50048]	Loss: 1.7777
Training Epoch: 7 [12288/50048]	Loss: 2.0175
Training Epoch: 7 [12416/50048]	Loss: 1.8540
Training Epoch: 7 [12544/50048]	Loss: 1.6979
Training Epoch: 7 [12672/50048]	Loss: 1.8951
Training Epoch: 7 [12800/50048]	Loss: 1.7588
Training Epoch: 7 [12928/50048]	Loss: 1.9621
Training Epoch: 7 [13056/50048]	Loss: 1.8460
Training Epoch: 7 [13184/50048]	Loss: 1.8333
Training Epoch: 7 [13312/50048]	Loss: 1.7505
Training Epoch: 7 [13440/50048]	Loss: 1.7392
Training Epoch: 7 [13568/50048]	Loss: 1.7995
Training Epoch: 7 [13696/50048]	Loss: 1.8802
Training Epoch: 7 [13824/50048]	Loss: 1.6329
Training Epoch: 7 [13952/50048]	Loss: 1.7061
Training Epoch: 7 [14080/50048]	Loss: 1.8113
Training Epoch: 7 [14208/50048]	Loss: 1.6419
Training Epoch: 7 [14336/50048]	Loss: 1.7026
Training Epoch: 7 [14464/50048]	Loss: 1.8030
Training Epoch: 7 [14592/50048]	Loss: 1.9306
Training Epoch: 7 [14720/50048]	Loss: 1.7652
Training Epoch: 7 [14848/50048]	Loss: 1.6297
Training Epoch: 7 [14976/50048]	Loss: 1.7838
Training Epoch: 7 [15104/50048]	Loss: 1.6833
Training Epoch: 7 [15232/50048]	Loss: 1.9309
Training Epoch: 7 [15360/50048]	Loss: 1.7706
Training Epoch: 7 [15488/50048]	Loss: 1.6674
Training Epoch: 7 [15616/50048]	Loss: 2.1503
Training Epoch: 7 [15744/50048]	Loss: 1.8992
Training Epoch: 7 [15872/50048]	Loss: 1.7861
Training Epoch: 7 [16000/50048]	Loss: 1.7792
Training Epoch: 7 [16128/50048]	Loss: 1.6337
Training Epoch: 7 [16256/50048]	Loss: 1.7715
Training Epoch: 7 [16384/50048]	Loss: 1.9001
Training Epoch: 7 [16512/50048]	Loss: 1.9656
Training Epoch: 7 [16640/50048]	Loss: 1.7959
Training Epoch: 7 [16768/50048]	Loss: 1.6620
Training Epoch: 7 [16896/50048]	Loss: 1.8495
Training Epoch: 7 [17024/50048]	Loss: 1.8994
Training Epoch: 7 [17152/50048]	Loss: 1.7435
Training Epoch: 7 [17280/50048]	Loss: 2.0204
Training Epoch: 7 [17408/50048]	Loss: 2.0104
Training Epoch: 7 [17536/50048]	Loss: 1.7792
Training Epoch: 7 [17664/50048]	Loss: 1.7798
Training Epoch: 7 [17792/50048]	Loss: 1.5519
Training Epoch: 7 [17920/50048]	Loss: 1.9961
Training Epoch: 7 [18048/50048]	Loss: 2.3716
Training Epoch: 7 [18176/50048]	Loss: 1.9266
Training Epoch: 7 [18304/50048]	Loss: 1.6672
Training Epoch: 7 [18432/50048]	Loss: 1.7927
Training Epoch: 7 [18560/50048]	Loss: 2.0734
Training Epoch: 7 [18688/50048]	Loss: 2.0480
Training Epoch: 7 [18816/50048]	Loss: 1.7898
Training Epoch: 7 [18944/50048]	Loss: 1.8128
Training Epoch: 7 [19072/50048]	Loss: 1.6981
Training Epoch: 7 [19200/50048]	Loss: 2.1061
Training Epoch: 7 [19328/50048]	Loss: 1.9712
Training Epoch: 7 [19456/50048]	Loss: 1.9719
Training Epoch: 7 [19584/50048]	Loss: 1.7597
Training Epoch: 7 [19712/50048]	Loss: 1.5420
Training Epoch: 7 [19840/50048]	Loss: 1.7519
Training Epoch: 7 [19968/50048]	Loss: 2.0447
Training Epoch: 7 [20096/50048]	Loss: 1.8469
Training Epoch: 7 [20224/50048]	Loss: 1.7634
Training Epoch: 7 [20352/50048]	Loss: 1.8220
Training Epoch: 7 [20480/50048]	Loss: 1.8281
Training Epoch: 7 [20608/50048]	Loss: 1.8334
Training Epoch: 7 [20736/50048]	Loss: 1.9275
Training Epoch: 7 [20864/50048]	Loss: 1.9038
Training Epoch: 7 [20992/50048]	Loss: 1.7274
Training Epoch: 7 [21120/50048]	Loss: 1.9197
Training Epoch: 7 [21248/50048]	Loss: 1.6871
Training Epoch: 7 [21376/50048]	Loss: 1.6929
Training Epoch: 7 [21504/50048]	Loss: 1.6622
Training Epoch: 7 [21632/50048]	Loss: 1.6877
Training Epoch: 7 [21760/50048]	Loss: 1.7374
Training Epoch: 7 [21888/50048]	Loss: 1.9663
Training Epoch: 7 [22016/50048]	Loss: 1.9034
Training Epoch: 7 [22144/50048]	Loss: 1.6926
Training Epoch: 7 [22272/50048]	Loss: 1.8184
Training Epoch: 7 [22400/50048]	Loss: 1.7208
Training Epoch: 7 [22528/50048]	Loss: 1.8103
Training Epoch: 7 [22656/50048]	Loss: 1.7163
Training Epoch: 7 [22784/50048]	Loss: 1.5544
Training Epoch: 7 [22912/50048]	Loss: 1.9267
Training Epoch: 7 [23040/50048]	Loss: 1.6368
Training Epoch: 7 [23168/50048]	Loss: 1.7976
Training Epoch: 7 [23296/50048]	Loss: 1.7938
Training Epoch: 7 [23424/50048]	Loss: 1.9450
Training Epoch: 7 [23552/50048]	Loss: 1.9289
Training Epoch: 7 [23680/50048]	Loss: 1.8377
Training Epoch: 7 [23808/50048]	Loss: 1.8754
Training Epoch: 7 [23936/50048]	Loss: 2.0848
Training Epoch: 7 [24064/50048]	Loss: 1.6492
Training Epoch: 7 [24192/50048]	Loss: 1.8502
Training Epoch: 7 [24320/50048]	Loss: 1.6937
Training Epoch: 7 [24448/50048]	Loss: 1.9320
Training Epoch: 7 [24576/50048]	Loss: 2.0437
Training Epoch: 7 [24704/50048]	Loss: 1.5509
Training Epoch: 7 [24832/50048]	Loss: 1.8466
Training Epoch: 7 [24960/50048]	Loss: 1.7786
Training Epoch: 7 [25088/50048]	Loss: 1.8747
Training Epoch: 7 [25216/50048]	Loss: 1.5490
Training Epoch: 7 [25344/50048]	Loss: 1.7087
Training Epoch: 7 [25472/50048]	Loss: 1.7337
Training Epoch: 7 [25600/50048]	Loss: 1.8204
Training Epoch: 7 [25728/50048]	Loss: 1.8901
Training Epoch: 7 [25856/50048]	Loss: 1.8552
Training Epoch: 7 [25984/50048]	Loss: 1.6933
Training Epoch: 7 [26112/50048]	Loss: 1.4324
Training Epoch: 7 [26240/50048]	Loss: 1.7184
Training Epoch: 7 [26368/50048]	Loss: 1.7936
Training Epoch: 7 [26496/50048]	Loss: 1.7527
Training Epoch: 7 [26624/50048]	Loss: 1.7585
Training Epoch: 7 [26752/50048]	Loss: 1.7417
Training Epoch: 7 [26880/50048]	Loss: 1.5919
Training Epoch: 7 [27008/50048]	Loss: 1.8698
Training Epoch: 7 [27136/50048]	Loss: 1.7513
Training Epoch: 7 [27264/50048]	Loss: 1.8228
Training Epoch: 7 [27392/50048]	Loss: 1.6964
Training Epoch: 7 [27520/50048]	Loss: 1.7165
Training Epoch: 7 [27648/50048]	Loss: 1.9697
Training Epoch: 7 [27776/50048]	Loss: 2.0226
Training Epoch: 7 [27904/50048]	Loss: 1.7615
Training Epoch: 7 [28032/50048]	Loss: 1.7874
Training Epoch: 7 [28160/50048]	Loss: 1.8253
Training Epoch: 7 [28288/50048]	Loss: 1.8298
Training Epoch: 7 [28416/50048]	Loss: 1.6979
Training Epoch: 7 [28544/50048]	Loss: 1.8328
Training Epoch: 7 [28672/50048]	Loss: 1.7015
Training Epoch: 7 [28800/50048]	Loss: 2.0567
Training Epoch: 7 [28928/50048]	Loss: 1.7444
Training Epoch: 7 [29056/50048]	Loss: 1.8187
Training Epoch: 7 [29184/50048]	Loss: 1.9190
Training Epoch: 7 [29312/50048]	Loss: 1.7261
Training Epoch: 7 [29440/50048]	Loss: 1.8914
Training Epoch: 7 [29568/50048]	Loss: 1.7452
Training Epoch: 7 [29696/50048]	Loss: 1.8492
Training Epoch: 7 [29824/50048]	Loss: 1.9937
Training Epoch: 7 [29952/50048]	Loss: 1.6801
Training Epoch: 7 [30080/50048]	Loss: 1.8293
Training Epoch: 7 [30208/50048]	Loss: 1.7582
Training Epoch: 7 [30336/50048]	Loss: 1.8746
Training Epoch: 7 [30464/50048]	Loss: 1.8000
Training Epoch: 7 [30592/50048]	Loss: 1.6436
Training Epoch: 7 [30720/50048]	Loss: 1.8622
Training Epoch: 7 [30848/50048]	Loss: 1.7065
Training Epoch: 7 [30976/50048]	Loss: 1.7674
Training Epoch: 7 [31104/50048]	Loss: 1.8371
Training Epoch: 7 [31232/50048]	Loss: 1.9801
Training Epoch: 7 [31360/50048]	Loss: 1.7892
Training Epoch: 7 [31488/50048]	Loss: 1.5528
Training Epoch: 7 [31616/50048]	Loss: 1.6348
Training Epoch: 7 [31744/50048]	Loss: 1.9913
Training Epoch: 7 [31872/50048]	Loss: 1.5913
Training Epoch: 7 [32000/50048]	Loss: 1.7731
Training Epoch: 7 [32128/50048]	Loss: 1.9615
Training Epoch: 7 [32256/50048]	Loss: 2.1065
Training Epoch: 7 [32384/50048]	Loss: 1.7059
Training Epoch: 7 [32512/50048]	Loss: 1.5074
Training Epoch: 7 [32640/50048]	Loss: 1.8035
Training Epoch: 7 [32768/50048]	Loss: 1.8890
Training Epoch: 7 [32896/50048]	Loss: 1.7262
Training Epoch: 7 [33024/50048]	Loss: 1.9193
Training Epoch: 7 [33152/50048]	Loss: 1.5998
Training Epoch: 7 [33280/50048]	Loss: 1.5456
Training Epoch: 7 [33408/50048]	Loss: 1.7503
Training Epoch: 7 [33536/50048]	Loss: 1.8129
Training Epoch: 7 [33664/50048]	Loss: 1.8633
Training Epoch: 7 [33792/50048]	Loss: 1.9602
Training Epoch: 7 [33920/50048]	Loss: 1.8037
Training Epoch: 7 [34048/50048]	Loss: 1.9259
Training Epoch: 7 [34176/50048]	Loss: 1.9579
Training Epoch: 7 [34304/50048]	Loss: 1.9820
Training Epoch: 7 [34432/50048]	Loss: 1.6336
Training Epoch: 7 [34560/50048]	Loss: 1.8347
Training Epoch: 7 [34688/50048]	Loss: 1.8679
Training Epoch: 7 [34816/50048]	Loss: 1.5627
Training Epoch: 7 [34944/50048]	Loss: 1.7052
Training Epoch: 7 [35072/50048]	Loss: 1.5516
Training Epoch: 7 [35200/50048]	Loss: 1.7899
Training Epoch: 7 [35328/50048]	Loss: 1.6630
Training Epoch: 7 [35456/50048]	Loss: 1.8036
Training Epoch: 7 [35584/50048]	Loss: 1.9754
Training Epoch: 7 [35712/50048]	Loss: 1.8006
Training Epoch: 7 [35840/50048]	Loss: 1.7943
Training Epoch: 7 [35968/50048]	Loss: 1.6813
Training Epoch: 7 [36096/50048]	Loss: 1.9874
Training Epoch: 7 [36224/50048]	Loss: 1.8301
Training Epoch: 7 [36352/50048]	Loss: 1.4074
Training Epoch: 7 [36480/50048]	Loss: 1.7871
Training Epoch: 7 [36608/50048]	Loss: 1.9539
Training Epoch: 7 [36736/50048]	Loss: 1.7614
Training Epoch: 7 [36864/50048]	Loss: 1.7187
Training Epoch: 7 [36992/50048]	Loss: 1.5551
Training Epoch: 7 [37120/50048]	Loss: 1.6411
Training Epoch: 7 [37248/50048]	Loss: 1.9065
Training Epoch: 7 [37376/50048]	Loss: 2.0653
Training Epoch: 7 [37504/50048]	Loss: 1.5231
Training Epoch: 7 [37632/50048]	Loss: 1.9440
Training Epoch: 7 [37760/50048]	Loss: 1.9772
Training Epoch: 7 [37888/50048]	Loss: 1.7824
Training Epoch: 7 [38016/50048]	Loss: 1.6926
Training Epoch: 7 [38144/50048]	Loss: 1.7063
Training Epoch: 7 [38272/50048]	Loss: 1.9059
Training Epoch: 7 [38400/50048]	Loss: 1.6840
Training Epoch: 7 [38528/50048]	Loss: 1.9538
Training Epoch: 7 [38656/50048]	Loss: 1.8291
Training Epoch: 7 [38784/50048]	Loss: 1.6813
Training Epoch: 7 [38912/50048]	Loss: 1.6408
Training Epoch: 7 [39040/50048]	Loss: 1.8667
Training Epoch: 7 [39168/50048]	Loss: 1.5424
Training Epoch: 7 [39296/50048]	Loss: 1.5786
Training Epoch: 7 [39424/50048]	Loss: 1.4529
Training Epoch: 7 [39552/50048]	Loss: 1.8716
Training Epoch: 7 [39680/50048]	Loss: 1.7653
Training Epoch: 7 [39808/50048]	Loss: 1.9112
Training Epoch: 7 [39936/50048]	Loss: 2.0511
Training Epoch: 7 [40064/50048]	Loss: 1.8878
Training Epoch: 7 [40192/50048]	Loss: 2.0186
Training Epoch: 7 [40320/50048]	Loss: 1.6475
Training Epoch: 7 [40448/50048]	Loss: 1.5242
Training Epoch: 7 [40576/50048]	Loss: 1.8240
Training Epoch: 7 [40704/50048]	Loss: 1.7700
Training Epoch: 7 [40832/50048]	Loss: 2.0908
Training Epoch: 7 [40960/50048]	Loss: 1.6252
Training Epoch: 7 [41088/50048]	Loss: 1.7469
Training Epoch: 7 [41216/50048]	Loss: 1.9753
Training Epoch: 7 [41344/50048]	Loss: 1.7427
Training Epoch: 7 [41472/50048]	Loss: 1.6733
Training Epoch: 7 [41600/50048]	Loss: 1.6958
Training Epoch: 7 [41728/50048]	Loss: 1.8645
Training Epoch: 7 [41856/50048]	Loss: 1.5483
Training Epoch: 7 [41984/50048]	Loss: 1.5526
Training Epoch: 7 [42112/50048]	Loss: 1.7174
Training Epoch: 7 [42240/50048]	Loss: 1.6106
Training Epoch: 7 [42368/50048]	Loss: 1.8721
Training Epoch: 7 [42496/50048]	Loss: 1.6404
Training Epoch: 7 [42624/50048]	Loss: 1.9030
Training Epoch: 7 [42752/50048]	Loss: 1.9235
Training Epoch: 7 [42880/50048]	Loss: 1.6344
Training Epoch: 7 [43008/50048]	Loss: 1.8099
Training Epoch: 7 [43136/50048]	Loss: 1.7086
Training Epoch: 7 [43264/50048]	Loss: 1.9558
Training Epoch: 7 [43392/50048]	Loss: 1.6211
Training Epoch: 7 [43520/50048]	Loss: 1.5899
Training Epoch: 7 [43648/50048]	Loss: 1.7075
Training Epoch: 7 [43776/50048]	Loss: 1.7464
Training Epoch: 7 [43904/50048]	Loss: 2.1251
Training Epoch: 7 [44032/50048]	Loss: 1.5638
Training Epoch: 7 [44160/50048]	Loss: 1.9498
Training Epoch: 7 [44288/50048]	Loss: 1.6853
Training Epoch: 7 [44416/50048]	Loss: 1.6742
Training Epoch: 7 [44544/50048]	Loss: 1.8938
Training Epoch: 7 [44672/50048]	Loss: 1.5844
Training Epoch: 7 [44800/50048]	Loss: 1.7355
Training Epoch: 7 [44928/50048]	Loss: 1.6471
Training Epoch: 7 [45056/50048]	Loss: 1.9909
Training Epoch: 7 [45184/50048]	Loss: 1.5736
Training Epoch: 7 [45312/50048]	Loss: 1.9453
Training Epoch: 7 [45440/50048]	Loss: 1.8540
Training Epoch: 7 [45568/50048]	Loss: 1.7444
Training Epoch: 7 [45696/50048]	Loss: 1.7340
Training Epoch: 7 [45824/50048]	Loss: 1.8117
Training Epoch: 7 [45952/50048]	Loss: 1.6527
Training Epoch: 7 [46080/50048]	Loss: 1.7741
Training Epoch: 7 [46208/50048]	Loss: 1.4994
Training Epoch: 7 [46336/50048]	Loss: 1.6941
Training Epoch: 7 [46464/50048]	Loss: 1.7700
Training Epoch: 7 [46592/50048]	Loss: 2.2270
Training Epoch: 7 [46720/50048]	Loss: 1.9752
Training Epoch: 7 [46848/50048]	Loss: 1.6614
Training Epoch: 7 [46976/50048]	Loss: 1.8289
Training Epoch: 7 [47104/50048]	Loss: 2.1127
Training Epoch: 7 [47232/50048]	Loss: 2.0179
Training Epoch: 7 [47360/50048]	Loss: 1.7707
Training Epoch: 7 [47488/50048]	Loss: 1.8885
Training Epoch: 7 [47616/50048]	Loss: 2.0392
Training Epoch: 7 [47744/50048]	Loss: 1.6727
Training Epoch: 7 [47872/50048]	Loss: 1.5350
Training Epoch: 7 [48000/50048]	Loss: 1.6391
Training Epoch: 7 [48128/50048]	Loss: 2.0555
Training Epoch: 7 [48256/50048]	Loss: 1.7132
Training Epoch: 7 [48384/50048]	Loss: 1.9059
Training Epoch: 7 [48512/50048]	Loss: 1.8671
Training Epoch: 7 [48640/50048]	Loss: 1.7234
Training Epoch: 7 [48768/50048]	Loss: 1.8146
Training Epoch: 7 [48896/50048]	Loss: 1.7845
Training Epoch: 7 [49024/50048]	Loss: 1.9203
Training Epoch: 7 [49152/50048]	Loss: 1.5201
Training Epoch: 7 [49280/50048]	Loss: 1.7188
Training Epoch: 7 [49408/50048]	Loss: 2.0874
Training Epoch: 7 [49536/50048]	Loss: 2.0040
Training Epoch: 7 [49664/50048]	Loss: 2.0449
Training Epoch: 7 [49792/50048]	Loss: 1.7970
Training Epoch: 7 [49920/50048]	Loss: 1.6448
Training Epoch: 7 [50048/50048]	Loss: 1.6988
Validation Epoch: 7, Average loss: 0.0159, Accuracy: 0.4641
Training Epoch: 8 [128/50048]	Loss: 1.7483
Training Epoch: 8 [256/50048]	Loss: 1.5669
Training Epoch: 8 [384/50048]	Loss: 1.8971
Training Epoch: 8 [512/50048]	Loss: 1.5320
Training Epoch: 8 [640/50048]	Loss: 1.8149
Training Epoch: 8 [768/50048]	Loss: 1.6328
Training Epoch: 8 [896/50048]	Loss: 1.8117
Training Epoch: 8 [1024/50048]	Loss: 1.8262
Training Epoch: 8 [1152/50048]	Loss: 1.6859
Training Epoch: 8 [1280/50048]	Loss: 1.6419
Training Epoch: 8 [1408/50048]	Loss: 1.8230
Training Epoch: 8 [1536/50048]	Loss: 1.7642
Training Epoch: 8 [1664/50048]	Loss: 1.5902
Training Epoch: 8 [1792/50048]	Loss: 1.4967
Training Epoch: 8 [1920/50048]	Loss: 1.6454
Training Epoch: 8 [2048/50048]	Loss: 1.7666
Training Epoch: 8 [2176/50048]	Loss: 1.6546
Training Epoch: 8 [2304/50048]	Loss: 1.6651
Training Epoch: 8 [2432/50048]	Loss: 1.5882
Training Epoch: 8 [2560/50048]	Loss: 1.5217
Training Epoch: 8 [2688/50048]	Loss: 1.6167
Training Epoch: 8 [2816/50048]	Loss: 1.6596
Training Epoch: 8 [2944/50048]	Loss: 1.6958
Training Epoch: 8 [3072/50048]	Loss: 1.8872
Training Epoch: 8 [3200/50048]	Loss: 1.5958
Training Epoch: 8 [3328/50048]	Loss: 1.7940
Training Epoch: 8 [3456/50048]	Loss: 1.7637
Training Epoch: 8 [3584/50048]	Loss: 1.6666
Training Epoch: 8 [3712/50048]	Loss: 1.7153
Training Epoch: 8 [3840/50048]	Loss: 1.7057
Training Epoch: 8 [3968/50048]	Loss: 1.5061
Training Epoch: 8 [4096/50048]	Loss: 1.6653
Training Epoch: 8 [4224/50048]	Loss: 1.6511
Training Epoch: 8 [4352/50048]	Loss: 1.5228
Training Epoch: 8 [4480/50048]	Loss: 1.5420
Training Epoch: 8 [4608/50048]	Loss: 1.7745
Training Epoch: 8 [4736/50048]	Loss: 1.9354
Training Epoch: 8 [4864/50048]	Loss: 1.6989
Training Epoch: 8 [4992/50048]	Loss: 1.7150
Training Epoch: 8 [5120/50048]	Loss: 1.6410
Training Epoch: 8 [5248/50048]	Loss: 1.8431
Training Epoch: 8 [5376/50048]	Loss: 1.5471
Training Epoch: 8 [5504/50048]	Loss: 1.5824
Training Epoch: 8 [5632/50048]	Loss: 1.9659
Training Epoch: 8 [5760/50048]	Loss: 1.6199
Training Epoch: 8 [5888/50048]	Loss: 1.4982
Training Epoch: 8 [6016/50048]	Loss: 1.7903
Training Epoch: 8 [6144/50048]	Loss: 1.8023
Training Epoch: 8 [6272/50048]	Loss: 1.8104
Training Epoch: 8 [6400/50048]	Loss: 1.8894
Training Epoch: 8 [6528/50048]	Loss: 1.7393
Training Epoch: 8 [6656/50048]	Loss: 1.9378
Training Epoch: 8 [6784/50048]	Loss: 1.6145
Training Epoch: 8 [6912/50048]	Loss: 1.8892
Training Epoch: 8 [7040/50048]	Loss: 1.7887
Training Epoch: 8 [7168/50048]	Loss: 1.5593
Training Epoch: 8 [7296/50048]	Loss: 1.6081
Training Epoch: 8 [7424/50048]	Loss: 1.5383
Training Epoch: 8 [7552/50048]	Loss: 1.7625
Training Epoch: 8 [7680/50048]	Loss: 1.4997
Training Epoch: 8 [7808/50048]	Loss: 1.6150
Training Epoch: 8 [7936/50048]	Loss: 1.9083
Training Epoch: 8 [8064/50048]	Loss: 1.7108
Training Epoch: 8 [8192/50048]	Loss: 1.6451
Training Epoch: 8 [8320/50048]	Loss: 1.5236
Training Epoch: 8 [8448/50048]	Loss: 1.8151
Training Epoch: 8 [8576/50048]	Loss: 1.6671
Training Epoch: 8 [8704/50048]	Loss: 1.7282
Training Epoch: 8 [8832/50048]	Loss: 1.6753
Training Epoch: 8 [8960/50048]	Loss: 1.7360
Training Epoch: 8 [9088/50048]	Loss: 1.6406
Training Epoch: 8 [9216/50048]	Loss: 1.5906
Training Epoch: 8 [9344/50048]	Loss: 1.8338
Training Epoch: 8 [9472/50048]	Loss: 1.6799
Training Epoch: 8 [9600/50048]	Loss: 1.4914
Training Epoch: 8 [9728/50048]	Loss: 1.5580
Training Epoch: 8 [9856/50048]	Loss: 1.6592
Training Epoch: 8 [9984/50048]	Loss: 1.9947
Training Epoch: 8 [10112/50048]	Loss: 1.5677
Training Epoch: 8 [10240/50048]	Loss: 1.9049
Training Epoch: 8 [10368/50048]	Loss: 1.7505
Training Epoch: 8 [10496/50048]	Loss: 1.7793
Training Epoch: 8 [10624/50048]	Loss: 1.3477
Training Epoch: 8 [10752/50048]	Loss: 1.7616
Training Epoch: 8 [10880/50048]	Loss: 1.7709
Training Epoch: 8 [11008/50048]	Loss: 1.6196
Training Epoch: 8 [11136/50048]	Loss: 1.6541
Training Epoch: 8 [11264/50048]	Loss: 1.5991
Training Epoch: 8 [11392/50048]	Loss: 1.6897
Training Epoch: 8 [11520/50048]	Loss: 1.6775
Training Epoch: 8 [11648/50048]	Loss: 1.4470
Training Epoch: 8 [11776/50048]	Loss: 1.7749
Training Epoch: 8 [11904/50048]	Loss: 1.9104
Training Epoch: 8 [12032/50048]	Loss: 1.8079
Training Epoch: 8 [12160/50048]	Loss: 1.6882
Training Epoch: 8 [12288/50048]	Loss: 1.8297
Training Epoch: 8 [12416/50048]	Loss: 1.7984
Training Epoch: 8 [12544/50048]	Loss: 1.7540
Training Epoch: 8 [12672/50048]	Loss: 1.5865
Training Epoch: 8 [12800/50048]	Loss: 1.7879
Training Epoch: 8 [12928/50048]	Loss: 1.6279
Training Epoch: 8 [13056/50048]	Loss: 2.0896
Training Epoch: 8 [13184/50048]	Loss: 1.9128
Training Epoch: 8 [13312/50048]	Loss: 1.7997
Training Epoch: 8 [13440/50048]	Loss: 1.8517
Training Epoch: 8 [13568/50048]	Loss: 1.6126
Training Epoch: 8 [13696/50048]	Loss: 1.4951
Training Epoch: 8 [13824/50048]	Loss: 1.7918
Training Epoch: 8 [13952/50048]	Loss: 1.9264
Training Epoch: 8 [14080/50048]	Loss: 1.7235
Training Epoch: 8 [14208/50048]	Loss: 1.7416
Training Epoch: 8 [14336/50048]	Loss: 1.7978
Training Epoch: 8 [14464/50048]	Loss: 1.7830
Training Epoch: 8 [14592/50048]	Loss: 1.6842
Training Epoch: 8 [14720/50048]	Loss: 1.7470
Training Epoch: 8 [14848/50048]	Loss: 1.7075
Training Epoch: 8 [14976/50048]	Loss: 1.5133
Training Epoch: 8 [15104/50048]	Loss: 2.0735
Training Epoch: 8 [15232/50048]	Loss: 1.6499
Training Epoch: 8 [15360/50048]	Loss: 1.7888
Training Epoch: 8 [15488/50048]	Loss: 1.7858
Training Epoch: 8 [15616/50048]	Loss: 1.8987
Training Epoch: 8 [15744/50048]	Loss: 1.5098
Training Epoch: 8 [15872/50048]	Loss: 1.7108
Training Epoch: 8 [16000/50048]	Loss: 1.9939
Training Epoch: 8 [16128/50048]	Loss: 1.8177
Training Epoch: 8 [16256/50048]	Loss: 1.6082
Training Epoch: 8 [16384/50048]	Loss: 1.7573
Training Epoch: 8 [16512/50048]	Loss: 1.7750
Training Epoch: 8 [16640/50048]	Loss: 2.0180
Training Epoch: 8 [16768/50048]	Loss: 1.5986
Training Epoch: 8 [16896/50048]	Loss: 1.7080
Training Epoch: 8 [17024/50048]	Loss: 1.7986
Training Epoch: 8 [17152/50048]	Loss: 1.5344
Training Epoch: 8 [17280/50048]	Loss: 1.5450
Training Epoch: 8 [17408/50048]	Loss: 1.5811
Training Epoch: 8 [17536/50048]	Loss: 1.4346
Training Epoch: 8 [17664/50048]	Loss: 1.5804
Training Epoch: 8 [17792/50048]	Loss: 1.5070
Training Epoch: 8 [17920/50048]	Loss: 1.6207
Training Epoch: 8 [18048/50048]	Loss: 1.6367
Training Epoch: 8 [18176/50048]	Loss: 1.7744
Training Epoch: 8 [18304/50048]	Loss: 1.5124
Training Epoch: 8 [18432/50048]	Loss: 1.5858
Training Epoch: 8 [18560/50048]	Loss: 1.6762
Training Epoch: 8 [18688/50048]	Loss: 1.6573
Training Epoch: 8 [18816/50048]	Loss: 1.4165
Training Epoch: 8 [18944/50048]	Loss: 1.5072
Training Epoch: 8 [19072/50048]	Loss: 1.8604
Training Epoch: 8 [19200/50048]	Loss: 1.5473
Training Epoch: 8 [19328/50048]	Loss: 1.8275
Training Epoch: 8 [19456/50048]	Loss: 2.0238
Training Epoch: 8 [19584/50048]	Loss: 1.9215
Training Epoch: 8 [19712/50048]	Loss: 1.5203
Training Epoch: 8 [19840/50048]	Loss: 1.7662
Training Epoch: 8 [19968/50048]	Loss: 1.7305
Training Epoch: 8 [20096/50048]	Loss: 1.7848
Training Epoch: 8 [20224/50048]	Loss: 1.8783
Training Epoch: 8 [20352/50048]	Loss: 1.8650
Training Epoch: 8 [20480/50048]	Loss: 1.4522
Training Epoch: 8 [20608/50048]	Loss: 1.8063
Training Epoch: 8 [20736/50048]	Loss: 1.7468
Training Epoch: 8 [20864/50048]	Loss: 1.7485
Training Epoch: 8 [20992/50048]	Loss: 1.6232
Training Epoch: 8 [21120/50048]	Loss: 1.5802
Training Epoch: 8 [21248/50048]	Loss: 1.6813
Training Epoch: 8 [21376/50048]	Loss: 2.1244
Training Epoch: 8 [21504/50048]	Loss: 1.6385
Training Epoch: 8 [21632/50048]	Loss: 1.8114
Training Epoch: 8 [21760/50048]	Loss: 1.6321
Training Epoch: 8 [21888/50048]	Loss: 1.9769
Training Epoch: 8 [22016/50048]	Loss: 1.6933
Training Epoch: 8 [22144/50048]	Loss: 1.8753
Training Epoch: 8 [22272/50048]	Loss: 1.7318
Training Epoch: 8 [22400/50048]	Loss: 1.6455
Training Epoch: 8 [22528/50048]	Loss: 1.7157
Training Epoch: 8 [22656/50048]	Loss: 1.5703
Training Epoch: 8 [22784/50048]	Loss: 1.7791
Training Epoch: 8 [22912/50048]	Loss: 1.6101
Training Epoch: 8 [23040/50048]	Loss: 1.6826
Training Epoch: 8 [23168/50048]	Loss: 2.0923
Training Epoch: 8 [23296/50048]	Loss: 2.0177
Training Epoch: 8 [23424/50048]	Loss: 1.5500
Training Epoch: 8 [23552/50048]	Loss: 1.6451
Training Epoch: 8 [23680/50048]	Loss: 1.8959
Training Epoch: 8 [23808/50048]	Loss: 1.6565
Training Epoch: 8 [23936/50048]	Loss: 1.6407
Training Epoch: 8 [24064/50048]	Loss: 2.0276
Training Epoch: 8 [24192/50048]	Loss: 1.6661
Training Epoch: 8 [24320/50048]	Loss: 1.5742
Training Epoch: 8 [24448/50048]	Loss: 1.5537
Training Epoch: 8 [24576/50048]	Loss: 1.5939
Training Epoch: 8 [24704/50048]	Loss: 1.7739
Training Epoch: 8 [24832/50048]	Loss: 1.7807
Training Epoch: 8 [24960/50048]	Loss: 1.4618
Training Epoch: 8 [25088/50048]	Loss: 1.8170
Training Epoch: 8 [25216/50048]	Loss: 1.5535
Training Epoch: 8 [25344/50048]	Loss: 1.6253
Training Epoch: 8 [25472/50048]	Loss: 1.6725
Training Epoch: 8 [25600/50048]	Loss: 1.7408
Training Epoch: 8 [25728/50048]	Loss: 1.9165
Training Epoch: 8 [25856/50048]	Loss: 1.5701
Training Epoch: 8 [25984/50048]	Loss: 1.6484
Training Epoch: 8 [26112/50048]	Loss: 1.6264
Training Epoch: 8 [26240/50048]	Loss: 1.6375
Training Epoch: 8 [26368/50048]	Loss: 1.7484
Training Epoch: 8 [26496/50048]	Loss: 1.7191
Training Epoch: 8 [26624/50048]	Loss: 1.7623
Training Epoch: 8 [26752/50048]	Loss: 1.7101
Training Epoch: 8 [26880/50048]	Loss: 1.7153
Training Epoch: 8 [27008/50048]	Loss: 1.6692
Training Epoch: 8 [27136/50048]	Loss: 1.6417
Training Epoch: 8 [27264/50048]	Loss: 1.5589
Training Epoch: 8 [27392/50048]	Loss: 1.7941
Training Epoch: 8 [27520/50048]	Loss: 1.6383
Training Epoch: 8 [27648/50048]	Loss: 1.4508
Training Epoch: 8 [27776/50048]	Loss: 1.5760
Training Epoch: 8 [27904/50048]	Loss: 1.7580
Training Epoch: 8 [28032/50048]	Loss: 1.8861
Training Epoch: 8 [28160/50048]	Loss: 1.5254
Training Epoch: 8 [28288/50048]	Loss: 1.6778
Training Epoch: 8 [28416/50048]	Loss: 1.9079
Training Epoch: 8 [28544/50048]	Loss: 1.7020
Training Epoch: 8 [28672/50048]	Loss: 1.7280
Training Epoch: 8 [28800/50048]	Loss: 1.7640
Training Epoch: 8 [28928/50048]	Loss: 1.6571
Training Epoch: 8 [29056/50048]	Loss: 1.7177
Training Epoch: 8 [29184/50048]	Loss: 1.6211
Training Epoch: 8 [29312/50048]	Loss: 1.7747
Training Epoch: 8 [29440/50048]	Loss: 1.7864
Training Epoch: 8 [29568/50048]	Loss: 1.5540
Training Epoch: 8 [29696/50048]	Loss: 1.7572
Training Epoch: 8 [29824/50048]	Loss: 1.6304
Training Epoch: 8 [29952/50048]	Loss: 1.8121
Training Epoch: 8 [30080/50048]	Loss: 1.6696
Training Epoch: 8 [30208/50048]	Loss: 1.7363
Training Epoch: 8 [30336/50048]	Loss: 1.7901
Training Epoch: 8 [30464/50048]	Loss: 1.4822
Training Epoch: 8 [30592/50048]	Loss: 1.5282
Training Epoch: 8 [30720/50048]	Loss: 1.4690
Training Epoch: 8 [30848/50048]	Loss: 1.6124
Training Epoch: 8 [30976/50048]	Loss: 1.8579
Training Epoch: 8 [31104/50048]	Loss: 1.7480
Training Epoch: 8 [31232/50048]	Loss: 1.5015
Training Epoch: 8 [31360/50048]	Loss: 1.7818
Training Epoch: 8 [31488/50048]	Loss: 1.7787
Training Epoch: 8 [31616/50048]	Loss: 1.7256
Training Epoch: 8 [31744/50048]	Loss: 1.9148
Training Epoch: 8 [31872/50048]	Loss: 1.6113
Training Epoch: 8 [32000/50048]	Loss: 1.7217
Training Epoch: 8 [32128/50048]	Loss: 1.7695
Training Epoch: 8 [32256/50048]	Loss: 1.6290
Training Epoch: 8 [32384/50048]	Loss: 1.7161
Training Epoch: 8 [32512/50048]	Loss: 1.9352
Training Epoch: 8 [32640/50048]	Loss: 1.6708
Training Epoch: 8 [32768/50048]	Loss: 1.7378
Training Epoch: 8 [32896/50048]	Loss: 1.8117
Training Epoch: 8 [33024/50048]	Loss: 1.7180
Training Epoch: 8 [33152/50048]	Loss: 1.7385
Training Epoch: 8 [33280/50048]	Loss: 1.6592
Training Epoch: 8 [33408/50048]	Loss: 1.6050
Training Epoch: 8 [33536/50048]	Loss: 1.6987
Training Epoch: 8 [33664/50048]	Loss: 1.7746
Training Epoch: 8 [33792/50048]	Loss: 1.7415
Training Epoch: 8 [33920/50048]	Loss: 1.5684
Training Epoch: 8 [34048/50048]	Loss: 1.4587
Training Epoch: 8 [34176/50048]	Loss: 1.7905
Training Epoch: 8 [34304/50048]	Loss: 1.7243
Training Epoch: 8 [34432/50048]	Loss: 1.6461
Training Epoch: 8 [34560/50048]	Loss: 1.6034
Training Epoch: 8 [34688/50048]	Loss: 1.6982
Training Epoch: 8 [34816/50048]	Loss: 1.6203
Training Epoch: 8 [34944/50048]	Loss: 1.6307
Training Epoch: 8 [35072/50048]	Loss: 1.6444
Training Epoch: 8 [35200/50048]	Loss: 1.6433
Training Epoch: 8 [35328/50048]	Loss: 1.6062
Training Epoch: 8 [35456/50048]	Loss: 1.4494
Training Epoch: 8 [35584/50048]	Loss: 1.6039
Training Epoch: 8 [35712/50048]	Loss: 1.6757
Training Epoch: 8 [35840/50048]	Loss: 1.4846
Training Epoch: 8 [35968/50048]	Loss: 1.8598
Training Epoch: 8 [36096/50048]	Loss: 1.5754
Training Epoch: 8 [36224/50048]	Loss: 1.8264
Training Epoch: 8 [36352/50048]	Loss: 1.6864
Training Epoch: 8 [36480/50048]	Loss: 1.7102
Training Epoch: 8 [36608/50048]	Loss: 1.9627
Training Epoch: 8 [36736/50048]	Loss: 2.0353
Training Epoch: 8 [36864/50048]	Loss: 1.8508
Training Epoch: 8 [36992/50048]	Loss: 1.5600
Training Epoch: 8 [37120/50048]	Loss: 1.7412
Training Epoch: 8 [37248/50048]	Loss: 1.6644
Training Epoch: 8 [37376/50048]	Loss: 1.8660
Training Epoch: 8 [37504/50048]	Loss: 1.6385
Training Epoch: 8 [37632/50048]	Loss: 1.6017
Training Epoch: 8 [37760/50048]	Loss: 1.9053
Training Epoch: 8 [37888/50048]	Loss: 1.8709
Training Epoch: 8 [38016/50048]	Loss: 1.6769
Training Epoch: 8 [38144/50048]	Loss: 1.7397
Training Epoch: 8 [38272/50048]	Loss: 1.6068
Training Epoch: 8 [38400/50048]	Loss: 1.6684
Training Epoch: 8 [38528/50048]	Loss: 1.4507
Training Epoch: 8 [38656/50048]	Loss: 1.9133
Training Epoch: 8 [38784/50048]	Loss: 1.8383
Training Epoch: 8 [38912/50048]	Loss: 1.7061
Training Epoch: 8 [39040/50048]	Loss: 1.8065
Training Epoch: 8 [39168/50048]	Loss: 1.7013
Training Epoch: 8 [39296/50048]	Loss: 1.6471
Training Epoch: 8 [39424/50048]	Loss: 1.6506
Training Epoch: 8 [39552/50048]	Loss: 1.7230
Training Epoch: 8 [39680/50048]	Loss: 1.7831
Training Epoch: 8 [39808/50048]	Loss: 1.5933
Training Epoch: 8 [39936/50048]	Loss: 1.8719
Training Epoch: 8 [40064/50048]	Loss: 1.8930
Training Epoch: 8 [40192/50048]	Loss: 1.5151
Training Epoch: 8 [40320/50048]	Loss: 1.7658
Training Epoch: 8 [40448/50048]	Loss: 1.7888
Training Epoch: 8 [40576/50048]	Loss: 1.7681
Training Epoch: 8 [40704/50048]	Loss: 1.5955
Training Epoch: 8 [40832/50048]	Loss: 1.5235
Training Epoch: 8 [40960/50048]	Loss: 1.6868
Training Epoch: 8 [41088/50048]	Loss: 1.7917
Training Epoch: 8 [41216/50048]	Loss: 1.5725
Training Epoch: 8 [41344/50048]	Loss: 1.6479
Training Epoch: 8 [41472/50048]	Loss: 1.8321
Training Epoch: 8 [41600/50048]	Loss: 1.8630
Training Epoch: 8 [41728/50048]	Loss: 1.5409
Training Epoch: 8 [41856/50048]	Loss: 1.5334
Training Epoch: 8 [41984/50048]	Loss: 1.3204
Training Epoch: 8 [42112/50048]	Loss: 1.8708
Training Epoch: 8 [42240/50048]	Loss: 1.5045
Training Epoch: 8 [42368/50048]	Loss: 1.5359
Training Epoch: 8 [42496/50048]	Loss: 1.7061
Training Epoch: 8 [42624/50048]	Loss: 1.7019
Training Epoch: 8 [42752/50048]	Loss: 1.5513
Training Epoch: 8 [42880/50048]	Loss: 1.6657
Training Epoch: 8 [43008/50048]	Loss: 1.6957
Training Epoch: 8 [43136/50048]	Loss: 1.7777
Training Epoch: 8 [43264/50048]	Loss: 1.7844
Training Epoch: 8 [43392/50048]	Loss: 1.8602
Training Epoch: 8 [43520/50048]	Loss: 1.6271
Training Epoch: 8 [43648/50048]	Loss: 1.6287
Training Epoch: 8 [43776/50048]	Loss: 1.8488
Training Epoch: 8 [43904/50048]	Loss: 1.4679
Training Epoch: 8 [44032/50048]	Loss: 1.7223
Training Epoch: 8 [44160/50048]	Loss: 1.6579
Training Epoch: 8 [44288/50048]	Loss: 1.6980
Training Epoch: 8 [44416/50048]	Loss: 1.7297
Training Epoch: 8 [44544/50048]	Loss: 2.2141
Training Epoch: 8 [44672/50048]	Loss: 1.8259
Training Epoch: 8 [44800/50048]	Loss: 1.7619
Training Epoch: 8 [44928/50048]	Loss: 1.5546
Training Epoch: 8 [45056/50048]	Loss: 1.5739
Training Epoch: 8 [45184/50048]	Loss: 1.7415
Training Epoch: 8 [45312/50048]	Loss: 1.6355
Training Epoch: 8 [45440/50048]	Loss: 1.7327
Training Epoch: 8 [45568/50048]	Loss: 1.7994
Training Epoch: 8 [45696/50048]	Loss: 2.0752
Training Epoch: 8 [45824/50048]	Loss: 1.9766
Training Epoch: 8 [45952/50048]	Loss: 1.6717
Training Epoch: 8 [46080/50048]	Loss: 1.7647
Training Epoch: 8 [46208/50048]	Loss: 1.6052
Training Epoch: 8 [46336/50048]	Loss: 1.6309
Training Epoch: 8 [46464/50048]	Loss: 1.6963
Training Epoch: 8 [46592/50048]	Loss: 1.8218
Training Epoch: 8 [46720/50048]	Loss: 1.9240
Training Epoch: 8 [46848/50048]	Loss: 2.0242
Training Epoch: 8 [46976/50048]	Loss: 1.8506
Training Epoch: 8 [47104/50048]	Loss: 1.6843
Training Epoch: 8 [47232/50048]	Loss: 1.6491
Training Epoch: 8 [47360/50048]	Loss: 1.7655
Training Epoch: 8 [47488/50048]	Loss: 1.6618
Training Epoch: 8 [47616/50048]	Loss: 1.6376
Training Epoch: 8 [47744/50048]	Loss: 1.6633
Training Epoch: 8 [47872/50048]	Loss: 1.7196
Training Epoch: 8 [48000/50048]	Loss: 1.5562
Training Epoch: 8 [48128/50048]	Loss: 1.5764
Training Epoch: 8 [48256/50048]	Loss: 1.5611
Training Epoch: 8 [48384/50048]	Loss: 1.8988
Training Epoch: 8 [48512/50048]	Loss: 1.6055
Training Epoch: 8 [48640/50048]	Loss: 1.7812
Training Epoch: 8 [48768/50048]	Loss: 1.9400
Training Epoch: 8 [48896/50048]	Loss: 1.7767
Training Epoch: 8 [49024/50048]	Loss: 1.5042
Training Epoch: 8 [49152/50048]	Loss: 1.5740
Training Epoch: 8 [49280/50048]	Loss: 1.5581
Training Epoch: 8 [49408/50048]	Loss: 1.7305
Training Epoch: 8 [49536/50048]	Loss: 1.8247
Training Epoch: 8 [49664/50048]	Loss: 1.7383
Training Epoch: 8 [49792/50048]	Loss: 1.5861
Training Epoch: 8 [49920/50048]	Loss: 1.7785
Training Epoch: 8 [50048/50048]	Loss: 1.4295
Validation Epoch: 8, Average loss: 0.0145, Accuracy: 0.4950
Training Epoch: 9 [128/50048]	Loss: 1.6733
Training Epoch: 9 [256/50048]	Loss: 1.5127
Training Epoch: 9 [384/50048]	Loss: 1.3330
Training Epoch: 9 [512/50048]	Loss: 1.6263
Training Epoch: 9 [640/50048]	Loss: 1.5421
Training Epoch: 9 [768/50048]	Loss: 1.4030
Training Epoch: 9 [896/50048]	Loss: 1.6872
Training Epoch: 9 [1024/50048]	Loss: 1.5054
Training Epoch: 9 [1152/50048]	Loss: 1.4741
Training Epoch: 9 [1280/50048]	Loss: 1.5969
Training Epoch: 9 [1408/50048]	Loss: 1.3188
Training Epoch: 9 [1536/50048]	Loss: 1.5012
Training Epoch: 9 [1664/50048]	Loss: 1.4666
Training Epoch: 9 [1792/50048]	Loss: 1.7832
Training Epoch: 9 [1920/50048]	Loss: 1.6834
Training Epoch: 9 [2048/50048]	Loss: 1.7255
Training Epoch: 9 [2176/50048]	Loss: 1.8179
Training Epoch: 9 [2304/50048]	Loss: 1.4928
Training Epoch: 9 [2432/50048]	Loss: 1.5651
Training Epoch: 9 [2560/50048]	Loss: 1.6499
Training Epoch: 9 [2688/50048]	Loss: 1.4162
Training Epoch: 9 [2816/50048]	Loss: 1.5812
Training Epoch: 9 [2944/50048]	Loss: 1.5754
Training Epoch: 9 [3072/50048]	Loss: 1.6727
Training Epoch: 9 [3200/50048]	Loss: 1.5115
Training Epoch: 9 [3328/50048]	Loss: 1.7256
Training Epoch: 9 [3456/50048]	Loss: 1.8613
Training Epoch: 9 [3584/50048]	Loss: 1.7427
Training Epoch: 9 [3712/50048]	Loss: 1.4931
Training Epoch: 9 [3840/50048]	Loss: 1.7752
Training Epoch: 9 [3968/50048]	Loss: 1.5197
Training Epoch: 9 [4096/50048]	Loss: 1.5140
Training Epoch: 9 [4224/50048]	Loss: 1.4179
Training Epoch: 9 [4352/50048]	Loss: 1.8010
Training Epoch: 9 [4480/50048]	Loss: 1.9193
Training Epoch: 9 [4608/50048]	Loss: 1.6015
Training Epoch: 9 [4736/50048]	Loss: 1.7286
Training Epoch: 9 [4864/50048]	Loss: 1.6068
Training Epoch: 9 [4992/50048]	Loss: 1.5268
Training Epoch: 9 [5120/50048]	Loss: 1.5881
Training Epoch: 9 [5248/50048]	Loss: 1.4207
Training Epoch: 9 [5376/50048]	Loss: 1.6521
Training Epoch: 9 [5504/50048]	Loss: 1.7819
Training Epoch: 9 [5632/50048]	Loss: 1.5455
Training Epoch: 9 [5760/50048]	Loss: 1.3751
Training Epoch: 9 [5888/50048]	Loss: 1.5784
Training Epoch: 9 [6016/50048]	Loss: 1.5898
Training Epoch: 9 [6144/50048]	Loss: 1.7824
Training Epoch: 9 [6272/50048]	Loss: 1.6316
Training Epoch: 9 [6400/50048]	Loss: 1.6623
Training Epoch: 9 [6528/50048]	Loss: 1.4704
Training Epoch: 9 [6656/50048]	Loss: 1.5873
Training Epoch: 9 [6784/50048]	Loss: 1.7394
Training Epoch: 9 [6912/50048]	Loss: 1.9241
Training Epoch: 9 [7040/50048]	Loss: 1.4506
Training Epoch: 9 [7168/50048]	Loss: 1.7217
Training Epoch: 9 [7296/50048]	Loss: 1.5944
Training Epoch: 9 [7424/50048]	Loss: 1.6067
Training Epoch: 9 [7552/50048]	Loss: 1.5734
Training Epoch: 9 [7680/50048]	Loss: 1.6882
Training Epoch: 9 [7808/50048]	Loss: 1.6555
Training Epoch: 9 [7936/50048]	Loss: 1.8450
Training Epoch: 9 [8064/50048]	Loss: 1.3558
Training Epoch: 9 [8192/50048]	Loss: 1.7113
Training Epoch: 9 [8320/50048]	Loss: 1.6385
Training Epoch: 9 [8448/50048]	Loss: 1.9039
Training Epoch: 9 [8576/50048]	Loss: 1.6518
Training Epoch: 9 [8704/50048]	Loss: 1.7424
Training Epoch: 9 [8832/50048]	Loss: 1.7566
Training Epoch: 9 [8960/50048]	Loss: 1.9227
Training Epoch: 9 [9088/50048]	Loss: 1.7704
Training Epoch: 9 [9216/50048]	Loss: 1.5706
Training Epoch: 9 [9344/50048]	Loss: 1.4546
Training Epoch: 9 [9472/50048]	Loss: 1.4599
Training Epoch: 9 [9600/50048]	Loss: 1.4976
Training Epoch: 9 [9728/50048]	Loss: 1.5767
Training Epoch: 9 [9856/50048]	Loss: 1.7093
Training Epoch: 9 [9984/50048]	Loss: 1.6861
Training Epoch: 9 [10112/50048]	Loss: 1.6500
Training Epoch: 9 [10240/50048]	Loss: 1.7462
Training Epoch: 9 [10368/50048]	Loss: 1.5423
Training Epoch: 9 [10496/50048]	Loss: 1.5233
Training Epoch: 9 [10624/50048]	Loss: 1.3546
Training Epoch: 9 [10752/50048]	Loss: 1.4702
Training Epoch: 9 [10880/50048]	Loss: 1.6612
Training Epoch: 9 [11008/50048]	Loss: 1.4103
Training Epoch: 9 [11136/50048]	Loss: 1.4605
Training Epoch: 9 [11264/50048]	Loss: 1.7412
Training Epoch: 9 [11392/50048]	Loss: 1.5266
Training Epoch: 9 [11520/50048]	Loss: 1.7632
Training Epoch: 9 [11648/50048]	Loss: 1.6180
Training Epoch: 9 [11776/50048]	Loss: 1.8762
Training Epoch: 9 [11904/50048]	Loss: 1.8056
Training Epoch: 9 [12032/50048]	Loss: 1.6244
Training Epoch: 9 [12160/50048]	Loss: 1.4280
Training Epoch: 9 [12288/50048]	Loss: 1.5737
Training Epoch: 9 [12416/50048]	Loss: 1.5211
Training Epoch: 9 [12544/50048]	Loss: 1.5605
Training Epoch: 9 [12672/50048]	Loss: 1.8463
Training Epoch: 9 [12800/50048]	Loss: 1.4477
Training Epoch: 9 [12928/50048]	Loss: 1.6314
Training Epoch: 9 [13056/50048]	Loss: 1.4627
Training Epoch: 9 [13184/50048]	Loss: 1.5361
Training Epoch: 9 [13312/50048]	Loss: 1.5599
Training Epoch: 9 [13440/50048]	Loss: 1.4406
Training Epoch: 9 [13568/50048]	Loss: 1.6399
Training Epoch: 9 [13696/50048]	Loss: 1.6677
Training Epoch: 9 [13824/50048]	Loss: 1.5239
Training Epoch: 9 [13952/50048]	Loss: 1.4610
Training Epoch: 9 [14080/50048]	Loss: 1.6441
Training Epoch: 9 [14208/50048]	Loss: 1.6974
Training Epoch: 9 [14336/50048]	Loss: 1.4083
Training Epoch: 9 [14464/50048]	Loss: 1.5810
Training Epoch: 9 [14592/50048]	Loss: 1.5386
Training Epoch: 9 [14720/50048]	Loss: 1.3347
Training Epoch: 9 [14848/50048]	Loss: 1.9550
Training Epoch: 9 [14976/50048]	Loss: 1.7247
Training Epoch: 9 [15104/50048]	Loss: 1.5872
Training Epoch: 9 [15232/50048]	Loss: 1.5002
Training Epoch: 9 [15360/50048]	Loss: 1.7193
Training Epoch: 9 [15488/50048]	Loss: 1.3749
Training Epoch: 9 [15616/50048]	Loss: 1.5804
Training Epoch: 9 [15744/50048]	Loss: 1.7379
Training Epoch: 9 [15872/50048]	Loss: 1.4897
Training Epoch: 9 [16000/50048]	Loss: 1.6179
Training Epoch: 9 [16128/50048]	Loss: 1.5283
Training Epoch: 9 [16256/50048]	Loss: 1.6164
Training Epoch: 9 [16384/50048]	Loss: 1.5549
Training Epoch: 9 [16512/50048]	Loss: 1.3781
Training Epoch: 9 [16640/50048]	Loss: 1.4873
Training Epoch: 9 [16768/50048]	Loss: 1.6039
Training Epoch: 9 [16896/50048]	Loss: 1.6202
Training Epoch: 9 [17024/50048]	Loss: 1.6103
Training Epoch: 9 [17152/50048]	Loss: 1.4915
Training Epoch: 9 [17280/50048]	Loss: 1.6661
Training Epoch: 9 [17408/50048]	Loss: 1.7894
Training Epoch: 9 [17536/50048]	Loss: 1.7030
Training Epoch: 9 [17664/50048]	Loss: 1.6457
Training Epoch: 9 [17792/50048]	Loss: 1.6415
Training Epoch: 9 [17920/50048]	Loss: 1.6840
Training Epoch: 9 [18048/50048]	Loss: 1.6163
Training Epoch: 9 [18176/50048]	Loss: 1.3405
Training Epoch: 9 [18304/50048]	Loss: 1.9251
Training Epoch: 9 [18432/50048]	Loss: 1.4763
Training Epoch: 9 [18560/50048]	Loss: 1.7745
Training Epoch: 9 [18688/50048]	Loss: 1.5582
Training Epoch: 9 [18816/50048]	Loss: 1.5359
Training Epoch: 9 [18944/50048]	Loss: 1.8298
Training Epoch: 9 [19072/50048]	Loss: 1.6820
Training Epoch: 9 [19200/50048]	Loss: 1.5940
Training Epoch: 9 [19328/50048]	Loss: 1.7072
Training Epoch: 9 [19456/50048]	Loss: 1.5545
Training Epoch: 9 [19584/50048]	Loss: 1.4720
Training Epoch: 9 [19712/50048]	Loss: 1.3792
Training Epoch: 9 [19840/50048]	Loss: 1.7190
Training Epoch: 9 [19968/50048]	Loss: 1.5053
Training Epoch: 9 [20096/50048]	Loss: 1.6217
Training Epoch: 9 [20224/50048]	Loss: 1.9972
Training Epoch: 9 [20352/50048]	Loss: 1.4533
Training Epoch: 9 [20480/50048]	Loss: 1.5158
Training Epoch: 9 [20608/50048]	Loss: 1.4878
Training Epoch: 9 [20736/50048]	Loss: 1.4434
Training Epoch: 9 [20864/50048]	Loss: 1.5111
Training Epoch: 9 [20992/50048]	Loss: 1.5808
Training Epoch: 9 [21120/50048]	Loss: 1.9512
Training Epoch: 9 [21248/50048]	Loss: 1.7978
Training Epoch: 9 [21376/50048]	Loss: 1.6194
Training Epoch: 9 [21504/50048]	Loss: 1.6460
Training Epoch: 9 [21632/50048]	Loss: 1.6374
Training Epoch: 9 [21760/50048]	Loss: 1.5106
Training Epoch: 9 [21888/50048]	Loss: 1.8328
Training Epoch: 9 [22016/50048]	Loss: 1.6518
Training Epoch: 9 [22144/50048]	Loss: 1.7038
Training Epoch: 9 [22272/50048]	Loss: 1.5394
Training Epoch: 9 [22400/50048]	Loss: 1.5356
Training Epoch: 9 [22528/50048]	Loss: 1.8573
Training Epoch: 9 [22656/50048]	Loss: 1.7886
Training Epoch: 9 [22784/50048]	Loss: 1.8152
Training Epoch: 9 [22912/50048]	Loss: 1.6098
Training Epoch: 9 [23040/50048]	Loss: 1.6685
Training Epoch: 9 [23168/50048]	Loss: 1.5241
Training Epoch: 9 [23296/50048]	Loss: 1.8451
Training Epoch: 9 [23424/50048]	Loss: 1.6873
Training Epoch: 9 [23552/50048]	Loss: 1.6489
Training Epoch: 9 [23680/50048]	Loss: 1.9117
Training Epoch: 9 [23808/50048]	Loss: 1.7784
Training Epoch: 9 [23936/50048]	Loss: 1.7890
Training Epoch: 9 [24064/50048]	Loss: 1.6818
Training Epoch: 9 [24192/50048]	Loss: 1.6829
Training Epoch: 9 [24320/50048]	Loss: 1.6784
Training Epoch: 9 [24448/50048]	Loss: 1.6592
Training Epoch: 9 [24576/50048]	Loss: 1.6727
Training Epoch: 9 [24704/50048]	Loss: 1.6030
Training Epoch: 9 [24832/50048]	Loss: 1.7934
Training Epoch: 9 [24960/50048]	Loss: 1.7393
Training Epoch: 9 [25088/50048]	Loss: 1.4538
Training Epoch: 9 [25216/50048]	Loss: 1.8467
Training Epoch: 9 [25344/50048]	Loss: 1.5766
Training Epoch: 9 [25472/50048]	Loss: 1.3987
Training Epoch: 9 [25600/50048]	Loss: 1.6008
Training Epoch: 9 [25728/50048]	Loss: 1.4854
Training Epoch: 9 [25856/50048]	Loss: 1.7435
Training Epoch: 9 [25984/50048]	Loss: 1.7507
Training Epoch: 9 [26112/50048]	Loss: 1.6027
Training Epoch: 9 [26240/50048]	Loss: 1.6625
Training Epoch: 9 [26368/50048]	Loss: 1.7765
Training Epoch: 9 [26496/50048]	Loss: 1.8954
Training Epoch: 9 [26624/50048]	Loss: 1.7031
Training Epoch: 9 [26752/50048]	Loss: 1.7959
Training Epoch: 9 [26880/50048]	Loss: 1.6700
Training Epoch: 9 [27008/50048]	Loss: 1.5109
Training Epoch: 9 [27136/50048]	Loss: 1.5777
Training Epoch: 9 [27264/50048]	Loss: 1.6391
Training Epoch: 9 [27392/50048]	Loss: 1.6021
Training Epoch: 9 [27520/50048]	Loss: 1.6987
Training Epoch: 9 [27648/50048]	Loss: 1.5394
Training Epoch: 9 [27776/50048]	Loss: 1.7779
Training Epoch: 9 [27904/50048]	Loss: 1.4872
Training Epoch: 9 [28032/50048]	Loss: 1.7569
Training Epoch: 9 [28160/50048]	Loss: 1.6973
Training Epoch: 9 [28288/50048]	Loss: 1.7580
Training Epoch: 9 [28416/50048]	Loss: 1.7796
Training Epoch: 9 [28544/50048]	Loss: 1.7009
Training Epoch: 9 [28672/50048]	Loss: 1.7033
Training Epoch: 9 [28800/50048]	Loss: 1.7819
Training Epoch: 9 [28928/50048]	Loss: 1.5464
Training Epoch: 9 [29056/50048]	Loss: 1.3213
Training Epoch: 9 [29184/50048]	Loss: 1.8377
Training Epoch: 9 [29312/50048]	Loss: 1.6024
Training Epoch: 9 [29440/50048]	Loss: 1.6329
Training Epoch: 9 [29568/50048]	Loss: 1.7551
Training Epoch: 9 [29696/50048]	Loss: 1.3698
Training Epoch: 9 [29824/50048]	Loss: 1.6724
Training Epoch: 9 [29952/50048]	Loss: 1.8665
Training Epoch: 9 [30080/50048]	Loss: 1.6650
Training Epoch: 9 [30208/50048]	Loss: 1.4200
Training Epoch: 9 [30336/50048]	Loss: 1.4166
Training Epoch: 9 [30464/50048]	Loss: 1.5923
Training Epoch: 9 [30592/50048]	Loss: 1.6400
Training Epoch: 9 [30720/50048]	Loss: 1.4109
Training Epoch: 9 [30848/50048]	Loss: 1.5791
Training Epoch: 9 [30976/50048]	Loss: 1.6379
Training Epoch: 9 [31104/50048]	Loss: 1.6972
Training Epoch: 9 [31232/50048]	Loss: 1.5917
Training Epoch: 9 [31360/50048]	Loss: 1.7193
Training Epoch: 9 [31488/50048]	Loss: 1.6691
Training Epoch: 9 [31616/50048]	Loss: 1.7385
Training Epoch: 9 [31744/50048]	Loss: 1.6365
Training Epoch: 9 [31872/50048]	Loss: 1.5842
Training Epoch: 9 [32000/50048]	Loss: 1.5218
Training Epoch: 9 [32128/50048]	Loss: 1.5936
Training Epoch: 9 [32256/50048]	Loss: 1.5764
Training Epoch: 9 [32384/50048]	Loss: 1.4833
Training Epoch: 9 [32512/50048]	Loss: 1.5673
Training Epoch: 9 [32640/50048]	Loss: 1.8142
Training Epoch: 9 [32768/50048]	Loss: 1.7292
Training Epoch: 9 [32896/50048]	Loss: 1.8400
Training Epoch: 9 [33024/50048]	Loss: 1.4753
Training Epoch: 9 [33152/50048]	Loss: 1.6325
Training Epoch: 9 [33280/50048]	Loss: 1.4224
Training Epoch: 9 [33408/50048]	Loss: 1.7024
Training Epoch: 9 [33536/50048]	Loss: 1.6022
Training Epoch: 9 [33664/50048]	Loss: 1.7064
Training Epoch: 9 [33792/50048]	Loss: 1.5624
Training Epoch: 9 [33920/50048]	Loss: 1.5206
Training Epoch: 9 [34048/50048]	Loss: 1.6175
Training Epoch: 9 [34176/50048]	Loss: 1.7990
Training Epoch: 9 [34304/50048]	Loss: 1.2469
Training Epoch: 9 [34432/50048]	Loss: 1.6312
Training Epoch: 9 [34560/50048]	Loss: 1.6384
Training Epoch: 9 [34688/50048]	Loss: 1.4065
Training Epoch: 9 [34816/50048]	Loss: 1.4240
Training Epoch: 9 [34944/50048]	Loss: 1.5992
Training Epoch: 9 [35072/50048]	Loss: 1.7502
Training Epoch: 9 [35200/50048]	Loss: 1.7062
Training Epoch: 9 [35328/50048]	Loss: 1.6928
Training Epoch: 9 [35456/50048]	Loss: 1.4627
Training Epoch: 9 [35584/50048]	Loss: 1.6417
Training Epoch: 9 [35712/50048]	Loss: 1.6806
Training Epoch: 9 [35840/50048]	Loss: 1.7639
Training Epoch: 9 [35968/50048]	Loss: 1.2528
Training Epoch: 9 [36096/50048]	Loss: 1.7795
Training Epoch: 9 [36224/50048]	Loss: 1.6654
Training Epoch: 9 [36352/50048]	Loss: 1.7639
Training Epoch: 9 [36480/50048]	Loss: 1.5962
Training Epoch: 9 [36608/50048]	Loss: 1.5770
Training Epoch: 9 [36736/50048]	Loss: 1.5207
Training Epoch: 9 [36864/50048]	Loss: 1.4909
Training Epoch: 9 [36992/50048]	Loss: 1.5459
Training Epoch: 9 [37120/50048]	Loss: 1.7268
Training Epoch: 9 [37248/50048]	Loss: 1.5207
Training Epoch: 9 [37376/50048]	Loss: 1.5550
Training Epoch: 9 [37504/50048]	Loss: 1.7282
Training Epoch: 9 [37632/50048]	Loss: 1.6709
Training Epoch: 9 [37760/50048]	Loss: 1.6008
Training Epoch: 9 [37888/50048]	Loss: 1.6826
Training Epoch: 9 [38016/50048]	Loss: 1.9887
Training Epoch: 9 [38144/50048]	Loss: 1.8522
Training Epoch: 9 [38272/50048]	Loss: 1.5734
Training Epoch: 9 [38400/50048]	Loss: 1.6273
Training Epoch: 9 [38528/50048]	Loss: 1.6445
Training Epoch: 9 [38656/50048]	Loss: 1.8401
Training Epoch: 9 [38784/50048]	Loss: 1.5941
Training Epoch: 9 [38912/50048]	Loss: 1.6953
Training Epoch: 9 [39040/50048]	Loss: 1.9075
Training Epoch: 9 [39168/50048]	Loss: 1.7447
Training Epoch: 9 [39296/50048]	Loss: 1.6906
Training Epoch: 9 [39424/50048]	Loss: 1.6075
Training Epoch: 9 [39552/50048]	Loss: 1.4962
Training Epoch: 9 [39680/50048]	Loss: 1.5278
Training Epoch: 9 [39808/50048]	Loss: 1.6753
Training Epoch: 9 [39936/50048]	Loss: 1.6141
Training Epoch: 9 [40064/50048]	Loss: 1.6732
Training Epoch: 9 [40192/50048]	Loss: 1.5842
Training Epoch: 9 [40320/50048]	Loss: 1.5826
Training Epoch: 9 [40448/50048]	Loss: 1.6213
Training Epoch: 9 [40576/50048]	Loss: 1.5877
Training Epoch: 9 [40704/50048]	Loss: 1.7584
Training Epoch: 9 [40832/50048]	Loss: 1.4416
Training Epoch: 9 [40960/50048]	Loss: 1.7063
Training Epoch: 9 [41088/50048]	Loss: 1.8334
Training Epoch: 9 [41216/50048]	Loss: 1.4724
Training Epoch: 9 [41344/50048]	Loss: 1.5702
Training Epoch: 9 [41472/50048]	Loss: 1.5682
Training Epoch: 9 [41600/50048]	Loss: 1.5921
Training Epoch: 9 [41728/50048]	Loss: 1.6285
Training Epoch: 9 [41856/50048]	Loss: 1.4830
Training Epoch: 9 [41984/50048]	Loss: 1.7998
Training Epoch: 9 [42112/50048]	Loss: 1.7931
Training Epoch: 9 [42240/50048]	Loss: 1.8014
Training Epoch: 9 [42368/50048]	Loss: 1.5802
Training Epoch: 9 [42496/50048]	Loss: 1.7155
Training Epoch: 9 [42624/50048]	Loss: 1.7960
Training Epoch: 9 [42752/50048]	Loss: 1.5667
Training Epoch: 9 [42880/50048]	Loss: 1.5577
Training Epoch: 9 [43008/50048]	Loss: 1.3783
Training Epoch: 9 [43136/50048]	Loss: 1.6736
Training Epoch: 9 [43264/50048]	Loss: 1.6814
Training Epoch: 9 [43392/50048]	Loss: 1.6843
Training Epoch: 9 [43520/50048]	Loss: 1.4781
Training Epoch: 9 [43648/50048]	Loss: 1.4412
Training Epoch: 9 [43776/50048]	Loss: 1.2898
Training Epoch: 9 [43904/50048]	Loss: 1.6094
Training Epoch: 9 [44032/50048]	Loss: 1.5659
Training Epoch: 9 [44160/50048]	Loss: 1.4895
Training Epoch: 9 [44288/50048]	Loss: 1.5493
Training Epoch: 9 [44416/50048]	Loss: 1.6042
Training Epoch: 9 [44544/50048]	Loss: 1.5790
Training Epoch: 9 [44672/50048]	Loss: 1.7779
Training Epoch: 9 [44800/50048]	Loss: 1.7872
Training Epoch: 9 [44928/50048]	Loss: 1.7100
Training Epoch: 9 [45056/50048]	Loss: 1.4848
Training Epoch: 9 [45184/50048]	Loss: 1.7888
Training Epoch: 9 [45312/50048]	Loss: 1.4532
Training Epoch: 9 [45440/50048]	Loss: 1.6944
Training Epoch: 9 [45568/50048]	Loss: 1.4632
Training Epoch: 9 [45696/50048]	Loss: 1.5082
Training Epoch: 9 [45824/50048]	Loss: 1.4284
Training Epoch: 9 [45952/50048]	Loss: 1.7759
Training Epoch: 9 [46080/50048]	Loss: 1.7056
Training Epoch: 9 [46208/50048]	Loss: 1.5953
Training Epoch: 9 [46336/50048]	Loss: 1.4867
Training Epoch: 9 [46464/50048]	Loss: 1.7515
Training Epoch: 9 [46592/50048]	Loss: 1.6116
Training Epoch: 9 [46720/50048]	Loss: 1.3846
Training Epoch: 9 [46848/50048]	Loss: 1.7037
Training Epoch: 9 [46976/50048]	Loss: 1.5574
Training Epoch: 9 [47104/50048]	Loss: 1.5773
Training Epoch: 9 [47232/50048]	Loss: 1.6863
Training Epoch: 9 [47360/50048]	Loss: 1.6671
Training Epoch: 9 [47488/50048]	Loss: 1.5689
Training Epoch: 9 [47616/50048]	Loss: 1.6030
Training Epoch: 9 [47744/50048]	Loss: 1.3981
Training Epoch: 9 [47872/50048]	Loss: 1.4495
Training Epoch: 9 [48000/50048]	Loss: 1.8090
Training Epoch: 9 [48128/50048]	Loss: 1.6648
Training Epoch: 9 [48256/50048]	Loss: 1.5944
Training Epoch: 9 [48384/50048]	Loss: 1.5397
Training Epoch: 9 [48512/50048]	Loss: 1.4076
Training Epoch: 9 [48640/50048]	Loss: 1.5948
Training Epoch: 9 [48768/50048]	Loss: 1.7133
Training Epoch: 9 [48896/50048]	Loss: 1.5151
Training Epoch: 9 [49024/50048]	Loss: 1.7488
Training Epoch: 9 [49152/50048]	Loss: 1.6559
Training Epoch: 9 [49280/50048]	Loss: 1.5418
Training Epoch: 9 [49408/50048]	Loss: 1.6508
Training Epoch: 9 [49536/50048]	Loss: 1.6038
Training Epoch: 9 [49664/50048]	Loss: 1.4444
Training Epoch: 9 [49792/50048]	Loss: 1.6553
Training Epoch: 9 [49920/50048]	Loss: 1.4097
Training Epoch: 9 [50048/50048]	Loss: 1.4692
Validation Epoch: 9, Average loss: 0.0148, Accuracy: 0.5021
[Training Loop] Model's accuracy 0.5020767405063291 surpasses threshold 0.5! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6007
Profiling... [256/50048]	Loss: 1.4032
Profiling... [384/50048]	Loss: 1.3781
Profiling... [512/50048]	Loss: 1.3872
Profiling... [640/50048]	Loss: 1.2470
Profiling... [768/50048]	Loss: 1.7715
Profiling... [896/50048]	Loss: 1.5841
Profiling... [1024/50048]	Loss: 1.3518
Profiling... [1152/50048]	Loss: 1.3217
Profiling... [1280/50048]	Loss: 1.5263
Profiling... [1408/50048]	Loss: 1.5385
Profiling... [1536/50048]	Loss: 1.5142
Profiling... [1664/50048]	Loss: 1.1932
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 10, Average loss: 0.0130, Accuracy: 0.5439
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.512632034247,
                        "time": 2.162355848000516,
                        "accuracy": 0.5439082278481012,
                        "total_cost": 695728.1652039479
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.3709
Profiling... [256/50048]	Loss: 1.5780
Profiling... [384/50048]	Loss: 1.5032
Profiling... [512/50048]	Loss: 1.4249
Profiling... [640/50048]	Loss: 1.6077
Profiling... [768/50048]	Loss: 1.4720
Profiling... [896/50048]	Loss: 1.4263
Profiling... [1024/50048]	Loss: 1.4406
Profiling... [1152/50048]	Loss: 1.6386
Profiling... [1280/50048]	Loss: 1.1116
Profiling... [1408/50048]	Loss: 1.4868
Profiling... [1536/50048]	Loss: 1.3840
Profiling... [1664/50048]	Loss: 1.4406
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 10, Average loss: 0.0131, Accuracy: 0.5380
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.53088880967589,
                        "time": 2.1830344589998276,
                        "accuracy": 0.5379746835443038,
                        "total_cost": 710128.2681334733
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5190
Profiling... [256/50048]	Loss: 1.5151
Profiling... [384/50048]	Loss: 1.7274
Profiling... [512/50048]	Loss: 1.5286
Profiling... [640/50048]	Loss: 1.4689
Profiling... [768/50048]	Loss: 1.4025
Profiling... [896/50048]	Loss: 1.6367
Profiling... [1024/50048]	Loss: 1.7128
Profiling... [1152/50048]	Loss: 1.5289
Profiling... [1280/50048]	Loss: 1.5189
Profiling... [1408/50048]	Loss: 1.5226
Profiling... [1536/50048]	Loss: 1.4997
Profiling... [1664/50048]	Loss: 1.6029
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 10, Average loss: 0.0129, Accuracy: 0.5417
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.54033412704217,
                        "time": 2.4458401869997033,
                        "accuracy": 0.5417325949367089,
                        "total_cost": 790098.3561363042
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5223
Profiling... [256/50048]	Loss: 1.5519
Profiling... [384/50048]	Loss: 1.3638
Profiling... [512/50048]	Loss: 1.3818
Profiling... [640/50048]	Loss: 1.4504
Profiling... [768/50048]	Loss: 1.3526
Profiling... [896/50048]	Loss: 1.6508
Profiling... [1024/50048]	Loss: 1.4929
Profiling... [1152/50048]	Loss: 1.3367
Profiling... [1280/50048]	Loss: 1.7143
Profiling... [1408/50048]	Loss: 1.5384
Profiling... [1536/50048]	Loss: 1.6038
Profiling... [1664/50048]	Loss: 1.3322
Profile done
epoch 1 train time consumed: 7.61s
Validation Epoch: 10, Average loss: 0.0129, Accuracy: 0.5428
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.52054222762065,
                        "time": 5.362292407000496,
                        "accuracy": 0.5428204113924051,
                        "total_cost": 1728750.7093146432
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6395
Profiling... [256/50048]	Loss: 1.5865
Profiling... [384/50048]	Loss: 1.5823
Profiling... [512/50048]	Loss: 1.6416
Profiling... [640/50048]	Loss: 1.6850
Profiling... [768/50048]	Loss: 1.4380
Profiling... [896/50048]	Loss: 1.4211
Profiling... [1024/50048]	Loss: 1.3963
Profiling... [1152/50048]	Loss: 1.6238
Profiling... [1280/50048]	Loss: 1.6069
Profiling... [1408/50048]	Loss: 1.5966
Profiling... [1536/50048]	Loss: 1.3684
Profiling... [1664/50048]	Loss: 1.2677
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 10, Average loss: 0.0132, Accuracy: 0.5355
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.4975566017518,
                        "time": 2.156671255001129,
                        "accuracy": 0.5355023734177216,
                        "total_cost": 704791.4040350873
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5425
Profiling... [256/50048]	Loss: 1.6959
Profiling... [384/50048]	Loss: 1.1975
Profiling... [512/50048]	Loss: 1.7094
Profiling... [640/50048]	Loss: 1.3554
Profiling... [768/50048]	Loss: 1.3929
Profiling... [896/50048]	Loss: 1.4680
Profiling... [1024/50048]	Loss: 1.1786
Profiling... [1152/50048]	Loss: 1.7771
Profiling... [1280/50048]	Loss: 1.5757
Profiling... [1408/50048]	Loss: 1.8132
Profiling... [1536/50048]	Loss: 1.4915
Profiling... [1664/50048]	Loss: 1.3343
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 10, Average loss: 0.0129, Accuracy: 0.5404
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.51398157138185,
                        "time": 2.164328826000201,
                        "accuracy": 0.5404469936708861,
                        "total_cost": 700822.7429990768
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5120
Profiling... [256/50048]	Loss: 1.6814
Profiling... [384/50048]	Loss: 1.4803
Profiling... [512/50048]	Loss: 1.7183
Profiling... [640/50048]	Loss: 1.4677
Profiling... [768/50048]	Loss: 1.4458
Profiling... [896/50048]	Loss: 1.6482
Profiling... [1024/50048]	Loss: 1.2975
Profiling... [1152/50048]	Loss: 1.2765
Profiling... [1280/50048]	Loss: 1.3363
Profiling... [1408/50048]	Loss: 1.4796
Profiling... [1536/50048]	Loss: 1.7252
Profiling... [1664/50048]	Loss: 1.6165
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 10, Average loss: 0.0131, Accuracy: 0.5340
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.52388273302279,
                        "time": 2.408756987000743,
                        "accuracy": 0.5340189873417721,
                        "total_cost": 789358.5859623175
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6155
Profiling... [256/50048]	Loss: 1.3372
Profiling... [384/50048]	Loss: 1.4434
Profiling... [512/50048]	Loss: 1.5158
Profiling... [640/50048]	Loss: 1.7443
Profiling... [768/50048]	Loss: 1.6088
Profiling... [896/50048]	Loss: 1.3641
Profiling... [1024/50048]	Loss: 1.6507
Profiling... [1152/50048]	Loss: 1.4275
Profiling... [1280/50048]	Loss: 1.4737
Profiling... [1408/50048]	Loss: 1.2899
Profiling... [1536/50048]	Loss: 1.3541
Profiling... [1664/50048]	Loss: 1.6889
Profile done
epoch 1 train time consumed: 7.24s
Validation Epoch: 10, Average loss: 0.0133, Accuracy: 0.5361
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.50525162135071,
                        "time": 5.056443027000569,
                        "accuracy": 0.5360957278481012,
                        "total_cost": 1650596.1225936557
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.2139
Profiling... [256/50048]	Loss: 1.5851
Profiling... [384/50048]	Loss: 1.3796
Profiling... [512/50048]	Loss: 1.8018
Profiling... [640/50048]	Loss: 1.5294
Profiling... [768/50048]	Loss: 1.4824
Profiling... [896/50048]	Loss: 1.4627
Profiling... [1024/50048]	Loss: 1.6369
Profiling... [1152/50048]	Loss: 1.3139
Profiling... [1280/50048]	Loss: 1.3168
Profiling... [1408/50048]	Loss: 1.5675
Profiling... [1536/50048]	Loss: 1.5002
Profiling... [1664/50048]	Loss: 1.7340
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 10, Average loss: 0.0130, Accuracy: 0.5379
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.48283945030292,
                        "time": 2.164983360999031,
                        "accuracy": 0.5378757911392406,
                        "total_cost": 704385.8348269691
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5328
Profiling... [256/50048]	Loss: 1.9476
Profiling... [384/50048]	Loss: 1.5327
Profiling... [512/50048]	Loss: 1.5163
Profiling... [640/50048]	Loss: 1.4355
Profiling... [768/50048]	Loss: 1.4608
Profiling... [896/50048]	Loss: 1.4721
Profiling... [1024/50048]	Loss: 1.4001
Profiling... [1152/50048]	Loss: 1.7998
Profiling... [1280/50048]	Loss: 1.2813
Profiling... [1408/50048]	Loss: 1.5908
Profiling... [1536/50048]	Loss: 1.4978
Profiling... [1664/50048]	Loss: 1.4691
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 10, Average loss: 0.0129, Accuracy: 0.5434
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.50020893523383,
                        "time": 2.164045628000167,
                        "accuracy": 0.5434137658227848,
                        "total_cost": 696905.3945967418
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4648
Profiling... [256/50048]	Loss: 1.3984
Profiling... [384/50048]	Loss: 1.4995
Profiling... [512/50048]	Loss: 1.5241
Profiling... [640/50048]	Loss: 1.3616
Profiling... [768/50048]	Loss: 1.4857
Profiling... [896/50048]	Loss: 1.2913
Profiling... [1024/50048]	Loss: 1.6663
Profiling... [1152/50048]	Loss: 1.6517
Profiling... [1280/50048]	Loss: 1.5361
Profiling... [1408/50048]	Loss: 1.3799
Profiling... [1536/50048]	Loss: 1.4191
Profiling... [1664/50048]	Loss: 1.4522
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 10, Average loss: 0.0130, Accuracy: 0.5416
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.50966074204652,
                        "time": 2.4292446119998203,
                        "accuracy": 0.5416337025316456,
                        "total_cost": 784880.6400209754
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6253
Profiling... [256/50048]	Loss: 1.7998
Profiling... [384/50048]	Loss: 1.5672
Profiling... [512/50048]	Loss: 1.5197
Profiling... [640/50048]	Loss: 1.4751
Profiling... [768/50048]	Loss: 1.5672
Profiling... [896/50048]	Loss: 1.4385
Profiling... [1024/50048]	Loss: 1.3890
Profiling... [1152/50048]	Loss: 1.3377
Profiling... [1280/50048]	Loss: 1.3180
Profiling... [1408/50048]	Loss: 1.4523
Profiling... [1536/50048]	Loss: 1.5473
Profiling... [1664/50048]	Loss: 1.4839
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 10, Average loss: 0.0129, Accuracy: 0.5411
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.49155625908885,
                        "time": 4.778508538998722,
                        "accuracy": 0.5411392405063291,
                        "total_cost": 1545330.5392200546
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.1813
Profiling... [256/50048]	Loss: 1.2819
Profiling... [384/50048]	Loss: 1.7477
Profiling... [512/50048]	Loss: 1.5400
Profiling... [640/50048]	Loss: 1.6187
Profiling... [768/50048]	Loss: 1.7100
Profiling... [896/50048]	Loss: 1.4207
Profiling... [1024/50048]	Loss: 1.5891
Profiling... [1152/50048]	Loss: 1.5993
Profiling... [1280/50048]	Loss: 1.8494
Profiling... [1408/50048]	Loss: 1.5464
Profiling... [1536/50048]	Loss: 1.6380
Profiling... [1664/50048]	Loss: 1.5811
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 10, Average loss: 0.0151, Accuracy: 0.4830
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.47000666844023,
                        "time": 2.160913341000196,
                        "accuracy": 0.48299050632911394,
                        "total_cost": 782955.0057809063
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4886
Profiling... [256/50048]	Loss: 1.4912
Profiling... [384/50048]	Loss: 1.6487
Profiling... [512/50048]	Loss: 1.5550
Profiling... [640/50048]	Loss: 1.4758
Profiling... [768/50048]	Loss: 1.6341
Profiling... [896/50048]	Loss: 1.5050
Profiling... [1024/50048]	Loss: 1.7891
Profiling... [1152/50048]	Loss: 1.6238
Profiling... [1280/50048]	Loss: 1.4360
Profiling... [1408/50048]	Loss: 1.6589
Profiling... [1536/50048]	Loss: 1.4509
Profiling... [1664/50048]	Loss: 1.5758
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 10, Average loss: 0.0156, Accuracy: 0.4736
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.48522100184596,
                        "time": 2.1636016850006854,
                        "accuracy": 0.4735957278481013,
                        "total_cost": 799479.9627849682
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.2872
Profiling... [256/50048]	Loss: 1.3338
Profiling... [384/50048]	Loss: 1.4773
Profiling... [512/50048]	Loss: 1.4407
Profiling... [640/50048]	Loss: 1.5906
Profiling... [768/50048]	Loss: 1.7089
Profiling... [896/50048]	Loss: 1.6155
Profiling... [1024/50048]	Loss: 1.3087
Profiling... [1152/50048]	Loss: 1.5539
Profiling... [1280/50048]	Loss: 1.6009
Profiling... [1408/50048]	Loss: 1.8530
Profiling... [1536/50048]	Loss: 1.6876
Profiling... [1664/50048]	Loss: 1.5407
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 10, Average loss: 0.0143, Accuracy: 0.5057
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.49444677208103,
                        "time": 2.402214231000471,
                        "accuracy": 0.5057357594936709,
                        "total_cost": 831239.402264066
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.4844
Profiling... [256/50048]	Loss: 1.4889
Profiling... [384/50048]	Loss: 1.3920
Profiling... [512/50048]	Loss: 1.5231
Profiling... [640/50048]	Loss: 1.8990
Profiling... [768/50048]	Loss: 1.4607
Profiling... [896/50048]	Loss: 1.5790
Profiling... [1024/50048]	Loss: 1.6061
Profiling... [1152/50048]	Loss: 1.5784
Profiling... [1280/50048]	Loss: 1.5032
Profiling... [1408/50048]	Loss: 1.4377
Profiling... [1536/50048]	Loss: 1.8063
Profiling... [1664/50048]	Loss: 1.7162
Profile done
epoch 1 train time consumed: 7.16s
Validation Epoch: 10, Average loss: 0.0199, Accuracy: 0.3808
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.4763151115738,
                        "time": 5.021673400999134,
                        "accuracy": 0.38083465189873417,
                        "total_cost": 2307544.3392386567
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.3987
Profiling... [256/50048]	Loss: 1.3506
Profiling... [384/50048]	Loss: 1.3368
Profiling... [512/50048]	Loss: 1.5039
Profiling... [640/50048]	Loss: 1.5749
Profiling... [768/50048]	Loss: 1.4743
Profiling... [896/50048]	Loss: 1.5663
Profiling... [1024/50048]	Loss: 1.4757
Profiling... [1152/50048]	Loss: 1.5638
Profiling... [1280/50048]	Loss: 1.4603
Profiling... [1408/50048]	Loss: 1.5879
Profiling... [1536/50048]	Loss: 1.4973
Profiling... [1664/50048]	Loss: 1.5770
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 10, Average loss: 0.0166, Accuracy: 0.4469
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.45469041381867,
                        "time": 2.1553401410001243,
                        "accuracy": 0.44689477848101267,
                        "total_cost": 844011.930408015
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4615
Profiling... [256/50048]	Loss: 1.6459
Profiling... [384/50048]	Loss: 1.6166
Profiling... [512/50048]	Loss: 1.3769
Profiling... [640/50048]	Loss: 1.8211
Profiling... [768/50048]	Loss: 1.3728
Profiling... [896/50048]	Loss: 1.6122
Profiling... [1024/50048]	Loss: 1.3581
Profiling... [1152/50048]	Loss: 1.4708
Profiling... [1280/50048]	Loss: 1.6715
Profiling... [1408/50048]	Loss: 1.6834
Profiling... [1536/50048]	Loss: 1.8052
Profiling... [1664/50048]	Loss: 1.5121
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 10, Average loss: 0.0164, Accuracy: 0.4615
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.47010455883192,
                        "time": 2.1643585700003314,
                        "accuracy": 0.4615308544303797,
                        "total_cost": 820666.150733359
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4924
Profiling... [256/50048]	Loss: 1.5856
Profiling... [384/50048]	Loss: 1.4670
Profiling... [512/50048]	Loss: 1.4774
Profiling... [640/50048]	Loss: 1.6042
Profiling... [768/50048]	Loss: 1.3817
Profiling... [896/50048]	Loss: 1.4899
Profiling... [1024/50048]	Loss: 1.4657
Profiling... [1152/50048]	Loss: 1.5553
Profiling... [1280/50048]	Loss: 1.5438
Profiling... [1408/50048]	Loss: 1.8738
Profiling... [1536/50048]	Loss: 1.3520
Profiling... [1664/50048]	Loss: 1.5844
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 10, Average loss: 0.0156, Accuracy: 0.4702
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.4796259239901,
                        "time": 2.42157044600026,
                        "accuracy": 0.4702333860759494,
                        "total_cost": 901201.0644042187
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.3580
Profiling... [256/50048]	Loss: 1.6285
Profiling... [384/50048]	Loss: 1.6327
Profiling... [512/50048]	Loss: 1.4400
Profiling... [640/50048]	Loss: 1.3969
Profiling... [768/50048]	Loss: 1.5083
Profiling... [896/50048]	Loss: 1.5956
Profiling... [1024/50048]	Loss: 1.5717
Profiling... [1152/50048]	Loss: 1.5624
Profiling... [1280/50048]	Loss: 1.4456
Profiling... [1408/50048]	Loss: 1.6263
Profiling... [1536/50048]	Loss: 1.3747
Profiling... [1664/50048]	Loss: 1.7135
Profile done
epoch 1 train time consumed: 7.63s
Validation Epoch: 10, Average loss: 0.0167, Accuracy: 0.4506
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.46062272394161,
                        "time": 5.4442739940004685,
                        "accuracy": 0.45055379746835444,
                        "total_cost": 2114615.289680252
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4724
Profiling... [256/50048]	Loss: 1.4417
Profiling... [384/50048]	Loss: 1.7508
Profiling... [512/50048]	Loss: 1.4684
Profiling... [640/50048]	Loss: 1.6085
Profiling... [768/50048]	Loss: 1.7416
Profiling... [896/50048]	Loss: 1.7836
Profiling... [1024/50048]	Loss: 1.6796
Profiling... [1152/50048]	Loss: 1.7106
Profiling... [1280/50048]	Loss: 1.5622
Profiling... [1408/50048]	Loss: 1.4995
Profiling... [1536/50048]	Loss: 1.3600
Profiling... [1664/50048]	Loss: 1.4624
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 10, Average loss: 0.0145, Accuracy: 0.4964
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.43841480070057,
                        "time": 2.1642073589991924,
                        "accuracy": 0.4964398734177215,
                        "total_cost": 762904.6498974046
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8170
Profiling... [256/50048]	Loss: 1.5509
Profiling... [384/50048]	Loss: 1.4645
Profiling... [512/50048]	Loss: 1.5649
Profiling... [640/50048]	Loss: 1.7126
Profiling... [768/50048]	Loss: 1.1243
Profiling... [896/50048]	Loss: 1.4131
Profiling... [1024/50048]	Loss: 1.7253
Profiling... [1152/50048]	Loss: 1.6500
Profiling... [1280/50048]	Loss: 1.2549
Profiling... [1408/50048]	Loss: 1.5267
Profiling... [1536/50048]	Loss: 1.6578
Profiling... [1664/50048]	Loss: 1.5758
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 10, Average loss: 0.0160, Accuracy: 0.4620
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.45548416238475,
                        "time": 2.1628269489992817,
                        "accuracy": 0.4620253164556962,
                        "total_cost": 819207.7416415086
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.3670
Profiling... [256/50048]	Loss: 1.3973
Profiling... [384/50048]	Loss: 1.5810
Profiling... [512/50048]	Loss: 1.6189
Profiling... [640/50048]	Loss: 1.4985
Profiling... [768/50048]	Loss: 1.7270
Profiling... [896/50048]	Loss: 1.9395
Profiling... [1024/50048]	Loss: 1.5731
Profiling... [1152/50048]	Loss: 1.8032
Profiling... [1280/50048]	Loss: 1.3926
Profiling... [1408/50048]	Loss: 1.8601
Profiling... [1536/50048]	Loss: 1.4933
Profiling... [1664/50048]	Loss: 1.4918
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 10, Average loss: 0.0165, Accuracy: 0.4362
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.46592190559684,
                        "time": 2.432591988999775,
                        "accuracy": 0.4362143987341772,
                        "total_cost": 975904.5077610523
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5145
Profiling... [256/50048]	Loss: 1.4497
Profiling... [384/50048]	Loss: 1.6485
Profiling... [512/50048]	Loss: 1.3737
Profiling... [640/50048]	Loss: 1.8720
Profiling... [768/50048]	Loss: 1.2991
Profiling... [896/50048]	Loss: 1.5187
Profiling... [1024/50048]	Loss: 1.6566
Profiling... [1152/50048]	Loss: 1.7997
Profiling... [1280/50048]	Loss: 1.5297
Profiling... [1408/50048]	Loss: 1.7160
Profiling... [1536/50048]	Loss: 1.6936
Profiling... [1664/50048]	Loss: 1.6638
Profile done
epoch 1 train time consumed: 7.32s
Validation Epoch: 10, Average loss: 0.0170, Accuracy: 0.4412
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.44699430363765,
                        "time": 5.047811665001063,
                        "accuracy": 0.4411590189873418,
                        "total_cost": 2002377.8351010717
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4902
Profiling... [256/50048]	Loss: 1.4562
Profiling... [384/50048]	Loss: 1.6051
Profiling... [512/50048]	Loss: 1.8567
Profiling... [640/50048]	Loss: 1.7343
Profiling... [768/50048]	Loss: 1.7237
Profiling... [896/50048]	Loss: 1.6250
Profiling... [1024/50048]	Loss: 1.5758
Profiling... [1152/50048]	Loss: 1.6948
Profiling... [1280/50048]	Loss: 1.9140
Profiling... [1408/50048]	Loss: 2.0512
Profiling... [1536/50048]	Loss: 1.5841
Profiling... [1664/50048]	Loss: 1.9287
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 10, Average loss: 0.0396, Accuracy: 0.0534
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.42633270935193,
                        "time": 2.1528490710006736,
                        "accuracy": 0.053401898734177215,
                        "total_cost": 7054966.1408199845
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7926
Profiling... [256/50048]	Loss: 1.4807
Profiling... [384/50048]	Loss: 1.6258
Profiling... [512/50048]	Loss: 1.5683
Profiling... [640/50048]	Loss: 1.5589
Profiling... [768/50048]	Loss: 1.9051
Profiling... [896/50048]	Loss: 1.8272
Profiling... [1024/50048]	Loss: 2.0317
Profiling... [1152/50048]	Loss: 1.8525
Profiling... [1280/50048]	Loss: 1.9550
Profiling... [1408/50048]	Loss: 2.0216
Profiling... [1536/50048]	Loss: 1.7679
Profiling... [1664/50048]	Loss: 1.7975
Profile done
epoch 1 train time consumed: 4.39s
Validation Epoch: 10, Average loss: 0.0229, Accuracy: 0.3312
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.44177593809822,
                        "time": 2.2639592140003515,
                        "accuracy": 0.331190664556962,
                        "total_cost": 1196268.2069558143
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4454
Profiling... [256/50048]	Loss: 1.4783
Profiling... [384/50048]	Loss: 1.5997
Profiling... [512/50048]	Loss: 1.6675
Profiling... [640/50048]	Loss: 1.4908
Profiling... [768/50048]	Loss: 1.6931
Profiling... [896/50048]	Loss: 1.6635
Profiling... [1024/50048]	Loss: 1.6766
Profiling... [1152/50048]	Loss: 1.9191
Profiling... [1280/50048]	Loss: 1.5729
Profiling... [1408/50048]	Loss: 2.1524
Profiling... [1536/50048]	Loss: 1.7868
Profiling... [1664/50048]	Loss: 1.8198
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 10, Average loss: 0.0274, Accuracy: 0.2926
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.44649244634802,
                        "time": 2.429575702000875,
                        "accuracy": 0.2926226265822785,
                        "total_cost": 1452983.1572358054
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5897
Profiling... [256/50048]	Loss: 1.5807
Profiling... [384/50048]	Loss: 1.7333
Profiling... [512/50048]	Loss: 1.6659
Profiling... [640/50048]	Loss: 1.8776
Profiling... [768/50048]	Loss: 2.0763
Profiling... [896/50048]	Loss: 2.1972
Profiling... [1024/50048]	Loss: 1.7317
Profiling... [1152/50048]	Loss: 1.8296
Profiling... [1280/50048]	Loss: 1.8301
Profiling... [1408/50048]	Loss: 1.8265
Profiling... [1536/50048]	Loss: 1.6027
Profiling... [1664/50048]	Loss: 1.8276
Profile done
epoch 1 train time consumed: 7.34s
Validation Epoch: 10, Average loss: 0.0267, Accuracy: 0.2932
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.42937553625961,
                        "time": 5.079125812999337,
                        "accuracy": 0.2932159810126582,
                        "total_cost": 3031373.0315965014
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5372
Profiling... [256/50048]	Loss: 1.6998
Profiling... [384/50048]	Loss: 1.5344
Profiling... [512/50048]	Loss: 1.7336
Profiling... [640/50048]	Loss: 1.8308
Profiling... [768/50048]	Loss: 1.7040
Profiling... [896/50048]	Loss: 1.8836
Profiling... [1024/50048]	Loss: 1.8335
Profiling... [1152/50048]	Loss: 1.5377
Profiling... [1280/50048]	Loss: 1.7968
Profiling... [1408/50048]	Loss: 1.4943
Profiling... [1536/50048]	Loss: 1.5056
Profiling... [1664/50048]	Loss: 2.1251
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 10, Average loss: 0.0278, Accuracy: 0.2565
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.409502332591,
                        "time": 2.156069196000317,
                        "accuracy": 0.2565268987341772,
                        "total_cost": 1470848.130008543
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7393
Profiling... [256/50048]	Loss: 1.5119
Profiling... [384/50048]	Loss: 1.7259
Profiling... [512/50048]	Loss: 1.8893
Profiling... [640/50048]	Loss: 1.8300
Profiling... [768/50048]	Loss: 1.6999
Profiling... [896/50048]	Loss: 1.9994
Profiling... [1024/50048]	Loss: 1.7401
Profiling... [1152/50048]	Loss: 1.8489
Profiling... [1280/50048]	Loss: 2.0966
Profiling... [1408/50048]	Loss: 1.6131
Profiling... [1536/50048]	Loss: 1.6348
Profiling... [1664/50048]	Loss: 1.7049
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 10, Average loss: 0.0258, Accuracy: 0.2999
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.42614252408814,
                        "time": 2.1679865530004463,
                        "accuracy": 0.299940664556962,
                        "total_cost": 1264909.002370455
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5261
Profiling... [256/50048]	Loss: 1.4193
Profiling... [384/50048]	Loss: 1.9461
Profiling... [512/50048]	Loss: 1.8799
Profiling... [640/50048]	Loss: 1.7658
Profiling... [768/50048]	Loss: 1.8970
Profiling... [896/50048]	Loss: 1.9595
Profiling... [1024/50048]	Loss: 1.3442
Profiling... [1152/50048]	Loss: 1.7664
Profiling... [1280/50048]	Loss: 1.9073
Profiling... [1408/50048]	Loss: 1.8401
Profiling... [1536/50048]	Loss: 1.7283
Profiling... [1664/50048]	Loss: 1.9017
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 10, Average loss: 0.0259, Accuracy: 0.2680
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.43531180993737,
                        "time": 2.42797217000043,
                        "accuracy": 0.267998417721519,
                        "total_cost": 1585438.9490895797
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.3444
Profiling... [256/50048]	Loss: 1.4693
Profiling... [384/50048]	Loss: 1.5547
Profiling... [512/50048]	Loss: 1.8508
Profiling... [640/50048]	Loss: 1.8830
Profiling... [768/50048]	Loss: 1.4588
Profiling... [896/50048]	Loss: 1.8222
Profiling... [1024/50048]	Loss: 2.0356
Profiling... [1152/50048]	Loss: 1.8733
Profiling... [1280/50048]	Loss: 1.7897
Profiling... [1408/50048]	Loss: 1.5030
Profiling... [1536/50048]	Loss: 1.5761
Profiling... [1664/50048]	Loss: 1.6849
Profile done
epoch 1 train time consumed: 7.58s
Validation Epoch: 10, Average loss: 0.0233, Accuracy: 0.3137
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.41637931955741,
                        "time": 5.427080874000239,
                        "accuracy": 0.3136867088607595,
                        "total_cost": 3027667.8167184186
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4445
Profiling... [256/50048]	Loss: 1.5654
Profiling... [384/50048]	Loss: 1.7790
Profiling... [512/50048]	Loss: 1.6347
Profiling... [640/50048]	Loss: 1.5943
Profiling... [768/50048]	Loss: 1.3876
Profiling... [896/50048]	Loss: 1.7745
Profiling... [1024/50048]	Loss: 1.8352
Profiling... [1152/50048]	Loss: 1.7205
Profiling... [1280/50048]	Loss: 1.7835
Profiling... [1408/50048]	Loss: 1.6360
Profiling... [1536/50048]	Loss: 1.8077
Profiling... [1664/50048]	Loss: 2.0774
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 10, Average loss: 0.0276, Accuracy: 0.2437
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.39403765845448,
                        "time": 2.1578133040002285,
                        "accuracy": 0.24367088607594936,
                        "total_cost": 1549702.2819638005
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5048
Profiling... [256/50048]	Loss: 1.7145
Profiling... [384/50048]	Loss: 1.8317
Profiling... [512/50048]	Loss: 1.7421
Profiling... [640/50048]	Loss: 1.8992
Profiling... [768/50048]	Loss: 1.7332
Profiling... [896/50048]	Loss: 1.7017
Profiling... [1024/50048]	Loss: 1.6520
Profiling... [1152/50048]	Loss: 1.5845
Profiling... [1280/50048]	Loss: 1.9064
Profiling... [1408/50048]	Loss: 1.8555
Profiling... [1536/50048]	Loss: 1.8190
Profiling... [1664/50048]	Loss: 1.9607
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 10, Average loss: 0.0276, Accuracy: 0.2493
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.41098971338307,
                        "time": 2.18153545000132,
                        "accuracy": 0.24930775316455697,
                        "total_cost": 1531315.0068712162
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.3919
Profiling... [256/50048]	Loss: 1.5442
Profiling... [384/50048]	Loss: 1.4455
Profiling... [512/50048]	Loss: 1.9836
Profiling... [640/50048]	Loss: 1.8282
Profiling... [768/50048]	Loss: 2.0113
Profiling... [896/50048]	Loss: 2.0708
Profiling... [1024/50048]	Loss: 1.5453
Profiling... [1152/50048]	Loss: 1.5262
Profiling... [1280/50048]	Loss: 1.9502
Profiling... [1408/50048]	Loss: 1.7559
Profiling... [1536/50048]	Loss: 1.7019
Profiling... [1664/50048]	Loss: 1.8065
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 10, Average loss: 0.0290, Accuracy: 0.2325
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.41964597656572,
                        "time": 2.441039498000464,
                        "accuracy": 0.23249604430379747,
                        "total_cost": 1837372.8182312297
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.3376
Profiling... [256/50048]	Loss: 1.8730
Profiling... [384/50048]	Loss: 1.7923
Profiling... [512/50048]	Loss: 1.4525
Profiling... [640/50048]	Loss: 1.9850
Profiling... [768/50048]	Loss: 1.5162
Profiling... [896/50048]	Loss: 1.5876
Profiling... [1024/50048]	Loss: 1.6146
Profiling... [1152/50048]	Loss: 1.8638
Profiling... [1280/50048]	Loss: 1.8381
Profiling... [1408/50048]	Loss: 1.9993
Profiling... [1536/50048]	Loss: 1.8999
Profiling... [1664/50048]	Loss: 1.8812
Profile done
epoch 1 train time consumed: 7.18s
Validation Epoch: 10, Average loss: 0.0266, Accuracy: 0.2947
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.40145208539488,
                        "time": 5.052768903000469,
                        "accuracy": 0.2946993670886076,
                        "total_cost": 3000463.0371643053
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6035
Profiling... [512/50176]	Loss: 1.6143
Profiling... [768/50176]	Loss: 1.4981
Profiling... [1024/50176]	Loss: 1.5365
Profiling... [1280/50176]	Loss: 1.4580
Profiling... [1536/50176]	Loss: 1.4934
Profiling... [1792/50176]	Loss: 1.5775
Profiling... [2048/50176]	Loss: 1.5489
Profiling... [2304/50176]	Loss: 1.4664
Profiling... [2560/50176]	Loss: 1.4907
Profiling... [2816/50176]	Loss: 1.5582
Profiling... [3072/50176]	Loss: 1.4676
Profiling... [3328/50176]	Loss: 1.5161
Profile done
epoch 1 train time consumed: 4.13s
Validation Epoch: 10, Average loss: 0.0063, Accuracy: 0.5459
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.38312862428083,
                        "time": 2.3993458439999813,
                        "accuracy": 0.5458984375,
                        "total_cost": 769164.1775398867
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5185
Profiling... [512/50176]	Loss: 1.5558
Profiling... [768/50176]	Loss: 1.3862
Profiling... [1024/50176]	Loss: 1.5129
Profiling... [1280/50176]	Loss: 1.5823
Profiling... [1536/50176]	Loss: 1.5284
Profiling... [1792/50176]	Loss: 1.4901
Profiling... [2048/50176]	Loss: 1.3461
Profiling... [2304/50176]	Loss: 1.6354
Profiling... [2560/50176]	Loss: 1.3470
Profiling... [2816/50176]	Loss: 1.3662
Profiling... [3072/50176]	Loss: 1.3903
Profiling... [3328/50176]	Loss: 1.4723
Profile done
epoch 1 train time consumed: 4.11s
Validation Epoch: 10, Average loss: 0.0064, Accuracy: 0.5407
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.40001306264111,
                        "time": 2.4486900869997044,
                        "accuracy": 0.54072265625,
                        "total_cost": 792496.4124803089
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5262
Profiling... [512/50176]	Loss: 1.4323
Profiling... [768/50176]	Loss: 1.4763
Profiling... [1024/50176]	Loss: 1.5095
Profiling... [1280/50176]	Loss: 1.2816
Profiling... [1536/50176]	Loss: 1.2707
Profiling... [1792/50176]	Loss: 1.5381
Profiling... [2048/50176]	Loss: 1.4760
Profiling... [2304/50176]	Loss: 1.4321
Profiling... [2560/50176]	Loss: 1.5329
Profiling... [2816/50176]	Loss: 1.4897
Profiling... [3072/50176]	Loss: 1.5620
Profiling... [3328/50176]	Loss: 1.3818
Profile done
epoch 1 train time consumed: 4.45s
Validation Epoch: 10, Average loss: 0.0064, Accuracy: 0.5399
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.40800876192637,
                        "time": 2.7684085900000355,
                        "accuracy": 0.53994140625,
                        "total_cost": 897266.8101428946
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7105
Profiling... [512/50176]	Loss: 1.4830
Profiling... [768/50176]	Loss: 1.5861
Profiling... [1024/50176]	Loss: 1.4627
Profiling... [1280/50176]	Loss: 1.5233
Profiling... [1536/50176]	Loss: 1.4739
Profiling... [1792/50176]	Loss: 1.6230
Profiling... [2048/50176]	Loss: 1.5707
Profiling... [2304/50176]	Loss: 1.3636
Profiling... [2560/50176]	Loss: 1.3165
Profiling... [2816/50176]	Loss: 1.3539
Profiling... [3072/50176]	Loss: 1.3551
Profiling... [3328/50176]	Loss: 1.4727
Profile done
epoch 1 train time consumed: 9.88s
Validation Epoch: 10, Average loss: 0.0064, Accuracy: 0.5434
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.38239357119427,
                        "time": 7.117131234999761,
                        "accuracy": 0.543359375,
                        "total_cost": 2292217.680287486
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5372
Profiling... [512/50176]	Loss: 1.5251
Profiling... [768/50176]	Loss: 1.4717
Profiling... [1024/50176]	Loss: 1.4393
Profiling... [1280/50176]	Loss: 1.4472
Profiling... [1536/50176]	Loss: 1.5141
Profiling... [1792/50176]	Loss: 1.5706
Profiling... [2048/50176]	Loss: 1.5324
Profiling... [2304/50176]	Loss: 1.5909
Profiling... [2560/50176]	Loss: 1.5306
Profiling... [2816/50176]	Loss: 1.4480
Profiling... [3072/50176]	Loss: 1.6010
Profiling... [3328/50176]	Loss: 1.3322
Profile done
epoch 1 train time consumed: 4.04s
Validation Epoch: 10, Average loss: 0.0065, Accuracy: 0.5322
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.3685436482752,
                        "time": 2.3910332539999217,
                        "accuracy": 0.5322265625,
                        "total_cost": 786189.2827830936
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5062
Profiling... [512/50176]	Loss: 1.4834
Profiling... [768/50176]	Loss: 1.5418
Profiling... [1024/50176]	Loss: 1.4602
Profiling... [1280/50176]	Loss: 1.4740
Profiling... [1536/50176]	Loss: 1.4711
Profiling... [1792/50176]	Loss: 1.4913
Profiling... [2048/50176]	Loss: 1.4558
Profiling... [2304/50176]	Loss: 1.4459
Profiling... [2560/50176]	Loss: 1.3925
Profiling... [2816/50176]	Loss: 1.2694
Profiling... [3072/50176]	Loss: 1.4990
Profiling... [3328/50176]	Loss: 1.4183
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 10, Average loss: 0.0065, Accuracy: 0.5345
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.38535544218195,
                        "time": 2.444763020999744,
                        "accuracy": 0.53447265625,
                        "total_cost": 800477.8610691654
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5553
Profiling... [512/50176]	Loss: 1.6500
Profiling... [768/50176]	Loss: 1.4642
Profiling... [1024/50176]	Loss: 1.3214
Profiling... [1280/50176]	Loss: 1.3582
Profiling... [1536/50176]	Loss: 1.5026
Profiling... [1792/50176]	Loss: 1.4029
Profiling... [2048/50176]	Loss: 1.4541
Profiling... [2304/50176]	Loss: 1.3967
Profiling... [2560/50176]	Loss: 1.5297
Profiling... [2816/50176]	Loss: 1.3868
Profiling... [3072/50176]	Loss: 1.5088
Profiling... [3328/50176]	Loss: 1.4863
Profile done
epoch 1 train time consumed: 4.47s
Validation Epoch: 10, Average loss: 0.0064, Accuracy: 0.5380
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.39285277959644,
                        "time": 2.792991742000595,
                        "accuracy": 0.53798828125,
                        "total_cost": 908520.8207778302
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.3791
Profiling... [512/50176]	Loss: 1.7443
Profiling... [768/50176]	Loss: 1.5582
Profiling... [1024/50176]	Loss: 1.5426
Profiling... [1280/50176]	Loss: 1.3993
Profiling... [1536/50176]	Loss: 1.5457
Profiling... [1792/50176]	Loss: 1.3149
Profiling... [2048/50176]	Loss: 1.2349
Profiling... [2304/50176]	Loss: 1.3450
Profiling... [2560/50176]	Loss: 1.4885
Profiling... [2816/50176]	Loss: 1.5597
Profiling... [3072/50176]	Loss: 1.2925
Profiling... [3328/50176]	Loss: 1.3820
Profile done
epoch 1 train time consumed: 9.83s
Validation Epoch: 10, Average loss: 0.0064, Accuracy: 0.5387
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.36834828734835,
                        "time": 7.003382087999853,
                        "accuracy": 0.538671875,
                        "total_cost": 2275210.424527871
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5826
Profiling... [512/50176]	Loss: 1.4449
Profiling... [768/50176]	Loss: 1.3426
Profiling... [1024/50176]	Loss: 1.4660
Profiling... [1280/50176]	Loss: 1.3833
Profiling... [1536/50176]	Loss: 1.3809
Profiling... [1792/50176]	Loss: 1.5348
Profiling... [2048/50176]	Loss: 1.4258
Profiling... [2304/50176]	Loss: 1.4204
Profiling... [2560/50176]	Loss: 1.4339
Profiling... [2816/50176]	Loss: 1.3022
Profiling... [3072/50176]	Loss: 1.4033
Profiling... [3328/50176]	Loss: 1.4766
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 10, Average loss: 0.0064, Accuracy: 0.5374
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.35267714256877,
                        "time": 2.3896938329999102,
                        "accuracy": 0.53740234375,
                        "total_cost": 778181.2372770924
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4579
Profiling... [512/50176]	Loss: 1.6018
Profiling... [768/50176]	Loss: 1.5184
Profiling... [1024/50176]	Loss: 1.4155
Profiling... [1280/50176]	Loss: 1.4699
Profiling... [1536/50176]	Loss: 1.3809
Profiling... [1792/50176]	Loss: 1.3894
Profiling... [2048/50176]	Loss: 1.4835
Profiling... [2304/50176]	Loss: 1.4199
Profiling... [2560/50176]	Loss: 1.4907
Profiling... [2816/50176]	Loss: 1.4112
Profiling... [3072/50176]	Loss: 1.5757
Profiling... [3328/50176]	Loss: 1.5093
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 10, Average loss: 0.0066, Accuracy: 0.5334
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.37038301007674,
                        "time": 2.438854711001113,
                        "accuracy": 0.5333984375,
                        "total_cost": 800151.5273002554
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.4745
Profiling... [512/50176]	Loss: 1.3373
Profiling... [768/50176]	Loss: 1.4712
Profiling... [1024/50176]	Loss: 1.6307
Profiling... [1280/50176]	Loss: 1.4964
Profiling... [1536/50176]	Loss: 1.5312
Profiling... [1792/50176]	Loss: 1.5577
Profiling... [2048/50176]	Loss: 1.5790
Profiling... [2304/50176]	Loss: 1.5244
Profiling... [2560/50176]	Loss: 1.5137
Profiling... [2816/50176]	Loss: 1.7187
Profiling... [3072/50176]	Loss: 1.5466
Profiling... [3328/50176]	Loss: 1.4509
Profile done
epoch 1 train time consumed: 4.50s
Validation Epoch: 10, Average loss: 0.0064, Accuracy: 0.5391
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.37869048915798,
                        "time": 2.790211895000539,
                        "accuracy": 0.5390625,
                        "total_cost": 905807.9195364068
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.4240
Profiling... [512/50176]	Loss: 1.3157
Profiling... [768/50176]	Loss: 1.4742
Profiling... [1024/50176]	Loss: 1.3659
Profiling... [1280/50176]	Loss: 1.5374
Profiling... [1536/50176]	Loss: 1.4394
Profiling... [1792/50176]	Loss: 1.5058
Profiling... [2048/50176]	Loss: 1.3953
Profiling... [2304/50176]	Loss: 1.4218
Profiling... [2560/50176]	Loss: 1.3440
Profiling... [2816/50176]	Loss: 1.5750
Profiling... [3072/50176]	Loss: 1.2870
Profiling... [3328/50176]	Loss: 1.5178
Profile done
epoch 1 train time consumed: 9.89s
Validation Epoch: 10, Average loss: 0.0065, Accuracy: 0.5332
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.35456211520788,
                        "time": 7.195240017001197,
                        "accuracy": 0.533203125,
                        "total_cost": 2361514.6722465465
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.4805
Profiling... [512/50176]	Loss: 1.6002
Profiling... [768/50176]	Loss: 1.5032
Profiling... [1024/50176]	Loss: 1.6683
Profiling... [1280/50176]	Loss: 1.3002
Profiling... [1536/50176]	Loss: 1.6893
Profiling... [1792/50176]	Loss: 1.4332
Profiling... [2048/50176]	Loss: 1.7544
Profiling... [2304/50176]	Loss: 1.5016
Profiling... [2560/50176]	Loss: 1.2455
Profiling... [2816/50176]	Loss: 1.4901
Profiling... [3072/50176]	Loss: 1.3961
Profiling... [3328/50176]	Loss: 1.6446
Profile done
epoch 1 train time consumed: 4.01s
Validation Epoch: 10, Average loss: 0.0089, Accuracy: 0.4148
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.3403632274608,
                        "time": 2.383278735998829,
                        "accuracy": 0.41484375,
                        "total_cost": 1005375.5873140068
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5525
Profiling... [512/50176]	Loss: 1.4930
Profiling... [768/50176]	Loss: 1.5233
Profiling... [1024/50176]	Loss: 1.4111
Profiling... [1280/50176]	Loss: 1.5626
Profiling... [1536/50176]	Loss: 1.4571
Profiling... [1792/50176]	Loss: 1.6954
Profiling... [2048/50176]	Loss: 1.4737
Profiling... [2304/50176]	Loss: 1.5814
Profiling... [2560/50176]	Loss: 1.4762
Profiling... [2816/50176]	Loss: 1.7486
Profiling... [3072/50176]	Loss: 1.3671
Profiling... [3328/50176]	Loss: 1.4936
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 10, Average loss: 0.0077, Accuracy: 0.4646
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.358340590128,
                        "time": 2.4491766190003545,
                        "accuracy": 0.4646484375,
                        "total_cost": 922430.5383036223
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6534
Profiling... [512/50176]	Loss: 1.5932
Profiling... [768/50176]	Loss: 1.5659
Profiling... [1024/50176]	Loss: 1.5835
Profiling... [1280/50176]	Loss: 1.5788
Profiling... [1536/50176]	Loss: 1.4377
Profiling... [1792/50176]	Loss: 1.4748
Profiling... [2048/50176]	Loss: 1.4061
Profiling... [2304/50176]	Loss: 1.5441
Profiling... [2560/50176]	Loss: 1.5014
Profiling... [2816/50176]	Loss: 1.5852
Profiling... [3072/50176]	Loss: 1.5025
Profiling... [3328/50176]	Loss: 1.3241
Profile done
epoch 1 train time consumed: 4.49s
Validation Epoch: 10, Average loss: 0.0077, Accuracy: 0.4870
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.366137887308,
                        "time": 2.7882606260009197,
                        "accuracy": 0.48701171875,
                        "total_cost": 1001917.5941033985
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5061
Profiling... [512/50176]	Loss: 1.4892
Profiling... [768/50176]	Loss: 1.3833
Profiling... [1024/50176]	Loss: 1.4789
Profiling... [1280/50176]	Loss: 1.3928
Profiling... [1536/50176]	Loss: 1.5359
Profiling... [1792/50176]	Loss: 1.6928
Profiling... [2048/50176]	Loss: 1.7172
Profiling... [2304/50176]	Loss: 1.6124
Profiling... [2560/50176]	Loss: 1.5766
Profiling... [2816/50176]	Loss: 1.4829
Profiling... [3072/50176]	Loss: 1.5859
Profiling... [3328/50176]	Loss: 1.7097
Profile done
epoch 1 train time consumed: 9.86s
Validation Epoch: 10, Average loss: 0.0077, Accuracy: 0.4721
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.34254684146951,
                        "time": 6.99895578199903,
                        "accuracy": 0.4720703125,
                        "total_cost": 2594565.3209230993
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5404
Profiling... [512/50176]	Loss: 1.5588
Profiling... [768/50176]	Loss: 1.4092
Profiling... [1024/50176]	Loss: 1.5556
Profiling... [1280/50176]	Loss: 1.5565
Profiling... [1536/50176]	Loss: 1.5308
Profiling... [1792/50176]	Loss: 1.5372
Profiling... [2048/50176]	Loss: 1.4332
Profiling... [2304/50176]	Loss: 1.4975
Profiling... [2560/50176]	Loss: 1.6141
Profiling... [2816/50176]	Loss: 1.4085
Profiling... [3072/50176]	Loss: 1.4232
Profiling... [3328/50176]	Loss: 1.4020
Profile done
epoch 1 train time consumed: 4.33s
Validation Epoch: 10, Average loss: 0.0076, Accuracy: 0.4670
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.3285599994507,
                        "time": 2.435748493000574,
                        "accuracy": 0.4669921875,
                        "total_cost": 912768.9877576388
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4485
Profiling... [512/50176]	Loss: 1.4633
Profiling... [768/50176]	Loss: 1.6095
Profiling... [1024/50176]	Loss: 1.5824
Profiling... [1280/50176]	Loss: 1.5181
Profiling... [1536/50176]	Loss: 1.5911
Profiling... [1792/50176]	Loss: 1.4344
Profiling... [2048/50176]	Loss: 1.4871
Profiling... [2304/50176]	Loss: 1.5062
Profiling... [2560/50176]	Loss: 1.3817
Profiling... [2816/50176]	Loss: 1.4648
Profiling... [3072/50176]	Loss: 1.4566
Profiling... [3328/50176]	Loss: 1.4909
Profile done
epoch 1 train time consumed: 4.13s
Validation Epoch: 10, Average loss: 0.0075, Accuracy: 0.4733
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.34638958229242,
                        "time": 2.4436940449995745,
                        "accuracy": 0.47333984375,
                        "total_cost": 903466.005496026
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6434
Profiling... [512/50176]	Loss: 1.5196
Profiling... [768/50176]	Loss: 1.5361
Profiling... [1024/50176]	Loss: 1.4515
Profiling... [1280/50176]	Loss: 1.7235
Profiling... [1536/50176]	Loss: 1.4160
Profiling... [1792/50176]	Loss: 1.3920
Profiling... [2048/50176]	Loss: 1.6127
Profiling... [2304/50176]	Loss: 1.6200
Profiling... [2560/50176]	Loss: 1.4531
Profiling... [2816/50176]	Loss: 1.3894
Profiling... [3072/50176]	Loss: 1.3590
Profiling... [3328/50176]	Loss: 1.4960
Profile done
epoch 1 train time consumed: 4.46s
Validation Epoch: 10, Average loss: 0.0095, Accuracy: 0.3956
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.35483718968358,
                        "time": 2.8027502479999384,
                        "accuracy": 0.39560546875,
                        "total_cost": 1239824.3506333965
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5028
Profiling... [512/50176]	Loss: 1.3475
Profiling... [768/50176]	Loss: 1.5721
Profiling... [1024/50176]	Loss: 1.4154
Profiling... [1280/50176]	Loss: 1.5386
Profiling... [1536/50176]	Loss: 1.4928
Profiling... [1792/50176]	Loss: 1.6281
Profiling... [2048/50176]	Loss: 1.5810
Profiling... [2304/50176]	Loss: 1.4656
Profiling... [2560/50176]	Loss: 1.4779
Profiling... [2816/50176]	Loss: 1.4490
Profiling... [3072/50176]	Loss: 1.2648
Profiling... [3328/50176]	Loss: 1.4662
Profile done
epoch 1 train time consumed: 9.96s
Validation Epoch: 10, Average loss: 0.0087, Accuracy: 0.4253
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.33133268898658,
                        "time": 7.107639480000216,
                        "accuracy": 0.42529296875,
                        "total_cost": 2924659.000725692
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.4560
Profiling... [512/50176]	Loss: 1.4852
Profiling... [768/50176]	Loss: 1.4748
Profiling... [1024/50176]	Loss: 1.5989
Profiling... [1280/50176]	Loss: 1.4689
Profiling... [1536/50176]	Loss: 1.4177
Profiling... [1792/50176]	Loss: 1.3655
Profiling... [2048/50176]	Loss: 1.4689
Profiling... [2304/50176]	Loss: 1.5401
Profiling... [2560/50176]	Loss: 1.4200
Profiling... [2816/50176]	Loss: 1.3616
Profiling... [3072/50176]	Loss: 1.4271
Profiling... [3328/50176]	Loss: 1.5846
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 10, Average loss: 0.0075, Accuracy: 0.4813
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.31835195356702,
                        "time": 2.449278325000705,
                        "accuracy": 0.48125,
                        "total_cost": 890646.6636366199
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5478
Profiling... [512/50176]	Loss: 1.4733
Profiling... [768/50176]	Loss: 1.5101
Profiling... [1024/50176]	Loss: 1.7089
Profiling... [1280/50176]	Loss: 1.4303
Profiling... [1536/50176]	Loss: 1.6646
Profiling... [1792/50176]	Loss: 1.2683
Profiling... [2048/50176]	Loss: 1.3964
Profiling... [2304/50176]	Loss: 1.4485
Profiling... [2560/50176]	Loss: 1.5743
Profiling... [2816/50176]	Loss: 1.5246
Profiling... [3072/50176]	Loss: 1.2890
Profiling... [3328/50176]	Loss: 1.5777
Profile done
epoch 1 train time consumed: 4.20s
Validation Epoch: 10, Average loss: 0.0090, Accuracy: 0.4283
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.33796509321037,
                        "time": 2.4924358859989297,
                        "accuracy": 0.4283203125,
                        "total_cost": 1018341.337827196
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5068
Profiling... [512/50176]	Loss: 1.4786
Profiling... [768/50176]	Loss: 1.4303
Profiling... [1024/50176]	Loss: 1.4931
Profiling... [1280/50176]	Loss: 1.6001
Profiling... [1536/50176]	Loss: 1.4169
Profiling... [1792/50176]	Loss: 1.4752
Profiling... [2048/50176]	Loss: 1.5055
Profiling... [2304/50176]	Loss: 1.4564
Profiling... [2560/50176]	Loss: 1.4865
Profiling... [2816/50176]	Loss: 1.5368
Profiling... [3072/50176]	Loss: 1.5164
Profiling... [3328/50176]	Loss: 1.5049
Profile done
epoch 1 train time consumed: 4.55s
Validation Epoch: 10, Average loss: 0.0075, Accuracy: 0.4745
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.34715891061009,
                        "time": 2.834358684000108,
                        "accuracy": 0.47451171875,
                        "total_cost": 1045311.9493163602
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7496
Profiling... [512/50176]	Loss: 1.4052
Profiling... [768/50176]	Loss: 1.6219
Profiling... [1024/50176]	Loss: 1.5119
Profiling... [1280/50176]	Loss: 1.5689
Profiling... [1536/50176]	Loss: 1.5412
Profiling... [1792/50176]	Loss: 1.4846
Profiling... [2048/50176]	Loss: 1.4929
Profiling... [2304/50176]	Loss: 1.4951
Profiling... [2560/50176]	Loss: 1.4914
Profiling... [2816/50176]	Loss: 1.5620
Profiling... [3072/50176]	Loss: 1.5279
Profiling... [3328/50176]	Loss: 1.5369
Profile done
epoch 1 train time consumed: 10.07s
Validation Epoch: 10, Average loss: 0.0072, Accuracy: 0.5067
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.32286285371146,
                        "time": 7.173584039001071,
                        "accuracy": 0.50673828125,
                        "total_cost": 2477368.0088436925
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6412
Profiling... [512/50176]	Loss: 1.5167
Profiling... [768/50176]	Loss: 1.6059
Profiling... [1024/50176]	Loss: 1.4242
Profiling... [1280/50176]	Loss: 1.6680
Profiling... [1536/50176]	Loss: 1.7608
Profiling... [1792/50176]	Loss: 1.5810
Profiling... [2048/50176]	Loss: 1.5039
Profiling... [2304/50176]	Loss: 1.7113
Profiling... [2560/50176]	Loss: 1.7702
Profiling... [2816/50176]	Loss: 1.7594
Profiling... [3072/50176]	Loss: 1.5400
Profiling... [3328/50176]	Loss: 1.8629
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 10, Average loss: 0.0108, Accuracy: 0.3393
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.31088982971097,
                        "time": 2.450063322999995,
                        "accuracy": 0.3392578125,
                        "total_cost": 1263820.804495104
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4837
Profiling... [512/50176]	Loss: 1.3331
Profiling... [768/50176]	Loss: 1.4946
Profiling... [1024/50176]	Loss: 1.7401
Profiling... [1280/50176]	Loss: 1.7420
Profiling... [1536/50176]	Loss: 1.6982
Profiling... [1792/50176]	Loss: 1.6281
Profiling... [2048/50176]	Loss: 1.7162
Profiling... [2304/50176]	Loss: 1.6166
Profiling... [2560/50176]	Loss: 1.6609
Profiling... [2816/50176]	Loss: 1.5772
Profiling... [3072/50176]	Loss: 1.7249
Profiling... [3328/50176]	Loss: 1.5801
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 10, Average loss: 0.0119, Accuracy: 0.3655
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.32817528182201,
                        "time": 2.4882642539996596,
                        "accuracy": 0.36552734375,
                        "total_cost": 1191282.2717519074
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.3936
Profiling... [512/50176]	Loss: 1.3979
Profiling... [768/50176]	Loss: 1.3165
Profiling... [1024/50176]	Loss: 1.5883
Profiling... [1280/50176]	Loss: 1.5251
Profiling... [1536/50176]	Loss: 1.4901
Profiling... [1792/50176]	Loss: 1.5372
Profiling... [2048/50176]	Loss: 1.7425
Profiling... [2304/50176]	Loss: 1.6939
Profiling... [2560/50176]	Loss: 1.9815
Profiling... [2816/50176]	Loss: 1.6860
Profiling... [3072/50176]	Loss: 1.7642
Profiling... [3328/50176]	Loss: 1.5199
Profile done
epoch 1 train time consumed: 4.60s
Validation Epoch: 10, Average loss: 0.0107, Accuracy: 0.3443
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.33638012556429,
                        "time": 2.8340066780001507,
                        "accuracy": 0.3443359375,
                        "total_cost": 1440311.9588701844
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.4373
Profiling... [512/50176]	Loss: 1.5989
Profiling... [768/50176]	Loss: 1.5467
Profiling... [1024/50176]	Loss: 1.7945
Profiling... [1280/50176]	Loss: 1.5643
Profiling... [1536/50176]	Loss: 1.6370
Profiling... [1792/50176]	Loss: 1.7388
Profiling... [2048/50176]	Loss: 1.6426
Profiling... [2304/50176]	Loss: 1.6089
Profiling... [2560/50176]	Loss: 1.5614
Profiling... [2816/50176]	Loss: 1.6624
Profiling... [3072/50176]	Loss: 1.6684
Profiling... [3328/50176]	Loss: 1.7397
Profile done
epoch 1 train time consumed: 10.02s
Validation Epoch: 10, Average loss: 0.0153, Accuracy: 0.2183
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.31198161104467,
                        "time": 7.144654136000099,
                        "accuracy": 0.21826171875,
                        "total_cost": 5728510.161839901
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.4104
Profiling... [512/50176]	Loss: 1.5896
Profiling... [768/50176]	Loss: 1.7119
Profiling... [1024/50176]	Loss: 1.5703
Profiling... [1280/50176]	Loss: 1.8060
Profiling... [1536/50176]	Loss: 1.5279
Profiling... [1792/50176]	Loss: 1.7414
Profiling... [2048/50176]	Loss: 1.4422
Profiling... [2304/50176]	Loss: 1.5509
Profiling... [2560/50176]	Loss: 1.6248
Profiling... [2816/50176]	Loss: 1.5126
Profiling... [3072/50176]	Loss: 1.6118
Profiling... [3328/50176]	Loss: 1.5949
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 10, Average loss: 0.0142, Accuracy: 0.2878
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.29648426388293,
                        "time": 2.4375125220012706,
                        "accuracy": 0.28779296875,
                        "total_cost": 1482192.8874877084
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5382
Profiling... [512/50176]	Loss: 1.5443
Profiling... [768/50176]	Loss: 1.4786
Profiling... [1024/50176]	Loss: 1.5822
Profiling... [1280/50176]	Loss: 1.5155
Profiling... [1536/50176]	Loss: 1.6098
Profiling... [1792/50176]	Loss: 1.6657
Profiling... [2048/50176]	Loss: 1.8137
Profiling... [2304/50176]	Loss: 1.7702
Profiling... [2560/50176]	Loss: 1.5324
Profiling... [2816/50176]	Loss: 1.6954
Profiling... [3072/50176]	Loss: 1.8709
Profiling... [3328/50176]	Loss: 1.5416
Profile done
epoch 1 train time consumed: 4.11s
Validation Epoch: 10, Average loss: 0.0172, Accuracy: 0.2397
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.31429227234852,
                        "time": 2.490972323999813,
                        "accuracy": 0.23974609375,
                        "total_cost": 1818257.5986181935
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6871
Profiling... [512/50176]	Loss: 1.4897
Profiling... [768/50176]	Loss: 1.6007
Profiling... [1024/50176]	Loss: 1.8485
Profiling... [1280/50176]	Loss: 1.6048
Profiling... [1536/50176]	Loss: 1.5823
Profiling... [1792/50176]	Loss: 1.6659
Profiling... [2048/50176]	Loss: 1.8526
Profiling... [2304/50176]	Loss: 1.7521
Profiling... [2560/50176]	Loss: 1.7763
Profiling... [2816/50176]	Loss: 1.6669
Profiling... [3072/50176]	Loss: 1.6445
Profiling... [3328/50176]	Loss: 1.8153
Profile done
epoch 1 train time consumed: 4.49s
Validation Epoch: 10, Average loss: 0.0165, Accuracy: 0.2044
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.32260161043648,
                        "time": 2.79097881800044,
                        "accuracy": 0.20439453125,
                        "total_cost": 2389600.593338169
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.4903
Profiling... [512/50176]	Loss: 1.5256
Profiling... [768/50176]	Loss: 1.7666
Profiling... [1024/50176]	Loss: 1.5567
Profiling... [1280/50176]	Loss: 1.6389
Profiling... [1536/50176]	Loss: 1.7470
Profiling... [1792/50176]	Loss: 1.6552
Profiling... [2048/50176]	Loss: 1.7661
Profiling... [2304/50176]	Loss: 1.6647
Profiling... [2560/50176]	Loss: 1.7304
Profiling... [2816/50176]	Loss: 1.6243
Profiling... [3072/50176]	Loss: 1.6124
Profiling... [3328/50176]	Loss: 1.6445
Profile done
epoch 1 train time consumed: 9.99s
Validation Epoch: 10, Average loss: 0.0146, Accuracy: 0.2756
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.29966031567103,
                        "time": 7.004836521999096,
                        "accuracy": 0.2755859375,
                        "total_cost": 4448145.658193614
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.4145
Profiling... [512/50176]	Loss: 1.5523
Profiling... [768/50176]	Loss: 1.5453
Profiling... [1024/50176]	Loss: 1.6554
Profiling... [1280/50176]	Loss: 1.7036
Profiling... [1536/50176]	Loss: 1.5797
Profiling... [1792/50176]	Loss: 1.7106
Profiling... [2048/50176]	Loss: 1.7107
Profiling... [2304/50176]	Loss: 1.7138
Profiling... [2560/50176]	Loss: 1.8994
Profiling... [2816/50176]	Loss: 1.5550
Profiling... [3072/50176]	Loss: 1.6826
Profiling... [3328/50176]	Loss: 1.6966
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 10, Average loss: 0.0137, Accuracy: 0.2744
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.28561412547545,
                        "time": 2.389459973999692,
                        "accuracy": 0.2744140625,
                        "total_cost": 1523812.1969421522
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5097
Profiling... [512/50176]	Loss: 1.4611
Profiling... [768/50176]	Loss: 1.5824
Profiling... [1024/50176]	Loss: 1.6419
Profiling... [1280/50176]	Loss: 1.5837
Profiling... [1536/50176]	Loss: 1.5432
Profiling... [1792/50176]	Loss: 1.6383
Profiling... [2048/50176]	Loss: 1.7718
Profiling... [2304/50176]	Loss: 1.6531
Profiling... [2560/50176]	Loss: 1.7466
Profiling... [2816/50176]	Loss: 1.6317
Profiling... [3072/50176]	Loss: 1.7732
Profiling... [3328/50176]	Loss: 1.5603
Profile done
epoch 1 train time consumed: 4.09s
Validation Epoch: 10, Average loss: 0.0122, Accuracy: 0.3091
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.30313261801406,
                        "time": 2.436029165999571,
                        "accuracy": 0.30908203125,
                        "total_cost": 1379262.01120734
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5704
Profiling... [512/50176]	Loss: 1.6320
Profiling... [768/50176]	Loss: 1.6807
Profiling... [1024/50176]	Loss: 1.6611
Profiling... [1280/50176]	Loss: 1.6151
Profiling... [1536/50176]	Loss: 1.7023
Profiling... [1792/50176]	Loss: 1.4911
Profiling... [2048/50176]	Loss: 1.7041
Profiling... [2304/50176]	Loss: 1.5817
Profiling... [2560/50176]	Loss: 1.6643
Profiling... [2816/50176]	Loss: 1.7391
Profiling... [3072/50176]	Loss: 1.5206
Profiling... [3328/50176]	Loss: 1.4621
Profile done
epoch 1 train time consumed: 4.57s
Validation Epoch: 10, Average loss: 0.0160, Accuracy: 0.2173
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.31046470261212,
                        "time": 2.798816418999195,
                        "accuracy": 0.21728515625,
                        "total_cost": 2254147.875436655
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5448
Profiling... [512/50176]	Loss: 1.5326
Profiling... [768/50176]	Loss: 1.3855
Profiling... [1024/50176]	Loss: 1.5289
Profiling... [1280/50176]	Loss: 1.5465
Profiling... [1536/50176]	Loss: 1.7677
Profiling... [1792/50176]	Loss: 1.7810
Profiling... [2048/50176]	Loss: 1.6295
Profiling... [2304/50176]	Loss: 1.5363
Profiling... [2560/50176]	Loss: 1.6025
Profiling... [2816/50176]	Loss: 1.5480
Profiling... [3072/50176]	Loss: 1.6449
Profiling... [3328/50176]	Loss: 1.8420
Profile done
epoch 1 train time consumed: 9.87s
Validation Epoch: 10, Average loss: 0.0103, Accuracy: 0.3342
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.28817924325274,
                        "time": 7.100216400000136,
                        "accuracy": 0.3341796875,
                        "total_cost": 3718172.9365284177
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4935
Profiling... [1024/50176]	Loss: 1.3890
Profiling... [1536/50176]	Loss: 1.5629
Profiling... [2048/50176]	Loss: 1.3445
Profiling... [2560/50176]	Loss: 1.5092
Profiling... [3072/50176]	Loss: 1.5207
Profiling... [3584/50176]	Loss: 1.4806
Profiling... [4096/50176]	Loss: 1.3606
Profiling... [4608/50176]	Loss: 1.5052
Profiling... [5120/50176]	Loss: 1.3506
Profiling... [5632/50176]	Loss: 1.4283
Profiling... [6144/50176]	Loss: 1.2667
Profiling... [6656/50176]	Loss: 1.4315
Profile done
epoch 1 train time consumed: 7.17s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.5432
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.2867526324721,
                        "time": 4.533053508999728,
                        "accuracy": 0.5431640625,
                        "total_cost": 1460487.5742767912
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5681
Profiling... [1024/50176]	Loss: 1.3893
Profiling... [1536/50176]	Loss: 1.4849
Profiling... [2048/50176]	Loss: 1.5062
Profiling... [2560/50176]	Loss: 1.6743
Profiling... [3072/50176]	Loss: 1.4846
Profiling... [3584/50176]	Loss: 1.4437
Profiling... [4096/50176]	Loss: 1.5510
Profiling... [4608/50176]	Loss: 1.4625
Profiling... [5120/50176]	Loss: 1.4044
Profiling... [5632/50176]	Loss: 1.4983
Profiling... [6144/50176]	Loss: 1.3043
Profiling... [6656/50176]	Loss: 1.4417
Profile done
epoch 1 train time consumed: 7.13s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.5400
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.31394972678194,
                        "time": 4.665774063001663,
                        "accuracy": 0.5400390625,
                        "total_cost": 1511947.038137248
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.3428
Profiling... [1024/50176]	Loss: 1.6231
Profiling... [1536/50176]	Loss: 1.4965
Profiling... [2048/50176]	Loss: 1.5398
Profiling... [2560/50176]	Loss: 1.5235
Profiling... [3072/50176]	Loss: 1.4752
Profiling... [3584/50176]	Loss: 1.5789
Profiling... [4096/50176]	Loss: 1.4682
Profiling... [4608/50176]	Loss: 1.4024
Profiling... [5120/50176]	Loss: 1.4985
Profiling... [5632/50176]	Loss: 1.3636
Profiling... [6144/50176]	Loss: 1.4514
Profiling... [6656/50176]	Loss: 1.3999
Profile done
epoch 1 train time consumed: 8.10s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.5444
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.32070603115957,
                        "time": 5.352059053999255,
                        "accuracy": 0.54443359375,
                        "total_cost": 1720338.9820209267
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4746
Profiling... [1024/50176]	Loss: 1.3877
Profiling... [1536/50176]	Loss: 1.4135
Profiling... [2048/50176]	Loss: 1.5424
Profiling... [2560/50176]	Loss: 1.4442
Profiling... [3072/50176]	Loss: 1.4611
Profiling... [3584/50176]	Loss: 1.4896
Profiling... [4096/50176]	Loss: 1.4147
Profiling... [4608/50176]	Loss: 1.4201
Profiling... [5120/50176]	Loss: 1.5200
Profiling... [5632/50176]	Loss: 1.4309
Profiling... [6144/50176]	Loss: 1.3759
Profiling... [6656/50176]	Loss: 1.4656
Profile done
epoch 1 train time consumed: 17.98s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.5439
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.27829370148012,
                        "time": 12.997761634000199,
                        "accuracy": 0.5439453125,
                        "total_cost": 4181685.6100769043
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4340
Profiling... [1024/50176]	Loss: 1.4504
Profiling... [1536/50176]	Loss: 1.5152
Profiling... [2048/50176]	Loss: 1.4420
Profiling... [2560/50176]	Loss: 1.4701
Profiling... [3072/50176]	Loss: 1.4405
Profiling... [3584/50176]	Loss: 1.4384
Profiling... [4096/50176]	Loss: 1.4968
Profiling... [4608/50176]	Loss: 1.5075
Profiling... [5120/50176]	Loss: 1.4888
Profiling... [5632/50176]	Loss: 1.4184
Profiling... [6144/50176]	Loss: 1.3772
Profiling... [6656/50176]	Loss: 1.4516
Profile done
epoch 1 train time consumed: 7.20s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.5443
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.28137633743444,
                        "time": 4.533062742999391,
                        "accuracy": 0.5443359375,
                        "total_cost": 1457346.3285710278
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5350
Profiling... [1024/50176]	Loss: 1.4675
Profiling... [1536/50176]	Loss: 1.6205
Profiling... [2048/50176]	Loss: 1.4278
Profiling... [2560/50176]	Loss: 1.4725
Profiling... [3072/50176]	Loss: 1.5139
Profiling... [3584/50176]	Loss: 1.5316
Profiling... [4096/50176]	Loss: 1.4430
Profiling... [4608/50176]	Loss: 1.4757
Profiling... [5120/50176]	Loss: 1.2987
Profiling... [5632/50176]	Loss: 1.4913
Profiling... [6144/50176]	Loss: 1.4667
Profiling... [6656/50176]	Loss: 1.4761
Profile done
epoch 1 train time consumed: 7.07s
Validation Epoch: 10, Average loss: 0.0031, Accuracy: 0.5462
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.30806968629183,
                        "time": 4.666408770999624,
                        "accuracy": 0.54619140625,
                        "total_cost": 1495119.7063528209
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.3938
Profiling... [1024/50176]	Loss: 1.3760
Profiling... [1536/50176]	Loss: 1.5949
Profiling... [2048/50176]	Loss: 1.4492
Profiling... [2560/50176]	Loss: 1.5070
Profiling... [3072/50176]	Loss: 1.5248
Profiling... [3584/50176]	Loss: 1.4523
Profiling... [4096/50176]	Loss: 1.4646
Profiling... [4608/50176]	Loss: 1.4320
Profiling... [5120/50176]	Loss: 1.3389
Profiling... [5632/50176]	Loss: 1.4370
Profiling... [6144/50176]	Loss: 1.4445
Profiling... [6656/50176]	Loss: 1.3915
Profile done
epoch 1 train time consumed: 8.04s
Validation Epoch: 10, Average loss: 0.0033, Accuracy: 0.5356
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.31508600305149,
                        "time": 5.373900038001011,
                        "accuracy": 0.53564453125,
                        "total_cost": 1755702.6195255811
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.3878
Profiling... [1024/50176]	Loss: 1.3930
Profiling... [1536/50176]	Loss: 1.4878
Profiling... [2048/50176]	Loss: 1.5159
Profiling... [2560/50176]	Loss: 1.4475
Profiling... [3072/50176]	Loss: 1.4858
Profiling... [3584/50176]	Loss: 1.5307
Profiling... [4096/50176]	Loss: 1.4186
Profiling... [4608/50176]	Loss: 1.5243
Profiling... [5120/50176]	Loss: 1.3497
Profiling... [5632/50176]	Loss: 1.5160
Profiling... [6144/50176]	Loss: 1.3193
Profiling... [6656/50176]	Loss: 1.5297
Profile done
epoch 1 train time consumed: 18.01s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.5439
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.27270443575424,
                        "time": 13.011012730999937,
                        "accuracy": 0.5439453125,
                        "total_cost": 4185948.799632296
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4223
Profiling... [1024/50176]	Loss: 1.3770
Profiling... [1536/50176]	Loss: 1.4661
Profiling... [2048/50176]	Loss: 1.4429
Profiling... [2560/50176]	Loss: 1.4494
Profiling... [3072/50176]	Loss: 1.5257
Profiling... [3584/50176]	Loss: 1.4585
Profiling... [4096/50176]	Loss: 1.5126
Profiling... [4608/50176]	Loss: 1.5213
Profiling... [5120/50176]	Loss: 1.4891
Profiling... [5632/50176]	Loss: 1.3856
Profiling... [6144/50176]	Loss: 1.4546
Profiling... [6656/50176]	Loss: 1.4674
Profile done
epoch 1 train time consumed: 7.15s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.5430
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.27685104971054,
                        "time": 4.524855607998688,
                        "accuracy": 0.54296875,
                        "total_cost": 1458370.7283333903
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5076
Profiling... [1024/50176]	Loss: 1.5098
Profiling... [1536/50176]	Loss: 1.4786
Profiling... [2048/50176]	Loss: 1.6375
Profiling... [2560/50176]	Loss: 1.5245
Profiling... [3072/50176]	Loss: 1.5540
Profiling... [3584/50176]	Loss: 1.4714
Profiling... [4096/50176]	Loss: 1.4196
Profiling... [4608/50176]	Loss: 1.4677
Profiling... [5120/50176]	Loss: 1.2856
Profiling... [5632/50176]	Loss: 1.5872
Profiling... [6144/50176]	Loss: 1.3702
Profiling... [6656/50176]	Loss: 1.4576
Profile done
epoch 1 train time consumed: 7.20s
Validation Epoch: 10, Average loss: 0.0031, Accuracy: 0.5443
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.30090030568628,
                        "time": 4.670898697000666,
                        "accuracy": 0.5443359375,
                        "total_cost": 1501659.5739191235
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7314
Profiling... [1024/50176]	Loss: 1.4746
Profiling... [1536/50176]	Loss: 1.5160
Profiling... [2048/50176]	Loss: 1.5248
Profiling... [2560/50176]	Loss: 1.4745
Profiling... [3072/50176]	Loss: 1.4992
Profiling... [3584/50176]	Loss: 1.2841
Profiling... [4096/50176]	Loss: 1.3557
Profiling... [4608/50176]	Loss: 1.3317
Profiling... [5120/50176]	Loss: 1.2937
Profiling... [5632/50176]	Loss: 1.4733
Profiling... [6144/50176]	Loss: 1.3149
Profiling... [6656/50176]	Loss: 1.4865
Profile done
epoch 1 train time consumed: 8.08s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.5468
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.30883223127694,
                        "time": 5.374321479999708,
                        "accuracy": 0.54677734375,
                        "total_cost": 1720090.0325342878
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4697
Profiling... [1024/50176]	Loss: 1.4041
Profiling... [1536/50176]	Loss: 1.4241
Profiling... [2048/50176]	Loss: 1.4336
Profiling... [2560/50176]	Loss: 1.5899
Profiling... [3072/50176]	Loss: 1.4094
Profiling... [3584/50176]	Loss: 1.5652
Profiling... [4096/50176]	Loss: 1.4337
Profiling... [4608/50176]	Loss: 1.4115
Profiling... [5120/50176]	Loss: 1.4692
Profiling... [5632/50176]	Loss: 1.4848
Profiling... [6144/50176]	Loss: 1.4163
Profiling... [6656/50176]	Loss: 1.3792
Profile done
epoch 1 train time consumed: 17.82s
Validation Epoch: 10, Average loss: 0.0031, Accuracy: 0.5461
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.26945351590939,
                        "time": 12.994575644999713,
                        "accuracy": 0.54609375,
                        "total_cost": 4164213.0822316674
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.5359
Profiling... [1024/50176]	Loss: 1.3667
Profiling... [1536/50176]	Loss: 1.3388
Profiling... [2048/50176]	Loss: 1.4667
Profiling... [2560/50176]	Loss: 1.4465
Profiling... [3072/50176]	Loss: 1.4436
Profiling... [3584/50176]	Loss: 1.3986
Profiling... [4096/50176]	Loss: 1.5743
Profiling... [4608/50176]	Loss: 1.5147
Profiling... [5120/50176]	Loss: 1.5164
Profiling... [5632/50176]	Loss: 1.5108
Profiling... [6144/50176]	Loss: 1.5613
Profiling... [6656/50176]	Loss: 1.3513
Profile done
epoch 1 train time consumed: 7.19s
Validation Epoch: 10, Average loss: 0.0035, Accuracy: 0.5103
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.27311567680803,
                        "time": 4.543144723000296,
                        "accuracy": 0.51025390625,
                        "total_cost": 1558146.477247183
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5091
Profiling... [1024/50176]	Loss: 1.3605
Profiling... [1536/50176]	Loss: 1.3988
Profiling... [2048/50176]	Loss: 1.4690
Profiling... [2560/50176]	Loss: 1.5225
Profiling... [3072/50176]	Loss: 1.5086
Profiling... [3584/50176]	Loss: 1.4665
Profiling... [4096/50176]	Loss: 1.4152
Profiling... [4608/50176]	Loss: 1.5150
Profiling... [5120/50176]	Loss: 1.4502
Profiling... [5632/50176]	Loss: 1.4543
Profiling... [6144/50176]	Loss: 1.4330
Profiling... [6656/50176]	Loss: 1.5033
Profile done
epoch 1 train time consumed: 7.29s
Validation Epoch: 10, Average loss: 0.0037, Accuracy: 0.4934
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.29759002509044,
                        "time": 4.6658076099993195,
                        "accuracy": 0.493359375,
                        "total_cost": 1655013.3090100517
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4937
Profiling... [1024/50176]	Loss: 1.5919
Profiling... [1536/50176]	Loss: 1.4687
Profiling... [2048/50176]	Loss: 1.5130
Profiling... [2560/50176]	Loss: 1.3949
Profiling... [3072/50176]	Loss: 1.3798
Profiling... [3584/50176]	Loss: 1.7092
Profiling... [4096/50176]	Loss: 1.5196
Profiling... [4608/50176]	Loss: 1.5432
Profiling... [5120/50176]	Loss: 1.4433
Profiling... [5632/50176]	Loss: 1.4083
Profiling... [6144/50176]	Loss: 1.4708
Profiling... [6656/50176]	Loss: 1.5633
Profile done
epoch 1 train time consumed: 8.11s
Validation Epoch: 10, Average loss: 0.0043, Accuracy: 0.4416
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.3040879132611,
                        "time": 5.36858329000097,
                        "accuracy": 0.4416015625,
                        "total_cost": 2127488.1149229854
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4824
Profiling... [1024/50176]	Loss: 1.5283
Profiling... [1536/50176]	Loss: 1.3933
Profiling... [2048/50176]	Loss: 1.5680
Profiling... [2560/50176]	Loss: 1.4573
Profiling... [3072/50176]	Loss: 1.4927
Profiling... [3584/50176]	Loss: 1.4179
Profiling... [4096/50176]	Loss: 1.4444
Profiling... [4608/50176]	Loss: 1.4237
Profiling... [5120/50176]	Loss: 1.5589
Profiling... [5632/50176]	Loss: 1.4601
Profiling... [6144/50176]	Loss: 1.5512
Profiling... [6656/50176]	Loss: 1.4642
Profile done
epoch 1 train time consumed: 17.97s
Validation Epoch: 10, Average loss: 0.0034, Accuracy: 0.5172
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.26522526420922,
                        "time": 13.038189316999706,
                        "accuracy": 0.5171875,
                        "total_cost": 4411713.605752165
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.5272
Profiling... [1024/50176]	Loss: 1.5031
Profiling... [1536/50176]	Loss: 1.3837
Profiling... [2048/50176]	Loss: 1.6098
Profiling... [2560/50176]	Loss: 1.5596
Profiling... [3072/50176]	Loss: 1.4392
Profiling... [3584/50176]	Loss: 1.4331
Profiling... [4096/50176]	Loss: 1.4736
Profiling... [4608/50176]	Loss: 1.4680
Profiling... [5120/50176]	Loss: 1.4657
Profiling... [5632/50176]	Loss: 1.4770
Profiling... [6144/50176]	Loss: 1.5198
Profiling... [6656/50176]	Loss: 1.5483
Profile done
epoch 1 train time consumed: 7.18s
Validation Epoch: 10, Average loss: 0.0038, Accuracy: 0.4786
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.26778816400869,
                        "time": 4.554900103999898,
                        "accuracy": 0.47861328125,
                        "total_cost": 1665452.1498404036
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4635
Profiling... [1024/50176]	Loss: 1.3818
Profiling... [1536/50176]	Loss: 1.4985
Profiling... [2048/50176]	Loss: 1.4303
Profiling... [2560/50176]	Loss: 1.5171
Profiling... [3072/50176]	Loss: 1.3633
Profiling... [3584/50176]	Loss: 1.5100
Profiling... [4096/50176]	Loss: 1.4045
Profiling... [4608/50176]	Loss: 1.5189
Profiling... [5120/50176]	Loss: 1.4127
Profiling... [5632/50176]	Loss: 1.4682
Profiling... [6144/50176]	Loss: 1.5963
Profiling... [6656/50176]	Loss: 1.4329
Profile done
epoch 1 train time consumed: 7.24s
Validation Epoch: 10, Average loss: 0.0034, Accuracy: 0.5211
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.29273387299827,
                        "time": 4.667521827999735,
                        "accuracy": 0.52109375,
                        "total_cost": 1567503.5824167025
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4512
Profiling... [1024/50176]	Loss: 1.4135
Profiling... [1536/50176]	Loss: 1.4310
Profiling... [2048/50176]	Loss: 1.3827
Profiling... [2560/50176]	Loss: 1.4457
Profiling... [3072/50176]	Loss: 1.4978
Profiling... [3584/50176]	Loss: 1.4048
Profiling... [4096/50176]	Loss: 1.5165
Profiling... [4608/50176]	Loss: 1.3789
Profiling... [5120/50176]	Loss: 1.5241
Profiling... [5632/50176]	Loss: 1.4383
Profiling... [6144/50176]	Loss: 1.5139
Profiling... [6656/50176]	Loss: 1.5272
Profile done
epoch 1 train time consumed: 8.02s
Validation Epoch: 10, Average loss: 0.0039, Accuracy: 0.4800
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.29936355716897,
                        "time": 5.380813032999868,
                        "accuracy": 0.47998046875,
                        "total_cost": 1961834.578867907
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.3701
Profiling... [1024/50176]	Loss: 1.3406
Profiling... [1536/50176]	Loss: 1.4255
Profiling... [2048/50176]	Loss: 1.3567
Profiling... [2560/50176]	Loss: 1.3721
Profiling... [3072/50176]	Loss: 1.5382
Profiling... [3584/50176]	Loss: 1.5212
Profiling... [4096/50176]	Loss: 1.4424
Profiling... [4608/50176]	Loss: 1.4962
Profiling... [5120/50176]	Loss: 1.5582
Profiling... [5632/50176]	Loss: 1.4588
Profiling... [6144/50176]	Loss: 1.5207
Profiling... [6656/50176]	Loss: 1.4974
Profile done
epoch 1 train time consumed: 17.89s
Validation Epoch: 10, Average loss: 0.0036, Accuracy: 0.4957
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.25932729728568,
                        "time": 13.003590455999074,
                        "accuracy": 0.495703125,
                        "total_cost": 4590708.056964212
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4914
Profiling... [1024/50176]	Loss: 1.5241
Profiling... [1536/50176]	Loss: 1.4788
Profiling... [2048/50176]	Loss: 1.4166
Profiling... [2560/50176]	Loss: 1.4809
Profiling... [3072/50176]	Loss: 1.4376
Profiling... [3584/50176]	Loss: 1.4579
Profiling... [4096/50176]	Loss: 1.4175
Profiling... [4608/50176]	Loss: 1.3874
Profiling... [5120/50176]	Loss: 1.4515
Profiling... [5632/50176]	Loss: 1.5283
Profiling... [6144/50176]	Loss: 1.5132
Profiling... [6656/50176]	Loss: 1.5528
Profile done
epoch 1 train time consumed: 7.26s
Validation Epoch: 10, Average loss: 0.0040, Accuracy: 0.4721
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.26330510234907,
                        "time": 4.556522628001403,
                        "accuracy": 0.4720703125,
                        "total_cost": 1689137.060276896
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4986
Profiling... [1024/50176]	Loss: 1.5103
Profiling... [1536/50176]	Loss: 1.4978
Profiling... [2048/50176]	Loss: 1.4392
Profiling... [2560/50176]	Loss: 1.4639
Profiling... [3072/50176]	Loss: 1.4832
Profiling... [3584/50176]	Loss: 1.5017
Profiling... [4096/50176]	Loss: 1.4805
Profiling... [4608/50176]	Loss: 1.5175
Profiling... [5120/50176]	Loss: 1.5214
Profiling... [5632/50176]	Loss: 1.3754
Profiling... [6144/50176]	Loss: 1.4021
Profiling... [6656/50176]	Loss: 1.5777
Profile done
epoch 1 train time consumed: 7.22s
Validation Epoch: 10, Average loss: 0.0039, Accuracy: 0.4738
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.2896184132072,
                        "time": 4.677793095999732,
                        "accuracy": 0.473828125,
                        "total_cost": 1727659.774944666
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4636
Profiling... [1024/50176]	Loss: 1.5651
Profiling... [1536/50176]	Loss: 1.5498
Profiling... [2048/50176]	Loss: 1.5145
Profiling... [2560/50176]	Loss: 1.5512
Profiling... [3072/50176]	Loss: 1.4479
Profiling... [3584/50176]	Loss: 1.5359
Profiling... [4096/50176]	Loss: 1.3725
Profiling... [4608/50176]	Loss: 1.3236
Profiling... [5120/50176]	Loss: 1.4799
Profiling... [5632/50176]	Loss: 1.4312
Profiling... [6144/50176]	Loss: 1.4687
Profiling... [6656/50176]	Loss: 1.4630
Profile done
epoch 1 train time consumed: 8.04s
Validation Epoch: 10, Average loss: 0.0039, Accuracy: 0.4663
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.29654910980467,
                        "time": 5.378017450999323,
                        "accuracy": 0.46630859375,
                        "total_cost": 2018305.1878933585
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.3516
Profiling... [1024/50176]	Loss: 1.4785
Profiling... [1536/50176]	Loss: 1.3974
Profiling... [2048/50176]	Loss: 1.4767
Profiling... [2560/50176]	Loss: 1.4523
Profiling... [3072/50176]	Loss: 1.4468
Profiling... [3584/50176]	Loss: 1.5199
Profiling... [4096/50176]	Loss: 1.5039
Profiling... [4608/50176]	Loss: 1.3680
Profiling... [5120/50176]	Loss: 1.4129
Profiling... [5632/50176]	Loss: 1.6202
Profiling... [6144/50176]	Loss: 1.4999
Profiling... [6656/50176]	Loss: 1.5341
Profile done
epoch 1 train time consumed: 17.93s
Validation Epoch: 10, Average loss: 0.0039, Accuracy: 0.4702
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.25668524153164,
                        "time": 13.007660155000849,
                        "accuracy": 0.47021484375,
                        "total_cost": 4841064.797042891
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4960
Profiling... [1024/50176]	Loss: 1.4986
Profiling... [1536/50176]	Loss: 1.5828
Profiling... [2048/50176]	Loss: 1.6632
Profiling... [2560/50176]	Loss: 1.4725
Profiling... [3072/50176]	Loss: 1.6615
Profiling... [3584/50176]	Loss: 1.4080
Profiling... [4096/50176]	Loss: 1.4937
Profiling... [4608/50176]	Loss: 1.5675
Profiling... [5120/50176]	Loss: 1.4716
Profiling... [5632/50176]	Loss: 1.5775
Profiling... [6144/50176]	Loss: 1.5756
Profiling... [6656/50176]	Loss: 1.6489
Profile done
epoch 1 train time consumed: 7.31s
Validation Epoch: 10, Average loss: 0.0048, Accuracy: 0.3797
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.26026131581082,
                        "time": 4.530841448999126,
                        "accuracy": 0.3796875,
                        "total_cost": 2088289.0629131775
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5479
Profiling... [1024/50176]	Loss: 1.4456
Profiling... [1536/50176]	Loss: 1.3915
Profiling... [2048/50176]	Loss: 1.5708
Profiling... [2560/50176]	Loss: 1.7666
Profiling... [3072/50176]	Loss: 1.4957
Profiling... [3584/50176]	Loss: 1.5727
Profiling... [4096/50176]	Loss: 1.5352
Profiling... [4608/50176]	Loss: 1.4890
Profiling... [5120/50176]	Loss: 1.4737
Profiling... [5632/50176]	Loss: 1.5087
Profiling... [6144/50176]	Loss: 1.5722
Profiling... [6656/50176]	Loss: 1.5454
Profile done
epoch 1 train time consumed: 7.29s
Validation Epoch: 10, Average loss: 0.0069, Accuracy: 0.2793
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.28512968259567,
                        "time": 4.6695830970002135,
                        "accuracy": 0.279296875,
                        "total_cost": 2925836.6817567768
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4032
Profiling... [1024/50176]	Loss: 1.6605
Profiling... [1536/50176]	Loss: 1.5577
Profiling... [2048/50176]	Loss: 1.4607
Profiling... [2560/50176]	Loss: 1.5663
Profiling... [3072/50176]	Loss: 1.6492
Profiling... [3584/50176]	Loss: 1.4692
Profiling... [4096/50176]	Loss: 1.5010
Profiling... [4608/50176]	Loss: 1.6229
Profiling... [5120/50176]	Loss: 1.6396
Profiling... [5632/50176]	Loss: 1.5672
Profiling... [6144/50176]	Loss: 1.6022
Profiling... [6656/50176]	Loss: 1.4373
Profile done
epoch 1 train time consumed: 8.08s
Validation Epoch: 10, Average loss: 0.0080, Accuracy: 0.2324
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.29200280000053,
                        "time": 5.375933912999244,
                        "accuracy": 0.232421875,
                        "total_cost": 4047762.005081784
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4412
Profiling... [1024/50176]	Loss: 1.5315
Profiling... [1536/50176]	Loss: 1.4145
Profiling... [2048/50176]	Loss: 1.6122
Profiling... [2560/50176]	Loss: 1.5941
Profiling... [3072/50176]	Loss: 1.6178
Profiling... [3584/50176]	Loss: 1.4331
Profiling... [4096/50176]	Loss: 1.5288
Profiling... [4608/50176]	Loss: 1.5815
Profiling... [5120/50176]	Loss: 1.6284
Profiling... [5632/50176]	Loss: 1.5370
Profiling... [6144/50176]	Loss: 1.7256
Profiling... [6656/50176]	Loss: 1.4841
Profile done
epoch 1 train time consumed: 17.98s
Validation Epoch: 10, Average loss: 0.0091, Accuracy: 0.2213
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.25084678726216,
                        "time": 13.029067593000946,
                        "accuracy": 0.2212890625,
                        "total_cost": 10303658.04353826
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4115
Profiling... [1024/50176]	Loss: 1.5576
Profiling... [1536/50176]	Loss: 1.5828
Profiling... [2048/50176]	Loss: 1.5043
Profiling... [2560/50176]	Loss: 1.4336
Profiling... [3072/50176]	Loss: 1.5404
Profiling... [3584/50176]	Loss: 1.5068
Profiling... [4096/50176]	Loss: 1.5778
Profiling... [4608/50176]	Loss: 1.4879
Profiling... [5120/50176]	Loss: 1.5926
Profiling... [5632/50176]	Loss: 1.5153
Profiling... [6144/50176]	Loss: 1.5755
Profiling... [6656/50176]	Loss: 1.4698
Profile done
epoch 1 train time consumed: 7.17s
Validation Epoch: 10, Average loss: 0.0053, Accuracy: 0.3654
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.25532169197349,
                        "time": 4.591187779000393,
                        "accuracy": 0.3654296875,
                        "total_cost": 2198666.087645298
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4811
Profiling... [1024/50176]	Loss: 1.4729
Profiling... [1536/50176]	Loss: 1.6103
Profiling... [2048/50176]	Loss: 1.5990
Profiling... [2560/50176]	Loss: 1.5798
Profiling... [3072/50176]	Loss: 1.5112
Profiling... [3584/50176]	Loss: 1.5502
Profiling... [4096/50176]	Loss: 1.5350
Profiling... [4608/50176]	Loss: 1.4730
Profiling... [5120/50176]	Loss: 1.6737
Profiling... [5632/50176]	Loss: 1.5711
Profiling... [6144/50176]	Loss: 1.7203
Profiling... [6656/50176]	Loss: 1.5126
Profile done
epoch 1 train time consumed: 7.26s
Validation Epoch: 10, Average loss: 0.0048, Accuracy: 0.3850
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.28128792924277,
                        "time": 4.707740175999788,
                        "accuracy": 0.3849609375,
                        "total_cost": 2140099.0348532777
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5606
Profiling... [1024/50176]	Loss: 1.5314
Profiling... [1536/50176]	Loss: 1.6164
Profiling... [2048/50176]	Loss: 1.4887
Profiling... [2560/50176]	Loss: 1.5597
Profiling... [3072/50176]	Loss: 1.4523
Profiling... [3584/50176]	Loss: 1.6962
Profiling... [4096/50176]	Loss: 1.4616
Profiling... [4608/50176]	Loss: 1.5619
Profiling... [5120/50176]	Loss: 1.5451
Profiling... [5632/50176]	Loss: 1.4678
Profiling... [6144/50176]	Loss: 1.6365
Profiling... [6656/50176]	Loss: 1.5804
Profile done
epoch 1 train time consumed: 8.16s
Validation Epoch: 10, Average loss: 0.0050, Accuracy: 0.3690
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.28868662680499,
                        "time": 5.455291366999518,
                        "accuracy": 0.36904296875,
                        "total_cost": 2586896.5677859583
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5511
Profiling... [1024/50176]	Loss: 1.4583
Profiling... [1536/50176]	Loss: 1.6055
Profiling... [2048/50176]	Loss: 1.6299
Profiling... [2560/50176]	Loss: 1.7189
Profiling... [3072/50176]	Loss: 1.5875
Profiling... [3584/50176]	Loss: 1.6888
Profiling... [4096/50176]	Loss: 1.5981
Profiling... [4608/50176]	Loss: 1.6233
Profiling... [5120/50176]	Loss: 1.5554
Profiling... [5632/50176]	Loss: 1.3528
Profiling... [6144/50176]	Loss: 1.5875
Profiling... [6656/50176]	Loss: 1.5140
Profile done
epoch 1 train time consumed: 18.07s
Validation Epoch: 10, Average loss: 0.0060, Accuracy: 0.3112
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.24811759822381,
                        "time": 13.022753617999115,
                        "accuracy": 0.31123046875,
                        "total_cost": 7322489.640242992
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4079
Profiling... [1024/50176]	Loss: 1.4502
Profiling... [1536/50176]	Loss: 1.4997
Profiling... [2048/50176]	Loss: 1.6234
Profiling... [2560/50176]	Loss: 1.5167
Profiling... [3072/50176]	Loss: 1.4512
Profiling... [3584/50176]	Loss: 1.5056
Profiling... [4096/50176]	Loss: 1.6730
Profiling... [4608/50176]	Loss: 1.5939
Profiling... [5120/50176]	Loss: 1.4647
Profiling... [5632/50176]	Loss: 1.5242
Profiling... [6144/50176]	Loss: 1.4664
Profiling... [6656/50176]	Loss: 1.5354
Profile done
epoch 1 train time consumed: 7.34s
Validation Epoch: 10, Average loss: 0.0077, Accuracy: 0.2568
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.25279895653419,
                        "time": 4.624194857000475,
                        "accuracy": 0.2568359375,
                        "total_cost": 3150782.199142529
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4432
Profiling... [1024/50176]	Loss: 1.3176
Profiling... [1536/50176]	Loss: 1.5877
Profiling... [2048/50176]	Loss: 1.5835
Profiling... [2560/50176]	Loss: 1.4683
Profiling... [3072/50176]	Loss: 1.6812
Profiling... [3584/50176]	Loss: 1.6049
Profiling... [4096/50176]	Loss: 1.6042
Profiling... [4608/50176]	Loss: 1.4719
Profiling... [5120/50176]	Loss: 1.6103
Profiling... [5632/50176]	Loss: 1.6867
Profiling... [6144/50176]	Loss: 1.5551
Profiling... [6656/50176]	Loss: 1.6176
Profile done
epoch 1 train time consumed: 7.44s
Validation Epoch: 10, Average loss: 0.0044, Accuracy: 0.4227
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.27690034623141,
                        "time": 4.747022672998355,
                        "accuracy": 0.42265625,
                        "total_cost": 1965495.5244946978
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4268
Profiling... [1024/50176]	Loss: 1.4369
Profiling... [1536/50176]	Loss: 1.5262
Profiling... [2048/50176]	Loss: 1.5175
Profiling... [2560/50176]	Loss: 1.6100
Profiling... [3072/50176]	Loss: 1.6371
Profiling... [3584/50176]	Loss: 1.4826
Profiling... [4096/50176]	Loss: 1.6065
Profiling... [4608/50176]	Loss: 1.5519
Profiling... [5120/50176]	Loss: 1.6946
Profiling... [5632/50176]	Loss: 1.6751
Profiling... [6144/50176]	Loss: 1.4349
Profiling... [6656/50176]	Loss: 1.4798
Profile done
epoch 1 train time consumed: 8.23s
Validation Epoch: 10, Average loss: 0.0073, Accuracy: 0.2637
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.28422914812413,
                        "time": 5.429479357999298,
                        "accuracy": 0.263671875,
                        "total_cost": 3603565.5590869416
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5298
Profiling... [1024/50176]	Loss: 1.3536
Profiling... [1536/50176]	Loss: 1.6734
Profiling... [2048/50176]	Loss: 1.5685
Profiling... [2560/50176]	Loss: 1.5971
Profiling... [3072/50176]	Loss: 1.5062
Profiling... [3584/50176]	Loss: 1.6388
Profiling... [4096/50176]	Loss: 1.6549
Profiling... [4608/50176]	Loss: 1.6507
Profiling... [5120/50176]	Loss: 1.4993
Profiling... [5632/50176]	Loss: 1.4957
Profiling... [6144/50176]	Loss: 1.6021
Profiling... [6656/50176]	Loss: 1.6011
Profile done
epoch 1 train time consumed: 18.03s
Validation Epoch: 10, Average loss: 0.0078, Accuracy: 0.2342
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.24591319685113,
                        "time": 13.02535618300135,
                        "accuracy": 0.2341796875,
                        "total_cost": 9733710.708898421
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4244
Profiling... [2048/50176]	Loss: 1.5124
Profiling... [3072/50176]	Loss: 1.4652
Profiling... [4096/50176]	Loss: 1.5024
Profiling... [5120/50176]	Loss: 1.4125
Profiling... [6144/50176]	Loss: 1.3890
Profiling... [7168/50176]	Loss: 1.4050
Profiling... [8192/50176]	Loss: 1.3405
Profiling... [9216/50176]	Loss: 1.4204
Profiling... [10240/50176]	Loss: 1.3848
Profiling... [11264/50176]	Loss: 1.3922
Profiling... [12288/50176]	Loss: 1.4840
Profiling... [13312/50176]	Loss: 1.4160
Profile done
epoch 1 train time consumed: 13.19s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5475
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.26456292725875,
                        "time": 8.937736022000536,
                        "accuracy": 0.5474609375,
                        "total_cost": 2857014.4401400215
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.4874
Profiling... [2048/50176]	Loss: 1.4628
Profiling... [3072/50176]	Loss: 1.5158
Profiling... [4096/50176]	Loss: 1.3437
Profiling... [5120/50176]	Loss: 1.4888
Profiling... [6144/50176]	Loss: 1.4808
Profiling... [7168/50176]	Loss: 1.5071
Profiling... [8192/50176]	Loss: 1.4192
Profiling... [9216/50176]	Loss: 1.3868
Profiling... [10240/50176]	Loss: 1.4029
Profiling... [11264/50176]	Loss: 1.3506
Profiling... [12288/50176]	Loss: 1.4077
Profiling... [13312/50176]	Loss: 1.3912
Profile done
epoch 1 train time consumed: 13.52s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5437
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.30392201952405,
                        "time": 9.163357622999683,
                        "accuracy": 0.54365234375,
                        "total_cost": 2949656.342808592
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4924
Profiling... [2048/50176]	Loss: 1.3410
Profiling... [3072/50176]	Loss: 1.4066
Profiling... [4096/50176]	Loss: 1.4920
Profiling... [5120/50176]	Loss: 1.4461
Profiling... [6144/50176]	Loss: 1.4173
Profiling... [7168/50176]	Loss: 1.4470
Profiling... [8192/50176]	Loss: 1.3736
Profiling... [9216/50176]	Loss: 1.4486
Profiling... [10240/50176]	Loss: 1.3756
Profiling... [11264/50176]	Loss: 1.3856
Profiling... [12288/50176]	Loss: 1.4695
Profiling... [13312/50176]	Loss: 1.3624
Profile done
epoch 1 train time consumed: 15.32s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5467
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.31114396647436,
                        "time": 10.62269528699835,
                        "accuracy": 0.5466796875,
                        "total_cost": 3400476.9478922915
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5287
Profiling... [2048/50176]	Loss: 1.4652
Profiling... [3072/50176]	Loss: 1.4706
Profiling... [4096/50176]	Loss: 1.4645
Profiling... [5120/50176]	Loss: 1.4969
Profiling... [6144/50176]	Loss: 1.4823
Profiling... [7168/50176]	Loss: 1.4809
Profiling... [8192/50176]	Loss: 1.4372
Profiling... [9216/50176]	Loss: 1.4659
Profiling... [10240/50176]	Loss: 1.4544
Profiling... [11264/50176]	Loss: 1.5047
Profiling... [12288/50176]	Loss: 1.4767
Profiling... [13312/50176]	Loss: 1.4008
Profile done
epoch 1 train time consumed: 37.64s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5422
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.2322677924346,
                        "time": 27.59026628799984,
                        "accuracy": 0.5421875,
                        "total_cost": 8905215.63186162
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5493
Profiling... [2048/50176]	Loss: 1.4478
Profiling... [3072/50176]	Loss: 1.4699
Profiling... [4096/50176]	Loss: 1.4250
Profiling... [5120/50176]	Loss: 1.4085
Profiling... [6144/50176]	Loss: 1.4400
Profiling... [7168/50176]	Loss: 1.3659
Profiling... [8192/50176]	Loss: 1.4327
Profiling... [9216/50176]	Loss: 1.4628
Profiling... [10240/50176]	Loss: 1.4274
Profiling... [11264/50176]	Loss: 1.4188
Profiling... [12288/50176]	Loss: 1.5047
Profiling... [13312/50176]	Loss: 1.3671
Profile done
epoch 1 train time consumed: 13.07s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5420
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.26105938501168,
                        "time": 8.858072327999253,
                        "accuracy": 0.5419921875,
                        "total_cost": 2860119.930049489
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.3875
Profiling... [2048/50176]	Loss: 1.5302
Profiling... [3072/50176]	Loss: 1.4642
Profiling... [4096/50176]	Loss: 1.5064
Profiling... [5120/50176]	Loss: 1.3976
Profiling... [6144/50176]	Loss: 1.5437
Profiling... [7168/50176]	Loss: 1.3719
Profiling... [8192/50176]	Loss: 1.4777
Profiling... [9216/50176]	Loss: 1.4724
Profiling... [10240/50176]	Loss: 1.4534
Profiling... [11264/50176]	Loss: 1.3911
Profiling... [12288/50176]	Loss: 1.3976
Profiling... [13312/50176]	Loss: 1.4807
Profile done
epoch 1 train time consumed: 13.33s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5445
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.29966990220235,
                        "time": 9.110524296000222,
                        "accuracy": 0.54453125,
                        "total_cost": 2927915.9860890238
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4936
Profiling... [2048/50176]	Loss: 1.4984
Profiling... [3072/50176]	Loss: 1.5222
Profiling... [4096/50176]	Loss: 1.5334
Profiling... [5120/50176]	Loss: 1.5121
Profiling... [6144/50176]	Loss: 1.3877
Profiling... [7168/50176]	Loss: 1.3988
Profiling... [8192/50176]	Loss: 1.4469
Profiling... [9216/50176]	Loss: 1.5583
Profiling... [10240/50176]	Loss: 1.4507
Profiling... [11264/50176]	Loss: 1.5176
Profiling... [12288/50176]	Loss: 1.3700
Profiling... [13312/50176]	Loss: 1.4570
Profile done
epoch 1 train time consumed: 15.14s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5450
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.30597089980537,
                        "time": 10.591674822999266,
                        "accuracy": 0.54501953125,
                        "total_cost": 3400874.625123577
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.4921
Profiling... [2048/50176]	Loss: 1.5134
Profiling... [3072/50176]	Loss: 1.4613
Profiling... [4096/50176]	Loss: 1.4791
Profiling... [5120/50176]	Loss: 1.4240
Profiling... [6144/50176]	Loss: 1.4628
Profiling... [7168/50176]	Loss: 1.5371
Profiling... [8192/50176]	Loss: 1.4308
Profiling... [9216/50176]	Loss: 1.3987
Profiling... [10240/50176]	Loss: 1.4369
Profiling... [11264/50176]	Loss: 1.5035
Profiling... [12288/50176]	Loss: 1.3600
Profiling... [13312/50176]	Loss: 1.4297
Profile done
epoch 1 train time consumed: 37.69s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5469
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.22693230610432,
                        "time": 27.659304742999666,
                        "accuracy": 0.546875,
                        "total_cost": 8850977.517759893
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4591
Profiling... [2048/50176]	Loss: 1.5349
Profiling... [3072/50176]	Loss: 1.4231
Profiling... [4096/50176]	Loss: 1.4100
Profiling... [5120/50176]	Loss: 1.4613
Profiling... [6144/50176]	Loss: 1.3862
Profiling... [7168/50176]	Loss: 1.4676
Profiling... [8192/50176]	Loss: 1.4986
Profiling... [9216/50176]	Loss: 1.3923
Profiling... [10240/50176]	Loss: 1.4021
Profiling... [11264/50176]	Loss: 1.3611
Profiling... [12288/50176]	Loss: 1.3695
Profiling... [13312/50176]	Loss: 1.3939
Profile done
epoch 1 train time consumed: 13.12s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5439
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.25560730961234,
                        "time": 8.874309593000362,
                        "accuracy": 0.5439453125,
                        "total_cost": 2855074.1096331505
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.4498
Profiling... [2048/50176]	Loss: 1.4279
Profiling... [3072/50176]	Loss: 1.5357
Profiling... [4096/50176]	Loss: 1.4971
Profiling... [5120/50176]	Loss: 1.4537
Profiling... [6144/50176]	Loss: 1.3449
Profiling... [7168/50176]	Loss: 1.4764
Profiling... [8192/50176]	Loss: 1.5415
Profiling... [9216/50176]	Loss: 1.4741
Profiling... [10240/50176]	Loss: 1.3565
Profiling... [11264/50176]	Loss: 1.3875
Profiling... [12288/50176]	Loss: 1.4013
Profiling... [13312/50176]	Loss: 1.5059
Profile done
epoch 1 train time consumed: 13.47s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5494
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.29479859665328,
                        "time": 9.190354935999494,
                        "accuracy": 0.5494140625,
                        "total_cost": 2927322.4396215947
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5378
Profiling... [2048/50176]	Loss: 1.4045
Profiling... [3072/50176]	Loss: 1.4448
Profiling... [4096/50176]	Loss: 1.3959
Profiling... [5120/50176]	Loss: 1.4716
Profiling... [6144/50176]	Loss: 1.4792
Profiling... [7168/50176]	Loss: 1.4677
Profiling... [8192/50176]	Loss: 1.3114
Profiling... [9216/50176]	Loss: 1.4526
Profiling... [10240/50176]	Loss: 1.4192
Profiling... [11264/50176]	Loss: 1.3961
Profiling... [12288/50176]	Loss: 1.4212
Profiling... [13312/50176]	Loss: 1.4507
Profile done
epoch 1 train time consumed: 15.31s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5485
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.30303633268153,
                        "time": 10.65114132600138,
                        "accuracy": 0.54853515625,
                        "total_cost": 3398049.716253244
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5465
Profiling... [2048/50176]	Loss: 1.5675
Profiling... [3072/50176]	Loss: 1.4950
Profiling... [4096/50176]	Loss: 1.4080
Profiling... [5120/50176]	Loss: 1.4830
Profiling... [6144/50176]	Loss: 1.5204
Profiling... [7168/50176]	Loss: 1.4136
Profiling... [8192/50176]	Loss: 1.5341
Profiling... [9216/50176]	Loss: 1.4765
Profiling... [10240/50176]	Loss: 1.4308
Profiling... [11264/50176]	Loss: 1.4211
Profiling... [12288/50176]	Loss: 1.3849
Profiling... [13312/50176]	Loss: 1.3386
Profile done
epoch 1 train time consumed: 37.66s
Validation Epoch: 10, Average loss: 0.0016, Accuracy: 0.5473
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.22532946043928,
                        "time": 27.731239945000198,
                        "accuracy": 0.547265625,
                        "total_cost": 8867662.7375875
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4661
Profiling... [2048/50176]	Loss: 1.4191
Profiling... [3072/50176]	Loss: 1.4517
Profiling... [4096/50176]	Loss: 1.4031
Profiling... [5120/50176]	Loss: 1.3853
Profiling... [6144/50176]	Loss: 1.4689
Profiling... [7168/50176]	Loss: 1.5291
Profiling... [8192/50176]	Loss: 1.4912
Profiling... [9216/50176]	Loss: 1.3907
Profiling... [10240/50176]	Loss: 1.4255
Profiling... [11264/50176]	Loss: 1.3690
Profiling... [12288/50176]	Loss: 1.4513
Profiling... [13312/50176]	Loss: 1.4158
Profile done
epoch 1 train time consumed: 13.15s
Validation Epoch: 10, Average loss: 0.0018, Accuracy: 0.5040
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.25012621505451,
                        "time": 8.85175215900017,
                        "accuracy": 0.50400390625,
                        "total_cost": 3073501.2340492746
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5259
Profiling... [2048/50176]	Loss: 1.4397
Profiling... [3072/50176]	Loss: 1.4672
Profiling... [4096/50176]	Loss: 1.4375
Profiling... [5120/50176]	Loss: 1.4185
Profiling... [6144/50176]	Loss: 1.5006
Profiling... [7168/50176]	Loss: 1.4556
Profiling... [8192/50176]	Loss: 1.3899
Profiling... [9216/50176]	Loss: 1.4212
Profiling... [10240/50176]	Loss: 1.4084
Profiling... [11264/50176]	Loss: 1.3601
Profiling... [12288/50176]	Loss: 1.4507
Profiling... [13312/50176]	Loss: 1.5042
Profile done
epoch 1 train time consumed: 13.45s
Validation Epoch: 10, Average loss: 0.0022, Accuracy: 0.4439
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.28859755570157,
                        "time": 9.104616527998587,
                        "accuracy": 0.4439453125,
                        "total_cost": 3588973.3431969793
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5143
Profiling... [2048/50176]	Loss: 1.5060
Profiling... [3072/50176]	Loss: 1.4460
Profiling... [4096/50176]	Loss: 1.5349
Profiling... [5120/50176]	Loss: 1.4083
Profiling... [6144/50176]	Loss: 1.4987
Profiling... [7168/50176]	Loss: 1.4375
Profiling... [8192/50176]	Loss: 1.4736
Profiling... [9216/50176]	Loss: 1.4391
Profiling... [10240/50176]	Loss: 1.4279
Profiling... [11264/50176]	Loss: 1.4699
Profiling... [12288/50176]	Loss: 1.4347
Profiling... [13312/50176]	Loss: 1.4143
Profile done
epoch 1 train time consumed: 15.11s
Validation Epoch: 10, Average loss: 0.0018, Accuracy: 0.4935
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.29491823881958,
                        "time": 10.44815101200038,
                        "accuracy": 0.49345703125,
                        "total_cost": 3705340.7111626123
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.4405
Profiling... [2048/50176]	Loss: 1.3724
Profiling... [3072/50176]	Loss: 1.4350
Profiling... [4096/50176]	Loss: 1.3378
Profiling... [5120/50176]	Loss: 1.3698
Profiling... [6144/50176]	Loss: 1.4855
Profiling... [7168/50176]	Loss: 1.4725
Profiling... [8192/50176]	Loss: 1.3939
Profiling... [9216/50176]	Loss: 1.4269
Profiling... [10240/50176]	Loss: 1.4572
Profiling... [11264/50176]	Loss: 1.4017
Profiling... [12288/50176]	Loss: 1.4521
Profiling... [13312/50176]	Loss: 1.4720
Profile done
epoch 1 train time consumed: 37.03s
Validation Epoch: 10, Average loss: 0.0019, Accuracy: 0.4916
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.21632226043329,
                        "time": 27.674752311999328,
                        "accuracy": 0.4916015625,
                        "total_cost": 9851640.07610306
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5554
Profiling... [2048/50176]	Loss: 1.4790
Profiling... [3072/50176]	Loss: 1.3907
Profiling... [4096/50176]	Loss: 1.4281
Profiling... [5120/50176]	Loss: 1.4724
Profiling... [6144/50176]	Loss: 1.3951
Profiling... [7168/50176]	Loss: 1.4073
Profiling... [8192/50176]	Loss: 1.4172
Profiling... [9216/50176]	Loss: 1.3935
Profiling... [10240/50176]	Loss: 1.4014
Profiling... [11264/50176]	Loss: 1.4009
Profiling... [12288/50176]	Loss: 1.4498
Profiling... [13312/50176]	Loss: 1.4753
Profile done
epoch 1 train time consumed: 13.10s
Validation Epoch: 10, Average loss: 0.0019, Accuracy: 0.4760
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.2407836579819,
                        "time": 8.857936148999215,
                        "accuracy": 0.4759765625,
                        "total_cost": 3256754.5299562155
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5693
Profiling... [2048/50176]	Loss: 1.4718
Profiling... [3072/50176]	Loss: 1.4363
Profiling... [4096/50176]	Loss: 1.4511
Profiling... [5120/50176]	Loss: 1.4435
Profiling... [6144/50176]	Loss: 1.5000
Profiling... [7168/50176]	Loss: 1.4100
Profiling... [8192/50176]	Loss: 1.5088
Profiling... [9216/50176]	Loss: 1.4198
Profiling... [10240/50176]	Loss: 1.5172
Profiling... [11264/50176]	Loss: 1.4459
Profiling... [12288/50176]	Loss: 1.4422
Profiling... [13312/50176]	Loss: 1.4632
Profile done
epoch 1 train time consumed: 13.49s
Validation Epoch: 10, Average loss: 0.0018, Accuracy: 0.4968
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.27894375747097,
                        "time": 9.174490010000227,
                        "accuracy": 0.49677734375,
                        "total_cost": 3231902.122649972
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4928
Profiling... [2048/50176]	Loss: 1.4705
Profiling... [3072/50176]	Loss: 1.4355
Profiling... [4096/50176]	Loss: 1.4689
Profiling... [5120/50176]	Loss: 1.4141
Profiling... [6144/50176]	Loss: 1.3701
Profiling... [7168/50176]	Loss: 1.4431
Profiling... [8192/50176]	Loss: 1.4163
Profiling... [9216/50176]	Loss: 1.4909
Profiling... [10240/50176]	Loss: 1.4572
Profiling... [11264/50176]	Loss: 1.4456
Profiling... [12288/50176]	Loss: 1.4333
Profiling... [13312/50176]	Loss: 1.4415
Profile done
epoch 1 train time consumed: 15.41s
Validation Epoch: 10, Average loss: 0.0022, Accuracy: 0.4336
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.28702879346918,
                        "time": 10.64146502299991,
                        "accuracy": 0.43359375,
                        "total_cost": 4294933.630904469
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.3910
Profiling... [2048/50176]	Loss: 1.5837
Profiling... [3072/50176]	Loss: 1.4544
Profiling... [4096/50176]	Loss: 1.4891
Profiling... [5120/50176]	Loss: 1.4444
Profiling... [6144/50176]	Loss: 1.5169
Profiling... [7168/50176]	Loss: 1.4142
Profiling... [8192/50176]	Loss: 1.3424
Profiling... [9216/50176]	Loss: 1.3917
Profiling... [10240/50176]	Loss: 1.4920
Profiling... [11264/50176]	Loss: 1.4288
Profiling... [12288/50176]	Loss: 1.4300
Profiling... [13312/50176]	Loss: 1.4476
Profile done
epoch 1 train time consumed: 37.73s
Validation Epoch: 10, Average loss: 0.0018, Accuracy: 0.4937
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.21120224532935,
                        "time": 27.88423938499909,
                        "accuracy": 0.49365234375,
                        "total_cost": 9884976.652407194
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5356
Profiling... [2048/50176]	Loss: 1.4622
Profiling... [3072/50176]	Loss: 1.4171
Profiling... [4096/50176]	Loss: 1.4424
Profiling... [5120/50176]	Loss: 1.4456
Profiling... [6144/50176]	Loss: 1.4970
Profiling... [7168/50176]	Loss: 1.4808
Profiling... [8192/50176]	Loss: 1.4370
Profiling... [9216/50176]	Loss: 1.3732
Profiling... [10240/50176]	Loss: 1.4292
Profiling... [11264/50176]	Loss: 1.4208
Profiling... [12288/50176]	Loss: 1.4852
Profiling... [13312/50176]	Loss: 1.5286
Profile done
epoch 1 train time consumed: 13.12s
Validation Epoch: 10, Average loss: 0.0018, Accuracy: 0.4976
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.23819888127773,
                        "time": 8.858868422999876,
                        "accuracy": 0.49755859375,
                        "total_cost": 3115817.902652753
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5291
Profiling... [2048/50176]	Loss: 1.3810
Profiling... [3072/50176]	Loss: 1.5251
Profiling... [4096/50176]	Loss: 1.4963
Profiling... [5120/50176]	Loss: 1.4238
Profiling... [6144/50176]	Loss: 1.4494
Profiling... [7168/50176]	Loss: 1.4195
Profiling... [8192/50176]	Loss: 1.4165
Profiling... [9216/50176]	Loss: 1.4019
Profiling... [10240/50176]	Loss: 1.4690
Profiling... [11264/50176]	Loss: 1.4564
Profiling... [12288/50176]	Loss: 1.4143
Profiling... [13312/50176]	Loss: 1.4689
Profile done
epoch 1 train time consumed: 13.48s
Validation Epoch: 10, Average loss: 0.0020, Accuracy: 0.4478
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.27678618061941,
                        "time": 9.186389188000248,
                        "accuracy": 0.44775390625,
                        "total_cost": 3590405.545233685
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5049
Profiling... [2048/50176]	Loss: 1.4799
Profiling... [3072/50176]	Loss: 1.4733
Profiling... [4096/50176]	Loss: 1.4005
Profiling... [5120/50176]	Loss: 1.4275
Profiling... [6144/50176]	Loss: 1.4384
Profiling... [7168/50176]	Loss: 1.4255
Profiling... [8192/50176]	Loss: 1.4277
Profiling... [9216/50176]	Loss: 1.4345
Profiling... [10240/50176]	Loss: 1.3894
Profiling... [11264/50176]	Loss: 1.4260
Profiling... [12288/50176]	Loss: 1.4361
Profiling... [13312/50176]	Loss: 1.3988
Profile done
epoch 1 train time consumed: 15.27s
Validation Epoch: 10, Average loss: 0.0017, Accuracy: 0.5086
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.28466207123316,
                        "time": 10.641213247999985,
                        "accuracy": 0.50859375,
                        "total_cost": 3661492.730494619
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5038
Profiling... [2048/50176]	Loss: 1.4883
Profiling... [3072/50176]	Loss: 1.4390
Profiling... [4096/50176]	Loss: 1.5070
Profiling... [5120/50176]	Loss: 1.4631
Profiling... [6144/50176]	Loss: 1.4065
Profiling... [7168/50176]	Loss: 1.4606
Profiling... [8192/50176]	Loss: 1.3775
Profiling... [9216/50176]	Loss: 1.4453
Profiling... [10240/50176]	Loss: 1.4976
Profiling... [11264/50176]	Loss: 1.4289
Profiling... [12288/50176]	Loss: 1.5382
Profiling... [13312/50176]	Loss: 1.3944
Profile done
epoch 1 train time consumed: 38.26s
Validation Epoch: 10, Average loss: 0.0019, Accuracy: 0.4629
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.20776005020758,
                        "time": 28.171380489000512,
                        "accuracy": 0.462890625,
                        "total_cost": 10650445.957022978
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4882
Profiling... [2048/50176]	Loss: 1.4909
Profiling... [3072/50176]	Loss: 1.5496
Profiling... [4096/50176]	Loss: 1.5103
Profiling... [5120/50176]	Loss: 1.4258
Profiling... [6144/50176]	Loss: 1.5409
Profiling... [7168/50176]	Loss: 1.5716
Profiling... [8192/50176]	Loss: 1.4887
Profiling... [9216/50176]	Loss: 1.5730
Profiling... [10240/50176]	Loss: 1.4994
Profiling... [11264/50176]	Loss: 1.5487
Profiling... [12288/50176]	Loss: 1.5226
Profiling... [13312/50176]	Loss: 1.4807
Profile done
epoch 1 train time consumed: 13.11s
Validation Epoch: 10, Average loss: 0.0029, Accuracy: 0.3412
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.23442945595113,
                        "time": 8.853638181999486,
                        "accuracy": 0.3412109375,
                        "total_cost": 4540847.058426755
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.4784
Profiling... [2048/50176]	Loss: 1.5310
Profiling... [3072/50176]	Loss: 1.5295
Profiling... [4096/50176]	Loss: 1.5714
Profiling... [5120/50176]	Loss: 1.5156
Profiling... [6144/50176]	Loss: 1.6197
Profiling... [7168/50176]	Loss: 1.4748
Profiling... [8192/50176]	Loss: 1.5178
Profiling... [9216/50176]	Loss: 1.5124
Profiling... [10240/50176]	Loss: 1.5105
Profiling... [11264/50176]	Loss: 1.4581
Profiling... [12288/50176]	Loss: 1.5413
Profiling... [13312/50176]	Loss: 1.5531
Profile done
epoch 1 train time consumed: 13.40s
Validation Epoch: 10, Average loss: 0.0022, Accuracy: 0.4309
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.27287086475614,
                        "time": 9.133952245998444,
                        "accuracy": 0.430859375,
                        "total_cost": 3709891.755401
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4998
Profiling... [2048/50176]	Loss: 1.4565
Profiling... [3072/50176]	Loss: 1.4298
Profiling... [4096/50176]	Loss: 1.5916
Profiling... [5120/50176]	Loss: 1.4584
Profiling... [6144/50176]	Loss: 1.5320
Profiling... [7168/50176]	Loss: 1.5112
Profiling... [8192/50176]	Loss: 1.4871
Profiling... [9216/50176]	Loss: 1.5629
Profiling... [10240/50176]	Loss: 1.5240
Profiling... [11264/50176]	Loss: 1.4651
Profiling... [12288/50176]	Loss: 1.4874
Profiling... [13312/50176]	Loss: 1.4074
Profile done
epoch 1 train time consumed: 15.18s
Validation Epoch: 10, Average loss: 0.0026, Accuracy: 0.3722
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.28047001137433,
                        "time": 10.554212562999965,
                        "accuracy": 0.37216796875,
                        "total_cost": 4962778.512961411
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5287
Profiling... [2048/50176]	Loss: 1.4332
Profiling... [3072/50176]	Loss: 1.5008
Profiling... [4096/50176]	Loss: 1.4931
Profiling... [5120/50176]	Loss: 1.4466
Profiling... [6144/50176]	Loss: 1.4784
Profiling... [7168/50176]	Loss: 1.5211
Profiling... [8192/50176]	Loss: 1.5019
Profiling... [9216/50176]	Loss: 1.5165
Profiling... [10240/50176]	Loss: 1.4535
Profiling... [11264/50176]	Loss: 1.4959
Profiling... [12288/50176]	Loss: 1.4785
Profiling... [13312/50176]	Loss: 1.6011
Profile done
epoch 1 train time consumed: 37.63s
Validation Epoch: 10, Average loss: 0.0040, Accuracy: 0.1977
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.20674625297347,
                        "time": 27.753334196999276,
                        "accuracy": 0.19765625,
                        "total_cost": 24572121.976789873
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5091
Profiling... [2048/50176]	Loss: 1.4944
Profiling... [3072/50176]	Loss: 1.4990
Profiling... [4096/50176]	Loss: 1.5015
Profiling... [5120/50176]	Loss: 1.5228
Profiling... [6144/50176]	Loss: 1.4374
Profiling... [7168/50176]	Loss: 1.5607
Profiling... [8192/50176]	Loss: 1.4787
Profiling... [9216/50176]	Loss: 1.4612
Profiling... [10240/50176]	Loss: 1.5121
Profiling... [11264/50176]	Loss: 1.5230
Profiling... [12288/50176]	Loss: 1.4986
Profiling... [13312/50176]	Loss: 1.5496
Profile done
epoch 1 train time consumed: 13.49s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.3061
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.23463713469366,
                        "time": 9.174120278001283,
                        "accuracy": 0.3060546875,
                        "total_cost": 5245699.916457658
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5404
Profiling... [2048/50176]	Loss: 1.4783
Profiling... [3072/50176]	Loss: 1.3777
Profiling... [4096/50176]	Loss: 1.4819
Profiling... [5120/50176]	Loss: 1.5130
Profiling... [6144/50176]	Loss: 1.4354
Profiling... [7168/50176]	Loss: 1.5278
Profiling... [8192/50176]	Loss: 1.4737
Profiling... [9216/50176]	Loss: 1.4997
Profiling... [10240/50176]	Loss: 1.5201
Profiling... [11264/50176]	Loss: 1.5300
Profiling... [12288/50176]	Loss: 1.4992
Profiling... [13312/50176]	Loss: 1.5460
Profile done
epoch 1 train time consumed: 13.87s
Validation Epoch: 10, Average loss: 0.0037, Accuracy: 0.2631
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.2728200539042,
                        "time": 9.446878189999552,
                        "accuracy": 0.2630859375,
                        "total_cost": 6283892.248136302
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5290
Profiling... [2048/50176]	Loss: 1.4732
Profiling... [3072/50176]	Loss: 1.5234
Profiling... [4096/50176]	Loss: 1.5230
Profiling... [5120/50176]	Loss: 1.5159
Profiling... [6144/50176]	Loss: 1.4474
Profiling... [7168/50176]	Loss: 1.5439
Profiling... [8192/50176]	Loss: 1.4737
Profiling... [9216/50176]	Loss: 1.5210
Profiling... [10240/50176]	Loss: 1.4112
Profiling... [11264/50176]	Loss: 1.5047
Profiling... [12288/50176]	Loss: 1.4901
Profiling... [13312/50176]	Loss: 1.4683
Profile done
epoch 1 train time consumed: 15.65s
Validation Epoch: 10, Average loss: 0.0036, Accuracy: 0.2994
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.28029166401683,
                        "time": 10.842486356001245,
                        "accuracy": 0.2994140625,
                        "total_cost": 6337160.975197074
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5728
Profiling... [2048/50176]	Loss: 1.4282
Profiling... [3072/50176]	Loss: 1.4596
Profiling... [4096/50176]	Loss: 1.4459
Profiling... [5120/50176]	Loss: 1.5501
Profiling... [6144/50176]	Loss: 1.4449
Profiling... [7168/50176]	Loss: 1.5398
Profiling... [8192/50176]	Loss: 1.5068
Profiling... [9216/50176]	Loss: 1.5926
Profiling... [10240/50176]	Loss: 1.5238
Profiling... [11264/50176]	Loss: 1.4469
Profiling... [12288/50176]	Loss: 1.5624
Profiling... [13312/50176]	Loss: 1.5494
Profile done
epoch 1 train time consumed: 38.26s
Validation Epoch: 10, Average loss: 0.0023, Accuracy: 0.4005
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.20365480098931,
                        "time": 28.564124309999897,
                        "accuracy": 0.40048828125,
                        "total_cost": 12481568.096444722
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4676
Profiling... [2048/50176]	Loss: 1.4326
Profiling... [3072/50176]	Loss: 1.5041
Profiling... [4096/50176]	Loss: 1.4144
Profiling... [5120/50176]	Loss: 1.5975
Profiling... [6144/50176]	Loss: 1.5229
Profiling... [7168/50176]	Loss: 1.5733
Profiling... [8192/50176]	Loss: 1.5125
Profiling... [9216/50176]	Loss: 1.4715
Profiling... [10240/50176]	Loss: 1.5300
Profiling... [11264/50176]	Loss: 1.5159
Profiling... [12288/50176]	Loss: 1.5268
Profiling... [13312/50176]	Loss: 1.5511
Profile done
epoch 1 train time consumed: 13.13s
Validation Epoch: 10, Average loss: 0.0037, Accuracy: 0.2691
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.23003062198937,
                        "time": 8.872400259999267,
                        "accuracy": 0.269140625,
                        "total_cost": 5768991.751059029
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5310
Profiling... [2048/50176]	Loss: 1.5307
Profiling... [3072/50176]	Loss: 1.4973
Profiling... [4096/50176]	Loss: 1.4511
Profiling... [5120/50176]	Loss: 1.5260
Profiling... [6144/50176]	Loss: 1.5910
Profiling... [7168/50176]	Loss: 1.4946
Profiling... [8192/50176]	Loss: 1.5430
Profiling... [9216/50176]	Loss: 1.4971
Profiling... [10240/50176]	Loss: 1.4625
Profiling... [11264/50176]	Loss: 1.5646
Profiling... [12288/50176]	Loss: 1.5392
Profiling... [13312/50176]	Loss: 1.5157
Profile done
epoch 1 train time consumed: 13.60s
Validation Epoch: 10, Average loss: 0.0024, Accuracy: 0.3873
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.26640657863457,
                        "time": 9.163898946999325,
                        "accuracy": 0.3873046875,
                        "total_cost": 4140622.0153864822
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5340
Profiling... [2048/50176]	Loss: 1.4216
Profiling... [3072/50176]	Loss: 1.5335
Profiling... [4096/50176]	Loss: 1.5008
Profiling... [5120/50176]	Loss: 1.4669
Profiling... [6144/50176]	Loss: 1.5402
Profiling... [7168/50176]	Loss: 1.4786
Profiling... [8192/50176]	Loss: 1.4708
Profiling... [9216/50176]	Loss: 1.5851
Profiling... [10240/50176]	Loss: 1.5616
Profiling... [11264/50176]	Loss: 1.4458
Profiling... [12288/50176]	Loss: 1.5323
Profiling... [13312/50176]	Loss: 1.4584
Profile done
epoch 1 train time consumed: 15.39s
Validation Epoch: 10, Average loss: 0.0027, Accuracy: 0.3658
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.27282418456765,
                        "time": 10.646712630001275,
                        "accuracy": 0.3658203125,
                        "total_cost": 5093141.76000061
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.4644
Profiling... [2048/50176]	Loss: 1.4138
Profiling... [3072/50176]	Loss: 1.4433
Profiling... [4096/50176]	Loss: 1.5178
Profiling... [5120/50176]	Loss: 1.4236
Profiling... [6144/50176]	Loss: 1.5693
Profiling... [7168/50176]	Loss: 1.4973
Profiling... [8192/50176]	Loss: 1.5747
Profiling... [9216/50176]	Loss: 1.4759
Profiling... [10240/50176]	Loss: 1.5396
Profiling... [11264/50176]	Loss: 1.4747
Profiling... [12288/50176]	Loss: 1.5020
Profiling... [13312/50176]	Loss: 1.4966
Profile done
epoch 1 train time consumed: 37.64s
Validation Epoch: 10, Average loss: 0.0032, Accuracy: 0.3159
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.20189342515701,
                        "time": 27.902030274000936,
                        "accuracy": 0.31591796875,
                        "total_cost": 15456086.01267687
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.25 pl: 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[GPU_0] Set GPU power limit to 150W.
[GPU_0] Set GPU power limit to 150W.
Training Epoch: 10 [128/50048]	Loss: 1.6946
Training Epoch: 10 [256/50048]	Loss: 1.6580
Training Epoch: 10 [384/50048]	Loss: 1.5292
Training Epoch: 10 [512/50048]	Loss: 1.7580
Training Epoch: 10 [640/50048]	Loss: 1.5820
Training Epoch: 10 [768/50048]	Loss: 1.5379
Training Epoch: 10 [896/50048]	Loss: 1.8042
Training Epoch: 10 [1024/50048]	Loss: 1.6928
Training Epoch: 10 [1152/50048]	Loss: 1.5183
Training Epoch: 10 [1280/50048]	Loss: 1.5713
Training Epoch: 10 [1408/50048]	Loss: 1.5011
Training Epoch: 10 [1536/50048]	Loss: 1.5205
Training Epoch: 10 [1664/50048]	Loss: 1.6194
Training Epoch: 10 [1792/50048]	Loss: 1.3848
Training Epoch: 10 [1920/50048]	Loss: 1.4824
Training Epoch: 10 [2048/50048]	Loss: 1.2896
Training Epoch: 10 [2176/50048]	Loss: 1.3245
Training Epoch: 10 [2304/50048]	Loss: 1.7500
Training Epoch: 10 [2432/50048]	Loss: 1.4979
Training Epoch: 10 [2560/50048]	Loss: 1.5518
Training Epoch: 10 [2688/50048]	Loss: 1.4091
Training Epoch: 10 [2816/50048]	Loss: 1.4690
Training Epoch: 10 [2944/50048]	Loss: 1.6576
Training Epoch: 10 [3072/50048]	Loss: 1.6353
Training Epoch: 10 [3200/50048]	Loss: 1.4886
Training Epoch: 10 [3328/50048]	Loss: 1.4995
Training Epoch: 10 [3456/50048]	Loss: 1.3666
Training Epoch: 10 [3584/50048]	Loss: 1.3304
Training Epoch: 10 [3712/50048]	Loss: 1.3777
Training Epoch: 10 [3840/50048]	Loss: 1.6876
Training Epoch: 10 [3968/50048]	Loss: 1.5178
Training Epoch: 10 [4096/50048]	Loss: 1.4402
Training Epoch: 10 [4224/50048]	Loss: 1.7063
Training Epoch: 10 [4352/50048]	Loss: 1.9910
Training Epoch: 10 [4480/50048]	Loss: 1.4802
Training Epoch: 10 [4608/50048]	Loss: 1.5997
Training Epoch: 10 [4736/50048]	Loss: 1.4515
Training Epoch: 10 [4864/50048]	Loss: 1.5487
Training Epoch: 10 [4992/50048]	Loss: 1.5537
Training Epoch: 10 [5120/50048]	Loss: 1.5226
Training Epoch: 10 [5248/50048]	Loss: 1.5675
Training Epoch: 10 [5376/50048]	Loss: 1.4933
Training Epoch: 10 [5504/50048]	Loss: 1.3647
Training Epoch: 10 [5632/50048]	Loss: 1.6294
Training Epoch: 10 [5760/50048]	Loss: 1.7378
Training Epoch: 10 [5888/50048]	Loss: 1.3062
Training Epoch: 10 [6016/50048]	Loss: 1.3348
Training Epoch: 10 [6144/50048]	Loss: 1.5296
Training Epoch: 10 [6272/50048]	Loss: 1.4987
Training Epoch: 10 [6400/50048]	Loss: 1.5958
Training Epoch: 10 [6528/50048]	Loss: 1.7473
Training Epoch: 10 [6656/50048]	Loss: 1.6726
Training Epoch: 10 [6784/50048]	Loss: 1.4853
Training Epoch: 10 [6912/50048]	Loss: 1.3116
Training Epoch: 10 [7040/50048]	Loss: 1.5750
Training Epoch: 10 [7168/50048]	Loss: 1.4536
Training Epoch: 10 [7296/50048]	Loss: 1.4136
Training Epoch: 10 [7424/50048]	Loss: 1.4300
Training Epoch: 10 [7552/50048]	Loss: 1.4140
Training Epoch: 10 [7680/50048]	Loss: 1.6373
Training Epoch: 10 [7808/50048]	Loss: 1.6190
Training Epoch: 10 [7936/50048]	Loss: 1.3273
Training Epoch: 10 [8064/50048]	Loss: 1.7358
Training Epoch: 10 [8192/50048]	Loss: 1.5404
Training Epoch: 10 [8320/50048]	Loss: 1.3209
Training Epoch: 10 [8448/50048]	Loss: 1.3063
Training Epoch: 10 [8576/50048]	Loss: 1.4729
Training Epoch: 10 [8704/50048]	Loss: 1.7286
Training Epoch: 10 [8832/50048]	Loss: 1.5009
Training Epoch: 10 [8960/50048]	Loss: 1.6785
Training Epoch: 10 [9088/50048]	Loss: 1.3534
Training Epoch: 10 [9216/50048]	Loss: 1.8529
Training Epoch: 10 [9344/50048]	Loss: 1.1787
Training Epoch: 10 [9472/50048]	Loss: 1.5515
Training Epoch: 10 [9600/50048]	Loss: 1.4518
Training Epoch: 10 [9728/50048]	Loss: 1.6045
Training Epoch: 10 [9856/50048]	Loss: 1.6947
Training Epoch: 10 [9984/50048]	Loss: 1.4612
Training Epoch: 10 [10112/50048]	Loss: 1.8018
Training Epoch: 10 [10240/50048]	Loss: 1.5393
Training Epoch: 10 [10368/50048]	Loss: 1.4547
Training Epoch: 10 [10496/50048]	Loss: 1.5443
Training Epoch: 10 [10624/50048]	Loss: 1.3733
Training Epoch: 10 [10752/50048]	Loss: 1.4457
Training Epoch: 10 [10880/50048]	Loss: 1.3664
Training Epoch: 10 [11008/50048]	Loss: 1.6287
Training Epoch: 10 [11136/50048]	Loss: 1.4445
Training Epoch: 10 [11264/50048]	Loss: 1.3433
Training Epoch: 10 [11392/50048]	Loss: 1.3983
Training Epoch: 10 [11520/50048]	Loss: 1.1788
Training Epoch: 10 [11648/50048]	Loss: 1.2396
Training Epoch: 10 [11776/50048]	Loss: 1.4572
Training Epoch: 10 [11904/50048]	Loss: 1.6937
Training Epoch: 10 [12032/50048]	Loss: 1.6694
Training Epoch: 10 [12160/50048]	Loss: 1.4736
Training Epoch: 10 [12288/50048]	Loss: 1.4831
Training Epoch: 10 [12416/50048]	Loss: 1.4818
Training Epoch: 10 [12544/50048]	Loss: 1.2789
Training Epoch: 10 [12672/50048]	Loss: 1.4423
Training Epoch: 10 [12800/50048]	Loss: 1.3871
Training Epoch: 10 [12928/50048]	Loss: 1.3098
Training Epoch: 10 [13056/50048]	Loss: 1.6762
Training Epoch: 10 [13184/50048]	Loss: 1.6128
Training Epoch: 10 [13312/50048]	Loss: 1.3733
Training Epoch: 10 [13440/50048]	Loss: 1.6611
Training Epoch: 10 [13568/50048]	Loss: 1.3654
Training Epoch: 10 [13696/50048]	Loss: 1.4632
Training Epoch: 10 [13824/50048]	Loss: 1.1976
Training Epoch: 10 [13952/50048]	Loss: 1.5262
Training Epoch: 10 [14080/50048]	Loss: 1.2991
Training Epoch: 10 [14208/50048]	Loss: 1.3141
Training Epoch: 10 [14336/50048]	Loss: 1.5223
Training Epoch: 10 [14464/50048]	Loss: 1.3726
Training Epoch: 10 [14592/50048]	Loss: 1.6397
Training Epoch: 10 [14720/50048]	Loss: 1.4918
Training Epoch: 10 [14848/50048]	Loss: 1.3357
Training Epoch: 10 [14976/50048]	Loss: 1.5349
Training Epoch: 10 [15104/50048]	Loss: 1.6920
Training Epoch: 10 [15232/50048]	Loss: 1.6636
Training Epoch: 10 [15360/50048]	Loss: 1.5470
Training Epoch: 10 [15488/50048]	Loss: 1.5143
Training Epoch: 10 [15616/50048]	Loss: 1.4953
Training Epoch: 10 [15744/50048]	Loss: 1.5688
Training Epoch: 10 [15872/50048]	Loss: 1.4442
Training Epoch: 10 [16000/50048]	Loss: 1.5756
Training Epoch: 10 [16128/50048]	Loss: 1.6253
Training Epoch: 10 [16256/50048]	Loss: 1.3802
Training Epoch: 10 [16384/50048]	Loss: 1.6310
Training Epoch: 10 [16512/50048]	Loss: 1.3954
Training Epoch: 10 [16640/50048]	Loss: 1.3917
Training Epoch: 10 [16768/50048]	Loss: 1.3945
Training Epoch: 10 [16896/50048]	Loss: 1.3241
Training Epoch: 10 [17024/50048]	Loss: 1.2901
Training Epoch: 10 [17152/50048]	Loss: 1.3377
Training Epoch: 10 [17280/50048]	Loss: 1.4028
Training Epoch: 10 [17408/50048]	Loss: 1.3951
Training Epoch: 10 [17536/50048]	Loss: 1.3352
Training Epoch: 10 [17664/50048]	Loss: 1.4591
Training Epoch: 10 [17792/50048]	Loss: 1.4158
Training Epoch: 10 [17920/50048]	Loss: 1.4851
Training Epoch: 10 [18048/50048]	Loss: 1.5872
Training Epoch: 10 [18176/50048]	Loss: 1.4890
Training Epoch: 10 [18304/50048]	Loss: 1.4432
Training Epoch: 10 [18432/50048]	Loss: 1.3343
Training Epoch: 10 [18560/50048]	Loss: 1.6821
Training Epoch: 10 [18688/50048]	Loss: 1.5386
Training Epoch: 10 [18816/50048]	Loss: 1.4934
Training Epoch: 10 [18944/50048]	Loss: 1.2955
Training Epoch: 10 [19072/50048]	Loss: 1.4245
Training Epoch: 10 [19200/50048]	Loss: 1.3776
Training Epoch: 10 [19328/50048]	Loss: 1.4769
Training Epoch: 10 [19456/50048]	Loss: 1.5530
Training Epoch: 10 [19584/50048]	Loss: 1.4692
Training Epoch: 10 [19712/50048]	Loss: 1.3854
Training Epoch: 10 [19840/50048]	Loss: 1.3280
Training Epoch: 10 [19968/50048]	Loss: 1.4885
Training Epoch: 10 [20096/50048]	Loss: 1.5163
Training Epoch: 10 [20224/50048]	Loss: 1.4311
Training Epoch: 10 [20352/50048]	Loss: 1.6237
Training Epoch: 10 [20480/50048]	Loss: 1.5627
Training Epoch: 10 [20608/50048]	Loss: 1.5526
Training Epoch: 10 [20736/50048]	Loss: 1.3767
Training Epoch: 10 [20864/50048]	Loss: 1.5089
Training Epoch: 10 [20992/50048]	Loss: 1.6590
Training Epoch: 10 [21120/50048]	Loss: 1.5185
Training Epoch: 10 [21248/50048]	Loss: 1.2163
Training Epoch: 10 [21376/50048]	Loss: 1.3684
Training Epoch: 10 [21504/50048]	Loss: 1.4005
Training Epoch: 10 [21632/50048]	Loss: 1.5650
Training Epoch: 10 [21760/50048]	Loss: 1.4249
Training Epoch: 10 [21888/50048]	Loss: 1.6025
Training Epoch: 10 [22016/50048]	Loss: 1.4932
Training Epoch: 10 [22144/50048]	Loss: 1.4549
Training Epoch: 10 [22272/50048]	Loss: 1.4809
Training Epoch: 10 [22400/50048]	Loss: 1.5363
Training Epoch: 10 [22528/50048]	Loss: 1.6821
Training Epoch: 10 [22656/50048]	Loss: 1.4679
Training Epoch: 10 [22784/50048]	Loss: 1.5395
Training Epoch: 10 [22912/50048]	Loss: 1.3656
Training Epoch: 10 [23040/50048]	Loss: 1.3815
Training Epoch: 10 [23168/50048]	Loss: 1.3636
Training Epoch: 10 [23296/50048]	Loss: 1.3331
Training Epoch: 10 [23424/50048]	Loss: 1.5627
Training Epoch: 10 [23552/50048]	Loss: 1.2163
Training Epoch: 10 [23680/50048]	Loss: 1.3862
Training Epoch: 10 [23808/50048]	Loss: 1.6294
Training Epoch: 10 [23936/50048]	Loss: 1.5420
Training Epoch: 10 [24064/50048]	Loss: 1.4346
Training Epoch: 10 [24192/50048]	Loss: 1.2103
Training Epoch: 10 [24320/50048]	Loss: 1.3204
Training Epoch: 10 [24448/50048]	Loss: 1.3214
Training Epoch: 10 [24576/50048]	Loss: 1.5391
Training Epoch: 10 [24704/50048]	Loss: 1.3536
Training Epoch: 10 [24832/50048]	Loss: 1.3343
Training Epoch: 10 [24960/50048]	Loss: 1.4687
Training Epoch: 10 [25088/50048]	Loss: 1.2714
Training Epoch: 10 [25216/50048]	Loss: 1.3732
Training Epoch: 10 [25344/50048]	Loss: 1.4656
Training Epoch: 10 [25472/50048]	Loss: 1.3419
Training Epoch: 10 [25600/50048]	Loss: 1.4915
Training Epoch: 10 [25728/50048]	Loss: 1.7554
Training Epoch: 10 [25856/50048]	Loss: 1.6483
Training Epoch: 10 [25984/50048]	Loss: 1.6756
Training Epoch: 10 [26112/50048]	Loss: 1.7032
Training Epoch: 10 [26240/50048]	Loss: 1.4665
Training Epoch: 10 [26368/50048]	Loss: 1.4627
Training Epoch: 10 [26496/50048]	Loss: 1.7178
Training Epoch: 10 [26624/50048]	Loss: 1.5614
Training Epoch: 10 [26752/50048]	Loss: 1.4944
Training Epoch: 10 [26880/50048]	Loss: 1.3247
Training Epoch: 10 [27008/50048]	Loss: 1.7505
Training Epoch: 10 [27136/50048]	Loss: 1.4914
Training Epoch: 10 [27264/50048]	Loss: 1.5811
Training Epoch: 10 [27392/50048]	Loss: 1.4894
Training Epoch: 10 [27520/50048]	Loss: 1.2707
Training Epoch: 10 [27648/50048]	Loss: 1.6085
Training Epoch: 10 [27776/50048]	Loss: 1.4096
Training Epoch: 10 [27904/50048]	Loss: 1.8086
Training Epoch: 10 [28032/50048]	Loss: 1.3492
Training Epoch: 10 [28160/50048]	Loss: 1.3153
Training Epoch: 10 [28288/50048]	Loss: 1.2255
Training Epoch: 10 [28416/50048]	Loss: 1.4795
Training Epoch: 10 [28544/50048]	Loss: 1.3561
Training Epoch: 10 [28672/50048]	Loss: 1.5714
Training Epoch: 10 [28800/50048]	Loss: 1.1421
Training Epoch: 10 [28928/50048]	Loss: 1.5924
Training Epoch: 10 [29056/50048]	Loss: 1.4811
Training Epoch: 10 [29184/50048]	Loss: 1.3980
Training Epoch: 10 [29312/50048]	Loss: 1.6417
Training Epoch: 10 [29440/50048]	Loss: 1.5250
Training Epoch: 10 [29568/50048]	Loss: 1.3970
Training Epoch: 10 [29696/50048]	Loss: 1.6246
Training Epoch: 10 [29824/50048]	Loss: 1.6388
Training Epoch: 10 [29952/50048]	Loss: 1.5565
Training Epoch: 10 [30080/50048]	Loss: 1.3369
Training Epoch: 10 [30208/50048]	Loss: 1.2601
Training Epoch: 10 [30336/50048]	Loss: 1.4062
Training Epoch: 10 [30464/50048]	Loss: 1.4546
Training Epoch: 10 [30592/50048]	Loss: 1.4944
Training Epoch: 10 [30720/50048]	Loss: 1.1724
Training Epoch: 10 [30848/50048]	Loss: 1.6265
Training Epoch: 10 [30976/50048]	Loss: 1.4237
Training Epoch: 10 [31104/50048]	Loss: 1.3695
Training Epoch: 10 [31232/50048]	Loss: 1.3858
Training Epoch: 10 [31360/50048]	Loss: 1.4925
Training Epoch: 10 [31488/50048]	Loss: 1.2314
Training Epoch: 10 [31616/50048]	Loss: 1.5568
Training Epoch: 10 [31744/50048]	Loss: 1.5130
Training Epoch: 10 [31872/50048]	Loss: 1.3566
Training Epoch: 10 [32000/50048]	Loss: 1.5650
Training Epoch: 10 [32128/50048]	Loss: 1.6665
Training Epoch: 10 [32256/50048]	Loss: 1.4215
Training Epoch: 10 [32384/50048]	Loss: 1.3935
Training Epoch: 10 [32512/50048]	Loss: 1.2409
Training Epoch: 10 [32640/50048]	Loss: 1.6120
Training Epoch: 10 [32768/50048]	Loss: 1.7802
Training Epoch: 10 [32896/50048]	Loss: 1.5935
Training Epoch: 10 [33024/50048]	Loss: 1.4500
Training Epoch: 10 [33152/50048]	Loss: 1.3440
Training Epoch: 10 [33280/50048]	Loss: 1.7163
Training Epoch: 10 [33408/50048]	Loss: 1.4924
Training Epoch: 10 [33536/50048]	Loss: 1.4810
Training Epoch: 10 [33664/50048]	Loss: 1.4248
Training Epoch: 10 [33792/50048]	Loss: 1.3401
Training Epoch: 10 [33920/50048]	Loss: 1.4580
Training Epoch: 10 [34048/50048]	Loss: 1.6396
Training Epoch: 10 [34176/50048]	Loss: 1.4377
Training Epoch: 10 [34304/50048]	Loss: 1.2759
Training Epoch: 10 [34432/50048]	Loss: 1.3764
Training Epoch: 10 [34560/50048]	Loss: 1.3598
Training Epoch: 10 [34688/50048]	Loss: 1.6061
Training Epoch: 10 [34816/50048]	Loss: 1.3385
Training Epoch: 10 [34944/50048]	Loss: 1.5908
Training Epoch: 10 [35072/50048]	Loss: 1.5353
Training Epoch: 10 [35200/50048]	Loss: 1.2521
Training Epoch: 10 [35328/50048]	Loss: 1.6331
Training Epoch: 10 [35456/50048]	Loss: 1.2652
Training Epoch: 10 [35584/50048]	Loss: 1.3865
Training Epoch: 10 [35712/50048]	Loss: 1.3182
Training Epoch: 10 [35840/50048]	Loss: 1.7874
Training Epoch: 10 [35968/50048]	Loss: 1.5134
Training Epoch: 10 [36096/50048]	Loss: 1.3263
Training Epoch: 10 [36224/50048]	Loss: 1.0799
Training Epoch: 10 [36352/50048]	Loss: 1.5077
Training Epoch: 10 [36480/50048]	Loss: 1.3829
Training Epoch: 10 [36608/50048]	Loss: 1.4637
Training Epoch: 10 [36736/50048]	Loss: 1.4987
Training Epoch: 10 [36864/50048]	Loss: 1.3529
Training Epoch: 10 [36992/50048]	Loss: 1.3708
Training Epoch: 10 [37120/50048]	Loss: 1.4554
Training Epoch: 10 [37248/50048]	Loss: 1.3992
Training Epoch: 10 [37376/50048]	Loss: 1.2696
Training Epoch: 10 [37504/50048]	Loss: 1.3822
Training Epoch: 10 [37632/50048]	Loss: 1.5015
Training Epoch: 10 [37760/50048]	Loss: 1.5634
Training Epoch: 10 [37888/50048]	Loss: 1.4431
Training Epoch: 10 [38016/50048]	Loss: 1.3944
Training Epoch: 10 [38144/50048]	Loss: 1.3601
Training Epoch: 10 [38272/50048]	Loss: 1.5581
Training Epoch: 10 [38400/50048]	Loss: 1.3476
Training Epoch: 10 [38528/50048]	Loss: 1.5101
Training Epoch: 10 [38656/50048]	Loss: 1.6328
Training Epoch: 10 [38784/50048]	Loss: 1.5859
Training Epoch: 10 [38912/50048]	Loss: 1.3885
Training Epoch: 10 [39040/50048]	Loss: 1.3059
Training Epoch: 10 [39168/50048]	Loss: 1.6025
Training Epoch: 10 [39296/50048]	Loss: 1.5308
Training Epoch: 10 [39424/50048]	Loss: 1.3358
Training Epoch: 10 [39552/50048]	Loss: 1.4144
Training Epoch: 10 [39680/50048]	Loss: 1.4514
Training Epoch: 10 [39808/50048]	Loss: 1.5305
Training Epoch: 10 [39936/50048]	Loss: 1.4988
Training Epoch: 10 [40064/50048]	Loss: 1.5320
Training Epoch: 10 [40192/50048]	Loss: 1.3295
Training Epoch: 10 [40320/50048]	Loss: 1.4242
Training Epoch: 10 [40448/50048]	Loss: 1.3578
Training Epoch: 10 [40576/50048]	Loss: 1.6503
Training Epoch: 10 [40704/50048]	Loss: 1.3475
Training Epoch: 10 [40832/50048]	Loss: 1.2463
Training Epoch: 10 [40960/50048]	Loss: 1.5510
Training Epoch: 10 [41088/50048]	Loss: 1.6148
Training Epoch: 10 [41216/50048]	Loss: 1.5336
Training Epoch: 10 [41344/50048]	Loss: 1.1158
Training Epoch: 10 [41472/50048]	Loss: 1.1822
Training Epoch: 10 [41600/50048]	Loss: 1.4220
Training Epoch: 10 [41728/50048]	Loss: 1.4643
Training Epoch: 10 [41856/50048]	Loss: 1.5112
Training Epoch: 10 [41984/50048]	Loss: 1.5264
Training Epoch: 10 [42112/50048]	Loss: 1.6098
Training Epoch: 10 [42240/50048]	Loss: 1.4508
Training Epoch: 10 [42368/50048]	Loss: 1.6160
Training Epoch: 10 [42496/50048]	Loss: 1.6571
Training Epoch: 10 [42624/50048]	Loss: 1.2228
Training Epoch: 10 [42752/50048]	Loss: 1.6986
Training Epoch: 10 [42880/50048]	Loss: 1.4340
Training Epoch: 10 [43008/50048]	Loss: 1.4787
Training Epoch: 10 [43136/50048]	Loss: 1.5319
Training Epoch: 10 [43264/50048]	Loss: 1.6089
Training Epoch: 10 [43392/50048]	Loss: 1.2326
Training Epoch: 10 [43520/50048]	Loss: 1.5714
Training Epoch: 10 [43648/50048]	Loss: 1.0669
Training Epoch: 10 [43776/50048]	Loss: 1.3611
Training Epoch: 10 [43904/50048]	Loss: 1.3043
Training Epoch: 10 [44032/50048]	Loss: 1.5096
Training Epoch: 10 [44160/50048]	Loss: 1.5318
Training Epoch: 10 [44288/50048]	Loss: 1.4240
Training Epoch: 10 [44416/50048]	Loss: 1.3884
Training Epoch: 10 [44544/50048]	Loss: 1.5813
Training Epoch: 10 [44672/50048]	Loss: 1.5970
Training Epoch: 10 [44800/50048]	Loss: 1.4466
Training Epoch: 10 [44928/50048]	Loss: 1.3678
Training Epoch: 10 [45056/50048]	Loss: 1.5073
Training Epoch: 10 [45184/50048]	Loss: 1.4339
Training Epoch: 10 [45312/50048]	Loss: 1.4387
Training Epoch: 10 [45440/50048]	Loss: 1.4089
Training Epoch: 10 [45568/50048]	Loss: 1.6895
Training Epoch: 10 [45696/50048]	Loss: 1.1049
Training Epoch: 10 [45824/50048]	Loss: 1.6441
Training Epoch: 10 [45952/50048]	Loss: 1.5231
Training Epoch: 10 [46080/50048]	Loss: 1.5510
Training Epoch: 10 [46208/50048]	Loss: 1.5265
Training Epoch: 10 [46336/50048]	Loss: 1.3800
Training Epoch: 10 [46464/50048]	Loss: 1.4487
Training Epoch: 10 [46592/50048]	Loss: 1.4242
Training Epoch: 10 [46720/50048]	Loss: 1.2469
Training Epoch: 10 [46848/50048]	Loss: 1.3849
Training Epoch: 10 [46976/50048]	Loss: 1.4218
Training Epoch: 10 [47104/50048]	Loss: 1.6158
Training Epoch: 10 [47232/50048]	Loss: 1.8443
Training Epoch: 10 [47360/50048]	Loss: 1.7715
Training Epoch: 10 [47488/50048]	Loss: 1.4540
Training Epoch: 10 [47616/50048]	Loss: 1.4366
Training Epoch: 10 [47744/50048]	Loss: 1.5439
Training Epoch: 10 [47872/50048]	Loss: 1.4599
Training Epoch: 10 [48000/50048]	Loss: 1.2564
Training Epoch: 10 [48128/50048]	Loss: 1.4951
Training Epoch: 10 [48256/50048]	Loss: 1.4344
Training Epoch: 10 [48384/50048]	Loss: 1.4799
Training Epoch: 10 [48512/50048]	Loss: 1.6898
Training Epoch: 10 [48640/50048]	Loss: 1.4072
Training Epoch: 10 [48768/50048]	Loss: 1.4765
Training Epoch: 10 [48896/50048]	Loss: 1.5048
Training Epoch: 10 [49024/50048]	Loss: 1.6649
Training Epoch: 10 [49152/50048]	Loss: 1.4159
Training Epoch: 10 [49280/50048]	Loss: 1.2637
Training Epoch: 10 [49408/50048]	Loss: 1.3383
Training Epoch: 10 [49536/50048]	Loss: 1.3578
Training Epoch: 10 [49664/50048]	Loss: 1.4782
Training Epoch: 10 [49792/50048]	Loss: 1.1906
Training Epoch: 10 [49920/50048]	Loss: 1.3788
Training Epoch: 10 [50048/50048]	Loss: 1.1721
Validation Epoch: 10, Average loss: 0.0122, Accuracy: 0.5648
Training Epoch: 11 [128/50048]	Loss: 1.7058
Training Epoch: 11 [256/50048]	Loss: 1.3390
Training Epoch: 11 [384/50048]	Loss: 1.3134
Training Epoch: 11 [512/50048]	Loss: 1.2649
Training Epoch: 11 [640/50048]	Loss: 1.5428
Training Epoch: 11 [768/50048]	Loss: 1.3048
Training Epoch: 11 [896/50048]	Loss: 1.4455
Training Epoch: 11 [1024/50048]	Loss: 1.2915
Training Epoch: 11 [1152/50048]	Loss: 1.3565
Training Epoch: 11 [1280/50048]	Loss: 1.4718
Training Epoch: 11 [1408/50048]	Loss: 1.4281
Training Epoch: 11 [1536/50048]	Loss: 1.5791
Training Epoch: 11 [1664/50048]	Loss: 1.6338
Training Epoch: 11 [1792/50048]	Loss: 1.2902
Training Epoch: 11 [1920/50048]	Loss: 1.5922
Training Epoch: 11 [2048/50048]	Loss: 1.3156
Training Epoch: 11 [2176/50048]	Loss: 1.2550
Training Epoch: 11 [2304/50048]	Loss: 1.2413
Training Epoch: 11 [2432/50048]	Loss: 1.4370
Training Epoch: 11 [2560/50048]	Loss: 1.4612
Training Epoch: 11 [2688/50048]	Loss: 1.4963
Training Epoch: 11 [2816/50048]	Loss: 1.3214
Training Epoch: 11 [2944/50048]	Loss: 1.5491
Training Epoch: 11 [3072/50048]	Loss: 1.3746
Training Epoch: 11 [3200/50048]	Loss: 1.4024
Training Epoch: 11 [3328/50048]	Loss: 1.3968
Training Epoch: 11 [3456/50048]	Loss: 1.3227
Training Epoch: 11 [3584/50048]	Loss: 1.3315
Training Epoch: 11 [3712/50048]	Loss: 1.6243
Training Epoch: 11 [3840/50048]	Loss: 1.3218
Training Epoch: 11 [3968/50048]	Loss: 1.5762
Training Epoch: 11 [4096/50048]	Loss: 1.2952
Training Epoch: 11 [4224/50048]	Loss: 1.5155
Training Epoch: 11 [4352/50048]	Loss: 1.4803
Training Epoch: 11 [4480/50048]	Loss: 1.5261
Training Epoch: 11 [4608/50048]	Loss: 1.4008
Training Epoch: 11 [4736/50048]	Loss: 1.5419
Training Epoch: 11 [4864/50048]	Loss: 1.4777
Training Epoch: 11 [4992/50048]	Loss: 1.6074
Training Epoch: 11 [5120/50048]	Loss: 1.4727
Training Epoch: 11 [5248/50048]	Loss: 1.1891
Training Epoch: 11 [5376/50048]	Loss: 1.6288
Training Epoch: 11 [5504/50048]	Loss: 1.3819
Training Epoch: 11 [5632/50048]	Loss: 1.4406
Training Epoch: 11 [5760/50048]	Loss: 1.4453
Training Epoch: 11 [5888/50048]	Loss: 1.3345
Training Epoch: 11 [6016/50048]	Loss: 1.3227
Training Epoch: 11 [6144/50048]	Loss: 1.5787
Training Epoch: 11 [6272/50048]	Loss: 1.4394
Training Epoch: 11 [6400/50048]	Loss: 1.4674
Training Epoch: 11 [6528/50048]	Loss: 1.2659
Training Epoch: 11 [6656/50048]	Loss: 1.4019
Training Epoch: 11 [6784/50048]	Loss: 1.5342
Training Epoch: 11 [6912/50048]	Loss: 1.3645
Training Epoch: 11 [7040/50048]	Loss: 1.4897
Training Epoch: 11 [7168/50048]	Loss: 1.5969
Training Epoch: 11 [7296/50048]	Loss: 1.4087
Training Epoch: 11 [7424/50048]	Loss: 1.2984
Training Epoch: 11 [7552/50048]	Loss: 1.3783
Training Epoch: 11 [7680/50048]	Loss: 1.3390
Training Epoch: 11 [7808/50048]	Loss: 1.4248
Training Epoch: 11 [7936/50048]	Loss: 1.4700
Training Epoch: 11 [8064/50048]	Loss: 1.3531
Training Epoch: 11 [8192/50048]	Loss: 1.5764
Training Epoch: 11 [8320/50048]	Loss: 1.6581
Training Epoch: 11 [8448/50048]	Loss: 1.8368
Training Epoch: 11 [8576/50048]	Loss: 1.7298
Training Epoch: 11 [8704/50048]	Loss: 1.2411
Training Epoch: 11 [8832/50048]	Loss: 1.2796
Training Epoch: 11 [8960/50048]	Loss: 1.2795
Training Epoch: 11 [9088/50048]	Loss: 1.2588
Training Epoch: 11 [9216/50048]	Loss: 1.7084
Training Epoch: 11 [9344/50048]	Loss: 1.5040
Training Epoch: 11 [9472/50048]	Loss: 1.4132
Training Epoch: 11 [9600/50048]	Loss: 1.3532
Training Epoch: 11 [9728/50048]	Loss: 1.6443
Training Epoch: 11 [9856/50048]	Loss: 1.4118
Training Epoch: 11 [9984/50048]	Loss: 1.4321
Training Epoch: 11 [10112/50048]	Loss: 1.4226
Training Epoch: 11 [10240/50048]	Loss: 1.3666
Training Epoch: 11 [10368/50048]	Loss: 1.5640
Training Epoch: 11 [10496/50048]	Loss: 1.5035
Training Epoch: 11 [10624/50048]	Loss: 1.4911
Training Epoch: 11 [10752/50048]	Loss: 1.5105
Training Epoch: 11 [10880/50048]	Loss: 1.3214
Training Epoch: 11 [11008/50048]	Loss: 1.6109
Training Epoch: 11 [11136/50048]	Loss: 1.6166
Training Epoch: 11 [11264/50048]	Loss: 1.3121
Training Epoch: 11 [11392/50048]	Loss: 1.5154
Training Epoch: 11 [11520/50048]	Loss: 1.4919
Training Epoch: 11 [11648/50048]	Loss: 1.5158
Training Epoch: 11 [11776/50048]	Loss: 1.5848
Training Epoch: 11 [11904/50048]	Loss: 1.4896
Training Epoch: 11 [12032/50048]	Loss: 1.4800
Training Epoch: 11 [12160/50048]	Loss: 1.3313
Training Epoch: 11 [12288/50048]	Loss: 1.4104
Training Epoch: 11 [12416/50048]	Loss: 1.3529
Training Epoch: 11 [12544/50048]	Loss: 1.4908
Training Epoch: 11 [12672/50048]	Loss: 1.4009
Training Epoch: 11 [12800/50048]	Loss: 1.3184
Training Epoch: 11 [12928/50048]	Loss: 1.4054
Training Epoch: 11 [13056/50048]	Loss: 1.6871
Training Epoch: 11 [13184/50048]	Loss: 1.4983
Training Epoch: 11 [13312/50048]	Loss: 1.3631
Training Epoch: 11 [13440/50048]	Loss: 1.6607
Training Epoch: 11 [13568/50048]	Loss: 1.3071
Training Epoch: 11 [13696/50048]	Loss: 1.6497
Training Epoch: 11 [13824/50048]	Loss: 1.6309
Training Epoch: 11 [13952/50048]	Loss: 1.4835
Training Epoch: 11 [14080/50048]	Loss: 1.4123
Training Epoch: 11 [14208/50048]	Loss: 1.2131
Training Epoch: 11 [14336/50048]	Loss: 1.3124
Training Epoch: 11 [14464/50048]	Loss: 1.4133
Training Epoch: 11 [14592/50048]	Loss: 1.4792
Training Epoch: 11 [14720/50048]	Loss: 1.4444
Training Epoch: 11 [14848/50048]	Loss: 1.3500
Training Epoch: 11 [14976/50048]	Loss: 1.4676
Training Epoch: 11 [15104/50048]	Loss: 1.3486
Training Epoch: 11 [15232/50048]	Loss: 1.7036
Training Epoch: 11 [15360/50048]	Loss: 1.3995
Training Epoch: 11 [15488/50048]	Loss: 1.1533
Training Epoch: 11 [15616/50048]	Loss: 1.3992
Training Epoch: 11 [15744/50048]	Loss: 1.5103
Training Epoch: 11 [15872/50048]	Loss: 1.6054
Training Epoch: 11 [16000/50048]	Loss: 1.2223
Training Epoch: 11 [16128/50048]	Loss: 1.2735
Training Epoch: 11 [16256/50048]	Loss: 1.5420
Training Epoch: 11 [16384/50048]	Loss: 1.3003
Training Epoch: 11 [16512/50048]	Loss: 1.1475
Training Epoch: 11 [16640/50048]	Loss: 1.2388
Training Epoch: 11 [16768/50048]	Loss: 1.5035
Training Epoch: 11 [16896/50048]	Loss: 1.4649
Training Epoch: 11 [17024/50048]	Loss: 1.3746
Training Epoch: 11 [17152/50048]	Loss: 1.2208
Training Epoch: 11 [17280/50048]	Loss: 1.1874
Training Epoch: 11 [17408/50048]	Loss: 1.2973
Training Epoch: 11 [17536/50048]	Loss: 1.6819
Training Epoch: 11 [17664/50048]	Loss: 1.3314
Training Epoch: 11 [17792/50048]	Loss: 1.5293
Training Epoch: 11 [17920/50048]	Loss: 1.3011
Training Epoch: 11 [18048/50048]	Loss: 1.4926
Training Epoch: 11 [18176/50048]	Loss: 1.5271
Training Epoch: 11 [18304/50048]	Loss: 1.4132
Training Epoch: 11 [18432/50048]	Loss: 1.3519
Training Epoch: 11 [18560/50048]	Loss: 1.3584
Training Epoch: 11 [18688/50048]	Loss: 1.3422
Training Epoch: 11 [18816/50048]	Loss: 1.3313
Training Epoch: 11 [18944/50048]	Loss: 1.6330
Training Epoch: 11 [19072/50048]	Loss: 1.2143
Training Epoch: 11 [19200/50048]	Loss: 1.4812
Training Epoch: 11 [19328/50048]	Loss: 1.6991
Training Epoch: 11 [19456/50048]	Loss: 1.2147
Training Epoch: 11 [19584/50048]	Loss: 1.3610
Training Epoch: 11 [19712/50048]	Loss: 1.2396
Training Epoch: 11 [19840/50048]	Loss: 1.6161
Training Epoch: 11 [19968/50048]	Loss: 1.3909
Training Epoch: 11 [20096/50048]	Loss: 1.5171
Training Epoch: 11 [20224/50048]	Loss: 1.3816
Training Epoch: 11 [20352/50048]	Loss: 1.4625
Training Epoch: 11 [20480/50048]	Loss: 1.4241
Training Epoch: 11 [20608/50048]	Loss: 1.3790
Training Epoch: 11 [20736/50048]	Loss: 1.2638
Training Epoch: 11 [20864/50048]	Loss: 1.3779
Training Epoch: 11 [20992/50048]	Loss: 1.4089
Training Epoch: 11 [21120/50048]	Loss: 1.4511
Training Epoch: 11 [21248/50048]	Loss: 1.3961
Training Epoch: 11 [21376/50048]	Loss: 1.3041
Training Epoch: 11 [21504/50048]	Loss: 1.3535
Training Epoch: 11 [21632/50048]	Loss: 1.3596
Training Epoch: 11 [21760/50048]	Loss: 1.5267
Training Epoch: 11 [21888/50048]	Loss: 1.3914
Training Epoch: 11 [22016/50048]	Loss: 1.2747
Training Epoch: 11 [22144/50048]	Loss: 1.5255
Training Epoch: 11 [22272/50048]	Loss: 1.4183
Training Epoch: 11 [22400/50048]	Loss: 1.4414
Training Epoch: 11 [22528/50048]	Loss: 1.0915
Training Epoch: 11 [22656/50048]	Loss: 1.1156
Training Epoch: 11 [22784/50048]	Loss: 1.5976
Training Epoch: 11 [22912/50048]	Loss: 1.4506
Training Epoch: 11 [23040/50048]	Loss: 1.4005
Training Epoch: 11 [23168/50048]	Loss: 1.5378
Training Epoch: 11 [23296/50048]	Loss: 1.2674
Training Epoch: 11 [23424/50048]	Loss: 1.3546
Training Epoch: 11 [23552/50048]	Loss: 1.4742
Training Epoch: 11 [23680/50048]	Loss: 1.3471
Training Epoch: 11 [23808/50048]	Loss: 1.5063
Training Epoch: 11 [23936/50048]	Loss: 1.2706
Training Epoch: 11 [24064/50048]	Loss: 1.6154
Training Epoch: 11 [24192/50048]	Loss: 1.3361
Training Epoch: 11 [24320/50048]	Loss: 1.4577
Training Epoch: 11 [24448/50048]	Loss: 1.4051
Training Epoch: 11 [24576/50048]	Loss: 1.2969
Training Epoch: 11 [24704/50048]	Loss: 1.4317
Training Epoch: 11 [24832/50048]	Loss: 1.4682
Training Epoch: 11 [24960/50048]	Loss: 1.3261
Training Epoch: 11 [25088/50048]	Loss: 1.3663
Training Epoch: 11 [25216/50048]	Loss: 1.5852
Training Epoch: 11 [25344/50048]	Loss: 1.3165
Training Epoch: 11 [25472/50048]	Loss: 1.1866
Training Epoch: 11 [25600/50048]	Loss: 1.5277
Training Epoch: 11 [25728/50048]	Loss: 1.2861
Training Epoch: 11 [25856/50048]	Loss: 1.8038
Training Epoch: 11 [25984/50048]	Loss: 1.3556
Training Epoch: 11 [26112/50048]	Loss: 1.3689
Training Epoch: 11 [26240/50048]	Loss: 1.3334
Training Epoch: 11 [26368/50048]	Loss: 1.4985
Training Epoch: 11 [26496/50048]	Loss: 1.5785
Training Epoch: 11 [26624/50048]	Loss: 1.3923
Training Epoch: 11 [26752/50048]	Loss: 1.2832
Training Epoch: 11 [26880/50048]	Loss: 1.7025
Training Epoch: 11 [27008/50048]	Loss: 1.3691
Training Epoch: 11 [27136/50048]	Loss: 1.2747
Training Epoch: 11 [27264/50048]	Loss: 1.4911
Training Epoch: 11 [27392/50048]	Loss: 1.2474
Training Epoch: 11 [27520/50048]	Loss: 1.5045
Training Epoch: 11 [27648/50048]	Loss: 1.1985
Training Epoch: 11 [27776/50048]	Loss: 1.4198
Training Epoch: 11 [27904/50048]	Loss: 1.3341
Training Epoch: 11 [28032/50048]	Loss: 1.4678
Training Epoch: 11 [28160/50048]	Loss: 1.4359
Training Epoch: 11 [28288/50048]	Loss: 1.4612
Training Epoch: 11 [28416/50048]	Loss: 1.3109
Training Epoch: 11 [28544/50048]	Loss: 1.1429
Training Epoch: 11 [28672/50048]	Loss: 1.4877
Training Epoch: 11 [28800/50048]	Loss: 1.5143
Training Epoch: 11 [28928/50048]	Loss: 1.4857
Training Epoch: 11 [29056/50048]	Loss: 1.7932
Training Epoch: 11 [29184/50048]	Loss: 1.2164
Training Epoch: 11 [29312/50048]	Loss: 1.3595
Training Epoch: 11 [29440/50048]	Loss: 1.3467
Training Epoch: 11 [29568/50048]	Loss: 1.4733
Training Epoch: 11 [29696/50048]	Loss: 1.1615
Training Epoch: 11 [29824/50048]	Loss: 1.4766
Training Epoch: 11 [29952/50048]	Loss: 1.2989
Training Epoch: 11 [30080/50048]	Loss: 1.3035
Training Epoch: 11 [30208/50048]	Loss: 1.3254
Training Epoch: 11 [30336/50048]	Loss: 1.3253
Training Epoch: 11 [30464/50048]	Loss: 1.4337
Training Epoch: 11 [30592/50048]	Loss: 1.3273
Training Epoch: 11 [30720/50048]	Loss: 1.6362
Training Epoch: 11 [30848/50048]	Loss: 1.3721
Training Epoch: 11 [30976/50048]	Loss: 1.2943
Training Epoch: 11 [31104/50048]	Loss: 1.2991
Training Epoch: 11 [31232/50048]	Loss: 1.3360
Training Epoch: 11 [31360/50048]	Loss: 1.3791
Training Epoch: 11 [31488/50048]	Loss: 1.3617
Training Epoch: 11 [31616/50048]	Loss: 1.4771
Training Epoch: 11 [31744/50048]	Loss: 1.3025
Training Epoch: 11 [31872/50048]	Loss: 1.1605
Training Epoch: 11 [32000/50048]	Loss: 1.3941
Training Epoch: 11 [32128/50048]	Loss: 1.3549
Training Epoch: 11 [32256/50048]	Loss: 1.5726
Training Epoch: 11 [32384/50048]	Loss: 1.3718
Training Epoch: 11 [32512/50048]	Loss: 1.2852
Training Epoch: 11 [32640/50048]	Loss: 1.5365
Training Epoch: 11 [32768/50048]	Loss: 1.3525
Training Epoch: 11 [32896/50048]	Loss: 1.3192
Training Epoch: 11 [33024/50048]	Loss: 1.3232
Training Epoch: 11 [33152/50048]	Loss: 1.4093
Training Epoch: 11 [33280/50048]	Loss: 1.2654
Training Epoch: 11 [33408/50048]	Loss: 1.3439
Training Epoch: 11 [33536/50048]	Loss: 1.2483
Training Epoch: 11 [33664/50048]	Loss: 1.4407
Training Epoch: 11 [33792/50048]	Loss: 1.2194
Training Epoch: 11 [33920/50048]	Loss: 1.3137
Training Epoch: 11 [34048/50048]	Loss: 1.4538
Training Epoch: 11 [34176/50048]	Loss: 1.4593
Training Epoch: 11 [34304/50048]	Loss: 1.1748
Training Epoch: 11 [34432/50048]	Loss: 1.4006
Training Epoch: 11 [34560/50048]	Loss: 1.4819
Training Epoch: 11 [34688/50048]	Loss: 1.2133
Training Epoch: 11 [34816/50048]	Loss: 1.4155
Training Epoch: 11 [34944/50048]	Loss: 1.4807
Training Epoch: 11 [35072/50048]	Loss: 1.4628
Training Epoch: 11 [35200/50048]	Loss: 1.6428
Training Epoch: 11 [35328/50048]	Loss: 1.4664
Training Epoch: 11 [35456/50048]	Loss: 1.2974
Training Epoch: 11 [35584/50048]	Loss: 1.2803
Training Epoch: 11 [35712/50048]	Loss: 1.4908
Training Epoch: 11 [35840/50048]	Loss: 1.2190
Training Epoch: 11 [35968/50048]	Loss: 1.1770
Training Epoch: 11 [36096/50048]	Loss: 1.3013
Training Epoch: 11 [36224/50048]	Loss: 1.5479
Training Epoch: 11 [36352/50048]	Loss: 1.3819
Training Epoch: 11 [36480/50048]	Loss: 1.5684
Training Epoch: 11 [36608/50048]	Loss: 1.3721
Training Epoch: 11 [36736/50048]	Loss: 1.3529
Training Epoch: 11 [36864/50048]	Loss: 1.3002
Training Epoch: 11 [36992/50048]	Loss: 1.2877
Training Epoch: 11 [37120/50048]	Loss: 1.3801
Training Epoch: 11 [37248/50048]	Loss: 1.7090
Training Epoch: 11 [37376/50048]	Loss: 1.4075
Training Epoch: 11 [37504/50048]	Loss: 1.3421
Training Epoch: 11 [37632/50048]	Loss: 1.2675
Training Epoch: 11 [37760/50048]	Loss: 1.5523
Training Epoch: 11 [37888/50048]	Loss: 1.3850
Training Epoch: 11 [38016/50048]	Loss: 1.3437
Training Epoch: 11 [38144/50048]	Loss: 1.3910
Training Epoch: 11 [38272/50048]	Loss: 1.3118
Training Epoch: 11 [38400/50048]	Loss: 1.3518
Training Epoch: 11 [38528/50048]	Loss: 1.4874
Training Epoch: 11 [38656/50048]	Loss: 1.1934
Training Epoch: 11 [38784/50048]	Loss: 1.2726
Training Epoch: 11 [38912/50048]	Loss: 1.5219
Training Epoch: 11 [39040/50048]	Loss: 1.4178
Training Epoch: 11 [39168/50048]	Loss: 1.4996
Training Epoch: 11 [39296/50048]	Loss: 1.6055
Training Epoch: 11 [39424/50048]	Loss: 1.5288
Training Epoch: 11 [39552/50048]	Loss: 1.5786
Training Epoch: 11 [39680/50048]	Loss: 1.6483
Training Epoch: 11 [39808/50048]	Loss: 1.4697
Training Epoch: 11 [39936/50048]	Loss: 1.4364
Training Epoch: 11 [40064/50048]	Loss: 1.2795
Training Epoch: 11 [40192/50048]	Loss: 1.3627
Training Epoch: 11 [40320/50048]	Loss: 1.7014
Training Epoch: 11 [40448/50048]	Loss: 1.4195
Training Epoch: 11 [40576/50048]	Loss: 1.2444
Training Epoch: 11 [40704/50048]	Loss: 1.4287
Training Epoch: 11 [40832/50048]	Loss: 1.7791
Training Epoch: 11 [40960/50048]	Loss: 1.3604
Training Epoch: 11 [41088/50048]	Loss: 1.4748
Training Epoch: 11 [41216/50048]	Loss: 1.5198
Training Epoch: 11 [41344/50048]	Loss: 1.2776
Training Epoch: 11 [41472/50048]	Loss: 1.5239
Training Epoch: 11 [41600/50048]	Loss: 1.1118
Training Epoch: 11 [41728/50048]	Loss: 1.2404
Training Epoch: 11 [41856/50048]	Loss: 1.4992
Training Epoch: 11 [41984/50048]	Loss: 1.6947
Training Epoch: 11 [42112/50048]	Loss: 1.1550
Training Epoch: 11 [42240/50048]	Loss: 1.4316
Training Epoch: 11 [42368/50048]	Loss: 1.2794
Training Epoch: 11 [42496/50048]	Loss: 1.3160
Training Epoch: 11 [42624/50048]	Loss: 1.1252
Training Epoch: 11 [42752/50048]	Loss: 1.2840
Training Epoch: 11 [42880/50048]	Loss: 1.2965
Training Epoch: 11 [43008/50048]	Loss: 1.4607
Training Epoch: 11 [43136/50048]	Loss: 1.2190
Training Epoch: 11 [43264/50048]	Loss: 1.4255
Training Epoch: 11 [43392/50048]	Loss: 1.3086
Training Epoch: 11 [43520/50048]	Loss: 1.3908
Training Epoch: 11 [43648/50048]	Loss: 1.2477
Training Epoch: 11 [43776/50048]	Loss: 1.5175
Training Epoch: 11 [43904/50048]	Loss: 1.5543
Training Epoch: 11 [44032/50048]	Loss: 1.5904
Training Epoch: 11 [44160/50048]	Loss: 1.5263
Training Epoch: 11 [44288/50048]	Loss: 1.2120
Training Epoch: 11 [44416/50048]	Loss: 1.5799
Training Epoch: 11 [44544/50048]	Loss: 1.2098
Training Epoch: 11 [44672/50048]	Loss: 1.1303
Training Epoch: 11 [44800/50048]	Loss: 1.2981
Training Epoch: 11 [44928/50048]	Loss: 1.2749
Training Epoch: 11 [45056/50048]	Loss: 1.5864
Training Epoch: 11 [45184/50048]	Loss: 1.4971
Training Epoch: 11 [45312/50048]	Loss: 1.2243
Training Epoch: 11 [45440/50048]	Loss: 1.4959
Training Epoch: 11 [45568/50048]	Loss: 1.4160
Training Epoch: 11 [45696/50048]	Loss: 1.6135
Training Epoch: 11 [45824/50048]	Loss: 1.3911
Training Epoch: 11 [45952/50048]	Loss: 1.4104
Training Epoch: 11 [46080/50048]	Loss: 1.4606
Training Epoch: 11 [46208/50048]	Loss: 1.7136
Training Epoch: 11 [46336/50048]	Loss: 1.5420
Training Epoch: 11 [46464/50048]	Loss: 1.3070
Training Epoch: 11 [46592/50048]	Loss: 1.3645
Training Epoch: 11 [46720/50048]	Loss: 1.2735
Training Epoch: 11 [46848/50048]	Loss: 1.2977
Training Epoch: 11 [46976/50048]	Loss: 1.3363
Training Epoch: 11 [47104/50048]	Loss: 1.3849
Training Epoch: 11 [47232/50048]	Loss: 1.5156
Training Epoch: 11 [47360/50048]	Loss: 1.5566
Training Epoch: 11 [47488/50048]	Loss: 1.3477
Training Epoch: 11 [47616/50048]	Loss: 1.4532
Training Epoch: 11 [47744/50048]	Loss: 1.2637
Training Epoch: 11 [47872/50048]	Loss: 1.4158
Training Epoch: 11 [48000/50048]	Loss: 1.5784
Training Epoch: 11 [48128/50048]	Loss: 1.1943
Training Epoch: 11 [48256/50048]	Loss: 1.4007
Training Epoch: 11 [48384/50048]	Loss: 1.3895
Training Epoch: 11 [48512/50048]	Loss: 1.4798
Training Epoch: 11 [48640/50048]	Loss: 1.6694
Training Epoch: 11 [48768/50048]	Loss: 1.2625
Training Epoch: 11 [48896/50048]	Loss: 1.4896
Training Epoch: 11 [49024/50048]	Loss: 1.5070
Training Epoch: 11 [49152/50048]	Loss: 1.3533
Training Epoch: 11 [49280/50048]	Loss: 1.5510
Training Epoch: 11 [49408/50048]	Loss: 1.3177
Training Epoch: 11 [49536/50048]	Loss: 1.4806
Training Epoch: 11 [49664/50048]	Loss: 1.2953
Training Epoch: 11 [49792/50048]	Loss: 1.4546
Training Epoch: 11 [49920/50048]	Loss: 1.3260
Training Epoch: 11 [50048/50048]	Loss: 1.5159
Validation Epoch: 11, Average loss: 0.0120, Accuracy: 0.5732
Training Epoch: 12 [128/50048]	Loss: 1.3661
Training Epoch: 12 [256/50048]	Loss: 1.2396
Training Epoch: 12 [384/50048]	Loss: 1.0936
Training Epoch: 12 [512/50048]	Loss: 1.4347
Training Epoch: 12 [640/50048]	Loss: 1.6112
Training Epoch: 12 [768/50048]	Loss: 1.3757
Training Epoch: 12 [896/50048]	Loss: 1.2326
Training Epoch: 12 [1024/50048]	Loss: 1.4890
Training Epoch: 12 [1152/50048]	Loss: 1.2330
Training Epoch: 12 [1280/50048]	Loss: 1.3102
Training Epoch: 12 [1408/50048]	Loss: 1.4711
Training Epoch: 12 [1536/50048]	Loss: 1.3086
Training Epoch: 12 [1664/50048]	Loss: 1.2498
Training Epoch: 12 [1792/50048]	Loss: 1.2491
Training Epoch: 12 [1920/50048]	Loss: 1.1226
Training Epoch: 12 [2048/50048]	Loss: 1.3833
Training Epoch: 12 [2176/50048]	Loss: 1.3424
Training Epoch: 12 [2304/50048]	Loss: 1.2465
Training Epoch: 12 [2432/50048]	Loss: 1.4523
Training Epoch: 12 [2560/50048]	Loss: 1.0735
Training Epoch: 12 [2688/50048]	Loss: 1.6807
Training Epoch: 12 [2816/50048]	Loss: 1.2526
Training Epoch: 12 [2944/50048]	Loss: 1.4769
Training Epoch: 12 [3072/50048]	Loss: 1.3643
Training Epoch: 12 [3200/50048]	Loss: 1.3919
Training Epoch: 12 [3328/50048]	Loss: 1.4424
Training Epoch: 12 [3456/50048]	Loss: 1.3186
Training Epoch: 12 [3584/50048]	Loss: 1.3959
Training Epoch: 12 [3712/50048]	Loss: 1.4311
Training Epoch: 12 [3840/50048]	Loss: 1.3550
Training Epoch: 12 [3968/50048]	Loss: 1.2451
Training Epoch: 12 [4096/50048]	Loss: 1.2961
Training Epoch: 12 [4224/50048]	Loss: 1.3461
Training Epoch: 12 [4352/50048]	Loss: 1.5501
Training Epoch: 12 [4480/50048]	Loss: 1.3401
Training Epoch: 12 [4608/50048]	Loss: 1.4494
Training Epoch: 12 [4736/50048]	Loss: 1.1341
Training Epoch: 12 [4864/50048]	Loss: 1.6009
Training Epoch: 12 [4992/50048]	Loss: 1.5855
Training Epoch: 12 [5120/50048]	Loss: 1.3826
Training Epoch: 12 [5248/50048]	Loss: 1.4644
Training Epoch: 12 [5376/50048]	Loss: 1.2411
Training Epoch: 12 [5504/50048]	Loss: 1.3575
Training Epoch: 12 [5632/50048]	Loss: 1.4462
Training Epoch: 12 [5760/50048]	Loss: 1.3499
Training Epoch: 12 [5888/50048]	Loss: 1.3757
Training Epoch: 12 [6016/50048]	Loss: 1.5176
Training Epoch: 12 [6144/50048]	Loss: 1.6607
Training Epoch: 12 [6272/50048]	Loss: 1.2955
Training Epoch: 12 [6400/50048]	Loss: 1.2790
Training Epoch: 12 [6528/50048]	Loss: 1.3333
Training Epoch: 12 [6656/50048]	Loss: 1.2886
Training Epoch: 12 [6784/50048]	Loss: 1.2843
Training Epoch: 12 [6912/50048]	Loss: 1.4754
Training Epoch: 12 [7040/50048]	Loss: 1.5005
Training Epoch: 12 [7168/50048]	Loss: 1.4490
Training Epoch: 12 [7296/50048]	Loss: 1.3290
Training Epoch: 12 [7424/50048]	Loss: 1.4304
Training Epoch: 12 [7552/50048]	Loss: 1.4088
Training Epoch: 12 [7680/50048]	Loss: 1.4142
Training Epoch: 12 [7808/50048]	Loss: 1.3626
Training Epoch: 12 [7936/50048]	Loss: 1.1678
Training Epoch: 12 [8064/50048]	Loss: 1.3327
Training Epoch: 12 [8192/50048]	Loss: 1.4334
Training Epoch: 12 [8320/50048]	Loss: 1.0700
Training Epoch: 12 [8448/50048]	Loss: 1.3601
Training Epoch: 12 [8576/50048]	Loss: 1.1283
Training Epoch: 12 [8704/50048]	Loss: 1.6327
Training Epoch: 12 [8832/50048]	Loss: 1.3186
Training Epoch: 12 [8960/50048]	Loss: 1.2578
Training Epoch: 12 [9088/50048]	Loss: 1.1278
Training Epoch: 12 [9216/50048]	Loss: 1.3268
Training Epoch: 12 [9344/50048]	Loss: 1.6308
Training Epoch: 12 [9472/50048]	Loss: 1.1305
Training Epoch: 12 [9600/50048]	Loss: 1.2466
Training Epoch: 12 [9728/50048]	Loss: 1.3187
Training Epoch: 12 [9856/50048]	Loss: 1.4714
Training Epoch: 12 [9984/50048]	Loss: 1.2635
Training Epoch: 12 [10112/50048]	Loss: 1.3782
Training Epoch: 12 [10240/50048]	Loss: 1.3301
Training Epoch: 12 [10368/50048]	Loss: 1.4221
Training Epoch: 12 [10496/50048]	Loss: 1.2634
Training Epoch: 12 [10624/50048]	Loss: 1.2850
Training Epoch: 12 [10752/50048]	Loss: 1.1845
Training Epoch: 12 [10880/50048]	Loss: 1.2286
Training Epoch: 12 [11008/50048]	Loss: 1.4630
Training Epoch: 12 [11136/50048]	Loss: 1.4612
Training Epoch: 12 [11264/50048]	Loss: 1.4649
Training Epoch: 12 [11392/50048]	Loss: 1.3216
Training Epoch: 12 [11520/50048]	Loss: 1.4907
Training Epoch: 12 [11648/50048]	Loss: 1.5219
Training Epoch: 12 [11776/50048]	Loss: 1.3874
Training Epoch: 12 [11904/50048]	Loss: 1.4143
Training Epoch: 12 [12032/50048]	Loss: 1.3152
Training Epoch: 12 [12160/50048]	Loss: 1.3621
Training Epoch: 12 [12288/50048]	Loss: 1.3805
Training Epoch: 12 [12416/50048]	Loss: 1.2824
Training Epoch: 12 [12544/50048]	Loss: 1.4805
Training Epoch: 12 [12672/50048]	Loss: 1.4323
Training Epoch: 12 [12800/50048]	Loss: 1.5564
Training Epoch: 12 [12928/50048]	Loss: 1.3935
Training Epoch: 12 [13056/50048]	Loss: 1.2661
Training Epoch: 12 [13184/50048]	Loss: 1.3471
Training Epoch: 12 [13312/50048]	Loss: 1.2706
Training Epoch: 12 [13440/50048]	Loss: 1.2883
Training Epoch: 12 [13568/50048]	Loss: 1.4281
Training Epoch: 12 [13696/50048]	Loss: 1.4006
Training Epoch: 12 [13824/50048]	Loss: 1.2019
Training Epoch: 12 [13952/50048]	Loss: 1.5347
Training Epoch: 12 [14080/50048]	Loss: 1.3998
Training Epoch: 12 [14208/50048]	Loss: 1.3713
Training Epoch: 12 [14336/50048]	Loss: 1.1625
Training Epoch: 12 [14464/50048]	Loss: 1.2305
Training Epoch: 12 [14592/50048]	Loss: 1.3717
Training Epoch: 12 [14720/50048]	Loss: 1.3034
Training Epoch: 12 [14848/50048]	Loss: 1.5660
Training Epoch: 12 [14976/50048]	Loss: 1.4640
Training Epoch: 12 [15104/50048]	Loss: 1.5669
Training Epoch: 12 [15232/50048]	Loss: 1.2823
Training Epoch: 12 [15360/50048]	Loss: 1.3846
Training Epoch: 12 [15488/50048]	Loss: 1.6824
Training Epoch: 12 [15616/50048]	Loss: 1.2773
Training Epoch: 12 [15744/50048]	Loss: 1.2473
Training Epoch: 12 [15872/50048]	Loss: 1.2195
Training Epoch: 12 [16000/50048]	Loss: 1.3864
Training Epoch: 12 [16128/50048]	Loss: 1.4723
Training Epoch: 12 [16256/50048]	Loss: 1.1129
Training Epoch: 12 [16384/50048]	Loss: 1.1906
Training Epoch: 12 [16512/50048]	Loss: 1.3568
Training Epoch: 12 [16640/50048]	Loss: 1.3829
Training Epoch: 12 [16768/50048]	Loss: 1.1771
Training Epoch: 12 [16896/50048]	Loss: 1.5196
Training Epoch: 12 [17024/50048]	Loss: 1.2670
Training Epoch: 12 [17152/50048]	Loss: 1.3009
Training Epoch: 12 [17280/50048]	Loss: 1.3592
Training Epoch: 12 [17408/50048]	Loss: 1.3585
Training Epoch: 12 [17536/50048]	Loss: 1.2507
Training Epoch: 12 [17664/50048]	Loss: 1.4011
Training Epoch: 12 [17792/50048]	Loss: 1.6388
Training Epoch: 12 [17920/50048]	Loss: 1.4957
Training Epoch: 12 [18048/50048]	Loss: 1.4527
Training Epoch: 12 [18176/50048]	Loss: 1.5739
Training Epoch: 12 [18304/50048]	Loss: 1.3683
Training Epoch: 12 [18432/50048]	Loss: 1.5419
Training Epoch: 12 [18560/50048]	Loss: 1.7285
Training Epoch: 12 [18688/50048]	Loss: 1.5158
Training Epoch: 12 [18816/50048]	Loss: 1.4820
Training Epoch: 12 [18944/50048]	Loss: 1.4211
Training Epoch: 12 [19072/50048]	Loss: 1.2457
Training Epoch: 12 [19200/50048]	Loss: 1.3760
Training Epoch: 12 [19328/50048]	Loss: 1.6395
Training Epoch: 12 [19456/50048]	Loss: 1.4508
Training Epoch: 12 [19584/50048]	Loss: 1.0725
Training Epoch: 12 [19712/50048]	Loss: 1.5584
Training Epoch: 12 [19840/50048]	Loss: 1.5024
Training Epoch: 12 [19968/50048]	Loss: 1.4213
Training Epoch: 12 [20096/50048]	Loss: 1.3841
Training Epoch: 12 [20224/50048]	Loss: 1.2540
Training Epoch: 12 [20352/50048]	Loss: 1.5285
Training Epoch: 12 [20480/50048]	Loss: 1.3226
Training Epoch: 12 [20608/50048]	Loss: 1.4152
Training Epoch: 12 [20736/50048]	Loss: 1.3524
Training Epoch: 12 [20864/50048]	Loss: 1.3354
Training Epoch: 12 [20992/50048]	Loss: 1.4714
Training Epoch: 12 [21120/50048]	Loss: 1.3406
Training Epoch: 12 [21248/50048]	Loss: 1.3777
Training Epoch: 12 [21376/50048]	Loss: 1.3405
Training Epoch: 12 [21504/50048]	Loss: 1.5553
Training Epoch: 12 [21632/50048]	Loss: 1.2426
Training Epoch: 12 [21760/50048]	Loss: 1.4532
Training Epoch: 12 [21888/50048]	Loss: 1.3656
Training Epoch: 12 [22016/50048]	Loss: 1.3464
Training Epoch: 12 [22144/50048]	Loss: 1.3581
Training Epoch: 12 [22272/50048]	Loss: 1.3597
Training Epoch: 12 [22400/50048]	Loss: 1.4484
Training Epoch: 12 [22528/50048]	Loss: 1.3678
Training Epoch: 12 [22656/50048]	Loss: 1.2579
Training Epoch: 12 [22784/50048]	Loss: 1.2903
Training Epoch: 12 [22912/50048]	Loss: 1.3624
Training Epoch: 12 [23040/50048]	Loss: 1.5585
Training Epoch: 12 [23168/50048]	Loss: 1.3192
Training Epoch: 12 [23296/50048]	Loss: 1.3641
Training Epoch: 12 [23424/50048]	Loss: 1.6013
Training Epoch: 12 [23552/50048]	Loss: 1.2001
Training Epoch: 12 [23680/50048]	Loss: 1.3642
Training Epoch: 12 [23808/50048]	Loss: 1.6117
Training Epoch: 12 [23936/50048]	Loss: 1.4595
Training Epoch: 12 [24064/50048]	Loss: 1.3184
Training Epoch: 12 [24192/50048]	Loss: 1.2032
Training Epoch: 12 [24320/50048]	Loss: 1.3888
Training Epoch: 12 [24448/50048]	Loss: 1.5905
Training Epoch: 12 [24576/50048]	Loss: 1.4402
Training Epoch: 12 [24704/50048]	Loss: 1.3334
Training Epoch: 12 [24832/50048]	Loss: 1.7770
Training Epoch: 12 [24960/50048]	Loss: 1.1508
Training Epoch: 12 [25088/50048]	Loss: 1.5186
Training Epoch: 12 [25216/50048]	Loss: 1.2133
Training Epoch: 12 [25344/50048]	Loss: 1.1368
Training Epoch: 12 [25472/50048]	Loss: 1.5392
Training Epoch: 12 [25600/50048]	Loss: 1.2254
Training Epoch: 12 [25728/50048]	Loss: 1.5657
Training Epoch: 12 [25856/50048]	Loss: 1.5237
Training Epoch: 12 [25984/50048]	Loss: 1.6966
Training Epoch: 12 [26112/50048]	Loss: 1.4357
Training Epoch: 12 [26240/50048]	Loss: 1.3946
Training Epoch: 12 [26368/50048]	Loss: 1.2827
Training Epoch: 12 [26496/50048]	Loss: 1.3319
Training Epoch: 12 [26624/50048]	Loss: 1.4647
Training Epoch: 12 [26752/50048]	Loss: 1.5696
Training Epoch: 12 [26880/50048]	Loss: 1.2435
Training Epoch: 12 [27008/50048]	Loss: 1.3312
Training Epoch: 12 [27136/50048]	Loss: 1.4348
Training Epoch: 12 [27264/50048]	Loss: 1.2456
Training Epoch: 12 [27392/50048]	Loss: 1.2264
Training Epoch: 12 [27520/50048]	Loss: 1.6427
Training Epoch: 12 [27648/50048]	Loss: 1.3567
Training Epoch: 12 [27776/50048]	Loss: 1.6986
Training Epoch: 12 [27904/50048]	Loss: 1.4756
Training Epoch: 12 [28032/50048]	Loss: 1.3655
Training Epoch: 12 [28160/50048]	Loss: 1.2906
Training Epoch: 12 [28288/50048]	Loss: 1.2905
Training Epoch: 12 [28416/50048]	Loss: 1.2595
Training Epoch: 12 [28544/50048]	Loss: 1.0514
Training Epoch: 12 [28672/50048]	Loss: 1.3374
Training Epoch: 12 [28800/50048]	Loss: 1.4425
Training Epoch: 12 [28928/50048]	Loss: 1.2323
Training Epoch: 12 [29056/50048]	Loss: 1.4877
Training Epoch: 12 [29184/50048]	Loss: 1.3365
Training Epoch: 12 [29312/50048]	Loss: 1.3284
Training Epoch: 12 [29440/50048]	Loss: 1.2562
Training Epoch: 12 [29568/50048]	Loss: 1.1699
Training Epoch: 12 [29696/50048]	Loss: 1.3936
Training Epoch: 12 [29824/50048]	Loss: 1.3869
Training Epoch: 12 [29952/50048]	Loss: 1.1780
Training Epoch: 12 [30080/50048]	Loss: 1.4263
Training Epoch: 12 [30208/50048]	Loss: 1.4933
Training Epoch: 12 [30336/50048]	Loss: 1.4002
Training Epoch: 12 [30464/50048]	Loss: 1.2007
Training Epoch: 12 [30592/50048]	Loss: 1.5409
Training Epoch: 12 [30720/50048]	Loss: 1.5212
Training Epoch: 12 [30848/50048]	Loss: 1.2749
Training Epoch: 12 [30976/50048]	Loss: 1.5191
Training Epoch: 12 [31104/50048]	Loss: 1.3209
Training Epoch: 12 [31232/50048]	Loss: 1.5754
Training Epoch: 12 [31360/50048]	Loss: 1.3901
Training Epoch: 12 [31488/50048]	Loss: 1.2701
Training Epoch: 12 [31616/50048]	Loss: 1.3382
Training Epoch: 12 [31744/50048]	Loss: 1.5190
Training Epoch: 12 [31872/50048]	Loss: 1.2196
Training Epoch: 12 [32000/50048]	Loss: 1.2510
Training Epoch: 12 [32128/50048]	Loss: 1.4057
Training Epoch: 12 [32256/50048]	Loss: 1.3251
Training Epoch: 12 [32384/50048]	Loss: 1.2168
Training Epoch: 12 [32512/50048]	Loss: 1.1899
Training Epoch: 12 [32640/50048]	Loss: 1.5622
Training Epoch: 12 [32768/50048]	Loss: 1.5693
Training Epoch: 12 [32896/50048]	Loss: 1.2862
Training Epoch: 12 [33024/50048]	Loss: 1.4585
Training Epoch: 12 [33152/50048]	Loss: 1.4656
Training Epoch: 12 [33280/50048]	Loss: 1.3010
Training Epoch: 12 [33408/50048]	Loss: 1.3611
Training Epoch: 12 [33536/50048]	Loss: 1.3870
Training Epoch: 12 [33664/50048]	Loss: 1.6754
Training Epoch: 12 [33792/50048]	Loss: 1.2876
Training Epoch: 12 [33920/50048]	Loss: 1.2592
Training Epoch: 12 [34048/50048]	Loss: 1.4952
Training Epoch: 12 [34176/50048]	Loss: 1.3169
Training Epoch: 12 [34304/50048]	Loss: 1.6612
Training Epoch: 12 [34432/50048]	Loss: 1.3189
Training Epoch: 12 [34560/50048]	Loss: 1.3710
Training Epoch: 12 [34688/50048]	Loss: 1.3586
Training Epoch: 12 [34816/50048]	Loss: 1.4150
Training Epoch: 12 [34944/50048]	Loss: 1.5354
Training Epoch: 12 [35072/50048]	Loss: 1.4032
Training Epoch: 12 [35200/50048]	Loss: 1.3263
Training Epoch: 12 [35328/50048]	Loss: 1.1557
Training Epoch: 12 [35456/50048]	Loss: 1.2404
Training Epoch: 12 [35584/50048]	Loss: 1.2939
Training Epoch: 12 [35712/50048]	Loss: 1.2912
Training Epoch: 12 [35840/50048]	Loss: 1.2538
Training Epoch: 12 [35968/50048]	Loss: 1.6916
Training Epoch: 12 [36096/50048]	Loss: 1.3393
Training Epoch: 12 [36224/50048]	Loss: 1.6114
Training Epoch: 12 [36352/50048]	Loss: 1.2977
Training Epoch: 12 [36480/50048]	Loss: 1.2546
Training Epoch: 12 [36608/50048]	Loss: 1.4412
Training Epoch: 12 [36736/50048]	Loss: 1.5936
Training Epoch: 12 [36864/50048]	Loss: 1.2777
Training Epoch: 12 [36992/50048]	Loss: 1.1699
Training Epoch: 12 [37120/50048]	Loss: 1.4383
Training Epoch: 12 [37248/50048]	Loss: 1.4950
Training Epoch: 12 [37376/50048]	Loss: 1.2670
Training Epoch: 12 [37504/50048]	Loss: 1.2788
Training Epoch: 12 [37632/50048]	Loss: 1.2570
Training Epoch: 12 [37760/50048]	Loss: 1.4916
Training Epoch: 12 [37888/50048]	Loss: 1.2932
Training Epoch: 12 [38016/50048]	Loss: 1.5498
Training Epoch: 12 [38144/50048]	Loss: 1.2965
Training Epoch: 12 [38272/50048]	Loss: 1.4147
Training Epoch: 12 [38400/50048]	Loss: 1.4121
Training Epoch: 12 [38528/50048]	Loss: 1.3389
Training Epoch: 12 [38656/50048]	Loss: 1.3927
Training Epoch: 12 [38784/50048]	Loss: 1.2509
Training Epoch: 12 [38912/50048]	Loss: 1.5291
Training Epoch: 12 [39040/50048]	Loss: 1.0841
Training Epoch: 12 [39168/50048]	Loss: 1.3257
Training Epoch: 12 [39296/50048]	Loss: 1.3257
Training Epoch: 12 [39424/50048]	Loss: 1.3621
Training Epoch: 12 [39552/50048]	Loss: 1.4580
Training Epoch: 12 [39680/50048]	Loss: 1.2337
Training Epoch: 12 [39808/50048]	Loss: 1.6401
Training Epoch: 12 [39936/50048]	Loss: 1.2993
Training Epoch: 12 [40064/50048]	Loss: 1.5418
Training Epoch: 12 [40192/50048]	Loss: 1.5103
Training Epoch: 12 [40320/50048]	Loss: 1.4436
Training Epoch: 12 [40448/50048]	Loss: 1.5513
Training Epoch: 12 [40576/50048]	Loss: 1.3474
Training Epoch: 12 [40704/50048]	Loss: 1.4353
Training Epoch: 12 [40832/50048]	Loss: 1.2107
Training Epoch: 12 [40960/50048]	Loss: 1.2082
Training Epoch: 12 [41088/50048]	Loss: 1.3224
Training Epoch: 12 [41216/50048]	Loss: 1.3882
Training Epoch: 12 [41344/50048]	Loss: 1.4081
Training Epoch: 12 [41472/50048]	Loss: 1.3391
Training Epoch: 12 [41600/50048]	Loss: 1.3613
Training Epoch: 12 [41728/50048]	Loss: 1.2853
Training Epoch: 12 [41856/50048]	Loss: 1.3301
Training Epoch: 12 [41984/50048]	Loss: 1.3911
Training Epoch: 12 [42112/50048]	Loss: 1.1853
Training Epoch: 12 [42240/50048]	Loss: 1.2921
Training Epoch: 12 [42368/50048]	Loss: 1.2178
Training Epoch: 12 [42496/50048]	Loss: 1.4372
Training Epoch: 12 [42624/50048]	Loss: 1.5661
Training Epoch: 12 [42752/50048]	Loss: 1.2024
Training Epoch: 12 [42880/50048]	Loss: 1.4263
Training Epoch: 12 [43008/50048]	Loss: 1.4215
Training Epoch: 12 [43136/50048]	Loss: 1.4146
Training Epoch: 12 [43264/50048]	Loss: 1.5058
Training Epoch: 12 [43392/50048]	Loss: 1.4716
Training Epoch: 12 [43520/50048]	Loss: 1.1659
Training Epoch: 12 [43648/50048]	Loss: 1.4669
Training Epoch: 12 [43776/50048]	Loss: 1.6384
Training Epoch: 12 [43904/50048]	Loss: 1.3069
Training Epoch: 12 [44032/50048]	Loss: 1.3412
Training Epoch: 12 [44160/50048]	Loss: 1.3258
Training Epoch: 12 [44288/50048]	Loss: 1.4409
Training Epoch: 12 [44416/50048]	Loss: 1.4014
Training Epoch: 12 [44544/50048]	Loss: 1.4124
Training Epoch: 12 [44672/50048]	Loss: 1.4283
Training Epoch: 12 [44800/50048]	Loss: 1.4526
Training Epoch: 12 [44928/50048]	Loss: 1.3397
Training Epoch: 12 [45056/50048]	Loss: 1.4965
Training Epoch: 12 [45184/50048]	Loss: 1.6204
Training Epoch: 12 [45312/50048]	Loss: 1.3711
Training Epoch: 12 [45440/50048]	Loss: 1.4394
Training Epoch: 12 [45568/50048]	Loss: 1.3094
Training Epoch: 12 [45696/50048]	Loss: 1.4934
Training Epoch: 12 [45824/50048]	Loss: 1.2569
Training Epoch: 12 [45952/50048]	Loss: 1.5855
Training Epoch: 12 [46080/50048]	Loss: 1.5602
Training Epoch: 12 [46208/50048]	Loss: 1.3836
Training Epoch: 12 [46336/50048]	Loss: 1.0954
Training Epoch: 12 [46464/50048]	Loss: 1.5146
Training Epoch: 12 [46592/50048]	Loss: 1.1551
Training Epoch: 12 [46720/50048]	Loss: 1.3794
Training Epoch: 12 [46848/50048]	Loss: 1.4417
Training Epoch: 12 [46976/50048]	Loss: 1.4964
Training Epoch: 12 [47104/50048]	Loss: 1.4045
Training Epoch: 12 [47232/50048]	Loss: 1.3821
Training Epoch: 12 [47360/50048]	Loss: 1.3243
Training Epoch: 12 [47488/50048]	Loss: 1.3157
Training Epoch: 12 [47616/50048]	Loss: 1.3325
Training Epoch: 12 [47744/50048]	Loss: 1.2691
Training Epoch: 12 [47872/50048]	Loss: 1.4222
Training Epoch: 12 [48000/50048]	Loss: 1.5281
Training Epoch: 12 [48128/50048]	Loss: 1.4228
Training Epoch: 12 [48256/50048]	Loss: 1.3768
Training Epoch: 12 [48384/50048]	Loss: 1.3514
Training Epoch: 12 [48512/50048]	Loss: 1.2871
Training Epoch: 12 [48640/50048]	Loss: 1.5256
Training Epoch: 12 [48768/50048]	Loss: 1.5092
Training Epoch: 12 [48896/50048]	Loss: 1.5462
Training Epoch: 12 [49024/50048]	Loss: 1.4371
Training Epoch: 12 [49152/50048]	Loss: 1.1383
Training Epoch: 12 [49280/50048]	Loss: 1.4436
Training Epoch: 12 [49408/50048]	Loss: 1.5476
Training Epoch: 12 [49536/50048]	Loss: 1.4877
Training Epoch: 12 [49664/50048]	Loss: 1.4430
Training Epoch: 12 [49792/50048]	Loss: 1.2201
Training Epoch: 12 [49920/50048]	Loss: 1.2588
Training Epoch: 12 [50048/50048]	Loss: 1.3462
Validation Epoch: 12, Average loss: 0.0118, Accuracy: 0.5802
Training Epoch: 13 [128/50048]	Loss: 1.2631
Training Epoch: 13 [256/50048]	Loss: 1.4149
Training Epoch: 13 [384/50048]	Loss: 1.4701
Training Epoch: 13 [512/50048]	Loss: 1.5101
Training Epoch: 13 [640/50048]	Loss: 1.1576
Training Epoch: 13 [768/50048]	Loss: 1.5587
Training Epoch: 13 [896/50048]	Loss: 1.2788
Training Epoch: 13 [1024/50048]	Loss: 1.4672
Training Epoch: 13 [1152/50048]	Loss: 1.1899
Training Epoch: 13 [1280/50048]	Loss: 1.3012
Training Epoch: 13 [1408/50048]	Loss: 1.2028
Training Epoch: 13 [1536/50048]	Loss: 1.3684
Training Epoch: 13 [1664/50048]	Loss: 1.1633
Training Epoch: 13 [1792/50048]	Loss: 1.3261
Training Epoch: 13 [1920/50048]	Loss: 1.3440
Training Epoch: 13 [2048/50048]	Loss: 1.1837
Training Epoch: 13 [2176/50048]	Loss: 1.3328
Training Epoch: 13 [2304/50048]	Loss: 1.4580
Training Epoch: 13 [2432/50048]	Loss: 1.3135
Training Epoch: 13 [2560/50048]	Loss: 1.2102
Training Epoch: 13 [2688/50048]	Loss: 1.3280
Training Epoch: 13 [2816/50048]	Loss: 1.4707
Training Epoch: 13 [2944/50048]	Loss: 1.3506
Training Epoch: 13 [3072/50048]	Loss: 1.2924
Training Epoch: 13 [3200/50048]	Loss: 1.5035
Training Epoch: 13 [3328/50048]	Loss: 1.2866
Training Epoch: 13 [3456/50048]	Loss: 1.1892
Training Epoch: 13 [3584/50048]	Loss: 1.3731
Training Epoch: 13 [3712/50048]	Loss: 1.3448
Training Epoch: 13 [3840/50048]	Loss: 1.4220
Training Epoch: 13 [3968/50048]	Loss: 1.4295
Training Epoch: 13 [4096/50048]	Loss: 1.2890
Training Epoch: 13 [4224/50048]	Loss: 1.3953
Training Epoch: 13 [4352/50048]	Loss: 1.2397
Training Epoch: 13 [4480/50048]	Loss: 1.3191
Training Epoch: 13 [4608/50048]	Loss: 1.1259
Training Epoch: 13 [4736/50048]	Loss: 1.2106
Training Epoch: 13 [4864/50048]	Loss: 1.1851
Training Epoch: 13 [4992/50048]	Loss: 1.2719
Training Epoch: 13 [5120/50048]	Loss: 1.3924
Training Epoch: 13 [5248/50048]	Loss: 1.2611
Training Epoch: 13 [5376/50048]	Loss: 1.3202
Training Epoch: 13 [5504/50048]	Loss: 1.4698
Training Epoch: 13 [5632/50048]	Loss: 1.3659
Training Epoch: 13 [5760/50048]	Loss: 1.3404
Training Epoch: 13 [5888/50048]	Loss: 1.4611
Training Epoch: 13 [6016/50048]	Loss: 1.2276
Training Epoch: 13 [6144/50048]	Loss: 1.5115
Training Epoch: 13 [6272/50048]	Loss: 1.3117
Training Epoch: 13 [6400/50048]	Loss: 1.4850
Training Epoch: 13 [6528/50048]	Loss: 1.3816
Training Epoch: 13 [6656/50048]	Loss: 1.3100
Training Epoch: 13 [6784/50048]	Loss: 1.4193
Training Epoch: 13 [6912/50048]	Loss: 1.2276
Training Epoch: 13 [7040/50048]	Loss: 1.3552
Training Epoch: 13 [7168/50048]	Loss: 1.4598
Training Epoch: 13 [7296/50048]	Loss: 1.4550
Training Epoch: 13 [7424/50048]	Loss: 1.4620
Training Epoch: 13 [7552/50048]	Loss: 1.2890
Training Epoch: 13 [7680/50048]	Loss: 1.4092
Training Epoch: 13 [7808/50048]	Loss: 1.3207
Training Epoch: 13 [7936/50048]	Loss: 1.3806
Training Epoch: 13 [8064/50048]	Loss: 1.4043
Training Epoch: 13 [8192/50048]	Loss: 1.4552
Training Epoch: 13 [8320/50048]	Loss: 1.2238
Training Epoch: 13 [8448/50048]	Loss: 1.2035
Training Epoch: 13 [8576/50048]	Loss: 1.3826
Training Epoch: 13 [8704/50048]	Loss: 1.3302
Training Epoch: 13 [8832/50048]	Loss: 1.5217
Training Epoch: 13 [8960/50048]	Loss: 1.4302
Training Epoch: 13 [9088/50048]	Loss: 1.2567
Training Epoch: 13 [9216/50048]	Loss: 1.3224
Training Epoch: 13 [9344/50048]	Loss: 1.1947
Training Epoch: 13 [9472/50048]	Loss: 1.4620
Training Epoch: 13 [9600/50048]	Loss: 1.1429
Training Epoch: 13 [9728/50048]	Loss: 1.4280
Training Epoch: 13 [9856/50048]	Loss: 1.2709
Training Epoch: 13 [9984/50048]	Loss: 1.2857
Training Epoch: 13 [10112/50048]	Loss: 1.3743
Training Epoch: 13 [10240/50048]	Loss: 1.3982
Training Epoch: 13 [10368/50048]	Loss: 1.3890
Training Epoch: 13 [10496/50048]	Loss: 1.4990
Training Epoch: 13 [10624/50048]	Loss: 1.2941
Training Epoch: 13 [10752/50048]	Loss: 1.5160
Training Epoch: 13 [10880/50048]	Loss: 1.3635
Training Epoch: 13 [11008/50048]	Loss: 1.2148
Training Epoch: 13 [11136/50048]	Loss: 1.1299
Training Epoch: 13 [11264/50048]	Loss: 1.5156
Training Epoch: 13 [11392/50048]	Loss: 1.2129
Training Epoch: 13 [11520/50048]	Loss: 1.3134
Training Epoch: 13 [11648/50048]	Loss: 1.3743
Training Epoch: 13 [11776/50048]	Loss: 1.3508
Training Epoch: 13 [11904/50048]	Loss: 0.9940
Training Epoch: 13 [12032/50048]	Loss: 1.3239
Training Epoch: 13 [12160/50048]	Loss: 1.6154
Training Epoch: 13 [12288/50048]	Loss: 1.4056
Training Epoch: 13 [12416/50048]	Loss: 1.1107
Training Epoch: 13 [12544/50048]	Loss: 1.3405
Training Epoch: 13 [12672/50048]	Loss: 1.5309
Training Epoch: 13 [12800/50048]	Loss: 1.1930
Training Epoch: 13 [12928/50048]	Loss: 1.3013
Training Epoch: 13 [13056/50048]	Loss: 1.3411
Training Epoch: 13 [13184/50048]	Loss: 1.3635
Training Epoch: 13 [13312/50048]	Loss: 1.3584
Training Epoch: 13 [13440/50048]	Loss: 1.5905
Training Epoch: 13 [13568/50048]	Loss: 1.3201
Training Epoch: 13 [13696/50048]	Loss: 1.3967
Training Epoch: 13 [13824/50048]	Loss: 1.3872
Training Epoch: 13 [13952/50048]	Loss: 1.2777
Training Epoch: 13 [14080/50048]	Loss: 1.3228
Training Epoch: 13 [14208/50048]	Loss: 1.6256
Training Epoch: 13 [14336/50048]	Loss: 1.3199
Training Epoch: 13 [14464/50048]	Loss: 1.3374
Training Epoch: 13 [14592/50048]	Loss: 1.2589
Training Epoch: 13 [14720/50048]	Loss: 1.2902
Training Epoch: 13 [14848/50048]	Loss: 1.3183
Training Epoch: 13 [14976/50048]	Loss: 1.4894
Training Epoch: 13 [15104/50048]	Loss: 1.3104
Training Epoch: 13 [15232/50048]	Loss: 1.4800
Training Epoch: 13 [15360/50048]	Loss: 1.3007
Training Epoch: 13 [15488/50048]	Loss: 1.4026
Training Epoch: 13 [15616/50048]	Loss: 1.0886
Training Epoch: 13 [15744/50048]	Loss: 1.2860
Training Epoch: 13 [15872/50048]	Loss: 1.3698
Training Epoch: 13 [16000/50048]	Loss: 1.5185
Training Epoch: 13 [16128/50048]	Loss: 1.4372
Training Epoch: 13 [16256/50048]	Loss: 1.1234
Training Epoch: 13 [16384/50048]	Loss: 1.4153
Training Epoch: 13 [16512/50048]	Loss: 1.2398
Training Epoch: 13 [16640/50048]	Loss: 1.1641
Training Epoch: 13 [16768/50048]	Loss: 1.2389
Training Epoch: 13 [16896/50048]	Loss: 1.3509
Training Epoch: 13 [17024/50048]	Loss: 1.2913
Training Epoch: 13 [17152/50048]	Loss: 1.4400
Training Epoch: 13 [17280/50048]	Loss: 1.3416
Training Epoch: 13 [17408/50048]	Loss: 1.3415
Training Epoch: 13 [17536/50048]	Loss: 1.5632
Training Epoch: 13 [17664/50048]	Loss: 1.5416
Training Epoch: 13 [17792/50048]	Loss: 1.1842
Training Epoch: 13 [17920/50048]	Loss: 1.4372
Training Epoch: 13 [18048/50048]	Loss: 1.3800
Training Epoch: 13 [18176/50048]	Loss: 1.1672
Training Epoch: 13 [18304/50048]	Loss: 1.2413
Training Epoch: 13 [18432/50048]	Loss: 1.4456
Training Epoch: 13 [18560/50048]	Loss: 1.1518
Training Epoch: 13 [18688/50048]	Loss: 1.1438
Training Epoch: 13 [18816/50048]	Loss: 1.2390
Training Epoch: 13 [18944/50048]	Loss: 1.4205
Training Epoch: 13 [19072/50048]	Loss: 1.3378
Training Epoch: 13 [19200/50048]	Loss: 1.6129
Training Epoch: 13 [19328/50048]	Loss: 1.2610
Training Epoch: 13 [19456/50048]	Loss: 1.3749
Training Epoch: 13 [19584/50048]	Loss: 1.4764
Training Epoch: 13 [19712/50048]	Loss: 1.2292
Training Epoch: 13 [19840/50048]	Loss: 1.4284
Training Epoch: 13 [19968/50048]	Loss: 1.2387
Training Epoch: 13 [20096/50048]	Loss: 1.3512
Training Epoch: 13 [20224/50048]	Loss: 1.4623
Training Epoch: 13 [20352/50048]	Loss: 1.2493
Training Epoch: 13 [20480/50048]	Loss: 1.5494
Training Epoch: 13 [20608/50048]	Loss: 1.3347
Training Epoch: 13 [20736/50048]	Loss: 1.4725
Training Epoch: 13 [20864/50048]	Loss: 1.3211
Training Epoch: 13 [20992/50048]	Loss: 1.5504
Training Epoch: 13 [21120/50048]	Loss: 1.3339
Training Epoch: 13 [21248/50048]	Loss: 1.3719
Training Epoch: 13 [21376/50048]	Loss: 1.5536
Training Epoch: 13 [21504/50048]	Loss: 1.3388
Training Epoch: 13 [21632/50048]	Loss: 1.0497
Training Epoch: 13 [21760/50048]	Loss: 1.2554
Training Epoch: 13 [21888/50048]	Loss: 1.4704
Training Epoch: 13 [22016/50048]	Loss: 1.5587
Training Epoch: 13 [22144/50048]	Loss: 1.5671
Training Epoch: 13 [22272/50048]	Loss: 1.1964
Training Epoch: 13 [22400/50048]	Loss: 1.3568
Training Epoch: 13 [22528/50048]	Loss: 1.2792
Training Epoch: 13 [22656/50048]	Loss: 1.5459
Training Epoch: 13 [22784/50048]	Loss: 1.1480
Training Epoch: 13 [22912/50048]	Loss: 1.3026
Training Epoch: 13 [23040/50048]	Loss: 1.3323
Training Epoch: 13 [23168/50048]	Loss: 1.1870
Training Epoch: 13 [23296/50048]	Loss: 1.6428
Training Epoch: 13 [23424/50048]	Loss: 1.3835
Training Epoch: 13 [23552/50048]	Loss: 1.4061
Training Epoch: 13 [23680/50048]	Loss: 1.5435
Training Epoch: 13 [23808/50048]	Loss: 1.4356
Training Epoch: 13 [23936/50048]	Loss: 1.2635
Training Epoch: 13 [24064/50048]	Loss: 1.1809
Training Epoch: 13 [24192/50048]	Loss: 1.1225
Training Epoch: 13 [24320/50048]	Loss: 1.5783
Training Epoch: 13 [24448/50048]	Loss: 1.1716
Training Epoch: 13 [24576/50048]	Loss: 1.3993
Training Epoch: 13 [24704/50048]	Loss: 1.5474
Training Epoch: 13 [24832/50048]	Loss: 1.3747
Training Epoch: 13 [24960/50048]	Loss: 1.2028
Training Epoch: 13 [25088/50048]	Loss: 1.3469
Training Epoch: 13 [25216/50048]	Loss: 1.1868
Training Epoch: 13 [25344/50048]	Loss: 1.3487
Training Epoch: 13 [25472/50048]	Loss: 1.4285
Training Epoch: 13 [25600/50048]	Loss: 1.2794
Training Epoch: 13 [25728/50048]	Loss: 1.4906
Training Epoch: 13 [25856/50048]	Loss: 1.2500
Training Epoch: 13 [25984/50048]	Loss: 1.2130
Training Epoch: 13 [26112/50048]	Loss: 1.1458
Training Epoch: 13 [26240/50048]	Loss: 1.5582
Training Epoch: 13 [26368/50048]	Loss: 1.3530
Training Epoch: 13 [26496/50048]	Loss: 1.4241
Training Epoch: 13 [26624/50048]	Loss: 1.4593
Training Epoch: 13 [26752/50048]	Loss: 1.5329
Training Epoch: 13 [26880/50048]	Loss: 1.3177
Training Epoch: 13 [27008/50048]	Loss: 1.3076
Training Epoch: 13 [27136/50048]	Loss: 1.3715
Training Epoch: 13 [27264/50048]	Loss: 1.6972
Training Epoch: 13 [27392/50048]	Loss: 1.3810
Training Epoch: 13 [27520/50048]	Loss: 1.4714
Training Epoch: 13 [27648/50048]	Loss: 1.3881
Training Epoch: 13 [27776/50048]	Loss: 1.3547
Training Epoch: 13 [27904/50048]	Loss: 1.0369
Training Epoch: 13 [28032/50048]	Loss: 1.2915
Training Epoch: 13 [28160/50048]	Loss: 1.6993
Training Epoch: 13 [28288/50048]	Loss: 1.1242
Training Epoch: 13 [28416/50048]	Loss: 1.2159
Training Epoch: 13 [28544/50048]	Loss: 1.3376
Training Epoch: 13 [28672/50048]	Loss: 1.1861
Training Epoch: 13 [28800/50048]	Loss: 1.0941
Training Epoch: 13 [28928/50048]	Loss: 1.1581
Training Epoch: 13 [29056/50048]	Loss: 1.4548
Training Epoch: 13 [29184/50048]	Loss: 1.2821
Training Epoch: 13 [29312/50048]	Loss: 1.3027
Training Epoch: 13 [29440/50048]	Loss: 1.5687
Training Epoch: 13 [29568/50048]	Loss: 1.1121
Training Epoch: 13 [29696/50048]	Loss: 1.2188
Training Epoch: 13 [29824/50048]	Loss: 1.2989
Training Epoch: 13 [29952/50048]	Loss: 1.2312
Training Epoch: 13 [30080/50048]	Loss: 1.4193
Training Epoch: 13 [30208/50048]	Loss: 1.4873
Training Epoch: 13 [30336/50048]	Loss: 1.3580
Training Epoch: 13 [30464/50048]	Loss: 1.3637
Training Epoch: 13 [30592/50048]	Loss: 1.3701
Training Epoch: 13 [30720/50048]	Loss: 1.2866
Training Epoch: 13 [30848/50048]	Loss: 1.3007
Training Epoch: 13 [30976/50048]	Loss: 1.3274
Training Epoch: 13 [31104/50048]	Loss: 1.8204
Training Epoch: 13 [31232/50048]	Loss: 1.4243
Training Epoch: 13 [31360/50048]	Loss: 1.4313
Training Epoch: 13 [31488/50048]	Loss: 1.4770
Training Epoch: 13 [31616/50048]	Loss: 1.3830
Training Epoch: 13 [31744/50048]	Loss: 1.5279
Training Epoch: 13 [31872/50048]	Loss: 1.2645
Training Epoch: 13 [32000/50048]	Loss: 1.5302
Training Epoch: 13 [32128/50048]	Loss: 1.3825
Training Epoch: 13 [32256/50048]	Loss: 1.2941
Training Epoch: 13 [32384/50048]	Loss: 1.3942
Training Epoch: 13 [32512/50048]	Loss: 1.3774
Training Epoch: 13 [32640/50048]	Loss: 1.2499
Training Epoch: 13 [32768/50048]	Loss: 1.4900
Training Epoch: 13 [32896/50048]	Loss: 1.3823
Training Epoch: 13 [33024/50048]	Loss: 1.3484
Training Epoch: 13 [33152/50048]	Loss: 1.2408
Training Epoch: 13 [33280/50048]	Loss: 1.2487
Training Epoch: 13 [33408/50048]	Loss: 1.2099
Training Epoch: 13 [33536/50048]	Loss: 1.0522
Training Epoch: 13 [33664/50048]	Loss: 1.2754
Training Epoch: 13 [33792/50048]	Loss: 1.5250
Training Epoch: 13 [33920/50048]	Loss: 1.4888
Training Epoch: 13 [34048/50048]	Loss: 1.2364
Training Epoch: 13 [34176/50048]	Loss: 1.5450
Training Epoch: 13 [34304/50048]	Loss: 1.2229
Training Epoch: 13 [34432/50048]	Loss: 1.3883
Training Epoch: 13 [34560/50048]	Loss: 1.4983
Training Epoch: 13 [34688/50048]	Loss: 1.2748
Training Epoch: 13 [34816/50048]	Loss: 1.3212
Training Epoch: 13 [34944/50048]	Loss: 1.3389
Training Epoch: 13 [35072/50048]	Loss: 1.2465
Training Epoch: 13 [35200/50048]	Loss: 1.5715
Training Epoch: 13 [35328/50048]	Loss: 1.3537
Training Epoch: 13 [35456/50048]	Loss: 1.2175
Training Epoch: 13 [35584/50048]	Loss: 1.3780
Training Epoch: 13 [35712/50048]	Loss: 1.3977
Training Epoch: 13 [35840/50048]	Loss: 1.7216
Training Epoch: 13 [35968/50048]	Loss: 1.3693
Training Epoch: 13 [36096/50048]	Loss: 1.2380
Training Epoch: 13 [36224/50048]	Loss: 1.3379
Training Epoch: 13 [36352/50048]	Loss: 1.3130
Training Epoch: 13 [36480/50048]	Loss: 1.6130
Training Epoch: 13 [36608/50048]	Loss: 1.1799
Training Epoch: 13 [36736/50048]	Loss: 1.2143
Training Epoch: 13 [36864/50048]	Loss: 1.4579
Training Epoch: 13 [36992/50048]	Loss: 1.2201
Training Epoch: 13 [37120/50048]	Loss: 1.3424
Training Epoch: 13 [37248/50048]	Loss: 1.4218
Training Epoch: 13 [37376/50048]	Loss: 1.2346
Training Epoch: 13 [37504/50048]	Loss: 1.3517
Training Epoch: 13 [37632/50048]	Loss: 1.2983
Training Epoch: 13 [37760/50048]	Loss: 1.7328
Training Epoch: 13 [37888/50048]	Loss: 1.1865
Training Epoch: 13 [38016/50048]	Loss: 1.3734
Training Epoch: 13 [38144/50048]	Loss: 1.5235
Training Epoch: 13 [38272/50048]	Loss: 1.3630
Training Epoch: 13 [38400/50048]	Loss: 1.4121
Training Epoch: 13 [38528/50048]	Loss: 1.3588
Training Epoch: 13 [38656/50048]	Loss: 1.3496
Training Epoch: 13 [38784/50048]	Loss: 1.2368
Training Epoch: 13 [38912/50048]	Loss: 1.3279
Training Epoch: 13 [39040/50048]	Loss: 1.4364
Training Epoch: 13 [39168/50048]	Loss: 1.4570
Training Epoch: 13 [39296/50048]	Loss: 1.5125
Training Epoch: 13 [39424/50048]	Loss: 1.3366
Training Epoch: 13 [39552/50048]	Loss: 1.5077
Training Epoch: 13 [39680/50048]	Loss: 1.1171
Training Epoch: 13 [39808/50048]	Loss: 1.5314
Training Epoch: 13 [39936/50048]	Loss: 1.2521
Training Epoch: 13 [40064/50048]	Loss: 1.4294
Training Epoch: 13 [40192/50048]	Loss: 1.3115
Training Epoch: 13 [40320/50048]	Loss: 1.2114
Training Epoch: 13 [40448/50048]	Loss: 1.3719
Training Epoch: 13 [40576/50048]	Loss: 1.3212
Training Epoch: 13 [40704/50048]	Loss: 1.3045
Training Epoch: 13 [40832/50048]	Loss: 1.2207
Training Epoch: 13 [40960/50048]	Loss: 1.3545
Training Epoch: 13 [41088/50048]	Loss: 1.2949
Training Epoch: 13 [41216/50048]	Loss: 1.2474
Training Epoch: 13 [41344/50048]	Loss: 1.2907
Training Epoch: 13 [41472/50048]	Loss: 1.4756
Training Epoch: 13 [41600/50048]	Loss: 1.2605
Training Epoch: 13 [41728/50048]	Loss: 1.2675
Training Epoch: 13 [41856/50048]	Loss: 1.3663
Training Epoch: 13 [41984/50048]	Loss: 1.4310
Training Epoch: 13 [42112/50048]	Loss: 1.5476
Training Epoch: 13 [42240/50048]	Loss: 1.4867
Training Epoch: 13 [42368/50048]	Loss: 1.4789
Training Epoch: 13 [42496/50048]	Loss: 1.4942
Training Epoch: 13 [42624/50048]	Loss: 1.2252
Training Epoch: 13 [42752/50048]	Loss: 1.4135
Training Epoch: 13 [42880/50048]	Loss: 1.5385
Training Epoch: 13 [43008/50048]	Loss: 1.4615
Training Epoch: 13 [43136/50048]	Loss: 1.2870
Training Epoch: 13 [43264/50048]	Loss: 1.4401
Training Epoch: 13 [43392/50048]	Loss: 1.3620
Training Epoch: 13 [43520/50048]	Loss: 1.2144
Training Epoch: 13 [43648/50048]	Loss: 1.2732
Training Epoch: 13 [43776/50048]	Loss: 1.2410
Training Epoch: 13 [43904/50048]	Loss: 1.1893
Training Epoch: 13 [44032/50048]	Loss: 1.2446
Training Epoch: 13 [44160/50048]	Loss: 1.4434
Training Epoch: 13 [44288/50048]	Loss: 1.3852
Training Epoch: 13 [44416/50048]	Loss: 1.3553
Training Epoch: 13 [44544/50048]	Loss: 1.1741
Training Epoch: 13 [44672/50048]	Loss: 1.3542
Training Epoch: 13 [44800/50048]	Loss: 1.3602
Training Epoch: 13 [44928/50048]	Loss: 1.3908
Training Epoch: 13 [45056/50048]	Loss: 1.1092
Training Epoch: 13 [45184/50048]	Loss: 1.4681
Training Epoch: 13 [45312/50048]	Loss: 1.5810
Training Epoch: 13 [45440/50048]	Loss: 1.4373
Training Epoch: 13 [45568/50048]	Loss: 1.2341
Training Epoch: 13 [45696/50048]	Loss: 1.2643
Training Epoch: 13 [45824/50048]	Loss: 1.2167
Training Epoch: 13 [45952/50048]	Loss: 1.0850
Training Epoch: 13 [46080/50048]	Loss: 1.2923
Training Epoch: 13 [46208/50048]	Loss: 1.2176
Training Epoch: 13 [46336/50048]	Loss: 1.2545
Training Epoch: 13 [46464/50048]	Loss: 1.4691
Training Epoch: 13 [46592/50048]	Loss: 1.3152
Training Epoch: 13 [46720/50048]	Loss: 1.5934
Training Epoch: 13 [46848/50048]	Loss: 1.2590
Training Epoch: 13 [46976/50048]	Loss: 1.3877
Training Epoch: 13 [47104/50048]	Loss: 1.3313
Training Epoch: 13 [47232/50048]	Loss: 1.5351
Training Epoch: 13 [47360/50048]	Loss: 1.5012
Training Epoch: 13 [47488/50048]	Loss: 1.1259
Training Epoch: 13 [47616/50048]	Loss: 1.4502
Training Epoch: 13 [47744/50048]	Loss: 1.5617
Training Epoch: 13 [47872/50048]	Loss: 1.2703
Training Epoch: 13 [48000/50048]	Loss: 1.3154
Training Epoch: 13 [48128/50048]	Loss: 1.5923
Training Epoch: 13 [48256/50048]	Loss: 1.2676
Training Epoch: 13 [48384/50048]	Loss: 1.2137
Training Epoch: 13 [48512/50048]	Loss: 1.5781
Training Epoch: 13 [48640/50048]	Loss: 1.4199
Training Epoch: 13 [48768/50048]	Loss: 1.3422
Training Epoch: 13 [48896/50048]	Loss: 1.2909
Training Epoch: 13 [49024/50048]	Loss: 1.2801
Training Epoch: 13 [49152/50048]	Loss: 1.2497
Training Epoch: 13 [49280/50048]	Loss: 1.2383
Training Epoch: 13 [49408/50048]	Loss: 1.3723
Training Epoch: 13 [49536/50048]	Loss: 1.2374
Training Epoch: 13 [49664/50048]	Loss: 1.4856
Training Epoch: 13 [49792/50048]	Loss: 1.3603
Training Epoch: 13 [49920/50048]	Loss: 1.4478
Training Epoch: 13 [50048/50048]	Loss: 1.0458
Validation Epoch: 13, Average loss: 0.0119, Accuracy: 0.5799
Training Epoch: 14 [128/50048]	Loss: 1.4334
Training Epoch: 14 [256/50048]	Loss: 1.4055
Training Epoch: 14 [384/50048]	Loss: 1.4010
Training Epoch: 14 [512/50048]	Loss: 1.3337
Training Epoch: 14 [640/50048]	Loss: 1.3716
Training Epoch: 14 [768/50048]	Loss: 1.2673
Training Epoch: 14 [896/50048]	Loss: 1.1789
Training Epoch: 14 [1024/50048]	Loss: 1.3673
Training Epoch: 14 [1152/50048]	Loss: 1.6277
Training Epoch: 14 [1280/50048]	Loss: 1.3234
Training Epoch: 14 [1408/50048]	Loss: 1.3479
Training Epoch: 14 [1536/50048]	Loss: 1.2987
Training Epoch: 14 [1664/50048]	Loss: 1.4710
Training Epoch: 14 [1792/50048]	Loss: 1.2720
Training Epoch: 14 [1920/50048]	Loss: 1.4603
Training Epoch: 14 [2048/50048]	Loss: 1.4714
Training Epoch: 14 [2176/50048]	Loss: 1.3207
Training Epoch: 14 [2304/50048]	Loss: 1.1566
Training Epoch: 14 [2432/50048]	Loss: 1.3030
Training Epoch: 14 [2560/50048]	Loss: 1.2070
Training Epoch: 14 [2688/50048]	Loss: 1.4948
Training Epoch: 14 [2816/50048]	Loss: 1.1465
Training Epoch: 14 [2944/50048]	Loss: 1.3738
Training Epoch: 14 [3072/50048]	Loss: 1.3478
Training Epoch: 14 [3200/50048]	Loss: 1.3931
Training Epoch: 14 [3328/50048]	Loss: 1.2698
Training Epoch: 14 [3456/50048]	Loss: 1.3776
Training Epoch: 14 [3584/50048]	Loss: 1.1488
Training Epoch: 14 [3712/50048]	Loss: 1.2458
Training Epoch: 14 [3840/50048]	Loss: 1.2364
Training Epoch: 14 [3968/50048]	Loss: 1.2467
Training Epoch: 14 [4096/50048]	Loss: 1.5607
Training Epoch: 14 [4224/50048]	Loss: 1.4369
Training Epoch: 14 [4352/50048]	Loss: 1.3337
Training Epoch: 14 [4480/50048]	Loss: 1.4318
Training Epoch: 14 [4608/50048]	Loss: 1.3770
Training Epoch: 14 [4736/50048]	Loss: 1.1288
Training Epoch: 14 [4864/50048]	Loss: 1.3060
Training Epoch: 14 [4992/50048]	Loss: 1.0982
Training Epoch: 14 [5120/50048]	Loss: 1.3000
Training Epoch: 14 [5248/50048]	Loss: 1.3242
Training Epoch: 14 [5376/50048]	Loss: 1.3572
Training Epoch: 14 [5504/50048]	Loss: 1.3064
Training Epoch: 14 [5632/50048]	Loss: 1.3457
Training Epoch: 14 [5760/50048]	Loss: 1.3418
Training Epoch: 14 [5888/50048]	Loss: 1.3267
Training Epoch: 14 [6016/50048]	Loss: 1.3758
Training Epoch: 14 [6144/50048]	Loss: 1.1706
Training Epoch: 14 [6272/50048]	Loss: 1.4469
Training Epoch: 14 [6400/50048]	Loss: 1.2936
Training Epoch: 14 [6528/50048]	Loss: 1.8382
Training Epoch: 14 [6656/50048]	Loss: 1.4528
Training Epoch: 14 [6784/50048]	Loss: 1.1730
Training Epoch: 14 [6912/50048]	Loss: 1.4112
Training Epoch: 14 [7040/50048]	Loss: 1.2516
Training Epoch: 14 [7168/50048]	Loss: 1.3240
Training Epoch: 14 [7296/50048]	Loss: 1.2424
Training Epoch: 14 [7424/50048]	Loss: 1.4057
Training Epoch: 14 [7552/50048]	Loss: 1.3803
Training Epoch: 14 [7680/50048]	Loss: 1.1543
Training Epoch: 14 [7808/50048]	Loss: 1.1417
Training Epoch: 14 [7936/50048]	Loss: 1.5322
Training Epoch: 14 [8064/50048]	Loss: 1.5504
Training Epoch: 14 [8192/50048]	Loss: 1.2738
Training Epoch: 14 [8320/50048]	Loss: 1.5594
Training Epoch: 14 [8448/50048]	Loss: 1.3873
Training Epoch: 14 [8576/50048]	Loss: 1.3864
Training Epoch: 14 [8704/50048]	Loss: 1.3791
Training Epoch: 14 [8832/50048]	Loss: 1.1279
Training Epoch: 14 [8960/50048]	Loss: 1.1115
Training Epoch: 14 [9088/50048]	Loss: 1.2064
Training Epoch: 14 [9216/50048]	Loss: 1.2608
Training Epoch: 14 [9344/50048]	Loss: 1.3655
Training Epoch: 14 [9472/50048]	Loss: 1.2338
Training Epoch: 14 [9600/50048]	Loss: 1.5513
Training Epoch: 14 [9728/50048]	Loss: 1.4853
Training Epoch: 14 [9856/50048]	Loss: 1.2594
Training Epoch: 14 [9984/50048]	Loss: 1.3190
Training Epoch: 14 [10112/50048]	Loss: 1.2870
Training Epoch: 14 [10240/50048]	Loss: 1.1787
Training Epoch: 14 [10368/50048]	Loss: 1.3472
Training Epoch: 14 [10496/50048]	Loss: 1.1607
Training Epoch: 14 [10624/50048]	Loss: 1.2907
Training Epoch: 14 [10752/50048]	Loss: 1.3281
Training Epoch: 14 [10880/50048]	Loss: 1.4530
Training Epoch: 14 [11008/50048]	Loss: 1.4753
Training Epoch: 14 [11136/50048]	Loss: 1.0345
Training Epoch: 14 [11264/50048]	Loss: 1.2752
Training Epoch: 14 [11392/50048]	Loss: 1.2012
Training Epoch: 14 [11520/50048]	Loss: 1.3480
Training Epoch: 14 [11648/50048]	Loss: 1.3142
Training Epoch: 14 [11776/50048]	Loss: 1.1874
Training Epoch: 14 [11904/50048]	Loss: 1.3514
Training Epoch: 14 [12032/50048]	Loss: 1.6071
Training Epoch: 14 [12160/50048]	Loss: 1.3423
Training Epoch: 14 [12288/50048]	Loss: 1.2863
Training Epoch: 14 [12416/50048]	Loss: 1.2578
Training Epoch: 14 [12544/50048]	Loss: 1.3458
Training Epoch: 14 [12672/50048]	Loss: 1.1558
Training Epoch: 14 [12800/50048]	Loss: 1.1664
Training Epoch: 14 [12928/50048]	Loss: 1.1028
Training Epoch: 14 [13056/50048]	Loss: 1.3942
Training Epoch: 14 [13184/50048]	Loss: 1.2830
Training Epoch: 14 [13312/50048]	Loss: 1.1917
Training Epoch: 14 [13440/50048]	Loss: 1.4328
Training Epoch: 14 [13568/50048]	Loss: 1.3280
Training Epoch: 14 [13696/50048]	Loss: 1.2596
Training Epoch: 14 [13824/50048]	Loss: 1.1586
Training Epoch: 14 [13952/50048]	Loss: 1.3965
Training Epoch: 14 [14080/50048]	Loss: 1.6139
Training Epoch: 14 [14208/50048]	Loss: 1.0795
Training Epoch: 14 [14336/50048]	Loss: 1.4284
Training Epoch: 14 [14464/50048]	Loss: 1.3938
Training Epoch: 14 [14592/50048]	Loss: 1.1762
Training Epoch: 14 [14720/50048]	Loss: 1.1895
Training Epoch: 14 [14848/50048]	Loss: 1.3792
Training Epoch: 14 [14976/50048]	Loss: 1.3042
Training Epoch: 14 [15104/50048]	Loss: 1.4072
Training Epoch: 14 [15232/50048]	Loss: 1.2394
Training Epoch: 14 [15360/50048]	Loss: 1.2492
Training Epoch: 14 [15488/50048]	Loss: 1.5033
Training Epoch: 14 [15616/50048]	Loss: 1.1584
Training Epoch: 14 [15744/50048]	Loss: 1.4121
Training Epoch: 14 [15872/50048]	Loss: 1.1502
Training Epoch: 14 [16000/50048]	Loss: 1.5310
Training Epoch: 14 [16128/50048]	Loss: 1.3847
Training Epoch: 14 [16256/50048]	Loss: 1.2220
Training Epoch: 14 [16384/50048]	Loss: 1.4407
Training Epoch: 14 [16512/50048]	Loss: 1.3968
Training Epoch: 14 [16640/50048]	Loss: 1.6161
Training Epoch: 14 [16768/50048]	Loss: 1.1523
Training Epoch: 14 [16896/50048]	Loss: 1.3510
Training Epoch: 14 [17024/50048]	Loss: 1.3329
Training Epoch: 14 [17152/50048]	Loss: 1.1931
Training Epoch: 14 [17280/50048]	Loss: 1.3174
Training Epoch: 14 [17408/50048]	Loss: 1.1816
Training Epoch: 14 [17536/50048]	Loss: 1.4034
Training Epoch: 14 [17664/50048]	Loss: 1.4742
Training Epoch: 14 [17792/50048]	Loss: 1.3268
Training Epoch: 14 [17920/50048]	Loss: 1.3462
Training Epoch: 14 [18048/50048]	Loss: 1.4020
Training Epoch: 14 [18176/50048]	Loss: 1.3487
Training Epoch: 14 [18304/50048]	Loss: 1.2669
Training Epoch: 14 [18432/50048]	Loss: 1.3357
Training Epoch: 14 [18560/50048]	Loss: 1.2323
Training Epoch: 14 [18688/50048]	Loss: 1.1176
Training Epoch: 14 [18816/50048]	Loss: 1.1527
Training Epoch: 14 [18944/50048]	Loss: 1.5438
Training Epoch: 14 [19072/50048]	Loss: 1.3751
Training Epoch: 14 [19200/50048]	Loss: 1.3623
Training Epoch: 14 [19328/50048]	Loss: 1.2654
Training Epoch: 14 [19456/50048]	Loss: 1.2856
Training Epoch: 14 [19584/50048]	Loss: 1.3388
Training Epoch: 14 [19712/50048]	Loss: 1.2254
Training Epoch: 14 [19840/50048]	Loss: 1.1401
Training Epoch: 14 [19968/50048]	Loss: 1.2903
Training Epoch: 14 [20096/50048]	Loss: 1.2594
Training Epoch: 14 [20224/50048]	Loss: 1.2230
Training Epoch: 14 [20352/50048]	Loss: 1.0438
Training Epoch: 14 [20480/50048]	Loss: 1.2562
Training Epoch: 14 [20608/50048]	Loss: 1.4138
Training Epoch: 14 [20736/50048]	Loss: 1.4616
Training Epoch: 14 [20864/50048]	Loss: 1.4617
Training Epoch: 14 [20992/50048]	Loss: 1.2521
Training Epoch: 14 [21120/50048]	Loss: 1.0518
Training Epoch: 14 [21248/50048]	Loss: 1.3363
Training Epoch: 14 [21376/50048]	Loss: 1.3969
Training Epoch: 14 [21504/50048]	Loss: 1.2013
Training Epoch: 14 [21632/50048]	Loss: 1.3046
Training Epoch: 14 [21760/50048]	Loss: 1.4269
Training Epoch: 14 [21888/50048]	Loss: 1.1546
Training Epoch: 14 [22016/50048]	Loss: 1.3731
Training Epoch: 14 [22144/50048]	Loss: 1.2802
Training Epoch: 14 [22272/50048]	Loss: 1.3759
Training Epoch: 14 [22400/50048]	Loss: 1.1453
Training Epoch: 14 [22528/50048]	Loss: 1.0755
Training Epoch: 14 [22656/50048]	Loss: 1.3389
Training Epoch: 14 [22784/50048]	Loss: 1.3735
Training Epoch: 14 [22912/50048]	Loss: 1.1598
Training Epoch: 14 [23040/50048]	Loss: 1.4908
Training Epoch: 14 [23168/50048]	Loss: 1.4623
Training Epoch: 14 [23296/50048]	Loss: 1.0756
Training Epoch: 14 [23424/50048]	Loss: 1.2717
Training Epoch: 14 [23552/50048]	Loss: 1.5806
Training Epoch: 14 [23680/50048]	Loss: 1.4049
Training Epoch: 14 [23808/50048]	Loss: 1.4870
Training Epoch: 14 [23936/50048]	Loss: 1.4044
Training Epoch: 14 [24064/50048]	Loss: 1.4806
Training Epoch: 14 [24192/50048]	Loss: 1.3848
Training Epoch: 14 [24320/50048]	Loss: 1.0525
Training Epoch: 14 [24448/50048]	Loss: 1.4040
Training Epoch: 14 [24576/50048]	Loss: 1.4314
Training Epoch: 14 [24704/50048]	Loss: 1.2088
Training Epoch: 14 [24832/50048]	Loss: 1.2982
Training Epoch: 14 [24960/50048]	Loss: 1.2792
Training Epoch: 14 [25088/50048]	Loss: 1.0747
Training Epoch: 14 [25216/50048]	Loss: 1.4501
Training Epoch: 14 [25344/50048]	Loss: 1.1910
Training Epoch: 14 [25472/50048]	Loss: 1.2913
Training Epoch: 14 [25600/50048]	Loss: 0.9594
Training Epoch: 14 [25728/50048]	Loss: 1.3831
Training Epoch: 14 [25856/50048]	Loss: 1.4954
Training Epoch: 14 [25984/50048]	Loss: 1.1998
Training Epoch: 14 [26112/50048]	Loss: 1.1695
Training Epoch: 14 [26240/50048]	Loss: 1.5174
Training Epoch: 14 [26368/50048]	Loss: 1.4586
Training Epoch: 14 [26496/50048]	Loss: 1.3266
Training Epoch: 14 [26624/50048]	Loss: 1.0454
Training Epoch: 14 [26752/50048]	Loss: 1.3448
Training Epoch: 14 [26880/50048]	Loss: 1.1158
Training Epoch: 14 [27008/50048]	Loss: 1.3470
Training Epoch: 14 [27136/50048]	Loss: 1.2727
Training Epoch: 14 [27264/50048]	Loss: 1.2692
Training Epoch: 14 [27392/50048]	Loss: 1.1634
Training Epoch: 14 [27520/50048]	Loss: 1.3360
Training Epoch: 14 [27648/50048]	Loss: 1.3383
Training Epoch: 14 [27776/50048]	Loss: 1.2075
Training Epoch: 14 [27904/50048]	Loss: 1.3424
Training Epoch: 14 [28032/50048]	Loss: 1.4580
Training Epoch: 14 [28160/50048]	Loss: 1.3006
Training Epoch: 14 [28288/50048]	Loss: 1.2856
Training Epoch: 14 [28416/50048]	Loss: 1.4838
Training Epoch: 14 [28544/50048]	Loss: 1.3288
Training Epoch: 14 [28672/50048]	Loss: 1.3224
Training Epoch: 14 [28800/50048]	Loss: 1.1915
Training Epoch: 14 [28928/50048]	Loss: 1.1870
Training Epoch: 14 [29056/50048]	Loss: 1.4597
Training Epoch: 14 [29184/50048]	Loss: 1.5624
Training Epoch: 14 [29312/50048]	Loss: 1.3830
Training Epoch: 14 [29440/50048]	Loss: 1.2792
Training Epoch: 14 [29568/50048]	Loss: 1.4150
Training Epoch: 14 [29696/50048]	Loss: 1.3037
Training Epoch: 14 [29824/50048]	Loss: 1.2825
Training Epoch: 14 [29952/50048]	Loss: 1.2734
Training Epoch: 14 [30080/50048]	Loss: 1.5670
Training Epoch: 14 [30208/50048]	Loss: 1.3251
Training Epoch: 14 [30336/50048]	Loss: 1.3713
Training Epoch: 14 [30464/50048]	Loss: 1.3964
Training Epoch: 14 [30592/50048]	Loss: 1.3077
Training Epoch: 14 [30720/50048]	Loss: 1.3774
Training Epoch: 14 [30848/50048]	Loss: 1.2497
Training Epoch: 14 [30976/50048]	Loss: 1.2842
Training Epoch: 14 [31104/50048]	Loss: 1.2700
Training Epoch: 14 [31232/50048]	Loss: 1.3073
Training Epoch: 14 [31360/50048]	Loss: 1.1370
Training Epoch: 14 [31488/50048]	Loss: 1.1358
Training Epoch: 14 [31616/50048]	Loss: 1.2458
Training Epoch: 14 [31744/50048]	Loss: 1.3811
Training Epoch: 14 [31872/50048]	Loss: 1.2524
Training Epoch: 14 [32000/50048]	Loss: 1.1558
Training Epoch: 14 [32128/50048]	Loss: 1.4073
Training Epoch: 14 [32256/50048]	Loss: 1.4427
Training Epoch: 14 [32384/50048]	Loss: 1.2163
Training Epoch: 14 [32512/50048]	Loss: 1.2786
Training Epoch: 14 [32640/50048]	Loss: 1.6232
Training Epoch: 14 [32768/50048]	Loss: 1.3122
Training Epoch: 14 [32896/50048]	Loss: 1.4498
Training Epoch: 14 [33024/50048]	Loss: 1.1758
Training Epoch: 14 [33152/50048]	Loss: 1.4564
Training Epoch: 14 [33280/50048]	Loss: 1.4613
Training Epoch: 14 [33408/50048]	Loss: 1.2657
Training Epoch: 14 [33536/50048]	Loss: 1.2139
Training Epoch: 14 [33664/50048]	Loss: 1.3634
Training Epoch: 14 [33792/50048]	Loss: 1.3930
Training Epoch: 14 [33920/50048]	Loss: 1.0578
Training Epoch: 14 [34048/50048]	Loss: 1.3705
Training Epoch: 14 [34176/50048]	Loss: 1.3682
Training Epoch: 14 [34304/50048]	Loss: 1.3999
Training Epoch: 14 [34432/50048]	Loss: 1.2613
Training Epoch: 14 [34560/50048]	Loss: 1.3849
Training Epoch: 14 [34688/50048]	Loss: 1.3521
Training Epoch: 14 [34816/50048]	Loss: 1.1020
Training Epoch: 14 [34944/50048]	Loss: 1.2561
Training Epoch: 14 [35072/50048]	Loss: 1.1231
Training Epoch: 14 [35200/50048]	Loss: 1.2913
Training Epoch: 14 [35328/50048]	Loss: 1.0308
Training Epoch: 14 [35456/50048]	Loss: 1.0869
Training Epoch: 14 [35584/50048]	Loss: 1.2718
Training Epoch: 14 [35712/50048]	Loss: 1.4025
Training Epoch: 14 [35840/50048]	Loss: 1.5441
Training Epoch: 14 [35968/50048]	Loss: 1.4582
Training Epoch: 14 [36096/50048]	Loss: 1.2394
Training Epoch: 14 [36224/50048]	Loss: 1.3985
Training Epoch: 14 [36352/50048]	Loss: 1.5553
Training Epoch: 14 [36480/50048]	Loss: 1.4628
Training Epoch: 14 [36608/50048]	Loss: 1.4311
Training Epoch: 14 [36736/50048]	Loss: 1.2884
Training Epoch: 14 [36864/50048]	Loss: 1.3138
Training Epoch: 14 [36992/50048]	Loss: 1.2594
Training Epoch: 14 [37120/50048]	Loss: 1.3613
Training Epoch: 14 [37248/50048]	Loss: 1.2864
Training Epoch: 14 [37376/50048]	Loss: 1.1348
Training Epoch: 14 [37504/50048]	Loss: 1.3332
Training Epoch: 14 [37632/50048]	Loss: 1.4695
Training Epoch: 14 [37760/50048]	Loss: 1.4336
Training Epoch: 14 [37888/50048]	Loss: 1.2532
Training Epoch: 14 [38016/50048]	Loss: 1.3711
Training Epoch: 14 [38144/50048]	Loss: 1.0875
Training Epoch: 14 [38272/50048]	Loss: 1.2889
Training Epoch: 14 [38400/50048]	Loss: 1.4787
Training Epoch: 14 [38528/50048]	Loss: 1.3805
Training Epoch: 14 [38656/50048]	Loss: 1.1703
Training Epoch: 14 [38784/50048]	Loss: 1.1498
Training Epoch: 14 [38912/50048]	Loss: 1.2798
Training Epoch: 14 [39040/50048]	Loss: 1.2813
Training Epoch: 14 [39168/50048]	Loss: 1.2285
Training Epoch: 14 [39296/50048]	Loss: 1.3018
Training Epoch: 14 [39424/50048]	Loss: 1.2003
Training Epoch: 14 [39552/50048]	Loss: 1.3867
Training Epoch: 14 [39680/50048]	Loss: 1.4659
Training Epoch: 14 [39808/50048]	Loss: 1.4691
Training Epoch: 14 [39936/50048]	Loss: 1.1794
Training Epoch: 14 [40064/50048]	Loss: 1.2424
Training Epoch: 14 [40192/50048]	Loss: 1.2426
Training Epoch: 14 [40320/50048]	Loss: 1.2365
Training Epoch: 14 [40448/50048]	Loss: 1.3227
Training Epoch: 14 [40576/50048]	Loss: 1.1905
Training Epoch: 14 [40704/50048]	Loss: 1.3702
Training Epoch: 14 [40832/50048]	Loss: 1.5671
Training Epoch: 14 [40960/50048]	Loss: 1.0641
Training Epoch: 14 [41088/50048]	Loss: 1.2606
Training Epoch: 14 [41216/50048]	Loss: 1.3440
Training Epoch: 14 [41344/50048]	Loss: 1.3961
Training Epoch: 14 [41472/50048]	Loss: 1.5605
Training Epoch: 14 [41600/50048]	Loss: 1.5013
Training Epoch: 14 [41728/50048]	Loss: 1.5750
Training Epoch: 14 [41856/50048]	Loss: 1.5056
Training Epoch: 14 [41984/50048]	Loss: 1.3118
Training Epoch: 14 [42112/50048]	Loss: 1.5665
Training Epoch: 14 [42240/50048]	Loss: 1.4167
Training Epoch: 14 [42368/50048]	Loss: 1.4560
Training Epoch: 14 [42496/50048]	Loss: 1.3894
Training Epoch: 14 [42624/50048]	Loss: 1.3224
Training Epoch: 14 [42752/50048]	Loss: 1.2223
Training Epoch: 14 [42880/50048]	Loss: 1.0881
Training Epoch: 14 [43008/50048]	Loss: 1.1796
Training Epoch: 14 [43136/50048]	Loss: 1.4333
Training Epoch: 14 [43264/50048]	Loss: 1.4301
Training Epoch: 14 [43392/50048]	Loss: 1.2010
Training Epoch: 14 [43520/50048]	Loss: 1.1176
Training Epoch: 14 [43648/50048]	Loss: 1.1980
Training Epoch: 14 [43776/50048]	Loss: 1.3981
Training Epoch: 14 [43904/50048]	Loss: 1.3051
Training Epoch: 14 [44032/50048]	Loss: 1.5077
Training Epoch: 14 [44160/50048]	Loss: 1.3706
Training Epoch: 14 [44288/50048]	Loss: 1.4531
Training Epoch: 14 [44416/50048]	Loss: 1.3087
Training Epoch: 14 [44544/50048]	Loss: 1.1912
Training Epoch: 14 [44672/50048]	Loss: 1.4933
Training Epoch: 14 [44800/50048]	Loss: 1.4246
Training Epoch: 14 [44928/50048]	Loss: 1.3877
Training Epoch: 14 [45056/50048]	Loss: 1.2629
Training Epoch: 14 [45184/50048]	Loss: 1.5154
Training Epoch: 14 [45312/50048]	Loss: 1.1294
Training Epoch: 14 [45440/50048]	Loss: 1.2886
Training Epoch: 14 [45568/50048]	Loss: 1.4033
Training Epoch: 14 [45696/50048]	Loss: 1.3063
Training Epoch: 14 [45824/50048]	Loss: 1.3089
Training Epoch: 14 [45952/50048]	Loss: 1.5442
Training Epoch: 14 [46080/50048]	Loss: 1.3412
Training Epoch: 14 [46208/50048]	Loss: 1.4266
Training Epoch: 14 [46336/50048]	Loss: 1.2664
Training Epoch: 14 [46464/50048]	Loss: 1.4528
Training Epoch: 14 [46592/50048]	Loss: 1.5251
Training Epoch: 14 [46720/50048]	Loss: 1.1751
Training Epoch: 14 [46848/50048]	Loss: 1.3330
Training Epoch: 14 [46976/50048]	Loss: 1.3297
Training Epoch: 14 [47104/50048]	Loss: 1.3338
Training Epoch: 14 [47232/50048]	Loss: 1.4051
Training Epoch: 14 [47360/50048]	Loss: 1.1308
Training Epoch: 14 [47488/50048]	Loss: 1.5686
Training Epoch: 14 [47616/50048]	Loss: 1.2859
Training Epoch: 14 [47744/50048]	Loss: 1.4220
Training Epoch: 14 [47872/50048]	Loss: 1.1327
Training Epoch: 14 [48000/50048]	Loss: 1.4359
Training Epoch: 14 [48128/50048]	Loss: 1.2111
Training Epoch: 14 [48256/50048]	Loss: 1.3033
Training Epoch: 14 [48384/50048]	Loss: 1.6607
Training Epoch: 14 [48512/50048]	Loss: 1.2909
Training Epoch: 14 [48640/50048]	Loss: 1.3843
Training Epoch: 14 [48768/50048]	Loss: 1.3271
Training Epoch: 14 [48896/50048]	Loss: 1.3803
Training Epoch: 14 [49024/50048]	Loss: 1.3286
Training Epoch: 14 [49152/50048]	Loss: 1.4285
Training Epoch: 14 [49280/50048]	Loss: 1.1559
Training Epoch: 14 [49408/50048]	Loss: 1.3483
Training Epoch: 14 [49536/50048]	Loss: 1.5409
Training Epoch: 14 [49664/50048]	Loss: 1.3184
Training Epoch: 14 [49792/50048]	Loss: 1.1266
Training Epoch: 14 [49920/50048]	Loss: 1.3610
Training Epoch: 14 [50048/50048]	Loss: 1.3321
Validation Epoch: 14, Average loss: 0.0117, Accuracy: 0.5814
Training Epoch: 15 [128/50048]	Loss: 1.0549
Training Epoch: 15 [256/50048]	Loss: 1.4874
Training Epoch: 15 [384/50048]	Loss: 1.3296
Training Epoch: 15 [512/50048]	Loss: 1.3671
Training Epoch: 15 [640/50048]	Loss: 1.1739
Training Epoch: 15 [768/50048]	Loss: 1.3905
Training Epoch: 15 [896/50048]	Loss: 1.1011
Training Epoch: 15 [1024/50048]	Loss: 1.3400
Training Epoch: 15 [1152/50048]	Loss: 1.4728
Training Epoch: 15 [1280/50048]	Loss: 1.2278
Training Epoch: 15 [1408/50048]	Loss: 1.2838
Training Epoch: 15 [1536/50048]	Loss: 1.1848
Training Epoch: 15 [1664/50048]	Loss: 1.1431
Training Epoch: 15 [1792/50048]	Loss: 1.0289
Training Epoch: 15 [1920/50048]	Loss: 1.2344
Training Epoch: 15 [2048/50048]	Loss: 1.3914
Training Epoch: 15 [2176/50048]	Loss: 1.1952
Training Epoch: 15 [2304/50048]	Loss: 1.2150
Training Epoch: 15 [2432/50048]	Loss: 1.2527
Training Epoch: 15 [2560/50048]	Loss: 1.2625
Training Epoch: 15 [2688/50048]	Loss: 1.1267
Training Epoch: 15 [2816/50048]	Loss: 1.1738
Training Epoch: 15 [2944/50048]	Loss: 1.2701
Training Epoch: 15 [3072/50048]	Loss: 1.3042
Training Epoch: 15 [3200/50048]	Loss: 1.5372
Training Epoch: 15 [3328/50048]	Loss: 1.2659
Training Epoch: 15 [3456/50048]	Loss: 1.0772
Training Epoch: 15 [3584/50048]	Loss: 1.1688
Training Epoch: 15 [3712/50048]	Loss: 1.2813
Training Epoch: 15 [3840/50048]	Loss: 1.2057
Training Epoch: 15 [3968/50048]	Loss: 1.2811
Training Epoch: 15 [4096/50048]	Loss: 1.2157
Training Epoch: 15 [4224/50048]	Loss: 1.4287
Training Epoch: 15 [4352/50048]	Loss: 1.3982
Training Epoch: 15 [4480/50048]	Loss: 1.5056
Training Epoch: 15 [4608/50048]	Loss: 1.2734
Training Epoch: 15 [4736/50048]	Loss: 1.2568
Training Epoch: 15 [4864/50048]	Loss: 1.4702
Training Epoch: 15 [4992/50048]	Loss: 1.3062
Training Epoch: 15 [5120/50048]	Loss: 1.4476
Training Epoch: 15 [5248/50048]	Loss: 1.0227
Training Epoch: 15 [5376/50048]	Loss: 1.2644
Training Epoch: 15 [5504/50048]	Loss: 1.3849
Training Epoch: 15 [5632/50048]	Loss: 1.2199
Training Epoch: 15 [5760/50048]	Loss: 1.1455
Training Epoch: 15 [5888/50048]	Loss: 1.3042
Training Epoch: 15 [6016/50048]	Loss: 1.1464
Training Epoch: 15 [6144/50048]	Loss: 1.4024
Training Epoch: 15 [6272/50048]	Loss: 1.2824
Training Epoch: 15 [6400/50048]	Loss: 1.0351
Training Epoch: 15 [6528/50048]	Loss: 1.2886
Training Epoch: 15 [6656/50048]	Loss: 1.1236
Training Epoch: 15 [6784/50048]	Loss: 1.2957
Training Epoch: 15 [6912/50048]	Loss: 1.2443
Training Epoch: 15 [7040/50048]	Loss: 1.3120
Training Epoch: 15 [7168/50048]	Loss: 1.1420
Training Epoch: 15 [7296/50048]	Loss: 1.2812
Training Epoch: 15 [7424/50048]	Loss: 1.1292
Training Epoch: 15 [7552/50048]	Loss: 1.3292
Training Epoch: 15 [7680/50048]	Loss: 1.4022
Training Epoch: 15 [7808/50048]	Loss: 1.2990
Training Epoch: 15 [7936/50048]	Loss: 1.0024
Training Epoch: 15 [8064/50048]	Loss: 1.3891
Training Epoch: 15 [8192/50048]	Loss: 1.3980
Training Epoch: 15 [8320/50048]	Loss: 1.0423
Training Epoch: 15 [8448/50048]	Loss: 1.2980
Training Epoch: 15 [8576/50048]	Loss: 1.4199
Training Epoch: 15 [8704/50048]	Loss: 1.2430
Training Epoch: 15 [8832/50048]	Loss: 1.3581
Training Epoch: 15 [8960/50048]	Loss: 1.1404
Training Epoch: 15 [9088/50048]	Loss: 1.5244
Training Epoch: 15 [9216/50048]	Loss: 1.3412
Training Epoch: 15 [9344/50048]	Loss: 1.2086
Training Epoch: 15 [9472/50048]	Loss: 1.4946
Training Epoch: 15 [9600/50048]	Loss: 1.2736
Training Epoch: 15 [9728/50048]	Loss: 1.1698
Training Epoch: 15 [9856/50048]	Loss: 1.3369
Training Epoch: 15 [9984/50048]	Loss: 1.3801
Training Epoch: 15 [10112/50048]	Loss: 1.3228
Training Epoch: 15 [10240/50048]	Loss: 1.4465
Training Epoch: 15 [10368/50048]	Loss: 1.2496
Training Epoch: 15 [10496/50048]	Loss: 1.2866
Training Epoch: 15 [10624/50048]	Loss: 1.7838
Training Epoch: 15 [10752/50048]	Loss: 1.3047
Training Epoch: 15 [10880/50048]	Loss: 1.4397
Training Epoch: 15 [11008/50048]	Loss: 1.2388
Training Epoch: 15 [11136/50048]	Loss: 1.1745
Training Epoch: 15 [11264/50048]	Loss: 1.5576
Training Epoch: 15 [11392/50048]	Loss: 1.3660
Training Epoch: 15 [11520/50048]	Loss: 1.5884
Training Epoch: 15 [11648/50048]	Loss: 1.4131
Training Epoch: 15 [11776/50048]	Loss: 1.1967
Training Epoch: 15 [11904/50048]	Loss: 1.2123
Training Epoch: 15 [12032/50048]	Loss: 1.2422
Training Epoch: 15 [12160/50048]	Loss: 1.4838
Training Epoch: 15 [12288/50048]	Loss: 1.2809
Training Epoch: 15 [12416/50048]	Loss: 1.1499
Training Epoch: 15 [12544/50048]	Loss: 1.1538
Training Epoch: 15 [12672/50048]	Loss: 1.1274
Training Epoch: 15 [12800/50048]	Loss: 1.2931
Training Epoch: 15 [12928/50048]	Loss: 1.4467
Training Epoch: 15 [13056/50048]	Loss: 1.2489
Training Epoch: 15 [13184/50048]	Loss: 1.2057
Training Epoch: 15 [13312/50048]	Loss: 1.2374
Training Epoch: 15 [13440/50048]	Loss: 1.1784
Training Epoch: 15 [13568/50048]	Loss: 1.1088
Training Epoch: 15 [13696/50048]	Loss: 1.3457
Training Epoch: 15 [13824/50048]	Loss: 1.4874
Training Epoch: 15 [13952/50048]	Loss: 1.1509
Training Epoch: 15 [14080/50048]	Loss: 1.3129
Training Epoch: 15 [14208/50048]	Loss: 1.4384
Training Epoch: 15 [14336/50048]	Loss: 1.1358
Training Epoch: 15 [14464/50048]	Loss: 1.5812
Training Epoch: 15 [14592/50048]	Loss: 1.4076
Training Epoch: 15 [14720/50048]	Loss: 1.2404
Training Epoch: 15 [14848/50048]	Loss: 1.1773
Training Epoch: 15 [14976/50048]	Loss: 1.3677
Training Epoch: 15 [15104/50048]	Loss: 1.0734
Training Epoch: 15 [15232/50048]	Loss: 1.3619
Training Epoch: 15 [15360/50048]	Loss: 1.6520
Training Epoch: 15 [15488/50048]	Loss: 1.3670
Training Epoch: 15 [15616/50048]	Loss: 1.3608
Training Epoch: 15 [15744/50048]	Loss: 1.2477
Training Epoch: 15 [15872/50048]	Loss: 1.2830
Training Epoch: 15 [16000/50048]	Loss: 1.2716
Training Epoch: 15 [16128/50048]	Loss: 1.0973
Training Epoch: 15 [16256/50048]	Loss: 1.1973
Training Epoch: 15 [16384/50048]	Loss: 1.3484
Training Epoch: 15 [16512/50048]	Loss: 1.3316
Training Epoch: 15 [16640/50048]	Loss: 1.1338
Training Epoch: 15 [16768/50048]	Loss: 1.1080
Training Epoch: 15 [16896/50048]	Loss: 1.2385
Training Epoch: 15 [17024/50048]	Loss: 1.4722
Training Epoch: 15 [17152/50048]	Loss: 1.2076
Training Epoch: 15 [17280/50048]	Loss: 1.4009
Training Epoch: 15 [17408/50048]	Loss: 1.4779
Training Epoch: 15 [17536/50048]	Loss: 1.4821
Training Epoch: 15 [17664/50048]	Loss: 1.0672
Training Epoch: 15 [17792/50048]	Loss: 1.1240
Training Epoch: 15 [17920/50048]	Loss: 1.5825
Training Epoch: 15 [18048/50048]	Loss: 1.2527
Training Epoch: 15 [18176/50048]	Loss: 1.2727
Training Epoch: 15 [18304/50048]	Loss: 1.3305
Training Epoch: 15 [18432/50048]	Loss: 1.4815
Training Epoch: 15 [18560/50048]	Loss: 1.2446
Training Epoch: 15 [18688/50048]	Loss: 1.4522
Training Epoch: 15 [18816/50048]	Loss: 1.1847
Training Epoch: 15 [18944/50048]	Loss: 1.1207
Training Epoch: 15 [19072/50048]	Loss: 1.3533
Training Epoch: 15 [19200/50048]	Loss: 1.6580
Training Epoch: 15 [19328/50048]	Loss: 1.3600
Training Epoch: 15 [19456/50048]	Loss: 1.2963
Training Epoch: 15 [19584/50048]	Loss: 1.1337
Training Epoch: 15 [19712/50048]	Loss: 1.4755
Training Epoch: 15 [19840/50048]	Loss: 1.3721
Training Epoch: 15 [19968/50048]	Loss: 1.0420
Training Epoch: 15 [20096/50048]	Loss: 1.2686
Training Epoch: 15 [20224/50048]	Loss: 1.1407
Training Epoch: 15 [20352/50048]	Loss: 1.3462
Training Epoch: 15 [20480/50048]	Loss: 1.3185
Training Epoch: 15 [20608/50048]	Loss: 1.2531
Training Epoch: 15 [20736/50048]	Loss: 1.3829
Training Epoch: 15 [20864/50048]	Loss: 1.3904
Training Epoch: 15 [20992/50048]	Loss: 1.0469
Training Epoch: 15 [21120/50048]	Loss: 1.3669
Training Epoch: 15 [21248/50048]	Loss: 1.3595
Training Epoch: 15 [21376/50048]	Loss: 1.4894
Training Epoch: 15 [21504/50048]	Loss: 1.2152
Training Epoch: 15 [21632/50048]	Loss: 1.1477
Training Epoch: 15 [21760/50048]	Loss: 1.2205
Training Epoch: 15 [21888/50048]	Loss: 1.3626
Training Epoch: 15 [22016/50048]	Loss: 1.3610
Training Epoch: 15 [22144/50048]	Loss: 1.2647
Training Epoch: 15 [22272/50048]	Loss: 1.0840
Training Epoch: 15 [22400/50048]	Loss: 1.2816
Training Epoch: 15 [22528/50048]	Loss: 1.1966
Training Epoch: 15 [22656/50048]	Loss: 1.3316
Training Epoch: 15 [22784/50048]	Loss: 1.3228
Training Epoch: 15 [22912/50048]	Loss: 1.3375
Training Epoch: 15 [23040/50048]	Loss: 1.1775
Training Epoch: 15 [23168/50048]	Loss: 1.3081
Training Epoch: 15 [23296/50048]	Loss: 1.2171
Training Epoch: 15 [23424/50048]	Loss: 1.3332
Training Epoch: 15 [23552/50048]	Loss: 1.1663
Training Epoch: 15 [23680/50048]	Loss: 1.2960
Training Epoch: 15 [23808/50048]	Loss: 1.2992
Training Epoch: 15 [23936/50048]	Loss: 1.4356
Training Epoch: 15 [24064/50048]	Loss: 1.5224
Training Epoch: 15 [24192/50048]	Loss: 1.4239
Training Epoch: 15 [24320/50048]	Loss: 1.4031
Training Epoch: 15 [24448/50048]	Loss: 1.6615
Training Epoch: 15 [24576/50048]	Loss: 1.3580
Training Epoch: 15 [24704/50048]	Loss: 1.3296
Training Epoch: 15 [24832/50048]	Loss: 1.1142
Training Epoch: 15 [24960/50048]	Loss: 1.3517
Training Epoch: 15 [25088/50048]	Loss: 1.3166
Training Epoch: 15 [25216/50048]	Loss: 1.1959
Training Epoch: 15 [25344/50048]	Loss: 1.1809
Training Epoch: 15 [25472/50048]	Loss: 1.2265
Training Epoch: 15 [25600/50048]	Loss: 1.0535
Training Epoch: 15 [25728/50048]	Loss: 1.3784
Training Epoch: 15 [25856/50048]	Loss: 1.3989
Training Epoch: 15 [25984/50048]	Loss: 1.3333
Training Epoch: 15 [26112/50048]	Loss: 1.1994
Training Epoch: 15 [26240/50048]	Loss: 1.3186
Training Epoch: 15 [26368/50048]	Loss: 1.3753
Training Epoch: 15 [26496/50048]	Loss: 1.4399
Training Epoch: 15 [26624/50048]	Loss: 1.3497
Training Epoch: 15 [26752/50048]	Loss: 1.2375
Training Epoch: 15 [26880/50048]	Loss: 1.2918
Training Epoch: 15 [27008/50048]	Loss: 1.5055
Training Epoch: 15 [27136/50048]	Loss: 1.1307
Training Epoch: 15 [27264/50048]	Loss: 1.2576
Training Epoch: 15 [27392/50048]	Loss: 1.3638
Training Epoch: 15 [27520/50048]	Loss: 1.3869
Training Epoch: 15 [27648/50048]	Loss: 1.3540
Training Epoch: 15 [27776/50048]	Loss: 1.2053
Training Epoch: 15 [27904/50048]	Loss: 1.2863
Training Epoch: 15 [28032/50048]	Loss: 1.5443
Training Epoch: 15 [28160/50048]	Loss: 1.6489
Training Epoch: 15 [28288/50048]	Loss: 1.2698
Training Epoch: 15 [28416/50048]	Loss: 1.3622
Training Epoch: 15 [28544/50048]	Loss: 1.2108
Training Epoch: 15 [28672/50048]	Loss: 1.4561
Training Epoch: 15 [28800/50048]	Loss: 1.3079
Training Epoch: 15 [28928/50048]	Loss: 1.4439
Training Epoch: 15 [29056/50048]	Loss: 1.2430
Training Epoch: 15 [29184/50048]	Loss: 1.3744
Training Epoch: 15 [29312/50048]	Loss: 1.2472
Training Epoch: 15 [29440/50048]	Loss: 1.3250
Training Epoch: 15 [29568/50048]	Loss: 1.2153
Training Epoch: 15 [29696/50048]	Loss: 1.1402
Training Epoch: 15 [29824/50048]	Loss: 1.1843
Training Epoch: 15 [29952/50048]	Loss: 1.3580
Training Epoch: 15 [30080/50048]	Loss: 1.2186
Training Epoch: 15 [30208/50048]	Loss: 1.1178
Training Epoch: 15 [30336/50048]	Loss: 1.3734
Training Epoch: 15 [30464/50048]	Loss: 1.2490
Training Epoch: 15 [30592/50048]	Loss: 1.2153
Training Epoch: 15 [30720/50048]	Loss: 1.4357
Training Epoch: 15 [30848/50048]	Loss: 1.2386
Training Epoch: 15 [30976/50048]	Loss: 1.3285
Training Epoch: 15 [31104/50048]	Loss: 1.0935
Training Epoch: 15 [31232/50048]	Loss: 1.4035
Training Epoch: 15 [31360/50048]	Loss: 1.1875
Training Epoch: 15 [31488/50048]	Loss: 1.1576
Training Epoch: 15 [31616/50048]	Loss: 1.1495
Training Epoch: 15 [31744/50048]	Loss: 1.2475
Training Epoch: 15 [31872/50048]	Loss: 1.1723
Training Epoch: 15 [32000/50048]	Loss: 1.4615
Training Epoch: 15 [32128/50048]	Loss: 1.2523
Training Epoch: 15 [32256/50048]	Loss: 1.3074
Training Epoch: 15 [32384/50048]	Loss: 1.3849
Training Epoch: 15 [32512/50048]	Loss: 1.2149
Training Epoch: 15 [32640/50048]	Loss: 1.2470
Training Epoch: 15 [32768/50048]	Loss: 1.3198
Training Epoch: 15 [32896/50048]	Loss: 1.3358
Training Epoch: 15 [33024/50048]	Loss: 1.4032
Training Epoch: 15 [33152/50048]	Loss: 1.3431
Training Epoch: 15 [33280/50048]	Loss: 1.2962
Training Epoch: 15 [33408/50048]	Loss: 1.2636
Training Epoch: 15 [33536/50048]	Loss: 1.3522
Training Epoch: 15 [33664/50048]	Loss: 1.3074
Training Epoch: 15 [33792/50048]	Loss: 1.3813
Training Epoch: 15 [33920/50048]	Loss: 1.4742
Training Epoch: 15 [34048/50048]	Loss: 1.1542
Training Epoch: 15 [34176/50048]	Loss: 1.5437
Training Epoch: 15 [34304/50048]	Loss: 1.1073
Training Epoch: 15 [34432/50048]	Loss: 1.2474
Training Epoch: 15 [34560/50048]	Loss: 1.3443
Training Epoch: 15 [34688/50048]	Loss: 1.2857
Training Epoch: 15 [34816/50048]	Loss: 1.3392
Training Epoch: 15 [34944/50048]	Loss: 1.3701
Training Epoch: 15 [35072/50048]	Loss: 1.3754
Training Epoch: 15 [35200/50048]	Loss: 1.1862
Training Epoch: 15 [35328/50048]	Loss: 1.0910
Training Epoch: 15 [35456/50048]	Loss: 1.3585
Training Epoch: 15 [35584/50048]	Loss: 1.2934
Training Epoch: 15 [35712/50048]	Loss: 1.4562
Training Epoch: 15 [35840/50048]	Loss: 1.1354
Training Epoch: 15 [35968/50048]	Loss: 1.2449
Training Epoch: 15 [36096/50048]	Loss: 1.0528
Training Epoch: 15 [36224/50048]	Loss: 1.3211
Training Epoch: 15 [36352/50048]	Loss: 1.3477
Training Epoch: 15 [36480/50048]	Loss: 1.2322
Training Epoch: 15 [36608/50048]	Loss: 1.5395
Training Epoch: 15 [36736/50048]	Loss: 1.3814
Training Epoch: 15 [36864/50048]	Loss: 1.2378
Training Epoch: 15 [36992/50048]	Loss: 1.4367
Training Epoch: 15 [37120/50048]	Loss: 1.4137
Training Epoch: 15 [37248/50048]	Loss: 1.4500
Training Epoch: 15 [37376/50048]	Loss: 1.5709
Training Epoch: 15 [37504/50048]	Loss: 1.3423
Training Epoch: 15 [37632/50048]	Loss: 1.1475
Training Epoch: 15 [37760/50048]	Loss: 1.0732
Training Epoch: 15 [37888/50048]	Loss: 1.4051
Training Epoch: 15 [38016/50048]	Loss: 1.3988
Training Epoch: 15 [38144/50048]	Loss: 1.3205
Training Epoch: 15 [38272/50048]	Loss: 1.3265
Training Epoch: 15 [38400/50048]	Loss: 1.3560
Training Epoch: 15 [38528/50048]	Loss: 1.1027
Training Epoch: 15 [38656/50048]	Loss: 1.3137
Training Epoch: 15 [38784/50048]	Loss: 1.3283
Training Epoch: 15 [38912/50048]	Loss: 1.3066
Training Epoch: 15 [39040/50048]	Loss: 1.1831
Training Epoch: 15 [39168/50048]	Loss: 1.4052
Training Epoch: 15 [39296/50048]	Loss: 1.3744
Training Epoch: 15 [39424/50048]	Loss: 1.5267
Training Epoch: 15 [39552/50048]	Loss: 1.2550
Training Epoch: 15 [39680/50048]	Loss: 1.1798
Training Epoch: 15 [39808/50048]	Loss: 1.0999
Training Epoch: 15 [39936/50048]	Loss: 1.3880
Training Epoch: 15 [40064/50048]	Loss: 1.1303
Training Epoch: 15 [40192/50048]	Loss: 1.3382
Training Epoch: 15 [40320/50048]	Loss: 1.2956
Training Epoch: 15 [40448/50048]	Loss: 1.1933
Training Epoch: 15 [40576/50048]	Loss: 1.3078
Training Epoch: 15 [40704/50048]	Loss: 1.1560
Training Epoch: 15 [40832/50048]	Loss: 1.4393
Training Epoch: 15 [40960/50048]	Loss: 1.4284
Training Epoch: 15 [41088/50048]	Loss: 1.3717
Training Epoch: 15 [41216/50048]	Loss: 1.2815
Training Epoch: 15 [41344/50048]	Loss: 1.2925
Training Epoch: 15 [41472/50048]	Loss: 1.1005
Training Epoch: 15 [41600/50048]	Loss: 1.2077
Training Epoch: 15 [41728/50048]	Loss: 1.2036
Training Epoch: 15 [41856/50048]	Loss: 1.1416
Training Epoch: 15 [41984/50048]	Loss: 1.4604
Training Epoch: 15 [42112/50048]	Loss: 1.0933
Training Epoch: 15 [42240/50048]	Loss: 1.2655
Training Epoch: 15 [42368/50048]	Loss: 1.2235
Training Epoch: 15 [42496/50048]	Loss: 1.4962
Training Epoch: 15 [42624/50048]	Loss: 1.2932
Training Epoch: 15 [42752/50048]	Loss: 0.9995
Training Epoch: 15 [42880/50048]	Loss: 1.2777
Training Epoch: 15 [43008/50048]	Loss: 1.2446
Training Epoch: 15 [43136/50048]	Loss: 1.2605
Training Epoch: 15 [43264/50048]	Loss: 1.2651
Training Epoch: 15 [43392/50048]	Loss: 1.3738
Training Epoch: 15 [43520/50048]	Loss: 1.4968
Training Epoch: 15 [43648/50048]	Loss: 1.1951
Training Epoch: 15 [43776/50048]	Loss: 1.3535
Training Epoch: 15 [43904/50048]	Loss: 1.3117
Training Epoch: 15 [44032/50048]	Loss: 1.3811
Training Epoch: 15 [44160/50048]	Loss: 1.3456
Training Epoch: 15 [44288/50048]	Loss: 1.1817
Training Epoch: 15 [44416/50048]	Loss: 1.2798
Training Epoch: 15 [44544/50048]	Loss: 1.4015
Training Epoch: 15 [44672/50048]	Loss: 1.2029
Training Epoch: 15 [44800/50048]	Loss: 1.2182
Training Epoch: 15 [44928/50048]	Loss: 1.6401
Training Epoch: 15 [45056/50048]	Loss: 1.4055
Training Epoch: 15 [45184/50048]	Loss: 1.3528
Training Epoch: 15 [45312/50048]	Loss: 1.3304
Training Epoch: 15 [45440/50048]	Loss: 1.2542
Training Epoch: 15 [45568/50048]	Loss: 1.3196
Training Epoch: 15 [45696/50048]	Loss: 1.2440
Training Epoch: 15 [45824/50048]	Loss: 1.2592
Training Epoch: 15 [45952/50048]	Loss: 1.2029
Training Epoch: 15 [46080/50048]	Loss: 1.3607
Training Epoch: 15 [46208/50048]	Loss: 1.1640
Training Epoch: 15 [46336/50048]	Loss: 1.2837
Training Epoch: 15 [46464/50048]	Loss: 1.1807
Training Epoch: 15 [46592/50048]	Loss: 1.1905
Training Epoch: 15 [46720/50048]	Loss: 1.4532
Training Epoch: 15 [46848/50048]	Loss: 1.4535
Training Epoch: 15 [46976/50048]	Loss: 1.2571
Training Epoch: 15 [47104/50048]	Loss: 1.0555
Training Epoch: 15 [47232/50048]	Loss: 1.1674
Training Epoch: 15 [47360/50048]	Loss: 1.3797
Training Epoch: 15 [47488/50048]	Loss: 1.1289
Training Epoch: 15 [47616/50048]	Loss: 1.2582
Training Epoch: 15 [47744/50048]	Loss: 1.3846
Training Epoch: 15 [47872/50048]	Loss: 1.0688
Training Epoch: 15 [48000/50048]	Loss: 1.2055
Training Epoch: 15 [48128/50048]	Loss: 1.2217
Training Epoch: 15 [48256/50048]	Loss: 1.4045
Training Epoch: 15 [48384/50048]	Loss: 1.2234
Training Epoch: 15 [48512/50048]	Loss: 1.2944
Training Epoch: 15 [48640/50048]	Loss: 1.0495
Training Epoch: 15 [48768/50048]	Loss: 1.1920
Training Epoch: 15 [48896/50048]	Loss: 1.3002
Training Epoch: 15 [49024/50048]	Loss: 1.4643
Training Epoch: 15 [49152/50048]	Loss: 1.3113
Training Epoch: 15 [49280/50048]	Loss: 1.4354
Training Epoch: 15 [49408/50048]	Loss: 1.2881
Training Epoch: 15 [49536/50048]	Loss: 1.1111
Training Epoch: 15 [49664/50048]	Loss: 1.3733
Training Epoch: 15 [49792/50048]	Loss: 1.5335
Training Epoch: 15 [49920/50048]	Loss: 1.3049
Training Epoch: 15 [50048/50048]	Loss: 1.4868
Validation Epoch: 15, Average loss: 0.0116, Accuracy: 0.5871
Training Epoch: 16 [128/50048]	Loss: 1.3434
Training Epoch: 16 [256/50048]	Loss: 1.1498
Training Epoch: 16 [384/50048]	Loss: 1.3173
Training Epoch: 16 [512/50048]	Loss: 1.5434
Training Epoch: 16 [640/50048]	Loss: 1.2877
Training Epoch: 16 [768/50048]	Loss: 1.6204
Training Epoch: 16 [896/50048]	Loss: 1.1486
Training Epoch: 16 [1024/50048]	Loss: 1.2963
Training Epoch: 16 [1152/50048]	Loss: 1.3840
Training Epoch: 16 [1280/50048]	Loss: 1.3636
Training Epoch: 16 [1408/50048]	Loss: 1.1627
Training Epoch: 16 [1536/50048]	Loss: 1.1305
Training Epoch: 16 [1664/50048]	Loss: 1.4229
Training Epoch: 16 [1792/50048]	Loss: 1.2612
Training Epoch: 16 [1920/50048]	Loss: 1.3753
Training Epoch: 16 [2048/50048]	Loss: 1.0967
Training Epoch: 16 [2176/50048]	Loss: 1.0948
Training Epoch: 16 [2304/50048]	Loss: 1.2674
Training Epoch: 16 [2432/50048]	Loss: 1.3191
Training Epoch: 16 [2560/50048]	Loss: 1.4254
Training Epoch: 16 [2688/50048]	Loss: 1.2550
Training Epoch: 16 [2816/50048]	Loss: 1.1657
Training Epoch: 16 [2944/50048]	Loss: 1.2775
Training Epoch: 16 [3072/50048]	Loss: 1.2939
Training Epoch: 16 [3200/50048]	Loss: 1.1457
Training Epoch: 16 [3328/50048]	Loss: 1.3674
Training Epoch: 16 [3456/50048]	Loss: 1.3351
Training Epoch: 16 [3584/50048]	Loss: 1.1181
Training Epoch: 16 [3712/50048]	Loss: 1.2221
Training Epoch: 16 [3840/50048]	Loss: 1.0343
Training Epoch: 16 [3968/50048]	Loss: 1.2410
Training Epoch: 16 [4096/50048]	Loss: 1.0452
Training Epoch: 16 [4224/50048]	Loss: 1.3369
Training Epoch: 16 [4352/50048]	Loss: 1.1497
Training Epoch: 16 [4480/50048]	Loss: 1.2035
Training Epoch: 16 [4608/50048]	Loss: 1.4709
Training Epoch: 16 [4736/50048]	Loss: 1.5645
Training Epoch: 16 [4864/50048]	Loss: 1.4634
Training Epoch: 16 [4992/50048]	Loss: 1.0041
Training Epoch: 16 [5120/50048]	Loss: 1.0659
Training Epoch: 16 [5248/50048]	Loss: 1.2863
Training Epoch: 16 [5376/50048]	Loss: 1.2686
Training Epoch: 16 [5504/50048]	Loss: 1.3309
Training Epoch: 16 [5632/50048]	Loss: 1.4543
Training Epoch: 16 [5760/50048]	Loss: 1.3068
Training Epoch: 16 [5888/50048]	Loss: 1.2211
Training Epoch: 16 [6016/50048]	Loss: 1.3920
Training Epoch: 16 [6144/50048]	Loss: 1.4544
Training Epoch: 16 [6272/50048]	Loss: 1.4204
Training Epoch: 16 [6400/50048]	Loss: 1.3439
Training Epoch: 16 [6528/50048]	Loss: 1.0538
Training Epoch: 16 [6656/50048]	Loss: 1.3897
Training Epoch: 16 [6784/50048]	Loss: 1.3238
Training Epoch: 16 [6912/50048]	Loss: 1.0960
Training Epoch: 16 [7040/50048]	Loss: 1.2244
Training Epoch: 16 [7168/50048]	Loss: 1.3814
Training Epoch: 16 [7296/50048]	Loss: 1.5267
Training Epoch: 16 [7424/50048]	Loss: 1.3452
Training Epoch: 16 [7552/50048]	Loss: 0.9426
Training Epoch: 16 [7680/50048]	Loss: 1.3566
Training Epoch: 16 [7808/50048]	Loss: 1.2521
Training Epoch: 16 [7936/50048]	Loss: 1.2528
Training Epoch: 16 [8064/50048]	Loss: 1.2786
Training Epoch: 16 [8192/50048]	Loss: 1.1251
Training Epoch: 16 [8320/50048]	Loss: 1.1748
Training Epoch: 16 [8448/50048]	Loss: 1.3801
Training Epoch: 16 [8576/50048]	Loss: 1.3070
Training Epoch: 16 [8704/50048]	Loss: 1.1544
Training Epoch: 16 [8832/50048]	Loss: 1.2258
Training Epoch: 16 [8960/50048]	Loss: 1.4221
Training Epoch: 16 [9088/50048]	Loss: 1.3862
Training Epoch: 16 [9216/50048]	Loss: 1.3226
Training Epoch: 16 [9344/50048]	Loss: 1.1944
Training Epoch: 16 [9472/50048]	Loss: 1.2542
Training Epoch: 16 [9600/50048]	Loss: 1.4417
Training Epoch: 16 [9728/50048]	Loss: 1.4420
Training Epoch: 16 [9856/50048]	Loss: 1.2409
Training Epoch: 16 [9984/50048]	Loss: 1.2847
Training Epoch: 16 [10112/50048]	Loss: 1.3469
Training Epoch: 16 [10240/50048]	Loss: 1.3312
Training Epoch: 16 [10368/50048]	Loss: 1.5235
Training Epoch: 16 [10496/50048]	Loss: 1.2542
Training Epoch: 16 [10624/50048]	Loss: 1.1999
Training Epoch: 16 [10752/50048]	Loss: 1.1887
Training Epoch: 16 [10880/50048]	Loss: 1.3516
Training Epoch: 16 [11008/50048]	Loss: 1.4312
Training Epoch: 16 [11136/50048]	Loss: 1.3591
Training Epoch: 16 [11264/50048]	Loss: 1.0704
Training Epoch: 16 [11392/50048]	Loss: 1.1962
Training Epoch: 16 [11520/50048]	Loss: 0.9167
Training Epoch: 16 [11648/50048]	Loss: 1.2798
Training Epoch: 16 [11776/50048]	Loss: 1.1255
Training Epoch: 16 [11904/50048]	Loss: 1.1449
Training Epoch: 16 [12032/50048]	Loss: 1.3264
Training Epoch: 16 [12160/50048]	Loss: 1.2492
Training Epoch: 16 [12288/50048]	Loss: 1.2961
Training Epoch: 16 [12416/50048]	Loss: 1.2040
Training Epoch: 16 [12544/50048]	Loss: 1.0855
Training Epoch: 16 [12672/50048]	Loss: 1.3455
Training Epoch: 16 [12800/50048]	Loss: 1.1546
Training Epoch: 16 [12928/50048]	Loss: 1.1868
Training Epoch: 16 [13056/50048]	Loss: 1.3631
Training Epoch: 16 [13184/50048]	Loss: 1.2088
Training Epoch: 16 [13312/50048]	Loss: 1.2014
Training Epoch: 16 [13440/50048]	Loss: 1.1768
Training Epoch: 16 [13568/50048]	Loss: 1.2650
Training Epoch: 16 [13696/50048]	Loss: 1.2060
Training Epoch: 16 [13824/50048]	Loss: 1.0697
Training Epoch: 16 [13952/50048]	Loss: 1.5814
Training Epoch: 16 [14080/50048]	Loss: 1.2992
Training Epoch: 16 [14208/50048]	Loss: 1.1935
Training Epoch: 16 [14336/50048]	Loss: 1.3765
Training Epoch: 16 [14464/50048]	Loss: 1.2795
Training Epoch: 16 [14592/50048]	Loss: 1.2199
Training Epoch: 16 [14720/50048]	Loss: 1.3269
Training Epoch: 16 [14848/50048]	Loss: 1.1936
Training Epoch: 16 [14976/50048]	Loss: 1.3314
Training Epoch: 16 [15104/50048]	Loss: 1.4174
Training Epoch: 16 [15232/50048]	Loss: 1.1232
Training Epoch: 16 [15360/50048]	Loss: 1.3175
Training Epoch: 16 [15488/50048]	Loss: 1.2964
Training Epoch: 16 [15616/50048]	Loss: 1.3316
Training Epoch: 16 [15744/50048]	Loss: 1.3340
Training Epoch: 16 [15872/50048]	Loss: 1.4171
Training Epoch: 16 [16000/50048]	Loss: 1.4395
Training Epoch: 16 [16128/50048]	Loss: 1.1632
Training Epoch: 16 [16256/50048]	Loss: 1.2402
Training Epoch: 16 [16384/50048]	Loss: 1.1365
Training Epoch: 16 [16512/50048]	Loss: 1.5132
Training Epoch: 16 [16640/50048]	Loss: 1.1980
Training Epoch: 16 [16768/50048]	Loss: 1.1746
Training Epoch: 16 [16896/50048]	Loss: 1.1706
Training Epoch: 16 [17024/50048]	Loss: 1.3146
Training Epoch: 16 [17152/50048]	Loss: 1.1986
Training Epoch: 16 [17280/50048]	Loss: 1.3324
Training Epoch: 16 [17408/50048]	Loss: 1.3906
Training Epoch: 16 [17536/50048]	Loss: 1.4188
Training Epoch: 16 [17664/50048]	Loss: 1.2729
Training Epoch: 16 [17792/50048]	Loss: 1.1619
Training Epoch: 16 [17920/50048]	Loss: 1.1923
Training Epoch: 16 [18048/50048]	Loss: 1.5019
Training Epoch: 16 [18176/50048]	Loss: 1.1232
Training Epoch: 16 [18304/50048]	Loss: 1.1321
Training Epoch: 16 [18432/50048]	Loss: 1.3377
Training Epoch: 16 [18560/50048]	Loss: 1.3562
Training Epoch: 16 [18688/50048]	Loss: 1.2815
Training Epoch: 16 [18816/50048]	Loss: 1.1088
Training Epoch: 16 [18944/50048]	Loss: 1.3716
Training Epoch: 16 [19072/50048]	Loss: 1.0603
Training Epoch: 16 [19200/50048]	Loss: 1.1923
Training Epoch: 16 [19328/50048]	Loss: 1.1088
Training Epoch: 16 [19456/50048]	Loss: 1.2612
Training Epoch: 16 [19584/50048]	Loss: 1.1676
Training Epoch: 16 [19712/50048]	Loss: 1.2518
Training Epoch: 16 [19840/50048]	Loss: 1.1690
Training Epoch: 16 [19968/50048]	Loss: 1.2358
Training Epoch: 16 [20096/50048]	Loss: 1.0364
Training Epoch: 16 [20224/50048]	Loss: 1.4817
Training Epoch: 16 [20352/50048]	Loss: 1.2993
Training Epoch: 16 [20480/50048]	Loss: 1.2635
Training Epoch: 16 [20608/50048]	Loss: 1.3320
Training Epoch: 16 [20736/50048]	Loss: 1.0954
Training Epoch: 16 [20864/50048]	Loss: 1.4418
Training Epoch: 16 [20992/50048]	Loss: 1.4334
Training Epoch: 16 [21120/50048]	Loss: 1.4661
Training Epoch: 16 [21248/50048]	Loss: 1.0337
Training Epoch: 16 [21376/50048]	Loss: 1.1587
Training Epoch: 16 [21504/50048]	Loss: 1.2853
Training Epoch: 16 [21632/50048]	Loss: 1.1678
Training Epoch: 16 [21760/50048]	Loss: 1.1695
Training Epoch: 16 [21888/50048]	Loss: 1.4478
Training Epoch: 16 [22016/50048]	Loss: 1.3077
Training Epoch: 16 [22144/50048]	Loss: 1.1290
Training Epoch: 16 [22272/50048]	Loss: 1.1315
Training Epoch: 16 [22400/50048]	Loss: 1.3210
Training Epoch: 16 [22528/50048]	Loss: 1.2535
Training Epoch: 16 [22656/50048]	Loss: 1.4792
Training Epoch: 16 [22784/50048]	Loss: 1.2879
Training Epoch: 16 [22912/50048]	Loss: 1.4194
Training Epoch: 16 [23040/50048]	Loss: 1.2829
Training Epoch: 16 [23168/50048]	Loss: 1.0921
Training Epoch: 16 [23296/50048]	Loss: 1.2878
Training Epoch: 16 [23424/50048]	Loss: 1.2647
Training Epoch: 16 [23552/50048]	Loss: 1.1796
Training Epoch: 16 [23680/50048]	Loss: 1.3577
Training Epoch: 16 [23808/50048]	Loss: 1.3196
Training Epoch: 16 [23936/50048]	Loss: 1.3353
Training Epoch: 16 [24064/50048]	Loss: 1.3336
Training Epoch: 16 [24192/50048]	Loss: 1.1218
Training Epoch: 16 [24320/50048]	Loss: 1.2267
Training Epoch: 16 [24448/50048]	Loss: 1.2623
Training Epoch: 16 [24576/50048]	Loss: 1.1911
Training Epoch: 16 [24704/50048]	Loss: 1.3516
Training Epoch: 16 [24832/50048]	Loss: 1.3307
Training Epoch: 16 [24960/50048]	Loss: 1.5125
Training Epoch: 16 [25088/50048]	Loss: 1.1715
Training Epoch: 16 [25216/50048]	Loss: 1.0577
Training Epoch: 16 [25344/50048]	Loss: 1.3712
Training Epoch: 16 [25472/50048]	Loss: 1.2921
Training Epoch: 16 [25600/50048]	Loss: 1.4943
Training Epoch: 16 [25728/50048]	Loss: 1.1983
Training Epoch: 16 [25856/50048]	Loss: 1.4238
Training Epoch: 16 [25984/50048]	Loss: 1.2300
Training Epoch: 16 [26112/50048]	Loss: 1.4925
Training Epoch: 16 [26240/50048]	Loss: 1.3836
Training Epoch: 16 [26368/50048]	Loss: 1.4220
Training Epoch: 16 [26496/50048]	Loss: 1.2447
Training Epoch: 16 [26624/50048]	Loss: 1.3554
Training Epoch: 16 [26752/50048]	Loss: 1.1954
Training Epoch: 16 [26880/50048]	Loss: 1.2616
Training Epoch: 16 [27008/50048]	Loss: 1.3490
Training Epoch: 16 [27136/50048]	Loss: 1.1413
Training Epoch: 16 [27264/50048]	Loss: 1.2159
Training Epoch: 16 [27392/50048]	Loss: 1.4934
Training Epoch: 16 [27520/50048]	Loss: 1.1132
Training Epoch: 16 [27648/50048]	Loss: 1.4856
Training Epoch: 16 [27776/50048]	Loss: 1.2751
Training Epoch: 16 [27904/50048]	Loss: 1.4212
Training Epoch: 16 [28032/50048]	Loss: 1.2781
Training Epoch: 16 [28160/50048]	Loss: 0.9770
Training Epoch: 16 [28288/50048]	Loss: 1.4273
Training Epoch: 16 [28416/50048]	Loss: 1.2314
Training Epoch: 16 [28544/50048]	Loss: 1.2930
Training Epoch: 16 [28672/50048]	Loss: 1.2726
Training Epoch: 16 [28800/50048]	Loss: 1.3235
Training Epoch: 16 [28928/50048]	Loss: 1.0770
Training Epoch: 16 [29056/50048]	Loss: 1.2903
Training Epoch: 16 [29184/50048]	Loss: 1.1568
Training Epoch: 16 [29312/50048]	Loss: 1.2322
Training Epoch: 16 [29440/50048]	Loss: 1.3068
Training Epoch: 16 [29568/50048]	Loss: 1.1405
Training Epoch: 16 [29696/50048]	Loss: 1.0604
Training Epoch: 16 [29824/50048]	Loss: 1.3028
Training Epoch: 16 [29952/50048]	Loss: 1.5594
Training Epoch: 16 [30080/50048]	Loss: 1.3950
Training Epoch: 16 [30208/50048]	Loss: 1.1021
Training Epoch: 16 [30336/50048]	Loss: 0.9959
Training Epoch: 16 [30464/50048]	Loss: 1.3150
Training Epoch: 16 [30592/50048]	Loss: 1.1624
Training Epoch: 16 [30720/50048]	Loss: 1.2609
Training Epoch: 16 [30848/50048]	Loss: 1.3414
Training Epoch: 16 [30976/50048]	Loss: 1.1112
Training Epoch: 16 [31104/50048]	Loss: 1.0586
Training Epoch: 16 [31232/50048]	Loss: 1.2952
Training Epoch: 16 [31360/50048]	Loss: 1.1957
Training Epoch: 16 [31488/50048]	Loss: 1.3934
Training Epoch: 16 [31616/50048]	Loss: 1.1101
Training Epoch: 16 [31744/50048]	Loss: 1.2413
Training Epoch: 16 [31872/50048]	Loss: 1.3841
Training Epoch: 16 [32000/50048]	Loss: 1.4689
Training Epoch: 16 [32128/50048]	Loss: 1.2830
Training Epoch: 16 [32256/50048]	Loss: 1.5447
Training Epoch: 16 [32384/50048]	Loss: 1.2592
Training Epoch: 16 [32512/50048]	Loss: 1.4292
Training Epoch: 16 [32640/50048]	Loss: 1.3241
Training Epoch: 16 [32768/50048]	Loss: 1.2301
Training Epoch: 16 [32896/50048]	Loss: 1.2441
Training Epoch: 16 [33024/50048]	Loss: 1.3370
Training Epoch: 16 [33152/50048]	Loss: 1.2281
Training Epoch: 16 [33280/50048]	Loss: 1.2640
Training Epoch: 16 [33408/50048]	Loss: 1.2308
Training Epoch: 16 [33536/50048]	Loss: 1.3030
Training Epoch: 16 [33664/50048]	Loss: 1.3315
Training Epoch: 16 [33792/50048]	Loss: 1.1333
Training Epoch: 16 [33920/50048]	Loss: 1.2835
Training Epoch: 16 [34048/50048]	Loss: 1.1957
Training Epoch: 16 [34176/50048]	Loss: 1.3130
Training Epoch: 16 [34304/50048]	Loss: 1.4758
Training Epoch: 16 [34432/50048]	Loss: 1.2405
Training Epoch: 16 [34560/50048]	Loss: 1.2332
Training Epoch: 16 [34688/50048]	Loss: 1.4692
Training Epoch: 16 [34816/50048]	Loss: 1.1476
Training Epoch: 16 [34944/50048]	Loss: 1.1412
Training Epoch: 16 [35072/50048]	Loss: 1.4326
Training Epoch: 16 [35200/50048]	Loss: 1.5805
Training Epoch: 16 [35328/50048]	Loss: 1.1133
Training Epoch: 16 [35456/50048]	Loss: 0.9151
Training Epoch: 16 [35584/50048]	Loss: 1.3187
Training Epoch: 16 [35712/50048]	Loss: 1.2853
Training Epoch: 16 [35840/50048]	Loss: 1.3440
Training Epoch: 16 [35968/50048]	Loss: 1.4758
Training Epoch: 16 [36096/50048]	Loss: 1.3160
Training Epoch: 16 [36224/50048]	Loss: 1.1116
Training Epoch: 16 [36352/50048]	Loss: 1.2259
Training Epoch: 16 [36480/50048]	Loss: 1.3952
Training Epoch: 16 [36608/50048]	Loss: 1.2230
Training Epoch: 16 [36736/50048]	Loss: 1.0888
Training Epoch: 16 [36864/50048]	Loss: 1.3647
Training Epoch: 16 [36992/50048]	Loss: 1.4323
Training Epoch: 16 [37120/50048]	Loss: 1.4682
Training Epoch: 16 [37248/50048]	Loss: 1.2996
Training Epoch: 16 [37376/50048]	Loss: 1.2336
Training Epoch: 16 [37504/50048]	Loss: 1.5148
Training Epoch: 16 [37632/50048]	Loss: 1.2827
Training Epoch: 16 [37760/50048]	Loss: 1.2605
Training Epoch: 16 [37888/50048]	Loss: 1.4866
Training Epoch: 16 [38016/50048]	Loss: 1.3421
Training Epoch: 16 [38144/50048]	Loss: 1.2421
Training Epoch: 16 [38272/50048]	Loss: 1.0599
Training Epoch: 16 [38400/50048]	Loss: 1.4065
Training Epoch: 16 [38528/50048]	Loss: 1.2225
Training Epoch: 16 [38656/50048]	Loss: 1.2316
Training Epoch: 16 [38784/50048]	Loss: 1.2272
Training Epoch: 16 [38912/50048]	Loss: 1.2164
Training Epoch: 16 [39040/50048]	Loss: 1.1350
Training Epoch: 16 [39168/50048]	Loss: 1.0625
Training Epoch: 16 [39296/50048]	Loss: 1.1502
Training Epoch: 16 [39424/50048]	Loss: 1.0862
Training Epoch: 16 [39552/50048]	Loss: 1.1820
Training Epoch: 16 [39680/50048]	Loss: 1.2284
Training Epoch: 16 [39808/50048]	Loss: 1.3417
Training Epoch: 16 [39936/50048]	Loss: 1.5634
Training Epoch: 16 [40064/50048]	Loss: 1.3469
Training Epoch: 16 [40192/50048]	Loss: 1.3902
Training Epoch: 16 [40320/50048]	Loss: 1.2672
Training Epoch: 16 [40448/50048]	Loss: 1.3214
Training Epoch: 16 [40576/50048]	Loss: 1.3096
Training Epoch: 16 [40704/50048]	Loss: 1.2219
Training Epoch: 16 [40832/50048]	Loss: 1.3133
Training Epoch: 16 [40960/50048]	Loss: 1.4270
Training Epoch: 16 [41088/50048]	Loss: 1.4167
Training Epoch: 16 [41216/50048]	Loss: 1.2165
Training Epoch: 16 [41344/50048]	Loss: 1.0754
Training Epoch: 16 [41472/50048]	Loss: 1.2792
Training Epoch: 16 [41600/50048]	Loss: 1.1575
Training Epoch: 16 [41728/50048]	Loss: 1.2444
Training Epoch: 16 [41856/50048]	Loss: 1.3031
Training Epoch: 16 [41984/50048]	Loss: 1.0889
Training Epoch: 16 [42112/50048]	Loss: 1.1936
Training Epoch: 16 [42240/50048]	Loss: 1.2250
Training Epoch: 16 [42368/50048]	Loss: 0.9989
Training Epoch: 16 [42496/50048]	Loss: 1.3400
Training Epoch: 16 [42624/50048]	Loss: 1.1315
Training Epoch: 16 [42752/50048]	Loss: 1.3132
Training Epoch: 16 [42880/50048]	Loss: 1.2696
Training Epoch: 16 [43008/50048]	Loss: 1.0665
Training Epoch: 16 [43136/50048]	Loss: 1.1172
Training Epoch: 16 [43264/50048]	Loss: 1.3222
Training Epoch: 16 [43392/50048]	Loss: 1.2289
Training Epoch: 16 [43520/50048]	Loss: 1.4040
Training Epoch: 16 [43648/50048]	Loss: 1.2057
Training Epoch: 16 [43776/50048]	Loss: 1.3649
Training Epoch: 16 [43904/50048]	Loss: 1.1448
Training Epoch: 16 [44032/50048]	Loss: 1.3697
Training Epoch: 16 [44160/50048]	Loss: 1.1704
Training Epoch: 16 [44288/50048]	Loss: 1.3988
Training Epoch: 16 [44416/50048]	Loss: 1.1748
Training Epoch: 16 [44544/50048]	Loss: 1.3314
Training Epoch: 16 [44672/50048]	Loss: 1.3643
Training Epoch: 16 [44800/50048]	Loss: 1.6090
Training Epoch: 16 [44928/50048]	Loss: 1.2315
Training Epoch: 16 [45056/50048]	Loss: 1.4656
Training Epoch: 16 [45184/50048]	Loss: 1.2151
Training Epoch: 16 [45312/50048]	Loss: 1.2903
Training Epoch: 16 [45440/50048]	Loss: 1.1393
Training Epoch: 16 [45568/50048]	Loss: 1.3037
Training Epoch: 16 [45696/50048]	Loss: 1.1852
Training Epoch: 16 [45824/50048]	Loss: 1.3945
Training Epoch: 16 [45952/50048]	Loss: 1.3468
Training Epoch: 16 [46080/50048]	Loss: 1.3651
Training Epoch: 16 [46208/50048]	Loss: 1.5667
Training Epoch: 16 [46336/50048]	Loss: 1.2520
Training Epoch: 16 [46464/50048]	Loss: 1.4483
Training Epoch: 16 [46592/50048]	Loss: 1.2649
Training Epoch: 16 [46720/50048]	Loss: 1.3690
Training Epoch: 16 [46848/50048]	Loss: 1.3699
Training Epoch: 16 [46976/50048]	Loss: 1.2420
Training Epoch: 16 [47104/50048]	Loss: 1.2324
Training Epoch: 16 [47232/50048]	Loss: 1.3927
Training Epoch: 16 [47360/50048]	Loss: 1.3282
Training Epoch: 16 [47488/50048]	Loss: 1.2737
Training Epoch: 16 [47616/50048]	Loss: 0.9394
Training Epoch: 16 [47744/50048]	Loss: 1.1307
Training Epoch: 16 [47872/50048]	Loss: 1.3715
Training Epoch: 16 [48000/50048]	Loss: 1.5893
Training Epoch: 16 [48128/50048]	Loss: 1.2781
Training Epoch: 16 [48256/50048]	Loss: 1.5131
Training Epoch: 16 [48384/50048]	Loss: 1.1755
Training Epoch: 16 [48512/50048]	Loss: 1.2506
Training Epoch: 16 [48640/50048]	Loss: 1.4267
Training Epoch: 16 [48768/50048]	Loss: 1.0366
Training Epoch: 16 [48896/50048]	Loss: 1.2257
Training Epoch: 16 [49024/50048]	Loss: 1.2901
Training Epoch: 16 [49152/50048]	Loss: 1.3349
Training Epoch: 16 [49280/50048]	Loss: 1.1853
Training Epoch: 16 [49408/50048]	Loss: 1.6325
Training Epoch: 16 [49536/50048]	Loss: 1.3931
Training Epoch: 16 [49664/50048]	Loss: 1.1622
Training Epoch: 16 [49792/50048]	Loss: 1.1502
Training Epoch: 16 [49920/50048]	Loss: 1.5983
Training Epoch: 16 [50048/50048]	Loss: 1.0109
Validation Epoch: 16, Average loss: 0.0116, Accuracy: 0.5899
Training Epoch: 17 [128/50048]	Loss: 1.4048
Training Epoch: 17 [256/50048]	Loss: 1.1743
Training Epoch: 17 [384/50048]	Loss: 1.1023
Training Epoch: 17 [512/50048]	Loss: 1.3003
Training Epoch: 17 [640/50048]	Loss: 1.1684
Training Epoch: 17 [768/50048]	Loss: 1.1920
Training Epoch: 17 [896/50048]	Loss: 1.3134
Training Epoch: 17 [1024/50048]	Loss: 1.1821
Training Epoch: 17 [1152/50048]	Loss: 1.2425
Training Epoch: 17 [1280/50048]	Loss: 1.1278
Training Epoch: 17 [1408/50048]	Loss: 1.0376
Training Epoch: 17 [1536/50048]	Loss: 1.0313
Training Epoch: 17 [1664/50048]	Loss: 1.1095
Training Epoch: 17 [1792/50048]	Loss: 1.3654
Training Epoch: 17 [1920/50048]	Loss: 1.3539
Training Epoch: 17 [2048/50048]	Loss: 1.3774
Training Epoch: 17 [2176/50048]	Loss: 1.1652
Training Epoch: 17 [2304/50048]	Loss: 1.3611
Training Epoch: 17 [2432/50048]	Loss: 1.2196
Training Epoch: 17 [2560/50048]	Loss: 1.2163
Training Epoch: 17 [2688/50048]	Loss: 1.2537
Training Epoch: 17 [2816/50048]	Loss: 1.1905
Training Epoch: 17 [2944/50048]	Loss: 1.2429
Training Epoch: 17 [3072/50048]	Loss: 1.3484
Training Epoch: 17 [3200/50048]	Loss: 1.6398
Training Epoch: 17 [3328/50048]	Loss: 1.1862
Training Epoch: 17 [3456/50048]	Loss: 1.0962
Training Epoch: 17 [3584/50048]	Loss: 1.1872
Training Epoch: 17 [3712/50048]	Loss: 1.5390
Training Epoch: 17 [3840/50048]	Loss: 1.1463
Training Epoch: 17 [3968/50048]	Loss: 1.1572
Training Epoch: 17 [4096/50048]	Loss: 1.4928
Training Epoch: 17 [4224/50048]	Loss: 1.3490
Training Epoch: 17 [4352/50048]	Loss: 1.4101
Training Epoch: 17 [4480/50048]	Loss: 1.2158
Training Epoch: 17 [4608/50048]	Loss: 1.2504
Training Epoch: 17 [4736/50048]	Loss: 1.0827
Training Epoch: 17 [4864/50048]	Loss: 1.0926
Training Epoch: 17 [4992/50048]	Loss: 1.3276
Training Epoch: 17 [5120/50048]	Loss: 1.3667
Training Epoch: 17 [5248/50048]	Loss: 1.2289
Training Epoch: 17 [5376/50048]	Loss: 1.3764
Training Epoch: 17 [5504/50048]	Loss: 1.1006
Training Epoch: 17 [5632/50048]	Loss: 1.2977
Training Epoch: 17 [5760/50048]	Loss: 1.2660
Training Epoch: 17 [5888/50048]	Loss: 1.1912
Training Epoch: 17 [6016/50048]	Loss: 1.1736
Training Epoch: 17 [6144/50048]	Loss: 1.4460
Training Epoch: 17 [6272/50048]	Loss: 1.0512
Training Epoch: 17 [6400/50048]	Loss: 1.2767
Training Epoch: 17 [6528/50048]	Loss: 1.3661
Training Epoch: 17 [6656/50048]	Loss: 1.2295
Training Epoch: 17 [6784/50048]	Loss: 1.0394
Training Epoch: 17 [6912/50048]	Loss: 1.0890
Training Epoch: 17 [7040/50048]	Loss: 1.4934
Training Epoch: 17 [7168/50048]	Loss: 1.0806
Training Epoch: 17 [7296/50048]	Loss: 1.1727
Training Epoch: 17 [7424/50048]	Loss: 0.9564
Training Epoch: 17 [7552/50048]	Loss: 1.2705
Training Epoch: 17 [7680/50048]	Loss: 1.4444
Training Epoch: 17 [7808/50048]	Loss: 1.1871
Training Epoch: 17 [7936/50048]	Loss: 1.2722
Training Epoch: 17 [8064/50048]	Loss: 1.2247
Training Epoch: 17 [8192/50048]	Loss: 1.2819
Training Epoch: 17 [8320/50048]	Loss: 1.3612
Training Epoch: 17 [8448/50048]	Loss: 1.3729
Training Epoch: 17 [8576/50048]	Loss: 1.1697
Training Epoch: 17 [8704/50048]	Loss: 1.1952
Training Epoch: 17 [8832/50048]	Loss: 1.4925
Training Epoch: 17 [8960/50048]	Loss: 1.2218
Training Epoch: 17 [9088/50048]	Loss: 1.2888
Training Epoch: 17 [9216/50048]	Loss: 1.0213
Training Epoch: 17 [9344/50048]	Loss: 1.3358
Training Epoch: 17 [9472/50048]	Loss: 1.3935
Training Epoch: 17 [9600/50048]	Loss: 1.2935
Training Epoch: 17 [9728/50048]	Loss: 1.2190
Training Epoch: 17 [9856/50048]	Loss: 1.2626
Training Epoch: 17 [9984/50048]	Loss: 1.0112
Training Epoch: 17 [10112/50048]	Loss: 1.2578
Training Epoch: 17 [10240/50048]	Loss: 1.4889
Training Epoch: 17 [10368/50048]	Loss: 1.2343
Training Epoch: 17 [10496/50048]	Loss: 0.9457
Training Epoch: 17 [10624/50048]	Loss: 1.1057
Training Epoch: 17 [10752/50048]	Loss: 1.1979
Training Epoch: 17 [10880/50048]	Loss: 1.1637
Training Epoch: 17 [11008/50048]	Loss: 1.3022
Training Epoch: 17 [11136/50048]	Loss: 1.0905
Training Epoch: 17 [11264/50048]	Loss: 1.3144
Training Epoch: 17 [11392/50048]	Loss: 0.9855
Training Epoch: 17 [11520/50048]	Loss: 1.2348
Training Epoch: 17 [11648/50048]	Loss: 1.2181
Training Epoch: 17 [11776/50048]	Loss: 1.2253
Training Epoch: 17 [11904/50048]	Loss: 1.0970
Training Epoch: 17 [12032/50048]	Loss: 1.0704
Training Epoch: 17 [12160/50048]	Loss: 1.1942
Training Epoch: 17 [12288/50048]	Loss: 1.3402
Training Epoch: 17 [12416/50048]	Loss: 1.1650
Training Epoch: 17 [12544/50048]	Loss: 1.3249
Training Epoch: 17 [12672/50048]	Loss: 1.2229
Training Epoch: 17 [12800/50048]	Loss: 1.3135
Training Epoch: 17 [12928/50048]	Loss: 1.2742
Training Epoch: 17 [13056/50048]	Loss: 1.5929
Training Epoch: 17 [13184/50048]	Loss: 1.0153
Training Epoch: 17 [13312/50048]	Loss: 1.2003
Training Epoch: 17 [13440/50048]	Loss: 1.2063
Training Epoch: 17 [13568/50048]	Loss: 1.1280
Training Epoch: 17 [13696/50048]	Loss: 1.0900
Training Epoch: 17 [13824/50048]	Loss: 1.3189
Training Epoch: 17 [13952/50048]	Loss: 1.2366
Training Epoch: 17 [14080/50048]	Loss: 1.1531
Training Epoch: 17 [14208/50048]	Loss: 1.3119
Training Epoch: 17 [14336/50048]	Loss: 1.3180
Training Epoch: 17 [14464/50048]	Loss: 1.1331
Training Epoch: 17 [14592/50048]	Loss: 1.3122
Training Epoch: 17 [14720/50048]	Loss: 1.3655
Training Epoch: 17 [14848/50048]	Loss: 1.1113
Training Epoch: 17 [14976/50048]	Loss: 1.3676
Training Epoch: 17 [15104/50048]	Loss: 1.2402
Training Epoch: 17 [15232/50048]	Loss: 1.1657
Training Epoch: 17 [15360/50048]	Loss: 1.1443
Training Epoch: 17 [15488/50048]	Loss: 1.3865
Training Epoch: 17 [15616/50048]	Loss: 1.4602
Training Epoch: 17 [15744/50048]	Loss: 1.1832
Training Epoch: 17 [15872/50048]	Loss: 1.2488
Training Epoch: 17 [16000/50048]	Loss: 1.2211
Training Epoch: 17 [16128/50048]	Loss: 1.1170
Training Epoch: 17 [16256/50048]	Loss: 1.3016
Training Epoch: 17 [16384/50048]	Loss: 1.2325
Training Epoch: 17 [16512/50048]	Loss: 1.2040
Training Epoch: 17 [16640/50048]	Loss: 1.4100
Training Epoch: 17 [16768/50048]	Loss: 1.4335
Training Epoch: 17 [16896/50048]	Loss: 1.2074
Training Epoch: 17 [17024/50048]	Loss: 1.2675
Training Epoch: 17 [17152/50048]	Loss: 1.4270
Training Epoch: 17 [17280/50048]	Loss: 1.2753
Training Epoch: 17 [17408/50048]	Loss: 1.4984
Training Epoch: 17 [17536/50048]	Loss: 1.2180
Training Epoch: 17 [17664/50048]	Loss: 1.1827
Training Epoch: 17 [17792/50048]	Loss: 1.4708
Training Epoch: 17 [17920/50048]	Loss: 1.0416
Training Epoch: 17 [18048/50048]	Loss: 1.2743
Training Epoch: 17 [18176/50048]	Loss: 1.2862
Training Epoch: 17 [18304/50048]	Loss: 1.2033
Training Epoch: 17 [18432/50048]	Loss: 1.3929
Training Epoch: 17 [18560/50048]	Loss: 1.1942
Training Epoch: 17 [18688/50048]	Loss: 1.1131
Training Epoch: 17 [18816/50048]	Loss: 1.2024
Training Epoch: 17 [18944/50048]	Loss: 1.6051
Training Epoch: 17 [19072/50048]	Loss: 1.0878
Training Epoch: 17 [19200/50048]	Loss: 1.4184
Training Epoch: 17 [19328/50048]	Loss: 1.3567
Training Epoch: 17 [19456/50048]	Loss: 1.2211
Training Epoch: 17 [19584/50048]	Loss: 1.0917
Training Epoch: 17 [19712/50048]	Loss: 1.4362
Training Epoch: 17 [19840/50048]	Loss: 1.3818
Training Epoch: 17 [19968/50048]	Loss: 1.2670
Training Epoch: 17 [20096/50048]	Loss: 1.1162
Training Epoch: 17 [20224/50048]	Loss: 1.2369
Training Epoch: 17 [20352/50048]	Loss: 1.2054
Training Epoch: 17 [20480/50048]	Loss: 1.4261
Training Epoch: 17 [20608/50048]	Loss: 1.1983
Training Epoch: 17 [20736/50048]	Loss: 1.0655
Training Epoch: 17 [20864/50048]	Loss: 1.1245
Training Epoch: 17 [20992/50048]	Loss: 1.2312
Training Epoch: 17 [21120/50048]	Loss: 1.0746
Training Epoch: 17 [21248/50048]	Loss: 1.2419
Training Epoch: 17 [21376/50048]	Loss: 1.3795
Training Epoch: 17 [21504/50048]	Loss: 1.3519
Training Epoch: 17 [21632/50048]	Loss: 1.2861
Training Epoch: 17 [21760/50048]	Loss: 1.2375
Training Epoch: 17 [21888/50048]	Loss: 1.4746
Training Epoch: 17 [22016/50048]	Loss: 1.3611
Training Epoch: 17 [22144/50048]	Loss: 1.3600
Training Epoch: 17 [22272/50048]	Loss: 1.1847
Training Epoch: 17 [22400/50048]	Loss: 1.2295
Training Epoch: 17 [22528/50048]	Loss: 1.1484
Training Epoch: 17 [22656/50048]	Loss: 1.0295
Training Epoch: 17 [22784/50048]	Loss: 1.1143
Training Epoch: 17 [22912/50048]	Loss: 1.3182
Training Epoch: 17 [23040/50048]	Loss: 1.3262
Training Epoch: 17 [23168/50048]	Loss: 1.3515
Training Epoch: 17 [23296/50048]	Loss: 1.1574
Training Epoch: 17 [23424/50048]	Loss: 0.9335
Training Epoch: 17 [23552/50048]	Loss: 1.4055
Training Epoch: 17 [23680/50048]	Loss: 1.1968
Training Epoch: 17 [23808/50048]	Loss: 1.3389
Training Epoch: 17 [23936/50048]	Loss: 1.3177
Training Epoch: 17 [24064/50048]	Loss: 1.1094
Training Epoch: 17 [24192/50048]	Loss: 1.3124
Training Epoch: 17 [24320/50048]	Loss: 1.3200
Training Epoch: 17 [24448/50048]	Loss: 1.4865
Training Epoch: 17 [24576/50048]	Loss: 1.2575
Training Epoch: 17 [24704/50048]	Loss: 1.3972
Training Epoch: 17 [24832/50048]	Loss: 1.0927
Training Epoch: 17 [24960/50048]	Loss: 1.1953
Training Epoch: 17 [25088/50048]	Loss: 1.1350
Training Epoch: 17 [25216/50048]	Loss: 1.2619
Training Epoch: 17 [25344/50048]	Loss: 0.9807
Training Epoch: 17 [25472/50048]	Loss: 1.0642
Training Epoch: 17 [25600/50048]	Loss: 1.4083
Training Epoch: 17 [25728/50048]	Loss: 1.4730
Training Epoch: 17 [25856/50048]	Loss: 1.1886
Training Epoch: 17 [25984/50048]	Loss: 1.1101
Training Epoch: 17 [26112/50048]	Loss: 0.8992
Training Epoch: 17 [26240/50048]	Loss: 1.4444
Training Epoch: 17 [26368/50048]	Loss: 1.1741
Training Epoch: 17 [26496/50048]	Loss: 1.2385
Training Epoch: 17 [26624/50048]	Loss: 1.2975
Training Epoch: 17 [26752/50048]	Loss: 1.2270
Training Epoch: 17 [26880/50048]	Loss: 1.2100
Training Epoch: 17 [27008/50048]	Loss: 1.3706
Training Epoch: 17 [27136/50048]	Loss: 1.2742
Training Epoch: 17 [27264/50048]	Loss: 0.9489
Training Epoch: 17 [27392/50048]	Loss: 1.3673
Training Epoch: 17 [27520/50048]	Loss: 1.3118
Training Epoch: 17 [27648/50048]	Loss: 1.3358
Training Epoch: 17 [27776/50048]	Loss: 1.3315
Training Epoch: 17 [27904/50048]	Loss: 1.1926
Training Epoch: 17 [28032/50048]	Loss: 1.2696
Training Epoch: 17 [28160/50048]	Loss: 1.2221
Training Epoch: 17 [28288/50048]	Loss: 1.3671
Training Epoch: 17 [28416/50048]	Loss: 1.4130
Training Epoch: 17 [28544/50048]	Loss: 1.1580
Training Epoch: 17 [28672/50048]	Loss: 1.0779
Training Epoch: 17 [28800/50048]	Loss: 1.1701
Training Epoch: 17 [28928/50048]	Loss: 1.0148
Training Epoch: 17 [29056/50048]	Loss: 1.4367
Training Epoch: 17 [29184/50048]	Loss: 1.2376
Training Epoch: 17 [29312/50048]	Loss: 1.1948
Training Epoch: 17 [29440/50048]	Loss: 1.1277
Training Epoch: 17 [29568/50048]	Loss: 1.2500
Training Epoch: 17 [29696/50048]	Loss: 1.3073
Training Epoch: 17 [29824/50048]	Loss: 1.3380
Training Epoch: 17 [29952/50048]	Loss: 1.0726
Training Epoch: 17 [30080/50048]	Loss: 1.3201
Training Epoch: 17 [30208/50048]	Loss: 1.3557
Training Epoch: 17 [30336/50048]	Loss: 1.5372
Training Epoch: 17 [30464/50048]	Loss: 1.1555
Training Epoch: 17 [30592/50048]	Loss: 1.1434
Training Epoch: 17 [30720/50048]	Loss: 1.1925
Training Epoch: 17 [30848/50048]	Loss: 1.3230
Training Epoch: 17 [30976/50048]	Loss: 1.4925
Training Epoch: 17 [31104/50048]	Loss: 1.1706
Training Epoch: 17 [31232/50048]	Loss: 1.2763
Training Epoch: 17 [31360/50048]	Loss: 1.0662
Training Epoch: 17 [31488/50048]	Loss: 1.4176
Training Epoch: 17 [31616/50048]	Loss: 1.1348
Training Epoch: 17 [31744/50048]	Loss: 1.1588
Training Epoch: 17 [31872/50048]	Loss: 1.1126
Training Epoch: 17 [32000/50048]	Loss: 1.2697
Training Epoch: 17 [32128/50048]	Loss: 1.1696
Training Epoch: 17 [32256/50048]	Loss: 1.2625
Training Epoch: 17 [32384/50048]	Loss: 1.3582
Training Epoch: 17 [32512/50048]	Loss: 1.2395
Training Epoch: 17 [32640/50048]	Loss: 1.2098
Training Epoch: 17 [32768/50048]	Loss: 1.1652
Training Epoch: 17 [32896/50048]	Loss: 1.1638
Training Epoch: 17 [33024/50048]	Loss: 1.3851
Training Epoch: 17 [33152/50048]	Loss: 1.0806
Training Epoch: 17 [33280/50048]	Loss: 1.1384
Training Epoch: 17 [33408/50048]	Loss: 1.0361
Training Epoch: 17 [33536/50048]	Loss: 1.3791
Training Epoch: 17 [33664/50048]	Loss: 1.3397
Training Epoch: 17 [33792/50048]	Loss: 1.1449
Training Epoch: 17 [33920/50048]	Loss: 1.3602
Training Epoch: 17 [34048/50048]	Loss: 1.2834
Training Epoch: 17 [34176/50048]	Loss: 1.1586
Training Epoch: 17 [34304/50048]	Loss: 1.4104
Training Epoch: 17 [34432/50048]	Loss: 1.2349
Training Epoch: 17 [34560/50048]	Loss: 1.4238
Training Epoch: 17 [34688/50048]	Loss: 1.6517
Training Epoch: 17 [34816/50048]	Loss: 1.4269
Training Epoch: 17 [34944/50048]	Loss: 1.2649
Training Epoch: 17 [35072/50048]	Loss: 1.2452
Training Epoch: 17 [35200/50048]	Loss: 1.5843
Training Epoch: 17 [35328/50048]	Loss: 0.9957
Training Epoch: 17 [35456/50048]	Loss: 1.2638
Training Epoch: 17 [35584/50048]	Loss: 1.3762
Training Epoch: 17 [35712/50048]	Loss: 1.3297
Training Epoch: 17 [35840/50048]	Loss: 1.1260
Training Epoch: 17 [35968/50048]	Loss: 1.3286
Training Epoch: 17 [36096/50048]	Loss: 1.1827
Training Epoch: 17 [36224/50048]	Loss: 1.2532
Training Epoch: 17 [36352/50048]	Loss: 1.2130
Training Epoch: 17 [36480/50048]	Loss: 1.3056
Training Epoch: 17 [36608/50048]	Loss: 1.4949
Training Epoch: 17 [36736/50048]	Loss: 1.3917
Training Epoch: 17 [36864/50048]	Loss: 1.1249
Training Epoch: 17 [36992/50048]	Loss: 1.3814
Training Epoch: 17 [37120/50048]	Loss: 1.1954
Training Epoch: 17 [37248/50048]	Loss: 1.0694
Training Epoch: 17 [37376/50048]	Loss: 1.1297
Training Epoch: 17 [37504/50048]	Loss: 1.1919
Training Epoch: 17 [37632/50048]	Loss: 1.3914
Training Epoch: 17 [37760/50048]	Loss: 1.3212
Training Epoch: 17 [37888/50048]	Loss: 1.2866
Training Epoch: 17 [38016/50048]	Loss: 1.0813
Training Epoch: 17 [38144/50048]	Loss: 1.4251
Training Epoch: 17 [38272/50048]	Loss: 1.6632
Training Epoch: 17 [38400/50048]	Loss: 1.2335
Training Epoch: 17 [38528/50048]	Loss: 1.2494
Training Epoch: 17 [38656/50048]	Loss: 1.3180
Training Epoch: 17 [38784/50048]	Loss: 1.4527
Training Epoch: 17 [38912/50048]	Loss: 1.1107
Training Epoch: 17 [39040/50048]	Loss: 1.2551
Training Epoch: 17 [39168/50048]	Loss: 1.2420
Training Epoch: 17 [39296/50048]	Loss: 1.2290
Training Epoch: 17 [39424/50048]	Loss: 1.3749
Training Epoch: 17 [39552/50048]	Loss: 0.9466
Training Epoch: 17 [39680/50048]	Loss: 1.5498
Training Epoch: 17 [39808/50048]	Loss: 1.3290
Training Epoch: 17 [39936/50048]	Loss: 1.3903
Training Epoch: 17 [40064/50048]	Loss: 1.1005
Training Epoch: 17 [40192/50048]	Loss: 1.4606
Training Epoch: 17 [40320/50048]	Loss: 1.2785
Training Epoch: 17 [40448/50048]	Loss: 1.3659
Training Epoch: 17 [40576/50048]	Loss: 1.1140
Training Epoch: 17 [40704/50048]	Loss: 1.2118
Training Epoch: 17 [40832/50048]	Loss: 1.1966
Training Epoch: 17 [40960/50048]	Loss: 1.3034
Training Epoch: 17 [41088/50048]	Loss: 1.2013
Training Epoch: 17 [41216/50048]	Loss: 1.2852
Training Epoch: 17 [41344/50048]	Loss: 1.3493
Training Epoch: 17 [41472/50048]	Loss: 1.1400
Training Epoch: 17 [41600/50048]	Loss: 1.4802
Training Epoch: 17 [41728/50048]	Loss: 1.0684
Training Epoch: 17 [41856/50048]	Loss: 1.1722
Training Epoch: 17 [41984/50048]	Loss: 1.3796
Training Epoch: 17 [42112/50048]	Loss: 1.2605
Training Epoch: 17 [42240/50048]	Loss: 1.3672
Training Epoch: 17 [42368/50048]	Loss: 1.2242
Training Epoch: 17 [42496/50048]	Loss: 1.3195
Training Epoch: 17 [42624/50048]	Loss: 1.5007
Training Epoch: 17 [42752/50048]	Loss: 1.0853
Training Epoch: 17 [42880/50048]	Loss: 1.2860
Training Epoch: 17 [43008/50048]	Loss: 1.2596
Training Epoch: 17 [43136/50048]	Loss: 1.0762
Training Epoch: 17 [43264/50048]	Loss: 1.3163
Training Epoch: 17 [43392/50048]	Loss: 1.4264
Training Epoch: 17 [43520/50048]	Loss: 1.1078
Training Epoch: 17 [43648/50048]	Loss: 1.4168
Training Epoch: 17 [43776/50048]	Loss: 1.3964
Training Epoch: 17 [43904/50048]	Loss: 1.1280
Training Epoch: 17 [44032/50048]	Loss: 0.9859
Training Epoch: 17 [44160/50048]	Loss: 1.3271
Training Epoch: 17 [44288/50048]	Loss: 1.2983
Training Epoch: 17 [44416/50048]	Loss: 1.2022
Training Epoch: 17 [44544/50048]	Loss: 1.1675
Training Epoch: 17 [44672/50048]	Loss: 1.2239
Training Epoch: 17 [44800/50048]	Loss: 1.2770
Training Epoch: 17 [44928/50048]	Loss: 1.3323
Training Epoch: 17 [45056/50048]	Loss: 1.4600
Training Epoch: 17 [45184/50048]	Loss: 1.2186
Training Epoch: 17 [45312/50048]	Loss: 1.1673
Training Epoch: 17 [45440/50048]	Loss: 1.3003
Training Epoch: 17 [45568/50048]	Loss: 1.1429
Training Epoch: 17 [45696/50048]	Loss: 1.1963
Training Epoch: 17 [45824/50048]	Loss: 1.2754
Training Epoch: 17 [45952/50048]	Loss: 1.3028
Training Epoch: 17 [46080/50048]	Loss: 1.2779
Training Epoch: 17 [46208/50048]	Loss: 1.1469
Training Epoch: 17 [46336/50048]	Loss: 1.4207
Training Epoch: 17 [46464/50048]	Loss: 1.2418
Training Epoch: 17 [46592/50048]	Loss: 1.4204
Training Epoch: 17 [46720/50048]	Loss: 1.2181
Training Epoch: 17 [46848/50048]	Loss: 1.3345
Training Epoch: 17 [46976/50048]	Loss: 1.4024
Training Epoch: 17 [47104/50048]	Loss: 1.3334
Training Epoch: 17 [47232/50048]	Loss: 1.3130
Training Epoch: 17 [47360/50048]	Loss: 1.0752
Training Epoch: 17 [47488/50048]	Loss: 1.1901
Training Epoch: 17 [47616/50048]	Loss: 1.2670
Training Epoch: 17 [47744/50048]	Loss: 1.1832
Training Epoch: 17 [47872/50048]	Loss: 1.1578
Training Epoch: 17 [48000/50048]	Loss: 1.2236
Training Epoch: 17 [48128/50048]	Loss: 1.4554
Training Epoch: 17 [48256/50048]	Loss: 1.2076
Training Epoch: 17 [48384/50048]	Loss: 1.2206
Training Epoch: 17 [48512/50048]	Loss: 1.2267
Training Epoch: 17 [48640/50048]	Loss: 1.2288
Training Epoch: 17 [48768/50048]	Loss: 1.4856
Training Epoch: 17 [48896/50048]	Loss: 1.1608
Training Epoch: 17 [49024/50048]	Loss: 1.0944
Training Epoch: 17 [49152/50048]	Loss: 1.2828
Training Epoch: 17 [49280/50048]	Loss: 1.2742
Training Epoch: 17 [49408/50048]	Loss: 1.2492
Training Epoch: 17 [49536/50048]	Loss: 1.3546
Training Epoch: 17 [49664/50048]	Loss: 1.2406
Training Epoch: 17 [49792/50048]	Loss: 1.0309
Training Epoch: 17 [49920/50048]	Loss: 1.2506
Training Epoch: 17 [50048/50048]	Loss: 1.2616
Validation Epoch: 17, Average loss: 0.0116, Accuracy: 0.5876
Training Epoch: 18 [128/50048]	Loss: 1.1895
Training Epoch: 18 [256/50048]	Loss: 1.3805
Training Epoch: 18 [384/50048]	Loss: 1.3146
Training Epoch: 18 [512/50048]	Loss: 1.2775
Training Epoch: 18 [640/50048]	Loss: 1.1322
Training Epoch: 18 [768/50048]	Loss: 1.3705
Training Epoch: 18 [896/50048]	Loss: 1.2575
Training Epoch: 18 [1024/50048]	Loss: 1.2988
Training Epoch: 18 [1152/50048]	Loss: 0.9893
Training Epoch: 18 [1280/50048]	Loss: 1.0266
Training Epoch: 18 [1408/50048]	Loss: 1.4472
Training Epoch: 18 [1536/50048]	Loss: 1.3051
Training Epoch: 18 [1664/50048]	Loss: 1.2202
Training Epoch: 18 [1792/50048]	Loss: 1.0777
Training Epoch: 18 [1920/50048]	Loss: 1.3498
Training Epoch: 18 [2048/50048]	Loss: 1.1606
Training Epoch: 18 [2176/50048]	Loss: 1.0492
Training Epoch: 18 [2304/50048]	Loss: 1.2242
Training Epoch: 18 [2432/50048]	Loss: 1.2310
Training Epoch: 18 [2560/50048]	Loss: 1.2112
Training Epoch: 18 [2688/50048]	Loss: 1.3814
Training Epoch: 18 [2816/50048]	Loss: 1.3458
Training Epoch: 18 [2944/50048]	Loss: 1.1017
Training Epoch: 18 [3072/50048]	Loss: 1.3663
Training Epoch: 18 [3200/50048]	Loss: 1.1632
Training Epoch: 18 [3328/50048]	Loss: 1.0764
Training Epoch: 18 [3456/50048]	Loss: 1.2494
Training Epoch: 18 [3584/50048]	Loss: 1.3639
Training Epoch: 18 [3712/50048]	Loss: 1.0417
Training Epoch: 18 [3840/50048]	Loss: 1.2045
Training Epoch: 18 [3968/50048]	Loss: 1.3386
Training Epoch: 18 [4096/50048]	Loss: 1.1041
Training Epoch: 18 [4224/50048]	Loss: 1.1934
Training Epoch: 18 [4352/50048]	Loss: 1.3549
Training Epoch: 18 [4480/50048]	Loss: 1.1283
Training Epoch: 18 [4608/50048]	Loss: 1.2431
Training Epoch: 18 [4736/50048]	Loss: 1.3823
Training Epoch: 18 [4864/50048]	Loss: 1.1821
Training Epoch: 18 [4992/50048]	Loss: 1.1530
Training Epoch: 18 [5120/50048]	Loss: 1.3267
Training Epoch: 18 [5248/50048]	Loss: 0.9589
Training Epoch: 18 [5376/50048]	Loss: 1.1981
Training Epoch: 18 [5504/50048]	Loss: 1.3749
Training Epoch: 18 [5632/50048]	Loss: 1.3102
Training Epoch: 18 [5760/50048]	Loss: 0.9749
Training Epoch: 18 [5888/50048]	Loss: 0.9083
Training Epoch: 18 [6016/50048]	Loss: 1.0698
Training Epoch: 18 [6144/50048]	Loss: 1.1404
Training Epoch: 18 [6272/50048]	Loss: 1.2450
Training Epoch: 18 [6400/50048]	Loss: 1.1238
Training Epoch: 18 [6528/50048]	Loss: 1.0632
Training Epoch: 18 [6656/50048]	Loss: 1.1053
Training Epoch: 18 [6784/50048]	Loss: 1.0807
Training Epoch: 18 [6912/50048]	Loss: 1.1957
Training Epoch: 18 [7040/50048]	Loss: 1.1859
Training Epoch: 18 [7168/50048]	Loss: 1.2010
Training Epoch: 18 [7296/50048]	Loss: 1.5081
Training Epoch: 18 [7424/50048]	Loss: 1.0486
Training Epoch: 18 [7552/50048]	Loss: 1.5900
Training Epoch: 18 [7680/50048]	Loss: 1.0394
Training Epoch: 18 [7808/50048]	Loss: 1.2022
Training Epoch: 18 [7936/50048]	Loss: 1.3347
Training Epoch: 18 [8064/50048]	Loss: 1.0824
Training Epoch: 18 [8192/50048]	Loss: 1.1696
Training Epoch: 18 [8320/50048]	Loss: 1.2949
Training Epoch: 18 [8448/50048]	Loss: 1.4656
Training Epoch: 18 [8576/50048]	Loss: 1.2528
Training Epoch: 18 [8704/50048]	Loss: 1.0950
Training Epoch: 18 [8832/50048]	Loss: 1.3336
Training Epoch: 18 [8960/50048]	Loss: 1.0348
Training Epoch: 18 [9088/50048]	Loss: 1.4889
Training Epoch: 18 [9216/50048]	Loss: 1.2711
Training Epoch: 18 [9344/50048]	Loss: 1.0967
Training Epoch: 18 [9472/50048]	Loss: 1.2372
Training Epoch: 18 [9600/50048]	Loss: 1.1551
Training Epoch: 18 [9728/50048]	Loss: 1.0193
Training Epoch: 18 [9856/50048]	Loss: 1.0547
Training Epoch: 18 [9984/50048]	Loss: 1.1069
Training Epoch: 18 [10112/50048]	Loss: 1.2244
Training Epoch: 18 [10240/50048]	Loss: 1.0016
Training Epoch: 18 [10368/50048]	Loss: 1.2019
Training Epoch: 18 [10496/50048]	Loss: 1.3269
Training Epoch: 18 [10624/50048]	Loss: 1.3647
Training Epoch: 18 [10752/50048]	Loss: 1.1710
Training Epoch: 18 [10880/50048]	Loss: 1.1448
Training Epoch: 18 [11008/50048]	Loss: 1.2141
Training Epoch: 18 [11136/50048]	Loss: 1.3153
Training Epoch: 18 [11264/50048]	Loss: 1.0369
Training Epoch: 18 [11392/50048]	Loss: 1.0132
Training Epoch: 18 [11520/50048]	Loss: 1.0794
Training Epoch: 18 [11648/50048]	Loss: 1.3917
Training Epoch: 18 [11776/50048]	Loss: 1.3007
Training Epoch: 18 [11904/50048]	Loss: 1.1936
Training Epoch: 18 [12032/50048]	Loss: 1.0833
Training Epoch: 18 [12160/50048]	Loss: 1.2930
Training Epoch: 18 [12288/50048]	Loss: 0.9720
Training Epoch: 18 [12416/50048]	Loss: 1.1048
Training Epoch: 18 [12544/50048]	Loss: 1.2408
Training Epoch: 18 [12672/50048]	Loss: 1.4878
Training Epoch: 18 [12800/50048]	Loss: 1.1534
Training Epoch: 18 [12928/50048]	Loss: 1.1517
Training Epoch: 18 [13056/50048]	Loss: 1.1760
Training Epoch: 18 [13184/50048]	Loss: 1.0226
Training Epoch: 18 [13312/50048]	Loss: 1.0439
Training Epoch: 18 [13440/50048]	Loss: 1.3571
Training Epoch: 18 [13568/50048]	Loss: 1.1909
Training Epoch: 18 [13696/50048]	Loss: 1.2417
Training Epoch: 18 [13824/50048]	Loss: 1.1132
Training Epoch: 18 [13952/50048]	Loss: 1.3126
Training Epoch: 18 [14080/50048]	Loss: 1.1423
Training Epoch: 18 [14208/50048]	Loss: 1.2554
Training Epoch: 18 [14336/50048]	Loss: 0.9641
Training Epoch: 18 [14464/50048]	Loss: 1.1865
Training Epoch: 18 [14592/50048]	Loss: 1.2808
Training Epoch: 18 [14720/50048]	Loss: 1.4662
Training Epoch: 18 [14848/50048]	Loss: 1.0373
Training Epoch: 18 [14976/50048]	Loss: 1.0367
Training Epoch: 18 [15104/50048]	Loss: 1.2843
Training Epoch: 18 [15232/50048]	Loss: 1.2738
Training Epoch: 18 [15360/50048]	Loss: 1.1672
Training Epoch: 18 [15488/50048]	Loss: 1.3270
Training Epoch: 18 [15616/50048]	Loss: 1.1422
Training Epoch: 18 [15744/50048]	Loss: 1.3874
Training Epoch: 18 [15872/50048]	Loss: 1.1802
Training Epoch: 18 [16000/50048]	Loss: 1.1581
Training Epoch: 18 [16128/50048]	Loss: 1.2727
Training Epoch: 18 [16256/50048]	Loss: 1.0788
Training Epoch: 18 [16384/50048]	Loss: 1.2997
Training Epoch: 18 [16512/50048]	Loss: 1.1814
Training Epoch: 18 [16640/50048]	Loss: 1.1711
Training Epoch: 18 [16768/50048]	Loss: 1.4957
Training Epoch: 18 [16896/50048]	Loss: 1.0493
Training Epoch: 18 [17024/50048]	Loss: 1.1288
Training Epoch: 18 [17152/50048]	Loss: 1.1739
Training Epoch: 18 [17280/50048]	Loss: 1.3252
Training Epoch: 18 [17408/50048]	Loss: 1.0138
Training Epoch: 18 [17536/50048]	Loss: 1.0510
Training Epoch: 18 [17664/50048]	Loss: 1.2676
Training Epoch: 18 [17792/50048]	Loss: 1.3905
Training Epoch: 18 [17920/50048]	Loss: 1.2044
Training Epoch: 18 [18048/50048]	Loss: 1.3852
Training Epoch: 18 [18176/50048]	Loss: 1.4312
Training Epoch: 18 [18304/50048]	Loss: 1.1040
Training Epoch: 18 [18432/50048]	Loss: 1.3785
Training Epoch: 18 [18560/50048]	Loss: 1.2016
Training Epoch: 18 [18688/50048]	Loss: 1.3032
Training Epoch: 18 [18816/50048]	Loss: 1.0167
Training Epoch: 18 [18944/50048]	Loss: 1.4231
Training Epoch: 18 [19072/50048]	Loss: 1.2047
Training Epoch: 18 [19200/50048]	Loss: 1.5060
Training Epoch: 18 [19328/50048]	Loss: 1.3201
Training Epoch: 18 [19456/50048]	Loss: 1.4826
Training Epoch: 18 [19584/50048]	Loss: 1.3662
Training Epoch: 18 [19712/50048]	Loss: 1.2758
Training Epoch: 18 [19840/50048]	Loss: 1.4190
Training Epoch: 18 [19968/50048]	Loss: 1.1828
Training Epoch: 18 [20096/50048]	Loss: 1.3768
Training Epoch: 18 [20224/50048]	Loss: 1.3031
Training Epoch: 18 [20352/50048]	Loss: 1.2817
Training Epoch: 18 [20480/50048]	Loss: 1.4043
Training Epoch: 18 [20608/50048]	Loss: 1.5376
Training Epoch: 18 [20736/50048]	Loss: 1.1405
Training Epoch: 18 [20864/50048]	Loss: 1.1276
Training Epoch: 18 [20992/50048]	Loss: 1.1346
Training Epoch: 18 [21120/50048]	Loss: 1.0889
Training Epoch: 18 [21248/50048]	Loss: 1.3423
Training Epoch: 18 [21376/50048]	Loss: 1.3169
Training Epoch: 18 [21504/50048]	Loss: 1.3053
Training Epoch: 18 [21632/50048]	Loss: 1.3332
Training Epoch: 18 [21760/50048]	Loss: 1.3043
Training Epoch: 18 [21888/50048]	Loss: 1.2810
Training Epoch: 18 [22016/50048]	Loss: 1.3625
Training Epoch: 18 [22144/50048]	Loss: 1.5586
Training Epoch: 18 [22272/50048]	Loss: 1.3146
Training Epoch: 18 [22400/50048]	Loss: 1.0389
Training Epoch: 18 [22528/50048]	Loss: 1.0940
Training Epoch: 18 [22656/50048]	Loss: 1.1249
Training Epoch: 18 [22784/50048]	Loss: 1.2812
Training Epoch: 18 [22912/50048]	Loss: 1.1475
Training Epoch: 18 [23040/50048]	Loss: 0.9403
Training Epoch: 18 [23168/50048]	Loss: 1.2195
Training Epoch: 18 [23296/50048]	Loss: 1.5594
Training Epoch: 18 [23424/50048]	Loss: 1.1519
Training Epoch: 18 [23552/50048]	Loss: 1.3485
Training Epoch: 18 [23680/50048]	Loss: 1.0356
Training Epoch: 18 [23808/50048]	Loss: 1.1938
Training Epoch: 18 [23936/50048]	Loss: 1.1751
Training Epoch: 18 [24064/50048]	Loss: 1.3958
Training Epoch: 18 [24192/50048]	Loss: 1.2691
Training Epoch: 18 [24320/50048]	Loss: 1.0930
Training Epoch: 18 [24448/50048]	Loss: 1.3869
Training Epoch: 18 [24576/50048]	Loss: 1.3489
Training Epoch: 18 [24704/50048]	Loss: 0.9586
Training Epoch: 18 [24832/50048]	Loss: 1.3728
Training Epoch: 18 [24960/50048]	Loss: 1.4111
Training Epoch: 18 [25088/50048]	Loss: 1.2368
Training Epoch: 18 [25216/50048]	Loss: 1.2870
Training Epoch: 18 [25344/50048]	Loss: 1.4342
Training Epoch: 18 [25472/50048]	Loss: 1.2582
Training Epoch: 18 [25600/50048]	Loss: 1.3909
Training Epoch: 18 [25728/50048]	Loss: 1.3193
Training Epoch: 18 [25856/50048]	Loss: 1.3186
Training Epoch: 18 [25984/50048]	Loss: 1.0363
Training Epoch: 18 [26112/50048]	Loss: 1.2034
Training Epoch: 18 [26240/50048]	Loss: 1.4198
Training Epoch: 18 [26368/50048]	Loss: 1.1943
Training Epoch: 18 [26496/50048]	Loss: 1.2037
Training Epoch: 18 [26624/50048]	Loss: 1.2385
Training Epoch: 18 [26752/50048]	Loss: 1.3207
Training Epoch: 18 [26880/50048]	Loss: 1.1666
Training Epoch: 18 [27008/50048]	Loss: 1.0946
Training Epoch: 18 [27136/50048]	Loss: 1.1994
Training Epoch: 18 [27264/50048]	Loss: 1.6419
Training Epoch: 18 [27392/50048]	Loss: 1.1740
Training Epoch: 18 [27520/50048]	Loss: 1.1778
Training Epoch: 18 [27648/50048]	Loss: 1.2151
Training Epoch: 18 [27776/50048]	Loss: 1.3266
Training Epoch: 18 [27904/50048]	Loss: 1.0845
Training Epoch: 18 [28032/50048]	Loss: 1.3741
Training Epoch: 18 [28160/50048]	Loss: 1.3253
Training Epoch: 18 [28288/50048]	Loss: 1.1454
Training Epoch: 18 [28416/50048]	Loss: 1.2516
Training Epoch: 18 [28544/50048]	Loss: 1.1723
Training Epoch: 18 [28672/50048]	Loss: 1.2344
Training Epoch: 18 [28800/50048]	Loss: 1.1361
Training Epoch: 18 [28928/50048]	Loss: 1.1862
Training Epoch: 18 [29056/50048]	Loss: 1.3921
Training Epoch: 18 [29184/50048]	Loss: 1.3680
Training Epoch: 18 [29312/50048]	Loss: 1.3092
Training Epoch: 18 [29440/50048]	Loss: 1.1867
Training Epoch: 18 [29568/50048]	Loss: 1.5316
Training Epoch: 18 [29696/50048]	Loss: 1.0431
Training Epoch: 18 [29824/50048]	Loss: 1.1770
Training Epoch: 18 [29952/50048]	Loss: 1.2038
Training Epoch: 18 [30080/50048]	Loss: 1.3007
Training Epoch: 18 [30208/50048]	Loss: 1.0211
Training Epoch: 18 [30336/50048]	Loss: 1.5065
Training Epoch: 18 [30464/50048]	Loss: 1.2748
Training Epoch: 18 [30592/50048]	Loss: 1.2960
Training Epoch: 18 [30720/50048]	Loss: 1.2386
Training Epoch: 18 [30848/50048]	Loss: 1.2767
Training Epoch: 18 [30976/50048]	Loss: 1.3193
Training Epoch: 18 [31104/50048]	Loss: 1.2520
Training Epoch: 18 [31232/50048]	Loss: 1.2401
Training Epoch: 18 [31360/50048]	Loss: 1.4196
Training Epoch: 18 [31488/50048]	Loss: 1.1323
Training Epoch: 18 [31616/50048]	Loss: 1.1662
Training Epoch: 18 [31744/50048]	Loss: 1.1041
Training Epoch: 18 [31872/50048]	Loss: 1.2990
Training Epoch: 18 [32000/50048]	Loss: 1.2288
Training Epoch: 18 [32128/50048]	Loss: 1.2275
Training Epoch: 18 [32256/50048]	Loss: 1.0690
Training Epoch: 18 [32384/50048]	Loss: 1.4677
Training Epoch: 18 [32512/50048]	Loss: 1.4567
Training Epoch: 18 [32640/50048]	Loss: 1.2227
Training Epoch: 18 [32768/50048]	Loss: 1.1591
Training Epoch: 18 [32896/50048]	Loss: 1.2602
Training Epoch: 18 [33024/50048]	Loss: 1.2027
Training Epoch: 18 [33152/50048]	Loss: 1.4034
Training Epoch: 18 [33280/50048]	Loss: 1.1521
Training Epoch: 18 [33408/50048]	Loss: 1.1000
Training Epoch: 18 [33536/50048]	Loss: 1.1382
Training Epoch: 18 [33664/50048]	Loss: 1.2407
Training Epoch: 18 [33792/50048]	Loss: 1.0846
Training Epoch: 18 [33920/50048]	Loss: 1.1194
Training Epoch: 18 [34048/50048]	Loss: 1.2057
Training Epoch: 18 [34176/50048]	Loss: 1.2265
Training Epoch: 18 [34304/50048]	Loss: 1.2157
Training Epoch: 18 [34432/50048]	Loss: 1.1181
Training Epoch: 18 [34560/50048]	Loss: 1.1620
Training Epoch: 18 [34688/50048]	Loss: 1.1191
Training Epoch: 18 [34816/50048]	Loss: 1.3175
Training Epoch: 18 [34944/50048]	Loss: 1.2098
Training Epoch: 18 [35072/50048]	Loss: 1.1475
Training Epoch: 18 [35200/50048]	Loss: 1.2248
Training Epoch: 18 [35328/50048]	Loss: 1.2662
Training Epoch: 18 [35456/50048]	Loss: 1.3833
Training Epoch: 18 [35584/50048]	Loss: 1.0964
Training Epoch: 18 [35712/50048]	Loss: 1.1201
Training Epoch: 18 [35840/50048]	Loss: 1.0392
Training Epoch: 18 [35968/50048]	Loss: 1.0167
Training Epoch: 18 [36096/50048]	Loss: 1.5088
Training Epoch: 18 [36224/50048]	Loss: 1.0078
Training Epoch: 18 [36352/50048]	Loss: 1.6224
Training Epoch: 18 [36480/50048]	Loss: 1.7169
Training Epoch: 18 [36608/50048]	Loss: 1.2236
Training Epoch: 18 [36736/50048]	Loss: 1.4483
Training Epoch: 18 [36864/50048]	Loss: 1.2634
Training Epoch: 18 [36992/50048]	Loss: 1.3765
Training Epoch: 18 [37120/50048]	Loss: 1.3674
Training Epoch: 18 [37248/50048]	Loss: 1.2025
Training Epoch: 18 [37376/50048]	Loss: 1.2156
Training Epoch: 18 [37504/50048]	Loss: 1.1793
Training Epoch: 18 [37632/50048]	Loss: 1.2770
Training Epoch: 18 [37760/50048]	Loss: 1.0819
Training Epoch: 18 [37888/50048]	Loss: 1.4003
Training Epoch: 18 [38016/50048]	Loss: 1.1012
Training Epoch: 18 [38144/50048]	Loss: 1.0080
Training Epoch: 18 [38272/50048]	Loss: 1.4487
Training Epoch: 18 [38400/50048]	Loss: 1.1505
Training Epoch: 18 [38528/50048]	Loss: 1.2350
Training Epoch: 18 [38656/50048]	Loss: 1.3667
Training Epoch: 18 [38784/50048]	Loss: 1.3919
Training Epoch: 18 [38912/50048]	Loss: 1.0296
Training Epoch: 18 [39040/50048]	Loss: 1.2235
Training Epoch: 18 [39168/50048]	Loss: 0.8562
Training Epoch: 18 [39296/50048]	Loss: 1.4003
Training Epoch: 18 [39424/50048]	Loss: 1.2287
Training Epoch: 18 [39552/50048]	Loss: 1.2469
Training Epoch: 18 [39680/50048]	Loss: 1.1629
Training Epoch: 18 [39808/50048]	Loss: 1.3946
Training Epoch: 18 [39936/50048]	Loss: 1.3399
Training Epoch: 18 [40064/50048]	Loss: 1.0493
Training Epoch: 18 [40192/50048]	Loss: 1.1904
Training Epoch: 18 [40320/50048]	Loss: 1.2172
Training Epoch: 18 [40448/50048]	Loss: 1.3013
Training Epoch: 18 [40576/50048]	Loss: 1.2718
Training Epoch: 18 [40704/50048]	Loss: 1.3545
Training Epoch: 18 [40832/50048]	Loss: 1.1257
Training Epoch: 18 [40960/50048]	Loss: 1.5360
Training Epoch: 18 [41088/50048]	Loss: 1.4524
Training Epoch: 18 [41216/50048]	Loss: 1.0662
Training Epoch: 18 [41344/50048]	Loss: 1.2232
Training Epoch: 18 [41472/50048]	Loss: 1.0559
Training Epoch: 18 [41600/50048]	Loss: 1.2716
Training Epoch: 18 [41728/50048]	Loss: 1.3705
Training Epoch: 18 [41856/50048]	Loss: 1.0873
Training Epoch: 18 [41984/50048]	Loss: 1.1880
Training Epoch: 18 [42112/50048]	Loss: 1.1520
Training Epoch: 18 [42240/50048]	Loss: 1.2204
Training Epoch: 18 [42368/50048]	Loss: 1.1038
Training Epoch: 18 [42496/50048]	Loss: 1.1044
Training Epoch: 18 [42624/50048]	Loss: 1.2424
Training Epoch: 18 [42752/50048]	Loss: 1.3370
Training Epoch: 18 [42880/50048]	Loss: 1.0479
Training Epoch: 18 [43008/50048]	Loss: 1.0544
Training Epoch: 18 [43136/50048]	Loss: 1.3556
Training Epoch: 18 [43264/50048]	Loss: 1.1960
Training Epoch: 18 [43392/50048]	Loss: 1.3309
Training Epoch: 18 [43520/50048]	Loss: 1.4240
Training Epoch: 18 [43648/50048]	Loss: 1.2481
Training Epoch: 18 [43776/50048]	Loss: 1.1678
Training Epoch: 18 [43904/50048]	Loss: 1.4095
Training Epoch: 18 [44032/50048]	Loss: 1.3345
Training Epoch: 18 [44160/50048]	Loss: 1.2677
Training Epoch: 18 [44288/50048]	Loss: 1.2485
Training Epoch: 18 [44416/50048]	Loss: 1.2085
Training Epoch: 18 [44544/50048]	Loss: 1.1332
Training Epoch: 18 [44672/50048]	Loss: 1.1326
Training Epoch: 18 [44800/50048]	Loss: 1.1990
Training Epoch: 18 [44928/50048]	Loss: 1.2682
Training Epoch: 18 [45056/50048]	Loss: 1.2233
Training Epoch: 18 [45184/50048]	Loss: 1.1511
Training Epoch: 18 [45312/50048]	Loss: 1.4904
Training Epoch: 18 [45440/50048]	Loss: 1.2925
Training Epoch: 18 [45568/50048]	Loss: 1.0424
Training Epoch: 18 [45696/50048]	Loss: 1.2008
Training Epoch: 18 [45824/50048]	Loss: 1.1294
Training Epoch: 18 [45952/50048]	Loss: 1.2215
Training Epoch: 18 [46080/50048]	Loss: 1.2182
Training Epoch: 18 [46208/50048]	Loss: 1.2662
Training Epoch: 18 [46336/50048]	Loss: 1.2383
Training Epoch: 18 [46464/50048]	Loss: 1.1842
Training Epoch: 18 [46592/50048]	Loss: 1.2146
Training Epoch: 18 [46720/50048]	Loss: 1.1709
Training Epoch: 18 [46848/50048]	Loss: 1.1996
Training Epoch: 18 [46976/50048]	Loss: 1.1653
Training Epoch: 18 [47104/50048]	Loss: 1.2126
Training Epoch: 18 [47232/50048]	Loss: 1.2799
Training Epoch: 18 [47360/50048]	Loss: 1.0792
Training Epoch: 18 [47488/50048]	Loss: 1.0802
Training Epoch: 18 [47616/50048]	Loss: 1.1548
Training Epoch: 18 [47744/50048]	Loss: 0.9724
Training Epoch: 18 [47872/50048]	Loss: 1.1238
Training Epoch: 18 [48000/50048]	Loss: 1.1818
Training Epoch: 18 [48128/50048]	Loss: 1.1640
Training Epoch: 18 [48256/50048]	Loss: 1.1319
Training Epoch: 18 [48384/50048]	Loss: 1.0257
Training Epoch: 18 [48512/50048]	Loss: 1.2337
Training Epoch: 18 [48640/50048]	Loss: 1.0687
Training Epoch: 18 [48768/50048]	Loss: 1.3249
Training Epoch: 18 [48896/50048]	Loss: 1.2011
Training Epoch: 18 [49024/50048]	Loss: 1.2912
Training Epoch: 18 [49152/50048]	Loss: 1.2242
Training Epoch: 18 [49280/50048]	Loss: 1.3826
Training Epoch: 18 [49408/50048]	Loss: 1.2169
Training Epoch: 18 [49536/50048]	Loss: 1.1828
Training Epoch: 18 [49664/50048]	Loss: 1.2801
Training Epoch: 18 [49792/50048]	Loss: 1.2569
Training Epoch: 18 [49920/50048]	Loss: 1.2917
Training Epoch: 18 [50048/50048]	Loss: 1.1819
Validation Epoch: 18, Average loss: 0.0114, Accuracy: 0.5954
Training Epoch: 19 [128/50048]	Loss: 1.2129
Training Epoch: 19 [256/50048]	Loss: 1.1062
Training Epoch: 19 [384/50048]	Loss: 1.2380
Training Epoch: 19 [512/50048]	Loss: 1.0702
Training Epoch: 19 [640/50048]	Loss: 1.1234
Training Epoch: 19 [768/50048]	Loss: 1.1866
Training Epoch: 19 [896/50048]	Loss: 1.1398
Training Epoch: 19 [1024/50048]	Loss: 1.1814
Training Epoch: 19 [1152/50048]	Loss: 1.0280
Training Epoch: 19 [1280/50048]	Loss: 1.2434
Training Epoch: 19 [1408/50048]	Loss: 1.1681
Training Epoch: 19 [1536/50048]	Loss: 1.0236
Training Epoch: 19 [1664/50048]	Loss: 1.1628
Training Epoch: 19 [1792/50048]	Loss: 1.1451
Training Epoch: 19 [1920/50048]	Loss: 1.1679
Training Epoch: 19 [2048/50048]	Loss: 1.2612
Training Epoch: 19 [2176/50048]	Loss: 1.0648
Training Epoch: 19 [2304/50048]	Loss: 1.2730
Training Epoch: 19 [2432/50048]	Loss: 1.3371
Training Epoch: 19 [2560/50048]	Loss: 1.2118
Training Epoch: 19 [2688/50048]	Loss: 1.0741
Training Epoch: 19 [2816/50048]	Loss: 1.0976
Training Epoch: 19 [2944/50048]	Loss: 1.2681
Training Epoch: 19 [3072/50048]	Loss: 1.1647
Training Epoch: 19 [3200/50048]	Loss: 1.2479
Training Epoch: 19 [3328/50048]	Loss: 1.0083
Training Epoch: 19 [3456/50048]	Loss: 0.8620
Training Epoch: 19 [3584/50048]	Loss: 1.2323
Training Epoch: 19 [3712/50048]	Loss: 1.4410
Training Epoch: 19 [3840/50048]	Loss: 1.1458
Training Epoch: 19 [3968/50048]	Loss: 1.2837
Training Epoch: 19 [4096/50048]	Loss: 0.8926
Training Epoch: 19 [4224/50048]	Loss: 1.0122
Training Epoch: 19 [4352/50048]	Loss: 1.1783
Training Epoch: 19 [4480/50048]	Loss: 1.2695
Training Epoch: 19 [4608/50048]	Loss: 1.4059
Training Epoch: 19 [4736/50048]	Loss: 1.3368
Training Epoch: 19 [4864/50048]	Loss: 1.0984
Training Epoch: 19 [4992/50048]	Loss: 1.1041
Training Epoch: 19 [5120/50048]	Loss: 1.2117
Training Epoch: 19 [5248/50048]	Loss: 1.1376
Training Epoch: 19 [5376/50048]	Loss: 1.0888
Training Epoch: 19 [5504/50048]	Loss: 1.2683
Training Epoch: 19 [5632/50048]	Loss: 1.3731
Training Epoch: 19 [5760/50048]	Loss: 1.4381
Training Epoch: 19 [5888/50048]	Loss: 1.2983
Training Epoch: 19 [6016/50048]	Loss: 1.2201
Training Epoch: 19 [6144/50048]	Loss: 1.1910
Training Epoch: 19 [6272/50048]	Loss: 0.9751
Training Epoch: 19 [6400/50048]	Loss: 1.1376
Training Epoch: 19 [6528/50048]	Loss: 1.2097
Training Epoch: 19 [6656/50048]	Loss: 1.1992
Training Epoch: 19 [6784/50048]	Loss: 1.0778
Training Epoch: 19 [6912/50048]	Loss: 1.0547
Training Epoch: 19 [7040/50048]	Loss: 1.0966
Training Epoch: 19 [7168/50048]	Loss: 1.3106
Training Epoch: 19 [7296/50048]	Loss: 1.3658
Training Epoch: 19 [7424/50048]	Loss: 1.3829
Training Epoch: 19 [7552/50048]	Loss: 1.3637
Training Epoch: 19 [7680/50048]	Loss: 1.0267
Training Epoch: 19 [7808/50048]	Loss: 1.0926
Training Epoch: 19 [7936/50048]	Loss: 0.9694
Training Epoch: 19 [8064/50048]	Loss: 1.0390
Training Epoch: 19 [8192/50048]	Loss: 1.0856
Training Epoch: 19 [8320/50048]	Loss: 1.1924
Training Epoch: 19 [8448/50048]	Loss: 1.1907
Training Epoch: 19 [8576/50048]	Loss: 1.2343
Training Epoch: 19 [8704/50048]	Loss: 1.2325
Training Epoch: 19 [8832/50048]	Loss: 1.0033
Training Epoch: 19 [8960/50048]	Loss: 1.5191
Training Epoch: 19 [9088/50048]	Loss: 0.9124
Training Epoch: 19 [9216/50048]	Loss: 1.1617
Training Epoch: 19 [9344/50048]	Loss: 1.4722
Training Epoch: 19 [9472/50048]	Loss: 1.4882
Training Epoch: 19 [9600/50048]	Loss: 1.1786
Training Epoch: 19 [9728/50048]	Loss: 1.5304
Training Epoch: 19 [9856/50048]	Loss: 1.4088
Training Epoch: 19 [9984/50048]	Loss: 1.3685
Training Epoch: 19 [10112/50048]	Loss: 1.1949
Training Epoch: 19 [10240/50048]	Loss: 1.2451
Training Epoch: 19 [10368/50048]	Loss: 0.9866
Training Epoch: 19 [10496/50048]	Loss: 1.3406
Training Epoch: 19 [10624/50048]	Loss: 1.3199
Training Epoch: 19 [10752/50048]	Loss: 1.0640
Training Epoch: 19 [10880/50048]	Loss: 1.1908
Training Epoch: 19 [11008/50048]	Loss: 1.1099
Training Epoch: 19 [11136/50048]	Loss: 1.1829
Training Epoch: 19 [11264/50048]	Loss: 1.4701
Training Epoch: 19 [11392/50048]	Loss: 1.2657
Training Epoch: 19 [11520/50048]	Loss: 1.3928
Training Epoch: 19 [11648/50048]	Loss: 1.5484
Training Epoch: 19 [11776/50048]	Loss: 1.2964
Training Epoch: 19 [11904/50048]	Loss: 1.1368
Training Epoch: 19 [12032/50048]	Loss: 1.1355
Training Epoch: 19 [12160/50048]	Loss: 1.1649
Training Epoch: 19 [12288/50048]	Loss: 0.9919
Training Epoch: 19 [12416/50048]	Loss: 1.4032
Training Epoch: 19 [12544/50048]	Loss: 1.2523
Training Epoch: 19 [12672/50048]	Loss: 1.1064
Training Epoch: 19 [12800/50048]	Loss: 1.1380
Training Epoch: 19 [12928/50048]	Loss: 1.0622
Training Epoch: 19 [13056/50048]	Loss: 1.1253
Training Epoch: 19 [13184/50048]	Loss: 1.4015
Training Epoch: 19 [13312/50048]	Loss: 1.1981
Training Epoch: 19 [13440/50048]	Loss: 1.0786
Training Epoch: 19 [13568/50048]	Loss: 1.0393
Training Epoch: 19 [13696/50048]	Loss: 1.2567
Training Epoch: 19 [13824/50048]	Loss: 1.2938
Training Epoch: 19 [13952/50048]	Loss: 1.1651
Training Epoch: 19 [14080/50048]	Loss: 1.3920
Training Epoch: 19 [14208/50048]	Loss: 1.1410
Training Epoch: 19 [14336/50048]	Loss: 1.3275
Training Epoch: 19 [14464/50048]	Loss: 1.0279
Training Epoch: 19 [14592/50048]	Loss: 1.3125
Training Epoch: 19 [14720/50048]	Loss: 1.0061
Training Epoch: 19 [14848/50048]	Loss: 1.2340
Training Epoch: 19 [14976/50048]	Loss: 1.1636
Training Epoch: 19 [15104/50048]	Loss: 1.4467
Training Epoch: 19 [15232/50048]	Loss: 1.0685
Training Epoch: 19 [15360/50048]	Loss: 1.2510
Training Epoch: 19 [15488/50048]	Loss: 1.3289
Training Epoch: 19 [15616/50048]	Loss: 1.3248
Training Epoch: 19 [15744/50048]	Loss: 1.2086
Training Epoch: 19 [15872/50048]	Loss: 1.3257
Training Epoch: 19 [16000/50048]	Loss: 1.1256
Training Epoch: 19 [16128/50048]	Loss: 0.9985
Training Epoch: 19 [16256/50048]	Loss: 1.4086
Training Epoch: 19 [16384/50048]	Loss: 1.1334
Training Epoch: 19 [16512/50048]	Loss: 1.1867
Training Epoch: 19 [16640/50048]	Loss: 1.2415
Training Epoch: 19 [16768/50048]	Loss: 1.0933
Training Epoch: 19 [16896/50048]	Loss: 1.1147
Training Epoch: 19 [17024/50048]	Loss: 1.0406
Training Epoch: 19 [17152/50048]	Loss: 1.3336
Training Epoch: 19 [17280/50048]	Loss: 1.1432
Training Epoch: 19 [17408/50048]	Loss: 1.2252
Training Epoch: 19 [17536/50048]	Loss: 1.1881
Training Epoch: 19 [17664/50048]	Loss: 1.0705
Training Epoch: 19 [17792/50048]	Loss: 1.2152
Training Epoch: 19 [17920/50048]	Loss: 1.1224
Training Epoch: 19 [18048/50048]	Loss: 1.2918
Training Epoch: 19 [18176/50048]	Loss: 1.2426
Training Epoch: 19 [18304/50048]	Loss: 1.3171
Training Epoch: 19 [18432/50048]	Loss: 1.1414
Training Epoch: 19 [18560/50048]	Loss: 1.3251
Training Epoch: 19 [18688/50048]	Loss: 1.1729
Training Epoch: 19 [18816/50048]	Loss: 1.2335
Training Epoch: 19 [18944/50048]	Loss: 1.3566
Training Epoch: 19 [19072/50048]	Loss: 1.2540
Training Epoch: 19 [19200/50048]	Loss: 1.2570
Training Epoch: 19 [19328/50048]	Loss: 1.1869
Training Epoch: 19 [19456/50048]	Loss: 1.1725
Training Epoch: 19 [19584/50048]	Loss: 1.0596
Training Epoch: 19 [19712/50048]	Loss: 1.2980
Training Epoch: 19 [19840/50048]	Loss: 1.3507
Training Epoch: 19 [19968/50048]	Loss: 1.1945
Training Epoch: 19 [20096/50048]	Loss: 1.2722
Training Epoch: 19 [20224/50048]	Loss: 1.2065
Training Epoch: 19 [20352/50048]	Loss: 1.3815
Training Epoch: 19 [20480/50048]	Loss: 1.1679
Training Epoch: 19 [20608/50048]	Loss: 1.0960
Training Epoch: 19 [20736/50048]	Loss: 1.1352
Training Epoch: 19 [20864/50048]	Loss: 1.1022
Training Epoch: 19 [20992/50048]	Loss: 1.1700
Training Epoch: 19 [21120/50048]	Loss: 1.3686
Training Epoch: 19 [21248/50048]	Loss: 1.1350
Training Epoch: 19 [21376/50048]	Loss: 1.1234
Training Epoch: 19 [21504/50048]	Loss: 1.0667
Training Epoch: 19 [21632/50048]	Loss: 1.3043
Training Epoch: 19 [21760/50048]	Loss: 1.2870
Training Epoch: 19 [21888/50048]	Loss: 1.1653
Training Epoch: 19 [22016/50048]	Loss: 1.1373
Training Epoch: 19 [22144/50048]	Loss: 1.1906
Training Epoch: 19 [22272/50048]	Loss: 1.3216
Training Epoch: 19 [22400/50048]	Loss: 1.2513
Training Epoch: 19 [22528/50048]	Loss: 1.1748
Training Epoch: 19 [22656/50048]	Loss: 1.3618
Training Epoch: 19 [22784/50048]	Loss: 1.1728
Training Epoch: 19 [22912/50048]	Loss: 1.1970
Training Epoch: 19 [23040/50048]	Loss: 0.9211
Training Epoch: 19 [23168/50048]	Loss: 1.3151
Training Epoch: 19 [23296/50048]	Loss: 1.1954
Training Epoch: 19 [23424/50048]	Loss: 1.1304
Training Epoch: 19 [23552/50048]	Loss: 0.9985
Training Epoch: 19 [23680/50048]	Loss: 1.1804
Training Epoch: 19 [23808/50048]	Loss: 1.3257
Training Epoch: 19 [23936/50048]	Loss: 1.0190
Training Epoch: 19 [24064/50048]	Loss: 1.2279
Training Epoch: 19 [24192/50048]	Loss: 1.3869
Training Epoch: 19 [24320/50048]	Loss: 1.1323
Training Epoch: 19 [24448/50048]	Loss: 1.2286
Training Epoch: 19 [24576/50048]	Loss: 1.3242
Training Epoch: 19 [24704/50048]	Loss: 1.3542
Training Epoch: 19 [24832/50048]	Loss: 1.3696
Training Epoch: 19 [24960/50048]	Loss: 0.9112
Training Epoch: 19 [25088/50048]	Loss: 1.1774
Training Epoch: 19 [25216/50048]	Loss: 1.3468
Training Epoch: 19 [25344/50048]	Loss: 1.1358
Training Epoch: 19 [25472/50048]	Loss: 1.2180
Training Epoch: 19 [25600/50048]	Loss: 1.3449
Training Epoch: 19 [25728/50048]	Loss: 1.3520
Training Epoch: 19 [25856/50048]	Loss: 1.0740
Training Epoch: 19 [25984/50048]	Loss: 1.2004
Training Epoch: 19 [26112/50048]	Loss: 1.4755
Training Epoch: 19 [26240/50048]	Loss: 1.1427
Training Epoch: 19 [26368/50048]	Loss: 1.2600
Training Epoch: 19 [26496/50048]	Loss: 1.3403
Training Epoch: 19 [26624/50048]	Loss: 1.2567
Training Epoch: 19 [26752/50048]	Loss: 1.3080
Training Epoch: 19 [26880/50048]	Loss: 1.0349
Training Epoch: 19 [27008/50048]	Loss: 1.0383
Training Epoch: 19 [27136/50048]	Loss: 1.3023
Training Epoch: 19 [27264/50048]	Loss: 1.3508
Training Epoch: 19 [27392/50048]	Loss: 1.2313
Training Epoch: 19 [27520/50048]	Loss: 1.1454
Training Epoch: 19 [27648/50048]	Loss: 1.1519
Training Epoch: 19 [27776/50048]	Loss: 1.1498
Training Epoch: 19 [27904/50048]	Loss: 1.2547
Training Epoch: 19 [28032/50048]	Loss: 1.1510
Training Epoch: 19 [28160/50048]	Loss: 1.0444
Training Epoch: 19 [28288/50048]	Loss: 1.0812
Training Epoch: 19 [28416/50048]	Loss: 1.1463
Training Epoch: 19 [28544/50048]	Loss: 1.1791
Training Epoch: 19 [28672/50048]	Loss: 0.9524
Training Epoch: 19 [28800/50048]	Loss: 1.3119
Training Epoch: 19 [28928/50048]	Loss: 1.1997
Training Epoch: 19 [29056/50048]	Loss: 1.0148
Training Epoch: 19 [29184/50048]	Loss: 1.3269
Training Epoch: 19 [29312/50048]	Loss: 1.3316
Training Epoch: 19 [29440/50048]	Loss: 1.0332
Training Epoch: 19 [29568/50048]	Loss: 1.2701
Training Epoch: 19 [29696/50048]	Loss: 1.1055
Training Epoch: 19 [29824/50048]	Loss: 1.1824
Training Epoch: 19 [29952/50048]	Loss: 1.4043
Training Epoch: 19 [30080/50048]	Loss: 1.2632
Training Epoch: 19 [30208/50048]	Loss: 1.1517
Training Epoch: 19 [30336/50048]	Loss: 1.1155
Training Epoch: 19 [30464/50048]	Loss: 1.1880
Training Epoch: 19 [30592/50048]	Loss: 1.0511
Training Epoch: 19 [30720/50048]	Loss: 1.2457
Training Epoch: 19 [30848/50048]	Loss: 1.4047
Training Epoch: 19 [30976/50048]	Loss: 1.0064
Training Epoch: 19 [31104/50048]	Loss: 1.1554
Training Epoch: 19 [31232/50048]	Loss: 1.2168
Training Epoch: 19 [31360/50048]	Loss: 1.2105
Training Epoch: 19 [31488/50048]	Loss: 1.3311
Training Epoch: 19 [31616/50048]	Loss: 1.0203
Training Epoch: 19 [31744/50048]	Loss: 1.2877
Training Epoch: 19 [31872/50048]	Loss: 1.0270
Training Epoch: 19 [32000/50048]	Loss: 1.1749
Training Epoch: 19 [32128/50048]	Loss: 1.4006
Training Epoch: 19 [32256/50048]	Loss: 1.1307
Training Epoch: 19 [32384/50048]	Loss: 1.1022
Training Epoch: 19 [32512/50048]	Loss: 1.2419
Training Epoch: 19 [32640/50048]	Loss: 1.1824
Training Epoch: 19 [32768/50048]	Loss: 1.0776
Training Epoch: 19 [32896/50048]	Loss: 1.2800
Training Epoch: 19 [33024/50048]	Loss: 1.3581
Training Epoch: 19 [33152/50048]	Loss: 1.1309
Training Epoch: 19 [33280/50048]	Loss: 1.1929
Training Epoch: 19 [33408/50048]	Loss: 1.0365
Training Epoch: 19 [33536/50048]	Loss: 1.3517
Training Epoch: 19 [33664/50048]	Loss: 1.2980
Training Epoch: 19 [33792/50048]	Loss: 1.2600
Training Epoch: 19 [33920/50048]	Loss: 1.1875
Training Epoch: 19 [34048/50048]	Loss: 1.0996
Training Epoch: 19 [34176/50048]	Loss: 1.4340
Training Epoch: 19 [34304/50048]	Loss: 1.2773
Training Epoch: 19 [34432/50048]	Loss: 1.3699
Training Epoch: 19 [34560/50048]	Loss: 1.2590
Training Epoch: 19 [34688/50048]	Loss: 1.0129
Training Epoch: 19 [34816/50048]	Loss: 1.1297
Training Epoch: 19 [34944/50048]	Loss: 1.0832
Training Epoch: 19 [35072/50048]	Loss: 1.1212
Training Epoch: 19 [35200/50048]	Loss: 0.9904
Training Epoch: 19 [35328/50048]	Loss: 1.1192
Training Epoch: 19 [35456/50048]	Loss: 1.3663
Training Epoch: 19 [35584/50048]	Loss: 1.2631
Training Epoch: 19 [35712/50048]	Loss: 1.2664
Training Epoch: 19 [35840/50048]	Loss: 1.1035
Training Epoch: 19 [35968/50048]	Loss: 0.9826
Training Epoch: 19 [36096/50048]	Loss: 1.2341
Training Epoch: 19 [36224/50048]	Loss: 1.1552
Training Epoch: 19 [36352/50048]	Loss: 1.3219
Training Epoch: 19 [36480/50048]	Loss: 1.0910
Training Epoch: 19 [36608/50048]	Loss: 1.1294
Training Epoch: 19 [36736/50048]	Loss: 1.2206
Training Epoch: 19 [36864/50048]	Loss: 1.1585
Training Epoch: 19 [36992/50048]	Loss: 1.3348
Training Epoch: 19 [37120/50048]	Loss: 1.2547
Training Epoch: 19 [37248/50048]	Loss: 1.3956
Training Epoch: 19 [37376/50048]	Loss: 1.2266
Training Epoch: 19 [37504/50048]	Loss: 1.1927
Training Epoch: 19 [37632/50048]	Loss: 1.0085
Training Epoch: 19 [37760/50048]	Loss: 1.4766
Training Epoch: 19 [37888/50048]	Loss: 1.2320
Training Epoch: 19 [38016/50048]	Loss: 1.1599
Training Epoch: 19 [38144/50048]	Loss: 1.0771
Training Epoch: 19 [38272/50048]	Loss: 1.0874
Training Epoch: 19 [38400/50048]	Loss: 1.1569
Training Epoch: 19 [38528/50048]	Loss: 1.5491
Training Epoch: 19 [38656/50048]	Loss: 1.2493
Training Epoch: 19 [38784/50048]	Loss: 1.2526
Training Epoch: 19 [38912/50048]	Loss: 1.4766
Training Epoch: 19 [39040/50048]	Loss: 1.0790
Training Epoch: 19 [39168/50048]	Loss: 1.0649
Training Epoch: 19 [39296/50048]	Loss: 1.1775
Training Epoch: 19 [39424/50048]	Loss: 1.4387
Training Epoch: 19 [39552/50048]	Loss: 1.3714
Training Epoch: 19 [39680/50048]	Loss: 1.2989
Training Epoch: 19 [39808/50048]	Loss: 1.1276
Training Epoch: 19 [39936/50048]	Loss: 1.3704
Training Epoch: 19 [40064/50048]	Loss: 1.2985
Training Epoch: 19 [40192/50048]	Loss: 1.2482
Training Epoch: 19 [40320/50048]	Loss: 1.5062
Training Epoch: 19 [40448/50048]	Loss: 1.3075
Training Epoch: 19 [40576/50048]	Loss: 1.0349
Training Epoch: 19 [40704/50048]	Loss: 1.0354
Training Epoch: 19 [40832/50048]	Loss: 1.5510
Training Epoch: 19 [40960/50048]	Loss: 0.9433
Training Epoch: 19 [41088/50048]	Loss: 1.2925
Training Epoch: 19 [41216/50048]	Loss: 0.9392
Training Epoch: 19 [41344/50048]	Loss: 1.1973
Training Epoch: 19 [41472/50048]	Loss: 1.0694
Training Epoch: 19 [41600/50048]	Loss: 1.2487
Training Epoch: 19 [41728/50048]	Loss: 1.3310
Training Epoch: 19 [41856/50048]	Loss: 1.3803
Training Epoch: 19 [41984/50048]	Loss: 1.2075
Training Epoch: 19 [42112/50048]	Loss: 1.1975
Training Epoch: 19 [42240/50048]	Loss: 1.2284
Training Epoch: 19 [42368/50048]	Loss: 0.9951
Training Epoch: 19 [42496/50048]	Loss: 1.1511
Training Epoch: 19 [42624/50048]	Loss: 1.2250
Training Epoch: 19 [42752/50048]	Loss: 1.1920
Training Epoch: 19 [42880/50048]	Loss: 1.1181
Training Epoch: 19 [43008/50048]	Loss: 1.1982
Training Epoch: 19 [43136/50048]	Loss: 1.1477
Training Epoch: 19 [43264/50048]	Loss: 1.2867
Training Epoch: 19 [43392/50048]	Loss: 1.3199
Training Epoch: 19 [43520/50048]	Loss: 1.2077
Training Epoch: 19 [43648/50048]	Loss: 1.2508
Training Epoch: 19 [43776/50048]	Loss: 1.2929
Training Epoch: 19 [43904/50048]	Loss: 1.4394
Training Epoch: 19 [44032/50048]	Loss: 1.2978
Training Epoch: 19 [44160/50048]	Loss: 1.2512
Training Epoch: 19 [44288/50048]	Loss: 1.2256
Training Epoch: 19 [44416/50048]	Loss: 1.2147
Training Epoch: 19 [44544/50048]	Loss: 1.3271
Training Epoch: 19 [44672/50048]	Loss: 1.1464
Training Epoch: 19 [44800/50048]	Loss: 1.1407
Training Epoch: 19 [44928/50048]	Loss: 1.1976
Training Epoch: 19 [45056/50048]	Loss: 1.1763
Training Epoch: 19 [45184/50048]	Loss: 1.2470
Training Epoch: 19 [45312/50048]	Loss: 1.2216
Training Epoch: 19 [45440/50048]	Loss: 1.3482
Training Epoch: 19 [45568/50048]	Loss: 0.9809
Training Epoch: 19 [45696/50048]	Loss: 1.4367
Training Epoch: 19 [45824/50048]	Loss: 1.3128
Training Epoch: 19 [45952/50048]	Loss: 1.0748
Training Epoch: 19 [46080/50048]	Loss: 1.3664
Training Epoch: 19 [46208/50048]	Loss: 1.3209
Training Epoch: 19 [46336/50048]	Loss: 1.5538
Training Epoch: 19 [46464/50048]	Loss: 1.0626
Training Epoch: 19 [46592/50048]	Loss: 1.2699
Training Epoch: 19 [46720/50048]	Loss: 1.1155
Training Epoch: 19 [46848/50048]	Loss: 1.4407
Training Epoch: 19 [46976/50048]	Loss: 1.0041
Training Epoch: 19 [47104/50048]	Loss: 1.0461
Training Epoch: 19 [47232/50048]	Loss: 1.3455
Training Epoch: 19 [47360/50048]	Loss: 1.1118
Training Epoch: 19 [47488/50048]	Loss: 1.2367
Training Epoch: 19 [47616/50048]	Loss: 1.2557
Training Epoch: 19 [47744/50048]	Loss: 0.9679
Training Epoch: 19 [47872/50048]	Loss: 1.1928
Training Epoch: 19 [48000/50048]	Loss: 1.2932
Training Epoch: 19 [48128/50048]	Loss: 1.2911
Training Epoch: 19 [48256/50048]	Loss: 1.6002
Training Epoch: 19 [48384/50048]	Loss: 1.0603
Training Epoch: 19 [48512/50048]	Loss: 1.0663
Training Epoch: 19 [48640/50048]	Loss: 1.1715
Training Epoch: 19 [48768/50048]	Loss: 1.5054
Training Epoch: 19 [48896/50048]	Loss: 1.1967
Training Epoch: 19 [49024/50048]	Loss: 0.9477
Training Epoch: 19 [49152/50048]	Loss: 1.1972
Training Epoch: 19 [49280/50048]	Loss: 1.0507
Training Epoch: 19 [49408/50048]	Loss: 1.3405
Training Epoch: 19 [49536/50048]	Loss: 1.3938
Training Epoch: 19 [49664/50048]	Loss: 1.0786
Training Epoch: 19 [49792/50048]	Loss: 1.1998
Training Epoch: 19 [49920/50048]	Loss: 1.4550
Training Epoch: 19 [50048/50048]	Loss: 1.5541
Validation Epoch: 19, Average loss: 0.0114, Accuracy: 0.5995
Training Epoch: 20 [128/50048]	Loss: 1.0980
Training Epoch: 20 [256/50048]	Loss: 1.0148
Training Epoch: 20 [384/50048]	Loss: 1.2105
Training Epoch: 20 [512/50048]	Loss: 1.1977
Training Epoch: 20 [640/50048]	Loss: 1.0101
Training Epoch: 20 [768/50048]	Loss: 1.3687
Training Epoch: 20 [896/50048]	Loss: 1.3656
Training Epoch: 20 [1024/50048]	Loss: 0.9949
Training Epoch: 20 [1152/50048]	Loss: 1.0663
Training Epoch: 20 [1280/50048]	Loss: 1.3488
Training Epoch: 20 [1408/50048]	Loss: 1.4961
Training Epoch: 20 [1536/50048]	Loss: 1.1459
Training Epoch: 20 [1664/50048]	Loss: 1.2588
Training Epoch: 20 [1792/50048]	Loss: 1.1533
Training Epoch: 20 [1920/50048]	Loss: 1.1631
Training Epoch: 20 [2048/50048]	Loss: 1.0379
Training Epoch: 20 [2176/50048]	Loss: 1.0667
Training Epoch: 20 [2304/50048]	Loss: 1.2908
Training Epoch: 20 [2432/50048]	Loss: 1.3620
Training Epoch: 20 [2560/50048]	Loss: 1.0434
Training Epoch: 20 [2688/50048]	Loss: 1.1303
Training Epoch: 20 [2816/50048]	Loss: 1.2774
Training Epoch: 20 [2944/50048]	Loss: 1.1312
Training Epoch: 20 [3072/50048]	Loss: 1.1890
Training Epoch: 20 [3200/50048]	Loss: 1.1866
Training Epoch: 20 [3328/50048]	Loss: 1.3063
Training Epoch: 20 [3456/50048]	Loss: 1.2981
Training Epoch: 20 [3584/50048]	Loss: 1.3230
Training Epoch: 20 [3712/50048]	Loss: 1.2620
Training Epoch: 20 [3840/50048]	Loss: 1.0717
Training Epoch: 20 [3968/50048]	Loss: 1.2579
Training Epoch: 20 [4096/50048]	Loss: 1.2277
Training Epoch: 20 [4224/50048]	Loss: 1.0202
Training Epoch: 20 [4352/50048]	Loss: 1.1168
Training Epoch: 20 [4480/50048]	Loss: 1.0618
Training Epoch: 20 [4608/50048]	Loss: 1.1161
Training Epoch: 20 [4736/50048]	Loss: 1.1594
Training Epoch: 20 [4864/50048]	Loss: 1.1213
Training Epoch: 20 [4992/50048]	Loss: 1.0957
Training Epoch: 20 [5120/50048]	Loss: 1.1889
Training Epoch: 20 [5248/50048]	Loss: 1.2678
Training Epoch: 20 [5376/50048]	Loss: 1.3355
Training Epoch: 20 [5504/50048]	Loss: 1.1697
Training Epoch: 20 [5632/50048]	Loss: 1.1612
Training Epoch: 20 [5760/50048]	Loss: 1.1887
Training Epoch: 20 [5888/50048]	Loss: 1.4771
Training Epoch: 20 [6016/50048]	Loss: 1.1076
Training Epoch: 20 [6144/50048]	Loss: 1.1245
Training Epoch: 20 [6272/50048]	Loss: 0.9173
Training Epoch: 20 [6400/50048]	Loss: 1.1079
Training Epoch: 20 [6528/50048]	Loss: 1.3627
Training Epoch: 20 [6656/50048]	Loss: 1.0520
Training Epoch: 20 [6784/50048]	Loss: 1.0170
Training Epoch: 20 [6912/50048]	Loss: 1.0180
Training Epoch: 20 [7040/50048]	Loss: 1.1606
Training Epoch: 20 [7168/50048]	Loss: 1.3742
Training Epoch: 20 [7296/50048]	Loss: 1.2535
Training Epoch: 20 [7424/50048]	Loss: 1.1864
Training Epoch: 20 [7552/50048]	Loss: 1.3056
Training Epoch: 20 [7680/50048]	Loss: 1.3275
Training Epoch: 20 [7808/50048]	Loss: 1.2258
Training Epoch: 20 [7936/50048]	Loss: 1.2224
Training Epoch: 20 [8064/50048]	Loss: 1.1211
Training Epoch: 20 [8192/50048]	Loss: 0.9842
Training Epoch: 20 [8320/50048]	Loss: 1.3133
Training Epoch: 20 [8448/50048]	Loss: 1.1008
Training Epoch: 20 [8576/50048]	Loss: 1.2906
Training Epoch: 20 [8704/50048]	Loss: 0.9496
Training Epoch: 20 [8832/50048]	Loss: 1.2077
Training Epoch: 20 [8960/50048]	Loss: 1.2104
Training Epoch: 20 [9088/50048]	Loss: 1.0537
Training Epoch: 20 [9216/50048]	Loss: 1.2483
Training Epoch: 20 [9344/50048]	Loss: 1.1158
Training Epoch: 20 [9472/50048]	Loss: 1.1819
Training Epoch: 20 [9600/50048]	Loss: 1.3122
Training Epoch: 20 [9728/50048]	Loss: 1.1863
Training Epoch: 20 [9856/50048]	Loss: 1.1256
Training Epoch: 20 [9984/50048]	Loss: 0.9822
Training Epoch: 20 [10112/50048]	Loss: 1.0947
Training Epoch: 20 [10240/50048]	Loss: 1.0107
Training Epoch: 20 [10368/50048]	Loss: 1.1692
Training Epoch: 20 [10496/50048]	Loss: 1.2329
Training Epoch: 20 [10624/50048]	Loss: 1.0593
Training Epoch: 20 [10752/50048]	Loss: 1.2945
Training Epoch: 20 [10880/50048]	Loss: 1.1232
Training Epoch: 20 [11008/50048]	Loss: 1.1679
Training Epoch: 20 [11136/50048]	Loss: 1.0757
Training Epoch: 20 [11264/50048]	Loss: 1.2340
Training Epoch: 20 [11392/50048]	Loss: 1.1831
Training Epoch: 20 [11520/50048]	Loss: 1.0850
Training Epoch: 20 [11648/50048]	Loss: 1.0450
Training Epoch: 20 [11776/50048]	Loss: 1.2135
Training Epoch: 20 [11904/50048]	Loss: 1.1549
Training Epoch: 20 [12032/50048]	Loss: 1.3325
Training Epoch: 20 [12160/50048]	Loss: 1.0458
Training Epoch: 20 [12288/50048]	Loss: 1.2007
Training Epoch: 20 [12416/50048]	Loss: 1.2605
Training Epoch: 20 [12544/50048]	Loss: 1.1402
Training Epoch: 20 [12672/50048]	Loss: 1.2036
Training Epoch: 20 [12800/50048]	Loss: 1.2269
Training Epoch: 20 [12928/50048]	Loss: 1.1701
Training Epoch: 20 [13056/50048]	Loss: 1.0881
Training Epoch: 20 [13184/50048]	Loss: 1.2899
Training Epoch: 20 [13312/50048]	Loss: 1.1216
Training Epoch: 20 [13440/50048]	Loss: 0.9282
Training Epoch: 20 [13568/50048]	Loss: 1.2677
Training Epoch: 20 [13696/50048]	Loss: 1.0077
Training Epoch: 20 [13824/50048]	Loss: 1.1429
Training Epoch: 20 [13952/50048]	Loss: 1.1943
Training Epoch: 20 [14080/50048]	Loss: 1.2542
Training Epoch: 20 [14208/50048]	Loss: 1.2976
Training Epoch: 20 [14336/50048]	Loss: 1.1603
Training Epoch: 20 [14464/50048]	Loss: 1.1827
Training Epoch: 20 [14592/50048]	Loss: 0.8551
Training Epoch: 20 [14720/50048]	Loss: 1.3975
Training Epoch: 20 [14848/50048]	Loss: 1.5016
Training Epoch: 20 [14976/50048]	Loss: 1.0807
Training Epoch: 20 [15104/50048]	Loss: 1.1831
Training Epoch: 20 [15232/50048]	Loss: 1.1197
Training Epoch: 20 [15360/50048]	Loss: 1.1353
Training Epoch: 20 [15488/50048]	Loss: 1.1500
Training Epoch: 20 [15616/50048]	Loss: 1.2581
Training Epoch: 20 [15744/50048]	Loss: 1.1938
Training Epoch: 20 [15872/50048]	Loss: 1.2624
Training Epoch: 20 [16000/50048]	Loss: 1.3306
Training Epoch: 20 [16128/50048]	Loss: 1.2181
Training Epoch: 20 [16256/50048]	Loss: 1.6573
Training Epoch: 20 [16384/50048]	Loss: 1.2661
Training Epoch: 20 [16512/50048]	Loss: 1.0377
Training Epoch: 20 [16640/50048]	Loss: 1.1009
Training Epoch: 20 [16768/50048]	Loss: 1.1810
Training Epoch: 20 [16896/50048]	Loss: 1.0323
Training Epoch: 20 [17024/50048]	Loss: 1.1933
Training Epoch: 20 [17152/50048]	Loss: 1.0642
Training Epoch: 20 [17280/50048]	Loss: 1.3929
Training Epoch: 20 [17408/50048]	Loss: 1.2691
Training Epoch: 20 [17536/50048]	Loss: 1.2069
Training Epoch: 20 [17664/50048]	Loss: 1.4676
Training Epoch: 20 [17792/50048]	Loss: 1.0657
Training Epoch: 20 [17920/50048]	Loss: 1.3161
Training Epoch: 20 [18048/50048]	Loss: 1.3094
Training Epoch: 20 [18176/50048]	Loss: 1.1086
Training Epoch: 20 [18304/50048]	Loss: 1.0666
Training Epoch: 20 [18432/50048]	Loss: 1.2090
Training Epoch: 20 [18560/50048]	Loss: 1.0042
Training Epoch: 20 [18688/50048]	Loss: 1.0722
Training Epoch: 20 [18816/50048]	Loss: 1.1684
Training Epoch: 20 [18944/50048]	Loss: 1.0992
Training Epoch: 20 [19072/50048]	Loss: 1.3942
Training Epoch: 20 [19200/50048]	Loss: 1.2631
Training Epoch: 20 [19328/50048]	Loss: 1.0420
Training Epoch: 20 [19456/50048]	Loss: 1.2671
Training Epoch: 20 [19584/50048]	Loss: 1.2157
Training Epoch: 20 [19712/50048]	Loss: 1.2199
Training Epoch: 20 [19840/50048]	Loss: 1.3971
Training Epoch: 20 [19968/50048]	Loss: 1.2507
Training Epoch: 20 [20096/50048]	Loss: 1.1146
Training Epoch: 20 [20224/50048]	Loss: 1.2069
Training Epoch: 20 [20352/50048]	Loss: 1.1439
Training Epoch: 20 [20480/50048]	Loss: 1.1350
Training Epoch: 20 [20608/50048]	Loss: 1.1865
Training Epoch: 20 [20736/50048]	Loss: 1.4006
Training Epoch: 20 [20864/50048]	Loss: 1.2214
Training Epoch: 20 [20992/50048]	Loss: 1.3833
Training Epoch: 20 [21120/50048]	Loss: 1.3487
Training Epoch: 20 [21248/50048]	Loss: 1.3210
Training Epoch: 20 [21376/50048]	Loss: 1.3855
Training Epoch: 20 [21504/50048]	Loss: 1.0554
Training Epoch: 20 [21632/50048]	Loss: 1.2596
Training Epoch: 20 [21760/50048]	Loss: 0.9467
Training Epoch: 20 [21888/50048]	Loss: 1.0448
Training Epoch: 20 [22016/50048]	Loss: 1.1300
Training Epoch: 20 [22144/50048]	Loss: 1.2982
Training Epoch: 20 [22272/50048]	Loss: 1.1555
Training Epoch: 20 [22400/50048]	Loss: 1.2441
Training Epoch: 20 [22528/50048]	Loss: 0.9678
Training Epoch: 20 [22656/50048]	Loss: 1.2570
Training Epoch: 20 [22784/50048]	Loss: 1.1600
Training Epoch: 20 [22912/50048]	Loss: 1.0894
Training Epoch: 20 [23040/50048]	Loss: 1.3405
Training Epoch: 20 [23168/50048]	Loss: 1.2275
Training Epoch: 20 [23296/50048]	Loss: 1.1241
Training Epoch: 20 [23424/50048]	Loss: 1.0394
Training Epoch: 20 [23552/50048]	Loss: 1.1822
Training Epoch: 20 [23680/50048]	Loss: 1.3898
Training Epoch: 20 [23808/50048]	Loss: 1.1237
Training Epoch: 20 [23936/50048]	Loss: 1.1702
Training Epoch: 20 [24064/50048]	Loss: 1.1202
Training Epoch: 20 [24192/50048]	Loss: 1.2244
Training Epoch: 20 [24320/50048]	Loss: 1.2683
Training Epoch: 20 [24448/50048]	Loss: 1.2270
Training Epoch: 20 [24576/50048]	Loss: 1.1832
Training Epoch: 20 [24704/50048]	Loss: 1.0758
Training Epoch: 20 [24832/50048]	Loss: 1.4533
Training Epoch: 20 [24960/50048]	Loss: 1.2182
Training Epoch: 20 [25088/50048]	Loss: 1.2171
Training Epoch: 20 [25216/50048]	Loss: 1.1692
Training Epoch: 20 [25344/50048]	Loss: 1.4526
Training Epoch: 20 [25472/50048]	Loss: 1.1780
Training Epoch: 20 [25600/50048]	Loss: 1.2792
Training Epoch: 20 [25728/50048]	Loss: 0.9738
Training Epoch: 20 [25856/50048]	Loss: 1.0284
Training Epoch: 20 [25984/50048]	Loss: 1.2262
Training Epoch: 20 [26112/50048]	Loss: 1.1672
Training Epoch: 20 [26240/50048]	Loss: 1.0823
Training Epoch: 20 [26368/50048]	Loss: 1.1542
Training Epoch: 20 [26496/50048]	Loss: 1.2225
Training Epoch: 20 [26624/50048]	Loss: 1.1338
Training Epoch: 20 [26752/50048]	Loss: 1.2603
Training Epoch: 20 [26880/50048]	Loss: 1.0301
Training Epoch: 20 [27008/50048]	Loss: 1.3289
Training Epoch: 20 [27136/50048]	Loss: 1.2182
Training Epoch: 20 [27264/50048]	Loss: 1.2299
Training Epoch: 20 [27392/50048]	Loss: 1.1604
Training Epoch: 20 [27520/50048]	Loss: 1.1304
Training Epoch: 20 [27648/50048]	Loss: 1.1943
Training Epoch: 20 [27776/50048]	Loss: 1.2189
Training Epoch: 20 [27904/50048]	Loss: 1.1963
Training Epoch: 20 [28032/50048]	Loss: 1.3160
Training Epoch: 20 [28160/50048]	Loss: 1.3294
Training Epoch: 20 [28288/50048]	Loss: 1.1365
Training Epoch: 20 [28416/50048]	Loss: 1.0811
Training Epoch: 20 [28544/50048]	Loss: 1.1881
Training Epoch: 20 [28672/50048]	Loss: 0.9814
Training Epoch: 20 [28800/50048]	Loss: 1.3582
Training Epoch: 20 [28928/50048]	Loss: 1.3308
Training Epoch: 20 [29056/50048]	Loss: 0.9638
Training Epoch: 20 [29184/50048]	Loss: 1.3074
Training Epoch: 20 [29312/50048]	Loss: 1.1440
Training Epoch: 20 [29440/50048]	Loss: 1.1974
Training Epoch: 20 [29568/50048]	Loss: 1.5088
Training Epoch: 20 [29696/50048]	Loss: 1.4495
Training Epoch: 20 [29824/50048]	Loss: 1.1810
Training Epoch: 20 [29952/50048]	Loss: 1.2458
Training Epoch: 20 [30080/50048]	Loss: 1.0974
Training Epoch: 20 [30208/50048]	Loss: 1.1543
Training Epoch: 20 [30336/50048]	Loss: 1.1795
Training Epoch: 20 [30464/50048]	Loss: 1.5624
Training Epoch: 20 [30592/50048]	Loss: 1.1852
Training Epoch: 20 [30720/50048]	Loss: 0.9428
Training Epoch: 20 [30848/50048]	Loss: 1.1220
Training Epoch: 20 [30976/50048]	Loss: 1.2253
Training Epoch: 20 [31104/50048]	Loss: 1.2272
Training Epoch: 20 [31232/50048]	Loss: 1.4360
Training Epoch: 20 [31360/50048]	Loss: 1.1960
Training Epoch: 20 [31488/50048]	Loss: 1.1671
Training Epoch: 20 [31616/50048]	Loss: 1.3016
Training Epoch: 20 [31744/50048]	Loss: 1.1197
Training Epoch: 20 [31872/50048]	Loss: 1.1098
Training Epoch: 20 [32000/50048]	Loss: 1.0017
Training Epoch: 20 [32128/50048]	Loss: 1.2321
Training Epoch: 20 [32256/50048]	Loss: 1.4352
Training Epoch: 20 [32384/50048]	Loss: 1.4422
Training Epoch: 20 [32512/50048]	Loss: 1.3885
Training Epoch: 20 [32640/50048]	Loss: 1.2388
Training Epoch: 20 [32768/50048]	Loss: 1.2676
Training Epoch: 20 [32896/50048]	Loss: 1.1045
Training Epoch: 20 [33024/50048]	Loss: 1.4416
Training Epoch: 20 [33152/50048]	Loss: 1.0492
Training Epoch: 20 [33280/50048]	Loss: 1.3720
Training Epoch: 20 [33408/50048]	Loss: 1.3064
Training Epoch: 20 [33536/50048]	Loss: 0.9475
Training Epoch: 20 [33664/50048]	Loss: 1.0842
Training Epoch: 20 [33792/50048]	Loss: 1.0766
Training Epoch: 20 [33920/50048]	Loss: 1.1119
Training Epoch: 20 [34048/50048]	Loss: 1.1493
Training Epoch: 20 [34176/50048]	Loss: 1.2355
Training Epoch: 20 [34304/50048]	Loss: 1.2030
Training Epoch: 20 [34432/50048]	Loss: 1.1168
Training Epoch: 20 [34560/50048]	Loss: 1.0738
Training Epoch: 20 [34688/50048]	Loss: 1.0568
Training Epoch: 20 [34816/50048]	Loss: 1.1920
Training Epoch: 20 [34944/50048]	Loss: 1.2099
Training Epoch: 20 [35072/50048]	Loss: 1.2481
Training Epoch: 20 [35200/50048]	Loss: 0.9675
Training Epoch: 20 [35328/50048]	Loss: 1.2207
Training Epoch: 20 [35456/50048]	Loss: 0.9828
Training Epoch: 20 [35584/50048]	Loss: 1.0370
Training Epoch: 20 [35712/50048]	Loss: 1.1243
Training Epoch: 20 [35840/50048]	Loss: 0.9693
Training Epoch: 20 [35968/50048]	Loss: 1.3292
Training Epoch: 20 [36096/50048]	Loss: 1.1259
Training Epoch: 20 [36224/50048]	Loss: 1.3351
Training Epoch: 20 [36352/50048]	Loss: 1.0788
Training Epoch: 20 [36480/50048]	Loss: 1.3524
Training Epoch: 20 [36608/50048]	Loss: 1.1707
Training Epoch: 20 [36736/50048]	Loss: 1.3456
Training Epoch: 20 [36864/50048]	Loss: 1.1540
Training Epoch: 20 [36992/50048]	Loss: 1.2256
Training Epoch: 20 [37120/50048]	Loss: 1.0183
Training Epoch: 20 [37248/50048]	Loss: 1.1518
Training Epoch: 20 [37376/50048]	Loss: 1.3298
Training Epoch: 20 [37504/50048]	Loss: 1.0608
Training Epoch: 20 [37632/50048]	Loss: 1.2980
Training Epoch: 20 [37760/50048]	Loss: 1.4560
Training Epoch: 20 [37888/50048]	Loss: 1.3719
Training Epoch: 20 [38016/50048]	Loss: 1.1556
Training Epoch: 20 [38144/50048]	Loss: 1.1944
Training Epoch: 20 [38272/50048]	Loss: 1.4460
Training Epoch: 20 [38400/50048]	Loss: 1.0148
Training Epoch: 20 [38528/50048]	Loss: 0.9511
Training Epoch: 20 [38656/50048]	Loss: 1.2583
Training Epoch: 20 [38784/50048]	Loss: 1.2557
Training Epoch: 20 [38912/50048]	Loss: 1.2120
Training Epoch: 20 [39040/50048]	Loss: 1.0452
Training Epoch: 20 [39168/50048]	Loss: 1.1362
Training Epoch: 20 [39296/50048]	Loss: 1.2420
Training Epoch: 20 [39424/50048]	Loss: 1.1997
Training Epoch: 20 [39552/50048]	Loss: 1.1999
Training Epoch: 20 [39680/50048]	Loss: 1.1493
Training Epoch: 20 [39808/50048]	Loss: 1.2965
Training Epoch: 20 [39936/50048]	Loss: 1.0269
Training Epoch: 20 [40064/50048]	Loss: 1.1306
Training Epoch: 20 [40192/50048]	Loss: 1.2543
Training Epoch: 20 [40320/50048]	Loss: 1.2371
Training Epoch: 20 [40448/50048]	Loss: 1.0547
Training Epoch: 20 [40576/50048]	Loss: 0.9954
Training Epoch: 20 [40704/50048]	Loss: 1.0898
Training Epoch: 20 [40832/50048]	Loss: 1.4113
Training Epoch: 20 [40960/50048]	Loss: 1.2826
Training Epoch: 20 [41088/50048]	Loss: 1.4581
Training Epoch: 20 [41216/50048]	Loss: 1.0555
Training Epoch: 20 [41344/50048]	Loss: 1.2395
Training Epoch: 20 [41472/50048]	Loss: 1.2882
Training Epoch: 20 [41600/50048]	Loss: 1.1757
Training Epoch: 20 [41728/50048]	Loss: 1.1050
Training Epoch: 20 [41856/50048]	Loss: 1.3180
Training Epoch: 20 [41984/50048]	Loss: 1.1988
Training Epoch: 20 [42112/50048]	Loss: 1.2123
Training Epoch: 20 [42240/50048]	Loss: 0.9664
Training Epoch: 20 [42368/50048]	Loss: 0.9774
Training Epoch: 20 [42496/50048]	Loss: 1.3391
Training Epoch: 20 [42624/50048]	Loss: 1.1089
Training Epoch: 20 [42752/50048]	Loss: 1.2463
Training Epoch: 20 [42880/50048]	Loss: 1.1090
Training Epoch: 20 [43008/50048]	Loss: 1.2717
Training Epoch: 20 [43136/50048]	Loss: 1.1972
Training Epoch: 20 [43264/50048]	Loss: 1.1695
Training Epoch: 20 [43392/50048]	Loss: 1.2434
Training Epoch: 20 [43520/50048]	Loss: 1.4134
Training Epoch: 20 [43648/50048]	Loss: 1.2228
Training Epoch: 20 [43776/50048]	Loss: 1.4131
Training Epoch: 20 [43904/50048]	Loss: 1.1682
Training Epoch: 20 [44032/50048]	Loss: 1.1233
Training Epoch: 20 [44160/50048]	Loss: 1.2211
Training Epoch: 20 [44288/50048]	Loss: 1.3307
Training Epoch: 20 [44416/50048]	Loss: 1.4033
Training Epoch: 20 [44544/50048]	Loss: 1.2883
Training Epoch: 20 [44672/50048]	Loss: 1.0028
Training Epoch: 20 [44800/50048]	Loss: 1.0872
Training Epoch: 20 [44928/50048]	Loss: 1.2065
Training Epoch: 20 [45056/50048]	Loss: 1.2697
Training Epoch: 20 [45184/50048]	Loss: 1.3342
Training Epoch: 20 [45312/50048]	Loss: 1.0319
Training Epoch: 20 [45440/50048]	Loss: 1.1046
Training Epoch: 20 [45568/50048]	Loss: 1.0410
Training Epoch: 20 [45696/50048]	Loss: 1.2381
Training Epoch: 20 [45824/50048]	Loss: 1.1723
Training Epoch: 20 [45952/50048]	Loss: 1.2467
Training Epoch: 20 [46080/50048]	Loss: 1.3362
Training Epoch: 20 [46208/50048]	Loss: 1.1187
Training Epoch: 20 [46336/50048]	Loss: 1.0536
Training Epoch: 20 [46464/50048]	Loss: 1.1932
Training Epoch: 20 [46592/50048]	Loss: 1.2555
Training Epoch: 20 [46720/50048]	Loss: 1.3490
Training Epoch: 20 [46848/50048]	Loss: 1.0024
Training Epoch: 20 [46976/50048]	Loss: 1.2362
Training Epoch: 20 [47104/50048]	Loss: 1.3972
Training Epoch: 20 [47232/50048]	Loss: 1.1265
Training Epoch: 20 [47360/50048]	Loss: 1.0342
Training Epoch: 20 [47488/50048]	Loss: 1.1732
Training Epoch: 20 [47616/50048]	Loss: 1.3632
Training Epoch: 20 [47744/50048]	Loss: 1.0871
Training Epoch: 20 [47872/50048]	Loss: 1.2895
Training Epoch: 20 [48000/50048]	Loss: 1.1057
Training Epoch: 20 [48128/50048]	Loss: 1.2987
Training Epoch: 20 [48256/50048]	Loss: 1.1293
Training Epoch: 20 [48384/50048]	Loss: 1.3532
Training Epoch: 20 [48512/50048]	Loss: 1.2074
Training Epoch: 20 [48640/50048]	Loss: 1.3236
Training Epoch: 20 [48768/50048]	Loss: 1.4076
Training Epoch: 20 [48896/50048]	Loss: 1.0491
Training Epoch: 20 [49024/50048]	Loss: 1.1872
Training Epoch: 20 [49152/50048]	Loss: 1.2323
Training Epoch: 20 [49280/50048]	Loss: 1.0885
Training Epoch: 20 [49408/50048]	Loss: 1.2076
Training Epoch: 20 [49536/50048]	Loss: 0.9456
Training Epoch: 20 [49664/50048]	Loss: 1.2204
Training Epoch: 20 [49792/50048]	Loss: 1.2802
Training Epoch: 20 [49920/50048]	Loss: 1.4303
Training Epoch: 20 [50048/50048]	Loss: 1.3402
Validation Epoch: 20, Average loss: 0.0115, Accuracy: 0.5956
Training Epoch: 21 [128/50048]	Loss: 1.1444
Training Epoch: 21 [256/50048]	Loss: 1.1842
Training Epoch: 21 [384/50048]	Loss: 1.2496
Training Epoch: 21 [512/50048]	Loss: 1.1697
Training Epoch: 21 [640/50048]	Loss: 1.1717
Training Epoch: 21 [768/50048]	Loss: 1.1407
Training Epoch: 21 [896/50048]	Loss: 1.2288
Training Epoch: 21 [1024/50048]	Loss: 1.2285
Training Epoch: 21 [1152/50048]	Loss: 1.1472
Training Epoch: 21 [1280/50048]	Loss: 0.9044
Training Epoch: 21 [1408/50048]	Loss: 1.2000
Training Epoch: 21 [1536/50048]	Loss: 1.2295
Training Epoch: 21 [1664/50048]	Loss: 1.2202
Training Epoch: 21 [1792/50048]	Loss: 1.2596
Training Epoch: 21 [1920/50048]	Loss: 1.0860
Training Epoch: 21 [2048/50048]	Loss: 1.3243
Training Epoch: 21 [2176/50048]	Loss: 1.2491
Training Epoch: 21 [2304/50048]	Loss: 0.9874
Training Epoch: 21 [2432/50048]	Loss: 1.0517
Training Epoch: 21 [2560/50048]	Loss: 1.1536
Training Epoch: 21 [2688/50048]	Loss: 1.1337
Training Epoch: 21 [2816/50048]	Loss: 1.0821
Training Epoch: 21 [2944/50048]	Loss: 1.1302
Training Epoch: 21 [3072/50048]	Loss: 1.2059
Training Epoch: 21 [3200/50048]	Loss: 1.1297
Training Epoch: 21 [3328/50048]	Loss: 0.8803
Training Epoch: 21 [3456/50048]	Loss: 1.0042
Training Epoch: 21 [3584/50048]	Loss: 1.1192
Training Epoch: 21 [3712/50048]	Loss: 1.1858
Training Epoch: 21 [3840/50048]	Loss: 1.1433
Training Epoch: 21 [3968/50048]	Loss: 1.0296
Training Epoch: 21 [4096/50048]	Loss: 1.3244
Training Epoch: 21 [4224/50048]	Loss: 0.8953
Training Epoch: 21 [4352/50048]	Loss: 1.2393
Training Epoch: 21 [4480/50048]	Loss: 0.9707
Training Epoch: 21 [4608/50048]	Loss: 1.1300
Training Epoch: 21 [4736/50048]	Loss: 1.0380
Training Epoch: 21 [4864/50048]	Loss: 1.1376
Training Epoch: 21 [4992/50048]	Loss: 1.1130
Training Epoch: 21 [5120/50048]	Loss: 1.1734
Training Epoch: 21 [5248/50048]	Loss: 1.1834
Training Epoch: 21 [5376/50048]	Loss: 1.1175
Training Epoch: 21 [5504/50048]	Loss: 1.1674
Training Epoch: 21 [5632/50048]	Loss: 1.0239
Training Epoch: 21 [5760/50048]	Loss: 1.0293
Training Epoch: 21 [5888/50048]	Loss: 1.3099
Training Epoch: 21 [6016/50048]	Loss: 1.1470
Training Epoch: 21 [6144/50048]	Loss: 1.1715
Training Epoch: 21 [6272/50048]	Loss: 1.0146
Training Epoch: 21 [6400/50048]	Loss: 0.9670
Training Epoch: 21 [6528/50048]	Loss: 1.2671
Training Epoch: 21 [6656/50048]	Loss: 1.1501
Training Epoch: 21 [6784/50048]	Loss: 1.0807
Training Epoch: 21 [6912/50048]	Loss: 1.2341
Training Epoch: 21 [7040/50048]	Loss: 1.1599
Training Epoch: 21 [7168/50048]	Loss: 1.3305
Training Epoch: 21 [7296/50048]	Loss: 1.1228
Training Epoch: 21 [7424/50048]	Loss: 0.9709
Training Epoch: 21 [7552/50048]	Loss: 1.2744
Training Epoch: 21 [7680/50048]	Loss: 1.1057
Training Epoch: 21 [7808/50048]	Loss: 1.0208
Training Epoch: 21 [7936/50048]	Loss: 1.0260
Training Epoch: 21 [8064/50048]	Loss: 1.1578
Training Epoch: 21 [8192/50048]	Loss: 1.2019
Training Epoch: 21 [8320/50048]	Loss: 1.1316
Training Epoch: 21 [8448/50048]	Loss: 1.1815
Training Epoch: 21 [8576/50048]	Loss: 1.0486
Training Epoch: 21 [8704/50048]	Loss: 1.1743
Training Epoch: 21 [8832/50048]	Loss: 1.1247
Training Epoch: 21 [8960/50048]	Loss: 1.0116
Training Epoch: 21 [9088/50048]	Loss: 1.2904
Training Epoch: 21 [9216/50048]	Loss: 1.1545
Training Epoch: 21 [9344/50048]	Loss: 1.1801
Training Epoch: 21 [9472/50048]	Loss: 1.2497
Training Epoch: 21 [9600/50048]	Loss: 1.0992
Training Epoch: 21 [9728/50048]	Loss: 1.2260
Training Epoch: 21 [9856/50048]	Loss: 0.9815
Training Epoch: 21 [9984/50048]	Loss: 1.1884
Training Epoch: 21 [10112/50048]	Loss: 0.9951
Training Epoch: 21 [10240/50048]	Loss: 1.1754
Training Epoch: 21 [10368/50048]	Loss: 1.2432
Training Epoch: 21 [10496/50048]	Loss: 1.2173
Training Epoch: 21 [10624/50048]	Loss: 1.1034
Training Epoch: 21 [10752/50048]	Loss: 1.0866
Training Epoch: 21 [10880/50048]	Loss: 1.4933
Training Epoch: 21 [11008/50048]	Loss: 1.0332
Training Epoch: 21 [11136/50048]	Loss: 1.0438
Training Epoch: 21 [11264/50048]	Loss: 1.2432
Training Epoch: 21 [11392/50048]	Loss: 1.2028
Training Epoch: 21 [11520/50048]	Loss: 1.0990
Training Epoch: 21 [11648/50048]	Loss: 1.1814
Training Epoch: 21 [11776/50048]	Loss: 1.2099
Training Epoch: 21 [11904/50048]	Loss: 0.9860
Training Epoch: 21 [12032/50048]	Loss: 1.3005
Training Epoch: 21 [12160/50048]	Loss: 1.2241
Training Epoch: 21 [12288/50048]	Loss: 1.1017
Training Epoch: 21 [12416/50048]	Loss: 1.4327
Training Epoch: 21 [12544/50048]	Loss: 0.9673
Training Epoch: 21 [12672/50048]	Loss: 1.0067
Training Epoch: 21 [12800/50048]	Loss: 1.2234
Training Epoch: 21 [12928/50048]	Loss: 0.9696
Training Epoch: 21 [13056/50048]	Loss: 1.0628
Training Epoch: 21 [13184/50048]	Loss: 0.9474
Training Epoch: 21 [13312/50048]	Loss: 1.0307
Training Epoch: 21 [13440/50048]	Loss: 1.1859
Training Epoch: 21 [13568/50048]	Loss: 1.1497
Training Epoch: 21 [13696/50048]	Loss: 1.0925
Training Epoch: 21 [13824/50048]	Loss: 1.0759
Training Epoch: 21 [13952/50048]	Loss: 1.2612
Training Epoch: 21 [14080/50048]	Loss: 1.2210
Training Epoch: 21 [14208/50048]	Loss: 1.0117
Training Epoch: 21 [14336/50048]	Loss: 1.0638
Training Epoch: 21 [14464/50048]	Loss: 1.2704
Training Epoch: 21 [14592/50048]	Loss: 1.2216
Training Epoch: 21 [14720/50048]	Loss: 1.0184
Training Epoch: 21 [14848/50048]	Loss: 1.1150
Training Epoch: 21 [14976/50048]	Loss: 1.3694
Training Epoch: 21 [15104/50048]	Loss: 1.2561
Training Epoch: 21 [15232/50048]	Loss: 1.1800
Training Epoch: 21 [15360/50048]	Loss: 1.1347
Training Epoch: 21 [15488/50048]	Loss: 1.2055
Training Epoch: 21 [15616/50048]	Loss: 0.9343
Training Epoch: 21 [15744/50048]	Loss: 1.0994
Training Epoch: 21 [15872/50048]	Loss: 1.1183
Training Epoch: 21 [16000/50048]	Loss: 1.1275
Training Epoch: 21 [16128/50048]	Loss: 0.9998
Training Epoch: 21 [16256/50048]	Loss: 1.0692
Training Epoch: 21 [16384/50048]	Loss: 1.3269
Training Epoch: 21 [16512/50048]	Loss: 1.0502
Training Epoch: 21 [16640/50048]	Loss: 1.0896
Training Epoch: 21 [16768/50048]	Loss: 1.2827
Training Epoch: 21 [16896/50048]	Loss: 1.1571
Training Epoch: 21 [17024/50048]	Loss: 0.9832
Training Epoch: 21 [17152/50048]	Loss: 1.2303
Training Epoch: 21 [17280/50048]	Loss: 1.0551
Training Epoch: 21 [17408/50048]	Loss: 1.2934
Training Epoch: 21 [17536/50048]	Loss: 1.1850
Training Epoch: 21 [17664/50048]	Loss: 1.2059
Training Epoch: 21 [17792/50048]	Loss: 1.1403
Training Epoch: 21 [17920/50048]	Loss: 1.0952
Training Epoch: 21 [18048/50048]	Loss: 1.1677
Training Epoch: 21 [18176/50048]	Loss: 1.1170
Training Epoch: 21 [18304/50048]	Loss: 1.0147
Training Epoch: 21 [18432/50048]	Loss: 1.3488
Training Epoch: 21 [18560/50048]	Loss: 1.1266
Training Epoch: 21 [18688/50048]	Loss: 1.1361
Training Epoch: 21 [18816/50048]	Loss: 1.1625
Training Epoch: 21 [18944/50048]	Loss: 1.1826
Training Epoch: 21 [19072/50048]	Loss: 1.3891
Training Epoch: 21 [19200/50048]	Loss: 1.1060
Training Epoch: 21 [19328/50048]	Loss: 1.3792
Training Epoch: 21 [19456/50048]	Loss: 1.2161
Training Epoch: 21 [19584/50048]	Loss: 1.2956
Training Epoch: 21 [19712/50048]	Loss: 1.2182
Training Epoch: 21 [19840/50048]	Loss: 1.3874
Training Epoch: 21 [19968/50048]	Loss: 1.2917
Training Epoch: 21 [20096/50048]	Loss: 0.9994
Training Epoch: 21 [20224/50048]	Loss: 1.2811
Training Epoch: 21 [20352/50048]	Loss: 1.2156
Training Epoch: 21 [20480/50048]	Loss: 1.1697
Training Epoch: 21 [20608/50048]	Loss: 1.1724
Training Epoch: 21 [20736/50048]	Loss: 1.1261
Training Epoch: 21 [20864/50048]	Loss: 1.1251
Training Epoch: 21 [20992/50048]	Loss: 1.2299
Training Epoch: 21 [21120/50048]	Loss: 1.0283
Training Epoch: 21 [21248/50048]	Loss: 1.4544
Training Epoch: 21 [21376/50048]	Loss: 1.3335
Training Epoch: 21 [21504/50048]	Loss: 1.0041
Training Epoch: 21 [21632/50048]	Loss: 1.2579
Training Epoch: 21 [21760/50048]	Loss: 1.0628
Training Epoch: 21 [21888/50048]	Loss: 1.0391
Training Epoch: 21 [22016/50048]	Loss: 1.1758
Training Epoch: 21 [22144/50048]	Loss: 1.0911
Training Epoch: 21 [22272/50048]	Loss: 1.3229
Training Epoch: 21 [22400/50048]	Loss: 1.1407
Training Epoch: 21 [22528/50048]	Loss: 1.1781
Training Epoch: 21 [22656/50048]	Loss: 1.1877
Training Epoch: 21 [22784/50048]	Loss: 0.9736
Training Epoch: 21 [22912/50048]	Loss: 1.3112
Training Epoch: 21 [23040/50048]	Loss: 1.1057
Training Epoch: 21 [23168/50048]	Loss: 1.3841
Training Epoch: 21 [23296/50048]	Loss: 1.2546
Training Epoch: 21 [23424/50048]	Loss: 1.1980
Training Epoch: 21 [23552/50048]	Loss: 1.1784
Training Epoch: 21 [23680/50048]	Loss: 1.0666
Training Epoch: 21 [23808/50048]	Loss: 0.9942
Training Epoch: 21 [23936/50048]	Loss: 1.0135
Training Epoch: 21 [24064/50048]	Loss: 1.1859
Training Epoch: 21 [24192/50048]	Loss: 1.2353
Training Epoch: 21 [24320/50048]	Loss: 1.0641
Training Epoch: 21 [24448/50048]	Loss: 1.4467
Training Epoch: 21 [24576/50048]	Loss: 1.2244
Training Epoch: 21 [24704/50048]	Loss: 1.1422
Training Epoch: 21 [24832/50048]	Loss: 1.0267
Training Epoch: 21 [24960/50048]	Loss: 1.0154
Training Epoch: 21 [25088/50048]	Loss: 1.4142
Training Epoch: 21 [25216/50048]	Loss: 1.1277
Training Epoch: 21 [25344/50048]	Loss: 1.0297
Training Epoch: 21 [25472/50048]	Loss: 1.5318
Training Epoch: 21 [25600/50048]	Loss: 1.1240
Training Epoch: 21 [25728/50048]	Loss: 1.3959
Training Epoch: 21 [25856/50048]	Loss: 1.1542
Training Epoch: 21 [25984/50048]	Loss: 1.1336
Training Epoch: 21 [26112/50048]	Loss: 1.2389
Training Epoch: 21 [26240/50048]	Loss: 1.1457
Training Epoch: 21 [26368/50048]	Loss: 1.0691
Training Epoch: 21 [26496/50048]	Loss: 1.3520
Training Epoch: 21 [26624/50048]	Loss: 1.1689
Training Epoch: 21 [26752/50048]	Loss: 1.3017
Training Epoch: 21 [26880/50048]	Loss: 1.1619
Training Epoch: 21 [27008/50048]	Loss: 1.2467
Training Epoch: 21 [27136/50048]	Loss: 1.0372
Training Epoch: 21 [27264/50048]	Loss: 1.0243
Training Epoch: 21 [27392/50048]	Loss: 1.3914
Training Epoch: 21 [27520/50048]	Loss: 1.1313
Training Epoch: 21 [27648/50048]	Loss: 0.9412
Training Epoch: 21 [27776/50048]	Loss: 1.1458
Training Epoch: 21 [27904/50048]	Loss: 1.3159
Training Epoch: 21 [28032/50048]	Loss: 1.0092
Training Epoch: 21 [28160/50048]	Loss: 1.2753
Training Epoch: 21 [28288/50048]	Loss: 1.0685
Training Epoch: 21 [28416/50048]	Loss: 0.9951
Training Epoch: 21 [28544/50048]	Loss: 1.1815
Training Epoch: 21 [28672/50048]	Loss: 0.9402
Training Epoch: 21 [28800/50048]	Loss: 1.0393
Training Epoch: 21 [28928/50048]	Loss: 1.0250
Training Epoch: 21 [29056/50048]	Loss: 1.6044
Training Epoch: 21 [29184/50048]	Loss: 1.1194
Training Epoch: 21 [29312/50048]	Loss: 1.3757
Training Epoch: 21 [29440/50048]	Loss: 0.9066
Training Epoch: 21 [29568/50048]	Loss: 1.5231
Training Epoch: 21 [29696/50048]	Loss: 0.9193
Training Epoch: 21 [29824/50048]	Loss: 1.1103
Training Epoch: 21 [29952/50048]	Loss: 1.3383
Training Epoch: 21 [30080/50048]	Loss: 1.0248
Training Epoch: 21 [30208/50048]	Loss: 1.2646
Training Epoch: 21 [30336/50048]	Loss: 1.0447
Training Epoch: 21 [30464/50048]	Loss: 1.0881
Training Epoch: 21 [30592/50048]	Loss: 1.2606
Training Epoch: 21 [30720/50048]	Loss: 1.2789
Training Epoch: 21 [30848/50048]	Loss: 1.2106
Training Epoch: 21 [30976/50048]	Loss: 1.2045
Training Epoch: 21 [31104/50048]	Loss: 1.2428
Training Epoch: 21 [31232/50048]	Loss: 1.2856
Training Epoch: 21 [31360/50048]	Loss: 0.9114
Training Epoch: 21 [31488/50048]	Loss: 1.4365
Training Epoch: 21 [31616/50048]	Loss: 1.1056
Training Epoch: 21 [31744/50048]	Loss: 1.0450
Training Epoch: 21 [31872/50048]	Loss: 1.0653
Training Epoch: 21 [32000/50048]	Loss: 1.3238
Training Epoch: 21 [32128/50048]	Loss: 1.3272
Training Epoch: 21 [32256/50048]	Loss: 1.1775
Training Epoch: 21 [32384/50048]	Loss: 1.3607
Training Epoch: 21 [32512/50048]	Loss: 1.2526
Training Epoch: 21 [32640/50048]	Loss: 1.1765
Training Epoch: 21 [32768/50048]	Loss: 1.1502
Training Epoch: 21 [32896/50048]	Loss: 1.2770
Training Epoch: 21 [33024/50048]	Loss: 0.8750
Training Epoch: 21 [33152/50048]	Loss: 1.0761
Training Epoch: 21 [33280/50048]	Loss: 1.2243
Training Epoch: 21 [33408/50048]	Loss: 1.1829
Training Epoch: 21 [33536/50048]	Loss: 1.1409
Training Epoch: 21 [33664/50048]	Loss: 1.1491
Training Epoch: 21 [33792/50048]	Loss: 1.0420
Training Epoch: 21 [33920/50048]	Loss: 1.3233
Training Epoch: 21 [34048/50048]	Loss: 1.0165
Training Epoch: 21 [34176/50048]	Loss: 1.0775
Training Epoch: 21 [34304/50048]	Loss: 1.0360
Training Epoch: 21 [34432/50048]	Loss: 1.4130
Training Epoch: 21 [34560/50048]	Loss: 1.4175
Training Epoch: 21 [34688/50048]	Loss: 1.2129
Training Epoch: 21 [34816/50048]	Loss: 1.1538
Training Epoch: 21 [34944/50048]	Loss: 1.0840
Training Epoch: 21 [35072/50048]	Loss: 1.2110
Training Epoch: 21 [35200/50048]	Loss: 1.0699
Training Epoch: 21 [35328/50048]	Loss: 1.2804
Training Epoch: 21 [35456/50048]	Loss: 1.1252
Training Epoch: 21 [35584/50048]	Loss: 1.0658
Training Epoch: 21 [35712/50048]	Loss: 1.1311
Training Epoch: 21 [35840/50048]	Loss: 1.3390
Training Epoch: 21 [35968/50048]	Loss: 1.3019
Training Epoch: 21 [36096/50048]	Loss: 1.2263
Training Epoch: 21 [36224/50048]	Loss: 0.9518
Training Epoch: 21 [36352/50048]	Loss: 1.3247
Training Epoch: 21 [36480/50048]	Loss: 1.1153
Training Epoch: 21 [36608/50048]	Loss: 1.1081
Training Epoch: 21 [36736/50048]	Loss: 1.1567
Training Epoch: 21 [36864/50048]	Loss: 0.9987
Training Epoch: 21 [36992/50048]	Loss: 1.1592
Training Epoch: 21 [37120/50048]	Loss: 1.2764
Training Epoch: 21 [37248/50048]	Loss: 1.2561
Training Epoch: 21 [37376/50048]	Loss: 1.2840
Training Epoch: 21 [37504/50048]	Loss: 1.0647
Training Epoch: 21 [37632/50048]	Loss: 1.3262
Training Epoch: 21 [37760/50048]	Loss: 1.1929
Training Epoch: 21 [37888/50048]	Loss: 1.1835
Training Epoch: 21 [38016/50048]	Loss: 1.0709
Training Epoch: 21 [38144/50048]	Loss: 1.0097
Training Epoch: 21 [38272/50048]	Loss: 1.1439
Training Epoch: 21 [38400/50048]	Loss: 1.0415
Training Epoch: 21 [38528/50048]	Loss: 1.0389
Training Epoch: 21 [38656/50048]	Loss: 0.9238
Training Epoch: 21 [38784/50048]	Loss: 0.9973
Training Epoch: 21 [38912/50048]	Loss: 1.2242
Training Epoch: 21 [39040/50048]	Loss: 1.0842
Training Epoch: 21 [39168/50048]	Loss: 1.1961
Training Epoch: 21 [39296/50048]	Loss: 1.2454
Training Epoch: 21 [39424/50048]	Loss: 1.3635
Training Epoch: 21 [39552/50048]	Loss: 1.3398
Training Epoch: 21 [39680/50048]	Loss: 0.9142
Training Epoch: 21 [39808/50048]	Loss: 1.0956
Training Epoch: 21 [39936/50048]	Loss: 1.1981
Training Epoch: 21 [40064/50048]	Loss: 1.2304
Training Epoch: 21 [40192/50048]	Loss: 0.9805
Training Epoch: 21 [40320/50048]	Loss: 1.2767
Training Epoch: 21 [40448/50048]	Loss: 1.0632
Training Epoch: 21 [40576/50048]	Loss: 1.3742
Training Epoch: 21 [40704/50048]	Loss: 1.5407
Training Epoch: 21 [40832/50048]	Loss: 1.0911
Training Epoch: 21 [40960/50048]	Loss: 1.1954
Training Epoch: 21 [41088/50048]	Loss: 1.1548
Training Epoch: 21 [41216/50048]	Loss: 1.2186
Training Epoch: 21 [41344/50048]	Loss: 1.1179
Training Epoch: 21 [41472/50048]	Loss: 1.1637
Training Epoch: 21 [41600/50048]	Loss: 1.1946
Training Epoch: 21 [41728/50048]	Loss: 1.1944
Training Epoch: 21 [41856/50048]	Loss: 1.3651
Training Epoch: 21 [41984/50048]	Loss: 1.1672
Training Epoch: 21 [42112/50048]	Loss: 1.2751
Training Epoch: 21 [42240/50048]	Loss: 1.2336
Training Epoch: 21 [42368/50048]	Loss: 1.4232
Training Epoch: 21 [42496/50048]	Loss: 1.2536
Training Epoch: 21 [42624/50048]	Loss: 1.2662
Training Epoch: 21 [42752/50048]	Loss: 1.3347
Training Epoch: 21 [42880/50048]	Loss: 1.2396
Training Epoch: 21 [43008/50048]	Loss: 1.1131
Training Epoch: 21 [43136/50048]	Loss: 1.3040
Training Epoch: 21 [43264/50048]	Loss: 1.1280
Training Epoch: 21 [43392/50048]	Loss: 1.3592
Training Epoch: 21 [43520/50048]	Loss: 1.1495
Training Epoch: 21 [43648/50048]	Loss: 1.2970
Training Epoch: 21 [43776/50048]	Loss: 1.3109
Training Epoch: 21 [43904/50048]	Loss: 1.1959
Training Epoch: 21 [44032/50048]	Loss: 1.1248
Training Epoch: 21 [44160/50048]	Loss: 1.2166
Training Epoch: 21 [44288/50048]	Loss: 1.0780
Training Epoch: 21 [44416/50048]	Loss: 1.1924
Training Epoch: 21 [44544/50048]	Loss: 1.2447
Training Epoch: 21 [44672/50048]	Loss: 1.2823
Training Epoch: 21 [44800/50048]	Loss: 1.4217
Training Epoch: 21 [44928/50048]	Loss: 1.1422
Training Epoch: 21 [45056/50048]	Loss: 1.1212
Training Epoch: 21 [45184/50048]	Loss: 1.0351
Training Epoch: 21 [45312/50048]	Loss: 1.1784
Training Epoch: 21 [45440/50048]	Loss: 1.0445
Training Epoch: 21 [45568/50048]	Loss: 1.0261
Training Epoch: 21 [45696/50048]	Loss: 1.1176
Training Epoch: 21 [45824/50048]	Loss: 1.0160
Training Epoch: 21 [45952/50048]	Loss: 1.2691
Training Epoch: 21 [46080/50048]	Loss: 1.1444
Training Epoch: 21 [46208/50048]	Loss: 1.4679
Training Epoch: 21 [46336/50048]	Loss: 0.9785
Training Epoch: 21 [46464/50048]	Loss: 1.0848
Training Epoch: 21 [46592/50048]	Loss: 1.0914
Training Epoch: 21 [46720/50048]	Loss: 1.1925
Training Epoch: 21 [46848/50048]	Loss: 1.2624
Training Epoch: 21 [46976/50048]	Loss: 0.9999
Training Epoch: 21 [47104/50048]	Loss: 1.3169
Training Epoch: 21 [47232/50048]	Loss: 1.1558
Training Epoch: 21 [47360/50048]	Loss: 1.3723
Training Epoch: 21 [47488/50048]	Loss: 1.2884
Training Epoch: 21 [47616/50048]	Loss: 1.2366
Training Epoch: 21 [47744/50048]	Loss: 1.4464
Training Epoch: 21 [47872/50048]	Loss: 1.2465
Training Epoch: 21 [48000/50048]	Loss: 1.3243
Training Epoch: 21 [48128/50048]	Loss: 1.1593
Training Epoch: 21 [48256/50048]	Loss: 1.3064
Training Epoch: 21 [48384/50048]	Loss: 1.1089
Training Epoch: 21 [48512/50048]	Loss: 1.3340
Training Epoch: 21 [48640/50048]	Loss: 1.1104
Training Epoch: 21 [48768/50048]	Loss: 1.2127
Training Epoch: 21 [48896/50048]	Loss: 1.1645
Training Epoch: 21 [49024/50048]	Loss: 1.0525
Training Epoch: 21 [49152/50048]	Loss: 1.0419
Training Epoch: 21 [49280/50048]	Loss: 1.2479
Training Epoch: 21 [49408/50048]	Loss: 1.0851
Training Epoch: 21 [49536/50048]	Loss: 1.3555
Training Epoch: 21 [49664/50048]	Loss: 1.4039
Training Epoch: 21 [49792/50048]	Loss: 1.1607
Training Epoch: 21 [49920/50048]	Loss: 1.1799
Training Epoch: 21 [50048/50048]	Loss: 1.0680
Validation Epoch: 21, Average loss: 0.0114, Accuracy: 0.5993
Training Epoch: 22 [128/50048]	Loss: 1.0594
Training Epoch: 22 [256/50048]	Loss: 0.8481
Training Epoch: 22 [384/50048]	Loss: 1.1293
Training Epoch: 22 [512/50048]	Loss: 1.0417
Training Epoch: 22 [640/50048]	Loss: 1.2616
Training Epoch: 22 [768/50048]	Loss: 1.0032
Training Epoch: 22 [896/50048]	Loss: 1.1024
Training Epoch: 22 [1024/50048]	Loss: 0.9859
Training Epoch: 22 [1152/50048]	Loss: 0.9942
Training Epoch: 22 [1280/50048]	Loss: 1.0422
Training Epoch: 22 [1408/50048]	Loss: 1.1450
Training Epoch: 22 [1536/50048]	Loss: 1.1085
Training Epoch: 22 [1664/50048]	Loss: 1.1755
Training Epoch: 22 [1792/50048]	Loss: 1.0754
Training Epoch: 22 [1920/50048]	Loss: 1.0555
Training Epoch: 22 [2048/50048]	Loss: 1.1947
Training Epoch: 22 [2176/50048]	Loss: 1.0609
Training Epoch: 22 [2304/50048]	Loss: 1.2391
Training Epoch: 22 [2432/50048]	Loss: 1.1768
Training Epoch: 22 [2560/50048]	Loss: 1.1963
Training Epoch: 22 [2688/50048]	Loss: 1.1259
Training Epoch: 22 [2816/50048]	Loss: 1.3635
Training Epoch: 22 [2944/50048]	Loss: 1.0685
Training Epoch: 22 [3072/50048]	Loss: 1.0926
Training Epoch: 22 [3200/50048]	Loss: 1.1010
Training Epoch: 22 [3328/50048]	Loss: 1.1430
Training Epoch: 22 [3456/50048]	Loss: 1.2570
Training Epoch: 22 [3584/50048]	Loss: 1.2827
Training Epoch: 22 [3712/50048]	Loss: 1.0266
Training Epoch: 22 [3840/50048]	Loss: 1.1773
Training Epoch: 22 [3968/50048]	Loss: 1.2101
Training Epoch: 22 [4096/50048]	Loss: 1.2610
Training Epoch: 22 [4224/50048]	Loss: 1.3473
Training Epoch: 22 [4352/50048]	Loss: 0.9843
Training Epoch: 22 [4480/50048]	Loss: 1.1105
Training Epoch: 22 [4608/50048]	Loss: 1.1408
Training Epoch: 22 [4736/50048]	Loss: 1.0003
Training Epoch: 22 [4864/50048]	Loss: 1.1575
Training Epoch: 22 [4992/50048]	Loss: 1.0320
Training Epoch: 22 [5120/50048]	Loss: 1.1003
Training Epoch: 22 [5248/50048]	Loss: 1.1195
Training Epoch: 22 [5376/50048]	Loss: 1.1881
Training Epoch: 22 [5504/50048]	Loss: 1.1369
Training Epoch: 22 [5632/50048]	Loss: 1.1148
Training Epoch: 22 [5760/50048]	Loss: 1.1783
Training Epoch: 22 [5888/50048]	Loss: 1.3219
Training Epoch: 22 [6016/50048]	Loss: 1.1027
Training Epoch: 22 [6144/50048]	Loss: 1.1674
Training Epoch: 22 [6272/50048]	Loss: 1.0747
Training Epoch: 22 [6400/50048]	Loss: 0.9290
Training Epoch: 22 [6528/50048]	Loss: 1.3624
Training Epoch: 22 [6656/50048]	Loss: 1.1855
Training Epoch: 22 [6784/50048]	Loss: 1.0922
Training Epoch: 22 [6912/50048]	Loss: 1.3815
Training Epoch: 22 [7040/50048]	Loss: 1.0797
Training Epoch: 22 [7168/50048]	Loss: 1.0944
Training Epoch: 22 [7296/50048]	Loss: 0.9998
Training Epoch: 22 [7424/50048]	Loss: 1.0996
Training Epoch: 22 [7552/50048]	Loss: 1.1143
Training Epoch: 22 [7680/50048]	Loss: 1.2611
Training Epoch: 22 [7808/50048]	Loss: 1.2072
Training Epoch: 22 [7936/50048]	Loss: 1.1297
Training Epoch: 22 [8064/50048]	Loss: 1.0131
Training Epoch: 22 [8192/50048]	Loss: 1.2592
Training Epoch: 22 [8320/50048]	Loss: 1.2548
Training Epoch: 22 [8448/50048]	Loss: 1.1667
Training Epoch: 22 [8576/50048]	Loss: 1.1369
Training Epoch: 22 [8704/50048]	Loss: 1.2165
Training Epoch: 22 [8832/50048]	Loss: 1.0887
Training Epoch: 22 [8960/50048]	Loss: 0.9287
Training Epoch: 22 [9088/50048]	Loss: 1.2110
Training Epoch: 22 [9216/50048]	Loss: 1.3539
Training Epoch: 22 [9344/50048]	Loss: 1.1473
Training Epoch: 22 [9472/50048]	Loss: 1.3182
Training Epoch: 22 [9600/50048]	Loss: 1.1431
Training Epoch: 22 [9728/50048]	Loss: 1.1374
Training Epoch: 22 [9856/50048]	Loss: 1.1959
Training Epoch: 22 [9984/50048]	Loss: 1.3226
Training Epoch: 22 [10112/50048]	Loss: 1.0370
Training Epoch: 22 [10240/50048]	Loss: 1.0824
Training Epoch: 22 [10368/50048]	Loss: 1.1107
Training Epoch: 22 [10496/50048]	Loss: 1.1645
Training Epoch: 22 [10624/50048]	Loss: 1.1005
Training Epoch: 22 [10752/50048]	Loss: 0.9210
Training Epoch: 22 [10880/50048]	Loss: 1.2111
Training Epoch: 22 [11008/50048]	Loss: 1.2755
Training Epoch: 22 [11136/50048]	Loss: 0.9384
Training Epoch: 22 [11264/50048]	Loss: 1.2955
Training Epoch: 22 [11392/50048]	Loss: 1.1479
Training Epoch: 22 [11520/50048]	Loss: 1.0306
Training Epoch: 22 [11648/50048]	Loss: 1.1054
Training Epoch: 22 [11776/50048]	Loss: 1.3123
Training Epoch: 22 [11904/50048]	Loss: 0.9374
Training Epoch: 22 [12032/50048]	Loss: 1.1232
Training Epoch: 22 [12160/50048]	Loss: 1.3556
Training Epoch: 22 [12288/50048]	Loss: 1.0976
Training Epoch: 22 [12416/50048]	Loss: 1.1650
Training Epoch: 22 [12544/50048]	Loss: 1.3901
Training Epoch: 22 [12672/50048]	Loss: 1.0946
Training Epoch: 22 [12800/50048]	Loss: 1.2185
Training Epoch: 22 [12928/50048]	Loss: 0.9221
Training Epoch: 22 [13056/50048]	Loss: 1.0282
Training Epoch: 22 [13184/50048]	Loss: 1.4050
Training Epoch: 22 [13312/50048]	Loss: 1.2834
Training Epoch: 22 [13440/50048]	Loss: 1.0384
Training Epoch: 22 [13568/50048]	Loss: 1.0763
Training Epoch: 22 [13696/50048]	Loss: 0.9194
Training Epoch: 22 [13824/50048]	Loss: 1.1698
Training Epoch: 22 [13952/50048]	Loss: 1.1685
Training Epoch: 22 [14080/50048]	Loss: 1.0932
Training Epoch: 22 [14208/50048]	Loss: 0.9563
Training Epoch: 22 [14336/50048]	Loss: 1.0042
Training Epoch: 22 [14464/50048]	Loss: 1.0355
Training Epoch: 22 [14592/50048]	Loss: 1.1984
Training Epoch: 22 [14720/50048]	Loss: 1.2726
Training Epoch: 22 [14848/50048]	Loss: 1.0771
Training Epoch: 22 [14976/50048]	Loss: 1.1425
Training Epoch: 22 [15104/50048]	Loss: 1.1598
Training Epoch: 22 [15232/50048]	Loss: 1.1348
Training Epoch: 22 [15360/50048]	Loss: 1.2398
Training Epoch: 22 [15488/50048]	Loss: 1.1090
Training Epoch: 22 [15616/50048]	Loss: 1.2729
Training Epoch: 22 [15744/50048]	Loss: 0.8249
Training Epoch: 22 [15872/50048]	Loss: 1.1267
Training Epoch: 22 [16000/50048]	Loss: 1.2623
Training Epoch: 22 [16128/50048]	Loss: 1.2020
Training Epoch: 22 [16256/50048]	Loss: 1.2388
Training Epoch: 22 [16384/50048]	Loss: 1.0274
Training Epoch: 22 [16512/50048]	Loss: 1.3446
Training Epoch: 22 [16640/50048]	Loss: 1.1016
Training Epoch: 22 [16768/50048]	Loss: 1.2241
Training Epoch: 22 [16896/50048]	Loss: 1.2056
Training Epoch: 22 [17024/50048]	Loss: 1.0910
Training Epoch: 22 [17152/50048]	Loss: 0.8649
Training Epoch: 22 [17280/50048]	Loss: 1.0477
Training Epoch: 22 [17408/50048]	Loss: 1.4073
Training Epoch: 22 [17536/50048]	Loss: 0.9589
Training Epoch: 22 [17664/50048]	Loss: 1.1661
Training Epoch: 22 [17792/50048]	Loss: 1.1801
Training Epoch: 22 [17920/50048]	Loss: 1.2807
Training Epoch: 22 [18048/50048]	Loss: 1.1626
Training Epoch: 22 [18176/50048]	Loss: 1.4607
Training Epoch: 22 [18304/50048]	Loss: 1.0556
Training Epoch: 22 [18432/50048]	Loss: 1.0817
Training Epoch: 22 [18560/50048]	Loss: 0.8924
Training Epoch: 22 [18688/50048]	Loss: 0.9821
Training Epoch: 22 [18816/50048]	Loss: 1.1544
Training Epoch: 22 [18944/50048]	Loss: 0.9883
Training Epoch: 22 [19072/50048]	Loss: 0.9911
Training Epoch: 22 [19200/50048]	Loss: 1.0330
Training Epoch: 22 [19328/50048]	Loss: 1.2612
Training Epoch: 22 [19456/50048]	Loss: 1.0408
Training Epoch: 22 [19584/50048]	Loss: 1.4351
Training Epoch: 22 [19712/50048]	Loss: 1.0622
Training Epoch: 22 [19840/50048]	Loss: 1.1893
Training Epoch: 22 [19968/50048]	Loss: 0.9696
Training Epoch: 22 [20096/50048]	Loss: 1.2424
Training Epoch: 22 [20224/50048]	Loss: 1.1230
Training Epoch: 22 [20352/50048]	Loss: 0.8933
Training Epoch: 22 [20480/50048]	Loss: 1.2199
Training Epoch: 22 [20608/50048]	Loss: 1.3123
Training Epoch: 22 [20736/50048]	Loss: 0.9203
Training Epoch: 22 [20864/50048]	Loss: 1.2284
Training Epoch: 22 [20992/50048]	Loss: 1.3207
Training Epoch: 22 [21120/50048]	Loss: 1.0601
Training Epoch: 22 [21248/50048]	Loss: 1.2081
Training Epoch: 22 [21376/50048]	Loss: 1.0047
Training Epoch: 22 [21504/50048]	Loss: 1.2155
Training Epoch: 22 [21632/50048]	Loss: 1.2809
Training Epoch: 22 [21760/50048]	Loss: 1.0968
Training Epoch: 22 [21888/50048]	Loss: 1.4451
Training Epoch: 22 [22016/50048]	Loss: 1.1833
Training Epoch: 22 [22144/50048]	Loss: 1.1529
Training Epoch: 22 [22272/50048]	Loss: 0.8640
Training Epoch: 22 [22400/50048]	Loss: 1.2454
Training Epoch: 22 [22528/50048]	Loss: 1.1802
Training Epoch: 22 [22656/50048]	Loss: 1.0286
Training Epoch: 22 [22784/50048]	Loss: 1.0360
Training Epoch: 22 [22912/50048]	Loss: 1.0006
Training Epoch: 22 [23040/50048]	Loss: 1.5799
Training Epoch: 22 [23168/50048]	Loss: 0.9168
Training Epoch: 22 [23296/50048]	Loss: 1.0227
Training Epoch: 22 [23424/50048]	Loss: 1.3889
Training Epoch: 22 [23552/50048]	Loss: 1.0930
Training Epoch: 22 [23680/50048]	Loss: 1.0109
Training Epoch: 22 [23808/50048]	Loss: 1.0327
Training Epoch: 22 [23936/50048]	Loss: 1.1556
Training Epoch: 22 [24064/50048]	Loss: 1.2260
Training Epoch: 22 [24192/50048]	Loss: 1.4616
Training Epoch: 22 [24320/50048]	Loss: 1.2427
Training Epoch: 22 [24448/50048]	Loss: 0.9890
Training Epoch: 22 [24576/50048]	Loss: 1.3952
Training Epoch: 22 [24704/50048]	Loss: 1.1136
Training Epoch: 22 [24832/50048]	Loss: 1.2291
Training Epoch: 22 [24960/50048]	Loss: 1.1148
Training Epoch: 22 [25088/50048]	Loss: 1.3190
Training Epoch: 22 [25216/50048]	Loss: 1.4159
Training Epoch: 22 [25344/50048]	Loss: 1.0763
Training Epoch: 22 [25472/50048]	Loss: 0.9897
Training Epoch: 22 [25600/50048]	Loss: 1.0222
Training Epoch: 22 [25728/50048]	Loss: 1.2982
Training Epoch: 22 [25856/50048]	Loss: 1.1940
Training Epoch: 22 [25984/50048]	Loss: 0.9558
Training Epoch: 22 [26112/50048]	Loss: 1.1245
Training Epoch: 22 [26240/50048]	Loss: 1.3002
Training Epoch: 22 [26368/50048]	Loss: 1.1581
Training Epoch: 22 [26496/50048]	Loss: 1.0101
Training Epoch: 22 [26624/50048]	Loss: 0.9844
Training Epoch: 22 [26752/50048]	Loss: 1.3430
Training Epoch: 22 [26880/50048]	Loss: 1.2601
Training Epoch: 22 [27008/50048]	Loss: 1.1347
Training Epoch: 22 [27136/50048]	Loss: 1.0934
Training Epoch: 22 [27264/50048]	Loss: 1.2113
Training Epoch: 22 [27392/50048]	Loss: 1.1312
Training Epoch: 22 [27520/50048]	Loss: 1.0486
Training Epoch: 22 [27648/50048]	Loss: 1.1261
Training Epoch: 22 [27776/50048]	Loss: 1.2273
Training Epoch: 22 [27904/50048]	Loss: 1.0420
Training Epoch: 22 [28032/50048]	Loss: 1.2956
Training Epoch: 22 [28160/50048]	Loss: 0.8761
Training Epoch: 22 [28288/50048]	Loss: 1.0687
Training Epoch: 22 [28416/50048]	Loss: 1.3025
Training Epoch: 22 [28544/50048]	Loss: 1.5371
Training Epoch: 22 [28672/50048]	Loss: 1.1496
Training Epoch: 22 [28800/50048]	Loss: 1.1184
Training Epoch: 22 [28928/50048]	Loss: 1.1321
Training Epoch: 22 [29056/50048]	Loss: 1.0683
Training Epoch: 22 [29184/50048]	Loss: 1.3066
Training Epoch: 22 [29312/50048]	Loss: 1.2120
Training Epoch: 22 [29440/50048]	Loss: 1.0535
Training Epoch: 22 [29568/50048]	Loss: 1.1393
Training Epoch: 22 [29696/50048]	Loss: 1.2208
Training Epoch: 22 [29824/50048]	Loss: 1.1110
Training Epoch: 22 [29952/50048]	Loss: 1.0795
Training Epoch: 22 [30080/50048]	Loss: 0.9771
Training Epoch: 22 [30208/50048]	Loss: 1.2547
Training Epoch: 22 [30336/50048]	Loss: 1.1021
Training Epoch: 22 [30464/50048]	Loss: 1.4988
Training Epoch: 22 [30592/50048]	Loss: 1.2935
Training Epoch: 22 [30720/50048]	Loss: 1.0761
Training Epoch: 22 [30848/50048]	Loss: 1.1545
Training Epoch: 22 [30976/50048]	Loss: 1.0322
Training Epoch: 22 [31104/50048]	Loss: 1.1000
Training Epoch: 22 [31232/50048]	Loss: 1.1521
Training Epoch: 22 [31360/50048]	Loss: 1.1868
Training Epoch: 22 [31488/50048]	Loss: 1.2159
Training Epoch: 22 [31616/50048]	Loss: 1.4153
Training Epoch: 22 [31744/50048]	Loss: 1.0321
Training Epoch: 22 [31872/50048]	Loss: 1.2488
Training Epoch: 22 [32000/50048]	Loss: 1.1238
Training Epoch: 22 [32128/50048]	Loss: 1.1766
Training Epoch: 22 [32256/50048]	Loss: 1.0139
Training Epoch: 22 [32384/50048]	Loss: 1.1362
Training Epoch: 22 [32512/50048]	Loss: 1.0300
Training Epoch: 22 [32640/50048]	Loss: 1.1478
Training Epoch: 22 [32768/50048]	Loss: 0.9758
Training Epoch: 22 [32896/50048]	Loss: 1.1594
Training Epoch: 22 [33024/50048]	Loss: 1.1497
Training Epoch: 22 [33152/50048]	Loss: 1.1005
Training Epoch: 22 [33280/50048]	Loss: 1.0788
Training Epoch: 22 [33408/50048]	Loss: 1.2405
Training Epoch: 22 [33536/50048]	Loss: 1.3497
Training Epoch: 22 [33664/50048]	Loss: 1.2367
Training Epoch: 22 [33792/50048]	Loss: 1.1146
Training Epoch: 22 [33920/50048]	Loss: 1.2500
Training Epoch: 22 [34048/50048]	Loss: 1.1087
Training Epoch: 22 [34176/50048]	Loss: 1.2761
Training Epoch: 22 [34304/50048]	Loss: 1.2155
Training Epoch: 22 [34432/50048]	Loss: 1.0275
Training Epoch: 22 [34560/50048]	Loss: 1.4202
Training Epoch: 22 [34688/50048]	Loss: 1.0770
Training Epoch: 22 [34816/50048]	Loss: 1.3439
Training Epoch: 22 [34944/50048]	Loss: 1.3609
Training Epoch: 22 [35072/50048]	Loss: 0.8882
Training Epoch: 22 [35200/50048]	Loss: 1.3103
Training Epoch: 22 [35328/50048]	Loss: 0.9854
Training Epoch: 22 [35456/50048]	Loss: 1.5661
Training Epoch: 22 [35584/50048]	Loss: 1.1388
Training Epoch: 22 [35712/50048]	Loss: 1.0826
Training Epoch: 22 [35840/50048]	Loss: 0.9588
Training Epoch: 22 [35968/50048]	Loss: 1.2239
Training Epoch: 22 [36096/50048]	Loss: 1.0048
Training Epoch: 22 [36224/50048]	Loss: 0.9773
Training Epoch: 22 [36352/50048]	Loss: 1.0685
Training Epoch: 22 [36480/50048]	Loss: 1.0613
Training Epoch: 22 [36608/50048]	Loss: 1.1301
Training Epoch: 22 [36736/50048]	Loss: 1.0165
Training Epoch: 22 [36864/50048]	Loss: 1.1545
Training Epoch: 22 [36992/50048]	Loss: 1.0807
Training Epoch: 22 [37120/50048]	Loss: 0.8635
Training Epoch: 22 [37248/50048]	Loss: 1.3510
Training Epoch: 22 [37376/50048]	Loss: 1.2768
Training Epoch: 22 [37504/50048]	Loss: 1.2536
Training Epoch: 22 [37632/50048]	Loss: 1.0542
Training Epoch: 22 [37760/50048]	Loss: 1.1565
Training Epoch: 22 [37888/50048]	Loss: 1.1074
Training Epoch: 22 [38016/50048]	Loss: 1.2331
Training Epoch: 22 [38144/50048]	Loss: 1.2776
Training Epoch: 22 [38272/50048]	Loss: 1.1945
Training Epoch: 22 [38400/50048]	Loss: 1.0208
Training Epoch: 22 [38528/50048]	Loss: 1.1937
Training Epoch: 22 [38656/50048]	Loss: 0.9509
Training Epoch: 22 [38784/50048]	Loss: 1.2956
Training Epoch: 22 [38912/50048]	Loss: 1.2303
Training Epoch: 22 [39040/50048]	Loss: 1.1003
Training Epoch: 22 [39168/50048]	Loss: 1.1901
Training Epoch: 22 [39296/50048]	Loss: 1.0340
Training Epoch: 22 [39424/50048]	Loss: 1.4193
Training Epoch: 22 [39552/50048]	Loss: 1.3699
Training Epoch: 22 [39680/50048]	Loss: 0.9889
Training Epoch: 22 [39808/50048]	Loss: 1.2239
Training Epoch: 22 [39936/50048]	Loss: 1.2810
Training Epoch: 22 [40064/50048]	Loss: 1.0529
Training Epoch: 22 [40192/50048]	Loss: 1.1141
Training Epoch: 22 [40320/50048]	Loss: 1.1496
Training Epoch: 22 [40448/50048]	Loss: 1.1410
Training Epoch: 22 [40576/50048]	Loss: 1.0795
Training Epoch: 22 [40704/50048]	Loss: 1.0308
Training Epoch: 22 [40832/50048]	Loss: 1.2017
Training Epoch: 22 [40960/50048]	Loss: 1.2823
Training Epoch: 22 [41088/50048]	Loss: 1.1025
Training Epoch: 22 [41216/50048]	Loss: 0.8400
Training Epoch: 22 [41344/50048]	Loss: 1.5039
Training Epoch: 22 [41472/50048]	Loss: 1.2125
Training Epoch: 22 [41600/50048]	Loss: 1.2679
Training Epoch: 22 [41728/50048]	Loss: 0.9985
Training Epoch: 22 [41856/50048]	Loss: 1.2572
Training Epoch: 22 [41984/50048]	Loss: 0.9509
Training Epoch: 22 [42112/50048]	Loss: 0.9470
Training Epoch: 22 [42240/50048]	Loss: 1.2500
Training Epoch: 22 [42368/50048]	Loss: 1.2664
Training Epoch: 22 [42496/50048]	Loss: 1.2160
Training Epoch: 22 [42624/50048]	Loss: 1.1454
Training Epoch: 22 [42752/50048]	Loss: 1.2853
Training Epoch: 22 [42880/50048]	Loss: 0.9752
Training Epoch: 22 [43008/50048]	Loss: 0.9662
Training Epoch: 22 [43136/50048]	Loss: 1.1866
Training Epoch: 22 [43264/50048]	Loss: 1.0115
Training Epoch: 22 [43392/50048]	Loss: 1.1429
Training Epoch: 22 [43520/50048]	Loss: 0.9206
Training Epoch: 22 [43648/50048]	Loss: 1.3857
Training Epoch: 22 [43776/50048]	Loss: 1.4167
Training Epoch: 22 [43904/50048]	Loss: 1.2688
Training Epoch: 22 [44032/50048]	Loss: 1.1600
Training Epoch: 22 [44160/50048]	Loss: 1.1691
Training Epoch: 22 [44288/50048]	Loss: 0.8742
Training Epoch: 22 [44416/50048]	Loss: 1.0545
Training Epoch: 22 [44544/50048]	Loss: 0.9409
Training Epoch: 22 [44672/50048]	Loss: 1.2178
Training Epoch: 22 [44800/50048]	Loss: 1.1088
Training Epoch: 22 [44928/50048]	Loss: 1.1535
Training Epoch: 22 [45056/50048]	Loss: 1.2341
Training Epoch: 22 [45184/50048]	Loss: 1.2573
Training Epoch: 22 [45312/50048]	Loss: 1.2637
Training Epoch: 22 [45440/50048]	Loss: 1.0816
Training Epoch: 22 [45568/50048]	Loss: 1.4584
Training Epoch: 22 [45696/50048]	Loss: 0.9817
Training Epoch: 22 [45824/50048]	Loss: 1.2708
Training Epoch: 22 [45952/50048]	Loss: 1.1105
Training Epoch: 22 [46080/50048]	Loss: 1.0062
Training Epoch: 22 [46208/50048]	Loss: 1.3005
Training Epoch: 22 [46336/50048]	Loss: 1.2237
Training Epoch: 22 [46464/50048]	Loss: 1.0572
Training Epoch: 22 [46592/50048]	Loss: 1.2214
Training Epoch: 22 [46720/50048]	Loss: 1.3012
Training Epoch: 22 [46848/50048]	Loss: 1.1506
Training Epoch: 22 [46976/50048]	Loss: 1.3990
Training Epoch: 22 [47104/50048]	Loss: 1.2901
Training Epoch: 22 [47232/50048]	Loss: 1.2182
Training Epoch: 22 [47360/50048]	Loss: 1.1245
Training Epoch: 22 [47488/50048]	Loss: 1.3329
Training Epoch: 22 [47616/50048]	Loss: 1.2929
Training Epoch: 22 [47744/50048]	Loss: 1.1904
Training Epoch: 22 [47872/50048]	Loss: 1.1689
Training Epoch: 22 [48000/50048]	Loss: 0.9620
Training Epoch: 22 [48128/50048]	Loss: 1.2264
Training Epoch: 22 [48256/50048]	Loss: 1.1871
Training Epoch: 22 [48384/50048]	Loss: 1.0765
Training Epoch: 22 [48512/50048]	Loss: 0.9774
Training Epoch: 22 [48640/50048]	Loss: 1.0092
Training Epoch: 22 [48768/50048]	Loss: 1.0431
Training Epoch: 22 [48896/50048]	Loss: 1.1134
Training Epoch: 22 [49024/50048]	Loss: 1.1665
Training Epoch: 22 [49152/50048]	Loss: 1.0921
Training Epoch: 22 [49280/50048]	Loss: 1.1943
Training Epoch: 22 [49408/50048]	Loss: 1.2041
Training Epoch: 22 [49536/50048]	Loss: 1.2655
Training Epoch: 22 [49664/50048]	Loss: 1.1050
Training Epoch: 22 [49792/50048]	Loss: 1.2479
Training Epoch: 22 [49920/50048]	Loss: 1.0456
Training Epoch: 22 [50048/50048]	Loss: 1.2238
Validation Epoch: 22, Average loss: 0.0113, Accuracy: 0.5997
Training Epoch: 23 [128/50048]	Loss: 1.1068
Training Epoch: 23 [256/50048]	Loss: 1.2386
Training Epoch: 23 [384/50048]	Loss: 1.0475
Training Epoch: 23 [512/50048]	Loss: 1.0976
Training Epoch: 23 [640/50048]	Loss: 1.3897
Training Epoch: 23 [768/50048]	Loss: 1.1406
Training Epoch: 23 [896/50048]	Loss: 1.1531
Training Epoch: 23 [1024/50048]	Loss: 1.0227
Training Epoch: 23 [1152/50048]	Loss: 1.2766
Training Epoch: 23 [1280/50048]	Loss: 1.4070
Training Epoch: 23 [1408/50048]	Loss: 1.2478
Training Epoch: 23 [1536/50048]	Loss: 1.0656
Training Epoch: 23 [1664/50048]	Loss: 1.1208
Training Epoch: 23 [1792/50048]	Loss: 1.5621
Training Epoch: 23 [1920/50048]	Loss: 1.0255
Training Epoch: 23 [2048/50048]	Loss: 1.1101
Training Epoch: 23 [2176/50048]	Loss: 1.2583
Training Epoch: 23 [2304/50048]	Loss: 1.0687
Training Epoch: 23 [2432/50048]	Loss: 1.1262
Training Epoch: 23 [2560/50048]	Loss: 1.0025
Training Epoch: 23 [2688/50048]	Loss: 1.1340
Training Epoch: 23 [2816/50048]	Loss: 1.0450
Training Epoch: 23 [2944/50048]	Loss: 1.0495
Training Epoch: 23 [3072/50048]	Loss: 1.1966
Training Epoch: 23 [3200/50048]	Loss: 1.5066
Training Epoch: 23 [3328/50048]	Loss: 1.1307
Training Epoch: 23 [3456/50048]	Loss: 0.9755
Training Epoch: 23 [3584/50048]	Loss: 1.0445
Training Epoch: 23 [3712/50048]	Loss: 0.8765
Training Epoch: 23 [3840/50048]	Loss: 1.2848
Training Epoch: 23 [3968/50048]	Loss: 0.9900
Training Epoch: 23 [4096/50048]	Loss: 0.9469
Training Epoch: 23 [4224/50048]	Loss: 1.3656
Training Epoch: 23 [4352/50048]	Loss: 1.0225
Training Epoch: 23 [4480/50048]	Loss: 0.9946
Training Epoch: 23 [4608/50048]	Loss: 0.9857
Training Epoch: 23 [4736/50048]	Loss: 0.9347
Training Epoch: 23 [4864/50048]	Loss: 1.1866
Training Epoch: 23 [4992/50048]	Loss: 1.1463
Training Epoch: 23 [5120/50048]	Loss: 1.0621
Training Epoch: 23 [5248/50048]	Loss: 0.9853
Training Epoch: 23 [5376/50048]	Loss: 1.0686
Training Epoch: 23 [5504/50048]	Loss: 1.1335
Training Epoch: 23 [5632/50048]	Loss: 1.0613
Training Epoch: 23 [5760/50048]	Loss: 1.2991
Training Epoch: 23 [5888/50048]	Loss: 1.0669
Training Epoch: 23 [6016/50048]	Loss: 1.0751
Training Epoch: 23 [6144/50048]	Loss: 1.1047
Training Epoch: 23 [6272/50048]	Loss: 1.3346
Training Epoch: 23 [6400/50048]	Loss: 0.9437
Training Epoch: 23 [6528/50048]	Loss: 1.1588
Training Epoch: 23 [6656/50048]	Loss: 1.3029
Training Epoch: 23 [6784/50048]	Loss: 1.0204
Training Epoch: 23 [6912/50048]	Loss: 1.1034
Training Epoch: 23 [7040/50048]	Loss: 1.0219
Training Epoch: 23 [7168/50048]	Loss: 1.1555
Training Epoch: 23 [7296/50048]	Loss: 0.9603
Training Epoch: 23 [7424/50048]	Loss: 0.9954
Training Epoch: 23 [7552/50048]	Loss: 1.2095
Training Epoch: 23 [7680/50048]	Loss: 0.9592
Training Epoch: 23 [7808/50048]	Loss: 1.1821
Training Epoch: 23 [7936/50048]	Loss: 1.0141
Training Epoch: 23 [8064/50048]	Loss: 1.1598
Training Epoch: 23 [8192/50048]	Loss: 1.1057
Training Epoch: 23 [8320/50048]	Loss: 0.8996
Training Epoch: 23 [8448/50048]	Loss: 1.1113
Training Epoch: 23 [8576/50048]	Loss: 1.1226
Training Epoch: 23 [8704/50048]	Loss: 0.9073
Training Epoch: 23 [8832/50048]	Loss: 0.8460
Training Epoch: 23 [8960/50048]	Loss: 1.1462
Training Epoch: 23 [9088/50048]	Loss: 0.9807
Training Epoch: 23 [9216/50048]	Loss: 1.0446
Training Epoch: 23 [9344/50048]	Loss: 1.1538
Training Epoch: 23 [9472/50048]	Loss: 1.0763
Training Epoch: 23 [9600/50048]	Loss: 1.0046
Training Epoch: 23 [9728/50048]	Loss: 0.9764
Training Epoch: 23 [9856/50048]	Loss: 0.9997
Training Epoch: 23 [9984/50048]	Loss: 1.2402
Training Epoch: 23 [10112/50048]	Loss: 0.9437
Training Epoch: 23 [10240/50048]	Loss: 0.9162
Training Epoch: 23 [10368/50048]	Loss: 1.1394
Training Epoch: 23 [10496/50048]	Loss: 0.9238
Training Epoch: 23 [10624/50048]	Loss: 1.0717
Training Epoch: 23 [10752/50048]	Loss: 1.1520
Training Epoch: 23 [10880/50048]	Loss: 1.0293
Training Epoch: 23 [11008/50048]	Loss: 1.1241
Training Epoch: 23 [11136/50048]	Loss: 1.2731
Training Epoch: 23 [11264/50048]	Loss: 0.9440
Training Epoch: 23 [11392/50048]	Loss: 1.0852
Training Epoch: 23 [11520/50048]	Loss: 0.8982
Training Epoch: 23 [11648/50048]	Loss: 0.9199
Training Epoch: 23 [11776/50048]	Loss: 1.1316
Training Epoch: 23 [11904/50048]	Loss: 1.1462
Training Epoch: 23 [12032/50048]	Loss: 1.2251
Training Epoch: 23 [12160/50048]	Loss: 1.1021
Training Epoch: 23 [12288/50048]	Loss: 0.9653
Training Epoch: 23 [12416/50048]	Loss: 1.0714
Training Epoch: 23 [12544/50048]	Loss: 1.0626
Training Epoch: 23 [12672/50048]	Loss: 0.9208
Training Epoch: 23 [12800/50048]	Loss: 1.0810
Training Epoch: 23 [12928/50048]	Loss: 1.2289
Training Epoch: 23 [13056/50048]	Loss: 1.0690
Training Epoch: 23 [13184/50048]	Loss: 1.2084
Training Epoch: 23 [13312/50048]	Loss: 1.1936
Training Epoch: 23 [13440/50048]	Loss: 1.1044
Training Epoch: 23 [13568/50048]	Loss: 1.0383
Training Epoch: 23 [13696/50048]	Loss: 1.2660
Training Epoch: 23 [13824/50048]	Loss: 1.1108
Training Epoch: 23 [13952/50048]	Loss: 1.1205
Training Epoch: 23 [14080/50048]	Loss: 1.1613
Training Epoch: 23 [14208/50048]	Loss: 1.0360
Training Epoch: 23 [14336/50048]	Loss: 1.2107
Training Epoch: 23 [14464/50048]	Loss: 0.9647
Training Epoch: 23 [14592/50048]	Loss: 1.1085
Training Epoch: 23 [14720/50048]	Loss: 1.3035
Training Epoch: 23 [14848/50048]	Loss: 1.0472
Training Epoch: 23 [14976/50048]	Loss: 1.0460
Training Epoch: 23 [15104/50048]	Loss: 1.0273
Training Epoch: 23 [15232/50048]	Loss: 1.0016
Training Epoch: 23 [15360/50048]	Loss: 1.0039
Training Epoch: 23 [15488/50048]	Loss: 1.1977
Training Epoch: 23 [15616/50048]	Loss: 1.1297
Training Epoch: 23 [15744/50048]	Loss: 1.2784
Training Epoch: 23 [15872/50048]	Loss: 0.9685
Training Epoch: 23 [16000/50048]	Loss: 1.1413
Training Epoch: 23 [16128/50048]	Loss: 1.1558
Training Epoch: 23 [16256/50048]	Loss: 1.2733
Training Epoch: 23 [16384/50048]	Loss: 1.0049
Training Epoch: 23 [16512/50048]	Loss: 1.0357
Training Epoch: 23 [16640/50048]	Loss: 1.0060
Training Epoch: 23 [16768/50048]	Loss: 1.2692
Training Epoch: 23 [16896/50048]	Loss: 1.1873
Training Epoch: 23 [17024/50048]	Loss: 1.2177
Training Epoch: 23 [17152/50048]	Loss: 1.1940
Training Epoch: 23 [17280/50048]	Loss: 1.1031
Training Epoch: 23 [17408/50048]	Loss: 1.0573
Training Epoch: 23 [17536/50048]	Loss: 1.2623
Training Epoch: 23 [17664/50048]	Loss: 1.1934
Training Epoch: 23 [17792/50048]	Loss: 1.2903
Training Epoch: 23 [17920/50048]	Loss: 1.1758
Training Epoch: 23 [18048/50048]	Loss: 1.2199
Training Epoch: 23 [18176/50048]	Loss: 0.9746
Training Epoch: 23 [18304/50048]	Loss: 1.1221
Training Epoch: 23 [18432/50048]	Loss: 1.3237
Training Epoch: 23 [18560/50048]	Loss: 1.0470
Training Epoch: 23 [18688/50048]	Loss: 1.1263
Training Epoch: 23 [18816/50048]	Loss: 1.2325
Training Epoch: 23 [18944/50048]	Loss: 1.0584
Training Epoch: 23 [19072/50048]	Loss: 0.9926
Training Epoch: 23 [19200/50048]	Loss: 0.9477
Training Epoch: 23 [19328/50048]	Loss: 1.1509
Training Epoch: 23 [19456/50048]	Loss: 1.3353
Training Epoch: 23 [19584/50048]	Loss: 1.0817
Training Epoch: 23 [19712/50048]	Loss: 1.2539
Training Epoch: 23 [19840/50048]	Loss: 1.2120
Training Epoch: 23 [19968/50048]	Loss: 1.2031
Training Epoch: 23 [20096/50048]	Loss: 1.3627
Training Epoch: 23 [20224/50048]	Loss: 1.1601
Training Epoch: 23 [20352/50048]	Loss: 1.0303
Training Epoch: 23 [20480/50048]	Loss: 1.0291
Training Epoch: 23 [20608/50048]	Loss: 1.2063
Training Epoch: 23 [20736/50048]	Loss: 1.0882
Training Epoch: 23 [20864/50048]	Loss: 0.9461
Training Epoch: 23 [20992/50048]	Loss: 1.1994
Training Epoch: 23 [21120/50048]	Loss: 0.9331
Training Epoch: 23 [21248/50048]	Loss: 1.1258
Training Epoch: 23 [21376/50048]	Loss: 1.3428
Training Epoch: 23 [21504/50048]	Loss: 1.2506
Training Epoch: 23 [21632/50048]	Loss: 0.8655
Training Epoch: 23 [21760/50048]	Loss: 1.0604
Training Epoch: 23 [21888/50048]	Loss: 1.0726
Training Epoch: 23 [22016/50048]	Loss: 1.1980
Training Epoch: 23 [22144/50048]	Loss: 0.9676
Training Epoch: 23 [22272/50048]	Loss: 1.3161
Training Epoch: 23 [22400/50048]	Loss: 1.1627
Training Epoch: 23 [22528/50048]	Loss: 1.1239
Training Epoch: 23 [22656/50048]	Loss: 1.1994
Training Epoch: 23 [22784/50048]	Loss: 1.1313
Training Epoch: 23 [22912/50048]	Loss: 1.3674
Training Epoch: 23 [23040/50048]	Loss: 1.1144
Training Epoch: 23 [23168/50048]	Loss: 1.1781
Training Epoch: 23 [23296/50048]	Loss: 0.9288
Training Epoch: 23 [23424/50048]	Loss: 1.0089
Training Epoch: 23 [23552/50048]	Loss: 1.1156
Training Epoch: 23 [23680/50048]	Loss: 1.1818
Training Epoch: 23 [23808/50048]	Loss: 1.0767
Training Epoch: 23 [23936/50048]	Loss: 1.1579
Training Epoch: 23 [24064/50048]	Loss: 1.1587
Training Epoch: 23 [24192/50048]	Loss: 0.9557
Training Epoch: 23 [24320/50048]	Loss: 1.0616
Training Epoch: 23 [24448/50048]	Loss: 1.0333
Training Epoch: 23 [24576/50048]	Loss: 1.1902
Training Epoch: 23 [24704/50048]	Loss: 1.2288
Training Epoch: 23 [24832/50048]	Loss: 1.0303
Training Epoch: 23 [24960/50048]	Loss: 1.2190
Training Epoch: 23 [25088/50048]	Loss: 1.1345
Training Epoch: 23 [25216/50048]	Loss: 1.0695
Training Epoch: 23 [25344/50048]	Loss: 1.0570
Training Epoch: 23 [25472/50048]	Loss: 1.1543
Training Epoch: 23 [25600/50048]	Loss: 1.0287
Training Epoch: 23 [25728/50048]	Loss: 1.1886
Training Epoch: 23 [25856/50048]	Loss: 1.1498
Training Epoch: 23 [25984/50048]	Loss: 1.2243
Training Epoch: 23 [26112/50048]	Loss: 1.2531
Training Epoch: 23 [26240/50048]	Loss: 1.3754
Training Epoch: 23 [26368/50048]	Loss: 1.3639
Training Epoch: 23 [26496/50048]	Loss: 1.0960
Training Epoch: 23 [26624/50048]	Loss: 1.3708
Training Epoch: 23 [26752/50048]	Loss: 0.9705
Training Epoch: 23 [26880/50048]	Loss: 1.0223
Training Epoch: 23 [27008/50048]	Loss: 0.9856
Training Epoch: 23 [27136/50048]	Loss: 1.2596
Training Epoch: 23 [27264/50048]	Loss: 1.1299
Training Epoch: 23 [27392/50048]	Loss: 1.3519
Training Epoch: 23 [27520/50048]	Loss: 1.0271
Training Epoch: 23 [27648/50048]	Loss: 1.1614
Training Epoch: 23 [27776/50048]	Loss: 1.3618
Training Epoch: 23 [27904/50048]	Loss: 1.1536
Training Epoch: 23 [28032/50048]	Loss: 0.8775
Training Epoch: 23 [28160/50048]	Loss: 0.9823
Training Epoch: 23 [28288/50048]	Loss: 1.4599
Training Epoch: 23 [28416/50048]	Loss: 1.0636
Training Epoch: 23 [28544/50048]	Loss: 1.0999
Training Epoch: 23 [28672/50048]	Loss: 1.1222
Training Epoch: 23 [28800/50048]	Loss: 1.1175
Training Epoch: 23 [28928/50048]	Loss: 0.9609
Training Epoch: 23 [29056/50048]	Loss: 1.1123
Training Epoch: 23 [29184/50048]	Loss: 1.0409
Training Epoch: 23 [29312/50048]	Loss: 1.0532
Training Epoch: 23 [29440/50048]	Loss: 1.2670
Training Epoch: 23 [29568/50048]	Loss: 1.1011
Training Epoch: 23 [29696/50048]	Loss: 1.0504
Training Epoch: 23 [29824/50048]	Loss: 1.3001
Training Epoch: 23 [29952/50048]	Loss: 1.2325
Training Epoch: 23 [30080/50048]	Loss: 1.3176
Training Epoch: 23 [30208/50048]	Loss: 1.0503
Training Epoch: 23 [30336/50048]	Loss: 1.2468
Training Epoch: 23 [30464/50048]	Loss: 1.0970
Training Epoch: 23 [30592/50048]	Loss: 1.1735
Training Epoch: 23 [30720/50048]	Loss: 1.2922
Training Epoch: 23 [30848/50048]	Loss: 1.2096
Training Epoch: 23 [30976/50048]	Loss: 1.1217
Training Epoch: 23 [31104/50048]	Loss: 1.0251
Training Epoch: 23 [31232/50048]	Loss: 1.1538
Training Epoch: 23 [31360/50048]	Loss: 0.8558
Training Epoch: 23 [31488/50048]	Loss: 1.1474
Training Epoch: 23 [31616/50048]	Loss: 1.1526
Training Epoch: 23 [31744/50048]	Loss: 1.0537
Training Epoch: 23 [31872/50048]	Loss: 1.3432
Training Epoch: 23 [32000/50048]	Loss: 1.1958
Training Epoch: 23 [32128/50048]	Loss: 1.0328
Training Epoch: 23 [32256/50048]	Loss: 1.0587
Training Epoch: 23 [32384/50048]	Loss: 1.1307
Training Epoch: 23 [32512/50048]	Loss: 1.2737
Training Epoch: 23 [32640/50048]	Loss: 1.2562
Training Epoch: 23 [32768/50048]	Loss: 0.9130
Training Epoch: 23 [32896/50048]	Loss: 1.2126
Training Epoch: 23 [33024/50048]	Loss: 1.1186
Training Epoch: 23 [33152/50048]	Loss: 1.0952
Training Epoch: 23 [33280/50048]	Loss: 1.1907
Training Epoch: 23 [33408/50048]	Loss: 1.1583
Training Epoch: 23 [33536/50048]	Loss: 1.1893
Training Epoch: 23 [33664/50048]	Loss: 1.3245
Training Epoch: 23 [33792/50048]	Loss: 1.0055
Training Epoch: 23 [33920/50048]	Loss: 1.1922
Training Epoch: 23 [34048/50048]	Loss: 0.9652
Training Epoch: 23 [34176/50048]	Loss: 1.1837
Training Epoch: 23 [34304/50048]	Loss: 1.0481
Training Epoch: 23 [34432/50048]	Loss: 0.9226
Training Epoch: 23 [34560/50048]	Loss: 1.1277
Training Epoch: 23 [34688/50048]	Loss: 1.0886
Training Epoch: 23 [34816/50048]	Loss: 1.2763
Training Epoch: 23 [34944/50048]	Loss: 1.0385
Training Epoch: 23 [35072/50048]	Loss: 1.2653
Training Epoch: 23 [35200/50048]	Loss: 1.1833
Training Epoch: 23 [35328/50048]	Loss: 1.0943
Training Epoch: 23 [35456/50048]	Loss: 1.2909
Training Epoch: 23 [35584/50048]	Loss: 1.1189
Training Epoch: 23 [35712/50048]	Loss: 1.3840
Training Epoch: 23 [35840/50048]	Loss: 1.1631
Training Epoch: 23 [35968/50048]	Loss: 1.1020
Training Epoch: 23 [36096/50048]	Loss: 1.1283
Training Epoch: 23 [36224/50048]	Loss: 1.3680
Training Epoch: 23 [36352/50048]	Loss: 1.1239
Training Epoch: 23 [36480/50048]	Loss: 1.1151
Training Epoch: 23 [36608/50048]	Loss: 1.1514
Training Epoch: 23 [36736/50048]	Loss: 0.8921
Training Epoch: 23 [36864/50048]	Loss: 1.1008
Training Epoch: 23 [36992/50048]	Loss: 0.9487
Training Epoch: 23 [37120/50048]	Loss: 1.0052
Training Epoch: 23 [37248/50048]	Loss: 1.0778
Training Epoch: 23 [37376/50048]	Loss: 1.2384
Training Epoch: 23 [37504/50048]	Loss: 1.1971
Training Epoch: 23 [37632/50048]	Loss: 1.2043
Training Epoch: 23 [37760/50048]	Loss: 1.1322
Training Epoch: 23 [37888/50048]	Loss: 1.4413
Training Epoch: 23 [38016/50048]	Loss: 1.3660
Training Epoch: 23 [38144/50048]	Loss: 1.1082
Training Epoch: 23 [38272/50048]	Loss: 0.8134
Training Epoch: 23 [38400/50048]	Loss: 1.1984
Training Epoch: 23 [38528/50048]	Loss: 1.1203
Training Epoch: 23 [38656/50048]	Loss: 0.9203
Training Epoch: 23 [38784/50048]	Loss: 1.1360
Training Epoch: 23 [38912/50048]	Loss: 1.0331
Training Epoch: 23 [39040/50048]	Loss: 1.1515
Training Epoch: 23 [39168/50048]	Loss: 1.2234
Training Epoch: 23 [39296/50048]	Loss: 0.9192
Training Epoch: 23 [39424/50048]	Loss: 1.2515
Training Epoch: 23 [39552/50048]	Loss: 0.9870
Training Epoch: 23 [39680/50048]	Loss: 1.1712
Training Epoch: 23 [39808/50048]	Loss: 1.1104
Training Epoch: 23 [39936/50048]	Loss: 1.2261
Training Epoch: 23 [40064/50048]	Loss: 0.9938
Training Epoch: 23 [40192/50048]	Loss: 1.3272
Training Epoch: 23 [40320/50048]	Loss: 0.9603
Training Epoch: 23 [40448/50048]	Loss: 1.2841
Training Epoch: 23 [40576/50048]	Loss: 1.0781
Training Epoch: 23 [40704/50048]	Loss: 1.0238
Training Epoch: 23 [40832/50048]	Loss: 0.9896
Training Epoch: 23 [40960/50048]	Loss: 0.8883
Training Epoch: 23 [41088/50048]	Loss: 1.0109
Training Epoch: 23 [41216/50048]	Loss: 1.0798
Training Epoch: 23 [41344/50048]	Loss: 1.1423
Training Epoch: 23 [41472/50048]	Loss: 1.0752
Training Epoch: 23 [41600/50048]	Loss: 1.0672
Training Epoch: 23 [41728/50048]	Loss: 1.1532
Training Epoch: 23 [41856/50048]	Loss: 1.1421
Training Epoch: 23 [41984/50048]	Loss: 1.0881
Training Epoch: 23 [42112/50048]	Loss: 1.2110
Training Epoch: 23 [42240/50048]	Loss: 0.9913
Training Epoch: 23 [42368/50048]	Loss: 1.1367
Training Epoch: 23 [42496/50048]	Loss: 1.2025
Training Epoch: 23 [42624/50048]	Loss: 1.3900
Training Epoch: 23 [42752/50048]	Loss: 1.0692
Training Epoch: 23 [42880/50048]	Loss: 0.9504
Training Epoch: 23 [43008/50048]	Loss: 1.1027
Training Epoch: 23 [43136/50048]	Loss: 1.2812
Training Epoch: 23 [43264/50048]	Loss: 1.0671
Training Epoch: 23 [43392/50048]	Loss: 1.1593
Training Epoch: 23 [43520/50048]	Loss: 1.1071
Training Epoch: 23 [43648/50048]	Loss: 1.0352
Training Epoch: 23 [43776/50048]	Loss: 1.0812
Training Epoch: 23 [43904/50048]	Loss: 1.1414
Training Epoch: 23 [44032/50048]	Loss: 1.0637
Training Epoch: 23 [44160/50048]	Loss: 1.1070
Training Epoch: 23 [44288/50048]	Loss: 1.0657
Training Epoch: 23 [44416/50048]	Loss: 1.2655
Training Epoch: 23 [44544/50048]	Loss: 0.9765
Training Epoch: 23 [44672/50048]	Loss: 1.2729
Training Epoch: 23 [44800/50048]	Loss: 1.2734
Training Epoch: 23 [44928/50048]	Loss: 1.1734
Training Epoch: 23 [45056/50048]	Loss: 1.1332
Training Epoch: 23 [45184/50048]	Loss: 1.0887
Training Epoch: 23 [45312/50048]	Loss: 1.1741
Training Epoch: 23 [45440/50048]	Loss: 1.3783
Training Epoch: 23 [45568/50048]	Loss: 0.8836
Training Epoch: 23 [45696/50048]	Loss: 1.1961
Training Epoch: 23 [45824/50048]	Loss: 1.0886
Training Epoch: 23 [45952/50048]	Loss: 0.9939
Training Epoch: 23 [46080/50048]	Loss: 0.9618
Training Epoch: 23 [46208/50048]	Loss: 1.2241
Training Epoch: 23 [46336/50048]	Loss: 1.1056
Training Epoch: 23 [46464/50048]	Loss: 0.9254
Training Epoch: 23 [46592/50048]	Loss: 0.9381
Training Epoch: 23 [46720/50048]	Loss: 1.1939
Training Epoch: 23 [46848/50048]	Loss: 0.9777
Training Epoch: 23 [46976/50048]	Loss: 0.9161
Training Epoch: 23 [47104/50048]	Loss: 1.2319
Training Epoch: 23 [47232/50048]	Loss: 1.2231
Training Epoch: 23 [47360/50048]	Loss: 1.1830
Training Epoch: 23 [47488/50048]	Loss: 1.0623
Training Epoch: 23 [47616/50048]	Loss: 1.1583
Training Epoch: 23 [47744/50048]	Loss: 1.2358
Training Epoch: 23 [47872/50048]	Loss: 1.0816
Training Epoch: 23 [48000/50048]	Loss: 1.5788
Training Epoch: 23 [48128/50048]	Loss: 1.0853
Training Epoch: 23 [48256/50048]	Loss: 1.5395
Training Epoch: 23 [48384/50048]	Loss: 1.1350
Training Epoch: 23 [48512/50048]	Loss: 1.2405
Training Epoch: 23 [48640/50048]	Loss: 1.1982
Training Epoch: 23 [48768/50048]	Loss: 1.0703
Training Epoch: 23 [48896/50048]	Loss: 1.1833
Training Epoch: 23 [49024/50048]	Loss: 0.9167
Training Epoch: 23 [49152/50048]	Loss: 1.3919
Training Epoch: 23 [49280/50048]	Loss: 1.4129
Training Epoch: 23 [49408/50048]	Loss: 1.3829
Training Epoch: 23 [49536/50048]	Loss: 1.2389
Training Epoch: 23 [49664/50048]	Loss: 0.8352
Training Epoch: 23 [49792/50048]	Loss: 0.9226
Training Epoch: 23 [49920/50048]	Loss: 0.9902
Training Epoch: 23 [50048/50048]	Loss: 1.1135
Validation Epoch: 23, Average loss: 0.0114, Accuracy: 0.5997
Training Epoch: 24 [128/50048]	Loss: 1.0350
Training Epoch: 24 [256/50048]	Loss: 1.2596
Training Epoch: 24 [384/50048]	Loss: 1.1331
Training Epoch: 24 [512/50048]	Loss: 1.0409
Training Epoch: 24 [640/50048]	Loss: 1.4193
Training Epoch: 24 [768/50048]	Loss: 1.2411
Training Epoch: 24 [896/50048]	Loss: 1.1562
Training Epoch: 24 [1024/50048]	Loss: 1.0581
Training Epoch: 24 [1152/50048]	Loss: 0.9684
Training Epoch: 24 [1280/50048]	Loss: 1.0563
Training Epoch: 24 [1408/50048]	Loss: 0.9411
Training Epoch: 24 [1536/50048]	Loss: 1.0884
Training Epoch: 24 [1664/50048]	Loss: 0.9992
Training Epoch: 24 [1792/50048]	Loss: 1.0335
Training Epoch: 24 [1920/50048]	Loss: 0.8846
Training Epoch: 24 [2048/50048]	Loss: 0.9485
Training Epoch: 24 [2176/50048]	Loss: 1.2294
Training Epoch: 24 [2304/50048]	Loss: 1.0423
Training Epoch: 24 [2432/50048]	Loss: 1.3301
Training Epoch: 24 [2560/50048]	Loss: 1.3409
Training Epoch: 24 [2688/50048]	Loss: 1.2157
Training Epoch: 24 [2816/50048]	Loss: 0.9411
Training Epoch: 24 [2944/50048]	Loss: 1.1699
Training Epoch: 24 [3072/50048]	Loss: 1.0328
Training Epoch: 24 [3200/50048]	Loss: 1.0748
Training Epoch: 24 [3328/50048]	Loss: 1.0925
Training Epoch: 24 [3456/50048]	Loss: 0.9618
Training Epoch: 24 [3584/50048]	Loss: 1.1342
Training Epoch: 24 [3712/50048]	Loss: 1.1995
Training Epoch: 24 [3840/50048]	Loss: 1.0720
Training Epoch: 24 [3968/50048]	Loss: 1.3051
Training Epoch: 24 [4096/50048]	Loss: 0.8754
Training Epoch: 24 [4224/50048]	Loss: 1.1961
Training Epoch: 24 [4352/50048]	Loss: 1.2166
Training Epoch: 24 [4480/50048]	Loss: 1.0343
Training Epoch: 24 [4608/50048]	Loss: 1.2037
Training Epoch: 24 [4736/50048]	Loss: 0.8197
Training Epoch: 24 [4864/50048]	Loss: 1.0411
Training Epoch: 24 [4992/50048]	Loss: 1.1484
Training Epoch: 24 [5120/50048]	Loss: 0.8732
Training Epoch: 24 [5248/50048]	Loss: 0.9998
Training Epoch: 24 [5376/50048]	Loss: 1.0751
Training Epoch: 24 [5504/50048]	Loss: 1.1317
Training Epoch: 24 [5632/50048]	Loss: 1.0632
Training Epoch: 24 [5760/50048]	Loss: 1.0544
Training Epoch: 24 [5888/50048]	Loss: 1.0867
Training Epoch: 24 [6016/50048]	Loss: 1.0864
Training Epoch: 24 [6144/50048]	Loss: 1.1915
Training Epoch: 24 [6272/50048]	Loss: 0.8959
Training Epoch: 24 [6400/50048]	Loss: 1.0052
Training Epoch: 24 [6528/50048]	Loss: 1.0665
Training Epoch: 24 [6656/50048]	Loss: 0.8900
Training Epoch: 24 [6784/50048]	Loss: 1.0520
Training Epoch: 24 [6912/50048]	Loss: 0.9639
Training Epoch: 24 [7040/50048]	Loss: 1.1434
Training Epoch: 24 [7168/50048]	Loss: 1.2045
Training Epoch: 24 [7296/50048]	Loss: 1.0095
Training Epoch: 24 [7424/50048]	Loss: 1.2088
Training Epoch: 24 [7552/50048]	Loss: 0.9988
Training Epoch: 24 [7680/50048]	Loss: 0.9347
Training Epoch: 24 [7808/50048]	Loss: 1.0584
Training Epoch: 24 [7936/50048]	Loss: 1.0522
Training Epoch: 24 [8064/50048]	Loss: 1.1351
Training Epoch: 24 [8192/50048]	Loss: 0.9843
Training Epoch: 24 [8320/50048]	Loss: 1.2640
Training Epoch: 24 [8448/50048]	Loss: 1.2008
Training Epoch: 24 [8576/50048]	Loss: 1.0701
Training Epoch: 24 [8704/50048]	Loss: 1.1591
Training Epoch: 24 [8832/50048]	Loss: 0.8717
Training Epoch: 24 [8960/50048]	Loss: 1.2538
Training Epoch: 24 [9088/50048]	Loss: 1.1219
Training Epoch: 24 [9216/50048]	Loss: 1.2615
Training Epoch: 24 [9344/50048]	Loss: 1.1915
Training Epoch: 24 [9472/50048]	Loss: 0.9619
Training Epoch: 24 [9600/50048]	Loss: 1.1122
Training Epoch: 24 [9728/50048]	Loss: 1.2602
Training Epoch: 24 [9856/50048]	Loss: 0.8994
Training Epoch: 24 [9984/50048]	Loss: 1.0584
Training Epoch: 24 [10112/50048]	Loss: 1.0112
Training Epoch: 24 [10240/50048]	Loss: 1.0581
Training Epoch: 24 [10368/50048]	Loss: 1.3227
Training Epoch: 24 [10496/50048]	Loss: 1.2122
Training Epoch: 24 [10624/50048]	Loss: 0.9540
Training Epoch: 24 [10752/50048]	Loss: 1.0814
Training Epoch: 24 [10880/50048]	Loss: 1.0474
Training Epoch: 24 [11008/50048]	Loss: 0.8321
Training Epoch: 24 [11136/50048]	Loss: 1.0146
Training Epoch: 24 [11264/50048]	Loss: 1.2928
Training Epoch: 24 [11392/50048]	Loss: 1.0792
Training Epoch: 24 [11520/50048]	Loss: 1.0109
Training Epoch: 24 [11648/50048]	Loss: 1.0106
Training Epoch: 24 [11776/50048]	Loss: 1.0818
Training Epoch: 24 [11904/50048]	Loss: 0.9804
Training Epoch: 24 [12032/50048]	Loss: 1.0998
Training Epoch: 24 [12160/50048]	Loss: 1.1004
Training Epoch: 24 [12288/50048]	Loss: 0.9452
Training Epoch: 24 [12416/50048]	Loss: 1.1853
Training Epoch: 24 [12544/50048]	Loss: 0.9878
Training Epoch: 24 [12672/50048]	Loss: 0.9438
Training Epoch: 24 [12800/50048]	Loss: 1.0309
Training Epoch: 24 [12928/50048]	Loss: 1.1218
Training Epoch: 24 [13056/50048]	Loss: 1.0609
Training Epoch: 24 [13184/50048]	Loss: 1.0584
Training Epoch: 24 [13312/50048]	Loss: 1.0337
Training Epoch: 24 [13440/50048]	Loss: 1.1129
Training Epoch: 24 [13568/50048]	Loss: 1.2167
Training Epoch: 24 [13696/50048]	Loss: 1.2582
Training Epoch: 24 [13824/50048]	Loss: 1.1869
Training Epoch: 24 [13952/50048]	Loss: 1.3168
Training Epoch: 24 [14080/50048]	Loss: 0.8333
Training Epoch: 24 [14208/50048]	Loss: 0.9131
Training Epoch: 24 [14336/50048]	Loss: 1.2507
Training Epoch: 24 [14464/50048]	Loss: 0.9506
Training Epoch: 24 [14592/50048]	Loss: 1.0933
Training Epoch: 24 [14720/50048]	Loss: 1.1009
Training Epoch: 24 [14848/50048]	Loss: 0.9730
Training Epoch: 24 [14976/50048]	Loss: 1.1392
Training Epoch: 24 [15104/50048]	Loss: 1.0279
Training Epoch: 24 [15232/50048]	Loss: 1.2703
Training Epoch: 24 [15360/50048]	Loss: 1.0064
Training Epoch: 24 [15488/50048]	Loss: 0.9854
Training Epoch: 24 [15616/50048]	Loss: 1.2070
Training Epoch: 24 [15744/50048]	Loss: 1.0983
Training Epoch: 24 [15872/50048]	Loss: 1.1853
Training Epoch: 24 [16000/50048]	Loss: 1.0008
Training Epoch: 24 [16128/50048]	Loss: 1.1177
Training Epoch: 24 [16256/50048]	Loss: 1.2218
Training Epoch: 24 [16384/50048]	Loss: 1.4306
Training Epoch: 24 [16512/50048]	Loss: 1.1048
Training Epoch: 24 [16640/50048]	Loss: 1.1084
Training Epoch: 24 [16768/50048]	Loss: 1.1076
Training Epoch: 24 [16896/50048]	Loss: 1.0514
Training Epoch: 24 [17024/50048]	Loss: 1.0313
Training Epoch: 24 [17152/50048]	Loss: 1.0043
Training Epoch: 24 [17280/50048]	Loss: 1.2778
Training Epoch: 24 [17408/50048]	Loss: 1.0823
Training Epoch: 24 [17536/50048]	Loss: 1.0201
Training Epoch: 24 [17664/50048]	Loss: 1.0414
Training Epoch: 24 [17792/50048]	Loss: 1.0404
Training Epoch: 24 [17920/50048]	Loss: 1.1259
Training Epoch: 24 [18048/50048]	Loss: 1.0235
Training Epoch: 24 [18176/50048]	Loss: 1.2076
Training Epoch: 24 [18304/50048]	Loss: 1.2079
Training Epoch: 24 [18432/50048]	Loss: 1.1982
Training Epoch: 24 [18560/50048]	Loss: 1.0423
Training Epoch: 24 [18688/50048]	Loss: 1.0259
Training Epoch: 24 [18816/50048]	Loss: 0.9993
Training Epoch: 24 [18944/50048]	Loss: 0.9429
Training Epoch: 24 [19072/50048]	Loss: 1.0509
Training Epoch: 24 [19200/50048]	Loss: 1.1801
Training Epoch: 24 [19328/50048]	Loss: 1.1774
Training Epoch: 24 [19456/50048]	Loss: 1.1605
Training Epoch: 24 [19584/50048]	Loss: 1.0558
Training Epoch: 24 [19712/50048]	Loss: 1.0385
Training Epoch: 24 [19840/50048]	Loss: 1.0900
Training Epoch: 24 [19968/50048]	Loss: 1.2278
Training Epoch: 24 [20096/50048]	Loss: 1.1124
Training Epoch: 24 [20224/50048]	Loss: 1.1939
Training Epoch: 24 [20352/50048]	Loss: 1.0506
Training Epoch: 24 [20480/50048]	Loss: 1.0037
Training Epoch: 24 [20608/50048]	Loss: 0.9362
Training Epoch: 24 [20736/50048]	Loss: 1.2182
Training Epoch: 24 [20864/50048]	Loss: 1.2538
Training Epoch: 24 [20992/50048]	Loss: 1.1477
Training Epoch: 24 [21120/50048]	Loss: 1.1163
Training Epoch: 24 [21248/50048]	Loss: 0.9804
Training Epoch: 24 [21376/50048]	Loss: 1.1981
Training Epoch: 24 [21504/50048]	Loss: 1.0596
Training Epoch: 24 [21632/50048]	Loss: 1.3373
Training Epoch: 24 [21760/50048]	Loss: 1.0907
Training Epoch: 24 [21888/50048]	Loss: 1.0605
Training Epoch: 24 [22016/50048]	Loss: 1.1720
Training Epoch: 24 [22144/50048]	Loss: 1.0506
Training Epoch: 24 [22272/50048]	Loss: 1.0225
Training Epoch: 24 [22400/50048]	Loss: 1.2393
Training Epoch: 24 [22528/50048]	Loss: 1.1516
Training Epoch: 24 [22656/50048]	Loss: 1.0293
Training Epoch: 24 [22784/50048]	Loss: 1.6397
Training Epoch: 24 [22912/50048]	Loss: 1.1969
Training Epoch: 24 [23040/50048]	Loss: 1.0107
Training Epoch: 24 [23168/50048]	Loss: 0.8974
Training Epoch: 24 [23296/50048]	Loss: 1.1082
Training Epoch: 24 [23424/50048]	Loss: 1.2280
Training Epoch: 24 [23552/50048]	Loss: 1.1089
Training Epoch: 24 [23680/50048]	Loss: 1.2769
Training Epoch: 24 [23808/50048]	Loss: 1.0483
Training Epoch: 24 [23936/50048]	Loss: 1.1591
Training Epoch: 24 [24064/50048]	Loss: 1.2866
Training Epoch: 24 [24192/50048]	Loss: 1.1162
Training Epoch: 24 [24320/50048]	Loss: 1.1917
Training Epoch: 24 [24448/50048]	Loss: 1.1300
Training Epoch: 24 [24576/50048]	Loss: 0.9666
Training Epoch: 24 [24704/50048]	Loss: 0.9170
Training Epoch: 24 [24832/50048]	Loss: 1.1004
Training Epoch: 24 [24960/50048]	Loss: 1.0866
Training Epoch: 24 [25088/50048]	Loss: 1.0365
Training Epoch: 24 [25216/50048]	Loss: 0.9053
Training Epoch: 24 [25344/50048]	Loss: 0.9588
Training Epoch: 24 [25472/50048]	Loss: 0.9738
Training Epoch: 24 [25600/50048]	Loss: 1.1484
Training Epoch: 24 [25728/50048]	Loss: 1.1237
Training Epoch: 24 [25856/50048]	Loss: 1.3090
Training Epoch: 24 [25984/50048]	Loss: 1.0959
Training Epoch: 24 [26112/50048]	Loss: 0.9089
Training Epoch: 24 [26240/50048]	Loss: 1.0631
Training Epoch: 24 [26368/50048]	Loss: 1.1056
Training Epoch: 24 [26496/50048]	Loss: 1.2347
Training Epoch: 24 [26624/50048]	Loss: 1.1728
Training Epoch: 24 [26752/50048]	Loss: 1.1264
Training Epoch: 24 [26880/50048]	Loss: 1.1576
Training Epoch: 24 [27008/50048]	Loss: 1.0578
Training Epoch: 24 [27136/50048]	Loss: 1.1185
Training Epoch: 24 [27264/50048]	Loss: 1.1937
Training Epoch: 24 [27392/50048]	Loss: 1.2305
Training Epoch: 24 [27520/50048]	Loss: 1.1183
Training Epoch: 24 [27648/50048]	Loss: 1.2488
Training Epoch: 24 [27776/50048]	Loss: 1.2019
Training Epoch: 24 [27904/50048]	Loss: 1.0766
Training Epoch: 24 [28032/50048]	Loss: 1.0641
Training Epoch: 24 [28160/50048]	Loss: 1.1351
Training Epoch: 24 [28288/50048]	Loss: 1.1688
Training Epoch: 24 [28416/50048]	Loss: 1.1465
Training Epoch: 24 [28544/50048]	Loss: 1.1602
Training Epoch: 24 [28672/50048]	Loss: 1.2295
Training Epoch: 24 [28800/50048]	Loss: 1.0580
Training Epoch: 24 [28928/50048]	Loss: 1.3867
Training Epoch: 24 [29056/50048]	Loss: 1.1819
Training Epoch: 24 [29184/50048]	Loss: 1.2313
Training Epoch: 24 [29312/50048]	Loss: 0.9966
Training Epoch: 24 [29440/50048]	Loss: 1.0648
Training Epoch: 24 [29568/50048]	Loss: 1.2704
Training Epoch: 24 [29696/50048]	Loss: 1.4444
Training Epoch: 24 [29824/50048]	Loss: 1.1376
Training Epoch: 24 [29952/50048]	Loss: 1.0321
Training Epoch: 24 [30080/50048]	Loss: 1.1186
Training Epoch: 24 [30208/50048]	Loss: 0.9942
Training Epoch: 24 [30336/50048]	Loss: 1.0344
Training Epoch: 24 [30464/50048]	Loss: 1.0574
Training Epoch: 24 [30592/50048]	Loss: 1.1144
Training Epoch: 24 [30720/50048]	Loss: 1.0398
Training Epoch: 24 [30848/50048]	Loss: 1.0643
Training Epoch: 24 [30976/50048]	Loss: 1.3131
Training Epoch: 24 [31104/50048]	Loss: 1.1477
Training Epoch: 24 [31232/50048]	Loss: 1.4726
Training Epoch: 24 [31360/50048]	Loss: 1.0760
Training Epoch: 24 [31488/50048]	Loss: 1.0447
Training Epoch: 24 [31616/50048]	Loss: 1.0113
Training Epoch: 24 [31744/50048]	Loss: 1.2802
Training Epoch: 24 [31872/50048]	Loss: 0.9375
Training Epoch: 24 [32000/50048]	Loss: 0.8593
Training Epoch: 24 [32128/50048]	Loss: 0.9243
Training Epoch: 24 [32256/50048]	Loss: 1.0977
Training Epoch: 24 [32384/50048]	Loss: 0.9910
Training Epoch: 24 [32512/50048]	Loss: 1.0969
Training Epoch: 24 [32640/50048]	Loss: 0.9485
Training Epoch: 24 [32768/50048]	Loss: 1.0342
Training Epoch: 24 [32896/50048]	Loss: 1.0115
Training Epoch: 24 [33024/50048]	Loss: 1.1582
Training Epoch: 24 [33152/50048]	Loss: 1.1610
Training Epoch: 24 [33280/50048]	Loss: 1.0467
Training Epoch: 24 [33408/50048]	Loss: 1.0575
Training Epoch: 24 [33536/50048]	Loss: 1.0952
Training Epoch: 24 [33664/50048]	Loss: 1.2908
Training Epoch: 24 [33792/50048]	Loss: 1.1647
Training Epoch: 24 [33920/50048]	Loss: 1.0909
Training Epoch: 24 [34048/50048]	Loss: 1.0889
Training Epoch: 24 [34176/50048]	Loss: 1.3547
Training Epoch: 24 [34304/50048]	Loss: 0.9032
Training Epoch: 24 [34432/50048]	Loss: 1.1163
Training Epoch: 24 [34560/50048]	Loss: 1.2697
Training Epoch: 24 [34688/50048]	Loss: 1.1402
Training Epoch: 24 [34816/50048]	Loss: 1.2246
Training Epoch: 24 [34944/50048]	Loss: 1.0030
Training Epoch: 24 [35072/50048]	Loss: 0.9772
Training Epoch: 24 [35200/50048]	Loss: 1.0803
Training Epoch: 24 [35328/50048]	Loss: 1.2374
Training Epoch: 24 [35456/50048]	Loss: 1.0994
Training Epoch: 24 [35584/50048]	Loss: 0.9672
Training Epoch: 24 [35712/50048]	Loss: 1.2993
Training Epoch: 24 [35840/50048]	Loss: 1.0095
Training Epoch: 24 [35968/50048]	Loss: 1.4370
Training Epoch: 24 [36096/50048]	Loss: 1.1835
Training Epoch: 24 [36224/50048]	Loss: 1.2091
Training Epoch: 24 [36352/50048]	Loss: 1.2319
Training Epoch: 24 [36480/50048]	Loss: 1.3545
Training Epoch: 24 [36608/50048]	Loss: 1.0875
Training Epoch: 24 [36736/50048]	Loss: 0.8428
Training Epoch: 24 [36864/50048]	Loss: 1.0190
Training Epoch: 24 [36992/50048]	Loss: 1.0576
Training Epoch: 24 [37120/50048]	Loss: 1.3360
Training Epoch: 24 [37248/50048]	Loss: 1.1619
Training Epoch: 24 [37376/50048]	Loss: 1.2515
Training Epoch: 24 [37504/50048]	Loss: 1.0739
Training Epoch: 24 [37632/50048]	Loss: 1.0083
Training Epoch: 24 [37760/50048]	Loss: 1.2220
Training Epoch: 24 [37888/50048]	Loss: 0.8083
Training Epoch: 24 [38016/50048]	Loss: 1.1145
Training Epoch: 24 [38144/50048]	Loss: 1.0896
Training Epoch: 24 [38272/50048]	Loss: 1.0217
Training Epoch: 24 [38400/50048]	Loss: 1.2597
Training Epoch: 24 [38528/50048]	Loss: 1.0976
Training Epoch: 24 [38656/50048]	Loss: 0.8009
Training Epoch: 24 [38784/50048]	Loss: 1.3909
Training Epoch: 24 [38912/50048]	Loss: 1.1039
Training Epoch: 24 [39040/50048]	Loss: 1.1658
Training Epoch: 24 [39168/50048]	Loss: 1.2397
Training Epoch: 24 [39296/50048]	Loss: 0.9282
Training Epoch: 24 [39424/50048]	Loss: 1.2779
Training Epoch: 24 [39552/50048]	Loss: 0.9408
Training Epoch: 24 [39680/50048]	Loss: 1.0754
Training Epoch: 24 [39808/50048]	Loss: 1.0716
Training Epoch: 24 [39936/50048]	Loss: 0.9718
Training Epoch: 24 [40064/50048]	Loss: 1.0722
Training Epoch: 24 [40192/50048]	Loss: 1.1846
Training Epoch: 24 [40320/50048]	Loss: 1.0110
Training Epoch: 24 [40448/50048]	Loss: 1.1103
Training Epoch: 24 [40576/50048]	Loss: 1.1486
Training Epoch: 24 [40704/50048]	Loss: 1.1353
Training Epoch: 24 [40832/50048]	Loss: 1.1809
Training Epoch: 24 [40960/50048]	Loss: 1.1572
Training Epoch: 24 [41088/50048]	Loss: 1.2780
Training Epoch: 24 [41216/50048]	Loss: 1.0193
Training Epoch: 24 [41344/50048]	Loss: 1.1078
Training Epoch: 24 [41472/50048]	Loss: 1.0602
Training Epoch: 24 [41600/50048]	Loss: 1.1979
Training Epoch: 24 [41728/50048]	Loss: 1.3718
Training Epoch: 24 [41856/50048]	Loss: 1.0928
Training Epoch: 24 [41984/50048]	Loss: 0.8283
Training Epoch: 24 [42112/50048]	Loss: 1.0458
Training Epoch: 24 [42240/50048]	Loss: 1.2512
Training Epoch: 24 [42368/50048]	Loss: 1.3410
Training Epoch: 24 [42496/50048]	Loss: 1.0124
Training Epoch: 24 [42624/50048]	Loss: 1.1039
Training Epoch: 24 [42752/50048]	Loss: 1.0179
Training Epoch: 24 [42880/50048]	Loss: 1.1112
Training Epoch: 24 [43008/50048]	Loss: 1.0528
Training Epoch: 24 [43136/50048]	Loss: 1.1889
Training Epoch: 24 [43264/50048]	Loss: 1.2959
Training Epoch: 24 [43392/50048]	Loss: 1.1777
Training Epoch: 24 [43520/50048]	Loss: 1.0541
Training Epoch: 24 [43648/50048]	Loss: 1.2682
Training Epoch: 24 [43776/50048]	Loss: 1.2289
Training Epoch: 24 [43904/50048]	Loss: 1.2202
Training Epoch: 24 [44032/50048]	Loss: 0.9744
Training Epoch: 24 [44160/50048]	Loss: 0.8885
Training Epoch: 24 [44288/50048]	Loss: 1.1998
Training Epoch: 24 [44416/50048]	Loss: 0.9545
Training Epoch: 24 [44544/50048]	Loss: 1.2703
Training Epoch: 24 [44672/50048]	Loss: 1.0312
Training Epoch: 24 [44800/50048]	Loss: 1.2272
Training Epoch: 24 [44928/50048]	Loss: 1.0620
Training Epoch: 24 [45056/50048]	Loss: 1.2443
Training Epoch: 24 [45184/50048]	Loss: 1.0513
Training Epoch: 24 [45312/50048]	Loss: 1.1010
Training Epoch: 24 [45440/50048]	Loss: 1.3014
Training Epoch: 24 [45568/50048]	Loss: 1.1537
Training Epoch: 24 [45696/50048]	Loss: 1.2755
Training Epoch: 24 [45824/50048]	Loss: 1.1510
Training Epoch: 24 [45952/50048]	Loss: 1.1583
Training Epoch: 24 [46080/50048]	Loss: 1.1039
Training Epoch: 24 [46208/50048]	Loss: 1.0239
Training Epoch: 24 [46336/50048]	Loss: 1.2218
Training Epoch: 24 [46464/50048]	Loss: 0.9873
Training Epoch: 24 [46592/50048]	Loss: 1.1205
Training Epoch: 24 [46720/50048]	Loss: 1.1509
Training Epoch: 24 [46848/50048]	Loss: 1.1095
Training Epoch: 24 [46976/50048]	Loss: 0.9665
Training Epoch: 24 [47104/50048]	Loss: 0.9841
Training Epoch: 24 [47232/50048]	Loss: 1.1842
Training Epoch: 24 [47360/50048]	Loss: 0.9878
Training Epoch: 24 [47488/50048]	Loss: 1.5639
Training Epoch: 24 [47616/50048]	Loss: 1.1593
Training Epoch: 24 [47744/50048]	Loss: 1.1743
Training Epoch: 24 [47872/50048]	Loss: 1.0927
Training Epoch: 24 [48000/50048]	Loss: 1.3215
Training Epoch: 24 [48128/50048]	Loss: 1.0951
Training Epoch: 24 [48256/50048]	Loss: 1.4531
Training Epoch: 24 [48384/50048]	Loss: 1.1213
Training Epoch: 24 [48512/50048]	Loss: 1.1740
Training Epoch: 24 [48640/50048]	Loss: 1.0643
Training Epoch: 24 [48768/50048]	Loss: 1.2097
Training Epoch: 24 [48896/50048]	Loss: 1.1062
Training Epoch: 24 [49024/50048]	Loss: 1.2440
Training Epoch: 24 [49152/50048]	Loss: 1.1910
Training Epoch: 24 [49280/50048]	Loss: 1.0813
Training Epoch: 24 [49408/50048]	Loss: 1.0649
Training Epoch: 24 [49536/50048]	Loss: 1.0873
Training Epoch: 24 [49664/50048]	Loss: 1.0104
Training Epoch: 24 [49792/50048]	Loss: 1.0746
Training Epoch: 24 [49920/50048]	Loss: 1.2455
Training Epoch: 24 [50048/50048]	Loss: 1.0992
Validation Epoch: 24, Average loss: 0.0115, Accuracy: 0.5980
Training Epoch: 25 [128/50048]	Loss: 0.9008
Training Epoch: 25 [256/50048]	Loss: 1.1993
Training Epoch: 25 [384/50048]	Loss: 1.1891
Training Epoch: 25 [512/50048]	Loss: 1.1908
Training Epoch: 25 [640/50048]	Loss: 1.0975
Training Epoch: 25 [768/50048]	Loss: 0.7460
Training Epoch: 25 [896/50048]	Loss: 1.1149
Training Epoch: 25 [1024/50048]	Loss: 1.1722
Training Epoch: 25 [1152/50048]	Loss: 0.9522
Training Epoch: 25 [1280/50048]	Loss: 0.9485
Training Epoch: 25 [1408/50048]	Loss: 1.0701
Training Epoch: 25 [1536/50048]	Loss: 0.9333
Training Epoch: 25 [1664/50048]	Loss: 1.1566
Training Epoch: 25 [1792/50048]	Loss: 1.0434
Training Epoch: 25 [1920/50048]	Loss: 1.0061
Training Epoch: 25 [2048/50048]	Loss: 0.9892
Training Epoch: 25 [2176/50048]	Loss: 1.0704
Training Epoch: 25 [2304/50048]	Loss: 1.1243
Training Epoch: 25 [2432/50048]	Loss: 0.9399
Training Epoch: 25 [2560/50048]	Loss: 0.9196
Training Epoch: 25 [2688/50048]	Loss: 1.2333
Training Epoch: 25 [2816/50048]	Loss: 1.0968
Training Epoch: 25 [2944/50048]	Loss: 0.9755
Training Epoch: 25 [3072/50048]	Loss: 1.1246
Training Epoch: 25 [3200/50048]	Loss: 1.0255
Training Epoch: 25 [3328/50048]	Loss: 1.0396
Training Epoch: 25 [3456/50048]	Loss: 1.1439
Training Epoch: 25 [3584/50048]	Loss: 1.2083
Training Epoch: 25 [3712/50048]	Loss: 0.9434
Training Epoch: 25 [3840/50048]	Loss: 1.2169
Training Epoch: 25 [3968/50048]	Loss: 1.0428
Training Epoch: 25 [4096/50048]	Loss: 1.1671
Training Epoch: 25 [4224/50048]	Loss: 1.1087
Training Epoch: 25 [4352/50048]	Loss: 1.1662
Training Epoch: 25 [4480/50048]	Loss: 1.0108
Training Epoch: 25 [4608/50048]	Loss: 1.0542
Training Epoch: 25 [4736/50048]	Loss: 0.9707
Training Epoch: 25 [4864/50048]	Loss: 1.0241
Training Epoch: 25 [4992/50048]	Loss: 1.0271
Training Epoch: 25 [5120/50048]	Loss: 1.1551
Training Epoch: 25 [5248/50048]	Loss: 0.9554
Training Epoch: 25 [5376/50048]	Loss: 1.0372
Training Epoch: 25 [5504/50048]	Loss: 1.2101
Training Epoch: 25 [5632/50048]	Loss: 0.9975
Training Epoch: 25 [5760/50048]	Loss: 0.9845
Training Epoch: 25 [5888/50048]	Loss: 1.3210
Training Epoch: 25 [6016/50048]	Loss: 1.2422
Training Epoch: 25 [6144/50048]	Loss: 1.1268
Training Epoch: 25 [6272/50048]	Loss: 1.0895
Training Epoch: 25 [6400/50048]	Loss: 1.0236
Training Epoch: 25 [6528/50048]	Loss: 1.1342
Training Epoch: 25 [6656/50048]	Loss: 1.2432
Training Epoch: 25 [6784/50048]	Loss: 1.2152
Training Epoch: 25 [6912/50048]	Loss: 1.1569
Training Epoch: 25 [7040/50048]	Loss: 1.0181
Training Epoch: 25 [7168/50048]	Loss: 1.0538
Training Epoch: 25 [7296/50048]	Loss: 1.2572
Training Epoch: 25 [7424/50048]	Loss: 1.0966
Training Epoch: 25 [7552/50048]	Loss: 1.2464
Training Epoch: 25 [7680/50048]	Loss: 0.9157
Training Epoch: 25 [7808/50048]	Loss: 1.0059
Training Epoch: 25 [7936/50048]	Loss: 1.2582
Training Epoch: 25 [8064/50048]	Loss: 0.9605
Training Epoch: 25 [8192/50048]	Loss: 1.0432
Training Epoch: 25 [8320/50048]	Loss: 1.2572
Training Epoch: 25 [8448/50048]	Loss: 1.1897
Training Epoch: 25 [8576/50048]	Loss: 0.9225
Training Epoch: 25 [8704/50048]	Loss: 1.5795
Training Epoch: 25 [8832/50048]	Loss: 1.0700
Training Epoch: 25 [8960/50048]	Loss: 1.3686
Training Epoch: 25 [9088/50048]	Loss: 0.8929
Training Epoch: 25 [9216/50048]	Loss: 0.9848
Training Epoch: 25 [9344/50048]	Loss: 1.0936
Training Epoch: 25 [9472/50048]	Loss: 0.8968
Training Epoch: 25 [9600/50048]	Loss: 1.1222
Training Epoch: 25 [9728/50048]	Loss: 1.0106
Training Epoch: 25 [9856/50048]	Loss: 0.9155
Training Epoch: 25 [9984/50048]	Loss: 1.2313
Training Epoch: 25 [10112/50048]	Loss: 1.1047
Training Epoch: 25 [10240/50048]	Loss: 1.0595
Training Epoch: 25 [10368/50048]	Loss: 1.0969
Training Epoch: 25 [10496/50048]	Loss: 1.1540
Training Epoch: 25 [10624/50048]	Loss: 1.1865
Training Epoch: 25 [10752/50048]	Loss: 1.0950
Training Epoch: 25 [10880/50048]	Loss: 1.1148
Training Epoch: 25 [11008/50048]	Loss: 1.1143
Training Epoch: 25 [11136/50048]	Loss: 0.9664
Training Epoch: 25 [11264/50048]	Loss: 1.1267
Training Epoch: 25 [11392/50048]	Loss: 1.0238
Training Epoch: 25 [11520/50048]	Loss: 1.0747
Training Epoch: 25 [11648/50048]	Loss: 1.2497
Training Epoch: 25 [11776/50048]	Loss: 0.9300
Training Epoch: 25 [11904/50048]	Loss: 1.0329
Training Epoch: 25 [12032/50048]	Loss: 1.0836
Training Epoch: 25 [12160/50048]	Loss: 0.8672
Training Epoch: 25 [12288/50048]	Loss: 1.0850
Training Epoch: 25 [12416/50048]	Loss: 1.2531
Training Epoch: 25 [12544/50048]	Loss: 1.1195
Training Epoch: 25 [12672/50048]	Loss: 1.2456
Training Epoch: 25 [12800/50048]	Loss: 1.0053
Training Epoch: 25 [12928/50048]	Loss: 1.1045
Training Epoch: 25 [13056/50048]	Loss: 1.0671
Training Epoch: 25 [13184/50048]	Loss: 0.9459
Training Epoch: 25 [13312/50048]	Loss: 1.1291
Training Epoch: 25 [13440/50048]	Loss: 0.9454
Training Epoch: 25 [13568/50048]	Loss: 1.1243
Training Epoch: 25 [13696/50048]	Loss: 1.0624
Training Epoch: 25 [13824/50048]	Loss: 1.1767
Training Epoch: 25 [13952/50048]	Loss: 1.1060
Training Epoch: 25 [14080/50048]	Loss: 1.1827
Training Epoch: 25 [14208/50048]	Loss: 1.1729
Training Epoch: 25 [14336/50048]	Loss: 1.0114
Training Epoch: 25 [14464/50048]	Loss: 1.2103
Training Epoch: 25 [14592/50048]	Loss: 0.9461
Training Epoch: 25 [14720/50048]	Loss: 1.1594
Training Epoch: 25 [14848/50048]	Loss: 0.8185
Training Epoch: 25 [14976/50048]	Loss: 0.9635
Training Epoch: 25 [15104/50048]	Loss: 0.9666
Training Epoch: 25 [15232/50048]	Loss: 0.9143
Training Epoch: 25 [15360/50048]	Loss: 1.1988
Training Epoch: 25 [15488/50048]	Loss: 1.1532
Training Epoch: 25 [15616/50048]	Loss: 1.1747
Training Epoch: 25 [15744/50048]	Loss: 1.0747
Training Epoch: 25 [15872/50048]	Loss: 1.1466
Training Epoch: 25 [16000/50048]	Loss: 1.3260
Training Epoch: 25 [16128/50048]	Loss: 1.1232
Training Epoch: 25 [16256/50048]	Loss: 1.1452
Training Epoch: 25 [16384/50048]	Loss: 1.0107
Training Epoch: 25 [16512/50048]	Loss: 0.9620
Training Epoch: 25 [16640/50048]	Loss: 1.1868
Training Epoch: 25 [16768/50048]	Loss: 1.2073
Training Epoch: 25 [16896/50048]	Loss: 1.3823
Training Epoch: 25 [17024/50048]	Loss: 0.9553
Training Epoch: 25 [17152/50048]	Loss: 1.0215
Training Epoch: 25 [17280/50048]	Loss: 1.3940
Training Epoch: 25 [17408/50048]	Loss: 0.8748
Training Epoch: 25 [17536/50048]	Loss: 0.9892
Training Epoch: 25 [17664/50048]	Loss: 1.0416
Training Epoch: 25 [17792/50048]	Loss: 1.3498
Training Epoch: 25 [17920/50048]	Loss: 0.9677
Training Epoch: 25 [18048/50048]	Loss: 0.9346
Training Epoch: 25 [18176/50048]	Loss: 1.1798
Training Epoch: 25 [18304/50048]	Loss: 1.2148
Training Epoch: 25 [18432/50048]	Loss: 1.2579
Training Epoch: 25 [18560/50048]	Loss: 0.9782
Training Epoch: 25 [18688/50048]	Loss: 1.0534
Training Epoch: 25 [18816/50048]	Loss: 1.0871
Training Epoch: 25 [18944/50048]	Loss: 1.1713
Training Epoch: 25 [19072/50048]	Loss: 1.1611
Training Epoch: 25 [19200/50048]	Loss: 1.0049
Training Epoch: 25 [19328/50048]	Loss: 0.8412
Training Epoch: 25 [19456/50048]	Loss: 0.9165
Training Epoch: 25 [19584/50048]	Loss: 0.9724
Training Epoch: 25 [19712/50048]	Loss: 1.2939
Training Epoch: 25 [19840/50048]	Loss: 0.9211
Training Epoch: 25 [19968/50048]	Loss: 1.1096
Training Epoch: 25 [20096/50048]	Loss: 0.9840
Training Epoch: 25 [20224/50048]	Loss: 0.9839
Training Epoch: 25 [20352/50048]	Loss: 1.0035
Training Epoch: 25 [20480/50048]	Loss: 0.9990
Training Epoch: 25 [20608/50048]	Loss: 1.2849
Training Epoch: 25 [20736/50048]	Loss: 1.2458
Training Epoch: 25 [20864/50048]	Loss: 1.1913
Training Epoch: 25 [20992/50048]	Loss: 0.9135
Training Epoch: 25 [21120/50048]	Loss: 1.2186
Training Epoch: 25 [21248/50048]	Loss: 1.1454
Training Epoch: 25 [21376/50048]	Loss: 1.0802
Training Epoch: 25 [21504/50048]	Loss: 1.0119
Training Epoch: 25 [21632/50048]	Loss: 1.0210
Training Epoch: 25 [21760/50048]	Loss: 1.2547
Training Epoch: 25 [21888/50048]	Loss: 1.0437
Training Epoch: 25 [22016/50048]	Loss: 1.0248
Training Epoch: 25 [22144/50048]	Loss: 1.2738
Training Epoch: 25 [22272/50048]	Loss: 1.0180
Training Epoch: 25 [22400/50048]	Loss: 1.0216
Training Epoch: 25 [22528/50048]	Loss: 1.1913
Training Epoch: 25 [22656/50048]	Loss: 1.3514
Training Epoch: 25 [22784/50048]	Loss: 0.9643
Training Epoch: 25 [22912/50048]	Loss: 1.1791
Training Epoch: 25 [23040/50048]	Loss: 1.2149
Training Epoch: 25 [23168/50048]	Loss: 1.1774
Training Epoch: 25 [23296/50048]	Loss: 1.0698
Training Epoch: 25 [23424/50048]	Loss: 1.2655
Training Epoch: 25 [23552/50048]	Loss: 1.0532
Training Epoch: 25 [23680/50048]	Loss: 1.0735
Training Epoch: 25 [23808/50048]	Loss: 0.9869
Training Epoch: 25 [23936/50048]	Loss: 1.0692
Training Epoch: 25 [24064/50048]	Loss: 0.9743
Training Epoch: 25 [24192/50048]	Loss: 1.0905
Training Epoch: 25 [24320/50048]	Loss: 0.9303
Training Epoch: 25 [24448/50048]	Loss: 1.0346
Training Epoch: 25 [24576/50048]	Loss: 1.1552
Training Epoch: 25 [24704/50048]	Loss: 0.9313
Training Epoch: 25 [24832/50048]	Loss: 1.1849
Training Epoch: 25 [24960/50048]	Loss: 1.2380
Training Epoch: 25 [25088/50048]	Loss: 1.1114
Training Epoch: 25 [25216/50048]	Loss: 0.8789
Training Epoch: 25 [25344/50048]	Loss: 1.0657
Training Epoch: 25 [25472/50048]	Loss: 0.9450
Training Epoch: 25 [25600/50048]	Loss: 0.9925
Training Epoch: 25 [25728/50048]	Loss: 1.0803
Training Epoch: 25 [25856/50048]	Loss: 0.9531
Training Epoch: 25 [25984/50048]	Loss: 1.0777
Training Epoch: 25 [26112/50048]	Loss: 1.0407
Training Epoch: 25 [26240/50048]	Loss: 1.0718
Training Epoch: 25 [26368/50048]	Loss: 0.8137
Training Epoch: 25 [26496/50048]	Loss: 0.9686
Training Epoch: 25 [26624/50048]	Loss: 1.0086
Training Epoch: 25 [26752/50048]	Loss: 1.2618
Training Epoch: 25 [26880/50048]	Loss: 0.9411
Training Epoch: 25 [27008/50048]	Loss: 1.0861
Training Epoch: 25 [27136/50048]	Loss: 1.0764
Training Epoch: 25 [27264/50048]	Loss: 0.8471
Training Epoch: 25 [27392/50048]	Loss: 1.0082
Training Epoch: 25 [27520/50048]	Loss: 1.0222
Training Epoch: 25 [27648/50048]	Loss: 1.2464
Training Epoch: 25 [27776/50048]	Loss: 1.0967
Training Epoch: 25 [27904/50048]	Loss: 1.1448
Training Epoch: 25 [28032/50048]	Loss: 1.1668
Training Epoch: 25 [28160/50048]	Loss: 1.0419
Training Epoch: 25 [28288/50048]	Loss: 1.0036
Training Epoch: 25 [28416/50048]	Loss: 1.0214
Training Epoch: 25 [28544/50048]	Loss: 1.1821
Training Epoch: 25 [28672/50048]	Loss: 1.0047
Training Epoch: 25 [28800/50048]	Loss: 0.9895
Training Epoch: 25 [28928/50048]	Loss: 1.0079
Training Epoch: 25 [29056/50048]	Loss: 0.8128
Training Epoch: 25 [29184/50048]	Loss: 1.0185
Training Epoch: 25 [29312/50048]	Loss: 1.1423
Training Epoch: 25 [29440/50048]	Loss: 0.9615
Training Epoch: 25 [29568/50048]	Loss: 1.2783
Training Epoch: 25 [29696/50048]	Loss: 1.1388
Training Epoch: 25 [29824/50048]	Loss: 0.7680
Training Epoch: 25 [29952/50048]	Loss: 0.9552
Training Epoch: 25 [30080/50048]	Loss: 1.2214
Training Epoch: 25 [30208/50048]	Loss: 1.1282
Training Epoch: 25 [30336/50048]	Loss: 1.0672
Training Epoch: 25 [30464/50048]	Loss: 0.9648
Training Epoch: 25 [30592/50048]	Loss: 1.3574
Training Epoch: 25 [30720/50048]	Loss: 1.0728
Training Epoch: 25 [30848/50048]	Loss: 0.9922
Training Epoch: 25 [30976/50048]	Loss: 1.0836
Training Epoch: 25 [31104/50048]	Loss: 0.9934
Training Epoch: 25 [31232/50048]	Loss: 1.1229
Training Epoch: 25 [31360/50048]	Loss: 1.1847
Training Epoch: 25 [31488/50048]	Loss: 1.0709
Training Epoch: 25 [31616/50048]	Loss: 1.1749
Training Epoch: 25 [31744/50048]	Loss: 0.9524
Training Epoch: 25 [31872/50048]	Loss: 1.2220
Training Epoch: 25 [32000/50048]	Loss: 1.3868
Training Epoch: 25 [32128/50048]	Loss: 1.0483
Training Epoch: 25 [32256/50048]	Loss: 0.9904
Training Epoch: 25 [32384/50048]	Loss: 1.1909
Training Epoch: 25 [32512/50048]	Loss: 1.0556
Training Epoch: 25 [32640/50048]	Loss: 1.1524
Training Epoch: 25 [32768/50048]	Loss: 1.0195
Training Epoch: 25 [32896/50048]	Loss: 0.8969
Training Epoch: 25 [33024/50048]	Loss: 1.1807
Training Epoch: 25 [33152/50048]	Loss: 1.4255
Training Epoch: 25 [33280/50048]	Loss: 1.1619
Training Epoch: 25 [33408/50048]	Loss: 1.3831
Training Epoch: 25 [33536/50048]	Loss: 1.0636
Training Epoch: 25 [33664/50048]	Loss: 0.9972
Training Epoch: 25 [33792/50048]	Loss: 1.0932
Training Epoch: 25 [33920/50048]	Loss: 1.2453
Training Epoch: 25 [34048/50048]	Loss: 1.2422
Training Epoch: 25 [34176/50048]	Loss: 1.2391
Training Epoch: 25 [34304/50048]	Loss: 0.8791
Training Epoch: 25 [34432/50048]	Loss: 1.2439
Training Epoch: 25 [34560/50048]	Loss: 0.8957
Training Epoch: 25 [34688/50048]	Loss: 1.3024
Training Epoch: 25 [34816/50048]	Loss: 1.0444
Training Epoch: 25 [34944/50048]	Loss: 0.9122
Training Epoch: 25 [35072/50048]	Loss: 1.0582
Training Epoch: 25 [35200/50048]	Loss: 1.0482
Training Epoch: 25 [35328/50048]	Loss: 1.2316
Training Epoch: 25 [35456/50048]	Loss: 1.0291
Training Epoch: 25 [35584/50048]	Loss: 1.1965
Training Epoch: 25 [35712/50048]	Loss: 0.8411
Training Epoch: 25 [35840/50048]	Loss: 0.9900
Training Epoch: 25 [35968/50048]	Loss: 1.2791
Training Epoch: 25 [36096/50048]	Loss: 0.9912
Training Epoch: 25 [36224/50048]	Loss: 1.1080
Training Epoch: 25 [36352/50048]	Loss: 1.1316
Training Epoch: 25 [36480/50048]	Loss: 1.1125
Training Epoch: 25 [36608/50048]	Loss: 1.1928
Training Epoch: 25 [36736/50048]	Loss: 1.0585
Training Epoch: 25 [36864/50048]	Loss: 1.2097
Training Epoch: 25 [36992/50048]	Loss: 1.1602
Training Epoch: 25 [37120/50048]	Loss: 1.0703
Training Epoch: 25 [37248/50048]	Loss: 0.8364
Training Epoch: 25 [37376/50048]	Loss: 1.2734
Training Epoch: 25 [37504/50048]	Loss: 1.2466
Training Epoch: 25 [37632/50048]	Loss: 1.2084
Training Epoch: 25 [37760/50048]	Loss: 1.0769
Training Epoch: 25 [37888/50048]	Loss: 1.0207
Training Epoch: 25 [38016/50048]	Loss: 1.0828
Training Epoch: 25 [38144/50048]	Loss: 1.0066
Training Epoch: 25 [38272/50048]	Loss: 1.0462
Training Epoch: 25 [38400/50048]	Loss: 1.0941
Training Epoch: 25 [38528/50048]	Loss: 1.1237
Training Epoch: 25 [38656/50048]	Loss: 0.7122
Training Epoch: 25 [38784/50048]	Loss: 1.2348
Training Epoch: 25 [38912/50048]	Loss: 0.9201
Training Epoch: 25 [39040/50048]	Loss: 0.9088
Training Epoch: 25 [39168/50048]	Loss: 0.9749
Training Epoch: 25 [39296/50048]	Loss: 1.3379
Training Epoch: 25 [39424/50048]	Loss: 0.9475
Training Epoch: 25 [39552/50048]	Loss: 1.0068
Training Epoch: 25 [39680/50048]	Loss: 1.2685
Training Epoch: 25 [39808/50048]	Loss: 1.2102
Training Epoch: 25 [39936/50048]	Loss: 1.1974
Training Epoch: 25 [40064/50048]	Loss: 1.1869
Training Epoch: 25 [40192/50048]	Loss: 1.0562
Training Epoch: 25 [40320/50048]	Loss: 0.9501
Training Epoch: 25 [40448/50048]	Loss: 0.9655
Training Epoch: 25 [40576/50048]	Loss: 1.2211
Training Epoch: 25 [40704/50048]	Loss: 0.9886
Training Epoch: 25 [40832/50048]	Loss: 1.3199
Training Epoch: 25 [40960/50048]	Loss: 1.1410
Training Epoch: 25 [41088/50048]	Loss: 0.9426
Training Epoch: 25 [41216/50048]	Loss: 0.8582
Training Epoch: 25 [41344/50048]	Loss: 1.2993
Training Epoch: 25 [41472/50048]	Loss: 1.1130
Training Epoch: 25 [41600/50048]	Loss: 1.0712
Training Epoch: 25 [41728/50048]	Loss: 0.9044
Training Epoch: 25 [41856/50048]	Loss: 0.9553
Training Epoch: 25 [41984/50048]	Loss: 1.2207
Training Epoch: 25 [42112/50048]	Loss: 1.4142
Training Epoch: 25 [42240/50048]	Loss: 1.1217
Training Epoch: 25 [42368/50048]	Loss: 1.1758
Training Epoch: 25 [42496/50048]	Loss: 1.1757
Training Epoch: 25 [42624/50048]	Loss: 1.0876
Training Epoch: 25 [42752/50048]	Loss: 0.9950
Training Epoch: 25 [42880/50048]	Loss: 1.2365
Training Epoch: 25 [43008/50048]	Loss: 1.1423
Training Epoch: 25 [43136/50048]	Loss: 0.9094
Training Epoch: 25 [43264/50048]	Loss: 1.1319
Training Epoch: 25 [43392/50048]	Loss: 1.0320
Training Epoch: 25 [43520/50048]	Loss: 1.1043
Training Epoch: 25 [43648/50048]	Loss: 1.0389
Training Epoch: 25 [43776/50048]	Loss: 0.9864
Training Epoch: 25 [43904/50048]	Loss: 1.1704
Training Epoch: 25 [44032/50048]	Loss: 1.0288
Training Epoch: 25 [44160/50048]	Loss: 1.0130
Training Epoch: 25 [44288/50048]	Loss: 1.1506
Training Epoch: 25 [44416/50048]	Loss: 1.1948
Training Epoch: 25 [44544/50048]	Loss: 1.1554
Training Epoch: 25 [44672/50048]	Loss: 1.0464
Training Epoch: 25 [44800/50048]	Loss: 1.1331
Training Epoch: 25 [44928/50048]	Loss: 1.1111
Training Epoch: 25 [45056/50048]	Loss: 1.2732
Training Epoch: 25 [45184/50048]	Loss: 1.1292
Training Epoch: 25 [45312/50048]	Loss: 1.2214
Training Epoch: 25 [45440/50048]	Loss: 1.0626
Training Epoch: 25 [45568/50048]	Loss: 1.2842
Training Epoch: 25 [45696/50048]	Loss: 1.0893
Training Epoch: 25 [45824/50048]	Loss: 1.0347
Training Epoch: 25 [45952/50048]	Loss: 1.2782
Training Epoch: 25 [46080/50048]	Loss: 1.1822
Training Epoch: 25 [46208/50048]	Loss: 1.0139
Training Epoch: 25 [46336/50048]	Loss: 1.2635
Training Epoch: 25 [46464/50048]	Loss: 1.2999
Training Epoch: 25 [46592/50048]	Loss: 1.0128
Training Epoch: 25 [46720/50048]	Loss: 1.0913
Training Epoch: 25 [46848/50048]	Loss: 1.1328
Training Epoch: 25 [46976/50048]	Loss: 1.2637
Training Epoch: 25 [47104/50048]	Loss: 1.0365
Training Epoch: 25 [47232/50048]	Loss: 1.2003
Training Epoch: 25 [47360/50048]	Loss: 1.0458
Training Epoch: 25 [47488/50048]	Loss: 0.9732
Training Epoch: 25 [47616/50048]	Loss: 0.9458
Training Epoch: 25 [47744/50048]	Loss: 1.1451
Training Epoch: 25 [47872/50048]	Loss: 1.0449
Training Epoch: 25 [48000/50048]	Loss: 0.8536
Training Epoch: 25 [48128/50048]	Loss: 1.2630
Training Epoch: 25 [48256/50048]	Loss: 0.9361
Training Epoch: 25 [48384/50048]	Loss: 1.1777
Training Epoch: 25 [48512/50048]	Loss: 1.1755
Training Epoch: 25 [48640/50048]	Loss: 1.2484
Training Epoch: 25 [48768/50048]	Loss: 0.9825
Training Epoch: 25 [48896/50048]	Loss: 1.0209
Training Epoch: 25 [49024/50048]	Loss: 1.0365
Training Epoch: 25 [49152/50048]	Loss: 1.1662
Training Epoch: 25 [49280/50048]	Loss: 1.0664
Training Epoch: 25 [49408/50048]	Loss: 0.9776
Training Epoch: 25 [49536/50048]	Loss: 1.1422
Training Epoch: 25 [49664/50048]	Loss: 0.8977
Training Epoch: 25 [49792/50048]	Loss: 1.0804
Training Epoch: 25 [49920/50048]	Loss: 1.0796
Training Epoch: 25 [50048/50048]	Loss: 0.8960
Validation Epoch: 25, Average loss: 0.0112, Accuracy: 0.6053
[Training Loop] Target accuracy 0.6 reached!
[Training Loop] Training done
Stopped Zeus monitor 0.
