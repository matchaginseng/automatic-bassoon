Files already downloaded and verified
Files already downloaded and verified
Job(cifar100,shufflenetv2,adam,0.6,bs1024~100)
[Training Loop] Testing batch sizes: [128, 256, 512, 1024]
[Training Loop] Testing power limits: [175, 150, 125, 100]
[Training Loop] Testing learning rates: [0.001, 0.005, 0.01]
[Training Loop] Testing dropout rates: [0.0, 0.25, 0.5]
[Training Loop] Reprofiling at accuracy thresholds [0.5, 0.4, 0.3]
[Training Loop] Model's accuracy 0.0 surpasses threshold 0.0! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
Launching Zeus monitor 0...
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6401
Profiling... [256/50048]	Loss: 4.6554
Profiling... [384/50048]	Loss: 4.6556
Profiling... [512/50048]	Loss: 4.7353
Profiling... [640/50048]	Loss: 4.8162
Profiling... [768/50048]	Loss: 4.7083
Profiling... [896/50048]	Loss: 4.7343
Profiling... [1024/50048]	Loss: 4.8013
Profiling... [1152/50048]	Loss: 4.5829
Profiling... [1280/50048]	Loss: 4.7069
Profiling... [1408/50048]	Loss: 4.6172
Profiling... [1536/50048]	Loss: 4.6036
Profiling... [1664/50048]	Loss: 4.7181
Profile done
epoch 1 train time consumed: 5.51s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 140.03594745793117,
                        "time": 2.150806485000004,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38060671.558560066
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6068
Profiling... [256/50048]	Loss: 4.5848
Profiling... [384/50048]	Loss: 4.7637
Profiling... [512/50048]	Loss: 4.7896
Profiling... [640/50048]	Loss: 4.7545
Profiling... [768/50048]	Loss: 4.7352
Profiling... [896/50048]	Loss: 4.6457
Profiling... [1024/50048]	Loss: 4.7411
Profiling... [1152/50048]	Loss: 4.6241
Profiling... [1280/50048]	Loss: 4.6506
Profiling... [1408/50048]	Loss: 4.6281
Profiling... [1536/50048]	Loss: 4.6209
Profiling... [1664/50048]	Loss: 4.6467
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.86342540042044,
                        "time": 2.1476735020000035,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38005230.29139206
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6486
Profiling... [256/50048]	Loss: 4.6238
Profiling... [384/50048]	Loss: 4.7320
Profiling... [512/50048]	Loss: 4.7218
Profiling... [640/50048]	Loss: 4.6942
Profiling... [768/50048]	Loss: 4.6432
Profiling... [896/50048]	Loss: 4.7010
Profiling... [1024/50048]	Loss: 4.7284
Profiling... [1152/50048]	Loss: 4.7363
Profiling... [1280/50048]	Loss: 4.7582
Profiling... [1408/50048]	Loss: 4.6248
Profiling... [1536/50048]	Loss: 4.5932
Profiling... [1664/50048]	Loss: 4.6951
Profile done
epoch 1 train time consumed: 3.28s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 125.23239751652744,
                        "time": 2.3595490639999923,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 41754580.236543864
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6103
Profiling... [256/50048]	Loss: 4.7020
Profiling... [384/50048]	Loss: 4.7483
Profiling... [512/50048]	Loss: 4.7112
Profiling... [640/50048]	Loss: 4.7918
Profiling... [768/50048]	Loss: 4.7247
Profiling... [896/50048]	Loss: 4.7978
Profiling... [1024/50048]	Loss: 4.6742
Profiling... [1152/50048]	Loss: 4.6795
Profiling... [1280/50048]	Loss: 4.6126
Profiling... [1408/50048]	Loss: 4.6319
Profiling... [1536/50048]	Loss: 4.6632
Profiling... [1664/50048]	Loss: 4.5153
Profile done
epoch 1 train time consumed: 6.43s
Validation Epoch: 0, Average loss: 0.0363, Accuracy: 0.0101
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.01242649248566,
                        "time": 4.766437062000023,
                        "accuracy": 0.010087025316455696,
                        "total_cost": 82693010.04818863
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.5974
Profiling... [256/50048]	Loss: 4.7104
Profiling... [384/50048]	Loss: 4.6589
Profiling... [512/50048]	Loss: 4.6467
Profiling... [640/50048]	Loss: 4.7214
Profiling... [768/50048]	Loss: 4.7595
Profiling... [896/50048]	Loss: 4.7303
Profiling... [1024/50048]	Loss: 4.6571
Profiling... [1152/50048]	Loss: 4.6545
Profiling... [1280/50048]	Loss: 4.6658
Profiling... [1408/50048]	Loss: 4.6904
Profiling... [1536/50048]	Loss: 4.6846
Profiling... [1664/50048]	Loss: 4.5943
Profile done
epoch 1 train time consumed: 3.08s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.40199286331618,
                        "time": 2.151674761999999,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38076036.58835198
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6265
Profiling... [256/50048]	Loss: 4.6539
Profiling... [384/50048]	Loss: 4.7655
Profiling... [512/50048]	Loss: 4.7742
Profiling... [640/50048]	Loss: 4.7394
Profiling... [768/50048]	Loss: 4.7467
Profiling... [896/50048]	Loss: 4.6741
Profiling... [1024/50048]	Loss: 4.6576
Profiling... [1152/50048]	Loss: 4.7537
Profiling... [1280/50048]	Loss: 4.6962
Profiling... [1408/50048]	Loss: 4.6753
Profiling... [1536/50048]	Loss: 4.5334
Profiling... [1664/50048]	Loss: 4.6043
Profile done
epoch 1 train time consumed: 3.16s
Validation Epoch: 0, Average loss: 0.0363, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.28978855559996,
                        "time": 2.1561843860000067,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38155838.894656114
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6743
Profiling... [256/50048]	Loss: 4.6416
Profiling... [384/50048]	Loss: 4.6590
Profiling... [512/50048]	Loss: 4.7142
Profiling... [640/50048]	Loss: 4.7756
Profiling... [768/50048]	Loss: 4.7133
Profiling... [896/50048]	Loss: 4.7381
Profiling... [1024/50048]	Loss: 4.7259
Profiling... [1152/50048]	Loss: 4.5750
Profiling... [1280/50048]	Loss: 4.7430
Profiling... [1408/50048]	Loss: 4.7033
Profiling... [1536/50048]	Loss: 4.5608
Profiling... [1664/50048]	Loss: 4.6634
Profile done
epoch 1 train time consumed: 3.30s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.74707367425528,
                        "time": 2.353910208000002,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 41654795.04076803
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6321
Profiling... [256/50048]	Loss: 4.6223
Profiling... [384/50048]	Loss: 4.7506
Profiling... [512/50048]	Loss: 4.7628
Profiling... [640/50048]	Loss: 4.7695
Profiling... [768/50048]	Loss: 4.8073
Profiling... [896/50048]	Loss: 4.7107
Profiling... [1024/50048]	Loss: 4.6400
Profiling... [1152/50048]	Loss: 4.5901
Profiling... [1280/50048]	Loss: 4.6359
Profiling... [1408/50048]	Loss: 4.5852
Profiling... [1536/50048]	Loss: 4.5901
Profiling... [1664/50048]	Loss: 4.6994
Profile done
epoch 1 train time consumed: 6.48s
Validation Epoch: 0, Average loss: 0.0363, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.36187562974209,
                        "time": 4.784516095000015,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 84666796.81712027
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6338
Profiling... [256/50048]	Loss: 4.6033
Profiling... [384/50048]	Loss: 4.7425
Profiling... [512/50048]	Loss: 4.6881
Profiling... [640/50048]	Loss: 4.7893
Profiling... [768/50048]	Loss: 4.7787
Profiling... [896/50048]	Loss: 4.7394
Profiling... [1024/50048]	Loss: 4.6066
Profiling... [1152/50048]	Loss: 4.6945
Profiling... [1280/50048]	Loss: 4.6539
Profiling... [1408/50048]	Loss: 4.5579
Profiling... [1536/50048]	Loss: 4.5775
Profiling... [1664/50048]	Loss: 4.6250
Profile done
epoch 1 train time consumed: 3.33s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.09674017173151,
                        "time": 2.154329270000005,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38123010.76192009
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6230
Profiling... [256/50048]	Loss: 4.6902
Profiling... [384/50048]	Loss: 4.6661
Profiling... [512/50048]	Loss: 4.7360
Profiling... [640/50048]	Loss: 4.7701
Profiling... [768/50048]	Loss: 4.7229
Profiling... [896/50048]	Loss: 4.7059
Profiling... [1024/50048]	Loss: 4.6720
Profiling... [1152/50048]	Loss: 4.7242
Profiling... [1280/50048]	Loss: 4.7221
Profiling... [1408/50048]	Loss: 4.6384
Profiling... [1536/50048]	Loss: 4.6038
Profiling... [1664/50048]	Loss: 4.7073
Profile done
epoch 1 train time consumed: 3.08s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.51547910667772,
                        "time": 2.1468176920000133,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 37990085.87763223
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6457
Profiling... [256/50048]	Loss: 4.6703
Profiling... [384/50048]	Loss: 4.7587
Profiling... [512/50048]	Loss: 4.7199
Profiling... [640/50048]	Loss: 4.7301
Profiling... [768/50048]	Loss: 4.7552
Profiling... [896/50048]	Loss: 4.6446
Profiling... [1024/50048]	Loss: 4.7286
Profiling... [1152/50048]	Loss: 4.7036
Profiling... [1280/50048]	Loss: 4.5783
Profiling... [1408/50048]	Loss: 4.6400
Profiling... [1536/50048]	Loss: 4.6599
Profiling... [1664/50048]	Loss: 4.6495
Profile done
epoch 1 train time consumed: 3.31s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.47605624448491,
                        "time": 2.368920856000045,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 41920423.4677768
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6315
Profiling... [256/50048]	Loss: 4.5701
Profiling... [384/50048]	Loss: 4.7565
Profiling... [512/50048]	Loss: 4.8117
Profiling... [640/50048]	Loss: 4.5963
Profiling... [768/50048]	Loss: 4.8155
Profiling... [896/50048]	Loss: 4.7875
Profiling... [1024/50048]	Loss: 4.6916
Profiling... [1152/50048]	Loss: 4.8317
Profiling... [1280/50048]	Loss: 4.6658
Profiling... [1408/50048]	Loss: 4.6636
Profiling... [1536/50048]	Loss: 4.6089
Profiling... [1664/50048]	Loss: 4.5888
Profile done
epoch 1 train time consumed: 6.65s
Validation Epoch: 0, Average loss: 0.0363, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.56537927489113,
                        "time": 4.944772477000015,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 87502693.75299226
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.5955
Profiling... [256/50048]	Loss: 5.9152
Profiling... [384/50048]	Loss: 5.9342
Profiling... [512/50048]	Loss: 4.9848
Profiling... [640/50048]	Loss: 5.2658
Profiling... [768/50048]	Loss: 4.9455
Profiling... [896/50048]	Loss: 4.9687
Profiling... [1024/50048]	Loss: 5.1386
Profiling... [1152/50048]	Loss: 5.0119
Profiling... [1280/50048]	Loss: 4.7936
Profiling... [1408/50048]	Loss: 4.7159
Profiling... [1536/50048]	Loss: 4.6502
Profiling... [1664/50048]	Loss: 4.5547
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.0367, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.96336284624823,
                        "time": 2.1578533279999874,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38185372.49228778
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.5851
Profiling... [256/50048]	Loss: 5.8548
Profiling... [384/50048]	Loss: 5.6753
Profiling... [512/50048]	Loss: 5.5004
Profiling... [640/50048]	Loss: 5.7025
Profiling... [768/50048]	Loss: 5.2881
Profiling... [896/50048]	Loss: 5.4033
Profiling... [1024/50048]	Loss: 5.0690
Profiling... [1152/50048]	Loss: 5.3149
Profiling... [1280/50048]	Loss: 4.8897
Profiling... [1408/50048]	Loss: 4.8544
Profiling... [1536/50048]	Loss: 4.7622
Profiling... [1664/50048]	Loss: 4.7824
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0379, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.01773466294593,
                        "time": 2.151825869999982,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38078710.59551968
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6396
Profiling... [256/50048]	Loss: 5.6080
Profiling... [384/50048]	Loss: 5.7239
Profiling... [512/50048]	Loss: 5.6847
Profiling... [640/50048]	Loss: 5.0540
Profiling... [768/50048]	Loss: 5.2134
Profiling... [896/50048]	Loss: 4.8511
Profiling... [1024/50048]	Loss: 4.7339
Profiling... [1152/50048]	Loss: 4.9558
Profiling... [1280/50048]	Loss: 4.9930
Profiling... [1408/50048]	Loss: 4.8645
Profiling... [1536/50048]	Loss: 4.8001
Profiling... [1664/50048]	Loss: 4.7394
Profile done
epoch 1 train time consumed: 3.41s
Validation Epoch: 0, Average loss: 0.0367, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.73301038274252,
                        "time": 2.3682500559999653,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 41908552.99097539
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.5733
Profiling... [256/50048]	Loss: 5.7580
Profiling... [384/50048]	Loss: 6.0721
Profiling... [512/50048]	Loss: 5.5390
Profiling... [640/50048]	Loss: 5.0815
Profiling... [768/50048]	Loss: 5.3051
Profiling... [896/50048]	Loss: 5.1428
Profiling... [1024/50048]	Loss: 4.6764
Profiling... [1152/50048]	Loss: 4.7416
Profiling... [1280/50048]	Loss: 4.8449
Profiling... [1408/50048]	Loss: 4.7703
Profiling... [1536/50048]	Loss: 4.7620
Profiling... [1664/50048]	Loss: 4.7317
Profile done
epoch 1 train time consumed: 6.52s
Validation Epoch: 0, Average loss: 0.0374, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.13668290464155,
                        "time": 4.7957983790000185,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 84866448.11478432
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6381
Profiling... [256/50048]	Loss: 5.6766
Profiling... [384/50048]	Loss: 5.5065
Profiling... [512/50048]	Loss: 5.0354
Profiling... [640/50048]	Loss: 5.6430
Profiling... [768/50048]	Loss: 4.9125
Profiling... [896/50048]	Loss: 4.9062
Profiling... [1024/50048]	Loss: 4.9620
Profiling... [1152/50048]	Loss: 4.6872
Profiling... [1280/50048]	Loss: 4.9048
Profiling... [1408/50048]	Loss: 4.6184
Profiling... [1536/50048]	Loss: 4.6348
Profiling... [1664/50048]	Loss: 4.6833
Profile done
epoch 1 train time consumed: 3.29s
Validation Epoch: 0, Average loss: 0.0366, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.05910230934823,
                        "time": 2.1531280089999996,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38101753.24726399
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6302
Profiling... [256/50048]	Loss: 5.9148
Profiling... [384/50048]	Loss: 6.2473
Profiling... [512/50048]	Loss: 5.4434
Profiling... [640/50048]	Loss: 5.4899
Profiling... [768/50048]	Loss: 4.9513
Profiling... [896/50048]	Loss: 4.9482
Profiling... [1024/50048]	Loss: 5.0360
Profiling... [1152/50048]	Loss: 4.8435
Profiling... [1280/50048]	Loss: 4.8361
Profiling... [1408/50048]	Loss: 4.7028
Profiling... [1536/50048]	Loss: 4.8519
Profiling... [1664/50048]	Loss: 4.7352
Profile done
epoch 1 train time consumed: 3.08s
Validation Epoch: 0, Average loss: 0.0373, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.83178921023682,
                        "time": 2.1493266869999843,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38034485.05315172
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6224
Profiling... [256/50048]	Loss: 5.7602
Profiling... [384/50048]	Loss: 5.7349
Profiling... [512/50048]	Loss: 5.4279
Profiling... [640/50048]	Loss: 5.0499
Profiling... [768/50048]	Loss: 4.9324
Profiling... [896/50048]	Loss: 5.0247
Profiling... [1024/50048]	Loss: 4.7576
Profiling... [1152/50048]	Loss: 4.8679
Profiling... [1280/50048]	Loss: 4.9308
Profiling... [1408/50048]	Loss: 4.8613
Profiling... [1536/50048]	Loss: 4.7761
Profiling... [1664/50048]	Loss: 4.8616
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 0, Average loss: 0.0382, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.4050692316063,
                        "time": 2.362910467000006,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 41814063.6240321
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6531
Profiling... [256/50048]	Loss: 5.6276
Profiling... [384/50048]	Loss: 6.0288
Profiling... [512/50048]	Loss: 5.2894
Profiling... [640/50048]	Loss: 5.2531
Profiling... [768/50048]	Loss: 5.0971
Profiling... [896/50048]	Loss: 4.6738
Profiling... [1024/50048]	Loss: 5.0418
Profiling... [1152/50048]	Loss: 4.8025
Profiling... [1280/50048]	Loss: 4.7284
Profiling... [1408/50048]	Loss: 4.7725
Profiling... [1536/50048]	Loss: 4.7233
Profiling... [1664/50048]	Loss: 4.7454
Profile done
epoch 1 train time consumed: 6.70s
Validation Epoch: 0, Average loss: 0.0375, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.99153953047353,
                        "time": 4.955807218000018,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 87697964.52972831
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6437
Profiling... [256/50048]	Loss: 5.6875
Profiling... [384/50048]	Loss: 5.8529
Profiling... [512/50048]	Loss: 5.1321
Profiling... [640/50048]	Loss: 5.5195
Profiling... [768/50048]	Loss: 4.8476
Profiling... [896/50048]	Loss: 4.8506
Profiling... [1024/50048]	Loss: 4.8032
Profiling... [1152/50048]	Loss: 4.7581
Profiling... [1280/50048]	Loss: 4.5870
Profiling... [1408/50048]	Loss: 4.7803
Profiling... [1536/50048]	Loss: 4.7280
Profiling... [1664/50048]	Loss: 4.7037
Profile done
epoch 1 train time consumed: 3.16s
Validation Epoch: 0, Average loss: 0.0369, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.01093564949528,
                        "time": 2.1575267259999578,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38179592.94329525
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6665
Profiling... [256/50048]	Loss: 5.7705
Profiling... [384/50048]	Loss: 5.7911
Profiling... [512/50048]	Loss: 5.4644
Profiling... [640/50048]	Loss: 5.3012
Profiling... [768/50048]	Loss: 5.0695
Profiling... [896/50048]	Loss: 5.2234
Profiling... [1024/50048]	Loss: 4.7465
Profiling... [1152/50048]	Loss: 4.6027
Profiling... [1280/50048]	Loss: 4.7355
Profiling... [1408/50048]	Loss: 4.6049
Profiling... [1536/50048]	Loss: 4.7858
Profiling... [1664/50048]	Loss: 4.8725
Profile done
epoch 1 train time consumed: 3.08s
Validation Epoch: 0, Average loss: 0.0372, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.66401327134892,
                        "time": 2.154126228999985,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38119417.74838373
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6361
Profiling... [256/50048]	Loss: 5.7884
Profiling... [384/50048]	Loss: 5.6983
Profiling... [512/50048]	Loss: 5.3980
Profiling... [640/50048]	Loss: 5.1520
Profiling... [768/50048]	Loss: 5.3408
Profiling... [896/50048]	Loss: 5.1269
Profiling... [1024/50048]	Loss: 5.3296
Profiling... [1152/50048]	Loss: 4.9138
Profiling... [1280/50048]	Loss: 4.8790
Profiling... [1408/50048]	Loss: 4.7440
Profiling... [1536/50048]	Loss: 4.5555
Profiling... [1664/50048]	Loss: 4.5823
Profile done
epoch 1 train time consumed: 3.30s
Validation Epoch: 0, Average loss: 0.0386, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.1501933941258,
                        "time": 2.356931526999972,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 41708260.3017915
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6308
Profiling... [256/50048]	Loss: 5.8843
Profiling... [384/50048]	Loss: 5.5374
Profiling... [512/50048]	Loss: 5.4834
Profiling... [640/50048]	Loss: 4.9504
Profiling... [768/50048]	Loss: 4.9182
Profiling... [896/50048]	Loss: 4.9190
Profiling... [1024/50048]	Loss: 4.6791
Profiling... [1152/50048]	Loss: 4.7158
Profiling... [1280/50048]	Loss: 4.7143
Profiling... [1408/50048]	Loss: 4.7137
Profiling... [1536/50048]	Loss: 4.7480
Profiling... [1664/50048]	Loss: 4.7306
Profile done
epoch 1 train time consumed: 6.64s
Validation Epoch: 0, Average loss: 0.0368, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.77455661223905,
                        "time": 4.893454754999993,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 86594575.34447987
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6329
Profiling... [256/50048]	Loss: 7.6625
Profiling... [384/50048]	Loss: 6.7979
Profiling... [512/50048]	Loss: 6.2191
Profiling... [640/50048]	Loss: 5.3952
Profiling... [768/50048]	Loss: 5.0262
Profiling... [896/50048]	Loss: 4.6973
Profiling... [1024/50048]	Loss: 4.7489
Profiling... [1152/50048]	Loss: 4.7765
Profiling... [1280/50048]	Loss: 4.9763
Profiling... [1408/50048]	Loss: 4.5847
Profiling... [1536/50048]	Loss: 4.7183
Profiling... [1664/50048]	Loss: 5.0445
Profile done
epoch 1 train time consumed: 3.09s
Validation Epoch: 0, Average loss: 0.0986, Accuracy: 0.0172
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.97202554554933,
                        "time": 2.156086513000048,
                        "accuracy": 0.01720727848101266,
                        "total_cost": 21927647.66324646
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6316
Profiling... [256/50048]	Loss: 7.8988
Profiling... [384/50048]	Loss: 7.5402
Profiling... [512/50048]	Loss: 6.9722
Profiling... [640/50048]	Loss: 5.6771
Profiling... [768/50048]	Loss: 5.4391
Profiling... [896/50048]	Loss: 5.0125
Profiling... [1024/50048]	Loss: 4.8484
Profiling... [1152/50048]	Loss: 4.7411
Profiling... [1280/50048]	Loss: 4.6872
Profiling... [1408/50048]	Loss: 4.9959
Profiling... [1536/50048]	Loss: 4.7784
Profiling... [1664/50048]	Loss: 4.7130
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.55381480036834,
                        "time": 2.1524143390000177,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38089124.142944306
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6409
Profiling... [256/50048]	Loss: 7.1054
Profiling... [384/50048]	Loss: 7.5494
Profiling... [512/50048]	Loss: 6.6969
Profiling... [640/50048]	Loss: 4.9199
Profiling... [768/50048]	Loss: 4.5985
Profiling... [896/50048]	Loss: 5.1456
Profiling... [1024/50048]	Loss: 4.9506
Profiling... [1152/50048]	Loss: 4.8419
Profiling... [1280/50048]	Loss: 4.7009
Profiling... [1408/50048]	Loss: 4.9087
Profiling... [1536/50048]	Loss: 5.0312
Profiling... [1664/50048]	Loss: 4.9352
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 0, Average loss: 0.0436, Accuracy: 0.0138
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.93465732086113,
                        "time": 2.3533981020000283,
                        "accuracy": 0.013844936708860759,
                        "total_cost": 29746952.00928036
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6160
Profiling... [256/50048]	Loss: 7.0158
Profiling... [384/50048]	Loss: 9.2999
Profiling... [512/50048]	Loss: 7.3046
Profiling... [640/50048]	Loss: 5.5666
Profiling... [768/50048]	Loss: 5.5098
Profiling... [896/50048]	Loss: 5.1978
Profiling... [1024/50048]	Loss: 5.2408
Profiling... [1152/50048]	Loss: 4.9117
Profiling... [1280/50048]	Loss: 4.6980
Profiling... [1408/50048]	Loss: 4.8972
Profiling... [1536/50048]	Loss: 4.8552
Profiling... [1664/50048]	Loss: 4.7715
Profile done
epoch 1 train time consumed: 6.45s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.69079875000678,
                        "time": 4.767614499000047,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 84367706.17430483
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6193
Profiling... [256/50048]	Loss: 7.9316
Profiling... [384/50048]	Loss: 8.1309
Profiling... [512/50048]	Loss: 5.3025
Profiling... [640/50048]	Loss: 4.8956
Profiling... [768/50048]	Loss: 4.7544
Profiling... [896/50048]	Loss: 5.0298
Profiling... [1024/50048]	Loss: 4.6530
Profiling... [1152/50048]	Loss: 4.7008
Profiling... [1280/50048]	Loss: 4.9142
Profiling... [1408/50048]	Loss: 5.3690
Profiling... [1536/50048]	Loss: 4.6998
Profiling... [1664/50048]	Loss: 5.3738
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.08770873113596,
                        "time": 2.157257297000001,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38174825.12771201
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6369
Profiling... [256/50048]	Loss: 8.2328
Profiling... [384/50048]	Loss: 7.7988
Profiling... [512/50048]	Loss: 6.4292
Profiling... [640/50048]	Loss: 5.0069
Profiling... [768/50048]	Loss: 4.8824
Profiling... [896/50048]	Loss: 4.9229
Profiling... [1024/50048]	Loss: 4.7549
Profiling... [1152/50048]	Loss: 5.0023
Profiling... [1280/50048]	Loss: 4.6551
Profiling... [1408/50048]	Loss: 4.8246
Profiling... [1536/50048]	Loss: 5.0731
Profiling... [1664/50048]	Loss: 4.9830
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.53105203161368,
                        "time": 2.152773608000018,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 38095481.76716831
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6285
Profiling... [256/50048]	Loss: 7.0671
Profiling... [384/50048]	Loss: 6.7469
Profiling... [512/50048]	Loss: 6.7640
Profiling... [640/50048]	Loss: 5.5812
Profiling... [768/50048]	Loss: 5.7143
Profiling... [896/50048]	Loss: 4.8867
Profiling... [1024/50048]	Loss: 5.1451
Profiling... [1152/50048]	Loss: 5.6379
Profiling... [1280/50048]	Loss: 5.5392
Profiling... [1408/50048]	Loss: 5.1142
Profiling... [1536/50048]	Loss: 4.8035
Profiling... [1664/50048]	Loss: 4.6825
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 0, Average loss: 0.0424, Accuracy: 0.0090
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.87886815886037,
                        "time": 2.372009534999961,
                        "accuracy": 0.008999208860759493,
                        "total_cost": 46126462.342153095
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6529
Profiling... [256/50048]	Loss: 7.7170
Profiling... [384/50048]	Loss: 7.1905
Profiling... [512/50048]	Loss: 5.1837
Profiling... [640/50048]	Loss: 5.2426
Profiling... [768/50048]	Loss: 5.4967
Profiling... [896/50048]	Loss: 4.9966
Profiling... [1024/50048]	Loss: 4.9767
Profiling... [1152/50048]	Loss: 4.9420
Profiling... [1280/50048]	Loss: 4.9838
Profiling... [1408/50048]	Loss: 4.7649
Profiling... [1536/50048]	Loss: 5.0769
Profiling... [1664/50048]	Loss: 4.6707
Profile done
epoch 1 train time consumed: 6.50s
Validation Epoch: 0, Average loss: 0.1938, Accuracy: 0.0102
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.63230589829435,
                        "time": 4.800644528000021,
                        "accuracy": 0.010185917721518988,
                        "total_cost": 82477869.48299842
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6556
Profiling... [256/50048]	Loss: 7.6326
Profiling... [384/50048]	Loss: 7.1523
Profiling... [512/50048]	Loss: 6.2977
Profiling... [640/50048]	Loss: 5.4220
Profiling... [768/50048]	Loss: 4.8756
Profiling... [896/50048]	Loss: 4.7819
Profiling... [1024/50048]	Loss: 4.6825
Profiling... [1152/50048]	Loss: 4.8554
Profiling... [1280/50048]	Loss: 4.8145
Profiling... [1408/50048]	Loss: 4.7880
Profiling... [1536/50048]	Loss: 4.8318
Profiling... [1664/50048]	Loss: 4.5228
Profile done
epoch 1 train time consumed: 3.28s
Validation Epoch: 0, Average loss: 0.0524, Accuracy: 0.0164
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.03334192137807,
                        "time": 2.1495525720000046,
                        "accuracy": 0.01641613924050633,
                        "total_cost": 22914748.38199523
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6111
Profiling... [256/50048]	Loss: 7.2414
Profiling... [384/50048]	Loss: 7.8326
Profiling... [512/50048]	Loss: 5.1997
Profiling... [640/50048]	Loss: 5.2028
Profiling... [768/50048]	Loss: 5.0643
Profiling... [896/50048]	Loss: 4.9883
Profiling... [1024/50048]	Loss: 4.8660
Profiling... [1152/50048]	Loss: 4.5788
Profiling... [1280/50048]	Loss: 4.7066
Profiling... [1408/50048]	Loss: 4.5958
Profiling... [1536/50048]	Loss: 4.6532
Profiling... [1664/50048]	Loss: 4.5497
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0363, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.43672741909181,
                        "time": 2.15605776000001,
                        "accuracy": 0.009988132911392405,
                        "total_cost": 37775839.72372295
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6056
Profiling... [256/50048]	Loss: 7.7377
Profiling... [384/50048]	Loss: 7.4491
Profiling... [512/50048]	Loss: 5.7761
Profiling... [640/50048]	Loss: 4.8602
Profiling... [768/50048]	Loss: 5.4216
Profiling... [896/50048]	Loss: 4.9161
Profiling... [1024/50048]	Loss: 5.4955
Profiling... [1152/50048]	Loss: 4.5821
Profiling... [1280/50048]	Loss: 5.2059
Profiling... [1408/50048]	Loss: 4.9487
Profiling... [1536/50048]	Loss: 4.7756
Profiling... [1664/50048]	Loss: 4.8890
Profile done
epoch 1 train time consumed: 3.31s
Validation Epoch: 0, Average loss: 0.0527, Accuracy: 0.0068
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.75140533928587,
                        "time": 2.3642456869999933,
                        "accuracy": 0.0068235759493670885,
                        "total_cost": 60634335.76398823
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6482
Profiling... [256/50048]	Loss: 8.3274
Profiling... [384/50048]	Loss: 7.0266
Profiling... [512/50048]	Loss: 5.3611
Profiling... [640/50048]	Loss: 5.1616
Profiling... [768/50048]	Loss: 5.2127
Profiling... [896/50048]	Loss: 5.1898
Profiling... [1024/50048]	Loss: 4.8496
Profiling... [1152/50048]	Loss: 6.2557
Profiling... [1280/50048]	Loss: 4.7002
Profiling... [1408/50048]	Loss: 4.9324
Profiling... [1536/50048]	Loss: 5.1349
Profiling... [1664/50048]	Loss: 4.9895
Profile done
epoch 1 train time consumed: 6.47s
Validation Epoch: 0, Average loss: 0.1525, Accuracy: 0.0101
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.55740052103756,
                        "time": 4.797268775999953,
                        "accuracy": 0.010087025316455696,
                        "total_cost": 83227910.05891684
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6388
Profiling... [512/50176]	Loss: 4.6590
Profiling... [768/50176]	Loss: 4.6540
Profiling... [1024/50176]	Loss: 4.6825
Profiling... [1280/50176]	Loss: 4.6100
Profiling... [1536/50176]	Loss: 4.6242
Profiling... [1792/50176]	Loss: 4.6111
Profiling... [2048/50176]	Loss: 4.6386
Profiling... [2304/50176]	Loss: 4.5724
Profiling... [2560/50176]	Loss: 4.5982
Profiling... [2816/50176]	Loss: 4.5471
Profiling... [3072/50176]	Loss: 4.5753
Profiling... [3328/50176]	Loss: 4.5669
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.1996098031437,
                        "time": 2.3712243490000446,
                        "accuracy": 0.009765625,
                        "total_cost": 42492340.3340808
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6321
Profiling... [512/50176]	Loss: 4.6670
Profiling... [768/50176]	Loss: 4.6691
Profiling... [1024/50176]	Loss: 4.6639
Profiling... [1280/50176]	Loss: 4.6270
Profiling... [1536/50176]	Loss: 4.6458
Profiling... [1792/50176]	Loss: 4.6530
Profiling... [2048/50176]	Loss: 4.6366
Profiling... [2304/50176]	Loss: 4.5628
Profiling... [2560/50176]	Loss: 4.6126
Profiling... [2816/50176]	Loss: 4.6160
Profiling... [3072/50176]	Loss: 4.5592
Profiling... [3328/50176]	Loss: 4.5073
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.6493074069597,
                        "time": 2.411838366999973,
                        "accuracy": 0.009765625,
                        "total_cost": 43220143.53663952
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6553
Profiling... [512/50176]	Loss: 4.6634
Profiling... [768/50176]	Loss: 4.6691
Profiling... [1024/50176]	Loss: 4.6973
Profiling... [1280/50176]	Loss: 4.7051
Profiling... [1536/50176]	Loss: 4.6411
Profiling... [1792/50176]	Loss: 4.6776
Profiling... [2048/50176]	Loss: 4.5466
Profiling... [2304/50176]	Loss: 4.5599
Profiling... [2560/50176]	Loss: 4.5487
Profiling... [2816/50176]	Loss: 4.5587
Profiling... [3072/50176]	Loss: 4.5242
Profiling... [3328/50176]	Loss: 4.6609
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.91796832672811,
                        "time": 2.7342760679999856,
                        "accuracy": 0.009765625,
                        "total_cost": 48998227.138559744
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6443
Profiling... [512/50176]	Loss: 4.6592
Profiling... [768/50176]	Loss: 4.6946
Profiling... [1024/50176]	Loss: 4.7078
Profiling... [1280/50176]	Loss: 4.7462
Profiling... [1536/50176]	Loss: 4.7130
Profiling... [1792/50176]	Loss: 4.6280
Profiling... [2048/50176]	Loss: 4.5780
Profiling... [2304/50176]	Loss: 4.5982
Profiling... [2560/50176]	Loss: 4.5991
Profiling... [2816/50176]	Loss: 4.5602
Profiling... [3072/50176]	Loss: 4.5148
Profiling... [3328/50176]	Loss: 4.6385
Profile done
epoch 1 train time consumed: 9.41s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.72895479759808,
                        "time": 7.091297453000038,
                        "accuracy": 0.009765625,
                        "total_cost": 127076050.3577607
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6331
Profiling... [512/50176]	Loss: 4.6608
Profiling... [768/50176]	Loss: 4.6334
Profiling... [1024/50176]	Loss: 4.7142
Profiling... [1280/50176]	Loss: 4.6531
Profiling... [1536/50176]	Loss: 4.6043
Profiling... [1792/50176]	Loss: 4.5704
Profiling... [2048/50176]	Loss: 4.6109
Profiling... [2304/50176]	Loss: 4.5190
Profiling... [2560/50176]	Loss: 4.5303
Profiling... [2816/50176]	Loss: 4.5451
Profiling... [3072/50176]	Loss: 4.5745
Profiling... [3328/50176]	Loss: 4.5942
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.3658874396218,
                        "time": 2.375397100999976,
                        "accuracy": 0.009765625,
                        "total_cost": 42567116.04991957
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6416
Profiling... [512/50176]	Loss: 4.6354
Profiling... [768/50176]	Loss: 4.6800
Profiling... [1024/50176]	Loss: 4.6907
Profiling... [1280/50176]	Loss: 4.6765
Profiling... [1536/50176]	Loss: 4.6669
Profiling... [1792/50176]	Loss: 4.5610
Profiling... [2048/50176]	Loss: 4.5542
Profiling... [2304/50176]	Loss: 4.6121
Profiling... [2560/50176]	Loss: 4.5529
Profiling... [2816/50176]	Loss: 4.5871
Profiling... [3072/50176]	Loss: 4.5998
Profiling... [3328/50176]	Loss: 4.5606
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.7394095336079,
                        "time": 2.404473175000021,
                        "accuracy": 0.009765625,
                        "total_cost": 43088159.29600038
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6056
Profiling... [512/50176]	Loss: 4.6905
Profiling... [768/50176]	Loss: 4.6232
Profiling... [1024/50176]	Loss: 4.6922
Profiling... [1280/50176]	Loss: 4.6696
Profiling... [1536/50176]	Loss: 4.6397
Profiling... [1792/50176]	Loss: 4.5986
Profiling... [2048/50176]	Loss: 4.5931
Profiling... [2304/50176]	Loss: 4.6199
Profiling... [2560/50176]	Loss: 4.5787
Profiling... [2816/50176]	Loss: 4.6419
Profiling... [3072/50176]	Loss: 4.5589
Profiling... [3328/50176]	Loss: 4.5642
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.99411157672456,
                        "time": 2.7132844000000205,
                        "accuracy": 0.009765625,
                        "total_cost": 48622056.448000364
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6273
Profiling... [512/50176]	Loss: 4.6872
Profiling... [768/50176]	Loss: 4.7040
Profiling... [1024/50176]	Loss: 4.6543
Profiling... [1280/50176]	Loss: 4.6778
Profiling... [1536/50176]	Loss: 4.6828
Profiling... [1792/50176]	Loss: 4.6282
Profiling... [2048/50176]	Loss: 4.5778
Profiling... [2304/50176]	Loss: 4.5194
Profiling... [2560/50176]	Loss: 4.5461
Profiling... [2816/50176]	Loss: 4.5524
Profiling... [3072/50176]	Loss: 4.5632
Profiling... [3328/50176]	Loss: 4.5606
Profile done
epoch 1 train time consumed: 9.42s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.86227002379947,
                        "time": 7.015364581000085,
                        "accuracy": 0.009765625,
                        "total_cost": 125715333.29152152
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6164
Profiling... [512/50176]	Loss: 4.6776
Profiling... [768/50176]	Loss: 4.6621
Profiling... [1024/50176]	Loss: 4.6535
Profiling... [1280/50176]	Loss: 4.6508
Profiling... [1536/50176]	Loss: 4.6344
Profiling... [1792/50176]	Loss: 4.6587
Profiling... [2048/50176]	Loss: 4.5941
Profiling... [2304/50176]	Loss: 4.5590
Profiling... [2560/50176]	Loss: 4.6434
Profiling... [2816/50176]	Loss: 4.5789
Profiling... [3072/50176]	Loss: 4.5068
Profiling... [3328/50176]	Loss: 4.5915
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.47800357155579,
                        "time": 2.3773783510000612,
                        "accuracy": 0.009765625,
                        "total_cost": 42602620.049921095
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6451
Profiling... [512/50176]	Loss: 4.6988
Profiling... [768/50176]	Loss: 4.7073
Profiling... [1024/50176]	Loss: 4.6551
Profiling... [1280/50176]	Loss: 4.6557
Profiling... [1536/50176]	Loss: 4.6935
Profiling... [1792/50176]	Loss: 4.6502
Profiling... [2048/50176]	Loss: 4.6217
Profiling... [2304/50176]	Loss: 4.5391
Profiling... [2560/50176]	Loss: 4.5890
Profiling... [2816/50176]	Loss: 4.5484
Profiling... [3072/50176]	Loss: 4.5971
Profiling... [3328/50176]	Loss: 4.5602
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.83989055452759,
                        "time": 2.421774809999988,
                        "accuracy": 0.009765625,
                        "total_cost": 43398204.595199786
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.5864
Profiling... [512/50176]	Loss: 4.6427
Profiling... [768/50176]	Loss: 4.6975
Profiling... [1024/50176]	Loss: 4.6542
Profiling... [1280/50176]	Loss: 4.6533
Profiling... [1536/50176]	Loss: 4.6499
Profiling... [1792/50176]	Loss: 4.6404
Profiling... [2048/50176]	Loss: 4.5837
Profiling... [2304/50176]	Loss: 4.6495
Profiling... [2560/50176]	Loss: 4.5135
Profiling... [2816/50176]	Loss: 4.5526
Profiling... [3072/50176]	Loss: 4.5406
Profiling... [3328/50176]	Loss: 4.6099
Profile done
epoch 1 train time consumed: 3.84s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.05627190716154,
                        "time": 2.708781108999915,
                        "accuracy": 0.009765625,
                        "total_cost": 48541357.47327848
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.5941
Profiling... [512/50176]	Loss: 4.6383
Profiling... [768/50176]	Loss: 4.6569
Profiling... [1024/50176]	Loss: 4.7073
Profiling... [1280/50176]	Loss: 4.6538
Profiling... [1536/50176]	Loss: 4.6988
Profiling... [1792/50176]	Loss: 4.7154
Profiling... [2048/50176]	Loss: 4.5852
Profiling... [2304/50176]	Loss: 4.6057
Profiling... [2560/50176]	Loss: 4.6169
Profiling... [2816/50176]	Loss: 4.5054
Profiling... [3072/50176]	Loss: 4.5812
Profiling... [3328/50176]	Loss: 4.5600
Profile done
epoch 1 train time consumed: 9.37s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.92747804247011,
                        "time": 6.963516905999995,
                        "accuracy": 0.009765625,
                        "total_cost": 124786222.95551991
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6194
Profiling... [512/50176]	Loss: 5.8977
Profiling... [768/50176]	Loss: 5.4832
Profiling... [1024/50176]	Loss: 4.9029
Profiling... [1280/50176]	Loss: 5.0089
Profiling... [1536/50176]	Loss: 4.8599
Profiling... [1792/50176]	Loss: 4.8555
Profiling... [2048/50176]	Loss: 4.7167
Profiling... [2304/50176]	Loss: 4.5952
Profiling... [2560/50176]	Loss: 4.7631
Profiling... [2816/50176]	Loss: 4.6224
Profiling... [3072/50176]	Loss: 4.6169
Profiling... [3328/50176]	Loss: 4.5871
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 0, Average loss: 0.0187, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.58200859589977,
                        "time": 2.3749334300000555,
                        "accuracy": 0.009765625,
                        "total_cost": 42558807.065601
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6057
Profiling... [512/50176]	Loss: 5.5399
Profiling... [768/50176]	Loss: 5.6270
Profiling... [1024/50176]	Loss: 4.9880
Profiling... [1280/50176]	Loss: 4.8143
Profiling... [1536/50176]	Loss: 4.8362
Profiling... [1792/50176]	Loss: 4.8597
Profiling... [2048/50176]	Loss: 4.8921
Profiling... [2304/50176]	Loss: 4.7463
Profiling... [2560/50176]	Loss: 4.6072
Profiling... [2816/50176]	Loss: 4.6596
Profiling... [3072/50176]	Loss: 4.6486
Profiling... [3328/50176]	Loss: 4.6040
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.90633544411708,
                        "time": 2.418137119999983,
                        "accuracy": 0.009765625,
                        "total_cost": 43333017.19039969
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6523
Profiling... [512/50176]	Loss: 5.5865
Profiling... [768/50176]	Loss: 5.4785
Profiling... [1024/50176]	Loss: 4.9897
Profiling... [1280/50176]	Loss: 4.9655
Profiling... [1536/50176]	Loss: 5.0300
Profiling... [1792/50176]	Loss: 4.9093
Profiling... [2048/50176]	Loss: 4.6747
Profiling... [2304/50176]	Loss: 4.6472
Profiling... [2560/50176]	Loss: 4.5820
Profiling... [2816/50176]	Loss: 4.6160
Profiling... [3072/50176]	Loss: 4.6714
Profiling... [3328/50176]	Loss: 4.5550
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.10702487328977,
                        "time": 2.725086129000033,
                        "accuracy": 0.009765625,
                        "total_cost": 48833543.43168059
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6398
Profiling... [512/50176]	Loss: 5.6517
Profiling... [768/50176]	Loss: 5.7225
Profiling... [1024/50176]	Loss: 5.1810
Profiling... [1280/50176]	Loss: 4.8578
Profiling... [1536/50176]	Loss: 4.8002
Profiling... [1792/50176]	Loss: 4.8907
Profiling... [2048/50176]	Loss: 4.7334
Profiling... [2304/50176]	Loss: 4.6643
Profiling... [2560/50176]	Loss: 4.7245
Profiling... [2816/50176]	Loss: 4.6590
Profiling... [3072/50176]	Loss: 4.6305
Profiling... [3328/50176]	Loss: 4.7783
Profile done
epoch 1 train time consumed: 9.42s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.97065380309041,
                        "time": 7.1450783389999515,
                        "accuracy": 0.009765625,
                        "total_cost": 128039803.83487913
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6085
Profiling... [512/50176]	Loss: 5.9085
Profiling... [768/50176]	Loss: 5.0358
Profiling... [1024/50176]	Loss: 5.2855
Profiling... [1280/50176]	Loss: 4.7968
Profiling... [1536/50176]	Loss: 4.8771
Profiling... [1792/50176]	Loss: 4.7786
Profiling... [2048/50176]	Loss: 4.6954
Profiling... [2304/50176]	Loss: 4.6291
Profiling... [2560/50176]	Loss: 4.9850
Profiling... [2816/50176]	Loss: 4.8237
Profiling... [3072/50176]	Loss: 4.9342
Profiling... [3328/50176]	Loss: 4.5852
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.67564654665793,
                        "time": 2.386431668,
                        "accuracy": 0.009765625,
                        "total_cost": 42764855.49056
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6549
Profiling... [512/50176]	Loss: 5.5601
Profiling... [768/50176]	Loss: 5.5213
Profiling... [1024/50176]	Loss: 5.1414
Profiling... [1280/50176]	Loss: 4.7803
Profiling... [1536/50176]	Loss: 4.7378
Profiling... [1792/50176]	Loss: 5.0009
Profiling... [2048/50176]	Loss: 4.6751
Profiling... [2304/50176]	Loss: 4.7235
Profiling... [2560/50176]	Loss: 4.7186
Profiling... [2816/50176]	Loss: 4.7186
Profiling... [3072/50176]	Loss: 4.6743
Profiling... [3328/50176]	Loss: 4.6307
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.95540715518233,
                        "time": 2.413922462999949,
                        "accuracy": 0.009765625,
                        "total_cost": 43257490.53695909
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6509
Profiling... [512/50176]	Loss: 5.5076
Profiling... [768/50176]	Loss: 5.4632
Profiling... [1024/50176]	Loss: 5.0904
Profiling... [1280/50176]	Loss: 4.8581
Profiling... [1536/50176]	Loss: 4.7411
Profiling... [1792/50176]	Loss: 4.7130
Profiling... [2048/50176]	Loss: 4.5978
Profiling... [2304/50176]	Loss: 4.6809
Profiling... [2560/50176]	Loss: 4.6845
Profiling... [2816/50176]	Loss: 4.6098
Profiling... [3072/50176]	Loss: 4.6611
Profiling... [3328/50176]	Loss: 4.8449
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.14162125464381,
                        "time": 2.7194030439999324,
                        "accuracy": 0.009765625,
                        "total_cost": 48731702.54847879
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6263
Profiling... [512/50176]	Loss: 5.6541
Profiling... [768/50176]	Loss: 5.2970
Profiling... [1024/50176]	Loss: 5.0156
Profiling... [1280/50176]	Loss: 4.7002
Profiling... [1536/50176]	Loss: 5.0211
Profiling... [1792/50176]	Loss: 5.0117
Profiling... [2048/50176]	Loss: 4.7790
Profiling... [2304/50176]	Loss: 4.6431
Profiling... [2560/50176]	Loss: 4.6813
Profiling... [2816/50176]	Loss: 4.6932
Profiling... [3072/50176]	Loss: 4.5978
Profiling... [3328/50176]	Loss: 4.5904
Profile done
epoch 1 train time consumed: 9.30s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.9894476205857,
                        "time": 6.918654218000029,
                        "accuracy": 0.009765625,
                        "total_cost": 123982283.58656052
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6461
Profiling... [512/50176]	Loss: 5.5301
Profiling... [768/50176]	Loss: 5.5187
Profiling... [1024/50176]	Loss: 5.3916
Profiling... [1280/50176]	Loss: 4.8479
Profiling... [1536/50176]	Loss: 4.6880
Profiling... [1792/50176]	Loss: 4.7330
Profiling... [2048/50176]	Loss: 4.8473
Profiling... [2304/50176]	Loss: 4.6183
Profiling... [2560/50176]	Loss: 4.7461
Profiling... [2816/50176]	Loss: 4.6114
Profiling... [3072/50176]	Loss: 4.6141
Profiling... [3328/50176]	Loss: 4.5723
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.72846510256133,
                        "time": 2.369466182999986,
                        "accuracy": 0.009765625,
                        "total_cost": 42460833.99935974
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6394
Profiling... [512/50176]	Loss: 5.6777
Profiling... [768/50176]	Loss: 5.4336
Profiling... [1024/50176]	Loss: 5.0875
Profiling... [1280/50176]	Loss: 4.7419
Profiling... [1536/50176]	Loss: 4.6093
Profiling... [1792/50176]	Loss: 4.6329
Profiling... [2048/50176]	Loss: 4.6617
Profiling... [2304/50176]	Loss: 4.8125
Profiling... [2560/50176]	Loss: 4.6239
Profiling... [2816/50176]	Loss: 4.5998
Profiling... [3072/50176]	Loss: 4.5746
Profiling... [3328/50176]	Loss: 4.6309
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.97328488590233,
                        "time": 2.424330286999975,
                        "accuracy": 0.009765625,
                        "total_cost": 43443998.743039556
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6465
Profiling... [512/50176]	Loss: 5.5223
Profiling... [768/50176]	Loss: 5.5946
Profiling... [1024/50176]	Loss: 4.9661
Profiling... [1280/50176]	Loss: 5.0804
Profiling... [1536/50176]	Loss: 4.8896
Profiling... [1792/50176]	Loss: 4.7492
Profiling... [2048/50176]	Loss: 4.7293
Profiling... [2304/50176]	Loss: 4.6761
Profiling... [2560/50176]	Loss: 4.8346
Profiling... [2816/50176]	Loss: 4.7464
Profiling... [3072/50176]	Loss: 4.6690
Profiling... [3328/50176]	Loss: 4.7029
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.14924257282459,
                        "time": 2.717008218999922,
                        "accuracy": 0.009765625,
                        "total_cost": 48688787.284478605
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6178
Profiling... [512/50176]	Loss: 5.6538
Profiling... [768/50176]	Loss: 5.2600
Profiling... [1024/50176]	Loss: 4.9753
Profiling... [1280/50176]	Loss: 4.7592
Profiling... [1536/50176]	Loss: 4.9308
Profiling... [1792/50176]	Loss: 4.8225
Profiling... [2048/50176]	Loss: 4.9576
Profiling... [2304/50176]	Loss: 4.8037
Profiling... [2560/50176]	Loss: 4.6822
Profiling... [2816/50176]	Loss: 4.6056
Profiling... [3072/50176]	Loss: 4.5648
Profiling... [3328/50176]	Loss: 4.5458
Profile done
epoch 1 train time consumed: 9.85s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.02565153524608,
                        "time": 7.096646839000073,
                        "accuracy": 0.009765625,
                        "total_cost": 127171911.35488132
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6445
Profiling... [512/50176]	Loss: 7.4174
Profiling... [768/50176]	Loss: 6.6029
Profiling... [1024/50176]	Loss: 5.5687
Profiling... [1280/50176]	Loss: 5.2431
Profiling... [1536/50176]	Loss: 4.9396
Profiling... [1792/50176]	Loss: 5.3749
Profiling... [2048/50176]	Loss: 5.0308
Profiling... [2304/50176]	Loss: 4.9812
Profiling... [2560/50176]	Loss: 4.9083
Profiling... [2816/50176]	Loss: 4.6756
Profiling... [3072/50176]	Loss: 4.6247
Profiling... [3328/50176]	Loss: 4.6362
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0104
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.73638572902082,
                        "time": 2.373487544999989,
                        "accuracy": 0.01044921875,
                        "total_cost": 39750370.84710263
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6169
Profiling... [512/50176]	Loss: 8.1873
Profiling... [768/50176]	Loss: 6.5792
Profiling... [1024/50176]	Loss: 5.0763
Profiling... [1280/50176]	Loss: 5.0599
Profiling... [1536/50176]	Loss: 4.7396
Profiling... [1792/50176]	Loss: 4.8648
Profiling... [2048/50176]	Loss: 4.7001
Profiling... [2304/50176]	Loss: 4.6413
Profiling... [2560/50176]	Loss: 4.7596
Profiling... [2816/50176]	Loss: 4.6280
Profiling... [3072/50176]	Loss: 4.6998
Profiling... [3328/50176]	Loss: 4.5796
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 0, Average loss: 0.0264, Accuracy: 0.0112
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.00117527277426,
                        "time": 2.409115134999979,
                        "accuracy": 0.01123046875,
                        "total_cost": 37540298.45147794
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6432
Profiling... [512/50176]	Loss: 7.2424
Profiling... [768/50176]	Loss: 5.6453
Profiling... [1024/50176]	Loss: 5.1299
Profiling... [1280/50176]	Loss: 4.9183
Profiling... [1536/50176]	Loss: 4.7069
Profiling... [1792/50176]	Loss: 4.5976
Profiling... [2048/50176]	Loss: 5.2815
Profiling... [2304/50176]	Loss: 4.7769
Profiling... [2560/50176]	Loss: 5.7826
Profiling... [2816/50176]	Loss: 5.1026
Profiling... [3072/50176]	Loss: 4.9723
Profiling... [3328/50176]	Loss: 4.9093
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.16664399686822,
                        "time": 2.709513496999989,
                        "accuracy": 0.009765625,
                        "total_cost": 48554481.8662398
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6256
Profiling... [512/50176]	Loss: 7.4807
Profiling... [768/50176]	Loss: 6.8462
Profiling... [1024/50176]	Loss: 5.1321
Profiling... [1280/50176]	Loss: 5.6571
Profiling... [1536/50176]	Loss: 5.1427
Profiling... [1792/50176]	Loss: 4.7303
Profiling... [2048/50176]	Loss: 4.8430
Profiling... [2304/50176]	Loss: 4.9365
Profiling... [2560/50176]	Loss: 4.6790
Profiling... [2816/50176]	Loss: 4.9692
Profiling... [3072/50176]	Loss: 4.6782
Profiling... [3328/50176]	Loss: 4.6374
Profile done
epoch 1 train time consumed: 9.42s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.04011857435928,
                        "time": 7.076797179000096,
                        "accuracy": 0.009765625,
                        "total_cost": 126816205.44768171
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6162
Profiling... [512/50176]	Loss: 7.5333
Profiling... [768/50176]	Loss: 6.0012
Profiling... [1024/50176]	Loss: 5.5901
Profiling... [1280/50176]	Loss: 5.1315
Profiling... [1536/50176]	Loss: 4.8228
Profiling... [1792/50176]	Loss: 4.6399
Profiling... [2048/50176]	Loss: 4.8720
Profiling... [2304/50176]	Loss: 4.6449
Profiling... [2560/50176]	Loss: 4.7746
Profiling... [2816/50176]	Loss: 4.7162
Profiling... [3072/50176]	Loss: 4.6832
Profiling... [3328/50176]	Loss: 4.6707
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 0, Average loss: 0.0190, Accuracy: 0.0102
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.77316394180997,
                        "time": 2.3657564069999353,
                        "accuracy": 0.01015625,
                        "total_cost": 40763802.705229655
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6465
Profiling... [512/50176]	Loss: 7.6595
Profiling... [768/50176]	Loss: 6.3459
Profiling... [1024/50176]	Loss: 5.6720
Profiling... [1280/50176]	Loss: 5.1080
Profiling... [1536/50176]	Loss: 4.6891
Profiling... [1792/50176]	Loss: 4.8430
Profiling... [2048/50176]	Loss: 4.7577
Profiling... [2304/50176]	Loss: 4.7973
Profiling... [2560/50176]	Loss: 4.6537
Profiling... [2816/50176]	Loss: 5.0032
Profiling... [3072/50176]	Loss: 4.7275
Profiling... [3328/50176]	Loss: 4.7216
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 0, Average loss: 0.0181, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.00380476074068,
                        "time": 2.4183197629999995,
                        "accuracy": 0.009765625,
                        "total_cost": 43336290.15295999
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6136
Profiling... [512/50176]	Loss: 7.5824
Profiling... [768/50176]	Loss: 6.1248
Profiling... [1024/50176]	Loss: 5.5700
Profiling... [1280/50176]	Loss: 5.7409
Profiling... [1536/50176]	Loss: 4.9233
Profiling... [1792/50176]	Loss: 4.8885
Profiling... [2048/50176]	Loss: 4.9354
Profiling... [2304/50176]	Loss: 5.1937
Profiling... [2560/50176]	Loss: 4.8362
Profiling... [2816/50176]	Loss: 4.7452
Profiling... [3072/50176]	Loss: 5.0346
Profiling... [3328/50176]	Loss: 4.6987
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 0, Average loss: 0.1217, Accuracy: 0.0096
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.15167479410813,
                        "time": 2.7087503819999483,
                        "accuracy": 0.0095703125,
                        "total_cost": 49531435.556570485
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6646
Profiling... [512/50176]	Loss: 7.6443
Profiling... [768/50176]	Loss: 6.6419
Profiling... [1024/50176]	Loss: 5.8174
Profiling... [1280/50176]	Loss: 5.0445
Profiling... [1536/50176]	Loss: 5.0033
Profiling... [1792/50176]	Loss: 4.8508
Profiling... [2048/50176]	Loss: 4.7924
Profiling... [2304/50176]	Loss: 4.7603
Profiling... [2560/50176]	Loss: 4.7290
Profiling... [2816/50176]	Loss: 5.4170
Profiling... [3072/50176]	Loss: 4.6177
Profiling... [3328/50176]	Loss: 4.6666
Profile done
epoch 1 train time consumed: 9.42s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.05211367948468,
                        "time": 7.084969075999993,
                        "accuracy": 0.009765625,
                        "total_cost": 126962645.84191987
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6300
Profiling... [512/50176]	Loss: 7.6979
Profiling... [768/50176]	Loss: 6.1783
Profiling... [1024/50176]	Loss: 5.2510
Profiling... [1280/50176]	Loss: 5.2064
Profiling... [1536/50176]	Loss: 4.9467
Profiling... [1792/50176]	Loss: 4.8162
Profiling... [2048/50176]	Loss: 4.8125
Profiling... [2304/50176]	Loss: 4.6812
Profiling... [2560/50176]	Loss: 4.7280
Profiling... [2816/50176]	Loss: 4.6644
Profiling... [3072/50176]	Loss: 4.6147
Profiling... [3328/50176]	Loss: 4.6168
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0102
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.84424134546668,
                        "time": 2.3760623590000023,
                        "accuracy": 0.01015625,
                        "total_cost": 40941382.185846195
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6415
Profiling... [512/50176]	Loss: 7.7690
Profiling... [768/50176]	Loss: 6.4300
Profiling... [1024/50176]	Loss: 5.2079
Profiling... [1280/50176]	Loss: 5.4457
Profiling... [1536/50176]	Loss: 5.2647
Profiling... [1792/50176]	Loss: 4.8840
Profiling... [2048/50176]	Loss: 4.7082
Profiling... [2304/50176]	Loss: 4.5852
Profiling... [2560/50176]	Loss: 4.6064
Profiling... [2816/50176]	Loss: 4.7449
Profiling... [3072/50176]	Loss: 4.7289
Profiling... [3328/50176]	Loss: 4.7991
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0216, Accuracy: 0.0103
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.04899294316004,
                        "time": 2.418119752999928,
                        "accuracy": 0.01025390625,
                        "total_cost": 41269243.78453211
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6441
Profiling... [512/50176]	Loss: 7.3190
Profiling... [768/50176]	Loss: 6.1108
Profiling... [1024/50176]	Loss: 5.0121
Profiling... [1280/50176]	Loss: 5.1386
Profiling... [1536/50176]	Loss: 4.6910
Profiling... [1792/50176]	Loss: 4.6231
Profiling... [2048/50176]	Loss: 4.6522
Profiling... [2304/50176]	Loss: 4.9168
Profiling... [2560/50176]	Loss: 4.6913
Profiling... [2816/50176]	Loss: 4.6646
Profiling... [3072/50176]	Loss: 4.7088
Profiling... [3328/50176]	Loss: 4.6237
Profile done
epoch 1 train time consumed: 4.06s
Validation Epoch: 0, Average loss: 0.0410, Accuracy: 0.0089
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.1908619688463,
                        "time": 2.713331749999952,
                        "accuracy": 0.00888671875,
                        "total_cost": 53431763.69230676
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6099
Profiling... [512/50176]	Loss: 7.5596
Profiling... [768/50176]	Loss: 6.0152
Profiling... [1024/50176]	Loss: 5.2394
Profiling... [1280/50176]	Loss: 4.8564
Profiling... [1536/50176]	Loss: 4.9407
Profiling... [1792/50176]	Loss: 5.3629
Profiling... [2048/50176]	Loss: 5.3336
Profiling... [2304/50176]	Loss: 4.8994
Profiling... [2560/50176]	Loss: 5.0780
Profiling... [2816/50176]	Loss: 4.6320
Profiling... [3072/50176]	Loss: 4.8000
Profiling... [3328/50176]	Loss: 4.9920
Profile done
epoch 1 train time consumed: 9.42s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.0774362249533,
                        "time": 6.9418568309999955,
                        "accuracy": 0.009765625,
                        "total_cost": 124398074.41151991
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6362
Profiling... [1024/50176]	Loss: 4.6462
Profiling... [1536/50176]	Loss: 4.6313
Profiling... [2048/50176]	Loss: 4.6509
Profiling... [2560/50176]	Loss: 4.6525
Profiling... [3072/50176]	Loss: 4.5778
Profiling... [3584/50176]	Loss: 4.5697
Profiling... [4096/50176]	Loss: 4.5402
Profiling... [4608/50176]	Loss: 4.5360
Profiling... [5120/50176]	Loss: 4.6023
Profiling... [5632/50176]	Loss: 4.5453
Profiling... [6144/50176]	Loss: 4.5670
Profiling... [6656/50176]	Loss: 4.5315
Profile done
epoch 1 train time consumed: 6.70s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.06666396653279,
                        "time": 4.502187491000086,
                        "accuracy": 0.009765625,
                        "total_cost": 80679199.83872154
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6286
Profiling... [1024/50176]	Loss: 4.6195
Profiling... [1536/50176]	Loss: 4.6533
Profiling... [2048/50176]	Loss: 4.6600
Profiling... [2560/50176]	Loss: 4.5832
Profiling... [3072/50176]	Loss: 4.5571
Profiling... [3584/50176]	Loss: 4.5411
Profiling... [4096/50176]	Loss: 4.5151
Profiling... [4608/50176]	Loss: 4.5097
Profiling... [5120/50176]	Loss: 4.5093
Profiling... [5632/50176]	Loss: 4.4934
Profiling... [6144/50176]	Loss: 4.4893
Profiling... [6656/50176]	Loss: 4.4705
Profile done
epoch 1 train time consumed: 6.56s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.37678251169717,
                        "time": 4.620630333999998,
                        "accuracy": 0.009765625,
                        "total_cost": 82801695.58527996
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6262
Profiling... [1024/50176]	Loss: 4.6209
Profiling... [1536/50176]	Loss: 4.6440
Profiling... [2048/50176]	Loss: 4.6797
Profiling... [2560/50176]	Loss: 4.6261
Profiling... [3072/50176]	Loss: 4.5879
Profiling... [3584/50176]	Loss: 4.6113
Profiling... [4096/50176]	Loss: 4.5437
Profiling... [4608/50176]	Loss: 4.5416
Profiling... [5120/50176]	Loss: 4.5601
Profiling... [5632/50176]	Loss: 4.5134
Profiling... [6144/50176]	Loss: 4.5591
Profiling... [6656/50176]	Loss: 4.5193
Profile done
epoch 1 train time consumed: 7.34s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.52471121239616,
                        "time": 5.232742048999967,
                        "accuracy": 0.009765625,
                        "total_cost": 93770737.5180794
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6505
Profiling... [1024/50176]	Loss: 4.6595
Profiling... [1536/50176]	Loss: 4.6533
Profiling... [2048/50176]	Loss: 4.6317
Profiling... [2560/50176]	Loss: 4.5920
Profiling... [3072/50176]	Loss: 4.5714
Profiling... [3584/50176]	Loss: 4.5701
Profiling... [4096/50176]	Loss: 4.5356
Profiling... [4608/50176]	Loss: 4.5510
Profiling... [5120/50176]	Loss: 4.5583
Profiling... [5632/50176]	Loss: 4.5571
Profiling... [6144/50176]	Loss: 4.5707
Profiling... [6656/50176]	Loss: 4.4939
Profile done
epoch 1 train time consumed: 17.54s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.19052589433808,
                        "time": 12.984752833000016,
                        "accuracy": 0.009765625,
                        "total_cost": 232686770.7673603
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6388
Profiling... [1024/50176]	Loss: 4.6060
Profiling... [1536/50176]	Loss: 4.6617
Profiling... [2048/50176]	Loss: 4.6293
Profiling... [2560/50176]	Loss: 4.6122
Profiling... [3072/50176]	Loss: 4.5711
Profiling... [3584/50176]	Loss: 4.5458
Profiling... [4096/50176]	Loss: 4.5200
Profiling... [4608/50176]	Loss: 4.5855
Profiling... [5120/50176]	Loss: 4.4866
Profiling... [5632/50176]	Loss: 4.5544
Profiling... [6144/50176]	Loss: 4.5146
Profiling... [6656/50176]	Loss: 4.5425
Profile done
epoch 1 train time consumed: 6.57s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.12158199435228,
                        "time": 4.506866060999982,
                        "accuracy": 0.009765625,
                        "total_cost": 80763039.81311968
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6432
Profiling... [1024/50176]	Loss: 4.6386
Profiling... [1536/50176]	Loss: 4.6344
Profiling... [2048/50176]	Loss: 4.6566
Profiling... [2560/50176]	Loss: 4.6400
Profiling... [3072/50176]	Loss: 4.6158
Profiling... [3584/50176]	Loss: 4.5283
Profiling... [4096/50176]	Loss: 4.5939
Profiling... [4608/50176]	Loss: 4.5400
Profiling... [5120/50176]	Loss: 4.5747
Profiling... [5632/50176]	Loss: 4.5073
Profiling... [6144/50176]	Loss: 4.5309
Profiling... [6656/50176]	Loss: 4.4840
Profile done
epoch 1 train time consumed: 6.61s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.40948566419479,
                        "time": 4.612109370000098,
                        "accuracy": 0.009765625,
                        "total_cost": 82648999.91040176
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6129
Profiling... [1024/50176]	Loss: 4.6580
Profiling... [1536/50176]	Loss: 4.6719
Profiling... [2048/50176]	Loss: 4.6428
Profiling... [2560/50176]	Loss: 4.5768
Profiling... [3072/50176]	Loss: 4.5848
Profiling... [3584/50176]	Loss: 4.5618
Profiling... [4096/50176]	Loss: 4.5112
Profiling... [4608/50176]	Loss: 4.5274
Profiling... [5120/50176]	Loss: 4.5949
Profiling... [5632/50176]	Loss: 4.5046
Profiling... [6144/50176]	Loss: 4.5457
Profiling... [6656/50176]	Loss: 4.5070
Profile done
epoch 1 train time consumed: 7.36s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.54765008686398,
                        "time": 5.2294631579999304,
                        "accuracy": 0.009765625,
                        "total_cost": 93711979.79135875
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6480
Profiling... [1024/50176]	Loss: 4.6421
Profiling... [1536/50176]	Loss: 4.6508
Profiling... [2048/50176]	Loss: 4.6711
Profiling... [2560/50176]	Loss: 4.6799
Profiling... [3072/50176]	Loss: 4.5676
Profiling... [3584/50176]	Loss: 4.5341
Profiling... [4096/50176]	Loss: 4.5116
Profiling... [4608/50176]	Loss: 4.5641
Profiling... [5120/50176]	Loss: 4.5576
Profiling... [5632/50176]	Loss: 4.5128
Profiling... [6144/50176]	Loss: 4.5339
Profiling... [6656/50176]	Loss: 4.4879
Profile done
epoch 1 train time consumed: 17.40s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.2676584904095,
                        "time": 12.991829975999963,
                        "accuracy": 0.009765625,
                        "total_cost": 232813593.16991934
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6239
Profiling... [1024/50176]	Loss: 4.6583
Profiling... [1536/50176]	Loss: 4.6529
Profiling... [2048/50176]	Loss: 4.6600
Profiling... [2560/50176]	Loss: 4.6141
Profiling... [3072/50176]	Loss: 4.6051
Profiling... [3584/50176]	Loss: 4.5869
Profiling... [4096/50176]	Loss: 4.5771
Profiling... [4608/50176]	Loss: 4.5758
Profiling... [5120/50176]	Loss: 4.5191
Profiling... [5632/50176]	Loss: 4.5373
Profiling... [6144/50176]	Loss: 4.5900
Profiling... [6656/50176]	Loss: 4.4965
Profile done
epoch 1 train time consumed: 6.67s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.20363740804616,
                        "time": 4.499976383999979,
                        "accuracy": 0.009765625,
                        "total_cost": 80639576.80127962
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6208
Profiling... [1024/50176]	Loss: 4.6249
Profiling... [1536/50176]	Loss: 4.6266
Profiling... [2048/50176]	Loss: 4.6006
Profiling... [2560/50176]	Loss: 4.6762
Profiling... [3072/50176]	Loss: 4.6119
Profiling... [3584/50176]	Loss: 4.5584
Profiling... [4096/50176]	Loss: 4.5557
Profiling... [4608/50176]	Loss: 4.5845
Profiling... [5120/50176]	Loss: 4.5406
Profiling... [5632/50176]	Loss: 4.5347
Profiling... [6144/50176]	Loss: 4.5712
Profiling... [6656/50176]	Loss: 4.5013
Profile done
epoch 1 train time consumed: 6.68s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.46531239914916,
                        "time": 4.621694379000019,
                        "accuracy": 0.009765625,
                        "total_cost": 82820763.27168033
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6266
Profiling... [1024/50176]	Loss: 4.6269
Profiling... [1536/50176]	Loss: 4.6102
Profiling... [2048/50176]	Loss: 4.6740
Profiling... [2560/50176]	Loss: 4.5963
Profiling... [3072/50176]	Loss: 4.5961
Profiling... [3584/50176]	Loss: 4.5763
Profiling... [4096/50176]	Loss: 4.5985
Profiling... [4608/50176]	Loss: 4.5474
Profiling... [5120/50176]	Loss: 4.4807
Profiling... [5632/50176]	Loss: 4.4852
Profiling... [6144/50176]	Loss: 4.5151
Profiling... [6656/50176]	Loss: 4.5244
Profile done
epoch 1 train time consumed: 7.41s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.57928229657792,
                        "time": 5.228691151000021,
                        "accuracy": 0.009765625,
                        "total_cost": 93698145.42592038
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6241
Profiling... [1024/50176]	Loss: 4.6536
Profiling... [1536/50176]	Loss: 4.6274
Profiling... [2048/50176]	Loss: 4.6194
Profiling... [2560/50176]	Loss: 4.6138
Profiling... [3072/50176]	Loss: 4.5986
Profiling... [3584/50176]	Loss: 4.5909
Profiling... [4096/50176]	Loss: 4.5546
Profiling... [4608/50176]	Loss: 4.5733
Profiling... [5120/50176]	Loss: 4.5456
Profiling... [5632/50176]	Loss: 4.5429
Profiling... [6144/50176]	Loss: 4.4896
Profiling... [6656/50176]	Loss: 4.5231
Profile done
epoch 1 train time consumed: 17.46s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.29401582500226,
                        "time": 13.016085229000055,
                        "accuracy": 0.009765625,
                        "total_cost": 233248247.303681
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6473
Profiling... [1024/50176]	Loss: 5.5295
Profiling... [1536/50176]	Loss: 5.3914
Profiling... [2048/50176]	Loss: 4.8894
Profiling... [2560/50176]	Loss: 4.7660
Profiling... [3072/50176]	Loss: 4.7017
Profiling... [3584/50176]	Loss: 4.6482
Profiling... [4096/50176]	Loss: 4.7049
Profiling... [4608/50176]	Loss: 4.6054
Profiling... [5120/50176]	Loss: 4.6558
Profiling... [5632/50176]	Loss: 4.6912
Profiling... [6144/50176]	Loss: 4.6170
Profiling... [6656/50176]	Loss: 4.5772
Profile done
epoch 1 train time consumed: 6.57s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.24057432500267,
                        "time": 4.512008852000008,
                        "accuracy": 0.009765625,
                        "total_cost": 80855198.62784015
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6393
Profiling... [1024/50176]	Loss: 5.6366
Profiling... [1536/50176]	Loss: 5.1889
Profiling... [2048/50176]	Loss: 4.9287
Profiling... [2560/50176]	Loss: 4.7493
Profiling... [3072/50176]	Loss: 4.7431
Profiling... [3584/50176]	Loss: 4.8143
Profiling... [4096/50176]	Loss: 4.7083
Profiling... [4608/50176]	Loss: 4.6871
Profiling... [5120/50176]	Loss: 4.6102
Profiling... [5632/50176]	Loss: 4.5680
Profiling... [6144/50176]	Loss: 4.7212
Profiling... [6656/50176]	Loss: 4.5696
Profile done
epoch 1 train time consumed: 6.61s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.49292439531118,
                        "time": 4.61085983199996,
                        "accuracy": 0.009765625,
                        "total_cost": 82626608.18943928
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6285
Profiling... [1024/50176]	Loss: 5.5670
Profiling... [1536/50176]	Loss: 5.2560
Profiling... [2048/50176]	Loss: 4.6610
Profiling... [2560/50176]	Loss: 4.7365
Profiling... [3072/50176]	Loss: 4.7953
Profiling... [3584/50176]	Loss: 4.7530
Profiling... [4096/50176]	Loss: 4.6313
Profiling... [4608/50176]	Loss: 4.6070
Profiling... [5120/50176]	Loss: 4.6090
Profiling... [5632/50176]	Loss: 4.6653
Profiling... [6144/50176]	Loss: 4.5518
Profiling... [6656/50176]	Loss: 4.7132
Profile done
epoch 1 train time consumed: 7.43s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.60888959152176,
                        "time": 5.222918255999957,
                        "accuracy": 0.009765625,
                        "total_cost": 93594695.14751923
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6193
Profiling... [1024/50176]	Loss: 5.5372
Profiling... [1536/50176]	Loss: 5.1473
Profiling... [2048/50176]	Loss: 4.9455
Profiling... [2560/50176]	Loss: 4.8554
Profiling... [3072/50176]	Loss: 4.9170
Profiling... [3584/50176]	Loss: 4.8381
Profiling... [4096/50176]	Loss: 4.8703
Profiling... [4608/50176]	Loss: 4.7178
Profiling... [5120/50176]	Loss: 4.7195
Profiling... [5632/50176]	Loss: 4.6301
Profiling... [6144/50176]	Loss: 4.5476
Profiling... [6656/50176]	Loss: 4.6004
Profile done
epoch 1 train time consumed: 17.45s
Validation Epoch: 0, Average loss: 0.0095, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.32792072360209,
                        "time": 13.03988511600005,
                        "accuracy": 0.009765625,
                        "total_cost": 233674741.27872092
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6125
Profiling... [1024/50176]	Loss: 5.6451
Profiling... [1536/50176]	Loss: 5.2364
Profiling... [2048/50176]	Loss: 4.8312
Profiling... [2560/50176]	Loss: 4.6216
Profiling... [3072/50176]	Loss: 4.8787
Profiling... [3584/50176]	Loss: 4.6474
Profiling... [4096/50176]	Loss: 4.7077
Profiling... [4608/50176]	Loss: 4.6739
Profiling... [5120/50176]	Loss: 4.7092
Profiling... [5632/50176]	Loss: 4.6253
Profiling... [6144/50176]	Loss: 4.5871
Profiling... [6656/50176]	Loss: 4.5447
Profile done
epoch 1 train time consumed: 6.48s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.29402424963101,
                        "time": 4.494714117000058,
                        "accuracy": 0.009765625,
                        "total_cost": 80545276.97664103
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6034
Profiling... [1024/50176]	Loss: 5.8186
Profiling... [1536/50176]	Loss: 5.0072
Profiling... [2048/50176]	Loss: 4.8567
Profiling... [2560/50176]	Loss: 4.8532
Profiling... [3072/50176]	Loss: 4.7072
Profiling... [3584/50176]	Loss: 4.8442
Profiling... [4096/50176]	Loss: 4.7411
Profiling... [4608/50176]	Loss: 4.9956
Profiling... [5120/50176]	Loss: 4.9622
Profiling... [5632/50176]	Loss: 4.7446
Profiling... [6144/50176]	Loss: 4.7169
Profiling... [6656/50176]	Loss: 4.6543
Profile done
epoch 1 train time consumed: 6.69s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.52221500289176,
                        "time": 4.616098951999902,
                        "accuracy": 0.009765625,
                        "total_cost": 82720493.21983825
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6301
Profiling... [1024/50176]	Loss: 5.5496
Profiling... [1536/50176]	Loss: 5.3638
Profiling... [2048/50176]	Loss: 4.8832
Profiling... [2560/50176]	Loss: 4.7941
Profiling... [3072/50176]	Loss: 4.6356
Profiling... [3584/50176]	Loss: 4.5881
Profiling... [4096/50176]	Loss: 4.8436
Profiling... [4608/50176]	Loss: 4.7068
Profiling... [5120/50176]	Loss: 4.6169
Profiling... [5632/50176]	Loss: 4.6012
Profiling... [6144/50176]	Loss: 4.6129
Profiling... [6656/50176]	Loss: 4.6919
Profile done
epoch 1 train time consumed: 7.40s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.63000867016446,
                        "time": 5.19061779499998,
                        "accuracy": 0.009765625,
                        "total_cost": 93015870.88639964
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6319
Profiling... [1024/50176]	Loss: 5.5601
Profiling... [1536/50176]	Loss: 5.2088
Profiling... [2048/50176]	Loss: 4.8024
Profiling... [2560/50176]	Loss: 4.7162
Profiling... [3072/50176]	Loss: 4.6668
Profiling... [3584/50176]	Loss: 4.8667
Profiling... [4096/50176]	Loss: 4.7880
Profiling... [4608/50176]	Loss: 4.6561
Profiling... [5120/50176]	Loss: 4.6708
Profiling... [5632/50176]	Loss: 4.6750
Profiling... [6144/50176]	Loss: 4.6143
Profiling... [6656/50176]	Loss: 4.6597
Profile done
epoch 1 train time consumed: 17.39s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.36743866831289,
                        "time": 12.997470811000085,
                        "accuracy": 0.009765625,
                        "total_cost": 232914676.93312153
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6275
Profiling... [1024/50176]	Loss: 5.6942
Profiling... [1536/50176]	Loss: 4.9931
Profiling... [2048/50176]	Loss: 4.7562
Profiling... [2560/50176]	Loss: 4.6330
Profiling... [3072/50176]	Loss: 4.7910
Profiling... [3584/50176]	Loss: 4.6628
Profiling... [4096/50176]	Loss: 4.5977
Profiling... [4608/50176]	Loss: 4.5923
Profiling... [5120/50176]	Loss: 4.5582
Profiling... [5632/50176]	Loss: 4.5361
Profiling... [6144/50176]	Loss: 4.5856
Profiling... [6656/50176]	Loss: 4.5530
Profile done
epoch 1 train time consumed: 6.57s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.31316381204306,
                        "time": 4.501680438999983,
                        "accuracy": 0.009765625,
                        "total_cost": 80670113.4668797
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6140
Profiling... [1024/50176]	Loss: 5.7806
Profiling... [1536/50176]	Loss: 5.1340
Profiling... [2048/50176]	Loss: 4.8279
Profiling... [2560/50176]	Loss: 4.6840
Profiling... [3072/50176]	Loss: 4.7796
Profiling... [3584/50176]	Loss: 4.7858
Profiling... [4096/50176]	Loss: 4.7523
Profiling... [4608/50176]	Loss: 4.6969
Profiling... [5120/50176]	Loss: 4.6399
Profiling... [5632/50176]	Loss: 4.6260
Profiling... [6144/50176]	Loss: 4.5736
Profiling... [6656/50176]	Loss: 4.5196
Profile done
epoch 1 train time consumed: 6.67s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.53440669177154,
                        "time": 4.61622515199997,
                        "accuracy": 0.009765625,
                        "total_cost": 82722754.72383946
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6337
Profiling... [1024/50176]	Loss: 5.6019
Profiling... [1536/50176]	Loss: 5.2480
Profiling... [2048/50176]	Loss: 4.8195
Profiling... [2560/50176]	Loss: 4.7728
Profiling... [3072/50176]	Loss: 4.8220
Profiling... [3584/50176]	Loss: 4.8130
Profiling... [4096/50176]	Loss: 4.7492
Profiling... [4608/50176]	Loss: 4.7754
Profiling... [5120/50176]	Loss: 4.6493
Profiling... [5632/50176]	Loss: 4.6390
Profiling... [6144/50176]	Loss: 4.6656
Profiling... [6656/50176]	Loss: 4.6295
Profile done
epoch 1 train time consumed: 7.43s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.62706812040261,
                        "time": 5.255347292000124,
                        "accuracy": 0.009765625,
                        "total_cost": 94175823.47264221
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6400
Profiling... [1024/50176]	Loss: 5.7262
Profiling... [1536/50176]	Loss: 5.1309
Profiling... [2048/50176]	Loss: 4.7948
Profiling... [2560/50176]	Loss: 4.7849
Profiling... [3072/50176]	Loss: 4.8478
Profiling... [3584/50176]	Loss: 4.7040
Profiling... [4096/50176]	Loss: 4.6811
Profiling... [4608/50176]	Loss: 4.6339
Profiling... [5120/50176]	Loss: 4.7113
Profiling... [5632/50176]	Loss: 4.6374
Profiling... [6144/50176]	Loss: 4.5963
Profiling... [6656/50176]	Loss: 4.5723
Profile done
epoch 1 train time consumed: 17.42s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.39061636766822,
                        "time": 13.008564989999968,
                        "accuracy": 0.009765625,
                        "total_cost": 233113484.62079945
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6355
Profiling... [1024/50176]	Loss: 7.4869
Profiling... [1536/50176]	Loss: 5.7125
Profiling... [2048/50176]	Loss: 4.9825
Profiling... [2560/50176]	Loss: 4.9205
Profiling... [3072/50176]	Loss: 4.7394
Profiling... [3584/50176]	Loss: 4.7001
Profiling... [4096/50176]	Loss: 5.0574
Profiling... [4608/50176]	Loss: 4.9301
Profiling... [5120/50176]	Loss: 4.8139
Profiling... [5632/50176]	Loss: 4.8925
Profiling... [6144/50176]	Loss: 4.6144
Profiling... [6656/50176]	Loss: 4.7168
Profile done
epoch 1 train time consumed: 6.65s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.34432506268928,
                        "time": 4.495751449999943,
                        "accuracy": 0.009765625,
                        "total_cost": 80563865.98399898
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6397
Profiling... [1024/50176]	Loss: 7.1061
Profiling... [1536/50176]	Loss: 5.7817
Profiling... [2048/50176]	Loss: 5.4280
Profiling... [2560/50176]	Loss: 4.8498
Profiling... [3072/50176]	Loss: 4.7450
Profiling... [3584/50176]	Loss: 4.7781
Profiling... [4096/50176]	Loss: 5.2826
Profiling... [4608/50176]	Loss: 4.9421
Profiling... [5120/50176]	Loss: 4.6590
Profiling... [5632/50176]	Loss: 4.6469
Profiling... [6144/50176]	Loss: 4.6648
Profiling... [6656/50176]	Loss: 4.6140
Profile done
epoch 1 train time consumed: 6.62s
Validation Epoch: 0, Average loss: 0.0097, Accuracy: 0.0116
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.55081182751384,
                        "time": 4.617926656000009,
                        "accuracy": 0.01162109375,
                        "total_cost": 69540542.58447072
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6492
Profiling... [1024/50176]	Loss: 7.4872
Profiling... [1536/50176]	Loss: 6.3956
Profiling... [2048/50176]	Loss: 5.4002
Profiling... [2560/50176]	Loss: 4.8128
Profiling... [3072/50176]	Loss: 4.7122
Profiling... [3584/50176]	Loss: 4.7314
Profiling... [4096/50176]	Loss: 4.9662
Profiling... [4608/50176]	Loss: 4.6235
Profiling... [5120/50176]	Loss: 4.6939
Profiling... [5632/50176]	Loss: 4.8435
Profiling... [6144/50176]	Loss: 4.6490
Profiling... [6656/50176]	Loss: 4.8809
Profile done
epoch 1 train time consumed: 7.32s
Validation Epoch: 0, Average loss: 0.0097, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.65200401578366,
                        "time": 5.200137666000046,
                        "accuracy": 0.009765625,
                        "total_cost": 93186466.97472082
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6234
Profiling... [1024/50176]	Loss: 7.3591
Profiling... [1536/50176]	Loss: 5.7358
Profiling... [2048/50176]	Loss: 5.1660
Profiling... [2560/50176]	Loss: 4.9851
Profiling... [3072/50176]	Loss: 4.8869
Profiling... [3584/50176]	Loss: 5.0138
Profiling... [4096/50176]	Loss: 5.5507
Profiling... [4608/50176]	Loss: 4.7304
Profiling... [5120/50176]	Loss: 4.9546
Profiling... [5632/50176]	Loss: 5.0836
Profiling... [6144/50176]	Loss: 4.7867
Profiling... [6656/50176]	Loss: 4.8029
Profile done
epoch 1 train time consumed: 17.43s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.42658009130295,
                        "time": 13.001228615999935,
                        "accuracy": 0.009765625,
                        "total_cost": 232982016.79871884
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6374
Profiling... [1024/50176]	Loss: 7.3773
Profiling... [1536/50176]	Loss: 5.9674
Profiling... [2048/50176]	Loss: 5.0100
Profiling... [2560/50176]	Loss: 5.3759
Profiling... [3072/50176]	Loss: 5.4851
Profiling... [3584/50176]	Loss: 5.3399
Profiling... [4096/50176]	Loss: 5.3145
Profiling... [4608/50176]	Loss: 4.9575
Profiling... [5120/50176]	Loss: 4.8400
Profiling... [5632/50176]	Loss: 4.9260
Profiling... [6144/50176]	Loss: 4.8297
Profiling... [6656/50176]	Loss: 5.1373
Profile done
epoch 1 train time consumed: 6.59s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.38609051595135,
                        "time": 4.498836148999999,
                        "accuracy": 0.009765625,
                        "total_cost": 80619143.79007998
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6387
Profiling... [1024/50176]	Loss: 7.1082
Profiling... [1536/50176]	Loss: 5.7377
Profiling... [2048/50176]	Loss: 4.8055
Profiling... [2560/50176]	Loss: 4.8371
Profiling... [3072/50176]	Loss: 4.7778
Profiling... [3584/50176]	Loss: 4.6889
Profiling... [4096/50176]	Loss: 4.7923
Profiling... [4608/50176]	Loss: 4.8468
Profiling... [5120/50176]	Loss: 4.6475
Profiling... [5632/50176]	Loss: 4.8635
Profiling... [6144/50176]	Loss: 4.6603
Profiling... [6656/50176]	Loss: 4.5460
Profile done
epoch 1 train time consumed: 6.57s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0104
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.57846285058717,
                        "time": 4.606593104000012,
                        "accuracy": 0.01044921875,
                        "total_cost": 77149671.42400022
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6468
Profiling... [1024/50176]	Loss: 7.0171
Profiling... [1536/50176]	Loss: 6.2742
Profiling... [2048/50176]	Loss: 5.7720
Profiling... [2560/50176]	Loss: 5.1245
Profiling... [3072/50176]	Loss: 4.9688
Profiling... [3584/50176]	Loss: 4.6993
Profiling... [4096/50176]	Loss: 4.6887
Profiling... [4608/50176]	Loss: 4.7836
Profiling... [5120/50176]	Loss: 4.9530
Profiling... [5632/50176]	Loss: 4.7403
Profiling... [6144/50176]	Loss: 4.7864
Profiling... [6656/50176]	Loss: 4.8949
Profile done
epoch 1 train time consumed: 7.38s
Validation Epoch: 0, Average loss: 0.0569, Accuracy: 0.0118
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.66995152883733,
                        "time": 5.2209258769999,
                        "accuracy": 0.01181640625,
                        "total_cost": 77321480.75689109
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6338
Profiling... [1024/50176]	Loss: 7.6124
Profiling... [1536/50176]	Loss: 5.4990
Profiling... [2048/50176]	Loss: 5.0162
Profiling... [2560/50176]	Loss: 4.7955
Profiling... [3072/50176]	Loss: 4.6484
Profiling... [3584/50176]	Loss: 4.7532
Profiling... [4096/50176]	Loss: 4.8245
Profiling... [4608/50176]	Loss: 4.7155
Profiling... [5120/50176]	Loss: 4.6624
Profiling... [5632/50176]	Loss: 4.6011
Profiling... [6144/50176]	Loss: 4.7484
Profiling... [6656/50176]	Loss: 4.6723
Profile done
epoch 1 train time consumed: 17.51s
Validation Epoch: 0, Average loss: 0.0138, Accuracy: 0.0117
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.45443766769662,
                        "time": 12.97773416300015,
                        "accuracy": 0.01171875,
                        "total_cost": 193800830.1674689
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6222
Profiling... [1024/50176]	Loss: 7.7980
Profiling... [1536/50176]	Loss: 5.9163
Profiling... [2048/50176]	Loss: 5.1029
Profiling... [2560/50176]	Loss: 4.8098
Profiling... [3072/50176]	Loss: 4.6181
Profiling... [3584/50176]	Loss: 4.7765
Profiling... [4096/50176]	Loss: 4.6935
Profiling... [4608/50176]	Loss: 4.9922
Profiling... [5120/50176]	Loss: 4.8177
Profiling... [5632/50176]	Loss: 4.6639
Profiling... [6144/50176]	Loss: 4.8493
Profiling... [6656/50176]	Loss: 4.7180
Profile done
epoch 1 train time consumed: 6.51s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0128
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.40416346998809,
                        "time": 4.4966138409999985,
                        "accuracy": 0.01279296875,
                        "total_cost": 61510931.32116029
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6338
Profiling... [1024/50176]	Loss: 7.7051
Profiling... [1536/50176]	Loss: 5.6856
Profiling... [2048/50176]	Loss: 5.0183
Profiling... [2560/50176]	Loss: 5.0558
Profiling... [3072/50176]	Loss: 4.8376
Profiling... [3584/50176]	Loss: 5.0113
Profiling... [4096/50176]	Loss: 4.9241
Profiling... [4608/50176]	Loss: 5.6274
Profiling... [5120/50176]	Loss: 5.6321
Profiling... [5632/50176]	Loss: 4.7485
Profiling... [6144/50176]	Loss: 4.6440
Profiling... [6656/50176]	Loss: 4.7859
Profile done
epoch 1 train time consumed: 6.64s
Validation Epoch: 0, Average loss: 0.0173, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.59962921895095,
                        "time": 4.606935815000043,
                        "accuracy": 0.00986328125,
                        "total_cost": 81738900.79683244
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6307
Profiling... [1024/50176]	Loss: 7.7735
Profiling... [1536/50176]	Loss: 6.2257
Profiling... [2048/50176]	Loss: 5.1171
Profiling... [2560/50176]	Loss: 4.8876
Profiling... [3072/50176]	Loss: 4.7064
Profiling... [3584/50176]	Loss: 4.7359
Profiling... [4096/50176]	Loss: 4.7492
Profiling... [4608/50176]	Loss: 4.6611
Profiling... [5120/50176]	Loss: 4.8401
Profiling... [5632/50176]	Loss: 4.7809
Profiling... [6144/50176]	Loss: 4.7517
Profiling... [6656/50176]	Loss: 4.7072
Profile done
epoch 1 train time consumed: 7.45s
Validation Epoch: 0, Average loss: 0.0537, Accuracy: 0.0158
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.6880567604737,
                        "time": 5.213526093999917,
                        "accuracy": 0.0158203125,
                        "total_cost": 57670609.63239415
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6278
Profiling... [1024/50176]	Loss: 7.1621
Profiling... [1536/50176]	Loss: 5.6472
Profiling... [2048/50176]	Loss: 5.3516
Profiling... [2560/50176]	Loss: 4.9177
Profiling... [3072/50176]	Loss: 4.7851
Profiling... [3584/50176]	Loss: 5.0080
Profiling... [4096/50176]	Loss: 5.0182
Profiling... [4608/50176]	Loss: 5.1324
Profiling... [5120/50176]	Loss: 5.0854
Profiling... [5632/50176]	Loss: 4.8113
Profiling... [6144/50176]	Loss: 4.8100
Profiling... [6656/50176]	Loss: 4.6446
Profile done
epoch 1 train time consumed: 17.51s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.47622703258003,
                        "time": 12.999175303999891,
                        "accuracy": 0.009765625,
                        "total_cost": 232945221.44767803
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6217
Profiling... [2048/50176]	Loss: 4.6372
Profiling... [3072/50176]	Loss: 4.6116
Profiling... [4096/50176]	Loss: 4.5964
Profiling... [5120/50176]	Loss: 4.5708
Profiling... [6144/50176]	Loss: 4.5164
Profiling... [7168/50176]	Loss: 4.5141
Profiling... [8192/50176]	Loss: 4.5194
Profiling... [9216/50176]	Loss: 4.5258
Profiling... [10240/50176]	Loss: 4.5194
Profiling... [11264/50176]	Loss: 4.5026
Profiling... [12288/50176]	Loss: 4.4817
Profiling... [13312/50176]	Loss: 4.4688
Profile done
epoch 1 train time consumed: 12.45s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.62217998042817,
                        "time": 8.788991253999939,
                        "accuracy": 0.009765625,
                        "total_cost": 157498723.2716789
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6257
Profiling... [2048/50176]	Loss: 4.6358
Profiling... [3072/50176]	Loss: 4.6495
Profiling... [4096/50176]	Loss: 4.5827
Profiling... [5120/50176]	Loss: 4.5397
Profiling... [6144/50176]	Loss: 4.5390
Profiling... [7168/50176]	Loss: 4.5231
Profiling... [8192/50176]	Loss: 4.5073
Profiling... [9216/50176]	Loss: 4.5198
Profiling... [10240/50176]	Loss: 4.5234
Profiling... [11264/50176]	Loss: 4.4800
Profiling... [12288/50176]	Loss: 4.4744
Profiling... [13312/50176]	Loss: 4.4972
Profile done
epoch 1 train time consumed: 12.83s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.91013840637406,
                        "time": 9.088695174000122,
                        "accuracy": 0.009765625,
                        "total_cost": 162869417.5180822
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6389
Profiling... [2048/50176]	Loss: 4.6561
Profiling... [3072/50176]	Loss: 4.5955
Profiling... [4096/50176]	Loss: 4.5986
Profiling... [5120/50176]	Loss: 4.5716
Profiling... [6144/50176]	Loss: 4.5340
Profiling... [7168/50176]	Loss: 4.5103
Profiling... [8192/50176]	Loss: 4.5384
Profiling... [9216/50176]	Loss: 4.4908
Profiling... [10240/50176]	Loss: 4.4609
Profiling... [11264/50176]	Loss: 4.4969
Profiling... [12288/50176]	Loss: 4.4502
Profiling... [13312/50176]	Loss: 4.5019
Profile done
epoch 1 train time consumed: 14.47s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.97786759839634,
                        "time": 10.33782941200002,
                        "accuracy": 0.009765625,
                        "total_cost": 185253903.06304035
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6531
Profiling... [2048/50176]	Loss: 4.6092
Profiling... [3072/50176]	Loss: 4.6034
Profiling... [4096/50176]	Loss: 4.6211
Profiling... [5120/50176]	Loss: 4.5998
Profiling... [6144/50176]	Loss: 4.5551
Profiling... [7168/50176]	Loss: 4.5265
Profiling... [8192/50176]	Loss: 4.5018
Profiling... [9216/50176]	Loss: 4.5048
Profiling... [10240/50176]	Loss: 4.5017
Profiling... [11264/50176]	Loss: 4.4829
Profiling... [12288/50176]	Loss: 4.4693
Profiling... [13312/50176]	Loss: 4.4752
Profile done
epoch 1 train time consumed: 36.39s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.45664509775138,
                        "time": 26.902487272999906,
                        "accuracy": 0.009765625,
                        "total_cost": 482092571.9321583
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6213
Profiling... [2048/50176]	Loss: 4.6588
Profiling... [3072/50176]	Loss: 4.6143
Profiling... [4096/50176]	Loss: 4.5882
Profiling... [5120/50176]	Loss: 4.5670
Profiling... [6144/50176]	Loss: 4.5525
Profiling... [7168/50176]	Loss: 4.5331
Profiling... [8192/50176]	Loss: 4.5215
Profiling... [9216/50176]	Loss: 4.5050
Profiling... [10240/50176]	Loss: 4.4765
Profiling... [11264/50176]	Loss: 4.4865
Profiling... [12288/50176]	Loss: 4.4959
Profiling... [13312/50176]	Loss: 4.4600
Profile done
epoch 1 train time consumed: 12.51s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.68056284029231,
                        "time": 8.81591299999991,
                        "accuracy": 0.009765625,
                        "total_cost": 157981160.95999837
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6225
Profiling... [2048/50176]	Loss: 4.6395
Profiling... [3072/50176]	Loss: 4.6451
Profiling... [4096/50176]	Loss: 4.5990
Profiling... [5120/50176]	Loss: 4.5457
Profiling... [6144/50176]	Loss: 4.5311
Profiling... [7168/50176]	Loss: 4.5332
Profiling... [8192/50176]	Loss: 4.5054
Profiling... [9216/50176]	Loss: 4.4876
Profiling... [10240/50176]	Loss: 4.5076
Profiling... [11264/50176]	Loss: 4.4926
Profiling... [12288/50176]	Loss: 4.5001
Profiling... [13312/50176]	Loss: 4.4666
Profile done
epoch 1 train time consumed: 12.95s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.95890280709156,
                        "time": 9.103794552999943,
                        "accuracy": 0.009765625,
                        "total_cost": 163139998.389759
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6266
Profiling... [2048/50176]	Loss: 4.6189
Profiling... [3072/50176]	Loss: 4.6324
Profiling... [4096/50176]	Loss: 4.5903
Profiling... [5120/50176]	Loss: 4.5657
Profiling... [6144/50176]	Loss: 4.5737
Profiling... [7168/50176]	Loss: 4.5234
Profiling... [8192/50176]	Loss: 4.5105
Profiling... [9216/50176]	Loss: 4.5004
Profiling... [10240/50176]	Loss: 4.5185
Profiling... [11264/50176]	Loss: 4.4873
Profiling... [12288/50176]	Loss: 4.4831
Profiling... [13312/50176]	Loss: 4.4690
Profile done
epoch 1 train time consumed: 14.51s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 117.01681033228203,
                        "time": 10.399001171000009,
                        "accuracy": 0.009765625,
                        "total_cost": 186350100.98432016
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6288
Profiling... [2048/50176]	Loss: 4.6196
Profiling... [3072/50176]	Loss: 4.6204
Profiling... [4096/50176]	Loss: 4.6003
Profiling... [5120/50176]	Loss: 4.5873
Profiling... [6144/50176]	Loss: 4.5643
Profiling... [7168/50176]	Loss: 4.5173
Profiling... [8192/50176]	Loss: 4.5324
Profiling... [9216/50176]	Loss: 4.5214
Profiling... [10240/50176]	Loss: 4.4982
Profiling... [11264/50176]	Loss: 4.5142
Profiling... [12288/50176]	Loss: 4.4719
Profiling... [13312/50176]	Loss: 4.4809
Profile done
epoch 1 train time consumed: 37.21s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.52303826808378,
                        "time": 27.803166668999893,
                        "accuracy": 0.009765625,
                        "total_cost": 498232746.7084781
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6312
Profiling... [2048/50176]	Loss: 4.6221
Profiling... [3072/50176]	Loss: 4.6283
Profiling... [4096/50176]	Loss: 4.6179
Profiling... [5120/50176]	Loss: 4.5414
Profiling... [6144/50176]	Loss: 4.5384
Profiling... [7168/50176]	Loss: 4.5734
Profiling... [8192/50176]	Loss: 4.5315
Profiling... [9216/50176]	Loss: 4.5057
Profiling... [10240/50176]	Loss: 4.4921
Profiling... [11264/50176]	Loss: 4.4683
Profiling... [12288/50176]	Loss: 4.4678
Profiling... [13312/50176]	Loss: 4.4850
Profile done
epoch 1 train time consumed: 12.65s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.72981089611018,
                        "time": 8.802020173000074,
                        "accuracy": 0.009765625,
                        "total_cost": 157732201.50016132
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6360
Profiling... [2048/50176]	Loss: 4.6161
Profiling... [3072/50176]	Loss: 4.6244
Profiling... [4096/50176]	Loss: 4.6119
Profiling... [5120/50176]	Loss: 4.5691
Profiling... [6144/50176]	Loss: 4.5233
Profiling... [7168/50176]	Loss: 4.5212
Profiling... [8192/50176]	Loss: 4.5220
Profiling... [9216/50176]	Loss: 4.5117
Profiling... [10240/50176]	Loss: 4.4810
Profiling... [11264/50176]	Loss: 4.4969
Profiling... [12288/50176]	Loss: 4.4973
Profiling... [13312/50176]	Loss: 4.4187
Profile done
epoch 1 train time consumed: 12.94s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.98163941617852,
                        "time": 9.105464937000079,
                        "accuracy": 0.009765625,
                        "total_cost": 163169931.67104143
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6431
Profiling... [2048/50176]	Loss: 4.6423
Profiling... [3072/50176]	Loss: 4.6084
Profiling... [4096/50176]	Loss: 4.6139
Profiling... [5120/50176]	Loss: 4.5607
Profiling... [6144/50176]	Loss: 4.5283
Profiling... [7168/50176]	Loss: 4.5587
Profiling... [8192/50176]	Loss: 4.5050
Profiling... [9216/50176]	Loss: 4.5383
Profiling... [10240/50176]	Loss: 4.4973
Profiling... [11264/50176]	Loss: 4.5350
Profiling... [12288/50176]	Loss: 4.4771
Profiling... [13312/50176]	Loss: 4.4776
Profile done
epoch 1 train time consumed: 14.46s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 117.03765347926232,
                        "time": 10.387882469000033,
                        "accuracy": 0.009765625,
                        "total_cost": 186150853.84448057
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6305
Profiling... [2048/50176]	Loss: 4.6330
Profiling... [3072/50176]	Loss: 4.6379
Profiling... [4096/50176]	Loss: 4.6246
Profiling... [5120/50176]	Loss: 4.5697
Profiling... [6144/50176]	Loss: 4.5220
Profiling... [7168/50176]	Loss: 4.5553
Profiling... [8192/50176]	Loss: 4.5151
Profiling... [9216/50176]	Loss: 4.5405
Profiling... [10240/50176]	Loss: 4.5089
Profiling... [11264/50176]	Loss: 4.4802
Profiling... [12288/50176]	Loss: 4.4937
Profiling... [13312/50176]	Loss: 4.4581
Profile done
epoch 1 train time consumed: 37.29s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.5906230274664,
                        "time": 27.618956654000158,
                        "accuracy": 0.009765625,
                        "total_cost": 494931703.23968285
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6319
Profiling... [2048/50176]	Loss: 5.6096
Profiling... [3072/50176]	Loss: 5.0055
Profiling... [4096/50176]	Loss: 4.7843
Profiling... [5120/50176]	Loss: 4.8370
Profiling... [6144/50176]	Loss: 4.6754
Profiling... [7168/50176]	Loss: 4.7121
Profiling... [8192/50176]	Loss: 4.7134
Profiling... [9216/50176]	Loss: 4.7269
Profiling... [10240/50176]	Loss: 4.5979
Profiling... [11264/50176]	Loss: 4.6332
Profiling... [12288/50176]	Loss: 4.6101
Profiling... [13312/50176]	Loss: 4.5897
Profile done
epoch 1 train time consumed: 12.54s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.78248897028115,
                        "time": 8.797615277999967,
                        "accuracy": 0.009765625,
                        "total_cost": 157653265.7817594
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6312
Profiling... [2048/50176]	Loss: 5.6292
Profiling... [3072/50176]	Loss: 5.0121
Profiling... [4096/50176]	Loss: 4.7403
Profiling... [5120/50176]	Loss: 4.6638
Profiling... [6144/50176]	Loss: 4.7191
Profiling... [7168/50176]	Loss: 4.6934
Profiling... [8192/50176]	Loss: 4.6290
Profiling... [9216/50176]	Loss: 4.7245
Profiling... [10240/50176]	Loss: 4.6265
Profiling... [11264/50176]	Loss: 4.5843
Profiling... [12288/50176]	Loss: 4.5767
Profiling... [13312/50176]	Loss: 4.5819
Profile done
epoch 1 train time consumed: 12.99s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.0193081383868,
                        "time": 9.122734546000174,
                        "accuracy": 0.009765625,
                        "total_cost": 163479403.06432313
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6276
Profiling... [2048/50176]	Loss: 5.5971
Profiling... [3072/50176]	Loss: 5.0798
Profiling... [4096/50176]	Loss: 4.6470
Profiling... [5120/50176]	Loss: 4.7515
Profiling... [6144/50176]	Loss: 4.7361
Profiling... [7168/50176]	Loss: 4.6822
Profiling... [8192/50176]	Loss: 4.6232
Profiling... [9216/50176]	Loss: 4.6651
Profiling... [10240/50176]	Loss: 4.5466
Profiling... [11264/50176]	Loss: 4.6041
Profiling... [12288/50176]	Loss: 4.5864
Profiling... [13312/50176]	Loss: 4.5323
Profile done
epoch 1 train time consumed: 14.57s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 117.07070702565031,
                        "time": 10.369203958000071,
                        "accuracy": 0.009765625,
                        "total_cost": 185816134.92736128
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6310
Profiling... [2048/50176]	Loss: 5.4873
Profiling... [3072/50176]	Loss: 4.9879
Profiling... [4096/50176]	Loss: 4.8098
Profiling... [5120/50176]	Loss: 4.7549
Profiling... [6144/50176]	Loss: 4.7946
Profiling... [7168/50176]	Loss: 4.7584
Profiling... [8192/50176]	Loss: 4.6132
Profiling... [9216/50176]	Loss: 4.6826
Profiling... [10240/50176]	Loss: 4.6955
Profiling... [11264/50176]	Loss: 4.6309
Profiling... [12288/50176]	Loss: 4.6973
Profiling... [13312/50176]	Loss: 4.5615
Profile done
epoch 1 train time consumed: 37.24s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.64853772844744,
                        "time": 27.804448753000088,
                        "accuracy": 0.009765625,
                        "total_cost": 498255721.65376157
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6373
Profiling... [2048/50176]	Loss: 5.4168
Profiling... [3072/50176]	Loss: 4.9678
Profiling... [4096/50176]	Loss: 4.9780
Profiling... [5120/50176]	Loss: 4.8670
Profiling... [6144/50176]	Loss: 4.8730
Profiling... [7168/50176]	Loss: 4.7809
Profiling... [8192/50176]	Loss: 4.7309
Profiling... [9216/50176]	Loss: 4.6507
Profiling... [10240/50176]	Loss: 4.5711
Profiling... [11264/50176]	Loss: 4.5215
Profiling... [12288/50176]	Loss: 4.4967
Profiling... [13312/50176]	Loss: 4.5181
Profile done
epoch 1 train time consumed: 12.61s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.82510483632781,
                        "time": 8.820551843999965,
                        "accuracy": 0.009765625,
                        "total_cost": 158064289.04447937
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6244
Profiling... [2048/50176]	Loss: 5.4827
Profiling... [3072/50176]	Loss: 4.9433
Profiling... [4096/50176]	Loss: 4.7457
Profiling... [5120/50176]	Loss: 4.7618
Profiling... [6144/50176]	Loss: 4.8949
Profiling... [7168/50176]	Loss: 4.7363
Profiling... [8192/50176]	Loss: 4.7496
Profiling... [9216/50176]	Loss: 4.6028
Profiling... [10240/50176]	Loss: 4.5654
Profiling... [11264/50176]	Loss: 4.5401
Profiling... [12288/50176]	Loss: 4.5128
Profiling... [13312/50176]	Loss: 4.5701
Profile done
epoch 1 train time consumed: 12.99s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.04747177242099,
                        "time": 9.123889866000127,
                        "accuracy": 0.009765625,
                        "total_cost": 163500106.39872226
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6393
Profiling... [2048/50176]	Loss: 5.5592
Profiling... [3072/50176]	Loss: 5.1073
Profiling... [4096/50176]	Loss: 4.8081
Profiling... [5120/50176]	Loss: 4.8241
Profiling... [6144/50176]	Loss: 4.7833
Profiling... [7168/50176]	Loss: 4.6562
Profiling... [8192/50176]	Loss: 4.6972
Profiling... [9216/50176]	Loss: 4.5758
Profiling... [10240/50176]	Loss: 4.6120
Profiling... [11264/50176]	Loss: 4.5737
Profiling... [12288/50176]	Loss: 4.5401
Profiling... [13312/50176]	Loss: 4.5042
Profile done
epoch 1 train time consumed: 14.49s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 117.09961882104568,
                        "time": 10.3802770100001,
                        "accuracy": 0.009765625,
                        "total_cost": 186014564.0192018
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6432
Profiling... [2048/50176]	Loss: 5.7303
Profiling... [3072/50176]	Loss: 4.8674
Profiling... [4096/50176]	Loss: 4.6942
Profiling... [5120/50176]	Loss: 4.7098
Profiling... [6144/50176]	Loss: 4.7486
Profiling... [7168/50176]	Loss: 4.7604
Profiling... [8192/50176]	Loss: 4.6660
Profiling... [9216/50176]	Loss: 4.6520
Profiling... [10240/50176]	Loss: 4.5866
Profiling... [11264/50176]	Loss: 4.5968
Profiling... [12288/50176]	Loss: 4.5907
Profiling... [13312/50176]	Loss: 4.6766
Profile done
epoch 1 train time consumed: 37.22s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.68100461402904,
                        "time": 27.678768211000033,
                        "accuracy": 0.009765625,
                        "total_cost": 496003526.3411206
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6436
Profiling... [2048/50176]	Loss: 5.4864
Profiling... [3072/50176]	Loss: 4.9488
Profiling... [4096/50176]	Loss: 4.7361
Profiling... [5120/50176]	Loss: 4.7331
Profiling... [6144/50176]	Loss: 4.7032
Profiling... [7168/50176]	Loss: 4.7992
Profiling... [8192/50176]	Loss: 4.6998
Profiling... [9216/50176]	Loss: 4.6911
Profiling... [10240/50176]	Loss: 4.7036
Profiling... [11264/50176]	Loss: 4.5794
Profiling... [12288/50176]	Loss: 4.5367
Profiling... [13312/50176]	Loss: 4.6187
Profile done
epoch 1 train time consumed: 12.54s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.84420467257144,
                        "time": 8.802505254999915,
                        "accuracy": 0.009765625,
                        "total_cost": 157740894.16959846
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6331
Profiling... [2048/50176]	Loss: 5.4032
Profiling... [3072/50176]	Loss: 5.0883
Profiling... [4096/50176]	Loss: 4.7450
Profiling... [5120/50176]	Loss: 4.8382
Profiling... [6144/50176]	Loss: 4.8081
Profiling... [7168/50176]	Loss: 4.7549
Profiling... [8192/50176]	Loss: 4.7593
Profiling... [9216/50176]	Loss: 4.7196
Profiling... [10240/50176]	Loss: 4.7628
Profiling... [11264/50176]	Loss: 4.6165
Profiling... [12288/50176]	Loss: 4.7390
Profiling... [13312/50176]	Loss: 4.6711
Profile done
epoch 1 train time consumed: 13.00s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.05781880242267,
                        "time": 9.111869931000001,
                        "accuracy": 0.009765625,
                        "total_cost": 163284709.16352004
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6207
Profiling... [2048/50176]	Loss: 5.3713
Profiling... [3072/50176]	Loss: 4.8824
Profiling... [4096/50176]	Loss: 4.7384
Profiling... [5120/50176]	Loss: 4.7867
Profiling... [6144/50176]	Loss: 4.7684
Profiling... [7168/50176]	Loss: 4.7833
Profiling... [8192/50176]	Loss: 4.6919
Profiling... [9216/50176]	Loss: 4.7128
Profiling... [10240/50176]	Loss: 5.0235
Profiling... [11264/50176]	Loss: 4.5898
Profiling... [12288/50176]	Loss: 4.5686
Profiling... [13312/50176]	Loss: 4.6253
Profile done
epoch 1 train time consumed: 14.58s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 117.10662286384935,
                        "time": 10.384284027000149,
                        "accuracy": 0.009765625,
                        "total_cost": 186086369.76384267
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6360
Profiling... [2048/50176]	Loss: 5.5096
Profiling... [3072/50176]	Loss: 4.8075
Profiling... [4096/50176]	Loss: 4.8502
Profiling... [5120/50176]	Loss: 4.6841
Profiling... [6144/50176]	Loss: 4.7280
Profiling... [7168/50176]	Loss: 4.6737
Profiling... [8192/50176]	Loss: 4.6637
Profiling... [9216/50176]	Loss: 4.7413
Profiling... [10240/50176]	Loss: 4.6593
Profiling... [11264/50176]	Loss: 4.6333
Profiling... [12288/50176]	Loss: 4.6436
Profiling... [13312/50176]	Loss: 4.5733
Profile done
epoch 1 train time consumed: 37.28s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.72584382589548,
                        "time": 27.81153489799999,
                        "accuracy": 0.009765625,
                        "total_cost": 498382705.37215984
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6129
Profiling... [2048/50176]	Loss: 6.8812
Profiling... [3072/50176]	Loss: 5.7009
Profiling... [4096/50176]	Loss: 5.1369
Profiling... [5120/50176]	Loss: 5.2587
Profiling... [6144/50176]	Loss: 5.0690
Profiling... [7168/50176]	Loss: 4.7309
Profiling... [8192/50176]	Loss: 5.0239
Profiling... [9216/50176]	Loss: 4.8850
Profiling... [10240/50176]	Loss: 4.7935
Profiling... [11264/50176]	Loss: 4.7717
Profiling... [12288/50176]	Loss: 4.7744
Profiling... [13312/50176]	Loss: 4.8392
Profile done
epoch 1 train time consumed: 12.61s
Validation Epoch: 0, Average loss: 0.0054, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.88548603179584,
                        "time": 8.80170447599994,
                        "accuracy": 0.009765625,
                        "total_cost": 157726544.20991892
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6239
Profiling... [2048/50176]	Loss: 7.1987
Profiling... [3072/50176]	Loss: 5.7303
Profiling... [4096/50176]	Loss: 5.0197
Profiling... [5120/50176]	Loss: 5.0537
Profiling... [6144/50176]	Loss: 4.6235
Profiling... [7168/50176]	Loss: 4.7560
Profiling... [8192/50176]	Loss: 4.6293
Profiling... [9216/50176]	Loss: 4.7670
Profiling... [10240/50176]	Loss: 4.6066
Profiling... [11264/50176]	Loss: 4.6757
Profiling... [12288/50176]	Loss: 4.7653
Profiling... [13312/50176]	Loss: 4.6802
Profile done
epoch 1 train time consumed: 12.97s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.08015063740281,
                        "time": 9.111854893999862,
                        "accuracy": 0.009765625,
                        "total_cost": 163284439.70047754
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6393
Profiling... [2048/50176]	Loss: 7.2150
Profiling... [3072/50176]	Loss: 5.6740
Profiling... [4096/50176]	Loss: 4.9288
Profiling... [5120/50176]	Loss: 4.9821
Profiling... [6144/50176]	Loss: 4.9451
Profiling... [7168/50176]	Loss: 4.7197
Profiling... [8192/50176]	Loss: 4.7686
Profiling... [9216/50176]	Loss: 4.8232
Profiling... [10240/50176]	Loss: 4.7835
Profiling... [11264/50176]	Loss: 4.7183
Profiling... [12288/50176]	Loss: 4.7018
Profiling... [13312/50176]	Loss: 4.9015
Profile done
epoch 1 train time consumed: 14.51s
Validation Epoch: 0, Average loss: 0.1540, Accuracy: 0.0111
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 117.11829071169922,
                        "time": 10.384436174999792,
                        "accuracy": 0.0111328125,
                        "total_cost": 163236049.34736514
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6261
Profiling... [2048/50176]	Loss: 7.2282
Profiling... [3072/50176]	Loss: 5.3441
Profiling... [4096/50176]	Loss: 5.2708
Profiling... [5120/50176]	Loss: 5.1527
Profiling... [6144/50176]	Loss: 4.8161
Profiling... [7168/50176]	Loss: 4.7942
Profiling... [8192/50176]	Loss: 4.6207
Profiling... [9216/50176]	Loss: 4.6986
Profiling... [10240/50176]	Loss: 4.8829
Profiling... [11264/50176]	Loss: 4.6367
Profiling... [12288/50176]	Loss: 4.9084
Profiling... [13312/50176]	Loss: 4.7680
Profile done
epoch 1 train time consumed: 37.76s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.74169135096405,
                        "time": 28.365784659000383,
                        "accuracy": 0.009765625,
                        "total_cost": 508314861.0892869
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6404
Profiling... [2048/50176]	Loss: 7.1750
Profiling... [3072/50176]	Loss: 5.8551
Profiling... [4096/50176]	Loss: 4.9401
Profiling... [5120/50176]	Loss: 4.8481
Profiling... [6144/50176]	Loss: 4.7026
Profiling... [7168/50176]	Loss: 4.6905
Profiling... [8192/50176]	Loss: 4.6867
Profiling... [9216/50176]	Loss: 4.6992
Profiling... [10240/50176]	Loss: 4.6165
Profiling... [11264/50176]	Loss: 4.7064
Profiling... [12288/50176]	Loss: 4.7885
Profiling... [13312/50176]	Loss: 4.6380
Profile done
epoch 1 train time consumed: 12.71s
Validation Epoch: 0, Average loss: 0.0056, Accuracy: 0.0096
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.8916354593137,
                        "time": 8.792561911999655,
                        "accuracy": 0.0095703125,
                        "total_cost": 160778274.9622794
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6216
Profiling... [2048/50176]	Loss: 7.3997
Profiling... [3072/50176]	Loss: 5.4982
Profiling... [4096/50176]	Loss: 4.9906
Profiling... [5120/50176]	Loss: 4.8325
Profiling... [6144/50176]	Loss: 4.7378
Profiling... [7168/50176]	Loss: 5.0557
Profiling... [8192/50176]	Loss: 4.7293
Profiling... [9216/50176]	Loss: 4.9252
Profiling... [10240/50176]	Loss: 4.7478
Profiling... [11264/50176]	Loss: 4.6685
Profiling... [12288/50176]	Loss: 4.6149
Profiling... [13312/50176]	Loss: 4.6288
Profile done
epoch 1 train time consumed: 12.94s
Validation Epoch: 0, Average loss: 0.0070, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.07350635877468,
                        "time": 9.11908478300029,
                        "accuracy": 0.0099609375,
                        "total_cost": 160209803.24643645
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6271
Profiling... [2048/50176]	Loss: 7.4037
Profiling... [3072/50176]	Loss: 5.3759
Profiling... [4096/50176]	Loss: 4.8607
Profiling... [5120/50176]	Loss: 4.9711
Profiling... [6144/50176]	Loss: 4.6333
Profiling... [7168/50176]	Loss: 4.6600
Profiling... [8192/50176]	Loss: 4.6619
Profiling... [9216/50176]	Loss: 4.7829
Profiling... [10240/50176]	Loss: 4.7171
Profiling... [11264/50176]	Loss: 4.6206
Profiling... [12288/50176]	Loss: 4.6925
Profiling... [13312/50176]	Loss: 4.5698
Profile done
epoch 1 train time consumed: 14.65s
Validation Epoch: 0, Average loss: 0.0051, Accuracy: 0.0120
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 117.11127240070563,
                        "time": 10.362512286999845,
                        "accuracy": 0.01201171875,
                        "total_cost": 150972536.7341766
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6263
Profiling... [2048/50176]	Loss: 6.8888
Profiling... [3072/50176]	Loss: 5.8580
Profiling... [4096/50176]	Loss: 5.0972
Profiling... [5120/50176]	Loss: 4.8572
Profiling... [6144/50176]	Loss: 4.7817
Profiling... [7168/50176]	Loss: 4.7889
Profiling... [8192/50176]	Loss: 4.8033
Profiling... [9216/50176]	Loss: 5.1069
Profiling... [10240/50176]	Loss: 4.7344
Profiling... [11264/50176]	Loss: 4.6493
Profiling... [12288/50176]	Loss: 4.7578
Profiling... [13312/50176]	Loss: 4.6880
Profile done
epoch 1 train time consumed: 38.08s
Validation Epoch: 0, Average loss: 0.0135, Accuracy: 0.0146
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 116.75276436267865,
                        "time": 28.389698443999805,
                        "accuracy": 0.01455078125,
                        "total_cost": 341438520.8835413
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6262
Profiling... [2048/50176]	Loss: 7.1562
Profiling... [3072/50176]	Loss: 5.7173
Profiling... [4096/50176]	Loss: 4.8712
Profiling... [5120/50176]	Loss: 4.8251
Profiling... [6144/50176]	Loss: 4.8000
Profiling... [7168/50176]	Loss: 4.9455
Profiling... [8192/50176]	Loss: 4.6736
Profiling... [9216/50176]	Loss: 4.7565
Profiling... [10240/50176]	Loss: 4.7774
Profiling... [11264/50176]	Loss: 4.7897
Profiling... [12288/50176]	Loss: 4.6772
Profiling... [13312/50176]	Loss: 4.6284
Profile done
epoch 1 train time consumed: 12.69s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.88156000806829,
                        "time": 8.79993026000011,
                        "accuracy": 0.009765625,
                        "total_cost": 157694750.25920197
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6350
Profiling... [2048/50176]	Loss: 7.3295
Profiling... [3072/50176]	Loss: 5.5340
Profiling... [4096/50176]	Loss: 4.8993
Profiling... [5120/50176]	Loss: 4.7745
Profiling... [6144/50176]	Loss: 4.7434
Profiling... [7168/50176]	Loss: 4.6184
Profiling... [8192/50176]	Loss: 4.7596
Profiling... [9216/50176]	Loss: 4.5972
Profiling... [10240/50176]	Loss: 4.7611
Profiling... [11264/50176]	Loss: 4.7022
Profiling... [12288/50176]	Loss: 4.6802
Profiling... [13312/50176]	Loss: 5.3375
Profile done
epoch 1 train time consumed: 12.89s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0107
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.05837033098976,
                        "time": 9.096502360000159,
                        "accuracy": 0.0107421875,
                        "total_cost": 148190292.99200258
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6248
Profiling... [2048/50176]	Loss: 7.0390
Profiling... [3072/50176]	Loss: 5.7755
Profiling... [4096/50176]	Loss: 5.2125
Profiling... [5120/50176]	Loss: 4.9369
Profiling... [6144/50176]	Loss: 4.8641
Profiling... [7168/50176]	Loss: 4.7078
Profiling... [8192/50176]	Loss: 5.0971
Profiling... [9216/50176]	Loss: 4.6332
Profiling... [10240/50176]	Loss: 4.7973
Profiling... [11264/50176]	Loss: 5.0254
Profiling... [12288/50176]	Loss: 5.0516
Profiling... [13312/50176]	Loss: 4.7854
Profile done
epoch 1 train time consumed: 14.62s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 117.10051619019066,
                        "time": 10.363487809000162,
                        "accuracy": 0.009765625,
                        "total_cost": 185713701.53728288
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6350
Profiling... [2048/50176]	Loss: 6.9638
Profiling... [3072/50176]	Loss: 5.5196
Profiling... [4096/50176]	Loss: 5.2351
Profiling... [5120/50176]	Loss: 4.7451
Profiling... [6144/50176]	Loss: 4.6556
Profiling... [7168/50176]	Loss: 4.7034
Profiling... [8192/50176]	Loss: 4.6417
Profiling... [9216/50176]	Loss: 4.6005
Profiling... [10240/50176]	Loss: 4.6536
Profiling... [11264/50176]	Loss: 4.7443
Profiling... [12288/50176]	Loss: 5.0633
Profiling... [13312/50176]	Loss: 4.8595
Profile done
epoch 1 train time consumed: 37.16s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 116.77107903625043,
                        "time": 27.63588574000005,
                        "accuracy": 0.009765625,
                        "total_cost": 495235072.4608008
                    },
                    
[Training Loop] The optimal parameters are lr: 0.01 dr: 0.0 bs: 128 pl: 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[GPU_0] Set GPU power limit to 175W.
[GPU_0] Set GPU power limit to 175W.
Training Epoch: 0 [128/50048]	Loss: 4.6230
Training Epoch: 0 [256/50048]	Loss: 7.6584
Training Epoch: 0 [384/50048]	Loss: 8.6925
Training Epoch: 0 [512/50048]	Loss: 6.4967
Training Epoch: 0 [640/50048]	Loss: 5.3485
Training Epoch: 0 [768/50048]	Loss: 4.8282
Training Epoch: 0 [896/50048]	Loss: 5.2508
Training Epoch: 0 [1024/50048]	Loss: 4.7131
Training Epoch: 0 [1152/50048]	Loss: 4.7204
Training Epoch: 0 [1280/50048]	Loss: 4.6241
Training Epoch: 0 [1408/50048]	Loss: 4.7733
Training Epoch: 0 [1536/50048]	Loss: 4.6146
Training Epoch: 0 [1664/50048]	Loss: 4.5689
Training Epoch: 0 [1792/50048]	Loss: 4.8361
Training Epoch: 0 [1920/50048]	Loss: 4.6190
Training Epoch: 0 [2048/50048]	Loss: 4.7787
Training Epoch: 0 [2176/50048]	Loss: 4.6096
Training Epoch: 0 [2304/50048]	Loss: 4.7200
Training Epoch: 0 [2432/50048]	Loss: 4.8311
Training Epoch: 0 [2560/50048]	Loss: 4.6807
Training Epoch: 0 [2688/50048]	Loss: 4.6125
Training Epoch: 0 [2816/50048]	Loss: 4.5775
Training Epoch: 0 [2944/50048]	Loss: 4.5602
Training Epoch: 0 [3072/50048]	Loss: 4.5678
Training Epoch: 0 [3200/50048]	Loss: 4.5690
Training Epoch: 0 [3328/50048]	Loss: 4.5544
Training Epoch: 0 [3456/50048]	Loss: 4.5609
Training Epoch: 0 [3584/50048]	Loss: 4.5098
Training Epoch: 0 [3712/50048]	Loss: 4.5913
Training Epoch: 0 [3840/50048]	Loss: 4.5776
Training Epoch: 0 [3968/50048]	Loss: 4.5030
Training Epoch: 0 [4096/50048]	Loss: 4.5717
Training Epoch: 0 [4224/50048]	Loss: 4.5788
Training Epoch: 0 [4352/50048]	Loss: 4.5264
Training Epoch: 0 [4480/50048]	Loss: 4.5541
Training Epoch: 0 [4608/50048]	Loss: 4.4990
Training Epoch: 0 [4736/50048]	Loss: 4.5495
Training Epoch: 0 [4864/50048]	Loss: 4.5318
Training Epoch: 0 [4992/50048]	Loss: 4.4850
Training Epoch: 0 [5120/50048]	Loss: 4.5040
Training Epoch: 0 [5248/50048]	Loss: 4.5913
Training Epoch: 0 [5376/50048]	Loss: 4.5400
Training Epoch: 0 [5504/50048]	Loss: 4.4882
Training Epoch: 0 [5632/50048]	Loss: 4.4762
Training Epoch: 0 [5760/50048]	Loss: 4.5557
Training Epoch: 0 [5888/50048]	Loss: 4.5081
Training Epoch: 0 [6016/50048]	Loss: 4.4831
Training Epoch: 0 [6144/50048]	Loss: 4.6242
Training Epoch: 0 [6272/50048]	Loss: 4.4676
Training Epoch: 0 [6400/50048]	Loss: 4.5170
Training Epoch: 0 [6528/50048]	Loss: 4.5321
Training Epoch: 0 [6656/50048]	Loss: 4.5703
Training Epoch: 0 [6784/50048]	Loss: 4.5235
Training Epoch: 0 [6912/50048]	Loss: 4.5007
Training Epoch: 0 [7040/50048]	Loss: 4.4671
Training Epoch: 0 [7168/50048]	Loss: 4.5247
Training Epoch: 0 [7296/50048]	Loss: 4.5226
Training Epoch: 0 [7424/50048]	Loss: 4.5326
Training Epoch: 0 [7552/50048]	Loss: 4.4608
Training Epoch: 0 [7680/50048]	Loss: 4.4492
Training Epoch: 0 [7808/50048]	Loss: 4.4078
Training Epoch: 0 [7936/50048]	Loss: 4.5269
Training Epoch: 0 [8064/50048]	Loss: 4.5167
Training Epoch: 0 [8192/50048]	Loss: 4.4757
Training Epoch: 0 [8320/50048]	Loss: 4.4682
Training Epoch: 0 [8448/50048]	Loss: 4.5106
Training Epoch: 0 [8576/50048]	Loss: 4.5184
Training Epoch: 0 [8704/50048]	Loss: 4.4862
Training Epoch: 0 [8832/50048]	Loss: 4.4421
Training Epoch: 0 [8960/50048]	Loss: 4.4304
Training Epoch: 0 [9088/50048]	Loss: 4.4714
Training Epoch: 0 [9216/50048]	Loss: 4.4936
Training Epoch: 0 [9344/50048]	Loss: 4.3837
Training Epoch: 0 [9472/50048]	Loss: 4.4374
Training Epoch: 0 [9600/50048]	Loss: 4.5457
Training Epoch: 0 [9728/50048]	Loss: 4.6084
Training Epoch: 0 [9856/50048]	Loss: 4.4965
Training Epoch: 0 [9984/50048]	Loss: 4.4251
Training Epoch: 0 [10112/50048]	Loss: 4.4019
Training Epoch: 0 [10240/50048]	Loss: 4.5532
Training Epoch: 0 [10368/50048]	Loss: 4.4151
Training Epoch: 0 [10496/50048]	Loss: 4.4096
Training Epoch: 0 [10624/50048]	Loss: 4.5001
Training Epoch: 0 [10752/50048]	Loss: 4.4025
Training Epoch: 0 [10880/50048]	Loss: 4.3595
Training Epoch: 0 [11008/50048]	Loss: 4.3499
Training Epoch: 0 [11136/50048]	Loss: 4.3817
Training Epoch: 0 [11264/50048]	Loss: 4.4732
Training Epoch: 0 [11392/50048]	Loss: 4.4963
Training Epoch: 0 [11520/50048]	Loss: 4.3931
Training Epoch: 0 [11648/50048]	Loss: 4.4213
Training Epoch: 0 [11776/50048]	Loss: 4.3732
Training Epoch: 0 [11904/50048]	Loss: 4.4005
Training Epoch: 0 [12032/50048]	Loss: 4.4440
Training Epoch: 0 [12160/50048]	Loss: 4.3258
Training Epoch: 0 [12288/50048]	Loss: 4.3485
Training Epoch: 0 [12416/50048]	Loss: 4.3830
Training Epoch: 0 [12544/50048]	Loss: 4.3652
Training Epoch: 0 [12672/50048]	Loss: 4.3207
Training Epoch: 0 [12800/50048]	Loss: 4.3476
Training Epoch: 0 [12928/50048]	Loss: 4.3314
Training Epoch: 0 [13056/50048]	Loss: 4.2489
Training Epoch: 0 [13184/50048]	Loss: 4.4705
Training Epoch: 0 [13312/50048]	Loss: 4.2134
Training Epoch: 0 [13440/50048]	Loss: 4.2559
Training Epoch: 0 [13568/50048]	Loss: 4.2756
Training Epoch: 0 [13696/50048]	Loss: 4.2132
Training Epoch: 0 [13824/50048]	Loss: 4.2459
Training Epoch: 0 [13952/50048]	Loss: 4.2110
Training Epoch: 0 [14080/50048]	Loss: 4.2113
Training Epoch: 0 [14208/50048]	Loss: 4.3349
Training Epoch: 0 [14336/50048]	Loss: 4.3503
Training Epoch: 0 [14464/50048]	Loss: 4.2234
Training Epoch: 0 [14592/50048]	Loss: 4.3726
Training Epoch: 0 [14720/50048]	Loss: 4.3004
Training Epoch: 0 [14848/50048]	Loss: 4.2894
Training Epoch: 0 [14976/50048]	Loss: 4.2101
Training Epoch: 0 [15104/50048]	Loss: 4.2955
Training Epoch: 0 [15232/50048]	Loss: 4.2233
Training Epoch: 0 [15360/50048]	Loss: 4.3052
Training Epoch: 0 [15488/50048]	Loss: 4.3257
Training Epoch: 0 [15616/50048]	Loss: 4.3463
Training Epoch: 0 [15744/50048]	Loss: 4.4088
Training Epoch: 0 [15872/50048]	Loss: 4.2146
Training Epoch: 0 [16000/50048]	Loss: 4.2155
Training Epoch: 0 [16128/50048]	Loss: 4.3066
Training Epoch: 0 [16256/50048]	Loss: 4.1837
Training Epoch: 0 [16384/50048]	Loss: 4.1861
Training Epoch: 0 [16512/50048]	Loss: 4.1432
Training Epoch: 0 [16640/50048]	Loss: 4.3362
Training Epoch: 0 [16768/50048]	Loss: 4.3664
Training Epoch: 0 [16896/50048]	Loss: 4.2058
Training Epoch: 0 [17024/50048]	Loss: 4.2344
Training Epoch: 0 [17152/50048]	Loss: 4.2856
Training Epoch: 0 [17280/50048]	Loss: 4.2125
Training Epoch: 0 [17408/50048]	Loss: 4.2430
Training Epoch: 0 [17536/50048]	Loss: 4.1815
Training Epoch: 0 [17664/50048]	Loss: 4.1000
Training Epoch: 0 [17792/50048]	Loss: 4.2347
Training Epoch: 0 [17920/50048]	Loss: 4.0277
Training Epoch: 0 [18048/50048]	Loss: 4.2298
Training Epoch: 0 [18176/50048]	Loss: 4.2478
Training Epoch: 0 [18304/50048]	Loss: 4.2061
Training Epoch: 0 [18432/50048]	Loss: 4.1125
Training Epoch: 0 [18560/50048]	Loss: 4.2497
Training Epoch: 0 [18688/50048]	Loss: 4.1962
Training Epoch: 0 [18816/50048]	Loss: 4.2451
Training Epoch: 0 [18944/50048]	Loss: 4.1912
Training Epoch: 0 [19072/50048]	Loss: 4.1161
Training Epoch: 0 [19200/50048]	Loss: 4.1237
Training Epoch: 0 [19328/50048]	Loss: 4.2809
Training Epoch: 0 [19456/50048]	Loss: 4.1346
Training Epoch: 0 [19584/50048]	Loss: 4.2881
Training Epoch: 0 [19712/50048]	Loss: 4.1684
Training Epoch: 0 [19840/50048]	Loss: 4.2279
Training Epoch: 0 [19968/50048]	Loss: 4.1850
Training Epoch: 0 [20096/50048]	Loss: 4.2204
Training Epoch: 0 [20224/50048]	Loss: 4.3174
Training Epoch: 0 [20352/50048]	Loss: 4.1563
Training Epoch: 0 [20480/50048]	Loss: 4.2772
Training Epoch: 0 [20608/50048]	Loss: 4.1394
Training Epoch: 0 [20736/50048]	Loss: 4.1825
Training Epoch: 0 [20864/50048]	Loss: 4.1579
Training Epoch: 0 [20992/50048]	Loss: 4.0616
Training Epoch: 0 [21120/50048]	Loss: 4.1018
Training Epoch: 0 [21248/50048]	Loss: 4.1126
Training Epoch: 0 [21376/50048]	Loss: 4.1290
Training Epoch: 0 [21504/50048]	Loss: 4.1579
Training Epoch: 0 [21632/50048]	Loss: 4.2510
Training Epoch: 0 [21760/50048]	Loss: 4.2265
Training Epoch: 0 [21888/50048]	Loss: 4.0644
Training Epoch: 0 [22016/50048]	Loss: 4.1095
Training Epoch: 0 [22144/50048]	Loss: 3.9624
Training Epoch: 0 [22272/50048]	Loss: 4.0782
Training Epoch: 0 [22400/50048]	Loss: 4.1949
Training Epoch: 0 [22528/50048]	Loss: 4.0596
Training Epoch: 0 [22656/50048]	Loss: 3.9904
Training Epoch: 0 [22784/50048]	Loss: 3.9674
Training Epoch: 0 [22912/50048]	Loss: 4.1285
Training Epoch: 0 [23040/50048]	Loss: 4.1661
Training Epoch: 0 [23168/50048]	Loss: 4.1509
Training Epoch: 0 [23296/50048]	Loss: 4.1384
Training Epoch: 0 [23424/50048]	Loss: 3.9439
Training Epoch: 0 [23552/50048]	Loss: 4.1733
Training Epoch: 0 [23680/50048]	Loss: 4.0850
Training Epoch: 0 [23808/50048]	Loss: 4.1700
Training Epoch: 0 [23936/50048]	Loss: 4.1730
Training Epoch: 0 [24064/50048]	Loss: 4.1778
Training Epoch: 0 [24192/50048]	Loss: 4.2120
Training Epoch: 0 [24320/50048]	Loss: 4.1208
Training Epoch: 0 [24448/50048]	Loss: 4.0714
Training Epoch: 0 [24576/50048]	Loss: 3.9341
Training Epoch: 0 [24704/50048]	Loss: 4.0690
Training Epoch: 0 [24832/50048]	Loss: 4.0283
Training Epoch: 0 [24960/50048]	Loss: 4.1649
Training Epoch: 0 [25088/50048]	Loss: 4.0602
Training Epoch: 0 [25216/50048]	Loss: 4.1162
Training Epoch: 0 [25344/50048]	Loss: 4.2391
Training Epoch: 0 [25472/50048]	Loss: 4.0951
Training Epoch: 0 [25600/50048]	Loss: 4.0642
Training Epoch: 0 [25728/50048]	Loss: 3.9660
Training Epoch: 0 [25856/50048]	Loss: 4.1358
Training Epoch: 0 [25984/50048]	Loss: 4.2113
Training Epoch: 0 [26112/50048]	Loss: 4.1727
Training Epoch: 0 [26240/50048]	Loss: 4.0514
Training Epoch: 0 [26368/50048]	Loss: 4.1276
Training Epoch: 0 [26496/50048]	Loss: 4.0395
Training Epoch: 0 [26624/50048]	Loss: 4.1154
Training Epoch: 0 [26752/50048]	Loss: 4.1136
Training Epoch: 0 [26880/50048]	Loss: 4.0851
Training Epoch: 0 [27008/50048]	Loss: 4.1319
Training Epoch: 0 [27136/50048]	Loss: 4.0341
Training Epoch: 0 [27264/50048]	Loss: 3.9938
Training Epoch: 0 [27392/50048]	Loss: 4.0812
Training Epoch: 0 [27520/50048]	Loss: 4.2022
Training Epoch: 0 [27648/50048]	Loss: 3.9644
Training Epoch: 0 [27776/50048]	Loss: 4.2783
Training Epoch: 0 [27904/50048]	Loss: 4.1448
Training Epoch: 0 [28032/50048]	Loss: 4.1673
Training Epoch: 0 [28160/50048]	Loss: 3.9508
Training Epoch: 0 [28288/50048]	Loss: 3.8728
Training Epoch: 0 [28416/50048]	Loss: 3.9735
Training Epoch: 0 [28544/50048]	Loss: 3.8615
Training Epoch: 0 [28672/50048]	Loss: 4.1112
Training Epoch: 0 [28800/50048]	Loss: 4.0465
Training Epoch: 0 [28928/50048]	Loss: 3.9349
Training Epoch: 0 [29056/50048]	Loss: 3.8441
Training Epoch: 0 [29184/50048]	Loss: 3.8214
Training Epoch: 0 [29312/50048]	Loss: 4.0402
Training Epoch: 0 [29440/50048]	Loss: 3.8728
Training Epoch: 0 [29568/50048]	Loss: 4.2786
Training Epoch: 0 [29696/50048]	Loss: 4.1629
Training Epoch: 0 [29824/50048]	Loss: 4.1776
Training Epoch: 0 [29952/50048]	Loss: 4.1326
Training Epoch: 0 [30080/50048]	Loss: 4.0082
Training Epoch: 0 [30208/50048]	Loss: 4.2121
Training Epoch: 0 [30336/50048]	Loss: 4.1039
Training Epoch: 0 [30464/50048]	Loss: 4.0776
Training Epoch: 0 [30592/50048]	Loss: 4.1015
Training Epoch: 0 [30720/50048]	Loss: 4.1156
Training Epoch: 0 [30848/50048]	Loss: 3.9822
Training Epoch: 0 [30976/50048]	Loss: 4.0441
Training Epoch: 0 [31104/50048]	Loss: 4.0978
Training Epoch: 0 [31232/50048]	Loss: 4.1748
Training Epoch: 0 [31360/50048]	Loss: 3.9421
Training Epoch: 0 [31488/50048]	Loss: 4.1105
Training Epoch: 0 [31616/50048]	Loss: 4.1664
Training Epoch: 0 [31744/50048]	Loss: 4.0612
Training Epoch: 0 [31872/50048]	Loss: 3.9825
Training Epoch: 0 [32000/50048]	Loss: 4.2278
Training Epoch: 0 [32128/50048]	Loss: 4.1136
Training Epoch: 0 [32256/50048]	Loss: 4.1385
Training Epoch: 0 [32384/50048]	Loss: 4.0612
Training Epoch: 0 [32512/50048]	Loss: 4.1885
Training Epoch: 0 [32640/50048]	Loss: 3.9102
Training Epoch: 0 [32768/50048]	Loss: 3.8835
Training Epoch: 0 [32896/50048]	Loss: 3.9630
Training Epoch: 0 [33024/50048]	Loss: 4.1118
Training Epoch: 0 [33152/50048]	Loss: 4.0052
Training Epoch: 0 [33280/50048]	Loss: 4.1361
Training Epoch: 0 [33408/50048]	Loss: 3.9394
Training Epoch: 0 [33536/50048]	Loss: 4.0075
Training Epoch: 0 [33664/50048]	Loss: 3.9862
Training Epoch: 0 [33792/50048]	Loss: 4.0643
Training Epoch: 0 [33920/50048]	Loss: 4.0731
Training Epoch: 0 [34048/50048]	Loss: 4.0800
Training Epoch: 0 [34176/50048]	Loss: 3.8148
Training Epoch: 0 [34304/50048]	Loss: 3.9042
Training Epoch: 0 [34432/50048]	Loss: 4.0712
Training Epoch: 0 [34560/50048]	Loss: 4.2073
Training Epoch: 0 [34688/50048]	Loss: 3.9626
Training Epoch: 0 [34816/50048]	Loss: 3.9666
Training Epoch: 0 [34944/50048]	Loss: 3.9866
Training Epoch: 0 [35072/50048]	Loss: 3.9708
Training Epoch: 0 [35200/50048]	Loss: 3.9192
Training Epoch: 0 [35328/50048]	Loss: 4.0930
Training Epoch: 0 [35456/50048]	Loss: 4.0724
Training Epoch: 0 [35584/50048]	Loss: 4.0272
Training Epoch: 0 [35712/50048]	Loss: 3.8687
Training Epoch: 0 [35840/50048]	Loss: 3.9592
Training Epoch: 0 [35968/50048]	Loss: 4.1150
Training Epoch: 0 [36096/50048]	Loss: 3.9901
Training Epoch: 0 [36224/50048]	Loss: 3.9207
Training Epoch: 0 [36352/50048]	Loss: 4.0443
Training Epoch: 0 [36480/50048]	Loss: 3.9494
Training Epoch: 0 [36608/50048]	Loss: 3.9733
Training Epoch: 0 [36736/50048]	Loss: 4.0336
Training Epoch: 0 [36864/50048]	Loss: 3.9779
Training Epoch: 0 [36992/50048]	Loss: 3.8914
Training Epoch: 0 [37120/50048]	Loss: 4.0653
Training Epoch: 0 [37248/50048]	Loss: 4.2673
Training Epoch: 0 [37376/50048]	Loss: 4.0203
Training Epoch: 0 [37504/50048]	Loss: 3.9886
Training Epoch: 0 [37632/50048]	Loss: 4.0390
Training Epoch: 0 [37760/50048]	Loss: 3.9609
Training Epoch: 0 [37888/50048]	Loss: 3.9151
Training Epoch: 0 [38016/50048]	Loss: 3.8331
Training Epoch: 0 [38144/50048]	Loss: 3.9891
Training Epoch: 0 [38272/50048]	Loss: 4.0093
Training Epoch: 0 [38400/50048]	Loss: 3.9255
Training Epoch: 0 [38528/50048]	Loss: 4.0588
Training Epoch: 0 [38656/50048]	Loss: 3.9048
Training Epoch: 0 [38784/50048]	Loss: 3.9905
Training Epoch: 0 [38912/50048]	Loss: 4.0328
Training Epoch: 0 [39040/50048]	Loss: 3.9225
Training Epoch: 0 [39168/50048]	Loss: 3.8775
Training Epoch: 0 [39296/50048]	Loss: 3.8987
Training Epoch: 0 [39424/50048]	Loss: 4.1789
Training Epoch: 0 [39552/50048]	Loss: 4.0963
Training Epoch: 0 [39680/50048]	Loss: 3.8754
Training Epoch: 0 [39808/50048]	Loss: 3.8903
Training Epoch: 0 [39936/50048]	Loss: 3.9028
Training Epoch: 0 [40064/50048]	Loss: 4.0028
Training Epoch: 0 [40192/50048]	Loss: 3.9739
Training Epoch: 0 [40320/50048]	Loss: 3.9374
Training Epoch: 0 [40448/50048]	Loss: 3.8397
Training Epoch: 0 [40576/50048]	Loss: 3.9407
Training Epoch: 0 [40704/50048]	Loss: 3.8474
Training Epoch: 0 [40832/50048]	Loss: 3.8325
Training Epoch: 0 [40960/50048]	Loss: 3.8657
Training Epoch: 0 [41088/50048]	Loss: 3.9718
Training Epoch: 0 [41216/50048]	Loss: 3.8488
Training Epoch: 0 [41344/50048]	Loss: 3.9627
Training Epoch: 0 [41472/50048]	Loss: 3.9111
Training Epoch: 0 [41600/50048]	Loss: 3.8432
Training Epoch: 0 [41728/50048]	Loss: 4.0325
Training Epoch: 0 [41856/50048]	Loss: 3.8150
Training Epoch: 0 [41984/50048]	Loss: 3.9667
Training Epoch: 0 [42112/50048]	Loss: 3.9995
Training Epoch: 0 [42240/50048]	Loss: 3.9964
Training Epoch: 0 [42368/50048]	Loss: 3.8470
Training Epoch: 0 [42496/50048]	Loss: 3.8964
Training Epoch: 0 [42624/50048]	Loss: 3.9286
Training Epoch: 0 [42752/50048]	Loss: 4.0353
Training Epoch: 0 [42880/50048]	Loss: 3.8522
Training Epoch: 0 [43008/50048]	Loss: 3.9094
Training Epoch: 0 [43136/50048]	Loss: 3.7873
Training Epoch: 0 [43264/50048]	Loss: 3.8095
Training Epoch: 0 [43392/50048]	Loss: 4.0410
Training Epoch: 0 [43520/50048]	Loss: 3.8433
Training Epoch: 0 [43648/50048]	Loss: 4.0465
Training Epoch: 0 [43776/50048]	Loss: 3.8999
Training Epoch: 0 [43904/50048]	Loss: 3.9008
Training Epoch: 0 [44032/50048]	Loss: 3.8543
Training Epoch: 0 [44160/50048]	Loss: 3.7237
Training Epoch: 0 [44288/50048]	Loss: 3.7681
Training Epoch: 0 [44416/50048]	Loss: 3.9531
Training Epoch: 0 [44544/50048]	Loss: 3.8519
Training Epoch: 0 [44672/50048]	Loss: 3.9942
Training Epoch: 0 [44800/50048]	Loss: 3.9490
Training Epoch: 0 [44928/50048]	Loss: 3.8425
Training Epoch: 0 [45056/50048]	Loss: 3.9783
Training Epoch: 0 [45184/50048]	Loss: 3.8287
Training Epoch: 0 [45312/50048]	Loss: 3.7963
Training Epoch: 0 [45440/50048]	Loss: 3.8481
Training Epoch: 0 [45568/50048]	Loss: 3.9162
Training Epoch: 0 [45696/50048]	Loss: 3.9616
Training Epoch: 0 [45824/50048]	Loss: 3.7870
Training Epoch: 0 [45952/50048]	Loss: 3.8451
Training Epoch: 0 [46080/50048]	Loss: 3.8555
Training Epoch: 0 [46208/50048]	Loss: 4.0361
Training Epoch: 0 [46336/50048]	Loss: 3.8943
Training Epoch: 0 [46464/50048]	Loss: 3.8091
Training Epoch: 0 [46592/50048]	Loss: 3.7633
Training Epoch: 0 [46720/50048]	Loss: 3.7796
Training Epoch: 0 [46848/50048]	Loss: 3.8722
Training Epoch: 0 [46976/50048]	Loss: 3.8739
Training Epoch: 0 [47104/50048]	Loss: 3.8755
Training Epoch: 0 [47232/50048]	Loss: 3.9273
Training Epoch: 0 [47360/50048]	Loss: 3.8683
Training Epoch: 0 [47488/50048]	Loss: 3.8159
Training Epoch: 0 [47616/50048]	Loss: 3.7663
Training Epoch: 0 [47744/50048]	Loss: 3.7088
Training Epoch: 0 [47872/50048]	Loss: 3.8429
Training Epoch: 0 [48000/50048]	Loss: 3.8081
Training Epoch: 0 [48128/50048]	Loss: 3.9571
Training Epoch: 0 [48256/50048]	Loss: 3.7739
Training Epoch: 0 [48384/50048]	Loss: 3.7985
Training Epoch: 0 [48512/50048]	Loss: 3.8033
Training Epoch: 0 [48640/50048]	Loss: 3.9581
Training Epoch: 0 [48768/50048]	Loss: 3.7520
Training Epoch: 0 [48896/50048]	Loss: 3.8393
Training Epoch: 0 [49024/50048]	Loss: 3.8996
Training Epoch: 0 [49152/50048]	Loss: 4.0848
Training Epoch: 0 [49280/50048]	Loss: 3.8135
Training Epoch: 0 [49408/50048]	Loss: 3.8023
Training Epoch: 0 [49536/50048]	Loss: 3.7475
Training Epoch: 0 [49664/50048]	Loss: 3.9306
Training Epoch: 0 [49792/50048]	Loss: 3.9982
Training Epoch: 0 [49920/50048]	Loss: 3.9041
Training Epoch: 0 [50048/50048]	Loss: 3.7861
Validation Epoch: 0, Average loss: 0.0312, Accuracy: 0.0750
Training Epoch: 1 [128/50048]	Loss: 3.8850
Training Epoch: 1 [256/50048]	Loss: 3.9300
Training Epoch: 1 [384/50048]	Loss: 3.9155
Training Epoch: 1 [512/50048]	Loss: 3.8100
Training Epoch: 1 [640/50048]	Loss: 4.0817
Training Epoch: 1 [768/50048]	Loss: 3.7665
Training Epoch: 1 [896/50048]	Loss: 3.8725
Training Epoch: 1 [1024/50048]	Loss: 3.9425
Training Epoch: 1 [1152/50048]	Loss: 3.7983
Training Epoch: 1 [1280/50048]	Loss: 3.9141
Training Epoch: 1 [1408/50048]	Loss: 3.7531
Training Epoch: 1 [1536/50048]	Loss: 3.8267
Training Epoch: 1 [1664/50048]	Loss: 3.6963
Training Epoch: 1 [1792/50048]	Loss: 3.8766
Training Epoch: 1 [1920/50048]	Loss: 3.6054
Training Epoch: 1 [2048/50048]	Loss: 3.7123
Training Epoch: 1 [2176/50048]	Loss: 3.7462
Training Epoch: 1 [2304/50048]	Loss: 3.7906
Training Epoch: 1 [2432/50048]	Loss: 3.8509
Training Epoch: 1 [2560/50048]	Loss: 3.6723
Training Epoch: 1 [2688/50048]	Loss: 3.8186
Training Epoch: 1 [2816/50048]	Loss: 3.6737
Training Epoch: 1 [2944/50048]	Loss: 3.6164
Training Epoch: 1 [3072/50048]	Loss: 3.9599
Training Epoch: 1 [3200/50048]	Loss: 3.7073
Training Epoch: 1 [3328/50048]	Loss: 3.7724
Training Epoch: 1 [3456/50048]	Loss: 3.7968
Training Epoch: 1 [3584/50048]	Loss: 3.6220
Training Epoch: 1 [3712/50048]	Loss: 3.8816
Training Epoch: 1 [3840/50048]	Loss: 3.7621
Training Epoch: 1 [3968/50048]	Loss: 3.7386
Training Epoch: 1 [4096/50048]	Loss: 3.7842
Training Epoch: 1 [4224/50048]	Loss: 3.6446
Training Epoch: 1 [4352/50048]	Loss: 3.5165
Training Epoch: 1 [4480/50048]	Loss: 3.6206
Training Epoch: 1 [4608/50048]	Loss: 3.7206
Training Epoch: 1 [4736/50048]	Loss: 3.8948
Training Epoch: 1 [4864/50048]	Loss: 3.8382
Training Epoch: 1 [4992/50048]	Loss: 3.5891
Training Epoch: 1 [5120/50048]	Loss: 3.6154
Training Epoch: 1 [5248/50048]	Loss: 3.7379
Training Epoch: 1 [5376/50048]	Loss: 3.6866
Training Epoch: 1 [5504/50048]	Loss: 3.5202
Training Epoch: 1 [5632/50048]	Loss: 3.7533
Training Epoch: 1 [5760/50048]	Loss: 3.7693
Training Epoch: 1 [5888/50048]	Loss: 3.7408
Training Epoch: 1 [6016/50048]	Loss: 3.9576
Training Epoch: 1 [6144/50048]	Loss: 3.7085
Training Epoch: 1 [6272/50048]	Loss: 3.6383
Training Epoch: 1 [6400/50048]	Loss: 3.8115
Training Epoch: 1 [6528/50048]	Loss: 3.7861
Training Epoch: 1 [6656/50048]	Loss: 3.6967
Training Epoch: 1 [6784/50048]	Loss: 3.7607
Training Epoch: 1 [6912/50048]	Loss: 3.8463
Training Epoch: 1 [7040/50048]	Loss: 3.6488
Training Epoch: 1 [7168/50048]	Loss: 3.9305
Training Epoch: 1 [7296/50048]	Loss: 3.7600
Training Epoch: 1 [7424/50048]	Loss: 3.7062
Training Epoch: 1 [7552/50048]	Loss: 3.7036
Training Epoch: 1 [7680/50048]	Loss: 3.7504
Training Epoch: 1 [7808/50048]	Loss: 3.7832
Training Epoch: 1 [7936/50048]	Loss: 3.8079
Training Epoch: 1 [8064/50048]	Loss: 3.8767
Training Epoch: 1 [8192/50048]	Loss: 3.7743
Training Epoch: 1 [8320/50048]	Loss: 3.6266
Training Epoch: 1 [8448/50048]	Loss: 3.9767
Training Epoch: 1 [8576/50048]	Loss: 3.8572
Training Epoch: 1 [8704/50048]	Loss: 3.6162
Training Epoch: 1 [8832/50048]	Loss: 3.8059
Training Epoch: 1 [8960/50048]	Loss: 3.9695
Training Epoch: 1 [9088/50048]	Loss: 3.7664
Training Epoch: 1 [9216/50048]	Loss: 3.7329
Training Epoch: 1 [9344/50048]	Loss: 3.7627
Training Epoch: 1 [9472/50048]	Loss: 3.7714
Training Epoch: 1 [9600/50048]	Loss: 3.7263
Training Epoch: 1 [9728/50048]	Loss: 3.6090
Training Epoch: 1 [9856/50048]	Loss: 3.8119
Training Epoch: 1 [9984/50048]	Loss: 3.8864
Training Epoch: 1 [10112/50048]	Loss: 3.7784
Training Epoch: 1 [10240/50048]	Loss: 3.8685
Training Epoch: 1 [10368/50048]	Loss: 3.7680
Training Epoch: 1 [10496/50048]	Loss: 3.6015
Training Epoch: 1 [10624/50048]	Loss: 3.5987
Training Epoch: 1 [10752/50048]	Loss: 3.8301
Training Epoch: 1 [10880/50048]	Loss: 3.8396
Training Epoch: 1 [11008/50048]	Loss: 3.6344
Training Epoch: 1 [11136/50048]	Loss: 3.7300
Training Epoch: 1 [11264/50048]	Loss: 3.7235
Training Epoch: 1 [11392/50048]	Loss: 3.8070
Training Epoch: 1 [11520/50048]	Loss: 3.6895
Training Epoch: 1 [11648/50048]	Loss: 3.7000
Training Epoch: 1 [11776/50048]	Loss: 3.5210
Training Epoch: 1 [11904/50048]	Loss: 3.7801
Training Epoch: 1 [12032/50048]	Loss: 3.9394
Training Epoch: 1 [12160/50048]	Loss: 3.6107
Training Epoch: 1 [12288/50048]	Loss: 3.6253
Training Epoch: 1 [12416/50048]	Loss: 3.5351
Training Epoch: 1 [12544/50048]	Loss: 3.6885
Training Epoch: 1 [12672/50048]	Loss: 4.0009
Training Epoch: 1 [12800/50048]	Loss: 3.6802
Training Epoch: 1 [12928/50048]	Loss: 3.6133
Training Epoch: 1 [13056/50048]	Loss: 3.7075
Training Epoch: 1 [13184/50048]	Loss: 3.6180
Training Epoch: 1 [13312/50048]	Loss: 3.6542
Training Epoch: 1 [13440/50048]	Loss: 3.7042
Training Epoch: 1 [13568/50048]	Loss: 3.6933
Training Epoch: 1 [13696/50048]	Loss: 3.7337
Training Epoch: 1 [13824/50048]	Loss: 3.7474
Training Epoch: 1 [13952/50048]	Loss: 3.6529
Training Epoch: 1 [14080/50048]	Loss: 3.7436
Training Epoch: 1 [14208/50048]	Loss: 3.8846
Training Epoch: 1 [14336/50048]	Loss: 3.7742
Training Epoch: 1 [14464/50048]	Loss: 3.6945
Training Epoch: 1 [14592/50048]	Loss: 3.6670
Training Epoch: 1 [14720/50048]	Loss: 3.7228
Training Epoch: 1 [14848/50048]	Loss: 3.8451
Training Epoch: 1 [14976/50048]	Loss: 3.5622
Training Epoch: 1 [15104/50048]	Loss: 3.6412
Training Epoch: 1 [15232/50048]	Loss: 3.6371
Training Epoch: 1 [15360/50048]	Loss: 3.5852
Training Epoch: 1 [15488/50048]	Loss: 3.7500
Training Epoch: 1 [15616/50048]	Loss: 3.7657
Training Epoch: 1 [15744/50048]	Loss: 3.9050
Training Epoch: 1 [15872/50048]	Loss: 3.4842
Training Epoch: 1 [16000/50048]	Loss: 3.5839
Training Epoch: 1 [16128/50048]	Loss: 3.6210
Training Epoch: 1 [16256/50048]	Loss: 3.6667
Training Epoch: 1 [16384/50048]	Loss: 3.6097
Training Epoch: 1 [16512/50048]	Loss: 3.6723
Training Epoch: 1 [16640/50048]	Loss: 3.5232
Training Epoch: 1 [16768/50048]	Loss: 3.5827
Training Epoch: 1 [16896/50048]	Loss: 3.6978
Training Epoch: 1 [17024/50048]	Loss: 3.6807
Training Epoch: 1 [17152/50048]	Loss: 3.7306
Training Epoch: 1 [17280/50048]	Loss: 3.6183
Training Epoch: 1 [17408/50048]	Loss: 3.6356
Training Epoch: 1 [17536/50048]	Loss: 3.6394
Training Epoch: 1 [17664/50048]	Loss: 3.4236
Training Epoch: 1 [17792/50048]	Loss: 3.6579
Training Epoch: 1 [17920/50048]	Loss: 3.6193
Training Epoch: 1 [18048/50048]	Loss: 3.4542
Training Epoch: 1 [18176/50048]	Loss: 3.5744
Training Epoch: 1 [18304/50048]	Loss: 3.7370
Training Epoch: 1 [18432/50048]	Loss: 3.4998
Training Epoch: 1 [18560/50048]	Loss: 3.4151
Training Epoch: 1 [18688/50048]	Loss: 3.7046
Training Epoch: 1 [18816/50048]	Loss: 3.5562
Training Epoch: 1 [18944/50048]	Loss: 3.5840
Training Epoch: 1 [19072/50048]	Loss: 3.5862
Training Epoch: 1 [19200/50048]	Loss: 3.4668
Training Epoch: 1 [19328/50048]	Loss: 3.6079
Training Epoch: 1 [19456/50048]	Loss: 3.6968
Training Epoch: 1 [19584/50048]	Loss: 3.3446
Training Epoch: 1 [19712/50048]	Loss: 3.5714
Training Epoch: 1 [19840/50048]	Loss: 3.5842
Training Epoch: 1 [19968/50048]	Loss: 3.4947
Training Epoch: 1 [20096/50048]	Loss: 3.4140
Training Epoch: 1 [20224/50048]	Loss: 3.4523
Training Epoch: 1 [20352/50048]	Loss: 3.2903
Training Epoch: 1 [20480/50048]	Loss: 3.6349
Training Epoch: 1 [20608/50048]	Loss: 3.4721
Training Epoch: 1 [20736/50048]	Loss: 3.6094
Training Epoch: 1 [20864/50048]	Loss: 3.4703
Training Epoch: 1 [20992/50048]	Loss: 3.7632
Training Epoch: 1 [21120/50048]	Loss: 3.4833
Training Epoch: 1 [21248/50048]	Loss: 3.4126
Training Epoch: 1 [21376/50048]	Loss: 3.5440
Training Epoch: 1 [21504/50048]	Loss: 3.6851
Training Epoch: 1 [21632/50048]	Loss: 3.3875
Training Epoch: 1 [21760/50048]	Loss: 3.6505
Training Epoch: 1 [21888/50048]	Loss: 3.6248
Training Epoch: 1 [22016/50048]	Loss: 3.5812
Training Epoch: 1 [22144/50048]	Loss: 3.5363
Training Epoch: 1 [22272/50048]	Loss: 3.4236
Training Epoch: 1 [22400/50048]	Loss: 3.7845
Training Epoch: 1 [22528/50048]	Loss: 3.5270
Training Epoch: 1 [22656/50048]	Loss: 3.5090
Training Epoch: 1 [22784/50048]	Loss: 3.4371
Training Epoch: 1 [22912/50048]	Loss: 3.5540
Training Epoch: 1 [23040/50048]	Loss: 3.4763
Training Epoch: 1 [23168/50048]	Loss: 3.6384
Training Epoch: 1 [23296/50048]	Loss: 3.5116
Training Epoch: 1 [23424/50048]	Loss: 4.0039
Training Epoch: 1 [23552/50048]	Loss: 3.7619
Training Epoch: 1 [23680/50048]	Loss: 3.7326
Training Epoch: 1 [23808/50048]	Loss: 3.5872
Training Epoch: 1 [23936/50048]	Loss: 3.6471
Training Epoch: 1 [24064/50048]	Loss: 3.6688
Training Epoch: 1 [24192/50048]	Loss: 3.5235
Training Epoch: 1 [24320/50048]	Loss: 3.6732
Training Epoch: 1 [24448/50048]	Loss: 3.6406
Training Epoch: 1 [24576/50048]	Loss: 3.6438
Training Epoch: 1 [24704/50048]	Loss: 3.5770
Training Epoch: 1 [24832/50048]	Loss: 3.3581
Training Epoch: 1 [24960/50048]	Loss: 3.4781
Training Epoch: 1 [25088/50048]	Loss: 3.4064
Training Epoch: 1 [25216/50048]	Loss: 3.5202
Training Epoch: 1 [25344/50048]	Loss: 3.4923
Training Epoch: 1 [25472/50048]	Loss: 3.6382
Training Epoch: 1 [25600/50048]	Loss: 3.4926
Training Epoch: 1 [25728/50048]	Loss: 3.6047
Training Epoch: 1 [25856/50048]	Loss: 3.5981
Training Epoch: 1 [25984/50048]	Loss: 3.8186
Training Epoch: 1 [26112/50048]	Loss: 3.4609
Training Epoch: 1 [26240/50048]	Loss: 3.5789
Training Epoch: 1 [26368/50048]	Loss: 3.7505
Training Epoch: 1 [26496/50048]	Loss: 3.4863
Training Epoch: 1 [26624/50048]	Loss: 3.5443
Training Epoch: 1 [26752/50048]	Loss: 3.6070
Training Epoch: 1 [26880/50048]	Loss: 3.5197
Training Epoch: 1 [27008/50048]	Loss: 3.5248
Training Epoch: 1 [27136/50048]	Loss: 3.4963
Training Epoch: 1 [27264/50048]	Loss: 3.6050
Training Epoch: 1 [27392/50048]	Loss: 3.7085
Training Epoch: 1 [27520/50048]	Loss: 3.5509
Training Epoch: 1 [27648/50048]	Loss: 3.4583
Training Epoch: 1 [27776/50048]	Loss: 3.4930
Training Epoch: 1 [27904/50048]	Loss: 3.4777
Training Epoch: 1 [28032/50048]	Loss: 3.2838
Training Epoch: 1 [28160/50048]	Loss: 3.6408
Training Epoch: 1 [28288/50048]	Loss: 3.5689
Training Epoch: 1 [28416/50048]	Loss: 3.2860
Training Epoch: 1 [28544/50048]	Loss: 3.5267
Training Epoch: 1 [28672/50048]	Loss: 3.5063
Training Epoch: 1 [28800/50048]	Loss: 3.5465
Training Epoch: 1 [28928/50048]	Loss: 3.4997
Training Epoch: 1 [29056/50048]	Loss: 3.5114
Training Epoch: 1 [29184/50048]	Loss: 3.5867
Training Epoch: 1 [29312/50048]	Loss: 3.3754
Training Epoch: 1 [29440/50048]	Loss: 3.3966
Training Epoch: 1 [29568/50048]	Loss: 3.5744
Training Epoch: 1 [29696/50048]	Loss: 3.5285
Training Epoch: 1 [29824/50048]	Loss: 3.5377
Training Epoch: 1 [29952/50048]	Loss: 3.4048
Training Epoch: 1 [30080/50048]	Loss: 3.5849
Training Epoch: 1 [30208/50048]	Loss: 3.6256
Training Epoch: 1 [30336/50048]	Loss: 3.5079
Training Epoch: 1 [30464/50048]	Loss: 3.6068
Training Epoch: 1 [30592/50048]	Loss: 3.4412
Training Epoch: 1 [30720/50048]	Loss: 3.5820
Training Epoch: 1 [30848/50048]	Loss: 3.3273
Training Epoch: 1 [30976/50048]	Loss: 3.4592
Training Epoch: 1 [31104/50048]	Loss: 3.4676
Training Epoch: 1 [31232/50048]	Loss: 3.5418
Training Epoch: 1 [31360/50048]	Loss: 3.4633
Training Epoch: 1 [31488/50048]	Loss: 3.5838
Training Epoch: 1 [31616/50048]	Loss: 3.2302
Training Epoch: 1 [31744/50048]	Loss: 3.5341
Training Epoch: 1 [31872/50048]	Loss: 3.6377
Training Epoch: 1 [32000/50048]	Loss: 3.3652
Training Epoch: 1 [32128/50048]	Loss: 3.4776
Training Epoch: 1 [32256/50048]	Loss: 3.4091
Training Epoch: 1 [32384/50048]	Loss: 3.4339
Training Epoch: 1 [32512/50048]	Loss: 3.6077
Training Epoch: 1 [32640/50048]	Loss: 3.3833
Training Epoch: 1 [32768/50048]	Loss: 3.2281
Training Epoch: 1 [32896/50048]	Loss: 3.5031
Training Epoch: 1 [33024/50048]	Loss: 3.3930
Training Epoch: 1 [33152/50048]	Loss: 3.4210
Training Epoch: 1 [33280/50048]	Loss: 3.5016
Training Epoch: 1 [33408/50048]	Loss: 3.4693
Training Epoch: 1 [33536/50048]	Loss: 3.4463
Training Epoch: 1 [33664/50048]	Loss: 3.6050
Training Epoch: 1 [33792/50048]	Loss: 3.5502
Training Epoch: 1 [33920/50048]	Loss: 3.3851
Training Epoch: 1 [34048/50048]	Loss: 3.2434
Training Epoch: 1 [34176/50048]	Loss: 3.5659
Training Epoch: 1 [34304/50048]	Loss: 3.4161
Training Epoch: 1 [34432/50048]	Loss: 3.6935
Training Epoch: 1 [34560/50048]	Loss: 3.5551
Training Epoch: 1 [34688/50048]	Loss: 3.5523
Training Epoch: 1 [34816/50048]	Loss: 3.4811
Training Epoch: 1 [34944/50048]	Loss: 3.5177
Training Epoch: 1 [35072/50048]	Loss: 3.5013
Training Epoch: 1 [35200/50048]	Loss: 3.6074
Training Epoch: 1 [35328/50048]	Loss: 3.3832
Training Epoch: 1 [35456/50048]	Loss: 3.4822
Training Epoch: 1 [35584/50048]	Loss: 3.4760
Training Epoch: 1 [35712/50048]	Loss: 3.3604
Training Epoch: 1 [35840/50048]	Loss: 3.4536
Training Epoch: 1 [35968/50048]	Loss: 3.2074
Training Epoch: 1 [36096/50048]	Loss: 3.3124
Training Epoch: 1 [36224/50048]	Loss: 3.1979
Training Epoch: 1 [36352/50048]	Loss: 3.2937
Training Epoch: 1 [36480/50048]	Loss: 3.6140
Training Epoch: 1 [36608/50048]	Loss: 3.2764
Training Epoch: 1 [36736/50048]	Loss: 3.4921
Training Epoch: 1 [36864/50048]	Loss: 3.5097
Training Epoch: 1 [36992/50048]	Loss: 3.4784
Training Epoch: 1 [37120/50048]	Loss: 3.3963
Training Epoch: 1 [37248/50048]	Loss: 3.3147
Training Epoch: 1 [37376/50048]	Loss: 3.3569
Training Epoch: 1 [37504/50048]	Loss: 3.6261
Training Epoch: 1 [37632/50048]	Loss: 3.4495
Training Epoch: 1 [37760/50048]	Loss: 3.4977
Training Epoch: 1 [37888/50048]	Loss: 3.6023
Training Epoch: 1 [38016/50048]	Loss: 3.5683
Training Epoch: 1 [38144/50048]	Loss: 3.4211
Training Epoch: 1 [38272/50048]	Loss: 3.7007
Training Epoch: 1 [38400/50048]	Loss: 3.4769
Training Epoch: 1 [38528/50048]	Loss: 3.4523
Training Epoch: 1 [38656/50048]	Loss: 3.3102
Training Epoch: 1 [38784/50048]	Loss: 3.4455
Training Epoch: 1 [38912/50048]	Loss: 3.4805
Training Epoch: 1 [39040/50048]	Loss: 3.6532
Training Epoch: 1 [39168/50048]	Loss: 3.4153
Training Epoch: 1 [39296/50048]	Loss: 3.4148
Training Epoch: 1 [39424/50048]	Loss: 3.4521
Training Epoch: 1 [39552/50048]	Loss: 3.5076
Training Epoch: 1 [39680/50048]	Loss: 3.4227
Training Epoch: 1 [39808/50048]	Loss: 3.3676
Training Epoch: 1 [39936/50048]	Loss: 3.2305
Training Epoch: 1 [40064/50048]	Loss: 3.1892
Training Epoch: 1 [40192/50048]	Loss: 3.3486
Training Epoch: 1 [40320/50048]	Loss: 3.4802
Training Epoch: 1 [40448/50048]	Loss: 3.4863
Training Epoch: 1 [40576/50048]	Loss: 3.5022
Training Epoch: 1 [40704/50048]	Loss: 3.3035
Training Epoch: 1 [40832/50048]	Loss: 3.4755
Training Epoch: 1 [40960/50048]	Loss: 3.4998
Training Epoch: 1 [41088/50048]	Loss: 3.2763
Training Epoch: 1 [41216/50048]	Loss: 3.4780
Training Epoch: 1 [41344/50048]	Loss: 3.1670
Training Epoch: 1 [41472/50048]	Loss: 3.4851
Training Epoch: 1 [41600/50048]	Loss: 3.4912
Training Epoch: 1 [41728/50048]	Loss: 3.5028
Training Epoch: 1 [41856/50048]	Loss: 3.2188
Training Epoch: 1 [41984/50048]	Loss: 3.4552
Training Epoch: 1 [42112/50048]	Loss: 3.5800
Training Epoch: 1 [42240/50048]	Loss: 3.4998
Training Epoch: 1 [42368/50048]	Loss: 3.6556
Training Epoch: 1 [42496/50048]	Loss: 3.4336
Training Epoch: 1 [42624/50048]	Loss: 3.5949
Training Epoch: 1 [42752/50048]	Loss: 3.5484
Training Epoch: 1 [42880/50048]	Loss: 3.3538
Training Epoch: 1 [43008/50048]	Loss: 3.2227
Training Epoch: 1 [43136/50048]	Loss: 3.1672
Training Epoch: 1 [43264/50048]	Loss: 3.5518
Training Epoch: 1 [43392/50048]	Loss: 3.3521
Training Epoch: 1 [43520/50048]	Loss: 3.3383
Training Epoch: 1 [43648/50048]	Loss: 3.2392
Training Epoch: 1 [43776/50048]	Loss: 3.5158
Training Epoch: 1 [43904/50048]	Loss: 3.4931
Training Epoch: 1 [44032/50048]	Loss: 3.4401
Training Epoch: 1 [44160/50048]	Loss: 3.3849
Training Epoch: 1 [44288/50048]	Loss: 3.1409
Training Epoch: 1 [44416/50048]	Loss: 3.6320
Training Epoch: 1 [44544/50048]	Loss: 3.3398
Training Epoch: 1 [44672/50048]	Loss: 3.4287
Training Epoch: 1 [44800/50048]	Loss: 3.2817
Training Epoch: 1 [44928/50048]	Loss: 3.3458
Training Epoch: 1 [45056/50048]	Loss: 3.1819
Training Epoch: 1 [45184/50048]	Loss: 3.4975
Training Epoch: 1 [45312/50048]	Loss: 3.5467
Training Epoch: 1 [45440/50048]	Loss: 3.4421
Training Epoch: 1 [45568/50048]	Loss: 3.4115
Training Epoch: 1 [45696/50048]	Loss: 3.3314
Training Epoch: 1 [45824/50048]	Loss: 3.4095
Training Epoch: 1 [45952/50048]	Loss: 3.2379
Training Epoch: 1 [46080/50048]	Loss: 3.4556
Training Epoch: 1 [46208/50048]	Loss: 3.3403
Training Epoch: 1 [46336/50048]	Loss: 3.3121
Training Epoch: 1 [46464/50048]	Loss: 3.1520
Training Epoch: 1 [46592/50048]	Loss: 3.2433
Training Epoch: 1 [46720/50048]	Loss: 3.3990
Training Epoch: 1 [46848/50048]	Loss: 3.3606
Training Epoch: 1 [46976/50048]	Loss: 3.4874
Training Epoch: 1 [47104/50048]	Loss: 3.4594
Training Epoch: 1 [47232/50048]	Loss: 3.7309
Training Epoch: 1 [47360/50048]	Loss: 3.2932
Training Epoch: 1 [47488/50048]	Loss: 3.3230
Training Epoch: 1 [47616/50048]	Loss: 3.5073
Training Epoch: 1 [47744/50048]	Loss: 3.4293
Training Epoch: 1 [47872/50048]	Loss: 3.5286
Training Epoch: 1 [48000/50048]	Loss: 3.3794
Training Epoch: 1 [48128/50048]	Loss: 3.3322
Training Epoch: 1 [48256/50048]	Loss: 3.3831
Training Epoch: 1 [48384/50048]	Loss: 3.2843
Training Epoch: 1 [48512/50048]	Loss: 3.3001
Training Epoch: 1 [48640/50048]	Loss: 3.2474
Training Epoch: 1 [48768/50048]	Loss: 3.3818
Training Epoch: 1 [48896/50048]	Loss: 3.3459
Training Epoch: 1 [49024/50048]	Loss: 3.2929
Training Epoch: 1 [49152/50048]	Loss: 3.2012
Training Epoch: 1 [49280/50048]	Loss: 3.2848
Training Epoch: 1 [49408/50048]	Loss: 3.4284
Training Epoch: 1 [49536/50048]	Loss: 3.4501
Training Epoch: 1 [49664/50048]	Loss: 3.5883
Training Epoch: 1 [49792/50048]	Loss: 3.5426
Training Epoch: 1 [49920/50048]	Loss: 3.2792
Training Epoch: 1 [50048/50048]	Loss: 3.4314
Validation Epoch: 1, Average loss: 0.0266, Accuracy: 0.1654
Training Epoch: 2 [128/50048]	Loss: 3.2317
Training Epoch: 2 [256/50048]	Loss: 3.2110
Training Epoch: 2 [384/50048]	Loss: 3.1816
Training Epoch: 2 [512/50048]	Loss: 3.1155
Training Epoch: 2 [640/50048]	Loss: 3.1976
Training Epoch: 2 [768/50048]	Loss: 3.2223
Training Epoch: 2 [896/50048]	Loss: 3.2116
Training Epoch: 2 [1024/50048]	Loss: 3.3932
Training Epoch: 2 [1152/50048]	Loss: 3.2610
Training Epoch: 2 [1280/50048]	Loss: 3.2551
Training Epoch: 2 [1408/50048]	Loss: 3.1855
Training Epoch: 2 [1536/50048]	Loss: 3.5579
Training Epoch: 2 [1664/50048]	Loss: 3.3775
Training Epoch: 2 [1792/50048]	Loss: 3.1923
Training Epoch: 2 [1920/50048]	Loss: 3.1862
Training Epoch: 2 [2048/50048]	Loss: 3.3591
Training Epoch: 2 [2176/50048]	Loss: 2.9538
Training Epoch: 2 [2304/50048]	Loss: 3.2188
Training Epoch: 2 [2432/50048]	Loss: 3.2924
Training Epoch: 2 [2560/50048]	Loss: 3.2437
Training Epoch: 2 [2688/50048]	Loss: 3.0517
Training Epoch: 2 [2816/50048]	Loss: 3.1185
Training Epoch: 2 [2944/50048]	Loss: 3.3486
Training Epoch: 2 [3072/50048]	Loss: 3.2270
Training Epoch: 2 [3200/50048]	Loss: 3.2276
Training Epoch: 2 [3328/50048]	Loss: 3.6054
Training Epoch: 2 [3456/50048]	Loss: 3.2416
Training Epoch: 2 [3584/50048]	Loss: 3.2321
Training Epoch: 2 [3712/50048]	Loss: 3.2635
Training Epoch: 2 [3840/50048]	Loss: 3.1053
Training Epoch: 2 [3968/50048]	Loss: 3.3630
Training Epoch: 2 [4096/50048]	Loss: 3.2214
Training Epoch: 2 [4224/50048]	Loss: 3.2018
Training Epoch: 2 [4352/50048]	Loss: 3.1885
Training Epoch: 2 [4480/50048]	Loss: 3.1900
Training Epoch: 2 [4608/50048]	Loss: 3.1730
Training Epoch: 2 [4736/50048]	Loss: 3.1673
Training Epoch: 2 [4864/50048]	Loss: 3.2738
Training Epoch: 2 [4992/50048]	Loss: 3.3275
Training Epoch: 2 [5120/50048]	Loss: 3.1109
Training Epoch: 2 [5248/50048]	Loss: 3.2775
Training Epoch: 2 [5376/50048]	Loss: 2.9375
Training Epoch: 2 [5504/50048]	Loss: 3.2395
Training Epoch: 2 [5632/50048]	Loss: 3.3366
Training Epoch: 2 [5760/50048]	Loss: 2.9688
Training Epoch: 2 [5888/50048]	Loss: 3.1585
Training Epoch: 2 [6016/50048]	Loss: 3.3275
Training Epoch: 2 [6144/50048]	Loss: 3.1905
Training Epoch: 2 [6272/50048]	Loss: 2.9832
Training Epoch: 2 [6400/50048]	Loss: 3.4299
Training Epoch: 2 [6528/50048]	Loss: 3.2316
Training Epoch: 2 [6656/50048]	Loss: 3.5057
Training Epoch: 2 [6784/50048]	Loss: 3.3086
Training Epoch: 2 [6912/50048]	Loss: 3.3193
Training Epoch: 2 [7040/50048]	Loss: 3.0978
Training Epoch: 2 [7168/50048]	Loss: 3.3247
Training Epoch: 2 [7296/50048]	Loss: 3.3522
Training Epoch: 2 [7424/50048]	Loss: 3.2419
Training Epoch: 2 [7552/50048]	Loss: 3.2571
Training Epoch: 2 [7680/50048]	Loss: 3.3211
Training Epoch: 2 [7808/50048]	Loss: 3.2322
Training Epoch: 2 [7936/50048]	Loss: 3.1297
Training Epoch: 2 [8064/50048]	Loss: 3.4537
Training Epoch: 2 [8192/50048]	Loss: 3.3409
Training Epoch: 2 [8320/50048]	Loss: 3.3818
Training Epoch: 2 [8448/50048]	Loss: 3.0559
Training Epoch: 2 [8576/50048]	Loss: 3.2744
Training Epoch: 2 [8704/50048]	Loss: 3.5180
Training Epoch: 2 [8832/50048]	Loss: 3.0265
Training Epoch: 2 [8960/50048]	Loss: 3.2197
Training Epoch: 2 [9088/50048]	Loss: 3.2288
Training Epoch: 2 [9216/50048]	Loss: 3.4399
Training Epoch: 2 [9344/50048]	Loss: 3.0688
Training Epoch: 2 [9472/50048]	Loss: 3.1863
Training Epoch: 2 [9600/50048]	Loss: 3.2598
Training Epoch: 2 [9728/50048]	Loss: 3.1184
Training Epoch: 2 [9856/50048]	Loss: 2.9225
Training Epoch: 2 [9984/50048]	Loss: 3.0701
Training Epoch: 2 [10112/50048]	Loss: 3.1855
Training Epoch: 2 [10240/50048]	Loss: 3.3181
Training Epoch: 2 [10368/50048]	Loss: 3.1382
Training Epoch: 2 [10496/50048]	Loss: 3.1947
Training Epoch: 2 [10624/50048]	Loss: 3.1017
Training Epoch: 2 [10752/50048]	Loss: 3.3797
Training Epoch: 2 [10880/50048]	Loss: 3.4764
Training Epoch: 2 [11008/50048]	Loss: 3.3654
Training Epoch: 2 [11136/50048]	Loss: 3.3663
Training Epoch: 2 [11264/50048]	Loss: 3.0740
Training Epoch: 2 [11392/50048]	Loss: 3.2631
Training Epoch: 2 [11520/50048]	Loss: 3.1512
Training Epoch: 2 [11648/50048]	Loss: 3.1931
Training Epoch: 2 [11776/50048]	Loss: 3.2117
Training Epoch: 2 [11904/50048]	Loss: 3.3814
Training Epoch: 2 [12032/50048]	Loss: 3.1829
Training Epoch: 2 [12160/50048]	Loss: 3.2518
Training Epoch: 2 [12288/50048]	Loss: 3.2191
Training Epoch: 2 [12416/50048]	Loss: 3.2033
Training Epoch: 2 [12544/50048]	Loss: 3.2379
Training Epoch: 2 [12672/50048]	Loss: 3.1335
Training Epoch: 2 [12800/50048]	Loss: 3.3925
Training Epoch: 2 [12928/50048]	Loss: 3.2928
Training Epoch: 2 [13056/50048]	Loss: 3.1984
Training Epoch: 2 [13184/50048]	Loss: 3.1325
Training Epoch: 2 [13312/50048]	Loss: 3.2506
Training Epoch: 2 [13440/50048]	Loss: 3.0378
Training Epoch: 2 [13568/50048]	Loss: 3.4203
Training Epoch: 2 [13696/50048]	Loss: 3.0541
Training Epoch: 2 [13824/50048]	Loss: 3.1149
Training Epoch: 2 [13952/50048]	Loss: 3.0446
Training Epoch: 2 [14080/50048]	Loss: 3.0557
Training Epoch: 2 [14208/50048]	Loss: 3.1524
Training Epoch: 2 [14336/50048]	Loss: 3.4343
Training Epoch: 2 [14464/50048]	Loss: 3.1130
Training Epoch: 2 [14592/50048]	Loss: 3.0790
Training Epoch: 2 [14720/50048]	Loss: 3.1346
Training Epoch: 2 [14848/50048]	Loss: 3.1498
Training Epoch: 2 [14976/50048]	Loss: 3.2030
Training Epoch: 2 [15104/50048]	Loss: 3.1810
Training Epoch: 2 [15232/50048]	Loss: 3.2378
Training Epoch: 2 [15360/50048]	Loss: 3.0931
Training Epoch: 2 [15488/50048]	Loss: 3.2243
Training Epoch: 2 [15616/50048]	Loss: 3.3626
Training Epoch: 2 [15744/50048]	Loss: 3.3095
Training Epoch: 2 [15872/50048]	Loss: 3.0728
Training Epoch: 2 [16000/50048]	Loss: 3.3567
Training Epoch: 2 [16128/50048]	Loss: 3.0591
Training Epoch: 2 [16256/50048]	Loss: 2.9725
Training Epoch: 2 [16384/50048]	Loss: 3.1818
Training Epoch: 2 [16512/50048]	Loss: 2.8568
Training Epoch: 2 [16640/50048]	Loss: 2.9713
Training Epoch: 2 [16768/50048]	Loss: 3.0221
Training Epoch: 2 [16896/50048]	Loss: 3.1064
Training Epoch: 2 [17024/50048]	Loss: 3.1204
Training Epoch: 2 [17152/50048]	Loss: 3.3395
Training Epoch: 2 [17280/50048]	Loss: 3.3322
Training Epoch: 2 [17408/50048]	Loss: 3.0672
Training Epoch: 2 [17536/50048]	Loss: 3.3103
Training Epoch: 2 [17664/50048]	Loss: 3.3247
Training Epoch: 2 [17792/50048]	Loss: 3.4062
Training Epoch: 2 [17920/50048]	Loss: 3.3391
Training Epoch: 2 [18048/50048]	Loss: 3.1129
Training Epoch: 2 [18176/50048]	Loss: 3.1279
Training Epoch: 2 [18304/50048]	Loss: 3.0948
Training Epoch: 2 [18432/50048]	Loss: 3.3882
Training Epoch: 2 [18560/50048]	Loss: 3.2433
Training Epoch: 2 [18688/50048]	Loss: 3.0733
Training Epoch: 2 [18816/50048]	Loss: 3.0810
Training Epoch: 2 [18944/50048]	Loss: 3.1845
Training Epoch: 2 [19072/50048]	Loss: 3.1061
Training Epoch: 2 [19200/50048]	Loss: 3.0190
Training Epoch: 2 [19328/50048]	Loss: 3.0775
Training Epoch: 2 [19456/50048]	Loss: 3.0713
Training Epoch: 2 [19584/50048]	Loss: 3.0332
Training Epoch: 2 [19712/50048]	Loss: 3.1662
Training Epoch: 2 [19840/50048]	Loss: 2.9793
Training Epoch: 2 [19968/50048]	Loss: 3.2220
Training Epoch: 2 [20096/50048]	Loss: 3.1381
Training Epoch: 2 [20224/50048]	Loss: 3.2792
Training Epoch: 2 [20352/50048]	Loss: 3.0350
Training Epoch: 2 [20480/50048]	Loss: 3.1728
Training Epoch: 2 [20608/50048]	Loss: 3.0741
Training Epoch: 2 [20736/50048]	Loss: 3.2728
Training Epoch: 2 [20864/50048]	Loss: 3.4649
Training Epoch: 2 [20992/50048]	Loss: 3.1648
Training Epoch: 2 [21120/50048]	Loss: 3.1798
Training Epoch: 2 [21248/50048]	Loss: 3.4279
Training Epoch: 2 [21376/50048]	Loss: 2.7641
Training Epoch: 2 [21504/50048]	Loss: 3.2077
Training Epoch: 2 [21632/50048]	Loss: 3.0795
Training Epoch: 2 [21760/50048]	Loss: 3.0576
Training Epoch: 2 [21888/50048]	Loss: 2.9387
Training Epoch: 2 [22016/50048]	Loss: 3.2527
Training Epoch: 2 [22144/50048]	Loss: 2.9940
Training Epoch: 2 [22272/50048]	Loss: 3.0747
Training Epoch: 2 [22400/50048]	Loss: 3.0352
Training Epoch: 2 [22528/50048]	Loss: 3.1783
Training Epoch: 2 [22656/50048]	Loss: 3.0775
Training Epoch: 2 [22784/50048]	Loss: 3.1928
Training Epoch: 2 [22912/50048]	Loss: 2.9845
Training Epoch: 2 [23040/50048]	Loss: 2.9855
Training Epoch: 2 [23168/50048]	Loss: 2.9775
Training Epoch: 2 [23296/50048]	Loss: 3.0756
Training Epoch: 2 [23424/50048]	Loss: 2.9216
Training Epoch: 2 [23552/50048]	Loss: 3.1667
Training Epoch: 2 [23680/50048]	Loss: 3.0424
Training Epoch: 2 [23808/50048]	Loss: 2.9994
Training Epoch: 2 [23936/50048]	Loss: 3.2931
Training Epoch: 2 [24064/50048]	Loss: 3.1058
Training Epoch: 2 [24192/50048]	Loss: 3.1081
Training Epoch: 2 [24320/50048]	Loss: 2.9329
Training Epoch: 2 [24448/50048]	Loss: 3.2130
Training Epoch: 2 [24576/50048]	Loss: 2.9287
Training Epoch: 2 [24704/50048]	Loss: 2.9544
Training Epoch: 2 [24832/50048]	Loss: 3.0220
Training Epoch: 2 [24960/50048]	Loss: 3.0847
Training Epoch: 2 [25088/50048]	Loss: 3.1494
Training Epoch: 2 [25216/50048]	Loss: 3.1565
Training Epoch: 2 [25344/50048]	Loss: 3.1631
Training Epoch: 2 [25472/50048]	Loss: 2.8804
Training Epoch: 2 [25600/50048]	Loss: 3.2573
Training Epoch: 2 [25728/50048]	Loss: 3.2175
Training Epoch: 2 [25856/50048]	Loss: 3.0375
Training Epoch: 2 [25984/50048]	Loss: 2.9034
Training Epoch: 2 [26112/50048]	Loss: 2.9402
Training Epoch: 2 [26240/50048]	Loss: 2.9397
Training Epoch: 2 [26368/50048]	Loss: 3.0822
Training Epoch: 2 [26496/50048]	Loss: 3.1613
Training Epoch: 2 [26624/50048]	Loss: 3.1589
Training Epoch: 2 [26752/50048]	Loss: 3.0466
Training Epoch: 2 [26880/50048]	Loss: 3.1001
Training Epoch: 2 [27008/50048]	Loss: 3.1304
Training Epoch: 2 [27136/50048]	Loss: 2.9347
Training Epoch: 2 [27264/50048]	Loss: 2.9705
Training Epoch: 2 [27392/50048]	Loss: 2.9327
Training Epoch: 2 [27520/50048]	Loss: 2.9263
Training Epoch: 2 [27648/50048]	Loss: 3.1548
Training Epoch: 2 [27776/50048]	Loss: 2.8811
Training Epoch: 2 [27904/50048]	Loss: 3.0579
Training Epoch: 2 [28032/50048]	Loss: 3.0533
Training Epoch: 2 [28160/50048]	Loss: 2.9276
Training Epoch: 2 [28288/50048]	Loss: 3.1324
Training Epoch: 2 [28416/50048]	Loss: 3.1420
Training Epoch: 2 [28544/50048]	Loss: 2.6957
Training Epoch: 2 [28672/50048]	Loss: 2.9469
Training Epoch: 2 [28800/50048]	Loss: 2.8764
Training Epoch: 2 [28928/50048]	Loss: 3.1936
Training Epoch: 2 [29056/50048]	Loss: 3.0115
Training Epoch: 2 [29184/50048]	Loss: 3.0192
Training Epoch: 2 [29312/50048]	Loss: 3.0303
Training Epoch: 2 [29440/50048]	Loss: 2.8914
Training Epoch: 2 [29568/50048]	Loss: 2.8860
Training Epoch: 2 [29696/50048]	Loss: 2.8615
Training Epoch: 2 [29824/50048]	Loss: 3.0566
Training Epoch: 2 [29952/50048]	Loss: 2.9977
Training Epoch: 2 [30080/50048]	Loss: 3.2785
Training Epoch: 2 [30208/50048]	Loss: 3.3421
Training Epoch: 2 [30336/50048]	Loss: 3.2982
Training Epoch: 2 [30464/50048]	Loss: 3.1630
Training Epoch: 2 [30592/50048]	Loss: 2.9829
Training Epoch: 2 [30720/50048]	Loss: 2.8269
Training Epoch: 2 [30848/50048]	Loss: 2.9135
Training Epoch: 2 [30976/50048]	Loss: 2.8050
Training Epoch: 2 [31104/50048]	Loss: 2.8769
Training Epoch: 2 [31232/50048]	Loss: 3.0044
Training Epoch: 2 [31360/50048]	Loss: 3.0894
Training Epoch: 2 [31488/50048]	Loss: 2.9720
Training Epoch: 2 [31616/50048]	Loss: 3.0138
Training Epoch: 2 [31744/50048]	Loss: 2.7780
Training Epoch: 2 [31872/50048]	Loss: 3.2158
Training Epoch: 2 [32000/50048]	Loss: 2.9503
Training Epoch: 2 [32128/50048]	Loss: 3.0256
Training Epoch: 2 [32256/50048]	Loss: 2.9865
Training Epoch: 2 [32384/50048]	Loss: 3.0627
Training Epoch: 2 [32512/50048]	Loss: 3.0242
Training Epoch: 2 [32640/50048]	Loss: 3.2123
Training Epoch: 2 [32768/50048]	Loss: 3.2314
Training Epoch: 2 [32896/50048]	Loss: 2.8709
Training Epoch: 2 [33024/50048]	Loss: 2.9893
Training Epoch: 2 [33152/50048]	Loss: 3.0327
Training Epoch: 2 [33280/50048]	Loss: 2.8523
Training Epoch: 2 [33408/50048]	Loss: 3.0548
Training Epoch: 2 [33536/50048]	Loss: 3.0685
Training Epoch: 2 [33664/50048]	Loss: 2.8222
Training Epoch: 2 [33792/50048]	Loss: 2.7852
Training Epoch: 2 [33920/50048]	Loss: 2.8886
Training Epoch: 2 [34048/50048]	Loss: 2.8333
Training Epoch: 2 [34176/50048]	Loss: 3.0401
Training Epoch: 2 [34304/50048]	Loss: 3.0003
Training Epoch: 2 [34432/50048]	Loss: 3.0548
Training Epoch: 2 [34560/50048]	Loss: 3.0051
Training Epoch: 2 [34688/50048]	Loss: 2.7716
Training Epoch: 2 [34816/50048]	Loss: 3.2219
Training Epoch: 2 [34944/50048]	Loss: 3.1212
Training Epoch: 2 [35072/50048]	Loss: 3.1733
Training Epoch: 2 [35200/50048]	Loss: 2.9652
Training Epoch: 2 [35328/50048]	Loss: 2.8563
Training Epoch: 2 [35456/50048]	Loss: 3.1246
Training Epoch: 2 [35584/50048]	Loss: 3.1023
Training Epoch: 2 [35712/50048]	Loss: 2.9848
Training Epoch: 2 [35840/50048]	Loss: 2.7861
Training Epoch: 2 [35968/50048]	Loss: 2.8860
Training Epoch: 2 [36096/50048]	Loss: 2.8087
Training Epoch: 2 [36224/50048]	Loss: 3.1049
Training Epoch: 2 [36352/50048]	Loss: 2.8207
Training Epoch: 2 [36480/50048]	Loss: 2.9294
Training Epoch: 2 [36608/50048]	Loss: 2.8865
Training Epoch: 2 [36736/50048]	Loss: 2.9308
Training Epoch: 2 [36864/50048]	Loss: 2.9030
Training Epoch: 2 [36992/50048]	Loss: 2.8185
Training Epoch: 2 [37120/50048]	Loss: 2.8879
Training Epoch: 2 [37248/50048]	Loss: 2.8109
Training Epoch: 2 [37376/50048]	Loss: 2.8294
Training Epoch: 2 [37504/50048]	Loss: 2.8132
Training Epoch: 2 [37632/50048]	Loss: 2.8963
Training Epoch: 2 [37760/50048]	Loss: 3.1520
Training Epoch: 2 [37888/50048]	Loss: 2.9075
Training Epoch: 2 [38016/50048]	Loss: 2.8868
Training Epoch: 2 [38144/50048]	Loss: 2.7837
Training Epoch: 2 [38272/50048]	Loss: 2.8874
Training Epoch: 2 [38400/50048]	Loss: 2.6148
Training Epoch: 2 [38528/50048]	Loss: 3.0007
Training Epoch: 2 [38656/50048]	Loss: 3.0092
Training Epoch: 2 [38784/50048]	Loss: 2.9928
Training Epoch: 2 [38912/50048]	Loss: 2.9222
Training Epoch: 2 [39040/50048]	Loss: 3.1730
Training Epoch: 2 [39168/50048]	Loss: 3.0259
Training Epoch: 2 [39296/50048]	Loss: 2.8262
Training Epoch: 2 [39424/50048]	Loss: 2.9378
Training Epoch: 2 [39552/50048]	Loss: 3.1569
Training Epoch: 2 [39680/50048]	Loss: 2.6462
Training Epoch: 2 [39808/50048]	Loss: 2.8660
Training Epoch: 2 [39936/50048]	Loss: 2.9380
Training Epoch: 2 [40064/50048]	Loss: 2.8628
Training Epoch: 2 [40192/50048]	Loss: 3.1183
Training Epoch: 2 [40320/50048]	Loss: 3.0479
Training Epoch: 2 [40448/50048]	Loss: 2.9682
Training Epoch: 2 [40576/50048]	Loss: 2.7774
Training Epoch: 2 [40704/50048]	Loss: 2.8589
Training Epoch: 2 [40832/50048]	Loss: 2.9335
Training Epoch: 2 [40960/50048]	Loss: 2.7628
Training Epoch: 2 [41088/50048]	Loss: 2.8741
Training Epoch: 2 [41216/50048]	Loss: 3.1822
Training Epoch: 2 [41344/50048]	Loss: 2.9559
Training Epoch: 2 [41472/50048]	Loss: 2.9414
Training Epoch: 2 [41600/50048]	Loss: 2.8648
Training Epoch: 2 [41728/50048]	Loss: 3.0210
Training Epoch: 2 [41856/50048]	Loss: 3.3190
Training Epoch: 2 [41984/50048]	Loss: 2.9980
Training Epoch: 2 [42112/50048]	Loss: 3.1942
Training Epoch: 2 [42240/50048]	Loss: 2.9185
Training Epoch: 2 [42368/50048]	Loss: 3.0200
Training Epoch: 2 [42496/50048]	Loss: 3.1711
Training Epoch: 2 [42624/50048]	Loss: 2.9447
Training Epoch: 2 [42752/50048]	Loss: 2.7234
Training Epoch: 2 [42880/50048]	Loss: 3.3443
Training Epoch: 2 [43008/50048]	Loss: 2.8968
Training Epoch: 2 [43136/50048]	Loss: 2.9914
Training Epoch: 2 [43264/50048]	Loss: 2.9546
Training Epoch: 2 [43392/50048]	Loss: 3.1345
Training Epoch: 2 [43520/50048]	Loss: 2.8334
Training Epoch: 2 [43648/50048]	Loss: 3.0310
Training Epoch: 2 [43776/50048]	Loss: 3.0586
Training Epoch: 2 [43904/50048]	Loss: 2.9391
Training Epoch: 2 [44032/50048]	Loss: 2.7644
Training Epoch: 2 [44160/50048]	Loss: 3.0469
Training Epoch: 2 [44288/50048]	Loss: 2.7925
Training Epoch: 2 [44416/50048]	Loss: 2.9999
Training Epoch: 2 [44544/50048]	Loss: 2.7274
Training Epoch: 2 [44672/50048]	Loss: 3.0165
Training Epoch: 2 [44800/50048]	Loss: 2.7416
Training Epoch: 2 [44928/50048]	Loss: 2.8154
Training Epoch: 2 [45056/50048]	Loss: 2.9928
Training Epoch: 2 [45184/50048]	Loss: 2.8599
Training Epoch: 2 [45312/50048]	Loss: 3.2869
Training Epoch: 2 [45440/50048]	Loss: 2.9297
Training Epoch: 2 [45568/50048]	Loss: 3.0749
Training Epoch: 2 [45696/50048]	Loss: 2.8968
Training Epoch: 2 [45824/50048]	Loss: 3.0829
Training Epoch: 2 [45952/50048]	Loss: 2.8851
Training Epoch: 2 [46080/50048]	Loss: 2.9118
Training Epoch: 2 [46208/50048]	Loss: 2.8677
Training Epoch: 2 [46336/50048]	Loss: 2.9205
Training Epoch: 2 [46464/50048]	Loss: 3.1797
Training Epoch: 2 [46592/50048]	Loss: 2.9216
Training Epoch: 2 [46720/50048]	Loss: 2.9711
Training Epoch: 2 [46848/50048]	Loss: 2.7946
Training Epoch: 2 [46976/50048]	Loss: 2.8533
Training Epoch: 2 [47104/50048]	Loss: 2.7187
Training Epoch: 2 [47232/50048]	Loss: 3.0986
Training Epoch: 2 [47360/50048]	Loss: 2.9365
Training Epoch: 2 [47488/50048]	Loss: 3.0092
Training Epoch: 2 [47616/50048]	Loss: 2.6489
Training Epoch: 2 [47744/50048]	Loss: 2.8689
Training Epoch: 2 [47872/50048]	Loss: 2.6761
Training Epoch: 2 [48000/50048]	Loss: 2.9860
Training Epoch: 2 [48128/50048]	Loss: 3.0458
Training Epoch: 2 [48256/50048]	Loss: 2.7772
Training Epoch: 2 [48384/50048]	Loss: 2.8722
Training Epoch: 2 [48512/50048]	Loss: 3.2022
Training Epoch: 2 [48640/50048]	Loss: 2.7420
Training Epoch: 2 [48768/50048]	Loss: 2.8441
Training Epoch: 2 [48896/50048]	Loss: 2.9300
Training Epoch: 2 [49024/50048]	Loss: 2.6494
Training Epoch: 2 [49152/50048]	Loss: 2.8405
Training Epoch: 2 [49280/50048]	Loss: 2.9423
Training Epoch: 2 [49408/50048]	Loss: 3.1215
Training Epoch: 2 [49536/50048]	Loss: 3.0019
Training Epoch: 2 [49664/50048]	Loss: 2.8580
Training Epoch: 2 [49792/50048]	Loss: 2.9204
Training Epoch: 2 [49920/50048]	Loss: 2.8940
Training Epoch: 2 [50048/50048]	Loss: 2.9486
Validation Epoch: 2, Average loss: 0.0284, Accuracy: 0.1766
Training Epoch: 3 [128/50048]	Loss: 2.6483
Training Epoch: 3 [256/50048]	Loss: 2.5946
Training Epoch: 3 [384/50048]	Loss: 3.0567
Training Epoch: 3 [512/50048]	Loss: 3.0320
Training Epoch: 3 [640/50048]	Loss: 2.7526
Training Epoch: 3 [768/50048]	Loss: 2.6973
Training Epoch: 3 [896/50048]	Loss: 2.7337
Training Epoch: 3 [1024/50048]	Loss: 2.9012
Training Epoch: 3 [1152/50048]	Loss: 2.8620
Training Epoch: 3 [1280/50048]	Loss: 2.9303
Training Epoch: 3 [1408/50048]	Loss: 2.8916
Training Epoch: 3 [1536/50048]	Loss: 2.6302
Training Epoch: 3 [1664/50048]	Loss: 2.7631
Training Epoch: 3 [1792/50048]	Loss: 2.7675
Training Epoch: 3 [1920/50048]	Loss: 2.8656
Training Epoch: 3 [2048/50048]	Loss: 2.8818
Training Epoch: 3 [2176/50048]	Loss: 2.7747
Training Epoch: 3 [2304/50048]	Loss: 2.7484
Training Epoch: 3 [2432/50048]	Loss: 2.7114
Training Epoch: 3 [2560/50048]	Loss: 2.6435
Training Epoch: 3 [2688/50048]	Loss: 2.4726
Training Epoch: 3 [2816/50048]	Loss: 2.9210
Training Epoch: 3 [2944/50048]	Loss: 2.8800
Training Epoch: 3 [3072/50048]	Loss: 2.8866
Training Epoch: 3 [3200/50048]	Loss: 2.8837
Training Epoch: 3 [3328/50048]	Loss: 2.7975
Training Epoch: 3 [3456/50048]	Loss: 2.7185
Training Epoch: 3 [3584/50048]	Loss: 2.9109
Training Epoch: 3 [3712/50048]	Loss: 2.6845
Training Epoch: 3 [3840/50048]	Loss: 2.9265
Training Epoch: 3 [3968/50048]	Loss: 2.6575
Training Epoch: 3 [4096/50048]	Loss: 2.8324
Training Epoch: 3 [4224/50048]	Loss: 2.8664
Training Epoch: 3 [4352/50048]	Loss: 2.9736
Training Epoch: 3 [4480/50048]	Loss: 2.8719
Training Epoch: 3 [4608/50048]	Loss: 2.4809
Training Epoch: 3 [4736/50048]	Loss: 2.9801
Training Epoch: 3 [4864/50048]	Loss: 3.0507
Training Epoch: 3 [4992/50048]	Loss: 2.7236
Training Epoch: 3 [5120/50048]	Loss: 3.0507
Training Epoch: 3 [5248/50048]	Loss: 2.8554
Training Epoch: 3 [5376/50048]	Loss: 2.7152
Training Epoch: 3 [5504/50048]	Loss: 2.8504
Training Epoch: 3 [5632/50048]	Loss: 2.9137
Training Epoch: 3 [5760/50048]	Loss: 2.9019
Training Epoch: 3 [5888/50048]	Loss: 2.7910
Training Epoch: 3 [6016/50048]	Loss: 2.9697
Training Epoch: 3 [6144/50048]	Loss: 2.9398
Training Epoch: 3 [6272/50048]	Loss: 2.9868
Training Epoch: 3 [6400/50048]	Loss: 2.4531
Training Epoch: 3 [6528/50048]	Loss: 2.6391
Training Epoch: 3 [6656/50048]	Loss: 2.8769
Training Epoch: 3 [6784/50048]	Loss: 2.6379
Training Epoch: 3 [6912/50048]	Loss: 2.8245
Training Epoch: 3 [7040/50048]	Loss: 2.7920
Training Epoch: 3 [7168/50048]	Loss: 2.9211
Training Epoch: 3 [7296/50048]	Loss: 2.5838
Training Epoch: 3 [7424/50048]	Loss: 2.9697
Training Epoch: 3 [7552/50048]	Loss: 2.5843
Training Epoch: 3 [7680/50048]	Loss: 2.6984
Training Epoch: 3 [7808/50048]	Loss: 2.8082
Training Epoch: 3 [7936/50048]	Loss: 2.6763
Training Epoch: 3 [8064/50048]	Loss: 3.0068
Training Epoch: 3 [8192/50048]	Loss: 2.7755
Training Epoch: 3 [8320/50048]	Loss: 2.5093
Training Epoch: 3 [8448/50048]	Loss: 2.7437
Training Epoch: 3 [8576/50048]	Loss: 2.6864
Training Epoch: 3 [8704/50048]	Loss: 2.9322
Training Epoch: 3 [8832/50048]	Loss: 2.5764
Training Epoch: 3 [8960/50048]	Loss: 2.7821
Training Epoch: 3 [9088/50048]	Loss: 2.6666
Training Epoch: 3 [9216/50048]	Loss: 2.6930
Training Epoch: 3 [9344/50048]	Loss: 2.5212
Training Epoch: 3 [9472/50048]	Loss: 3.0493
Training Epoch: 3 [9600/50048]	Loss: 3.0258
Training Epoch: 3 [9728/50048]	Loss: 2.6971
Training Epoch: 3 [9856/50048]	Loss: 2.7860
Training Epoch: 3 [9984/50048]	Loss: 2.6014
Training Epoch: 3 [10112/50048]	Loss: 2.6618
Training Epoch: 3 [10240/50048]	Loss: 2.7102
Training Epoch: 3 [10368/50048]	Loss: 3.1711
Training Epoch: 3 [10496/50048]	Loss: 2.6723
Training Epoch: 3 [10624/50048]	Loss: 2.9956
Training Epoch: 3 [10752/50048]	Loss: 2.7476
Training Epoch: 3 [10880/50048]	Loss: 2.8763
Training Epoch: 3 [11008/50048]	Loss: 2.6773
Training Epoch: 3 [11136/50048]	Loss: 2.4799
Training Epoch: 3 [11264/50048]	Loss: 2.6537
Training Epoch: 3 [11392/50048]	Loss: 2.7195
Training Epoch: 3 [11520/50048]	Loss: 2.7650
Training Epoch: 3 [11648/50048]	Loss: 2.9604
Training Epoch: 3 [11776/50048]	Loss: 2.9305
Training Epoch: 3 [11904/50048]	Loss: 2.7171
Training Epoch: 3 [12032/50048]	Loss: 2.7872
Training Epoch: 3 [12160/50048]	Loss: 2.6978
Training Epoch: 3 [12288/50048]	Loss: 2.4695
Training Epoch: 3 [12416/50048]	Loss: 2.9905
Training Epoch: 3 [12544/50048]	Loss: 2.8608
Training Epoch: 3 [12672/50048]	Loss: 2.4129
Training Epoch: 3 [12800/50048]	Loss: 3.0693
Training Epoch: 3 [12928/50048]	Loss: 2.8230
Training Epoch: 3 [13056/50048]	Loss: 2.8843
Training Epoch: 3 [13184/50048]	Loss: 2.5969
Training Epoch: 3 [13312/50048]	Loss: 2.6289
Training Epoch: 3 [13440/50048]	Loss: 2.9996
Training Epoch: 3 [13568/50048]	Loss: 2.5598
Training Epoch: 3 [13696/50048]	Loss: 2.5879
Training Epoch: 3 [13824/50048]	Loss: 2.7580
Training Epoch: 3 [13952/50048]	Loss: 2.7814
Training Epoch: 3 [14080/50048]	Loss: 3.0363
Training Epoch: 3 [14208/50048]	Loss: 2.6918
Training Epoch: 3 [14336/50048]	Loss: 2.5311
Training Epoch: 3 [14464/50048]	Loss: 2.8794
Training Epoch: 3 [14592/50048]	Loss: 2.9475
Training Epoch: 3 [14720/50048]	Loss: 2.9543
Training Epoch: 3 [14848/50048]	Loss: 2.9563
Training Epoch: 3 [14976/50048]	Loss: 2.7368
Training Epoch: 3 [15104/50048]	Loss: 2.8766
Training Epoch: 3 [15232/50048]	Loss: 2.4953
Training Epoch: 3 [15360/50048]	Loss: 2.8574
Training Epoch: 3 [15488/50048]	Loss: 2.5489
Training Epoch: 3 [15616/50048]	Loss: 2.8949
Training Epoch: 3 [15744/50048]	Loss: 2.8337
Training Epoch: 3 [15872/50048]	Loss: 2.6206
Training Epoch: 3 [16000/50048]	Loss: 2.5931
Training Epoch: 3 [16128/50048]	Loss: 2.9140
Training Epoch: 3 [16256/50048]	Loss: 2.6694
Training Epoch: 3 [16384/50048]	Loss: 2.6868
Training Epoch: 3 [16512/50048]	Loss: 2.8175
Training Epoch: 3 [16640/50048]	Loss: 2.7407
Training Epoch: 3 [16768/50048]	Loss: 2.7098
Training Epoch: 3 [16896/50048]	Loss: 2.7489
Training Epoch: 3 [17024/50048]	Loss: 2.7393
Training Epoch: 3 [17152/50048]	Loss: 2.7835
Training Epoch: 3 [17280/50048]	Loss: 2.5475
Training Epoch: 3 [17408/50048]	Loss: 2.5397
Training Epoch: 3 [17536/50048]	Loss: 2.3599
Training Epoch: 3 [17664/50048]	Loss: 2.6308
Training Epoch: 3 [17792/50048]	Loss: 2.6857
Training Epoch: 3 [17920/50048]	Loss: 2.7273
Training Epoch: 3 [18048/50048]	Loss: 2.4899
Training Epoch: 3 [18176/50048]	Loss: 3.0692
Training Epoch: 3 [18304/50048]	Loss: 2.8693
Training Epoch: 3 [18432/50048]	Loss: 2.7413
Training Epoch: 3 [18560/50048]	Loss: 2.7034
Training Epoch: 3 [18688/50048]	Loss: 2.6001
Training Epoch: 3 [18816/50048]	Loss: 2.9266
Training Epoch: 3 [18944/50048]	Loss: 2.6340
Training Epoch: 3 [19072/50048]	Loss: 2.9198
Training Epoch: 3 [19200/50048]	Loss: 2.6486
Training Epoch: 3 [19328/50048]	Loss: 2.6294
Training Epoch: 3 [19456/50048]	Loss: 2.5176
Training Epoch: 3 [19584/50048]	Loss: 2.9219
Training Epoch: 3 [19712/50048]	Loss: 2.8114
Training Epoch: 3 [19840/50048]	Loss: 2.7326
Training Epoch: 3 [19968/50048]	Loss: 2.8342
Training Epoch: 3 [20096/50048]	Loss: 2.7221
Training Epoch: 3 [20224/50048]	Loss: 2.6193
Training Epoch: 3 [20352/50048]	Loss: 2.8305
Training Epoch: 3 [20480/50048]	Loss: 2.5987
Training Epoch: 3 [20608/50048]	Loss: 2.7883
Training Epoch: 3 [20736/50048]	Loss: 2.9147
Training Epoch: 3 [20864/50048]	Loss: 2.4320
Training Epoch: 3 [20992/50048]	Loss: 2.8133
Training Epoch: 3 [21120/50048]	Loss: 2.6688
Training Epoch: 3 [21248/50048]	Loss: 2.7027
Training Epoch: 3 [21376/50048]	Loss: 2.7968
Training Epoch: 3 [21504/50048]	Loss: 2.7693
Training Epoch: 3 [21632/50048]	Loss: 2.8739
Training Epoch: 3 [21760/50048]	Loss: 2.6571
Training Epoch: 3 [21888/50048]	Loss: 2.6429
Training Epoch: 3 [22016/50048]	Loss: 2.6266
Training Epoch: 3 [22144/50048]	Loss: 2.6504
Training Epoch: 3 [22272/50048]	Loss: 2.9554
Training Epoch: 3 [22400/50048]	Loss: 2.6146
Training Epoch: 3 [22528/50048]	Loss: 2.6367
Training Epoch: 3 [22656/50048]	Loss: 2.7367
Training Epoch: 3 [22784/50048]	Loss: 2.5920
Training Epoch: 3 [22912/50048]	Loss: 2.7323
Training Epoch: 3 [23040/50048]	Loss: 2.6737
Training Epoch: 3 [23168/50048]	Loss: 2.6008
Training Epoch: 3 [23296/50048]	Loss: 2.5026
Training Epoch: 3 [23424/50048]	Loss: 2.6344
Training Epoch: 3 [23552/50048]	Loss: 2.5721
Training Epoch: 3 [23680/50048]	Loss: 2.8151
Training Epoch: 3 [23808/50048]	Loss: 2.8864
Training Epoch: 3 [23936/50048]	Loss: 2.6403
Training Epoch: 3 [24064/50048]	Loss: 2.7582
Training Epoch: 3 [24192/50048]	Loss: 2.5600
Training Epoch: 3 [24320/50048]	Loss: 2.6819
Training Epoch: 3 [24448/50048]	Loss: 2.6794
Training Epoch: 3 [24576/50048]	Loss: 2.7615
Training Epoch: 3 [24704/50048]	Loss: 2.8037
Training Epoch: 3 [24832/50048]	Loss: 2.5896
Training Epoch: 3 [24960/50048]	Loss: 2.4255
Training Epoch: 3 [25088/50048]	Loss: 2.8847
Training Epoch: 3 [25216/50048]	Loss: 2.5862
Training Epoch: 3 [25344/50048]	Loss: 2.6704
Training Epoch: 3 [25472/50048]	Loss: 2.5051
Training Epoch: 3 [25600/50048]	Loss: 2.7094
Training Epoch: 3 [25728/50048]	Loss: 2.9449
Training Epoch: 3 [25856/50048]	Loss: 2.6068
Training Epoch: 3 [25984/50048]	Loss: 2.5107
Training Epoch: 3 [26112/50048]	Loss: 2.7427
Training Epoch: 3 [26240/50048]	Loss: 2.5995
Training Epoch: 3 [26368/50048]	Loss: 2.4156
Training Epoch: 3 [26496/50048]	Loss: 2.6561
Training Epoch: 3 [26624/50048]	Loss: 2.4489
Training Epoch: 3 [26752/50048]	Loss: 2.8857
Training Epoch: 3 [26880/50048]	Loss: 2.5556
Training Epoch: 3 [27008/50048]	Loss: 2.4425
Training Epoch: 3 [27136/50048]	Loss: 2.5186
Training Epoch: 3 [27264/50048]	Loss: 2.6417
Training Epoch: 3 [27392/50048]	Loss: 2.7941
Training Epoch: 3 [27520/50048]	Loss: 2.5379
Training Epoch: 3 [27648/50048]	Loss: 2.7931
Training Epoch: 3 [27776/50048]	Loss: 2.5693
Training Epoch: 3 [27904/50048]	Loss: 2.3953
Training Epoch: 3 [28032/50048]	Loss: 2.4596
Training Epoch: 3 [28160/50048]	Loss: 2.8526
Training Epoch: 3 [28288/50048]	Loss: 2.8624
Training Epoch: 3 [28416/50048]	Loss: 2.5764
Training Epoch: 3 [28544/50048]	Loss: 2.7572
Training Epoch: 3 [28672/50048]	Loss: 2.6400
Training Epoch: 3 [28800/50048]	Loss: 2.7943
Training Epoch: 3 [28928/50048]	Loss: 2.4867
Training Epoch: 3 [29056/50048]	Loss: 2.6408
Training Epoch: 3 [29184/50048]	Loss: 2.5982
Training Epoch: 3 [29312/50048]	Loss: 2.3804
Training Epoch: 3 [29440/50048]	Loss: 2.7228
Training Epoch: 3 [29568/50048]	Loss: 2.5589
Training Epoch: 3 [29696/50048]	Loss: 2.8612
Training Epoch: 3 [29824/50048]	Loss: 2.6539
Training Epoch: 3 [29952/50048]	Loss: 2.6744
Training Epoch: 3 [30080/50048]	Loss: 2.7056
Training Epoch: 3 [30208/50048]	Loss: 2.4933
Training Epoch: 3 [30336/50048]	Loss: 2.7193
Training Epoch: 3 [30464/50048]	Loss: 2.7537
Training Epoch: 3 [30592/50048]	Loss: 3.0371
Training Epoch: 3 [30720/50048]	Loss: 2.6470
Training Epoch: 3 [30848/50048]	Loss: 2.9240
Training Epoch: 3 [30976/50048]	Loss: 2.7510
Training Epoch: 3 [31104/50048]	Loss: 2.7833
Training Epoch: 3 [31232/50048]	Loss: 2.6787
Training Epoch: 3 [31360/50048]	Loss: 2.6354
Training Epoch: 3 [31488/50048]	Loss: 2.8734
Training Epoch: 3 [31616/50048]	Loss: 2.9563
Training Epoch: 3 [31744/50048]	Loss: 2.6920
Training Epoch: 3 [31872/50048]	Loss: 2.5606
Training Epoch: 3 [32000/50048]	Loss: 2.5916
Training Epoch: 3 [32128/50048]	Loss: 2.5157
Training Epoch: 3 [32256/50048]	Loss: 2.6230
Training Epoch: 3 [32384/50048]	Loss: 2.5997
Training Epoch: 3 [32512/50048]	Loss: 2.2477
Training Epoch: 3 [32640/50048]	Loss: 2.6732
Training Epoch: 3 [32768/50048]	Loss: 2.8738
Training Epoch: 3 [32896/50048]	Loss: 2.6955
Training Epoch: 3 [33024/50048]	Loss: 2.8979
Training Epoch: 3 [33152/50048]	Loss: 2.6023
Training Epoch: 3 [33280/50048]	Loss: 2.3257
Training Epoch: 3 [33408/50048]	Loss: 2.4038
Training Epoch: 3 [33536/50048]	Loss: 2.8365
Training Epoch: 3 [33664/50048]	Loss: 2.6652
Training Epoch: 3 [33792/50048]	Loss: 2.4391
Training Epoch: 3 [33920/50048]	Loss: 2.6875
Training Epoch: 3 [34048/50048]	Loss: 2.6056
Training Epoch: 3 [34176/50048]	Loss: 2.6918
Training Epoch: 3 [34304/50048]	Loss: 2.4166
Training Epoch: 3 [34432/50048]	Loss: 2.4874
Training Epoch: 3 [34560/50048]	Loss: 2.7990
Training Epoch: 3 [34688/50048]	Loss: 2.5886
Training Epoch: 3 [34816/50048]	Loss: 2.5858
Training Epoch: 3 [34944/50048]	Loss: 2.7101
Training Epoch: 3 [35072/50048]	Loss: 2.6916
Training Epoch: 3 [35200/50048]	Loss: 2.4330
Training Epoch: 3 [35328/50048]	Loss: 2.6288
Training Epoch: 3 [35456/50048]	Loss: 2.3430
Training Epoch: 3 [35584/50048]	Loss: 2.4579
Training Epoch: 3 [35712/50048]	Loss: 2.8000
Training Epoch: 3 [35840/50048]	Loss: 2.7642
Training Epoch: 3 [35968/50048]	Loss: 2.7125
Training Epoch: 3 [36096/50048]	Loss: 2.4445
Training Epoch: 3 [36224/50048]	Loss: 2.6769
Training Epoch: 3 [36352/50048]	Loss: 2.6361
Training Epoch: 3 [36480/50048]	Loss: 2.8749
Training Epoch: 3 [36608/50048]	Loss: 2.8799
Training Epoch: 3 [36736/50048]	Loss: 2.6238
Training Epoch: 3 [36864/50048]	Loss: 2.3741
Training Epoch: 3 [36992/50048]	Loss: 2.7051
Training Epoch: 3 [37120/50048]	Loss: 2.7196
Training Epoch: 3 [37248/50048]	Loss: 2.7568
Training Epoch: 3 [37376/50048]	Loss: 2.8300
Training Epoch: 3 [37504/50048]	Loss: 2.7374
Training Epoch: 3 [37632/50048]	Loss: 2.5732
Training Epoch: 3 [37760/50048]	Loss: 2.5549
Training Epoch: 3 [37888/50048]	Loss: 2.5479
Training Epoch: 3 [38016/50048]	Loss: 2.4512
Training Epoch: 3 [38144/50048]	Loss: 2.6817
Training Epoch: 3 [38272/50048]	Loss: 2.8607
Training Epoch: 3 [38400/50048]	Loss: 2.8426
Training Epoch: 3 [38528/50048]	Loss: 2.8997
Training Epoch: 3 [38656/50048]	Loss: 2.8160
Training Epoch: 3 [38784/50048]	Loss: 2.6733
Training Epoch: 3 [38912/50048]	Loss: 2.7873
Training Epoch: 3 [39040/50048]	Loss: 2.8312
Training Epoch: 3 [39168/50048]	Loss: 2.5871
Training Epoch: 3 [39296/50048]	Loss: 2.4982
Training Epoch: 3 [39424/50048]	Loss: 2.2788
Training Epoch: 3 [39552/50048]	Loss: 2.5946
Training Epoch: 3 [39680/50048]	Loss: 2.6406
Training Epoch: 3 [39808/50048]	Loss: 2.5203
Training Epoch: 3 [39936/50048]	Loss: 2.6013
Training Epoch: 3 [40064/50048]	Loss: 2.7310
Training Epoch: 3 [40192/50048]	Loss: 2.6104
Training Epoch: 3 [40320/50048]	Loss: 2.6649
Training Epoch: 3 [40448/50048]	Loss: 2.2371
Training Epoch: 3 [40576/50048]	Loss: 2.6987
Training Epoch: 3 [40704/50048]	Loss: 2.4401
Training Epoch: 3 [40832/50048]	Loss: 2.6025
Training Epoch: 3 [40960/50048]	Loss: 2.4860
Training Epoch: 3 [41088/50048]	Loss: 2.6927
Training Epoch: 3 [41216/50048]	Loss: 2.6379
Training Epoch: 3 [41344/50048]	Loss: 2.6148
Training Epoch: 3 [41472/50048]	Loss: 2.5524
Training Epoch: 3 [41600/50048]	Loss: 2.8565
Training Epoch: 3 [41728/50048]	Loss: 2.7219
Training Epoch: 3 [41856/50048]	Loss: 2.5388
Training Epoch: 3 [41984/50048]	Loss: 2.5088
Training Epoch: 3 [42112/50048]	Loss: 2.4974
Training Epoch: 3 [42240/50048]	Loss: 2.7119
Training Epoch: 3 [42368/50048]	Loss: 2.4877
Training Epoch: 3 [42496/50048]	Loss: 2.6147
Training Epoch: 3 [42624/50048]	Loss: 2.5356
Training Epoch: 3 [42752/50048]	Loss: 2.5956
Training Epoch: 3 [42880/50048]	Loss: 2.6233
Training Epoch: 3 [43008/50048]	Loss: 2.8048
Training Epoch: 3 [43136/50048]	Loss: 2.5254
Training Epoch: 3 [43264/50048]	Loss: 2.6129
Training Epoch: 3 [43392/50048]	Loss: 2.5714
Training Epoch: 3 [43520/50048]	Loss: 2.6023
Training Epoch: 3 [43648/50048]	Loss: 2.5464
Training Epoch: 3 [43776/50048]	Loss: 2.4835
Training Epoch: 3 [43904/50048]	Loss: 2.6638
Training Epoch: 3 [44032/50048]	Loss: 2.7124
Training Epoch: 3 [44160/50048]	Loss: 2.7012
Training Epoch: 3 [44288/50048]	Loss: 2.6498
Training Epoch: 3 [44416/50048]	Loss: 2.7027
Training Epoch: 3 [44544/50048]	Loss: 2.4678
Training Epoch: 3 [44672/50048]	Loss: 2.4732
Training Epoch: 3 [44800/50048]	Loss: 2.5301
Training Epoch: 3 [44928/50048]	Loss: 2.5211
Training Epoch: 3 [45056/50048]	Loss: 2.5842
Training Epoch: 3 [45184/50048]	Loss: 2.7583
Training Epoch: 3 [45312/50048]	Loss: 2.4499
Training Epoch: 3 [45440/50048]	Loss: 2.4692
Training Epoch: 3 [45568/50048]	Loss: 2.4807
Training Epoch: 3 [45696/50048]	Loss: 2.4701
Training Epoch: 3 [45824/50048]	Loss: 2.5093
Training Epoch: 3 [45952/50048]	Loss: 2.6166
Training Epoch: 3 [46080/50048]	Loss: 2.4635
Training Epoch: 3 [46208/50048]	Loss: 2.7856
Training Epoch: 3 [46336/50048]	Loss: 2.3621
Training Epoch: 3 [46464/50048]	Loss: 2.6508
Training Epoch: 3 [46592/50048]	Loss: 2.3933
Training Epoch: 3 [46720/50048]	Loss: 2.7646
Training Epoch: 3 [46848/50048]	Loss: 2.4504
Training Epoch: 3 [46976/50048]	Loss: 2.4471
Training Epoch: 3 [47104/50048]	Loss: 2.4737
Training Epoch: 3 [47232/50048]	Loss: 2.5589
Training Epoch: 3 [47360/50048]	Loss: 2.3040
Training Epoch: 3 [47488/50048]	Loss: 2.5147
Training Epoch: 3 [47616/50048]	Loss: 2.5765
Training Epoch: 3 [47744/50048]	Loss: 2.7770
Training Epoch: 3 [47872/50048]	Loss: 2.5791
Training Epoch: 3 [48000/50048]	Loss: 2.6152
Training Epoch: 3 [48128/50048]	Loss: 2.6229
Training Epoch: 3 [48256/50048]	Loss: 2.4443
Training Epoch: 3 [48384/50048]	Loss: 2.5185
Training Epoch: 3 [48512/50048]	Loss: 2.2384
Training Epoch: 3 [48640/50048]	Loss: 2.2679
Training Epoch: 3 [48768/50048]	Loss: 2.5434
Training Epoch: 3 [48896/50048]	Loss: 2.3916
Training Epoch: 3 [49024/50048]	Loss: 2.4144
Training Epoch: 3 [49152/50048]	Loss: 2.7055
Training Epoch: 3 [49280/50048]	Loss: 2.3915
Training Epoch: 3 [49408/50048]	Loss: 2.5123
Training Epoch: 3 [49536/50048]	Loss: 2.2658
Training Epoch: 3 [49664/50048]	Loss: 2.5289
Training Epoch: 3 [49792/50048]	Loss: 2.3733
Training Epoch: 3 [49920/50048]	Loss: 2.2584
Training Epoch: 3 [50048/50048]	Loss: 2.7924
Validation Epoch: 3, Average loss: 0.0394, Accuracy: 0.1245
Training Epoch: 4 [128/50048]	Loss: 2.5248
Training Epoch: 4 [256/50048]	Loss: 2.6540
Training Epoch: 4 [384/50048]	Loss: 2.7098
Training Epoch: 4 [512/50048]	Loss: 2.2453
Training Epoch: 4 [640/50048]	Loss: 2.4326
Training Epoch: 4 [768/50048]	Loss: 2.4802
Training Epoch: 4 [896/50048]	Loss: 2.2731
Training Epoch: 4 [1024/50048]	Loss: 2.3995
Training Epoch: 4 [1152/50048]	Loss: 2.5742
Training Epoch: 4 [1280/50048]	Loss: 2.4833
Training Epoch: 4 [1408/50048]	Loss: 2.7457
Training Epoch: 4 [1536/50048]	Loss: 2.5572
Training Epoch: 4 [1664/50048]	Loss: 2.4945
Training Epoch: 4 [1792/50048]	Loss: 2.4091
Training Epoch: 4 [1920/50048]	Loss: 2.1830
Training Epoch: 4 [2048/50048]	Loss: 2.6016
Training Epoch: 4 [2176/50048]	Loss: 2.4897
Training Epoch: 4 [2304/50048]	Loss: 2.4500
Training Epoch: 4 [2432/50048]	Loss: 2.5777
Training Epoch: 4 [2560/50048]	Loss: 2.3952
Training Epoch: 4 [2688/50048]	Loss: 2.4912
Training Epoch: 4 [2816/50048]	Loss: 2.6390
Training Epoch: 4 [2944/50048]	Loss: 2.5661
Training Epoch: 4 [3072/50048]	Loss: 2.4164
Training Epoch: 4 [3200/50048]	Loss: 2.3406
Training Epoch: 4 [3328/50048]	Loss: 2.4806
Training Epoch: 4 [3456/50048]	Loss: 2.4897
Training Epoch: 4 [3584/50048]	Loss: 2.3305
Training Epoch: 4 [3712/50048]	Loss: 2.4677
Training Epoch: 4 [3840/50048]	Loss: 2.6388
Training Epoch: 4 [3968/50048]	Loss: 2.5189
Training Epoch: 4 [4096/50048]	Loss: 2.2860
Training Epoch: 4 [4224/50048]	Loss: 2.5590
Training Epoch: 4 [4352/50048]	Loss: 2.5743
Training Epoch: 4 [4480/50048]	Loss: 2.4167
Training Epoch: 4 [4608/50048]	Loss: 2.3884
Training Epoch: 4 [4736/50048]	Loss: 2.4700
Training Epoch: 4 [4864/50048]	Loss: 2.5881
Training Epoch: 4 [4992/50048]	Loss: 2.7018
Training Epoch: 4 [5120/50048]	Loss: 2.2643
Training Epoch: 4 [5248/50048]	Loss: 2.2215
Training Epoch: 4 [5376/50048]	Loss: 2.5406
Training Epoch: 4 [5504/50048]	Loss: 2.7036
Training Epoch: 4 [5632/50048]	Loss: 2.5116
Training Epoch: 4 [5760/50048]	Loss: 2.5737
Training Epoch: 4 [5888/50048]	Loss: 2.2688
Training Epoch: 4 [6016/50048]	Loss: 2.3737
Training Epoch: 4 [6144/50048]	Loss: 2.5298
Training Epoch: 4 [6272/50048]	Loss: 2.7092
Training Epoch: 4 [6400/50048]	Loss: 2.2271
Training Epoch: 4 [6528/50048]	Loss: 2.2807
Training Epoch: 4 [6656/50048]	Loss: 2.4121
Training Epoch: 4 [6784/50048]	Loss: 2.2846
Training Epoch: 4 [6912/50048]	Loss: 2.6442
Training Epoch: 4 [7040/50048]	Loss: 2.5353
Training Epoch: 4 [7168/50048]	Loss: 2.5979
Training Epoch: 4 [7296/50048]	Loss: 2.4836
Training Epoch: 4 [7424/50048]	Loss: 2.4393
Training Epoch: 4 [7552/50048]	Loss: 2.3250
Training Epoch: 4 [7680/50048]	Loss: 2.3742
Training Epoch: 4 [7808/50048]	Loss: 2.1469
Training Epoch: 4 [7936/50048]	Loss: 2.8581
Training Epoch: 4 [8064/50048]	Loss: 2.4656
Training Epoch: 4 [8192/50048]	Loss: 2.6215
Training Epoch: 4 [8320/50048]	Loss: 2.5221
Training Epoch: 4 [8448/50048]	Loss: 2.4818
Training Epoch: 4 [8576/50048]	Loss: 2.6352
Training Epoch: 4 [8704/50048]	Loss: 2.3268
Training Epoch: 4 [8832/50048]	Loss: 2.0430
Training Epoch: 4 [8960/50048]	Loss: 2.4463
Training Epoch: 4 [9088/50048]	Loss: 2.5029
Training Epoch: 4 [9216/50048]	Loss: 2.4826
Training Epoch: 4 [9344/50048]	Loss: 2.8731
Training Epoch: 4 [9472/50048]	Loss: 2.4573
Training Epoch: 4 [9600/50048]	Loss: 2.1065
Training Epoch: 4 [9728/50048]	Loss: 2.5675
Training Epoch: 4 [9856/50048]	Loss: 2.3936
Training Epoch: 4 [9984/50048]	Loss: 2.9907
Training Epoch: 4 [10112/50048]	Loss: 2.4307
Training Epoch: 4 [10240/50048]	Loss: 2.3993
Training Epoch: 4 [10368/50048]	Loss: 2.3803
Training Epoch: 4 [10496/50048]	Loss: 2.5054
Training Epoch: 4 [10624/50048]	Loss: 2.5262
Training Epoch: 4 [10752/50048]	Loss: 2.6713
Training Epoch: 4 [10880/50048]	Loss: 2.2999
Training Epoch: 4 [11008/50048]	Loss: 2.4062
Training Epoch: 4 [11136/50048]	Loss: 2.3240
Training Epoch: 4 [11264/50048]	Loss: 2.4184
Training Epoch: 4 [11392/50048]	Loss: 2.5152
Training Epoch: 4 [11520/50048]	Loss: 2.4450
Training Epoch: 4 [11648/50048]	Loss: 2.2883
Training Epoch: 4 [11776/50048]	Loss: 2.4215
Training Epoch: 4 [11904/50048]	Loss: 2.4151
Training Epoch: 4 [12032/50048]	Loss: 2.4419
Training Epoch: 4 [12160/50048]	Loss: 2.4063
Training Epoch: 4 [12288/50048]	Loss: 2.3195
Training Epoch: 4 [12416/50048]	Loss: 2.3474
Training Epoch: 4 [12544/50048]	Loss: 2.3203
Training Epoch: 4 [12672/50048]	Loss: 2.1535
Training Epoch: 4 [12800/50048]	Loss: 2.2930
Training Epoch: 4 [12928/50048]	Loss: 2.3477
Training Epoch: 4 [13056/50048]	Loss: 2.5249
Training Epoch: 4 [13184/50048]	Loss: 2.4229
Training Epoch: 4 [13312/50048]	Loss: 2.5463
Training Epoch: 4 [13440/50048]	Loss: 2.1361
Training Epoch: 4 [13568/50048]	Loss: 2.7597
Training Epoch: 4 [13696/50048]	Loss: 2.4425
Training Epoch: 4 [13824/50048]	Loss: 2.7314
Training Epoch: 4 [13952/50048]	Loss: 2.3838
Training Epoch: 4 [14080/50048]	Loss: 2.6002
Training Epoch: 4 [14208/50048]	Loss: 2.3590
Training Epoch: 4 [14336/50048]	Loss: 2.4273
Training Epoch: 4 [14464/50048]	Loss: 2.5118
Training Epoch: 4 [14592/50048]	Loss: 2.3922
Training Epoch: 4 [14720/50048]	Loss: 2.3340
Training Epoch: 4 [14848/50048]	Loss: 2.4583
Training Epoch: 4 [14976/50048]	Loss: 2.5137
Training Epoch: 4 [15104/50048]	Loss: 2.2622
Training Epoch: 4 [15232/50048]	Loss: 2.2387
Training Epoch: 4 [15360/50048]	Loss: 2.3628
Training Epoch: 4 [15488/50048]	Loss: 2.1294
Training Epoch: 4 [15616/50048]	Loss: 2.1574
Training Epoch: 4 [15744/50048]	Loss: 2.6593
Training Epoch: 4 [15872/50048]	Loss: 2.3625
Training Epoch: 4 [16000/50048]	Loss: 2.4769
Training Epoch: 4 [16128/50048]	Loss: 2.4463
Training Epoch: 4 [16256/50048]	Loss: 2.6310
Training Epoch: 4 [16384/50048]	Loss: 2.3441
Training Epoch: 4 [16512/50048]	Loss: 2.4645
Training Epoch: 4 [16640/50048]	Loss: 2.3156
Training Epoch: 4 [16768/50048]	Loss: 2.1205
Training Epoch: 4 [16896/50048]	Loss: 2.5589
Training Epoch: 4 [17024/50048]	Loss: 2.4079
Training Epoch: 4 [17152/50048]	Loss: 2.4003
Training Epoch: 4 [17280/50048]	Loss: 2.4170
Training Epoch: 4 [17408/50048]	Loss: 2.5237
Training Epoch: 4 [17536/50048]	Loss: 2.2715
Training Epoch: 4 [17664/50048]	Loss: 2.3110
Training Epoch: 4 [17792/50048]	Loss: 2.3732
Training Epoch: 4 [17920/50048]	Loss: 2.4337
Training Epoch: 4 [18048/50048]	Loss: 2.5156
Training Epoch: 4 [18176/50048]	Loss: 2.3269
Training Epoch: 4 [18304/50048]	Loss: 2.4527
Training Epoch: 4 [18432/50048]	Loss: 2.2332
Training Epoch: 4 [18560/50048]	Loss: 2.4166
Training Epoch: 4 [18688/50048]	Loss: 2.6967
Training Epoch: 4 [18816/50048]	Loss: 2.1855
Training Epoch: 4 [18944/50048]	Loss: 2.3853
Training Epoch: 4 [19072/50048]	Loss: 2.5644
Training Epoch: 4 [19200/50048]	Loss: 2.5844
Training Epoch: 4 [19328/50048]	Loss: 2.6310
Training Epoch: 4 [19456/50048]	Loss: 2.3101
Training Epoch: 4 [19584/50048]	Loss: 2.3369
Training Epoch: 4 [19712/50048]	Loss: 2.2345
Training Epoch: 4 [19840/50048]	Loss: 2.4487
Training Epoch: 4 [19968/50048]	Loss: 2.4679
Training Epoch: 4 [20096/50048]	Loss: 2.3958
Training Epoch: 4 [20224/50048]	Loss: 2.4549
Training Epoch: 4 [20352/50048]	Loss: 2.5068
Training Epoch: 4 [20480/50048]	Loss: 2.3157
Training Epoch: 4 [20608/50048]	Loss: 2.5465
Training Epoch: 4 [20736/50048]	Loss: 2.3698
Training Epoch: 4 [20864/50048]	Loss: 2.2543
Training Epoch: 4 [20992/50048]	Loss: 2.2987
Training Epoch: 4 [21120/50048]	Loss: 2.3945
Training Epoch: 4 [21248/50048]	Loss: 2.6420
Training Epoch: 4 [21376/50048]	Loss: 2.7637
Training Epoch: 4 [21504/50048]	Loss: 2.4387
Training Epoch: 4 [21632/50048]	Loss: 2.2029
Training Epoch: 4 [21760/50048]	Loss: 2.5073
Training Epoch: 4 [21888/50048]	Loss: 2.3175
Training Epoch: 4 [22016/50048]	Loss: 2.6133
Training Epoch: 4 [22144/50048]	Loss: 2.3944
Training Epoch: 4 [22272/50048]	Loss: 2.1952
Training Epoch: 4 [22400/50048]	Loss: 2.3226
Training Epoch: 4 [22528/50048]	Loss: 2.5572
Training Epoch: 4 [22656/50048]	Loss: 2.2453
Training Epoch: 4 [22784/50048]	Loss: 2.5612
Training Epoch: 4 [22912/50048]	Loss: 2.2528
Training Epoch: 4 [23040/50048]	Loss: 2.3583
Training Epoch: 4 [23168/50048]	Loss: 2.1106
Training Epoch: 4 [23296/50048]	Loss: 2.4764
Training Epoch: 4 [23424/50048]	Loss: 2.1926
Training Epoch: 4 [23552/50048]	Loss: 2.7639
Training Epoch: 4 [23680/50048]	Loss: 2.5278
Training Epoch: 4 [23808/50048]	Loss: 2.4652
Training Epoch: 4 [23936/50048]	Loss: 2.2530
Training Epoch: 4 [24064/50048]	Loss: 2.0836
Training Epoch: 4 [24192/50048]	Loss: 2.4336
Training Epoch: 4 [24320/50048]	Loss: 2.6013
Training Epoch: 4 [24448/50048]	Loss: 2.2562
Training Epoch: 4 [24576/50048]	Loss: 2.2114
Training Epoch: 4 [24704/50048]	Loss: 2.2878
Training Epoch: 4 [24832/50048]	Loss: 2.2083
Training Epoch: 4 [24960/50048]	Loss: 2.2395
Training Epoch: 4 [25088/50048]	Loss: 2.2582
Training Epoch: 4 [25216/50048]	Loss: 2.2014
Training Epoch: 4 [25344/50048]	Loss: 2.3074
Training Epoch: 4 [25472/50048]	Loss: 2.3034
Training Epoch: 4 [25600/50048]	Loss: 2.2514
Training Epoch: 4 [25728/50048]	Loss: 2.3441
Training Epoch: 4 [25856/50048]	Loss: 2.5975
Training Epoch: 4 [25984/50048]	Loss: 2.4802
Training Epoch: 4 [26112/50048]	Loss: 2.2473
Training Epoch: 4 [26240/50048]	Loss: 2.3959
Training Epoch: 4 [26368/50048]	Loss: 2.5435
Training Epoch: 4 [26496/50048]	Loss: 2.6705
Training Epoch: 4 [26624/50048]	Loss: 2.4566
Training Epoch: 4 [26752/50048]	Loss: 2.4679
Training Epoch: 4 [26880/50048]	Loss: 2.3582
Training Epoch: 4 [27008/50048]	Loss: 2.5064
Training Epoch: 4 [27136/50048]	Loss: 2.1535
Training Epoch: 4 [27264/50048]	Loss: 2.6909
Training Epoch: 4 [27392/50048]	Loss: 2.2636
Training Epoch: 4 [27520/50048]	Loss: 2.4264
Training Epoch: 4 [27648/50048]	Loss: 2.4811
Training Epoch: 4 [27776/50048]	Loss: 2.2984
Training Epoch: 4 [27904/50048]	Loss: 2.2788
Training Epoch: 4 [28032/50048]	Loss: 2.3495
Training Epoch: 4 [28160/50048]	Loss: 2.5015
Training Epoch: 4 [28288/50048]	Loss: 2.3687
Training Epoch: 4 [28416/50048]	Loss: 2.3939
Training Epoch: 4 [28544/50048]	Loss: 2.5390
Training Epoch: 4 [28672/50048]	Loss: 2.4785
Training Epoch: 4 [28800/50048]	Loss: 2.3386
Training Epoch: 4 [28928/50048]	Loss: 2.3071
Training Epoch: 4 [29056/50048]	Loss: 2.3269
Training Epoch: 4 [29184/50048]	Loss: 2.4506
Training Epoch: 4 [29312/50048]	Loss: 2.3822
Training Epoch: 4 [29440/50048]	Loss: 2.3387
Training Epoch: 4 [29568/50048]	Loss: 2.3578
Training Epoch: 4 [29696/50048]	Loss: 2.3416
Training Epoch: 4 [29824/50048]	Loss: 2.4195
Training Epoch: 4 [29952/50048]	Loss: 2.2530
Training Epoch: 4 [30080/50048]	Loss: 2.3544
Training Epoch: 4 [30208/50048]	Loss: 2.3311
Training Epoch: 4 [30336/50048]	Loss: 2.5278
Training Epoch: 4 [30464/50048]	Loss: 2.4429
Training Epoch: 4 [30592/50048]	Loss: 2.5699
Training Epoch: 4 [30720/50048]	Loss: 2.4508
Training Epoch: 4 [30848/50048]	Loss: 2.3950
Training Epoch: 4 [30976/50048]	Loss: 2.4155
Training Epoch: 4 [31104/50048]	Loss: 2.2110
Training Epoch: 4 [31232/50048]	Loss: 2.2273
Training Epoch: 4 [31360/50048]	Loss: 2.1524
Training Epoch: 4 [31488/50048]	Loss: 2.2288
Training Epoch: 4 [31616/50048]	Loss: 2.5038
Training Epoch: 4 [31744/50048]	Loss: 2.3690
Training Epoch: 4 [31872/50048]	Loss: 2.6670
Training Epoch: 4 [32000/50048]	Loss: 2.2163
Training Epoch: 4 [32128/50048]	Loss: 2.2290
Training Epoch: 4 [32256/50048]	Loss: 2.1716
Training Epoch: 4 [32384/50048]	Loss: 2.2959
Training Epoch: 4 [32512/50048]	Loss: 2.3159
Training Epoch: 4 [32640/50048]	Loss: 2.4250
Training Epoch: 4 [32768/50048]	Loss: 2.4043
Training Epoch: 4 [32896/50048]	Loss: 2.4266
Training Epoch: 4 [33024/50048]	Loss: 2.3224
Training Epoch: 4 [33152/50048]	Loss: 2.6131
Training Epoch: 4 [33280/50048]	Loss: 2.2840
Training Epoch: 4 [33408/50048]	Loss: 2.1759
Training Epoch: 4 [33536/50048]	Loss: 2.2558
Training Epoch: 4 [33664/50048]	Loss: 2.3791
Training Epoch: 4 [33792/50048]	Loss: 2.3954
Training Epoch: 4 [33920/50048]	Loss: 2.2486
Training Epoch: 4 [34048/50048]	Loss: 2.3507
Training Epoch: 4 [34176/50048]	Loss: 2.3421
Training Epoch: 4 [34304/50048]	Loss: 2.4348
Training Epoch: 4 [34432/50048]	Loss: 2.2781
Training Epoch: 4 [34560/50048]	Loss: 2.3340
Training Epoch: 4 [34688/50048]	Loss: 2.3978
Training Epoch: 4 [34816/50048]	Loss: 2.2006
Training Epoch: 4 [34944/50048]	Loss: 2.2024
Training Epoch: 4 [35072/50048]	Loss: 2.6322
Training Epoch: 4 [35200/50048]	Loss: 2.4147
Training Epoch: 4 [35328/50048]	Loss: 2.1227
Training Epoch: 4 [35456/50048]	Loss: 2.4388
Training Epoch: 4 [35584/50048]	Loss: 2.1674
Training Epoch: 4 [35712/50048]	Loss: 2.3829
Training Epoch: 4 [35840/50048]	Loss: 2.4761
Training Epoch: 4 [35968/50048]	Loss: 2.4237
Training Epoch: 4 [36096/50048]	Loss: 2.4124
Training Epoch: 4 [36224/50048]	Loss: 2.4818
Training Epoch: 4 [36352/50048]	Loss: 2.4872
Training Epoch: 4 [36480/50048]	Loss: 2.2581
Training Epoch: 4 [36608/50048]	Loss: 2.3285
Training Epoch: 4 [36736/50048]	Loss: 2.2298
Training Epoch: 4 [36864/50048]	Loss: 2.3308
Training Epoch: 4 [36992/50048]	Loss: 2.3236
Training Epoch: 4 [37120/50048]	Loss: 2.3379
Training Epoch: 4 [37248/50048]	Loss: 2.3659
Training Epoch: 4 [37376/50048]	Loss: 2.2847
Training Epoch: 4 [37504/50048]	Loss: 2.4811
Training Epoch: 4 [37632/50048]	Loss: 2.1058
Training Epoch: 4 [37760/50048]	Loss: 2.3622
Training Epoch: 4 [37888/50048]	Loss: 2.4225
Training Epoch: 4 [38016/50048]	Loss: 2.3998
Training Epoch: 4 [38144/50048]	Loss: 2.3999
Training Epoch: 4 [38272/50048]	Loss: 2.4611
Training Epoch: 4 [38400/50048]	Loss: 2.7041
Training Epoch: 4 [38528/50048]	Loss: 2.5655
Training Epoch: 4 [38656/50048]	Loss: 2.2636
Training Epoch: 4 [38784/50048]	Loss: 2.6699
Training Epoch: 4 [38912/50048]	Loss: 2.4921
Training Epoch: 4 [39040/50048]	Loss: 2.2237
Training Epoch: 4 [39168/50048]	Loss: 2.3459
Training Epoch: 4 [39296/50048]	Loss: 2.3006
Training Epoch: 4 [39424/50048]	Loss: 2.3410
Training Epoch: 4 [39552/50048]	Loss: 2.5766
Training Epoch: 4 [39680/50048]	Loss: 2.2843
Training Epoch: 4 [39808/50048]	Loss: 2.4278
Training Epoch: 4 [39936/50048]	Loss: 2.1353
Training Epoch: 4 [40064/50048]	Loss: 2.5438
Training Epoch: 4 [40192/50048]	Loss: 2.4038
Training Epoch: 4 [40320/50048]	Loss: 2.4365
Training Epoch: 4 [40448/50048]	Loss: 2.5485
Training Epoch: 4 [40576/50048]	Loss: 2.4893
Training Epoch: 4 [40704/50048]	Loss: 2.3717
Training Epoch: 4 [40832/50048]	Loss: 2.0828
Training Epoch: 4 [40960/50048]	Loss: 2.3207
Training Epoch: 4 [41088/50048]	Loss: 2.2449
Training Epoch: 4 [41216/50048]	Loss: 2.3044
Training Epoch: 4 [41344/50048]	Loss: 2.4586
Training Epoch: 4 [41472/50048]	Loss: 2.5112
Training Epoch: 4 [41600/50048]	Loss: 2.3912
Training Epoch: 4 [41728/50048]	Loss: 2.3054
Training Epoch: 4 [41856/50048]	Loss: 2.4841
Training Epoch: 4 [41984/50048]	Loss: 2.3048
Training Epoch: 4 [42112/50048]	Loss: 2.6256
Training Epoch: 4 [42240/50048]	Loss: 2.1769
Training Epoch: 4 [42368/50048]	Loss: 2.3017
Training Epoch: 4 [42496/50048]	Loss: 2.5348
Training Epoch: 4 [42624/50048]	Loss: 2.2320
Training Epoch: 4 [42752/50048]	Loss: 2.0851
Training Epoch: 4 [42880/50048]	Loss: 2.6177
Training Epoch: 4 [43008/50048]	Loss: 2.0703
Training Epoch: 4 [43136/50048]	Loss: 2.1083
Training Epoch: 4 [43264/50048]	Loss: 2.2100
Training Epoch: 4 [43392/50048]	Loss: 2.2820
Training Epoch: 4 [43520/50048]	Loss: 2.5543
Training Epoch: 4 [43648/50048]	Loss: 2.2833
Training Epoch: 4 [43776/50048]	Loss: 2.2703
Training Epoch: 4 [43904/50048]	Loss: 2.4903
Training Epoch: 4 [44032/50048]	Loss: 2.3306
Training Epoch: 4 [44160/50048]	Loss: 2.4836
Training Epoch: 4 [44288/50048]	Loss: 2.4586
Training Epoch: 4 [44416/50048]	Loss: 2.0180
Training Epoch: 4 [44544/50048]	Loss: 2.1411
Training Epoch: 4 [44672/50048]	Loss: 2.5107
Training Epoch: 4 [44800/50048]	Loss: 2.3585
Training Epoch: 4 [44928/50048]	Loss: 2.5248
Training Epoch: 4 [45056/50048]	Loss: 2.2910
Training Epoch: 4 [45184/50048]	Loss: 2.0816
Training Epoch: 4 [45312/50048]	Loss: 2.2532
Training Epoch: 4 [45440/50048]	Loss: 2.4700
Training Epoch: 4 [45568/50048]	Loss: 2.4059
Training Epoch: 4 [45696/50048]	Loss: 2.6049
Training Epoch: 4 [45824/50048]	Loss: 2.2670
Training Epoch: 4 [45952/50048]	Loss: 2.2863
Training Epoch: 4 [46080/50048]	Loss: 2.2852
Training Epoch: 4 [46208/50048]	Loss: 2.4068
Training Epoch: 4 [46336/50048]	Loss: 2.2294
Training Epoch: 4 [46464/50048]	Loss: 2.4387
Training Epoch: 4 [46592/50048]	Loss: 2.2206
Training Epoch: 4 [46720/50048]	Loss: 2.3989
Training Epoch: 4 [46848/50048]	Loss: 2.3047
Training Epoch: 4 [46976/50048]	Loss: 2.3037
Training Epoch: 4 [47104/50048]	Loss: 2.2767
Training Epoch: 4 [47232/50048]	Loss: 2.1469
Training Epoch: 4 [47360/50048]	Loss: 2.0966
Training Epoch: 4 [47488/50048]	Loss: 2.6153
Training Epoch: 4 [47616/50048]	Loss: 2.2724
Training Epoch: 4 [47744/50048]	Loss: 2.3646
Training Epoch: 4 [47872/50048]	Loss: 2.0880
Training Epoch: 4 [48000/50048]	Loss: 2.1431
Training Epoch: 4 [48128/50048]	Loss: 2.2770
Training Epoch: 4 [48256/50048]	Loss: 2.2620
Training Epoch: 4 [48384/50048]	Loss: 2.5266
Training Epoch: 4 [48512/50048]	Loss: 2.2821
Training Epoch: 4 [48640/50048]	Loss: 2.4391
Training Epoch: 4 [48768/50048]	Loss: 2.3050
Training Epoch: 4 [48896/50048]	Loss: 2.2137
Training Epoch: 4 [49024/50048]	Loss: 2.2078
Training Epoch: 4 [49152/50048]	Loss: 2.3583
Training Epoch: 4 [49280/50048]	Loss: 2.3710
Training Epoch: 4 [49408/50048]	Loss: 2.1503
Training Epoch: 4 [49536/50048]	Loss: 2.0816
Training Epoch: 4 [49664/50048]	Loss: 2.0098
Training Epoch: 4 [49792/50048]	Loss: 2.3373
Training Epoch: 4 [49920/50048]	Loss: 2.1514
Training Epoch: 4 [50048/50048]	Loss: 1.9918
Validation Epoch: 4, Average loss: 0.0279, Accuracy: 0.2443
Training Epoch: 5 [128/50048]	Loss: 2.0617
Training Epoch: 5 [256/50048]	Loss: 2.1550
Training Epoch: 5 [384/50048]	Loss: 2.4739
Training Epoch: 5 [512/50048]	Loss: 2.2422
Training Epoch: 5 [640/50048]	Loss: 2.3560
Training Epoch: 5 [768/50048]	Loss: 2.2024
Training Epoch: 5 [896/50048]	Loss: 2.0386
Training Epoch: 5 [1024/50048]	Loss: 2.2004
Training Epoch: 5 [1152/50048]	Loss: 2.4186
Training Epoch: 5 [1280/50048]	Loss: 2.2255
Training Epoch: 5 [1408/50048]	Loss: 2.4717
Training Epoch: 5 [1536/50048]	Loss: 2.2017
Training Epoch: 5 [1664/50048]	Loss: 2.2107
Training Epoch: 5 [1792/50048]	Loss: 2.0296
Training Epoch: 5 [1920/50048]	Loss: 2.3829
Training Epoch: 5 [2048/50048]	Loss: 2.2678
Training Epoch: 5 [2176/50048]	Loss: 2.4906
Training Epoch: 5 [2304/50048]	Loss: 2.5058
Training Epoch: 5 [2432/50048]	Loss: 2.4096
Training Epoch: 5 [2560/50048]	Loss: 2.4492
Training Epoch: 5 [2688/50048]	Loss: 2.3739
Training Epoch: 5 [2816/50048]	Loss: 2.1644
Training Epoch: 5 [2944/50048]	Loss: 2.2239
Training Epoch: 5 [3072/50048]	Loss: 2.1569
Training Epoch: 5 [3200/50048]	Loss: 2.4433
Training Epoch: 5 [3328/50048]	Loss: 2.1753
Training Epoch: 5 [3456/50048]	Loss: 2.2006
Training Epoch: 5 [3584/50048]	Loss: 2.0534
Training Epoch: 5 [3712/50048]	Loss: 2.4302
Training Epoch: 5 [3840/50048]	Loss: 2.2529
Training Epoch: 5 [3968/50048]	Loss: 2.1434
Training Epoch: 5 [4096/50048]	Loss: 2.3258
Training Epoch: 5 [4224/50048]	Loss: 2.0280
Training Epoch: 5 [4352/50048]	Loss: 2.3329
Training Epoch: 5 [4480/50048]	Loss: 2.1850
Training Epoch: 5 [4608/50048]	Loss: 2.1887
Training Epoch: 5 [4736/50048]	Loss: 2.1657
Training Epoch: 5 [4864/50048]	Loss: 2.2828
Training Epoch: 5 [4992/50048]	Loss: 2.1329
Training Epoch: 5 [5120/50048]	Loss: 2.1257
Training Epoch: 5 [5248/50048]	Loss: 2.0784
Training Epoch: 5 [5376/50048]	Loss: 2.3824
Training Epoch: 5 [5504/50048]	Loss: 2.1259
Training Epoch: 5 [5632/50048]	Loss: 2.1943
Training Epoch: 5 [5760/50048]	Loss: 1.9537
Training Epoch: 5 [5888/50048]	Loss: 2.2472
Training Epoch: 5 [6016/50048]	Loss: 2.0141
Training Epoch: 5 [6144/50048]	Loss: 1.9789
Training Epoch: 5 [6272/50048]	Loss: 2.3466
Training Epoch: 5 [6400/50048]	Loss: 2.0032
Training Epoch: 5 [6528/50048]	Loss: 2.2025
Training Epoch: 5 [6656/50048]	Loss: 2.0749
Training Epoch: 5 [6784/50048]	Loss: 1.9730
Training Epoch: 5 [6912/50048]	Loss: 2.1908
Training Epoch: 5 [7040/50048]	Loss: 2.1249
Training Epoch: 5 [7168/50048]	Loss: 2.3398
Training Epoch: 5 [7296/50048]	Loss: 2.2854
Training Epoch: 5 [7424/50048]	Loss: 2.2628
Training Epoch: 5 [7552/50048]	Loss: 2.3474
Training Epoch: 5 [7680/50048]	Loss: 2.2942
Training Epoch: 5 [7808/50048]	Loss: 2.2603
Training Epoch: 5 [7936/50048]	Loss: 2.1104
Training Epoch: 5 [8064/50048]	Loss: 2.1902
Training Epoch: 5 [8192/50048]	Loss: 2.0850
Training Epoch: 5 [8320/50048]	Loss: 2.0314
Training Epoch: 5 [8448/50048]	Loss: 2.1923
Training Epoch: 5 [8576/50048]	Loss: 2.1717
Training Epoch: 5 [8704/50048]	Loss: 1.9169
Training Epoch: 5 [8832/50048]	Loss: 2.3062
Training Epoch: 5 [8960/50048]	Loss: 2.1654
Training Epoch: 5 [9088/50048]	Loss: 2.1609
Training Epoch: 5 [9216/50048]	Loss: 2.2552
Training Epoch: 5 [9344/50048]	Loss: 2.1285
Training Epoch: 5 [9472/50048]	Loss: 2.2303
Training Epoch: 5 [9600/50048]	Loss: 2.2714
Training Epoch: 5 [9728/50048]	Loss: 2.2118
Training Epoch: 5 [9856/50048]	Loss: 2.2743
Training Epoch: 5 [9984/50048]	Loss: 2.0132
Training Epoch: 5 [10112/50048]	Loss: 2.0360
Training Epoch: 5 [10240/50048]	Loss: 1.9840
Training Epoch: 5 [10368/50048]	Loss: 2.2998
Training Epoch: 5 [10496/50048]	Loss: 2.1706
Training Epoch: 5 [10624/50048]	Loss: 2.1562
Training Epoch: 5 [10752/50048]	Loss: 2.2064
Training Epoch: 5 [10880/50048]	Loss: 2.4986
Training Epoch: 5 [11008/50048]	Loss: 2.0912
Training Epoch: 5 [11136/50048]	Loss: 2.0237
Training Epoch: 5 [11264/50048]	Loss: 2.0936
Training Epoch: 5 [11392/50048]	Loss: 2.4078
Training Epoch: 5 [11520/50048]	Loss: 2.3005
Training Epoch: 5 [11648/50048]	Loss: 2.2283
Training Epoch: 5 [11776/50048]	Loss: 2.2733
Training Epoch: 5 [11904/50048]	Loss: 2.1603
Training Epoch: 5 [12032/50048]	Loss: 1.8776
Training Epoch: 5 [12160/50048]	Loss: 2.2325
Training Epoch: 5 [12288/50048]	Loss: 2.0596
Training Epoch: 5 [12416/50048]	Loss: 2.3735
Training Epoch: 5 [12544/50048]	Loss: 2.1911
Training Epoch: 5 [12672/50048]	Loss: 2.3027
Training Epoch: 5 [12800/50048]	Loss: 2.2650
Training Epoch: 5 [12928/50048]	Loss: 2.1329
Training Epoch: 5 [13056/50048]	Loss: 1.9838
Training Epoch: 5 [13184/50048]	Loss: 2.0519
Training Epoch: 5 [13312/50048]	Loss: 2.3068
Training Epoch: 5 [13440/50048]	Loss: 1.9263
Training Epoch: 5 [13568/50048]	Loss: 2.3440
Training Epoch: 5 [13696/50048]	Loss: 2.0746
Training Epoch: 5 [13824/50048]	Loss: 2.3706
Training Epoch: 5 [13952/50048]	Loss: 2.1448
Training Epoch: 5 [14080/50048]	Loss: 1.9583
Training Epoch: 5 [14208/50048]	Loss: 2.2247
Training Epoch: 5 [14336/50048]	Loss: 2.2582
Training Epoch: 5 [14464/50048]	Loss: 2.1474
Training Epoch: 5 [14592/50048]	Loss: 2.4167
Training Epoch: 5 [14720/50048]	Loss: 2.3186
Training Epoch: 5 [14848/50048]	Loss: 2.3348
Training Epoch: 5 [14976/50048]	Loss: 2.0552
Training Epoch: 5 [15104/50048]	Loss: 2.2570
Training Epoch: 5 [15232/50048]	Loss: 2.0323
Training Epoch: 5 [15360/50048]	Loss: 2.0029
Training Epoch: 5 [15488/50048]	Loss: 2.3442
Training Epoch: 5 [15616/50048]	Loss: 2.2350
Training Epoch: 5 [15744/50048]	Loss: 2.3403
Training Epoch: 5 [15872/50048]	Loss: 2.2757
Training Epoch: 5 [16000/50048]	Loss: 2.2835
Training Epoch: 5 [16128/50048]	Loss: 2.2052
Training Epoch: 5 [16256/50048]	Loss: 2.0802
Training Epoch: 5 [16384/50048]	Loss: 2.3757
Training Epoch: 5 [16512/50048]	Loss: 2.3116
Training Epoch: 5 [16640/50048]	Loss: 2.3125
Training Epoch: 5 [16768/50048]	Loss: 2.2133
Training Epoch: 5 [16896/50048]	Loss: 2.1951
Training Epoch: 5 [17024/50048]	Loss: 2.0832
Training Epoch: 5 [17152/50048]	Loss: 2.2592
Training Epoch: 5 [17280/50048]	Loss: 2.1413
Training Epoch: 5 [17408/50048]	Loss: 2.1004
Training Epoch: 5 [17536/50048]	Loss: 2.4657
Training Epoch: 5 [17664/50048]	Loss: 2.1922
Training Epoch: 5 [17792/50048]	Loss: 2.1735
Training Epoch: 5 [17920/50048]	Loss: 2.2010
Training Epoch: 5 [18048/50048]	Loss: 2.1843
Training Epoch: 5 [18176/50048]	Loss: 2.2424
Training Epoch: 5 [18304/50048]	Loss: 2.1491
Training Epoch: 5 [18432/50048]	Loss: 2.4439
Training Epoch: 5 [18560/50048]	Loss: 2.2314
Training Epoch: 5 [18688/50048]	Loss: 2.3234
Training Epoch: 5 [18816/50048]	Loss: 1.9094
Training Epoch: 5 [18944/50048]	Loss: 2.0918
Training Epoch: 5 [19072/50048]	Loss: 2.2165
Training Epoch: 5 [19200/50048]	Loss: 2.0377
Training Epoch: 5 [19328/50048]	Loss: 2.0300
Training Epoch: 5 [19456/50048]	Loss: 2.2618
Training Epoch: 5 [19584/50048]	Loss: 2.2799
Training Epoch: 5 [19712/50048]	Loss: 2.4059
Training Epoch: 5 [19840/50048]	Loss: 2.2168
Training Epoch: 5 [19968/50048]	Loss: 2.2714
Training Epoch: 5 [20096/50048]	Loss: 2.2068
Training Epoch: 5 [20224/50048]	Loss: 1.8952
Training Epoch: 5 [20352/50048]	Loss: 2.5139
Training Epoch: 5 [20480/50048]	Loss: 2.4657
Training Epoch: 5 [20608/50048]	Loss: 1.8997
Training Epoch: 5 [20736/50048]	Loss: 2.0443
Training Epoch: 5 [20864/50048]	Loss: 2.2805
Training Epoch: 5 [20992/50048]	Loss: 2.3298
Training Epoch: 5 [21120/50048]	Loss: 2.4694
Training Epoch: 5 [21248/50048]	Loss: 2.3008
Training Epoch: 5 [21376/50048]	Loss: 2.2433
Training Epoch: 5 [21504/50048]	Loss: 2.2977
Training Epoch: 5 [21632/50048]	Loss: 2.2620
Training Epoch: 5 [21760/50048]	Loss: 2.0955
Training Epoch: 5 [21888/50048]	Loss: 2.2766
Training Epoch: 5 [22016/50048]	Loss: 2.3306
Training Epoch: 5 [22144/50048]	Loss: 1.9773
Training Epoch: 5 [22272/50048]	Loss: 2.5258
Training Epoch: 5 [22400/50048]	Loss: 2.0898
Training Epoch: 5 [22528/50048]	Loss: 2.2484
Training Epoch: 5 [22656/50048]	Loss: 2.1480
Training Epoch: 5 [22784/50048]	Loss: 2.0299
Training Epoch: 5 [22912/50048]	Loss: 2.1962
Training Epoch: 5 [23040/50048]	Loss: 1.9848
Training Epoch: 5 [23168/50048]	Loss: 2.3342
Training Epoch: 5 [23296/50048]	Loss: 2.1374
Training Epoch: 5 [23424/50048]	Loss: 1.9462
Training Epoch: 5 [23552/50048]	Loss: 2.1406
Training Epoch: 5 [23680/50048]	Loss: 2.0239
Training Epoch: 5 [23808/50048]	Loss: 2.1629
Training Epoch: 5 [23936/50048]	Loss: 2.1700
Training Epoch: 5 [24064/50048]	Loss: 2.0467
Training Epoch: 5 [24192/50048]	Loss: 2.3140
Training Epoch: 5 [24320/50048]	Loss: 2.3338
Training Epoch: 5 [24448/50048]	Loss: 2.0836
Training Epoch: 5 [24576/50048]	Loss: 2.1836
Training Epoch: 5 [24704/50048]	Loss: 2.1545
Training Epoch: 5 [24832/50048]	Loss: 2.1903
Training Epoch: 5 [24960/50048]	Loss: 2.0403
Training Epoch: 5 [25088/50048]	Loss: 2.3602
Training Epoch: 5 [25216/50048]	Loss: 2.0392
Training Epoch: 5 [25344/50048]	Loss: 2.2044
Training Epoch: 5 [25472/50048]	Loss: 2.1245
Training Epoch: 5 [25600/50048]	Loss: 2.0961
Training Epoch: 5 [25728/50048]	Loss: 2.1912
Training Epoch: 5 [25856/50048]	Loss: 1.9359
Training Epoch: 5 [25984/50048]	Loss: 1.9800
Training Epoch: 5 [26112/50048]	Loss: 2.1010
Training Epoch: 5 [26240/50048]	Loss: 2.4871
Training Epoch: 5 [26368/50048]	Loss: 2.3133
Training Epoch: 5 [26496/50048]	Loss: 2.4582
Training Epoch: 5 [26624/50048]	Loss: 2.1573
Training Epoch: 5 [26752/50048]	Loss: 2.3101
Training Epoch: 5 [26880/50048]	Loss: 2.2084
Training Epoch: 5 [27008/50048]	Loss: 2.1304
Training Epoch: 5 [27136/50048]	Loss: 2.4974
Training Epoch: 5 [27264/50048]	Loss: 2.1781
Training Epoch: 5 [27392/50048]	Loss: 1.9866
Training Epoch: 5 [27520/50048]	Loss: 2.0788
Training Epoch: 5 [27648/50048]	Loss: 2.1488
Training Epoch: 5 [27776/50048]	Loss: 2.1740
Training Epoch: 5 [27904/50048]	Loss: 2.0177
Training Epoch: 5 [28032/50048]	Loss: 2.1399
Training Epoch: 5 [28160/50048]	Loss: 1.8934
Training Epoch: 5 [28288/50048]	Loss: 2.3255
Training Epoch: 5 [28416/50048]	Loss: 2.1179
Training Epoch: 5 [28544/50048]	Loss: 2.1330
Training Epoch: 5 [28672/50048]	Loss: 2.2815
Training Epoch: 5 [28800/50048]	Loss: 2.0696
Training Epoch: 5 [28928/50048]	Loss: 2.2207
Training Epoch: 5 [29056/50048]	Loss: 1.9733
Training Epoch: 5 [29184/50048]	Loss: 2.3114
Training Epoch: 5 [29312/50048]	Loss: 2.1404
Training Epoch: 5 [29440/50048]	Loss: 2.4042
Training Epoch: 5 [29568/50048]	Loss: 2.1072
Training Epoch: 5 [29696/50048]	Loss: 2.3642
Training Epoch: 5 [29824/50048]	Loss: 2.4229
Training Epoch: 5 [29952/50048]	Loss: 2.1993
Training Epoch: 5 [30080/50048]	Loss: 2.1914
Training Epoch: 5 [30208/50048]	Loss: 2.0104
Training Epoch: 5 [30336/50048]	Loss: 1.9623
Training Epoch: 5 [30464/50048]	Loss: 2.3163
Training Epoch: 5 [30592/50048]	Loss: 2.1246
Training Epoch: 5 [30720/50048]	Loss: 2.1460
Training Epoch: 5 [30848/50048]	Loss: 2.2063
Training Epoch: 5 [30976/50048]	Loss: 2.0131
Training Epoch: 5 [31104/50048]	Loss: 2.4067
Training Epoch: 5 [31232/50048]	Loss: 2.2690
Training Epoch: 5 [31360/50048]	Loss: 2.0364
Training Epoch: 5 [31488/50048]	Loss: 2.2090
Training Epoch: 5 [31616/50048]	Loss: 2.1722
Training Epoch: 5 [31744/50048]	Loss: 2.2175
Training Epoch: 5 [31872/50048]	Loss: 2.2483
Training Epoch: 5 [32000/50048]	Loss: 2.2722
Training Epoch: 5 [32128/50048]	Loss: 2.1866
Training Epoch: 5 [32256/50048]	Loss: 1.9985
Training Epoch: 5 [32384/50048]	Loss: 2.5279
Training Epoch: 5 [32512/50048]	Loss: 2.3512
Training Epoch: 5 [32640/50048]	Loss: 2.3048
Training Epoch: 5 [32768/50048]	Loss: 2.1455
Training Epoch: 5 [32896/50048]	Loss: 2.3448
Training Epoch: 5 [33024/50048]	Loss: 2.1283
Training Epoch: 5 [33152/50048]	Loss: 2.0480
Training Epoch: 5 [33280/50048]	Loss: 2.0242
Training Epoch: 5 [33408/50048]	Loss: 2.2986
Training Epoch: 5 [33536/50048]	Loss: 2.2857
Training Epoch: 5 [33664/50048]	Loss: 2.1796
Training Epoch: 5 [33792/50048]	Loss: 2.0060
Training Epoch: 5 [33920/50048]	Loss: 2.1551
Training Epoch: 5 [34048/50048]	Loss: 2.2739
Training Epoch: 5 [34176/50048]	Loss: 2.2554
Training Epoch: 5 [34304/50048]	Loss: 2.3518
Training Epoch: 5 [34432/50048]	Loss: 1.9661
Training Epoch: 5 [34560/50048]	Loss: 2.2676
Training Epoch: 5 [34688/50048]	Loss: 1.8673
Training Epoch: 5 [34816/50048]	Loss: 2.1521
Training Epoch: 5 [34944/50048]	Loss: 2.3536
Training Epoch: 5 [35072/50048]	Loss: 2.3260
Training Epoch: 5 [35200/50048]	Loss: 2.2213
Training Epoch: 5 [35328/50048]	Loss: 2.3478
Training Epoch: 5 [35456/50048]	Loss: 1.9570
Training Epoch: 5 [35584/50048]	Loss: 2.4559
Training Epoch: 5 [35712/50048]	Loss: 2.1927
Training Epoch: 5 [35840/50048]	Loss: 1.8002
Training Epoch: 5 [35968/50048]	Loss: 1.7853
Training Epoch: 5 [36096/50048]	Loss: 2.2308
Training Epoch: 5 [36224/50048]	Loss: 2.2619
Training Epoch: 5 [36352/50048]	Loss: 1.9952
Training Epoch: 5 [36480/50048]	Loss: 2.2793
Training Epoch: 5 [36608/50048]	Loss: 2.1332
Training Epoch: 5 [36736/50048]	Loss: 2.3860
Training Epoch: 5 [36864/50048]	Loss: 2.1615
Training Epoch: 5 [36992/50048]	Loss: 2.0106
Training Epoch: 5 [37120/50048]	Loss: 1.9646
Training Epoch: 5 [37248/50048]	Loss: 2.1041
Training Epoch: 5 [37376/50048]	Loss: 2.2766
Training Epoch: 5 [37504/50048]	Loss: 2.1972
Training Epoch: 5 [37632/50048]	Loss: 2.1788
Training Epoch: 5 [37760/50048]	Loss: 2.1022
Training Epoch: 5 [37888/50048]	Loss: 1.8087
Training Epoch: 5 [38016/50048]	Loss: 2.0321
Training Epoch: 5 [38144/50048]	Loss: 2.0411
Training Epoch: 5 [38272/50048]	Loss: 2.0908
Training Epoch: 5 [38400/50048]	Loss: 2.1928
Training Epoch: 5 [38528/50048]	Loss: 2.2186
Training Epoch: 5 [38656/50048]	Loss: 2.0932
Training Epoch: 5 [38784/50048]	Loss: 2.1461
Training Epoch: 5 [38912/50048]	Loss: 2.1487
Training Epoch: 5 [39040/50048]	Loss: 2.1070
Training Epoch: 5 [39168/50048]	Loss: 2.1346
Training Epoch: 5 [39296/50048]	Loss: 2.1577
Training Epoch: 5 [39424/50048]	Loss: 2.2099
Training Epoch: 5 [39552/50048]	Loss: 1.7309
Training Epoch: 5 [39680/50048]	Loss: 2.1480
Training Epoch: 5 [39808/50048]	Loss: 2.5208
Training Epoch: 5 [39936/50048]	Loss: 2.0765
Training Epoch: 5 [40064/50048]	Loss: 2.1855
Training Epoch: 5 [40192/50048]	Loss: 2.3011
Training Epoch: 5 [40320/50048]	Loss: 2.1972
Training Epoch: 5 [40448/50048]	Loss: 2.0364
Training Epoch: 5 [40576/50048]	Loss: 2.4574
Training Epoch: 5 [40704/50048]	Loss: 2.1706
Training Epoch: 5 [40832/50048]	Loss: 2.2566
Training Epoch: 5 [40960/50048]	Loss: 2.3157
Training Epoch: 5 [41088/50048]	Loss: 1.9875
Training Epoch: 5 [41216/50048]	Loss: 1.9534
Training Epoch: 5 [41344/50048]	Loss: 2.2231
Training Epoch: 5 [41472/50048]	Loss: 2.3225
Training Epoch: 5 [41600/50048]	Loss: 2.3960
Training Epoch: 5 [41728/50048]	Loss: 2.2917
Training Epoch: 5 [41856/50048]	Loss: 2.2769
Training Epoch: 5 [41984/50048]	Loss: 1.9960
Training Epoch: 5 [42112/50048]	Loss: 2.1977
Training Epoch: 5 [42240/50048]	Loss: 1.8950
Training Epoch: 5 [42368/50048]	Loss: 2.1794
Training Epoch: 5 [42496/50048]	Loss: 2.0774
Training Epoch: 5 [42624/50048]	Loss: 1.9767
Training Epoch: 5 [42752/50048]	Loss: 2.3565
Training Epoch: 5 [42880/50048]	Loss: 2.3982
Training Epoch: 5 [43008/50048]	Loss: 2.3018
Training Epoch: 5 [43136/50048]	Loss: 2.0409
Training Epoch: 5 [43264/50048]	Loss: 2.1090
Training Epoch: 5 [43392/50048]	Loss: 2.2072
Training Epoch: 5 [43520/50048]	Loss: 2.1769
Training Epoch: 5 [43648/50048]	Loss: 2.1496
Training Epoch: 5 [43776/50048]	Loss: 2.1242
Training Epoch: 5 [43904/50048]	Loss: 2.0187
Training Epoch: 5 [44032/50048]	Loss: 1.8887
Training Epoch: 5 [44160/50048]	Loss: 2.4409
Training Epoch: 5 [44288/50048]	Loss: 2.2965
Training Epoch: 5 [44416/50048]	Loss: 2.2835
Training Epoch: 5 [44544/50048]	Loss: 1.9678
Training Epoch: 5 [44672/50048]	Loss: 2.1168
Training Epoch: 5 [44800/50048]	Loss: 2.0109
Training Epoch: 5 [44928/50048]	Loss: 2.1303
Training Epoch: 5 [45056/50048]	Loss: 2.2791
Training Epoch: 5 [45184/50048]	Loss: 2.4264
Training Epoch: 5 [45312/50048]	Loss: 2.0531
Training Epoch: 5 [45440/50048]	Loss: 2.2499
Training Epoch: 5 [45568/50048]	Loss: 2.4329
Training Epoch: 5 [45696/50048]	Loss: 2.0515
Training Epoch: 5 [45824/50048]	Loss: 2.1608
Training Epoch: 5 [45952/50048]	Loss: 2.3119
Training Epoch: 5 [46080/50048]	Loss: 1.9936
Training Epoch: 5 [46208/50048]	Loss: 1.9097
Training Epoch: 5 [46336/50048]	Loss: 2.2675
Training Epoch: 5 [46464/50048]	Loss: 2.2742
Training Epoch: 5 [46592/50048]	Loss: 2.1549
Training Epoch: 5 [46720/50048]	Loss: 2.3732
Training Epoch: 5 [46848/50048]	Loss: 2.0725
Training Epoch: 5 [46976/50048]	Loss: 1.9672
Training Epoch: 5 [47104/50048]	Loss: 2.2399
Training Epoch: 5 [47232/50048]	Loss: 2.2653
Training Epoch: 5 [47360/50048]	Loss: 2.0972
Training Epoch: 5 [47488/50048]	Loss: 2.0347
Training Epoch: 5 [47616/50048]	Loss: 2.3276
Training Epoch: 5 [47744/50048]	Loss: 2.1073
Training Epoch: 5 [47872/50048]	Loss: 2.1815
Training Epoch: 5 [48000/50048]	Loss: 2.0696
Training Epoch: 5 [48128/50048]	Loss: 2.0294
Training Epoch: 5 [48256/50048]	Loss: 2.0063
Training Epoch: 5 [48384/50048]	Loss: 2.0753
Training Epoch: 5 [48512/50048]	Loss: 2.3685
Training Epoch: 5 [48640/50048]	Loss: 2.2689
Training Epoch: 5 [48768/50048]	Loss: 2.1442
Training Epoch: 5 [48896/50048]	Loss: 1.9628
Training Epoch: 5 [49024/50048]	Loss: 2.1488
Training Epoch: 5 [49152/50048]	Loss: 2.2620
Training Epoch: 5 [49280/50048]	Loss: 2.0821
Training Epoch: 5 [49408/50048]	Loss: 1.8409
Training Epoch: 5 [49536/50048]	Loss: 2.2838
Training Epoch: 5 [49664/50048]	Loss: 2.1590
Training Epoch: 5 [49792/50048]	Loss: 1.9761
Training Epoch: 5 [49920/50048]	Loss: 2.1999
Training Epoch: 5 [50048/50048]	Loss: 2.1944
Validation Epoch: 5, Average loss: 0.0276, Accuracy: 0.2521
Training Epoch: 6 [128/50048]	Loss: 2.0696
Training Epoch: 6 [256/50048]	Loss: 1.8794
Training Epoch: 6 [384/50048]	Loss: 1.8398
Training Epoch: 6 [512/50048]	Loss: 2.1308
Training Epoch: 6 [640/50048]	Loss: 1.8297
Training Epoch: 6 [768/50048]	Loss: 2.0984
Training Epoch: 6 [896/50048]	Loss: 2.3109
Training Epoch: 6 [1024/50048]	Loss: 1.9305
Training Epoch: 6 [1152/50048]	Loss: 2.0669
Training Epoch: 6 [1280/50048]	Loss: 1.8831
Training Epoch: 6 [1408/50048]	Loss: 2.2061
Training Epoch: 6 [1536/50048]	Loss: 1.7346
Training Epoch: 6 [1664/50048]	Loss: 1.9122
Training Epoch: 6 [1792/50048]	Loss: 2.0932
Training Epoch: 6 [1920/50048]	Loss: 2.0079
Training Epoch: 6 [2048/50048]	Loss: 2.1866
Training Epoch: 6 [2176/50048]	Loss: 1.7843
Training Epoch: 6 [2304/50048]	Loss: 2.1563
Training Epoch: 6 [2432/50048]	Loss: 2.1092
Training Epoch: 6 [2560/50048]	Loss: 2.0356
Training Epoch: 6 [2688/50048]	Loss: 2.1867
Training Epoch: 6 [2816/50048]	Loss: 2.1670
Training Epoch: 6 [2944/50048]	Loss: 1.7740
Training Epoch: 6 [3072/50048]	Loss: 1.8001
Training Epoch: 6 [3200/50048]	Loss: 2.1803
Training Epoch: 6 [3328/50048]	Loss: 1.7904
Training Epoch: 6 [3456/50048]	Loss: 1.9462
Training Epoch: 6 [3584/50048]	Loss: 2.0821
Training Epoch: 6 [3712/50048]	Loss: 2.0398
Training Epoch: 6 [3840/50048]	Loss: 2.1828
Training Epoch: 6 [3968/50048]	Loss: 2.2991
Training Epoch: 6 [4096/50048]	Loss: 2.0279
Training Epoch: 6 [4224/50048]	Loss: 2.1359
Training Epoch: 6 [4352/50048]	Loss: 2.1028
Training Epoch: 6 [4480/50048]	Loss: 1.9080
Training Epoch: 6 [4608/50048]	Loss: 2.0163
Training Epoch: 6 [4736/50048]	Loss: 1.7752
Training Epoch: 6 [4864/50048]	Loss: 1.9414
Training Epoch: 6 [4992/50048]	Loss: 1.8736
Training Epoch: 6 [5120/50048]	Loss: 1.9351
Training Epoch: 6 [5248/50048]	Loss: 2.2155
Training Epoch: 6 [5376/50048]	Loss: 1.9496
Training Epoch: 6 [5504/50048]	Loss: 2.3056
Training Epoch: 6 [5632/50048]	Loss: 1.9210
Training Epoch: 6 [5760/50048]	Loss: 1.9879
Training Epoch: 6 [5888/50048]	Loss: 1.9532
Training Epoch: 6 [6016/50048]	Loss: 2.0326
Training Epoch: 6 [6144/50048]	Loss: 1.8243
Training Epoch: 6 [6272/50048]	Loss: 2.2559
Training Epoch: 6 [6400/50048]	Loss: 2.2156
Training Epoch: 6 [6528/50048]	Loss: 2.0874
Training Epoch: 6 [6656/50048]	Loss: 2.1045
Training Epoch: 6 [6784/50048]	Loss: 2.2209
Training Epoch: 6 [6912/50048]	Loss: 2.3493
Training Epoch: 6 [7040/50048]	Loss: 2.1161
Training Epoch: 6 [7168/50048]	Loss: 2.1349
Training Epoch: 6 [7296/50048]	Loss: 1.9281
Training Epoch: 6 [7424/50048]	Loss: 2.0725
Training Epoch: 6 [7552/50048]	Loss: 2.1000
Training Epoch: 6 [7680/50048]	Loss: 2.2948
Training Epoch: 6 [7808/50048]	Loss: 1.9611
Training Epoch: 6 [7936/50048]	Loss: 2.2676
Training Epoch: 6 [8064/50048]	Loss: 2.1562
Training Epoch: 6 [8192/50048]	Loss: 2.4503
Training Epoch: 6 [8320/50048]	Loss: 2.2289
Training Epoch: 6 [8448/50048]	Loss: 1.8789
Training Epoch: 6 [8576/50048]	Loss: 2.2474
Training Epoch: 6 [8704/50048]	Loss: 2.1626
Training Epoch: 6 [8832/50048]	Loss: 2.0303
Training Epoch: 6 [8960/50048]	Loss: 2.1808
Training Epoch: 6 [9088/50048]	Loss: 1.7903
Training Epoch: 6 [9216/50048]	Loss: 1.9130
Training Epoch: 6 [9344/50048]	Loss: 2.0170
Training Epoch: 6 [9472/50048]	Loss: 1.9553
Training Epoch: 6 [9600/50048]	Loss: 2.2356
Training Epoch: 6 [9728/50048]	Loss: 2.0561
Training Epoch: 6 [9856/50048]	Loss: 1.8897
Training Epoch: 6 [9984/50048]	Loss: 1.8525
Training Epoch: 6 [10112/50048]	Loss: 2.0350
Training Epoch: 6 [10240/50048]	Loss: 2.2117
Training Epoch: 6 [10368/50048]	Loss: 2.1882
Training Epoch: 6 [10496/50048]	Loss: 2.0631
Training Epoch: 6 [10624/50048]	Loss: 2.0921
Training Epoch: 6 [10752/50048]	Loss: 2.2015
Training Epoch: 6 [10880/50048]	Loss: 2.1633
Training Epoch: 6 [11008/50048]	Loss: 2.2665
Training Epoch: 6 [11136/50048]	Loss: 2.0603
Training Epoch: 6 [11264/50048]	Loss: 1.9526
Training Epoch: 6 [11392/50048]	Loss: 1.8349
Training Epoch: 6 [11520/50048]	Loss: 1.9683
Training Epoch: 6 [11648/50048]	Loss: 1.8656
Training Epoch: 6 [11776/50048]	Loss: 1.9142
Training Epoch: 6 [11904/50048]	Loss: 2.2593
Training Epoch: 6 [12032/50048]	Loss: 1.9859
Training Epoch: 6 [12160/50048]	Loss: 1.9204
Training Epoch: 6 [12288/50048]	Loss: 2.0017
Training Epoch: 6 [12416/50048]	Loss: 2.0150
Training Epoch: 6 [12544/50048]	Loss: 1.9438
Training Epoch: 6 [12672/50048]	Loss: 2.0171
Training Epoch: 6 [12800/50048]	Loss: 2.3079
Training Epoch: 6 [12928/50048]	Loss: 2.1713
Training Epoch: 6 [13056/50048]	Loss: 1.9258
Training Epoch: 6 [13184/50048]	Loss: 2.0762
Training Epoch: 6 [13312/50048]	Loss: 2.0724
Training Epoch: 6 [13440/50048]	Loss: 1.9353
Training Epoch: 6 [13568/50048]	Loss: 2.1231
Training Epoch: 6 [13696/50048]	Loss: 2.0454
Training Epoch: 6 [13824/50048]	Loss: 1.9819
Training Epoch: 6 [13952/50048]	Loss: 2.0302
Training Epoch: 6 [14080/50048]	Loss: 1.8928
Training Epoch: 6 [14208/50048]	Loss: 1.9659
Training Epoch: 6 [14336/50048]	Loss: 2.1912
Training Epoch: 6 [14464/50048]	Loss: 2.0755
Training Epoch: 6 [14592/50048]	Loss: 2.2054
Training Epoch: 6 [14720/50048]	Loss: 2.2976
Training Epoch: 6 [14848/50048]	Loss: 2.0495
Training Epoch: 6 [14976/50048]	Loss: 2.1083
Training Epoch: 6 [15104/50048]	Loss: 1.8927
Training Epoch: 6 [15232/50048]	Loss: 2.0664
Training Epoch: 6 [15360/50048]	Loss: 2.0948
Training Epoch: 6 [15488/50048]	Loss: 1.7050
Training Epoch: 6 [15616/50048]	Loss: 2.0527
Training Epoch: 6 [15744/50048]	Loss: 1.7693
Training Epoch: 6 [15872/50048]	Loss: 2.0167
Training Epoch: 6 [16000/50048]	Loss: 1.9130
Training Epoch: 6 [16128/50048]	Loss: 1.9796
Training Epoch: 6 [16256/50048]	Loss: 2.0559
Training Epoch: 6 [16384/50048]	Loss: 2.2050
Training Epoch: 6 [16512/50048]	Loss: 1.9792
Training Epoch: 6 [16640/50048]	Loss: 1.9524
Training Epoch: 6 [16768/50048]	Loss: 1.8963
Training Epoch: 6 [16896/50048]	Loss: 1.8521
Training Epoch: 6 [17024/50048]	Loss: 2.0555
Training Epoch: 6 [17152/50048]	Loss: 2.1955
Training Epoch: 6 [17280/50048]	Loss: 2.1077
Training Epoch: 6 [17408/50048]	Loss: 2.1966
Training Epoch: 6 [17536/50048]	Loss: 2.0127
Training Epoch: 6 [17664/50048]	Loss: 2.0326
Training Epoch: 6 [17792/50048]	Loss: 2.0769
Training Epoch: 6 [17920/50048]	Loss: 2.2785
Training Epoch: 6 [18048/50048]	Loss: 2.0163
Training Epoch: 6 [18176/50048]	Loss: 1.7784
Training Epoch: 6 [18304/50048]	Loss: 2.0237
Training Epoch: 6 [18432/50048]	Loss: 2.1242
Training Epoch: 6 [18560/50048]	Loss: 2.1217
Training Epoch: 6 [18688/50048]	Loss: 1.8710
Training Epoch: 6 [18816/50048]	Loss: 2.0904
Training Epoch: 6 [18944/50048]	Loss: 2.0135
Training Epoch: 6 [19072/50048]	Loss: 1.8750
Training Epoch: 6 [19200/50048]	Loss: 1.8483
Training Epoch: 6 [19328/50048]	Loss: 2.0906
Training Epoch: 6 [19456/50048]	Loss: 2.0912
Training Epoch: 6 [19584/50048]	Loss: 2.2337
Training Epoch: 6 [19712/50048]	Loss: 2.0596
Training Epoch: 6 [19840/50048]	Loss: 2.0347
Training Epoch: 6 [19968/50048]	Loss: 2.1409
Training Epoch: 6 [20096/50048]	Loss: 2.1526
Training Epoch: 6 [20224/50048]	Loss: 2.0958
Training Epoch: 6 [20352/50048]	Loss: 2.0739
Training Epoch: 6 [20480/50048]	Loss: 1.8007
Training Epoch: 6 [20608/50048]	Loss: 1.8817
Training Epoch: 6 [20736/50048]	Loss: 1.8862
Training Epoch: 6 [20864/50048]	Loss: 1.9345
Training Epoch: 6 [20992/50048]	Loss: 2.1404
Training Epoch: 6 [21120/50048]	Loss: 1.8342
Training Epoch: 6 [21248/50048]	Loss: 2.1209
Training Epoch: 6 [21376/50048]	Loss: 1.9304
Training Epoch: 6 [21504/50048]	Loss: 2.0946
Training Epoch: 6 [21632/50048]	Loss: 2.0031
Training Epoch: 6 [21760/50048]	Loss: 2.3818
Training Epoch: 6 [21888/50048]	Loss: 2.1184
Training Epoch: 6 [22016/50048]	Loss: 1.8428
Training Epoch: 6 [22144/50048]	Loss: 1.9862
Training Epoch: 6 [22272/50048]	Loss: 2.2824
Training Epoch: 6 [22400/50048]	Loss: 2.1849
Training Epoch: 6 [22528/50048]	Loss: 2.1891
Training Epoch: 6 [22656/50048]	Loss: 1.8330
Training Epoch: 6 [22784/50048]	Loss: 1.9743
Training Epoch: 6 [22912/50048]	Loss: 2.0789
Training Epoch: 6 [23040/50048]	Loss: 2.0129
Training Epoch: 6 [23168/50048]	Loss: 2.0908
Training Epoch: 6 [23296/50048]	Loss: 2.1237
Training Epoch: 6 [23424/50048]	Loss: 2.3167
Training Epoch: 6 [23552/50048]	Loss: 2.0835
Training Epoch: 6 [23680/50048]	Loss: 2.1662
Training Epoch: 6 [23808/50048]	Loss: 1.8307
Training Epoch: 6 [23936/50048]	Loss: 1.8812
Training Epoch: 6 [24064/50048]	Loss: 1.7774
Training Epoch: 6 [24192/50048]	Loss: 2.0246
Training Epoch: 6 [24320/50048]	Loss: 2.0783
Training Epoch: 6 [24448/50048]	Loss: 2.1968
Training Epoch: 6 [24576/50048]	Loss: 2.0192
Training Epoch: 6 [24704/50048]	Loss: 2.3652
Training Epoch: 6 [24832/50048]	Loss: 1.8736
Training Epoch: 6 [24960/50048]	Loss: 2.2230
Training Epoch: 6 [25088/50048]	Loss: 1.8063
Training Epoch: 6 [25216/50048]	Loss: 2.1818
Training Epoch: 6 [25344/50048]	Loss: 2.1150
Training Epoch: 6 [25472/50048]	Loss: 1.7687
Training Epoch: 6 [25600/50048]	Loss: 1.7680
Training Epoch: 6 [25728/50048]	Loss: 2.2023
Training Epoch: 6 [25856/50048]	Loss: 2.4670
Training Epoch: 6 [25984/50048]	Loss: 2.0333
Training Epoch: 6 [26112/50048]	Loss: 2.1612
Training Epoch: 6 [26240/50048]	Loss: 2.1352
Training Epoch: 6 [26368/50048]	Loss: 2.1159
Training Epoch: 6 [26496/50048]	Loss: 2.2316
Training Epoch: 6 [26624/50048]	Loss: 1.8486
Training Epoch: 6 [26752/50048]	Loss: 1.7967
Training Epoch: 6 [26880/50048]	Loss: 1.9029
Training Epoch: 6 [27008/50048]	Loss: 2.2451
Training Epoch: 6 [27136/50048]	Loss: 1.8780
Training Epoch: 6 [27264/50048]	Loss: 2.0123
Training Epoch: 6 [27392/50048]	Loss: 2.0022
Training Epoch: 6 [27520/50048]	Loss: 1.9843
Training Epoch: 6 [27648/50048]	Loss: 2.2683
Training Epoch: 6 [27776/50048]	Loss: 1.7850
Training Epoch: 6 [27904/50048]	Loss: 2.1261
Training Epoch: 6 [28032/50048]	Loss: 2.2447
Training Epoch: 6 [28160/50048]	Loss: 2.0469
Training Epoch: 6 [28288/50048]	Loss: 1.9116
Training Epoch: 6 [28416/50048]	Loss: 2.0342
Training Epoch: 6 [28544/50048]	Loss: 1.9525
Training Epoch: 6 [28672/50048]	Loss: 2.1339
Training Epoch: 6 [28800/50048]	Loss: 2.1902
Training Epoch: 6 [28928/50048]	Loss: 1.9458
Training Epoch: 6 [29056/50048]	Loss: 2.1091
Training Epoch: 6 [29184/50048]	Loss: 2.1921
Training Epoch: 6 [29312/50048]	Loss: 1.9249
Training Epoch: 6 [29440/50048]	Loss: 1.8745
Training Epoch: 6 [29568/50048]	Loss: 1.7471
Training Epoch: 6 [29696/50048]	Loss: 2.1693
Training Epoch: 6 [29824/50048]	Loss: 1.9829
Training Epoch: 6 [29952/50048]	Loss: 2.1550
Training Epoch: 6 [30080/50048]	Loss: 2.3622
Training Epoch: 6 [30208/50048]	Loss: 1.8244
Training Epoch: 6 [30336/50048]	Loss: 1.8471
Training Epoch: 6 [30464/50048]	Loss: 1.8523
Training Epoch: 6 [30592/50048]	Loss: 1.6102
Training Epoch: 6 [30720/50048]	Loss: 1.7686
Training Epoch: 6 [30848/50048]	Loss: 1.7146
Training Epoch: 6 [30976/50048]	Loss: 1.8852
Training Epoch: 6 [31104/50048]	Loss: 2.0308
Training Epoch: 6 [31232/50048]	Loss: 1.9095
Training Epoch: 6 [31360/50048]	Loss: 2.0087
Training Epoch: 6 [31488/50048]	Loss: 1.9127
Training Epoch: 6 [31616/50048]	Loss: 1.7669
Training Epoch: 6 [31744/50048]	Loss: 1.8788
Training Epoch: 6 [31872/50048]	Loss: 1.9620
Training Epoch: 6 [32000/50048]	Loss: 1.8007
Training Epoch: 6 [32128/50048]	Loss: 2.3186
Training Epoch: 6 [32256/50048]	Loss: 2.0002
Training Epoch: 6 [32384/50048]	Loss: 2.0344
Training Epoch: 6 [32512/50048]	Loss: 2.1722
Training Epoch: 6 [32640/50048]	Loss: 2.1929
Training Epoch: 6 [32768/50048]	Loss: 1.9090
Training Epoch: 6 [32896/50048]	Loss: 1.9638
Training Epoch: 6 [33024/50048]	Loss: 2.1493
Training Epoch: 6 [33152/50048]	Loss: 1.8377
Training Epoch: 6 [33280/50048]	Loss: 2.0748
Training Epoch: 6 [33408/50048]	Loss: 2.0521
Training Epoch: 6 [33536/50048]	Loss: 1.9120
Training Epoch: 6 [33664/50048]	Loss: 1.8154
Training Epoch: 6 [33792/50048]	Loss: 2.0237
Training Epoch: 6 [33920/50048]	Loss: 2.1725
Training Epoch: 6 [34048/50048]	Loss: 2.1484
Training Epoch: 6 [34176/50048]	Loss: 1.8731
Training Epoch: 6 [34304/50048]	Loss: 2.0532
Training Epoch: 6 [34432/50048]	Loss: 2.1102
Training Epoch: 6 [34560/50048]	Loss: 2.0445
Training Epoch: 6 [34688/50048]	Loss: 1.5927
Training Epoch: 6 [34816/50048]	Loss: 1.9625
Training Epoch: 6 [34944/50048]	Loss: 1.7308
Training Epoch: 6 [35072/50048]	Loss: 2.0672
Training Epoch: 6 [35200/50048]	Loss: 2.0926
Training Epoch: 6 [35328/50048]	Loss: 1.9325
Training Epoch: 6 [35456/50048]	Loss: 2.1859
Training Epoch: 6 [35584/50048]	Loss: 1.9652
Training Epoch: 6 [35712/50048]	Loss: 1.9432
Training Epoch: 6 [35840/50048]	Loss: 2.0210
Training Epoch: 6 [35968/50048]	Loss: 1.7773
Training Epoch: 6 [36096/50048]	Loss: 2.2681
Training Epoch: 6 [36224/50048]	Loss: 2.0054
Training Epoch: 6 [36352/50048]	Loss: 2.0494
Training Epoch: 6 [36480/50048]	Loss: 1.9066
Training Epoch: 6 [36608/50048]	Loss: 2.0718
Training Epoch: 6 [36736/50048]	Loss: 1.8925
Training Epoch: 6 [36864/50048]	Loss: 1.9972
Training Epoch: 6 [36992/50048]	Loss: 1.8472
Training Epoch: 6 [37120/50048]	Loss: 2.4088
Training Epoch: 6 [37248/50048]	Loss: 1.9228
Training Epoch: 6 [37376/50048]	Loss: 1.9238
Training Epoch: 6 [37504/50048]	Loss: 2.0531
Training Epoch: 6 [37632/50048]	Loss: 1.9102
Training Epoch: 6 [37760/50048]	Loss: 2.1527
Training Epoch: 6 [37888/50048]	Loss: 2.0015
Training Epoch: 6 [38016/50048]	Loss: 1.8971
Training Epoch: 6 [38144/50048]	Loss: 2.2499
Training Epoch: 6 [38272/50048]	Loss: 1.8944
Training Epoch: 6 [38400/50048]	Loss: 2.0116
Training Epoch: 6 [38528/50048]	Loss: 1.8381
Training Epoch: 6 [38656/50048]	Loss: 2.0124
Training Epoch: 6 [38784/50048]	Loss: 1.9394
Training Epoch: 6 [38912/50048]	Loss: 1.8405
Training Epoch: 6 [39040/50048]	Loss: 2.0242
Training Epoch: 6 [39168/50048]	Loss: 1.9907
Training Epoch: 6 [39296/50048]	Loss: 1.8815
Training Epoch: 6 [39424/50048]	Loss: 1.9037
Training Epoch: 6 [39552/50048]	Loss: 2.0608
Training Epoch: 6 [39680/50048]	Loss: 1.9377
Training Epoch: 6 [39808/50048]	Loss: 2.0013
Training Epoch: 6 [39936/50048]	Loss: 2.2962
Training Epoch: 6 [40064/50048]	Loss: 1.9890
Training Epoch: 6 [40192/50048]	Loss: 2.2688
Training Epoch: 6 [40320/50048]	Loss: 2.1976
Training Epoch: 6 [40448/50048]	Loss: 2.0193
Training Epoch: 6 [40576/50048]	Loss: 2.1646
Training Epoch: 6 [40704/50048]	Loss: 2.2301
Training Epoch: 6 [40832/50048]	Loss: 2.2480
Training Epoch: 6 [40960/50048]	Loss: 1.7886
Training Epoch: 6 [41088/50048]	Loss: 2.0073
Training Epoch: 6 [41216/50048]	Loss: 1.8345
Training Epoch: 6 [41344/50048]	Loss: 2.1385
Training Epoch: 6 [41472/50048]	Loss: 1.9941
Training Epoch: 6 [41600/50048]	Loss: 1.9874
Training Epoch: 6 [41728/50048]	Loss: 1.9515
Training Epoch: 6 [41856/50048]	Loss: 1.8350
Training Epoch: 6 [41984/50048]	Loss: 1.9352
Training Epoch: 6 [42112/50048]	Loss: 2.2018
Training Epoch: 6 [42240/50048]	Loss: 1.9309
Training Epoch: 6 [42368/50048]	Loss: 2.1065
Training Epoch: 6 [42496/50048]	Loss: 2.0311
Training Epoch: 6 [42624/50048]	Loss: 2.1339
Training Epoch: 6 [42752/50048]	Loss: 2.1243
Training Epoch: 6 [42880/50048]	Loss: 1.8717
Training Epoch: 6 [43008/50048]	Loss: 1.8361
Training Epoch: 6 [43136/50048]	Loss: 2.2039
Training Epoch: 6 [43264/50048]	Loss: 1.9310
Training Epoch: 6 [43392/50048]	Loss: 1.7971
Training Epoch: 6 [43520/50048]	Loss: 2.1851
Training Epoch: 6 [43648/50048]	Loss: 1.8920
Training Epoch: 6 [43776/50048]	Loss: 1.9610
Training Epoch: 6 [43904/50048]	Loss: 2.4260
Training Epoch: 6 [44032/50048]	Loss: 2.0664
Training Epoch: 6 [44160/50048]	Loss: 2.1279
Training Epoch: 6 [44288/50048]	Loss: 2.1054
Training Epoch: 6 [44416/50048]	Loss: 1.9327
Training Epoch: 6 [44544/50048]	Loss: 2.0824
Training Epoch: 6 [44672/50048]	Loss: 2.0556
Training Epoch: 6 [44800/50048]	Loss: 1.9481
Training Epoch: 6 [44928/50048]	Loss: 2.1382
Training Epoch: 6 [45056/50048]	Loss: 1.8561
Training Epoch: 6 [45184/50048]	Loss: 2.1204
Training Epoch: 6 [45312/50048]	Loss: 1.9297
Training Epoch: 6 [45440/50048]	Loss: 1.9258
Training Epoch: 6 [45568/50048]	Loss: 2.0597
Training Epoch: 6 [45696/50048]	Loss: 1.9970
Training Epoch: 6 [45824/50048]	Loss: 2.1239
Training Epoch: 6 [45952/50048]	Loss: 1.9620
Training Epoch: 6 [46080/50048]	Loss: 2.0356
Training Epoch: 6 [46208/50048]	Loss: 2.1417
Training Epoch: 6 [46336/50048]	Loss: 2.1162
Training Epoch: 6 [46464/50048]	Loss: 2.1081
Training Epoch: 6 [46592/50048]	Loss: 1.7819
Training Epoch: 6 [46720/50048]	Loss: 1.8265
Training Epoch: 6 [46848/50048]	Loss: 1.9408
Training Epoch: 6 [46976/50048]	Loss: 2.1641
Training Epoch: 6 [47104/50048]	Loss: 2.3920
Training Epoch: 6 [47232/50048]	Loss: 1.8725
Training Epoch: 6 [47360/50048]	Loss: 2.2139
Training Epoch: 6 [47488/50048]	Loss: 2.1517
Training Epoch: 6 [47616/50048]	Loss: 1.9127
Training Epoch: 6 [47744/50048]	Loss: 1.9066
Training Epoch: 6 [47872/50048]	Loss: 1.9136
Training Epoch: 6 [48000/50048]	Loss: 1.8744
Training Epoch: 6 [48128/50048]	Loss: 2.1276
Training Epoch: 6 [48256/50048]	Loss: 2.0340
Training Epoch: 6 [48384/50048]	Loss: 1.6377
Training Epoch: 6 [48512/50048]	Loss: 2.0158
Training Epoch: 6 [48640/50048]	Loss: 1.8560
Training Epoch: 6 [48768/50048]	Loss: 2.0925
Training Epoch: 6 [48896/50048]	Loss: 1.9486
Training Epoch: 6 [49024/50048]	Loss: 2.0398
Training Epoch: 6 [49152/50048]	Loss: 2.0726
Training Epoch: 6 [49280/50048]	Loss: 1.9058
Training Epoch: 6 [49408/50048]	Loss: 1.9488
Training Epoch: 6 [49536/50048]	Loss: 1.8219
Training Epoch: 6 [49664/50048]	Loss: 1.7502
Training Epoch: 6 [49792/50048]	Loss: 2.1696
Training Epoch: 6 [49920/50048]	Loss: 1.8161
Training Epoch: 6 [50048/50048]	Loss: 1.7565
Validation Epoch: 6, Average loss: 0.0213, Accuracy: 0.3384
[Training Loop] Model's accuracy 0.3384098101265823 surpasses threshold 0.3! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7265
Profiling... [256/50048]	Loss: 2.1995
Profiling... [384/50048]	Loss: 1.9362
Profiling... [512/50048]	Loss: 1.7368
Profiling... [640/50048]	Loss: 1.7615
Profiling... [768/50048]	Loss: 2.1681
Profiling... [896/50048]	Loss: 1.7916
Profiling... [1024/50048]	Loss: 1.6948
Profiling... [1152/50048]	Loss: 1.7892
Profiling... [1280/50048]	Loss: 1.7726
Profiling... [1408/50048]	Loss: 1.7994
Profiling... [1536/50048]	Loss: 2.1849
Profiling... [1664/50048]	Loss: 1.6802
Profile done
epoch 1 train time consumed: 3.35s
Validation Epoch: 7, Average loss: 0.0149, Accuracy: 0.4812
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.5629147049708,
                        "time": 2.156434259999969,
                        "accuracy": 0.48121044303797467,
                        "total_cost": 784222.3728927139
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9645
Profiling... [256/50048]	Loss: 1.8903
Profiling... [384/50048]	Loss: 2.0192
Profiling... [512/50048]	Loss: 1.8717
Profiling... [640/50048]	Loss: 1.6750
Profiling... [768/50048]	Loss: 1.9439
Profiling... [896/50048]	Loss: 1.7909
Profiling... [1024/50048]	Loss: 1.6222
Profiling... [1152/50048]	Loss: 1.9684
Profiling... [1280/50048]	Loss: 1.7680
Profiling... [1408/50048]	Loss: 1.8256
Profiling... [1536/50048]	Loss: 1.9487
Profiling... [1664/50048]	Loss: 1.8098
Profile done
epoch 1 train time consumed: 3.31s
Validation Epoch: 7, Average loss: 0.0148, Accuracy: 0.4806
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.59168883748559,
                        "time": 2.1591092799999387,
                        "accuracy": 0.48061708860759494,
                        "total_cost": 786164.5641744633
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9031
Profiling... [256/50048]	Loss: 1.9814
Profiling... [384/50048]	Loss: 1.7197
Profiling... [512/50048]	Loss: 1.8701
Profiling... [640/50048]	Loss: 1.7313
Profiling... [768/50048]	Loss: 1.7816
Profiling... [896/50048]	Loss: 1.9035
Profiling... [1024/50048]	Loss: 1.9936
Profiling... [1152/50048]	Loss: 1.6865
Profiling... [1280/50048]	Loss: 1.5554
Profiling... [1408/50048]	Loss: 1.6764
Profiling... [1536/50048]	Loss: 1.8540
Profiling... [1664/50048]	Loss: 2.0008
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 7, Average loss: 0.0148, Accuracy: 0.4818
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.60902397579692,
                        "time": 2.374173960999997,
                        "accuracy": 0.48180379746835444,
                        "total_cost": 862343.6456045965
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8657
Profiling... [256/50048]	Loss: 1.5932
Profiling... [384/50048]	Loss: 1.8017
Profiling... [512/50048]	Loss: 1.8587
Profiling... [640/50048]	Loss: 1.9554
Profiling... [768/50048]	Loss: 1.9248
Profiling... [896/50048]	Loss: 1.8155
Profiling... [1024/50048]	Loss: 1.6754
Profiling... [1152/50048]	Loss: 1.6264
Profiling... [1280/50048]	Loss: 1.9204
Profiling... [1408/50048]	Loss: 1.6265
Profiling... [1536/50048]	Loss: 1.9515
Profiling... [1664/50048]	Loss: 1.9805
Profile done
epoch 1 train time consumed: 6.16s
Validation Epoch: 7, Average loss: 0.0148, Accuracy: 0.4789
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.56445172171568,
                        "time": 4.24838861499984,
                        "accuracy": 0.478935917721519,
                        "total_cost": 1552332.9533561254
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1475
Profiling... [256/50048]	Loss: 1.7225
Profiling... [384/50048]	Loss: 1.8260
Profiling... [512/50048]	Loss: 1.9268
Profiling... [640/50048]	Loss: 1.7205
Profiling... [768/50048]	Loss: 1.7680
Profiling... [896/50048]	Loss: 1.7688
Profiling... [1024/50048]	Loss: 1.9139
Profiling... [1152/50048]	Loss: 1.9064
Profiling... [1280/50048]	Loss: 1.7007
Profiling... [1408/50048]	Loss: 1.8129
Profiling... [1536/50048]	Loss: 1.6551
Profiling... [1664/50048]	Loss: 1.7735
Profile done
epoch 1 train time consumed: 3.27s
Validation Epoch: 7, Average loss: 0.0147, Accuracy: 0.4864
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.46833233208716,
                        "time": 2.160758371999691,
                        "accuracy": 0.48635284810126583,
                        "total_cost": 777486.3796442971
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7553
Profiling... [256/50048]	Loss: 1.8671
Profiling... [384/50048]	Loss: 2.1354
Profiling... [512/50048]	Loss: 1.7215
Profiling... [640/50048]	Loss: 1.7257
Profiling... [768/50048]	Loss: 1.9456
Profiling... [896/50048]	Loss: 1.9201
Profiling... [1024/50048]	Loss: 1.8953
Profiling... [1152/50048]	Loss: 1.9974
Profiling... [1280/50048]	Loss: 1.7219
Profiling... [1408/50048]	Loss: 1.9226
Profiling... [1536/50048]	Loss: 2.0024
Profiling... [1664/50048]	Loss: 1.8406
Profile done
epoch 1 train time consumed: 3.36s
Validation Epoch: 7, Average loss: 0.0148, Accuracy: 0.4839
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.49827926873247,
                        "time": 2.15696374800018,
                        "accuracy": 0.48388053797468356,
                        "total_cost": 780086.4599348292
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7879
Profiling... [256/50048]	Loss: 1.9066
Profiling... [384/50048]	Loss: 1.7443
Profiling... [512/50048]	Loss: 1.7328
Profiling... [640/50048]	Loss: 1.8224
Profiling... [768/50048]	Loss: 1.8574
Profiling... [896/50048]	Loss: 1.8081
Profiling... [1024/50048]	Loss: 1.7894
Profiling... [1152/50048]	Loss: 1.9458
Profiling... [1280/50048]	Loss: 1.7861
Profiling... [1408/50048]	Loss: 1.9463
Profiling... [1536/50048]	Loss: 1.6464
Profiling... [1664/50048]	Loss: 2.0857
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 7, Average loss: 0.0147, Accuracy: 0.4847
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.51200745470577,
                        "time": 2.3712568089999877,
                        "accuracy": 0.4846716772151899,
                        "total_cost": 856187.7268325603
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9142
Profiling... [256/50048]	Loss: 2.0005
Profiling... [384/50048]	Loss: 1.8148
Profiling... [512/50048]	Loss: 1.6912
Profiling... [640/50048]	Loss: 2.0636
Profiling... [768/50048]	Loss: 1.7486
Profiling... [896/50048]	Loss: 2.0585
Profiling... [1024/50048]	Loss: 1.8151
Profiling... [1152/50048]	Loss: 1.8868
Profiling... [1280/50048]	Loss: 1.7575
Profiling... [1408/50048]	Loss: 2.0450
Profiling... [1536/50048]	Loss: 1.6381
Profiling... [1664/50048]	Loss: 1.7488
Profile done
epoch 1 train time consumed: 6.98s
Validation Epoch: 7, Average loss: 0.0147, Accuracy: 0.4875
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.46137376831182,
                        "time": 5.129548385000362,
                        "accuracy": 0.48753955696202533,
                        "total_cost": 1841226.9416017525
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6746
Profiling... [256/50048]	Loss: 1.6993
Profiling... [384/50048]	Loss: 1.8232
Profiling... [512/50048]	Loss: 1.8673
Profiling... [640/50048]	Loss: 1.7954
Profiling... [768/50048]	Loss: 2.1733
Profiling... [896/50048]	Loss: 1.4761
Profiling... [1024/50048]	Loss: 1.8901
Profiling... [1152/50048]	Loss: 1.9678
Profiling... [1280/50048]	Loss: 1.7002
Profiling... [1408/50048]	Loss: 1.8784
Profiling... [1536/50048]	Loss: 2.1207
Profiling... [1664/50048]	Loss: 1.6509
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 7, Average loss: 0.0146, Accuracy: 0.4851
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.37724880281094,
                        "time": 2.157496323000032,
                        "accuracy": 0.48506724683544306,
                        "total_cost": 778370.1311275957
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6433
Profiling... [256/50048]	Loss: 1.8911
Profiling... [384/50048]	Loss: 1.7930
Profiling... [512/50048]	Loss: 1.8590
Profiling... [640/50048]	Loss: 1.7010
Profiling... [768/50048]	Loss: 1.8330
Profiling... [896/50048]	Loss: 2.0711
Profiling... [1024/50048]	Loss: 1.8239
Profiling... [1152/50048]	Loss: 1.8219
Profiling... [1280/50048]	Loss: 1.7080
Profiling... [1408/50048]	Loss: 1.8237
Profiling... [1536/50048]	Loss: 1.8643
Profiling... [1664/50048]	Loss: 1.8606
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 7, Average loss: 0.0149, Accuracy: 0.4803
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.40409801209908,
                        "time": 2.1606756290002522,
                        "accuracy": 0.48032041139240506,
                        "total_cost": 787220.8344819532
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6978
Profiling... [256/50048]	Loss: 1.7985
Profiling... [384/50048]	Loss: 1.5476
Profiling... [512/50048]	Loss: 1.6592
Profiling... [640/50048]	Loss: 1.7058
Profiling... [768/50048]	Loss: 1.6996
Profiling... [896/50048]	Loss: 2.0086
Profiling... [1024/50048]	Loss: 1.9675
Profiling... [1152/50048]	Loss: 2.0022
Profiling... [1280/50048]	Loss: 1.7994
Profiling... [1408/50048]	Loss: 1.9211
Profiling... [1536/50048]	Loss: 1.8944
Profiling... [1664/50048]	Loss: 1.5490
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 7, Average loss: 0.0148, Accuracy: 0.4813
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.4177992315542,
                        "time": 2.372185293000257,
                        "accuracy": 0.481309335443038,
                        "total_cost": 862506.4915745335
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8261
Profiling... [256/50048]	Loss: 1.9570
Profiling... [384/50048]	Loss: 1.6796
Profiling... [512/50048]	Loss: 1.9013
Profiling... [640/50048]	Loss: 1.7679
Profiling... [768/50048]	Loss: 1.8248
Profiling... [896/50048]	Loss: 1.7862
Profiling... [1024/50048]	Loss: 1.8602
Profiling... [1152/50048]	Loss: 1.7467
Profiling... [1280/50048]	Loss: 2.0984
Profiling... [1408/50048]	Loss: 1.8862
Profiling... [1536/50048]	Loss: 1.7894
Profiling... [1664/50048]	Loss: 1.8480
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 7, Average loss: 0.0150, Accuracy: 0.4785
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.36636805997436,
                        "time": 4.81798653099986,
                        "accuracy": 0.47854034810126583,
                        "total_cost": 1761915.471224913
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9744
Profiling... [256/50048]	Loss: 1.8417
Profiling... [384/50048]	Loss: 1.7402
Profiling... [512/50048]	Loss: 1.6493
Profiling... [640/50048]	Loss: 1.7267
Profiling... [768/50048]	Loss: 2.0716
Profiling... [896/50048]	Loss: 2.0651
Profiling... [1024/50048]	Loss: 1.8785
Profiling... [1152/50048]	Loss: 1.7440
Profiling... [1280/50048]	Loss: 1.8945
Profiling... [1408/50048]	Loss: 1.7476
Profiling... [1536/50048]	Loss: 1.9151
Profiling... [1664/50048]	Loss: 1.8428
Profile done
epoch 1 train time consumed: 3.37s
Validation Epoch: 7, Average loss: 0.0157, Accuracy: 0.4634
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.27819420488979,
                        "time": 2.1526503830000365,
                        "accuracy": 0.4634098101265823,
                        "total_cost": 812917.2253002272
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9511
Profiling... [256/50048]	Loss: 2.0619
Profiling... [384/50048]	Loss: 1.8934
Profiling... [512/50048]	Loss: 2.0789
Profiling... [640/50048]	Loss: 1.7837
Profiling... [768/50048]	Loss: 1.9850
Profiling... [896/50048]	Loss: 1.8504
Profiling... [1024/50048]	Loss: 1.8250
Profiling... [1152/50048]	Loss: 2.1040
Profiling... [1280/50048]	Loss: 2.0301
Profiling... [1408/50048]	Loss: 1.6780
Profiling... [1536/50048]	Loss: 1.8918
Profiling... [1664/50048]	Loss: 1.7817
Profile done
epoch 1 train time consumed: 3.27s
Validation Epoch: 7, Average loss: 0.0173, Accuracy: 0.4188
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.30523910769439,
                        "time": 2.1611627929996757,
                        "accuracy": 0.418809335443038,
                        "total_cost": 903044.5521823439
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7384
Profiling... [256/50048]	Loss: 1.7875
Profiling... [384/50048]	Loss: 1.9348
Profiling... [512/50048]	Loss: 1.7947
Profiling... [640/50048]	Loss: 2.0549
Profiling... [768/50048]	Loss: 2.0370
Profiling... [896/50048]	Loss: 1.9549
Profiling... [1024/50048]	Loss: 1.6706
Profiling... [1152/50048]	Loss: 1.6497
Profiling... [1280/50048]	Loss: 1.8036
Profiling... [1408/50048]	Loss: 1.7680
Profiling... [1536/50048]	Loss: 1.8831
Profiling... [1664/50048]	Loss: 2.1954
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 7, Average loss: 0.0165, Accuracy: 0.4335
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.32067657770432,
                        "time": 2.3653597120000995,
                        "accuracy": 0.43354430379746833,
                        "total_cost": 954776.5844788722
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9378
Profiling... [256/50048]	Loss: 1.6425
Profiling... [384/50048]	Loss: 1.7537
Profiling... [512/50048]	Loss: 1.9266
Profiling... [640/50048]	Loss: 1.9663
Profiling... [768/50048]	Loss: 1.7220
Profiling... [896/50048]	Loss: 1.8684
Profiling... [1024/50048]	Loss: 1.7391
Profiling... [1152/50048]	Loss: 1.7538
Profiling... [1280/50048]	Loss: 1.7052
Profiling... [1408/50048]	Loss: 1.7964
Profiling... [1536/50048]	Loss: 1.7536
Profiling... [1664/50048]	Loss: 1.9499
Profile done
epoch 1 train time consumed: 6.65s
Validation Epoch: 7, Average loss: 0.0168, Accuracy: 0.4326
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.2724462988102,
                        "time": 4.783613371000229,
                        "accuracy": 0.43255537974683544,
                        "total_cost": 1935318.2947695483
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0055
Profiling... [256/50048]	Loss: 1.9092
Profiling... [384/50048]	Loss: 1.8079
Profiling... [512/50048]	Loss: 1.9533
Profiling... [640/50048]	Loss: 1.9126
Profiling... [768/50048]	Loss: 2.0047
Profiling... [896/50048]	Loss: 1.6572
Profiling... [1024/50048]	Loss: 1.9102
Profiling... [1152/50048]	Loss: 1.8172
Profiling... [1280/50048]	Loss: 1.8660
Profiling... [1408/50048]	Loss: 1.9093
Profiling... [1536/50048]	Loss: 1.7058
Profiling... [1664/50048]	Loss: 1.7511
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 7, Average loss: 0.0173, Accuracy: 0.4235
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.18728691962912,
                        "time": 2.1586195970003246,
                        "accuracy": 0.42345727848101267,
                        "total_cost": 892081.5597505312
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1010
Profiling... [256/50048]	Loss: 2.1315
Profiling... [384/50048]	Loss: 1.8470
Profiling... [512/50048]	Loss: 1.8592
Profiling... [640/50048]	Loss: 1.8317
Profiling... [768/50048]	Loss: 1.9774
Profiling... [896/50048]	Loss: 1.9519
Profiling... [1024/50048]	Loss: 1.6011
Profiling... [1152/50048]	Loss: 1.6624
Profiling... [1280/50048]	Loss: 2.0181
Profiling... [1408/50048]	Loss: 1.8793
Profiling... [1536/50048]	Loss: 1.9199
Profiling... [1664/50048]	Loss: 1.8503
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 7, Average loss: 0.0165, Accuracy: 0.4365
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.21460982639265,
                        "time": 2.156035831999816,
                        "accuracy": 0.4365110759493671,
                        "total_cost": 864368.1486875565
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9062
Profiling... [256/50048]	Loss: 1.8934
Profiling... [384/50048]	Loss: 1.9455
Profiling... [512/50048]	Loss: 1.7432
Profiling... [640/50048]	Loss: 1.8454
Profiling... [768/50048]	Loss: 1.7053
Profiling... [896/50048]	Loss: 2.0496
Profiling... [1024/50048]	Loss: 1.9405
Profiling... [1152/50048]	Loss: 2.0316
Profiling... [1280/50048]	Loss: 1.8079
Profiling... [1408/50048]	Loss: 1.9764
Profiling... [1536/50048]	Loss: 2.0642
Profiling... [1664/50048]	Loss: 1.9761
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 7, Average loss: 0.0160, Accuracy: 0.4507
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.22788187048367,
                        "time": 2.378152402000069,
                        "accuracy": 0.4506526898734177,
                        "total_cost": 923497.5840639283
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0735
Profiling... [256/50048]	Loss: 2.0828
Profiling... [384/50048]	Loss: 1.7795
Profiling... [512/50048]	Loss: 2.0330
Profiling... [640/50048]	Loss: 2.0926
Profiling... [768/50048]	Loss: 1.8852
Profiling... [896/50048]	Loss: 1.7421
Profiling... [1024/50048]	Loss: 1.8298
Profiling... [1152/50048]	Loss: 2.0088
Profiling... [1280/50048]	Loss: 1.6202
Profiling... [1408/50048]	Loss: 1.6696
Profiling... [1536/50048]	Loss: 1.8173
Profiling... [1664/50048]	Loss: 1.8422
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 7, Average loss: 0.0169, Accuracy: 0.4216
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.17720322814398,
                        "time": 5.013210931000231,
                        "accuracy": 0.4215783227848101,
                        "total_cost": 2081017.6081393405
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6979
Profiling... [256/50048]	Loss: 1.9187
Profiling... [384/50048]	Loss: 1.7984
Profiling... [512/50048]	Loss: 1.9107
Profiling... [640/50048]	Loss: 1.8339
Profiling... [768/50048]	Loss: 2.0959
Profiling... [896/50048]	Loss: 1.7075
Profiling... [1024/50048]	Loss: 1.9833
Profiling... [1152/50048]	Loss: 1.7066
Profiling... [1280/50048]	Loss: 1.9182
Profiling... [1408/50048]	Loss: 1.8504
Profiling... [1536/50048]	Loss: 1.7993
Profiling... [1664/50048]	Loss: 1.8566
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 7, Average loss: 0.0159, Accuracy: 0.4529
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.09707262161967,
                        "time": 2.161313544999757,
                        "accuracy": 0.45292721518987344,
                        "total_cost": 835078.700705583
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7442
Profiling... [256/50048]	Loss: 1.8024
Profiling... [384/50048]	Loss: 1.8168
Profiling... [512/50048]	Loss: 1.9960
Profiling... [640/50048]	Loss: 1.9759
Profiling... [768/50048]	Loss: 1.9173
Profiling... [896/50048]	Loss: 1.8277
Profiling... [1024/50048]	Loss: 1.6302
Profiling... [1152/50048]	Loss: 1.6763
Profiling... [1280/50048]	Loss: 1.6910
Profiling... [1408/50048]	Loss: 1.9150
Profiling... [1536/50048]	Loss: 1.5598
Profiling... [1664/50048]	Loss: 2.0672
Profile done
epoch 1 train time consumed: 3.40s
Validation Epoch: 7, Average loss: 0.0177, Accuracy: 0.4078
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.11509245032543,
                        "time": 2.1628806240000813,
                        "accuracy": 0.40783227848101267,
                        "total_cost": 928087.6702789873
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5858
Profiling... [256/50048]	Loss: 2.0860
Profiling... [384/50048]	Loss: 2.1463
Profiling... [512/50048]	Loss: 1.8225
Profiling... [640/50048]	Loss: 1.8384
Profiling... [768/50048]	Loss: 1.7841
Profiling... [896/50048]	Loss: 1.8775
Profiling... [1024/50048]	Loss: 1.5770
Profiling... [1152/50048]	Loss: 1.6461
Profiling... [1280/50048]	Loss: 1.9722
Profiling... [1408/50048]	Loss: 1.7163
Profiling... [1536/50048]	Loss: 1.5798
Profiling... [1664/50048]	Loss: 1.9948
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 7, Average loss: 0.0179, Accuracy: 0.4240
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.13274454285953,
                        "time": 2.373039069999777,
                        "accuracy": 0.4239517405063291,
                        "total_cost": 979549.7873271764
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7650
Profiling... [256/50048]	Loss: 1.9742
Profiling... [384/50048]	Loss: 1.9202
Profiling... [512/50048]	Loss: 1.8169
Profiling... [640/50048]	Loss: 1.8219
Profiling... [768/50048]	Loss: 1.9779
Profiling... [896/50048]	Loss: 1.9473
Profiling... [1024/50048]	Loss: 1.9209
Profiling... [1152/50048]	Loss: 1.5692
Profiling... [1280/50048]	Loss: 1.8759
Profiling... [1408/50048]	Loss: 2.1078
Profiling... [1536/50048]	Loss: 1.9764
Profiling... [1664/50048]	Loss: 1.8382
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 7, Average loss: 0.0157, Accuracy: 0.4603
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.08696775339813,
                        "time": 5.03577229199982,
                        "accuracy": 0.4603441455696203,
                        "total_cost": 1914350.7299512096
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6560
Profiling... [256/50048]	Loss: 2.0996
Profiling... [384/50048]	Loss: 2.1813
Profiling... [512/50048]	Loss: 1.8878
Profiling... [640/50048]	Loss: 1.9395
Profiling... [768/50048]	Loss: 1.9641
Profiling... [896/50048]	Loss: 2.2776
Profiling... [1024/50048]	Loss: 1.8805
Profiling... [1152/50048]	Loss: 1.9844
Profiling... [1280/50048]	Loss: 2.2256
Profiling... [1408/50048]	Loss: 1.7095
Profiling... [1536/50048]	Loss: 2.2870
Profiling... [1664/50048]	Loss: 1.8680
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 7, Average loss: 0.0282, Accuracy: 0.2543
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.00738667986609,
                        "time": 2.1611231940000835,
                        "accuracy": 0.2542523734177215,
                        "total_cost": 1487484.871296207
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7894
Profiling... [256/50048]	Loss: 1.7438
Profiling... [384/50048]	Loss: 1.7718
Profiling... [512/50048]	Loss: 2.0464
Profiling... [640/50048]	Loss: 2.0900
Profiling... [768/50048]	Loss: 2.1962
Profiling... [896/50048]	Loss: 2.0356
Profiling... [1024/50048]	Loss: 2.1580
Profiling... [1152/50048]	Loss: 1.9933
Profiling... [1280/50048]	Loss: 1.7969
Profiling... [1408/50048]	Loss: 2.1723
Profiling... [1536/50048]	Loss: 2.1009
Profiling... [1664/50048]	Loss: 1.9643
Profile done
epoch 1 train time consumed: 3.23s
Validation Epoch: 7, Average loss: 0.0293, Accuracy: 0.2648
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.02967135382454,
                        "time": 2.160427102000085,
                        "accuracy": 0.26483386075949367,
                        "total_cost": 1427592.1582148434
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7652
Profiling... [256/50048]	Loss: 1.8314
Profiling... [384/50048]	Loss: 1.8737
Profiling... [512/50048]	Loss: 1.9679
Profiling... [640/50048]	Loss: 2.0719
Profiling... [768/50048]	Loss: 2.0544
Profiling... [896/50048]	Loss: 2.0285
Profiling... [1024/50048]	Loss: 1.7879
Profiling... [1152/50048]	Loss: 1.8447
Profiling... [1280/50048]	Loss: 1.9793
Profiling... [1408/50048]	Loss: 2.0690
Profiling... [1536/50048]	Loss: 1.8123
Profiling... [1664/50048]	Loss: 1.9985
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 7, Average loss: 0.0241, Accuracy: 0.3001
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.04570235439114,
                        "time": 2.3732529809999505,
                        "accuracy": 0.3001384493670886,
                        "total_cost": 1383758.9704044522
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0937
Profiling... [256/50048]	Loss: 1.7932
Profiling... [384/50048]	Loss: 1.8728
Profiling... [512/50048]	Loss: 1.7868
Profiling... [640/50048]	Loss: 1.8281
Profiling... [768/50048]	Loss: 1.8226
Profiling... [896/50048]	Loss: 2.1657
Profiling... [1024/50048]	Loss: 2.0434
Profiling... [1152/50048]	Loss: 1.8512
Profiling... [1280/50048]	Loss: 1.9768
Profiling... [1408/50048]	Loss: 1.9063
Profiling... [1536/50048]	Loss: 1.9578
Profiling... [1664/50048]	Loss: 2.1375
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 7, Average loss: 0.0227, Accuracy: 0.3501
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.99894812624396,
                        "time": 4.908746820000033,
                        "accuracy": 0.3500791139240506,
                        "total_cost": 2453818.7493423894
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0454
Profiling... [256/50048]	Loss: 1.8555
Profiling... [384/50048]	Loss: 1.8907
Profiling... [512/50048]	Loss: 2.1786
Profiling... [640/50048]	Loss: 1.9911
Profiling... [768/50048]	Loss: 1.9443
Profiling... [896/50048]	Loss: 2.0963
Profiling... [1024/50048]	Loss: 1.6480
Profiling... [1152/50048]	Loss: 1.7869
Profiling... [1280/50048]	Loss: 2.0014
Profiling... [1408/50048]	Loss: 1.8975
Profiling... [1536/50048]	Loss: 2.2669
Profiling... [1664/50048]	Loss: 1.7664
Profile done
epoch 1 train time consumed: 3.25s
Validation Epoch: 7, Average loss: 0.0225, Accuracy: 0.3056
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.91498030511329,
                        "time": 2.15340196499983,
                        "accuracy": 0.3055775316455696,
                        "total_cost": 1233223.3389202908
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9090
Profiling... [256/50048]	Loss: 2.1535
Profiling... [384/50048]	Loss: 2.0730
Profiling... [512/50048]	Loss: 1.9440
Profiling... [640/50048]	Loss: 2.0721
Profiling... [768/50048]	Loss: 1.9031
Profiling... [896/50048]	Loss: 2.1081
Profiling... [1024/50048]	Loss: 1.9766
Profiling... [1152/50048]	Loss: 1.8960
Profiling... [1280/50048]	Loss: 1.9537
Profiling... [1408/50048]	Loss: 1.9776
Profiling... [1536/50048]	Loss: 2.0802
Profiling... [1664/50048]	Loss: 2.3015
Profile done
epoch 1 train time consumed: 3.33s
Validation Epoch: 7, Average loss: 0.0274, Accuracy: 0.2687
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.94425622278298,
                        "time": 2.1542161140000644,
                        "accuracy": 0.268690664556962,
                        "total_cost": 1403055.147344319
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7712
Profiling... [256/50048]	Loss: 1.8525
Profiling... [384/50048]	Loss: 1.8163
Profiling... [512/50048]	Loss: 1.6495
Profiling... [640/50048]	Loss: 2.1234
Profiling... [768/50048]	Loss: 1.8011
Profiling... [896/50048]	Loss: 1.8473
Profiling... [1024/50048]	Loss: 1.9495
Profiling... [1152/50048]	Loss: 2.2355
Profiling... [1280/50048]	Loss: 1.8495
Profiling... [1408/50048]	Loss: 1.8975
Profiling... [1536/50048]	Loss: 2.0630
Profiling... [1664/50048]	Loss: 2.2769
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 7, Average loss: 0.0279, Accuracy: 0.2949
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.95845230823271,
                        "time": 2.378029138000329,
                        "accuracy": 0.29489715189873417,
                        "total_cost": 1411187.2443344677
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8849
Profiling... [256/50048]	Loss: 1.9912
Profiling... [384/50048]	Loss: 1.9599
Profiling... [512/50048]	Loss: 1.8493
Profiling... [640/50048]	Loss: 1.9133
Profiling... [768/50048]	Loss: 2.1047
Profiling... [896/50048]	Loss: 1.5863
Profiling... [1024/50048]	Loss: 2.1036
Profiling... [1152/50048]	Loss: 2.1284
Profiling... [1280/50048]	Loss: 1.8843
Profiling... [1408/50048]	Loss: 2.0127
Profiling... [1536/50048]	Loss: 1.9959
Profiling... [1664/50048]	Loss: 1.8124
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 7, Average loss: 0.0285, Accuracy: 0.2739
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.91258350788586,
                        "time": 5.00948323800003,
                        "accuracy": 0.27393196202531644,
                        "total_cost": 3200282.143669622
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7846
Profiling... [256/50048]	Loss: 1.7856
Profiling... [384/50048]	Loss: 1.8575
Profiling... [512/50048]	Loss: 1.6472
Profiling... [640/50048]	Loss: 1.9262
Profiling... [768/50048]	Loss: 2.0319
Profiling... [896/50048]	Loss: 2.2580
Profiling... [1024/50048]	Loss: 2.2710
Profiling... [1152/50048]	Loss: 2.0848
Profiling... [1280/50048]	Loss: 2.0175
Profiling... [1408/50048]	Loss: 2.0349
Profiling... [1536/50048]	Loss: 2.0222
Profiling... [1664/50048]	Loss: 1.7881
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 7, Average loss: 0.0194, Accuracy: 0.3787
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.8363819589747,
                        "time": 2.1629501639999944,
                        "accuracy": 0.3786590189873418,
                        "total_cost": 999623.0374025569
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7987
Profiling... [256/50048]	Loss: 1.9709
Profiling... [384/50048]	Loss: 1.8443
Profiling... [512/50048]	Loss: 1.7402
Profiling... [640/50048]	Loss: 1.8908
Profiling... [768/50048]	Loss: 1.6733
Profiling... [896/50048]	Loss: 1.9996
Profiling... [1024/50048]	Loss: 2.1545
Profiling... [1152/50048]	Loss: 2.0126
Profiling... [1280/50048]	Loss: 1.9841
Profiling... [1408/50048]	Loss: 1.8611
Profiling... [1536/50048]	Loss: 1.8156
Profiling... [1664/50048]	Loss: 1.8085
Profile done
epoch 1 train time consumed: 3.40s
Validation Epoch: 7, Average loss: 0.0266, Accuracy: 0.2885
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.8544458788237,
                        "time": 2.1600290619999214,
                        "accuracy": 0.2884691455696203,
                        "total_cost": 1310383.074430943
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8417
Profiling... [256/50048]	Loss: 1.6090
Profiling... [384/50048]	Loss: 1.9521
Profiling... [512/50048]	Loss: 1.9498
Profiling... [640/50048]	Loss: 2.1977
Profiling... [768/50048]	Loss: 2.0703
Profiling... [896/50048]	Loss: 2.0049
Profiling... [1024/50048]	Loss: 2.2780
Profiling... [1152/50048]	Loss: 2.1671
Profiling... [1280/50048]	Loss: 1.9961
Profiling... [1408/50048]	Loss: 2.2042
Profiling... [1536/50048]	Loss: 1.8420
Profiling... [1664/50048]	Loss: 1.9060
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 7, Average loss: 0.0245, Accuracy: 0.3032
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.86938918878602,
                        "time": 2.3724279979996936,
                        "accuracy": 0.3032041139240506,
                        "total_cost": 1369291.776014435
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0565
Profiling... [256/50048]	Loss: 1.7910
Profiling... [384/50048]	Loss: 1.9411
Profiling... [512/50048]	Loss: 2.1148
Profiling... [640/50048]	Loss: 1.7185
Profiling... [768/50048]	Loss: 2.3926
Profiling... [896/50048]	Loss: 1.8234
Profiling... [1024/50048]	Loss: 2.0444
Profiling... [1152/50048]	Loss: 2.0137
Profiling... [1280/50048]	Loss: 1.9392
Profiling... [1408/50048]	Loss: 1.9732
Profiling... [1536/50048]	Loss: 1.9832
Profiling... [1664/50048]	Loss: 1.9969
Profile done
epoch 1 train time consumed: 6.70s
Validation Epoch: 7, Average loss: 0.0242, Accuracy: 0.3141
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.82595402808622,
                        "time": 4.784908248999727,
                        "accuracy": 0.31408227848101267,
                        "total_cost": 2666049.633951485
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8077
Profiling... [512/50176]	Loss: 1.8902
Profiling... [768/50176]	Loss: 1.8435
Profiling... [1024/50176]	Loss: 1.6391
Profiling... [1280/50176]	Loss: 2.0064
Profiling... [1536/50176]	Loss: 1.6655
Profiling... [1792/50176]	Loss: 1.6786
Profiling... [2048/50176]	Loss: 1.7427
Profiling... [2304/50176]	Loss: 1.8193
Profiling... [2560/50176]	Loss: 1.7781
Profiling... [2816/50176]	Loss: 1.8191
Profiling... [3072/50176]	Loss: 1.8284
Profiling... [3328/50176]	Loss: 1.8578
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 7, Average loss: 0.0073, Accuracy: 0.4768
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.75104112806721,
                        "time": 2.3810504070002025,
                        "accuracy": 0.4767578125,
                        "total_cost": 873994.7417747569
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9365
Profiling... [512/50176]	Loss: 1.8141
Profiling... [768/50176]	Loss: 1.7688
Profiling... [1024/50176]	Loss: 1.7769
Profiling... [1280/50176]	Loss: 1.6885
Profiling... [1536/50176]	Loss: 1.7842
Profiling... [1792/50176]	Loss: 1.7869
Profiling... [2048/50176]	Loss: 1.8791
Profiling... [2304/50176]	Loss: 1.7921
Profiling... [2560/50176]	Loss: 1.8956
Profiling... [2816/50176]	Loss: 1.9344
Profiling... [3072/50176]	Loss: 1.8843
Profiling... [3328/50176]	Loss: 1.7294
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 7, Average loss: 0.0072, Accuracy: 0.4844
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.78410885124097,
                        "time": 2.426288617999944,
                        "accuracy": 0.484375,
                        "total_cost": 876594.5974709475
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9655
Profiling... [512/50176]	Loss: 1.8782
Profiling... [768/50176]	Loss: 1.8344
Profiling... [1024/50176]	Loss: 1.7496
Profiling... [1280/50176]	Loss: 1.7062
Profiling... [1536/50176]	Loss: 1.7957
Profiling... [1792/50176]	Loss: 1.8723
Profiling... [2048/50176]	Loss: 1.9136
Profiling... [2304/50176]	Loss: 1.7110
Profiling... [2560/50176]	Loss: 1.9033
Profiling... [2816/50176]	Loss: 1.9617
Profiling... [3072/50176]	Loss: 1.8800
Profiling... [3328/50176]	Loss: 1.6880
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 7, Average loss: 0.0073, Accuracy: 0.4836
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.79719485827529,
                        "time": 2.733931092000148,
                        "accuracy": 0.48359375,
                        "total_cost": 989338.5534863217
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7649
Profiling... [512/50176]	Loss: 1.9583
Profiling... [768/50176]	Loss: 1.7162
Profiling... [1024/50176]	Loss: 1.8901
Profiling... [1280/50176]	Loss: 1.6876
Profiling... [1536/50176]	Loss: 1.7580
Profiling... [1792/50176]	Loss: 1.9798
Profiling... [2048/50176]	Loss: 1.6773
Profiling... [2304/50176]	Loss: 1.8718
Profiling... [2560/50176]	Loss: 1.9247
Profiling... [2816/50176]	Loss: 1.6782
Profiling... [3072/50176]	Loss: 1.7626
Profiling... [3328/50176]	Loss: 1.8289
Profile done
epoch 1 train time consumed: 9.65s
Validation Epoch: 7, Average loss: 0.0073, Accuracy: 0.4827
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.7482579264007,
                        "time": 7.1429445300000225,
                        "accuracy": 0.48271484375,
                        "total_cost": 2589552.2148007364
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8216
Profiling... [512/50176]	Loss: 1.9131
Profiling... [768/50176]	Loss: 2.0954
Profiling... [1024/50176]	Loss: 1.7699
Profiling... [1280/50176]	Loss: 1.8700
Profiling... [1536/50176]	Loss: 1.7828
Profiling... [1792/50176]	Loss: 1.9384
Profiling... [2048/50176]	Loss: 1.8581
Profiling... [2304/50176]	Loss: 1.8385
Profiling... [2560/50176]	Loss: 1.7280
Profiling... [2816/50176]	Loss: 1.7773
Profiling... [3072/50176]	Loss: 1.6709
Profiling... [3328/50176]	Loss: 1.7024
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 7, Average loss: 0.0073, Accuracy: 0.4771
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.66321293212445,
                        "time": 2.3791458529999545,
                        "accuracy": 0.47705078125,
                        "total_cost": 872759.3385006998
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8916
Profiling... [512/50176]	Loss: 1.8539
Profiling... [768/50176]	Loss: 1.7254
Profiling... [1024/50176]	Loss: 1.9560
Profiling... [1280/50176]	Loss: 1.8052
Profiling... [1536/50176]	Loss: 1.8352
Profiling... [1792/50176]	Loss: 1.7537
Profiling... [2048/50176]	Loss: 1.8621
Profiling... [2304/50176]	Loss: 1.7703
Profiling... [2560/50176]	Loss: 1.7918
Profiling... [2816/50176]	Loss: 1.9548
Profiling... [3072/50176]	Loss: 1.8808
Profiling... [3328/50176]	Loss: 1.7025
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4742
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.69712913479815,
                        "time": 2.4073882190000404,
                        "accuracy": 0.47421875,
                        "total_cost": 888393.6755453197
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8066
Profiling... [512/50176]	Loss: 2.0270
Profiling... [768/50176]	Loss: 1.9733
Profiling... [1024/50176]	Loss: 1.6917
Profiling... [1280/50176]	Loss: 1.8303
Profiling... [1536/50176]	Loss: 1.8790
Profiling... [1792/50176]	Loss: 1.8078
Profiling... [2048/50176]	Loss: 1.8033
Profiling... [2304/50176]	Loss: 1.7275
Profiling... [2560/50176]	Loss: 1.6506
Profiling... [2816/50176]	Loss: 1.7794
Profiling... [3072/50176]	Loss: 1.6404
Profiling... [3328/50176]	Loss: 1.9747
Profile done
epoch 1 train time consumed: 4.09s
Validation Epoch: 7, Average loss: 0.0073, Accuracy: 0.4771
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.71249046906125,
                        "time": 2.731944728000144,
                        "accuracy": 0.47705078125,
                        "total_cost": 1002179.1100463169
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7379
Profiling... [512/50176]	Loss: 1.8154
Profiling... [768/50176]	Loss: 1.8099
Profiling... [1024/50176]	Loss: 1.7985
Profiling... [1280/50176]	Loss: 1.6613
Profiling... [1536/50176]	Loss: 1.8351
Profiling... [1792/50176]	Loss: 1.8376
Profiling... [2048/50176]	Loss: 1.8595
Profiling... [2304/50176]	Loss: 2.0531
Profiling... [2560/50176]	Loss: 1.8286
Profiling... [2816/50176]	Loss: 1.8690
Profiling... [3072/50176]	Loss: 1.8062
Profiling... [3328/50176]	Loss: 1.8478
Profile done
epoch 1 train time consumed: 9.62s
Validation Epoch: 7, Average loss: 0.0072, Accuracy: 0.4829
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.66678948253653,
                        "time": 7.116243721000046,
                        "accuracy": 0.48291015625,
                        "total_cost": 2578828.8671450117
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9926
Profiling... [512/50176]	Loss: 1.9655
Profiling... [768/50176]	Loss: 1.9175
Profiling... [1024/50176]	Loss: 1.7580
Profiling... [1280/50176]	Loss: 1.6872
Profiling... [1536/50176]	Loss: 1.8479
Profiling... [1792/50176]	Loss: 1.7213
Profiling... [2048/50176]	Loss: 1.8727
Profiling... [2304/50176]	Loss: 1.7663
Profiling... [2560/50176]	Loss: 1.9060
Profiling... [2816/50176]	Loss: 1.6778
Profiling... [3072/50176]	Loss: 1.7308
Profiling... [3328/50176]	Loss: 1.7510
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 7, Average loss: 0.0073, Accuracy: 0.4813
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.58533854256426,
                        "time": 2.3768486620001568,
                        "accuracy": 0.48125,
                        "total_cost": 864308.6043636934
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0008
Profiling... [512/50176]	Loss: 1.8659
Profiling... [768/50176]	Loss: 1.8401
Profiling... [1024/50176]	Loss: 1.9081
Profiling... [1280/50176]	Loss: 1.7878
Profiling... [1536/50176]	Loss: 1.8511
Profiling... [1792/50176]	Loss: 1.8445
Profiling... [2048/50176]	Loss: 1.9328
Profiling... [2304/50176]	Loss: 1.7045
Profiling... [2560/50176]	Loss: 1.8559
Profiling... [2816/50176]	Loss: 1.5897
Profiling... [3072/50176]	Loss: 1.8043
Profiling... [3328/50176]	Loss: 1.7143
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 7, Average loss: 0.0072, Accuracy: 0.4837
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.61878054539449,
                        "time": 2.426423366000108,
                        "accuracy": 0.48369140625,
                        "total_cost": 877882.2273111637
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7622
Profiling... [512/50176]	Loss: 1.9415
Profiling... [768/50176]	Loss: 2.0200
Profiling... [1024/50176]	Loss: 1.6958
Profiling... [1280/50176]	Loss: 1.7898
Profiling... [1536/50176]	Loss: 1.6491
Profiling... [1792/50176]	Loss: 1.9570
Profiling... [2048/50176]	Loss: 1.9000
Profiling... [2304/50176]	Loss: 2.1732
Profiling... [2560/50176]	Loss: 1.8873
Profiling... [2816/50176]	Loss: 1.9410
Profiling... [3072/50176]	Loss: 1.8655
Profiling... [3328/50176]	Loss: 1.7787
Profile done
epoch 1 train time consumed: 4.11s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.4799
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.63447284336098,
                        "time": 2.7152087440003925,
                        "accuracy": 0.4798828125,
                        "total_cost": 990161.5932537044
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8559
Profiling... [512/50176]	Loss: 1.9285
Profiling... [768/50176]	Loss: 1.9350
Profiling... [1024/50176]	Loss: 2.0221
Profiling... [1280/50176]	Loss: 1.7162
Profiling... [1536/50176]	Loss: 1.8290
Profiling... [1792/50176]	Loss: 1.8157
Profiling... [2048/50176]	Loss: 1.7968
Profiling... [2304/50176]	Loss: 1.7301
Profiling... [2560/50176]	Loss: 1.6124
Profiling... [2816/50176]	Loss: 1.7360
Profiling... [3072/50176]	Loss: 1.7542
Profiling... [3328/50176]	Loss: 1.9614
Profile done
epoch 1 train time consumed: 9.76s
Validation Epoch: 7, Average loss: 0.0073, Accuracy: 0.4782
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.58405677286744,
                        "time": 7.232524587000171,
                        "accuracy": 0.47822265625,
                        "total_cost": 2646657.966082154
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7067
Profiling... [512/50176]	Loss: 1.8266
Profiling... [768/50176]	Loss: 1.7143
Profiling... [1024/50176]	Loss: 1.9142
Profiling... [1280/50176]	Loss: 1.6640
Profiling... [1536/50176]	Loss: 1.6112
Profiling... [1792/50176]	Loss: 1.8385
Profiling... [2048/50176]	Loss: 1.9233
Profiling... [2304/50176]	Loss: 1.9902
Profiling... [2560/50176]	Loss: 1.9021
Profiling... [2816/50176]	Loss: 1.9042
Profiling... [3072/50176]	Loss: 1.6936
Profiling... [3328/50176]	Loss: 1.6035
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 7, Average loss: 0.0080, Accuracy: 0.4392
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.50226282068841,
                        "time": 2.389595716000258,
                        "accuracy": 0.43916015625,
                        "total_cost": 952224.9328602318
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8318
Profiling... [512/50176]	Loss: 1.8733
Profiling... [768/50176]	Loss: 1.7484
Profiling... [1024/50176]	Loss: 1.6212
Profiling... [1280/50176]	Loss: 1.8622
Profiling... [1536/50176]	Loss: 1.9675
Profiling... [1792/50176]	Loss: 1.8638
Profiling... [2048/50176]	Loss: 1.8830
Profiling... [2304/50176]	Loss: 1.8492
Profiling... [2560/50176]	Loss: 1.8057
Profiling... [2816/50176]	Loss: 1.9201
Profiling... [3072/50176]	Loss: 1.7856
Profiling... [3328/50176]	Loss: 1.8055
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 7, Average loss: 0.0083, Accuracy: 0.4334
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.53568551236653,
                        "time": 2.425398677999965,
                        "accuracy": 0.4333984375,
                        "total_cost": 979340.791116705
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0125
Profiling... [512/50176]	Loss: 1.8276
Profiling... [768/50176]	Loss: 1.7264
Profiling... [1024/50176]	Loss: 1.8801
Profiling... [1280/50176]	Loss: 1.8655
Profiling... [1536/50176]	Loss: 1.7241
Profiling... [1792/50176]	Loss: 1.8115
Profiling... [2048/50176]	Loss: 1.7214
Profiling... [2304/50176]	Loss: 1.7380
Profiling... [2560/50176]	Loss: 1.9666
Profiling... [2816/50176]	Loss: 1.8284
Profiling... [3072/50176]	Loss: 1.8713
Profiling... [3328/50176]	Loss: 1.8088
Profile done
epoch 1 train time consumed: 4.18s
Validation Epoch: 7, Average loss: 0.0089, Accuracy: 0.4032
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.55059374395043,
                        "time": 2.7347395950000646,
                        "accuracy": 0.40322265625,
                        "total_cost": 1186886.2567789091
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7526
Profiling... [512/50176]	Loss: 2.0371
Profiling... [768/50176]	Loss: 1.6893
Profiling... [1024/50176]	Loss: 1.8081
Profiling... [1280/50176]	Loss: 1.7463
Profiling... [1536/50176]	Loss: 1.8197
Profiling... [1792/50176]	Loss: 1.8518
Profiling... [2048/50176]	Loss: 1.7994
Profiling... [2304/50176]	Loss: 1.8123
Profiling... [2560/50176]	Loss: 1.7887
Profiling... [2816/50176]	Loss: 1.7931
Profiling... [3072/50176]	Loss: 1.7251
Profiling... [3328/50176]	Loss: 1.6677
Profile done
epoch 1 train time consumed: 9.56s
Validation Epoch: 7, Average loss: 0.0094, Accuracy: 0.3895
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.5069718010381,
                        "time": 6.973834984000405,
                        "accuracy": 0.389453125,
                        "total_cost": 3133679.1101626693
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8184
Profiling... [512/50176]	Loss: 2.0099
Profiling... [768/50176]	Loss: 1.8103
Profiling... [1024/50176]	Loss: 1.8000
Profiling... [1280/50176]	Loss: 1.7744
Profiling... [1536/50176]	Loss: 1.9103
Profiling... [1792/50176]	Loss: 1.6005
Profiling... [2048/50176]	Loss: 1.7995
Profiling... [2304/50176]	Loss: 1.7084
Profiling... [2560/50176]	Loss: 1.8113
Profiling... [2816/50176]	Loss: 1.8408
Profiling... [3072/50176]	Loss: 1.6282
Profiling... [3328/50176]	Loss: 1.8232
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 7, Average loss: 0.0081, Accuracy: 0.4509
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.42798750385312,
                        "time": 2.3826605350000136,
                        "accuracy": 0.45087890625,
                        "total_cost": 924783.9893264077
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9535
Profiling... [512/50176]	Loss: 1.7921
Profiling... [768/50176]	Loss: 1.6311
Profiling... [1024/50176]	Loss: 1.9400
Profiling... [1280/50176]	Loss: 1.8879
Profiling... [1536/50176]	Loss: 1.7267
Profiling... [1792/50176]	Loss: 1.8098
Profiling... [2048/50176]	Loss: 1.7314
Profiling... [2304/50176]	Loss: 1.7968
Profiling... [2560/50176]	Loss: 1.7500
Profiling... [2816/50176]	Loss: 1.6510
Profiling... [3072/50176]	Loss: 1.7560
Profiling... [3328/50176]	Loss: 1.7915
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 7, Average loss: 0.0079, Accuracy: 0.4521
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.46405958934209,
                        "time": 2.4156346899999335,
                        "accuracy": 0.45205078125,
                        "total_cost": 935151.7313631197
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9943
Profiling... [512/50176]	Loss: 1.7602
Profiling... [768/50176]	Loss: 1.9479
Profiling... [1024/50176]	Loss: 1.6800
Profiling... [1280/50176]	Loss: 1.6532
Profiling... [1536/50176]	Loss: 1.8844
Profiling... [1792/50176]	Loss: 1.7180
Profiling... [2048/50176]	Loss: 1.8688
Profiling... [2304/50176]	Loss: 1.6653
Profiling... [2560/50176]	Loss: 1.7302
Profiling... [2816/50176]	Loss: 1.7069
Profiling... [3072/50176]	Loss: 1.8031
Profiling... [3328/50176]	Loss: 1.8397
Profile done
epoch 1 train time consumed: 4.12s
Validation Epoch: 7, Average loss: 0.0084, Accuracy: 0.4315
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.47923237669565,
                        "time": 2.721895895000216,
                        "accuracy": 0.43154296875,
                        "total_cost": 1103787.608925184
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8853
Profiling... [512/50176]	Loss: 1.5750
Profiling... [768/50176]	Loss: 1.8122
Profiling... [1024/50176]	Loss: 1.8468
Profiling... [1280/50176]	Loss: 1.8413
Profiling... [1536/50176]	Loss: 1.8244
Profiling... [1792/50176]	Loss: 1.5900
Profiling... [2048/50176]	Loss: 1.6774
Profiling... [2304/50176]	Loss: 1.6787
Profiling... [2560/50176]	Loss: 1.9068
Profiling... [2816/50176]	Loss: 1.8519
Profiling... [3072/50176]	Loss: 1.6386
Profiling... [3328/50176]	Loss: 1.6819
Profile done
epoch 1 train time consumed: 9.79s
Validation Epoch: 7, Average loss: 0.0079, Accuracy: 0.4550
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.43297473016783,
                        "time": 7.15725112400014,
                        "accuracy": 0.45498046875,
                        "total_cost": 2752907.064650838
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0200
Profiling... [512/50176]	Loss: 1.7318
Profiling... [768/50176]	Loss: 1.6803
Profiling... [1024/50176]	Loss: 1.8088
Profiling... [1280/50176]	Loss: 1.8893
Profiling... [1536/50176]	Loss: 1.5905
Profiling... [1792/50176]	Loss: 1.8699
Profiling... [2048/50176]	Loss: 1.7123
Profiling... [2304/50176]	Loss: 1.6256
Profiling... [2560/50176]	Loss: 1.8165
Profiling... [2816/50176]	Loss: 1.8008
Profiling... [3072/50176]	Loss: 1.7258
Profiling... [3328/50176]	Loss: 1.8576
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 7, Average loss: 0.0079, Accuracy: 0.4485
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.36657398716478,
                        "time": 2.3913567970002987,
                        "accuracy": 0.44853515625,
                        "total_cost": 933009.2271335805
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9342
Profiling... [512/50176]	Loss: 1.7808
Profiling... [768/50176]	Loss: 1.7256
Profiling... [1024/50176]	Loss: 1.7452
Profiling... [1280/50176]	Loss: 1.9131
Profiling... [1536/50176]	Loss: 1.8100
Profiling... [1792/50176]	Loss: 1.8316
Profiling... [2048/50176]	Loss: 1.6883
Profiling... [2304/50176]	Loss: 1.6706
Profiling... [2560/50176]	Loss: 1.8545
Profiling... [2816/50176]	Loss: 2.0102
Profiling... [3072/50176]	Loss: 1.7970
Profiling... [3328/50176]	Loss: 1.7795
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 7, Average loss: 0.0082, Accuracy: 0.4447
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.39839366253463,
                        "time": 2.420821136000086,
                        "accuracy": 0.4447265625,
                        "total_cost": 952593.648597311
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7453
Profiling... [512/50176]	Loss: 2.0229
Profiling... [768/50176]	Loss: 1.8259
Profiling... [1024/50176]	Loss: 1.7725
Profiling... [1280/50176]	Loss: 1.7610
Profiling... [1536/50176]	Loss: 1.8155
Profiling... [1792/50176]	Loss: 1.5978
Profiling... [2048/50176]	Loss: 1.7670
Profiling... [2304/50176]	Loss: 1.7494
Profiling... [2560/50176]	Loss: 1.8593
Profiling... [2816/50176]	Loss: 1.6544
Profiling... [3072/50176]	Loss: 1.7855
Profiling... [3328/50176]	Loss: 1.6787
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 7, Average loss: 0.0087, Accuracy: 0.4255
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.410942139673,
                        "time": 2.7377411959996607,
                        "accuracy": 0.42548828125,
                        "total_cost": 1126011.5270212055
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9245
Profiling... [512/50176]	Loss: 2.0392
Profiling... [768/50176]	Loss: 1.7985
Profiling... [1024/50176]	Loss: 1.8100
Profiling... [1280/50176]	Loss: 1.8656
Profiling... [1536/50176]	Loss: 1.7765
Profiling... [1792/50176]	Loss: 1.6633
Profiling... [2048/50176]	Loss: 1.8718
Profiling... [2304/50176]	Loss: 1.7628
Profiling... [2560/50176]	Loss: 1.9079
Profiling... [2816/50176]	Loss: 1.8978
Profiling... [3072/50176]	Loss: 1.5943
Profiling... [3328/50176]	Loss: 1.7619
Profile done
epoch 1 train time consumed: 9.68s
Validation Epoch: 7, Average loss: 0.0082, Accuracy: 0.4375
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.3683897899861,
                        "time": 7.039669041999787,
                        "accuracy": 0.4375,
                        "total_cost": 2815867.6167999147
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7829
Profiling... [512/50176]	Loss: 1.9218
Profiling... [768/50176]	Loss: 2.0437
Profiling... [1024/50176]	Loss: 1.8971
Profiling... [1280/50176]	Loss: 1.8536
Profiling... [1536/50176]	Loss: 1.9446
Profiling... [1792/50176]	Loss: 2.0447
Profiling... [2048/50176]	Loss: 2.0106
Profiling... [2304/50176]	Loss: 1.9414
Profiling... [2560/50176]	Loss: 1.8439
Profiling... [2816/50176]	Loss: 1.7313
Profiling... [3072/50176]	Loss: 1.9511
Profiling... [3328/50176]	Loss: 1.9353
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 7, Average loss: 0.0103, Accuracy: 0.3479
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.28806204922445,
                        "time": 2.379826980999951,
                        "accuracy": 0.34794921875,
                        "total_cost": 1196926.7330765962
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7229
Profiling... [512/50176]	Loss: 1.8601
Profiling... [768/50176]	Loss: 2.0244
Profiling... [1024/50176]	Loss: 1.9967
Profiling... [1280/50176]	Loss: 1.8317
Profiling... [1536/50176]	Loss: 2.1789
Profiling... [1792/50176]	Loss: 1.8535
Profiling... [2048/50176]	Loss: 1.8250
Profiling... [2304/50176]	Loss: 1.9104
Profiling... [2560/50176]	Loss: 1.7552
Profiling... [2816/50176]	Loss: 1.8666
Profiling... [3072/50176]	Loss: 2.0265
Profiling... [3328/50176]	Loss: 2.0077
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 7, Average loss: 0.0116, Accuracy: 0.2960
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.32465107537335,
                        "time": 2.4319883380003375,
                        "accuracy": 0.29599609375,
                        "total_cost": 1437849.9180787215
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9812
Profiling... [512/50176]	Loss: 2.0294
Profiling... [768/50176]	Loss: 1.6459
Profiling... [1024/50176]	Loss: 1.7653
Profiling... [1280/50176]	Loss: 1.8320
Profiling... [1536/50176]	Loss: 1.9568
Profiling... [1792/50176]	Loss: 1.9810
Profiling... [2048/50176]	Loss: 1.7947
Profiling... [2304/50176]	Loss: 1.9027
Profiling... [2560/50176]	Loss: 1.8613
Profiling... [2816/50176]	Loss: 1.9347
Profiling... [3072/50176]	Loss: 1.9389
Profiling... [3328/50176]	Loss: 1.9038
Profile done
epoch 1 train time consumed: 4.18s
Validation Epoch: 7, Average loss: 0.0148, Accuracy: 0.2311
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.33664324369664,
                        "time": 2.7267800390000048,
                        "accuracy": 0.2310546875,
                        "total_cost": 2065253.5206627257
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6481
Profiling... [512/50176]	Loss: 1.8829
Profiling... [768/50176]	Loss: 1.8503
Profiling... [1024/50176]	Loss: 1.7875
Profiling... [1280/50176]	Loss: 1.7339
Profiling... [1536/50176]	Loss: 1.9258
Profiling... [1792/50176]	Loss: 1.8558
Profiling... [2048/50176]	Loss: 1.8583
Profiling... [2304/50176]	Loss: 1.6911
Profiling... [2560/50176]	Loss: 1.7033
Profiling... [2816/50176]	Loss: 1.9064
Profiling... [3072/50176]	Loss: 1.8121
Profiling... [3328/50176]	Loss: 1.7008
Profile done
epoch 1 train time consumed: 9.78s
Validation Epoch: 7, Average loss: 0.0126, Accuracy: 0.2788
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.28870312322209,
                        "time": 7.233141544000318,
                        "accuracy": 0.27880859375,
                        "total_cost": 4540031.399946961
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7735
Profiling... [512/50176]	Loss: 1.8821
Profiling... [768/50176]	Loss: 1.7892
Profiling... [1024/50176]	Loss: 1.7820
Profiling... [1280/50176]	Loss: 1.9333
Profiling... [1536/50176]	Loss: 1.8644
Profiling... [1792/50176]	Loss: 1.7552
Profiling... [2048/50176]	Loss: 2.0395
Profiling... [2304/50176]	Loss: 1.7696
Profiling... [2560/50176]	Loss: 1.8935
Profiling... [2816/50176]	Loss: 1.9183
Profiling... [3072/50176]	Loss: 1.8325
Profiling... [3328/50176]	Loss: 1.9778
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 7, Average loss: 0.0178, Accuracy: 0.2028
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.21249467976918,
                        "time": 2.392595791999611,
                        "accuracy": 0.20283203125,
                        "total_cost": 2064290.639991961
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7519
Profiling... [512/50176]	Loss: 1.8240
Profiling... [768/50176]	Loss: 1.9250
Profiling... [1024/50176]	Loss: 1.7599
Profiling... [1280/50176]	Loss: 1.9151
Profiling... [1536/50176]	Loss: 2.1228
Profiling... [1792/50176]	Loss: 1.8135
Profiling... [2048/50176]	Loss: 1.9786
Profiling... [2304/50176]	Loss: 1.8486
Profiling... [2560/50176]	Loss: 1.8796
Profiling... [2816/50176]	Loss: 1.7743
Profiling... [3072/50176]	Loss: 1.9151
Profiling... [3328/50176]	Loss: 1.9212
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 7, Average loss: 0.0201, Accuracy: 0.1658
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.24663000391543,
                        "time": 2.4280278900000667,
                        "accuracy": 0.1658203125,
                        "total_cost": 2562441.683674982
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9118
Profiling... [512/50176]	Loss: 1.8675
Profiling... [768/50176]	Loss: 1.9209
Profiling... [1024/50176]	Loss: 1.8842
Profiling... [1280/50176]	Loss: 1.9553
Profiling... [1536/50176]	Loss: 1.9288
Profiling... [1792/50176]	Loss: 1.7712
Profiling... [2048/50176]	Loss: 1.8595
Profiling... [2304/50176]	Loss: 2.0560
Profiling... [2560/50176]	Loss: 1.7954
Profiling... [2816/50176]	Loss: 1.6633
Profiling... [3072/50176]	Loss: 1.7592
Profiling... [3328/50176]	Loss: 1.9185
Profile done
epoch 1 train time consumed: 4.21s
Validation Epoch: 7, Average loss: 0.0131, Accuracy: 0.3013
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.25774908655953,
                        "time": 2.7457235239999136,
                        "accuracy": 0.30126953125,
                        "total_cost": 1594922.7082683453
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7579
Profiling... [512/50176]	Loss: 1.6289
Profiling... [768/50176]	Loss: 1.8561
Profiling... [1024/50176]	Loss: 1.8528
Profiling... [1280/50176]	Loss: 1.8893
Profiling... [1536/50176]	Loss: 1.6975
Profiling... [1792/50176]	Loss: 1.9592
Profiling... [2048/50176]	Loss: 1.9498
Profiling... [2304/50176]	Loss: 1.9181
Profiling... [2560/50176]	Loss: 1.9113
Profiling... [2816/50176]	Loss: 2.0764
Profiling... [3072/50176]	Loss: 1.9813
Profiling... [3328/50176]	Loss: 1.9431
Profile done
epoch 1 train time consumed: 9.83s
Validation Epoch: 7, Average loss: 0.0133, Accuracy: 0.2583
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.21638074663788,
                        "time": 6.828983894999965,
                        "accuracy": 0.25830078125,
                        "total_cost": 4626668.861943265
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6775
Profiling... [512/50176]	Loss: 1.8501
Profiling... [768/50176]	Loss: 1.9296
Profiling... [1024/50176]	Loss: 1.6281
Profiling... [1280/50176]	Loss: 1.7500
Profiling... [1536/50176]	Loss: 1.8701
Profiling... [1792/50176]	Loss: 1.9576
Profiling... [2048/50176]	Loss: 1.8818
Profiling... [2304/50176]	Loss: 1.8424
Profiling... [2560/50176]	Loss: 1.9372
Profiling... [2816/50176]	Loss: 2.0090
Profiling... [3072/50176]	Loss: 1.8658
Profiling... [3328/50176]	Loss: 1.8986
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 7, Average loss: 0.0195, Accuracy: 0.1769
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.14310452951135,
                        "time": 2.3814874129998316,
                        "accuracy": 0.17685546875,
                        "total_cost": 2356502.177855162
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8177
Profiling... [512/50176]	Loss: 1.7190
Profiling... [768/50176]	Loss: 2.0472
Profiling... [1024/50176]	Loss: 1.8849
Profiling... [1280/50176]	Loss: 1.9539
Profiling... [1536/50176]	Loss: 1.7976
Profiling... [1792/50176]	Loss: 1.8911
Profiling... [2048/50176]	Loss: 1.7240
Profiling... [2304/50176]	Loss: 1.8736
Profiling... [2560/50176]	Loss: 2.0317
Profiling... [2816/50176]	Loss: 1.8716
Profiling... [3072/50176]	Loss: 2.0699
Profiling... [3328/50176]	Loss: 1.8566
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 7, Average loss: 0.0097, Accuracy: 0.3822
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.1756279768712,
                        "time": 2.422887744000036,
                        "accuracy": 0.3822265625,
                        "total_cost": 1109303.7397159082
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9110
Profiling... [512/50176]	Loss: 1.8021
Profiling... [768/50176]	Loss: 1.9704
Profiling... [1024/50176]	Loss: 1.8739
Profiling... [1280/50176]	Loss: 1.9648
Profiling... [1536/50176]	Loss: 1.9601
Profiling... [1792/50176]	Loss: 1.8331
Profiling... [2048/50176]	Loss: 1.7929
Profiling... [2304/50176]	Loss: 1.8695
Profiling... [2560/50176]	Loss: 1.8070
Profiling... [2816/50176]	Loss: 1.8077
Profiling... [3072/50176]	Loss: 1.8809
Profiling... [3328/50176]	Loss: 1.8183
Profile done
epoch 1 train time consumed: 4.16s
Validation Epoch: 7, Average loss: 0.0110, Accuracy: 0.3464
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.19260593911983,
                        "time": 2.7259903889998895,
                        "accuracy": 0.34638671875,
                        "total_cost": 1377213.0750177058
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7786
Profiling... [512/50176]	Loss: 1.8612
Profiling... [768/50176]	Loss: 1.8664
Profiling... [1024/50176]	Loss: 1.9305
Profiling... [1280/50176]	Loss: 1.8790
Profiling... [1536/50176]	Loss: 1.9632
Profiling... [1792/50176]	Loss: 1.7365
Profiling... [2048/50176]	Loss: 2.0062
Profiling... [2304/50176]	Loss: 1.9300
Profiling... [2560/50176]	Loss: 2.0003
Profiling... [2816/50176]	Loss: 1.6754
Profiling... [3072/50176]	Loss: 2.0031
Profiling... [3328/50176]	Loss: 1.8888
Profile done
epoch 1 train time consumed: 9.90s
Validation Epoch: 7, Average loss: 0.0122, Accuracy: 0.3092
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.14677335308681,
                        "time": 7.136938812000153,
                        "accuracy": 0.3091796875,
                        "total_cost": 4039606.554360162
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8065
Profiling... [1024/50176]	Loss: 1.8308
Profiling... [1536/50176]	Loss: 1.8708
Profiling... [2048/50176]	Loss: 1.8956
Profiling... [2560/50176]	Loss: 1.7912
Profiling... [3072/50176]	Loss: 1.7869
Profiling... [3584/50176]	Loss: 1.7471
Profiling... [4096/50176]	Loss: 1.7577
Profiling... [4608/50176]	Loss: 1.7994
Profiling... [5120/50176]	Loss: 1.7813
Profiling... [5632/50176]	Loss: 1.6981
Profiling... [6144/50176]	Loss: 1.7853
Profiling... [6656/50176]	Loss: 1.7561
Profile done
epoch 1 train time consumed: 6.90s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4873
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.1228245365776,
                        "time": 4.506379315000231,
                        "accuracy": 0.4873046875,
                        "total_cost": 1618322.992481045
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8918
Profiling... [1024/50176]	Loss: 1.8434
Profiling... [1536/50176]	Loss: 1.7737
Profiling... [2048/50176]	Loss: 1.7761
Profiling... [2560/50176]	Loss: 1.7953
Profiling... [3072/50176]	Loss: 1.8318
Profiling... [3584/50176]	Loss: 1.8104
Profiling... [4096/50176]	Loss: 1.8083
Profiling... [4608/50176]	Loss: 1.8838
Profiling... [5120/50176]	Loss: 1.6616
Profiling... [5632/50176]	Loss: 1.7605
Profiling... [6144/50176]	Loss: 1.7692
Profiling... [6656/50176]	Loss: 1.6909
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4848
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.17197928708497,
                        "time": 4.633507824999924,
                        "accuracy": 0.484765625,
                        "total_cost": 1672692.5911361533
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8477
Profiling... [1024/50176]	Loss: 1.8959
Profiling... [1536/50176]	Loss: 1.7391
Profiling... [2048/50176]	Loss: 1.6880
Profiling... [2560/50176]	Loss: 1.8126
Profiling... [3072/50176]	Loss: 1.7781
Profiling... [3584/50176]	Loss: 1.7609
Profiling... [4096/50176]	Loss: 1.8981
Profiling... [4608/50176]	Loss: 1.6845
Profiling... [5120/50176]	Loss: 1.8013
Profiling... [5632/50176]	Loss: 1.6876
Profiling... [6144/50176]	Loss: 1.7865
Profiling... [6656/50176]	Loss: 1.7521
Profile done
epoch 1 train time consumed: 7.58s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4881
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.18636237481132,
                        "time": 5.242609485999765,
                        "accuracy": 0.4880859375,
                        "total_cost": 1879703.1210307279
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.8394
Profiling... [1024/50176]	Loss: 1.9224
Profiling... [1536/50176]	Loss: 1.7238
Profiling... [2048/50176]	Loss: 1.8601
Profiling... [2560/50176]	Loss: 1.8441
Profiling... [3072/50176]	Loss: 1.8006
Profiling... [3584/50176]	Loss: 1.7849
Profiling... [4096/50176]	Loss: 1.8111
Profiling... [4608/50176]	Loss: 1.8204
Profiling... [5120/50176]	Loss: 1.7118
Profiling... [5632/50176]	Loss: 1.8034
Profiling... [6144/50176]	Loss: 1.8343
Profiling... [6656/50176]	Loss: 1.8127
Profile done
epoch 1 train time consumed: 17.66s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4859
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.089835907154,
                        "time": 12.999002442999881,
                        "accuracy": 0.4859375,
                        "total_cost": 4681312.776900278
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8423
Profiling... [1024/50176]	Loss: 1.8241
Profiling... [1536/50176]	Loss: 1.9343
Profiling... [2048/50176]	Loss: 1.8294
Profiling... [2560/50176]	Loss: 1.8947
Profiling... [3072/50176]	Loss: 1.7636
Profiling... [3584/50176]	Loss: 1.8638
Profiling... [4096/50176]	Loss: 1.7731
Profiling... [4608/50176]	Loss: 1.8686
Profiling... [5120/50176]	Loss: 1.6495
Profiling... [5632/50176]	Loss: 1.7493
Profiling... [6144/50176]	Loss: 1.6583
Profiling... [6656/50176]	Loss: 1.7778
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4888
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.06146041814462,
                        "time": 4.505742604000261,
                        "accuracy": 0.48876953125,
                        "total_cost": 1613244.9043693242
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8725
Profiling... [1024/50176]	Loss: 1.8872
Profiling... [1536/50176]	Loss: 1.8723
Profiling... [2048/50176]	Loss: 2.0206
Profiling... [2560/50176]	Loss: 1.8403
Profiling... [3072/50176]	Loss: 1.8683
Profiling... [3584/50176]	Loss: 1.8280
Profiling... [4096/50176]	Loss: 1.8663
Profiling... [4608/50176]	Loss: 1.6507
Profiling... [5120/50176]	Loss: 1.6020
Profiling... [5632/50176]	Loss: 1.8356
Profiling... [6144/50176]	Loss: 1.7390
Profiling... [6656/50176]	Loss: 1.7705
Profile done
epoch 1 train time consumed: 6.88s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4831
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.11098381503145,
                        "time": 4.62273267799992,
                        "accuracy": 0.48310546875,
                        "total_cost": 1674537.4891804848
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8835
Profiling... [1024/50176]	Loss: 1.9072
Profiling... [1536/50176]	Loss: 1.8100
Profiling... [2048/50176]	Loss: 1.8993
Profiling... [2560/50176]	Loss: 1.8479
Profiling... [3072/50176]	Loss: 1.6907
Profiling... [3584/50176]	Loss: 1.7951
Profiling... [4096/50176]	Loss: 1.8290
Profiling... [4608/50176]	Loss: 1.7845
Profiling... [5120/50176]	Loss: 1.8273
Profiling... [5632/50176]	Loss: 1.7896
Profiling... [6144/50176]	Loss: 1.7683
Profiling... [6656/50176]	Loss: 1.8175
Profile done
epoch 1 train time consumed: 7.53s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4838
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.12143752069785,
                        "time": 5.260919767999894,
                        "accuracy": 0.4837890625,
                        "total_cost": 1903021.4421186538
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7774
Profiling... [1024/50176]	Loss: 1.7744
Profiling... [1536/50176]	Loss: 1.8791
Profiling... [2048/50176]	Loss: 1.9748
Profiling... [2560/50176]	Loss: 1.7902
Profiling... [3072/50176]	Loss: 1.7673
Profiling... [3584/50176]	Loss: 1.7386
Profiling... [4096/50176]	Loss: 1.8604
Profiling... [4608/50176]	Loss: 1.7807
Profiling... [5120/50176]	Loss: 1.7858
Profiling... [5632/50176]	Loss: 1.7921
Profiling... [6144/50176]	Loss: 1.7204
Profiling... [6656/50176]	Loss: 1.7542
Profile done
epoch 1 train time consumed: 17.67s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4860
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.02741425513894,
                        "time": 13.019040697000037,
                        "accuracy": 0.48603515625,
                        "total_cost": 4687587.0864022635
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7980
Profiling... [1024/50176]	Loss: 1.7619
Profiling... [1536/50176]	Loss: 1.8620
Profiling... [2048/50176]	Loss: 1.7203
Profiling... [2560/50176]	Loss: 1.6106
Profiling... [3072/50176]	Loss: 1.8272
Profiling... [3584/50176]	Loss: 1.6567
Profiling... [4096/50176]	Loss: 1.8597
Profiling... [4608/50176]	Loss: 1.9147
Profiling... [5120/50176]	Loss: 1.8074
Profiling... [5632/50176]	Loss: 1.7148
Profiling... [6144/50176]	Loss: 1.9108
Profiling... [6656/50176]	Loss: 1.8781
Profile done
epoch 1 train time consumed: 6.74s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4881
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.99528948837653,
                        "time": 4.506572225999662,
                        "accuracy": 0.4880859375,
                        "total_cost": 1615801.8065208872
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8546
Profiling... [1024/50176]	Loss: 1.9625
Profiling... [1536/50176]	Loss: 1.8076
Profiling... [2048/50176]	Loss: 1.8191
Profiling... [2560/50176]	Loss: 1.8476
Profiling... [3072/50176]	Loss: 1.8604
Profiling... [3584/50176]	Loss: 1.8787
Profiling... [4096/50176]	Loss: 1.8163
Profiling... [4608/50176]	Loss: 1.8407
Profiling... [5120/50176]	Loss: 1.7775
Profiling... [5632/50176]	Loss: 1.7465
Profiling... [6144/50176]	Loss: 1.7093
Profiling... [6656/50176]	Loss: 1.8439
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4848
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.04518861828672,
                        "time": 4.641905878999751,
                        "accuracy": 0.484765625,
                        "total_cost": 1675724.2818629239
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8487
Profiling... [1024/50176]	Loss: 1.8089
Profiling... [1536/50176]	Loss: 1.8881
Profiling... [2048/50176]	Loss: 1.7048
Profiling... [2560/50176]	Loss: 1.7574
Profiling... [3072/50176]	Loss: 1.7738
Profiling... [3584/50176]	Loss: 1.7339
Profiling... [4096/50176]	Loss: 1.7110
Profiling... [4608/50176]	Loss: 1.7841
Profiling... [5120/50176]	Loss: 1.8538
Profiling... [5632/50176]	Loss: 1.7587
Profiling... [6144/50176]	Loss: 1.8020
Profiling... [6656/50176]	Loss: 1.7843
Profile done
epoch 1 train time consumed: 7.60s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4861
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.0581985733764,
                        "time": 5.244038988000284,
                        "accuracy": 0.4861328125,
                        "total_cost": 1887769.7602443772
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.8538
Profiling... [1024/50176]	Loss: 1.8728
Profiling... [1536/50176]	Loss: 1.7810
Profiling... [2048/50176]	Loss: 1.8365
Profiling... [2560/50176]	Loss: 1.8420
Profiling... [3072/50176]	Loss: 1.8738
Profiling... [3584/50176]	Loss: 1.7456
Profiling... [4096/50176]	Loss: 1.8222
Profiling... [4608/50176]	Loss: 1.8380
Profiling... [5120/50176]	Loss: 1.8888
Profiling... [5632/50176]	Loss: 1.8569
Profiling... [6144/50176]	Loss: 1.6790
Profiling... [6656/50176]	Loss: 1.7425
Profile done
epoch 1 train time consumed: 17.63s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4876
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.96454651428496,
                        "time": 13.060276816000169,
                        "accuracy": 0.48759765625,
                        "total_cost": 4687365.522586081
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8370
Profiling... [1024/50176]	Loss: 1.7712
Profiling... [1536/50176]	Loss: 1.7387
Profiling... [2048/50176]	Loss: 1.7746
Profiling... [2560/50176]	Loss: 1.7695
Profiling... [3072/50176]	Loss: 1.8499
Profiling... [3584/50176]	Loss: 1.7953
Profiling... [4096/50176]	Loss: 1.7693
Profiling... [4608/50176]	Loss: 1.8538
Profiling... [5120/50176]	Loss: 1.6844
Profiling... [5632/50176]	Loss: 1.8039
Profiling... [6144/50176]	Loss: 1.7087
Profiling... [6656/50176]	Loss: 1.7333
Profile done
epoch 1 train time consumed: 6.69s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4579
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.92870422439928,
                        "time": 4.519337794999956,
                        "accuracy": 0.45791015625,
                        "total_cost": 1727160.0189038008
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7786
Profiling... [1024/50176]	Loss: 1.7705
Profiling... [1536/50176]	Loss: 1.8175
Profiling... [2048/50176]	Loss: 1.6100
Profiling... [2560/50176]	Loss: 1.6938
Profiling... [3072/50176]	Loss: 1.7310
Profiling... [3584/50176]	Loss: 1.7968
Profiling... [4096/50176]	Loss: 1.6815
Profiling... [4608/50176]	Loss: 1.7298
Profiling... [5120/50176]	Loss: 1.7385
Profiling... [5632/50176]	Loss: 1.8011
Profiling... [6144/50176]	Loss: 1.9703
Profiling... [6656/50176]	Loss: 1.7530
Profile done
epoch 1 train time consumed: 6.99s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4535
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.98095708701834,
                        "time": 4.644509491000008,
                        "accuracy": 0.453515625,
                        "total_cost": 1792196.5994556448
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8940
Profiling... [1024/50176]	Loss: 1.7837
Profiling... [1536/50176]	Loss: 1.8916
Profiling... [2048/50176]	Loss: 1.7039
Profiling... [2560/50176]	Loss: 1.7629
Profiling... [3072/50176]	Loss: 1.8510
Profiling... [3584/50176]	Loss: 1.6794
Profiling... [4096/50176]	Loss: 1.8006
Profiling... [4608/50176]	Loss: 1.7768
Profiling... [5120/50176]	Loss: 1.7874
Profiling... [5632/50176]	Loss: 1.6222
Profiling... [6144/50176]	Loss: 1.7159
Profiling... [6656/50176]	Loss: 1.7583
Profile done
epoch 1 train time consumed: 7.63s
Validation Epoch: 7, Average loss: 0.0041, Accuracy: 0.4409
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.9876976633591,
                        "time": 5.273138772000038,
                        "accuracy": 0.44091796875,
                        "total_cost": 2092904.690902341
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7579
Profiling... [1024/50176]	Loss: 1.7877
Profiling... [1536/50176]	Loss: 1.7616
Profiling... [2048/50176]	Loss: 1.7920
Profiling... [2560/50176]	Loss: 1.8227
Profiling... [3072/50176]	Loss: 1.7534
Profiling... [3584/50176]	Loss: 1.7416
Profiling... [4096/50176]	Loss: 1.8123
Profiling... [4608/50176]	Loss: 1.7931
Profiling... [5120/50176]	Loss: 1.8252
Profiling... [5632/50176]	Loss: 1.7962
Profiling... [6144/50176]	Loss: 1.7161
Profiling... [6656/50176]	Loss: 1.7089
Profile done
epoch 1 train time consumed: 17.73s
Validation Epoch: 7, Average loss: 0.0045, Accuracy: 0.4095
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.8956310411318,
                        "time": 13.0077667999999,
                        "accuracy": 0.40947265625,
                        "total_cost": 5559245.9111852655
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7775
Profiling... [1024/50176]	Loss: 1.8007
Profiling... [1536/50176]	Loss: 1.6257
Profiling... [2048/50176]	Loss: 1.7958
Profiling... [2560/50176]	Loss: 1.6297
Profiling... [3072/50176]	Loss: 1.7385
Profiling... [3584/50176]	Loss: 1.7870
Profiling... [4096/50176]	Loss: 1.7944
Profiling... [4608/50176]	Loss: 1.8792
Profiling... [5120/50176]	Loss: 1.7066
Profiling... [5632/50176]	Loss: 1.7752
Profiling... [6144/50176]	Loss: 1.7801
Profiling... [6656/50176]	Loss: 1.7362
Profile done
epoch 1 train time consumed: 6.74s
Validation Epoch: 7, Average loss: 0.0041, Accuracy: 0.4467
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.86014944081376,
                        "time": 4.527057666000019,
                        "accuracy": 0.4466796875,
                        "total_cost": 1773608.9500376112
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8829
Profiling... [1024/50176]	Loss: 1.6827
Profiling... [1536/50176]	Loss: 1.8458
Profiling... [2048/50176]	Loss: 1.7994
Profiling... [2560/50176]	Loss: 1.8087
Profiling... [3072/50176]	Loss: 1.8027
Profiling... [3584/50176]	Loss: 1.7843
Profiling... [4096/50176]	Loss: 1.7095
Profiling... [4608/50176]	Loss: 1.6584
Profiling... [5120/50176]	Loss: 1.8330
Profiling... [5632/50176]	Loss: 1.7601
Profiling... [6144/50176]	Loss: 1.8032
Profiling... [6656/50176]	Loss: 1.6934
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4572
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.91024116200308,
                        "time": 4.635822079999798,
                        "accuracy": 0.4572265625,
                        "total_cost": 1774325.7512515245
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9462
Profiling... [1024/50176]	Loss: 1.7167
Profiling... [1536/50176]	Loss: 1.7900
Profiling... [2048/50176]	Loss: 1.7083
Profiling... [2560/50176]	Loss: 1.7593
Profiling... [3072/50176]	Loss: 1.7622
Profiling... [3584/50176]	Loss: 1.6466
Profiling... [4096/50176]	Loss: 1.7053
Profiling... [4608/50176]	Loss: 1.7730
Profiling... [5120/50176]	Loss: 1.6981
Profiling... [5632/50176]	Loss: 1.7612
Profiling... [6144/50176]	Loss: 1.6936
Profiling... [6656/50176]	Loss: 1.7137
Profile done
epoch 1 train time consumed: 7.57s
Validation Epoch: 7, Average loss: 0.0041, Accuracy: 0.4418
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.92299868175202,
                        "time": 5.259595329000149,
                        "accuracy": 0.441796875,
                        "total_cost": 2083376.3991088124
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9426
Profiling... [1024/50176]	Loss: 1.6931
Profiling... [1536/50176]	Loss: 1.7547
Profiling... [2048/50176]	Loss: 1.7068
Profiling... [2560/50176]	Loss: 1.8322
Profiling... [3072/50176]	Loss: 1.8080
Profiling... [3584/50176]	Loss: 1.7422
Profiling... [4096/50176]	Loss: 1.7807
Profiling... [4608/50176]	Loss: 1.7608
Profiling... [5120/50176]	Loss: 1.7591
Profiling... [5632/50176]	Loss: 1.7664
Profiling... [6144/50176]	Loss: 1.8572
Profiling... [6656/50176]	Loss: 1.7527
Profile done
epoch 1 train time consumed: 17.65s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.4723
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.8335235866914,
                        "time": 13.005434105000404,
                        "accuracy": 0.472265625,
                        "total_cost": 4819217.931381456
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8055
Profiling... [1024/50176]	Loss: 1.7925
Profiling... [1536/50176]	Loss: 1.7826
Profiling... [2048/50176]	Loss: 1.8928
Profiling... [2560/50176]	Loss: 1.7046
Profiling... [3072/50176]	Loss: 1.7023
Profiling... [3584/50176]	Loss: 1.7953
Profiling... [4096/50176]	Loss: 1.8097
Profiling... [4608/50176]	Loss: 1.8412
Profiling... [5120/50176]	Loss: 1.8138
Profiling... [5632/50176]	Loss: 1.7420
Profiling... [6144/50176]	Loss: 1.7191
Profiling... [6656/50176]	Loss: 1.6652
Profile done
epoch 1 train time consumed: 6.72s
Validation Epoch: 7, Average loss: 0.0041, Accuracy: 0.4373
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.80173952371786,
                        "time": 4.520174074999886,
                        "accuracy": 0.4373046875,
                        "total_cost": 1808877.1644483688
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8876
Profiling... [1024/50176]	Loss: 1.7179
Profiling... [1536/50176]	Loss: 1.8407
Profiling... [2048/50176]	Loss: 1.8612
Profiling... [2560/50176]	Loss: 1.8659
Profiling... [3072/50176]	Loss: 1.7357
Profiling... [3584/50176]	Loss: 1.7871
Profiling... [4096/50176]	Loss: 1.8320
Profiling... [4608/50176]	Loss: 1.8390
Profiling... [5120/50176]	Loss: 1.8371
Profiling... [5632/50176]	Loss: 1.6483
Profiling... [6144/50176]	Loss: 1.6676
Profiling... [6656/50176]	Loss: 1.6714
Profile done
epoch 1 train time consumed: 6.90s
Validation Epoch: 7, Average loss: 0.0042, Accuracy: 0.4362
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.85036054572298,
                        "time": 4.634989096000027,
                        "accuracy": 0.43623046875,
                        "total_cost": 1859391.193201712
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9124
Profiling... [1024/50176]	Loss: 1.7939
Profiling... [1536/50176]	Loss: 1.7171
Profiling... [2048/50176]	Loss: 1.7136
Profiling... [2560/50176]	Loss: 1.8011
Profiling... [3072/50176]	Loss: 1.7586
Profiling... [3584/50176]	Loss: 1.7317
Profiling... [4096/50176]	Loss: 1.7841
Profiling... [4608/50176]	Loss: 1.8190
Profiling... [5120/50176]	Loss: 1.8075
Profiling... [5632/50176]	Loss: 1.6354
Profiling... [6144/50176]	Loss: 1.8220
Profiling... [6656/50176]	Loss: 1.7514
Profile done
epoch 1 train time consumed: 7.61s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4562
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.86173736177085,
                        "time": 5.247239379999883,
                        "accuracy": 0.45625,
                        "total_cost": 2012639.7621917357
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7779
Profiling... [1024/50176]	Loss: 1.7135
Profiling... [1536/50176]	Loss: 1.7174
Profiling... [2048/50176]	Loss: 1.8002
Profiling... [2560/50176]	Loss: 1.7974
Profiling... [3072/50176]	Loss: 1.7853
Profiling... [3584/50176]	Loss: 1.7906
Profiling... [4096/50176]	Loss: 1.9092
Profiling... [4608/50176]	Loss: 1.7294
Profiling... [5120/50176]	Loss: 1.7384
Profiling... [5632/50176]	Loss: 1.8740
Profiling... [6144/50176]	Loss: 1.7202
Profiling... [6656/50176]	Loss: 1.7572
Profile done
epoch 1 train time consumed: 17.82s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4414
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.77277707411226,
                        "time": 13.044357686999774,
                        "accuracy": 0.44140625,
                        "total_cost": 5171568.357323805
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9333
Profiling... [1024/50176]	Loss: 1.7386
Profiling... [1536/50176]	Loss: 1.8317
Profiling... [2048/50176]	Loss: 1.8103
Profiling... [2560/50176]	Loss: 2.0405
Profiling... [3072/50176]	Loss: 1.9439
Profiling... [3584/50176]	Loss: 1.8513
Profiling... [4096/50176]	Loss: 1.7528
Profiling... [4608/50176]	Loss: 1.9183
Profiling... [5120/50176]	Loss: 1.8035
Profiling... [5632/50176]	Loss: 1.6826
Profiling... [6144/50176]	Loss: 1.7804
Profiling... [6656/50176]	Loss: 1.7389
Profile done
epoch 1 train time consumed: 6.62s
Validation Epoch: 7, Average loss: 0.0061, Accuracy: 0.3002
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.73898548010627,
                        "time": 4.5223962319996645,
                        "accuracy": 0.3001953125,
                        "total_cost": 2636348.0962080024
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8928
Profiling... [1024/50176]	Loss: 1.8278
Profiling... [1536/50176]	Loss: 1.8280
Profiling... [2048/50176]	Loss: 1.7849
Profiling... [2560/50176]	Loss: 1.8933
Profiling... [3072/50176]	Loss: 1.8271
Profiling... [3584/50176]	Loss: 1.7394
Profiling... [4096/50176]	Loss: 1.7260
Profiling... [4608/50176]	Loss: 1.7161
Profiling... [5120/50176]	Loss: 1.8112
Profiling... [5632/50176]	Loss: 1.8173
Profiling... [6144/50176]	Loss: 1.9392
Profiling... [6656/50176]	Loss: 1.8054
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 7, Average loss: 0.0059, Accuracy: 0.3388
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.78962733211031,
                        "time": 4.635504006999781,
                        "accuracy": 0.33876953125,
                        "total_cost": 2394587.252967313
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8624
Profiling... [1024/50176]	Loss: 1.7119
Profiling... [1536/50176]	Loss: 1.8341
Profiling... [2048/50176]	Loss: 1.6854
Profiling... [2560/50176]	Loss: 1.9228
Profiling... [3072/50176]	Loss: 1.7864
Profiling... [3584/50176]	Loss: 1.6906
Profiling... [4096/50176]	Loss: 1.7607
Profiling... [4608/50176]	Loss: 1.7501
Profiling... [5120/50176]	Loss: 1.8591
Profiling... [5632/50176]	Loss: 1.8247
Profiling... [6144/50176]	Loss: 1.8563
Profiling... [6656/50176]	Loss: 1.8066
Profile done
epoch 1 train time consumed: 7.61s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.2436
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.80166152972124,
                        "time": 5.223200112000086,
                        "accuracy": 0.2435546875,
                        "total_cost": 3752997.0331612485
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.8207
Profiling... [1024/50176]	Loss: 1.8063
Profiling... [1536/50176]	Loss: 1.8278
Profiling... [2048/50176]	Loss: 1.7569
Profiling... [2560/50176]	Loss: 1.7354
Profiling... [3072/50176]	Loss: 1.9425
Profiling... [3584/50176]	Loss: 1.9170
Profiling... [4096/50176]	Loss: 1.5964
Profiling... [4608/50176]	Loss: 1.7407
Profiling... [5120/50176]	Loss: 1.9203
Profiling... [5632/50176]	Loss: 1.9369
Profiling... [6144/50176]	Loss: 1.7931
Profiling... [6656/50176]	Loss: 1.8325
Profile done
epoch 1 train time consumed: 17.77s
Validation Epoch: 7, Average loss: 0.0054, Accuracy: 0.3433
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.71348127562452,
                        "time": 13.08330030399975,
                        "accuracy": 0.34326171875,
                        "total_cost": 6670063.768070427
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8115
Profiling... [1024/50176]	Loss: 1.8201
Profiling... [1536/50176]	Loss: 1.8803
Profiling... [2048/50176]	Loss: 1.8722
Profiling... [2560/50176]	Loss: 1.8919
Profiling... [3072/50176]	Loss: 1.9527
Profiling... [3584/50176]	Loss: 1.7386
Profiling... [4096/50176]	Loss: 1.8872
Profiling... [4608/50176]	Loss: 1.7000
Profiling... [5120/50176]	Loss: 1.8331
Profiling... [5632/50176]	Loss: 1.7431
Profiling... [6144/50176]	Loss: 1.7019
Profiling... [6656/50176]	Loss: 1.7204
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 7, Average loss: 0.0056, Accuracy: 0.3326
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.68210383056287,
                        "time": 4.525718767999933,
                        "accuracy": 0.3326171875,
                        "total_cost": 2381118.036481468
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9600
Profiling... [1024/50176]	Loss: 1.8497
Profiling... [1536/50176]	Loss: 1.9167
Profiling... [2048/50176]	Loss: 1.8165
Profiling... [2560/50176]	Loss: 1.7947
Profiling... [3072/50176]	Loss: 1.8508
Profiling... [3584/50176]	Loss: 1.8221
Profiling... [4096/50176]	Loss: 1.8673
Profiling... [4608/50176]	Loss: 1.8764
Profiling... [5120/50176]	Loss: 1.8965
Profiling... [5632/50176]	Loss: 1.7307
Profiling... [6144/50176]	Loss: 1.8731
Profiling... [6656/50176]	Loss: 1.6554
Profile done
epoch 1 train time consumed: 6.93s
Validation Epoch: 7, Average loss: 0.0190, Accuracy: 0.0820
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.72950256122024,
                        "time": 4.63750565600003,
                        "accuracy": 0.08203125,
                        "total_cost": 9893345.399466733
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8506
Profiling... [1024/50176]	Loss: 1.9664
Profiling... [1536/50176]	Loss: 1.8605
Profiling... [2048/50176]	Loss: 1.7971
Profiling... [2560/50176]	Loss: 1.8883
Profiling... [3072/50176]	Loss: 1.9007
Profiling... [3584/50176]	Loss: 1.9586
Profiling... [4096/50176]	Loss: 1.8413
Profiling... [4608/50176]	Loss: 1.7635
Profiling... [5120/50176]	Loss: 1.6738
Profiling... [5632/50176]	Loss: 1.7882
Profiling... [6144/50176]	Loss: 1.9470
Profiling... [6656/50176]	Loss: 1.8628
Profile done
epoch 1 train time consumed: 7.64s
Validation Epoch: 7, Average loss: 0.0060, Accuracy: 0.3163
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.74081348755884,
                        "time": 5.2669212299997525,
                        "accuracy": 0.31630859375,
                        "total_cost": 2913961.9772027037
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9239
Profiling... [1024/50176]	Loss: 1.7525
Profiling... [1536/50176]	Loss: 1.7465
Profiling... [2048/50176]	Loss: 1.9008
Profiling... [2560/50176]	Loss: 1.8446
Profiling... [3072/50176]	Loss: 1.7456
Profiling... [3584/50176]	Loss: 1.8244
Profiling... [4096/50176]	Loss: 1.8158
Profiling... [4608/50176]	Loss: 1.7519
Profiling... [5120/50176]	Loss: 1.8279
Profiling... [5632/50176]	Loss: 1.7543
Profiling... [6144/50176]	Loss: 1.7269
Profiling... [6656/50176]	Loss: 1.7598
Profile done
epoch 1 train time consumed: 17.67s
Validation Epoch: 7, Average loss: 0.0053, Accuracy: 0.3551
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.65489213207795,
                        "time": 13.016230594000262,
                        "accuracy": 0.355078125,
                        "total_cost": 6415039.940717401
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8166
Profiling... [1024/50176]	Loss: 1.8008
Profiling... [1536/50176]	Loss: 1.8206
Profiling... [2048/50176]	Loss: 1.9377
Profiling... [2560/50176]	Loss: 1.9067
Profiling... [3072/50176]	Loss: 1.8864
Profiling... [3584/50176]	Loss: 1.9033
Profiling... [4096/50176]	Loss: 1.7421
Profiling... [4608/50176]	Loss: 1.7312
Profiling... [5120/50176]	Loss: 1.8724
Profiling... [5632/50176]	Loss: 1.7926
Profiling... [6144/50176]	Loss: 1.6614
Profiling... [6656/50176]	Loss: 1.8368
Profile done
epoch 1 train time consumed: 6.70s
Validation Epoch: 7, Average loss: 0.0044, Accuracy: 0.4135
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.62175014824916,
                        "time": 4.520579040000484,
                        "accuracy": 0.4134765625,
                        "total_cost": 1913291.8374305312
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9554
Profiling... [1024/50176]	Loss: 1.7404
Profiling... [1536/50176]	Loss: 1.9256
Profiling... [2048/50176]	Loss: 1.8848
Profiling... [2560/50176]	Loss: 1.7750
Profiling... [3072/50176]	Loss: 1.7884
Profiling... [3584/50176]	Loss: 1.8729
Profiling... [4096/50176]	Loss: 1.7404
Profiling... [4608/50176]	Loss: 1.7824
Profiling... [5120/50176]	Loss: 1.8432
Profiling... [5632/50176]	Loss: 1.7568
Profiling... [6144/50176]	Loss: 1.8658
Profiling... [6656/50176]	Loss: 1.7440
Profile done
epoch 1 train time consumed: 6.89s
Validation Epoch: 7, Average loss: 0.0062, Accuracy: 0.3123
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.66888307703296,
                        "time": 4.637761468999997,
                        "accuracy": 0.3123046875,
                        "total_cost": 2598770.654298935
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8280
Profiling... [1024/50176]	Loss: 1.7585
Profiling... [1536/50176]	Loss: 1.7808
Profiling... [2048/50176]	Loss: 1.7898
Profiling... [2560/50176]	Loss: 1.9170
Profiling... [3072/50176]	Loss: 1.7813
Profiling... [3584/50176]	Loss: 1.8055
Profiling... [4096/50176]	Loss: 1.7881
Profiling... [4608/50176]	Loss: 1.8600
Profiling... [5120/50176]	Loss: 1.7470
Profiling... [5632/50176]	Loss: 1.7972
Profiling... [6144/50176]	Loss: 1.7357
Profiling... [6656/50176]	Loss: 1.7912
Profile done
epoch 1 train time consumed: 7.62s
Validation Epoch: 7, Average loss: 0.0055, Accuracy: 0.3204
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.68162193659968,
                        "time": 5.228105444999528,
                        "accuracy": 0.32041015625,
                        "total_cost": 2855460.2125690808
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.8705
Profiling... [1024/50176]	Loss: 1.9317
Profiling... [1536/50176]	Loss: 1.8079
Profiling... [2048/50176]	Loss: 1.9299
Profiling... [2560/50176]	Loss: 1.8423
Profiling... [3072/50176]	Loss: 1.8409
Profiling... [3584/50176]	Loss: 1.8162
Profiling... [4096/50176]	Loss: 1.7380
Profiling... [4608/50176]	Loss: 1.8736
Profiling... [5120/50176]	Loss: 1.8828
Profiling... [5632/50176]	Loss: 1.8743
Profiling... [6144/50176]	Loss: 1.7214
Profiling... [6656/50176]	Loss: 1.8196
Profile done
epoch 1 train time consumed: 17.71s
Validation Epoch: 7, Average loss: 0.0052, Accuracy: 0.3563
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.59745809626949,
                        "time": 13.002608050000163,
                        "accuracy": 0.35625,
                        "total_cost": 6387246.059649202
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8773
Profiling... [2048/50176]	Loss: 1.9136
Profiling... [3072/50176]	Loss: 1.7879
Profiling... [4096/50176]	Loss: 1.8350
Profiling... [5120/50176]	Loss: 1.8770
Profiling... [6144/50176]	Loss: 1.6927
Profiling... [7168/50176]	Loss: 1.8046
Profiling... [8192/50176]	Loss: 1.8094
Profiling... [9216/50176]	Loss: 1.7107
Profiling... [10240/50176]	Loss: 1.6834
Profiling... [11264/50176]	Loss: 1.7559
Profiling... [12288/50176]	Loss: 1.6967
Profiling... [13312/50176]	Loss: 1.7286
Profile done
epoch 1 train time consumed: 12.76s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4881
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.62350897641251,
                        "time": 8.823836841999764,
                        "accuracy": 0.4880859375,
                        "total_cost": 3163728.6156189633
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8832
Profiling... [2048/50176]	Loss: 1.7876
Profiling... [3072/50176]	Loss: 1.8761
Profiling... [4096/50176]	Loss: 1.8245
Profiling... [5120/50176]	Loss: 1.7677
Profiling... [6144/50176]	Loss: 1.8582
Profiling... [7168/50176]	Loss: 1.7002
Profiling... [8192/50176]	Loss: 1.7104
Profiling... [9216/50176]	Loss: 1.7277
Profiling... [10240/50176]	Loss: 1.7373
Profiling... [11264/50176]	Loss: 1.7161
Profiling... [12288/50176]	Loss: 1.6882
Profiling... [13312/50176]	Loss: 1.7634
Profile done
epoch 1 train time consumed: 13.11s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4927
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.6965369964485,
                        "time": 9.076196013999834,
                        "accuracy": 0.49267578125,
                        "total_cost": 3223893.6089371066
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8499
Profiling... [2048/50176]	Loss: 1.7789
Profiling... [3072/50176]	Loss: 1.8493
Profiling... [4096/50176]	Loss: 1.7619
Profiling... [5120/50176]	Loss: 1.9202
Profiling... [6144/50176]	Loss: 1.7561
Profiling... [7168/50176]	Loss: 1.7695
Profiling... [8192/50176]	Loss: 1.7687
Profiling... [9216/50176]	Loss: 1.8912
Profiling... [10240/50176]	Loss: 1.7366
Profiling... [11264/50176]	Loss: 1.9331
Profiling... [12288/50176]	Loss: 1.7612
Profiling... [13312/50176]	Loss: 1.7148
Profile done
epoch 1 train time consumed: 14.70s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4903
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.69975271767021,
                        "time": 10.373579811000127,
                        "accuracy": 0.49033203125,
                        "total_cost": 3702341.1713428055
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7733
Profiling... [2048/50176]	Loss: 1.8010
Profiling... [3072/50176]	Loss: 1.7735
Profiling... [4096/50176]	Loss: 1.7985
Profiling... [5120/50176]	Loss: 1.9092
Profiling... [6144/50176]	Loss: 1.7849
Profiling... [7168/50176]	Loss: 1.7268
Profiling... [8192/50176]	Loss: 1.7665
Profiling... [9216/50176]	Loss: 1.7672
Profiling... [10240/50176]	Loss: 1.7272
Profiling... [11264/50176]	Loss: 1.6718
Profiling... [12288/50176]	Loss: 1.7925
Profiling... [13312/50176]	Loss: 1.7187
Profile done
epoch 1 train time consumed: 37.40s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4895
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.4961433897705,
                        "time": 27.650156259999676,
                        "accuracy": 0.489453125,
                        "total_cost": 9886089.389050163
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8564
Profiling... [2048/50176]	Loss: 1.9002
Profiling... [3072/50176]	Loss: 1.7495
Profiling... [4096/50176]	Loss: 1.7979
Profiling... [5120/50176]	Loss: 1.7847
Profiling... [6144/50176]	Loss: 1.7469
Profiling... [7168/50176]	Loss: 1.7947
Profiling... [8192/50176]	Loss: 1.7365
Profiling... [9216/50176]	Loss: 1.7681
Profiling... [10240/50176]	Loss: 1.7526
Profiling... [11264/50176]	Loss: 1.7205
Profiling... [12288/50176]	Loss: 1.7780
Profiling... [13312/50176]	Loss: 1.6534
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4904
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.54153967924015,
                        "time": 8.806343301000197,
                        "accuracy": 0.4904296875,
                        "total_cost": 3142367.0241721133
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7433
Profiling... [2048/50176]	Loss: 1.7961
Profiling... [3072/50176]	Loss: 1.8217
Profiling... [4096/50176]	Loss: 1.6793
Profiling... [5120/50176]	Loss: 1.7187
Profiling... [6144/50176]	Loss: 1.7572
Profiling... [7168/50176]	Loss: 1.7369
Profiling... [8192/50176]	Loss: 1.7678
Profiling... [9216/50176]	Loss: 1.7970
Profiling... [10240/50176]	Loss: 1.7589
Profiling... [11264/50176]	Loss: 1.7283
Profiling... [12288/50176]	Loss: 1.8231
Profiling... [13312/50176]	Loss: 1.7725
Profile done
epoch 1 train time consumed: 13.13s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4895
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.61665248183955,
                        "time": 9.104261506000512,
                        "accuracy": 0.489453125,
                        "total_cost": 3255154.951866105
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8603
Profiling... [2048/50176]	Loss: 1.7768
Profiling... [3072/50176]	Loss: 1.8287
Profiling... [4096/50176]	Loss: 1.8869
Profiling... [5120/50176]	Loss: 1.7767
Profiling... [6144/50176]	Loss: 1.7544
Profiling... [7168/50176]	Loss: 1.7744
Profiling... [8192/50176]	Loss: 1.7617
Profiling... [9216/50176]	Loss: 1.7413
Profiling... [10240/50176]	Loss: 1.7274
Profiling... [11264/50176]	Loss: 1.7281
Profiling... [12288/50176]	Loss: 1.7387
Profiling... [13312/50176]	Loss: 1.7750
Profile done
epoch 1 train time consumed: 14.68s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4887
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.62132411031476,
                        "time": 10.351836152999567,
                        "accuracy": 0.488671875,
                        "total_cost": 3707132.371337975
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8897
Profiling... [2048/50176]	Loss: 1.8242
Profiling... [3072/50176]	Loss: 1.7923
Profiling... [4096/50176]	Loss: 1.7915
Profiling... [5120/50176]	Loss: 1.8381
Profiling... [6144/50176]	Loss: 1.6857
Profiling... [7168/50176]	Loss: 1.7551
Profiling... [8192/50176]	Loss: 1.7032
Profiling... [9216/50176]	Loss: 1.7744
Profiling... [10240/50176]	Loss: 1.8326
Profiling... [11264/50176]	Loss: 1.7741
Profiling... [12288/50176]	Loss: 1.7818
Profiling... [13312/50176]	Loss: 1.6640
Profile done
epoch 1 train time consumed: 37.50s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4864
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.42483606869133,
                        "time": 27.65876881299937,
                        "accuracy": 0.48642578125,
                        "total_cost": 9950715.461332034
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9426
Profiling... [2048/50176]	Loss: 1.7896
Profiling... [3072/50176]	Loss: 1.8562
Profiling... [4096/50176]	Loss: 1.9294
Profiling... [5120/50176]	Loss: 1.8883
Profiling... [6144/50176]	Loss: 1.7531
Profiling... [7168/50176]	Loss: 1.7413
Profiling... [8192/50176]	Loss: 1.7599
Profiling... [9216/50176]	Loss: 1.6381
Profiling... [10240/50176]	Loss: 1.7369
Profiling... [11264/50176]	Loss: 1.7680
Profiling... [12288/50176]	Loss: 1.6962
Profiling... [13312/50176]	Loss: 1.7189
Profile done
epoch 1 train time consumed: 12.79s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4869
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.46676051499074,
                        "time": 8.821389992999684,
                        "accuracy": 0.4869140625,
                        "total_cost": 3170463.471210476
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7641
Profiling... [2048/50176]	Loss: 1.8068
Profiling... [3072/50176]	Loss: 1.7939
Profiling... [4096/50176]	Loss: 1.7670
Profiling... [5120/50176]	Loss: 1.8578
Profiling... [6144/50176]	Loss: 1.8100
Profiling... [7168/50176]	Loss: 1.7937
Profiling... [8192/50176]	Loss: 1.7721
Profiling... [9216/50176]	Loss: 1.8501
Profiling... [10240/50176]	Loss: 1.7532
Profiling... [11264/50176]	Loss: 1.7169
Profiling... [12288/50176]	Loss: 1.7857
Profiling... [13312/50176]	Loss: 1.8185
Profile done
epoch 1 train time consumed: 13.11s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4904
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.54072289959845,
                        "time": 9.101007932000357,
                        "accuracy": 0.4904296875,
                        "total_cost": 3247512.1891964637
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8320
Profiling... [2048/50176]	Loss: 1.9241
Profiling... [3072/50176]	Loss: 1.8257
Profiling... [4096/50176]	Loss: 1.7266
Profiling... [5120/50176]	Loss: 1.7905
Profiling... [6144/50176]	Loss: 1.8227
Profiling... [7168/50176]	Loss: 1.6999
Profiling... [8192/50176]	Loss: 1.7370
Profiling... [9216/50176]	Loss: 1.7448
Profiling... [10240/50176]	Loss: 1.7770
Profiling... [11264/50176]	Loss: 1.7499
Profiling... [12288/50176]	Loss: 1.6681
Profiling... [13312/50176]	Loss: 1.7140
Profile done
epoch 1 train time consumed: 14.67s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4926
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.54486046977301,
                        "time": 10.368748771000355,
                        "accuracy": 0.492578125,
                        "total_cost": 3683742.62443153
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9033
Profiling... [2048/50176]	Loss: 1.7903
Profiling... [3072/50176]	Loss: 1.8866
Profiling... [4096/50176]	Loss: 1.7836
Profiling... [5120/50176]	Loss: 1.8501
Profiling... [6144/50176]	Loss: 1.6859
Profiling... [7168/50176]	Loss: 1.7760
Profiling... [8192/50176]	Loss: 1.6873
Profiling... [9216/50176]	Loss: 1.7490
Profiling... [10240/50176]	Loss: 1.7942
Profiling... [11264/50176]	Loss: 1.7103
Profiling... [12288/50176]	Loss: 1.6847
Profiling... [13312/50176]	Loss: 1.8988
Profile done
epoch 1 train time consumed: 37.42s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4860
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.35302809502373,
                        "time": 27.65200140700017,
                        "accuracy": 0.48603515625,
                        "total_cost": 9956276.174672354
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8868
Profiling... [2048/50176]	Loss: 1.7627
Profiling... [3072/50176]	Loss: 1.7473
Profiling... [4096/50176]	Loss: 1.7059
Profiling... [5120/50176]	Loss: 1.7238
Profiling... [6144/50176]	Loss: 1.8056
Profiling... [7168/50176]	Loss: 1.7445
Profiling... [8192/50176]	Loss: 1.6958
Profiling... [9216/50176]	Loss: 1.6834
Profiling... [10240/50176]	Loss: 1.6222
Profiling... [11264/50176]	Loss: 1.6775
Profiling... [12288/50176]	Loss: 1.6652
Profiling... [13312/50176]	Loss: 1.7984
Profile done
epoch 1 train time consumed: 12.79s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4416
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.39757716491154,
                        "time": 8.80945889099985,
                        "accuracy": 0.4416015625,
                        "total_cost": 3491054.916557216
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8681
Profiling... [2048/50176]	Loss: 1.7392
Profiling... [3072/50176]	Loss: 1.7415
Profiling... [4096/50176]	Loss: 1.7211
Profiling... [5120/50176]	Loss: 1.7531
Profiling... [6144/50176]	Loss: 1.7793
Profiling... [7168/50176]	Loss: 1.6841
Profiling... [8192/50176]	Loss: 1.7479
Profiling... [9216/50176]	Loss: 1.6732
Profiling... [10240/50176]	Loss: 1.6660
Profiling... [11264/50176]	Loss: 1.6219
Profiling... [12288/50176]	Loss: 1.6974
Profiling... [13312/50176]	Loss: 1.6504
Profile done
epoch 1 train time consumed: 13.08s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4539
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.46896900060526,
                        "time": 9.105248556000333,
                        "accuracy": 0.45390625,
                        "total_cost": 3510457.274602538
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8875
Profiling... [2048/50176]	Loss: 1.7951
Profiling... [3072/50176]	Loss: 1.7240
Profiling... [4096/50176]	Loss: 1.7557
Profiling... [5120/50176]	Loss: 1.7984
Profiling... [6144/50176]	Loss: 1.7719
Profiling... [7168/50176]	Loss: 1.7346
Profiling... [8192/50176]	Loss: 1.7145
Profiling... [9216/50176]	Loss: 1.6524
Profiling... [10240/50176]	Loss: 1.6498
Profiling... [11264/50176]	Loss: 1.7315
Profiling... [12288/50176]	Loss: 1.7673
Profiling... [13312/50176]	Loss: 1.7405
Profile done
epoch 1 train time consumed: 14.72s
Validation Epoch: 7, Average loss: 0.0023, Accuracy: 0.4083
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.47296958287622,
                        "time": 10.37251593000019,
                        "accuracy": 0.40830078125,
                        "total_cost": 4445718.379947463
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8208
Profiling... [2048/50176]	Loss: 1.7602
Profiling... [3072/50176]	Loss: 1.7452
Profiling... [4096/50176]	Loss: 1.7798
Profiling... [5120/50176]	Loss: 1.6884
Profiling... [6144/50176]	Loss: 1.7910
Profiling... [7168/50176]	Loss: 1.7393
Profiling... [8192/50176]	Loss: 1.7145
Profiling... [9216/50176]	Loss: 1.6865
Profiling... [10240/50176]	Loss: 1.6894
Profiling... [11264/50176]	Loss: 1.6947
Profiling... [12288/50176]	Loss: 1.6511
Profiling... [13312/50176]	Loss: 1.6875
Profile done
epoch 1 train time consumed: 38.13s
Validation Epoch: 7, Average loss: 0.0024, Accuracy: 0.3941
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.2778054121561,
                        "time": 28.254087377999895,
                        "accuracy": 0.394140625,
                        "total_cost": 12544926.804106988
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8595
Profiling... [2048/50176]	Loss: 1.8008
Profiling... [3072/50176]	Loss: 1.8218
Profiling... [4096/50176]	Loss: 1.6692
Profiling... [5120/50176]	Loss: 1.6445
Profiling... [6144/50176]	Loss: 1.7005
Profiling... [7168/50176]	Loss: 1.7690
Profiling... [8192/50176]	Loss: 1.7468
Profiling... [9216/50176]	Loss: 1.6779
Profiling... [10240/50176]	Loss: 1.6618
Profiling... [11264/50176]	Loss: 1.7986
Profiling... [12288/50176]	Loss: 1.7007
Profiling... [13312/50176]	Loss: 1.6694
Profile done
epoch 1 train time consumed: 12.89s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4684
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.3185317123866,
                        "time": 8.821808145999967,
                        "accuracy": 0.468359375,
                        "total_cost": 3296221.892750613
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7916
Profiling... [2048/50176]	Loss: 1.8458
Profiling... [3072/50176]	Loss: 1.7441
Profiling... [4096/50176]	Loss: 1.6565
Profiling... [5120/50176]	Loss: 1.7532
Profiling... [6144/50176]	Loss: 1.8475
Profiling... [7168/50176]	Loss: 1.7648
Profiling... [8192/50176]	Loss: 1.6867
Profiling... [9216/50176]	Loss: 1.6975
Profiling... [10240/50176]	Loss: 1.7950
Profiling... [11264/50176]	Loss: 1.6910
Profiling... [12288/50176]	Loss: 1.7213
Profiling... [13312/50176]	Loss: 1.6876
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4441
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.38714171234845,
                        "time": 9.092592550000518,
                        "accuracy": 0.444140625,
                        "total_cost": 3582657.3987688934
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8454
Profiling... [2048/50176]	Loss: 1.8985
Profiling... [3072/50176]	Loss: 1.7478
Profiling... [4096/50176]	Loss: 1.6978
Profiling... [5120/50176]	Loss: 1.7128
Profiling... [6144/50176]	Loss: 1.6762
Profiling... [7168/50176]	Loss: 1.6494
Profiling... [8192/50176]	Loss: 1.7458
Profiling... [9216/50176]	Loss: 1.7716
Profiling... [10240/50176]	Loss: 1.7221
Profiling... [11264/50176]	Loss: 1.7335
Profiling... [12288/50176]	Loss: 1.6829
Profiling... [13312/50176]	Loss: 1.7109
Profile done
epoch 1 train time consumed: 14.68s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4684
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.39303152591324,
                        "time": 10.364223186000345,
                        "accuracy": 0.468359375,
                        "total_cost": 3872537.1036932063
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8643
Profiling... [2048/50176]	Loss: 1.8410
Profiling... [3072/50176]	Loss: 1.6985
Profiling... [4096/50176]	Loss: 1.7164
Profiling... [5120/50176]	Loss: 1.7746
Profiling... [6144/50176]	Loss: 1.7408
Profiling... [7168/50176]	Loss: 1.7156
Profiling... [8192/50176]	Loss: 1.8265
Profiling... [9216/50176]	Loss: 1.7307
Profiling... [10240/50176]	Loss: 1.7463
Profiling... [11264/50176]	Loss: 1.6445
Profiling... [12288/50176]	Loss: 1.7342
Profiling... [13312/50176]	Loss: 1.7234
Profile done
epoch 1 train time consumed: 37.48s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4604
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.20773884673785,
                        "time": 27.904826633000084,
                        "accuracy": 0.4603515625,
                        "total_cost": 10607859.424339447
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9183
Profiling... [2048/50176]	Loss: 1.8265
Profiling... [3072/50176]	Loss: 1.7843
Profiling... [4096/50176]	Loss: 1.7273
Profiling... [5120/50176]	Loss: 1.7687
Profiling... [6144/50176]	Loss: 1.8101
Profiling... [7168/50176]	Loss: 1.7448
Profiling... [8192/50176]	Loss: 1.7916
Profiling... [9216/50176]	Loss: 1.7686
Profiling... [10240/50176]	Loss: 1.7068
Profiling... [11264/50176]	Loss: 1.7472
Profiling... [12288/50176]	Loss: 1.6779
Profiling... [13312/50176]	Loss: 1.7151
Profile done
epoch 1 train time consumed: 12.79s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4493
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.24934002405477,
                        "time": 8.833368497000265,
                        "accuracy": 0.44931640625,
                        "total_cost": 3440425.2003096007
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8657
Profiling... [2048/50176]	Loss: 1.7364
Profiling... [3072/50176]	Loss: 1.7498
Profiling... [4096/50176]	Loss: 1.7161
Profiling... [5120/50176]	Loss: 1.8355
Profiling... [6144/50176]	Loss: 1.7272
Profiling... [7168/50176]	Loss: 1.7388
Profiling... [8192/50176]	Loss: 1.6654
Profiling... [9216/50176]	Loss: 1.6913
Profiling... [10240/50176]	Loss: 1.6921
Profiling... [11264/50176]	Loss: 1.6515
Profiling... [12288/50176]	Loss: 1.6795
Profiling... [13312/50176]	Loss: 1.6898
Profile done
epoch 1 train time consumed: 13.12s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4503
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.31910655275061,
                        "time": 9.108273316000123,
                        "accuracy": 0.45029296875,
                        "total_cost": 3539801.7311368943
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7858
Profiling... [2048/50176]	Loss: 1.7646
Profiling... [3072/50176]	Loss: 1.7697
Profiling... [4096/50176]	Loss: 1.7219
Profiling... [5120/50176]	Loss: 1.7233
Profiling... [6144/50176]	Loss: 1.7224
Profiling... [7168/50176]	Loss: 1.7372
Profiling... [8192/50176]	Loss: 1.7549
Profiling... [9216/50176]	Loss: 1.6755
Profiling... [10240/50176]	Loss: 1.7006
Profiling... [11264/50176]	Loss: 1.6802
Profiling... [12288/50176]	Loss: 1.7176
Profiling... [13312/50176]	Loss: 1.6704
Profile done
epoch 1 train time consumed: 14.81s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4598
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.32161548100582,
                        "time": 10.369852718999937,
                        "accuracy": 0.459765625,
                        "total_cost": 3947063.736713655
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9001
Profiling... [2048/50176]	Loss: 1.7407
Profiling... [3072/50176]	Loss: 1.6442
Profiling... [4096/50176]	Loss: 1.7945
Profiling... [5120/50176]	Loss: 1.7980
Profiling... [6144/50176]	Loss: 1.7839
Profiling... [7168/50176]	Loss: 1.7943
Profiling... [8192/50176]	Loss: 1.6924
Profiling... [9216/50176]	Loss: 1.8380
Profiling... [10240/50176]	Loss: 1.6853
Profiling... [11264/50176]	Loss: 1.6603
Profiling... [12288/50176]	Loss: 1.6613
Profiling... [13312/50176]	Loss: 1.6975
Profile done
epoch 1 train time consumed: 38.17s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4430
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.13236165400016,
                        "time": 28.39069577400005,
                        "accuracy": 0.44296875,
                        "total_cost": 11216077.342814835
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8783
Profiling... [2048/50176]	Loss: 1.8106
Profiling... [3072/50176]	Loss: 1.7605
Profiling... [4096/50176]	Loss: 1.6599
Profiling... [5120/50176]	Loss: 1.7632
Profiling... [6144/50176]	Loss: 1.7736
Profiling... [7168/50176]	Loss: 1.7592
Profiling... [8192/50176]	Loss: 1.8904
Profiling... [9216/50176]	Loss: 1.7465
Profiling... [10240/50176]	Loss: 1.7999
Profiling... [11264/50176]	Loss: 1.6861
Profiling... [12288/50176]	Loss: 1.7603
Profiling... [13312/50176]	Loss: 1.7174
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4068
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.17184623261772,
                        "time": 8.840860309000163,
                        "accuracy": 0.4068359375,
                        "total_cost": 3802885.663400934
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8399
Profiling... [2048/50176]	Loss: 1.8033
Profiling... [3072/50176]	Loss: 1.7891
Profiling... [4096/50176]	Loss: 1.7346
Profiling... [5120/50176]	Loss: 1.7286
Profiling... [6144/50176]	Loss: 1.7382
Profiling... [7168/50176]	Loss: 1.7949
Profiling... [8192/50176]	Loss: 1.7419
Profiling... [9216/50176]	Loss: 1.6906
Profiling... [10240/50176]	Loss: 1.7548
Profiling... [11264/50176]	Loss: 1.7490
Profiling... [12288/50176]	Loss: 1.7128
Profiling... [13312/50176]	Loss: 1.7489
Profile done
epoch 1 train time consumed: 13.11s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.2460
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.23868114928335,
                        "time": 9.09800174799966,
                        "accuracy": 0.24599609375,
                        "total_cost": 6472258.488453907
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7594
Profiling... [2048/50176]	Loss: 1.7089
Profiling... [3072/50176]	Loss: 1.8059
Profiling... [4096/50176]	Loss: 1.8300
Profiling... [5120/50176]	Loss: 1.7837
Profiling... [6144/50176]	Loss: 1.6951
Profiling... [7168/50176]	Loss: 1.7903
Profiling... [8192/50176]	Loss: 1.7018
Profiling... [9216/50176]	Loss: 1.7941
Profiling... [10240/50176]	Loss: 1.8254
Profiling... [11264/50176]	Loss: 1.7352
Profiling... [12288/50176]	Loss: 1.6976
Profiling... [13312/50176]	Loss: 1.7924
Profile done
epoch 1 train time consumed: 14.70s
Validation Epoch: 7, Average loss: 0.0051, Accuracy: 0.1910
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.24506165139509,
                        "time": 10.36425206399963,
                        "accuracy": 0.191015625,
                        "total_cost": 9495265.694625428
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7932
Profiling... [2048/50176]	Loss: 1.6825
Profiling... [3072/50176]	Loss: 1.8101
Profiling... [4096/50176]	Loss: 1.8194
Profiling... [5120/50176]	Loss: 1.7937
Profiling... [6144/50176]	Loss: 1.7342
Profiling... [7168/50176]	Loss: 1.6769
Profiling... [8192/50176]	Loss: 1.7942
Profiling... [9216/50176]	Loss: 1.7776
Profiling... [10240/50176]	Loss: 1.7368
Profiling... [11264/50176]	Loss: 1.7554
Profiling... [12288/50176]	Loss: 1.7818
Profiling... [13312/50176]	Loss: 1.6866
Profile done
epoch 1 train time consumed: 38.07s
Validation Epoch: 7, Average loss: 0.0028, Accuracy: 0.3491
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.06607796111842,
                        "time": 28.271783035000226,
                        "accuracy": 0.34912109375,
                        "total_cost": 14171478.37726445
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7751
Profiling... [2048/50176]	Loss: 1.7888
Profiling... [3072/50176]	Loss: 1.8539
Profiling... [4096/50176]	Loss: 1.8132
Profiling... [5120/50176]	Loss: 1.8040
Profiling... [6144/50176]	Loss: 1.8440
Profiling... [7168/50176]	Loss: 1.7561
Profiling... [8192/50176]	Loss: 1.7224
Profiling... [9216/50176]	Loss: 1.7902
Profiling... [10240/50176]	Loss: 1.7086
Profiling... [11264/50176]	Loss: 1.7284
Profiling... [12288/50176]	Loss: 1.7641
Profiling... [13312/50176]	Loss: 1.6854
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 7, Average loss: 0.0037, Accuracy: 0.2718
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.10925575848377,
                        "time": 8.814037769000606,
                        "accuracy": 0.27177734375,
                        "total_cost": 5675442.214175021
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8257
Profiling... [2048/50176]	Loss: 1.8076
Profiling... [3072/50176]	Loss: 1.7597
Profiling... [4096/50176]	Loss: 1.7650
Profiling... [5120/50176]	Loss: 1.7661
Profiling... [6144/50176]	Loss: 1.8288
Profiling... [7168/50176]	Loss: 1.8312
Profiling... [8192/50176]	Loss: 1.8156
Profiling... [9216/50176]	Loss: 1.6663
Profiling... [10240/50176]	Loss: 1.7033
Profiling... [11264/50176]	Loss: 1.7680
Profiling... [12288/50176]	Loss: 1.7642
Profiling... [13312/50176]	Loss: 1.6752
Profile done
epoch 1 train time consumed: 13.23s
Validation Epoch: 7, Average loss: 0.0034, Accuracy: 0.2846
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.17430222520458,
                        "time": 9.112358363000567,
                        "accuracy": 0.2845703125,
                        "total_cost": 5603756.412661983
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8922
Profiling... [2048/50176]	Loss: 1.7887
Profiling... [3072/50176]	Loss: 1.8314
Profiling... [4096/50176]	Loss: 1.8479
Profiling... [5120/50176]	Loss: 1.7164
Profiling... [6144/50176]	Loss: 1.7696
Profiling... [7168/50176]	Loss: 1.7151
Profiling... [8192/50176]	Loss: 1.7862
Profiling... [9216/50176]	Loss: 1.6394
Profiling... [10240/50176]	Loss: 1.6823
Profiling... [11264/50176]	Loss: 1.7854
Profiling... [12288/50176]	Loss: 1.7796
Profiling... [13312/50176]	Loss: 1.7854
Profile done
epoch 1 train time consumed: 14.74s
Validation Epoch: 7, Average loss: 0.0055, Accuracy: 0.1813
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.17819874653051,
                        "time": 10.352036579000014,
                        "accuracy": 0.18134765625,
                        "total_cost": 9989687.425723225
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8494
Profiling... [2048/50176]	Loss: 1.7776
Profiling... [3072/50176]	Loss: 1.7468
Profiling... [4096/50176]	Loss: 1.7805
Profiling... [5120/50176]	Loss: 1.7503
Profiling... [6144/50176]	Loss: 1.8110
Profiling... [7168/50176]	Loss: 1.7904
Profiling... [8192/50176]	Loss: 1.7534
Profiling... [9216/50176]	Loss: 1.7187
Profiling... [10240/50176]	Loss: 1.7489
Profiling... [11264/50176]	Loss: 1.7888
Profiling... [12288/50176]	Loss: 1.7100
Profiling... [13312/50176]	Loss: 1.7716
Profile done
epoch 1 train time consumed: 37.37s
Validation Epoch: 7, Average loss: 0.0023, Accuracy: 0.3902
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.01498979702149,
                        "time": 27.574314952999885,
                        "accuracy": 0.390234375,
                        "total_cost": 12365658.75770165
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8384
Profiling... [2048/50176]	Loss: 1.7472
Profiling... [3072/50176]	Loss: 1.8196
Profiling... [4096/50176]	Loss: 1.8540
Profiling... [5120/50176]	Loss: 1.7114
Profiling... [6144/50176]	Loss: 1.6996
Profiling... [7168/50176]	Loss: 1.8441
Profiling... [8192/50176]	Loss: 1.7477
Profiling... [9216/50176]	Loss: 1.7752
Profiling... [10240/50176]	Loss: 1.7272
Profiling... [11264/50176]	Loss: 1.8260
Profiling... [12288/50176]	Loss: 1.7821
Profiling... [13312/50176]	Loss: 1.7130
Profile done
epoch 1 train time consumed: 12.96s
Validation Epoch: 7, Average loss: 0.0060, Accuracy: 0.1297
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.05493958521531,
                        "time": 8.810903505999704,
                        "accuracy": 0.1296875,
                        "total_cost": 11889411.959903214
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8483
Profiling... [2048/50176]	Loss: 1.8012
Profiling... [3072/50176]	Loss: 1.7850
Profiling... [4096/50176]	Loss: 1.6992
Profiling... [5120/50176]	Loss: 1.7398
Profiling... [6144/50176]	Loss: 1.8126
Profiling... [7168/50176]	Loss: 1.7747
Profiling... [8192/50176]	Loss: 1.7269
Profiling... [9216/50176]	Loss: 1.7641
Profiling... [10240/50176]	Loss: 1.7402
Profiling... [11264/50176]	Loss: 1.6939
Profiling... [12288/50176]	Loss: 1.7555
Profiling... [13312/50176]	Loss: 1.6679
Profile done
epoch 1 train time consumed: 13.21s
Validation Epoch: 7, Average loss: 0.0045, Accuracy: 0.2036
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.11643037800262,
                        "time": 9.12336010800027,
                        "accuracy": 0.20361328125,
                        "total_cost": 7841276.409370016
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8314
Profiling... [2048/50176]	Loss: 1.8061
Profiling... [3072/50176]	Loss: 1.7974
Profiling... [4096/50176]	Loss: 1.8312
Profiling... [5120/50176]	Loss: 1.7962
Profiling... [6144/50176]	Loss: 1.8515
Profiling... [7168/50176]	Loss: 1.7487
Profiling... [8192/50176]	Loss: 1.7786
Profiling... [9216/50176]	Loss: 1.7223
Profiling... [10240/50176]	Loss: 1.7547
Profiling... [11264/50176]	Loss: 1.6529
Profiling... [12288/50176]	Loss: 1.7816
Profiling... [13312/50176]	Loss: 1.7971
Profile done
epoch 1 train time consumed: 14.86s
Validation Epoch: 7, Average loss: 0.0035, Accuracy: 0.2576
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.12052615812752,
                        "time": 10.3730599670007,
                        "accuracy": 0.2576171875,
                        "total_cost": 7046445.587894334
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8767
Profiling... [2048/50176]	Loss: 1.8132
Profiling... [3072/50176]	Loss: 1.7480
Profiling... [4096/50176]	Loss: 1.7547
Profiling... [5120/50176]	Loss: 1.7065
Profiling... [6144/50176]	Loss: 1.8328
Profiling... [7168/50176]	Loss: 1.8143
Profiling... [8192/50176]	Loss: 1.7786
Profiling... [9216/50176]	Loss: 1.7244
Profiling... [10240/50176]	Loss: 1.7072
Profiling... [11264/50176]	Loss: 1.8057
Profiling... [12288/50176]	Loss: 1.7806
Profiling... [13312/50176]	Loss: 1.6365
Profile done
epoch 1 train time consumed: 37.90s
Validation Epoch: 7, Average loss: 0.0035, Accuracy: 0.2406
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.94895510866091,
                        "time": 28.029128253000636,
                        "accuracy": 0.240625,
                        "total_cost": 20384820.547636826
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.25 bs: 128 pl: 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[GPU_0] Set GPU power limit to 175W.
[GPU_0] Set GPU power limit to 175W.
Training Epoch: 7 [128/50048]	Loss: 1.7777
Training Epoch: 7 [256/50048]	Loss: 2.1015
Training Epoch: 7 [384/50048]	Loss: 2.2482
Training Epoch: 7 [512/50048]	Loss: 1.9919
Training Epoch: 7 [640/50048]	Loss: 1.8327
Training Epoch: 7 [768/50048]	Loss: 1.9115
Training Epoch: 7 [896/50048]	Loss: 1.8071
Training Epoch: 7 [1024/50048]	Loss: 1.9564
Training Epoch: 7 [1152/50048]	Loss: 2.0179
Training Epoch: 7 [1280/50048]	Loss: 2.0168
Training Epoch: 7 [1408/50048]	Loss: 2.2543
Training Epoch: 7 [1536/50048]	Loss: 1.8856
Training Epoch: 7 [1664/50048]	Loss: 1.9171
Training Epoch: 7 [1792/50048]	Loss: 1.9380
Training Epoch: 7 [1920/50048]	Loss: 2.0525
Training Epoch: 7 [2048/50048]	Loss: 1.9057
Training Epoch: 7 [2176/50048]	Loss: 1.8341
Training Epoch: 7 [2304/50048]	Loss: 1.9206
Training Epoch: 7 [2432/50048]	Loss: 1.8396
Training Epoch: 7 [2560/50048]	Loss: 1.8740
Training Epoch: 7 [2688/50048]	Loss: 1.7979
Training Epoch: 7 [2816/50048]	Loss: 1.9707
Training Epoch: 7 [2944/50048]	Loss: 1.7114
Training Epoch: 7 [3072/50048]	Loss: 1.8567
Training Epoch: 7 [3200/50048]	Loss: 2.0106
Training Epoch: 7 [3328/50048]	Loss: 1.8046
Training Epoch: 7 [3456/50048]	Loss: 1.9384
Training Epoch: 7 [3584/50048]	Loss: 1.8940
Training Epoch: 7 [3712/50048]	Loss: 1.8603
Training Epoch: 7 [3840/50048]	Loss: 1.7649
Training Epoch: 7 [3968/50048]	Loss: 1.8830
Training Epoch: 7 [4096/50048]	Loss: 1.5603
Training Epoch: 7 [4224/50048]	Loss: 1.6562
Training Epoch: 7 [4352/50048]	Loss: 1.8255
Training Epoch: 7 [4480/50048]	Loss: 1.7923
Training Epoch: 7 [4608/50048]	Loss: 1.7976
Training Epoch: 7 [4736/50048]	Loss: 1.7113
Training Epoch: 7 [4864/50048]	Loss: 1.6498
Training Epoch: 7 [4992/50048]	Loss: 1.7491
Training Epoch: 7 [5120/50048]	Loss: 1.8029
Training Epoch: 7 [5248/50048]	Loss: 1.9153
Training Epoch: 7 [5376/50048]	Loss: 1.7490
Training Epoch: 7 [5504/50048]	Loss: 1.6992
Training Epoch: 7 [5632/50048]	Loss: 1.7100
Training Epoch: 7 [5760/50048]	Loss: 1.5406
Training Epoch: 7 [5888/50048]	Loss: 1.6891
Training Epoch: 7 [6016/50048]	Loss: 1.6556
Training Epoch: 7 [6144/50048]	Loss: 1.7229
Training Epoch: 7 [6272/50048]	Loss: 1.7277
Training Epoch: 7 [6400/50048]	Loss: 1.7162
Training Epoch: 7 [6528/50048]	Loss: 1.8111
Training Epoch: 7 [6656/50048]	Loss: 2.0295
Training Epoch: 7 [6784/50048]	Loss: 1.4875
Training Epoch: 7 [6912/50048]	Loss: 1.5792
Training Epoch: 7 [7040/50048]	Loss: 1.7919
Training Epoch: 7 [7168/50048]	Loss: 1.8301
Training Epoch: 7 [7296/50048]	Loss: 1.7715
Training Epoch: 7 [7424/50048]	Loss: 1.8204
Training Epoch: 7 [7552/50048]	Loss: 1.8263
Training Epoch: 7 [7680/50048]	Loss: 1.8329
Training Epoch: 7 [7808/50048]	Loss: 1.8244
Training Epoch: 7 [7936/50048]	Loss: 1.7153
Training Epoch: 7 [8064/50048]	Loss: 1.9171
Training Epoch: 7 [8192/50048]	Loss: 1.5245
Training Epoch: 7 [8320/50048]	Loss: 1.8105
Training Epoch: 7 [8448/50048]	Loss: 1.9118
Training Epoch: 7 [8576/50048]	Loss: 1.8741
Training Epoch: 7 [8704/50048]	Loss: 1.9685
Training Epoch: 7 [8832/50048]	Loss: 1.7281
Training Epoch: 7 [8960/50048]	Loss: 1.8789
Training Epoch: 7 [9088/50048]	Loss: 1.7572
Training Epoch: 7 [9216/50048]	Loss: 1.9413
Training Epoch: 7 [9344/50048]	Loss: 1.9197
Training Epoch: 7 [9472/50048]	Loss: 1.7975
Training Epoch: 7 [9600/50048]	Loss: 1.5625
Training Epoch: 7 [9728/50048]	Loss: 1.9259
Training Epoch: 7 [9856/50048]	Loss: 1.8335
Training Epoch: 7 [9984/50048]	Loss: 1.7269
Training Epoch: 7 [10112/50048]	Loss: 1.7526
Training Epoch: 7 [10240/50048]	Loss: 1.9448
Training Epoch: 7 [10368/50048]	Loss: 1.8133
Training Epoch: 7 [10496/50048]	Loss: 1.7858
Training Epoch: 7 [10624/50048]	Loss: 1.7020
Training Epoch: 7 [10752/50048]	Loss: 2.1032
Training Epoch: 7 [10880/50048]	Loss: 1.7955
Training Epoch: 7 [11008/50048]	Loss: 1.6914
Training Epoch: 7 [11136/50048]	Loss: 1.6937
Training Epoch: 7 [11264/50048]	Loss: 1.9258
Training Epoch: 7 [11392/50048]	Loss: 1.6601
Training Epoch: 7 [11520/50048]	Loss: 1.9103
Training Epoch: 7 [11648/50048]	Loss: 1.7779
Training Epoch: 7 [11776/50048]	Loss: 1.8615
Training Epoch: 7 [11904/50048]	Loss: 1.9970
Training Epoch: 7 [12032/50048]	Loss: 1.7509
Training Epoch: 7 [12160/50048]	Loss: 1.7265
Training Epoch: 7 [12288/50048]	Loss: 1.6365
Training Epoch: 7 [12416/50048]	Loss: 1.6024
Training Epoch: 7 [12544/50048]	Loss: 1.7430
Training Epoch: 7 [12672/50048]	Loss: 1.7050
Training Epoch: 7 [12800/50048]	Loss: 1.6806
Training Epoch: 7 [12928/50048]	Loss: 1.8650
Training Epoch: 7 [13056/50048]	Loss: 1.8147
Training Epoch: 7 [13184/50048]	Loss: 1.6265
Training Epoch: 7 [13312/50048]	Loss: 1.8410
Training Epoch: 7 [13440/50048]	Loss: 1.9528
Training Epoch: 7 [13568/50048]	Loss: 1.5826
Training Epoch: 7 [13696/50048]	Loss: 1.9156
Training Epoch: 7 [13824/50048]	Loss: 1.7106
Training Epoch: 7 [13952/50048]	Loss: 1.9316
Training Epoch: 7 [14080/50048]	Loss: 1.7915
Training Epoch: 7 [14208/50048]	Loss: 1.7821
Training Epoch: 7 [14336/50048]	Loss: 1.8413
Training Epoch: 7 [14464/50048]	Loss: 1.8513
Training Epoch: 7 [14592/50048]	Loss: 1.8240
Training Epoch: 7 [14720/50048]	Loss: 1.6819
Training Epoch: 7 [14848/50048]	Loss: 1.8684
Training Epoch: 7 [14976/50048]	Loss: 1.9624
Training Epoch: 7 [15104/50048]	Loss: 1.8497
Training Epoch: 7 [15232/50048]	Loss: 1.8841
Training Epoch: 7 [15360/50048]	Loss: 1.7359
Training Epoch: 7 [15488/50048]	Loss: 1.8062
Training Epoch: 7 [15616/50048]	Loss: 1.9256
Training Epoch: 7 [15744/50048]	Loss: 1.5945
Training Epoch: 7 [15872/50048]	Loss: 1.6994
Training Epoch: 7 [16000/50048]	Loss: 1.8219
Training Epoch: 7 [16128/50048]	Loss: 1.8576
Training Epoch: 7 [16256/50048]	Loss: 1.7369
Training Epoch: 7 [16384/50048]	Loss: 1.7106
Training Epoch: 7 [16512/50048]	Loss: 1.4708
Training Epoch: 7 [16640/50048]	Loss: 1.8379
Training Epoch: 7 [16768/50048]	Loss: 1.7089
Training Epoch: 7 [16896/50048]	Loss: 1.9222
Training Epoch: 7 [17024/50048]	Loss: 1.6513
Training Epoch: 7 [17152/50048]	Loss: 1.8011
Training Epoch: 7 [17280/50048]	Loss: 1.6776
Training Epoch: 7 [17408/50048]	Loss: 1.8313
Training Epoch: 7 [17536/50048]	Loss: 1.9189
Training Epoch: 7 [17664/50048]	Loss: 1.7339
Training Epoch: 7 [17792/50048]	Loss: 1.8993
Training Epoch: 7 [17920/50048]	Loss: 1.8302
Training Epoch: 7 [18048/50048]	Loss: 1.8672
Training Epoch: 7 [18176/50048]	Loss: 1.6980
Training Epoch: 7 [18304/50048]	Loss: 1.5939
Training Epoch: 7 [18432/50048]	Loss: 1.7675
Training Epoch: 7 [18560/50048]	Loss: 1.7186
Training Epoch: 7 [18688/50048]	Loss: 1.7509
Training Epoch: 7 [18816/50048]	Loss: 1.9954
Training Epoch: 7 [18944/50048]	Loss: 1.8966
Training Epoch: 7 [19072/50048]	Loss: 2.0188
Training Epoch: 7 [19200/50048]	Loss: 1.5851
Training Epoch: 7 [19328/50048]	Loss: 1.8891
Training Epoch: 7 [19456/50048]	Loss: 1.7834
Training Epoch: 7 [19584/50048]	Loss: 1.5417
Training Epoch: 7 [19712/50048]	Loss: 1.7835
Training Epoch: 7 [19840/50048]	Loss: 1.5308
Training Epoch: 7 [19968/50048]	Loss: 1.8366
Training Epoch: 7 [20096/50048]	Loss: 1.7408
Training Epoch: 7 [20224/50048]	Loss: 1.8765
Training Epoch: 7 [20352/50048]	Loss: 1.9809
Training Epoch: 7 [20480/50048]	Loss: 2.0875
Training Epoch: 7 [20608/50048]	Loss: 1.8477
Training Epoch: 7 [20736/50048]	Loss: 1.8671
Training Epoch: 7 [20864/50048]	Loss: 1.6968
Training Epoch: 7 [20992/50048]	Loss: 1.7196
Training Epoch: 7 [21120/50048]	Loss: 1.7933
Training Epoch: 7 [21248/50048]	Loss: 1.6379
Training Epoch: 7 [21376/50048]	Loss: 1.8713
Training Epoch: 7 [21504/50048]	Loss: 1.9067
Training Epoch: 7 [21632/50048]	Loss: 1.7267
Training Epoch: 7 [21760/50048]	Loss: 1.7847
Training Epoch: 7 [21888/50048]	Loss: 1.7627
Training Epoch: 7 [22016/50048]	Loss: 2.0035
Training Epoch: 7 [22144/50048]	Loss: 1.7768
Training Epoch: 7 [22272/50048]	Loss: 1.7860
Training Epoch: 7 [22400/50048]	Loss: 1.9817
Training Epoch: 7 [22528/50048]	Loss: 1.7042
Training Epoch: 7 [22656/50048]	Loss: 1.6551
Training Epoch: 7 [22784/50048]	Loss: 1.4523
Training Epoch: 7 [22912/50048]	Loss: 1.6672
Training Epoch: 7 [23040/50048]	Loss: 1.5457
Training Epoch: 7 [23168/50048]	Loss: 1.6722
Training Epoch: 7 [23296/50048]	Loss: 1.6733
Training Epoch: 7 [23424/50048]	Loss: 1.7264
Training Epoch: 7 [23552/50048]	Loss: 1.3931
Training Epoch: 7 [23680/50048]	Loss: 1.8181
Training Epoch: 7 [23808/50048]	Loss: 1.7122
Training Epoch: 7 [23936/50048]	Loss: 1.7529
Training Epoch: 7 [24064/50048]	Loss: 1.8656
Training Epoch: 7 [24192/50048]	Loss: 1.5767
Training Epoch: 7 [24320/50048]	Loss: 1.6990
Training Epoch: 7 [24448/50048]	Loss: 1.4150
Training Epoch: 7 [24576/50048]	Loss: 1.9091
Training Epoch: 7 [24704/50048]	Loss: 1.7344
Training Epoch: 7 [24832/50048]	Loss: 1.9617
Training Epoch: 7 [24960/50048]	Loss: 1.8643
Training Epoch: 7 [25088/50048]	Loss: 1.7392
Training Epoch: 7 [25216/50048]	Loss: 1.7686
Training Epoch: 7 [25344/50048]	Loss: 1.8772
Training Epoch: 7 [25472/50048]	Loss: 1.7342
Training Epoch: 7 [25600/50048]	Loss: 1.5943
Training Epoch: 7 [25728/50048]	Loss: 1.4334
Training Epoch: 7 [25856/50048]	Loss: 1.8142
Training Epoch: 7 [25984/50048]	Loss: 1.8656
Training Epoch: 7 [26112/50048]	Loss: 1.8013
Training Epoch: 7 [26240/50048]	Loss: 1.8960
Training Epoch: 7 [26368/50048]	Loss: 1.8201
Training Epoch: 7 [26496/50048]	Loss: 1.9679
Training Epoch: 7 [26624/50048]	Loss: 1.7978
Training Epoch: 7 [26752/50048]	Loss: 1.8682
Training Epoch: 7 [26880/50048]	Loss: 1.6954
Training Epoch: 7 [27008/50048]	Loss: 1.5339
Training Epoch: 7 [27136/50048]	Loss: 1.7154
Training Epoch: 7 [27264/50048]	Loss: 1.8412
Training Epoch: 7 [27392/50048]	Loss: 1.8498
Training Epoch: 7 [27520/50048]	Loss: 1.6733
Training Epoch: 7 [27648/50048]	Loss: 1.8895
Training Epoch: 7 [27776/50048]	Loss: 1.8522
Training Epoch: 7 [27904/50048]	Loss: 1.6441
Training Epoch: 7 [28032/50048]	Loss: 1.6731
Training Epoch: 7 [28160/50048]	Loss: 1.8747
Training Epoch: 7 [28288/50048]	Loss: 1.7691
Training Epoch: 7 [28416/50048]	Loss: 1.4870
Training Epoch: 7 [28544/50048]	Loss: 1.9334
Training Epoch: 7 [28672/50048]	Loss: 2.0850
Training Epoch: 7 [28800/50048]	Loss: 1.8368
Training Epoch: 7 [28928/50048]	Loss: 1.4908
Training Epoch: 7 [29056/50048]	Loss: 1.9329
Training Epoch: 7 [29184/50048]	Loss: 1.9465
Training Epoch: 7 [29312/50048]	Loss: 1.6743
Training Epoch: 7 [29440/50048]	Loss: 1.7207
Training Epoch: 7 [29568/50048]	Loss: 1.8407
Training Epoch: 7 [29696/50048]	Loss: 1.6214
Training Epoch: 7 [29824/50048]	Loss: 1.7090
Training Epoch: 7 [29952/50048]	Loss: 1.8692
Training Epoch: 7 [30080/50048]	Loss: 1.7897
Training Epoch: 7 [30208/50048]	Loss: 2.1864
Training Epoch: 7 [30336/50048]	Loss: 1.9560
Training Epoch: 7 [30464/50048]	Loss: 1.6680
Training Epoch: 7 [30592/50048]	Loss: 1.6957
Training Epoch: 7 [30720/50048]	Loss: 1.8001
Training Epoch: 7 [30848/50048]	Loss: 1.7190
Training Epoch: 7 [30976/50048]	Loss: 1.7196
Training Epoch: 7 [31104/50048]	Loss: 1.5182
Training Epoch: 7 [31232/50048]	Loss: 1.6627
Training Epoch: 7 [31360/50048]	Loss: 1.6424
Training Epoch: 7 [31488/50048]	Loss: 1.8879
Training Epoch: 7 [31616/50048]	Loss: 1.5546
Training Epoch: 7 [31744/50048]	Loss: 1.7440
Training Epoch: 7 [31872/50048]	Loss: 1.5967
Training Epoch: 7 [32000/50048]	Loss: 1.8509
Training Epoch: 7 [32128/50048]	Loss: 1.7325
Training Epoch: 7 [32256/50048]	Loss: 1.9745
Training Epoch: 7 [32384/50048]	Loss: 1.7774
Training Epoch: 7 [32512/50048]	Loss: 1.8677
Training Epoch: 7 [32640/50048]	Loss: 1.7846
Training Epoch: 7 [32768/50048]	Loss: 1.8139
Training Epoch: 7 [32896/50048]	Loss: 1.7394
Training Epoch: 7 [33024/50048]	Loss: 1.6373
Training Epoch: 7 [33152/50048]	Loss: 1.7800
Training Epoch: 7 [33280/50048]	Loss: 1.7156
Training Epoch: 7 [33408/50048]	Loss: 1.8574
Training Epoch: 7 [33536/50048]	Loss: 1.5784
Training Epoch: 7 [33664/50048]	Loss: 1.8670
Training Epoch: 7 [33792/50048]	Loss: 1.5384
Training Epoch: 7 [33920/50048]	Loss: 1.6175
Training Epoch: 7 [34048/50048]	Loss: 1.7108
Training Epoch: 7 [34176/50048]	Loss: 1.7443
Training Epoch: 7 [34304/50048]	Loss: 1.7454
Training Epoch: 7 [34432/50048]	Loss: 1.7604
Training Epoch: 7 [34560/50048]	Loss: 1.5946
Training Epoch: 7 [34688/50048]	Loss: 1.9209
Training Epoch: 7 [34816/50048]	Loss: 1.7510
Training Epoch: 7 [34944/50048]	Loss: 1.5813
Training Epoch: 7 [35072/50048]	Loss: 1.6006
Training Epoch: 7 [35200/50048]	Loss: 1.7453
Training Epoch: 7 [35328/50048]	Loss: 1.7248
Training Epoch: 7 [35456/50048]	Loss: 2.1490
Training Epoch: 7 [35584/50048]	Loss: 1.9229
Training Epoch: 7 [35712/50048]	Loss: 1.7568
Training Epoch: 7 [35840/50048]	Loss: 1.8109
Training Epoch: 7 [35968/50048]	Loss: 1.4934
Training Epoch: 7 [36096/50048]	Loss: 1.7892
Training Epoch: 7 [36224/50048]	Loss: 1.6742
Training Epoch: 7 [36352/50048]	Loss: 1.6428
Training Epoch: 7 [36480/50048]	Loss: 1.5650
Training Epoch: 7 [36608/50048]	Loss: 1.4663
Training Epoch: 7 [36736/50048]	Loss: 1.7524
Training Epoch: 7 [36864/50048]	Loss: 1.8049
Training Epoch: 7 [36992/50048]	Loss: 1.8704
Training Epoch: 7 [37120/50048]	Loss: 1.7056
Training Epoch: 7 [37248/50048]	Loss: 1.7719
Training Epoch: 7 [37376/50048]	Loss: 1.7267
Training Epoch: 7 [37504/50048]	Loss: 1.9546
Training Epoch: 7 [37632/50048]	Loss: 1.9338
Training Epoch: 7 [37760/50048]	Loss: 1.6675
Training Epoch: 7 [37888/50048]	Loss: 1.6174
Training Epoch: 7 [38016/50048]	Loss: 1.5296
Training Epoch: 7 [38144/50048]	Loss: 1.9028
Training Epoch: 7 [38272/50048]	Loss: 1.7415
Training Epoch: 7 [38400/50048]	Loss: 1.5146
Training Epoch: 7 [38528/50048]	Loss: 1.6265
Training Epoch: 7 [38656/50048]	Loss: 1.6330
Training Epoch: 7 [38784/50048]	Loss: 1.7955
Training Epoch: 7 [38912/50048]	Loss: 1.9189
Training Epoch: 7 [39040/50048]	Loss: 1.7879
Training Epoch: 7 [39168/50048]	Loss: 1.5331
Training Epoch: 7 [39296/50048]	Loss: 1.5977
Training Epoch: 7 [39424/50048]	Loss: 1.5304
Training Epoch: 7 [39552/50048]	Loss: 1.7872
Training Epoch: 7 [39680/50048]	Loss: 1.7241
Training Epoch: 7 [39808/50048]	Loss: 1.5267
Training Epoch: 7 [39936/50048]	Loss: 1.6647
Training Epoch: 7 [40064/50048]	Loss: 1.7683
Training Epoch: 7 [40192/50048]	Loss: 1.5933
Training Epoch: 7 [40320/50048]	Loss: 1.6225
Training Epoch: 7 [40448/50048]	Loss: 1.8959
Training Epoch: 7 [40576/50048]	Loss: 1.7879
Training Epoch: 7 [40704/50048]	Loss: 1.6502
Training Epoch: 7 [40832/50048]	Loss: 1.8480
Training Epoch: 7 [40960/50048]	Loss: 1.5716
Training Epoch: 7 [41088/50048]	Loss: 1.8212
Training Epoch: 7 [41216/50048]	Loss: 1.6587
Training Epoch: 7 [41344/50048]	Loss: 1.5762
Training Epoch: 7 [41472/50048]	Loss: 1.6032
Training Epoch: 7 [41600/50048]	Loss: 1.5947
Training Epoch: 7 [41728/50048]	Loss: 1.9490
Training Epoch: 7 [41856/50048]	Loss: 1.4447
Training Epoch: 7 [41984/50048]	Loss: 1.7050
Training Epoch: 7 [42112/50048]	Loss: 1.7092
Training Epoch: 7 [42240/50048]	Loss: 1.8459
Training Epoch: 7 [42368/50048]	Loss: 1.8676
Training Epoch: 7 [42496/50048]	Loss: 1.7094
Training Epoch: 7 [42624/50048]	Loss: 1.7846
Training Epoch: 7 [42752/50048]	Loss: 1.7351
Training Epoch: 7 [42880/50048]	Loss: 1.9063
Training Epoch: 7 [43008/50048]	Loss: 1.6288
Training Epoch: 7 [43136/50048]	Loss: 1.6176
Training Epoch: 7 [43264/50048]	Loss: 1.7756
Training Epoch: 7 [43392/50048]	Loss: 1.7511
Training Epoch: 7 [43520/50048]	Loss: 1.7958
Training Epoch: 7 [43648/50048]	Loss: 1.8438
Training Epoch: 7 [43776/50048]	Loss: 1.7172
Training Epoch: 7 [43904/50048]	Loss: 1.9336
Training Epoch: 7 [44032/50048]	Loss: 1.7001
Training Epoch: 7 [44160/50048]	Loss: 1.7804
Training Epoch: 7 [44288/50048]	Loss: 1.6979
Training Epoch: 7 [44416/50048]	Loss: 1.6297
Training Epoch: 7 [44544/50048]	Loss: 1.7064
Training Epoch: 7 [44672/50048]	Loss: 2.0321
Training Epoch: 7 [44800/50048]	Loss: 1.6274
Training Epoch: 7 [44928/50048]	Loss: 1.6462
Training Epoch: 7 [45056/50048]	Loss: 1.3346
Training Epoch: 7 [45184/50048]	Loss: 1.7584
Training Epoch: 7 [45312/50048]	Loss: 1.6807
Training Epoch: 7 [45440/50048]	Loss: 1.5911
Training Epoch: 7 [45568/50048]	Loss: 1.5430
Training Epoch: 7 [45696/50048]	Loss: 1.7212
Training Epoch: 7 [45824/50048]	Loss: 1.8163
Training Epoch: 7 [45952/50048]	Loss: 1.5450
Training Epoch: 7 [46080/50048]	Loss: 1.7832
Training Epoch: 7 [46208/50048]	Loss: 1.5136
Training Epoch: 7 [46336/50048]	Loss: 1.6137
Training Epoch: 7 [46464/50048]	Loss: 1.7568
Training Epoch: 7 [46592/50048]	Loss: 1.7912
Training Epoch: 7 [46720/50048]	Loss: 1.9509
Training Epoch: 7 [46848/50048]	Loss: 1.7613
Training Epoch: 7 [46976/50048]	Loss: 1.9900
Training Epoch: 7 [47104/50048]	Loss: 1.5467
Training Epoch: 7 [47232/50048]	Loss: 1.8674
Training Epoch: 7 [47360/50048]	Loss: 1.7045
Training Epoch: 7 [47488/50048]	Loss: 1.7343
Training Epoch: 7 [47616/50048]	Loss: 1.5974
Training Epoch: 7 [47744/50048]	Loss: 1.9304
Training Epoch: 7 [47872/50048]	Loss: 1.7839
Training Epoch: 7 [48000/50048]	Loss: 1.8903
Training Epoch: 7 [48128/50048]	Loss: 1.6536
Training Epoch: 7 [48256/50048]	Loss: 1.6170
Training Epoch: 7 [48384/50048]	Loss: 1.6681
Training Epoch: 7 [48512/50048]	Loss: 1.6286
Training Epoch: 7 [48640/50048]	Loss: 1.5154
Training Epoch: 7 [48768/50048]	Loss: 1.7795
Training Epoch: 7 [48896/50048]	Loss: 1.6883
Training Epoch: 7 [49024/50048]	Loss: 1.8772
Training Epoch: 7 [49152/50048]	Loss: 1.7334
Training Epoch: 7 [49280/50048]	Loss: 1.5751
Training Epoch: 7 [49408/50048]	Loss: 1.7977
Training Epoch: 7 [49536/50048]	Loss: 1.7716
Training Epoch: 7 [49664/50048]	Loss: 1.7043
Training Epoch: 7 [49792/50048]	Loss: 1.5192
Training Epoch: 7 [49920/50048]	Loss: 1.6086
Training Epoch: 7 [50048/50048]	Loss: 1.5017
Validation Epoch: 7, Average loss: 0.0133, Accuracy: 0.5320
[Training Loop] Model's accuracy 0.5320411392405063 surpasses threshold 0.4! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8221
Profiling... [256/50048]	Loss: 1.6152
Profiling... [384/50048]	Loss: 1.7832
Profiling... [512/50048]	Loss: 2.1057
Profiling... [640/50048]	Loss: 1.7143
Profiling... [768/50048]	Loss: 1.7921
Profiling... [896/50048]	Loss: 1.9327
Profiling... [1024/50048]	Loss: 1.9406
Profiling... [1152/50048]	Loss: 1.8451
Profiling... [1280/50048]	Loss: 1.7950
Profiling... [1408/50048]	Loss: 1.5830
Profiling... [1536/50048]	Loss: 1.5770
Profiling... [1664/50048]	Loss: 1.8721
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 8, Average loss: 0.0134, Accuracy: 0.5312
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.30088409340502,
                        "time": 2.178788672000337,
                        "accuracy": 0.5311511075949367,
                        "total_cost": 717852.2498551102
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7337
Profiling... [256/50048]	Loss: 1.4029
Profiling... [384/50048]	Loss: 1.7613
Profiling... [512/50048]	Loss: 1.4928
Profiling... [640/50048]	Loss: 1.5424
Profiling... [768/50048]	Loss: 1.7698
Profiling... [896/50048]	Loss: 1.7326
Profiling... [1024/50048]	Loss: 1.7120
Profiling... [1152/50048]	Loss: 1.5661
Profiling... [1280/50048]	Loss: 1.6435
Profiling... [1408/50048]	Loss: 1.7984
Profiling... [1536/50048]	Loss: 1.7382
Profiling... [1664/50048]	Loss: 1.4734
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 8, Average loss: 0.0134, Accuracy: 0.5293
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.32002282093244,
                        "time": 2.1693823819996396,
                        "accuracy": 0.5292721518987342,
                        "total_cost": 717290.5573965923
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7057
Profiling... [256/50048]	Loss: 1.7302
Profiling... [384/50048]	Loss: 2.0581
Profiling... [512/50048]	Loss: 1.8363
Profiling... [640/50048]	Loss: 1.7161
Profiling... [768/50048]	Loss: 1.6094
Profiling... [896/50048]	Loss: 1.7111
Profiling... [1024/50048]	Loss: 1.9171
Profiling... [1152/50048]	Loss: 1.8820
Profiling... [1280/50048]	Loss: 1.5155
Profiling... [1408/50048]	Loss: 1.6278
Profiling... [1536/50048]	Loss: 1.7468
Profiling... [1664/50048]	Loss: 1.4885
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 8, Average loss: 0.0133, Accuracy: 0.5320
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.33070831317006,
                        "time": 2.3846231750003426,
                        "accuracy": 0.5320411392405063,
                        "total_cost": 784354.8644015996
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7432
Profiling... [256/50048]	Loss: 1.5138
Profiling... [384/50048]	Loss: 1.4352
Profiling... [512/50048]	Loss: 1.8895
Profiling... [640/50048]	Loss: 1.6416
Profiling... [768/50048]	Loss: 1.7063
Profiling... [896/50048]	Loss: 1.5288
Profiling... [1024/50048]	Loss: 1.5791
Profiling... [1152/50048]	Loss: 1.7440
Profiling... [1280/50048]	Loss: 1.6795
Profiling... [1408/50048]	Loss: 1.4666
Profiling... [1536/50048]	Loss: 1.6448
Profiling... [1664/50048]	Loss: 1.5162
Profile done
epoch 1 train time consumed: 6.25s
Validation Epoch: 8, Average loss: 0.0134, Accuracy: 0.5295
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 120.30677536905284,
                        "time": 4.217135194000548,
                        "accuracy": 0.5294699367088608,
                        "total_cost": 1393844.3106655527
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6797
Profiling... [256/50048]	Loss: 1.5143
Profiling... [384/50048]	Loss: 1.6114
Profiling... [512/50048]	Loss: 1.7092
Profiling... [640/50048]	Loss: 1.6800
Profiling... [768/50048]	Loss: 1.6124
Profiling... [896/50048]	Loss: 1.4650
Profiling... [1024/50048]	Loss: 1.6420
Profiling... [1152/50048]	Loss: 1.7810
Profiling... [1280/50048]	Loss: 1.8556
Profiling... [1408/50048]	Loss: 1.9358
Profiling... [1536/50048]	Loss: 1.6805
Profiling... [1664/50048]	Loss: 1.7202
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 8, Average loss: 0.0135, Accuracy: 0.5281
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.26235470001318,
                        "time": 2.176998423000441,
                        "accuracy": 0.5280854430379747,
                        "total_cost": 721426.2938841911
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5249
Profiling... [256/50048]	Loss: 1.7680
Profiling... [384/50048]	Loss: 1.6509
Profiling... [512/50048]	Loss: 1.8042
Profiling... [640/50048]	Loss: 1.6627
Profiling... [768/50048]	Loss: 1.6105
Profiling... [896/50048]	Loss: 1.5419
Profiling... [1024/50048]	Loss: 1.4881
Profiling... [1152/50048]	Loss: 1.8523
Profiling... [1280/50048]	Loss: 1.8148
Profiling... [1408/50048]	Loss: 1.6079
Profiling... [1536/50048]	Loss: 1.8414
Profiling... [1664/50048]	Loss: 1.7581
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 8, Average loss: 0.0134, Accuracy: 0.5271
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.27730284755381,
                        "time": 2.1782584259999567,
                        "accuracy": 0.5270965189873418,
                        "total_cost": 723198.1445871526
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8679
Profiling... [256/50048]	Loss: 1.9003
Profiling... [384/50048]	Loss: 1.5910
Profiling... [512/50048]	Loss: 1.6782
Profiling... [640/50048]	Loss: 1.8237
Profiling... [768/50048]	Loss: 1.7397
Profiling... [896/50048]	Loss: 1.3421
Profiling... [1024/50048]	Loss: 1.6273
Profiling... [1152/50048]	Loss: 1.6223
Profiling... [1280/50048]	Loss: 1.5822
Profiling... [1408/50048]	Loss: 1.5212
Profiling... [1536/50048]	Loss: 1.7087
Profiling... [1664/50048]	Loss: 1.5953
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 8, Average loss: 0.0133, Accuracy: 0.5308
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.28835925703882,
                        "time": 2.3793445439996503,
                        "accuracy": 0.5307555379746836,
                        "total_cost": 784514.2733485712
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7592
Profiling... [256/50048]	Loss: 1.8182
Profiling... [384/50048]	Loss: 1.4432
Profiling... [512/50048]	Loss: 2.0305
Profiling... [640/50048]	Loss: 1.5347
Profiling... [768/50048]	Loss: 1.7101
Profiling... [896/50048]	Loss: 1.5316
Profiling... [1024/50048]	Loss: 1.8499
Profiling... [1152/50048]	Loss: 1.6587
Profiling... [1280/50048]	Loss: 1.9072
Profiling... [1408/50048]	Loss: 1.5464
Profiling... [1536/50048]	Loss: 1.9167
Profiling... [1664/50048]	Loss: 1.7559
Profile done
epoch 1 train time consumed: 6.81s
Validation Epoch: 8, Average loss: 0.0134, Accuracy: 0.5278
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.26303573179175,
                        "time": 4.774082232999717,
                        "accuracy": 0.5277887658227848,
                        "total_cost": 1582952.2052681844
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5988
Profiling... [256/50048]	Loss: 1.5292
Profiling... [384/50048]	Loss: 1.5334
Profiling... [512/50048]	Loss: 1.5739
Profiling... [640/50048]	Loss: 1.6929
Profiling... [768/50048]	Loss: 1.7199
Profiling... [896/50048]	Loss: 1.5891
Profiling... [1024/50048]	Loss: 1.4332
Profiling... [1152/50048]	Loss: 1.7512
Profiling... [1280/50048]	Loss: 1.6212
Profiling... [1408/50048]	Loss: 1.6328
Profiling... [1536/50048]	Loss: 1.7781
Profiling... [1664/50048]	Loss: 1.5596
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 8, Average loss: 0.0135, Accuracy: 0.5284
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.21590118866199,
                        "time": 2.173516500000005,
                        "accuracy": 0.5283821202531646,
                        "total_cost": 719868.0139247631
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4474
Profiling... [256/50048]	Loss: 1.8285
Profiling... [384/50048]	Loss: 1.5303
Profiling... [512/50048]	Loss: 1.6343
Profiling... [640/50048]	Loss: 1.6997
Profiling... [768/50048]	Loss: 1.5788
Profiling... [896/50048]	Loss: 1.9973
Profiling... [1024/50048]	Loss: 1.5892
Profiling... [1152/50048]	Loss: 1.7150
Profiling... [1280/50048]	Loss: 1.7295
Profiling... [1408/50048]	Loss: 1.5186
Profiling... [1536/50048]	Loss: 1.4755
Profiling... [1664/50048]	Loss: 1.5651
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 8, Average loss: 0.0133, Accuracy: 0.5351
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.23272249267123,
                        "time": 2.1674024799995095,
                        "accuracy": 0.5351068037974683,
                        "total_cost": 708821.923601392
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6642
Profiling... [256/50048]	Loss: 1.6520
Profiling... [384/50048]	Loss: 1.8351
Profiling... [512/50048]	Loss: 1.5289
Profiling... [640/50048]	Loss: 1.5012
Profiling... [768/50048]	Loss: 1.8330
Profiling... [896/50048]	Loss: 1.7387
Profiling... [1024/50048]	Loss: 1.8539
Profiling... [1152/50048]	Loss: 1.8772
Profiling... [1280/50048]	Loss: 1.5952
Profiling... [1408/50048]	Loss: 1.6073
Profiling... [1536/50048]	Loss: 1.7778
Profiling... [1664/50048]	Loss: 1.5602
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 8, Average loss: 0.0135, Accuracy: 0.5273
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.2416542151238,
                        "time": 2.386343532000865,
                        "accuracy": 0.5272943037974683,
                        "total_cost": 791986.7806130403
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6442
Profiling... [256/50048]	Loss: 1.6121
Profiling... [384/50048]	Loss: 1.6733
Profiling... [512/50048]	Loss: 1.6990
Profiling... [640/50048]	Loss: 1.5021
Profiling... [768/50048]	Loss: 1.6799
Profiling... [896/50048]	Loss: 1.8615
Profiling... [1024/50048]	Loss: 1.5311
Profiling... [1152/50048]	Loss: 1.6933
Profiling... [1280/50048]	Loss: 1.5650
Profiling... [1408/50048]	Loss: 1.8603
Profiling... [1536/50048]	Loss: 1.5946
Profiling... [1664/50048]	Loss: 1.6642
Profile done
epoch 1 train time consumed: 6.99s
Validation Epoch: 8, Average loss: 0.0134, Accuracy: 0.5303
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.21378490307688,
                        "time": 4.939578669999719,
                        "accuracy": 0.5302610759493671,
                        "total_cost": 1630189.9318223614
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7250
Profiling... [256/50048]	Loss: 1.6162
Profiling... [384/50048]	Loss: 1.8147
Profiling... [512/50048]	Loss: 1.7455
Profiling... [640/50048]	Loss: 2.0925
Profiling... [768/50048]	Loss: 1.6189
Profiling... [896/50048]	Loss: 1.4887
Profiling... [1024/50048]	Loss: 1.9013
Profiling... [1152/50048]	Loss: 1.7927
Profiling... [1280/50048]	Loss: 2.1159
Profiling... [1408/50048]	Loss: 1.9840
Profiling... [1536/50048]	Loss: 2.0725
Profiling... [1664/50048]	Loss: 1.9291
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 8, Average loss: 0.0154, Accuracy: 0.4819
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.17336110167898,
                        "time": 2.1753838859995085,
                        "accuracy": 0.4819026898734177,
                        "total_cost": 789977.2880494008
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6909
Profiling... [256/50048]	Loss: 1.6903
Profiling... [384/50048]	Loss: 1.7910
Profiling... [512/50048]	Loss: 1.8953
Profiling... [640/50048]	Loss: 1.7968
Profiling... [768/50048]	Loss: 1.8436
Profiling... [896/50048]	Loss: 2.0526
Profiling... [1024/50048]	Loss: 1.9301
Profiling... [1152/50048]	Loss: 2.0898
Profiling... [1280/50048]	Loss: 1.9806
Profiling... [1408/50048]	Loss: 1.6039
Profiling... [1536/50048]	Loss: 1.8077
Profiling... [1664/50048]	Loss: 1.8569
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 8, Average loss: 0.0162, Accuracy: 0.4550
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.18876205040176,
                        "time": 2.170004044000052,
                        "accuracy": 0.45500395569620256,
                        "total_cost": 834609.6840387941
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9907
Profiling... [256/50048]	Loss: 1.7933
Profiling... [384/50048]	Loss: 1.8523
Profiling... [512/50048]	Loss: 1.6987
Profiling... [640/50048]	Loss: 1.7117
Profiling... [768/50048]	Loss: 1.6968
Profiling... [896/50048]	Loss: 1.8066
Profiling... [1024/50048]	Loss: 1.8205
Profiling... [1152/50048]	Loss: 1.9297
Profiling... [1280/50048]	Loss: 1.7454
Profiling... [1408/50048]	Loss: 1.5417
Profiling... [1536/50048]	Loss: 1.9002
Profiling... [1664/50048]	Loss: 1.7479
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 8, Average loss: 0.0160, Accuracy: 0.4618
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.1975211177399,
                        "time": 2.3925412360003975,
                        "accuracy": 0.4618275316455696,
                        "total_cost": 906604.0623610929
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5935
Profiling... [256/50048]	Loss: 1.6853
Profiling... [384/50048]	Loss: 1.9227
Profiling... [512/50048]	Loss: 1.7818
Profiling... [640/50048]	Loss: 1.6097
Profiling... [768/50048]	Loss: 1.6237
Profiling... [896/50048]	Loss: 1.6892
Profiling... [1024/50048]	Loss: 1.5551
Profiling... [1152/50048]	Loss: 1.9123
Profiling... [1280/50048]	Loss: 1.7473
Profiling... [1408/50048]	Loss: 1.9769
Profiling... [1536/50048]	Loss: 1.8018
Profiling... [1664/50048]	Loss: 1.9483
Profile done
epoch 1 train time consumed: 7.09s
Validation Epoch: 8, Average loss: 0.0163, Accuracy: 0.4512
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.1714980202581,
                        "time": 4.988427616000081,
                        "accuracy": 0.45124604430379744,
                        "total_cost": 1934587.225350371
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6556
Profiling... [256/50048]	Loss: 1.6900
Profiling... [384/50048]	Loss: 1.6887
Profiling... [512/50048]	Loss: 1.9213
Profiling... [640/50048]	Loss: 1.6906
Profiling... [768/50048]	Loss: 1.8625
Profiling... [896/50048]	Loss: 1.7956
Profiling... [1024/50048]	Loss: 1.7678
Profiling... [1152/50048]	Loss: 1.7570
Profiling... [1280/50048]	Loss: 1.7581
Profiling... [1408/50048]	Loss: 1.6960
Profiling... [1536/50048]	Loss: 1.9913
Profiling... [1664/50048]	Loss: 2.2479
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 8, Average loss: 0.0214, Accuracy: 0.3613
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.13055421308344,
                        "time": 2.1833994210001038,
                        "accuracy": 0.36125395569620256,
                        "total_cost": 1057690.5599238388
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5854
Profiling... [256/50048]	Loss: 1.8797
Profiling... [384/50048]	Loss: 1.6638
Profiling... [512/50048]	Loss: 1.6416
Profiling... [640/50048]	Loss: 1.5273
Profiling... [768/50048]	Loss: 1.8715
Profiling... [896/50048]	Loss: 1.6622
Profiling... [1024/50048]	Loss: 1.8970
Profiling... [1152/50048]	Loss: 1.8213
Profiling... [1280/50048]	Loss: 2.1570
Profiling... [1408/50048]	Loss: 1.9917
Profiling... [1536/50048]	Loss: 1.6350
Profiling... [1664/50048]	Loss: 1.6297
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 8, Average loss: 0.0149, Accuracy: 0.4895
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.14455409409805,
                        "time": 2.168646471000102,
                        "accuracy": 0.48951740506329117,
                        "total_cost": 775280.1606225818
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6881
Profiling... [256/50048]	Loss: 1.6016
Profiling... [384/50048]	Loss: 1.7912
Profiling... [512/50048]	Loss: 1.7844
Profiling... [640/50048]	Loss: 1.5929
Profiling... [768/50048]	Loss: 1.8460
Profiling... [896/50048]	Loss: 1.7349
Profiling... [1024/50048]	Loss: 1.5564
Profiling... [1152/50048]	Loss: 1.7195
Profiling... [1280/50048]	Loss: 1.7729
Profiling... [1408/50048]	Loss: 1.7298
Profiling... [1536/50048]	Loss: 2.1115
Profiling... [1664/50048]	Loss: 1.6028
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 8, Average loss: 0.0172, Accuracy: 0.4429
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.15360207468427,
                        "time": 2.3865343309998934,
                        "accuracy": 0.442939082278481,
                        "total_cost": 942891.5276037981
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7382
Profiling... [256/50048]	Loss: 1.5931
Profiling... [384/50048]	Loss: 1.6187
Profiling... [512/50048]	Loss: 1.8395
Profiling... [640/50048]	Loss: 1.7568
Profiling... [768/50048]	Loss: 1.5639
Profiling... [896/50048]	Loss: 1.5974
Profiling... [1024/50048]	Loss: 1.8174
Profiling... [1152/50048]	Loss: 1.7994
Profiling... [1280/50048]	Loss: 1.5837
Profiling... [1408/50048]	Loss: 1.8247
Profiling... [1536/50048]	Loss: 1.8129
Profiling... [1664/50048]	Loss: 1.7233
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 8, Average loss: 0.0172, Accuracy: 0.4398
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 120.1271004671822,
                        "time": 4.946452168999713,
                        "accuracy": 0.4397745253164557,
                        "total_cost": 1968347.5957413742
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6710
Profiling... [256/50048]	Loss: 1.2790
Profiling... [384/50048]	Loss: 1.7603
Profiling... [512/50048]	Loss: 1.9399
Profiling... [640/50048]	Loss: 1.7719
Profiling... [768/50048]	Loss: 1.7147
Profiling... [896/50048]	Loss: 1.4944
Profiling... [1024/50048]	Loss: 1.7983
Profiling... [1152/50048]	Loss: 1.7282
Profiling... [1280/50048]	Loss: 1.6312
Profiling... [1408/50048]	Loss: 1.4985
Profiling... [1536/50048]	Loss: 1.7642
Profiling... [1664/50048]	Loss: 1.3954
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 8, Average loss: 0.0159, Accuracy: 0.4677
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.08636030895585,
                        "time": 2.17124217699984,
                        "accuracy": 0.4676621835443038,
                        "total_cost": 812482.5875277895
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6043
Profiling... [256/50048]	Loss: 1.4065
Profiling... [384/50048]	Loss: 1.6507
Profiling... [512/50048]	Loss: 1.7150
Profiling... [640/50048]	Loss: 1.5661
Profiling... [768/50048]	Loss: 1.5117
Profiling... [896/50048]	Loss: 1.6842
Profiling... [1024/50048]	Loss: 1.7511
Profiling... [1152/50048]	Loss: 2.1371
Profiling... [1280/50048]	Loss: 1.7982
Profiling... [1408/50048]	Loss: 2.1585
Profiling... [1536/50048]	Loss: 1.6273
Profiling... [1664/50048]	Loss: 1.7913
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 8, Average loss: 0.0170, Accuracy: 0.4402
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.09745768384009,
                        "time": 2.1714487970002665,
                        "accuracy": 0.44017009493670883,
                        "total_cost": 863310.669775707
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7631
Profiling... [256/50048]	Loss: 1.7962
Profiling... [384/50048]	Loss: 1.8919
Profiling... [512/50048]	Loss: 2.0017
Profiling... [640/50048]	Loss: 1.6867
Profiling... [768/50048]	Loss: 1.7441
Profiling... [896/50048]	Loss: 1.8028
Profiling... [1024/50048]	Loss: 1.8182
Profiling... [1152/50048]	Loss: 1.8570
Profiling... [1280/50048]	Loss: 1.4948
Profiling... [1408/50048]	Loss: 1.5505
Profiling... [1536/50048]	Loss: 1.8102
Profiling... [1664/50048]	Loss: 2.0093
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 8, Average loss: 0.0219, Accuracy: 0.3657
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.10874609938111,
                        "time": 2.382401191000099,
                        "accuracy": 0.3657041139240506,
                        "total_cost": 1140047.903621897
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6107
Profiling... [256/50048]	Loss: 1.8733
Profiling... [384/50048]	Loss: 1.8801
Profiling... [512/50048]	Loss: 1.9466
Profiling... [640/50048]	Loss: 1.6894
Profiling... [768/50048]	Loss: 1.7418
Profiling... [896/50048]	Loss: 1.8139
Profiling... [1024/50048]	Loss: 1.7117
Profiling... [1152/50048]	Loss: 1.9001
Profiling... [1280/50048]	Loss: 1.6335
Profiling... [1408/50048]	Loss: 1.8956
Profiling... [1536/50048]	Loss: 1.6919
Profiling... [1664/50048]	Loss: 1.6683
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 8, Average loss: 0.0161, Accuracy: 0.4606
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 120.08406204987465,
                        "time": 4.797007770000164,
                        "accuracy": 0.4606408227848101,
                        "total_cost": 1822409.8217673444
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7507
Profiling... [256/50048]	Loss: 1.6613
Profiling... [384/50048]	Loss: 1.7059
Profiling... [512/50048]	Loss: 2.2578
Profiling... [640/50048]	Loss: 1.9452
Profiling... [768/50048]	Loss: 1.7537
Profiling... [896/50048]	Loss: 2.1530
Profiling... [1024/50048]	Loss: 1.9463
Profiling... [1152/50048]	Loss: 1.8809
Profiling... [1280/50048]	Loss: 2.0864
Profiling... [1408/50048]	Loss: 1.9142
Profiling... [1536/50048]	Loss: 1.9384
Profiling... [1664/50048]	Loss: 2.3198
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 8, Average loss: 0.0217, Accuracy: 0.3510
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.04187595766425,
                        "time": 2.1612458340005105,
                        "accuracy": 0.3509691455696203,
                        "total_cost": 1077638.948393154
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7593
Profiling... [256/50048]	Loss: 1.7600
Profiling... [384/50048]	Loss: 1.6212
Profiling... [512/50048]	Loss: 1.8872
Profiling... [640/50048]	Loss: 1.9837
Profiling... [768/50048]	Loss: 1.8639
Profiling... [896/50048]	Loss: 2.1795
Profiling... [1024/50048]	Loss: 1.9575
Profiling... [1152/50048]	Loss: 2.1110
Profiling... [1280/50048]	Loss: 1.8699
Profiling... [1408/50048]	Loss: 1.8025
Profiling... [1536/50048]	Loss: 2.0656
Profiling... [1664/50048]	Loss: 1.9336
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 8, Average loss: 0.0200, Accuracy: 0.3688
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.05920094450777,
                        "time": 2.187470363999637,
                        "accuracy": 0.36876977848101267,
                        "total_cost": 1038065.8503978968
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5910
Profiling... [256/50048]	Loss: 1.8837
Profiling... [384/50048]	Loss: 1.9647
Profiling... [512/50048]	Loss: 2.0678
Profiling... [640/50048]	Loss: 2.2131
Profiling... [768/50048]	Loss: 1.7030
Profiling... [896/50048]	Loss: 1.8701
Profiling... [1024/50048]	Loss: 1.8229
Profiling... [1152/50048]	Loss: 1.9615
Profiling... [1280/50048]	Loss: 1.8920
Profiling... [1408/50048]	Loss: 2.1269
Profiling... [1536/50048]	Loss: 2.0244
Profiling... [1664/50048]	Loss: 2.0525
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 8, Average loss: 0.0321, Accuracy: 0.2428
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.06808027259105,
                        "time": 2.3782489900004293,
                        "accuracy": 0.24278085443037975,
                        "total_cost": 1714276.7465192503
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8650
Profiling... [256/50048]	Loss: 1.5878
Profiling... [384/50048]	Loss: 1.8034
Profiling... [512/50048]	Loss: 1.9812
Profiling... [640/50048]	Loss: 1.9582
Profiling... [768/50048]	Loss: 1.9236
Profiling... [896/50048]	Loss: 2.0649
Profiling... [1024/50048]	Loss: 2.0097
Profiling... [1152/50048]	Loss: 1.9839
Profiling... [1280/50048]	Loss: 2.0028
Profiling... [1408/50048]	Loss: 2.2017
Profiling... [1536/50048]	Loss: 1.9283
Profiling... [1664/50048]	Loss: 2.0271
Profile done
epoch 1 train time consumed: 6.87s
Validation Epoch: 8, Average loss: 0.0280, Accuracy: 0.2668
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 120.04501483108494,
                        "time": 4.800434160000805,
                        "accuracy": 0.2668117088607595,
                        "total_cost": 3148572.383075398
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4480
Profiling... [256/50048]	Loss: 1.7250
Profiling... [384/50048]	Loss: 2.0817
Profiling... [512/50048]	Loss: 1.7429
Profiling... [640/50048]	Loss: 2.0986
Profiling... [768/50048]	Loss: 1.9972
Profiling... [896/50048]	Loss: 1.8067
Profiling... [1024/50048]	Loss: 1.9988
Profiling... [1152/50048]	Loss: 1.8461
Profiling... [1280/50048]	Loss: 2.2175
Profiling... [1408/50048]	Loss: 2.2129
Profiling... [1536/50048]	Loss: 1.9885
Profiling... [1664/50048]	Loss: 2.0269
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 8, Average loss: 0.0214, Accuracy: 0.3511
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.99863538631205,
                        "time": 2.1704598459991757,
                        "accuracy": 0.35106803797468356,
                        "total_cost": 1081928.37844511
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8229
Profiling... [256/50048]	Loss: 1.6081
Profiling... [384/50048]	Loss: 1.9114
Profiling... [512/50048]	Loss: 1.9313
Profiling... [640/50048]	Loss: 2.0504
Profiling... [768/50048]	Loss: 1.8415
Profiling... [896/50048]	Loss: 1.9884
Profiling... [1024/50048]	Loss: 2.1108
Profiling... [1152/50048]	Loss: 1.9547
Profiling... [1280/50048]	Loss: 1.9780
Profiling... [1408/50048]	Loss: 1.7400
Profiling... [1536/50048]	Loss: 1.7375
Profiling... [1664/50048]	Loss: 1.7670
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 8, Average loss: 0.0196, Accuracy: 0.3671
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.01595011611992,
                        "time": 2.164315912999882,
                        "accuracy": 0.3670886075949367,
                        "total_cost": 1031781.6378352885
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8001
Profiling... [256/50048]	Loss: 2.0657
Profiling... [384/50048]	Loss: 1.7281
Profiling... [512/50048]	Loss: 2.1981
Profiling... [640/50048]	Loss: 1.8444
Profiling... [768/50048]	Loss: 1.9292
Profiling... [896/50048]	Loss: 2.0248
Profiling... [1024/50048]	Loss: 2.0382
Profiling... [1152/50048]	Loss: 2.2602
Profiling... [1280/50048]	Loss: 1.8307
Profiling... [1408/50048]	Loss: 1.7099
Profiling... [1536/50048]	Loss: 1.9232
Profiling... [1664/50048]	Loss: 1.6358
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 8, Average loss: 0.0199, Accuracy: 0.3680
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.02536794504881,
                        "time": 2.3958781549999912,
                        "accuracy": 0.36797863924050633,
                        "total_cost": 1139410.3690104769
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6218
Profiling... [256/50048]	Loss: 1.7997
Profiling... [384/50048]	Loss: 2.0178
Profiling... [512/50048]	Loss: 2.0321
Profiling... [640/50048]	Loss: 1.9754
Profiling... [768/50048]	Loss: 1.6538
Profiling... [896/50048]	Loss: 1.8507
Profiling... [1024/50048]	Loss: 1.7277
Profiling... [1152/50048]	Loss: 2.1136
Profiling... [1280/50048]	Loss: 2.1107
Profiling... [1408/50048]	Loss: 1.8904
Profiling... [1536/50048]	Loss: 2.1401
Profiling... [1664/50048]	Loss: 1.9242
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 8, Average loss: 0.0206, Accuracy: 0.3382
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 120.00147870357767,
                        "time": 5.066226070000084,
                        "accuracy": 0.3382120253164557,
                        "total_cost": 2621401.6530620316
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8027
Profiling... [256/50048]	Loss: 1.7631
Profiling... [384/50048]	Loss: 2.0145
Profiling... [512/50048]	Loss: 1.7454
Profiling... [640/50048]	Loss: 2.0547
Profiling... [768/50048]	Loss: 2.1312
Profiling... [896/50048]	Loss: 1.8863
Profiling... [1024/50048]	Loss: 2.0886
Profiling... [1152/50048]	Loss: 1.8753
Profiling... [1280/50048]	Loss: 1.9419
Profiling... [1408/50048]	Loss: 2.1502
Profiling... [1536/50048]	Loss: 1.9898
Profiling... [1664/50048]	Loss: 1.7336
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 8, Average loss: 0.0201, Accuracy: 0.3694
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.9616767155898,
                        "time": 2.17778540099971,
                        "accuracy": 0.3693631329113924,
                        "total_cost": 1031809.6507654851
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5389
Profiling... [256/50048]	Loss: 1.8708
Profiling... [384/50048]	Loss: 1.7206
Profiling... [512/50048]	Loss: 2.1770
Profiling... [640/50048]	Loss: 1.7487
Profiling... [768/50048]	Loss: 1.7760
Profiling... [896/50048]	Loss: 1.9061
Profiling... [1024/50048]	Loss: 2.0859
Profiling... [1152/50048]	Loss: 2.1520
Profiling... [1280/50048]	Loss: 1.8529
Profiling... [1408/50048]	Loss: 1.9363
Profiling... [1536/50048]	Loss: 1.9383
Profiling... [1664/50048]	Loss: 2.1953
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 8, Average loss: 0.0576, Accuracy: 0.1362
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.97544447900744,
                        "time": 2.172707378999803,
                        "accuracy": 0.1361748417721519,
                        "total_cost": 2792173.5496572633
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7774
Profiling... [256/50048]	Loss: 2.0100
Profiling... [384/50048]	Loss: 1.8060
Profiling... [512/50048]	Loss: 2.0661
Profiling... [640/50048]	Loss: 1.8762
Profiling... [768/50048]	Loss: 1.9222
Profiling... [896/50048]	Loss: 1.9870
Profiling... [1024/50048]	Loss: 2.0756
Profiling... [1152/50048]	Loss: 1.9719
Profiling... [1280/50048]	Loss: 1.6852
Profiling... [1408/50048]	Loss: 1.7839
Profiling... [1536/50048]	Loss: 2.2070
Profiling... [1664/50048]	Loss: 1.9311
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 8, Average loss: 0.0436, Accuracy: 0.1500
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.98518438665566,
                        "time": 2.3868032079999466,
                        "accuracy": 0.15001977848101267,
                        "total_cost": 2784236.6228587376
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9547
Profiling... [256/50048]	Loss: 1.7093
Profiling... [384/50048]	Loss: 1.7056
Profiling... [512/50048]	Loss: 1.9532
Profiling... [640/50048]	Loss: 2.0385
Profiling... [768/50048]	Loss: 2.0920
Profiling... [896/50048]	Loss: 1.7051
Profiling... [1024/50048]	Loss: 1.9260
Profiling... [1152/50048]	Loss: 1.8964
Profiling... [1280/50048]	Loss: 2.0547
Profiling... [1408/50048]	Loss: 2.2568
Profiling... [1536/50048]	Loss: 2.2809
Profiling... [1664/50048]	Loss: 1.9982
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 8, Average loss: 0.0311, Accuracy: 0.2473
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.96131022869896,
                        "time": 4.789793820999876,
                        "accuracy": 0.24732990506329114,
                        "total_cost": 3389052.037441575
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6778
Profiling... [512/50176]	Loss: 1.8419
Profiling... [768/50176]	Loss: 1.6308
Profiling... [1024/50176]	Loss: 1.6657
Profiling... [1280/50176]	Loss: 1.9372
Profiling... [1536/50176]	Loss: 1.6172
Profiling... [1792/50176]	Loss: 1.5910
Profiling... [2048/50176]	Loss: 1.7382
Profiling... [2304/50176]	Loss: 1.5577
Profiling... [2560/50176]	Loss: 1.6266
Profiling... [2816/50176]	Loss: 1.8988
Profiling... [3072/50176]	Loss: 1.7083
Profiling... [3328/50176]	Loss: 1.5793
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 8, Average loss: 0.0067, Accuracy: 0.5236
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.92580877956976,
                        "time": 2.40491516199927,
                        "accuracy": 0.5236328125,
                        "total_cost": 803731.4379527585
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7306
Profiling... [512/50176]	Loss: 1.5621
Profiling... [768/50176]	Loss: 1.6347
Profiling... [1024/50176]	Loss: 1.6185
Profiling... [1280/50176]	Loss: 1.7052
Profiling... [1536/50176]	Loss: 1.4997
Profiling... [1792/50176]	Loss: 1.7630
Profiling... [2048/50176]	Loss: 1.7383
Profiling... [2304/50176]	Loss: 1.5603
Profiling... [2560/50176]	Loss: 1.9118
Profiling... [2816/50176]	Loss: 1.7298
Profiling... [3072/50176]	Loss: 1.9292
Profiling... [3328/50176]	Loss: 1.8079
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 8, Average loss: 0.0067, Accuracy: 0.5244
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.94567553569334,
                        "time": 2.434954043000289,
                        "accuracy": 0.5244140625,
                        "total_cost": 812558.2206809158
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6537
Profiling... [512/50176]	Loss: 1.6337
Profiling... [768/50176]	Loss: 1.5373
Profiling... [1024/50176]	Loss: 1.5031
Profiling... [1280/50176]	Loss: 1.6127
Profiling... [1536/50176]	Loss: 1.6407
Profiling... [1792/50176]	Loss: 1.6802
Profiling... [2048/50176]	Loss: 1.5871
Profiling... [2304/50176]	Loss: 1.7198
Profiling... [2560/50176]	Loss: 1.6607
Profiling... [2816/50176]	Loss: 1.5654
Profiling... [3072/50176]	Loss: 1.7483
Profiling... [3328/50176]	Loss: 1.6573
Profile done
epoch 1 train time consumed: 4.29s
Validation Epoch: 8, Average loss: 0.0067, Accuracy: 0.5252
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.95516014683709,
                        "time": 2.7560980339994785,
                        "accuracy": 0.5251953125,
                        "total_cost": 918357.6937387626
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7105
Profiling... [512/50176]	Loss: 1.7864
Profiling... [768/50176]	Loss: 1.6920
Profiling... [1024/50176]	Loss: 1.6564
Profiling... [1280/50176]	Loss: 1.7488
Profiling... [1536/50176]	Loss: 1.7427
Profiling... [1792/50176]	Loss: 1.8044
Profiling... [2048/50176]	Loss: 1.9080
Profiling... [2304/50176]	Loss: 1.6823
Profiling... [2560/50176]	Loss: 1.6927
Profiling... [2816/50176]	Loss: 1.5967
Profiling... [3072/50176]	Loss: 1.7630
Profiling... [3328/50176]	Loss: 1.6438
Profile done
epoch 1 train time consumed: 9.82s
Validation Epoch: 8, Average loss: 0.0066, Accuracy: 0.5271
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.9284525713383,
                        "time": 7.1360775220000505,
                        "accuracy": 0.5271484375,
                        "total_cost": 2368997.947281232
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8035
Profiling... [512/50176]	Loss: 1.9032
Profiling... [768/50176]	Loss: 1.6364
Profiling... [1024/50176]	Loss: 1.6034
Profiling... [1280/50176]	Loss: 1.5129
Profiling... [1536/50176]	Loss: 1.8316
Profiling... [1792/50176]	Loss: 1.6718
Profiling... [2048/50176]	Loss: 1.8565
Profiling... [2304/50176]	Loss: 1.4483
Profiling... [2560/50176]	Loss: 1.6525
Profiling... [2816/50176]	Loss: 1.6195
Profiling... [3072/50176]	Loss: 1.7301
Profiling... [3328/50176]	Loss: 1.6268
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 8, Average loss: 0.0067, Accuracy: 0.5263
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.88439468745072,
                        "time": 2.4110663900000873,
                        "accuracy": 0.52626953125,
                        "total_cost": 801750.0409872251
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6251
Profiling... [512/50176]	Loss: 1.5059
Profiling... [768/50176]	Loss: 1.7736
Profiling... [1024/50176]	Loss: 1.7117
Profiling... [1280/50176]	Loss: 1.5346
Profiling... [1536/50176]	Loss: 1.6202
Profiling... [1792/50176]	Loss: 1.6394
Profiling... [2048/50176]	Loss: 1.4555
Profiling... [2304/50176]	Loss: 1.6288
Profiling... [2560/50176]	Loss: 1.6445
Profiling... [2816/50176]	Loss: 1.7569
Profiling... [3072/50176]	Loss: 1.5815
Profiling... [3328/50176]	Loss: 1.6524
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 8, Average loss: 0.0066, Accuracy: 0.5271
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.90748835274529,
                        "time": 2.429464220000227,
                        "accuracy": 0.52705078125,
                        "total_cost": 806670.3506096733
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7323
Profiling... [512/50176]	Loss: 1.5466
Profiling... [768/50176]	Loss: 1.6919
Profiling... [1024/50176]	Loss: 1.5835
Profiling... [1280/50176]	Loss: 1.6822
Profiling... [1536/50176]	Loss: 1.5115
Profiling... [1792/50176]	Loss: 1.7308
Profiling... [2048/50176]	Loss: 1.6258
Profiling... [2304/50176]	Loss: 1.4892
Profiling... [2560/50176]	Loss: 1.4903
Profiling... [2816/50176]	Loss: 1.6581
Profiling... [3072/50176]	Loss: 1.6507
Profiling... [3328/50176]	Loss: 1.6930
Profile done
epoch 1 train time consumed: 4.37s
Validation Epoch: 8, Average loss: 0.0067, Accuracy: 0.5221
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.91646084054294,
                        "time": 2.7543234469994786,
                        "accuracy": 0.5220703125,
                        "total_cost": 923259.9358441948
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6373
Profiling... [512/50176]	Loss: 1.7256
Profiling... [768/50176]	Loss: 1.8084
Profiling... [1024/50176]	Loss: 1.6345
Profiling... [1280/50176]	Loss: 1.7216
Profiling... [1536/50176]	Loss: 1.5360
Profiling... [1792/50176]	Loss: 1.6785
Profiling... [2048/50176]	Loss: 1.6295
Profiling... [2304/50176]	Loss: 1.4867
Profiling... [2560/50176]	Loss: 1.5943
Profiling... [2816/50176]	Loss: 1.7582
Profiling... [3072/50176]	Loss: 1.6020
Profiling... [3328/50176]	Loss: 1.5140
Profile done
epoch 1 train time consumed: 10.09s
Validation Epoch: 8, Average loss: 0.0067, Accuracy: 0.5227
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.88909815886355,
                        "time": 7.2147105439998995,
                        "accuracy": 0.52265625,
                        "total_cost": 2415687.8353602053
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8274
Profiling... [512/50176]	Loss: 1.6638
Profiling... [768/50176]	Loss: 1.6763
Profiling... [1024/50176]	Loss: 1.5301
Profiling... [1280/50176]	Loss: 1.7720
Profiling... [1536/50176]	Loss: 1.5469
Profiling... [1792/50176]	Loss: 1.7014
Profiling... [2048/50176]	Loss: 1.6969
Profiling... [2304/50176]	Loss: 1.6047
Profiling... [2560/50176]	Loss: 1.6473
Profiling... [2816/50176]	Loss: 1.6280
Profiling... [3072/50176]	Loss: 1.5801
Profiling... [3328/50176]	Loss: 1.8225
Profile done
epoch 1 train time consumed: 4.05s
Validation Epoch: 8, Average loss: 0.0067, Accuracy: 0.5268
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.84769639775445,
                        "time": 2.4094251119995533,
                        "accuracy": 0.5267578125,
                        "total_cost": 800461.5870788282
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6930
Profiling... [512/50176]	Loss: 1.7134
Profiling... [768/50176]	Loss: 1.7854
Profiling... [1024/50176]	Loss: 1.8041
Profiling... [1280/50176]	Loss: 1.7050
Profiling... [1536/50176]	Loss: 1.8164
Profiling... [1792/50176]	Loss: 1.6131
Profiling... [2048/50176]	Loss: 1.7785
Profiling... [2304/50176]	Loss: 1.7330
Profiling... [2560/50176]	Loss: 1.6011
Profiling... [2816/50176]	Loss: 1.6571
Profiling... [3072/50176]	Loss: 1.6054
Profiling... [3328/50176]	Loss: 1.7879
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 8, Average loss: 0.0067, Accuracy: 0.5260
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.86608395668662,
                        "time": 2.4412085750000188,
                        "accuracy": 0.5259765625,
                        "total_cost": 812225.355811369
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5572
Profiling... [512/50176]	Loss: 1.6317
Profiling... [768/50176]	Loss: 1.4169
Profiling... [1024/50176]	Loss: 1.8842
Profiling... [1280/50176]	Loss: 1.6993
Profiling... [1536/50176]	Loss: 1.5053
Profiling... [1792/50176]	Loss: 1.5150
Profiling... [2048/50176]	Loss: 1.6857
Profiling... [2304/50176]	Loss: 1.7767
Profiling... [2560/50176]	Loss: 1.6777
Profiling... [2816/50176]	Loss: 1.7926
Profiling... [3072/50176]	Loss: 1.6727
Profiling... [3328/50176]	Loss: 1.6896
Profile done
epoch 1 train time consumed: 4.34s
Validation Epoch: 8, Average loss: 0.0067, Accuracy: 0.5261
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.8749865873268,
                        "time": 2.753500785000142,
                        "accuracy": 0.52607421875,
                        "total_cost": 915959.4220754137
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6791
Profiling... [512/50176]	Loss: 1.6688
Profiling... [768/50176]	Loss: 1.6497
Profiling... [1024/50176]	Loss: 1.5944
Profiling... [1280/50176]	Loss: 1.7784
Profiling... [1536/50176]	Loss: 1.7762
Profiling... [1792/50176]	Loss: 1.6698
Profiling... [2048/50176]	Loss: 1.6164
Profiling... [2304/50176]	Loss: 1.5942
Profiling... [2560/50176]	Loss: 1.6252
Profiling... [2816/50176]	Loss: 1.6599
Profiling... [3072/50176]	Loss: 1.6039
Profiling... [3328/50176]	Loss: 1.7268
Profile done
epoch 1 train time consumed: 10.02s
Validation Epoch: 8, Average loss: 0.0067, Accuracy: 0.5258
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.85128018213521,
                        "time": 7.003909472000487,
                        "accuracy": 0.52578125,
                        "total_cost": 2331167.491423639
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6117
Profiling... [512/50176]	Loss: 1.7736
Profiling... [768/50176]	Loss: 1.6544
Profiling... [1024/50176]	Loss: 1.6342
Profiling... [1280/50176]	Loss: 1.6913
Profiling... [1536/50176]	Loss: 1.8166
Profiling... [1792/50176]	Loss: 1.8274
Profiling... [2048/50176]	Loss: 1.9086
Profiling... [2304/50176]	Loss: 1.8208
Profiling... [2560/50176]	Loss: 1.8741
Profiling... [2816/50176]	Loss: 1.7301
Profiling... [3072/50176]	Loss: 1.7805
Profiling... [3328/50176]	Loss: 1.7187
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 8, Average loss: 0.0073, Accuracy: 0.4880
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.81032625058168,
                        "time": 2.410475502999361,
                        "accuracy": 0.48798828125,
                        "total_cost": 864433.0801230449
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7291
Profiling... [512/50176]	Loss: 1.7060
Profiling... [768/50176]	Loss: 1.8951
Profiling... [1024/50176]	Loss: 1.8385
Profiling... [1280/50176]	Loss: 1.6973
Profiling... [1536/50176]	Loss: 1.6794
Profiling... [1792/50176]	Loss: 1.8378
Profiling... [2048/50176]	Loss: 1.6079
Profiling... [2304/50176]	Loss: 1.6213
Profiling... [2560/50176]	Loss: 1.7504
Profiling... [2816/50176]	Loss: 1.7368
Profiling... [3072/50176]	Loss: 1.8197
Profiling... [3328/50176]	Loss: 1.5819
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 8, Average loss: 0.0078, Accuracy: 0.4707
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.83166943445573,
                        "time": 2.438741949999894,
                        "accuracy": 0.470703125,
                        "total_cost": 906685.803817388
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5858
Profiling... [512/50176]	Loss: 1.7520
Profiling... [768/50176]	Loss: 1.6747
Profiling... [1024/50176]	Loss: 1.6527
Profiling... [1280/50176]	Loss: 1.7332
Profiling... [1536/50176]	Loss: 1.5537
Profiling... [1792/50176]	Loss: 1.7487
Profiling... [2048/50176]	Loss: 1.7066
Profiling... [2304/50176]	Loss: 1.6974
Profiling... [2560/50176]	Loss: 1.8403
Profiling... [2816/50176]	Loss: 1.7806
Profiling... [3072/50176]	Loss: 1.7953
Profiling... [3328/50176]	Loss: 1.7595
Profile done
epoch 1 train time consumed: 4.21s
Validation Epoch: 8, Average loss: 0.0096, Accuracy: 0.3984
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.839948692844,
                        "time": 2.7455257889996574,
                        "accuracy": 0.3984375,
                        "total_cost": 1205877.9935998495
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5747
Profiling... [512/50176]	Loss: 1.6103
Profiling... [768/50176]	Loss: 1.8039
Profiling... [1024/50176]	Loss: 1.6875
Profiling... [1280/50176]	Loss: 1.7276
Profiling... [1536/50176]	Loss: 1.6547
Profiling... [1792/50176]	Loss: 1.6684
Profiling... [2048/50176]	Loss: 1.8036
Profiling... [2304/50176]	Loss: 1.7120
Profiling... [2560/50176]	Loss: 1.7886
Profiling... [2816/50176]	Loss: 1.8951
Profiling... [3072/50176]	Loss: 1.7464
Profiling... [3328/50176]	Loss: 1.7194
Profile done
epoch 1 train time consumed: 9.82s
Validation Epoch: 8, Average loss: 0.0104, Accuracy: 0.3729
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.81400242322863,
                        "time": 7.193667784000354,
                        "accuracy": 0.37294921875,
                        "total_cost": 3375504.757509462
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7708
Profiling... [512/50176]	Loss: 1.6421
Profiling... [768/50176]	Loss: 1.6899
Profiling... [1024/50176]	Loss: 1.7636
Profiling... [1280/50176]	Loss: 1.6660
Profiling... [1536/50176]	Loss: 1.6236
Profiling... [1792/50176]	Loss: 1.6857
Profiling... [2048/50176]	Loss: 1.7294
Profiling... [2304/50176]	Loss: 1.7546
Profiling... [2560/50176]	Loss: 1.6778
Profiling... [2816/50176]	Loss: 1.8115
Profiling... [3072/50176]	Loss: 1.6746
Profiling... [3328/50176]	Loss: 1.9816
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 8, Average loss: 0.0090, Accuracy: 0.4314
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.77775618273235,
                        "time": 2.4075412049996885,
                        "accuracy": 0.4314453125,
                        "total_cost": 976530.9731460937
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5903
Profiling... [512/50176]	Loss: 1.7051
Profiling... [768/50176]	Loss: 1.7630
Profiling... [1024/50176]	Loss: 1.7685
Profiling... [1280/50176]	Loss: 1.6078
Profiling... [1536/50176]	Loss: 1.6427
Profiling... [1792/50176]	Loss: 1.7057
Profiling... [2048/50176]	Loss: 1.7072
Profiling... [2304/50176]	Loss: 1.6053
Profiling... [2560/50176]	Loss: 1.8793
Profiling... [2816/50176]	Loss: 1.6883
Profiling... [3072/50176]	Loss: 1.8334
Profiling... [3328/50176]	Loss: 1.7074
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 8, Average loss: 0.0080, Accuracy: 0.4660
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.79690898720206,
                        "time": 2.4356706500002474,
                        "accuracy": 0.466015625,
                        "total_cost": 914652.5156748624
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6765
Profiling... [512/50176]	Loss: 1.6118
Profiling... [768/50176]	Loss: 1.6735
Profiling... [1024/50176]	Loss: 1.9676
Profiling... [1280/50176]	Loss: 1.7922
Profiling... [1536/50176]	Loss: 1.9263
Profiling... [1792/50176]	Loss: 1.6641
Profiling... [2048/50176]	Loss: 1.7894
Profiling... [2304/50176]	Loss: 1.5882
Profiling... [2560/50176]	Loss: 1.7038
Profiling... [2816/50176]	Loss: 1.6221
Profiling... [3072/50176]	Loss: 1.6259
Profiling... [3328/50176]	Loss: 1.6663
Profile done
epoch 1 train time consumed: 4.28s
Validation Epoch: 8, Average loss: 0.0085, Accuracy: 0.4478
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.80633905628332,
                        "time": 2.7460765880005056,
                        "accuracy": 0.44775390625,
                        "total_cost": 1073275.7351574495
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7188
Profiling... [512/50176]	Loss: 1.6228
Profiling... [768/50176]	Loss: 1.7113
Profiling... [1024/50176]	Loss: 1.6892
Profiling... [1280/50176]	Loss: 1.7273
Profiling... [1536/50176]	Loss: 1.7163
Profiling... [1792/50176]	Loss: 1.6787
Profiling... [2048/50176]	Loss: 1.7695
Profiling... [2304/50176]	Loss: 1.7382
Profiling... [2560/50176]	Loss: 2.0067
Profiling... [2816/50176]	Loss: 1.7303
Profiling... [3072/50176]	Loss: 1.7062
Profiling... [3328/50176]	Loss: 1.6859
Profile done
epoch 1 train time consumed: 9.98s
Validation Epoch: 8, Average loss: 0.0078, Accuracy: 0.4651
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.78245821246458,
                        "time": 6.857746433000102,
                        "accuracy": 0.46513671875,
                        "total_cost": 2580113.7115129502
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6471
Profiling... [512/50176]	Loss: 1.7444
Profiling... [768/50176]	Loss: 1.7150
Profiling... [1024/50176]	Loss: 1.8462
Profiling... [1280/50176]	Loss: 1.7707
Profiling... [1536/50176]	Loss: 1.7533
Profiling... [1792/50176]	Loss: 1.8646
Profiling... [2048/50176]	Loss: 1.6092
Profiling... [2304/50176]	Loss: 1.7620
Profiling... [2560/50176]	Loss: 1.6770
Profiling... [2816/50176]	Loss: 1.8387
Profiling... [3072/50176]	Loss: 1.8383
Profiling... [3328/50176]	Loss: 1.7057
Profile done
epoch 1 train time consumed: 4.13s
Validation Epoch: 8, Average loss: 0.0080, Accuracy: 0.4556
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.74624240356533,
                        "time": 2.3922197609999785,
                        "accuracy": 0.45556640625,
                        "total_cost": 918940.5812887377
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6907
Profiling... [512/50176]	Loss: 1.6973
Profiling... [768/50176]	Loss: 1.7985
Profiling... [1024/50176]	Loss: 1.5866
Profiling... [1280/50176]	Loss: 1.7251
Profiling... [1536/50176]	Loss: 1.7506
Profiling... [1792/50176]	Loss: 1.6208
Profiling... [2048/50176]	Loss: 1.8265
Profiling... [2304/50176]	Loss: 1.7754
Profiling... [2560/50176]	Loss: 1.7448
Profiling... [2816/50176]	Loss: 1.7478
Profiling... [3072/50176]	Loss: 1.9502
Profiling... [3328/50176]	Loss: 1.7780
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 8, Average loss: 0.0080, Accuracy: 0.4615
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.76502640784807,
                        "time": 2.4451780829995187,
                        "accuracy": 0.4615234375,
                        "total_cost": 927160.2041335457
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7094
Profiling... [512/50176]	Loss: 1.5699
Profiling... [768/50176]	Loss: 1.5873
Profiling... [1024/50176]	Loss: 1.7500
Profiling... [1280/50176]	Loss: 1.7161
Profiling... [1536/50176]	Loss: 1.7672
Profiling... [1792/50176]	Loss: 1.7464
Profiling... [2048/50176]	Loss: 1.8423
Profiling... [2304/50176]	Loss: 1.7670
Profiling... [2560/50176]	Loss: 1.6037
Profiling... [2816/50176]	Loss: 1.7905
Profiling... [3072/50176]	Loss: 1.7627
Profiling... [3328/50176]	Loss: 1.7604
Profile done
epoch 1 train time consumed: 4.24s
Validation Epoch: 8, Average loss: 0.0080, Accuracy: 0.4574
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.77358757212552,
                        "time": 2.736271393999232,
                        "accuracy": 0.457421875,
                        "total_cost": 1046839.9526145653
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5897
Profiling... [512/50176]	Loss: 1.7793
Profiling... [768/50176]	Loss: 1.8242
Profiling... [1024/50176]	Loss: 1.5916
Profiling... [1280/50176]	Loss: 1.6224
Profiling... [1536/50176]	Loss: 1.7529
Profiling... [1792/50176]	Loss: 1.6738
Profiling... [2048/50176]	Loss: 1.6797
Profiling... [2304/50176]	Loss: 1.8144
Profiling... [2560/50176]	Loss: 1.6066
Profiling... [2816/50176]	Loss: 1.7436
Profiling... [3072/50176]	Loss: 1.9233
Profiling... [3328/50176]	Loss: 1.7307
Profile done
epoch 1 train time consumed: 9.82s
Validation Epoch: 8, Average loss: 0.0083, Accuracy: 0.4446
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.75049129290485,
                        "time": 7.0626970949997485,
                        "accuracy": 0.44462890625,
                        "total_cost": 2779783.262516923
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6660
Profiling... [512/50176]	Loss: 1.6744
Profiling... [768/50176]	Loss: 1.9259
Profiling... [1024/50176]	Loss: 1.8615
Profiling... [1280/50176]	Loss: 1.7595
Profiling... [1536/50176]	Loss: 1.8381
Profiling... [1792/50176]	Loss: 1.7512
Profiling... [2048/50176]	Loss: 2.1464
Profiling... [2304/50176]	Loss: 1.9374
Profiling... [2560/50176]	Loss: 1.8672
Profiling... [2816/50176]	Loss: 1.9041
Profiling... [3072/50176]	Loss: 2.0384
Profiling... [3328/50176]	Loss: 1.7501
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 8, Average loss: 0.0122, Accuracy: 0.3070
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.7057738151271,
                        "time": 2.404529856999943,
                        "accuracy": 0.30703125,
                        "total_cost": 1370520.8345241407
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7376
Profiling... [512/50176]	Loss: 1.7328
Profiling... [768/50176]	Loss: 1.7741
Profiling... [1024/50176]	Loss: 1.7700
Profiling... [1280/50176]	Loss: 1.8405
Profiling... [1536/50176]	Loss: 1.9455
Profiling... [1792/50176]	Loss: 1.6298
Profiling... [2048/50176]	Loss: 1.7519
Profiling... [2304/50176]	Loss: 1.9143
Profiling... [2560/50176]	Loss: 1.9151
Profiling... [2816/50176]	Loss: 1.8561
Profiling... [3072/50176]	Loss: 1.7326
Profiling... [3328/50176]	Loss: 1.8140
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.2459
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.72623769833189,
                        "time": 2.439877396999691,
                        "accuracy": 0.2458984375,
                        "total_cost": 1736402.0235994624
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5999
Profiling... [512/50176]	Loss: 1.5845
Profiling... [768/50176]	Loss: 1.7757
Profiling... [1024/50176]	Loss: 1.8801
Profiling... [1280/50176]	Loss: 1.8172
Profiling... [1536/50176]	Loss: 1.8167
Profiling... [1792/50176]	Loss: 1.6668
Profiling... [2048/50176]	Loss: 1.8008
Profiling... [2304/50176]	Loss: 2.0646
Profiling... [2560/50176]	Loss: 1.8156
Profiling... [2816/50176]	Loss: 1.8545
Profiling... [3072/50176]	Loss: 1.8319
Profiling... [3328/50176]	Loss: 1.8283
Profile done
epoch 1 train time consumed: 4.33s
Validation Epoch: 8, Average loss: 0.0115, Accuracy: 0.3155
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.73519682220774,
                        "time": 2.7396197219995884,
                        "accuracy": 0.31552734375,
                        "total_cost": 1519467.2057639314
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6100
Profiling... [512/50176]	Loss: 1.7649
Profiling... [768/50176]	Loss: 1.8942
Profiling... [1024/50176]	Loss: 1.8247
Profiling... [1280/50176]	Loss: 2.0894
Profiling... [1536/50176]	Loss: 1.8014
Profiling... [1792/50176]	Loss: 1.9780
Profiling... [2048/50176]	Loss: 1.7536
Profiling... [2304/50176]	Loss: 1.7211
Profiling... [2560/50176]	Loss: 1.7895
Profiling... [2816/50176]	Loss: 1.8794
Profiling... [3072/50176]	Loss: 1.7322
Profiling... [3328/50176]	Loss: 1.9074
Profile done
epoch 1 train time consumed: 9.98s
Validation Epoch: 8, Average loss: 0.0188, Accuracy: 0.1986
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.71176647827362,
                        "time": 6.957718940000632,
                        "accuracy": 0.1986328125,
                        "total_cost": 6129907.738682956
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8212
Profiling... [512/50176]	Loss: 1.6872
Profiling... [768/50176]	Loss: 1.7894
Profiling... [1024/50176]	Loss: 2.0973
Profiling... [1280/50176]	Loss: 1.7716
Profiling... [1536/50176]	Loss: 1.8241
Profiling... [1792/50176]	Loss: 1.8890
Profiling... [2048/50176]	Loss: 1.8576
Profiling... [2304/50176]	Loss: 1.9481
Profiling... [2560/50176]	Loss: 1.8231
Profiling... [2816/50176]	Loss: 1.9673
Profiling... [3072/50176]	Loss: 1.7995
Profiling... [3328/50176]	Loss: 1.8712
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 8, Average loss: 0.0131, Accuracy: 0.3135
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.6703027199368,
                        "time": 2.4029276889996254,
                        "accuracy": 0.3134765625,
                        "total_cost": 1341447.4824571118
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4695
Profiling... [512/50176]	Loss: 1.6523
Profiling... [768/50176]	Loss: 1.6663
Profiling... [1024/50176]	Loss: 1.9499
Profiling... [1280/50176]	Loss: 1.8676
Profiling... [1536/50176]	Loss: 1.9186
Profiling... [1792/50176]	Loss: 2.0108
Profiling... [2048/50176]	Loss: 1.6839
Profiling... [2304/50176]	Loss: 1.8739
Profiling... [2560/50176]	Loss: 1.8399
Profiling... [2816/50176]	Loss: 1.7974
Profiling... [3072/50176]	Loss: 1.9268
Profiling... [3328/50176]	Loss: 1.8413
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 8, Average loss: 0.0145, Accuracy: 0.2710
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.6902234759341,
                        "time": 2.435862203999932,
                        "accuracy": 0.27099609375,
                        "total_cost": 1572996.421465902
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6830
Profiling... [512/50176]	Loss: 1.7161
Profiling... [768/50176]	Loss: 1.7394
Profiling... [1024/50176]	Loss: 2.0727
Profiling... [1280/50176]	Loss: 1.6864
Profiling... [1536/50176]	Loss: 1.8608
Profiling... [1792/50176]	Loss: 1.8307
Profiling... [2048/50176]	Loss: 1.8651
Profiling... [2304/50176]	Loss: 2.0403
Profiling... [2560/50176]	Loss: 1.9593
Profiling... [2816/50176]	Loss: 1.9504
Profiling... [3072/50176]	Loss: 1.8531
Profiling... [3328/50176]	Loss: 2.0725
Profile done
epoch 1 train time consumed: 4.28s
Validation Epoch: 8, Average loss: 0.0144, Accuracy: 0.2638
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.69862616767652,
                        "time": 2.7487860390001515,
                        "accuracy": 0.26376953125,
                        "total_cost": 1823704.0288368275
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.4508
Profiling... [512/50176]	Loss: 1.6514
Profiling... [768/50176]	Loss: 2.0752
Profiling... [1024/50176]	Loss: 1.8387
Profiling... [1280/50176]	Loss: 1.9356
Profiling... [1536/50176]	Loss: 1.9130
Profiling... [1792/50176]	Loss: 1.7793
Profiling... [2048/50176]	Loss: 1.9743
Profiling... [2304/50176]	Loss: 1.7474
Profiling... [2560/50176]	Loss: 2.0591
Profiling... [2816/50176]	Loss: 1.8707
Profiling... [3072/50176]	Loss: 1.9859
Profiling... [3328/50176]	Loss: 1.7416
Profile done
epoch 1 train time consumed: 9.87s
Validation Epoch: 8, Average loss: 0.0107, Accuracy: 0.3301
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.67556207450085,
                        "time": 7.090416205999645,
                        "accuracy": 0.330078125,
                        "total_cost": 3759179.2429441903
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6205
Profiling... [512/50176]	Loss: 1.6348
Profiling... [768/50176]	Loss: 1.8042
Profiling... [1024/50176]	Loss: 1.8520
Profiling... [1280/50176]	Loss: 1.7661
Profiling... [1536/50176]	Loss: 1.8447
Profiling... [1792/50176]	Loss: 1.8347
Profiling... [2048/50176]	Loss: 1.8571
Profiling... [2304/50176]	Loss: 1.9108
Profiling... [2560/50176]	Loss: 1.8622
Profiling... [2816/50176]	Loss: 1.8878
Profiling... [3072/50176]	Loss: 1.9702
Profiling... [3328/50176]	Loss: 1.8181
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 8, Average loss: 0.0096, Accuracy: 0.3802
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.63774199445275,
                        "time": 2.3997103589999824,
                        "accuracy": 0.38017578125,
                        "total_cost": 1104618.7935597145
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7404
Profiling... [512/50176]	Loss: 1.8325
Profiling... [768/50176]	Loss: 1.7855
Profiling... [1024/50176]	Loss: 2.0703
Profiling... [1280/50176]	Loss: 1.7985
Profiling... [1536/50176]	Loss: 2.0501
Profiling... [1792/50176]	Loss: 1.9743
Profiling... [2048/50176]	Loss: 1.8462
Profiling... [2304/50176]	Loss: 1.8043
Profiling... [2560/50176]	Loss: 1.9483
Profiling... [2816/50176]	Loss: 1.7186
Profiling... [3072/50176]	Loss: 1.9406
Profiling... [3328/50176]	Loss: 1.8413
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 8, Average loss: 0.0141, Accuracy: 0.2842
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.65462344355383,
                        "time": 2.4280056329998843,
                        "accuracy": 0.2841796875,
                        "total_cost": 1495184.2248576607
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7827
Profiling... [512/50176]	Loss: 1.8124
Profiling... [768/50176]	Loss: 1.9663
Profiling... [1024/50176]	Loss: 1.8569
Profiling... [1280/50176]	Loss: 1.8250
Profiling... [1536/50176]	Loss: 1.7749
Profiling... [1792/50176]	Loss: 1.7472
Profiling... [2048/50176]	Loss: 1.8315
Profiling... [2304/50176]	Loss: 1.7464
Profiling... [2560/50176]	Loss: 1.7705
Profiling... [2816/50176]	Loss: 1.9376
Profiling... [3072/50176]	Loss: 1.8175
Profiling... [3328/50176]	Loss: 1.8435
Profile done
epoch 1 train time consumed: 4.22s
Validation Epoch: 8, Average loss: 0.0172, Accuracy: 0.2213
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.66532055538384,
                        "time": 2.7345728159998544,
                        "accuracy": 0.2212890625,
                        "total_cost": 2162557.1431031507
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5866
Profiling... [512/50176]	Loss: 1.8470
Profiling... [768/50176]	Loss: 1.8139
Profiling... [1024/50176]	Loss: 1.8541
Profiling... [1280/50176]	Loss: 1.8758
Profiling... [1536/50176]	Loss: 1.8835
Profiling... [1792/50176]	Loss: 1.8501
Profiling... [2048/50176]	Loss: 1.8861
Profiling... [2304/50176]	Loss: 1.8843
Profiling... [2560/50176]	Loss: 1.8269
Profiling... [2816/50176]	Loss: 2.0001
Profiling... [3072/50176]	Loss: 1.9372
Profiling... [3328/50176]	Loss: 1.9878
Profile done
epoch 1 train time consumed: 9.79s
Validation Epoch: 8, Average loss: 0.0111, Accuracy: 0.3469
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.64278425992137,
                        "time": 7.0041212179994545,
                        "accuracy": 0.346875,
                        "total_cost": 3533610.7045763014
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7380
Profiling... [1024/50176]	Loss: 1.6390
Profiling... [1536/50176]	Loss: 1.5784
Profiling... [2048/50176]	Loss: 1.6855
Profiling... [2560/50176]	Loss: 1.6321
Profiling... [3072/50176]	Loss: 1.6988
Profiling... [3584/50176]	Loss: 1.6788
Profiling... [4096/50176]	Loss: 1.6302
Profiling... [4608/50176]	Loss: 1.6034
Profiling... [5120/50176]	Loss: 1.7540
Profiling... [5632/50176]	Loss: 1.6898
Profiling... [6144/50176]	Loss: 1.6794
Profiling... [6656/50176]	Loss: 1.7678
Profile done
epoch 1 train time consumed: 6.89s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5263
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.62493442557289,
                        "time": 4.54906260000007,
                        "accuracy": 0.52626953125,
                        "total_cost": 1512696.2663203054
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6883
Profiling... [1024/50176]	Loss: 1.5915
Profiling... [1536/50176]	Loss: 1.7068
Profiling... [2048/50176]	Loss: 1.6679
Profiling... [2560/50176]	Loss: 1.7145
Profiling... [3072/50176]	Loss: 1.6399
Profiling... [3584/50176]	Loss: 1.7132
Profiling... [4096/50176]	Loss: 1.9262
Profiling... [4608/50176]	Loss: 1.6598
Profiling... [5120/50176]	Loss: 1.5726
Profiling... [5632/50176]	Loss: 1.5964
Profiling... [6144/50176]	Loss: 1.5924
Profiling... [6656/50176]	Loss: 1.6527
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5221
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.65718343136187,
                        "time": 4.666800044000411,
                        "accuracy": 0.5220703125,
                        "total_cost": 1564329.532145293
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7514
Profiling... [1024/50176]	Loss: 1.6860
Profiling... [1536/50176]	Loss: 1.6074
Profiling... [2048/50176]	Loss: 1.8307
Profiling... [2560/50176]	Loss: 1.6566
Profiling... [3072/50176]	Loss: 1.6539
Profiling... [3584/50176]	Loss: 1.5292
Profiling... [4096/50176]	Loss: 1.7084
Profiling... [4608/50176]	Loss: 1.5595
Profiling... [5120/50176]	Loss: 1.6937
Profiling... [5632/50176]	Loss: 1.6203
Profiling... [6144/50176]	Loss: 1.5330
Profiling... [6656/50176]	Loss: 1.6168
Profile done
epoch 1 train time consumed: 7.85s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5288
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.66736594400733,
                        "time": 5.28894206699988,
                        "accuracy": 0.52880859375,
                        "total_cost": 1750283.321156747
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6337
Profiling... [1024/50176]	Loss: 1.6978
Profiling... [1536/50176]	Loss: 1.6397
Profiling... [2048/50176]	Loss: 1.6793
Profiling... [2560/50176]	Loss: 1.6714
Profiling... [3072/50176]	Loss: 1.6779
Profiling... [3584/50176]	Loss: 1.5846
Profiling... [4096/50176]	Loss: 1.6200
Profiling... [4608/50176]	Loss: 1.7375
Profiling... [5120/50176]	Loss: 1.5579
Profiling... [5632/50176]	Loss: 1.6059
Profiling... [6144/50176]	Loss: 1.6906
Profiling... [6656/50176]	Loss: 1.7015
Profile done
epoch 1 train time consumed: 17.79s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5278
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.61142640015258,
                        "time": 13.01923944400005,
                        "accuracy": 0.52783203125,
                        "total_cost": 4316461.995124531
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6029
Profiling... [1024/50176]	Loss: 1.6734
Profiling... [1536/50176]	Loss: 1.7409
Profiling... [2048/50176]	Loss: 1.7034
Profiling... [2560/50176]	Loss: 1.7208
Profiling... [3072/50176]	Loss: 1.5827
Profiling... [3584/50176]	Loss: 1.6486
Profiling... [4096/50176]	Loss: 1.5284
Profiling... [4608/50176]	Loss: 1.6055
Profiling... [5120/50176]	Loss: 1.6392
Profiling... [5632/50176]	Loss: 1.7180
Profiling... [6144/50176]	Loss: 1.6725
Profiling... [6656/50176]	Loss: 1.6340
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5271
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.59965584879123,
                        "time": 4.538159201999406,
                        "accuracy": 0.5271484375,
                        "total_cost": 1506554.5183369648
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6414
Profiling... [1024/50176]	Loss: 1.6435
Profiling... [1536/50176]	Loss: 1.6995
Profiling... [2048/50176]	Loss: 1.7008
Profiling... [2560/50176]	Loss: 1.6723
Profiling... [3072/50176]	Loss: 1.6673
Profiling... [3584/50176]	Loss: 1.6079
Profiling... [4096/50176]	Loss: 1.5719
Profiling... [4608/50176]	Loss: 1.6879
Profiling... [5120/50176]	Loss: 1.6657
Profiling... [5632/50176]	Loss: 1.5328
Profiling... [6144/50176]	Loss: 1.6064
Profiling... [6656/50176]	Loss: 1.6408
Profile done
epoch 1 train time consumed: 7.06s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5243
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.62846674744287,
                        "time": 4.6450051130004795,
                        "accuracy": 0.52431640625,
                        "total_cost": 1550353.7274160662
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6609
Profiling... [1024/50176]	Loss: 1.7155
Profiling... [1536/50176]	Loss: 1.7606
Profiling... [2048/50176]	Loss: 1.6229
Profiling... [2560/50176]	Loss: 1.6992
Profiling... [3072/50176]	Loss: 1.6380
Profiling... [3584/50176]	Loss: 1.6658
Profiling... [4096/50176]	Loss: 1.5691
Profiling... [4608/50176]	Loss: 1.6685
Profiling... [5120/50176]	Loss: 1.7344
Profiling... [5632/50176]	Loss: 1.5482
Profiling... [6144/50176]	Loss: 1.7225
Profiling... [6656/50176]	Loss: 1.6109
Profile done
epoch 1 train time consumed: 7.81s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5285
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.63880186879922,
                        "time": 5.257664018000469,
                        "accuracy": 0.528515625,
                        "total_cost": 1740896.881052631
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.8291
Profiling... [1024/50176]	Loss: 1.7127
Profiling... [1536/50176]	Loss: 1.7203
Profiling... [2048/50176]	Loss: 1.6083
Profiling... [2560/50176]	Loss: 1.6913
Profiling... [3072/50176]	Loss: 1.6145
Profiling... [3584/50176]	Loss: 1.6913
Profiling... [4096/50176]	Loss: 1.6366
Profiling... [4608/50176]	Loss: 1.6966
Profiling... [5120/50176]	Loss: 1.6155
Profiling... [5632/50176]	Loss: 1.7610
Profiling... [6144/50176]	Loss: 1.5711
Profiling... [6656/50176]	Loss: 1.5690
Profile done
epoch 1 train time consumed: 17.84s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5296
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.58193164324375,
                        "time": 13.078999684999872,
                        "accuracy": 0.52958984375,
                        "total_cost": 4321882.2488511475
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7435
Profiling... [1024/50176]	Loss: 1.6019
Profiling... [1536/50176]	Loss: 1.4812
Profiling... [2048/50176]	Loss: 1.5290
Profiling... [2560/50176]	Loss: 1.5743
Profiling... [3072/50176]	Loss: 1.7038
Profiling... [3584/50176]	Loss: 1.5553
Profiling... [4096/50176]	Loss: 1.7068
Profiling... [4608/50176]	Loss: 1.7012
Profiling... [5120/50176]	Loss: 1.6357
Profiling... [5632/50176]	Loss: 1.6456
Profiling... [6144/50176]	Loss: 1.6616
Profiling... [6656/50176]	Loss: 1.7759
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5300
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.56596273054873,
                        "time": 4.552860786000565,
                        "accuracy": 0.52998046875,
                        "total_cost": 1503358.4906049403
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7344
Profiling... [1024/50176]	Loss: 1.5607
Profiling... [1536/50176]	Loss: 1.6654
Profiling... [2048/50176]	Loss: 1.6202
Profiling... [2560/50176]	Loss: 1.5566
Profiling... [3072/50176]	Loss: 1.6617
Profiling... [3584/50176]	Loss: 1.6507
Profiling... [4096/50176]	Loss: 1.7055
Profiling... [4608/50176]	Loss: 1.6444
Profiling... [5120/50176]	Loss: 1.8038
Profiling... [5632/50176]	Loss: 1.5355
Profiling... [6144/50176]	Loss: 1.7005
Profiling... [6656/50176]	Loss: 1.5300
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5293
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.59698647894623,
                        "time": 4.645432958000129,
                        "accuracy": 0.529296875,
                        "total_cost": 1535906.9853756884
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6340
Profiling... [1024/50176]	Loss: 1.6673
Profiling... [1536/50176]	Loss: 1.5414
Profiling... [2048/50176]	Loss: 1.5384
Profiling... [2560/50176]	Loss: 1.6311
Profiling... [3072/50176]	Loss: 1.6076
Profiling... [3584/50176]	Loss: 1.7553
Profiling... [4096/50176]	Loss: 1.6891
Profiling... [4608/50176]	Loss: 1.6921
Profiling... [5120/50176]	Loss: 1.7826
Profiling... [5632/50176]	Loss: 1.6619
Profiling... [6144/50176]	Loss: 1.6632
Profiling... [6656/50176]	Loss: 1.6347
Profile done
epoch 1 train time consumed: 7.76s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5265
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.60848723252604,
                        "time": 5.270650168999964,
                        "accuracy": 0.52646484375,
                        "total_cost": 1751995.0107304647
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6434
Profiling... [1024/50176]	Loss: 1.5770
Profiling... [1536/50176]	Loss: 1.6307
Profiling... [2048/50176]	Loss: 1.6409
Profiling... [2560/50176]	Loss: 1.6147
Profiling... [3072/50176]	Loss: 1.6531
Profiling... [3584/50176]	Loss: 1.5747
Profiling... [4096/50176]	Loss: 1.6590
Profiling... [4608/50176]	Loss: 1.6910
Profiling... [5120/50176]	Loss: 1.6206
Profiling... [5632/50176]	Loss: 1.4866
Profiling... [6144/50176]	Loss: 1.7150
Profiling... [6656/50176]	Loss: 1.7600
Profile done
epoch 1 train time consumed: 17.87s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.5250
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.55353338230137,
                        "time": 13.033658741000181,
                        "accuracy": 0.525,
                        "total_cost": 4344552.913666727
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6023
Profiling... [1024/50176]	Loss: 1.5416
Profiling... [1536/50176]	Loss: 1.7189
Profiling... [2048/50176]	Loss: 1.7204
Profiling... [2560/50176]	Loss: 1.7488
Profiling... [3072/50176]	Loss: 1.6825
Profiling... [3584/50176]	Loss: 1.6642
Profiling... [4096/50176]	Loss: 1.8357
Profiling... [4608/50176]	Loss: 1.7414
Profiling... [5120/50176]	Loss: 1.7465
Profiling... [5632/50176]	Loss: 1.7125
Profiling... [6144/50176]	Loss: 1.7537
Profiling... [6656/50176]	Loss: 1.7133
Profile done
epoch 1 train time consumed: 6.87s
Validation Epoch: 8, Average loss: 0.0037, Accuracy: 0.4867
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.5350399484368,
                        "time": 4.5413179810002475,
                        "accuracy": 0.48671875,
                        "total_cost": 1632833.4313708756
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5699
Profiling... [1024/50176]	Loss: 1.5684
Profiling... [1536/50176]	Loss: 1.6491
Profiling... [2048/50176]	Loss: 1.7684
Profiling... [2560/50176]	Loss: 1.6138
Profiling... [3072/50176]	Loss: 1.7930
Profiling... [3584/50176]	Loss: 1.6830
Profiling... [4096/50176]	Loss: 1.7421
Profiling... [4608/50176]	Loss: 1.7594
Profiling... [5120/50176]	Loss: 1.7244
Profiling... [5632/50176]	Loss: 1.9349
Profiling... [6144/50176]	Loss: 1.5840
Profiling... [6656/50176]	Loss: 1.6344
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 8, Average loss: 0.0055, Accuracy: 0.3504
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.56705651231158,
                        "time": 4.652667888999531,
                        "accuracy": 0.350390625,
                        "total_cost": 2323740.484138004
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7732
Profiling... [1024/50176]	Loss: 1.5386
Profiling... [1536/50176]	Loss: 1.6911
Profiling... [2048/50176]	Loss: 1.7122
Profiling... [2560/50176]	Loss: 1.6003
Profiling... [3072/50176]	Loss: 1.5784
Profiling... [3584/50176]	Loss: 1.6960
Profiling... [4096/50176]	Loss: 1.6972
Profiling... [4608/50176]	Loss: 1.6454
Profiling... [5120/50176]	Loss: 1.6373
Profiling... [5632/50176]	Loss: 1.7079
Profiling... [6144/50176]	Loss: 1.6907
Profiling... [6656/50176]	Loss: 1.8649
Profile done
epoch 1 train time consumed: 7.86s
Validation Epoch: 8, Average loss: 0.0039, Accuracy: 0.4659
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.57621344760486,
                        "time": 5.299374550999346,
                        "accuracy": 0.46591796875,
                        "total_cost": 1990458.85461975
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6238
Profiling... [1024/50176]	Loss: 1.7132
Profiling... [1536/50176]	Loss: 1.6524
Profiling... [2048/50176]	Loss: 1.6838
Profiling... [2560/50176]	Loss: 1.7395
Profiling... [3072/50176]	Loss: 1.6921
Profiling... [3584/50176]	Loss: 1.5982
Profiling... [4096/50176]	Loss: 1.7308
Profiling... [4608/50176]	Loss: 1.7213
Profiling... [5120/50176]	Loss: 1.6768
Profiling... [5632/50176]	Loss: 1.7432
Profiling... [6144/50176]	Loss: 1.7526
Profiling... [6656/50176]	Loss: 1.6887
Profile done
epoch 1 train time consumed: 17.84s
Validation Epoch: 8, Average loss: 0.0047, Accuracy: 0.4083
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.52145962214601,
                        "time": 13.048243166000248,
                        "accuracy": 0.40830078125,
                        "total_cost": 5592550.048665975
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.5756
Profiling... [1024/50176]	Loss: 1.7187
Profiling... [1536/50176]	Loss: 1.6112
Profiling... [2048/50176]	Loss: 1.6608
Profiling... [2560/50176]	Loss: 1.7865
Profiling... [3072/50176]	Loss: 1.7509
Profiling... [3584/50176]	Loss: 1.6339
Profiling... [4096/50176]	Loss: 1.6304
Profiling... [4608/50176]	Loss: 1.7688
Profiling... [5120/50176]	Loss: 1.7086
Profiling... [5632/50176]	Loss: 1.7135
Profiling... [6144/50176]	Loss: 1.5810
Profiling... [6656/50176]	Loss: 1.6724
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 8, Average loss: 0.0040, Accuracy: 0.4512
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.50347687474235,
                        "time": 4.545741966000605,
                        "accuracy": 0.451171875,
                        "total_cost": 1763196.883782053
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7363
Profiling... [1024/50176]	Loss: 1.7676
Profiling... [1536/50176]	Loss: 1.7190
Profiling... [2048/50176]	Loss: 1.6974
Profiling... [2560/50176]	Loss: 1.7134
Profiling... [3072/50176]	Loss: 1.6232
Profiling... [3584/50176]	Loss: 1.6709
Profiling... [4096/50176]	Loss: 1.7295
Profiling... [4608/50176]	Loss: 1.8235
Profiling... [5120/50176]	Loss: 1.7720
Profiling... [5632/50176]	Loss: 1.6440
Profiling... [6144/50176]	Loss: 1.6930
Profiling... [6656/50176]	Loss: 1.6505
Profile done
epoch 1 train time consumed: 7.02s
Validation Epoch: 8, Average loss: 0.0040, Accuracy: 0.4701
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.53521780647144,
                        "time": 4.667458230000193,
                        "accuracy": 0.4701171875,
                        "total_cost": 1737450.1761861956
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7028
Profiling... [1024/50176]	Loss: 1.7313
Profiling... [1536/50176]	Loss: 1.6316
Profiling... [2048/50176]	Loss: 1.7391
Profiling... [2560/50176]	Loss: 1.7433
Profiling... [3072/50176]	Loss: 1.8730
Profiling... [3584/50176]	Loss: 1.6238
Profiling... [4096/50176]	Loss: 1.6022
Profiling... [4608/50176]	Loss: 1.6412
Profiling... [5120/50176]	Loss: 1.7289
Profiling... [5632/50176]	Loss: 1.7702
Profiling... [6144/50176]	Loss: 1.7118
Profiling... [6656/50176]	Loss: 1.7746
Profile done
epoch 1 train time consumed: 7.78s
Validation Epoch: 8, Average loss: 0.0060, Accuracy: 0.3392
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.54405818387721,
                        "time": 5.260812706999786,
                        "accuracy": 0.33916015625,
                        "total_cost": 2714476.3521288848
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7238
Profiling... [1024/50176]	Loss: 1.7316
Profiling... [1536/50176]	Loss: 1.6361
Profiling... [2048/50176]	Loss: 1.6960
Profiling... [2560/50176]	Loss: 1.6301
Profiling... [3072/50176]	Loss: 1.6702
Profiling... [3584/50176]	Loss: 1.6631
Profiling... [4096/50176]	Loss: 1.6997
Profiling... [4608/50176]	Loss: 1.7455
Profiling... [5120/50176]	Loss: 1.6997
Profiling... [5632/50176]	Loss: 1.7108
Profiling... [6144/50176]	Loss: 1.7803
Profiling... [6656/50176]	Loss: 1.7304
Profile done
epoch 1 train time consumed: 17.83s
Validation Epoch: 8, Average loss: 0.0038, Accuracy: 0.4739
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.48973683077774,
                        "time": 13.048241401000269,
                        "accuracy": 0.47392578125,
                        "total_cost": 4818143.12602359
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7859
Profiling... [1024/50176]	Loss: 1.6564
Profiling... [1536/50176]	Loss: 1.6265
Profiling... [2048/50176]	Loss: 1.6641
Profiling... [2560/50176]	Loss: 1.6724
Profiling... [3072/50176]	Loss: 1.7357
Profiling... [3584/50176]	Loss: 1.7643
Profiling... [4096/50176]	Loss: 1.7637
Profiling... [4608/50176]	Loss: 1.7442
Profiling... [5120/50176]	Loss: 1.5520
Profiling... [5632/50176]	Loss: 1.6165
Profiling... [6144/50176]	Loss: 1.7886
Profiling... [6656/50176]	Loss: 1.6907
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 8, Average loss: 0.0038, Accuracy: 0.4832
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.47318428915281,
                        "time": 4.541035930999897,
                        "accuracy": 0.483203125,
                        "total_cost": 1644611.2345092592
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6131
Profiling... [1024/50176]	Loss: 1.7267
Profiling... [1536/50176]	Loss: 1.7947
Profiling... [2048/50176]	Loss: 1.6711
Profiling... [2560/50176]	Loss: 1.5783
Profiling... [3072/50176]	Loss: 1.7071
Profiling... [3584/50176]	Loss: 1.7106
Profiling... [4096/50176]	Loss: 1.7535
Profiling... [4608/50176]	Loss: 1.7149
Profiling... [5120/50176]	Loss: 1.6817
Profiling... [5632/50176]	Loss: 1.7715
Profiling... [6144/50176]	Loss: 1.6042
Profiling... [6656/50176]	Loss: 1.6768
Profile done
epoch 1 train time consumed: 7.16s
Validation Epoch: 8, Average loss: 0.0041, Accuracy: 0.4580
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.5038348773281,
                        "time": 4.6546900020002795,
                        "accuracy": 0.4580078125,
                        "total_cost": 1778508.4186747335
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6923
Profiling... [1024/50176]	Loss: 1.6068
Profiling... [1536/50176]	Loss: 1.7408
Profiling... [2048/50176]	Loss: 1.7132
Profiling... [2560/50176]	Loss: 1.6791
Profiling... [3072/50176]	Loss: 1.6422
Profiling... [3584/50176]	Loss: 1.7672
Profiling... [4096/50176]	Loss: 1.6491
Profiling... [4608/50176]	Loss: 1.6434
Profiling... [5120/50176]	Loss: 1.8142
Profiling... [5632/50176]	Loss: 1.6860
Profiling... [6144/50176]	Loss: 1.5857
Profiling... [6656/50176]	Loss: 1.7171
Profile done
epoch 1 train time consumed: 7.77s
Validation Epoch: 8, Average loss: 0.0047, Accuracy: 0.4088
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.51091202384504,
                        "time": 5.275383522999618,
                        "accuracy": 0.4087890625,
                        "total_cost": 2258358.163692144
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7297
Profiling... [1024/50176]	Loss: 1.6335
Profiling... [1536/50176]	Loss: 1.7071
Profiling... [2048/50176]	Loss: 1.7734
Profiling... [2560/50176]	Loss: 1.7007
Profiling... [3072/50176]	Loss: 1.6711
Profiling... [3584/50176]	Loss: 1.7636
Profiling... [4096/50176]	Loss: 1.5966
Profiling... [4608/50176]	Loss: 1.7652
Profiling... [5120/50176]	Loss: 1.6934
Profiling... [5632/50176]	Loss: 1.6741
Profiling... [6144/50176]	Loss: 1.6969
Profiling... [6656/50176]	Loss: 1.7043
Profile done
epoch 1 train time consumed: 17.83s
Validation Epoch: 8, Average loss: 0.0038, Accuracy: 0.4748
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.4573226610482,
                        "time": 13.033404862999305,
                        "accuracy": 0.4748046875,
                        "total_cost": 4803755.967604845
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6479
Profiling... [1024/50176]	Loss: 1.6811
Profiling... [1536/50176]	Loss: 1.8511
Profiling... [2048/50176]	Loss: 1.7972
Profiling... [2560/50176]	Loss: 1.8049
Profiling... [3072/50176]	Loss: 1.7820
Profiling... [3584/50176]	Loss: 1.8914
Profiling... [4096/50176]	Loss: 1.7075
Profiling... [4608/50176]	Loss: 1.8951
Profiling... [5120/50176]	Loss: 1.8222
Profiling... [5632/50176]	Loss: 1.8079
Profiling... [6144/50176]	Loss: 1.7685
Profiling... [6656/50176]	Loss: 1.8694
Profile done
epoch 1 train time consumed: 6.93s
Validation Epoch: 8, Average loss: 0.0057, Accuracy: 0.3354
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.44155198390182,
                        "time": 4.525297015000433,
                        "accuracy": 0.33544921875,
                        "total_cost": 2360795.415103574
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6871
Profiling... [1024/50176]	Loss: 1.7056
Profiling... [1536/50176]	Loss: 1.7314
Profiling... [2048/50176]	Loss: 1.6897
Profiling... [2560/50176]	Loss: 1.7220
Profiling... [3072/50176]	Loss: 1.8071
Profiling... [3584/50176]	Loss: 1.9757
Profiling... [4096/50176]	Loss: 1.7845
Profiling... [4608/50176]	Loss: 1.7843
Profiling... [5120/50176]	Loss: 1.6788
Profiling... [5632/50176]	Loss: 1.8920
Profiling... [6144/50176]	Loss: 1.8563
Profiling... [6656/50176]	Loss: 1.7867
Profile done
epoch 1 train time consumed: 7.04s
Validation Epoch: 8, Average loss: 0.0095, Accuracy: 0.1990
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.47276476181376,
                        "time": 4.656040132999806,
                        "accuracy": 0.1990234375,
                        "total_cost": 4094025.4751401627
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5927
Profiling... [1024/50176]	Loss: 1.7481
Profiling... [1536/50176]	Loss: 1.7658
Profiling... [2048/50176]	Loss: 1.7278
Profiling... [2560/50176]	Loss: 1.8031
Profiling... [3072/50176]	Loss: 1.8038
Profiling... [3584/50176]	Loss: 1.7408
Profiling... [4096/50176]	Loss: 1.8382
Profiling... [4608/50176]	Loss: 1.9123
Profiling... [5120/50176]	Loss: 1.7333
Profiling... [5632/50176]	Loss: 1.8232
Profiling... [6144/50176]	Loss: 1.6449
Profiling... [6656/50176]	Loss: 1.8432
Profile done
epoch 1 train time consumed: 7.84s
Validation Epoch: 8, Average loss: 0.0160, Accuracy: 0.0801
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.48237961466249,
                        "time": 5.257022540000435,
                        "accuracy": 0.080078125,
                        "total_cost": 11488517.550830219
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7294
Profiling... [1024/50176]	Loss: 1.7320
Profiling... [1536/50176]	Loss: 1.7368
Profiling... [2048/50176]	Loss: 1.8372
Profiling... [2560/50176]	Loss: 1.9369
Profiling... [3072/50176]	Loss: 1.7416
Profiling... [3584/50176]	Loss: 1.9027
Profiling... [4096/50176]	Loss: 1.7991
Profiling... [4608/50176]	Loss: 1.7589
Profiling... [5120/50176]	Loss: 1.7835
Profiling... [5632/50176]	Loss: 1.8451
Profiling... [6144/50176]	Loss: 1.7808
Profiling... [6656/50176]	Loss: 1.8035
Profile done
epoch 1 train time consumed: 17.85s
Validation Epoch: 8, Average loss: 0.0075, Accuracy: 0.2417
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.42929248066383,
                        "time": 13.02940708099959,
                        "accuracy": 0.24169921875,
                        "total_cost": 9433817.167333843
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4679
Profiling... [1024/50176]	Loss: 1.5938
Profiling... [1536/50176]	Loss: 1.7613
Profiling... [2048/50176]	Loss: 1.7118
Profiling... [2560/50176]	Loss: 1.7964
Profiling... [3072/50176]	Loss: 1.8152
Profiling... [3584/50176]	Loss: 1.9196
Profiling... [4096/50176]	Loss: 1.7660
Profiling... [4608/50176]	Loss: 1.7294
Profiling... [5120/50176]	Loss: 1.7190
Profiling... [5632/50176]	Loss: 1.9107
Profiling... [6144/50176]	Loss: 1.7483
Profiling... [6656/50176]	Loss: 1.8047
Profile done
epoch 1 train time consumed: 7.05s
Validation Epoch: 8, Average loss: 0.0054, Accuracy: 0.3721
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.4169659033779,
                        "time": 4.525187081000695,
                        "accuracy": 0.3720703125,
                        "total_cost": 2128381.9551583324
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7267
Profiling... [1024/50176]	Loss: 1.7670
Profiling... [1536/50176]	Loss: 1.7474
Profiling... [2048/50176]	Loss: 1.8739
Profiling... [2560/50176]	Loss: 1.8088
Profiling... [3072/50176]	Loss: 1.9725
Profiling... [3584/50176]	Loss: 1.7716
Profiling... [4096/50176]	Loss: 1.8763
Profiling... [4608/50176]	Loss: 1.8654
Profiling... [5120/50176]	Loss: 1.6635
Profiling... [5632/50176]	Loss: 1.8521
Profiling... [6144/50176]	Loss: 1.8011
Profiling... [6656/50176]	Loss: 1.7124
Profile done
epoch 1 train time consumed: 7.09s
Validation Epoch: 8, Average loss: 0.0086, Accuracy: 0.2453
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.44910274249644,
                        "time": 4.650284967000516,
                        "accuracy": 0.2453125,
                        "total_cost": 3317400.7407901776
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6493
Profiling... [1024/50176]	Loss: 1.7305
Profiling... [1536/50176]	Loss: 1.9044
Profiling... [2048/50176]	Loss: 1.8040
Profiling... [2560/50176]	Loss: 1.7789
Profiling... [3072/50176]	Loss: 1.8591
Profiling... [3584/50176]	Loss: 1.6712
Profiling... [4096/50176]	Loss: 1.8787
Profiling... [4608/50176]	Loss: 1.7273
Profiling... [5120/50176]	Loss: 1.8758
Profiling... [5632/50176]	Loss: 1.6630
Profiling... [6144/50176]	Loss: 1.7432
Profiling... [6656/50176]	Loss: 1.7558
Profile done
epoch 1 train time consumed: 7.85s
Validation Epoch: 8, Average loss: 0.0044, Accuracy: 0.4103
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.45722740415609,
                        "time": 5.289879991999442,
                        "accuracy": 0.41025390625,
                        "total_cost": 2256478.2065372528
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5634
Profiling... [1024/50176]	Loss: 1.6503
Profiling... [1536/50176]	Loss: 1.7337
Profiling... [2048/50176]	Loss: 1.9476
Profiling... [2560/50176]	Loss: 1.9254
Profiling... [3072/50176]	Loss: 1.7917
Profiling... [3584/50176]	Loss: 1.9287
Profiling... [4096/50176]	Loss: 1.8167
Profiling... [4608/50176]	Loss: 1.8938
Profiling... [5120/50176]	Loss: 1.8138
Profiling... [5632/50176]	Loss: 1.8843
Profiling... [6144/50176]	Loss: 1.7817
Profiling... [6656/50176]	Loss: 1.7963
Profile done
epoch 1 train time consumed: 17.85s
Validation Epoch: 8, Average loss: 0.0083, Accuracy: 0.2192
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.40644428589377,
                        "time": 13.005281344999275,
                        "accuracy": 0.21923828125,
                        "total_cost": 10381053.08251167
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6870
Profiling... [1024/50176]	Loss: 1.6802
Profiling... [1536/50176]	Loss: 1.7365
Profiling... [2048/50176]	Loss: 1.7612
Profiling... [2560/50176]	Loss: 1.7203
Profiling... [3072/50176]	Loss: 1.7973
Profiling... [3584/50176]	Loss: 1.8688
Profiling... [4096/50176]	Loss: 1.7511
Profiling... [4608/50176]	Loss: 1.8279
Profiling... [5120/50176]	Loss: 1.7994
Profiling... [5632/50176]	Loss: 1.6984
Profiling... [6144/50176]	Loss: 1.7804
Profiling... [6656/50176]	Loss: 1.9115
Profile done
epoch 1 train time consumed: 7.04s
Validation Epoch: 8, Average loss: 0.0055, Accuracy: 0.3455
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.39091387292055,
                        "time": 4.549913707999622,
                        "accuracy": 0.3455078125,
                        "total_cost": 2304535.151140566
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6556
Profiling... [1024/50176]	Loss: 1.7149
Profiling... [1536/50176]	Loss: 1.6825
Profiling... [2048/50176]	Loss: 1.8645
Profiling... [2560/50176]	Loss: 1.9093
Profiling... [3072/50176]	Loss: 1.8833
Profiling... [3584/50176]	Loss: 1.7361
Profiling... [4096/50176]	Loss: 1.8317
Profiling... [4608/50176]	Loss: 1.6818
Profiling... [5120/50176]	Loss: 1.8493
Profiling... [5632/50176]	Loss: 1.8449
Profiling... [6144/50176]	Loss: 1.7554
Profiling... [6656/50176]	Loss: 1.8553
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 8, Average loss: 0.0075, Accuracy: 0.2715
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.41998547664808,
                        "time": 4.6530785139993895,
                        "accuracy": 0.271484375,
                        "total_cost": 2999394.495355002
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6027
Profiling... [1024/50176]	Loss: 1.6232
Profiling... [1536/50176]	Loss: 1.7440
Profiling... [2048/50176]	Loss: 1.6862
Profiling... [2560/50176]	Loss: 1.9985
Profiling... [3072/50176]	Loss: 1.8001
Profiling... [3584/50176]	Loss: 1.7809
Profiling... [4096/50176]	Loss: 1.8874
Profiling... [4608/50176]	Loss: 1.7379
Profiling... [5120/50176]	Loss: 1.6021
Profiling... [5632/50176]	Loss: 1.8623
Profiling... [6144/50176]	Loss: 1.8567
Profiling... [6656/50176]	Loss: 1.7835
Profile done
epoch 1 train time consumed: 7.79s
Validation Epoch: 8, Average loss: 0.0050, Accuracy: 0.3933
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.42774380605756,
                        "time": 5.2550625039993974,
                        "accuracy": 0.39326171875,
                        "total_cost": 2338483.239922255
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6634
Profiling... [1024/50176]	Loss: 1.7244
Profiling... [1536/50176]	Loss: 1.7144
Profiling... [2048/50176]	Loss: 1.8386
Profiling... [2560/50176]	Loss: 1.8846
Profiling... [3072/50176]	Loss: 1.8826
Profiling... [3584/50176]	Loss: 1.7870
Profiling... [4096/50176]	Loss: 1.7242
Profiling... [4608/50176]	Loss: 1.6684
Profiling... [5120/50176]	Loss: 1.8986
Profiling... [5632/50176]	Loss: 1.7164
Profiling... [6144/50176]	Loss: 1.9684
Profiling... [6656/50176]	Loss: 1.7804
Profile done
epoch 1 train time consumed: 17.93s
Validation Epoch: 8, Average loss: 0.0079, Accuracy: 0.2219
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.37665533270977,
                        "time": 13.074545946000399,
                        "accuracy": 0.221875,
                        "total_cost": 10312317.929239752
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6764
Profiling... [2048/50176]	Loss: 1.5498
Profiling... [3072/50176]	Loss: 1.7079
Profiling... [4096/50176]	Loss: 1.6967
Profiling... [5120/50176]	Loss: 1.6542
Profiling... [6144/50176]	Loss: 1.6586
Profiling... [7168/50176]	Loss: 1.6461
Profiling... [8192/50176]	Loss: 1.6706
Profiling... [9216/50176]	Loss: 1.6974
Profiling... [10240/50176]	Loss: 1.6298
Profiling... [11264/50176]	Loss: 1.6913
Profiling... [12288/50176]	Loss: 1.6511
Profiling... [13312/50176]	Loss: 1.5846
Profile done
epoch 1 train time consumed: 12.95s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5263
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.39676351073955,
                        "time": 8.86235153800044,
                        "accuracy": 0.52626953125,
                        "total_cost": 2946990.8992571514
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7449
Profiling... [2048/50176]	Loss: 1.6209
Profiling... [3072/50176]	Loss: 1.6095
Profiling... [4096/50176]	Loss: 1.6770
Profiling... [5120/50176]	Loss: 1.5984
Profiling... [6144/50176]	Loss: 1.6951
Profiling... [7168/50176]	Loss: 1.6104
Profiling... [8192/50176]	Loss: 1.5298
Profiling... [9216/50176]	Loss: 1.6731
Profiling... [10240/50176]	Loss: 1.5726
Profiling... [11264/50176]	Loss: 1.6278
Profiling... [12288/50176]	Loss: 1.6383
Profiling... [13312/50176]	Loss: 1.5790
Profile done
epoch 1 train time consumed: 13.35s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5259
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.44637950882944,
                        "time": 9.134751194000273,
                        "accuracy": 0.52587890625,
                        "total_cost": 3039828.066787092
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6798
Profiling... [2048/50176]	Loss: 1.6979
Profiling... [3072/50176]	Loss: 1.6634
Profiling... [4096/50176]	Loss: 1.6290
Profiling... [5120/50176]	Loss: 1.6118
Profiling... [6144/50176]	Loss: 1.7239
Profiling... [7168/50176]	Loss: 1.6817
Profiling... [8192/50176]	Loss: 1.6130
Profiling... [9216/50176]	Loss: 1.7180
Profiling... [10240/50176]	Loss: 1.6649
Profiling... [11264/50176]	Loss: 1.5599
Profiling... [12288/50176]	Loss: 1.6631
Profiling... [13312/50176]	Loss: 1.7301
Profile done
epoch 1 train time consumed: 14.89s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5288
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.44946689802411,
                        "time": 10.41568896499939,
                        "accuracy": 0.52880859375,
                        "total_cost": 3446890.9741973975
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6803
Profiling... [2048/50176]	Loss: 1.6312
Profiling... [3072/50176]	Loss: 1.6919
Profiling... [4096/50176]	Loss: 1.6439
Profiling... [5120/50176]	Loss: 1.6465
Profiling... [6144/50176]	Loss: 1.6267
Profiling... [7168/50176]	Loss: 1.6241
Profiling... [8192/50176]	Loss: 1.6266
Profiling... [9216/50176]	Loss: 1.7361
Profiling... [10240/50176]	Loss: 1.6792
Profiling... [11264/50176]	Loss: 1.6070
Profiling... [12288/50176]	Loss: 1.6695
Profiling... [13312/50176]	Loss: 1.6656
Profile done
epoch 1 train time consumed: 37.55s
Validation Epoch: 8, Average loss: 0.0016, Accuracy: 0.5324
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.32592674452948,
                        "time": 27.816384167999786,
                        "accuracy": 0.532421875,
                        "total_cost": 9142876.087501029
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6420
Profiling... [2048/50176]	Loss: 1.6216
Profiling... [3072/50176]	Loss: 1.6503
Profiling... [4096/50176]	Loss: 1.6843
Profiling... [5120/50176]	Loss: 1.6178
Profiling... [6144/50176]	Loss: 1.6859
Profiling... [7168/50176]	Loss: 1.6532
Profiling... [8192/50176]	Loss: 1.6068
Profiling... [9216/50176]	Loss: 1.6120
Profiling... [10240/50176]	Loss: 1.7237
Profiling... [11264/50176]	Loss: 1.6636
Profiling... [12288/50176]	Loss: 1.6817
Profiling... [13312/50176]	Loss: 1.7272
Profile done
epoch 1 train time consumed: 12.97s
Validation Epoch: 8, Average loss: 0.0016, Accuracy: 0.5323
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.35985233650167,
                        "time": 8.866153768000004,
                        "accuracy": 0.53232421875,
                        "total_cost": 2914721.6203001295
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6702
Profiling... [2048/50176]	Loss: 1.5507
Profiling... [3072/50176]	Loss: 1.7320
Profiling... [4096/50176]	Loss: 1.6460
Profiling... [5120/50176]	Loss: 1.6413
Profiling... [6144/50176]	Loss: 1.6809
Profiling... [7168/50176]	Loss: 1.6431
Profiling... [8192/50176]	Loss: 1.6079
Profiling... [9216/50176]	Loss: 1.5820
Profiling... [10240/50176]	Loss: 1.7207
Profiling... [11264/50176]	Loss: 1.6305
Profiling... [12288/50176]	Loss: 1.6429
Profiling... [13312/50176]	Loss: 1.5658
Profile done
epoch 1 train time consumed: 13.23s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5283
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.40993898646508,
                        "time": 9.138774072999695,
                        "accuracy": 0.5283203125,
                        "total_cost": 3027113.334346664
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5919
Profiling... [2048/50176]	Loss: 1.6484
Profiling... [3072/50176]	Loss: 1.7729
Profiling... [4096/50176]	Loss: 1.6960
Profiling... [5120/50176]	Loss: 1.6087
Profiling... [6144/50176]	Loss: 1.5107
Profiling... [7168/50176]	Loss: 1.6269
Profiling... [8192/50176]	Loss: 1.6076
Profiling... [9216/50176]	Loss: 1.6720
Profiling... [10240/50176]	Loss: 1.5852
Profiling... [11264/50176]	Loss: 1.6584
Profiling... [12288/50176]	Loss: 1.5951
Profiling... [13312/50176]	Loss: 1.6326
Profile done
epoch 1 train time consumed: 14.96s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5255
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.4161108028635,
                        "time": 10.435995883999567,
                        "accuracy": 0.52548828125,
                        "total_cost": 3475432.93516581
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6270
Profiling... [2048/50176]	Loss: 1.5888
Profiling... [3072/50176]	Loss: 1.6974
Profiling... [4096/50176]	Loss: 1.7136
Profiling... [5120/50176]	Loss: 1.6082
Profiling... [6144/50176]	Loss: 1.5857
Profiling... [7168/50176]	Loss: 1.6703
Profiling... [8192/50176]	Loss: 1.5967
Profiling... [9216/50176]	Loss: 1.6363
Profiling... [10240/50176]	Loss: 1.6709
Profiling... [11264/50176]	Loss: 1.6424
Profiling... [12288/50176]	Loss: 1.7101
Profiling... [13312/50176]	Loss: 1.6707
Profile done
epoch 1 train time consumed: 37.66s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5268
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.29253337747475,
                        "time": 27.9787646220002,
                        "accuracy": 0.5267578125,
                        "total_cost": 9295132.77764634
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6462
Profiling... [2048/50176]	Loss: 1.6010
Profiling... [3072/50176]	Loss: 1.6238
Profiling... [4096/50176]	Loss: 1.7447
Profiling... [5120/50176]	Loss: 1.6774
Profiling... [6144/50176]	Loss: 1.6021
Profiling... [7168/50176]	Loss: 1.6697
Profiling... [8192/50176]	Loss: 1.6725
Profiling... [9216/50176]	Loss: 1.6619
Profiling... [10240/50176]	Loss: 1.6542
Profiling... [11264/50176]	Loss: 1.6703
Profiling... [12288/50176]	Loss: 1.7003
Profiling... [13312/50176]	Loss: 1.5994
Profile done
epoch 1 train time consumed: 12.93s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5286
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.32661920000018,
                        "time": 8.83720459899996,
                        "accuracy": 0.52861328125,
                        "total_cost": 2925599.6012207516
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7156
Profiling... [2048/50176]	Loss: 1.7277
Profiling... [3072/50176]	Loss: 1.6545
Profiling... [4096/50176]	Loss: 1.6375
Profiling... [5120/50176]	Loss: 1.6831
Profiling... [6144/50176]	Loss: 1.6192
Profiling... [7168/50176]	Loss: 1.6594
Profiling... [8192/50176]	Loss: 1.5843
Profiling... [9216/50176]	Loss: 1.6649
Profiling... [10240/50176]	Loss: 1.6311
Profiling... [11264/50176]	Loss: 1.6527
Profiling... [12288/50176]	Loss: 1.6306
Profiling... [13312/50176]	Loss: 1.6801
Profile done
epoch 1 train time consumed: 13.37s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5271
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.37458339773025,
                        "time": 9.152008627999749,
                        "accuracy": 0.52705078125,
                        "total_cost": 3038799.233162043
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6314
Profiling... [2048/50176]	Loss: 1.5896
Profiling... [3072/50176]	Loss: 1.6589
Profiling... [4096/50176]	Loss: 1.6104
Profiling... [5120/50176]	Loss: 1.6779
Profiling... [6144/50176]	Loss: 1.5880
Profiling... [7168/50176]	Loss: 1.6130
Profiling... [8192/50176]	Loss: 1.5942
Profiling... [9216/50176]	Loss: 1.6512
Profiling... [10240/50176]	Loss: 1.6625
Profiling... [11264/50176]	Loss: 1.6968
Profiling... [12288/50176]	Loss: 1.6180
Profiling... [13312/50176]	Loss: 1.6778
Profile done
epoch 1 train time consumed: 14.84s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5246
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.37910438120848,
                        "time": 10.418434983000225,
                        "accuracy": 0.524609375,
                        "total_cost": 3475397.52225175
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5919
Profiling... [2048/50176]	Loss: 1.6108
Profiling... [3072/50176]	Loss: 1.6525
Profiling... [4096/50176]	Loss: 1.6795
Profiling... [5120/50176]	Loss: 1.5937
Profiling... [6144/50176]	Loss: 1.6274
Profiling... [7168/50176]	Loss: 1.5577
Profiling... [8192/50176]	Loss: 1.6544
Profiling... [9216/50176]	Loss: 1.7396
Profiling... [10240/50176]	Loss: 1.7247
Profiling... [11264/50176]	Loss: 1.6294
Profiling... [12288/50176]	Loss: 1.6269
Profiling... [13312/50176]	Loss: 1.6603
Profile done
epoch 1 train time consumed: 38.28s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5263
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.25467712717479,
                        "time": 28.254700927999693,
                        "accuracy": 0.52626953125,
                        "total_cost": 9395513.836143153
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7364
Profiling... [2048/50176]	Loss: 1.6205
Profiling... [3072/50176]	Loss: 1.7414
Profiling... [4096/50176]	Loss: 1.7300
Profiling... [5120/50176]	Loss: 1.6832
Profiling... [6144/50176]	Loss: 1.6212
Profiling... [7168/50176]	Loss: 1.6480
Profiling... [8192/50176]	Loss: 1.7314
Profiling... [9216/50176]	Loss: 1.6130
Profiling... [10240/50176]	Loss: 1.5915
Profiling... [11264/50176]	Loss: 1.5645
Profiling... [12288/50176]	Loss: 1.7113
Profiling... [13312/50176]	Loss: 1.7020
Profile done
epoch 1 train time consumed: 12.92s
Validation Epoch: 8, Average loss: 0.0023, Accuracy: 0.4337
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.28910618008322,
                        "time": 8.855070034000164,
                        "accuracy": 0.43369140625,
                        "total_cost": 3573133.4161063484
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7403
Profiling... [2048/50176]	Loss: 1.7183
Profiling... [3072/50176]	Loss: 1.7302
Profiling... [4096/50176]	Loss: 1.7079
Profiling... [5120/50176]	Loss: 1.6858
Profiling... [6144/50176]	Loss: 1.6602
Profiling... [7168/50176]	Loss: 1.6389
Profiling... [8192/50176]	Loss: 1.6394
Profiling... [9216/50176]	Loss: 1.7120
Profiling... [10240/50176]	Loss: 1.7307
Profiling... [11264/50176]	Loss: 1.6156
Profiling... [12288/50176]	Loss: 1.5789
Profiling... [13312/50176]	Loss: 1.6381
Profile done
epoch 1 train time consumed: 13.35s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4797
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.33934979021981,
                        "time": 9.153071128000192,
                        "accuracy": 0.4796875,
                        "total_cost": 3339231.1607036535
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6380
Profiling... [2048/50176]	Loss: 1.6600
Profiling... [3072/50176]	Loss: 1.6389
Profiling... [4096/50176]	Loss: 1.7699
Profiling... [5120/50176]	Loss: 1.6924
Profiling... [6144/50176]	Loss: 1.6691
Profiling... [7168/50176]	Loss: 1.6561
Profiling... [8192/50176]	Loss: 1.6555
Profiling... [9216/50176]	Loss: 1.6569
Profiling... [10240/50176]	Loss: 1.6595
Profiling... [11264/50176]	Loss: 1.7261
Profiling... [12288/50176]	Loss: 1.7704
Profiling... [13312/50176]	Loss: 1.7409
Profile done
epoch 1 train time consumed: 14.88s
Validation Epoch: 8, Average loss: 0.0032, Accuracy: 0.3250
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.34324128841203,
                        "time": 10.408110761999524,
                        "accuracy": 0.325,
                        "total_cost": 5604367.333384358
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6549
Profiling... [2048/50176]	Loss: 1.6081
Profiling... [3072/50176]	Loss: 1.6387
Profiling... [4096/50176]	Loss: 1.7157
Profiling... [5120/50176]	Loss: 1.7726
Profiling... [6144/50176]	Loss: 1.6613
Profiling... [7168/50176]	Loss: 1.6549
Profiling... [8192/50176]	Loss: 1.7642
Profiling... [9216/50176]	Loss: 1.6996
Profiling... [10240/50176]	Loss: 1.6086
Profiling... [11264/50176]	Loss: 1.6806
Profiling... [12288/50176]	Loss: 1.6950
Profiling... [13312/50176]	Loss: 1.7970
Profile done
epoch 1 train time consumed: 37.75s
Validation Epoch: 8, Average loss: 0.0021, Accuracy: 0.4461
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.2242203485037,
                        "time": 27.803322134000155,
                        "accuracy": 0.44609375,
                        "total_cost": 10907082.588469412
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6562
Profiling... [2048/50176]	Loss: 1.5858
Profiling... [3072/50176]	Loss: 1.5677
Profiling... [4096/50176]	Loss: 1.7058
Profiling... [5120/50176]	Loss: 1.6913
Profiling... [6144/50176]	Loss: 1.7273
Profiling... [7168/50176]	Loss: 1.7259
Profiling... [8192/50176]	Loss: 1.6773
Profiling... [9216/50176]	Loss: 1.7333
Profiling... [10240/50176]	Loss: 1.7268
Profiling... [11264/50176]	Loss: 1.6931
Profiling... [12288/50176]	Loss: 1.6867
Profiling... [13312/50176]	Loss: 1.7063
Profile done
epoch 1 train time consumed: 12.94s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4849
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.25250065470426,
                        "time": 8.865831013999923,
                        "accuracy": 0.48486328125,
                        "total_cost": 3199913.228013668
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6012
Profiling... [2048/50176]	Loss: 1.6104
Profiling... [3072/50176]	Loss: 1.7320
Profiling... [4096/50176]	Loss: 1.6297
Profiling... [5120/50176]	Loss: 1.6226
Profiling... [6144/50176]	Loss: 1.6548
Profiling... [7168/50176]	Loss: 1.7657
Profiling... [8192/50176]	Loss: 1.7365
Profiling... [9216/50176]	Loss: 1.7038
Profiling... [10240/50176]	Loss: 1.7437
Profiling... [11264/50176]	Loss: 1.6774
Profiling... [12288/50176]	Loss: 1.6690
Profiling... [13312/50176]	Loss: 1.5164
Profile done
epoch 1 train time consumed: 13.31s
Validation Epoch: 8, Average loss: 0.0021, Accuracy: 0.4495
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.29975820881947,
                        "time": 9.131105296999522,
                        "accuracy": 0.44951171875,
                        "total_cost": 3554842.64441085
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6937
Profiling... [2048/50176]	Loss: 1.6355
Profiling... [3072/50176]	Loss: 1.6337
Profiling... [4096/50176]	Loss: 1.6717
Profiling... [5120/50176]	Loss: 1.7050
Profiling... [6144/50176]	Loss: 1.6899
Profiling... [7168/50176]	Loss: 1.6953
Profiling... [8192/50176]	Loss: 1.6892
Profiling... [9216/50176]	Loss: 1.6955
Profiling... [10240/50176]	Loss: 1.5859
Profiling... [11264/50176]	Loss: 1.7707
Profiling... [12288/50176]	Loss: 1.6647
Profiling... [13312/50176]	Loss: 1.6678
Profile done
epoch 1 train time consumed: 14.87s
Validation Epoch: 8, Average loss: 0.0021, Accuracy: 0.4427
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.30524322350259,
                        "time": 10.406573729999764,
                        "accuracy": 0.44267578125,
                        "total_cost": 4113959.8773791254
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5740
Profiling... [2048/50176]	Loss: 1.6621
Profiling... [3072/50176]	Loss: 1.7699
Profiling... [4096/50176]	Loss: 1.6899
Profiling... [5120/50176]	Loss: 1.6272
Profiling... [6144/50176]	Loss: 1.6694
Profiling... [7168/50176]	Loss: 1.7224
Profiling... [8192/50176]	Loss: 1.6069
Profiling... [9216/50176]	Loss: 1.6681
Profiling... [10240/50176]	Loss: 1.7238
Profiling... [11264/50176]	Loss: 1.7005
Profiling... [12288/50176]	Loss: 1.6673
Profiling... [13312/50176]	Loss: 1.6764
Profile done
epoch 1 train time consumed: 37.84s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4755
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.1861975207836,
                        "time": 27.94404535799913,
                        "accuracy": 0.47548828125,
                        "total_cost": 10284602.440241208
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5924
Profiling... [2048/50176]	Loss: 1.6300
Profiling... [3072/50176]	Loss: 1.7476
Profiling... [4096/50176]	Loss: 1.6188
Profiling... [5120/50176]	Loss: 1.6392
Profiling... [6144/50176]	Loss: 1.6481
Profiling... [7168/50176]	Loss: 1.6672
Profiling... [8192/50176]	Loss: 1.6601
Profiling... [9216/50176]	Loss: 1.8343
Profiling... [10240/50176]	Loss: 1.6491
Profiling... [11264/50176]	Loss: 1.6498
Profiling... [12288/50176]	Loss: 1.7584
Profiling... [13312/50176]	Loss: 1.6607
Profile done
epoch 1 train time consumed: 12.98s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4723
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.22042777741382,
                        "time": 8.866572277999694,
                        "accuracy": 0.472265625,
                        "total_cost": 3285545.3933365284
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6404
Profiling... [2048/50176]	Loss: 1.6793
Profiling... [3072/50176]	Loss: 1.6519
Profiling... [4096/50176]	Loss: 1.6693
Profiling... [5120/50176]	Loss: 1.7026
Profiling... [6144/50176]	Loss: 1.7459
Profiling... [7168/50176]	Loss: 1.6303
Profiling... [8192/50176]	Loss: 1.6963
Profiling... [9216/50176]	Loss: 1.6309
Profiling... [10240/50176]	Loss: 1.7506
Profiling... [11264/50176]	Loss: 1.6754
Profiling... [12288/50176]	Loss: 1.6793
Profiling... [13312/50176]	Loss: 1.7514
Profile done
epoch 1 train time consumed: 13.25s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4747
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.26739107094228,
                        "time": 9.13468019100037,
                        "accuracy": 0.47470703125,
                        "total_cost": 3367485.476706987
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6365
Profiling... [2048/50176]	Loss: 1.6830
Profiling... [3072/50176]	Loss: 1.6888
Profiling... [4096/50176]	Loss: 1.6641
Profiling... [5120/50176]	Loss: 1.7040
Profiling... [6144/50176]	Loss: 1.6878
Profiling... [7168/50176]	Loss: 1.7306
Profiling... [8192/50176]	Loss: 1.5296
Profiling... [9216/50176]	Loss: 1.7130
Profiling... [10240/50176]	Loss: 1.6953
Profiling... [11264/50176]	Loss: 1.6998
Profiling... [12288/50176]	Loss: 1.6240
Profiling... [13312/50176]	Loss: 1.6544
Profile done
epoch 1 train time consumed: 14.95s
Validation Epoch: 8, Average loss: 0.0021, Accuracy: 0.4609
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.27223335828944,
                        "time": 10.414684546999524,
                        "accuracy": 0.4609375,
                        "total_cost": 3954049.726318463
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6397
Profiling... [2048/50176]	Loss: 1.6053
Profiling... [3072/50176]	Loss: 1.6568
Profiling... [4096/50176]	Loss: 1.7288
Profiling... [5120/50176]	Loss: 1.6716
Profiling... [6144/50176]	Loss: 1.6492
Profiling... [7168/50176]	Loss: 1.6329
Profiling... [8192/50176]	Loss: 1.6750
Profiling... [9216/50176]	Loss: 1.5787
Profiling... [10240/50176]	Loss: 1.6009
Profiling... [11264/50176]	Loss: 1.7182
Profiling... [12288/50176]	Loss: 1.6757
Profiling... [13312/50176]	Loss: 1.6747
Profile done
epoch 1 train time consumed: 37.68s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4793
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.15738956097653,
                        "time": 27.974243571999978,
                        "accuracy": 0.479296875,
                        "total_cost": 10213904.743484914
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6412
Profiling... [2048/50176]	Loss: 1.7437
Profiling... [3072/50176]	Loss: 1.7903
Profiling... [4096/50176]	Loss: 1.6975
Profiling... [5120/50176]	Loss: 1.6839
Profiling... [6144/50176]	Loss: 1.7791
Profiling... [7168/50176]	Loss: 1.6973
Profiling... [8192/50176]	Loss: 1.8198
Profiling... [9216/50176]	Loss: 1.7984
Profiling... [10240/50176]	Loss: 1.7593
Profiling... [11264/50176]	Loss: 1.7607
Profiling... [12288/50176]	Loss: 1.7540
Profiling... [13312/50176]	Loss: 1.7353
Profile done
epoch 1 train time consumed: 12.98s
Validation Epoch: 8, Average loss: 0.0045, Accuracy: 0.2483
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.18943864967238,
                        "time": 8.84712633900017,
                        "accuracy": 0.24833984375,
                        "total_cost": 6234388.674592333
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6672
Profiling... [2048/50176]	Loss: 1.7697
Profiling... [3072/50176]	Loss: 1.6671
Profiling... [4096/50176]	Loss: 1.7246
Profiling... [5120/50176]	Loss: 1.7942
Profiling... [6144/50176]	Loss: 1.7963
Profiling... [7168/50176]	Loss: 1.7471
Profiling... [8192/50176]	Loss: 1.7273
Profiling... [9216/50176]	Loss: 1.6506
Profiling... [10240/50176]	Loss: 1.8274
Profiling... [11264/50176]	Loss: 1.6527
Profiling... [12288/50176]	Loss: 1.7284
Profiling... [13312/50176]	Loss: 1.7000
Profile done
epoch 1 train time consumed: 13.31s
Validation Epoch: 8, Average loss: 0.0048, Accuracy: 0.2381
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.23625036571143,
                        "time": 9.166431000000557,
                        "accuracy": 0.2380859375,
                        "total_cost": 6737589.972108695
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7167
Profiling... [2048/50176]	Loss: 1.7324
Profiling... [3072/50176]	Loss: 1.7072
Profiling... [4096/50176]	Loss: 1.7953
Profiling... [5120/50176]	Loss: 1.7374
Profiling... [6144/50176]	Loss: 1.7201
Profiling... [7168/50176]	Loss: 1.7353
Profiling... [8192/50176]	Loss: 1.7711
Profiling... [9216/50176]	Loss: 1.7889
Profiling... [10240/50176]	Loss: 1.6060
Profiling... [11264/50176]	Loss: 1.7897
Profiling... [12288/50176]	Loss: 1.7523
Profiling... [13312/50176]	Loss: 1.7889
Profile done
epoch 1 train time consumed: 15.02s
Validation Epoch: 8, Average loss: 0.0029, Accuracy: 0.3188
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.24130265385372,
                        "time": 10.418805190999592,
                        "accuracy": 0.31884765625,
                        "total_cost": 5718376.38660682
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6998
Profiling... [2048/50176]	Loss: 1.6939
Profiling... [3072/50176]	Loss: 1.7640
Profiling... [4096/50176]	Loss: 1.7590
Profiling... [5120/50176]	Loss: 1.7291
Profiling... [6144/50176]	Loss: 1.6681
Profiling... [7168/50176]	Loss: 1.8121
Profiling... [8192/50176]	Loss: 1.7347
Profiling... [9216/50176]	Loss: 1.8604
Profiling... [10240/50176]	Loss: 1.8903
Profiling... [11264/50176]	Loss: 1.7714
Profiling... [12288/50176]	Loss: 1.7766
Profiling... [13312/50176]	Loss: 1.7335
Profile done
epoch 1 train time consumed: 38.42s
Validation Epoch: 8, Average loss: 0.0037, Accuracy: 0.2787
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.12466094025261,
                        "time": 28.485106696999537,
                        "accuracy": 0.2787109375,
                        "total_cost": 17885533.006665442
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6354
Profiling... [2048/50176]	Loss: 1.6659
Profiling... [3072/50176]	Loss: 1.7080
Profiling... [4096/50176]	Loss: 1.7578
Profiling... [5120/50176]	Loss: 1.8194
Profiling... [6144/50176]	Loss: 1.7237
Profiling... [7168/50176]	Loss: 1.8329
Profiling... [8192/50176]	Loss: 1.6896
Profiling... [9216/50176]	Loss: 1.8434
Profiling... [10240/50176]	Loss: 1.7717
Profiling... [11264/50176]	Loss: 1.7181
Profiling... [12288/50176]	Loss: 1.6754
Profiling... [13312/50176]	Loss: 1.7062
Profile done
epoch 1 train time consumed: 13.09s
Validation Epoch: 8, Average loss: 0.0026, Accuracy: 0.3727
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.15690179456705,
                        "time": 8.889315563999844,
                        "accuracy": 0.37265625,
                        "total_cost": 4174437.497559675
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6311
Profiling... [2048/50176]	Loss: 1.6955
Profiling... [3072/50176]	Loss: 1.8197
Profiling... [4096/50176]	Loss: 1.7402
Profiling... [5120/50176]	Loss: 1.6779
Profiling... [6144/50176]	Loss: 1.7202
Profiling... [7168/50176]	Loss: 1.8593
Profiling... [8192/50176]	Loss: 1.8144
Profiling... [9216/50176]	Loss: 1.9226
Profiling... [10240/50176]	Loss: 1.7580
Profiling... [11264/50176]	Loss: 1.7063
Profiling... [12288/50176]	Loss: 1.7499
Profiling... [13312/50176]	Loss: 1.7096
Profile done
epoch 1 train time consumed: 13.33s
Validation Epoch: 8, Average loss: 0.0028, Accuracy: 0.3330
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.20482343643994,
                        "time": 9.159277529000065,
                        "accuracy": 0.3330078125,
                        "total_cost": 4813321.211720855
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6509
Profiling... [2048/50176]	Loss: 1.6452
Profiling... [3072/50176]	Loss: 1.8709
Profiling... [4096/50176]	Loss: 1.7179
Profiling... [5120/50176]	Loss: 1.7629
Profiling... [6144/50176]	Loss: 1.7843
Profiling... [7168/50176]	Loss: 1.7679
Profiling... [8192/50176]	Loss: 1.8357
Profiling... [9216/50176]	Loss: 1.7357
Profiling... [10240/50176]	Loss: 1.7602
Profiling... [11264/50176]	Loss: 1.6373
Profiling... [12288/50176]	Loss: 1.7554
Profiling... [13312/50176]	Loss: 1.7284
Profile done
epoch 1 train time consumed: 14.99s
Validation Epoch: 8, Average loss: 0.0024, Accuracy: 0.3806
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.20772652732474,
                        "time": 10.425862374000644,
                        "accuracy": 0.38056640625,
                        "total_cost": 4794237.971313614
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6218
Profiling... [2048/50176]	Loss: 1.6379
Profiling... [3072/50176]	Loss: 1.7213
Profiling... [4096/50176]	Loss: 1.8072
Profiling... [5120/50176]	Loss: 1.7542
Profiling... [6144/50176]	Loss: 1.7779
Profiling... [7168/50176]	Loss: 1.7195
Profiling... [8192/50176]	Loss: 1.8392
Profiling... [9216/50176]	Loss: 1.6388
Profiling... [10240/50176]	Loss: 1.8104
Profiling... [11264/50176]	Loss: 1.7599
Profiling... [12288/50176]	Loss: 1.7621
Profiling... [13312/50176]	Loss: 1.8740
Profile done
epoch 1 train time consumed: 38.32s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.2562
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.09458601856808,
                        "time": 28.23335210299956,
                        "accuracy": 0.25615234375,
                        "total_cost": 19288664.49431003
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6527
Profiling... [2048/50176]	Loss: 1.7287
Profiling... [3072/50176]	Loss: 1.6312
Profiling... [4096/50176]	Loss: 1.8845
Profiling... [5120/50176]	Loss: 1.7044
Profiling... [6144/50176]	Loss: 1.7110
Profiling... [7168/50176]	Loss: 1.6678
Profiling... [8192/50176]	Loss: 1.7611
Profiling... [9216/50176]	Loss: 1.6841
Profiling... [10240/50176]	Loss: 1.8062
Profiling... [11264/50176]	Loss: 1.7778
Profiling... [12288/50176]	Loss: 1.7000
Profiling... [13312/50176]	Loss: 1.6855
Profile done
epoch 1 train time consumed: 13.03s
Validation Epoch: 8, Average loss: 0.0048, Accuracy: 0.2387
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.12360270455113,
                        "time": 8.880997789000503,
                        "accuracy": 0.238671875,
                        "total_cost": 6511762.699627209
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6831
Profiling... [2048/50176]	Loss: 1.7968
Profiling... [3072/50176]	Loss: 1.7919
Profiling... [4096/50176]	Loss: 1.7557
Profiling... [5120/50176]	Loss: 1.7089
Profiling... [6144/50176]	Loss: 1.7106
Profiling... [7168/50176]	Loss: 1.7522
Profiling... [8192/50176]	Loss: 1.7726
Profiling... [9216/50176]	Loss: 1.8118
Profiling... [10240/50176]	Loss: 1.7451
Profiling... [11264/50176]	Loss: 1.7316
Profiling... [12288/50176]	Loss: 1.7757
Profiling... [13312/50176]	Loss: 1.6944
Profile done
epoch 1 train time consumed: 13.41s
Validation Epoch: 8, Average loss: 0.0029, Accuracy: 0.3337
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.16990707352626,
                        "time": 9.121716759000265,
                        "accuracy": 0.33369140625,
                        "total_cost": 4783762.491111641
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6196
Profiling... [2048/50176]	Loss: 1.7236
Profiling... [3072/50176]	Loss: 1.6925
Profiling... [4096/50176]	Loss: 1.7228
Profiling... [5120/50176]	Loss: 1.6615
Profiling... [6144/50176]	Loss: 1.7690
Profiling... [7168/50176]	Loss: 1.6277
Profiling... [8192/50176]	Loss: 1.8763
Profiling... [9216/50176]	Loss: 1.7494
Profiling... [10240/50176]	Loss: 1.7148
Profiling... [11264/50176]	Loss: 1.9036
Profiling... [12288/50176]	Loss: 1.7675
Profiling... [13312/50176]	Loss: 1.7917
Profile done
epoch 1 train time consumed: 14.85s
Validation Epoch: 8, Average loss: 0.0027, Accuracy: 0.3441
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.17471517445993,
                        "time": 10.40546951700071,
                        "accuracy": 0.344140625,
                        "total_cost": 5291317.075614437
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6362
Profiling... [2048/50176]	Loss: 1.7644
Profiling... [3072/50176]	Loss: 1.7442
Profiling... [4096/50176]	Loss: 1.7311
Profiling... [5120/50176]	Loss: 1.7601
Profiling... [6144/50176]	Loss: 1.7573
Profiling... [7168/50176]	Loss: 1.7909
Profiling... [8192/50176]	Loss: 1.7659
Profiling... [9216/50176]	Loss: 1.7984
Profiling... [10240/50176]	Loss: 1.7267
Profiling... [11264/50176]	Loss: 1.7207
Profiling... [12288/50176]	Loss: 1.7765
Profiling... [13312/50176]	Loss: 1.6253
Profile done
epoch 1 train time consumed: 38.54s
Validation Epoch: 8, Average loss: 0.0038, Accuracy: 0.2472
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.06325982389966,
                        "time": 28.486251717000414,
                        "accuracy": 0.24716796875,
                        "total_cost": 20168851.472487055
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.5 bs: 128 pl: 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[GPU_0] Set GPU power limit to 150W.
[GPU_0] Set GPU power limit to 150W.
[Training Loop] Model's accuracy 0.5320411392405063 surpasses threshold 0.5! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6873
Profiling... [256/50048]	Loss: 2.0202
Profiling... [384/50048]	Loss: 1.9215
Profiling... [512/50048]	Loss: 1.7613
Profiling... [640/50048]	Loss: 1.6679
Profiling... [768/50048]	Loss: 1.5161
Profiling... [896/50048]	Loss: 1.7935
Profiling... [1024/50048]	Loss: 1.7961
Profiling... [1152/50048]	Loss: 1.8149
Profiling... [1280/50048]	Loss: 1.7518
Profiling... [1408/50048]	Loss: 1.7569
Profiling... [1536/50048]	Loss: 2.0290
Profiling... [1664/50048]	Loss: 1.7311
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 8, Average loss: 0.0139, Accuracy: 0.5195
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.03670914312274,
                        "time": 2.174311327000396,
                        "accuracy": 0.5194818037974683,
                        "total_cost": 732469.3173919476
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8414
Profiling... [256/50048]	Loss: 1.7855
Profiling... [384/50048]	Loss: 1.9940
Profiling... [512/50048]	Loss: 1.6892
Profiling... [640/50048]	Loss: 1.5366
Profiling... [768/50048]	Loss: 1.8913
Profiling... [896/50048]	Loss: 1.8977
Profiling... [1024/50048]	Loss: 1.9279
Profiling... [1152/50048]	Loss: 1.8993
Profiling... [1280/50048]	Loss: 1.8981
Profiling... [1408/50048]	Loss: 1.7109
Profiling... [1536/50048]	Loss: 1.6193
Profiling... [1664/50048]	Loss: 1.8854
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 8, Average loss: 0.0137, Accuracy: 0.5266
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.04940196795224,
                        "time": 2.170888842999375,
                        "accuracy": 0.5266020569620253,
                        "total_cost": 721428.1495909285
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8067
Profiling... [256/50048]	Loss: 1.6035
Profiling... [384/50048]	Loss: 2.0013
Profiling... [512/50048]	Loss: 1.7743
Profiling... [640/50048]	Loss: 1.9373
Profiling... [768/50048]	Loss: 1.6780
Profiling... [896/50048]	Loss: 1.9082
Profiling... [1024/50048]	Loss: 1.6762
Profiling... [1152/50048]	Loss: 2.1353
Profiling... [1280/50048]	Loss: 1.8977
Profiling... [1408/50048]	Loss: 1.8728
Profiling... [1536/50048]	Loss: 2.0279
Profiling... [1664/50048]	Loss: 1.6717
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 8, Average loss: 0.0138, Accuracy: 0.5220
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.05608645226216,
                        "time": 2.3987633480001023,
                        "accuracy": 0.5219541139240507,
                        "total_cost": 804253.8121676735
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6760
Profiling... [256/50048]	Loss: 1.7457
Profiling... [384/50048]	Loss: 1.8526
Profiling... [512/50048]	Loss: 1.7451
Profiling... [640/50048]	Loss: 1.6081
Profiling... [768/50048]	Loss: 1.9470
Profiling... [896/50048]	Loss: 1.8405
Profiling... [1024/50048]	Loss: 1.6861
Profiling... [1152/50048]	Loss: 1.6967
Profiling... [1280/50048]	Loss: 1.7506
Profiling... [1408/50048]	Loss: 1.9495
Profiling... [1536/50048]	Loss: 1.8678
Profiling... [1664/50048]	Loss: 1.5520
Profile done
epoch 1 train time consumed: 7.25s
Validation Epoch: 8, Average loss: 0.0138, Accuracy: 0.5236
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.0385140525157,
                        "time": 5.096929294000802,
                        "accuracy": 0.5236352848101266,
                        "total_cost": 1703404.3585767364
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5713
Profiling... [256/50048]	Loss: 1.8390
Profiling... [384/50048]	Loss: 1.9588
Profiling... [512/50048]	Loss: 1.8438
Profiling... [640/50048]	Loss: 1.8576
Profiling... [768/50048]	Loss: 1.6520
Profiling... [896/50048]	Loss: 1.8091
Profiling... [1024/50048]	Loss: 2.0438
Profiling... [1152/50048]	Loss: 1.6964
Profiling... [1280/50048]	Loss: 1.8742
Profiling... [1408/50048]	Loss: 1.8668
Profiling... [1536/50048]	Loss: 2.0056
Profiling... [1664/50048]	Loss: 1.6914
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 8, Average loss: 0.0137, Accuracy: 0.5262
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.00993086008887,
                        "time": 2.1879705150004156,
                        "accuracy": 0.5262064873417721,
                        "total_cost": 727651.310532745
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9050
Profiling... [256/50048]	Loss: 1.8350
Profiling... [384/50048]	Loss: 1.9352
Profiling... [512/50048]	Loss: 1.8709
Profiling... [640/50048]	Loss: 1.9715
Profiling... [768/50048]	Loss: 1.8461
Profiling... [896/50048]	Loss: 1.8464
Profiling... [1024/50048]	Loss: 1.6941
Profiling... [1152/50048]	Loss: 1.6384
Profiling... [1280/50048]	Loss: 1.7732
Profiling... [1408/50048]	Loss: 1.5550
Profiling... [1536/50048]	Loss: 1.7892
Profiling... [1664/50048]	Loss: 1.8727
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 8, Average loss: 0.0136, Accuracy: 0.5293
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.01995826329006,
                        "time": 2.170900674999757,
                        "accuracy": 0.5292721518987342,
                        "total_cost": 717792.5699700243
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8477
Profiling... [256/50048]	Loss: 1.7704
Profiling... [384/50048]	Loss: 1.7702
Profiling... [512/50048]	Loss: 1.9805
Profiling... [640/50048]	Loss: 1.7545
Profiling... [768/50048]	Loss: 1.7381
Profiling... [896/50048]	Loss: 1.9077
Profiling... [1024/50048]	Loss: 1.8391
Profiling... [1152/50048]	Loss: 1.6614
Profiling... [1280/50048]	Loss: 1.6674
Profiling... [1408/50048]	Loss: 2.0540
Profiling... [1536/50048]	Loss: 2.0419
Profiling... [1664/50048]	Loss: 1.5904
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 8, Average loss: 0.0137, Accuracy: 0.5235
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.02758544149245,
                        "time": 2.380816173999847,
                        "accuracy": 0.5235363924050633,
                        "total_cost": 795824.0085965486
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6852
Profiling... [256/50048]	Loss: 1.6574
Profiling... [384/50048]	Loss: 1.8977
Profiling... [512/50048]	Loss: 1.8522
Profiling... [640/50048]	Loss: 1.5985
Profiling... [768/50048]	Loss: 1.8441
Profiling... [896/50048]	Loss: 1.6847
Profiling... [1024/50048]	Loss: 1.9765
Profiling... [1152/50048]	Loss: 1.7405
Profiling... [1280/50048]	Loss: 1.6846
Profiling... [1408/50048]	Loss: 1.5280
Profiling... [1536/50048]	Loss: 1.7738
Profiling... [1664/50048]	Loss: 1.7886
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 8, Average loss: 0.0139, Accuracy: 0.5208
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.01107070713614,
                        "time": 4.815148763999787,
                        "accuracy": 0.5207674050632911,
                        "total_cost": 1618094.8068313755
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7045
Profiling... [256/50048]	Loss: 1.8832
Profiling... [384/50048]	Loss: 1.7026
Profiling... [512/50048]	Loss: 1.8610
Profiling... [640/50048]	Loss: 1.7922
Profiling... [768/50048]	Loss: 1.7744
Profiling... [896/50048]	Loss: 1.6000
Profiling... [1024/50048]	Loss: 1.8310
Profiling... [1152/50048]	Loss: 1.9904
Profiling... [1280/50048]	Loss: 1.7922
Profiling... [1408/50048]	Loss: 1.6946
Profiling... [1536/50048]	Loss: 1.7781
Profiling... [1664/50048]	Loss: 1.7834
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 8, Average loss: 0.0141, Accuracy: 0.5173
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.9813507551355,
                        "time": 2.172889555999973,
                        "accuracy": 0.517306170886076,
                        "total_cost": 735068.8889882531
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5485
Profiling... [256/50048]	Loss: 1.8321
Profiling... [384/50048]	Loss: 2.0490
Profiling... [512/50048]	Loss: 1.8367
Profiling... [640/50048]	Loss: 1.8514
Profiling... [768/50048]	Loss: 1.7255
Profiling... [896/50048]	Loss: 1.9559
Profiling... [1024/50048]	Loss: 1.8128
Profiling... [1152/50048]	Loss: 1.8732
Profiling... [1280/50048]	Loss: 1.8464
Profiling... [1408/50048]	Loss: 1.7903
Profiling... [1536/50048]	Loss: 2.0254
Profiling... [1664/50048]	Loss: 2.0310
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 8, Average loss: 0.0137, Accuracy: 0.5283
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.99385834265455,
                        "time": 2.186353889000202,
                        "accuracy": 0.5282832278481012,
                        "total_cost": 724255.3054988314
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4589
Profiling... [256/50048]	Loss: 1.9114
Profiling... [384/50048]	Loss: 2.0585
Profiling... [512/50048]	Loss: 1.6070
Profiling... [640/50048]	Loss: 1.7225
Profiling... [768/50048]	Loss: 1.8975
Profiling... [896/50048]	Loss: 1.8219
Profiling... [1024/50048]	Loss: 1.8848
Profiling... [1152/50048]	Loss: 1.7562
Profiling... [1280/50048]	Loss: 1.5616
Profiling... [1408/50048]	Loss: 1.4971
Profiling... [1536/50048]	Loss: 1.9890
Profiling... [1664/50048]	Loss: 1.7292
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 8, Average loss: 0.0138, Accuracy: 0.5235
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.99937292560695,
                        "time": 2.385308165999959,
                        "accuracy": 0.5235363924050633,
                        "total_cost": 797325.525227338
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0278
Profiling... [256/50048]	Loss: 1.9077
Profiling... [384/50048]	Loss: 1.8931
Profiling... [512/50048]	Loss: 2.1763
Profiling... [640/50048]	Loss: 1.7658
Profiling... [768/50048]	Loss: 1.8369
Profiling... [896/50048]	Loss: 1.9590
Profiling... [1024/50048]	Loss: 1.8378
Profiling... [1152/50048]	Loss: 1.6609
Profiling... [1280/50048]	Loss: 1.5289
Profiling... [1408/50048]	Loss: 1.8553
Profiling... [1536/50048]	Loss: 1.7561
Profiling... [1664/50048]	Loss: 1.6838
Profile done
epoch 1 train time consumed: 7.36s
Validation Epoch: 8, Average loss: 0.0137, Accuracy: 0.5229
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.98020327375679,
                        "time": 5.093326356999569,
                        "accuracy": 0.5229430379746836,
                        "total_cost": 1704453.5403453927
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7507
Profiling... [256/50048]	Loss: 1.5613
Profiling... [384/50048]	Loss: 1.8526
Profiling... [512/50048]	Loss: 2.0244
Profiling... [640/50048]	Loss: 1.8809
Profiling... [768/50048]	Loss: 1.7675
Profiling... [896/50048]	Loss: 2.1340
Profiling... [1024/50048]	Loss: 1.9193
Profiling... [1152/50048]	Loss: 1.9971
Profiling... [1280/50048]	Loss: 1.6827
Profiling... [1408/50048]	Loss: 2.0094
Profiling... [1536/50048]	Loss: 1.7949
Profiling... [1664/50048]	Loss: 2.2474
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 8, Average loss: 0.0180, Accuracy: 0.4348
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.95135694629278,
                        "time": 2.1781688189994384,
                        "accuracy": 0.43482990506329117,
                        "total_cost": 876617.5897433263
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9344
Profiling... [256/50048]	Loss: 1.6576
Profiling... [384/50048]	Loss: 1.9251
Profiling... [512/50048]	Loss: 2.0172
Profiling... [640/50048]	Loss: 1.8790
Profiling... [768/50048]	Loss: 1.7460
Profiling... [896/50048]	Loss: 1.7540
Profiling... [1024/50048]	Loss: 1.9638
Profiling... [1152/50048]	Loss: 1.7845
Profiling... [1280/50048]	Loss: 2.0519
Profiling... [1408/50048]	Loss: 1.7566
Profiling... [1536/50048]	Loss: 1.9399
Profiling... [1664/50048]	Loss: 2.0792
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 8, Average loss: 0.0173, Accuracy: 0.4318
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.96345907681827,
                        "time": 2.1702494020000813,
                        "accuracy": 0.4317642405063291,
                        "total_cost": 879632.0068207383
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8581
Profiling... [256/50048]	Loss: 1.8204
Profiling... [384/50048]	Loss: 1.9330
Profiling... [512/50048]	Loss: 1.7592
Profiling... [640/50048]	Loss: 1.9139
Profiling... [768/50048]	Loss: 1.8874
Profiling... [896/50048]	Loss: 1.8563
Profiling... [1024/50048]	Loss: 1.8609
Profiling... [1152/50048]	Loss: 2.1516
Profiling... [1280/50048]	Loss: 1.8305
Profiling... [1408/50048]	Loss: 2.0274
Profiling... [1536/50048]	Loss: 2.0086
Profiling... [1664/50048]	Loss: 1.6508
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 8, Average loss: 0.0179, Accuracy: 0.4419
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.97013190303026,
                        "time": 2.4132475509995857,
                        "accuracy": 0.44185126582278483,
                        "total_cost": 955792.9423117428
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8474
Profiling... [256/50048]	Loss: 1.7318
Profiling... [384/50048]	Loss: 1.9485
Profiling... [512/50048]	Loss: 1.8959
Profiling... [640/50048]	Loss: 1.8494
Profiling... [768/50048]	Loss: 1.7350
Profiling... [896/50048]	Loss: 1.8124
Profiling... [1024/50048]	Loss: 1.8685
Profiling... [1152/50048]	Loss: 1.9414
Profiling... [1280/50048]	Loss: 1.8077
Profiling... [1408/50048]	Loss: 1.8418
Profiling... [1536/50048]	Loss: 1.6666
Profiling... [1664/50048]	Loss: 1.7225
Profile done
epoch 1 train time consumed: 7.03s
Validation Epoch: 8, Average loss: 0.0186, Accuracy: 0.4268
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.95258305057068,
                        "time": 4.836797463000039,
                        "accuracy": 0.42681962025316456,
                        "total_cost": 1983131.7864978844
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9812
Profiling... [256/50048]	Loss: 1.7838
Profiling... [384/50048]	Loss: 1.8583
Profiling... [512/50048]	Loss: 1.8221
Profiling... [640/50048]	Loss: 1.5680
Profiling... [768/50048]	Loss: 1.5651
Profiling... [896/50048]	Loss: 1.7954
Profiling... [1024/50048]	Loss: 1.7083
Profiling... [1152/50048]	Loss: 1.7566
Profiling... [1280/50048]	Loss: 1.9727
Profiling... [1408/50048]	Loss: 1.9363
Profiling... [1536/50048]	Loss: 2.1249
Profiling... [1664/50048]	Loss: 1.7713
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 8, Average loss: 0.0164, Accuracy: 0.4584
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.92210109800115,
                        "time": 2.177696040000228,
                        "accuracy": 0.45836629746835444,
                        "total_cost": 831424.1450667537
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9813
Profiling... [256/50048]	Loss: 1.7452
Profiling... [384/50048]	Loss: 1.7118
Profiling... [512/50048]	Loss: 1.9936
Profiling... [640/50048]	Loss: 1.7876
Profiling... [768/50048]	Loss: 2.1737
Profiling... [896/50048]	Loss: 1.9950
Profiling... [1024/50048]	Loss: 1.8179
Profiling... [1152/50048]	Loss: 1.9114
Profiling... [1280/50048]	Loss: 1.8217
Profiling... [1408/50048]	Loss: 1.7382
Profiling... [1536/50048]	Loss: 1.8181
Profiling... [1664/50048]	Loss: 2.0322
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 8, Average loss: 0.0178, Accuracy: 0.4435
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.93373513230347,
                        "time": 2.175143155999649,
                        "accuracy": 0.4435324367088608,
                        "total_cost": 858223.7076604189
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6745
Profiling... [256/50048]	Loss: 2.1634
Profiling... [384/50048]	Loss: 1.8298
Profiling... [512/50048]	Loss: 1.6830
Profiling... [640/50048]	Loss: 1.9091
Profiling... [768/50048]	Loss: 1.9708
Profiling... [896/50048]	Loss: 1.6528
Profiling... [1024/50048]	Loss: 1.7159
Profiling... [1152/50048]	Loss: 2.0546
Profiling... [1280/50048]	Loss: 1.7302
Profiling... [1408/50048]	Loss: 1.9652
Profiling... [1536/50048]	Loss: 2.0711
Profiling... [1664/50048]	Loss: 1.8975
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 8, Average loss: 0.0179, Accuracy: 0.4359
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.94102392146338,
                        "time": 2.375641835999886,
                        "accuracy": 0.43591772151898733,
                        "total_cost": 953705.9421473227
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0709
Profiling... [256/50048]	Loss: 1.7568
Profiling... [384/50048]	Loss: 1.7400
Profiling... [512/50048]	Loss: 1.8261
Profiling... [640/50048]	Loss: 1.7763
Profiling... [768/50048]	Loss: 2.0680
Profiling... [896/50048]	Loss: 1.7594
Profiling... [1024/50048]	Loss: 1.8369
Profiling... [1152/50048]	Loss: 2.2318
Profiling... [1280/50048]	Loss: 1.7015
Profiling... [1408/50048]	Loss: 1.9209
Profiling... [1536/50048]	Loss: 1.5772
Profiling... [1664/50048]	Loss: 1.9393
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 8, Average loss: 0.0189, Accuracy: 0.4234
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.92466370742801,
                        "time": 4.893085674999384,
                        "accuracy": 0.4233583860759494,
                        "total_cost": 2022612.5696049775
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9057
Profiling... [256/50048]	Loss: 1.8706
Profiling... [384/50048]	Loss: 1.8165
Profiling... [512/50048]	Loss: 1.9226
Profiling... [640/50048]	Loss: 2.0724
Profiling... [768/50048]	Loss: 1.7358
Profiling... [896/50048]	Loss: 1.7996
Profiling... [1024/50048]	Loss: 1.8504
Profiling... [1152/50048]	Loss: 1.8381
Profiling... [1280/50048]	Loss: 2.0715
Profiling... [1408/50048]	Loss: 2.0076
Profiling... [1536/50048]	Loss: 1.8270
Profiling... [1664/50048]	Loss: 1.8725
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 8, Average loss: 0.0174, Accuracy: 0.4444
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.89769578494464,
                        "time": 2.1730130299993107,
                        "accuracy": 0.4444224683544304,
                        "total_cost": 855666.190005959
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9965
Profiling... [256/50048]	Loss: 1.9246
Profiling... [384/50048]	Loss: 1.8173
Profiling... [512/50048]	Loss: 1.7707
Profiling... [640/50048]	Loss: 1.7436
Profiling... [768/50048]	Loss: 1.6876
Profiling... [896/50048]	Loss: 1.9141
Profiling... [1024/50048]	Loss: 1.7856
Profiling... [1152/50048]	Loss: 1.8884
Profiling... [1280/50048]	Loss: 1.8962
Profiling... [1408/50048]	Loss: 1.7539
Profiling... [1536/50048]	Loss: 1.8139
Profiling... [1664/50048]	Loss: 1.8133
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 8, Average loss: 0.0172, Accuracy: 0.4479
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.90920004317067,
                        "time": 2.169539925999743,
                        "accuracy": 0.44788370253164556,
                        "total_cost": 847696.5893241654
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6931
Profiling... [256/50048]	Loss: 1.7683
Profiling... [384/50048]	Loss: 1.9464
Profiling... [512/50048]	Loss: 1.8221
Profiling... [640/50048]	Loss: 1.5639
Profiling... [768/50048]	Loss: 1.8480
Profiling... [896/50048]	Loss: 1.7539
Profiling... [1024/50048]	Loss: 1.9854
Profiling... [1152/50048]	Loss: 1.9060
Profiling... [1280/50048]	Loss: 1.8314
Profiling... [1408/50048]	Loss: 2.0525
Profiling... [1536/50048]	Loss: 1.7919
Profiling... [1664/50048]	Loss: 1.9514
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 8, Average loss: 0.0178, Accuracy: 0.4376
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.9158449875086,
                        "time": 2.3960193559996696,
                        "accuracy": 0.4375988924050633,
                        "total_cost": 958191.1530795515
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6587
Profiling... [256/50048]	Loss: 1.8879
Profiling... [384/50048]	Loss: 1.9603
Profiling... [512/50048]	Loss: 2.0242
Profiling... [640/50048]	Loss: 1.9455
Profiling... [768/50048]	Loss: 1.6495
Profiling... [896/50048]	Loss: 1.8420
Profiling... [1024/50048]	Loss: 2.0851
Profiling... [1152/50048]	Loss: 1.8572
Profiling... [1280/50048]	Loss: 1.8569
Profiling... [1408/50048]	Loss: 2.0061
Profiling... [1536/50048]	Loss: 2.0296
Profiling... [1664/50048]	Loss: 1.8346
Profile done
epoch 1 train time consumed: 7.22s
Validation Epoch: 8, Average loss: 0.0199, Accuracy: 0.3921
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.89806322578575,
                        "time": 5.047311630999502,
                        "accuracy": 0.3921083860759494,
                        "total_cost": 2252641.276725528
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8525
Profiling... [256/50048]	Loss: 1.8334
Profiling... [384/50048]	Loss: 2.0799
Profiling... [512/50048]	Loss: 2.0192
Profiling... [640/50048]	Loss: 2.1823
Profiling... [768/50048]	Loss: 1.9659
Profiling... [896/50048]	Loss: 2.0177
Profiling... [1024/50048]	Loss: 2.1044
Profiling... [1152/50048]	Loss: 2.3181
Profiling... [1280/50048]	Loss: 2.0087
Profiling... [1408/50048]	Loss: 2.1728
Profiling... [1536/50048]	Loss: 1.9973
Profiling... [1664/50048]	Loss: 2.3273
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 8, Average loss: 0.0254, Accuracy: 0.3222
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.8701094847606,
                        "time": 2.1782665669998096,
                        "accuracy": 0.32219145569620256,
                        "total_cost": 1183137.0524747889
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9093
Profiling... [256/50048]	Loss: 1.6846
Profiling... [384/50048]	Loss: 1.8857
Profiling... [512/50048]	Loss: 1.7052
Profiling... [640/50048]	Loss: 2.0634
Profiling... [768/50048]	Loss: 2.0688
Profiling... [896/50048]	Loss: 1.9576
Profiling... [1024/50048]	Loss: 2.0299
Profiling... [1152/50048]	Loss: 1.9657
Profiling... [1280/50048]	Loss: 2.0910
Profiling... [1408/50048]	Loss: 2.0200
Profiling... [1536/50048]	Loss: 2.3135
Profiling... [1664/50048]	Loss: 2.0274
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 8, Average loss: 0.0250, Accuracy: 0.3018
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.8820881977243,
                        "time": 2.167304127000534,
                        "accuracy": 0.30181962025316456,
                        "total_cost": 1256638.7231782915
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0597
Profiling... [256/50048]	Loss: 1.6326
Profiling... [384/50048]	Loss: 1.9417
Profiling... [512/50048]	Loss: 2.1464
Profiling... [640/50048]	Loss: 2.1707
Profiling... [768/50048]	Loss: 2.0176
Profiling... [896/50048]	Loss: 2.2952
Profiling... [1024/50048]	Loss: 2.2257
Profiling... [1152/50048]	Loss: 2.1684
Profiling... [1280/50048]	Loss: 2.0818
Profiling... [1408/50048]	Loss: 2.2653
Profiling... [1536/50048]	Loss: 2.0890
Profiling... [1664/50048]	Loss: 2.0543
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 8, Average loss: 0.0245, Accuracy: 0.3240
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.8896663705301,
                        "time": 2.4008755989998463,
                        "accuracy": 0.3239715189873418,
                        "total_cost": 1296883.229545216
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7955
Profiling... [256/50048]	Loss: 1.8844
Profiling... [384/50048]	Loss: 1.9008
Profiling... [512/50048]	Loss: 1.9773
Profiling... [640/50048]	Loss: 2.2203
Profiling... [768/50048]	Loss: 1.8876
Profiling... [896/50048]	Loss: 2.1693
Profiling... [1024/50048]	Loss: 1.9415
Profiling... [1152/50048]	Loss: 2.2828
Profiling... [1280/50048]	Loss: 1.9502
Profiling... [1408/50048]	Loss: 2.2905
Profiling... [1536/50048]	Loss: 1.8489
Profiling... [1664/50048]	Loss: 1.9736
Profile done
epoch 1 train time consumed: 7.02s
Validation Epoch: 8, Average loss: 0.0232, Accuracy: 0.3412
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.87364106189956,
                        "time": 4.803775692000272,
                        "accuracy": 0.34117879746835444,
                        "total_cost": 2463988.8303083135
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8112
Profiling... [256/50048]	Loss: 2.1635
Profiling... [384/50048]	Loss: 1.7818
Profiling... [512/50048]	Loss: 2.0261
Profiling... [640/50048]	Loss: 2.1312
Profiling... [768/50048]	Loss: 2.1127
Profiling... [896/50048]	Loss: 2.0858
Profiling... [1024/50048]	Loss: 1.9244
Profiling... [1152/50048]	Loss: 2.2959
Profiling... [1280/50048]	Loss: 2.1898
Profiling... [1408/50048]	Loss: 2.1502
Profiling... [1536/50048]	Loss: 2.2173
Profiling... [1664/50048]	Loss: 1.8794
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 8, Average loss: 0.0218, Accuracy: 0.3607
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.84477757142962,
                        "time": 2.183677274000729,
                        "accuracy": 0.3606606012658228,
                        "total_cost": 1059565.4795919086
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8285
Profiling... [256/50048]	Loss: 1.8492
Profiling... [384/50048]	Loss: 2.3496
Profiling... [512/50048]	Loss: 1.9862
Profiling... [640/50048]	Loss: 2.1306
Profiling... [768/50048]	Loss: 2.1163
Profiling... [896/50048]	Loss: 1.8800
Profiling... [1024/50048]	Loss: 1.8777
Profiling... [1152/50048]	Loss: 2.2062
Profiling... [1280/50048]	Loss: 2.1092
Profiling... [1408/50048]	Loss: 2.0606
Profiling... [1536/50048]	Loss: 2.1053
Profiling... [1664/50048]	Loss: 2.0672
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 8, Average loss: 0.0386, Accuracy: 0.1923
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.85685012688114,
                        "time": 2.189236866000101,
                        "accuracy": 0.19234572784810128,
                        "total_cost": 1991811.5979813773
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6241
Profiling... [256/50048]	Loss: 1.6298
Profiling... [384/50048]	Loss: 1.9514
Profiling... [512/50048]	Loss: 2.2457
Profiling... [640/50048]	Loss: 1.9858
Profiling... [768/50048]	Loss: 1.9755
Profiling... [896/50048]	Loss: 2.3141
Profiling... [1024/50048]	Loss: 1.9188
Profiling... [1152/50048]	Loss: 2.1774
Profiling... [1280/50048]	Loss: 2.0916
Profiling... [1408/50048]	Loss: 2.0151
Profiling... [1536/50048]	Loss: 2.2055
Profiling... [1664/50048]	Loss: 2.1099
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 8, Average loss: 0.0297, Accuracy: 0.2486
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.86306923850319,
                        "time": 2.386896137999429,
                        "accuracy": 0.24861550632911392,
                        "total_cost": 1680131.8241065193
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8535
Profiling... [256/50048]	Loss: 2.0163
Profiling... [384/50048]	Loss: 1.8928
Profiling... [512/50048]	Loss: 1.7985
Profiling... [640/50048]	Loss: 2.1717
Profiling... [768/50048]	Loss: 2.0308
Profiling... [896/50048]	Loss: 2.1296
Profiling... [1024/50048]	Loss: 2.0255
Profiling... [1152/50048]	Loss: 1.9827
Profiling... [1280/50048]	Loss: 2.0285
Profiling... [1408/50048]	Loss: 1.9426
Profiling... [1536/50048]	Loss: 2.0233
Profiling... [1664/50048]	Loss: 2.0681
Profile done
epoch 1 train time consumed: 7.08s
Validation Epoch: 8, Average loss: 0.0272, Accuracy: 0.3062
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.84611951412097,
                        "time": 4.908565181000085,
                        "accuracy": 0.3061708860759494,
                        "total_cost": 2805619.1680548294
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7159
Profiling... [256/50048]	Loss: 1.8923
Profiling... [384/50048]	Loss: 1.8696
Profiling... [512/50048]	Loss: 2.3419
Profiling... [640/50048]	Loss: 2.2050
Profiling... [768/50048]	Loss: 2.0854
Profiling... [896/50048]	Loss: 2.2926
Profiling... [1024/50048]	Loss: 1.9461
Profiling... [1152/50048]	Loss: 1.8566
Profiling... [1280/50048]	Loss: 2.3901
Profiling... [1408/50048]	Loss: 2.3168
Profiling... [1536/50048]	Loss: 2.1596
Profiling... [1664/50048]	Loss: 1.7839
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 8, Average loss: 0.0209, Accuracy: 0.3804
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.8203366873744,
                        "time": 2.173728337999819,
                        "accuracy": 0.380439082278481,
                        "total_cost": 999903.7345787573
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9536
Profiling... [256/50048]	Loss: 1.9850
Profiling... [384/50048]	Loss: 1.9009
Profiling... [512/50048]	Loss: 1.8421
Profiling... [640/50048]	Loss: 2.0893
Profiling... [768/50048]	Loss: 2.1033
Profiling... [896/50048]	Loss: 2.0600
Profiling... [1024/50048]	Loss: 2.2266
Profiling... [1152/50048]	Loss: 2.0281
Profiling... [1280/50048]	Loss: 2.4329
Profiling... [1408/50048]	Loss: 2.0609
Profiling... [1536/50048]	Loss: 1.9670
Profiling... [1664/50048]	Loss: 2.2260
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 8, Average loss: 0.0263, Accuracy: 0.3044
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.83205654165113,
                        "time": 2.17345886899966,
                        "accuracy": 0.3043908227848101,
                        "total_cost": 1249562.3179278097
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8708
Profiling... [256/50048]	Loss: 1.9251
Profiling... [384/50048]	Loss: 1.8519
Profiling... [512/50048]	Loss: 2.4522
Profiling... [640/50048]	Loss: 1.8242
Profiling... [768/50048]	Loss: 2.0121
Profiling... [896/50048]	Loss: 2.1238
Profiling... [1024/50048]	Loss: 2.1434
Profiling... [1152/50048]	Loss: 2.0107
Profiling... [1280/50048]	Loss: 1.9460
Profiling... [1408/50048]	Loss: 2.2482
Profiling... [1536/50048]	Loss: 1.8044
Profiling... [1664/50048]	Loss: 1.9619
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 8, Average loss: 0.0274, Accuracy: 0.2695
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.83846164599018,
                        "time": 2.3833417510004438,
                        "accuracy": 0.26948180379746833,
                        "total_cost": 1547729.0137872975
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8566
Profiling... [256/50048]	Loss: 1.8570
Profiling... [384/50048]	Loss: 1.8491
Profiling... [512/50048]	Loss: 1.9741
Profiling... [640/50048]	Loss: 1.8665
Profiling... [768/50048]	Loss: 2.0142
Profiling... [896/50048]	Loss: 1.9063
Profiling... [1024/50048]	Loss: 2.1614
Profiling... [1152/50048]	Loss: 1.9333
Profiling... [1280/50048]	Loss: 2.1071
Profiling... [1408/50048]	Loss: 2.1938
Profiling... [1536/50048]	Loss: 1.8216
Profiling... [1664/50048]	Loss: 2.0600
Profile done
epoch 1 train time consumed: 7.30s
Validation Epoch: 8, Average loss: 0.0264, Accuracy: 0.2945
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.82193510276127,
                        "time": 5.10369263299981,
                        "accuracy": 0.294501582278481,
                        "total_cost": 3032738.2415569047
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7166
Profiling... [512/50176]	Loss: 1.5462
Profiling... [768/50176]	Loss: 1.7516
Profiling... [1024/50176]	Loss: 1.7937
Profiling... [1280/50176]	Loss: 1.7477
Profiling... [1536/50176]	Loss: 1.7058
Profiling... [1792/50176]	Loss: 1.7792
Profiling... [2048/50176]	Loss: 1.9781
Profiling... [2304/50176]	Loss: 1.6645
Profiling... [2560/50176]	Loss: 1.7084
Profiling... [2816/50176]	Loss: 1.6154
Profiling... [3072/50176]	Loss: 1.6614
Profiling... [3328/50176]	Loss: 1.8768
Profile done
epoch 1 train time consumed: 4.09s
Validation Epoch: 8, Average loss: 0.0069, Accuracy: 0.5165
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.79774320254089,
                        "time": 2.408888371000103,
                        "accuracy": 0.51650390625,
                        "total_cost": 816170.9133734513
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7856
Profiling... [512/50176]	Loss: 1.7292
Profiling... [768/50176]	Loss: 1.7770
Profiling... [1024/50176]	Loss: 1.6961
Profiling... [1280/50176]	Loss: 1.5290
Profiling... [1536/50176]	Loss: 1.6869
Profiling... [1792/50176]	Loss: 1.7800
Profiling... [2048/50176]	Loss: 1.8396
Profiling... [2304/50176]	Loss: 1.6265
Profiling... [2560/50176]	Loss: 1.7625
Profiling... [2816/50176]	Loss: 1.8900
Profiling... [3072/50176]	Loss: 1.6967
Profiling... [3328/50176]	Loss: 1.7432
Profile done
epoch 1 train time consumed: 4.01s
Validation Epoch: 8, Average loss: 0.0070, Accuracy: 0.5114
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.81240421297296,
                        "time": 2.439915404000203,
                        "accuracy": 0.51142578125,
                        "total_cost": 834891.8090449425
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9666
Profiling... [512/50176]	Loss: 1.9239
Profiling... [768/50176]	Loss: 1.8122
Profiling... [1024/50176]	Loss: 1.8144
Profiling... [1280/50176]	Loss: 1.7461
Profiling... [1536/50176]	Loss: 1.6487
Profiling... [1792/50176]	Loss: 1.8167
Profiling... [2048/50176]	Loss: 1.8669
Profiling... [2304/50176]	Loss: 1.7069
Profiling... [2560/50176]	Loss: 1.6280
Profiling... [2816/50176]	Loss: 1.9590
Profiling... [3072/50176]	Loss: 1.8292
Profiling... [3328/50176]	Loss: 1.8651
Profile done
epoch 1 train time consumed: 4.40s
Validation Epoch: 8, Average loss: 0.0069, Accuracy: 0.5131
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.82053918379596,
                        "time": 2.7553305689998524,
                        "accuracy": 0.5130859375,
                        "total_cost": 939770.152197894
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8334
Profiling... [512/50176]	Loss: 1.8639
Profiling... [768/50176]	Loss: 1.8422
Profiling... [1024/50176]	Loss: 1.6823
Profiling... [1280/50176]	Loss: 1.5921
Profiling... [1536/50176]	Loss: 1.8234
Profiling... [1792/50176]	Loss: 1.8802
Profiling... [2048/50176]	Loss: 1.8625
Profiling... [2304/50176]	Loss: 1.7108
Profiling... [2560/50176]	Loss: 1.6907
Profiling... [2816/50176]	Loss: 1.6863
Profiling... [3072/50176]	Loss: 1.8445
Profiling... [3328/50176]	Loss: 1.8258
Profile done
epoch 1 train time consumed: 9.93s
Validation Epoch: 8, Average loss: 0.0069, Accuracy: 0.5174
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.8046602488112,
                        "time": 7.136113847000161,
                        "accuracy": 0.5173828125,
                        "total_cost": 2413725.1819222895
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6515
Profiling... [512/50176]	Loss: 1.8578
Profiling... [768/50176]	Loss: 1.9268
Profiling... [1024/50176]	Loss: 1.7057
Profiling... [1280/50176]	Loss: 1.6596
Profiling... [1536/50176]	Loss: 1.8668
Profiling... [1792/50176]	Loss: 1.9071
Profiling... [2048/50176]	Loss: 1.8394
Profiling... [2304/50176]	Loss: 1.6448
Profiling... [2560/50176]	Loss: 1.7850
Profiling... [2816/50176]	Loss: 1.7546
Profiling... [3072/50176]	Loss: 1.6664
Profiling... [3328/50176]	Loss: 1.8303
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 8, Average loss: 0.0068, Accuracy: 0.5211
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.77445999444456,
                        "time": 2.396941986999991,
                        "accuracy": 0.52109375,
                        "total_cost": 804970.0226206866
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6204
Profiling... [512/50176]	Loss: 1.8562
Profiling... [768/50176]	Loss: 1.7486
Profiling... [1024/50176]	Loss: 1.7969
Profiling... [1280/50176]	Loss: 1.8423
Profiling... [1536/50176]	Loss: 1.6978
Profiling... [1792/50176]	Loss: 1.7843
Profiling... [2048/50176]	Loss: 2.0007
Profiling... [2304/50176]	Loss: 1.6269
Profiling... [2560/50176]	Loss: 1.7215
Profiling... [2816/50176]	Loss: 1.7047
Profiling... [3072/50176]	Loss: 1.6977
Profiling... [3328/50176]	Loss: 1.8393
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 8, Average loss: 0.0068, Accuracy: 0.5216
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.79002410817311,
                        "time": 2.438841481000054,
                        "accuracy": 0.52158203125,
                        "total_cost": 818274.4680681701
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7788
Profiling... [512/50176]	Loss: 1.8293
Profiling... [768/50176]	Loss: 1.8435
Profiling... [1024/50176]	Loss: 1.7988
Profiling... [1280/50176]	Loss: 1.7874
Profiling... [1536/50176]	Loss: 1.6336
Profiling... [1792/50176]	Loss: 1.9349
Profiling... [2048/50176]	Loss: 1.7357
Profiling... [2304/50176]	Loss: 1.7719
Profiling... [2560/50176]	Loss: 1.9108
Profiling... [2816/50176]	Loss: 1.8406
Profiling... [3072/50176]	Loss: 1.6586
Profiling... [3328/50176]	Loss: 1.6728
Profile done
epoch 1 train time consumed: 4.42s
Validation Epoch: 8, Average loss: 0.0068, Accuracy: 0.5202
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.79836527767107,
                        "time": 2.7408685909995256,
                        "accuracy": 0.52021484375,
                        "total_cost": 922026.7533454383
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7786
Profiling... [512/50176]	Loss: 1.7821
Profiling... [768/50176]	Loss: 1.9027
Profiling... [1024/50176]	Loss: 1.8886
Profiling... [1280/50176]	Loss: 1.7489
Profiling... [1536/50176]	Loss: 1.9180
Profiling... [1792/50176]	Loss: 2.0358
Profiling... [2048/50176]	Loss: 1.7925
Profiling... [2304/50176]	Loss: 1.8015
Profiling... [2560/50176]	Loss: 1.8269
Profiling... [2816/50176]	Loss: 1.8167
Profiling... [3072/50176]	Loss: 1.8210
Profiling... [3328/50176]	Loss: 1.8540
Profile done
epoch 1 train time consumed: 10.28s
Validation Epoch: 8, Average loss: 0.0068, Accuracy: 0.5240
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.7790733455546,
                        "time": 7.458632722999937,
                        "accuracy": 0.5240234375,
                        "total_cost": 2490844.1743600233
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7213
Profiling... [512/50176]	Loss: 1.7159
Profiling... [768/50176]	Loss: 1.8320
Profiling... [1024/50176]	Loss: 1.8142
Profiling... [1280/50176]	Loss: 1.7943
Profiling... [1536/50176]	Loss: 1.7460
Profiling... [1792/50176]	Loss: 1.4650
Profiling... [2048/50176]	Loss: 1.7755
Profiling... [2304/50176]	Loss: 1.9207
Profiling... [2560/50176]	Loss: 1.7778
Profiling... [2816/50176]	Loss: 1.7051
Profiling... [3072/50176]	Loss: 1.8255
Profiling... [3328/50176]	Loss: 1.9030
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 8, Average loss: 0.0069, Accuracy: 0.5180
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.75284830980411,
                        "time": 2.405114873000457,
                        "accuracy": 0.51796875,
                        "total_cost": 812587.8303953279
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8615
Profiling... [512/50176]	Loss: 1.7955
Profiling... [768/50176]	Loss: 1.7949
Profiling... [1024/50176]	Loss: 1.8792
Profiling... [1280/50176]	Loss: 1.8121
Profiling... [1536/50176]	Loss: 1.7804
Profiling... [1792/50176]	Loss: 1.8763
Profiling... [2048/50176]	Loss: 1.7476
Profiling... [2304/50176]	Loss: 1.8622
Profiling... [2560/50176]	Loss: 1.7512
Profiling... [2816/50176]	Loss: 1.7448
Profiling... [3072/50176]	Loss: 1.7423
Profiling... [3328/50176]	Loss: 1.8886
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 8, Average loss: 0.0068, Accuracy: 0.5201
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.7678900626165,
                        "time": 2.439604381999743,
                        "accuracy": 0.5201171875,
                        "total_cost": 820835.7214689334
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6861
Profiling... [512/50176]	Loss: 1.6243
Profiling... [768/50176]	Loss: 1.7715
Profiling... [1024/50176]	Loss: 1.8689
Profiling... [1280/50176]	Loss: 1.6720
Profiling... [1536/50176]	Loss: 1.9027
Profiling... [1792/50176]	Loss: 1.7843
Profiling... [2048/50176]	Loss: 1.7128
Profiling... [2304/50176]	Loss: 1.8242
Profiling... [2560/50176]	Loss: 1.7200
Profiling... [2816/50176]	Loss: 1.7913
Profiling... [3072/50176]	Loss: 1.4648
Profiling... [3328/50176]	Loss: 1.8084
Profile done
epoch 1 train time consumed: 4.37s
Validation Epoch: 8, Average loss: 0.0069, Accuracy: 0.5173
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.77516064972706,
                        "time": 2.748277978999795,
                        "accuracy": 0.51728515625,
                        "total_cost": 929755.3593293623
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8147
Profiling... [512/50176]	Loss: 1.7868
Profiling... [768/50176]	Loss: 1.6690
Profiling... [1024/50176]	Loss: 1.8774
Profiling... [1280/50176]	Loss: 1.7988
Profiling... [1536/50176]	Loss: 1.6188
Profiling... [1792/50176]	Loss: 1.8798
Profiling... [2048/50176]	Loss: 1.7174
Profiling... [2304/50176]	Loss: 1.7303
Profiling... [2560/50176]	Loss: 1.6735
Profiling... [2816/50176]	Loss: 1.8027
Profiling... [3072/50176]	Loss: 1.7492
Profiling... [3328/50176]	Loss: 1.8557
Profile done
epoch 1 train time consumed: 9.97s
Validation Epoch: 8, Average loss: 0.0069, Accuracy: 0.5135
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.76021818529992,
                        "time": 7.005227513000136,
                        "accuracy": 0.5134765625,
                        "total_cost": 2387479.593628042
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6589
Profiling... [512/50176]	Loss: 1.7626
Profiling... [768/50176]	Loss: 1.9540
Profiling... [1024/50176]	Loss: 1.9296
Profiling... [1280/50176]	Loss: 1.8293
Profiling... [1536/50176]	Loss: 2.0685
Profiling... [1792/50176]	Loss: 1.8123
Profiling... [2048/50176]	Loss: 1.6748
Profiling... [2304/50176]	Loss: 2.0159
Profiling... [2560/50176]	Loss: 1.9790
Profiling... [2816/50176]	Loss: 1.7826
Profiling... [3072/50176]	Loss: 1.7579
Profiling... [3328/50176]	Loss: 1.8394
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 8, Average loss: 0.0076, Accuracy: 0.4863
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.73056270961412,
                        "time": 2.405961897999987,
                        "accuracy": 0.486328125,
                        "total_cost": 865759.7833767021
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8460
Profiling... [512/50176]	Loss: 1.7911
Profiling... [768/50176]	Loss: 2.0966
Profiling... [1024/50176]	Loss: 1.8302
Profiling... [1280/50176]	Loss: 1.7059
Profiling... [1536/50176]	Loss: 1.8782
Profiling... [1792/50176]	Loss: 1.7518
Profiling... [2048/50176]	Loss: 1.7596
Profiling... [2304/50176]	Loss: 1.7368
Profiling... [2560/50176]	Loss: 1.9091
Profiling... [2816/50176]	Loss: 1.9613
Profiling... [3072/50176]	Loss: 2.0769
Profiling... [3328/50176]	Loss: 2.1272
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 8, Average loss: 0.0076, Accuracy: 0.4798
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.74585172695612,
                        "time": 2.4396785179997096,
                        "accuracy": 0.47978515625,
                        "total_cost": 889864.4217902462
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8513
Profiling... [512/50176]	Loss: 1.8088
Profiling... [768/50176]	Loss: 1.7573
Profiling... [1024/50176]	Loss: 1.9419
Profiling... [1280/50176]	Loss: 1.8883
Profiling... [1536/50176]	Loss: 1.8283
Profiling... [1792/50176]	Loss: 1.9360
Profiling... [2048/50176]	Loss: 1.9929
Profiling... [2304/50176]	Loss: 1.7550
Profiling... [2560/50176]	Loss: 1.7606
Profiling... [2816/50176]	Loss: 2.0329
Profiling... [3072/50176]	Loss: 1.9261
Profiling... [3328/50176]	Loss: 1.9768
Profile done
epoch 1 train time consumed: 4.46s
Validation Epoch: 8, Average loss: 0.0088, Accuracy: 0.4354
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.75401039702788,
                        "time": 2.750162240999998,
                        "accuracy": 0.4353515625,
                        "total_cost": 1105493.6599084784
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8057
Profiling... [512/50176]	Loss: 1.8640
Profiling... [768/50176]	Loss: 1.6687
Profiling... [1024/50176]	Loss: 1.7284
Profiling... [1280/50176]	Loss: 1.8710
Profiling... [1536/50176]	Loss: 1.6846
Profiling... [1792/50176]	Loss: 1.8700
Profiling... [2048/50176]	Loss: 1.8380
Profiling... [2304/50176]	Loss: 2.2002
Profiling... [2560/50176]	Loss: 1.8700
Profiling... [2816/50176]	Loss: 2.1215
Profiling... [3072/50176]	Loss: 1.7009
Profiling... [3328/50176]	Loss: 1.9184
Profile done
epoch 1 train time consumed: 9.95s
Validation Epoch: 8, Average loss: 0.0084, Accuracy: 0.4469
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.73686165457312,
                        "time": 7.105153088000407,
                        "accuracy": 0.446875,
                        "total_cost": 2782437.5729232356
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7179
Profiling... [512/50176]	Loss: 1.9476
Profiling... [768/50176]	Loss: 1.8097
Profiling... [1024/50176]	Loss: 1.6616
Profiling... [1280/50176]	Loss: 1.8991
Profiling... [1536/50176]	Loss: 1.9034
Profiling... [1792/50176]	Loss: 1.9024
Profiling... [2048/50176]	Loss: 1.9328
Profiling... [2304/50176]	Loss: 1.9243
Profiling... [2560/50176]	Loss: 1.9056
Profiling... [2816/50176]	Loss: 1.8467
Profiling... [3072/50176]	Loss: 2.0595
Profiling... [3328/50176]	Loss: 1.7272
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 8, Average loss: 0.0090, Accuracy: 0.4295
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.70837194894321,
                        "time": 2.4238250329999573,
                        "accuracy": 0.4294921875,
                        "total_cost": 987606.7437780635
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8032
Profiling... [512/50176]	Loss: 1.8301
Profiling... [768/50176]	Loss: 1.7443
Profiling... [1024/50176]	Loss: 1.8167
Profiling... [1280/50176]	Loss: 1.7248
Profiling... [1536/50176]	Loss: 1.8242
Profiling... [1792/50176]	Loss: 1.9292
Profiling... [2048/50176]	Loss: 1.8001
Profiling... [2304/50176]	Loss: 1.8502
Profiling... [2560/50176]	Loss: 1.7638
Profiling... [2816/50176]	Loss: 1.8212
Profiling... [3072/50176]	Loss: 1.8630
Profiling... [3328/50176]	Loss: 1.8323
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 8, Average loss: 0.0083, Accuracy: 0.4429
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.72393045239416,
                        "time": 2.433600473000297,
                        "accuracy": 0.44287109375,
                        "total_cost": 961634.4096177579
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8105
Profiling... [512/50176]	Loss: 1.8481
Profiling... [768/50176]	Loss: 1.7418
Profiling... [1024/50176]	Loss: 1.8631
Profiling... [1280/50176]	Loss: 1.7631
Profiling... [1536/50176]	Loss: 1.9393
Profiling... [1792/50176]	Loss: 1.7095
Profiling... [2048/50176]	Loss: 1.7481
Profiling... [2304/50176]	Loss: 1.6842
Profiling... [2560/50176]	Loss: 1.9445
Profiling... [2816/50176]	Loss: 1.6716
Profiling... [3072/50176]	Loss: 1.8430
Profiling... [3328/50176]	Loss: 1.9758
Profile done
epoch 1 train time consumed: 4.41s
Validation Epoch: 8, Average loss: 0.0085, Accuracy: 0.4499
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.7315358368806,
                        "time": 2.7341164260005826,
                        "accuracy": 0.44990234375,
                        "total_cost": 1063498.2929005956
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7298
Profiling... [512/50176]	Loss: 2.0580
Profiling... [768/50176]	Loss: 1.7082
Profiling... [1024/50176]	Loss: 1.8045
Profiling... [1280/50176]	Loss: 1.7743
Profiling... [1536/50176]	Loss: 1.7712
Profiling... [1792/50176]	Loss: 1.8766
Profiling... [2048/50176]	Loss: 1.7622
Profiling... [2304/50176]	Loss: 1.7341
Profiling... [2560/50176]	Loss: 1.7392
Profiling... [2816/50176]	Loss: 2.0267
Profiling... [3072/50176]	Loss: 2.0050
Profiling... [3328/50176]	Loss: 1.6104
Profile done
epoch 1 train time consumed: 10.36s
Validation Epoch: 8, Average loss: 0.0084, Accuracy: 0.4410
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.71468826826886,
                        "time": 7.12442564399953,
                        "accuracy": 0.441015625,
                        "total_cost": 2827052.868478113
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7379
Profiling... [512/50176]	Loss: 1.9139
Profiling... [768/50176]	Loss: 1.7853
Profiling... [1024/50176]	Loss: 1.7734
Profiling... [1280/50176]	Loss: 2.0495
Profiling... [1536/50176]	Loss: 1.7278
Profiling... [1792/50176]	Loss: 1.6429
Profiling... [2048/50176]	Loss: 1.8223
Profiling... [2304/50176]	Loss: 1.8902
Profiling... [2560/50176]	Loss: 1.8051
Profiling... [2816/50176]	Loss: 1.8668
Profiling... [3072/50176]	Loss: 1.8544
Profiling... [3328/50176]	Loss: 1.9273
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 8, Average loss: 0.0094, Accuracy: 0.4109
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.68658980153303,
                        "time": 2.4048032219998277,
                        "accuracy": 0.4109375,
                        "total_cost": 1024098.7105094322
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6624
Profiling... [512/50176]	Loss: 1.8481
Profiling... [768/50176]	Loss: 1.8547
Profiling... [1024/50176]	Loss: 1.6764
Profiling... [1280/50176]	Loss: 1.7092
Profiling... [1536/50176]	Loss: 1.7656
Profiling... [1792/50176]	Loss: 1.8661
Profiling... [2048/50176]	Loss: 1.6757
Profiling... [2304/50176]	Loss: 1.8089
Profiling... [2560/50176]	Loss: 1.8384
Profiling... [2816/50176]	Loss: 1.7464
Profiling... [3072/50176]	Loss: 1.8977
Profiling... [3328/50176]	Loss: 1.8818
Profile done
epoch 1 train time consumed: 4.13s
Validation Epoch: 8, Average loss: 0.0074, Accuracy: 0.4856
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.70267141558797,
                        "time": 2.4347370109999247,
                        "accuracy": 0.48564453125,
                        "total_cost": 877347.4208147727
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8993
Profiling... [512/50176]	Loss: 1.7909
Profiling... [768/50176]	Loss: 1.7606
Profiling... [1024/50176]	Loss: 1.7360
Profiling... [1280/50176]	Loss: 1.8741
Profiling... [1536/50176]	Loss: 1.8121
Profiling... [1792/50176]	Loss: 1.5972
Profiling... [2048/50176]	Loss: 1.9903
Profiling... [2304/50176]	Loss: 1.9809
Profiling... [2560/50176]	Loss: 1.9302
Profiling... [2816/50176]	Loss: 2.1192
Profiling... [3072/50176]	Loss: 1.8091
Profiling... [3328/50176]	Loss: 2.0925
Profile done
epoch 1 train time consumed: 4.45s
Validation Epoch: 8, Average loss: 0.0088, Accuracy: 0.4310
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.71045294791368,
                        "time": 2.7472795989997394,
                        "accuracy": 0.43095703125,
                        "total_cost": 1115595.975845804
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7586
Profiling... [512/50176]	Loss: 1.7270
Profiling... [768/50176]	Loss: 1.8943
Profiling... [1024/50176]	Loss: 1.7508
Profiling... [1280/50176]	Loss: 1.8277
Profiling... [1536/50176]	Loss: 1.8189
Profiling... [1792/50176]	Loss: 1.8533
Profiling... [2048/50176]	Loss: 1.8791
Profiling... [2304/50176]	Loss: 1.9211
Profiling... [2560/50176]	Loss: 1.8395
Profiling... [2816/50176]	Loss: 1.9067
Profiling... [3072/50176]	Loss: 1.9251
Profiling... [3328/50176]	Loss: 1.7382
Profile done
epoch 1 train time consumed: 9.92s
Validation Epoch: 8, Average loss: 0.0079, Accuracy: 0.4701
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.6943744281936,
                        "time": 7.058131218000199,
                        "accuracy": 0.4701171875,
                        "total_cost": 2627372.4849722385
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8148
Profiling... [512/50176]	Loss: 1.8117
Profiling... [768/50176]	Loss: 2.3114
Profiling... [1024/50176]	Loss: 2.1687
Profiling... [1280/50176]	Loss: 1.9336
Profiling... [1536/50176]	Loss: 1.8504
Profiling... [1792/50176]	Loss: 1.7972
Profiling... [2048/50176]	Loss: 1.9207
Profiling... [2304/50176]	Loss: 1.9816
Profiling... [2560/50176]	Loss: 1.8102
Profiling... [2816/50176]	Loss: 2.0606
Profiling... [3072/50176]	Loss: 1.8146
Profiling... [3328/50176]	Loss: 2.0892
Profile done
epoch 1 train time consumed: 4.09s
Validation Epoch: 8, Average loss: 0.0405, Accuracy: 0.0806
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.66492770236822,
                        "time": 2.401674543999434,
                        "accuracy": 0.08056640625,
                        "total_cost": 5216728.22163271
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7205
Profiling... [512/50176]	Loss: 1.7916
Profiling... [768/50176]	Loss: 1.8985
Profiling... [1024/50176]	Loss: 1.9410
Profiling... [1280/50176]	Loss: 1.9401
Profiling... [1536/50176]	Loss: 1.9944
Profiling... [1792/50176]	Loss: 1.9016
Profiling... [2048/50176]	Loss: 1.9364
Profiling... [2304/50176]	Loss: 2.0276
Profiling... [2560/50176]	Loss: 1.9730
Profiling... [2816/50176]	Loss: 1.9781
Profiling... [3072/50176]	Loss: 2.0508
Profiling... [3328/50176]	Loss: 1.9933
Profile done
epoch 1 train time consumed: 4.13s
Validation Epoch: 8, Average loss: 0.0132, Accuracy: 0.3060
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.6803543008816,
                        "time": 2.4368188419994112,
                        "accuracy": 0.30595703125,
                        "total_cost": 1393801.2655164204
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6457
Profiling... [512/50176]	Loss: 1.7844
Profiling... [768/50176]	Loss: 1.9142
Profiling... [1024/50176]	Loss: 1.8019
Profiling... [1280/50176]	Loss: 2.0206
Profiling... [1536/50176]	Loss: 1.9814
Profiling... [1792/50176]	Loss: 1.8832
Profiling... [2048/50176]	Loss: 1.9990
Profiling... [2304/50176]	Loss: 1.9111
Profiling... [2560/50176]	Loss: 1.9853
Profiling... [2816/50176]	Loss: 1.9623
Profiling... [3072/50176]	Loss: 1.9256
Profiling... [3328/50176]	Loss: 1.9535
Profile done
epoch 1 train time consumed: 4.44s
Validation Epoch: 8, Average loss: 0.0141, Accuracy: 0.2851
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.68656386265042,
                        "time": 2.7604821479999373,
                        "accuracy": 0.28505859375,
                        "total_cost": 1694684.4841438467
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8607
Profiling... [512/50176]	Loss: 1.5942
Profiling... [768/50176]	Loss: 2.0544
Profiling... [1024/50176]	Loss: 1.9593
Profiling... [1280/50176]	Loss: 2.0633
Profiling... [1536/50176]	Loss: 2.0010
Profiling... [1792/50176]	Loss: 1.8803
Profiling... [2048/50176]	Loss: 1.9620
Profiling... [2304/50176]	Loss: 2.1885
Profiling... [2560/50176]	Loss: 1.8909
Profiling... [2816/50176]	Loss: 1.7592
Profiling... [3072/50176]	Loss: 2.0381
Profiling... [3328/50176]	Loss: 1.8260
Profile done
epoch 1 train time consumed: 10.01s
Validation Epoch: 8, Average loss: 0.0282, Accuracy: 0.1212
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.66940032165736,
                        "time": 6.811147669000093,
                        "accuracy": 0.12119140625,
                        "total_cost": 9835275.280296668
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7451
Profiling... [512/50176]	Loss: 2.0026
Profiling... [768/50176]	Loss: 2.1381
Profiling... [1024/50176]	Loss: 1.8575
Profiling... [1280/50176]	Loss: 1.9865
Profiling... [1536/50176]	Loss: 2.0547
Profiling... [1792/50176]	Loss: 2.1717
Profiling... [2048/50176]	Loss: 1.9959
Profiling... [2304/50176]	Loss: 2.1134
Profiling... [2560/50176]	Loss: 1.8688
Profiling... [2816/50176]	Loss: 1.9933
Profiling... [3072/50176]	Loss: 2.0027
Profiling... [3328/50176]	Loss: 1.7714
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 8, Average loss: 0.0091, Accuracy: 0.3989
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.63837256104206,
                        "time": 2.399334120000276,
                        "accuracy": 0.39892578125,
                        "total_cost": 1052535.3104138297
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8224
Profiling... [512/50176]	Loss: 1.7636
Profiling... [768/50176]	Loss: 1.9637
Profiling... [1024/50176]	Loss: 1.7910
Profiling... [1280/50176]	Loss: 1.9523
Profiling... [1536/50176]	Loss: 1.8877
Profiling... [1792/50176]	Loss: 1.9428
Profiling... [2048/50176]	Loss: 1.9100
Profiling... [2304/50176]	Loss: 1.9170
Profiling... [2560/50176]	Loss: 1.7968
Profiling... [2816/50176]	Loss: 1.8142
Profiling... [3072/50176]	Loss: 1.7075
Profiling... [3328/50176]	Loss: 2.0652
Profile done
epoch 1 train time consumed: 4.23s
Validation Epoch: 8, Average loss: 0.0104, Accuracy: 0.3613
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.6522935173417,
                        "time": 2.4382354089993896,
                        "accuracy": 0.361328125,
                        "total_cost": 1180896.717007272
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6184
Profiling... [512/50176]	Loss: 1.8282
Profiling... [768/50176]	Loss: 1.9705
Profiling... [1024/50176]	Loss: 1.9012
Profiling... [1280/50176]	Loss: 2.0927
Profiling... [1536/50176]	Loss: 1.9069
Profiling... [1792/50176]	Loss: 2.0431
Profiling... [2048/50176]	Loss: 2.0149
Profiling... [2304/50176]	Loss: 1.9767
Profiling... [2560/50176]	Loss: 1.9076
Profiling... [2816/50176]	Loss: 1.8796
Profiling... [3072/50176]	Loss: 1.9645
Profiling... [3328/50176]	Loss: 1.9967
Profile done
epoch 1 train time consumed: 4.34s
Validation Epoch: 8, Average loss: 0.0135, Accuracy: 0.2711
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.65746724849028,
                        "time": 2.764638098999967,
                        "accuracy": 0.27109375,
                        "total_cost": 1784665.5163573273
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8189
Profiling... [512/50176]	Loss: 1.8886
Profiling... [768/50176]	Loss: 1.9353
Profiling... [1024/50176]	Loss: 1.8472
Profiling... [1280/50176]	Loss: 1.9795
Profiling... [1536/50176]	Loss: 1.9800
Profiling... [1792/50176]	Loss: 1.9068
Profiling... [2048/50176]	Loss: 1.9204
Profiling... [2304/50176]	Loss: 1.8944
Profiling... [2560/50176]	Loss: 1.9876
Profiling... [2816/50176]	Loss: 2.0539
Profiling... [3072/50176]	Loss: 1.9611
Profiling... [3328/50176]	Loss: 1.8949
Profile done
epoch 1 train time consumed: 10.01s
Validation Epoch: 8, Average loss: 0.0110, Accuracy: 0.3556
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.64127540964502,
                        "time": 7.142516105000141,
                        "accuracy": 0.35556640625,
                        "total_cost": 3515349.8654656005
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7968
Profiling... [512/50176]	Loss: 1.9231
Profiling... [768/50176]	Loss: 2.0339
Profiling... [1024/50176]	Loss: 2.0859
Profiling... [1280/50176]	Loss: 1.8067
Profiling... [1536/50176]	Loss: 2.1493
Profiling... [1792/50176]	Loss: 2.1065
Profiling... [2048/50176]	Loss: 1.9742
Profiling... [2304/50176]	Loss: 2.0879
Profiling... [2560/50176]	Loss: 2.1305
Profiling... [2816/50176]	Loss: 2.0606
Profiling... [3072/50176]	Loss: 1.9779
Profiling... [3328/50176]	Loss: 1.9517
Profile done
epoch 1 train time consumed: 4.12s
Validation Epoch: 8, Average loss: 0.0151, Accuracy: 0.2602
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.61359371599477,
                        "time": 2.409165058000326,
                        "accuracy": 0.26015625,
                        "total_cost": 1620579.498474694
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6483
Profiling... [512/50176]	Loss: 1.7262
Profiling... [768/50176]	Loss: 1.8984
Profiling... [1024/50176]	Loss: 2.0283
Profiling... [1280/50176]	Loss: 1.9213
Profiling... [1536/50176]	Loss: 2.0485
Profiling... [1792/50176]	Loss: 1.8625
Profiling... [2048/50176]	Loss: 2.0049
Profiling... [2304/50176]	Loss: 2.0027
Profiling... [2560/50176]	Loss: 2.0948
Profiling... [2816/50176]	Loss: 2.0997
Profiling... [3072/50176]	Loss: 1.9590
Profiling... [3328/50176]	Loss: 2.1337
Profile done
epoch 1 train time consumed: 4.48s
Validation Epoch: 8, Average loss: 0.0113, Accuracy: 0.3273
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.62486071193517,
                        "time": 2.448458085999846,
                        "accuracy": 0.32734375,
                        "total_cost": 1308960.886071517
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7694
Profiling... [512/50176]	Loss: 1.8934
Profiling... [768/50176]	Loss: 1.9305
Profiling... [1024/50176]	Loss: 1.8785
Profiling... [1280/50176]	Loss: 2.0856
Profiling... [1536/50176]	Loss: 2.1105
Profiling... [1792/50176]	Loss: 2.0157
Profiling... [2048/50176]	Loss: 1.9583
Profiling... [2304/50176]	Loss: 2.2511
Profiling... [2560/50176]	Loss: 1.8419
Profiling... [2816/50176]	Loss: 2.0244
Profiling... [3072/50176]	Loss: 1.8189
Profiling... [3328/50176]	Loss: 1.9111
Profile done
epoch 1 train time consumed: 4.40s
Validation Epoch: 8, Average loss: 0.0117, Accuracy: 0.3175
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.62893990858188,
                        "time": 2.7436421050006174,
                        "accuracy": 0.31748046875,
                        "total_cost": 1512336.7124457418
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9146
Profiling... [512/50176]	Loss: 1.8674
Profiling... [768/50176]	Loss: 1.9489
Profiling... [1024/50176]	Loss: 1.9968
Profiling... [1280/50176]	Loss: 2.0654
Profiling... [1536/50176]	Loss: 2.0355
Profiling... [1792/50176]	Loss: 1.9396
Profiling... [2048/50176]	Loss: 2.0225
Profiling... [2304/50176]	Loss: 1.8544
Profiling... [2560/50176]	Loss: 2.0287
Profiling... [2816/50176]	Loss: 2.0000
Profiling... [3072/50176]	Loss: 1.9558
Profiling... [3328/50176]	Loss: 2.0075
Profile done
epoch 1 train time consumed: 9.93s
Validation Epoch: 8, Average loss: 0.0114, Accuracy: 0.3422
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.61265805236744,
                        "time": 7.034983690000445,
                        "accuracy": 0.3421875,
                        "total_cost": 3597799.8779910957
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9048
Profiling... [1024/50176]	Loss: 1.6899
Profiling... [1536/50176]	Loss: 1.7496
Profiling... [2048/50176]	Loss: 1.7896
Profiling... [2560/50176]	Loss: 1.7395
Profiling... [3072/50176]	Loss: 1.8567
Profiling... [3584/50176]	Loss: 1.7543
Profiling... [4096/50176]	Loss: 1.8001
Profiling... [4608/50176]	Loss: 1.7594
Profiling... [5120/50176]	Loss: 1.6582
Profiling... [5632/50176]	Loss: 1.6588
Profiling... [6144/50176]	Loss: 1.8266
Profiling... [6656/50176]	Loss: 1.6732
Profile done
epoch 1 train time consumed: 7.05s
Validation Epoch: 8, Average loss: 0.0034, Accuracy: 0.5202
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.60453056143449,
                        "time": 4.552707729000758,
                        "accuracy": 0.52021484375,
                        "total_cost": 1531528.487022594
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7300
Profiling... [1024/50176]	Loss: 1.7099
Profiling... [1536/50176]	Loss: 1.9144
Profiling... [2048/50176]	Loss: 1.8481
Profiling... [2560/50176]	Loss: 1.7815
Profiling... [3072/50176]	Loss: 1.7636
Profiling... [3584/50176]	Loss: 1.8004
Profiling... [4096/50176]	Loss: 1.7895
Profiling... [4608/50176]	Loss: 1.7051
Profiling... [5120/50176]	Loss: 1.7898
Profiling... [5632/50176]	Loss: 1.8108
Profiling... [6144/50176]	Loss: 1.8314
Profiling... [6656/50176]	Loss: 1.7497
Profile done
epoch 1 train time consumed: 7.15s
Validation Epoch: 8, Average loss: 0.0034, Accuracy: 0.5182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.62908305399444,
                        "time": 4.658911639999133,
                        "accuracy": 0.5181640625,
                        "total_cost": 1573458.2847490474
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8755
Profiling... [1024/50176]	Loss: 1.7681
Profiling... [1536/50176]	Loss: 1.6384
Profiling... [2048/50176]	Loss: 1.7554
Profiling... [2560/50176]	Loss: 1.7468
Profiling... [3072/50176]	Loss: 1.8830
Profiling... [3584/50176]	Loss: 1.8521
Profiling... [4096/50176]	Loss: 1.6875
Profiling... [4608/50176]	Loss: 1.8077
Profiling... [5120/50176]	Loss: 1.8534
Profiling... [5632/50176]	Loss: 1.7274
Profiling... [6144/50176]	Loss: 1.8062
Profiling... [6656/50176]	Loss: 1.9360
Profile done
epoch 1 train time consumed: 8.00s
Validation Epoch: 8, Average loss: 0.0034, Accuracy: 0.5174
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.63620591785109,
                        "time": 5.291115660998912,
                        "accuracy": 0.5173828125,
                        "total_cost": 1789671.4353548605
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7167
Profiling... [1024/50176]	Loss: 1.7938
Profiling... [1536/50176]	Loss: 1.8492
Profiling... [2048/50176]	Loss: 1.7404
Profiling... [2560/50176]	Loss: 1.8468
Profiling... [3072/50176]	Loss: 1.8681
Profiling... [3584/50176]	Loss: 1.7580
Profiling... [4096/50176]	Loss: 1.7824
Profiling... [4608/50176]	Loss: 1.6728
Profiling... [5120/50176]	Loss: 1.7784
Profiling... [5632/50176]	Loss: 1.8790
Profiling... [6144/50176]	Loss: 1.6655
Profiling... [6656/50176]	Loss: 1.7810
Profile done
epoch 1 train time consumed: 18.08s
Validation Epoch: 8, Average loss: 0.0034, Accuracy: 0.5159
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.59581264841215,
                        "time": 13.014080713001022,
                        "accuracy": 0.51591796875,
                        "total_cost": 4414391.943535459
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8598
Profiling... [1024/50176]	Loss: 1.7598
Profiling... [1536/50176]	Loss: 1.9019
Profiling... [2048/50176]	Loss: 1.7058
Profiling... [2560/50176]	Loss: 1.7424
Profiling... [3072/50176]	Loss: 1.8322
Profiling... [3584/50176]	Loss: 1.8568
Profiling... [4096/50176]	Loss: 1.6846
Profiling... [4608/50176]	Loss: 1.6999
Profiling... [5120/50176]	Loss: 1.7479
Profiling... [5632/50176]	Loss: 1.8055
Profiling... [6144/50176]	Loss: 1.7659
Profiling... [6656/50176]	Loss: 1.7290
Profile done
epoch 1 train time consumed: 7.09s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.5150
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.5852522818936,
                        "time": 4.549823114000901,
                        "accuracy": 0.5150390625,
                        "total_cost": 1545939.1392282166
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7715
Profiling... [1024/50176]	Loss: 1.8811
Profiling... [1536/50176]	Loss: 1.7309
Profiling... [2048/50176]	Loss: 1.8475
Profiling... [2560/50176]	Loss: 1.6688
Profiling... [3072/50176]	Loss: 1.8591
Profiling... [3584/50176]	Loss: 1.7299
Profiling... [4096/50176]	Loss: 1.6325
Profiling... [4608/50176]	Loss: 1.6733
Profiling... [5120/50176]	Loss: 1.7200
Profiling... [5632/50176]	Loss: 1.7822
Profiling... [6144/50176]	Loss: 1.7274
Profiling... [6656/50176]	Loss: 1.6684
Profile done
epoch 1 train time consumed: 7.15s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.5070
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.60863484810375,
                        "time": 4.684929840999757,
                        "accuracy": 0.50703125,
                        "total_cost": 1616986.5706994538
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7203
Profiling... [1024/50176]	Loss: 1.8117
Profiling... [1536/50176]	Loss: 1.7872
Profiling... [2048/50176]	Loss: 1.7404
Profiling... [2560/50176]	Loss: 1.6948
Profiling... [3072/50176]	Loss: 1.7478
Profiling... [3584/50176]	Loss: 1.7575
Profiling... [4096/50176]	Loss: 1.7494
Profiling... [4608/50176]	Loss: 1.8941
Profiling... [5120/50176]	Loss: 1.6441
Profiling... [5632/50176]	Loss: 1.6868
Profiling... [6144/50176]	Loss: 1.7048
Profiling... [6656/50176]	Loss: 1.7297
Profile done
epoch 1 train time consumed: 7.96s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.5176
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.61529720761905,
                        "time": 5.264363360000061,
                        "accuracy": 0.517578125,
                        "total_cost": 1779950.781343417
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6450
Profiling... [1024/50176]	Loss: 1.7386
Profiling... [1536/50176]	Loss: 1.7463
Profiling... [2048/50176]	Loss: 1.8257
Profiling... [2560/50176]	Loss: 1.8029
Profiling... [3072/50176]	Loss: 1.8124
Profiling... [3584/50176]	Loss: 1.7657
Profiling... [4096/50176]	Loss: 1.8547
Profiling... [4608/50176]	Loss: 1.8793
Profiling... [5120/50176]	Loss: 1.8291
Profiling... [5632/50176]	Loss: 1.8320
Profiling... [6144/50176]	Loss: 1.7192
Profiling... [6656/50176]	Loss: 1.8035
Profile done
epoch 1 train time consumed: 17.97s
Validation Epoch: 8, Average loss: 0.0034, Accuracy: 0.5239
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.57712150849335,
                        "time": 13.0488296909989,
                        "accuracy": 0.52392578125,
                        "total_cost": 4358528.016080154
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7174
Profiling... [1024/50176]	Loss: 1.8754
Profiling... [1536/50176]	Loss: 1.8390
Profiling... [2048/50176]	Loss: 1.6840
Profiling... [2560/50176]	Loss: 1.7697
Profiling... [3072/50176]	Loss: 1.8596
Profiling... [3584/50176]	Loss: 1.8081
Profiling... [4096/50176]	Loss: 1.7031
Profiling... [4608/50176]	Loss: 1.8200
Profiling... [5120/50176]	Loss: 1.7717
Profiling... [5632/50176]	Loss: 1.7750
Profiling... [6144/50176]	Loss: 1.8175
Profiling... [6656/50176]	Loss: 1.8220
Profile done
epoch 1 train time consumed: 7.15s
Validation Epoch: 8, Average loss: 0.0034, Accuracy: 0.5224
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.56578560914195,
                        "time": 4.558526886999971,
                        "accuracy": 0.52236328125,
                        "total_cost": 1527178.9458784724
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7753
Profiling... [1024/50176]	Loss: 1.7281
Profiling... [1536/50176]	Loss: 1.7793
Profiling... [2048/50176]	Loss: 1.8725
Profiling... [2560/50176]	Loss: 1.6840
Profiling... [3072/50176]	Loss: 1.7791
Profiling... [3584/50176]	Loss: 1.7059
Profiling... [4096/50176]	Loss: 1.5527
Profiling... [4608/50176]	Loss: 1.7850
Profiling... [5120/50176]	Loss: 1.6622
Profiling... [5632/50176]	Loss: 1.7665
Profiling... [6144/50176]	Loss: 1.7537
Profiling... [6656/50176]	Loss: 1.6827
Profile done
epoch 1 train time consumed: 7.19s
Validation Epoch: 8, Average loss: 0.0034, Accuracy: 0.5234
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.58971811708335,
                        "time": 4.667207231999782,
                        "accuracy": 0.5234375,
                        "total_cost": 1560379.7312954494
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7404
Profiling... [1024/50176]	Loss: 1.7757
Profiling... [1536/50176]	Loss: 1.7204
Profiling... [2048/50176]	Loss: 1.7434
Profiling... [2560/50176]	Loss: 1.8402
Profiling... [3072/50176]	Loss: 1.8248
Profiling... [3584/50176]	Loss: 1.8322
Profiling... [4096/50176]	Loss: 1.6975
Profiling... [4608/50176]	Loss: 1.7296
Profiling... [5120/50176]	Loss: 1.7430
Profiling... [5632/50176]	Loss: 1.7350
Profiling... [6144/50176]	Loss: 1.8452
Profiling... [6656/50176]	Loss: 1.6908
Profile done
epoch 1 train time consumed: 7.95s
Validation Epoch: 8, Average loss: 0.0034, Accuracy: 0.5166
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.59640032801329,
                        "time": 5.288722156999938,
                        "accuracy": 0.5166015625,
                        "total_cost": 1791567.1276642513
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7550
Profiling... [1024/50176]	Loss: 1.9057
Profiling... [1536/50176]	Loss: 1.7192
Profiling... [2048/50176]	Loss: 1.7407
Profiling... [2560/50176]	Loss: 1.7627
Profiling... [3072/50176]	Loss: 1.7254
Profiling... [3584/50176]	Loss: 1.7104
Profiling... [4096/50176]	Loss: 1.7606
Profiling... [4608/50176]	Loss: 1.6587
Profiling... [5120/50176]	Loss: 1.7762
Profiling... [5632/50176]	Loss: 1.7991
Profiling... [6144/50176]	Loss: 1.7336
Profiling... [6656/50176]	Loss: 1.7759
Profile done
epoch 1 train time consumed: 18.00s
Validation Epoch: 8, Average loss: 0.0034, Accuracy: 0.5239
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.55776289087925,
                        "time": 13.038802176999525,
                        "accuracy": 0.52392578125,
                        "total_cost": 4355178.658188844
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7294
Profiling... [1024/50176]	Loss: 1.8563
Profiling... [1536/50176]	Loss: 1.8134
Profiling... [2048/50176]	Loss: 1.7625
Profiling... [2560/50176]	Loss: 1.8412
Profiling... [3072/50176]	Loss: 1.7440
Profiling... [3584/50176]	Loss: 1.7585
Profiling... [4096/50176]	Loss: 1.7629
Profiling... [4608/50176]	Loss: 1.9371
Profiling... [5120/50176]	Loss: 1.8013
Profiling... [5632/50176]	Loss: 1.8766
Profiling... [6144/50176]	Loss: 1.8193
Profiling... [6656/50176]	Loss: 1.8171
Profile done
epoch 1 train time consumed: 7.03s
Validation Epoch: 8, Average loss: 0.0043, Accuracy: 0.4323
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.54656716767661,
                        "time": 4.544115729999248,
                        "accuracy": 0.43232421875,
                        "total_cost": 1839407.135341914
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7659
Profiling... [1024/50176]	Loss: 1.7374
Profiling... [1536/50176]	Loss: 1.8870
Profiling... [2048/50176]	Loss: 1.7103
Profiling... [2560/50176]	Loss: 1.7393
Profiling... [3072/50176]	Loss: 1.8028
Profiling... [3584/50176]	Loss: 1.7636
Profiling... [4096/50176]	Loss: 1.8428
Profiling... [4608/50176]	Loss: 1.9126
Profiling... [5120/50176]	Loss: 1.8592
Profiling... [5632/50176]	Loss: 1.9539
Profiling... [6144/50176]	Loss: 1.6485
Profiling... [6656/50176]	Loss: 1.8512
Profile done
epoch 1 train time consumed: 7.27s
Validation Epoch: 8, Average loss: 0.0038, Accuracy: 0.4788
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.57049386338718,
                        "time": 4.661007579999932,
                        "accuracy": 0.47880859375,
                        "total_cost": 1703554.0655435198
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7411
Profiling... [1024/50176]	Loss: 1.7757
Profiling... [1536/50176]	Loss: 1.8231
Profiling... [2048/50176]	Loss: 1.8163
Profiling... [2560/50176]	Loss: 1.8637
Profiling... [3072/50176]	Loss: 1.8792
Profiling... [3584/50176]	Loss: 1.8535
Profiling... [4096/50176]	Loss: 1.8485
Profiling... [4608/50176]	Loss: 1.6895
Profiling... [5120/50176]	Loss: 1.6999
Profiling... [5632/50176]	Loss: 1.7448
Profiling... [6144/50176]	Loss: 1.7763
Profiling... [6656/50176]	Loss: 1.8185
Profile done
epoch 1 train time consumed: 7.83s
Validation Epoch: 8, Average loss: 0.0042, Accuracy: 0.4599
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.5759984876153,
                        "time": 5.291737601999557,
                        "accuracy": 0.45986328125,
                        "total_cost": 2013759.5631308574
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7093
Profiling... [1024/50176]	Loss: 1.7550
Profiling... [1536/50176]	Loss: 1.8669
Profiling... [2048/50176]	Loss: 1.7580
Profiling... [2560/50176]	Loss: 1.7881
Profiling... [3072/50176]	Loss: 1.9629
Profiling... [3584/50176]	Loss: 1.8703
Profiling... [4096/50176]	Loss: 1.8421
Profiling... [4608/50176]	Loss: 1.6896
Profiling... [5120/50176]	Loss: 1.7977
Profiling... [5632/50176]	Loss: 1.7053
Profiling... [6144/50176]	Loss: 1.7826
Profiling... [6656/50176]	Loss: 1.7926
Profile done
epoch 1 train time consumed: 18.05s
Validation Epoch: 8, Average loss: 0.0047, Accuracy: 0.4146
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.53881902662442,
                        "time": 13.142336376999083,
                        "accuracy": 0.4146484375,
                        "total_cost": 5546647.853881855
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7280
Profiling... [1024/50176]	Loss: 1.7527
Profiling... [1536/50176]	Loss: 1.9052
Profiling... [2048/50176]	Loss: 1.9252
Profiling... [2560/50176]	Loss: 1.9035
Profiling... [3072/50176]	Loss: 1.8369
Profiling... [3584/50176]	Loss: 1.7186
Profiling... [4096/50176]	Loss: 1.7538
Profiling... [4608/50176]	Loss: 1.8735
Profiling... [5120/50176]	Loss: 1.9384
Profiling... [5632/50176]	Loss: 1.8068
Profiling... [6144/50176]	Loss: 1.8115
Profiling... [6656/50176]	Loss: 1.7367
Profile done
epoch 1 train time consumed: 7.15s
Validation Epoch: 8, Average loss: 0.0045, Accuracy: 0.4155
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.52685431759697,
                        "time": 4.5423203139998805,
                        "accuracy": 0.41552734375,
                        "total_cost": 1913005.4060370824
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9292
Profiling... [1024/50176]	Loss: 1.8476
Profiling... [1536/50176]	Loss: 1.8467
Profiling... [2048/50176]	Loss: 1.8110
Profiling... [2560/50176]	Loss: 1.8003
Profiling... [3072/50176]	Loss: 1.8094
Profiling... [3584/50176]	Loss: 1.7728
Profiling... [4096/50176]	Loss: 1.7597
Profiling... [4608/50176]	Loss: 1.9288
Profiling... [5120/50176]	Loss: 1.7877
Profiling... [5632/50176]	Loss: 1.9143
Profiling... [6144/50176]	Loss: 1.9076
Profiling... [6656/50176]	Loss: 1.8695
Profile done
epoch 1 train time consumed: 7.30s
Validation Epoch: 8, Average loss: 0.0041, Accuracy: 0.4513
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.55128594769374,
                        "time": 4.648158418998719,
                        "accuracy": 0.45126953125,
                        "total_cost": 1802531.8950109724
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8526
Profiling... [1024/50176]	Loss: 1.7487
Profiling... [1536/50176]	Loss: 1.8255
Profiling... [2048/50176]	Loss: 1.7617
Profiling... [2560/50176]	Loss: 1.8814
Profiling... [3072/50176]	Loss: 1.7759
Profiling... [3584/50176]	Loss: 1.8155
Profiling... [4096/50176]	Loss: 1.7490
Profiling... [4608/50176]	Loss: 1.7755
Profiling... [5120/50176]	Loss: 1.8269
Profiling... [5632/50176]	Loss: 1.8617
Profiling... [6144/50176]	Loss: 1.7738
Profiling... [6656/50176]	Loss: 1.6976
Profile done
epoch 1 train time consumed: 7.97s
Validation Epoch: 8, Average loss: 0.0040, Accuracy: 0.4693
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.55663638548103,
                        "time": 5.275395773998753,
                        "accuracy": 0.4693359375,
                        "total_cost": 1967022.311070696
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.8078
Profiling... [1024/50176]	Loss: 1.8181
Profiling... [1536/50176]	Loss: 1.9177
Profiling... [2048/50176]	Loss: 1.7576
Profiling... [2560/50176]	Loss: 1.7089
Profiling... [3072/50176]	Loss: 1.8351
Profiling... [3584/50176]	Loss: 1.8789
Profiling... [4096/50176]	Loss: 1.8633
Profiling... [4608/50176]	Loss: 1.8369
Profiling... [5120/50176]	Loss: 1.8593
Profiling... [5632/50176]	Loss: 1.8176
Profiling... [6144/50176]	Loss: 1.8190
Profiling... [6656/50176]	Loss: 1.7743
Profile done
epoch 1 train time consumed: 18.02s
Validation Epoch: 8, Average loss: 0.0045, Accuracy: 0.4349
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.51696078051027,
                        "time": 13.077276670999709,
                        "accuracy": 0.43486328125,
                        "total_cost": 5262627.396009764
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7607
Profiling... [1024/50176]	Loss: 1.7905
Profiling... [1536/50176]	Loss: 1.8833
Profiling... [2048/50176]	Loss: 1.8755
Profiling... [2560/50176]	Loss: 1.7734
Profiling... [3072/50176]	Loss: 1.8684
Profiling... [3584/50176]	Loss: 1.8096
Profiling... [4096/50176]	Loss: 1.8085
Profiling... [4608/50176]	Loss: 1.7043
Profiling... [5120/50176]	Loss: 1.9487
Profiling... [5632/50176]	Loss: 1.8783
Profiling... [6144/50176]	Loss: 1.8888
Profiling... [6656/50176]	Loss: 1.7759
Profile done
epoch 1 train time consumed: 7.08s
Validation Epoch: 8, Average loss: 0.0039, Accuracy: 0.4789
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.50662480853194,
                        "time": 4.545113493000827,
                        "accuracy": 0.47890625,
                        "total_cost": 1660857.132842064
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8888
Profiling... [1024/50176]	Loss: 1.6091
Profiling... [1536/50176]	Loss: 1.7223
Profiling... [2048/50176]	Loss: 1.8179
Profiling... [2560/50176]	Loss: 1.7727
Profiling... [3072/50176]	Loss: 1.9510
Profiling... [3584/50176]	Loss: 1.8340
Profiling... [4096/50176]	Loss: 1.7642
Profiling... [4608/50176]	Loss: 1.8067
Profiling... [5120/50176]	Loss: 1.8414
Profiling... [5632/50176]	Loss: 1.9052
Profiling... [6144/50176]	Loss: 1.8653
Profiling... [6656/50176]	Loss: 1.8194
Profile done
epoch 1 train time consumed: 7.27s
Validation Epoch: 8, Average loss: 0.0044, Accuracy: 0.4390
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.52906764729633,
                        "time": 4.653391679999913,
                        "accuracy": 0.43896484375,
                        "total_cost": 1855145.248177941
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8020
Profiling... [1024/50176]	Loss: 1.8306
Profiling... [1536/50176]	Loss: 1.7505
Profiling... [2048/50176]	Loss: 1.8103
Profiling... [2560/50176]	Loss: 1.8589
Profiling... [3072/50176]	Loss: 1.8272
Profiling... [3584/50176]	Loss: 1.7442
Profiling... [4096/50176]	Loss: 1.7501
Profiling... [4608/50176]	Loss: 1.8272
Profiling... [5120/50176]	Loss: 1.7194
Profiling... [5632/50176]	Loss: 1.7625
Profiling... [6144/50176]	Loss: 1.7414
Profiling... [6656/50176]	Loss: 1.8495
Profile done
epoch 1 train time consumed: 7.98s
Validation Epoch: 8, Average loss: 0.0040, Accuracy: 0.4685
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.53514171794662,
                        "time": 5.289041524001732,
                        "accuracy": 0.46845703125,
                        "total_cost": 1975810.3837838448
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6669
Profiling... [1024/50176]	Loss: 1.7551
Profiling... [1536/50176]	Loss: 1.8663
Profiling... [2048/50176]	Loss: 1.8128
Profiling... [2560/50176]	Loss: 1.9127
Profiling... [3072/50176]	Loss: 1.8652
Profiling... [3584/50176]	Loss: 1.8350
Profiling... [4096/50176]	Loss: 1.7310
Profiling... [4608/50176]	Loss: 1.8165
Profiling... [5120/50176]	Loss: 1.7567
Profiling... [5632/50176]	Loss: 1.7478
Profiling... [6144/50176]	Loss: 1.8454
Profiling... [6656/50176]	Loss: 1.8535
Profile done
epoch 1 train time consumed: 17.95s
Validation Epoch: 8, Average loss: 0.0040, Accuracy: 0.4677
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.49665021857285,
                        "time": 13.042426500000147,
                        "accuracy": 0.46767578125,
                        "total_cost": 4880356.710795629
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7187
Profiling... [1024/50176]	Loss: 1.7745
Profiling... [1536/50176]	Loss: 1.8941
Profiling... [2048/50176]	Loss: 1.9652
Profiling... [2560/50176]	Loss: 1.9456
Profiling... [3072/50176]	Loss: 1.8357
Profiling... [3584/50176]	Loss: 1.9211
Profiling... [4096/50176]	Loss: 1.8369
Profiling... [4608/50176]	Loss: 1.9124
Profiling... [5120/50176]	Loss: 1.9489
Profiling... [5632/50176]	Loss: 2.0085
Profiling... [6144/50176]	Loss: 1.8046
Profiling... [6656/50176]	Loss: 1.9022
Profile done
epoch 1 train time consumed: 7.17s
Validation Epoch: 8, Average loss: 0.0060, Accuracy: 0.3157
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.48694494936025,
                        "time": 4.579034217000299,
                        "accuracy": 0.31572265625,
                        "total_cost": 2538085.1583249415
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8359
Profiling... [1024/50176]	Loss: 1.7300
Profiling... [1536/50176]	Loss: 1.8550
Profiling... [2048/50176]	Loss: 1.7202
Profiling... [2560/50176]	Loss: 1.7077
Profiling... [3072/50176]	Loss: 1.8737
Profiling... [3584/50176]	Loss: 1.9063
Profiling... [4096/50176]	Loss: 2.0098
Profiling... [4608/50176]	Loss: 1.9197
Profiling... [5120/50176]	Loss: 1.8730
Profiling... [5632/50176]	Loss: 1.9698
Profiling... [6144/50176]	Loss: 1.8333
Profiling... [6656/50176]	Loss: 2.0297
Profile done
epoch 1 train time consumed: 7.22s
Validation Epoch: 8, Average loss: 0.0062, Accuracy: 0.3097
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.50950844638209,
                        "time": 4.679331272000127,
                        "accuracy": 0.30966796875,
                        "total_cost": 2644390.29940846
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9134
Profiling... [1024/50176]	Loss: 1.8341
Profiling... [1536/50176]	Loss: 1.9104
Profiling... [2048/50176]	Loss: 1.8209
Profiling... [2560/50176]	Loss: 1.8812
Profiling... [3072/50176]	Loss: 2.1200
Profiling... [3584/50176]	Loss: 1.9875
Profiling... [4096/50176]	Loss: 1.7951
Profiling... [4608/50176]	Loss: 1.8802
Profiling... [5120/50176]	Loss: 2.0047
Profiling... [5632/50176]	Loss: 1.8269
Profiling... [6144/50176]	Loss: 1.8210
Profiling... [6656/50176]	Loss: 1.8521
Profile done
epoch 1 train time consumed: 8.00s
Validation Epoch: 8, Average loss: 0.0063, Accuracy: 0.3254
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.51526403099142,
                        "time": 5.306999100999747,
                        "accuracy": 0.325390625,
                        "total_cost": 2854184.390453645
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7305
Profiling... [1024/50176]	Loss: 1.8433
Profiling... [1536/50176]	Loss: 1.8425
Profiling... [2048/50176]	Loss: 2.0867
Profiling... [2560/50176]	Loss: 1.9152
Profiling... [3072/50176]	Loss: 1.8784
Profiling... [3584/50176]	Loss: 1.9331
Profiling... [4096/50176]	Loss: 1.8756
Profiling... [4608/50176]	Loss: 1.8399
Profiling... [5120/50176]	Loss: 1.8438
Profiling... [5632/50176]	Loss: 1.8771
Profiling... [6144/50176]	Loss: 1.9667
Profiling... [6656/50176]	Loss: 1.8313
Profile done
epoch 1 train time consumed: 17.98s
Validation Epoch: 8, Average loss: 0.0051, Accuracy: 0.3513
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.47831751038201,
                        "time": 13.006749733000106,
                        "accuracy": 0.35126953125,
                        "total_cost": 6479870.870596661
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8496
Profiling... [1024/50176]	Loss: 1.7942
Profiling... [1536/50176]	Loss: 1.8368
Profiling... [2048/50176]	Loss: 1.9639
Profiling... [2560/50176]	Loss: 1.8101
Profiling... [3072/50176]	Loss: 2.0110
Profiling... [3584/50176]	Loss: 1.8269
Profiling... [4096/50176]	Loss: 2.0082
Profiling... [4608/50176]	Loss: 1.9889
Profiling... [5120/50176]	Loss: 1.8967
Profiling... [5632/50176]	Loss: 1.7725
Profiling... [6144/50176]	Loss: 1.7791
Profiling... [6656/50176]	Loss: 1.8864
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 8, Average loss: 0.0059, Accuracy: 0.3429
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.46756172058507,
                        "time": 4.544904192998729,
                        "accuracy": 0.34287109375,
                        "total_cost": 2319700.4596564285
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8422
Profiling... [1024/50176]	Loss: 1.7214
Profiling... [1536/50176]	Loss: 1.9038
Profiling... [2048/50176]	Loss: 1.7495
Profiling... [2560/50176]	Loss: 1.9625
Profiling... [3072/50176]	Loss: 2.0264
Profiling... [3584/50176]	Loss: 1.8344
Profiling... [4096/50176]	Loss: 1.8697
Profiling... [4608/50176]	Loss: 1.9572
Profiling... [5120/50176]	Loss: 1.9679
Profiling... [5632/50176]	Loss: 1.9952
Profiling... [6144/50176]	Loss: 1.8214
Profiling... [6656/50176]	Loss: 1.8590
Profile done
epoch 1 train time consumed: 7.15s
Validation Epoch: 8, Average loss: 0.0065, Accuracy: 0.3148
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.49000153595946,
                        "time": 4.654352747000303,
                        "accuracy": 0.31484375,
                        "total_cost": 2587034.7774890023
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7440
Profiling... [1024/50176]	Loss: 1.7244
Profiling... [1536/50176]	Loss: 1.9834
Profiling... [2048/50176]	Loss: 1.9267
Profiling... [2560/50176]	Loss: 1.8843
Profiling... [3072/50176]	Loss: 1.8931
Profiling... [3584/50176]	Loss: 1.8147
Profiling... [4096/50176]	Loss: 1.9244
Profiling... [4608/50176]	Loss: 1.9418
Profiling... [5120/50176]	Loss: 1.9326
Profiling... [5632/50176]	Loss: 1.9697
Profiling... [6144/50176]	Loss: 2.0017
Profiling... [6656/50176]	Loss: 1.9339
Profile done
epoch 1 train time consumed: 8.04s
Validation Epoch: 8, Average loss: 0.0060, Accuracy: 0.3451
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.49649437495196,
                        "time": 5.292544370000542,
                        "accuracy": 0.3451171875,
                        "total_cost": 2683712.3687156118
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7212
Profiling... [1024/50176]	Loss: 1.9196
Profiling... [1536/50176]	Loss: 1.7931
Profiling... [2048/50176]	Loss: 1.7922
Profiling... [2560/50176]	Loss: 1.9380
Profiling... [3072/50176]	Loss: 1.9058
Profiling... [3584/50176]	Loss: 2.0200
Profiling... [4096/50176]	Loss: 1.8427
Profiling... [4608/50176]	Loss: 1.9677
Profiling... [5120/50176]	Loss: 1.9806
Profiling... [5632/50176]	Loss: 1.9948
Profiling... [6144/50176]	Loss: 1.8370
Profiling... [6656/50176]	Loss: 1.9936
Profile done
epoch 1 train time consumed: 18.09s
Validation Epoch: 8, Average loss: 0.0083, Accuracy: 0.2174
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.45821767660453,
                        "time": 13.036406716000783,
                        "accuracy": 0.2173828125,
                        "total_cost": 10494717.356277362
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7622
Profiling... [1024/50176]	Loss: 1.8071
Profiling... [1536/50176]	Loss: 1.9131
Profiling... [2048/50176]	Loss: 2.0305
Profiling... [2560/50176]	Loss: 1.8750
Profiling... [3072/50176]	Loss: 1.8263
Profiling... [3584/50176]	Loss: 1.9242
Profiling... [4096/50176]	Loss: 1.9350
Profiling... [4608/50176]	Loss: 1.9587
Profiling... [5120/50176]	Loss: 1.7410
Profiling... [5632/50176]	Loss: 2.1384
Profiling... [6144/50176]	Loss: 2.0065
Profiling... [6656/50176]	Loss: 1.9085
Profile done
epoch 1 train time consumed: 7.05s
Validation Epoch: 8, Average loss: 0.0075, Accuracy: 0.2595
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.44828404597854,
                        "time": 4.5386260129998846,
                        "accuracy": 0.25947265625,
                        "total_cost": 3061052.99785314
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.8163
Profiling... [1024/50176]	Loss: 1.8181
Profiling... [1536/50176]	Loss: 1.7575
Profiling... [2048/50176]	Loss: 1.8881
Profiling... [2560/50176]	Loss: 1.7853
Profiling... [3072/50176]	Loss: 1.9205
Profiling... [3584/50176]	Loss: 1.8852
Profiling... [4096/50176]	Loss: 1.9261
Profiling... [4608/50176]	Loss: 1.8730
Profiling... [5120/50176]	Loss: 1.8326
Profiling... [5632/50176]	Loss: 1.8807
Profiling... [6144/50176]	Loss: 1.7576
Profiling... [6656/50176]	Loss: 2.0599
Profile done
epoch 1 train time consumed: 7.21s
Validation Epoch: 8, Average loss: 0.0092, Accuracy: 0.2262
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.47251475595057,
                        "time": 4.66265349799869,
                        "accuracy": 0.226171875,
                        "total_cost": 3607718.0778988134
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8671
Profiling... [1024/50176]	Loss: 1.8933
Profiling... [1536/50176]	Loss: 1.9151
Profiling... [2048/50176]	Loss: 1.8570
Profiling... [2560/50176]	Loss: 1.7884
Profiling... [3072/50176]	Loss: 2.0201
Profiling... [3584/50176]	Loss: 1.9310
Profiling... [4096/50176]	Loss: 1.9159
Profiling... [4608/50176]	Loss: 1.8692
Profiling... [5120/50176]	Loss: 1.9601
Profiling... [5632/50176]	Loss: 1.8223
Profiling... [6144/50176]	Loss: 1.9969
Profiling... [6656/50176]	Loss: 1.8935
Profile done
epoch 1 train time consumed: 8.04s
Validation Epoch: 8, Average loss: 0.0094, Accuracy: 0.1943
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.47835148417438,
                        "time": 5.299955407001107,
                        "accuracy": 0.1943359375,
                        "total_cost": 4772623.160475369
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6859
Profiling... [1024/50176]	Loss: 1.7651
Profiling... [1536/50176]	Loss: 2.0727
Profiling... [2048/50176]	Loss: 1.9483
Profiling... [2560/50176]	Loss: 1.9373
Profiling... [3072/50176]	Loss: 1.7560
Profiling... [3584/50176]	Loss: 1.9034
Profiling... [4096/50176]	Loss: 1.9309
Profiling... [4608/50176]	Loss: 1.9589
Profiling... [5120/50176]	Loss: 1.9626
Profiling... [5632/50176]	Loss: 1.8239
Profiling... [6144/50176]	Loss: 1.9258
Profiling... [6656/50176]	Loss: 1.9227
Profile done
epoch 1 train time consumed: 18.01s
Validation Epoch: 8, Average loss: 0.0054, Accuracy: 0.3688
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.4411910565559,
                        "time": 13.112278354999944,
                        "accuracy": 0.36884765625,
                        "total_cost": 6221128.623817819
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7172
Profiling... [2048/50176]	Loss: 1.7517
Profiling... [3072/50176]	Loss: 1.7087
Profiling... [4096/50176]	Loss: 1.6978
Profiling... [5120/50176]	Loss: 1.7684
Profiling... [6144/50176]	Loss: 1.8191
Profiling... [7168/50176]	Loss: 1.7290
Profiling... [8192/50176]	Loss: 1.7868
Profiling... [9216/50176]	Loss: 1.7928
Profiling... [10240/50176]	Loss: 1.7820
Profiling... [11264/50176]	Loss: 1.7082
Profiling... [12288/50176]	Loss: 1.7347
Profiling... [13312/50176]	Loss: 1.6765
Profile done
epoch 1 train time consumed: 13.15s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5189
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.45729880183954,
                        "time": 8.862379961999977,
                        "accuracy": 0.5189453125,
                        "total_cost": 2988593.318009778
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8048
Profiling... [2048/50176]	Loss: 1.7534
Profiling... [3072/50176]	Loss: 1.7241
Profiling... [4096/50176]	Loss: 1.7955
Profiling... [5120/50176]	Loss: 1.6556
Profiling... [6144/50176]	Loss: 1.7722
Profiling... [7168/50176]	Loss: 1.7522
Profiling... [8192/50176]	Loss: 1.7455
Profiling... [9216/50176]	Loss: 1.7866
Profiling... [10240/50176]	Loss: 1.8117
Profiling... [11264/50176]	Loss: 1.7574
Profiling... [12288/50176]	Loss: 1.7681
Profiling... [13312/50176]	Loss: 1.7697
Profile done
epoch 1 train time consumed: 13.43s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5213
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.49515468327508,
                        "time": 9.143591080000988,
                        "accuracy": 0.5212890625,
                        "total_cost": 3069560.7372352513
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7759
Profiling... [2048/50176]	Loss: 1.7650
Profiling... [3072/50176]	Loss: 1.7361
Profiling... [4096/50176]	Loss: 1.8300
Profiling... [5120/50176]	Loss: 1.7850
Profiling... [6144/50176]	Loss: 1.7626
Profiling... [7168/50176]	Loss: 1.7431
Profiling... [8192/50176]	Loss: 1.7600
Profiling... [9216/50176]	Loss: 1.8282
Profiling... [10240/50176]	Loss: 1.7911
Profiling... [11264/50176]	Loss: 1.8400
Profiling... [12288/50176]	Loss: 1.7675
Profiling... [13312/50176]	Loss: 1.8356
Profile done
epoch 1 train time consumed: 15.02s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5216
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.4991320139675,
                        "time": 10.437250164000943,
                        "accuracy": 0.52158203125,
                        "total_cost": 3501882.0995861613
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6765
Profiling... [2048/50176]	Loss: 1.7665
Profiling... [3072/50176]	Loss: 1.8651
Profiling... [4096/50176]	Loss: 1.8543
Profiling... [5120/50176]	Loss: 1.7514
Profiling... [6144/50176]	Loss: 1.8524
Profiling... [7168/50176]	Loss: 1.7963
Profiling... [8192/50176]	Loss: 1.7526
Profiling... [9216/50176]	Loss: 1.8341
Profiling... [10240/50176]	Loss: 1.7595
Profiling... [11264/50176]	Loss: 1.7722
Profiling... [12288/50176]	Loss: 1.6715
Profiling... [13312/50176]	Loss: 1.8290
Profile done
epoch 1 train time consumed: 38.15s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5199
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.40787266341455,
                        "time": 28.04262348299926,
                        "accuracy": 0.519921875,
                        "total_cost": 9438839.459341599
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9379
Profiling... [2048/50176]	Loss: 1.8426
Profiling... [3072/50176]	Loss: 1.6924
Profiling... [4096/50176]	Loss: 1.6933
Profiling... [5120/50176]	Loss: 1.7038
Profiling... [6144/50176]	Loss: 1.7907
Profiling... [7168/50176]	Loss: 1.7488
Profiling... [8192/50176]	Loss: 1.8654
Profiling... [9216/50176]	Loss: 1.7837
Profiling... [10240/50176]	Loss: 1.6994
Profiling... [11264/50176]	Loss: 1.6365
Profiling... [12288/50176]	Loss: 1.8568
Profiling... [13312/50176]	Loss: 1.8498
Profile done
epoch 1 train time consumed: 13.14s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5213
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.4370109823164,
                        "time": 8.912239896999381,
                        "accuracy": 0.5212890625,
                        "total_cost": 2991894.697531452
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8134
Profiling... [2048/50176]	Loss: 1.8625
Profiling... [3072/50176]	Loss: 1.7003
Profiling... [4096/50176]	Loss: 1.7584
Profiling... [5120/50176]	Loss: 1.7634
Profiling... [6144/50176]	Loss: 1.8516
Profiling... [7168/50176]	Loss: 1.7578
Profiling... [8192/50176]	Loss: 1.8106
Profiling... [9216/50176]	Loss: 1.7696
Profiling... [10240/50176]	Loss: 1.7878
Profiling... [11264/50176]	Loss: 1.7668
Profiling... [12288/50176]	Loss: 1.7112
Profiling... [13312/50176]	Loss: 1.8026
Profile done
epoch 1 train time consumed: 13.47s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5109
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.4752732409967,
                        "time": 9.140714556000603,
                        "accuracy": 0.5109375,
                        "total_cost": 3130764.6185690137
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7543
Profiling... [2048/50176]	Loss: 1.7142
Profiling... [3072/50176]	Loss: 1.7615
Profiling... [4096/50176]	Loss: 1.7368
Profiling... [5120/50176]	Loss: 1.7754
Profiling... [6144/50176]	Loss: 1.7656
Profiling... [7168/50176]	Loss: 1.7992
Profiling... [8192/50176]	Loss: 1.7521
Profiling... [9216/50176]	Loss: 1.7251
Profiling... [10240/50176]	Loss: 1.7519
Profiling... [11264/50176]	Loss: 1.8097
Profiling... [12288/50176]	Loss: 1.7437
Profiling... [13312/50176]	Loss: 1.7600
Profile done
epoch 1 train time consumed: 15.01s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5229
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.47957134900955,
                        "time": 10.391794462999314,
                        "accuracy": 0.52294921875,
                        "total_cost": 3477515.5327161103
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7656
Profiling... [2048/50176]	Loss: 1.7591
Profiling... [3072/50176]	Loss: 1.6931
Profiling... [4096/50176]	Loss: 1.7861
Profiling... [5120/50176]	Loss: 1.8344
Profiling... [6144/50176]	Loss: 1.7003
Profiling... [7168/50176]	Loss: 1.7810
Profiling... [8192/50176]	Loss: 1.7700
Profiling... [9216/50176]	Loss: 1.9378
Profiling... [10240/50176]	Loss: 1.8301
Profiling... [11264/50176]	Loss: 1.7154
Profiling... [12288/50176]	Loss: 1.7679
Profiling... [13312/50176]	Loss: 1.7430
Profile done
epoch 1 train time consumed: 37.77s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5180
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.39145388573868,
                        "time": 27.731972384999608,
                        "accuracy": 0.51796875,
                        "total_cost": 9369474.832941044
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8286
Profiling... [2048/50176]	Loss: 1.7714
Profiling... [3072/50176]	Loss: 1.7617
Profiling... [4096/50176]	Loss: 1.7334
Profiling... [5120/50176]	Loss: 1.7041
Profiling... [6144/50176]	Loss: 1.7050
Profiling... [7168/50176]	Loss: 1.8006
Profiling... [8192/50176]	Loss: 1.7091
Profiling... [9216/50176]	Loss: 1.7758
Profiling... [10240/50176]	Loss: 1.7235
Profiling... [11264/50176]	Loss: 1.7325
Profiling... [12288/50176]	Loss: 1.7225
Profiling... [13312/50176]	Loss: 1.6322
Profile done
epoch 1 train time consumed: 13.19s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.4167549205596,
                        "time": 8.875516688000062,
                        "accuracy": 0.5181640625,
                        "total_cost": 2997535.9790607067
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7602
Profiling... [2048/50176]	Loss: 1.8052
Profiling... [3072/50176]	Loss: 1.7576
Profiling... [4096/50176]	Loss: 1.6907
Profiling... [5120/50176]	Loss: 1.6987
Profiling... [6144/50176]	Loss: 1.7539
Profiling... [7168/50176]	Loss: 1.7799
Profiling... [8192/50176]	Loss: 1.8077
Profiling... [9216/50176]	Loss: 1.7637
Profiling... [10240/50176]	Loss: 1.6910
Profiling... [11264/50176]	Loss: 1.7464
Profiling... [12288/50176]	Loss: 1.7572
Profiling... [13312/50176]	Loss: 1.7169
Profile done
epoch 1 train time consumed: 13.48s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5215
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.45546687955562,
                        "time": 9.157608110999718,
                        "accuracy": 0.521484375,
                        "total_cost": 3073114.931631366
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6961
Profiling... [2048/50176]	Loss: 1.7970
Profiling... [3072/50176]	Loss: 1.8720
Profiling... [4096/50176]	Loss: 1.7442
Profiling... [5120/50176]	Loss: 1.7396
Profiling... [6144/50176]	Loss: 1.7586
Profiling... [7168/50176]	Loss: 1.7919
Profiling... [8192/50176]	Loss: 1.6656
Profiling... [9216/50176]	Loss: 1.8085
Profiling... [10240/50176]	Loss: 1.7588
Profiling... [11264/50176]	Loss: 1.7897
Profiling... [12288/50176]	Loss: 1.6665
Profiling... [13312/50176]	Loss: 1.7409
Profile done
epoch 1 train time consumed: 15.04s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5249
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.46000756363783,
                        "time": 10.418158759001017,
                        "accuracy": 0.52490234375,
                        "total_cost": 3473365.6736985715
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7054
Profiling... [2048/50176]	Loss: 1.7935
Profiling... [3072/50176]	Loss: 1.8880
Profiling... [4096/50176]	Loss: 1.8120
Profiling... [5120/50176]	Loss: 1.8554
Profiling... [6144/50176]	Loss: 1.7858
Profiling... [7168/50176]	Loss: 1.8101
Profiling... [8192/50176]	Loss: 1.6769
Profiling... [9216/50176]	Loss: 1.6170
Profiling... [10240/50176]	Loss: 1.7506
Profiling... [11264/50176]	Loss: 1.6028
Profiling... [12288/50176]	Loss: 1.7301
Profiling... [13312/50176]	Loss: 1.7723
Profile done
epoch 1 train time consumed: 37.72s
Validation Epoch: 8, Average loss: 0.0017, Accuracy: 0.5216
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.37187950192319,
                        "time": 27.868928613001117,
                        "accuracy": 0.52158203125,
                        "total_cost": 9350518.643418461
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7303
Profiling... [2048/50176]	Loss: 1.6780
Profiling... [3072/50176]	Loss: 1.8304
Profiling... [4096/50176]	Loss: 1.7862
Profiling... [5120/50176]	Loss: 1.7375
Profiling... [6144/50176]	Loss: 1.8602
Profiling... [7168/50176]	Loss: 1.7056
Profiling... [8192/50176]	Loss: 1.8383
Profiling... [9216/50176]	Loss: 1.8587
Profiling... [10240/50176]	Loss: 1.8072
Profiling... [11264/50176]	Loss: 1.7737
Profiling... [12288/50176]	Loss: 1.8189
Profiling... [13312/50176]	Loss: 1.8371
Profile done
epoch 1 train time consumed: 13.17s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4603
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.39892342877151,
                        "time": 8.870855802000733,
                        "accuracy": 0.46025390625,
                        "total_cost": 3372920.3473764723
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7003
Profiling... [2048/50176]	Loss: 1.7788
Profiling... [3072/50176]	Loss: 1.7560
Profiling... [4096/50176]	Loss: 1.6999
Profiling... [5120/50176]	Loss: 1.7860
Profiling... [6144/50176]	Loss: 1.7918
Profiling... [7168/50176]	Loss: 1.7660
Profiling... [8192/50176]	Loss: 1.7457
Profiling... [9216/50176]	Loss: 1.8164
Profiling... [10240/50176]	Loss: 1.8154
Profiling... [11264/50176]	Loss: 1.8043
Profiling... [12288/50176]	Loss: 1.8095
Profiling... [13312/50176]	Loss: 1.7687
Profile done
epoch 1 train time consumed: 13.47s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4884
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.43735653313392,
                        "time": 9.149742120000155,
                        "accuracy": 0.48837890625,
                        "total_cost": 3278611.8534373683
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7072
Profiling... [2048/50176]	Loss: 1.8349
Profiling... [3072/50176]	Loss: 1.7611
Profiling... [4096/50176]	Loss: 1.6970
Profiling... [5120/50176]	Loss: 1.7972
Profiling... [6144/50176]	Loss: 1.8486
Profiling... [7168/50176]	Loss: 1.8997
Profiling... [8192/50176]	Loss: 1.8329
Profiling... [9216/50176]	Loss: 1.8622
Profiling... [10240/50176]	Loss: 1.8569
Profiling... [11264/50176]	Loss: 1.7801
Profiling... [12288/50176]	Loss: 1.8715
Profiling... [13312/50176]	Loss: 1.8143
Profile done
epoch 1 train time consumed: 15.08s
Validation Epoch: 8, Average loss: 0.0021, Accuracy: 0.4647
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.4417910567971,
                        "time": 10.429368035000152,
                        "accuracy": 0.46474609375,
                        "total_cost": 3927175.3558983556
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7304
Profiling... [2048/50176]	Loss: 1.7577
Profiling... [3072/50176]	Loss: 1.7601
Profiling... [4096/50176]	Loss: 1.7803
Profiling... [5120/50176]	Loss: 1.7829
Profiling... [6144/50176]	Loss: 1.7722
Profiling... [7168/50176]	Loss: 1.7615
Profiling... [8192/50176]	Loss: 1.7246
Profiling... [9216/50176]	Loss: 1.8423
Profiling... [10240/50176]	Loss: 1.8324
Profiling... [11264/50176]	Loss: 1.7993
Profiling... [12288/50176]	Loss: 1.8423
Profiling... [13312/50176]	Loss: 1.7861
Profile done
epoch 1 train time consumed: 37.89s
Validation Epoch: 8, Average loss: 0.0021, Accuracy: 0.4498
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.3563748692462,
                        "time": 27.675136433999796,
                        "accuracy": 0.4498046875,
                        "total_cost": 10767226.332984721
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6940
Profiling... [2048/50176]	Loss: 1.8265
Profiling... [3072/50176]	Loss: 1.8806
Profiling... [4096/50176]	Loss: 1.7571
Profiling... [5120/50176]	Loss: 1.8277
Profiling... [6144/50176]	Loss: 1.7146
Profiling... [7168/50176]	Loss: 1.8265
Profiling... [8192/50176]	Loss: 1.8065
Profiling... [9216/50176]	Loss: 1.8130
Profiling... [10240/50176]	Loss: 1.7503
Profiling... [11264/50176]	Loss: 1.8994
Profiling... [12288/50176]	Loss: 1.8102
Profiling... [13312/50176]	Loss: 1.9207
Profile done
epoch 1 train time consumed: 13.11s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4670
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.38246512754067,
                        "time": 8.891722571999708,
                        "accuracy": 0.4669921875,
                        "total_cost": 3332071.6957389116
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7089
Profiling... [2048/50176]	Loss: 1.7940
Profiling... [3072/50176]	Loss: 1.6998
Profiling... [4096/50176]	Loss: 1.7954
Profiling... [5120/50176]	Loss: 1.7505
Profiling... [6144/50176]	Loss: 1.7932
Profiling... [7168/50176]	Loss: 1.7521
Profiling... [8192/50176]	Loss: 1.6857
Profiling... [9216/50176]	Loss: 1.7499
Profiling... [10240/50176]	Loss: 1.7778
Profiling... [11264/50176]	Loss: 1.8136
Profiling... [12288/50176]	Loss: 1.7471
Profiling... [13312/50176]	Loss: 1.8120
Profile done
epoch 1 train time consumed: 13.47s
Validation Epoch: 8, Average loss: 0.0021, Accuracy: 0.4621
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.4194790097796,
                        "time": 9.147769147000872,
                        "accuracy": 0.462109375,
                        "total_cost": 3464243.9373257738
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7793
Profiling... [2048/50176]	Loss: 1.7434
Profiling... [3072/50176]	Loss: 1.8400
Profiling... [4096/50176]	Loss: 1.6519
Profiling... [5120/50176]	Loss: 1.6910
Profiling... [6144/50176]	Loss: 1.7607
Profiling... [7168/50176]	Loss: 1.7940
Profiling... [8192/50176]	Loss: 1.7850
Profiling... [9216/50176]	Loss: 1.7348
Profiling... [10240/50176]	Loss: 1.7699
Profiling... [11264/50176]	Loss: 1.7619
Profiling... [12288/50176]	Loss: 1.8024
Profiling... [13312/50176]	Loss: 1.8034
Profile done
epoch 1 train time consumed: 15.08s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4791
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.42513483913875,
                        "time": 10.41003911199914,
                        "accuracy": 0.4791015625,
                        "total_cost": 3802443.964268744
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6942
Profiling... [2048/50176]	Loss: 1.7866
Profiling... [3072/50176]	Loss: 1.8501
Profiling... [4096/50176]	Loss: 1.7662
Profiling... [5120/50176]	Loss: 1.8046
Profiling... [6144/50176]	Loss: 1.8353
Profiling... [7168/50176]	Loss: 1.7334
Profiling... [8192/50176]	Loss: 1.8028
Profiling... [9216/50176]	Loss: 1.8141
Profiling... [10240/50176]	Loss: 1.8143
Profiling... [11264/50176]	Loss: 1.7651
Profiling... [12288/50176]	Loss: 1.7504
Profiling... [13312/50176]	Loss: 1.8289
Profile done
epoch 1 train time consumed: 37.79s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4872
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.33798661640809,
                        "time": 27.934548477000135,
                        "accuracy": 0.48720703125,
                        "total_cost": 10033816.570612196
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7428
Profiling... [2048/50176]	Loss: 1.7882
Profiling... [3072/50176]	Loss: 1.7819
Profiling... [4096/50176]	Loss: 1.7680
Profiling... [5120/50176]	Loss: 1.7507
Profiling... [6144/50176]	Loss: 1.7888
Profiling... [7168/50176]	Loss: 1.8626
Profiling... [8192/50176]	Loss: 1.7228
Profiling... [9216/50176]	Loss: 1.7871
Profiling... [10240/50176]	Loss: 1.8027
Profiling... [11264/50176]	Loss: 1.8048
Profiling... [12288/50176]	Loss: 1.7533
Profiling... [13312/50176]	Loss: 1.7611
Profile done
epoch 1 train time consumed: 13.10s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4876
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.36377018453187,
                        "time": 8.884125710999797,
                        "accuracy": 0.48759765625,
                        "total_cost": 3188534.603266901
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7357
Profiling... [2048/50176]	Loss: 1.7917
Profiling... [3072/50176]	Loss: 1.7705
Profiling... [4096/50176]	Loss: 1.7487
Profiling... [5120/50176]	Loss: 1.8054
Profiling... [6144/50176]	Loss: 1.7531
Profiling... [7168/50176]	Loss: 1.7948
Profiling... [8192/50176]	Loss: 1.7452
Profiling... [9216/50176]	Loss: 1.8269
Profiling... [10240/50176]	Loss: 1.7863
Profiling... [11264/50176]	Loss: 1.7996
Profiling... [12288/50176]	Loss: 1.7419
Profiling... [13312/50176]	Loss: 1.7978
Profile done
epoch 1 train time consumed: 13.52s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4833
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.39936816654675,
                        "time": 9.152033636000851,
                        "accuracy": 0.48330078125,
                        "total_cost": 3313890.5386368004
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8067
Profiling... [2048/50176]	Loss: 1.8513
Profiling... [3072/50176]	Loss: 1.7357
Profiling... [4096/50176]	Loss: 1.7868
Profiling... [5120/50176]	Loss: 1.8040
Profiling... [6144/50176]	Loss: 1.7047
Profiling... [7168/50176]	Loss: 1.7702
Profiling... [8192/50176]	Loss: 1.8078
Profiling... [9216/50176]	Loss: 1.7393
Profiling... [10240/50176]	Loss: 1.8046
Profiling... [11264/50176]	Loss: 1.7091
Profiling... [12288/50176]	Loss: 1.7364
Profiling... [13312/50176]	Loss: 1.7715
Profile done
epoch 1 train time consumed: 15.06s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4806
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.40402934026558,
                        "time": 10.391845142999955,
                        "accuracy": 0.48056640625,
                        "total_cost": 3784228.103283056
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8040
Profiling... [2048/50176]	Loss: 1.7060
Profiling... [3072/50176]	Loss: 1.7906
Profiling... [4096/50176]	Loss: 1.8160
Profiling... [5120/50176]	Loss: 1.8313
Profiling... [6144/50176]	Loss: 1.8050
Profiling... [7168/50176]	Loss: 1.6838
Profiling... [8192/50176]	Loss: 1.8053
Profiling... [9216/50176]	Loss: 1.7599
Profiling... [10240/50176]	Loss: 1.7271
Profiling... [11264/50176]	Loss: 1.8125
Profiling... [12288/50176]	Loss: 1.7975
Profiling... [13312/50176]	Loss: 1.8470
Profile done
epoch 1 train time consumed: 37.70s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4848
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.32066196131362,
                        "time": 27.651409361000333,
                        "accuracy": 0.484765625,
                        "total_cost": 9982136.497766439
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7739
Profiling... [2048/50176]	Loss: 1.7562
Profiling... [3072/50176]	Loss: 1.8330
Profiling... [4096/50176]	Loss: 1.9427
Profiling... [5120/50176]	Loss: 1.7803
Profiling... [6144/50176]	Loss: 1.7630
Profiling... [7168/50176]	Loss: 1.7421
Profiling... [8192/50176]	Loss: 1.8780
Profiling... [9216/50176]	Loss: 1.8187
Profiling... [10240/50176]	Loss: 1.8971
Profiling... [11264/50176]	Loss: 1.8415
Profiling... [12288/50176]	Loss: 1.9258
Profiling... [13312/50176]	Loss: 1.8806
Profile done
epoch 1 train time consumed: 13.09s
Validation Epoch: 8, Average loss: 0.0022, Accuracy: 0.4190
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.34836742766997,
                        "time": 8.85481741500007,
                        "accuracy": 0.41904296875,
                        "total_cost": 3697933.537096277
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8037
Profiling... [2048/50176]	Loss: 1.7162
Profiling... [3072/50176]	Loss: 1.8679
Profiling... [4096/50176]	Loss: 1.8661
Profiling... [5120/50176]	Loss: 1.8040
Profiling... [6144/50176]	Loss: 1.7925
Profiling... [7168/50176]	Loss: 1.8737
Profiling... [8192/50176]	Loss: 1.8566
Profiling... [9216/50176]	Loss: 1.8773
Profiling... [10240/50176]	Loss: 1.8949
Profiling... [11264/50176]	Loss: 1.8987
Profiling... [12288/50176]	Loss: 1.7860
Profiling... [13312/50176]	Loss: 1.8930
Profile done
epoch 1 train time consumed: 13.44s
Validation Epoch: 8, Average loss: 0.0029, Accuracy: 0.3339
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.38391225403663,
                        "time": 9.149174238000342,
                        "accuracy": 0.33388671875,
                        "total_cost": 4795355.43565271
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6843
Profiling... [2048/50176]	Loss: 1.9605
Profiling... [3072/50176]	Loss: 1.8834
Profiling... [4096/50176]	Loss: 1.8374
Profiling... [5120/50176]	Loss: 1.8048
Profiling... [6144/50176]	Loss: 1.8077
Profiling... [7168/50176]	Loss: 1.8956
Profiling... [8192/50176]	Loss: 1.8051
Profiling... [9216/50176]	Loss: 1.8583
Profiling... [10240/50176]	Loss: 1.8250
Profiling... [11264/50176]	Loss: 1.8370
Profiling... [12288/50176]	Loss: 1.7479
Profiling... [13312/50176]	Loss: 1.8604
Profile done
epoch 1 train time consumed: 15.11s
Validation Epoch: 8, Average loss: 0.0032, Accuracy: 0.3169
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.3889890158988,
                        "time": 10.442699371000344,
                        "accuracy": 0.31689453125,
                        "total_cost": 5766815.800564751
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8094
Profiling... [2048/50176]	Loss: 1.8869
Profiling... [3072/50176]	Loss: 1.8702
Profiling... [4096/50176]	Loss: 1.9060
Profiling... [5120/50176]	Loss: 1.8946
Profiling... [6144/50176]	Loss: 1.8978
Profiling... [7168/50176]	Loss: 1.8970
Profiling... [8192/50176]	Loss: 1.8648
Profiling... [9216/50176]	Loss: 1.7960
Profiling... [10240/50176]	Loss: 1.8461
Profiling... [11264/50176]	Loss: 1.8761
Profiling... [12288/50176]	Loss: 1.9172
Profiling... [13312/50176]	Loss: 1.7761
Profile done
epoch 1 train time consumed: 37.92s
Validation Epoch: 8, Average loss: 0.0029, Accuracy: 0.3397
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.30290442912278,
                        "time": 27.98144623799999,
                        "accuracy": 0.33974609375,
                        "total_cost": 14412978.343919512
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7671
Profiling... [2048/50176]	Loss: 1.6921
Profiling... [3072/50176]	Loss: 1.8172
Profiling... [4096/50176]	Loss: 1.9050
Profiling... [5120/50176]	Loss: 1.7660
Profiling... [6144/50176]	Loss: 1.8835
Profiling... [7168/50176]	Loss: 1.8523
Profiling... [8192/50176]	Loss: 1.8809
Profiling... [9216/50176]	Loss: 1.8424
Profiling... [10240/50176]	Loss: 1.8535
Profiling... [11264/50176]	Loss: 1.8538
Profiling... [12288/50176]	Loss: 1.7973
Profiling... [13312/50176]	Loss: 1.9636
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 8, Average loss: 0.0028, Accuracy: 0.3312
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.33060682176863,
                        "time": 8.892366048999975,
                        "accuracy": 0.33125,
                        "total_cost": 4697853.761735836
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8224
Profiling... [2048/50176]	Loss: 1.8977
Profiling... [3072/50176]	Loss: 1.9206
Profiling... [4096/50176]	Loss: 1.7953
Profiling... [5120/50176]	Loss: 1.9687
Profiling... [6144/50176]	Loss: 1.8907
Profiling... [7168/50176]	Loss: 1.8048
Profiling... [8192/50176]	Loss: 1.8155
Profiling... [9216/50176]	Loss: 1.8856
Profiling... [10240/50176]	Loss: 1.8179
Profiling... [11264/50176]	Loss: 1.8646
Profiling... [12288/50176]	Loss: 1.8460
Profiling... [13312/50176]	Loss: 1.8935
Profile done
epoch 1 train time consumed: 13.41s
Validation Epoch: 8, Average loss: 0.0029, Accuracy: 0.3229
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.36613479056398,
                        "time": 9.127493539999705,
                        "accuracy": 0.3228515625,
                        "total_cost": 4947510.110005889
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6871
Profiling... [2048/50176]	Loss: 1.8120
Profiling... [3072/50176]	Loss: 1.8654
Profiling... [4096/50176]	Loss: 1.8687
Profiling... [5120/50176]	Loss: 1.8329
Profiling... [6144/50176]	Loss: 1.8507
Profiling... [7168/50176]	Loss: 1.8797
Profiling... [8192/50176]	Loss: 1.8389
Profiling... [9216/50176]	Loss: 1.7710
Profiling... [10240/50176]	Loss: 1.8671
Profiling... [11264/50176]	Loss: 1.8154
Profiling... [12288/50176]	Loss: 1.9252
Profiling... [13312/50176]	Loss: 1.8386
Profile done
epoch 1 train time consumed: 15.12s
Validation Epoch: 8, Average loss: 0.0027, Accuracy: 0.3595
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.3699046786926,
                        "time": 10.435414387000492,
                        "accuracy": 0.35947265625,
                        "total_cost": 5080212.600245825
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7810
Profiling... [2048/50176]	Loss: 1.8035
Profiling... [3072/50176]	Loss: 1.8953
Profiling... [4096/50176]	Loss: 1.8891
Profiling... [5120/50176]	Loss: 1.8999
Profiling... [6144/50176]	Loss: 1.8482
Profiling... [7168/50176]	Loss: 1.8283
Profiling... [8192/50176]	Loss: 1.8611
Profiling... [9216/50176]	Loss: 1.8912
Profiling... [10240/50176]	Loss: 1.9170
Profiling... [11264/50176]	Loss: 1.8745
Profiling... [12288/50176]	Loss: 1.8086
Profiling... [13312/50176]	Loss: 1.8264
Profile done
epoch 1 train time consumed: 37.95s
Validation Epoch: 8, Average loss: 0.0033, Accuracy: 0.3173
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.28621818466091,
                        "time": 27.812467625000863,
                        "accuracy": 0.31728515625,
                        "total_cost": 15340086.791013097
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8199
Profiling... [2048/50176]	Loss: 1.8649
Profiling... [3072/50176]	Loss: 1.7355
Profiling... [4096/50176]	Loss: 1.8369
Profiling... [5120/50176]	Loss: 1.8343
Profiling... [6144/50176]	Loss: 1.8732
Profiling... [7168/50176]	Loss: 1.7987
Profiling... [8192/50176]	Loss: 1.8472
Profiling... [9216/50176]	Loss: 1.8639
Profiling... [10240/50176]	Loss: 1.8603
Profiling... [11264/50176]	Loss: 1.8213
Profiling... [12288/50176]	Loss: 1.9859
Profiling... [13312/50176]	Loss: 1.8277
Profile done
epoch 1 train time consumed: 13.11s
Validation Epoch: 8, Average loss: 0.0028, Accuracy: 0.3377
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.3124752590801,
                        "time": 8.874608041998727,
                        "accuracy": 0.3376953125,
                        "total_cost": 4598987.163464927
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7828
Profiling... [2048/50176]	Loss: 1.8180
Profiling... [3072/50176]	Loss: 1.8607
Profiling... [4096/50176]	Loss: 1.8250
Profiling... [5120/50176]	Loss: 1.8153
Profiling... [6144/50176]	Loss: 1.8236
Profiling... [7168/50176]	Loss: 1.8456
Profiling... [8192/50176]	Loss: 1.8562
Profiling... [9216/50176]	Loss: 1.8464
Profiling... [10240/50176]	Loss: 1.8352
Profiling... [11264/50176]	Loss: 1.8658
Profiling... [12288/50176]	Loss: 1.8043
Profiling... [13312/50176]	Loss: 1.8101
Profile done
epoch 1 train time consumed: 13.59s
Validation Epoch: 8, Average loss: 0.0026, Accuracy: 0.3669
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.3466852605411,
                        "time": 9.151523214999543,
                        "accuracy": 0.36689453125,
                        "total_cost": 4365059.78208123
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7380
Profiling... [2048/50176]	Loss: 1.7782
Profiling... [3072/50176]	Loss: 1.9535
Profiling... [4096/50176]	Loss: 1.8476
Profiling... [5120/50176]	Loss: 1.8341
Profiling... [6144/50176]	Loss: 1.8361
Profiling... [7168/50176]	Loss: 1.8619
Profiling... [8192/50176]	Loss: 1.8251
Profiling... [9216/50176]	Loss: 1.8237
Profiling... [10240/50176]	Loss: 1.8364
Profiling... [11264/50176]	Loss: 1.8513
Profiling... [12288/50176]	Loss: 1.8416
Profiling... [13312/50176]	Loss: 1.8611
Profile done
epoch 1 train time consumed: 15.13s
Validation Epoch: 8, Average loss: 0.0023, Accuracy: 0.4012
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.3504625985874,
                        "time": 10.42947605900008,
                        "accuracy": 0.401171875,
                        "total_cost": 4549566.966340833
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7763
Profiling... [2048/50176]	Loss: 1.7459
Profiling... [3072/50176]	Loss: 1.9249
Profiling... [4096/50176]	Loss: 1.8976
Profiling... [5120/50176]	Loss: 1.8941
Profiling... [6144/50176]	Loss: 1.8742
Profiling... [7168/50176]	Loss: 1.9094
Profiling... [8192/50176]	Loss: 1.8255
Profiling... [9216/50176]	Loss: 1.8397
Profiling... [10240/50176]	Loss: 1.7845
Profiling... [11264/50176]	Loss: 1.8602
Profiling... [12288/50176]	Loss: 1.8905
Profiling... [13312/50176]	Loss: 1.9298
Profile done
epoch 1 train time consumed: 38.25s
Validation Epoch: 8, Average loss: 0.0031, Accuracy: 0.3195
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.26512517682445,
                        "time": 28.391383268000936,
                        "accuracy": 0.31953125,
                        "total_cost": 15549315.041643545
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.25 bs: 128 pl: 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.0
[GPU_0] Set GPU power limit to 150W.
[GPU_0] Set GPU power limit to 150W.
Training Epoch: 8 [128/50048]	Loss: 1.4254
Training Epoch: 8 [256/50048]	Loss: 1.7772
Training Epoch: 8 [384/50048]	Loss: 1.7563
Training Epoch: 8 [512/50048]	Loss: 1.7809
Training Epoch: 8 [640/50048]	Loss: 1.7136
Training Epoch: 8 [768/50048]	Loss: 1.6053
Training Epoch: 8 [896/50048]	Loss: 1.5530
Training Epoch: 8 [1024/50048]	Loss: 2.0523
Training Epoch: 8 [1152/50048]	Loss: 1.7680
Training Epoch: 8 [1280/50048]	Loss: 1.9424
Training Epoch: 8 [1408/50048]	Loss: 1.8413
Training Epoch: 8 [1536/50048]	Loss: 1.8781
Training Epoch: 8 [1664/50048]	Loss: 1.4237
Training Epoch: 8 [1792/50048]	Loss: 1.6211
Training Epoch: 8 [1920/50048]	Loss: 1.8831
Training Epoch: 8 [2048/50048]	Loss: 1.7335
Training Epoch: 8 [2176/50048]	Loss: 1.8873
Training Epoch: 8 [2304/50048]	Loss: 1.5250
Training Epoch: 8 [2432/50048]	Loss: 1.7958
Training Epoch: 8 [2560/50048]	Loss: 1.6758
Training Epoch: 8 [2688/50048]	Loss: 2.0554
Training Epoch: 8 [2816/50048]	Loss: 1.5939
Training Epoch: 8 [2944/50048]	Loss: 1.9590
Training Epoch: 8 [3072/50048]	Loss: 1.5997
Training Epoch: 8 [3200/50048]	Loss: 1.4213
Training Epoch: 8 [3328/50048]	Loss: 1.6895
Training Epoch: 8 [3456/50048]	Loss: 1.6347
Training Epoch: 8 [3584/50048]	Loss: 1.6970
Training Epoch: 8 [3712/50048]	Loss: 1.6675
Training Epoch: 8 [3840/50048]	Loss: 1.7567
Training Epoch: 8 [3968/50048]	Loss: 1.6108
Training Epoch: 8 [4096/50048]	Loss: 1.5738
Training Epoch: 8 [4224/50048]	Loss: 1.7376
Training Epoch: 8 [4352/50048]	Loss: 1.6908
Training Epoch: 8 [4480/50048]	Loss: 1.6910
Training Epoch: 8 [4608/50048]	Loss: 2.0445
Training Epoch: 8 [4736/50048]	Loss: 1.2150
Training Epoch: 8 [4864/50048]	Loss: 1.6835
Training Epoch: 8 [4992/50048]	Loss: 1.5586
Training Epoch: 8 [5120/50048]	Loss: 2.0130
Training Epoch: 8 [5248/50048]	Loss: 1.5753
Training Epoch: 8 [5376/50048]	Loss: 1.8010
Training Epoch: 8 [5504/50048]	Loss: 1.7365
Training Epoch: 8 [5632/50048]	Loss: 1.3687
Training Epoch: 8 [5760/50048]	Loss: 1.6540
Training Epoch: 8 [5888/50048]	Loss: 1.7313
Training Epoch: 8 [6016/50048]	Loss: 1.7183
Training Epoch: 8 [6144/50048]	Loss: 1.6921
Training Epoch: 8 [6272/50048]	Loss: 1.7850
Training Epoch: 8 [6400/50048]	Loss: 1.6795
Training Epoch: 8 [6528/50048]	Loss: 1.7406
Training Epoch: 8 [6656/50048]	Loss: 1.8640
Training Epoch: 8 [6784/50048]	Loss: 1.4945
Training Epoch: 8 [6912/50048]	Loss: 1.6874
Training Epoch: 8 [7040/50048]	Loss: 1.8266
Training Epoch: 8 [7168/50048]	Loss: 1.6843
Training Epoch: 8 [7296/50048]	Loss: 1.7845
Training Epoch: 8 [7424/50048]	Loss: 1.5815
Training Epoch: 8 [7552/50048]	Loss: 1.5700
Training Epoch: 8 [7680/50048]	Loss: 1.5755
Training Epoch: 8 [7808/50048]	Loss: 1.7454
Training Epoch: 8 [7936/50048]	Loss: 1.7770
Training Epoch: 8 [8064/50048]	Loss: 1.9568
Training Epoch: 8 [8192/50048]	Loss: 1.8756
Training Epoch: 8 [8320/50048]	Loss: 1.8060
Training Epoch: 8 [8448/50048]	Loss: 1.7371
Training Epoch: 8 [8576/50048]	Loss: 1.6700
Training Epoch: 8 [8704/50048]	Loss: 1.6061
Training Epoch: 8 [8832/50048]	Loss: 1.9215
Training Epoch: 8 [8960/50048]	Loss: 1.7859
Training Epoch: 8 [9088/50048]	Loss: 1.7765
Training Epoch: 8 [9216/50048]	Loss: 1.7136
Training Epoch: 8 [9344/50048]	Loss: 1.5268
Training Epoch: 8 [9472/50048]	Loss: 1.5883
Training Epoch: 8 [9600/50048]	Loss: 1.5388
Training Epoch: 8 [9728/50048]	Loss: 1.8747
Training Epoch: 8 [9856/50048]	Loss: 1.8795
Training Epoch: 8 [9984/50048]	Loss: 1.7181
Training Epoch: 8 [10112/50048]	Loss: 1.5956
Training Epoch: 8 [10240/50048]	Loss: 1.5388
Training Epoch: 8 [10368/50048]	Loss: 1.5626
Training Epoch: 8 [10496/50048]	Loss: 1.6136
Training Epoch: 8 [10624/50048]	Loss: 1.9690
Training Epoch: 8 [10752/50048]	Loss: 1.7930
Training Epoch: 8 [10880/50048]	Loss: 2.0577
Training Epoch: 8 [11008/50048]	Loss: 1.7116
Training Epoch: 8 [11136/50048]	Loss: 1.5722
Training Epoch: 8 [11264/50048]	Loss: 1.8231
Training Epoch: 8 [11392/50048]	Loss: 1.6810
Training Epoch: 8 [11520/50048]	Loss: 1.8336
Training Epoch: 8 [11648/50048]	Loss: 1.9191
Training Epoch: 8 [11776/50048]	Loss: 1.5071
Training Epoch: 8 [11904/50048]	Loss: 1.9500
Training Epoch: 8 [12032/50048]	Loss: 1.6035
Training Epoch: 8 [12160/50048]	Loss: 1.7057
Training Epoch: 8 [12288/50048]	Loss: 1.7015
Training Epoch: 8 [12416/50048]	Loss: 1.6584
Training Epoch: 8 [12544/50048]	Loss: 1.6513
Training Epoch: 8 [12672/50048]	Loss: 1.9427
Training Epoch: 8 [12800/50048]	Loss: 1.8881
Training Epoch: 8 [12928/50048]	Loss: 1.7031
Training Epoch: 8 [13056/50048]	Loss: 1.5788
Training Epoch: 8 [13184/50048]	Loss: 1.5631
Training Epoch: 8 [13312/50048]	Loss: 1.7137
Training Epoch: 8 [13440/50048]	Loss: 1.3811
Training Epoch: 8 [13568/50048]	Loss: 1.3945
Training Epoch: 8 [13696/50048]	Loss: 1.9055
Training Epoch: 8 [13824/50048]	Loss: 1.3600
Training Epoch: 8 [13952/50048]	Loss: 1.5221
Training Epoch: 8 [14080/50048]	Loss: 1.4361
Training Epoch: 8 [14208/50048]	Loss: 1.8465
Training Epoch: 8 [14336/50048]	Loss: 1.8575
Training Epoch: 8 [14464/50048]	Loss: 1.7266
Training Epoch: 8 [14592/50048]	Loss: 1.7306
Training Epoch: 8 [14720/50048]	Loss: 1.5649
Training Epoch: 8 [14848/50048]	Loss: 1.7244
Training Epoch: 8 [14976/50048]	Loss: 1.6197
Training Epoch: 8 [15104/50048]	Loss: 1.7525
Training Epoch: 8 [15232/50048]	Loss: 1.7086
Training Epoch: 8 [15360/50048]	Loss: 1.9150
Training Epoch: 8 [15488/50048]	Loss: 1.6271
Training Epoch: 8 [15616/50048]	Loss: 1.7203
Training Epoch: 8 [15744/50048]	Loss: 1.6034
Training Epoch: 8 [15872/50048]	Loss: 1.7632
Training Epoch: 8 [16000/50048]	Loss: 1.7120
Training Epoch: 8 [16128/50048]	Loss: 1.6665
Training Epoch: 8 [16256/50048]	Loss: 1.4972
Training Epoch: 8 [16384/50048]	Loss: 1.7756
Training Epoch: 8 [16512/50048]	Loss: 1.5545
Training Epoch: 8 [16640/50048]	Loss: 1.6957
Training Epoch: 8 [16768/50048]	Loss: 1.5625
Training Epoch: 8 [16896/50048]	Loss: 1.7340
Training Epoch: 8 [17024/50048]	Loss: 1.4770
Training Epoch: 8 [17152/50048]	Loss: 1.4737
Training Epoch: 8 [17280/50048]	Loss: 1.6161
Training Epoch: 8 [17408/50048]	Loss: 1.8689
Training Epoch: 8 [17536/50048]	Loss: 1.4192
Training Epoch: 8 [17664/50048]	Loss: 1.8254
Training Epoch: 8 [17792/50048]	Loss: 1.8335
Training Epoch: 8 [17920/50048]	Loss: 1.6584
Training Epoch: 8 [18048/50048]	Loss: 1.6992
Training Epoch: 8 [18176/50048]	Loss: 1.5844
Training Epoch: 8 [18304/50048]	Loss: 1.5382
Training Epoch: 8 [18432/50048]	Loss: 1.5300
Training Epoch: 8 [18560/50048]	Loss: 1.6510
Training Epoch: 8 [18688/50048]	Loss: 1.7984
Training Epoch: 8 [18816/50048]	Loss: 1.5702
Training Epoch: 8 [18944/50048]	Loss: 1.5133
Training Epoch: 8 [19072/50048]	Loss: 1.8435
Training Epoch: 8 [19200/50048]	Loss: 1.6866
Training Epoch: 8 [19328/50048]	Loss: 1.6958
Training Epoch: 8 [19456/50048]	Loss: 1.6243
Training Epoch: 8 [19584/50048]	Loss: 1.9961
Training Epoch: 8 [19712/50048]	Loss: 1.5041
Training Epoch: 8 [19840/50048]	Loss: 1.6308
Training Epoch: 8 [19968/50048]	Loss: 1.8476
Training Epoch: 8 [20096/50048]	Loss: 1.6607
Training Epoch: 8 [20224/50048]	Loss: 1.4749
Training Epoch: 8 [20352/50048]	Loss: 1.6411
Training Epoch: 8 [20480/50048]	Loss: 1.6137
Training Epoch: 8 [20608/50048]	Loss: 1.8988
Training Epoch: 8 [20736/50048]	Loss: 1.5441
Training Epoch: 8 [20864/50048]	Loss: 1.7501
Training Epoch: 8 [20992/50048]	Loss: 1.4926
Training Epoch: 8 [21120/50048]	Loss: 1.5078
Training Epoch: 8 [21248/50048]	Loss: 1.3924
Training Epoch: 8 [21376/50048]	Loss: 1.7101
Training Epoch: 8 [21504/50048]	Loss: 1.7575
Training Epoch: 8 [21632/50048]	Loss: 1.7658
Training Epoch: 8 [21760/50048]	Loss: 1.7663
Training Epoch: 8 [21888/50048]	Loss: 1.7267
Training Epoch: 8 [22016/50048]	Loss: 1.8522
Training Epoch: 8 [22144/50048]	Loss: 1.8243
Training Epoch: 8 [22272/50048]	Loss: 1.9226
Training Epoch: 8 [22400/50048]	Loss: 1.5340
Training Epoch: 8 [22528/50048]	Loss: 1.5837
Training Epoch: 8 [22656/50048]	Loss: 1.3897
Training Epoch: 8 [22784/50048]	Loss: 1.3990
Training Epoch: 8 [22912/50048]	Loss: 1.4986
Training Epoch: 8 [23040/50048]	Loss: 1.4405
Training Epoch: 8 [23168/50048]	Loss: 1.6423
Training Epoch: 8 [23296/50048]	Loss: 1.7735
Training Epoch: 8 [23424/50048]	Loss: 1.3458
Training Epoch: 8 [23552/50048]	Loss: 1.7377
Training Epoch: 8 [23680/50048]	Loss: 1.6952
Training Epoch: 8 [23808/50048]	Loss: 1.5161
Training Epoch: 8 [23936/50048]	Loss: 1.6152
Training Epoch: 8 [24064/50048]	Loss: 1.6167
Training Epoch: 8 [24192/50048]	Loss: 1.7781
Training Epoch: 8 [24320/50048]	Loss: 1.5632
Training Epoch: 8 [24448/50048]	Loss: 1.4536
Training Epoch: 8 [24576/50048]	Loss: 1.6680
Training Epoch: 8 [24704/50048]	Loss: 1.7506
Training Epoch: 8 [24832/50048]	Loss: 1.5363
Training Epoch: 8 [24960/50048]	Loss: 1.5651
Training Epoch: 8 [25088/50048]	Loss: 1.6299
Training Epoch: 8 [25216/50048]	Loss: 1.5728
Training Epoch: 8 [25344/50048]	Loss: 1.7255
Training Epoch: 8 [25472/50048]	Loss: 1.6275
Training Epoch: 8 [25600/50048]	Loss: 1.6031
Training Epoch: 8 [25728/50048]	Loss: 1.7479
Training Epoch: 8 [25856/50048]	Loss: 1.7800
Training Epoch: 8 [25984/50048]	Loss: 1.8729
Training Epoch: 8 [26112/50048]	Loss: 1.5325
Training Epoch: 8 [26240/50048]	Loss: 1.4873
Training Epoch: 8 [26368/50048]	Loss: 1.7049
Training Epoch: 8 [26496/50048]	Loss: 2.0739
Training Epoch: 8 [26624/50048]	Loss: 1.9038
Training Epoch: 8 [26752/50048]	Loss: 1.8454
Training Epoch: 8 [26880/50048]	Loss: 1.4588
Training Epoch: 8 [27008/50048]	Loss: 1.5253
Training Epoch: 8 [27136/50048]	Loss: 1.5428
Training Epoch: 8 [27264/50048]	Loss: 1.6688
Training Epoch: 8 [27392/50048]	Loss: 1.6200
Training Epoch: 8 [27520/50048]	Loss: 1.4724
Training Epoch: 8 [27648/50048]	Loss: 2.0287
Training Epoch: 8 [27776/50048]	Loss: 1.5503
Training Epoch: 8 [27904/50048]	Loss: 1.7171
Training Epoch: 8 [28032/50048]	Loss: 1.8068
Training Epoch: 8 [28160/50048]	Loss: 2.0501
Training Epoch: 8 [28288/50048]	Loss: 1.8262
Training Epoch: 8 [28416/50048]	Loss: 1.7216
Training Epoch: 8 [28544/50048]	Loss: 1.8025
Training Epoch: 8 [28672/50048]	Loss: 1.6157
Training Epoch: 8 [28800/50048]	Loss: 1.7162
Training Epoch: 8 [28928/50048]	Loss: 1.7050
Training Epoch: 8 [29056/50048]	Loss: 1.8547
Training Epoch: 8 [29184/50048]	Loss: 1.6955
Training Epoch: 8 [29312/50048]	Loss: 1.4691
Training Epoch: 8 [29440/50048]	Loss: 1.8095
Training Epoch: 8 [29568/50048]	Loss: 1.6619
Training Epoch: 8 [29696/50048]	Loss: 1.5761
Training Epoch: 8 [29824/50048]	Loss: 1.6007
Training Epoch: 8 [29952/50048]	Loss: 1.6314
Training Epoch: 8 [30080/50048]	Loss: 1.7043
Training Epoch: 8 [30208/50048]	Loss: 1.4836
Training Epoch: 8 [30336/50048]	Loss: 1.7376
Training Epoch: 8 [30464/50048]	Loss: 1.6547
Training Epoch: 8 [30592/50048]	Loss: 1.8466
Training Epoch: 8 [30720/50048]	Loss: 1.6144
Training Epoch: 8 [30848/50048]	Loss: 1.7933
Training Epoch: 8 [30976/50048]	Loss: 1.6468
Training Epoch: 8 [31104/50048]	Loss: 1.6688
Training Epoch: 8 [31232/50048]	Loss: 1.7074
Training Epoch: 8 [31360/50048]	Loss: 1.4301
Training Epoch: 8 [31488/50048]	Loss: 1.4546
Training Epoch: 8 [31616/50048]	Loss: 1.7491
Training Epoch: 8 [31744/50048]	Loss: 1.6719
Training Epoch: 8 [31872/50048]	Loss: 1.6016
Training Epoch: 8 [32000/50048]	Loss: 1.6604
Training Epoch: 8 [32128/50048]	Loss: 1.6123
Training Epoch: 8 [32256/50048]	Loss: 1.5466
Training Epoch: 8 [32384/50048]	Loss: 1.4858
Training Epoch: 8 [32512/50048]	Loss: 1.7553
Training Epoch: 8 [32640/50048]	Loss: 1.5797
Training Epoch: 8 [32768/50048]	Loss: 1.7812
Training Epoch: 8 [32896/50048]	Loss: 1.5693
Training Epoch: 8 [33024/50048]	Loss: 1.6313
Training Epoch: 8 [33152/50048]	Loss: 1.4178
Training Epoch: 8 [33280/50048]	Loss: 1.5913
Training Epoch: 8 [33408/50048]	Loss: 1.6824
Training Epoch: 8 [33536/50048]	Loss: 1.6348
Training Epoch: 8 [33664/50048]	Loss: 1.6055
Training Epoch: 8 [33792/50048]	Loss: 1.6199
Training Epoch: 8 [33920/50048]	Loss: 1.7203
Training Epoch: 8 [34048/50048]	Loss: 1.8105
Training Epoch: 8 [34176/50048]	Loss: 1.5108
Training Epoch: 8 [34304/50048]	Loss: 1.6405
Training Epoch: 8 [34432/50048]	Loss: 1.6134
Training Epoch: 8 [34560/50048]	Loss: 1.5648
Training Epoch: 8 [34688/50048]	Loss: 1.5508
Training Epoch: 8 [34816/50048]	Loss: 1.4385
Training Epoch: 8 [34944/50048]	Loss: 1.7953
Training Epoch: 8 [35072/50048]	Loss: 1.8928
Training Epoch: 8 [35200/50048]	Loss: 1.6296
Training Epoch: 8 [35328/50048]	Loss: 1.6293
Training Epoch: 8 [35456/50048]	Loss: 1.7857
Training Epoch: 8 [35584/50048]	Loss: 1.6793
Training Epoch: 8 [35712/50048]	Loss: 1.7446
Training Epoch: 8 [35840/50048]	Loss: 1.6291
Training Epoch: 8 [35968/50048]	Loss: 1.7304
Training Epoch: 8 [36096/50048]	Loss: 1.6850
Training Epoch: 8 [36224/50048]	Loss: 1.7928
Training Epoch: 8 [36352/50048]	Loss: 1.5602
Training Epoch: 8 [36480/50048]	Loss: 1.6754
Training Epoch: 8 [36608/50048]	Loss: 1.4033
Training Epoch: 8 [36736/50048]	Loss: 1.5617
Training Epoch: 8 [36864/50048]	Loss: 1.6463
Training Epoch: 8 [36992/50048]	Loss: 1.8651
Training Epoch: 8 [37120/50048]	Loss: 1.4060
Training Epoch: 8 [37248/50048]	Loss: 1.7103
Training Epoch: 8 [37376/50048]	Loss: 1.9337
Training Epoch: 8 [37504/50048]	Loss: 1.4824
Training Epoch: 8 [37632/50048]	Loss: 1.6533
Training Epoch: 8 [37760/50048]	Loss: 1.8431
Training Epoch: 8 [37888/50048]	Loss: 1.9030
Training Epoch: 8 [38016/50048]	Loss: 1.7819
Training Epoch: 8 [38144/50048]	Loss: 1.6215
Training Epoch: 8 [38272/50048]	Loss: 1.4267
Training Epoch: 8 [38400/50048]	Loss: 1.5139
Training Epoch: 8 [38528/50048]	Loss: 1.6080
Training Epoch: 8 [38656/50048]	Loss: 1.7534
Training Epoch: 8 [38784/50048]	Loss: 1.6870
Training Epoch: 8 [38912/50048]	Loss: 1.9285
Training Epoch: 8 [39040/50048]	Loss: 1.6924
Training Epoch: 8 [39168/50048]	Loss: 1.5448
Training Epoch: 8 [39296/50048]	Loss: 1.6348
Training Epoch: 8 [39424/50048]	Loss: 1.6151
Training Epoch: 8 [39552/50048]	Loss: 1.8754
Training Epoch: 8 [39680/50048]	Loss: 1.6687
Training Epoch: 8 [39808/50048]	Loss: 1.5720
Training Epoch: 8 [39936/50048]	Loss: 1.4287
Training Epoch: 8 [40064/50048]	Loss: 1.7449
Training Epoch: 8 [40192/50048]	Loss: 1.5657
Training Epoch: 8 [40320/50048]	Loss: 1.6901
Training Epoch: 8 [40448/50048]	Loss: 1.7148
Training Epoch: 8 [40576/50048]	Loss: 1.6643
Training Epoch: 8 [40704/50048]	Loss: 1.5036
Training Epoch: 8 [40832/50048]	Loss: 1.8392
Training Epoch: 8 [40960/50048]	Loss: 1.7477
Training Epoch: 8 [41088/50048]	Loss: 1.8611
Training Epoch: 8 [41216/50048]	Loss: 1.6584
Training Epoch: 8 [41344/50048]	Loss: 1.4987
Training Epoch: 8 [41472/50048]	Loss: 1.8792
Training Epoch: 8 [41600/50048]	Loss: 1.7587
Training Epoch: 8 [41728/50048]	Loss: 1.7242
Training Epoch: 8 [41856/50048]	Loss: 1.6368
Training Epoch: 8 [41984/50048]	Loss: 1.4250
Training Epoch: 8 [42112/50048]	Loss: 1.5196
Training Epoch: 8 [42240/50048]	Loss: 1.7507
Training Epoch: 8 [42368/50048]	Loss: 1.7836
Training Epoch: 8 [42496/50048]	Loss: 1.5438
Training Epoch: 8 [42624/50048]	Loss: 1.7273
Training Epoch: 8 [42752/50048]	Loss: 1.4213
Training Epoch: 8 [42880/50048]	Loss: 1.3179
Training Epoch: 8 [43008/50048]	Loss: 1.5859
Training Epoch: 8 [43136/50048]	Loss: 1.7940
Training Epoch: 8 [43264/50048]	Loss: 1.7255
Training Epoch: 8 [43392/50048]	Loss: 1.5556
Training Epoch: 8 [43520/50048]	Loss: 1.5784
Training Epoch: 8 [43648/50048]	Loss: 1.5991
Training Epoch: 8 [43776/50048]	Loss: 1.7390
Training Epoch: 8 [43904/50048]	Loss: 1.5379
Training Epoch: 8 [44032/50048]	Loss: 1.8658
Training Epoch: 8 [44160/50048]	Loss: 1.6455
Training Epoch: 8 [44288/50048]	Loss: 1.6580
Training Epoch: 8 [44416/50048]	Loss: 1.7548
Training Epoch: 8 [44544/50048]	Loss: 1.6513
Training Epoch: 8 [44672/50048]	Loss: 1.6801
Training Epoch: 8 [44800/50048]	Loss: 1.6968
Training Epoch: 8 [44928/50048]	Loss: 2.0126
Training Epoch: 8 [45056/50048]	Loss: 1.5207
Training Epoch: 8 [45184/50048]	Loss: 1.6614
Training Epoch: 8 [45312/50048]	Loss: 1.6856
Training Epoch: 8 [45440/50048]	Loss: 1.6816
Training Epoch: 8 [45568/50048]	Loss: 1.6251
Training Epoch: 8 [45696/50048]	Loss: 1.4422
Training Epoch: 8 [45824/50048]	Loss: 1.3846
Training Epoch: 8 [45952/50048]	Loss: 1.6464
Training Epoch: 8 [46080/50048]	Loss: 1.6462
Training Epoch: 8 [46208/50048]	Loss: 1.8152
Training Epoch: 8 [46336/50048]	Loss: 1.8098
Training Epoch: 8 [46464/50048]	Loss: 1.4333
Training Epoch: 8 [46592/50048]	Loss: 1.7822
Training Epoch: 8 [46720/50048]	Loss: 1.6468
Training Epoch: 8 [46848/50048]	Loss: 1.4907
Training Epoch: 8 [46976/50048]	Loss: 1.5676
Training Epoch: 8 [47104/50048]	Loss: 1.6029
Training Epoch: 8 [47232/50048]	Loss: 1.4633
Training Epoch: 8 [47360/50048]	Loss: 1.8993
Training Epoch: 8 [47488/50048]	Loss: 1.7167
Training Epoch: 8 [47616/50048]	Loss: 1.5941
Training Epoch: 8 [47744/50048]	Loss: 1.5992
Training Epoch: 8 [47872/50048]	Loss: 1.6682
Training Epoch: 8 [48000/50048]	Loss: 1.8856
Training Epoch: 8 [48128/50048]	Loss: 1.8031
Training Epoch: 8 [48256/50048]	Loss: 1.7272
Training Epoch: 8 [48384/50048]	Loss: 1.7552
Training Epoch: 8 [48512/50048]	Loss: 1.6972
Training Epoch: 8 [48640/50048]	Loss: 1.5220
Training Epoch: 8 [48768/50048]	Loss: 1.6466
Training Epoch: 8 [48896/50048]	Loss: 1.7526
Training Epoch: 8 [49024/50048]	Loss: 1.7937
Training Epoch: 8 [49152/50048]	Loss: 1.6609
Training Epoch: 8 [49280/50048]	Loss: 1.8739
Training Epoch: 8 [49408/50048]	Loss: 1.7137
Training Epoch: 8 [49536/50048]	Loss: 1.5195
Training Epoch: 8 [49664/50048]	Loss: 1.8216
Training Epoch: 8 [49792/50048]	Loss: 1.4408
Training Epoch: 8 [49920/50048]	Loss: 1.6276
Training Epoch: 8 [50048/50048]	Loss: 1.7018
Validation Epoch: 8, Average loss: 0.0130, Accuracy: 0.5402
Training Epoch: 9 [128/50048]	Loss: 1.7415
Training Epoch: 9 [256/50048]	Loss: 1.8142
Training Epoch: 9 [384/50048]	Loss: 1.4978
Training Epoch: 9 [512/50048]	Loss: 1.5994
Training Epoch: 9 [640/50048]	Loss: 1.6004
Training Epoch: 9 [768/50048]	Loss: 1.5546
Training Epoch: 9 [896/50048]	Loss: 1.7554
Training Epoch: 9 [1024/50048]	Loss: 1.6659
Training Epoch: 9 [1152/50048]	Loss: 1.8659
Training Epoch: 9 [1280/50048]	Loss: 1.6239
Training Epoch: 9 [1408/50048]	Loss: 1.8276
Training Epoch: 9 [1536/50048]	Loss: 1.4273
Training Epoch: 9 [1664/50048]	Loss: 1.6013
Training Epoch: 9 [1792/50048]	Loss: 1.4603
Training Epoch: 9 [1920/50048]	Loss: 1.7942
Training Epoch: 9 [2048/50048]	Loss: 1.6375
Training Epoch: 9 [2176/50048]	Loss: 1.7858
Training Epoch: 9 [2304/50048]	Loss: 1.7133
Training Epoch: 9 [2432/50048]	Loss: 1.6728
Training Epoch: 9 [2560/50048]	Loss: 1.4023
Training Epoch: 9 [2688/50048]	Loss: 1.7088
Training Epoch: 9 [2816/50048]	Loss: 1.5606
Training Epoch: 9 [2944/50048]	Loss: 1.6009
Training Epoch: 9 [3072/50048]	Loss: 1.9592
Training Epoch: 9 [3200/50048]	Loss: 1.4874
Training Epoch: 9 [3328/50048]	Loss: 1.7487
Training Epoch: 9 [3456/50048]	Loss: 1.7121
Training Epoch: 9 [3584/50048]	Loss: 1.7173
Training Epoch: 9 [3712/50048]	Loss: 1.5918
Training Epoch: 9 [3840/50048]	Loss: 1.6504
Training Epoch: 9 [3968/50048]	Loss: 1.6873
Training Epoch: 9 [4096/50048]	Loss: 1.9291
Training Epoch: 9 [4224/50048]	Loss: 1.5915
Training Epoch: 9 [4352/50048]	Loss: 1.4594
Training Epoch: 9 [4480/50048]	Loss: 1.5639
Training Epoch: 9 [4608/50048]	Loss: 1.5478
Training Epoch: 9 [4736/50048]	Loss: 1.8149
Training Epoch: 9 [4864/50048]	Loss: 1.8064
Training Epoch: 9 [4992/50048]	Loss: 1.7689
Training Epoch: 9 [5120/50048]	Loss: 1.4232
Training Epoch: 9 [5248/50048]	Loss: 1.4469
Training Epoch: 9 [5376/50048]	Loss: 1.8657
Training Epoch: 9 [5504/50048]	Loss: 1.3769
Training Epoch: 9 [5632/50048]	Loss: 1.6866
Training Epoch: 9 [5760/50048]	Loss: 1.4775
Training Epoch: 9 [5888/50048]	Loss: 1.5770
Training Epoch: 9 [6016/50048]	Loss: 1.9719
Training Epoch: 9 [6144/50048]	Loss: 1.3896
Training Epoch: 9 [6272/50048]	Loss: 1.5111
Training Epoch: 9 [6400/50048]	Loss: 1.8736
Training Epoch: 9 [6528/50048]	Loss: 1.5075
Training Epoch: 9 [6656/50048]	Loss: 1.4844
Training Epoch: 9 [6784/50048]	Loss: 2.0061
Training Epoch: 9 [6912/50048]	Loss: 1.6682
Training Epoch: 9 [7040/50048]	Loss: 1.5224
Training Epoch: 9 [7168/50048]	Loss: 1.6706
Training Epoch: 9 [7296/50048]	Loss: 1.7455
Training Epoch: 9 [7424/50048]	Loss: 1.8640
Training Epoch: 9 [7552/50048]	Loss: 1.5786
Training Epoch: 9 [7680/50048]	Loss: 1.6427
Training Epoch: 9 [7808/50048]	Loss: 1.5438
Training Epoch: 9 [7936/50048]	Loss: 1.6928
Training Epoch: 9 [8064/50048]	Loss: 1.7709
Training Epoch: 9 [8192/50048]	Loss: 1.4088
Training Epoch: 9 [8320/50048]	Loss: 1.7099
Training Epoch: 9 [8448/50048]	Loss: 1.6053
Training Epoch: 9 [8576/50048]	Loss: 1.8311
Training Epoch: 9 [8704/50048]	Loss: 1.6643
Training Epoch: 9 [8832/50048]	Loss: 1.6313
Training Epoch: 9 [8960/50048]	Loss: 1.6046
Training Epoch: 9 [9088/50048]	Loss: 1.5499
Training Epoch: 9 [9216/50048]	Loss: 1.6414
Training Epoch: 9 [9344/50048]	Loss: 1.6338
Training Epoch: 9 [9472/50048]	Loss: 1.4815
Training Epoch: 9 [9600/50048]	Loss: 1.7219
Training Epoch: 9 [9728/50048]	Loss: 1.7699
Training Epoch: 9 [9856/50048]	Loss: 1.4339
Training Epoch: 9 [9984/50048]	Loss: 1.5480
Training Epoch: 9 [10112/50048]	Loss: 1.5015
Training Epoch: 9 [10240/50048]	Loss: 1.4375
Training Epoch: 9 [10368/50048]	Loss: 1.7960
Training Epoch: 9 [10496/50048]	Loss: 1.7947
Training Epoch: 9 [10624/50048]	Loss: 1.7379
Training Epoch: 9 [10752/50048]	Loss: 1.6403
Training Epoch: 9 [10880/50048]	Loss: 1.6423
Training Epoch: 9 [11008/50048]	Loss: 1.7041
Training Epoch: 9 [11136/50048]	Loss: 1.6042
Training Epoch: 9 [11264/50048]	Loss: 1.6754
Training Epoch: 9 [11392/50048]	Loss: 1.6454
Training Epoch: 9 [11520/50048]	Loss: 1.4911
Training Epoch: 9 [11648/50048]	Loss: 1.5840
Training Epoch: 9 [11776/50048]	Loss: 1.7415
Training Epoch: 9 [11904/50048]	Loss: 1.7834
Training Epoch: 9 [12032/50048]	Loss: 1.5173
Training Epoch: 9 [12160/50048]	Loss: 1.7140
Training Epoch: 9 [12288/50048]	Loss: 1.4037
Training Epoch: 9 [12416/50048]	Loss: 1.5261
Training Epoch: 9 [12544/50048]	Loss: 1.4521
Training Epoch: 9 [12672/50048]	Loss: 1.7095
Training Epoch: 9 [12800/50048]	Loss: 1.8443
Training Epoch: 9 [12928/50048]	Loss: 1.6683
Training Epoch: 9 [13056/50048]	Loss: 1.6692
Training Epoch: 9 [13184/50048]	Loss: 1.7375
Training Epoch: 9 [13312/50048]	Loss: 1.5898
Training Epoch: 9 [13440/50048]	Loss: 1.5941
Training Epoch: 9 [13568/50048]	Loss: 1.6797
Training Epoch: 9 [13696/50048]	Loss: 1.5684
Training Epoch: 9 [13824/50048]	Loss: 1.7737
Training Epoch: 9 [13952/50048]	Loss: 1.6624
Training Epoch: 9 [14080/50048]	Loss: 1.6155
Training Epoch: 9 [14208/50048]	Loss: 1.6193
Training Epoch: 9 [14336/50048]	Loss: 1.6335
Training Epoch: 9 [14464/50048]	Loss: 1.1314
Training Epoch: 9 [14592/50048]	Loss: 1.7728
Training Epoch: 9 [14720/50048]	Loss: 1.6709
Training Epoch: 9 [14848/50048]	Loss: 1.8009
Training Epoch: 9 [14976/50048]	Loss: 1.6604
Training Epoch: 9 [15104/50048]	Loss: 1.6045
Training Epoch: 9 [15232/50048]	Loss: 1.3423
Training Epoch: 9 [15360/50048]	Loss: 1.5652
Training Epoch: 9 [15488/50048]	Loss: 1.4736
Training Epoch: 9 [15616/50048]	Loss: 1.3847
Training Epoch: 9 [15744/50048]	Loss: 1.7824
Training Epoch: 9 [15872/50048]	Loss: 1.6294
Training Epoch: 9 [16000/50048]	Loss: 1.6376
Training Epoch: 9 [16128/50048]	Loss: 1.7560
Training Epoch: 9 [16256/50048]	Loss: 1.6146
Training Epoch: 9 [16384/50048]	Loss: 1.6202
Training Epoch: 9 [16512/50048]	Loss: 1.6495
Training Epoch: 9 [16640/50048]	Loss: 1.5472
Training Epoch: 9 [16768/50048]	Loss: 1.3583
Training Epoch: 9 [16896/50048]	Loss: 1.5910
Training Epoch: 9 [17024/50048]	Loss: 1.5637
Training Epoch: 9 [17152/50048]	Loss: 1.7960
Training Epoch: 9 [17280/50048]	Loss: 1.5222
Training Epoch: 9 [17408/50048]	Loss: 1.5884
Training Epoch: 9 [17536/50048]	Loss: 1.6756
Training Epoch: 9 [17664/50048]	Loss: 1.6715
Training Epoch: 9 [17792/50048]	Loss: 1.6323
Training Epoch: 9 [17920/50048]	Loss: 1.6351
Training Epoch: 9 [18048/50048]	Loss: 1.7170
Training Epoch: 9 [18176/50048]	Loss: 1.7009
Training Epoch: 9 [18304/50048]	Loss: 1.6035
Training Epoch: 9 [18432/50048]	Loss: 1.6488
Training Epoch: 9 [18560/50048]	Loss: 1.6072
Training Epoch: 9 [18688/50048]	Loss: 1.6057
Training Epoch: 9 [18816/50048]	Loss: 1.8411
Training Epoch: 9 [18944/50048]	Loss: 1.5861
Training Epoch: 9 [19072/50048]	Loss: 1.5087
Training Epoch: 9 [19200/50048]	Loss: 1.5880
Training Epoch: 9 [19328/50048]	Loss: 1.4982
Training Epoch: 9 [19456/50048]	Loss: 1.6660
Training Epoch: 9 [19584/50048]	Loss: 1.4382
Training Epoch: 9 [19712/50048]	Loss: 1.6301
Training Epoch: 9 [19840/50048]	Loss: 1.8037
Training Epoch: 9 [19968/50048]	Loss: 1.6551
Training Epoch: 9 [20096/50048]	Loss: 1.6724
Training Epoch: 9 [20224/50048]	Loss: 1.6136
Training Epoch: 9 [20352/50048]	Loss: 1.5651
Training Epoch: 9 [20480/50048]	Loss: 1.5143
Training Epoch: 9 [20608/50048]	Loss: 1.7303
Training Epoch: 9 [20736/50048]	Loss: 1.6260
Training Epoch: 9 [20864/50048]	Loss: 1.7335
Training Epoch: 9 [20992/50048]	Loss: 1.6971
Training Epoch: 9 [21120/50048]	Loss: 1.8524
Training Epoch: 9 [21248/50048]	Loss: 1.5282
Training Epoch: 9 [21376/50048]	Loss: 1.5436
Training Epoch: 9 [21504/50048]	Loss: 1.4791
Training Epoch: 9 [21632/50048]	Loss: 1.6365
Training Epoch: 9 [21760/50048]	Loss: 1.4766
Training Epoch: 9 [21888/50048]	Loss: 1.7695
Training Epoch: 9 [22016/50048]	Loss: 1.6406
Training Epoch: 9 [22144/50048]	Loss: 1.5461
Training Epoch: 9 [22272/50048]	Loss: 2.0144
Training Epoch: 9 [22400/50048]	Loss: 1.7311
Training Epoch: 9 [22528/50048]	Loss: 1.6802
Training Epoch: 9 [22656/50048]	Loss: 1.5314
Training Epoch: 9 [22784/50048]	Loss: 1.5436
Training Epoch: 9 [22912/50048]	Loss: 1.5955
Training Epoch: 9 [23040/50048]	Loss: 1.8097
Training Epoch: 9 [23168/50048]	Loss: 1.5464
Training Epoch: 9 [23296/50048]	Loss: 1.4763
Training Epoch: 9 [23424/50048]	Loss: 1.7097
Training Epoch: 9 [23552/50048]	Loss: 1.5278
Training Epoch: 9 [23680/50048]	Loss: 1.2386
Training Epoch: 9 [23808/50048]	Loss: 1.8202
Training Epoch: 9 [23936/50048]	Loss: 1.5827
Training Epoch: 9 [24064/50048]	Loss: 1.5732
Training Epoch: 9 [24192/50048]	Loss: 1.8042
Training Epoch: 9 [24320/50048]	Loss: 1.8448
Training Epoch: 9 [24448/50048]	Loss: 1.7764
Training Epoch: 9 [24576/50048]	Loss: 1.6672
Training Epoch: 9 [24704/50048]	Loss: 1.5248
Training Epoch: 9 [24832/50048]	Loss: 1.6733
Training Epoch: 9 [24960/50048]	Loss: 1.5102
Training Epoch: 9 [25088/50048]	Loss: 1.5346
Training Epoch: 9 [25216/50048]	Loss: 1.3190
Training Epoch: 9 [25344/50048]	Loss: 1.4493
Training Epoch: 9 [25472/50048]	Loss: 1.7438
Training Epoch: 9 [25600/50048]	Loss: 1.7506
Training Epoch: 9 [25728/50048]	Loss: 1.6470
Training Epoch: 9 [25856/50048]	Loss: 1.5651
Training Epoch: 9 [25984/50048]	Loss: 1.4537
Training Epoch: 9 [26112/50048]	Loss: 1.6247
Training Epoch: 9 [26240/50048]	Loss: 1.6133
Training Epoch: 9 [26368/50048]	Loss: 1.5640
Training Epoch: 9 [26496/50048]	Loss: 1.6841
Training Epoch: 9 [26624/50048]	Loss: 1.5616
Training Epoch: 9 [26752/50048]	Loss: 1.7386
Training Epoch: 9 [26880/50048]	Loss: 1.4176
Training Epoch: 9 [27008/50048]	Loss: 1.4769
Training Epoch: 9 [27136/50048]	Loss: 1.5595
Training Epoch: 9 [27264/50048]	Loss: 1.6785
Training Epoch: 9 [27392/50048]	Loss: 1.6695
Training Epoch: 9 [27520/50048]	Loss: 1.5966
Training Epoch: 9 [27648/50048]	Loss: 1.9596
Training Epoch: 9 [27776/50048]	Loss: 1.5494
Training Epoch: 9 [27904/50048]	Loss: 1.5009
Training Epoch: 9 [28032/50048]	Loss: 1.4385
Training Epoch: 9 [28160/50048]	Loss: 1.4089
Training Epoch: 9 [28288/50048]	Loss: 1.6305
Training Epoch: 9 [28416/50048]	Loss: 1.7852
Training Epoch: 9 [28544/50048]	Loss: 1.6600
Training Epoch: 9 [28672/50048]	Loss: 1.5625
Training Epoch: 9 [28800/50048]	Loss: 1.5698
Training Epoch: 9 [28928/50048]	Loss: 1.8701
Training Epoch: 9 [29056/50048]	Loss: 1.5277
Training Epoch: 9 [29184/50048]	Loss: 1.7903
Training Epoch: 9 [29312/50048]	Loss: 1.5357
Training Epoch: 9 [29440/50048]	Loss: 1.6189
Training Epoch: 9 [29568/50048]	Loss: 1.6128
Training Epoch: 9 [29696/50048]	Loss: 1.4935
Training Epoch: 9 [29824/50048]	Loss: 1.6931
Training Epoch: 9 [29952/50048]	Loss: 1.6248
Training Epoch: 9 [30080/50048]	Loss: 1.5348
Training Epoch: 9 [30208/50048]	Loss: 1.5878
Training Epoch: 9 [30336/50048]	Loss: 1.5743
Training Epoch: 9 [30464/50048]	Loss: 1.6887
Training Epoch: 9 [30592/50048]	Loss: 1.5832
Training Epoch: 9 [30720/50048]	Loss: 1.6361
Training Epoch: 9 [30848/50048]	Loss: 1.4582
Training Epoch: 9 [30976/50048]	Loss: 1.6566
Training Epoch: 9 [31104/50048]	Loss: 1.8323
Training Epoch: 9 [31232/50048]	Loss: 1.6682
Training Epoch: 9 [31360/50048]	Loss: 1.5078
Training Epoch: 9 [31488/50048]	Loss: 1.6532
Training Epoch: 9 [31616/50048]	Loss: 1.3638
Training Epoch: 9 [31744/50048]	Loss: 1.8048
Training Epoch: 9 [31872/50048]	Loss: 1.5360
Training Epoch: 9 [32000/50048]	Loss: 1.5508
Training Epoch: 9 [32128/50048]	Loss: 1.6865
Training Epoch: 9 [32256/50048]	Loss: 1.3928
Training Epoch: 9 [32384/50048]	Loss: 1.4219
Training Epoch: 9 [32512/50048]	Loss: 1.6325
Training Epoch: 9 [32640/50048]	Loss: 1.5587
Training Epoch: 9 [32768/50048]	Loss: 1.3627
Training Epoch: 9 [32896/50048]	Loss: 1.6534
Training Epoch: 9 [33024/50048]	Loss: 1.6465
Training Epoch: 9 [33152/50048]	Loss: 1.5455
Training Epoch: 9 [33280/50048]	Loss: 1.5798
Training Epoch: 9 [33408/50048]	Loss: 1.6776
Training Epoch: 9 [33536/50048]	Loss: 1.7015
Training Epoch: 9 [33664/50048]	Loss: 1.5959
Training Epoch: 9 [33792/50048]	Loss: 1.6111
Training Epoch: 9 [33920/50048]	Loss: 1.6340
Training Epoch: 9 [34048/50048]	Loss: 1.7157
Training Epoch: 9 [34176/50048]	Loss: 1.8478
Training Epoch: 9 [34304/50048]	Loss: 1.5231
Training Epoch: 9 [34432/50048]	Loss: 1.4647
Training Epoch: 9 [34560/50048]	Loss: 1.5251
Training Epoch: 9 [34688/50048]	Loss: 1.4730
Training Epoch: 9 [34816/50048]	Loss: 1.5609
Training Epoch: 9 [34944/50048]	Loss: 1.5112
Training Epoch: 9 [35072/50048]	Loss: 1.5600
Training Epoch: 9 [35200/50048]	Loss: 1.7753
Training Epoch: 9 [35328/50048]	Loss: 2.0385
Training Epoch: 9 [35456/50048]	Loss: 1.7074
Training Epoch: 9 [35584/50048]	Loss: 1.5411
Training Epoch: 9 [35712/50048]	Loss: 1.5465
Training Epoch: 9 [35840/50048]	Loss: 1.6543
Training Epoch: 9 [35968/50048]	Loss: 1.7769
Training Epoch: 9 [36096/50048]	Loss: 1.5291
Training Epoch: 9 [36224/50048]	Loss: 1.5132
Training Epoch: 9 [36352/50048]	Loss: 1.8358
Training Epoch: 9 [36480/50048]	Loss: 1.6028
Training Epoch: 9 [36608/50048]	Loss: 1.6115
Training Epoch: 9 [36736/50048]	Loss: 1.6174
Training Epoch: 9 [36864/50048]	Loss: 1.5017
Training Epoch: 9 [36992/50048]	Loss: 1.7360
Training Epoch: 9 [37120/50048]	Loss: 1.5935
Training Epoch: 9 [37248/50048]	Loss: 1.7391
Training Epoch: 9 [37376/50048]	Loss: 1.6724
Training Epoch: 9 [37504/50048]	Loss: 1.6038
Training Epoch: 9 [37632/50048]	Loss: 1.5133
Training Epoch: 9 [37760/50048]	Loss: 1.7550
Training Epoch: 9 [37888/50048]	Loss: 1.2908
Training Epoch: 9 [38016/50048]	Loss: 1.5656
Training Epoch: 9 [38144/50048]	Loss: 1.6924
Training Epoch: 9 [38272/50048]	Loss: 1.5462
Training Epoch: 9 [38400/50048]	Loss: 1.5686
Training Epoch: 9 [38528/50048]	Loss: 1.5981
Training Epoch: 9 [38656/50048]	Loss: 1.6330
Training Epoch: 9 [38784/50048]	Loss: 1.7646
Training Epoch: 9 [38912/50048]	Loss: 1.4000
Training Epoch: 9 [39040/50048]	Loss: 1.6955
Training Epoch: 9 [39168/50048]	Loss: 1.6330
Training Epoch: 9 [39296/50048]	Loss: 1.6640
Training Epoch: 9 [39424/50048]	Loss: 1.7734
Training Epoch: 9 [39552/50048]	Loss: 1.7268
Training Epoch: 9 [39680/50048]	Loss: 1.8318
Training Epoch: 9 [39808/50048]	Loss: 1.3415
Training Epoch: 9 [39936/50048]	Loss: 1.6662
Training Epoch: 9 [40064/50048]	Loss: 1.6117
Training Epoch: 9 [40192/50048]	Loss: 1.5950
Training Epoch: 9 [40320/50048]	Loss: 1.7918
Training Epoch: 9 [40448/50048]	Loss: 1.7849
Training Epoch: 9 [40576/50048]	Loss: 1.5431
Training Epoch: 9 [40704/50048]	Loss: 1.6333
Training Epoch: 9 [40832/50048]	Loss: 1.3822
Training Epoch: 9 [40960/50048]	Loss: 1.4979
Training Epoch: 9 [41088/50048]	Loss: 1.7849
Training Epoch: 9 [41216/50048]	Loss: 1.5348
Training Epoch: 9 [41344/50048]	Loss: 1.6263
Training Epoch: 9 [41472/50048]	Loss: 1.5631
Training Epoch: 9 [41600/50048]	Loss: 1.7144
Training Epoch: 9 [41728/50048]	Loss: 1.7502
Training Epoch: 9 [41856/50048]	Loss: 1.6325
Training Epoch: 9 [41984/50048]	Loss: 1.6285
Training Epoch: 9 [42112/50048]	Loss: 1.6386
Training Epoch: 9 [42240/50048]	Loss: 2.0567
Training Epoch: 9 [42368/50048]	Loss: 1.6071
Training Epoch: 9 [42496/50048]	Loss: 1.5778
Training Epoch: 9 [42624/50048]	Loss: 1.6049
Training Epoch: 9 [42752/50048]	Loss: 1.5653
Training Epoch: 9 [42880/50048]	Loss: 1.9077
Training Epoch: 9 [43008/50048]	Loss: 1.5649
Training Epoch: 9 [43136/50048]	Loss: 1.8202
Training Epoch: 9 [43264/50048]	Loss: 1.4759
Training Epoch: 9 [43392/50048]	Loss: 1.8293
Training Epoch: 9 [43520/50048]	Loss: 1.6025
Training Epoch: 9 [43648/50048]	Loss: 1.6161
Training Epoch: 9 [43776/50048]	Loss: 1.6532
Training Epoch: 9 [43904/50048]	Loss: 1.6977
Training Epoch: 9 [44032/50048]	Loss: 1.3961
Training Epoch: 9 [44160/50048]	Loss: 1.6290
Training Epoch: 9 [44288/50048]	Loss: 1.2884
Training Epoch: 9 [44416/50048]	Loss: 1.5225
Training Epoch: 9 [44544/50048]	Loss: 1.7521
Training Epoch: 9 [44672/50048]	Loss: 1.7391
Training Epoch: 9 [44800/50048]	Loss: 1.6749
Training Epoch: 9 [44928/50048]	Loss: 1.5905
Training Epoch: 9 [45056/50048]	Loss: 1.5368
Training Epoch: 9 [45184/50048]	Loss: 1.7029
Training Epoch: 9 [45312/50048]	Loss: 1.8010
Training Epoch: 9 [45440/50048]	Loss: 1.8925
Training Epoch: 9 [45568/50048]	Loss: 1.7722
Training Epoch: 9 [45696/50048]	Loss: 1.7643
Training Epoch: 9 [45824/50048]	Loss: 1.5281
Training Epoch: 9 [45952/50048]	Loss: 1.6171
Training Epoch: 9 [46080/50048]	Loss: 1.6407
Training Epoch: 9 [46208/50048]	Loss: 1.5379
Training Epoch: 9 [46336/50048]	Loss: 1.5552
Training Epoch: 9 [46464/50048]	Loss: 1.4936
Training Epoch: 9 [46592/50048]	Loss: 1.5306
Training Epoch: 9 [46720/50048]	Loss: 1.4275
Training Epoch: 9 [46848/50048]	Loss: 1.7418
Training Epoch: 9 [46976/50048]	Loss: 1.6542
Training Epoch: 9 [47104/50048]	Loss: 1.5011
Training Epoch: 9 [47232/50048]	Loss: 1.6290
Training Epoch: 9 [47360/50048]	Loss: 1.3399
Training Epoch: 9 [47488/50048]	Loss: 1.4705
Training Epoch: 9 [47616/50048]	Loss: 1.6699
Training Epoch: 9 [47744/50048]	Loss: 1.6066
Training Epoch: 9 [47872/50048]	Loss: 1.8264
Training Epoch: 9 [48000/50048]	Loss: 1.4823
Training Epoch: 9 [48128/50048]	Loss: 1.3453
Training Epoch: 9 [48256/50048]	Loss: 1.5800
Training Epoch: 9 [48384/50048]	Loss: 1.8258
Training Epoch: 9 [48512/50048]	Loss: 1.7035
Training Epoch: 9 [48640/50048]	Loss: 1.8275
Training Epoch: 9 [48768/50048]	Loss: 1.5323
Training Epoch: 9 [48896/50048]	Loss: 1.3330
Training Epoch: 9 [49024/50048]	Loss: 1.5219
Training Epoch: 9 [49152/50048]	Loss: 1.5345
Training Epoch: 9 [49280/50048]	Loss: 1.6134
Training Epoch: 9 [49408/50048]	Loss: 1.6950
Training Epoch: 9 [49536/50048]	Loss: 1.2655
Training Epoch: 9 [49664/50048]	Loss: 1.6494
Training Epoch: 9 [49792/50048]	Loss: 1.8778
Training Epoch: 9 [49920/50048]	Loss: 1.5442
Training Epoch: 9 [50048/50048]	Loss: 1.7548
Validation Epoch: 9, Average loss: 0.0129, Accuracy: 0.5497
Training Epoch: 10 [128/50048]	Loss: 1.5556
Training Epoch: 10 [256/50048]	Loss: 1.6419
Training Epoch: 10 [384/50048]	Loss: 1.7904
Training Epoch: 10 [512/50048]	Loss: 1.6580
Training Epoch: 10 [640/50048]	Loss: 2.0421
Training Epoch: 10 [768/50048]	Loss: 1.5605
Training Epoch: 10 [896/50048]	Loss: 1.6590
Training Epoch: 10 [1024/50048]	Loss: 1.5460
Training Epoch: 10 [1152/50048]	Loss: 1.7488
Training Epoch: 10 [1280/50048]	Loss: 1.5892
Training Epoch: 10 [1408/50048]	Loss: 1.4372
Training Epoch: 10 [1536/50048]	Loss: 1.3915
Training Epoch: 10 [1664/50048]	Loss: 1.7084
Training Epoch: 10 [1792/50048]	Loss: 1.6350
Training Epoch: 10 [1920/50048]	Loss: 1.5458
Training Epoch: 10 [2048/50048]	Loss: 1.5021
Training Epoch: 10 [2176/50048]	Loss: 1.5154
Training Epoch: 10 [2304/50048]	Loss: 1.5865
Training Epoch: 10 [2432/50048]	Loss: 1.6980
Training Epoch: 10 [2560/50048]	Loss: 1.5799
Training Epoch: 10 [2688/50048]	Loss: 1.5417
Training Epoch: 10 [2816/50048]	Loss: 1.5435
Training Epoch: 10 [2944/50048]	Loss: 1.4480
Training Epoch: 10 [3072/50048]	Loss: 1.4763
Training Epoch: 10 [3200/50048]	Loss: 1.4445
Training Epoch: 10 [3328/50048]	Loss: 1.7491
Training Epoch: 10 [3456/50048]	Loss: 1.7118
Training Epoch: 10 [3584/50048]	Loss: 1.5991
Training Epoch: 10 [3712/50048]	Loss: 1.6048
Training Epoch: 10 [3840/50048]	Loss: 1.7759
Training Epoch: 10 [3968/50048]	Loss: 1.4091
Training Epoch: 10 [4096/50048]	Loss: 1.6108
Training Epoch: 10 [4224/50048]	Loss: 1.8296
Training Epoch: 10 [4352/50048]	Loss: 1.3572
Training Epoch: 10 [4480/50048]	Loss: 1.3834
Training Epoch: 10 [4608/50048]	Loss: 1.4910
Training Epoch: 10 [4736/50048]	Loss: 1.6225
Training Epoch: 10 [4864/50048]	Loss: 1.5823
Training Epoch: 10 [4992/50048]	Loss: 1.6373
Training Epoch: 10 [5120/50048]	Loss: 1.6739
Training Epoch: 10 [5248/50048]	Loss: 1.5347
Training Epoch: 10 [5376/50048]	Loss: 1.4973
Training Epoch: 10 [5504/50048]	Loss: 1.5404
Training Epoch: 10 [5632/50048]	Loss: 1.5600
Training Epoch: 10 [5760/50048]	Loss: 2.0226
Training Epoch: 10 [5888/50048]	Loss: 1.4372
Training Epoch: 10 [6016/50048]	Loss: 1.5416
Training Epoch: 10 [6144/50048]	Loss: 1.5995
Training Epoch: 10 [6272/50048]	Loss: 1.3714
Training Epoch: 10 [6400/50048]	Loss: 1.5206
Training Epoch: 10 [6528/50048]	Loss: 1.7662
Training Epoch: 10 [6656/50048]	Loss: 1.4348
Training Epoch: 10 [6784/50048]	Loss: 1.4851
Training Epoch: 10 [6912/50048]	Loss: 1.6867
Training Epoch: 10 [7040/50048]	Loss: 1.8135
Training Epoch: 10 [7168/50048]	Loss: 1.4228
Training Epoch: 10 [7296/50048]	Loss: 1.4696
Training Epoch: 10 [7424/50048]	Loss: 1.6764
Training Epoch: 10 [7552/50048]	Loss: 1.4870
Training Epoch: 10 [7680/50048]	Loss: 1.5658
Training Epoch: 10 [7808/50048]	Loss: 1.5386
Training Epoch: 10 [7936/50048]	Loss: 1.3943
Training Epoch: 10 [8064/50048]	Loss: 1.9815
Training Epoch: 10 [8192/50048]	Loss: 1.5740
Training Epoch: 10 [8320/50048]	Loss: 1.5649
Training Epoch: 10 [8448/50048]	Loss: 1.8558
Training Epoch: 10 [8576/50048]	Loss: 1.3724
Training Epoch: 10 [8704/50048]	Loss: 1.4759
Training Epoch: 10 [8832/50048]	Loss: 1.5445
Training Epoch: 10 [8960/50048]	Loss: 1.4774
Training Epoch: 10 [9088/50048]	Loss: 1.4316
Training Epoch: 10 [9216/50048]	Loss: 1.4002
Training Epoch: 10 [9344/50048]	Loss: 1.7575
Training Epoch: 10 [9472/50048]	Loss: 1.4511
Training Epoch: 10 [9600/50048]	Loss: 1.8403
Training Epoch: 10 [9728/50048]	Loss: 1.4080
Training Epoch: 10 [9856/50048]	Loss: 1.5830
Training Epoch: 10 [9984/50048]	Loss: 1.6564
Training Epoch: 10 [10112/50048]	Loss: 1.2863
Training Epoch: 10 [10240/50048]	Loss: 1.6770
Training Epoch: 10 [10368/50048]	Loss: 1.6686
Training Epoch: 10 [10496/50048]	Loss: 1.6189
Training Epoch: 10 [10624/50048]	Loss: 1.4887
Training Epoch: 10 [10752/50048]	Loss: 1.4779
Training Epoch: 10 [10880/50048]	Loss: 1.6470
Training Epoch: 10 [11008/50048]	Loss: 1.3081
Training Epoch: 10 [11136/50048]	Loss: 1.6067
Training Epoch: 10 [11264/50048]	Loss: 1.8168
Training Epoch: 10 [11392/50048]	Loss: 1.7456
Training Epoch: 10 [11520/50048]	Loss: 1.3799
Training Epoch: 10 [11648/50048]	Loss: 1.5943
Training Epoch: 10 [11776/50048]	Loss: 1.2434
Training Epoch: 10 [11904/50048]	Loss: 1.5880
Training Epoch: 10 [12032/50048]	Loss: 1.7824
Training Epoch: 10 [12160/50048]	Loss: 1.7342
Training Epoch: 10 [12288/50048]	Loss: 1.6205
Training Epoch: 10 [12416/50048]	Loss: 1.7056
Training Epoch: 10 [12544/50048]	Loss: 1.6573
Training Epoch: 10 [12672/50048]	Loss: 1.5419
Training Epoch: 10 [12800/50048]	Loss: 1.7064
Training Epoch: 10 [12928/50048]	Loss: 1.6129
Training Epoch: 10 [13056/50048]	Loss: 1.6127
Training Epoch: 10 [13184/50048]	Loss: 1.4490
Training Epoch: 10 [13312/50048]	Loss: 1.6417
Training Epoch: 10 [13440/50048]	Loss: 1.9476
Training Epoch: 10 [13568/50048]	Loss: 1.8073
Training Epoch: 10 [13696/50048]	Loss: 1.5577
Training Epoch: 10 [13824/50048]	Loss: 1.7132
Training Epoch: 10 [13952/50048]	Loss: 1.7667
Training Epoch: 10 [14080/50048]	Loss: 1.5844
Training Epoch: 10 [14208/50048]	Loss: 1.6519
Training Epoch: 10 [14336/50048]	Loss: 1.6453
Training Epoch: 10 [14464/50048]	Loss: 1.4547
Training Epoch: 10 [14592/50048]	Loss: 1.4575
Training Epoch: 10 [14720/50048]	Loss: 1.6034
Training Epoch: 10 [14848/50048]	Loss: 1.5731
Training Epoch: 10 [14976/50048]	Loss: 1.5258
Training Epoch: 10 [15104/50048]	Loss: 1.5522
Training Epoch: 10 [15232/50048]	Loss: 1.5062
Training Epoch: 10 [15360/50048]	Loss: 1.7213
Training Epoch: 10 [15488/50048]	Loss: 1.6161
Training Epoch: 10 [15616/50048]	Loss: 1.4430
Training Epoch: 10 [15744/50048]	Loss: 1.5078
Training Epoch: 10 [15872/50048]	Loss: 1.5225
Training Epoch: 10 [16000/50048]	Loss: 1.6276
Training Epoch: 10 [16128/50048]	Loss: 1.5520
Training Epoch: 10 [16256/50048]	Loss: 1.3878
Training Epoch: 10 [16384/50048]	Loss: 1.6087
Training Epoch: 10 [16512/50048]	Loss: 1.5426
Training Epoch: 10 [16640/50048]	Loss: 1.6151
Training Epoch: 10 [16768/50048]	Loss: 1.5807
Training Epoch: 10 [16896/50048]	Loss: 1.5293
Training Epoch: 10 [17024/50048]	Loss: 1.5815
Training Epoch: 10 [17152/50048]	Loss: 1.4687
Training Epoch: 10 [17280/50048]	Loss: 1.5403
Training Epoch: 10 [17408/50048]	Loss: 1.3701
Training Epoch: 10 [17536/50048]	Loss: 1.4479
Training Epoch: 10 [17664/50048]	Loss: 1.4076
Training Epoch: 10 [17792/50048]	Loss: 1.4300
Training Epoch: 10 [17920/50048]	Loss: 1.5779
Training Epoch: 10 [18048/50048]	Loss: 1.6358
Training Epoch: 10 [18176/50048]	Loss: 1.4224
Training Epoch: 10 [18304/50048]	Loss: 1.8356
Training Epoch: 10 [18432/50048]	Loss: 1.7046
Training Epoch: 10 [18560/50048]	Loss: 1.3664
Training Epoch: 10 [18688/50048]	Loss: 1.8453
Training Epoch: 10 [18816/50048]	Loss: 1.4160
Training Epoch: 10 [18944/50048]	Loss: 1.6622
Training Epoch: 10 [19072/50048]	Loss: 1.7470
Training Epoch: 10 [19200/50048]	Loss: 1.7715
Training Epoch: 10 [19328/50048]	Loss: 1.4963
Training Epoch: 10 [19456/50048]	Loss: 1.5391
Training Epoch: 10 [19584/50048]	Loss: 1.4598
Training Epoch: 10 [19712/50048]	Loss: 1.5152
Training Epoch: 10 [19840/50048]	Loss: 1.7041
Training Epoch: 10 [19968/50048]	Loss: 1.5952
Training Epoch: 10 [20096/50048]	Loss: 1.7258
Training Epoch: 10 [20224/50048]	Loss: 1.4905
Training Epoch: 10 [20352/50048]	Loss: 1.5431
Training Epoch: 10 [20480/50048]	Loss: 1.8694
Training Epoch: 10 [20608/50048]	Loss: 1.5167
Training Epoch: 10 [20736/50048]	Loss: 1.3415
Training Epoch: 10 [20864/50048]	Loss: 1.4797
Training Epoch: 10 [20992/50048]	Loss: 1.6752
Training Epoch: 10 [21120/50048]	Loss: 1.5603
Training Epoch: 10 [21248/50048]	Loss: 1.7073
Training Epoch: 10 [21376/50048]	Loss: 1.6553
Training Epoch: 10 [21504/50048]	Loss: 1.8924
Training Epoch: 10 [21632/50048]	Loss: 1.4845
Training Epoch: 10 [21760/50048]	Loss: 1.5279
Training Epoch: 10 [21888/50048]	Loss: 1.8933
Training Epoch: 10 [22016/50048]	Loss: 1.6530
Training Epoch: 10 [22144/50048]	Loss: 1.6888
Training Epoch: 10 [22272/50048]	Loss: 1.6533
Training Epoch: 10 [22400/50048]	Loss: 1.7944
Training Epoch: 10 [22528/50048]	Loss: 1.3834
Training Epoch: 10 [22656/50048]	Loss: 1.5320
Training Epoch: 10 [22784/50048]	Loss: 1.3692
Training Epoch: 10 [22912/50048]	Loss: 1.4757
Training Epoch: 10 [23040/50048]	Loss: 1.2891
Training Epoch: 10 [23168/50048]	Loss: 1.5493
Training Epoch: 10 [23296/50048]	Loss: 1.2422
Training Epoch: 10 [23424/50048]	Loss: 1.6842
Training Epoch: 10 [23552/50048]	Loss: 1.5237
Training Epoch: 10 [23680/50048]	Loss: 1.5735
Training Epoch: 10 [23808/50048]	Loss: 1.5453
Training Epoch: 10 [23936/50048]	Loss: 1.5527
Training Epoch: 10 [24064/50048]	Loss: 1.5819
Training Epoch: 10 [24192/50048]	Loss: 1.7675
Training Epoch: 10 [24320/50048]	Loss: 1.2672
Training Epoch: 10 [24448/50048]	Loss: 1.3260
Training Epoch: 10 [24576/50048]	Loss: 1.8664
Training Epoch: 10 [24704/50048]	Loss: 1.8228
Training Epoch: 10 [24832/50048]	Loss: 1.3745
Training Epoch: 10 [24960/50048]	Loss: 1.5613
Training Epoch: 10 [25088/50048]	Loss: 1.6042
Training Epoch: 10 [25216/50048]	Loss: 1.5465
Training Epoch: 10 [25344/50048]	Loss: 1.3216
Training Epoch: 10 [25472/50048]	Loss: 1.6602
Training Epoch: 10 [25600/50048]	Loss: 1.4861
Training Epoch: 10 [25728/50048]	Loss: 1.5254
Training Epoch: 10 [25856/50048]	Loss: 1.6037
Training Epoch: 10 [25984/50048]	Loss: 1.6280
Training Epoch: 10 [26112/50048]	Loss: 1.4473
Training Epoch: 10 [26240/50048]	Loss: 1.5258
Training Epoch: 10 [26368/50048]	Loss: 1.5854
Training Epoch: 10 [26496/50048]	Loss: 1.4959
Training Epoch: 10 [26624/50048]	Loss: 1.3872
Training Epoch: 10 [26752/50048]	Loss: 1.4464
Training Epoch: 10 [26880/50048]	Loss: 1.6215
Training Epoch: 10 [27008/50048]	Loss: 1.3978
Training Epoch: 10 [27136/50048]	Loss: 1.5157
Training Epoch: 10 [27264/50048]	Loss: 1.5176
Training Epoch: 10 [27392/50048]	Loss: 1.5188
Training Epoch: 10 [27520/50048]	Loss: 1.4585
Training Epoch: 10 [27648/50048]	Loss: 1.6601
Training Epoch: 10 [27776/50048]	Loss: 1.6565
Training Epoch: 10 [27904/50048]	Loss: 1.7162
Training Epoch: 10 [28032/50048]	Loss: 1.6358
Training Epoch: 10 [28160/50048]	Loss: 1.5539
Training Epoch: 10 [28288/50048]	Loss: 1.8463
Training Epoch: 10 [28416/50048]	Loss: 1.5691
Training Epoch: 10 [28544/50048]	Loss: 1.8187
Training Epoch: 10 [28672/50048]	Loss: 1.5078
Training Epoch: 10 [28800/50048]	Loss: 1.7161
Training Epoch: 10 [28928/50048]	Loss: 1.5640
Training Epoch: 10 [29056/50048]	Loss: 1.6466
Training Epoch: 10 [29184/50048]	Loss: 1.4174
Training Epoch: 10 [29312/50048]	Loss: 1.7074
Training Epoch: 10 [29440/50048]	Loss: 1.5537
Training Epoch: 10 [29568/50048]	Loss: 1.4883
Training Epoch: 10 [29696/50048]	Loss: 1.5702
Training Epoch: 10 [29824/50048]	Loss: 1.7848
Training Epoch: 10 [29952/50048]	Loss: 1.7181
Training Epoch: 10 [30080/50048]	Loss: 1.2917
Training Epoch: 10 [30208/50048]	Loss: 1.7036
Training Epoch: 10 [30336/50048]	Loss: 1.5202
Training Epoch: 10 [30464/50048]	Loss: 1.7055
Training Epoch: 10 [30592/50048]	Loss: 1.7618
Training Epoch: 10 [30720/50048]	Loss: 1.3564
Training Epoch: 10 [30848/50048]	Loss: 1.6402
Training Epoch: 10 [30976/50048]	Loss: 1.6367
Training Epoch: 10 [31104/50048]	Loss: 1.2875
Training Epoch: 10 [31232/50048]	Loss: 1.5168
Training Epoch: 10 [31360/50048]	Loss: 1.5337
Training Epoch: 10 [31488/50048]	Loss: 1.7788
Training Epoch: 10 [31616/50048]	Loss: 1.5194
Training Epoch: 10 [31744/50048]	Loss: 1.3452
Training Epoch: 10 [31872/50048]	Loss: 1.5726
Training Epoch: 10 [32000/50048]	Loss: 1.7596
Training Epoch: 10 [32128/50048]	Loss: 1.4335
Training Epoch: 10 [32256/50048]	Loss: 1.7334
Training Epoch: 10 [32384/50048]	Loss: 1.6413
Training Epoch: 10 [32512/50048]	Loss: 1.8138
Training Epoch: 10 [32640/50048]	Loss: 1.7504
Training Epoch: 10 [32768/50048]	Loss: 1.6051
Training Epoch: 10 [32896/50048]	Loss: 1.6608
Training Epoch: 10 [33024/50048]	Loss: 1.5056
Training Epoch: 10 [33152/50048]	Loss: 1.6043
Training Epoch: 10 [33280/50048]	Loss: 1.3672
Training Epoch: 10 [33408/50048]	Loss: 1.7044
Training Epoch: 10 [33536/50048]	Loss: 1.4291
Training Epoch: 10 [33664/50048]	Loss: 1.9091
Training Epoch: 10 [33792/50048]	Loss: 1.5892
Training Epoch: 10 [33920/50048]	Loss: 1.5378
Training Epoch: 10 [34048/50048]	Loss: 1.5387
Training Epoch: 10 [34176/50048]	Loss: 1.7565
Training Epoch: 10 [34304/50048]	Loss: 1.6573
Training Epoch: 10 [34432/50048]	Loss: 1.3502
Training Epoch: 10 [34560/50048]	Loss: 1.5974
Training Epoch: 10 [34688/50048]	Loss: 1.6620
Training Epoch: 10 [34816/50048]	Loss: 1.4747
Training Epoch: 10 [34944/50048]	Loss: 1.7264
Training Epoch: 10 [35072/50048]	Loss: 1.4358
Training Epoch: 10 [35200/50048]	Loss: 1.5880
Training Epoch: 10 [35328/50048]	Loss: 1.8497
Training Epoch: 10 [35456/50048]	Loss: 1.5651
Training Epoch: 10 [35584/50048]	Loss: 1.5882
Training Epoch: 10 [35712/50048]	Loss: 1.6088
Training Epoch: 10 [35840/50048]	Loss: 1.6292
Training Epoch: 10 [35968/50048]	Loss: 1.6177
Training Epoch: 10 [36096/50048]	Loss: 1.4110
Training Epoch: 10 [36224/50048]	Loss: 1.4632
Training Epoch: 10 [36352/50048]	Loss: 1.3558
Training Epoch: 10 [36480/50048]	Loss: 1.6356
Training Epoch: 10 [36608/50048]	Loss: 1.7609
Training Epoch: 10 [36736/50048]	Loss: 1.5152
Training Epoch: 10 [36864/50048]	Loss: 1.5337
Training Epoch: 10 [36992/50048]	Loss: 1.3881
Training Epoch: 10 [37120/50048]	Loss: 1.5691
Training Epoch: 10 [37248/50048]	Loss: 1.6657
Training Epoch: 10 [37376/50048]	Loss: 1.4085
Training Epoch: 10 [37504/50048]	Loss: 1.5801
Training Epoch: 10 [37632/50048]	Loss: 1.2725
Training Epoch: 10 [37760/50048]	Loss: 1.5210
Training Epoch: 10 [37888/50048]	Loss: 1.7029
Training Epoch: 10 [38016/50048]	Loss: 1.5928
Training Epoch: 10 [38144/50048]	Loss: 1.7370
Training Epoch: 10 [38272/50048]	Loss: 1.5295
Training Epoch: 10 [38400/50048]	Loss: 1.6761
Training Epoch: 10 [38528/50048]	Loss: 1.5377
Training Epoch: 10 [38656/50048]	Loss: 1.6527
Training Epoch: 10 [38784/50048]	Loss: 1.4467
Training Epoch: 10 [38912/50048]	Loss: 1.4024
Training Epoch: 10 [39040/50048]	Loss: 1.4053
Training Epoch: 10 [39168/50048]	Loss: 1.9066
Training Epoch: 10 [39296/50048]	Loss: 1.5855
Training Epoch: 10 [39424/50048]	Loss: 1.4544
Training Epoch: 10 [39552/50048]	Loss: 1.5968
Training Epoch: 10 [39680/50048]	Loss: 1.5294
Training Epoch: 10 [39808/50048]	Loss: 1.6922
Training Epoch: 10 [39936/50048]	Loss: 1.6787
Training Epoch: 10 [40064/50048]	Loss: 1.6281
Training Epoch: 10 [40192/50048]	Loss: 1.8097
Training Epoch: 10 [40320/50048]	Loss: 1.3376
Training Epoch: 10 [40448/50048]	Loss: 1.4867
Training Epoch: 10 [40576/50048]	Loss: 1.4802
Training Epoch: 10 [40704/50048]	Loss: 1.4340
Training Epoch: 10 [40832/50048]	Loss: 1.2458
Training Epoch: 10 [40960/50048]	Loss: 1.5672
Training Epoch: 10 [41088/50048]	Loss: 1.4873
Training Epoch: 10 [41216/50048]	Loss: 1.3380
Training Epoch: 10 [41344/50048]	Loss: 1.6835
Training Epoch: 10 [41472/50048]	Loss: 1.5605
Training Epoch: 10 [41600/50048]	Loss: 1.5172
Training Epoch: 10 [41728/50048]	Loss: 1.4935
Training Epoch: 10 [41856/50048]	Loss: 1.5501
Training Epoch: 10 [41984/50048]	Loss: 1.6558
Training Epoch: 10 [42112/50048]	Loss: 1.5916
Training Epoch: 10 [42240/50048]	Loss: 1.9711
Training Epoch: 10 [42368/50048]	Loss: 1.5995
Training Epoch: 10 [42496/50048]	Loss: 1.4786
Training Epoch: 10 [42624/50048]	Loss: 1.5980
Training Epoch: 10 [42752/50048]	Loss: 1.6926
Training Epoch: 10 [42880/50048]	Loss: 1.7967
Training Epoch: 10 [43008/50048]	Loss: 1.6881
Training Epoch: 10 [43136/50048]	Loss: 1.7133
Training Epoch: 10 [43264/50048]	Loss: 1.8266
Training Epoch: 10 [43392/50048]	Loss: 1.7423
Training Epoch: 10 [43520/50048]	Loss: 1.4513
Training Epoch: 10 [43648/50048]	Loss: 1.5703
Training Epoch: 10 [43776/50048]	Loss: 1.7103
Training Epoch: 10 [43904/50048]	Loss: 1.4683
Training Epoch: 10 [44032/50048]	Loss: 1.7958
Training Epoch: 10 [44160/50048]	Loss: 1.7089
Training Epoch: 10 [44288/50048]	Loss: 1.6259
Training Epoch: 10 [44416/50048]	Loss: 1.4538
Training Epoch: 10 [44544/50048]	Loss: 1.5358
Training Epoch: 10 [44672/50048]	Loss: 1.6997
Training Epoch: 10 [44800/50048]	Loss: 1.7538
Training Epoch: 10 [44928/50048]	Loss: 1.6405
Training Epoch: 10 [45056/50048]	Loss: 1.4901
Training Epoch: 10 [45184/50048]	Loss: 1.6780
Training Epoch: 10 [45312/50048]	Loss: 1.4638
Training Epoch: 10 [45440/50048]	Loss: 1.7984
Training Epoch: 10 [45568/50048]	Loss: 1.5605
Training Epoch: 10 [45696/50048]	Loss: 1.5076
Training Epoch: 10 [45824/50048]	Loss: 1.5326
Training Epoch: 10 [45952/50048]	Loss: 1.6453
Training Epoch: 10 [46080/50048]	Loss: 1.6275
Training Epoch: 10 [46208/50048]	Loss: 1.4912
Training Epoch: 10 [46336/50048]	Loss: 1.5993
Training Epoch: 10 [46464/50048]	Loss: 1.7722
Training Epoch: 10 [46592/50048]	Loss: 1.4100
Training Epoch: 10 [46720/50048]	Loss: 1.4422
Training Epoch: 10 [46848/50048]	Loss: 1.6464
Training Epoch: 10 [46976/50048]	Loss: 1.4732
Training Epoch: 10 [47104/50048]	Loss: 1.5681
Training Epoch: 10 [47232/50048]	Loss: 1.6217
Training Epoch: 10 [47360/50048]	Loss: 1.5591
Training Epoch: 10 [47488/50048]	Loss: 1.7931
Training Epoch: 10 [47616/50048]	Loss: 1.8722
Training Epoch: 10 [47744/50048]	Loss: 1.6200
Training Epoch: 10 [47872/50048]	Loss: 1.7347
Training Epoch: 10 [48000/50048]	Loss: 1.6569
Training Epoch: 10 [48128/50048]	Loss: 1.6613
Training Epoch: 10 [48256/50048]	Loss: 1.4764
Training Epoch: 10 [48384/50048]	Loss: 1.6643
Training Epoch: 10 [48512/50048]	Loss: 1.7054
Training Epoch: 10 [48640/50048]	Loss: 1.6441
Training Epoch: 10 [48768/50048]	Loss: 1.4306
Training Epoch: 10 [48896/50048]	Loss: 1.5656
Training Epoch: 10 [49024/50048]	Loss: 1.4456
Training Epoch: 10 [49152/50048]	Loss: 1.6038
Training Epoch: 10 [49280/50048]	Loss: 1.5278
Training Epoch: 10 [49408/50048]	Loss: 1.3800
Training Epoch: 10 [49536/50048]	Loss: 1.4821
Training Epoch: 10 [49664/50048]	Loss: 1.6406
Training Epoch: 10 [49792/50048]	Loss: 1.5410
Training Epoch: 10 [49920/50048]	Loss: 1.5502
Training Epoch: 10 [50048/50048]	Loss: 1.4776
Validation Epoch: 10, Average loss: 0.0127, Accuracy: 0.5552
Training Epoch: 11 [128/50048]	Loss: 1.7282
Training Epoch: 11 [256/50048]	Loss: 1.3211
Training Epoch: 11 [384/50048]	Loss: 1.3338
Training Epoch: 11 [512/50048]	Loss: 1.4996
Training Epoch: 11 [640/50048]	Loss: 1.6244
Training Epoch: 11 [768/50048]	Loss: 1.4672
Training Epoch: 11 [896/50048]	Loss: 1.5711
Training Epoch: 11 [1024/50048]	Loss: 1.3127
Training Epoch: 11 [1152/50048]	Loss: 1.5963
Training Epoch: 11 [1280/50048]	Loss: 1.7139
Training Epoch: 11 [1408/50048]	Loss: 1.2864
Training Epoch: 11 [1536/50048]	Loss: 1.6616
Training Epoch: 11 [1664/50048]	Loss: 1.4821
Training Epoch: 11 [1792/50048]	Loss: 1.4913
Training Epoch: 11 [1920/50048]	Loss: 1.6508
Training Epoch: 11 [2048/50048]	Loss: 1.7016
Training Epoch: 11 [2176/50048]	Loss: 1.7047
Training Epoch: 11 [2304/50048]	Loss: 1.4462
Training Epoch: 11 [2432/50048]	Loss: 1.7314
Training Epoch: 11 [2560/50048]	Loss: 1.7452
Training Epoch: 11 [2688/50048]	Loss: 1.4922
Training Epoch: 11 [2816/50048]	Loss: 1.6788
Training Epoch: 11 [2944/50048]	Loss: 1.4490
Training Epoch: 11 [3072/50048]	Loss: 1.5080
Training Epoch: 11 [3200/50048]	Loss: 1.4547
Training Epoch: 11 [3328/50048]	Loss: 1.4443
Training Epoch: 11 [3456/50048]	Loss: 1.2363
Training Epoch: 11 [3584/50048]	Loss: 1.7339
Training Epoch: 11 [3712/50048]	Loss: 1.6143
Training Epoch: 11 [3840/50048]	Loss: 1.3551
Training Epoch: 11 [3968/50048]	Loss: 1.6687
Training Epoch: 11 [4096/50048]	Loss: 1.5498
Training Epoch: 11 [4224/50048]	Loss: 1.7517
Training Epoch: 11 [4352/50048]	Loss: 1.4867
Training Epoch: 11 [4480/50048]	Loss: 1.8401
Training Epoch: 11 [4608/50048]	Loss: 1.5173
Training Epoch: 11 [4736/50048]	Loss: 1.5276
Training Epoch: 11 [4864/50048]	Loss: 1.3294
Training Epoch: 11 [4992/50048]	Loss: 1.3737
Training Epoch: 11 [5120/50048]	Loss: 1.5140
Training Epoch: 11 [5248/50048]	Loss: 1.5403
Training Epoch: 11 [5376/50048]	Loss: 1.4626
Training Epoch: 11 [5504/50048]	Loss: 1.3970
Training Epoch: 11 [5632/50048]	Loss: 1.2724
Training Epoch: 11 [5760/50048]	Loss: 1.4425
Training Epoch: 11 [5888/50048]	Loss: 1.8166
Training Epoch: 11 [6016/50048]	Loss: 1.3537
Training Epoch: 11 [6144/50048]	Loss: 1.6249
Training Epoch: 11 [6272/50048]	Loss: 1.8506
Training Epoch: 11 [6400/50048]	Loss: 1.4703
Training Epoch: 11 [6528/50048]	Loss: 1.5916
Training Epoch: 11 [6656/50048]	Loss: 1.4788
Training Epoch: 11 [6784/50048]	Loss: 1.2982
Training Epoch: 11 [6912/50048]	Loss: 1.5127
Training Epoch: 11 [7040/50048]	Loss: 1.4653
Training Epoch: 11 [7168/50048]	Loss: 1.7054
Training Epoch: 11 [7296/50048]	Loss: 1.3684
Training Epoch: 11 [7424/50048]	Loss: 1.8067
Training Epoch: 11 [7552/50048]	Loss: 1.4557
Training Epoch: 11 [7680/50048]	Loss: 1.8735
Training Epoch: 11 [7808/50048]	Loss: 1.1839
Training Epoch: 11 [7936/50048]	Loss: 1.4664
Training Epoch: 11 [8064/50048]	Loss: 1.6683
Training Epoch: 11 [8192/50048]	Loss: 1.6851
Training Epoch: 11 [8320/50048]	Loss: 1.4819
Training Epoch: 11 [8448/50048]	Loss: 1.7220
Training Epoch: 11 [8576/50048]	Loss: 1.6661
Training Epoch: 11 [8704/50048]	Loss: 1.2503
Training Epoch: 11 [8832/50048]	Loss: 1.4483
Training Epoch: 11 [8960/50048]	Loss: 1.6723
Training Epoch: 11 [9088/50048]	Loss: 1.4540
Training Epoch: 11 [9216/50048]	Loss: 1.3009
Training Epoch: 11 [9344/50048]	Loss: 1.4478
Training Epoch: 11 [9472/50048]	Loss: 1.5363
Training Epoch: 11 [9600/50048]	Loss: 1.9513
Training Epoch: 11 [9728/50048]	Loss: 1.5425
Training Epoch: 11 [9856/50048]	Loss: 1.6433
Training Epoch: 11 [9984/50048]	Loss: 1.3361
Training Epoch: 11 [10112/50048]	Loss: 1.4284
Training Epoch: 11 [10240/50048]	Loss: 1.8112
Training Epoch: 11 [10368/50048]	Loss: 1.4845
Training Epoch: 11 [10496/50048]	Loss: 1.7759
Training Epoch: 11 [10624/50048]	Loss: 1.4881
Training Epoch: 11 [10752/50048]	Loss: 1.6281
Training Epoch: 11 [10880/50048]	Loss: 1.7106
Training Epoch: 11 [11008/50048]	Loss: 1.6950
Training Epoch: 11 [11136/50048]	Loss: 1.5348
Training Epoch: 11 [11264/50048]	Loss: 1.6211
Training Epoch: 11 [11392/50048]	Loss: 1.7166
Training Epoch: 11 [11520/50048]	Loss: 1.3101
Training Epoch: 11 [11648/50048]	Loss: 1.5432
Training Epoch: 11 [11776/50048]	Loss: 1.3648
Training Epoch: 11 [11904/50048]	Loss: 1.5493
Training Epoch: 11 [12032/50048]	Loss: 1.5581
Training Epoch: 11 [12160/50048]	Loss: 1.6091
Training Epoch: 11 [12288/50048]	Loss: 1.6709
Training Epoch: 11 [12416/50048]	Loss: 1.3141
Training Epoch: 11 [12544/50048]	Loss: 1.4474
Training Epoch: 11 [12672/50048]	Loss: 1.4338
Training Epoch: 11 [12800/50048]	Loss: 1.5870
Training Epoch: 11 [12928/50048]	Loss: 1.5990
Training Epoch: 11 [13056/50048]	Loss: 1.5013
Training Epoch: 11 [13184/50048]	Loss: 1.4876
Training Epoch: 11 [13312/50048]	Loss: 1.3926
Training Epoch: 11 [13440/50048]	Loss: 1.7028
Training Epoch: 11 [13568/50048]	Loss: 1.3845
Training Epoch: 11 [13696/50048]	Loss: 1.6090
Training Epoch: 11 [13824/50048]	Loss: 1.5989
Training Epoch: 11 [13952/50048]	Loss: 1.5702
Training Epoch: 11 [14080/50048]	Loss: 1.4969
Training Epoch: 11 [14208/50048]	Loss: 1.4687
Training Epoch: 11 [14336/50048]	Loss: 1.3866
Training Epoch: 11 [14464/50048]	Loss: 1.6920
Training Epoch: 11 [14592/50048]	Loss: 1.4212
Training Epoch: 11 [14720/50048]	Loss: 1.3472
Training Epoch: 11 [14848/50048]	Loss: 1.6399
Training Epoch: 11 [14976/50048]	Loss: 1.7093
Training Epoch: 11 [15104/50048]	Loss: 1.4734
Training Epoch: 11 [15232/50048]	Loss: 1.6655
Training Epoch: 11 [15360/50048]	Loss: 1.5504
Training Epoch: 11 [15488/50048]	Loss: 1.3565
Training Epoch: 11 [15616/50048]	Loss: 1.3538
Training Epoch: 11 [15744/50048]	Loss: 1.6370
Training Epoch: 11 [15872/50048]	Loss: 1.7720
Training Epoch: 11 [16000/50048]	Loss: 1.7541
Training Epoch: 11 [16128/50048]	Loss: 1.5005
Training Epoch: 11 [16256/50048]	Loss: 1.5449
Training Epoch: 11 [16384/50048]	Loss: 1.5437
Training Epoch: 11 [16512/50048]	Loss: 1.6442
Training Epoch: 11 [16640/50048]	Loss: 1.3404
Training Epoch: 11 [16768/50048]	Loss: 1.6679
Training Epoch: 11 [16896/50048]	Loss: 1.7505
Training Epoch: 11 [17024/50048]	Loss: 1.2754
Training Epoch: 11 [17152/50048]	Loss: 1.6487
Training Epoch: 11 [17280/50048]	Loss: 1.7864
Training Epoch: 11 [17408/50048]	Loss: 1.4734
Training Epoch: 11 [17536/50048]	Loss: 1.7349
Training Epoch: 11 [17664/50048]	Loss: 1.7306
Training Epoch: 11 [17792/50048]	Loss: 1.6313
Training Epoch: 11 [17920/50048]	Loss: 1.5177
Training Epoch: 11 [18048/50048]	Loss: 1.4539
Training Epoch: 11 [18176/50048]	Loss: 1.5506
Training Epoch: 11 [18304/50048]	Loss: 1.6814
Training Epoch: 11 [18432/50048]	Loss: 1.4069
Training Epoch: 11 [18560/50048]	Loss: 1.4409
Training Epoch: 11 [18688/50048]	Loss: 1.5532
Training Epoch: 11 [18816/50048]	Loss: 1.4298
Training Epoch: 11 [18944/50048]	Loss: 1.7014
Training Epoch: 11 [19072/50048]	Loss: 1.4214
Training Epoch: 11 [19200/50048]	Loss: 1.7299
Training Epoch: 11 [19328/50048]	Loss: 1.5138
Training Epoch: 11 [19456/50048]	Loss: 1.5857
Training Epoch: 11 [19584/50048]	Loss: 1.6821
Training Epoch: 11 [19712/50048]	Loss: 1.4280
Training Epoch: 11 [19840/50048]	Loss: 1.4397
Training Epoch: 11 [19968/50048]	Loss: 1.3406
Training Epoch: 11 [20096/50048]	Loss: 1.5905
Training Epoch: 11 [20224/50048]	Loss: 1.4663
Training Epoch: 11 [20352/50048]	Loss: 1.2729
Training Epoch: 11 [20480/50048]	Loss: 1.6694
Training Epoch: 11 [20608/50048]	Loss: 1.6683
Training Epoch: 11 [20736/50048]	Loss: 1.6598
Training Epoch: 11 [20864/50048]	Loss: 1.5263
Training Epoch: 11 [20992/50048]	Loss: 1.4152
Training Epoch: 11 [21120/50048]	Loss: 1.6659
Training Epoch: 11 [21248/50048]	Loss: 1.6896
Training Epoch: 11 [21376/50048]	Loss: 1.5632
Training Epoch: 11 [21504/50048]	Loss: 1.5173
Training Epoch: 11 [21632/50048]	Loss: 1.6571
Training Epoch: 11 [21760/50048]	Loss: 1.6037
Training Epoch: 11 [21888/50048]	Loss: 1.5869
Training Epoch: 11 [22016/50048]	Loss: 1.5256
Training Epoch: 11 [22144/50048]	Loss: 1.5845
Training Epoch: 11 [22272/50048]	Loss: 1.4305
Training Epoch: 11 [22400/50048]	Loss: 1.3476
Training Epoch: 11 [22528/50048]	Loss: 1.4406
Training Epoch: 11 [22656/50048]	Loss: 1.3841
Training Epoch: 11 [22784/50048]	Loss: 1.3781
Training Epoch: 11 [22912/50048]	Loss: 1.6937
Training Epoch: 11 [23040/50048]	Loss: 1.4738
Training Epoch: 11 [23168/50048]	Loss: 1.3964
Training Epoch: 11 [23296/50048]	Loss: 1.8598
Training Epoch: 11 [23424/50048]	Loss: 1.3687
Training Epoch: 11 [23552/50048]	Loss: 1.5117
Training Epoch: 11 [23680/50048]	Loss: 1.4637
Training Epoch: 11 [23808/50048]	Loss: 1.6683
Training Epoch: 11 [23936/50048]	Loss: 1.5146
Training Epoch: 11 [24064/50048]	Loss: 1.6757
Training Epoch: 11 [24192/50048]	Loss: 1.3969
Training Epoch: 11 [24320/50048]	Loss: 1.4723
Training Epoch: 11 [24448/50048]	Loss: 1.6178
Training Epoch: 11 [24576/50048]	Loss: 1.4666
Training Epoch: 11 [24704/50048]	Loss: 1.6043
Training Epoch: 11 [24832/50048]	Loss: 1.4013
Training Epoch: 11 [24960/50048]	Loss: 1.4874
Training Epoch: 11 [25088/50048]	Loss: 1.6158
Training Epoch: 11 [25216/50048]	Loss: 1.4299
Training Epoch: 11 [25344/50048]	Loss: 1.7446
Training Epoch: 11 [25472/50048]	Loss: 1.6800
Training Epoch: 11 [25600/50048]	Loss: 1.7542
Training Epoch: 11 [25728/50048]	Loss: 1.8138
Training Epoch: 11 [25856/50048]	Loss: 1.3202
Training Epoch: 11 [25984/50048]	Loss: 1.6129
Training Epoch: 11 [26112/50048]	Loss: 1.3923
Training Epoch: 11 [26240/50048]	Loss: 1.6088
Training Epoch: 11 [26368/50048]	Loss: 1.7258
Training Epoch: 11 [26496/50048]	Loss: 1.3829
Training Epoch: 11 [26624/50048]	Loss: 1.5408
Training Epoch: 11 [26752/50048]	Loss: 1.4258
Training Epoch: 11 [26880/50048]	Loss: 1.4288
Training Epoch: 11 [27008/50048]	Loss: 1.5236
Training Epoch: 11 [27136/50048]	Loss: 1.4915
Training Epoch: 11 [27264/50048]	Loss: 1.5857
Training Epoch: 11 [27392/50048]	Loss: 1.8130
Training Epoch: 11 [27520/50048]	Loss: 1.5741
Training Epoch: 11 [27648/50048]	Loss: 1.4595
Training Epoch: 11 [27776/50048]	Loss: 1.5196
Training Epoch: 11 [27904/50048]	Loss: 1.3031
Training Epoch: 11 [28032/50048]	Loss: 1.7485
Training Epoch: 11 [28160/50048]	Loss: 1.6285
Training Epoch: 11 [28288/50048]	Loss: 1.6270
Training Epoch: 11 [28416/50048]	Loss: 1.5672
Training Epoch: 11 [28544/50048]	Loss: 1.6084
Training Epoch: 11 [28672/50048]	Loss: 1.6616
Training Epoch: 11 [28800/50048]	Loss: 1.2955
Training Epoch: 11 [28928/50048]	Loss: 1.3421
Training Epoch: 11 [29056/50048]	Loss: 1.6368
Training Epoch: 11 [29184/50048]	Loss: 1.4171
Training Epoch: 11 [29312/50048]	Loss: 1.5504
Training Epoch: 11 [29440/50048]	Loss: 1.5990
Training Epoch: 11 [29568/50048]	Loss: 1.6120
Training Epoch: 11 [29696/50048]	Loss: 1.3954
Training Epoch: 11 [29824/50048]	Loss: 1.4555
Training Epoch: 11 [29952/50048]	Loss: 1.6651
Training Epoch: 11 [30080/50048]	Loss: 1.5353
Training Epoch: 11 [30208/50048]	Loss: 1.4484
Training Epoch: 11 [30336/50048]	Loss: 1.6613
Training Epoch: 11 [30464/50048]	Loss: 1.6524
Training Epoch: 11 [30592/50048]	Loss: 1.3940
Training Epoch: 11 [30720/50048]	Loss: 1.6956
Training Epoch: 11 [30848/50048]	Loss: 1.5159
Training Epoch: 11 [30976/50048]	Loss: 1.7270
Training Epoch: 11 [31104/50048]	Loss: 1.5276
Training Epoch: 11 [31232/50048]	Loss: 1.4434
Training Epoch: 11 [31360/50048]	Loss: 1.5005
Training Epoch: 11 [31488/50048]	Loss: 1.4528
Training Epoch: 11 [31616/50048]	Loss: 1.6074
Training Epoch: 11 [31744/50048]	Loss: 1.6024
Training Epoch: 11 [31872/50048]	Loss: 1.5113
Training Epoch: 11 [32000/50048]	Loss: 1.5242
Training Epoch: 11 [32128/50048]	Loss: 1.6521
Training Epoch: 11 [32256/50048]	Loss: 1.5497
Training Epoch: 11 [32384/50048]	Loss: 1.5433
Training Epoch: 11 [32512/50048]	Loss: 1.3683
Training Epoch: 11 [32640/50048]	Loss: 1.3901
Training Epoch: 11 [32768/50048]	Loss: 1.4849
Training Epoch: 11 [32896/50048]	Loss: 1.5487
Training Epoch: 11 [33024/50048]	Loss: 1.8056
Training Epoch: 11 [33152/50048]	Loss: 1.5509
Training Epoch: 11 [33280/50048]	Loss: 1.5304
Training Epoch: 11 [33408/50048]	Loss: 1.5877
Training Epoch: 11 [33536/50048]	Loss: 1.5582
Training Epoch: 11 [33664/50048]	Loss: 1.6504
Training Epoch: 11 [33792/50048]	Loss: 1.9411
Training Epoch: 11 [33920/50048]	Loss: 1.4405
Training Epoch: 11 [34048/50048]	Loss: 1.1907
Training Epoch: 11 [34176/50048]	Loss: 1.7090
Training Epoch: 11 [34304/50048]	Loss: 1.4463
Training Epoch: 11 [34432/50048]	Loss: 1.7293
Training Epoch: 11 [34560/50048]	Loss: 1.7261
Training Epoch: 11 [34688/50048]	Loss: 1.5526
Training Epoch: 11 [34816/50048]	Loss: 1.3473
Training Epoch: 11 [34944/50048]	Loss: 1.5390
Training Epoch: 11 [35072/50048]	Loss: 1.7662
Training Epoch: 11 [35200/50048]	Loss: 1.6399
Training Epoch: 11 [35328/50048]	Loss: 1.6848
Training Epoch: 11 [35456/50048]	Loss: 1.6153
Training Epoch: 11 [35584/50048]	Loss: 1.6618
Training Epoch: 11 [35712/50048]	Loss: 1.4382
Training Epoch: 11 [35840/50048]	Loss: 1.7017
Training Epoch: 11 [35968/50048]	Loss: 1.5932
Training Epoch: 11 [36096/50048]	Loss: 1.5989
Training Epoch: 11 [36224/50048]	Loss: 1.5776
Training Epoch: 11 [36352/50048]	Loss: 1.5466
Training Epoch: 11 [36480/50048]	Loss: 1.7874
Training Epoch: 11 [36608/50048]	Loss: 1.4308
Training Epoch: 11 [36736/50048]	Loss: 1.3716
Training Epoch: 11 [36864/50048]	Loss: 1.1166
Training Epoch: 11 [36992/50048]	Loss: 1.6667
Training Epoch: 11 [37120/50048]	Loss: 1.3969
Training Epoch: 11 [37248/50048]	Loss: 1.5685
Training Epoch: 11 [37376/50048]	Loss: 1.2284
Training Epoch: 11 [37504/50048]	Loss: 1.4099
Training Epoch: 11 [37632/50048]	Loss: 1.7130
Training Epoch: 11 [37760/50048]	Loss: 1.4762
Training Epoch: 11 [37888/50048]	Loss: 1.5282
Training Epoch: 11 [38016/50048]	Loss: 1.5009
Training Epoch: 11 [38144/50048]	Loss: 1.3641
Training Epoch: 11 [38272/50048]	Loss: 1.8246
Training Epoch: 11 [38400/50048]	Loss: 1.6518
Training Epoch: 11 [38528/50048]	Loss: 1.6852
Training Epoch: 11 [38656/50048]	Loss: 1.5256
Training Epoch: 11 [38784/50048]	Loss: 1.5215
Training Epoch: 11 [38912/50048]	Loss: 1.5471
Training Epoch: 11 [39040/50048]	Loss: 1.3603
Training Epoch: 11 [39168/50048]	Loss: 1.5897
Training Epoch: 11 [39296/50048]	Loss: 1.3763
Training Epoch: 11 [39424/50048]	Loss: 1.6709
Training Epoch: 11 [39552/50048]	Loss: 1.5399
Training Epoch: 11 [39680/50048]	Loss: 1.6094
Training Epoch: 11 [39808/50048]	Loss: 1.5910
Training Epoch: 11 [39936/50048]	Loss: 1.5707
Training Epoch: 11 [40064/50048]	Loss: 1.7604
Training Epoch: 11 [40192/50048]	Loss: 1.6325
Training Epoch: 11 [40320/50048]	Loss: 1.5329
Training Epoch: 11 [40448/50048]	Loss: 1.3955
Training Epoch: 11 [40576/50048]	Loss: 1.5734
Training Epoch: 11 [40704/50048]	Loss: 1.7753
Training Epoch: 11 [40832/50048]	Loss: 1.6118
Training Epoch: 11 [40960/50048]	Loss: 1.5664
Training Epoch: 11 [41088/50048]	Loss: 1.4814
Training Epoch: 11 [41216/50048]	Loss: 1.3526
Training Epoch: 11 [41344/50048]	Loss: 1.8148
Training Epoch: 11 [41472/50048]	Loss: 1.2526
Training Epoch: 11 [41600/50048]	Loss: 1.6131
Training Epoch: 11 [41728/50048]	Loss: 1.4726
Training Epoch: 11 [41856/50048]	Loss: 1.4122
Training Epoch: 11 [41984/50048]	Loss: 1.5097
Training Epoch: 11 [42112/50048]	Loss: 1.5370
Training Epoch: 11 [42240/50048]	Loss: 1.5868
Training Epoch: 11 [42368/50048]	Loss: 1.5119
Training Epoch: 11 [42496/50048]	Loss: 1.6477
Training Epoch: 11 [42624/50048]	Loss: 1.4570
Training Epoch: 11 [42752/50048]	Loss: 1.5781
Training Epoch: 11 [42880/50048]	Loss: 1.5553
Training Epoch: 11 [43008/50048]	Loss: 1.7038
Training Epoch: 11 [43136/50048]	Loss: 1.3017
Training Epoch: 11 [43264/50048]	Loss: 1.6218
Training Epoch: 11 [43392/50048]	Loss: 1.7179
Training Epoch: 11 [43520/50048]	Loss: 1.5322
Training Epoch: 11 [43648/50048]	Loss: 1.3080
Training Epoch: 11 [43776/50048]	Loss: 1.5844
Training Epoch: 11 [43904/50048]	Loss: 1.7285
Training Epoch: 11 [44032/50048]	Loss: 1.6055
Training Epoch: 11 [44160/50048]	Loss: 1.4079
Training Epoch: 11 [44288/50048]	Loss: 1.4575
Training Epoch: 11 [44416/50048]	Loss: 1.1943
Training Epoch: 11 [44544/50048]	Loss: 1.7006
Training Epoch: 11 [44672/50048]	Loss: 1.6314
Training Epoch: 11 [44800/50048]	Loss: 1.4840
Training Epoch: 11 [44928/50048]	Loss: 1.5465
Training Epoch: 11 [45056/50048]	Loss: 1.3621
Training Epoch: 11 [45184/50048]	Loss: 1.7181
Training Epoch: 11 [45312/50048]	Loss: 1.6735
Training Epoch: 11 [45440/50048]	Loss: 1.5212
Training Epoch: 11 [45568/50048]	Loss: 1.5827
Training Epoch: 11 [45696/50048]	Loss: 1.4016
Training Epoch: 11 [45824/50048]	Loss: 1.6272
Training Epoch: 11 [45952/50048]	Loss: 1.6707
Training Epoch: 11 [46080/50048]	Loss: 1.3891
Training Epoch: 11 [46208/50048]	Loss: 1.7148
Training Epoch: 11 [46336/50048]	Loss: 1.4908
Training Epoch: 11 [46464/50048]	Loss: 1.5879
Training Epoch: 11 [46592/50048]	Loss: 1.6968
Training Epoch: 11 [46720/50048]	Loss: 1.5400
Training Epoch: 11 [46848/50048]	Loss: 1.7083
Training Epoch: 11 [46976/50048]	Loss: 1.4745
Training Epoch: 11 [47104/50048]	Loss: 1.5332
Training Epoch: 11 [47232/50048]	Loss: 1.4659
Training Epoch: 11 [47360/50048]	Loss: 1.3224
Training Epoch: 11 [47488/50048]	Loss: 1.5134
Training Epoch: 11 [47616/50048]	Loss: 1.5901
Training Epoch: 11 [47744/50048]	Loss: 1.7119
Training Epoch: 11 [47872/50048]	Loss: 1.6860
Training Epoch: 11 [48000/50048]	Loss: 1.5174
Training Epoch: 11 [48128/50048]	Loss: 1.6430
Training Epoch: 11 [48256/50048]	Loss: 1.6325
Training Epoch: 11 [48384/50048]	Loss: 1.4354
Training Epoch: 11 [48512/50048]	Loss: 1.6053
Training Epoch: 11 [48640/50048]	Loss: 1.4466
Training Epoch: 11 [48768/50048]	Loss: 1.5182
Training Epoch: 11 [48896/50048]	Loss: 1.5303
Training Epoch: 11 [49024/50048]	Loss: 1.4487
Training Epoch: 11 [49152/50048]	Loss: 1.7513
Training Epoch: 11 [49280/50048]	Loss: 1.5104
Training Epoch: 11 [49408/50048]	Loss: 1.6760
Training Epoch: 11 [49536/50048]	Loss: 1.6310
Training Epoch: 11 [49664/50048]	Loss: 1.5560
Training Epoch: 11 [49792/50048]	Loss: 1.5405
Training Epoch: 11 [49920/50048]	Loss: 1.5750
Training Epoch: 11 [50048/50048]	Loss: 1.4946
Validation Epoch: 11, Average loss: 0.0126, Accuracy: 0.5543
Training Epoch: 12 [128/50048]	Loss: 1.4433
Training Epoch: 12 [256/50048]	Loss: 1.4231
Training Epoch: 12 [384/50048]	Loss: 1.3763
Training Epoch: 12 [512/50048]	Loss: 1.5347
Training Epoch: 12 [640/50048]	Loss: 1.4792
Training Epoch: 12 [768/50048]	Loss: 1.5528
Training Epoch: 12 [896/50048]	Loss: 1.4629
Training Epoch: 12 [1024/50048]	Loss: 1.7798
Training Epoch: 12 [1152/50048]	Loss: 1.4284
Training Epoch: 12 [1280/50048]	Loss: 1.6890
Training Epoch: 12 [1408/50048]	Loss: 1.4639
Training Epoch: 12 [1536/50048]	Loss: 1.6591
Training Epoch: 12 [1664/50048]	Loss: 1.4030
Training Epoch: 12 [1792/50048]	Loss: 1.3825
Training Epoch: 12 [1920/50048]	Loss: 1.4978
Training Epoch: 12 [2048/50048]	Loss: 1.6487
Training Epoch: 12 [2176/50048]	Loss: 1.5520
Training Epoch: 12 [2304/50048]	Loss: 1.5532
Training Epoch: 12 [2432/50048]	Loss: 1.3868
Training Epoch: 12 [2560/50048]	Loss: 1.4074
Training Epoch: 12 [2688/50048]	Loss: 1.4562
Training Epoch: 12 [2816/50048]	Loss: 1.4755
Training Epoch: 12 [2944/50048]	Loss: 1.4752
Training Epoch: 12 [3072/50048]	Loss: 1.4451
Training Epoch: 12 [3200/50048]	Loss: 1.6272
Training Epoch: 12 [3328/50048]	Loss: 1.4795
Training Epoch: 12 [3456/50048]	Loss: 1.3675
Training Epoch: 12 [3584/50048]	Loss: 1.7181
Training Epoch: 12 [3712/50048]	Loss: 1.5204
Training Epoch: 12 [3840/50048]	Loss: 1.5828
Training Epoch: 12 [3968/50048]	Loss: 1.6180
Training Epoch: 12 [4096/50048]	Loss: 1.6617
Training Epoch: 12 [4224/50048]	Loss: 1.3485
Training Epoch: 12 [4352/50048]	Loss: 1.6971
Training Epoch: 12 [4480/50048]	Loss: 1.3420
Training Epoch: 12 [4608/50048]	Loss: 1.6883
Training Epoch: 12 [4736/50048]	Loss: 1.5285
Training Epoch: 12 [4864/50048]	Loss: 1.3968
Training Epoch: 12 [4992/50048]	Loss: 1.4962
Training Epoch: 12 [5120/50048]	Loss: 1.4588
Training Epoch: 12 [5248/50048]	Loss: 1.5323
Training Epoch: 12 [5376/50048]	Loss: 1.7741
Training Epoch: 12 [5504/50048]	Loss: 1.3914
Training Epoch: 12 [5632/50048]	Loss: 1.5359
Training Epoch: 12 [5760/50048]	Loss: 1.6243
Training Epoch: 12 [5888/50048]	Loss: 1.6221
Training Epoch: 12 [6016/50048]	Loss: 1.2750
Training Epoch: 12 [6144/50048]	Loss: 1.3451
Training Epoch: 12 [6272/50048]	Loss: 1.8752
Training Epoch: 12 [6400/50048]	Loss: 1.5870
Training Epoch: 12 [6528/50048]	Loss: 1.5999
Training Epoch: 12 [6656/50048]	Loss: 1.4604
Training Epoch: 12 [6784/50048]	Loss: 1.5587
Training Epoch: 12 [6912/50048]	Loss: 1.6465
Training Epoch: 12 [7040/50048]	Loss: 1.6776
Training Epoch: 12 [7168/50048]	Loss: 1.6101
Training Epoch: 12 [7296/50048]	Loss: 1.4918
Training Epoch: 12 [7424/50048]	Loss: 1.5985
Training Epoch: 12 [7552/50048]	Loss: 1.6446
Training Epoch: 12 [7680/50048]	Loss: 1.4623
Training Epoch: 12 [7808/50048]	Loss: 1.4859
Training Epoch: 12 [7936/50048]	Loss: 1.7759
Training Epoch: 12 [8064/50048]	Loss: 1.6272
Training Epoch: 12 [8192/50048]	Loss: 1.5937
Training Epoch: 12 [8320/50048]	Loss: 1.6330
Training Epoch: 12 [8448/50048]	Loss: 1.5319
Training Epoch: 12 [8576/50048]	Loss: 1.4142
Training Epoch: 12 [8704/50048]	Loss: 1.2198
Training Epoch: 12 [8832/50048]	Loss: 1.4313
Training Epoch: 12 [8960/50048]	Loss: 1.4452
Training Epoch: 12 [9088/50048]	Loss: 1.7088
Training Epoch: 12 [9216/50048]	Loss: 1.3781
Training Epoch: 12 [9344/50048]	Loss: 1.4553
Training Epoch: 12 [9472/50048]	Loss: 1.4751
Training Epoch: 12 [9600/50048]	Loss: 1.4161
Training Epoch: 12 [9728/50048]	Loss: 1.5120
Training Epoch: 12 [9856/50048]	Loss: 1.4888
Training Epoch: 12 [9984/50048]	Loss: 1.4252
Training Epoch: 12 [10112/50048]	Loss: 1.2634
Training Epoch: 12 [10240/50048]	Loss: 1.4469
Training Epoch: 12 [10368/50048]	Loss: 1.6215
Training Epoch: 12 [10496/50048]	Loss: 1.4005
Training Epoch: 12 [10624/50048]	Loss: 1.0449
Training Epoch: 12 [10752/50048]	Loss: 1.3079
Training Epoch: 12 [10880/50048]	Loss: 1.7343
Training Epoch: 12 [11008/50048]	Loss: 1.5853
Training Epoch: 12 [11136/50048]	Loss: 1.2977
Training Epoch: 12 [11264/50048]	Loss: 1.2992
Training Epoch: 12 [11392/50048]	Loss: 1.4066
Training Epoch: 12 [11520/50048]	Loss: 1.5062
Training Epoch: 12 [11648/50048]	Loss: 1.4702
Training Epoch: 12 [11776/50048]	Loss: 1.3704
Training Epoch: 12 [11904/50048]	Loss: 1.5569
Training Epoch: 12 [12032/50048]	Loss: 1.4696
Training Epoch: 12 [12160/50048]	Loss: 1.4637
Training Epoch: 12 [12288/50048]	Loss: 1.3777
Training Epoch: 12 [12416/50048]	Loss: 1.3084
Training Epoch: 12 [12544/50048]	Loss: 1.4099
Training Epoch: 12 [12672/50048]	Loss: 1.6725
Training Epoch: 12 [12800/50048]	Loss: 1.3821
Training Epoch: 12 [12928/50048]	Loss: 1.4452
Training Epoch: 12 [13056/50048]	Loss: 1.3756
Training Epoch: 12 [13184/50048]	Loss: 1.6529
Training Epoch: 12 [13312/50048]	Loss: 1.3607
Training Epoch: 12 [13440/50048]	Loss: 1.4991
Training Epoch: 12 [13568/50048]	Loss: 1.4912
Training Epoch: 12 [13696/50048]	Loss: 1.7622
Training Epoch: 12 [13824/50048]	Loss: 1.4827
Training Epoch: 12 [13952/50048]	Loss: 1.6925
Training Epoch: 12 [14080/50048]	Loss: 1.6111
Training Epoch: 12 [14208/50048]	Loss: 1.3992
Training Epoch: 12 [14336/50048]	Loss: 1.4612
Training Epoch: 12 [14464/50048]	Loss: 1.4487
Training Epoch: 12 [14592/50048]	Loss: 1.3493
Training Epoch: 12 [14720/50048]	Loss: 1.4807
Training Epoch: 12 [14848/50048]	Loss: 1.6314
Training Epoch: 12 [14976/50048]	Loss: 1.6552
Training Epoch: 12 [15104/50048]	Loss: 1.4585
Training Epoch: 12 [15232/50048]	Loss: 1.4357
Training Epoch: 12 [15360/50048]	Loss: 1.3822
Training Epoch: 12 [15488/50048]	Loss: 1.6508
Training Epoch: 12 [15616/50048]	Loss: 1.5917
Training Epoch: 12 [15744/50048]	Loss: 1.5947
Training Epoch: 12 [15872/50048]	Loss: 1.6772
Training Epoch: 12 [16000/50048]	Loss: 1.4785
Training Epoch: 12 [16128/50048]	Loss: 1.7247
Training Epoch: 12 [16256/50048]	Loss: 1.5284
Training Epoch: 12 [16384/50048]	Loss: 1.3051
Training Epoch: 12 [16512/50048]	Loss: 1.5520
Training Epoch: 12 [16640/50048]	Loss: 1.5373
Training Epoch: 12 [16768/50048]	Loss: 1.4566
Training Epoch: 12 [16896/50048]	Loss: 1.1887
Training Epoch: 12 [17024/50048]	Loss: 1.5105
Training Epoch: 12 [17152/50048]	Loss: 1.3891
Training Epoch: 12 [17280/50048]	Loss: 1.3676
Training Epoch: 12 [17408/50048]	Loss: 1.6049
Training Epoch: 12 [17536/50048]	Loss: 1.5611
Training Epoch: 12 [17664/50048]	Loss: 1.2753
Training Epoch: 12 [17792/50048]	Loss: 1.3802
Training Epoch: 12 [17920/50048]	Loss: 1.5173
Training Epoch: 12 [18048/50048]	Loss: 1.5784
Training Epoch: 12 [18176/50048]	Loss: 1.3654
Training Epoch: 12 [18304/50048]	Loss: 1.5610
Training Epoch: 12 [18432/50048]	Loss: 1.2740
Training Epoch: 12 [18560/50048]	Loss: 1.5530
Training Epoch: 12 [18688/50048]	Loss: 1.6567
Training Epoch: 12 [18816/50048]	Loss: 1.4323
Training Epoch: 12 [18944/50048]	Loss: 1.4555
Training Epoch: 12 [19072/50048]	Loss: 1.5534
Training Epoch: 12 [19200/50048]	Loss: 1.6599
Training Epoch: 12 [19328/50048]	Loss: 1.4353
Training Epoch: 12 [19456/50048]	Loss: 1.3876
Training Epoch: 12 [19584/50048]	Loss: 1.5013
Training Epoch: 12 [19712/50048]	Loss: 1.6917
Training Epoch: 12 [19840/50048]	Loss: 1.6619
Training Epoch: 12 [19968/50048]	Loss: 1.4742
Training Epoch: 12 [20096/50048]	Loss: 1.4092
Training Epoch: 12 [20224/50048]	Loss: 1.2065
Training Epoch: 12 [20352/50048]	Loss: 1.3809
Training Epoch: 12 [20480/50048]	Loss: 1.4263
Training Epoch: 12 [20608/50048]	Loss: 1.2629
Training Epoch: 12 [20736/50048]	Loss: 1.4384
Training Epoch: 12 [20864/50048]	Loss: 1.3836
Training Epoch: 12 [20992/50048]	Loss: 1.6331
Training Epoch: 12 [21120/50048]	Loss: 1.6415
Training Epoch: 12 [21248/50048]	Loss: 1.2560
Training Epoch: 12 [21376/50048]	Loss: 1.5043
Training Epoch: 12 [21504/50048]	Loss: 1.3595
Training Epoch: 12 [21632/50048]	Loss: 1.6637
Training Epoch: 12 [21760/50048]	Loss: 1.4510
Training Epoch: 12 [21888/50048]	Loss: 1.6008
Training Epoch: 12 [22016/50048]	Loss: 1.2790
Training Epoch: 12 [22144/50048]	Loss: 1.5235
Training Epoch: 12 [22272/50048]	Loss: 1.5357
Training Epoch: 12 [22400/50048]	Loss: 1.4256
Training Epoch: 12 [22528/50048]	Loss: 1.6131
Training Epoch: 12 [22656/50048]	Loss: 1.4342
Training Epoch: 12 [22784/50048]	Loss: 1.3546
Training Epoch: 12 [22912/50048]	Loss: 1.7077
Training Epoch: 12 [23040/50048]	Loss: 1.4899
Training Epoch: 12 [23168/50048]	Loss: 1.5668
Training Epoch: 12 [23296/50048]	Loss: 1.5938
Training Epoch: 12 [23424/50048]	Loss: 1.6634
Training Epoch: 12 [23552/50048]	Loss: 1.3912
Training Epoch: 12 [23680/50048]	Loss: 1.1097
Training Epoch: 12 [23808/50048]	Loss: 1.4582
Training Epoch: 12 [23936/50048]	Loss: 1.3980
Training Epoch: 12 [24064/50048]	Loss: 1.6293
Training Epoch: 12 [24192/50048]	Loss: 1.5506
Training Epoch: 12 [24320/50048]	Loss: 1.5093
Training Epoch: 12 [24448/50048]	Loss: 1.6061
Training Epoch: 12 [24576/50048]	Loss: 1.4322
Training Epoch: 12 [24704/50048]	Loss: 1.3932
Training Epoch: 12 [24832/50048]	Loss: 1.6721
Training Epoch: 12 [24960/50048]	Loss: 1.3181
Training Epoch: 12 [25088/50048]	Loss: 1.5028
Training Epoch: 12 [25216/50048]	Loss: 1.3484
Training Epoch: 12 [25344/50048]	Loss: 1.5408
Training Epoch: 12 [25472/50048]	Loss: 1.5917
Training Epoch: 12 [25600/50048]	Loss: 1.4542
Training Epoch: 12 [25728/50048]	Loss: 1.3553
Training Epoch: 12 [25856/50048]	Loss: 1.6237
Training Epoch: 12 [25984/50048]	Loss: 1.2796
Training Epoch: 12 [26112/50048]	Loss: 1.5481
Training Epoch: 12 [26240/50048]	Loss: 1.5141
Training Epoch: 12 [26368/50048]	Loss: 1.3515
Training Epoch: 12 [26496/50048]	Loss: 1.4565
Training Epoch: 12 [26624/50048]	Loss: 1.4594
Training Epoch: 12 [26752/50048]	Loss: 1.4517
Training Epoch: 12 [26880/50048]	Loss: 1.5871
Training Epoch: 12 [27008/50048]	Loss: 1.5605
Training Epoch: 12 [27136/50048]	Loss: 1.5853
Training Epoch: 12 [27264/50048]	Loss: 1.7800
Training Epoch: 12 [27392/50048]	Loss: 1.7764
Training Epoch: 12 [27520/50048]	Loss: 1.5835
Training Epoch: 12 [27648/50048]	Loss: 1.3663
Training Epoch: 12 [27776/50048]	Loss: 1.2947
Training Epoch: 12 [27904/50048]	Loss: 1.5108
Training Epoch: 12 [28032/50048]	Loss: 1.4235
Training Epoch: 12 [28160/50048]	Loss: 1.6553
Training Epoch: 12 [28288/50048]	Loss: 1.7067
Training Epoch: 12 [28416/50048]	Loss: 1.6257
Training Epoch: 12 [28544/50048]	Loss: 1.4810
Training Epoch: 12 [28672/50048]	Loss: 1.6213
Training Epoch: 12 [28800/50048]	Loss: 1.3517
Training Epoch: 12 [28928/50048]	Loss: 1.5029
Training Epoch: 12 [29056/50048]	Loss: 1.7514
Training Epoch: 12 [29184/50048]	Loss: 1.4830
Training Epoch: 12 [29312/50048]	Loss: 1.4083
Training Epoch: 12 [29440/50048]	Loss: 1.3985
Training Epoch: 12 [29568/50048]	Loss: 1.5012
Training Epoch: 12 [29696/50048]	Loss: 1.4727
Training Epoch: 12 [29824/50048]	Loss: 1.6469
Training Epoch: 12 [29952/50048]	Loss: 1.7550
Training Epoch: 12 [30080/50048]	Loss: 1.7262
Training Epoch: 12 [30208/50048]	Loss: 1.2000
Training Epoch: 12 [30336/50048]	Loss: 1.4367
Training Epoch: 12 [30464/50048]	Loss: 1.6203
Training Epoch: 12 [30592/50048]	Loss: 1.4197
Training Epoch: 12 [30720/50048]	Loss: 1.4994
Training Epoch: 12 [30848/50048]	Loss: 1.7263
Training Epoch: 12 [30976/50048]	Loss: 1.1736
Training Epoch: 12 [31104/50048]	Loss: 1.4641
Training Epoch: 12 [31232/50048]	Loss: 1.5395
Training Epoch: 12 [31360/50048]	Loss: 1.3873
Training Epoch: 12 [31488/50048]	Loss: 1.3812
Training Epoch: 12 [31616/50048]	Loss: 1.5667
Training Epoch: 12 [31744/50048]	Loss: 1.5059
Training Epoch: 12 [31872/50048]	Loss: 1.3858
Training Epoch: 12 [32000/50048]	Loss: 1.5299
Training Epoch: 12 [32128/50048]	Loss: 1.7073
Training Epoch: 12 [32256/50048]	Loss: 1.3429
Training Epoch: 12 [32384/50048]	Loss: 1.8377
Training Epoch: 12 [32512/50048]	Loss: 1.4594
Training Epoch: 12 [32640/50048]	Loss: 1.3806
Training Epoch: 12 [32768/50048]	Loss: 1.8166
Training Epoch: 12 [32896/50048]	Loss: 1.5771
Training Epoch: 12 [33024/50048]	Loss: 1.5104
Training Epoch: 12 [33152/50048]	Loss: 1.6303
Training Epoch: 12 [33280/50048]	Loss: 1.5245
Training Epoch: 12 [33408/50048]	Loss: 1.8001
Training Epoch: 12 [33536/50048]	Loss: 1.5148
Training Epoch: 12 [33664/50048]	Loss: 1.4289
Training Epoch: 12 [33792/50048]	Loss: 1.3870
Training Epoch: 12 [33920/50048]	Loss: 1.8173
Training Epoch: 12 [34048/50048]	Loss: 1.4676
Training Epoch: 12 [34176/50048]	Loss: 1.2745
Training Epoch: 12 [34304/50048]	Loss: 1.3669
Training Epoch: 12 [34432/50048]	Loss: 1.7698
Training Epoch: 12 [34560/50048]	Loss: 1.5002
Training Epoch: 12 [34688/50048]	Loss: 1.3274
Training Epoch: 12 [34816/50048]	Loss: 1.6463
Training Epoch: 12 [34944/50048]	Loss: 1.4314
Training Epoch: 12 [35072/50048]	Loss: 1.4339
Training Epoch: 12 [35200/50048]	Loss: 1.5522
Training Epoch: 12 [35328/50048]	Loss: 1.3254
Training Epoch: 12 [35456/50048]	Loss: 1.5716
Training Epoch: 12 [35584/50048]	Loss: 1.4421
Training Epoch: 12 [35712/50048]	Loss: 1.3890
Training Epoch: 12 [35840/50048]	Loss: 1.2514
Training Epoch: 12 [35968/50048]	Loss: 1.4396
Training Epoch: 12 [36096/50048]	Loss: 1.4223
Training Epoch: 12 [36224/50048]	Loss: 1.5880
Training Epoch: 12 [36352/50048]	Loss: 1.6011
Training Epoch: 12 [36480/50048]	Loss: 1.5017
Training Epoch: 12 [36608/50048]	Loss: 1.5215
Training Epoch: 12 [36736/50048]	Loss: 1.5174
Training Epoch: 12 [36864/50048]	Loss: 1.4330
Training Epoch: 12 [36992/50048]	Loss: 1.5062
Training Epoch: 12 [37120/50048]	Loss: 1.5891
Training Epoch: 12 [37248/50048]	Loss: 1.3609
Training Epoch: 12 [37376/50048]	Loss: 1.6654
Training Epoch: 12 [37504/50048]	Loss: 1.3698
Training Epoch: 12 [37632/50048]	Loss: 1.5854
Training Epoch: 12 [37760/50048]	Loss: 1.6731
Training Epoch: 12 [37888/50048]	Loss: 1.7790
Training Epoch: 12 [38016/50048]	Loss: 1.5387
Training Epoch: 12 [38144/50048]	Loss: 1.4912
Training Epoch: 12 [38272/50048]	Loss: 1.5336
Training Epoch: 12 [38400/50048]	Loss: 1.4974
Training Epoch: 12 [38528/50048]	Loss: 1.5383
Training Epoch: 12 [38656/50048]	Loss: 1.2741
Training Epoch: 12 [38784/50048]	Loss: 1.4212
Training Epoch: 12 [38912/50048]	Loss: 1.5338
Training Epoch: 12 [39040/50048]	Loss: 1.4448
Training Epoch: 12 [39168/50048]	Loss: 1.6749
Training Epoch: 12 [39296/50048]	Loss: 1.5803
Training Epoch: 12 [39424/50048]	Loss: 1.3687
Training Epoch: 12 [39552/50048]	Loss: 1.5618
Training Epoch: 12 [39680/50048]	Loss: 1.7995
Training Epoch: 12 [39808/50048]	Loss: 1.5277
Training Epoch: 12 [39936/50048]	Loss: 1.4883
Training Epoch: 12 [40064/50048]	Loss: 1.6285
Training Epoch: 12 [40192/50048]	Loss: 1.5608
Training Epoch: 12 [40320/50048]	Loss: 1.8822
Training Epoch: 12 [40448/50048]	Loss: 1.5425
Training Epoch: 12 [40576/50048]	Loss: 1.5818
Training Epoch: 12 [40704/50048]	Loss: 1.7188
Training Epoch: 12 [40832/50048]	Loss: 1.5333
Training Epoch: 12 [40960/50048]	Loss: 1.4755
Training Epoch: 12 [41088/50048]	Loss: 1.6317
Training Epoch: 12 [41216/50048]	Loss: 1.6772
Training Epoch: 12 [41344/50048]	Loss: 1.7983
Training Epoch: 12 [41472/50048]	Loss: 1.6375
Training Epoch: 12 [41600/50048]	Loss: 1.7183
Training Epoch: 12 [41728/50048]	Loss: 1.4466
Training Epoch: 12 [41856/50048]	Loss: 1.3219
Training Epoch: 12 [41984/50048]	Loss: 1.8639
Training Epoch: 12 [42112/50048]	Loss: 1.3333
Training Epoch: 12 [42240/50048]	Loss: 1.5258
Training Epoch: 12 [42368/50048]	Loss: 1.2856
Training Epoch: 12 [42496/50048]	Loss: 1.5479
Training Epoch: 12 [42624/50048]	Loss: 1.7178
Training Epoch: 12 [42752/50048]	Loss: 1.6814
Training Epoch: 12 [42880/50048]	Loss: 1.6412
Training Epoch: 12 [43008/50048]	Loss: 1.6384
Training Epoch: 12 [43136/50048]	Loss: 1.3423
Training Epoch: 12 [43264/50048]	Loss: 1.5691
Training Epoch: 12 [43392/50048]	Loss: 1.4765
Training Epoch: 12 [43520/50048]	Loss: 1.4856
Training Epoch: 12 [43648/50048]	Loss: 1.2190
Training Epoch: 12 [43776/50048]	Loss: 1.5170
Training Epoch: 12 [43904/50048]	Loss: 1.6936
Training Epoch: 12 [44032/50048]	Loss: 1.2582
Training Epoch: 12 [44160/50048]	Loss: 1.2023
Training Epoch: 12 [44288/50048]	Loss: 1.4121
Training Epoch: 12 [44416/50048]	Loss: 1.4465
Training Epoch: 12 [44544/50048]	Loss: 1.4021
Training Epoch: 12 [44672/50048]	Loss: 1.4059
Training Epoch: 12 [44800/50048]	Loss: 1.5120
Training Epoch: 12 [44928/50048]	Loss: 1.5651
Training Epoch: 12 [45056/50048]	Loss: 1.6833
Training Epoch: 12 [45184/50048]	Loss: 1.4331
Training Epoch: 12 [45312/50048]	Loss: 1.4320
Training Epoch: 12 [45440/50048]	Loss: 1.5987
Training Epoch: 12 [45568/50048]	Loss: 1.4180
Training Epoch: 12 [45696/50048]	Loss: 1.4115
Training Epoch: 12 [45824/50048]	Loss: 1.6510
Training Epoch: 12 [45952/50048]	Loss: 1.5406
Training Epoch: 12 [46080/50048]	Loss: 1.3609
Training Epoch: 12 [46208/50048]	Loss: 1.6924
Training Epoch: 12 [46336/50048]	Loss: 1.5181
Training Epoch: 12 [46464/50048]	Loss: 1.6721
Training Epoch: 12 [46592/50048]	Loss: 1.6861
Training Epoch: 12 [46720/50048]	Loss: 1.4157
Training Epoch: 12 [46848/50048]	Loss: 1.4281
Training Epoch: 12 [46976/50048]	Loss: 1.4120
Training Epoch: 12 [47104/50048]	Loss: 1.4760
Training Epoch: 12 [47232/50048]	Loss: 1.4367
Training Epoch: 12 [47360/50048]	Loss: 1.4079
Training Epoch: 12 [47488/50048]	Loss: 1.4112
Training Epoch: 12 [47616/50048]	Loss: 1.4976
Training Epoch: 12 [47744/50048]	Loss: 1.7829
Training Epoch: 12 [47872/50048]	Loss: 1.7088
Training Epoch: 12 [48000/50048]	Loss: 1.3737
Training Epoch: 12 [48128/50048]	Loss: 1.6261
Training Epoch: 12 [48256/50048]	Loss: 1.5403
Training Epoch: 12 [48384/50048]	Loss: 1.6087
Training Epoch: 12 [48512/50048]	Loss: 1.7322
Training Epoch: 12 [48640/50048]	Loss: 1.3111
Training Epoch: 12 [48768/50048]	Loss: 1.3716
Training Epoch: 12 [48896/50048]	Loss: 1.6459
Training Epoch: 12 [49024/50048]	Loss: 1.1906
Training Epoch: 12 [49152/50048]	Loss: 1.3693
Training Epoch: 12 [49280/50048]	Loss: 1.6397
Training Epoch: 12 [49408/50048]	Loss: 1.5347
Training Epoch: 12 [49536/50048]	Loss: 1.5204
Training Epoch: 12 [49664/50048]	Loss: 1.2954
Training Epoch: 12 [49792/50048]	Loss: 1.3774
Training Epoch: 12 [49920/50048]	Loss: 1.8142
Training Epoch: 12 [50048/50048]	Loss: 1.8554
Validation Epoch: 12, Average loss: 0.0124, Accuracy: 0.5631
Training Epoch: 13 [128/50048]	Loss: 1.4059
Training Epoch: 13 [256/50048]	Loss: 1.5020
Training Epoch: 13 [384/50048]	Loss: 1.3101
Training Epoch: 13 [512/50048]	Loss: 1.4122
Training Epoch: 13 [640/50048]	Loss: 1.5115
Training Epoch: 13 [768/50048]	Loss: 1.3174
Training Epoch: 13 [896/50048]	Loss: 1.1657
Training Epoch: 13 [1024/50048]	Loss: 1.5585
Training Epoch: 13 [1152/50048]	Loss: 1.5342
Training Epoch: 13 [1280/50048]	Loss: 1.3910
Training Epoch: 13 [1408/50048]	Loss: 1.0805
Training Epoch: 13 [1536/50048]	Loss: 1.5067
Training Epoch: 13 [1664/50048]	Loss: 1.4161
Training Epoch: 13 [1792/50048]	Loss: 1.6920
Training Epoch: 13 [1920/50048]	Loss: 1.4193
Training Epoch: 13 [2048/50048]	Loss: 1.4932
Training Epoch: 13 [2176/50048]	Loss: 1.3714
Training Epoch: 13 [2304/50048]	Loss: 1.4623
Training Epoch: 13 [2432/50048]	Loss: 1.2684
Training Epoch: 13 [2560/50048]	Loss: 1.5113
Training Epoch: 13 [2688/50048]	Loss: 1.5726
Training Epoch: 13 [2816/50048]	Loss: 1.5966
Training Epoch: 13 [2944/50048]	Loss: 1.2435
Training Epoch: 13 [3072/50048]	Loss: 1.5727
Training Epoch: 13 [3200/50048]	Loss: 1.5169
Training Epoch: 13 [3328/50048]	Loss: 1.3320
Training Epoch: 13 [3456/50048]	Loss: 1.4877
Training Epoch: 13 [3584/50048]	Loss: 1.3775
Training Epoch: 13 [3712/50048]	Loss: 1.2126
Training Epoch: 13 [3840/50048]	Loss: 1.3611
Training Epoch: 13 [3968/50048]	Loss: 1.3475
Training Epoch: 13 [4096/50048]	Loss: 1.6405
Training Epoch: 13 [4224/50048]	Loss: 1.4294
Training Epoch: 13 [4352/50048]	Loss: 1.4493
Training Epoch: 13 [4480/50048]	Loss: 1.4307
Training Epoch: 13 [4608/50048]	Loss: 1.4894
Training Epoch: 13 [4736/50048]	Loss: 1.5752
Training Epoch: 13 [4864/50048]	Loss: 1.3931
Training Epoch: 13 [4992/50048]	Loss: 1.4959
Training Epoch: 13 [5120/50048]	Loss: 1.4731
Training Epoch: 13 [5248/50048]	Loss: 1.6349
Training Epoch: 13 [5376/50048]	Loss: 1.6186
Training Epoch: 13 [5504/50048]	Loss: 1.4601
Training Epoch: 13 [5632/50048]	Loss: 1.7215
Training Epoch: 13 [5760/50048]	Loss: 1.6388
Training Epoch: 13 [5888/50048]	Loss: 1.6318
Training Epoch: 13 [6016/50048]	Loss: 1.5075
Training Epoch: 13 [6144/50048]	Loss: 1.4791
Training Epoch: 13 [6272/50048]	Loss: 1.3649
Training Epoch: 13 [6400/50048]	Loss: 1.4364
Training Epoch: 13 [6528/50048]	Loss: 1.5564
Training Epoch: 13 [6656/50048]	Loss: 1.5527
Training Epoch: 13 [6784/50048]	Loss: 1.5711
Training Epoch: 13 [6912/50048]	Loss: 1.3462
Training Epoch: 13 [7040/50048]	Loss: 1.3755
Training Epoch: 13 [7168/50048]	Loss: 1.3202
Training Epoch: 13 [7296/50048]	Loss: 1.4937
Training Epoch: 13 [7424/50048]	Loss: 1.3644
Training Epoch: 13 [7552/50048]	Loss: 1.5859
Training Epoch: 13 [7680/50048]	Loss: 1.5450
Training Epoch: 13 [7808/50048]	Loss: 1.3901
Training Epoch: 13 [7936/50048]	Loss: 1.5840
Training Epoch: 13 [8064/50048]	Loss: 1.5601
Training Epoch: 13 [8192/50048]	Loss: 1.3316
Training Epoch: 13 [8320/50048]	Loss: 1.3570
Training Epoch: 13 [8448/50048]	Loss: 1.5396
Training Epoch: 13 [8576/50048]	Loss: 1.2534
Training Epoch: 13 [8704/50048]	Loss: 1.6706
Training Epoch: 13 [8832/50048]	Loss: 1.4953
Training Epoch: 13 [8960/50048]	Loss: 1.4031
Training Epoch: 13 [9088/50048]	Loss: 1.2936
Training Epoch: 13 [9216/50048]	Loss: 1.6435
Training Epoch: 13 [9344/50048]	Loss: 1.4216
Training Epoch: 13 [9472/50048]	Loss: 1.4921
Training Epoch: 13 [9600/50048]	Loss: 1.2691
Training Epoch: 13 [9728/50048]	Loss: 1.3261
Training Epoch: 13 [9856/50048]	Loss: 1.6118
Training Epoch: 13 [9984/50048]	Loss: 1.6325
Training Epoch: 13 [10112/50048]	Loss: 1.4864
Training Epoch: 13 [10240/50048]	Loss: 1.3895
Training Epoch: 13 [10368/50048]	Loss: 1.4131
Training Epoch: 13 [10496/50048]	Loss: 1.5751
Training Epoch: 13 [10624/50048]	Loss: 1.4368
Training Epoch: 13 [10752/50048]	Loss: 1.5415
Training Epoch: 13 [10880/50048]	Loss: 1.3325
Training Epoch: 13 [11008/50048]	Loss: 1.5011
Training Epoch: 13 [11136/50048]	Loss: 1.2436
Training Epoch: 13 [11264/50048]	Loss: 1.5117
Training Epoch: 13 [11392/50048]	Loss: 1.5213
Training Epoch: 13 [11520/50048]	Loss: 1.4495
Training Epoch: 13 [11648/50048]	Loss: 1.2607
Training Epoch: 13 [11776/50048]	Loss: 1.7288
Training Epoch: 13 [11904/50048]	Loss: 1.4743
Training Epoch: 13 [12032/50048]	Loss: 1.3904
Training Epoch: 13 [12160/50048]	Loss: 1.3451
Training Epoch: 13 [12288/50048]	Loss: 1.5703
Training Epoch: 13 [12416/50048]	Loss: 1.4709
Training Epoch: 13 [12544/50048]	Loss: 1.5138
Training Epoch: 13 [12672/50048]	Loss: 1.5245
Training Epoch: 13 [12800/50048]	Loss: 1.6162
Training Epoch: 13 [12928/50048]	Loss: 1.3652
Training Epoch: 13 [13056/50048]	Loss: 1.6202
Training Epoch: 13 [13184/50048]	Loss: 1.3082
Training Epoch: 13 [13312/50048]	Loss: 1.5085
Training Epoch: 13 [13440/50048]	Loss: 1.4395
Training Epoch: 13 [13568/50048]	Loss: 1.5396
Training Epoch: 13 [13696/50048]	Loss: 1.5232
Training Epoch: 13 [13824/50048]	Loss: 1.4138
Training Epoch: 13 [13952/50048]	Loss: 1.5960
Training Epoch: 13 [14080/50048]	Loss: 1.6183
Training Epoch: 13 [14208/50048]	Loss: 1.3745
Training Epoch: 13 [14336/50048]	Loss: 1.4813
Training Epoch: 13 [14464/50048]	Loss: 1.5961
Training Epoch: 13 [14592/50048]	Loss: 1.3888
Training Epoch: 13 [14720/50048]	Loss: 1.7041
Training Epoch: 13 [14848/50048]	Loss: 1.4333
Training Epoch: 13 [14976/50048]	Loss: 1.6877
Training Epoch: 13 [15104/50048]	Loss: 1.3815
Training Epoch: 13 [15232/50048]	Loss: 1.3541
Training Epoch: 13 [15360/50048]	Loss: 1.4721
Training Epoch: 13 [15488/50048]	Loss: 1.4961
Training Epoch: 13 [15616/50048]	Loss: 1.3233
Training Epoch: 13 [15744/50048]	Loss: 1.5286
Training Epoch: 13 [15872/50048]	Loss: 1.3759
Training Epoch: 13 [16000/50048]	Loss: 1.7150
Training Epoch: 13 [16128/50048]	Loss: 1.6656
Training Epoch: 13 [16256/50048]	Loss: 1.2300
Training Epoch: 13 [16384/50048]	Loss: 1.5075
Training Epoch: 13 [16512/50048]	Loss: 1.2360
Training Epoch: 13 [16640/50048]	Loss: 1.3523
Training Epoch: 13 [16768/50048]	Loss: 1.5213
Training Epoch: 13 [16896/50048]	Loss: 1.3998
Training Epoch: 13 [17024/50048]	Loss: 1.4798
Training Epoch: 13 [17152/50048]	Loss: 1.3346
Training Epoch: 13 [17280/50048]	Loss: 1.5203
Training Epoch: 13 [17408/50048]	Loss: 1.6637
Training Epoch: 13 [17536/50048]	Loss: 1.5329
Training Epoch: 13 [17664/50048]	Loss: 1.6284
Training Epoch: 13 [17792/50048]	Loss: 1.4527
Training Epoch: 13 [17920/50048]	Loss: 1.6971
Training Epoch: 13 [18048/50048]	Loss: 1.4794
Training Epoch: 13 [18176/50048]	Loss: 1.4784
Training Epoch: 13 [18304/50048]	Loss: 1.5842
Training Epoch: 13 [18432/50048]	Loss: 1.4520
Training Epoch: 13 [18560/50048]	Loss: 1.5221
Training Epoch: 13 [18688/50048]	Loss: 1.4860
Training Epoch: 13 [18816/50048]	Loss: 1.4007
Training Epoch: 13 [18944/50048]	Loss: 1.5145
Training Epoch: 13 [19072/50048]	Loss: 1.6310
Training Epoch: 13 [19200/50048]	Loss: 1.3604
Training Epoch: 13 [19328/50048]	Loss: 1.5765
Training Epoch: 13 [19456/50048]	Loss: 1.6485
Training Epoch: 13 [19584/50048]	Loss: 1.6079
Training Epoch: 13 [19712/50048]	Loss: 1.4944
Training Epoch: 13 [19840/50048]	Loss: 1.2708
Training Epoch: 13 [19968/50048]	Loss: 1.3786
Training Epoch: 13 [20096/50048]	Loss: 1.5601
Training Epoch: 13 [20224/50048]	Loss: 1.5151
Training Epoch: 13 [20352/50048]	Loss: 1.4087
Training Epoch: 13 [20480/50048]	Loss: 1.6670
Training Epoch: 13 [20608/50048]	Loss: 1.3518
Training Epoch: 13 [20736/50048]	Loss: 1.4726
Training Epoch: 13 [20864/50048]	Loss: 1.7009
Training Epoch: 13 [20992/50048]	Loss: 1.3079
Training Epoch: 13 [21120/50048]	Loss: 1.3724
Training Epoch: 13 [21248/50048]	Loss: 1.6790
Training Epoch: 13 [21376/50048]	Loss: 1.8213
Training Epoch: 13 [21504/50048]	Loss: 1.5038
Training Epoch: 13 [21632/50048]	Loss: 1.5885
Training Epoch: 13 [21760/50048]	Loss: 1.4912
Training Epoch: 13 [21888/50048]	Loss: 1.4875
Training Epoch: 13 [22016/50048]	Loss: 1.3875
Training Epoch: 13 [22144/50048]	Loss: 1.4701
Training Epoch: 13 [22272/50048]	Loss: 1.6491
Training Epoch: 13 [22400/50048]	Loss: 1.4263
Training Epoch: 13 [22528/50048]	Loss: 1.4257
Training Epoch: 13 [22656/50048]	Loss: 1.3959
Training Epoch: 13 [22784/50048]	Loss: 1.3058
Training Epoch: 13 [22912/50048]	Loss: 1.3076
Training Epoch: 13 [23040/50048]	Loss: 1.4190
Training Epoch: 13 [23168/50048]	Loss: 1.4658
Training Epoch: 13 [23296/50048]	Loss: 1.3604
Training Epoch: 13 [23424/50048]	Loss: 1.6192
Training Epoch: 13 [23552/50048]	Loss: 1.2116
Training Epoch: 13 [23680/50048]	Loss: 1.1908
Training Epoch: 13 [23808/50048]	Loss: 1.7349
Training Epoch: 13 [23936/50048]	Loss: 1.5789
Training Epoch: 13 [24064/50048]	Loss: 1.5639
Training Epoch: 13 [24192/50048]	Loss: 1.5281
Training Epoch: 13 [24320/50048]	Loss: 1.4971
Training Epoch: 13 [24448/50048]	Loss: 1.4628
Training Epoch: 13 [24576/50048]	Loss: 1.3222
Training Epoch: 13 [24704/50048]	Loss: 1.4913
Training Epoch: 13 [24832/50048]	Loss: 1.7041
Training Epoch: 13 [24960/50048]	Loss: 1.5074
Training Epoch: 13 [25088/50048]	Loss: 1.1315
Training Epoch: 13 [25216/50048]	Loss: 1.4139
Training Epoch: 13 [25344/50048]	Loss: 1.4981
Training Epoch: 13 [25472/50048]	Loss: 1.4730
Training Epoch: 13 [25600/50048]	Loss: 1.3540
Training Epoch: 13 [25728/50048]	Loss: 1.5762
Training Epoch: 13 [25856/50048]	Loss: 1.3393
Training Epoch: 13 [25984/50048]	Loss: 1.4940
Training Epoch: 13 [26112/50048]	Loss: 1.4971
Training Epoch: 13 [26240/50048]	Loss: 1.3653
Training Epoch: 13 [26368/50048]	Loss: 1.6595
Training Epoch: 13 [26496/50048]	Loss: 1.1217
Training Epoch: 13 [26624/50048]	Loss: 1.5390
Training Epoch: 13 [26752/50048]	Loss: 1.5578
Training Epoch: 13 [26880/50048]	Loss: 1.5464
Training Epoch: 13 [27008/50048]	Loss: 1.3960
Training Epoch: 13 [27136/50048]	Loss: 1.4259
Training Epoch: 13 [27264/50048]	Loss: 1.2974
Training Epoch: 13 [27392/50048]	Loss: 1.6008
Training Epoch: 13 [27520/50048]	Loss: 1.5851
Training Epoch: 13 [27648/50048]	Loss: 1.3173
Training Epoch: 13 [27776/50048]	Loss: 1.1621
Training Epoch: 13 [27904/50048]	Loss: 1.4897
Training Epoch: 13 [28032/50048]	Loss: 1.8329
Training Epoch: 13 [28160/50048]	Loss: 1.4646
Training Epoch: 13 [28288/50048]	Loss: 1.3259
Training Epoch: 13 [28416/50048]	Loss: 1.7520
Training Epoch: 13 [28544/50048]	Loss: 1.3548
Training Epoch: 13 [28672/50048]	Loss: 1.5804
Training Epoch: 13 [28800/50048]	Loss: 1.6380
Training Epoch: 13 [28928/50048]	Loss: 1.4631
Training Epoch: 13 [29056/50048]	Loss: 1.2961
Training Epoch: 13 [29184/50048]	Loss: 1.4433
Training Epoch: 13 [29312/50048]	Loss: 1.5517
Training Epoch: 13 [29440/50048]	Loss: 1.4564
Training Epoch: 13 [29568/50048]	Loss: 1.1449
Training Epoch: 13 [29696/50048]	Loss: 1.4953
Training Epoch: 13 [29824/50048]	Loss: 1.5879
Training Epoch: 13 [29952/50048]	Loss: 1.7046
Training Epoch: 13 [30080/50048]	Loss: 1.4267
Training Epoch: 13 [30208/50048]	Loss: 1.4421
Training Epoch: 13 [30336/50048]	Loss: 1.6519
Training Epoch: 13 [30464/50048]	Loss: 1.3674
Training Epoch: 13 [30592/50048]	Loss: 1.3679
Training Epoch: 13 [30720/50048]	Loss: 1.3583
Training Epoch: 13 [30848/50048]	Loss: 1.4619
Training Epoch: 13 [30976/50048]	Loss: 1.4397
Training Epoch: 13 [31104/50048]	Loss: 1.3821
Training Epoch: 13 [31232/50048]	Loss: 1.7882
Training Epoch: 13 [31360/50048]	Loss: 1.2662
Training Epoch: 13 [31488/50048]	Loss: 1.4886
Training Epoch: 13 [31616/50048]	Loss: 1.4613
Training Epoch: 13 [31744/50048]	Loss: 1.3164
Training Epoch: 13 [31872/50048]	Loss: 1.5143
Training Epoch: 13 [32000/50048]	Loss: 1.4741
Training Epoch: 13 [32128/50048]	Loss: 1.6105
Training Epoch: 13 [32256/50048]	Loss: 1.2753
Training Epoch: 13 [32384/50048]	Loss: 1.5476
Training Epoch: 13 [32512/50048]	Loss: 1.8030
Training Epoch: 13 [32640/50048]	Loss: 1.4598
Training Epoch: 13 [32768/50048]	Loss: 1.5992
Training Epoch: 13 [32896/50048]	Loss: 1.5942
Training Epoch: 13 [33024/50048]	Loss: 1.2729
Training Epoch: 13 [33152/50048]	Loss: 1.2008
Training Epoch: 13 [33280/50048]	Loss: 1.5346
Training Epoch: 13 [33408/50048]	Loss: 1.2405
Training Epoch: 13 [33536/50048]	Loss: 1.5018
Training Epoch: 13 [33664/50048]	Loss: 1.4079
Training Epoch: 13 [33792/50048]	Loss: 1.1862
Training Epoch: 13 [33920/50048]	Loss: 1.6393
Training Epoch: 13 [34048/50048]	Loss: 1.6457
Training Epoch: 13 [34176/50048]	Loss: 1.4986
Training Epoch: 13 [34304/50048]	Loss: 1.3164
Training Epoch: 13 [34432/50048]	Loss: 1.4795
Training Epoch: 13 [34560/50048]	Loss: 1.5512
Training Epoch: 13 [34688/50048]	Loss: 1.3550
Training Epoch: 13 [34816/50048]	Loss: 1.4563
Training Epoch: 13 [34944/50048]	Loss: 1.3085
Training Epoch: 13 [35072/50048]	Loss: 1.3508
Training Epoch: 13 [35200/50048]	Loss: 1.5404
Training Epoch: 13 [35328/50048]	Loss: 1.3999
Training Epoch: 13 [35456/50048]	Loss: 1.6981
Training Epoch: 13 [35584/50048]	Loss: 1.6044
Training Epoch: 13 [35712/50048]	Loss: 1.4525
Training Epoch: 13 [35840/50048]	Loss: 1.5899
Training Epoch: 13 [35968/50048]	Loss: 1.3927
Training Epoch: 13 [36096/50048]	Loss: 1.5184
Training Epoch: 13 [36224/50048]	Loss: 1.4823
Training Epoch: 13 [36352/50048]	Loss: 1.5879
Training Epoch: 13 [36480/50048]	Loss: 1.4492
Training Epoch: 13 [36608/50048]	Loss: 1.5922
Training Epoch: 13 [36736/50048]	Loss: 1.6135
Training Epoch: 13 [36864/50048]	Loss: 1.5200
Training Epoch: 13 [36992/50048]	Loss: 1.2612
Training Epoch: 13 [37120/50048]	Loss: 1.4431
Training Epoch: 13 [37248/50048]	Loss: 1.6687
Training Epoch: 13 [37376/50048]	Loss: 1.4779
Training Epoch: 13 [37504/50048]	Loss: 1.6674
Training Epoch: 13 [37632/50048]	Loss: 1.5562
Training Epoch: 13 [37760/50048]	Loss: 1.4147
Training Epoch: 13 [37888/50048]	Loss: 1.3219
Training Epoch: 13 [38016/50048]	Loss: 1.7820
Training Epoch: 13 [38144/50048]	Loss: 1.4386
Training Epoch: 13 [38272/50048]	Loss: 1.4818
Training Epoch: 13 [38400/50048]	Loss: 1.5402
Training Epoch: 13 [38528/50048]	Loss: 1.3543
Training Epoch: 13 [38656/50048]	Loss: 1.4467
Training Epoch: 13 [38784/50048]	Loss: 1.6352
Training Epoch: 13 [38912/50048]	Loss: 1.3753
Training Epoch: 13 [39040/50048]	Loss: 1.5960
Training Epoch: 13 [39168/50048]	Loss: 1.4011
Training Epoch: 13 [39296/50048]	Loss: 1.5781
Training Epoch: 13 [39424/50048]	Loss: 1.7610
Training Epoch: 13 [39552/50048]	Loss: 1.3582
Training Epoch: 13 [39680/50048]	Loss: 1.3969
Training Epoch: 13 [39808/50048]	Loss: 1.4101
Training Epoch: 13 [39936/50048]	Loss: 1.5903
Training Epoch: 13 [40064/50048]	Loss: 1.4176
Training Epoch: 13 [40192/50048]	Loss: 1.6860
Training Epoch: 13 [40320/50048]	Loss: 1.6071
Training Epoch: 13 [40448/50048]	Loss: 1.5774
Training Epoch: 13 [40576/50048]	Loss: 1.5806
Training Epoch: 13 [40704/50048]	Loss: 1.3536
Training Epoch: 13 [40832/50048]	Loss: 1.4637
Training Epoch: 13 [40960/50048]	Loss: 1.4221
Training Epoch: 13 [41088/50048]	Loss: 1.6750
Training Epoch: 13 [41216/50048]	Loss: 1.4701
Training Epoch: 13 [41344/50048]	Loss: 1.5012
Training Epoch: 13 [41472/50048]	Loss: 1.5164
Training Epoch: 13 [41600/50048]	Loss: 1.5627
Training Epoch: 13 [41728/50048]	Loss: 1.3987
Training Epoch: 13 [41856/50048]	Loss: 1.4170
Training Epoch: 13 [41984/50048]	Loss: 1.5754
Training Epoch: 13 [42112/50048]	Loss: 1.4320
Training Epoch: 13 [42240/50048]	Loss: 1.3384
Training Epoch: 13 [42368/50048]	Loss: 1.1178
Training Epoch: 13 [42496/50048]	Loss: 1.5915
Training Epoch: 13 [42624/50048]	Loss: 1.3818
Training Epoch: 13 [42752/50048]	Loss: 1.5786
Training Epoch: 13 [42880/50048]	Loss: 1.7201
Training Epoch: 13 [43008/50048]	Loss: 1.6013
Training Epoch: 13 [43136/50048]	Loss: 1.6752
Training Epoch: 13 [43264/50048]	Loss: 1.3752
Training Epoch: 13 [43392/50048]	Loss: 1.5404
Training Epoch: 13 [43520/50048]	Loss: 1.3504
Training Epoch: 13 [43648/50048]	Loss: 1.4435
Training Epoch: 13 [43776/50048]	Loss: 1.5412
Training Epoch: 13 [43904/50048]	Loss: 1.8445
Training Epoch: 13 [44032/50048]	Loss: 1.4127
Training Epoch: 13 [44160/50048]	Loss: 1.3388
Training Epoch: 13 [44288/50048]	Loss: 1.5268
Training Epoch: 13 [44416/50048]	Loss: 1.4209
Training Epoch: 13 [44544/50048]	Loss: 1.2136
Training Epoch: 13 [44672/50048]	Loss: 1.3412
Training Epoch: 13 [44800/50048]	Loss: 1.3052
Training Epoch: 13 [44928/50048]	Loss: 1.5285
Training Epoch: 13 [45056/50048]	Loss: 1.4570
Training Epoch: 13 [45184/50048]	Loss: 1.6486
Training Epoch: 13 [45312/50048]	Loss: 1.3931
Training Epoch: 13 [45440/50048]	Loss: 1.4697
Training Epoch: 13 [45568/50048]	Loss: 1.5300
Training Epoch: 13 [45696/50048]	Loss: 1.3583
Training Epoch: 13 [45824/50048]	Loss: 1.3160
Training Epoch: 13 [45952/50048]	Loss: 1.2845
Training Epoch: 13 [46080/50048]	Loss: 1.4748
Training Epoch: 13 [46208/50048]	Loss: 1.7234
Training Epoch: 13 [46336/50048]	Loss: 1.7293
Training Epoch: 13 [46464/50048]	Loss: 1.3423
Training Epoch: 13 [46592/50048]	Loss: 1.3530
Training Epoch: 13 [46720/50048]	Loss: 1.4080
Training Epoch: 13 [46848/50048]	Loss: 1.6856
Training Epoch: 13 [46976/50048]	Loss: 1.2606
Training Epoch: 13 [47104/50048]	Loss: 1.3216
Training Epoch: 13 [47232/50048]	Loss: 1.3756
Training Epoch: 13 [47360/50048]	Loss: 1.5301
Training Epoch: 13 [47488/50048]	Loss: 1.5133
Training Epoch: 13 [47616/50048]	Loss: 1.6241
Training Epoch: 13 [47744/50048]	Loss: 1.4092
Training Epoch: 13 [47872/50048]	Loss: 1.4662
Training Epoch: 13 [48000/50048]	Loss: 1.1699
Training Epoch: 13 [48128/50048]	Loss: 1.4474
Training Epoch: 13 [48256/50048]	Loss: 1.6636
Training Epoch: 13 [48384/50048]	Loss: 1.6246
Training Epoch: 13 [48512/50048]	Loss: 1.4653
Training Epoch: 13 [48640/50048]	Loss: 1.4835
Training Epoch: 13 [48768/50048]	Loss: 1.6319
Training Epoch: 13 [48896/50048]	Loss: 1.2881
Training Epoch: 13 [49024/50048]	Loss: 1.2671
Training Epoch: 13 [49152/50048]	Loss: 1.3566
Training Epoch: 13 [49280/50048]	Loss: 1.4881
Training Epoch: 13 [49408/50048]	Loss: 1.6137
Training Epoch: 13 [49536/50048]	Loss: 1.5476
Training Epoch: 13 [49664/50048]	Loss: 1.6373
Training Epoch: 13 [49792/50048]	Loss: 1.1909
Training Epoch: 13 [49920/50048]	Loss: 1.4452
Training Epoch: 13 [50048/50048]	Loss: 1.2542
Validation Epoch: 13, Average loss: 0.0122, Accuracy: 0.5660
Training Epoch: 14 [128/50048]	Loss: 1.3445
Training Epoch: 14 [256/50048]	Loss: 1.4268
Training Epoch: 14 [384/50048]	Loss: 1.3679
Training Epoch: 14 [512/50048]	Loss: 1.4116
Training Epoch: 14 [640/50048]	Loss: 1.5310
Training Epoch: 14 [768/50048]	Loss: 1.4029
Training Epoch: 14 [896/50048]	Loss: 1.6596
Training Epoch: 14 [1024/50048]	Loss: 1.2124
Training Epoch: 14 [1152/50048]	Loss: 1.7172
Training Epoch: 14 [1280/50048]	Loss: 1.6021
Training Epoch: 14 [1408/50048]	Loss: 1.4493
Training Epoch: 14 [1536/50048]	Loss: 1.3811
Training Epoch: 14 [1664/50048]	Loss: 1.6313
Training Epoch: 14 [1792/50048]	Loss: 1.3852
Training Epoch: 14 [1920/50048]	Loss: 1.2684
Training Epoch: 14 [2048/50048]	Loss: 1.3038
Training Epoch: 14 [2176/50048]	Loss: 1.3642
Training Epoch: 14 [2304/50048]	Loss: 1.5963
Training Epoch: 14 [2432/50048]	Loss: 1.5455
Training Epoch: 14 [2560/50048]	Loss: 1.4981
Training Epoch: 14 [2688/50048]	Loss: 1.3286
Training Epoch: 14 [2816/50048]	Loss: 1.3963
Training Epoch: 14 [2944/50048]	Loss: 1.4016
Training Epoch: 14 [3072/50048]	Loss: 1.4786
Training Epoch: 14 [3200/50048]	Loss: 1.3822
Training Epoch: 14 [3328/50048]	Loss: 1.6169
Training Epoch: 14 [3456/50048]	Loss: 1.5321
Training Epoch: 14 [3584/50048]	Loss: 1.6130
Training Epoch: 14 [3712/50048]	Loss: 1.5906
Training Epoch: 14 [3840/50048]	Loss: 1.5213
Training Epoch: 14 [3968/50048]	Loss: 1.3085
Training Epoch: 14 [4096/50048]	Loss: 1.4214
Training Epoch: 14 [4224/50048]	Loss: 1.5845
Training Epoch: 14 [4352/50048]	Loss: 1.3529
Training Epoch: 14 [4480/50048]	Loss: 1.6731
Training Epoch: 14 [4608/50048]	Loss: 1.5653
Training Epoch: 14 [4736/50048]	Loss: 1.3218
Training Epoch: 14 [4864/50048]	Loss: 1.3069
Training Epoch: 14 [4992/50048]	Loss: 1.4335
Training Epoch: 14 [5120/50048]	Loss: 1.5561
Training Epoch: 14 [5248/50048]	Loss: 1.5685
Training Epoch: 14 [5376/50048]	Loss: 1.1961
Training Epoch: 14 [5504/50048]	Loss: 1.4481
Training Epoch: 14 [5632/50048]	Loss: 1.4821
Training Epoch: 14 [5760/50048]	Loss: 1.7754
Training Epoch: 14 [5888/50048]	Loss: 1.3408
Training Epoch: 14 [6016/50048]	Loss: 1.5459
Training Epoch: 14 [6144/50048]	Loss: 1.5251
Training Epoch: 14 [6272/50048]	Loss: 1.5035
Training Epoch: 14 [6400/50048]	Loss: 1.1704
Training Epoch: 14 [6528/50048]	Loss: 1.1967
Training Epoch: 14 [6656/50048]	Loss: 1.7912
Training Epoch: 14 [6784/50048]	Loss: 1.1841
Training Epoch: 14 [6912/50048]	Loss: 1.6880
Training Epoch: 14 [7040/50048]	Loss: 1.5007
Training Epoch: 14 [7168/50048]	Loss: 1.6639
Training Epoch: 14 [7296/50048]	Loss: 1.3556
Training Epoch: 14 [7424/50048]	Loss: 1.7838
Training Epoch: 14 [7552/50048]	Loss: 1.5879
Training Epoch: 14 [7680/50048]	Loss: 1.4790
Training Epoch: 14 [7808/50048]	Loss: 1.3372
Training Epoch: 14 [7936/50048]	Loss: 1.2767
Training Epoch: 14 [8064/50048]	Loss: 1.6142
Training Epoch: 14 [8192/50048]	Loss: 1.4162
Training Epoch: 14 [8320/50048]	Loss: 1.6698
Training Epoch: 14 [8448/50048]	Loss: 1.4180
Training Epoch: 14 [8576/50048]	Loss: 1.3332
Training Epoch: 14 [8704/50048]	Loss: 1.4277
Training Epoch: 14 [8832/50048]	Loss: 1.5516
Training Epoch: 14 [8960/50048]	Loss: 1.7469
Training Epoch: 14 [9088/50048]	Loss: 1.3322
Training Epoch: 14 [9216/50048]	Loss: 1.4151
Training Epoch: 14 [9344/50048]	Loss: 1.5474
Training Epoch: 14 [9472/50048]	Loss: 1.5459
Training Epoch: 14 [9600/50048]	Loss: 1.4942
Training Epoch: 14 [9728/50048]	Loss: 1.3633
Training Epoch: 14 [9856/50048]	Loss: 1.3238
Training Epoch: 14 [9984/50048]	Loss: 1.5014
Training Epoch: 14 [10112/50048]	Loss: 1.2562
Training Epoch: 14 [10240/50048]	Loss: 1.3366
Training Epoch: 14 [10368/50048]	Loss: 1.4709
Training Epoch: 14 [10496/50048]	Loss: 1.5458
Training Epoch: 14 [10624/50048]	Loss: 1.4193
Training Epoch: 14 [10752/50048]	Loss: 1.2402
Training Epoch: 14 [10880/50048]	Loss: 1.4237
Training Epoch: 14 [11008/50048]	Loss: 1.3985
Training Epoch: 14 [11136/50048]	Loss: 1.3769
Training Epoch: 14 [11264/50048]	Loss: 1.4103
Training Epoch: 14 [11392/50048]	Loss: 1.3837
Training Epoch: 14 [11520/50048]	Loss: 1.3945
Training Epoch: 14 [11648/50048]	Loss: 1.3941
Training Epoch: 14 [11776/50048]	Loss: 1.6939
Training Epoch: 14 [11904/50048]	Loss: 1.4318
Training Epoch: 14 [12032/50048]	Loss: 1.2829
Training Epoch: 14 [12160/50048]	Loss: 1.2809
Training Epoch: 14 [12288/50048]	Loss: 1.3776
Training Epoch: 14 [12416/50048]	Loss: 1.1994
Training Epoch: 14 [12544/50048]	Loss: 1.5714
Training Epoch: 14 [12672/50048]	Loss: 1.2876
Training Epoch: 14 [12800/50048]	Loss: 1.3419
Training Epoch: 14 [12928/50048]	Loss: 1.3018
Training Epoch: 14 [13056/50048]	Loss: 1.3492
Training Epoch: 14 [13184/50048]	Loss: 1.5339
Training Epoch: 14 [13312/50048]	Loss: 1.3843
Training Epoch: 14 [13440/50048]	Loss: 1.4232
Training Epoch: 14 [13568/50048]	Loss: 1.4837
Training Epoch: 14 [13696/50048]	Loss: 1.3725
Training Epoch: 14 [13824/50048]	Loss: 1.5445
Training Epoch: 14 [13952/50048]	Loss: 1.3723
Training Epoch: 14 [14080/50048]	Loss: 1.4663
Training Epoch: 14 [14208/50048]	Loss: 1.4754
Training Epoch: 14 [14336/50048]	Loss: 1.4257
Training Epoch: 14 [14464/50048]	Loss: 1.2848
Training Epoch: 14 [14592/50048]	Loss: 1.4868
Training Epoch: 14 [14720/50048]	Loss: 1.6702
Training Epoch: 14 [14848/50048]	Loss: 1.4529
Training Epoch: 14 [14976/50048]	Loss: 1.3749
Training Epoch: 14 [15104/50048]	Loss: 1.5504
Training Epoch: 14 [15232/50048]	Loss: 1.4684
Training Epoch: 14 [15360/50048]	Loss: 1.5865
Training Epoch: 14 [15488/50048]	Loss: 1.7021
Training Epoch: 14 [15616/50048]	Loss: 1.3402
Training Epoch: 14 [15744/50048]	Loss: 1.4393
Training Epoch: 14 [15872/50048]	Loss: 1.5261
Training Epoch: 14 [16000/50048]	Loss: 1.4880
Training Epoch: 14 [16128/50048]	Loss: 1.1375
Training Epoch: 14 [16256/50048]	Loss: 1.6397
Training Epoch: 14 [16384/50048]	Loss: 1.1234
Training Epoch: 14 [16512/50048]	Loss: 1.2687
Training Epoch: 14 [16640/50048]	Loss: 1.5225
Training Epoch: 14 [16768/50048]	Loss: 1.4134
Training Epoch: 14 [16896/50048]	Loss: 1.3783
Training Epoch: 14 [17024/50048]	Loss: 1.4440
Training Epoch: 14 [17152/50048]	Loss: 1.4862
Training Epoch: 14 [17280/50048]	Loss: 1.4893
Training Epoch: 14 [17408/50048]	Loss: 1.5208
Training Epoch: 14 [17536/50048]	Loss: 1.2811
Training Epoch: 14 [17664/50048]	Loss: 1.4062
Training Epoch: 14 [17792/50048]	Loss: 1.6104
Training Epoch: 14 [17920/50048]	Loss: 1.5499
Training Epoch: 14 [18048/50048]	Loss: 1.4621
Training Epoch: 14 [18176/50048]	Loss: 1.3706
Training Epoch: 14 [18304/50048]	Loss: 1.6087
Training Epoch: 14 [18432/50048]	Loss: 1.5943
Training Epoch: 14 [18560/50048]	Loss: 1.4999
Training Epoch: 14 [18688/50048]	Loss: 1.4411
Training Epoch: 14 [18816/50048]	Loss: 1.4197
Training Epoch: 14 [18944/50048]	Loss: 1.5089
Training Epoch: 14 [19072/50048]	Loss: 1.5456
Training Epoch: 14 [19200/50048]	Loss: 1.3547
Training Epoch: 14 [19328/50048]	Loss: 1.2687
Training Epoch: 14 [19456/50048]	Loss: 1.3250
Training Epoch: 14 [19584/50048]	Loss: 1.3043
Training Epoch: 14 [19712/50048]	Loss: 1.3209
Training Epoch: 14 [19840/50048]	Loss: 1.4329
Training Epoch: 14 [19968/50048]	Loss: 1.2199
Training Epoch: 14 [20096/50048]	Loss: 1.6013
Training Epoch: 14 [20224/50048]	Loss: 1.1735
Training Epoch: 14 [20352/50048]	Loss: 1.4738
Training Epoch: 14 [20480/50048]	Loss: 1.4506
Training Epoch: 14 [20608/50048]	Loss: 1.5471
Training Epoch: 14 [20736/50048]	Loss: 1.3382
Training Epoch: 14 [20864/50048]	Loss: 1.5580
Training Epoch: 14 [20992/50048]	Loss: 1.5127
Training Epoch: 14 [21120/50048]	Loss: 1.4172
Training Epoch: 14 [21248/50048]	Loss: 1.3914
Training Epoch: 14 [21376/50048]	Loss: 1.4130
Training Epoch: 14 [21504/50048]	Loss: 1.6550
Training Epoch: 14 [21632/50048]	Loss: 1.5177
Training Epoch: 14 [21760/50048]	Loss: 1.4238
Training Epoch: 14 [21888/50048]	Loss: 1.2595
Training Epoch: 14 [22016/50048]	Loss: 1.3789
Training Epoch: 14 [22144/50048]	Loss: 1.1916
Training Epoch: 14 [22272/50048]	Loss: 1.4222
Training Epoch: 14 [22400/50048]	Loss: 1.3362
Training Epoch: 14 [22528/50048]	Loss: 1.4888
Training Epoch: 14 [22656/50048]	Loss: 1.2193
Training Epoch: 14 [22784/50048]	Loss: 1.3573
Training Epoch: 14 [22912/50048]	Loss: 1.3976
Training Epoch: 14 [23040/50048]	Loss: 1.4685
Training Epoch: 14 [23168/50048]	Loss: 1.4346
Training Epoch: 14 [23296/50048]	Loss: 1.4505
Training Epoch: 14 [23424/50048]	Loss: 1.2101
Training Epoch: 14 [23552/50048]	Loss: 1.2132
Training Epoch: 14 [23680/50048]	Loss: 1.4035
Training Epoch: 14 [23808/50048]	Loss: 1.3572
Training Epoch: 14 [23936/50048]	Loss: 1.3502
Training Epoch: 14 [24064/50048]	Loss: 1.3872
Training Epoch: 14 [24192/50048]	Loss: 1.3661
Training Epoch: 14 [24320/50048]	Loss: 1.3718
Training Epoch: 14 [24448/50048]	Loss: 1.4873
Training Epoch: 14 [24576/50048]	Loss: 1.4838
Training Epoch: 14 [24704/50048]	Loss: 1.4321
Training Epoch: 14 [24832/50048]	Loss: 1.5600
Training Epoch: 14 [24960/50048]	Loss: 1.6267
Training Epoch: 14 [25088/50048]	Loss: 1.5129
Training Epoch: 14 [25216/50048]	Loss: 1.3531
Training Epoch: 14 [25344/50048]	Loss: 1.4130
Training Epoch: 14 [25472/50048]	Loss: 1.3279
Training Epoch: 14 [25600/50048]	Loss: 1.2106
Training Epoch: 14 [25728/50048]	Loss: 1.3593
Training Epoch: 14 [25856/50048]	Loss: 1.4624
Training Epoch: 14 [25984/50048]	Loss: 1.4602
Training Epoch: 14 [26112/50048]	Loss: 1.3859
Training Epoch: 14 [26240/50048]	Loss: 1.6114
Training Epoch: 14 [26368/50048]	Loss: 1.6307
Training Epoch: 14 [26496/50048]	Loss: 1.4775
Training Epoch: 14 [26624/50048]	Loss: 1.5940
Training Epoch: 14 [26752/50048]	Loss: 1.2670
Training Epoch: 14 [26880/50048]	Loss: 1.2389
Training Epoch: 14 [27008/50048]	Loss: 1.2675
Training Epoch: 14 [27136/50048]	Loss: 1.4643
Training Epoch: 14 [27264/50048]	Loss: 1.3957
Training Epoch: 14 [27392/50048]	Loss: 1.4450
Training Epoch: 14 [27520/50048]	Loss: 1.4909
Training Epoch: 14 [27648/50048]	Loss: 1.6129
Training Epoch: 14 [27776/50048]	Loss: 1.3815
Training Epoch: 14 [27904/50048]	Loss: 1.5250
Training Epoch: 14 [28032/50048]	Loss: 1.6690
Training Epoch: 14 [28160/50048]	Loss: 1.3480
Training Epoch: 14 [28288/50048]	Loss: 1.4414
Training Epoch: 14 [28416/50048]	Loss: 1.4236
Training Epoch: 14 [28544/50048]	Loss: 1.3291
Training Epoch: 14 [28672/50048]	Loss: 1.5951
Training Epoch: 14 [28800/50048]	Loss: 1.5244
Training Epoch: 14 [28928/50048]	Loss: 1.5032
Training Epoch: 14 [29056/50048]	Loss: 1.3716
Training Epoch: 14 [29184/50048]	Loss: 1.4774
Training Epoch: 14 [29312/50048]	Loss: 1.3803
Training Epoch: 14 [29440/50048]	Loss: 1.4101
Training Epoch: 14 [29568/50048]	Loss: 1.2938
Training Epoch: 14 [29696/50048]	Loss: 1.4516
Training Epoch: 14 [29824/50048]	Loss: 1.5040
Training Epoch: 14 [29952/50048]	Loss: 1.4999
Training Epoch: 14 [30080/50048]	Loss: 1.3810
Training Epoch: 14 [30208/50048]	Loss: 1.3550
Training Epoch: 14 [30336/50048]	Loss: 1.2361
Training Epoch: 14 [30464/50048]	Loss: 1.4209
Training Epoch: 14 [30592/50048]	Loss: 1.4930
Training Epoch: 14 [30720/50048]	Loss: 1.3882
Training Epoch: 14 [30848/50048]	Loss: 1.2844
Training Epoch: 14 [30976/50048]	Loss: 1.6413
Training Epoch: 14 [31104/50048]	Loss: 1.5245
Training Epoch: 14 [31232/50048]	Loss: 1.2748
Training Epoch: 14 [31360/50048]	Loss: 1.5687
Training Epoch: 14 [31488/50048]	Loss: 1.4150
Training Epoch: 14 [31616/50048]	Loss: 1.4138
Training Epoch: 14 [31744/50048]	Loss: 1.6379
Training Epoch: 14 [31872/50048]	Loss: 1.4997
Training Epoch: 14 [32000/50048]	Loss: 1.5813
Training Epoch: 14 [32128/50048]	Loss: 1.5040
Training Epoch: 14 [32256/50048]	Loss: 1.4980
Training Epoch: 14 [32384/50048]	Loss: 1.1972
Training Epoch: 14 [32512/50048]	Loss: 1.5442
Training Epoch: 14 [32640/50048]	Loss: 1.5629
Training Epoch: 14 [32768/50048]	Loss: 1.6612
Training Epoch: 14 [32896/50048]	Loss: 1.5164
Training Epoch: 14 [33024/50048]	Loss: 1.5031
Training Epoch: 14 [33152/50048]	Loss: 1.2705
Training Epoch: 14 [33280/50048]	Loss: 1.3431
Training Epoch: 14 [33408/50048]	Loss: 1.3421
Training Epoch: 14 [33536/50048]	Loss: 1.3203
Training Epoch: 14 [33664/50048]	Loss: 1.5216
Training Epoch: 14 [33792/50048]	Loss: 1.5302
Training Epoch: 14 [33920/50048]	Loss: 1.1832
Training Epoch: 14 [34048/50048]	Loss: 1.3706
Training Epoch: 14 [34176/50048]	Loss: 1.3536
Training Epoch: 14 [34304/50048]	Loss: 1.5801
Training Epoch: 14 [34432/50048]	Loss: 1.4742
Training Epoch: 14 [34560/50048]	Loss: 1.5568
Training Epoch: 14 [34688/50048]	Loss: 1.5807
Training Epoch: 14 [34816/50048]	Loss: 1.3574
Training Epoch: 14 [34944/50048]	Loss: 1.4189
Training Epoch: 14 [35072/50048]	Loss: 1.4804
Training Epoch: 14 [35200/50048]	Loss: 1.4210
Training Epoch: 14 [35328/50048]	Loss: 1.3262
Training Epoch: 14 [35456/50048]	Loss: 1.5246
Training Epoch: 14 [35584/50048]	Loss: 1.4335
Training Epoch: 14 [35712/50048]	Loss: 1.4018
Training Epoch: 14 [35840/50048]	Loss: 1.4176
Training Epoch: 14 [35968/50048]	Loss: 1.3546
Training Epoch: 14 [36096/50048]	Loss: 1.6825
Training Epoch: 14 [36224/50048]	Loss: 1.3623
Training Epoch: 14 [36352/50048]	Loss: 1.4147
Training Epoch: 14 [36480/50048]	Loss: 1.1862
Training Epoch: 14 [36608/50048]	Loss: 1.5536
Training Epoch: 14 [36736/50048]	Loss: 1.5013
Training Epoch: 14 [36864/50048]	Loss: 1.3820
Training Epoch: 14 [36992/50048]	Loss: 1.4430
Training Epoch: 14 [37120/50048]	Loss: 1.3510
Training Epoch: 14 [37248/50048]	Loss: 1.1938
Training Epoch: 14 [37376/50048]	Loss: 1.2832
Training Epoch: 14 [37504/50048]	Loss: 1.6855
Training Epoch: 14 [37632/50048]	Loss: 1.3526
Training Epoch: 14 [37760/50048]	Loss: 1.6587
Training Epoch: 14 [37888/50048]	Loss: 1.3777
Training Epoch: 14 [38016/50048]	Loss: 1.4181
Training Epoch: 14 [38144/50048]	Loss: 1.7078
Training Epoch: 14 [38272/50048]	Loss: 1.2856
Training Epoch: 14 [38400/50048]	Loss: 1.4438
Training Epoch: 14 [38528/50048]	Loss: 1.4229
Training Epoch: 14 [38656/50048]	Loss: 1.7759
Training Epoch: 14 [38784/50048]	Loss: 1.4704
Training Epoch: 14 [38912/50048]	Loss: 1.2969
Training Epoch: 14 [39040/50048]	Loss: 1.3085
Training Epoch: 14 [39168/50048]	Loss: 1.3537
Training Epoch: 14 [39296/50048]	Loss: 1.4399
Training Epoch: 14 [39424/50048]	Loss: 1.7038
Training Epoch: 14 [39552/50048]	Loss: 1.3311
Training Epoch: 14 [39680/50048]	Loss: 1.4362
Training Epoch: 14 [39808/50048]	Loss: 1.3582
Training Epoch: 14 [39936/50048]	Loss: 1.2280
Training Epoch: 14 [40064/50048]	Loss: 1.4823
Training Epoch: 14 [40192/50048]	Loss: 1.3273
Training Epoch: 14 [40320/50048]	Loss: 1.4558
Training Epoch: 14 [40448/50048]	Loss: 1.2652
Training Epoch: 14 [40576/50048]	Loss: 1.2556
Training Epoch: 14 [40704/50048]	Loss: 1.4712
Training Epoch: 14 [40832/50048]	Loss: 1.5640
Training Epoch: 14 [40960/50048]	Loss: 1.5925
Training Epoch: 14 [41088/50048]	Loss: 1.7533
Training Epoch: 14 [41216/50048]	Loss: 1.2563
Training Epoch: 14 [41344/50048]	Loss: 1.4614
Training Epoch: 14 [41472/50048]	Loss: 1.5123
Training Epoch: 14 [41600/50048]	Loss: 1.4773
Training Epoch: 14 [41728/50048]	Loss: 1.5503
Training Epoch: 14 [41856/50048]	Loss: 1.4980
Training Epoch: 14 [41984/50048]	Loss: 1.4386
Training Epoch: 14 [42112/50048]	Loss: 1.3112
Training Epoch: 14 [42240/50048]	Loss: 1.2952
Training Epoch: 14 [42368/50048]	Loss: 1.2615
Training Epoch: 14 [42496/50048]	Loss: 1.5474
Training Epoch: 14 [42624/50048]	Loss: 1.3825
Training Epoch: 14 [42752/50048]	Loss: 1.6331
Training Epoch: 14 [42880/50048]	Loss: 1.8508
Training Epoch: 14 [43008/50048]	Loss: 1.4126
Training Epoch: 14 [43136/50048]	Loss: 1.3293
Training Epoch: 14 [43264/50048]	Loss: 1.3518
Training Epoch: 14 [43392/50048]	Loss: 1.3531
Training Epoch: 14 [43520/50048]	Loss: 1.5880
Training Epoch: 14 [43648/50048]	Loss: 1.5367
Training Epoch: 14 [43776/50048]	Loss: 1.6377
Training Epoch: 14 [43904/50048]	Loss: 1.4151
Training Epoch: 14 [44032/50048]	Loss: 1.2959
Training Epoch: 14 [44160/50048]	Loss: 1.3894
Training Epoch: 14 [44288/50048]	Loss: 1.1987
Training Epoch: 14 [44416/50048]	Loss: 1.5597
Training Epoch: 14 [44544/50048]	Loss: 1.6568
Training Epoch: 14 [44672/50048]	Loss: 1.2951
Training Epoch: 14 [44800/50048]	Loss: 1.2308
Training Epoch: 14 [44928/50048]	Loss: 1.5616
Training Epoch: 14 [45056/50048]	Loss: 1.5816
Training Epoch: 14 [45184/50048]	Loss: 1.4533
Training Epoch: 14 [45312/50048]	Loss: 1.4967
Training Epoch: 14 [45440/50048]	Loss: 1.3904
Training Epoch: 14 [45568/50048]	Loss: 1.6207
Training Epoch: 14 [45696/50048]	Loss: 1.4969
Training Epoch: 14 [45824/50048]	Loss: 1.3403
Training Epoch: 14 [45952/50048]	Loss: 1.5001
Training Epoch: 14 [46080/50048]	Loss: 1.5854
Training Epoch: 14 [46208/50048]	Loss: 1.5514
Training Epoch: 14 [46336/50048]	Loss: 1.5574
Training Epoch: 14 [46464/50048]	Loss: 1.4056
Training Epoch: 14 [46592/50048]	Loss: 1.3245
Training Epoch: 14 [46720/50048]	Loss: 1.1699
Training Epoch: 14 [46848/50048]	Loss: 1.4730
Training Epoch: 14 [46976/50048]	Loss: 1.2354
Training Epoch: 14 [47104/50048]	Loss: 1.3031
Training Epoch: 14 [47232/50048]	Loss: 1.7376
Training Epoch: 14 [47360/50048]	Loss: 1.3507
Training Epoch: 14 [47488/50048]	Loss: 1.5000
Training Epoch: 14 [47616/50048]	Loss: 1.6385
Training Epoch: 14 [47744/50048]	Loss: 1.4816
Training Epoch: 14 [47872/50048]	Loss: 1.1678
Training Epoch: 14 [48000/50048]	Loss: 1.5697
Training Epoch: 14 [48128/50048]	Loss: 1.5936
Training Epoch: 14 [48256/50048]	Loss: 1.3001
Training Epoch: 14 [48384/50048]	Loss: 1.3054
Training Epoch: 14 [48512/50048]	Loss: 1.3171
Training Epoch: 14 [48640/50048]	Loss: 1.4291
Training Epoch: 14 [48768/50048]	Loss: 1.3650
Training Epoch: 14 [48896/50048]	Loss: 1.3734
Training Epoch: 14 [49024/50048]	Loss: 1.2963
Training Epoch: 14 [49152/50048]	Loss: 1.3809
Training Epoch: 14 [49280/50048]	Loss: 1.5050
Training Epoch: 14 [49408/50048]	Loss: 1.3625
Training Epoch: 14 [49536/50048]	Loss: 1.6503
Training Epoch: 14 [49664/50048]	Loss: 1.2759
Training Epoch: 14 [49792/50048]	Loss: 1.3939
Training Epoch: 14 [49920/50048]	Loss: 1.4695
Training Epoch: 14 [50048/50048]	Loss: 1.6527
Validation Epoch: 14, Average loss: 0.0122, Accuracy: 0.5686
Training Epoch: 15 [128/50048]	Loss: 1.2132
Training Epoch: 15 [256/50048]	Loss: 1.4280
Training Epoch: 15 [384/50048]	Loss: 1.7155
Training Epoch: 15 [512/50048]	Loss: 1.1809
Training Epoch: 15 [640/50048]	Loss: 1.3971
Training Epoch: 15 [768/50048]	Loss: 1.2072
Training Epoch: 15 [896/50048]	Loss: 1.4668
Training Epoch: 15 [1024/50048]	Loss: 1.2897
Training Epoch: 15 [1152/50048]	Loss: 1.5564
Training Epoch: 15 [1280/50048]	Loss: 1.2721
Training Epoch: 15 [1408/50048]	Loss: 1.1744
Training Epoch: 15 [1536/50048]	Loss: 1.3243
Training Epoch: 15 [1664/50048]	Loss: 1.3005
Training Epoch: 15 [1792/50048]	Loss: 1.4403
Training Epoch: 15 [1920/50048]	Loss: 1.4714
Training Epoch: 15 [2048/50048]	Loss: 1.4838
Training Epoch: 15 [2176/50048]	Loss: 1.4618
Training Epoch: 15 [2304/50048]	Loss: 1.3492
Training Epoch: 15 [2432/50048]	Loss: 1.3567
Training Epoch: 15 [2560/50048]	Loss: 1.5746
Training Epoch: 15 [2688/50048]	Loss: 1.3728
Training Epoch: 15 [2816/50048]	Loss: 1.3352
Training Epoch: 15 [2944/50048]	Loss: 1.3498
Training Epoch: 15 [3072/50048]	Loss: 1.1052
Training Epoch: 15 [3200/50048]	Loss: 1.4964
Training Epoch: 15 [3328/50048]	Loss: 1.6574
Training Epoch: 15 [3456/50048]	Loss: 1.4421
Training Epoch: 15 [3584/50048]	Loss: 1.4956
Training Epoch: 15 [3712/50048]	Loss: 1.3940
Training Epoch: 15 [3840/50048]	Loss: 1.4635
Training Epoch: 15 [3968/50048]	Loss: 1.2294
Training Epoch: 15 [4096/50048]	Loss: 1.6740
Training Epoch: 15 [4224/50048]	Loss: 1.5473
Training Epoch: 15 [4352/50048]	Loss: 1.3269
Training Epoch: 15 [4480/50048]	Loss: 1.0935
Training Epoch: 15 [4608/50048]	Loss: 1.1942
Training Epoch: 15 [4736/50048]	Loss: 1.4478
Training Epoch: 15 [4864/50048]	Loss: 1.4431
Training Epoch: 15 [4992/50048]	Loss: 1.4144
Training Epoch: 15 [5120/50048]	Loss: 1.2838
Training Epoch: 15 [5248/50048]	Loss: 1.1537
Training Epoch: 15 [5376/50048]	Loss: 1.4233
Training Epoch: 15 [5504/50048]	Loss: 1.4782
Training Epoch: 15 [5632/50048]	Loss: 1.2623
Training Epoch: 15 [5760/50048]	Loss: 1.3041
Training Epoch: 15 [5888/50048]	Loss: 1.6901
Training Epoch: 15 [6016/50048]	Loss: 1.3032
Training Epoch: 15 [6144/50048]	Loss: 1.3893
Training Epoch: 15 [6272/50048]	Loss: 1.4022
Training Epoch: 15 [6400/50048]	Loss: 1.6074
Training Epoch: 15 [6528/50048]	Loss: 1.3464
Training Epoch: 15 [6656/50048]	Loss: 1.1962
Training Epoch: 15 [6784/50048]	Loss: 1.4835
Training Epoch: 15 [6912/50048]	Loss: 1.7958
Training Epoch: 15 [7040/50048]	Loss: 1.5495
Training Epoch: 15 [7168/50048]	Loss: 1.2633
Training Epoch: 15 [7296/50048]	Loss: 1.5265
Training Epoch: 15 [7424/50048]	Loss: 1.2810
Training Epoch: 15 [7552/50048]	Loss: 1.6438
Training Epoch: 15 [7680/50048]	Loss: 1.2421
Training Epoch: 15 [7808/50048]	Loss: 1.4529
Training Epoch: 15 [7936/50048]	Loss: 1.5338
Training Epoch: 15 [8064/50048]	Loss: 1.5698
Training Epoch: 15 [8192/50048]	Loss: 1.5608
Training Epoch: 15 [8320/50048]	Loss: 1.6321
Training Epoch: 15 [8448/50048]	Loss: 1.4158
Training Epoch: 15 [8576/50048]	Loss: 1.6689
Training Epoch: 15 [8704/50048]	Loss: 1.1239
Training Epoch: 15 [8832/50048]	Loss: 1.2477
Training Epoch: 15 [8960/50048]	Loss: 1.3141
Training Epoch: 15 [9088/50048]	Loss: 1.3539
Training Epoch: 15 [9216/50048]	Loss: 1.3465
Training Epoch: 15 [9344/50048]	Loss: 1.5518
Training Epoch: 15 [9472/50048]	Loss: 1.3869
Training Epoch: 15 [9600/50048]	Loss: 1.5837
Training Epoch: 15 [9728/50048]	Loss: 1.4028
Training Epoch: 15 [9856/50048]	Loss: 1.2295
Training Epoch: 15 [9984/50048]	Loss: 1.3893
Training Epoch: 15 [10112/50048]	Loss: 1.6563
Training Epoch: 15 [10240/50048]	Loss: 1.5230
Training Epoch: 15 [10368/50048]	Loss: 1.2665
Training Epoch: 15 [10496/50048]	Loss: 1.3620
Training Epoch: 15 [10624/50048]	Loss: 1.4175
Training Epoch: 15 [10752/50048]	Loss: 1.4630
Training Epoch: 15 [10880/50048]	Loss: 1.5442
Training Epoch: 15 [11008/50048]	Loss: 1.4351
Training Epoch: 15 [11136/50048]	Loss: 1.3126
Training Epoch: 15 [11264/50048]	Loss: 1.5924
Training Epoch: 15 [11392/50048]	Loss: 1.2081
Training Epoch: 15 [11520/50048]	Loss: 1.3365
Training Epoch: 15 [11648/50048]	Loss: 1.4301
Training Epoch: 15 [11776/50048]	Loss: 1.4812
Training Epoch: 15 [11904/50048]	Loss: 1.3008
Training Epoch: 15 [12032/50048]	Loss: 1.3044
Training Epoch: 15 [12160/50048]	Loss: 1.5908
Training Epoch: 15 [12288/50048]	Loss: 1.5090
Training Epoch: 15 [12416/50048]	Loss: 1.6237
Training Epoch: 15 [12544/50048]	Loss: 1.4010
Training Epoch: 15 [12672/50048]	Loss: 1.3085
Training Epoch: 15 [12800/50048]	Loss: 1.8713
Training Epoch: 15 [12928/50048]	Loss: 1.2441
Training Epoch: 15 [13056/50048]	Loss: 1.4196
Training Epoch: 15 [13184/50048]	Loss: 1.2351
Training Epoch: 15 [13312/50048]	Loss: 1.6488
Training Epoch: 15 [13440/50048]	Loss: 1.1971
Training Epoch: 15 [13568/50048]	Loss: 1.2825
Training Epoch: 15 [13696/50048]	Loss: 1.5661
Training Epoch: 15 [13824/50048]	Loss: 1.4844
Training Epoch: 15 [13952/50048]	Loss: 1.4508
Training Epoch: 15 [14080/50048]	Loss: 1.4514
Training Epoch: 15 [14208/50048]	Loss: 1.5106
Training Epoch: 15 [14336/50048]	Loss: 1.2633
Training Epoch: 15 [14464/50048]	Loss: 1.2533
Training Epoch: 15 [14592/50048]	Loss: 1.2893
Training Epoch: 15 [14720/50048]	Loss: 1.3254
Training Epoch: 15 [14848/50048]	Loss: 1.5633
Training Epoch: 15 [14976/50048]	Loss: 1.3895
Training Epoch: 15 [15104/50048]	Loss: 1.2532
Training Epoch: 15 [15232/50048]	Loss: 1.4196
Training Epoch: 15 [15360/50048]	Loss: 1.5025
Training Epoch: 15 [15488/50048]	Loss: 1.3245
Training Epoch: 15 [15616/50048]	Loss: 1.4114
Training Epoch: 15 [15744/50048]	Loss: 1.3717
Training Epoch: 15 [15872/50048]	Loss: 1.2242
Training Epoch: 15 [16000/50048]	Loss: 1.3711
Training Epoch: 15 [16128/50048]	Loss: 1.3537
Training Epoch: 15 [16256/50048]	Loss: 1.6830
Training Epoch: 15 [16384/50048]	Loss: 1.3703
Training Epoch: 15 [16512/50048]	Loss: 1.4973
Training Epoch: 15 [16640/50048]	Loss: 1.4671
Training Epoch: 15 [16768/50048]	Loss: 1.3909
Training Epoch: 15 [16896/50048]	Loss: 1.4677
Training Epoch: 15 [17024/50048]	Loss: 1.5705
Training Epoch: 15 [17152/50048]	Loss: 1.4388
Training Epoch: 15 [17280/50048]	Loss: 1.2076
Training Epoch: 15 [17408/50048]	Loss: 1.3064
Training Epoch: 15 [17536/50048]	Loss: 1.5325
Training Epoch: 15 [17664/50048]	Loss: 1.3893
Training Epoch: 15 [17792/50048]	Loss: 1.3696
Training Epoch: 15 [17920/50048]	Loss: 1.4639
Training Epoch: 15 [18048/50048]	Loss: 1.4249
Training Epoch: 15 [18176/50048]	Loss: 1.3917
Training Epoch: 15 [18304/50048]	Loss: 1.4119
Training Epoch: 15 [18432/50048]	Loss: 1.3619
Training Epoch: 15 [18560/50048]	Loss: 1.5435
Training Epoch: 15 [18688/50048]	Loss: 1.2881
Training Epoch: 15 [18816/50048]	Loss: 1.3493
Training Epoch: 15 [18944/50048]	Loss: 1.3912
Training Epoch: 15 [19072/50048]	Loss: 1.3087
Training Epoch: 15 [19200/50048]	Loss: 1.2793
Training Epoch: 15 [19328/50048]	Loss: 1.3430
Training Epoch: 15 [19456/50048]	Loss: 1.4365
Training Epoch: 15 [19584/50048]	Loss: 1.2631
Training Epoch: 15 [19712/50048]	Loss: 1.3337
Training Epoch: 15 [19840/50048]	Loss: 1.4935
Training Epoch: 15 [19968/50048]	Loss: 1.2999
Training Epoch: 15 [20096/50048]	Loss: 1.5994
Training Epoch: 15 [20224/50048]	Loss: 1.5173
Training Epoch: 15 [20352/50048]	Loss: 1.3509
Training Epoch: 15 [20480/50048]	Loss: 1.3712
Training Epoch: 15 [20608/50048]	Loss: 1.5160
Training Epoch: 15 [20736/50048]	Loss: 1.4934
Training Epoch: 15 [20864/50048]	Loss: 1.4971
Training Epoch: 15 [20992/50048]	Loss: 1.4093
Training Epoch: 15 [21120/50048]	Loss: 1.3146
Training Epoch: 15 [21248/50048]	Loss: 1.4577
Training Epoch: 15 [21376/50048]	Loss: 1.1123
Training Epoch: 15 [21504/50048]	Loss: 1.4145
Training Epoch: 15 [21632/50048]	Loss: 1.4393
Training Epoch: 15 [21760/50048]	Loss: 1.4694
Training Epoch: 15 [21888/50048]	Loss: 1.2450
Training Epoch: 15 [22016/50048]	Loss: 1.3544
Training Epoch: 15 [22144/50048]	Loss: 1.3471
Training Epoch: 15 [22272/50048]	Loss: 1.4261
Training Epoch: 15 [22400/50048]	Loss: 1.4615
Training Epoch: 15 [22528/50048]	Loss: 1.4037
Training Epoch: 15 [22656/50048]	Loss: 1.4151
Training Epoch: 15 [22784/50048]	Loss: 1.7280
Training Epoch: 15 [22912/50048]	Loss: 1.4031
Training Epoch: 15 [23040/50048]	Loss: 1.3274
Training Epoch: 15 [23168/50048]	Loss: 1.4771
Training Epoch: 15 [23296/50048]	Loss: 1.5635
Training Epoch: 15 [23424/50048]	Loss: 1.5355
Training Epoch: 15 [23552/50048]	Loss: 1.5561
Training Epoch: 15 [23680/50048]	Loss: 1.3169
Training Epoch: 15 [23808/50048]	Loss: 1.2320
Training Epoch: 15 [23936/50048]	Loss: 1.2757
Training Epoch: 15 [24064/50048]	Loss: 1.4112
Training Epoch: 15 [24192/50048]	Loss: 1.4478
Training Epoch: 15 [24320/50048]	Loss: 1.2907
Training Epoch: 15 [24448/50048]	Loss: 1.4077
Training Epoch: 15 [24576/50048]	Loss: 1.4740
Training Epoch: 15 [24704/50048]	Loss: 1.3112
Training Epoch: 15 [24832/50048]	Loss: 1.4933
Training Epoch: 15 [24960/50048]	Loss: 1.4005
Training Epoch: 15 [25088/50048]	Loss: 1.3987
Training Epoch: 15 [25216/50048]	Loss: 1.3048
Training Epoch: 15 [25344/50048]	Loss: 1.5338
Training Epoch: 15 [25472/50048]	Loss: 1.1864
Training Epoch: 15 [25600/50048]	Loss: 1.3731
Training Epoch: 15 [25728/50048]	Loss: 1.5868
Training Epoch: 15 [25856/50048]	Loss: 1.5519
Training Epoch: 15 [25984/50048]	Loss: 1.2879
Training Epoch: 15 [26112/50048]	Loss: 1.4938
Training Epoch: 15 [26240/50048]	Loss: 1.5076
Training Epoch: 15 [26368/50048]	Loss: 1.6447
Training Epoch: 15 [26496/50048]	Loss: 1.2926
Training Epoch: 15 [26624/50048]	Loss: 1.1642
Training Epoch: 15 [26752/50048]	Loss: 1.3361
Training Epoch: 15 [26880/50048]	Loss: 1.5664
Training Epoch: 15 [27008/50048]	Loss: 1.4713
Training Epoch: 15 [27136/50048]	Loss: 1.3102
Training Epoch: 15 [27264/50048]	Loss: 1.6958
Training Epoch: 15 [27392/50048]	Loss: 1.4500
Training Epoch: 15 [27520/50048]	Loss: 1.4562
Training Epoch: 15 [27648/50048]	Loss: 1.2942
Training Epoch: 15 [27776/50048]	Loss: 1.5714
Training Epoch: 15 [27904/50048]	Loss: 1.5689
Training Epoch: 15 [28032/50048]	Loss: 1.4371
Training Epoch: 15 [28160/50048]	Loss: 1.3946
Training Epoch: 15 [28288/50048]	Loss: 1.3038
Training Epoch: 15 [28416/50048]	Loss: 1.2555
Training Epoch: 15 [28544/50048]	Loss: 1.3536
Training Epoch: 15 [28672/50048]	Loss: 1.5169
Training Epoch: 15 [28800/50048]	Loss: 1.2850
Training Epoch: 15 [28928/50048]	Loss: 1.4393
Training Epoch: 15 [29056/50048]	Loss: 1.4407
Training Epoch: 15 [29184/50048]	Loss: 1.5031
Training Epoch: 15 [29312/50048]	Loss: 1.4605
Training Epoch: 15 [29440/50048]	Loss: 1.2852
Training Epoch: 15 [29568/50048]	Loss: 1.3769
Training Epoch: 15 [29696/50048]	Loss: 1.5159
Training Epoch: 15 [29824/50048]	Loss: 1.3100
Training Epoch: 15 [29952/50048]	Loss: 1.3617
Training Epoch: 15 [30080/50048]	Loss: 1.3958
Training Epoch: 15 [30208/50048]	Loss: 1.3831
Training Epoch: 15 [30336/50048]	Loss: 1.6468
Training Epoch: 15 [30464/50048]	Loss: 1.3425
Training Epoch: 15 [30592/50048]	Loss: 1.7322
Training Epoch: 15 [30720/50048]	Loss: 1.3472
Training Epoch: 15 [30848/50048]	Loss: 1.3714
Training Epoch: 15 [30976/50048]	Loss: 1.4000
Training Epoch: 15 [31104/50048]	Loss: 1.3611
Training Epoch: 15 [31232/50048]	Loss: 1.3791
Training Epoch: 15 [31360/50048]	Loss: 1.3398
Training Epoch: 15 [31488/50048]	Loss: 1.2536
Training Epoch: 15 [31616/50048]	Loss: 1.5659
Training Epoch: 15 [31744/50048]	Loss: 1.4204
Training Epoch: 15 [31872/50048]	Loss: 1.3313
Training Epoch: 15 [32000/50048]	Loss: 1.4464
Training Epoch: 15 [32128/50048]	Loss: 1.4371
Training Epoch: 15 [32256/50048]	Loss: 1.3712
Training Epoch: 15 [32384/50048]	Loss: 1.2585
Training Epoch: 15 [32512/50048]	Loss: 1.5667
Training Epoch: 15 [32640/50048]	Loss: 1.3709
Training Epoch: 15 [32768/50048]	Loss: 1.4473
Training Epoch: 15 [32896/50048]	Loss: 1.5793
Training Epoch: 15 [33024/50048]	Loss: 1.4979
Training Epoch: 15 [33152/50048]	Loss: 1.5792
Training Epoch: 15 [33280/50048]	Loss: 1.6094
Training Epoch: 15 [33408/50048]	Loss: 1.4537
Training Epoch: 15 [33536/50048]	Loss: 1.2902
Training Epoch: 15 [33664/50048]	Loss: 1.4288
Training Epoch: 15 [33792/50048]	Loss: 1.5957
Training Epoch: 15 [33920/50048]	Loss: 1.3705
Training Epoch: 15 [34048/50048]	Loss: 1.2937
Training Epoch: 15 [34176/50048]	Loss: 1.3768
Training Epoch: 15 [34304/50048]	Loss: 1.2587
Training Epoch: 15 [34432/50048]	Loss: 1.4890
Training Epoch: 15 [34560/50048]	Loss: 1.3849
Training Epoch: 15 [34688/50048]	Loss: 1.2687
Training Epoch: 15 [34816/50048]	Loss: 1.4860
Training Epoch: 15 [34944/50048]	Loss: 1.4216
Training Epoch: 15 [35072/50048]	Loss: 1.8148
Training Epoch: 15 [35200/50048]	Loss: 1.3301
Training Epoch: 15 [35328/50048]	Loss: 1.3998
Training Epoch: 15 [35456/50048]	Loss: 1.5387
Training Epoch: 15 [35584/50048]	Loss: 1.6085
Training Epoch: 15 [35712/50048]	Loss: 1.2951
Training Epoch: 15 [35840/50048]	Loss: 1.2725
Training Epoch: 15 [35968/50048]	Loss: 1.6232
Training Epoch: 15 [36096/50048]	Loss: 1.5016
Training Epoch: 15 [36224/50048]	Loss: 1.3289
Training Epoch: 15 [36352/50048]	Loss: 1.3447
Training Epoch: 15 [36480/50048]	Loss: 1.4255
Training Epoch: 15 [36608/50048]	Loss: 1.5368
Training Epoch: 15 [36736/50048]	Loss: 1.5507
Training Epoch: 15 [36864/50048]	Loss: 1.5021
Training Epoch: 15 [36992/50048]	Loss: 1.3809
Training Epoch: 15 [37120/50048]	Loss: 1.1721
Training Epoch: 15 [37248/50048]	Loss: 1.3367
Training Epoch: 15 [37376/50048]	Loss: 1.3527
Training Epoch: 15 [37504/50048]	Loss: 1.4803
Training Epoch: 15 [37632/50048]	Loss: 1.3032
Training Epoch: 15 [37760/50048]	Loss: 1.3202
Training Epoch: 15 [37888/50048]	Loss: 1.4234
Training Epoch: 15 [38016/50048]	Loss: 1.3741
Training Epoch: 15 [38144/50048]	Loss: 1.4849
Training Epoch: 15 [38272/50048]	Loss: 1.4010
Training Epoch: 15 [38400/50048]	Loss: 1.3407
Training Epoch: 15 [38528/50048]	Loss: 1.3528
Training Epoch: 15 [38656/50048]	Loss: 1.2811
Training Epoch: 15 [38784/50048]	Loss: 1.4972
Training Epoch: 15 [38912/50048]	Loss: 1.3870
Training Epoch: 15 [39040/50048]	Loss: 1.3581
Training Epoch: 15 [39168/50048]	Loss: 1.4290
Training Epoch: 15 [39296/50048]	Loss: 1.5760
Training Epoch: 15 [39424/50048]	Loss: 1.3446
Training Epoch: 15 [39552/50048]	Loss: 1.2526
Training Epoch: 15 [39680/50048]	Loss: 1.5075
Training Epoch: 15 [39808/50048]	Loss: 1.1772
Training Epoch: 15 [39936/50048]	Loss: 1.3981
Training Epoch: 15 [40064/50048]	Loss: 1.5042
Training Epoch: 15 [40192/50048]	Loss: 1.2053
Training Epoch: 15 [40320/50048]	Loss: 1.3559
Training Epoch: 15 [40448/50048]	Loss: 1.4951
Training Epoch: 15 [40576/50048]	Loss: 1.3913
Training Epoch: 15 [40704/50048]	Loss: 1.6704
Training Epoch: 15 [40832/50048]	Loss: 1.3160
Training Epoch: 15 [40960/50048]	Loss: 1.6225
Training Epoch: 15 [41088/50048]	Loss: 1.3042
Training Epoch: 15 [41216/50048]	Loss: 1.4763
Training Epoch: 15 [41344/50048]	Loss: 1.4927
Training Epoch: 15 [41472/50048]	Loss: 1.5451
Training Epoch: 15 [41600/50048]	Loss: 1.5543
Training Epoch: 15 [41728/50048]	Loss: 1.3464
Training Epoch: 15 [41856/50048]	Loss: 1.4744
Training Epoch: 15 [41984/50048]	Loss: 1.3477
Training Epoch: 15 [42112/50048]	Loss: 1.0672
Training Epoch: 15 [42240/50048]	Loss: 1.2189
Training Epoch: 15 [42368/50048]	Loss: 1.4290
Training Epoch: 15 [42496/50048]	Loss: 1.1623
Training Epoch: 15 [42624/50048]	Loss: 1.3513
Training Epoch: 15 [42752/50048]	Loss: 1.4734
Training Epoch: 15 [42880/50048]	Loss: 1.6573
Training Epoch: 15 [43008/50048]	Loss: 1.4722
Training Epoch: 15 [43136/50048]	Loss: 1.4249
Training Epoch: 15 [43264/50048]	Loss: 1.4543
Training Epoch: 15 [43392/50048]	Loss: 1.4259
Training Epoch: 15 [43520/50048]	Loss: 1.2701
Training Epoch: 15 [43648/50048]	Loss: 1.3446
Training Epoch: 15 [43776/50048]	Loss: 1.4784
Training Epoch: 15 [43904/50048]	Loss: 1.0790
Training Epoch: 15 [44032/50048]	Loss: 1.5174
Training Epoch: 15 [44160/50048]	Loss: 1.5573
Training Epoch: 15 [44288/50048]	Loss: 1.6465
Training Epoch: 15 [44416/50048]	Loss: 1.4795
Training Epoch: 15 [44544/50048]	Loss: 1.2802
Training Epoch: 15 [44672/50048]	Loss: 1.2390
Training Epoch: 15 [44800/50048]	Loss: 1.4694
Training Epoch: 15 [44928/50048]	Loss: 1.3751
Training Epoch: 15 [45056/50048]	Loss: 1.4697
Training Epoch: 15 [45184/50048]	Loss: 1.4279
Training Epoch: 15 [45312/50048]	Loss: 1.5229
Training Epoch: 15 [45440/50048]	Loss: 1.3385
Training Epoch: 15 [45568/50048]	Loss: 1.6970
Training Epoch: 15 [45696/50048]	Loss: 1.4789
Training Epoch: 15 [45824/50048]	Loss: 1.2367
Training Epoch: 15 [45952/50048]	Loss: 1.3674
Training Epoch: 15 [46080/50048]	Loss: 1.3945
Training Epoch: 15 [46208/50048]	Loss: 1.1063
Training Epoch: 15 [46336/50048]	Loss: 1.5639
Training Epoch: 15 [46464/50048]	Loss: 1.2143
Training Epoch: 15 [46592/50048]	Loss: 1.7516
Training Epoch: 15 [46720/50048]	Loss: 1.2686
Training Epoch: 15 [46848/50048]	Loss: 1.3623
Training Epoch: 15 [46976/50048]	Loss: 1.2246
Training Epoch: 15 [47104/50048]	Loss: 1.2185
Training Epoch: 15 [47232/50048]	Loss: 1.1710
Training Epoch: 15 [47360/50048]	Loss: 1.5313
Training Epoch: 15 [47488/50048]	Loss: 1.6518
Training Epoch: 15 [47616/50048]	Loss: 1.3070
Training Epoch: 15 [47744/50048]	Loss: 1.5596
Training Epoch: 15 [47872/50048]	Loss: 1.1482
Training Epoch: 15 [48000/50048]	Loss: 1.6464
Training Epoch: 15 [48128/50048]	Loss: 1.0888
Training Epoch: 15 [48256/50048]	Loss: 1.4360
Training Epoch: 15 [48384/50048]	Loss: 1.2428
Training Epoch: 15 [48512/50048]	Loss: 1.4330
Training Epoch: 15 [48640/50048]	Loss: 1.5702
Training Epoch: 15 [48768/50048]	Loss: 1.4230
Training Epoch: 15 [48896/50048]	Loss: 1.2189
Training Epoch: 15 [49024/50048]	Loss: 1.2882
Training Epoch: 15 [49152/50048]	Loss: 1.5169
Training Epoch: 15 [49280/50048]	Loss: 1.3688
Training Epoch: 15 [49408/50048]	Loss: 1.6591
Training Epoch: 15 [49536/50048]	Loss: 1.3690
Training Epoch: 15 [49664/50048]	Loss: 1.3389
Training Epoch: 15 [49792/50048]	Loss: 1.3964
Training Epoch: 15 [49920/50048]	Loss: 1.3697
Training Epoch: 15 [50048/50048]	Loss: 1.3449
Validation Epoch: 15, Average loss: 0.0121, Accuracy: 0.5720
Training Epoch: 16 [128/50048]	Loss: 1.3713
Training Epoch: 16 [256/50048]	Loss: 1.5682
Training Epoch: 16 [384/50048]	Loss: 1.4364
Training Epoch: 16 [512/50048]	Loss: 1.2294
Training Epoch: 16 [640/50048]	Loss: 1.4498
Training Epoch: 16 [768/50048]	Loss: 1.3538
Training Epoch: 16 [896/50048]	Loss: 1.4704
Training Epoch: 16 [1024/50048]	Loss: 1.3130
Training Epoch: 16 [1152/50048]	Loss: 1.4851
Training Epoch: 16 [1280/50048]	Loss: 1.7074
Training Epoch: 16 [1408/50048]	Loss: 1.3557
Training Epoch: 16 [1536/50048]	Loss: 1.4048
Training Epoch: 16 [1664/50048]	Loss: 1.3874
Training Epoch: 16 [1792/50048]	Loss: 1.1138
Training Epoch: 16 [1920/50048]	Loss: 1.3501
Training Epoch: 16 [2048/50048]	Loss: 1.3123
Training Epoch: 16 [2176/50048]	Loss: 1.4266
Training Epoch: 16 [2304/50048]	Loss: 1.3211
Training Epoch: 16 [2432/50048]	Loss: 1.3685
Training Epoch: 16 [2560/50048]	Loss: 1.3877
Training Epoch: 16 [2688/50048]	Loss: 1.4992
Training Epoch: 16 [2816/50048]	Loss: 1.2681
Training Epoch: 16 [2944/50048]	Loss: 1.4662
Training Epoch: 16 [3072/50048]	Loss: 1.1913
Training Epoch: 16 [3200/50048]	Loss: 1.2788
Training Epoch: 16 [3328/50048]	Loss: 1.5784
Training Epoch: 16 [3456/50048]	Loss: 1.1323
Training Epoch: 16 [3584/50048]	Loss: 1.4661
Training Epoch: 16 [3712/50048]	Loss: 1.4042
Training Epoch: 16 [3840/50048]	Loss: 1.3092
Training Epoch: 16 [3968/50048]	Loss: 1.4669
Training Epoch: 16 [4096/50048]	Loss: 1.3514
Training Epoch: 16 [4224/50048]	Loss: 1.3173
Training Epoch: 16 [4352/50048]	Loss: 1.2614
Training Epoch: 16 [4480/50048]	Loss: 1.5590
Training Epoch: 16 [4608/50048]	Loss: 1.2836
Training Epoch: 16 [4736/50048]	Loss: 1.3773
Training Epoch: 16 [4864/50048]	Loss: 1.1161
Training Epoch: 16 [4992/50048]	Loss: 1.2940
Training Epoch: 16 [5120/50048]	Loss: 1.2920
Training Epoch: 16 [5248/50048]	Loss: 1.2188
Training Epoch: 16 [5376/50048]	Loss: 1.5327
Training Epoch: 16 [5504/50048]	Loss: 1.5405
Training Epoch: 16 [5632/50048]	Loss: 1.4123
Training Epoch: 16 [5760/50048]	Loss: 1.0613
Training Epoch: 16 [5888/50048]	Loss: 1.3021
Training Epoch: 16 [6016/50048]	Loss: 1.1814
Training Epoch: 16 [6144/50048]	Loss: 1.0553
Training Epoch: 16 [6272/50048]	Loss: 1.3989
Training Epoch: 16 [6400/50048]	Loss: 1.4847
Training Epoch: 16 [6528/50048]	Loss: 1.5533
Training Epoch: 16 [6656/50048]	Loss: 1.4191
Training Epoch: 16 [6784/50048]	Loss: 1.2160
Training Epoch: 16 [6912/50048]	Loss: 1.1346
Training Epoch: 16 [7040/50048]	Loss: 1.1012
Training Epoch: 16 [7168/50048]	Loss: 1.3801
Training Epoch: 16 [7296/50048]	Loss: 1.2785
Training Epoch: 16 [7424/50048]	Loss: 1.4006
Training Epoch: 16 [7552/50048]	Loss: 1.1661
Training Epoch: 16 [7680/50048]	Loss: 1.2732
Training Epoch: 16 [7808/50048]	Loss: 1.5049
Training Epoch: 16 [7936/50048]	Loss: 1.3116
Training Epoch: 16 [8064/50048]	Loss: 1.5510
Training Epoch: 16 [8192/50048]	Loss: 1.4752
Training Epoch: 16 [8320/50048]	Loss: 1.2112
Training Epoch: 16 [8448/50048]	Loss: 1.5590
Training Epoch: 16 [8576/50048]	Loss: 1.4575
Training Epoch: 16 [8704/50048]	Loss: 1.4317
Training Epoch: 16 [8832/50048]	Loss: 1.1957
Training Epoch: 16 [8960/50048]	Loss: 1.5342
Training Epoch: 16 [9088/50048]	Loss: 1.0828
Training Epoch: 16 [9216/50048]	Loss: 1.4243
Training Epoch: 16 [9344/50048]	Loss: 1.4247
Training Epoch: 16 [9472/50048]	Loss: 1.4841
Training Epoch: 16 [9600/50048]	Loss: 1.3411
Training Epoch: 16 [9728/50048]	Loss: 1.2642
Training Epoch: 16 [9856/50048]	Loss: 1.2637
Training Epoch: 16 [9984/50048]	Loss: 1.3911
Training Epoch: 16 [10112/50048]	Loss: 1.5119
Training Epoch: 16 [10240/50048]	Loss: 1.4033
Training Epoch: 16 [10368/50048]	Loss: 1.2888
Training Epoch: 16 [10496/50048]	Loss: 1.3477
Training Epoch: 16 [10624/50048]	Loss: 1.3469
Training Epoch: 16 [10752/50048]	Loss: 1.3269
Training Epoch: 16 [10880/50048]	Loss: 1.4410
Training Epoch: 16 [11008/50048]	Loss: 1.2473
Training Epoch: 16 [11136/50048]	Loss: 1.6785
Training Epoch: 16 [11264/50048]	Loss: 1.3794
Training Epoch: 16 [11392/50048]	Loss: 1.2204
Training Epoch: 16 [11520/50048]	Loss: 1.4432
Training Epoch: 16 [11648/50048]	Loss: 1.2473
Training Epoch: 16 [11776/50048]	Loss: 1.5230
Training Epoch: 16 [11904/50048]	Loss: 1.2616
Training Epoch: 16 [12032/50048]	Loss: 1.4988
Training Epoch: 16 [12160/50048]	Loss: 1.3813
Training Epoch: 16 [12288/50048]	Loss: 1.3541
Training Epoch: 16 [12416/50048]	Loss: 1.4180
Training Epoch: 16 [12544/50048]	Loss: 1.3685
Training Epoch: 16 [12672/50048]	Loss: 1.3614
Training Epoch: 16 [12800/50048]	Loss: 1.3975
Training Epoch: 16 [12928/50048]	Loss: 1.1054
Training Epoch: 16 [13056/50048]	Loss: 1.4256
Training Epoch: 16 [13184/50048]	Loss: 1.4141
Training Epoch: 16 [13312/50048]	Loss: 1.4197
Training Epoch: 16 [13440/50048]	Loss: 1.3381
Training Epoch: 16 [13568/50048]	Loss: 1.4465
Training Epoch: 16 [13696/50048]	Loss: 1.2472
Training Epoch: 16 [13824/50048]	Loss: 1.3441
Training Epoch: 16 [13952/50048]	Loss: 1.4378
Training Epoch: 16 [14080/50048]	Loss: 1.3351
Training Epoch: 16 [14208/50048]	Loss: 1.1448
Training Epoch: 16 [14336/50048]	Loss: 1.1906
Training Epoch: 16 [14464/50048]	Loss: 1.4335
Training Epoch: 16 [14592/50048]	Loss: 1.3745
Training Epoch: 16 [14720/50048]	Loss: 1.2278
Training Epoch: 16 [14848/50048]	Loss: 1.3955
Training Epoch: 16 [14976/50048]	Loss: 1.4360
Training Epoch: 16 [15104/50048]	Loss: 1.5420
Training Epoch: 16 [15232/50048]	Loss: 1.4347
Training Epoch: 16 [15360/50048]	Loss: 1.4114
Training Epoch: 16 [15488/50048]	Loss: 1.3846
Training Epoch: 16 [15616/50048]	Loss: 1.2246
Training Epoch: 16 [15744/50048]	Loss: 1.4578
Training Epoch: 16 [15872/50048]	Loss: 1.3357
Training Epoch: 16 [16000/50048]	Loss: 1.3716
Training Epoch: 16 [16128/50048]	Loss: 1.4531
Training Epoch: 16 [16256/50048]	Loss: 1.2412
Training Epoch: 16 [16384/50048]	Loss: 1.1567
Training Epoch: 16 [16512/50048]	Loss: 1.5027
Training Epoch: 16 [16640/50048]	Loss: 1.4760
Training Epoch: 16 [16768/50048]	Loss: 1.2679
Training Epoch: 16 [16896/50048]	Loss: 1.4963
Training Epoch: 16 [17024/50048]	Loss: 1.4000
Training Epoch: 16 [17152/50048]	Loss: 1.3346
Training Epoch: 16 [17280/50048]	Loss: 1.4715
Training Epoch: 16 [17408/50048]	Loss: 1.4673
Training Epoch: 16 [17536/50048]	Loss: 1.5185
Training Epoch: 16 [17664/50048]	Loss: 1.5733
Training Epoch: 16 [17792/50048]	Loss: 1.4470
Training Epoch: 16 [17920/50048]	Loss: 1.5231
Training Epoch: 16 [18048/50048]	Loss: 1.2969
Training Epoch: 16 [18176/50048]	Loss: 1.2169
Training Epoch: 16 [18304/50048]	Loss: 1.2073
Training Epoch: 16 [18432/50048]	Loss: 1.5929
Training Epoch: 16 [18560/50048]	Loss: 1.4416
Training Epoch: 16 [18688/50048]	Loss: 1.2034
Training Epoch: 16 [18816/50048]	Loss: 1.3820
Training Epoch: 16 [18944/50048]	Loss: 1.2987
Training Epoch: 16 [19072/50048]	Loss: 1.4137
Training Epoch: 16 [19200/50048]	Loss: 1.7108
Training Epoch: 16 [19328/50048]	Loss: 1.1958
Training Epoch: 16 [19456/50048]	Loss: 1.6154
Training Epoch: 16 [19584/50048]	Loss: 1.3081
Training Epoch: 16 [19712/50048]	Loss: 1.3968
Training Epoch: 16 [19840/50048]	Loss: 1.4387
Training Epoch: 16 [19968/50048]	Loss: 1.5009
Training Epoch: 16 [20096/50048]	Loss: 1.1729
Training Epoch: 16 [20224/50048]	Loss: 1.2935
Training Epoch: 16 [20352/50048]	Loss: 1.2059
Training Epoch: 16 [20480/50048]	Loss: 1.4011
Training Epoch: 16 [20608/50048]	Loss: 1.3496
Training Epoch: 16 [20736/50048]	Loss: 1.4899
Training Epoch: 16 [20864/50048]	Loss: 1.4413
Training Epoch: 16 [20992/50048]	Loss: 1.2368
Training Epoch: 16 [21120/50048]	Loss: 1.4619
Training Epoch: 16 [21248/50048]	Loss: 1.2328
Training Epoch: 16 [21376/50048]	Loss: 1.3885
Training Epoch: 16 [21504/50048]	Loss: 1.2768
Training Epoch: 16 [21632/50048]	Loss: 1.4264
Training Epoch: 16 [21760/50048]	Loss: 1.4682
Training Epoch: 16 [21888/50048]	Loss: 1.3813
Training Epoch: 16 [22016/50048]	Loss: 1.5268
Training Epoch: 16 [22144/50048]	Loss: 1.2858
Training Epoch: 16 [22272/50048]	Loss: 1.2205
Training Epoch: 16 [22400/50048]	Loss: 1.3792
Training Epoch: 16 [22528/50048]	Loss: 1.3093
Training Epoch: 16 [22656/50048]	Loss: 1.3074
Training Epoch: 16 [22784/50048]	Loss: 1.3293
Training Epoch: 16 [22912/50048]	Loss: 1.4060
Training Epoch: 16 [23040/50048]	Loss: 1.4507
Training Epoch: 16 [23168/50048]	Loss: 1.4258
Training Epoch: 16 [23296/50048]	Loss: 1.3658
Training Epoch: 16 [23424/50048]	Loss: 1.6225
Training Epoch: 16 [23552/50048]	Loss: 1.4565
Training Epoch: 16 [23680/50048]	Loss: 1.2574
Training Epoch: 16 [23808/50048]	Loss: 1.4879
Training Epoch: 16 [23936/50048]	Loss: 1.4912
Training Epoch: 16 [24064/50048]	Loss: 1.3595
Training Epoch: 16 [24192/50048]	Loss: 1.4653
Training Epoch: 16 [24320/50048]	Loss: 1.4439
Training Epoch: 16 [24448/50048]	Loss: 1.3828
Training Epoch: 16 [24576/50048]	Loss: 1.6627
Training Epoch: 16 [24704/50048]	Loss: 1.3802
Training Epoch: 16 [24832/50048]	Loss: 1.4244
Training Epoch: 16 [24960/50048]	Loss: 1.4382
Training Epoch: 16 [25088/50048]	Loss: 1.3290
Training Epoch: 16 [25216/50048]	Loss: 1.3367
Training Epoch: 16 [25344/50048]	Loss: 1.2826
Training Epoch: 16 [25472/50048]	Loss: 1.3316
Training Epoch: 16 [25600/50048]	Loss: 1.3968
Training Epoch: 16 [25728/50048]	Loss: 1.2412
Training Epoch: 16 [25856/50048]	Loss: 1.5865
Training Epoch: 16 [25984/50048]	Loss: 1.4052
Training Epoch: 16 [26112/50048]	Loss: 1.3628
Training Epoch: 16 [26240/50048]	Loss: 1.6569
Training Epoch: 16 [26368/50048]	Loss: 1.3987
Training Epoch: 16 [26496/50048]	Loss: 1.3176
Training Epoch: 16 [26624/50048]	Loss: 1.5157
Training Epoch: 16 [26752/50048]	Loss: 1.6911
Training Epoch: 16 [26880/50048]	Loss: 1.2185
Training Epoch: 16 [27008/50048]	Loss: 1.7384
Training Epoch: 16 [27136/50048]	Loss: 1.6104
Training Epoch: 16 [27264/50048]	Loss: 1.3533
Training Epoch: 16 [27392/50048]	Loss: 1.2138
Training Epoch: 16 [27520/50048]	Loss: 1.5035
Training Epoch: 16 [27648/50048]	Loss: 1.2193
Training Epoch: 16 [27776/50048]	Loss: 1.4230
Training Epoch: 16 [27904/50048]	Loss: 1.4805
Training Epoch: 16 [28032/50048]	Loss: 1.2604
Training Epoch: 16 [28160/50048]	Loss: 1.1137
Training Epoch: 16 [28288/50048]	Loss: 1.2784
Training Epoch: 16 [28416/50048]	Loss: 1.3991
Training Epoch: 16 [28544/50048]	Loss: 1.2039
Training Epoch: 16 [28672/50048]	Loss: 1.5166
Training Epoch: 16 [28800/50048]	Loss: 1.4020
Training Epoch: 16 [28928/50048]	Loss: 1.6860
Training Epoch: 16 [29056/50048]	Loss: 1.6326
Training Epoch: 16 [29184/50048]	Loss: 1.3439
Training Epoch: 16 [29312/50048]	Loss: 1.4377
Training Epoch: 16 [29440/50048]	Loss: 1.6151
Training Epoch: 16 [29568/50048]	Loss: 1.4549
Training Epoch: 16 [29696/50048]	Loss: 1.2436
Training Epoch: 16 [29824/50048]	Loss: 1.2158
Training Epoch: 16 [29952/50048]	Loss: 1.3764
Training Epoch: 16 [30080/50048]	Loss: 1.3974
Training Epoch: 16 [30208/50048]	Loss: 1.2871
Training Epoch: 16 [30336/50048]	Loss: 1.4304
Training Epoch: 16 [30464/50048]	Loss: 1.2945
Training Epoch: 16 [30592/50048]	Loss: 1.4140
Training Epoch: 16 [30720/50048]	Loss: 1.3718
Training Epoch: 16 [30848/50048]	Loss: 1.1612
Training Epoch: 16 [30976/50048]	Loss: 1.2459
Training Epoch: 16 [31104/50048]	Loss: 1.2846
Training Epoch: 16 [31232/50048]	Loss: 1.3595
Training Epoch: 16 [31360/50048]	Loss: 1.5130
Training Epoch: 16 [31488/50048]	Loss: 1.4052
Training Epoch: 16 [31616/50048]	Loss: 1.3332
Training Epoch: 16 [31744/50048]	Loss: 1.5135
Training Epoch: 16 [31872/50048]	Loss: 1.1659
Training Epoch: 16 [32000/50048]	Loss: 1.6565
Training Epoch: 16 [32128/50048]	Loss: 1.5216
Training Epoch: 16 [32256/50048]	Loss: 1.3745
Training Epoch: 16 [32384/50048]	Loss: 1.3731
Training Epoch: 16 [32512/50048]	Loss: 1.5842
Training Epoch: 16 [32640/50048]	Loss: 1.3033
Training Epoch: 16 [32768/50048]	Loss: 1.5438
Training Epoch: 16 [32896/50048]	Loss: 1.3439
Training Epoch: 16 [33024/50048]	Loss: 1.3547
Training Epoch: 16 [33152/50048]	Loss: 1.1990
Training Epoch: 16 [33280/50048]	Loss: 1.3583
Training Epoch: 16 [33408/50048]	Loss: 1.3696
Training Epoch: 16 [33536/50048]	Loss: 1.3452
Training Epoch: 16 [33664/50048]	Loss: 1.2936
Training Epoch: 16 [33792/50048]	Loss: 1.3446
Training Epoch: 16 [33920/50048]	Loss: 1.7621
Training Epoch: 16 [34048/50048]	Loss: 1.3471
Training Epoch: 16 [34176/50048]	Loss: 1.3802
Training Epoch: 16 [34304/50048]	Loss: 1.1465
Training Epoch: 16 [34432/50048]	Loss: 1.4177
Training Epoch: 16 [34560/50048]	Loss: 1.2533
Training Epoch: 16 [34688/50048]	Loss: 1.3755
Training Epoch: 16 [34816/50048]	Loss: 1.5240
Training Epoch: 16 [34944/50048]	Loss: 1.5357
Training Epoch: 16 [35072/50048]	Loss: 1.3324
Training Epoch: 16 [35200/50048]	Loss: 1.6519
Training Epoch: 16 [35328/50048]	Loss: 1.4875
Training Epoch: 16 [35456/50048]	Loss: 1.5625
Training Epoch: 16 [35584/50048]	Loss: 1.1257
Training Epoch: 16 [35712/50048]	Loss: 1.3794
Training Epoch: 16 [35840/50048]	Loss: 1.3244
Training Epoch: 16 [35968/50048]	Loss: 1.3614
Training Epoch: 16 [36096/50048]	Loss: 1.3925
Training Epoch: 16 [36224/50048]	Loss: 1.5002
Training Epoch: 16 [36352/50048]	Loss: 1.1460
Training Epoch: 16 [36480/50048]	Loss: 1.2884
Training Epoch: 16 [36608/50048]	Loss: 1.3253
Training Epoch: 16 [36736/50048]	Loss: 1.4115
Training Epoch: 16 [36864/50048]	Loss: 1.7985
Training Epoch: 16 [36992/50048]	Loss: 1.3670
Training Epoch: 16 [37120/50048]	Loss: 1.1826
Training Epoch: 16 [37248/50048]	Loss: 1.4112
Training Epoch: 16 [37376/50048]	Loss: 1.5153
Training Epoch: 16 [37504/50048]	Loss: 1.3136
Training Epoch: 16 [37632/50048]	Loss: 1.0652
Training Epoch: 16 [37760/50048]	Loss: 1.3578
Training Epoch: 16 [37888/50048]	Loss: 1.3921
Training Epoch: 16 [38016/50048]	Loss: 1.3542
Training Epoch: 16 [38144/50048]	Loss: 1.2547
Training Epoch: 16 [38272/50048]	Loss: 1.2320
Training Epoch: 16 [38400/50048]	Loss: 1.3593
Training Epoch: 16 [38528/50048]	Loss: 1.4354
Training Epoch: 16 [38656/50048]	Loss: 1.1492
Training Epoch: 16 [38784/50048]	Loss: 1.2770
Training Epoch: 16 [38912/50048]	Loss: 1.4327
Training Epoch: 16 [39040/50048]	Loss: 1.4905
Training Epoch: 16 [39168/50048]	Loss: 1.3438
Training Epoch: 16 [39296/50048]	Loss: 1.3311
Training Epoch: 16 [39424/50048]	Loss: 1.4381
Training Epoch: 16 [39552/50048]	Loss: 1.4298
Training Epoch: 16 [39680/50048]	Loss: 1.4688
Training Epoch: 16 [39808/50048]	Loss: 1.5994
Training Epoch: 16 [39936/50048]	Loss: 1.2353
Training Epoch: 16 [40064/50048]	Loss: 1.3616
Training Epoch: 16 [40192/50048]	Loss: 1.7801
Training Epoch: 16 [40320/50048]	Loss: 1.1381
Training Epoch: 16 [40448/50048]	Loss: 1.4457
Training Epoch: 16 [40576/50048]	Loss: 1.3352
Training Epoch: 16 [40704/50048]	Loss: 1.6044
Training Epoch: 16 [40832/50048]	Loss: 1.3195
Training Epoch: 16 [40960/50048]	Loss: 1.3903
Training Epoch: 16 [41088/50048]	Loss: 1.3948
Training Epoch: 16 [41216/50048]	Loss: 1.2103
Training Epoch: 16 [41344/50048]	Loss: 1.2822
Training Epoch: 16 [41472/50048]	Loss: 1.5873
Training Epoch: 16 [41600/50048]	Loss: 1.2926
Training Epoch: 16 [41728/50048]	Loss: 1.4190
Training Epoch: 16 [41856/50048]	Loss: 1.3079
Training Epoch: 16 [41984/50048]	Loss: 1.2621
Training Epoch: 16 [42112/50048]	Loss: 1.4713
Training Epoch: 16 [42240/50048]	Loss: 1.6337
Training Epoch: 16 [42368/50048]	Loss: 1.3603
Training Epoch: 16 [42496/50048]	Loss: 1.4171
Training Epoch: 16 [42624/50048]	Loss: 1.4911
Training Epoch: 16 [42752/50048]	Loss: 1.3671
Training Epoch: 16 [42880/50048]	Loss: 1.3005
Training Epoch: 16 [43008/50048]	Loss: 1.1670
Training Epoch: 16 [43136/50048]	Loss: 1.3914
Training Epoch: 16 [43264/50048]	Loss: 1.4406
Training Epoch: 16 [43392/50048]	Loss: 1.3643
Training Epoch: 16 [43520/50048]	Loss: 1.3062
Training Epoch: 16 [43648/50048]	Loss: 1.5737
Training Epoch: 16 [43776/50048]	Loss: 1.2224
Training Epoch: 16 [43904/50048]	Loss: 1.5843
Training Epoch: 16 [44032/50048]	Loss: 1.1929
Training Epoch: 16 [44160/50048]	Loss: 1.4397
Training Epoch: 16 [44288/50048]	Loss: 1.5322
Training Epoch: 16 [44416/50048]	Loss: 1.2999
Training Epoch: 16 [44544/50048]	Loss: 1.2738
Training Epoch: 16 [44672/50048]	Loss: 1.5760
Training Epoch: 16 [44800/50048]	Loss: 1.3202
Training Epoch: 16 [44928/50048]	Loss: 1.1277
Training Epoch: 16 [45056/50048]	Loss: 1.3678
Training Epoch: 16 [45184/50048]	Loss: 1.2850
Training Epoch: 16 [45312/50048]	Loss: 1.2975
Training Epoch: 16 [45440/50048]	Loss: 1.2558
Training Epoch: 16 [45568/50048]	Loss: 1.3684
Training Epoch: 16 [45696/50048]	Loss: 1.4507
Training Epoch: 16 [45824/50048]	Loss: 1.2793
Training Epoch: 16 [45952/50048]	Loss: 1.3270
Training Epoch: 16 [46080/50048]	Loss: 1.5725
Training Epoch: 16 [46208/50048]	Loss: 1.4282
Training Epoch: 16 [46336/50048]	Loss: 1.4969
Training Epoch: 16 [46464/50048]	Loss: 1.2975
Training Epoch: 16 [46592/50048]	Loss: 1.1896
Training Epoch: 16 [46720/50048]	Loss: 1.5517
Training Epoch: 16 [46848/50048]	Loss: 1.4146
Training Epoch: 16 [46976/50048]	Loss: 1.4684
Training Epoch: 16 [47104/50048]	Loss: 1.1525
Training Epoch: 16 [47232/50048]	Loss: 1.0272
Training Epoch: 16 [47360/50048]	Loss: 1.3745
Training Epoch: 16 [47488/50048]	Loss: 1.3416
Training Epoch: 16 [47616/50048]	Loss: 1.3163
Training Epoch: 16 [47744/50048]	Loss: 1.5772
Training Epoch: 16 [47872/50048]	Loss: 1.4063
Training Epoch: 16 [48000/50048]	Loss: 1.6649
Training Epoch: 16 [48128/50048]	Loss: 1.2079
Training Epoch: 16 [48256/50048]	Loss: 1.3849
Training Epoch: 16 [48384/50048]	Loss: 1.8386
Training Epoch: 16 [48512/50048]	Loss: 1.5924
Training Epoch: 16 [48640/50048]	Loss: 1.4619
Training Epoch: 16 [48768/50048]	Loss: 1.7281
Training Epoch: 16 [48896/50048]	Loss: 1.4252
Training Epoch: 16 [49024/50048]	Loss: 1.2116
Training Epoch: 16 [49152/50048]	Loss: 1.2860
Training Epoch: 16 [49280/50048]	Loss: 1.4231
Training Epoch: 16 [49408/50048]	Loss: 1.5334
Training Epoch: 16 [49536/50048]	Loss: 1.4130
Training Epoch: 16 [49664/50048]	Loss: 1.2889
Training Epoch: 16 [49792/50048]	Loss: 1.7415
Training Epoch: 16 [49920/50048]	Loss: 1.4697
Training Epoch: 16 [50048/50048]	Loss: 1.5879
Validation Epoch: 16, Average loss: 0.0121, Accuracy: 0.5760
Training Epoch: 17 [128/50048]	Loss: 1.5993
Training Epoch: 17 [256/50048]	Loss: 1.4483
Training Epoch: 17 [384/50048]	Loss: 1.2760
Training Epoch: 17 [512/50048]	Loss: 1.3869
Training Epoch: 17 [640/50048]	Loss: 1.4504
Training Epoch: 17 [768/50048]	Loss: 1.2515
Training Epoch: 17 [896/50048]	Loss: 1.4217
Training Epoch: 17 [1024/50048]	Loss: 1.5560
Training Epoch: 17 [1152/50048]	Loss: 1.4910
Training Epoch: 17 [1280/50048]	Loss: 1.3427
Training Epoch: 17 [1408/50048]	Loss: 1.1979
Training Epoch: 17 [1536/50048]	Loss: 1.1186
Training Epoch: 17 [1664/50048]	Loss: 1.1559
Training Epoch: 17 [1792/50048]	Loss: 1.3586
Training Epoch: 17 [1920/50048]	Loss: 1.3960
Training Epoch: 17 [2048/50048]	Loss: 1.2728
Training Epoch: 17 [2176/50048]	Loss: 1.3757
Training Epoch: 17 [2304/50048]	Loss: 1.3563
Training Epoch: 17 [2432/50048]	Loss: 1.2404
Training Epoch: 17 [2560/50048]	Loss: 1.3806
Training Epoch: 17 [2688/50048]	Loss: 1.2951
Training Epoch: 17 [2816/50048]	Loss: 1.3177
Training Epoch: 17 [2944/50048]	Loss: 1.2367
Training Epoch: 17 [3072/50048]	Loss: 1.3530
Training Epoch: 17 [3200/50048]	Loss: 1.3312
Training Epoch: 17 [3328/50048]	Loss: 1.3607
Training Epoch: 17 [3456/50048]	Loss: 1.3583
Training Epoch: 17 [3584/50048]	Loss: 1.2637
Training Epoch: 17 [3712/50048]	Loss: 1.1910
Training Epoch: 17 [3840/50048]	Loss: 1.1923
Training Epoch: 17 [3968/50048]	Loss: 1.1701
Training Epoch: 17 [4096/50048]	Loss: 1.3526
Training Epoch: 17 [4224/50048]	Loss: 1.2127
Training Epoch: 17 [4352/50048]	Loss: 1.1940
Training Epoch: 17 [4480/50048]	Loss: 1.4168
Training Epoch: 17 [4608/50048]	Loss: 1.4145
Training Epoch: 17 [4736/50048]	Loss: 1.4648
Training Epoch: 17 [4864/50048]	Loss: 1.2526
Training Epoch: 17 [4992/50048]	Loss: 1.3571
Training Epoch: 17 [5120/50048]	Loss: 1.2942
Training Epoch: 17 [5248/50048]	Loss: 1.3587
Training Epoch: 17 [5376/50048]	Loss: 1.4105
Training Epoch: 17 [5504/50048]	Loss: 1.2000
Training Epoch: 17 [5632/50048]	Loss: 1.4339
Training Epoch: 17 [5760/50048]	Loss: 1.3641
Training Epoch: 17 [5888/50048]	Loss: 1.3605
Training Epoch: 17 [6016/50048]	Loss: 1.2132
Training Epoch: 17 [6144/50048]	Loss: 1.0840
Training Epoch: 17 [6272/50048]	Loss: 1.4023
Training Epoch: 17 [6400/50048]	Loss: 1.3631
Training Epoch: 17 [6528/50048]	Loss: 1.3587
Training Epoch: 17 [6656/50048]	Loss: 1.2813
Training Epoch: 17 [6784/50048]	Loss: 1.3569
Training Epoch: 17 [6912/50048]	Loss: 1.4162
Training Epoch: 17 [7040/50048]	Loss: 1.5330
Training Epoch: 17 [7168/50048]	Loss: 1.2082
Training Epoch: 17 [7296/50048]	Loss: 1.1318
Training Epoch: 17 [7424/50048]	Loss: 1.5908
Training Epoch: 17 [7552/50048]	Loss: 1.3855
Training Epoch: 17 [7680/50048]	Loss: 1.5091
Training Epoch: 17 [7808/50048]	Loss: 1.4099
Training Epoch: 17 [7936/50048]	Loss: 1.2039
Training Epoch: 17 [8064/50048]	Loss: 1.3543
Training Epoch: 17 [8192/50048]	Loss: 1.2467
Training Epoch: 17 [8320/50048]	Loss: 1.5882
Training Epoch: 17 [8448/50048]	Loss: 1.6270
Training Epoch: 17 [8576/50048]	Loss: 1.4538
Training Epoch: 17 [8704/50048]	Loss: 1.2905
Training Epoch: 17 [8832/50048]	Loss: 1.1890
Training Epoch: 17 [8960/50048]	Loss: 1.3555
Training Epoch: 17 [9088/50048]	Loss: 1.5423
Training Epoch: 17 [9216/50048]	Loss: 1.3460
Training Epoch: 17 [9344/50048]	Loss: 1.3028
Training Epoch: 17 [9472/50048]	Loss: 1.1686
Training Epoch: 17 [9600/50048]	Loss: 1.2968
Training Epoch: 17 [9728/50048]	Loss: 1.3587
Training Epoch: 17 [9856/50048]	Loss: 1.4602
Training Epoch: 17 [9984/50048]	Loss: 1.3290
Training Epoch: 17 [10112/50048]	Loss: 1.2800
Training Epoch: 17 [10240/50048]	Loss: 1.3946
Training Epoch: 17 [10368/50048]	Loss: 1.2194
Training Epoch: 17 [10496/50048]	Loss: 1.2572
Training Epoch: 17 [10624/50048]	Loss: 1.2949
Training Epoch: 17 [10752/50048]	Loss: 1.4021
Training Epoch: 17 [10880/50048]	Loss: 1.3812
Training Epoch: 17 [11008/50048]	Loss: 1.1138
Training Epoch: 17 [11136/50048]	Loss: 1.3348
Training Epoch: 17 [11264/50048]	Loss: 1.3014
Training Epoch: 17 [11392/50048]	Loss: 1.2933
Training Epoch: 17 [11520/50048]	Loss: 1.2109
Training Epoch: 17 [11648/50048]	Loss: 1.3521
Training Epoch: 17 [11776/50048]	Loss: 1.1439
Training Epoch: 17 [11904/50048]	Loss: 1.4082
Training Epoch: 17 [12032/50048]	Loss: 1.4157
Training Epoch: 17 [12160/50048]	Loss: 1.2955
Training Epoch: 17 [12288/50048]	Loss: 1.3217
Training Epoch: 17 [12416/50048]	Loss: 1.3970
Training Epoch: 17 [12544/50048]	Loss: 1.3830
Training Epoch: 17 [12672/50048]	Loss: 1.4344
Training Epoch: 17 [12800/50048]	Loss: 1.3489
Training Epoch: 17 [12928/50048]	Loss: 1.2732
Training Epoch: 17 [13056/50048]	Loss: 1.3765
Training Epoch: 17 [13184/50048]	Loss: 1.2069
Training Epoch: 17 [13312/50048]	Loss: 1.4309
Training Epoch: 17 [13440/50048]	Loss: 1.3062
Training Epoch: 17 [13568/50048]	Loss: 1.3084
Training Epoch: 17 [13696/50048]	Loss: 1.1797
Training Epoch: 17 [13824/50048]	Loss: 1.3874
Training Epoch: 17 [13952/50048]	Loss: 1.3476
Training Epoch: 17 [14080/50048]	Loss: 1.3609
Training Epoch: 17 [14208/50048]	Loss: 1.1622
Training Epoch: 17 [14336/50048]	Loss: 1.3839
Training Epoch: 17 [14464/50048]	Loss: 1.3949
Training Epoch: 17 [14592/50048]	Loss: 1.4131
Training Epoch: 17 [14720/50048]	Loss: 1.3461
Training Epoch: 17 [14848/50048]	Loss: 1.3751
Training Epoch: 17 [14976/50048]	Loss: 1.4207
Training Epoch: 17 [15104/50048]	Loss: 1.5674
Training Epoch: 17 [15232/50048]	Loss: 1.2537
Training Epoch: 17 [15360/50048]	Loss: 1.3638
Training Epoch: 17 [15488/50048]	Loss: 1.2338
Training Epoch: 17 [15616/50048]	Loss: 1.4200
Training Epoch: 17 [15744/50048]	Loss: 1.3304
Training Epoch: 17 [15872/50048]	Loss: 1.2559
Training Epoch: 17 [16000/50048]	Loss: 1.3893
Training Epoch: 17 [16128/50048]	Loss: 1.3781
Training Epoch: 17 [16256/50048]	Loss: 1.5140
Training Epoch: 17 [16384/50048]	Loss: 1.2087
Training Epoch: 17 [16512/50048]	Loss: 1.6275
Training Epoch: 17 [16640/50048]	Loss: 1.4142
Training Epoch: 17 [16768/50048]	Loss: 1.2827
Training Epoch: 17 [16896/50048]	Loss: 1.2880
Training Epoch: 17 [17024/50048]	Loss: 1.3302
Training Epoch: 17 [17152/50048]	Loss: 1.3622
Training Epoch: 17 [17280/50048]	Loss: 1.7088
Training Epoch: 17 [17408/50048]	Loss: 1.2349
Training Epoch: 17 [17536/50048]	Loss: 1.4136
Training Epoch: 17 [17664/50048]	Loss: 1.4727
Training Epoch: 17 [17792/50048]	Loss: 1.2108
Training Epoch: 17 [17920/50048]	Loss: 1.6168
Training Epoch: 17 [18048/50048]	Loss: 1.4459
Training Epoch: 17 [18176/50048]	Loss: 1.4722
Training Epoch: 17 [18304/50048]	Loss: 1.3410
Training Epoch: 17 [18432/50048]	Loss: 1.3615
Training Epoch: 17 [18560/50048]	Loss: 1.4009
Training Epoch: 17 [18688/50048]	Loss: 1.3796
Training Epoch: 17 [18816/50048]	Loss: 1.4415
Training Epoch: 17 [18944/50048]	Loss: 1.1498
Training Epoch: 17 [19072/50048]	Loss: 1.3959
Training Epoch: 17 [19200/50048]	Loss: 1.2463
Training Epoch: 17 [19328/50048]	Loss: 1.3214
Training Epoch: 17 [19456/50048]	Loss: 1.2334
Training Epoch: 17 [19584/50048]	Loss: 1.4985
Training Epoch: 17 [19712/50048]	Loss: 1.4276
Training Epoch: 17 [19840/50048]	Loss: 1.2017
Training Epoch: 17 [19968/50048]	Loss: 1.2065
Training Epoch: 17 [20096/50048]	Loss: 1.4968
Training Epoch: 17 [20224/50048]	Loss: 1.3040
Training Epoch: 17 [20352/50048]	Loss: 1.3132
Training Epoch: 17 [20480/50048]	Loss: 1.3757
Training Epoch: 17 [20608/50048]	Loss: 1.2963
Training Epoch: 17 [20736/50048]	Loss: 1.6445
Training Epoch: 17 [20864/50048]	Loss: 1.3835
Training Epoch: 17 [20992/50048]	Loss: 1.2157
Training Epoch: 17 [21120/50048]	Loss: 1.3730
Training Epoch: 17 [21248/50048]	Loss: 1.7324
Training Epoch: 17 [21376/50048]	Loss: 1.5102
Training Epoch: 17 [21504/50048]	Loss: 1.3125
Training Epoch: 17 [21632/50048]	Loss: 1.2385
Training Epoch: 17 [21760/50048]	Loss: 1.2948
Training Epoch: 17 [21888/50048]	Loss: 1.4361
Training Epoch: 17 [22016/50048]	Loss: 1.3698
Training Epoch: 17 [22144/50048]	Loss: 1.0811
Training Epoch: 17 [22272/50048]	Loss: 1.4984
Training Epoch: 17 [22400/50048]	Loss: 1.3843
Training Epoch: 17 [22528/50048]	Loss: 1.2183
Training Epoch: 17 [22656/50048]	Loss: 1.4819
Training Epoch: 17 [22784/50048]	Loss: 1.5293
Training Epoch: 17 [22912/50048]	Loss: 1.2232
Training Epoch: 17 [23040/50048]	Loss: 1.3602
Training Epoch: 17 [23168/50048]	Loss: 1.3800
Training Epoch: 17 [23296/50048]	Loss: 1.2448
Training Epoch: 17 [23424/50048]	Loss: 1.3769
Training Epoch: 17 [23552/50048]	Loss: 1.4023
Training Epoch: 17 [23680/50048]	Loss: 1.3934
Training Epoch: 17 [23808/50048]	Loss: 1.4425
Training Epoch: 17 [23936/50048]	Loss: 1.2142
Training Epoch: 17 [24064/50048]	Loss: 1.0838
Training Epoch: 17 [24192/50048]	Loss: 1.3262
Training Epoch: 17 [24320/50048]	Loss: 1.3770
Training Epoch: 17 [24448/50048]	Loss: 1.3769
Training Epoch: 17 [24576/50048]	Loss: 1.2693
Training Epoch: 17 [24704/50048]	Loss: 1.2796
Training Epoch: 17 [24832/50048]	Loss: 1.5423
Training Epoch: 17 [24960/50048]	Loss: 1.3888
Training Epoch: 17 [25088/50048]	Loss: 1.3834
Training Epoch: 17 [25216/50048]	Loss: 1.0771
Training Epoch: 17 [25344/50048]	Loss: 1.3527
Training Epoch: 17 [25472/50048]	Loss: 1.4901
Training Epoch: 17 [25600/50048]	Loss: 1.6164
Training Epoch: 17 [25728/50048]	Loss: 1.3784
Training Epoch: 17 [25856/50048]	Loss: 1.4573
Training Epoch: 17 [25984/50048]	Loss: 1.3040
Training Epoch: 17 [26112/50048]	Loss: 1.4931
Training Epoch: 17 [26240/50048]	Loss: 1.2491
Training Epoch: 17 [26368/50048]	Loss: 1.4992
Training Epoch: 17 [26496/50048]	Loss: 1.5003
Training Epoch: 17 [26624/50048]	Loss: 1.1237
Training Epoch: 17 [26752/50048]	Loss: 1.2520
Training Epoch: 17 [26880/50048]	Loss: 1.3008
Training Epoch: 17 [27008/50048]	Loss: 1.2138
Training Epoch: 17 [27136/50048]	Loss: 1.3938
Training Epoch: 17 [27264/50048]	Loss: 1.3255
Training Epoch: 17 [27392/50048]	Loss: 1.2476
Training Epoch: 17 [27520/50048]	Loss: 1.3589
Training Epoch: 17 [27648/50048]	Loss: 1.2461
Training Epoch: 17 [27776/50048]	Loss: 1.5270
Training Epoch: 17 [27904/50048]	Loss: 1.4620
Training Epoch: 17 [28032/50048]	Loss: 1.1841
Training Epoch: 17 [28160/50048]	Loss: 1.3562
Training Epoch: 17 [28288/50048]	Loss: 1.1957
Training Epoch: 17 [28416/50048]	Loss: 1.3092
Training Epoch: 17 [28544/50048]	Loss: 1.4013
Training Epoch: 17 [28672/50048]	Loss: 1.4428
Training Epoch: 17 [28800/50048]	Loss: 1.2673
Training Epoch: 17 [28928/50048]	Loss: 1.3124
Training Epoch: 17 [29056/50048]	Loss: 1.3166
Training Epoch: 17 [29184/50048]	Loss: 1.3205
Training Epoch: 17 [29312/50048]	Loss: 1.2377
Training Epoch: 17 [29440/50048]	Loss: 1.3414
Training Epoch: 17 [29568/50048]	Loss: 1.5030
Training Epoch: 17 [29696/50048]	Loss: 1.4138
Training Epoch: 17 [29824/50048]	Loss: 1.2325
Training Epoch: 17 [29952/50048]	Loss: 1.4214
Training Epoch: 17 [30080/50048]	Loss: 1.1768
Training Epoch: 17 [30208/50048]	Loss: 1.4203
Training Epoch: 17 [30336/50048]	Loss: 1.3219
Training Epoch: 17 [30464/50048]	Loss: 1.0766
Training Epoch: 17 [30592/50048]	Loss: 1.1770
Training Epoch: 17 [30720/50048]	Loss: 1.6050
Training Epoch: 17 [30848/50048]	Loss: 1.2216
Training Epoch: 17 [30976/50048]	Loss: 1.1412
Training Epoch: 17 [31104/50048]	Loss: 1.1904
Training Epoch: 17 [31232/50048]	Loss: 1.5543
Training Epoch: 17 [31360/50048]	Loss: 1.3066
Training Epoch: 17 [31488/50048]	Loss: 1.4265
Training Epoch: 17 [31616/50048]	Loss: 1.1687
Training Epoch: 17 [31744/50048]	Loss: 1.2873
Training Epoch: 17 [31872/50048]	Loss: 1.3485
Training Epoch: 17 [32000/50048]	Loss: 1.2817
Training Epoch: 17 [32128/50048]	Loss: 1.3332
Training Epoch: 17 [32256/50048]	Loss: 1.5295
Training Epoch: 17 [32384/50048]	Loss: 1.3731
Training Epoch: 17 [32512/50048]	Loss: 1.5493
Training Epoch: 17 [32640/50048]	Loss: 1.3251
Training Epoch: 17 [32768/50048]	Loss: 1.2951
Training Epoch: 17 [32896/50048]	Loss: 1.4202
Training Epoch: 17 [33024/50048]	Loss: 1.3917
Training Epoch: 17 [33152/50048]	Loss: 1.2277
Training Epoch: 17 [33280/50048]	Loss: 1.5871
Training Epoch: 17 [33408/50048]	Loss: 1.2659
Training Epoch: 17 [33536/50048]	Loss: 1.4155
Training Epoch: 17 [33664/50048]	Loss: 1.4485
Training Epoch: 17 [33792/50048]	Loss: 1.5551
Training Epoch: 17 [33920/50048]	Loss: 1.2693
Training Epoch: 17 [34048/50048]	Loss: 1.3008
Training Epoch: 17 [34176/50048]	Loss: 1.5183
Training Epoch: 17 [34304/50048]	Loss: 1.6801
Training Epoch: 17 [34432/50048]	Loss: 1.2245
Training Epoch: 17 [34560/50048]	Loss: 1.2823
Training Epoch: 17 [34688/50048]	Loss: 1.2730
Training Epoch: 17 [34816/50048]	Loss: 1.3714
Training Epoch: 17 [34944/50048]	Loss: 1.2944
Training Epoch: 17 [35072/50048]	Loss: 1.3856
Training Epoch: 17 [35200/50048]	Loss: 1.5602
Training Epoch: 17 [35328/50048]	Loss: 1.4912
Training Epoch: 17 [35456/50048]	Loss: 1.2478
Training Epoch: 17 [35584/50048]	Loss: 1.3149
Training Epoch: 17 [35712/50048]	Loss: 1.3241
Training Epoch: 17 [35840/50048]	Loss: 1.3815
Training Epoch: 17 [35968/50048]	Loss: 1.0426
Training Epoch: 17 [36096/50048]	Loss: 1.3706
Training Epoch: 17 [36224/50048]	Loss: 1.2836
Training Epoch: 17 [36352/50048]	Loss: 1.3789
Training Epoch: 17 [36480/50048]	Loss: 1.4214
Training Epoch: 17 [36608/50048]	Loss: 1.3010
Training Epoch: 17 [36736/50048]	Loss: 1.2557
Training Epoch: 17 [36864/50048]	Loss: 1.1605
Training Epoch: 17 [36992/50048]	Loss: 1.4272
Training Epoch: 17 [37120/50048]	Loss: 1.5810
Training Epoch: 17 [37248/50048]	Loss: 1.3760
Training Epoch: 17 [37376/50048]	Loss: 1.4474
Training Epoch: 17 [37504/50048]	Loss: 1.5579
Training Epoch: 17 [37632/50048]	Loss: 1.2179
Training Epoch: 17 [37760/50048]	Loss: 1.4619
Training Epoch: 17 [37888/50048]	Loss: 1.3456
Training Epoch: 17 [38016/50048]	Loss: 1.2603
Training Epoch: 17 [38144/50048]	Loss: 1.3439
Training Epoch: 17 [38272/50048]	Loss: 1.1300
Training Epoch: 17 [38400/50048]	Loss: 1.2977
Training Epoch: 17 [38528/50048]	Loss: 1.2885
Training Epoch: 17 [38656/50048]	Loss: 1.4713
Training Epoch: 17 [38784/50048]	Loss: 1.3475
Training Epoch: 17 [38912/50048]	Loss: 0.9697
Training Epoch: 17 [39040/50048]	Loss: 1.3774
Training Epoch: 17 [39168/50048]	Loss: 1.5508
Training Epoch: 17 [39296/50048]	Loss: 1.2614
Training Epoch: 17 [39424/50048]	Loss: 1.4037
Training Epoch: 17 [39552/50048]	Loss: 1.4017
Training Epoch: 17 [39680/50048]	Loss: 1.3119
Training Epoch: 17 [39808/50048]	Loss: 1.1233
Training Epoch: 17 [39936/50048]	Loss: 1.1840
Training Epoch: 17 [40064/50048]	Loss: 1.3966
Training Epoch: 17 [40192/50048]	Loss: 1.1276
Training Epoch: 17 [40320/50048]	Loss: 1.4211
Training Epoch: 17 [40448/50048]	Loss: 1.3153
Training Epoch: 17 [40576/50048]	Loss: 1.2795
Training Epoch: 17 [40704/50048]	Loss: 1.3909
Training Epoch: 17 [40832/50048]	Loss: 1.6462
Training Epoch: 17 [40960/50048]	Loss: 1.3948
Training Epoch: 17 [41088/50048]	Loss: 1.2222
Training Epoch: 17 [41216/50048]	Loss: 1.4413
Training Epoch: 17 [41344/50048]	Loss: 1.1295
Training Epoch: 17 [41472/50048]	Loss: 1.3300
Training Epoch: 17 [41600/50048]	Loss: 1.3466
Training Epoch: 17 [41728/50048]	Loss: 1.4330
Training Epoch: 17 [41856/50048]	Loss: 1.1836
Training Epoch: 17 [41984/50048]	Loss: 1.3755
Training Epoch: 17 [42112/50048]	Loss: 1.1177
Training Epoch: 17 [42240/50048]	Loss: 1.4661
Training Epoch: 17 [42368/50048]	Loss: 1.3302
Training Epoch: 17 [42496/50048]	Loss: 1.1113
Training Epoch: 17 [42624/50048]	Loss: 1.3848
Training Epoch: 17 [42752/50048]	Loss: 1.3724
Training Epoch: 17 [42880/50048]	Loss: 1.3654
Training Epoch: 17 [43008/50048]	Loss: 1.5193
Training Epoch: 17 [43136/50048]	Loss: 1.2526
Training Epoch: 17 [43264/50048]	Loss: 1.3742
Training Epoch: 17 [43392/50048]	Loss: 1.2982
Training Epoch: 17 [43520/50048]	Loss: 1.2700
Training Epoch: 17 [43648/50048]	Loss: 1.2684
Training Epoch: 17 [43776/50048]	Loss: 1.3784
Training Epoch: 17 [43904/50048]	Loss: 1.3588
Training Epoch: 17 [44032/50048]	Loss: 1.1359
Training Epoch: 17 [44160/50048]	Loss: 1.6104
Training Epoch: 17 [44288/50048]	Loss: 1.5106
Training Epoch: 17 [44416/50048]	Loss: 1.5117
Training Epoch: 17 [44544/50048]	Loss: 1.1657
Training Epoch: 17 [44672/50048]	Loss: 1.3076
Training Epoch: 17 [44800/50048]	Loss: 1.5150
Training Epoch: 17 [44928/50048]	Loss: 1.5326
Training Epoch: 17 [45056/50048]	Loss: 1.3325
Training Epoch: 17 [45184/50048]	Loss: 1.2441
Training Epoch: 17 [45312/50048]	Loss: 1.5152
Training Epoch: 17 [45440/50048]	Loss: 1.4365
Training Epoch: 17 [45568/50048]	Loss: 1.4619
Training Epoch: 17 [45696/50048]	Loss: 1.5905
Training Epoch: 17 [45824/50048]	Loss: 1.3218
Training Epoch: 17 [45952/50048]	Loss: 1.3166
Training Epoch: 17 [46080/50048]	Loss: 1.4324
Training Epoch: 17 [46208/50048]	Loss: 1.3841
Training Epoch: 17 [46336/50048]	Loss: 1.3044
Training Epoch: 17 [46464/50048]	Loss: 1.4737
Training Epoch: 17 [46592/50048]	Loss: 1.4580
Training Epoch: 17 [46720/50048]	Loss: 1.2226
Training Epoch: 17 [46848/50048]	Loss: 1.3803
Training Epoch: 17 [46976/50048]	Loss: 1.3635
Training Epoch: 17 [47104/50048]	Loss: 1.5223
Training Epoch: 17 [47232/50048]	Loss: 1.3801
Training Epoch: 17 [47360/50048]	Loss: 1.3879
Training Epoch: 17 [47488/50048]	Loss: 1.5748
Training Epoch: 17 [47616/50048]	Loss: 1.2761
Training Epoch: 17 [47744/50048]	Loss: 1.2207
Training Epoch: 17 [47872/50048]	Loss: 1.3421
Training Epoch: 17 [48000/50048]	Loss: 1.3229
Training Epoch: 17 [48128/50048]	Loss: 1.2817
Training Epoch: 17 [48256/50048]	Loss: 1.4464
Training Epoch: 17 [48384/50048]	Loss: 1.1673
Training Epoch: 17 [48512/50048]	Loss: 1.3016
Training Epoch: 17 [48640/50048]	Loss: 1.4196
Training Epoch: 17 [48768/50048]	Loss: 1.4627
Training Epoch: 17 [48896/50048]	Loss: 1.4780
Training Epoch: 17 [49024/50048]	Loss: 1.3677
Training Epoch: 17 [49152/50048]	Loss: 1.1826
Training Epoch: 17 [49280/50048]	Loss: 1.4014
Training Epoch: 17 [49408/50048]	Loss: 1.0360
Training Epoch: 17 [49536/50048]	Loss: 1.3188
Training Epoch: 17 [49664/50048]	Loss: 1.2881
Training Epoch: 17 [49792/50048]	Loss: 1.5679
Training Epoch: 17 [49920/50048]	Loss: 1.3214
Training Epoch: 17 [50048/50048]	Loss: 1.5782
Validation Epoch: 17, Average loss: 0.0119, Accuracy: 0.5817
Training Epoch: 18 [128/50048]	Loss: 1.0302
Training Epoch: 18 [256/50048]	Loss: 1.0799
Training Epoch: 18 [384/50048]	Loss: 1.2424
Training Epoch: 18 [512/50048]	Loss: 1.3994
Training Epoch: 18 [640/50048]	Loss: 1.3497
Training Epoch: 18 [768/50048]	Loss: 1.2423
Training Epoch: 18 [896/50048]	Loss: 1.2401
Training Epoch: 18 [1024/50048]	Loss: 1.4229
Training Epoch: 18 [1152/50048]	Loss: 1.3995
Training Epoch: 18 [1280/50048]	Loss: 1.4239
Training Epoch: 18 [1408/50048]	Loss: 1.1326
Training Epoch: 18 [1536/50048]	Loss: 1.2456
Training Epoch: 18 [1664/50048]	Loss: 1.3784
Training Epoch: 18 [1792/50048]	Loss: 1.2046
Training Epoch: 18 [1920/50048]	Loss: 1.5674
Training Epoch: 18 [2048/50048]	Loss: 1.3885
Training Epoch: 18 [2176/50048]	Loss: 1.4456
Training Epoch: 18 [2304/50048]	Loss: 1.4096
Training Epoch: 18 [2432/50048]	Loss: 1.2231
Training Epoch: 18 [2560/50048]	Loss: 1.2458
Training Epoch: 18 [2688/50048]	Loss: 1.2846
Training Epoch: 18 [2816/50048]	Loss: 1.1461
Training Epoch: 18 [2944/50048]	Loss: 1.4090
Training Epoch: 18 [3072/50048]	Loss: 1.1976
Training Epoch: 18 [3200/50048]	Loss: 1.2731
Training Epoch: 18 [3328/50048]	Loss: 1.3289
Training Epoch: 18 [3456/50048]	Loss: 1.0670
Training Epoch: 18 [3584/50048]	Loss: 1.3659
Training Epoch: 18 [3712/50048]	Loss: 1.2692
Training Epoch: 18 [3840/50048]	Loss: 1.3287
Training Epoch: 18 [3968/50048]	Loss: 1.2952
Training Epoch: 18 [4096/50048]	Loss: 1.2012
Training Epoch: 18 [4224/50048]	Loss: 1.2069
Training Epoch: 18 [4352/50048]	Loss: 1.2254
Training Epoch: 18 [4480/50048]	Loss: 1.3342
Training Epoch: 18 [4608/50048]	Loss: 1.3045
Training Epoch: 18 [4736/50048]	Loss: 1.5539
Training Epoch: 18 [4864/50048]	Loss: 1.4023
Training Epoch: 18 [4992/50048]	Loss: 1.2827
Training Epoch: 18 [5120/50048]	Loss: 1.3828
Training Epoch: 18 [5248/50048]	Loss: 1.2565
Training Epoch: 18 [5376/50048]	Loss: 1.4268
Training Epoch: 18 [5504/50048]	Loss: 1.3864
Training Epoch: 18 [5632/50048]	Loss: 1.4677
Training Epoch: 18 [5760/50048]	Loss: 1.3153
Training Epoch: 18 [5888/50048]	Loss: 1.1668
Training Epoch: 18 [6016/50048]	Loss: 1.2916
Training Epoch: 18 [6144/50048]	Loss: 1.1855
Training Epoch: 18 [6272/50048]	Loss: 1.3231
Training Epoch: 18 [6400/50048]	Loss: 1.5752
Training Epoch: 18 [6528/50048]	Loss: 1.6312
Training Epoch: 18 [6656/50048]	Loss: 1.1736
Training Epoch: 18 [6784/50048]	Loss: 1.4113
Training Epoch: 18 [6912/50048]	Loss: 1.6257
Training Epoch: 18 [7040/50048]	Loss: 1.2548
Training Epoch: 18 [7168/50048]	Loss: 1.3258
Training Epoch: 18 [7296/50048]	Loss: 1.1537
Training Epoch: 18 [7424/50048]	Loss: 1.3311
Training Epoch: 18 [7552/50048]	Loss: 1.2716
Training Epoch: 18 [7680/50048]	Loss: 1.2993
Training Epoch: 18 [7808/50048]	Loss: 1.1579
Training Epoch: 18 [7936/50048]	Loss: 1.4789
Training Epoch: 18 [8064/50048]	Loss: 1.5841
Training Epoch: 18 [8192/50048]	Loss: 1.4688
Training Epoch: 18 [8320/50048]	Loss: 1.2880
Training Epoch: 18 [8448/50048]	Loss: 1.2803
Training Epoch: 18 [8576/50048]	Loss: 1.4016
Training Epoch: 18 [8704/50048]	Loss: 1.4416
Training Epoch: 18 [8832/50048]	Loss: 1.3925
Training Epoch: 18 [8960/50048]	Loss: 1.4531
Training Epoch: 18 [9088/50048]	Loss: 1.1687
Training Epoch: 18 [9216/50048]	Loss: 1.2464
Training Epoch: 18 [9344/50048]	Loss: 1.3300
Training Epoch: 18 [9472/50048]	Loss: 0.9665
Training Epoch: 18 [9600/50048]	Loss: 1.4772
Training Epoch: 18 [9728/50048]	Loss: 1.2792
Training Epoch: 18 [9856/50048]	Loss: 1.5519
Training Epoch: 18 [9984/50048]	Loss: 1.3257
Training Epoch: 18 [10112/50048]	Loss: 1.3658
Training Epoch: 18 [10240/50048]	Loss: 1.3046
Training Epoch: 18 [10368/50048]	Loss: 1.3457
Training Epoch: 18 [10496/50048]	Loss: 1.3241
Training Epoch: 18 [10624/50048]	Loss: 1.5324
Training Epoch: 18 [10752/50048]	Loss: 1.1935
Training Epoch: 18 [10880/50048]	Loss: 1.3448
Training Epoch: 18 [11008/50048]	Loss: 1.3653
Training Epoch: 18 [11136/50048]	Loss: 1.3420
Training Epoch: 18 [11264/50048]	Loss: 1.0998
Training Epoch: 18 [11392/50048]	Loss: 1.5052
Training Epoch: 18 [11520/50048]	Loss: 1.3520
Training Epoch: 18 [11648/50048]	Loss: 1.3455
Training Epoch: 18 [11776/50048]	Loss: 1.4367
Training Epoch: 18 [11904/50048]	Loss: 1.4339
Training Epoch: 18 [12032/50048]	Loss: 1.3280
Training Epoch: 18 [12160/50048]	Loss: 1.5738
Training Epoch: 18 [12288/50048]	Loss: 1.3186
Training Epoch: 18 [12416/50048]	Loss: 1.3275
Training Epoch: 18 [12544/50048]	Loss: 1.1263
Training Epoch: 18 [12672/50048]	Loss: 1.1738
Training Epoch: 18 [12800/50048]	Loss: 1.0831
Training Epoch: 18 [12928/50048]	Loss: 1.2971
Training Epoch: 18 [13056/50048]	Loss: 1.2115
Training Epoch: 18 [13184/50048]	Loss: 1.4355
Training Epoch: 18 [13312/50048]	Loss: 1.5379
Training Epoch: 18 [13440/50048]	Loss: 1.4702
Training Epoch: 18 [13568/50048]	Loss: 1.3494
Training Epoch: 18 [13696/50048]	Loss: 1.1489
Training Epoch: 18 [13824/50048]	Loss: 1.3020
Training Epoch: 18 [13952/50048]	Loss: 1.1359
Training Epoch: 18 [14080/50048]	Loss: 1.1550
Training Epoch: 18 [14208/50048]	Loss: 1.3170
Training Epoch: 18 [14336/50048]	Loss: 1.4655
Training Epoch: 18 [14464/50048]	Loss: 1.4805
Training Epoch: 18 [14592/50048]	Loss: 1.4975
Training Epoch: 18 [14720/50048]	Loss: 1.3309
Training Epoch: 18 [14848/50048]	Loss: 1.3307
Training Epoch: 18 [14976/50048]	Loss: 1.3168
Training Epoch: 18 [15104/50048]	Loss: 1.2840
Training Epoch: 18 [15232/50048]	Loss: 1.2854
Training Epoch: 18 [15360/50048]	Loss: 1.2368
Training Epoch: 18 [15488/50048]	Loss: 1.1789
Training Epoch: 18 [15616/50048]	Loss: 1.0732
Training Epoch: 18 [15744/50048]	Loss: 1.2946
Training Epoch: 18 [15872/50048]	Loss: 1.2989
Training Epoch: 18 [16000/50048]	Loss: 1.2855
Training Epoch: 18 [16128/50048]	Loss: 1.3630
Training Epoch: 18 [16256/50048]	Loss: 1.3064
Training Epoch: 18 [16384/50048]	Loss: 1.2260
Training Epoch: 18 [16512/50048]	Loss: 1.5627
Training Epoch: 18 [16640/50048]	Loss: 1.1705
Training Epoch: 18 [16768/50048]	Loss: 1.2260
Training Epoch: 18 [16896/50048]	Loss: 1.1586
Training Epoch: 18 [17024/50048]	Loss: 1.1682
Training Epoch: 18 [17152/50048]	Loss: 1.1854
Training Epoch: 18 [17280/50048]	Loss: 1.1370
Training Epoch: 18 [17408/50048]	Loss: 1.3909
Training Epoch: 18 [17536/50048]	Loss: 1.2105
Training Epoch: 18 [17664/50048]	Loss: 1.2539
Training Epoch: 18 [17792/50048]	Loss: 1.3707
Training Epoch: 18 [17920/50048]	Loss: 1.4431
Training Epoch: 18 [18048/50048]	Loss: 1.1942
Training Epoch: 18 [18176/50048]	Loss: 1.5260
Training Epoch: 18 [18304/50048]	Loss: 1.1765
Training Epoch: 18 [18432/50048]	Loss: 1.4235
Training Epoch: 18 [18560/50048]	Loss: 1.5555
Training Epoch: 18 [18688/50048]	Loss: 1.0753
Training Epoch: 18 [18816/50048]	Loss: 1.0892
Training Epoch: 18 [18944/50048]	Loss: 1.2724
Training Epoch: 18 [19072/50048]	Loss: 1.4869
Training Epoch: 18 [19200/50048]	Loss: 1.2621
Training Epoch: 18 [19328/50048]	Loss: 1.3012
Training Epoch: 18 [19456/50048]	Loss: 1.4848
Training Epoch: 18 [19584/50048]	Loss: 1.2403
Training Epoch: 18 [19712/50048]	Loss: 1.3296
Training Epoch: 18 [19840/50048]	Loss: 1.5350
Training Epoch: 18 [19968/50048]	Loss: 1.2323
Training Epoch: 18 [20096/50048]	Loss: 1.3009
Training Epoch: 18 [20224/50048]	Loss: 1.3125
Training Epoch: 18 [20352/50048]	Loss: 1.2850
Training Epoch: 18 [20480/50048]	Loss: 1.2985
Training Epoch: 18 [20608/50048]	Loss: 1.3778
Training Epoch: 18 [20736/50048]	Loss: 1.2271
Training Epoch: 18 [20864/50048]	Loss: 1.3099
Training Epoch: 18 [20992/50048]	Loss: 1.1233
Training Epoch: 18 [21120/50048]	Loss: 1.2407
Training Epoch: 18 [21248/50048]	Loss: 1.4228
Training Epoch: 18 [21376/50048]	Loss: 1.4435
Training Epoch: 18 [21504/50048]	Loss: 1.4035
Training Epoch: 18 [21632/50048]	Loss: 1.3915
Training Epoch: 18 [21760/50048]	Loss: 1.4170
Training Epoch: 18 [21888/50048]	Loss: 1.3690
Training Epoch: 18 [22016/50048]	Loss: 1.3237
Training Epoch: 18 [22144/50048]	Loss: 1.2689
Training Epoch: 18 [22272/50048]	Loss: 1.1350
Training Epoch: 18 [22400/50048]	Loss: 1.3857
Training Epoch: 18 [22528/50048]	Loss: 1.0604
Training Epoch: 18 [22656/50048]	Loss: 1.3265
Training Epoch: 18 [22784/50048]	Loss: 1.2419
Training Epoch: 18 [22912/50048]	Loss: 1.1450
Training Epoch: 18 [23040/50048]	Loss: 1.2658
Training Epoch: 18 [23168/50048]	Loss: 1.4576
Training Epoch: 18 [23296/50048]	Loss: 1.3097
Training Epoch: 18 [23424/50048]	Loss: 1.4135
Training Epoch: 18 [23552/50048]	Loss: 1.0568
Training Epoch: 18 [23680/50048]	Loss: 1.3367
Training Epoch: 18 [23808/50048]	Loss: 1.2911
Training Epoch: 18 [23936/50048]	Loss: 1.5781
Training Epoch: 18 [24064/50048]	Loss: 1.3329
Training Epoch: 18 [24192/50048]	Loss: 1.3206
Training Epoch: 18 [24320/50048]	Loss: 1.1845
Training Epoch: 18 [24448/50048]	Loss: 1.5088
Training Epoch: 18 [24576/50048]	Loss: 1.2668
Training Epoch: 18 [24704/50048]	Loss: 1.4227
Training Epoch: 18 [24832/50048]	Loss: 1.2695
Training Epoch: 18 [24960/50048]	Loss: 1.5313
Training Epoch: 18 [25088/50048]	Loss: 1.2799
Training Epoch: 18 [25216/50048]	Loss: 1.2909
Training Epoch: 18 [25344/50048]	Loss: 1.4659
Training Epoch: 18 [25472/50048]	Loss: 1.3029
Training Epoch: 18 [25600/50048]	Loss: 1.2746
Training Epoch: 18 [25728/50048]	Loss: 1.3301
Training Epoch: 18 [25856/50048]	Loss: 1.2865
Training Epoch: 18 [25984/50048]	Loss: 1.1682
Training Epoch: 18 [26112/50048]	Loss: 1.2293
Training Epoch: 18 [26240/50048]	Loss: 1.4851
Training Epoch: 18 [26368/50048]	Loss: 1.3356
Training Epoch: 18 [26496/50048]	Loss: 1.2770
Training Epoch: 18 [26624/50048]	Loss: 1.1959
Training Epoch: 18 [26752/50048]	Loss: 1.7042
Training Epoch: 18 [26880/50048]	Loss: 1.2395
Training Epoch: 18 [27008/50048]	Loss: 1.6097
Training Epoch: 18 [27136/50048]	Loss: 1.2863
Training Epoch: 18 [27264/50048]	Loss: 1.4357
Training Epoch: 18 [27392/50048]	Loss: 1.3061
Training Epoch: 18 [27520/50048]	Loss: 1.3750
Training Epoch: 18 [27648/50048]	Loss: 1.2707
Training Epoch: 18 [27776/50048]	Loss: 1.3999
Training Epoch: 18 [27904/50048]	Loss: 1.3977
Training Epoch: 18 [28032/50048]	Loss: 1.2917
Training Epoch: 18 [28160/50048]	Loss: 1.1973
Training Epoch: 18 [28288/50048]	Loss: 1.2953
Training Epoch: 18 [28416/50048]	Loss: 1.2775
Training Epoch: 18 [28544/50048]	Loss: 1.6820
Training Epoch: 18 [28672/50048]	Loss: 1.2807
Training Epoch: 18 [28800/50048]	Loss: 1.4502
Training Epoch: 18 [28928/50048]	Loss: 1.3931
Training Epoch: 18 [29056/50048]	Loss: 1.3231
Training Epoch: 18 [29184/50048]	Loss: 1.3291
Training Epoch: 18 [29312/50048]	Loss: 1.1018
Training Epoch: 18 [29440/50048]	Loss: 1.2505
Training Epoch: 18 [29568/50048]	Loss: 1.2519
Training Epoch: 18 [29696/50048]	Loss: 1.3008
Training Epoch: 18 [29824/50048]	Loss: 1.2047
Training Epoch: 18 [29952/50048]	Loss: 1.3542
Training Epoch: 18 [30080/50048]	Loss: 1.7098
Training Epoch: 18 [30208/50048]	Loss: 1.0732
Training Epoch: 18 [30336/50048]	Loss: 1.3098
Training Epoch: 18 [30464/50048]	Loss: 1.2737
Training Epoch: 18 [30592/50048]	Loss: 1.3328
Training Epoch: 18 [30720/50048]	Loss: 1.3463
Training Epoch: 18 [30848/50048]	Loss: 1.3435
Training Epoch: 18 [30976/50048]	Loss: 1.2517
Training Epoch: 18 [31104/50048]	Loss: 1.4482
Training Epoch: 18 [31232/50048]	Loss: 1.1836
Training Epoch: 18 [31360/50048]	Loss: 1.1792
Training Epoch: 18 [31488/50048]	Loss: 1.2842
Training Epoch: 18 [31616/50048]	Loss: 1.0576
Training Epoch: 18 [31744/50048]	Loss: 1.2641
Training Epoch: 18 [31872/50048]	Loss: 1.3796
Training Epoch: 18 [32000/50048]	Loss: 1.1571
Training Epoch: 18 [32128/50048]	Loss: 1.3239
Training Epoch: 18 [32256/50048]	Loss: 1.4239
Training Epoch: 18 [32384/50048]	Loss: 1.3460
Training Epoch: 18 [32512/50048]	Loss: 1.2292
Training Epoch: 18 [32640/50048]	Loss: 1.2285
Training Epoch: 18 [32768/50048]	Loss: 1.3368
Training Epoch: 18 [32896/50048]	Loss: 1.1033
Training Epoch: 18 [33024/50048]	Loss: 1.2245
Training Epoch: 18 [33152/50048]	Loss: 1.1161
Training Epoch: 18 [33280/50048]	Loss: 1.4098
Training Epoch: 18 [33408/50048]	Loss: 1.3159
Training Epoch: 18 [33536/50048]	Loss: 1.6177
Training Epoch: 18 [33664/50048]	Loss: 1.2342
Training Epoch: 18 [33792/50048]	Loss: 1.2484
Training Epoch: 18 [33920/50048]	Loss: 1.6356
Training Epoch: 18 [34048/50048]	Loss: 1.3009
Training Epoch: 18 [34176/50048]	Loss: 1.3754
Training Epoch: 18 [34304/50048]	Loss: 1.4240
Training Epoch: 18 [34432/50048]	Loss: 1.4081
Training Epoch: 18 [34560/50048]	Loss: 1.3745
Training Epoch: 18 [34688/50048]	Loss: 1.3527
Training Epoch: 18 [34816/50048]	Loss: 1.3020
Training Epoch: 18 [34944/50048]	Loss: 1.1303
Training Epoch: 18 [35072/50048]	Loss: 1.5386
Training Epoch: 18 [35200/50048]	Loss: 1.2431
Training Epoch: 18 [35328/50048]	Loss: 1.3121
Training Epoch: 18 [35456/50048]	Loss: 1.4278
Training Epoch: 18 [35584/50048]	Loss: 1.2406
Training Epoch: 18 [35712/50048]	Loss: 1.1233
Training Epoch: 18 [35840/50048]	Loss: 1.1768
Training Epoch: 18 [35968/50048]	Loss: 1.4699
Training Epoch: 18 [36096/50048]	Loss: 1.3818
Training Epoch: 18 [36224/50048]	Loss: 1.1443
Training Epoch: 18 [36352/50048]	Loss: 1.3310
Training Epoch: 18 [36480/50048]	Loss: 1.2428
Training Epoch: 18 [36608/50048]	Loss: 1.3650
Training Epoch: 18 [36736/50048]	Loss: 1.3597
Training Epoch: 18 [36864/50048]	Loss: 1.3564
Training Epoch: 18 [36992/50048]	Loss: 1.4363
Training Epoch: 18 [37120/50048]	Loss: 1.3226
Training Epoch: 18 [37248/50048]	Loss: 1.3615
Training Epoch: 18 [37376/50048]	Loss: 0.9761
Training Epoch: 18 [37504/50048]	Loss: 1.2684
Training Epoch: 18 [37632/50048]	Loss: 1.5896
Training Epoch: 18 [37760/50048]	Loss: 1.2858
Training Epoch: 18 [37888/50048]	Loss: 1.3815
Training Epoch: 18 [38016/50048]	Loss: 1.2447
Training Epoch: 18 [38144/50048]	Loss: 1.3689
Training Epoch: 18 [38272/50048]	Loss: 1.2559
Training Epoch: 18 [38400/50048]	Loss: 1.3977
Training Epoch: 18 [38528/50048]	Loss: 1.4083
Training Epoch: 18 [38656/50048]	Loss: 1.5534
Training Epoch: 18 [38784/50048]	Loss: 1.3138
Training Epoch: 18 [38912/50048]	Loss: 1.4397
Training Epoch: 18 [39040/50048]	Loss: 1.3787
Training Epoch: 18 [39168/50048]	Loss: 1.3236
Training Epoch: 18 [39296/50048]	Loss: 1.2470
Training Epoch: 18 [39424/50048]	Loss: 1.2003
Training Epoch: 18 [39552/50048]	Loss: 1.3682
Training Epoch: 18 [39680/50048]	Loss: 1.3443
Training Epoch: 18 [39808/50048]	Loss: 1.2080
Training Epoch: 18 [39936/50048]	Loss: 1.0646
Training Epoch: 18 [40064/50048]	Loss: 1.4735
Training Epoch: 18 [40192/50048]	Loss: 1.3630
Training Epoch: 18 [40320/50048]	Loss: 1.0857
Training Epoch: 18 [40448/50048]	Loss: 1.2450
Training Epoch: 18 [40576/50048]	Loss: 1.5930
Training Epoch: 18 [40704/50048]	Loss: 1.2450
Training Epoch: 18 [40832/50048]	Loss: 1.3249
Training Epoch: 18 [40960/50048]	Loss: 1.3538
Training Epoch: 18 [41088/50048]	Loss: 1.4137
Training Epoch: 18 [41216/50048]	Loss: 1.3187
Training Epoch: 18 [41344/50048]	Loss: 1.2426
Training Epoch: 18 [41472/50048]	Loss: 1.5204
Training Epoch: 18 [41600/50048]	Loss: 1.4078
Training Epoch: 18 [41728/50048]	Loss: 1.1283
Training Epoch: 18 [41856/50048]	Loss: 1.7540
Training Epoch: 18 [41984/50048]	Loss: 1.3758
Training Epoch: 18 [42112/50048]	Loss: 1.5209
Training Epoch: 18 [42240/50048]	Loss: 1.3410
Training Epoch: 18 [42368/50048]	Loss: 1.3851
Training Epoch: 18 [42496/50048]	Loss: 1.2311
Training Epoch: 18 [42624/50048]	Loss: 1.3992
Training Epoch: 18 [42752/50048]	Loss: 1.3616
Training Epoch: 18 [42880/50048]	Loss: 1.3073
Training Epoch: 18 [43008/50048]	Loss: 1.0451
Training Epoch: 18 [43136/50048]	Loss: 1.3380
Training Epoch: 18 [43264/50048]	Loss: 1.4328
Training Epoch: 18 [43392/50048]	Loss: 1.6179
Training Epoch: 18 [43520/50048]	Loss: 1.3211
Training Epoch: 18 [43648/50048]	Loss: 1.3819
Training Epoch: 18 [43776/50048]	Loss: 1.4433
Training Epoch: 18 [43904/50048]	Loss: 1.4020
Training Epoch: 18 [44032/50048]	Loss: 1.3936
Training Epoch: 18 [44160/50048]	Loss: 1.3869
Training Epoch: 18 [44288/50048]	Loss: 1.3357
Training Epoch: 18 [44416/50048]	Loss: 1.2874
Training Epoch: 18 [44544/50048]	Loss: 1.3079
Training Epoch: 18 [44672/50048]	Loss: 1.3452
Training Epoch: 18 [44800/50048]	Loss: 1.0910
Training Epoch: 18 [44928/50048]	Loss: 1.0702
Training Epoch: 18 [45056/50048]	Loss: 1.0588
Training Epoch: 18 [45184/50048]	Loss: 1.4074
Training Epoch: 18 [45312/50048]	Loss: 1.3588
Training Epoch: 18 [45440/50048]	Loss: 1.3377
Training Epoch: 18 [45568/50048]	Loss: 1.4731
Training Epoch: 18 [45696/50048]	Loss: 1.0271
Training Epoch: 18 [45824/50048]	Loss: 1.3779
Training Epoch: 18 [45952/50048]	Loss: 1.3858
Training Epoch: 18 [46080/50048]	Loss: 1.1950
Training Epoch: 18 [46208/50048]	Loss: 1.2376
Training Epoch: 18 [46336/50048]	Loss: 1.2186
Training Epoch: 18 [46464/50048]	Loss: 1.3640
Training Epoch: 18 [46592/50048]	Loss: 1.4081
Training Epoch: 18 [46720/50048]	Loss: 1.4697
Training Epoch: 18 [46848/50048]	Loss: 1.1996
Training Epoch: 18 [46976/50048]	Loss: 1.4805
Training Epoch: 18 [47104/50048]	Loss: 1.3439
Training Epoch: 18 [47232/50048]	Loss: 1.2960
Training Epoch: 18 [47360/50048]	Loss: 1.2525
Training Epoch: 18 [47488/50048]	Loss: 1.3510
Training Epoch: 18 [47616/50048]	Loss: 1.3420
Training Epoch: 18 [47744/50048]	Loss: 1.4186
Training Epoch: 18 [47872/50048]	Loss: 1.3908
Training Epoch: 18 [48000/50048]	Loss: 1.2692
Training Epoch: 18 [48128/50048]	Loss: 1.2779
Training Epoch: 18 [48256/50048]	Loss: 1.3966
Training Epoch: 18 [48384/50048]	Loss: 1.2458
Training Epoch: 18 [48512/50048]	Loss: 1.3090
Training Epoch: 18 [48640/50048]	Loss: 1.2248
Training Epoch: 18 [48768/50048]	Loss: 1.5518
Training Epoch: 18 [48896/50048]	Loss: 1.4623
Training Epoch: 18 [49024/50048]	Loss: 1.3792
Training Epoch: 18 [49152/50048]	Loss: 1.1947
Training Epoch: 18 [49280/50048]	Loss: 1.4082
Training Epoch: 18 [49408/50048]	Loss: 1.1959
Training Epoch: 18 [49536/50048]	Loss: 1.3345
Training Epoch: 18 [49664/50048]	Loss: 1.4089
Training Epoch: 18 [49792/50048]	Loss: 1.2691
Training Epoch: 18 [49920/50048]	Loss: 1.5500
Training Epoch: 18 [50048/50048]	Loss: 1.1848
Validation Epoch: 18, Average loss: 0.0120, Accuracy: 0.5794
Training Epoch: 19 [128/50048]	Loss: 1.1718
Training Epoch: 19 [256/50048]	Loss: 1.4058
Training Epoch: 19 [384/50048]	Loss: 1.0736
Training Epoch: 19 [512/50048]	Loss: 1.3016
Training Epoch: 19 [640/50048]	Loss: 1.5537
Training Epoch: 19 [768/50048]	Loss: 1.1604
Training Epoch: 19 [896/50048]	Loss: 1.1376
Training Epoch: 19 [1024/50048]	Loss: 1.0502
Training Epoch: 19 [1152/50048]	Loss: 1.5304
Training Epoch: 19 [1280/50048]	Loss: 1.3632
Training Epoch: 19 [1408/50048]	Loss: 1.0801
Training Epoch: 19 [1536/50048]	Loss: 1.2897
Training Epoch: 19 [1664/50048]	Loss: 1.3598
Training Epoch: 19 [1792/50048]	Loss: 1.1600
Training Epoch: 19 [1920/50048]	Loss: 1.1796
Training Epoch: 19 [2048/50048]	Loss: 1.2570
Training Epoch: 19 [2176/50048]	Loss: 1.2079
Training Epoch: 19 [2304/50048]	Loss: 1.1602
Training Epoch: 19 [2432/50048]	Loss: 1.0737
Training Epoch: 19 [2560/50048]	Loss: 1.3424
Training Epoch: 19 [2688/50048]	Loss: 1.3483
Training Epoch: 19 [2816/50048]	Loss: 1.1444
Training Epoch: 19 [2944/50048]	Loss: 1.1338
Training Epoch: 19 [3072/50048]	Loss: 1.2267
Training Epoch: 19 [3200/50048]	Loss: 1.1795
Training Epoch: 19 [3328/50048]	Loss: 1.3300
Training Epoch: 19 [3456/50048]	Loss: 1.4025
Training Epoch: 19 [3584/50048]	Loss: 1.4658
Training Epoch: 19 [3712/50048]	Loss: 1.3203
Training Epoch: 19 [3840/50048]	Loss: 1.2436
Training Epoch: 19 [3968/50048]	Loss: 1.1964
Training Epoch: 19 [4096/50048]	Loss: 1.3641
Training Epoch: 19 [4224/50048]	Loss: 1.1894
Training Epoch: 19 [4352/50048]	Loss: 1.3735
Training Epoch: 19 [4480/50048]	Loss: 1.2207
Training Epoch: 19 [4608/50048]	Loss: 1.1346
Training Epoch: 19 [4736/50048]	Loss: 1.5326
Training Epoch: 19 [4864/50048]	Loss: 1.2795
Training Epoch: 19 [4992/50048]	Loss: 1.2591
Training Epoch: 19 [5120/50048]	Loss: 1.4297
Training Epoch: 19 [5248/50048]	Loss: 1.4608
Training Epoch: 19 [5376/50048]	Loss: 1.2374
Training Epoch: 19 [5504/50048]	Loss: 1.6077
Training Epoch: 19 [5632/50048]	Loss: 1.3206
Training Epoch: 19 [5760/50048]	Loss: 1.3514
Training Epoch: 19 [5888/50048]	Loss: 1.2173
Training Epoch: 19 [6016/50048]	Loss: 1.4614
Training Epoch: 19 [6144/50048]	Loss: 1.3567
Training Epoch: 19 [6272/50048]	Loss: 1.1510
Training Epoch: 19 [6400/50048]	Loss: 1.1891
Training Epoch: 19 [6528/50048]	Loss: 1.5148
Training Epoch: 19 [6656/50048]	Loss: 1.1645
Training Epoch: 19 [6784/50048]	Loss: 1.5628
Training Epoch: 19 [6912/50048]	Loss: 1.4661
Training Epoch: 19 [7040/50048]	Loss: 1.1823
Training Epoch: 19 [7168/50048]	Loss: 1.2835
Training Epoch: 19 [7296/50048]	Loss: 1.1934
Training Epoch: 19 [7424/50048]	Loss: 1.3544
Training Epoch: 19 [7552/50048]	Loss: 1.4328
Training Epoch: 19 [7680/50048]	Loss: 1.2373
Training Epoch: 19 [7808/50048]	Loss: 1.0330
Training Epoch: 19 [7936/50048]	Loss: 1.2815
Training Epoch: 19 [8064/50048]	Loss: 1.4146
Training Epoch: 19 [8192/50048]	Loss: 1.1506
Training Epoch: 19 [8320/50048]	Loss: 1.2872
Training Epoch: 19 [8448/50048]	Loss: 1.3823
Training Epoch: 19 [8576/50048]	Loss: 1.1227
Training Epoch: 19 [8704/50048]	Loss: 1.1818
Training Epoch: 19 [8832/50048]	Loss: 1.1303
Training Epoch: 19 [8960/50048]	Loss: 1.1575
Training Epoch: 19 [9088/50048]	Loss: 1.4160
Training Epoch: 19 [9216/50048]	Loss: 1.3083
Training Epoch: 19 [9344/50048]	Loss: 1.1481
Training Epoch: 19 [9472/50048]	Loss: 1.4323
Training Epoch: 19 [9600/50048]	Loss: 1.2837
Training Epoch: 19 [9728/50048]	Loss: 1.4269
Training Epoch: 19 [9856/50048]	Loss: 1.3803
Training Epoch: 19 [9984/50048]	Loss: 1.4986
Training Epoch: 19 [10112/50048]	Loss: 1.3825
Training Epoch: 19 [10240/50048]	Loss: 1.3424
Training Epoch: 19 [10368/50048]	Loss: 0.9982
Training Epoch: 19 [10496/50048]	Loss: 1.2724
Training Epoch: 19 [10624/50048]	Loss: 1.2133
Training Epoch: 19 [10752/50048]	Loss: 1.3195
Training Epoch: 19 [10880/50048]	Loss: 1.3073
Training Epoch: 19 [11008/50048]	Loss: 1.0533
Training Epoch: 19 [11136/50048]	Loss: 1.2949
Training Epoch: 19 [11264/50048]	Loss: 1.2089
Training Epoch: 19 [11392/50048]	Loss: 1.3959
Training Epoch: 19 [11520/50048]	Loss: 1.4880
Training Epoch: 19 [11648/50048]	Loss: 1.1935
Training Epoch: 19 [11776/50048]	Loss: 1.3530
Training Epoch: 19 [11904/50048]	Loss: 1.5445
Training Epoch: 19 [12032/50048]	Loss: 1.0948
Training Epoch: 19 [12160/50048]	Loss: 1.1828
Training Epoch: 19 [12288/50048]	Loss: 1.6415
Training Epoch: 19 [12416/50048]	Loss: 1.1152
Training Epoch: 19 [12544/50048]	Loss: 1.3130
Training Epoch: 19 [12672/50048]	Loss: 1.2509
Training Epoch: 19 [12800/50048]	Loss: 1.1112
Training Epoch: 19 [12928/50048]	Loss: 1.2461
Training Epoch: 19 [13056/50048]	Loss: 1.2536
Training Epoch: 19 [13184/50048]	Loss: 1.4553
Training Epoch: 19 [13312/50048]	Loss: 1.1076
Training Epoch: 19 [13440/50048]	Loss: 1.1792
Training Epoch: 19 [13568/50048]	Loss: 1.2324
Training Epoch: 19 [13696/50048]	Loss: 1.3619
Training Epoch: 19 [13824/50048]	Loss: 1.3438
Training Epoch: 19 [13952/50048]	Loss: 1.4070
Training Epoch: 19 [14080/50048]	Loss: 1.0882
Training Epoch: 19 [14208/50048]	Loss: 1.4998
Training Epoch: 19 [14336/50048]	Loss: 1.2476
Training Epoch: 19 [14464/50048]	Loss: 1.4259
Training Epoch: 19 [14592/50048]	Loss: 1.1203
Training Epoch: 19 [14720/50048]	Loss: 1.1710
Training Epoch: 19 [14848/50048]	Loss: 1.0819
Training Epoch: 19 [14976/50048]	Loss: 1.2713
Training Epoch: 19 [15104/50048]	Loss: 1.5632
Training Epoch: 19 [15232/50048]	Loss: 1.1543
Training Epoch: 19 [15360/50048]	Loss: 1.2629
Training Epoch: 19 [15488/50048]	Loss: 1.3246
Training Epoch: 19 [15616/50048]	Loss: 1.2474
Training Epoch: 19 [15744/50048]	Loss: 1.3933
Training Epoch: 19 [15872/50048]	Loss: 1.1660
Training Epoch: 19 [16000/50048]	Loss: 1.2209
Training Epoch: 19 [16128/50048]	Loss: 1.4035
Training Epoch: 19 [16256/50048]	Loss: 1.4984
Training Epoch: 19 [16384/50048]	Loss: 1.2459
Training Epoch: 19 [16512/50048]	Loss: 1.3045
Training Epoch: 19 [16640/50048]	Loss: 1.3738
Training Epoch: 19 [16768/50048]	Loss: 1.2449
Training Epoch: 19 [16896/50048]	Loss: 1.3791
Training Epoch: 19 [17024/50048]	Loss: 1.2367
Training Epoch: 19 [17152/50048]	Loss: 1.5182
Training Epoch: 19 [17280/50048]	Loss: 1.4841
Training Epoch: 19 [17408/50048]	Loss: 1.2289
Training Epoch: 19 [17536/50048]	Loss: 1.2500
Training Epoch: 19 [17664/50048]	Loss: 1.2100
Training Epoch: 19 [17792/50048]	Loss: 1.4196
Training Epoch: 19 [17920/50048]	Loss: 1.4028
Training Epoch: 19 [18048/50048]	Loss: 1.6121
Training Epoch: 19 [18176/50048]	Loss: 0.9406
Training Epoch: 19 [18304/50048]	Loss: 1.2652
Training Epoch: 19 [18432/50048]	Loss: 1.0824
Training Epoch: 19 [18560/50048]	Loss: 1.3824
Training Epoch: 19 [18688/50048]	Loss: 1.4142
Training Epoch: 19 [18816/50048]	Loss: 1.3181
Training Epoch: 19 [18944/50048]	Loss: 1.3090
Training Epoch: 19 [19072/50048]	Loss: 1.3731
Training Epoch: 19 [19200/50048]	Loss: 1.0610
Training Epoch: 19 [19328/50048]	Loss: 1.2393
Training Epoch: 19 [19456/50048]	Loss: 1.1907
Training Epoch: 19 [19584/50048]	Loss: 1.3111
Training Epoch: 19 [19712/50048]	Loss: 1.1729
Training Epoch: 19 [19840/50048]	Loss: 1.1747
Training Epoch: 19 [19968/50048]	Loss: 1.3003
Training Epoch: 19 [20096/50048]	Loss: 1.3658
Training Epoch: 19 [20224/50048]	Loss: 1.3777
Training Epoch: 19 [20352/50048]	Loss: 1.2994
Training Epoch: 19 [20480/50048]	Loss: 1.1549
Training Epoch: 19 [20608/50048]	Loss: 1.2388
Training Epoch: 19 [20736/50048]	Loss: 1.5093
Training Epoch: 19 [20864/50048]	Loss: 1.2373
Training Epoch: 19 [20992/50048]	Loss: 1.1769
Training Epoch: 19 [21120/50048]	Loss: 1.1447
Training Epoch: 19 [21248/50048]	Loss: 1.3713
Training Epoch: 19 [21376/50048]	Loss: 1.1007
Training Epoch: 19 [21504/50048]	Loss: 1.2962
Training Epoch: 19 [21632/50048]	Loss: 1.3115
Training Epoch: 19 [21760/50048]	Loss: 1.5580
Training Epoch: 19 [21888/50048]	Loss: 1.3011
Training Epoch: 19 [22016/50048]	Loss: 1.0685
Training Epoch: 19 [22144/50048]	Loss: 1.2517
Training Epoch: 19 [22272/50048]	Loss: 1.3378
Training Epoch: 19 [22400/50048]	Loss: 1.6095
Training Epoch: 19 [22528/50048]	Loss: 1.2091
Training Epoch: 19 [22656/50048]	Loss: 1.6273
Training Epoch: 19 [22784/50048]	Loss: 1.2725
Training Epoch: 19 [22912/50048]	Loss: 1.3644
Training Epoch: 19 [23040/50048]	Loss: 1.3250
Training Epoch: 19 [23168/50048]	Loss: 1.3850
Training Epoch: 19 [23296/50048]	Loss: 1.3880
Training Epoch: 19 [23424/50048]	Loss: 1.0275
Training Epoch: 19 [23552/50048]	Loss: 1.3904
Training Epoch: 19 [23680/50048]	Loss: 1.2686
Training Epoch: 19 [23808/50048]	Loss: 1.1929
Training Epoch: 19 [23936/50048]	Loss: 1.1850
Training Epoch: 19 [24064/50048]	Loss: 1.2290
Training Epoch: 19 [24192/50048]	Loss: 1.2851
Training Epoch: 19 [24320/50048]	Loss: 1.5291
Training Epoch: 19 [24448/50048]	Loss: 1.5055
Training Epoch: 19 [24576/50048]	Loss: 1.2470
Training Epoch: 19 [24704/50048]	Loss: 1.1390
Training Epoch: 19 [24832/50048]	Loss: 1.2407
Training Epoch: 19 [24960/50048]	Loss: 1.2041
Training Epoch: 19 [25088/50048]	Loss: 1.3663
Training Epoch: 19 [25216/50048]	Loss: 1.3449
Training Epoch: 19 [25344/50048]	Loss: 1.1420
Training Epoch: 19 [25472/50048]	Loss: 1.1688
Training Epoch: 19 [25600/50048]	Loss: 1.2399
Training Epoch: 19 [25728/50048]	Loss: 1.3961
Training Epoch: 19 [25856/50048]	Loss: 1.1680
Training Epoch: 19 [25984/50048]	Loss: 1.4796
Training Epoch: 19 [26112/50048]	Loss: 1.2451
Training Epoch: 19 [26240/50048]	Loss: 1.3939
Training Epoch: 19 [26368/50048]	Loss: 1.4364
Training Epoch: 19 [26496/50048]	Loss: 1.4194
Training Epoch: 19 [26624/50048]	Loss: 1.3988
Training Epoch: 19 [26752/50048]	Loss: 1.1456
Training Epoch: 19 [26880/50048]	Loss: 1.1585
Training Epoch: 19 [27008/50048]	Loss: 1.4453
Training Epoch: 19 [27136/50048]	Loss: 1.2124
Training Epoch: 19 [27264/50048]	Loss: 1.2009
Training Epoch: 19 [27392/50048]	Loss: 1.2286
Training Epoch: 19 [27520/50048]	Loss: 1.5026
Training Epoch: 19 [27648/50048]	Loss: 1.1705
Training Epoch: 19 [27776/50048]	Loss: 1.3459
Training Epoch: 19 [27904/50048]	Loss: 1.1468
Training Epoch: 19 [28032/50048]	Loss: 1.2486
Training Epoch: 19 [28160/50048]	Loss: 1.4963
Training Epoch: 19 [28288/50048]	Loss: 1.1430
Training Epoch: 19 [28416/50048]	Loss: 1.3934
Training Epoch: 19 [28544/50048]	Loss: 1.3373
Training Epoch: 19 [28672/50048]	Loss: 1.1806
Training Epoch: 19 [28800/50048]	Loss: 1.1651
Training Epoch: 19 [28928/50048]	Loss: 1.2582
Training Epoch: 19 [29056/50048]	Loss: 1.2166
Training Epoch: 19 [29184/50048]	Loss: 1.2825
Training Epoch: 19 [29312/50048]	Loss: 1.2984
Training Epoch: 19 [29440/50048]	Loss: 1.4698
Training Epoch: 19 [29568/50048]	Loss: 1.3650
Training Epoch: 19 [29696/50048]	Loss: 1.0763
Training Epoch: 19 [29824/50048]	Loss: 1.2560
Training Epoch: 19 [29952/50048]	Loss: 1.3752
Training Epoch: 19 [30080/50048]	Loss: 1.3380
Training Epoch: 19 [30208/50048]	Loss: 1.4768
Training Epoch: 19 [30336/50048]	Loss: 1.0740
Training Epoch: 19 [30464/50048]	Loss: 1.0715
Training Epoch: 19 [30592/50048]	Loss: 1.4914
Training Epoch: 19 [30720/50048]	Loss: 1.1652
Training Epoch: 19 [30848/50048]	Loss: 1.2677
Training Epoch: 19 [30976/50048]	Loss: 1.1920
Training Epoch: 19 [31104/50048]	Loss: 1.2358
Training Epoch: 19 [31232/50048]	Loss: 1.2199
Training Epoch: 19 [31360/50048]	Loss: 1.1107
Training Epoch: 19 [31488/50048]	Loss: 1.2960
Training Epoch: 19 [31616/50048]	Loss: 1.3660
Training Epoch: 19 [31744/50048]	Loss: 1.3041
Training Epoch: 19 [31872/50048]	Loss: 1.1756
Training Epoch: 19 [32000/50048]	Loss: 1.2039
Training Epoch: 19 [32128/50048]	Loss: 1.3464
Training Epoch: 19 [32256/50048]	Loss: 1.4507
Training Epoch: 19 [32384/50048]	Loss: 1.3368
Training Epoch: 19 [32512/50048]	Loss: 1.5431
Training Epoch: 19 [32640/50048]	Loss: 1.2444
Training Epoch: 19 [32768/50048]	Loss: 1.2841
Training Epoch: 19 [32896/50048]	Loss: 1.2319
Training Epoch: 19 [33024/50048]	Loss: 1.2109
Training Epoch: 19 [33152/50048]	Loss: 1.3884
Training Epoch: 19 [33280/50048]	Loss: 1.0677
Training Epoch: 19 [33408/50048]	Loss: 1.1431
Training Epoch: 19 [33536/50048]	Loss: 1.2674
Training Epoch: 19 [33664/50048]	Loss: 1.0421
Training Epoch: 19 [33792/50048]	Loss: 1.4315
Training Epoch: 19 [33920/50048]	Loss: 1.2723
Training Epoch: 19 [34048/50048]	Loss: 1.2803
Training Epoch: 19 [34176/50048]	Loss: 1.5868
Training Epoch: 19 [34304/50048]	Loss: 1.3040
Training Epoch: 19 [34432/50048]	Loss: 1.2544
Training Epoch: 19 [34560/50048]	Loss: 1.3736
Training Epoch: 19 [34688/50048]	Loss: 1.2790
Training Epoch: 19 [34816/50048]	Loss: 1.5466
Training Epoch: 19 [34944/50048]	Loss: 1.4507
Training Epoch: 19 [35072/50048]	Loss: 1.3207
Training Epoch: 19 [35200/50048]	Loss: 1.2625
Training Epoch: 19 [35328/50048]	Loss: 1.2305
Training Epoch: 19 [35456/50048]	Loss: 1.2728
Training Epoch: 19 [35584/50048]	Loss: 1.3496
Training Epoch: 19 [35712/50048]	Loss: 1.1476
Training Epoch: 19 [35840/50048]	Loss: 1.3181
Training Epoch: 19 [35968/50048]	Loss: 1.1919
Training Epoch: 19 [36096/50048]	Loss: 1.2150
Training Epoch: 19 [36224/50048]	Loss: 1.2070
Training Epoch: 19 [36352/50048]	Loss: 1.4890
Training Epoch: 19 [36480/50048]	Loss: 1.2699
Training Epoch: 19 [36608/50048]	Loss: 1.3895
Training Epoch: 19 [36736/50048]	Loss: 1.1306
Training Epoch: 19 [36864/50048]	Loss: 1.4309
Training Epoch: 19 [36992/50048]	Loss: 1.2904
Training Epoch: 19 [37120/50048]	Loss: 1.3341
Training Epoch: 19 [37248/50048]	Loss: 1.1850
Training Epoch: 19 [37376/50048]	Loss: 1.1404
Training Epoch: 19 [37504/50048]	Loss: 1.2108
Training Epoch: 19 [37632/50048]	Loss: 1.3651
Training Epoch: 19 [37760/50048]	Loss: 1.5562
Training Epoch: 19 [37888/50048]	Loss: 1.1626
Training Epoch: 19 [38016/50048]	Loss: 1.2805
Training Epoch: 19 [38144/50048]	Loss: 1.1363
Training Epoch: 19 [38272/50048]	Loss: 1.4607
Training Epoch: 19 [38400/50048]	Loss: 1.3346
Training Epoch: 19 [38528/50048]	Loss: 1.2586
Training Epoch: 19 [38656/50048]	Loss: 1.1666
Training Epoch: 19 [38784/50048]	Loss: 1.1360
Training Epoch: 19 [38912/50048]	Loss: 1.3567
Training Epoch: 19 [39040/50048]	Loss: 1.2687
Training Epoch: 19 [39168/50048]	Loss: 1.3149
Training Epoch: 19 [39296/50048]	Loss: 1.6489
Training Epoch: 19 [39424/50048]	Loss: 1.3427
Training Epoch: 19 [39552/50048]	Loss: 1.5315
Training Epoch: 19 [39680/50048]	Loss: 1.2128
Training Epoch: 19 [39808/50048]	Loss: 1.1077
Training Epoch: 19 [39936/50048]	Loss: 1.2321
Training Epoch: 19 [40064/50048]	Loss: 1.1766
Training Epoch: 19 [40192/50048]	Loss: 1.3296
Training Epoch: 19 [40320/50048]	Loss: 1.2388
Training Epoch: 19 [40448/50048]	Loss: 1.5046
Training Epoch: 19 [40576/50048]	Loss: 1.3778
Training Epoch: 19 [40704/50048]	Loss: 1.2027
Training Epoch: 19 [40832/50048]	Loss: 1.4762
Training Epoch: 19 [40960/50048]	Loss: 1.2922
Training Epoch: 19 [41088/50048]	Loss: 1.5282
Training Epoch: 19 [41216/50048]	Loss: 1.2035
Training Epoch: 19 [41344/50048]	Loss: 1.3275
Training Epoch: 19 [41472/50048]	Loss: 1.2557
Training Epoch: 19 [41600/50048]	Loss: 1.2881
Training Epoch: 19 [41728/50048]	Loss: 1.3310
Training Epoch: 19 [41856/50048]	Loss: 1.2817
Training Epoch: 19 [41984/50048]	Loss: 1.1027
Training Epoch: 19 [42112/50048]	Loss: 1.1909
Training Epoch: 19 [42240/50048]	Loss: 1.1523
Training Epoch: 19 [42368/50048]	Loss: 1.3572
Training Epoch: 19 [42496/50048]	Loss: 1.5114
Training Epoch: 19 [42624/50048]	Loss: 1.2598
Training Epoch: 19 [42752/50048]	Loss: 1.1589
Training Epoch: 19 [42880/50048]	Loss: 0.9006
Training Epoch: 19 [43008/50048]	Loss: 1.2496
Training Epoch: 19 [43136/50048]	Loss: 1.5330
Training Epoch: 19 [43264/50048]	Loss: 1.4183
Training Epoch: 19 [43392/50048]	Loss: 1.3269
Training Epoch: 19 [43520/50048]	Loss: 1.2469
Training Epoch: 19 [43648/50048]	Loss: 1.2880
Training Epoch: 19 [43776/50048]	Loss: 1.2903
Training Epoch: 19 [43904/50048]	Loss: 1.2658
Training Epoch: 19 [44032/50048]	Loss: 1.3961
Training Epoch: 19 [44160/50048]	Loss: 1.5032
Training Epoch: 19 [44288/50048]	Loss: 1.3611
Training Epoch: 19 [44416/50048]	Loss: 1.3340
Training Epoch: 19 [44544/50048]	Loss: 1.3743
Training Epoch: 19 [44672/50048]	Loss: 1.3028
Training Epoch: 19 [44800/50048]	Loss: 1.2285
Training Epoch: 19 [44928/50048]	Loss: 1.2389
Training Epoch: 19 [45056/50048]	Loss: 1.2097
Training Epoch: 19 [45184/50048]	Loss: 1.3781
Training Epoch: 19 [45312/50048]	Loss: 1.3538
Training Epoch: 19 [45440/50048]	Loss: 1.3531
Training Epoch: 19 [45568/50048]	Loss: 1.4004
Training Epoch: 19 [45696/50048]	Loss: 1.2604
Training Epoch: 19 [45824/50048]	Loss: 1.2684
Training Epoch: 19 [45952/50048]	Loss: 1.1285
Training Epoch: 19 [46080/50048]	Loss: 1.3218
Training Epoch: 19 [46208/50048]	Loss: 1.4688
Training Epoch: 19 [46336/50048]	Loss: 1.2214
Training Epoch: 19 [46464/50048]	Loss: 1.3651
Training Epoch: 19 [46592/50048]	Loss: 1.3839
Training Epoch: 19 [46720/50048]	Loss: 1.1329
Training Epoch: 19 [46848/50048]	Loss: 1.2377
Training Epoch: 19 [46976/50048]	Loss: 1.3511
Training Epoch: 19 [47104/50048]	Loss: 1.1152
Training Epoch: 19 [47232/50048]	Loss: 1.2851
Training Epoch: 19 [47360/50048]	Loss: 1.1805
Training Epoch: 19 [47488/50048]	Loss: 1.2761
Training Epoch: 19 [47616/50048]	Loss: 1.2003
Training Epoch: 19 [47744/50048]	Loss: 1.9004
Training Epoch: 19 [47872/50048]	Loss: 1.2326
Training Epoch: 19 [48000/50048]	Loss: 1.1131
Training Epoch: 19 [48128/50048]	Loss: 1.2116
Training Epoch: 19 [48256/50048]	Loss: 1.4089
Training Epoch: 19 [48384/50048]	Loss: 1.2054
Training Epoch: 19 [48512/50048]	Loss: 1.1858
Training Epoch: 19 [48640/50048]	Loss: 1.0955
Training Epoch: 19 [48768/50048]	Loss: 1.2451
Training Epoch: 19 [48896/50048]	Loss: 1.2689
Training Epoch: 19 [49024/50048]	Loss: 1.3591
Training Epoch: 19 [49152/50048]	Loss: 1.1051
Training Epoch: 19 [49280/50048]	Loss: 1.4527
Training Epoch: 19 [49408/50048]	Loss: 1.2541
Training Epoch: 19 [49536/50048]	Loss: 1.4561
Training Epoch: 19 [49664/50048]	Loss: 1.3499
Training Epoch: 19 [49792/50048]	Loss: 1.2417
Training Epoch: 19 [49920/50048]	Loss: 1.3226
Training Epoch: 19 [50048/50048]	Loss: 1.3534
Validation Epoch: 19, Average loss: 0.0117, Accuracy: 0.5877
Training Epoch: 20 [128/50048]	Loss: 1.3087
Training Epoch: 20 [256/50048]	Loss: 1.5639
Training Epoch: 20 [384/50048]	Loss: 1.3496
Training Epoch: 20 [512/50048]	Loss: 1.2202
Training Epoch: 20 [640/50048]	Loss: 1.0907
Training Epoch: 20 [768/50048]	Loss: 1.2632
Training Epoch: 20 [896/50048]	Loss: 1.2678
Training Epoch: 20 [1024/50048]	Loss: 1.1606
Training Epoch: 20 [1152/50048]	Loss: 1.1977
Training Epoch: 20 [1280/50048]	Loss: 1.2592
Training Epoch: 20 [1408/50048]	Loss: 1.2981
Training Epoch: 20 [1536/50048]	Loss: 1.2106
Training Epoch: 20 [1664/50048]	Loss: 1.0682
Training Epoch: 20 [1792/50048]	Loss: 1.5622
Training Epoch: 20 [1920/50048]	Loss: 1.1835
Training Epoch: 20 [2048/50048]	Loss: 1.4685
Training Epoch: 20 [2176/50048]	Loss: 1.2247
Training Epoch: 20 [2304/50048]	Loss: 1.1601
Training Epoch: 20 [2432/50048]	Loss: 1.2145
Training Epoch: 20 [2560/50048]	Loss: 1.2958
Training Epoch: 20 [2688/50048]	Loss: 1.0282
Training Epoch: 20 [2816/50048]	Loss: 1.1445
Training Epoch: 20 [2944/50048]	Loss: 1.1410
Training Epoch: 20 [3072/50048]	Loss: 1.1530
Training Epoch: 20 [3200/50048]	Loss: 1.4140
Training Epoch: 20 [3328/50048]	Loss: 1.4904
Training Epoch: 20 [3456/50048]	Loss: 1.3774
Training Epoch: 20 [3584/50048]	Loss: 1.0922
Training Epoch: 20 [3712/50048]	Loss: 1.3038
Training Epoch: 20 [3840/50048]	Loss: 1.3790
Training Epoch: 20 [3968/50048]	Loss: 1.3024
Training Epoch: 20 [4096/50048]	Loss: 1.2225
Training Epoch: 20 [4224/50048]	Loss: 1.2545
Training Epoch: 20 [4352/50048]	Loss: 1.2499
Training Epoch: 20 [4480/50048]	Loss: 1.3462
Training Epoch: 20 [4608/50048]	Loss: 1.3281
Training Epoch: 20 [4736/50048]	Loss: 1.0912
Training Epoch: 20 [4864/50048]	Loss: 1.3034
Training Epoch: 20 [4992/50048]	Loss: 1.2119
Training Epoch: 20 [5120/50048]	Loss: 1.1320
Training Epoch: 20 [5248/50048]	Loss: 1.4541
Training Epoch: 20 [5376/50048]	Loss: 1.0207
Training Epoch: 20 [5504/50048]	Loss: 1.2069
Training Epoch: 20 [5632/50048]	Loss: 1.1929
Training Epoch: 20 [5760/50048]	Loss: 1.0135
Training Epoch: 20 [5888/50048]	Loss: 1.1675
Training Epoch: 20 [6016/50048]	Loss: 1.1947
Training Epoch: 20 [6144/50048]	Loss: 1.4131
Training Epoch: 20 [6272/50048]	Loss: 1.1407
Training Epoch: 20 [6400/50048]	Loss: 1.2327
Training Epoch: 20 [6528/50048]	Loss: 1.2451
Training Epoch: 20 [6656/50048]	Loss: 1.1874
Training Epoch: 20 [6784/50048]	Loss: 1.2105
Training Epoch: 20 [6912/50048]	Loss: 1.3549
Training Epoch: 20 [7040/50048]	Loss: 1.4701
Training Epoch: 20 [7168/50048]	Loss: 1.3066
Training Epoch: 20 [7296/50048]	Loss: 1.3637
Training Epoch: 20 [7424/50048]	Loss: 0.9436
Training Epoch: 20 [7552/50048]	Loss: 1.3647
Training Epoch: 20 [7680/50048]	Loss: 1.3060
Training Epoch: 20 [7808/50048]	Loss: 1.1913
Training Epoch: 20 [7936/50048]	Loss: 1.1813
Training Epoch: 20 [8064/50048]	Loss: 1.4585
Training Epoch: 20 [8192/50048]	Loss: 1.0951
Training Epoch: 20 [8320/50048]	Loss: 1.4667
Training Epoch: 20 [8448/50048]	Loss: 1.2645
Training Epoch: 20 [8576/50048]	Loss: 1.3538
Training Epoch: 20 [8704/50048]	Loss: 1.0900
Training Epoch: 20 [8832/50048]	Loss: 1.0165
Training Epoch: 20 [8960/50048]	Loss: 1.3141
Training Epoch: 20 [9088/50048]	Loss: 1.1200
Training Epoch: 20 [9216/50048]	Loss: 1.1080
Training Epoch: 20 [9344/50048]	Loss: 1.3244
Training Epoch: 20 [9472/50048]	Loss: 1.1881
Training Epoch: 20 [9600/50048]	Loss: 1.5112
Training Epoch: 20 [9728/50048]	Loss: 1.3160
Training Epoch: 20 [9856/50048]	Loss: 1.1636
Training Epoch: 20 [9984/50048]	Loss: 1.0916
Training Epoch: 20 [10112/50048]	Loss: 1.1810
Training Epoch: 20 [10240/50048]	Loss: 1.3182
Training Epoch: 20 [10368/50048]	Loss: 1.4839
Training Epoch: 20 [10496/50048]	Loss: 1.3292
Training Epoch: 20 [10624/50048]	Loss: 1.2665
Training Epoch: 20 [10752/50048]	Loss: 1.3555
Training Epoch: 20 [10880/50048]	Loss: 1.3969
Training Epoch: 20 [11008/50048]	Loss: 1.2337
Training Epoch: 20 [11136/50048]	Loss: 1.3214
Training Epoch: 20 [11264/50048]	Loss: 1.3423
Training Epoch: 20 [11392/50048]	Loss: 1.2394
Training Epoch: 20 [11520/50048]	Loss: 1.2330
Training Epoch: 20 [11648/50048]	Loss: 1.2997
Training Epoch: 20 [11776/50048]	Loss: 1.2109
Training Epoch: 20 [11904/50048]	Loss: 1.1946
Training Epoch: 20 [12032/50048]	Loss: 1.3047
Training Epoch: 20 [12160/50048]	Loss: 1.4871
Training Epoch: 20 [12288/50048]	Loss: 1.3273
Training Epoch: 20 [12416/50048]	Loss: 1.4721
Training Epoch: 20 [12544/50048]	Loss: 1.4909
Training Epoch: 20 [12672/50048]	Loss: 1.1639
Training Epoch: 20 [12800/50048]	Loss: 1.2200
Training Epoch: 20 [12928/50048]	Loss: 1.1630
Training Epoch: 20 [13056/50048]	Loss: 1.2259
Training Epoch: 20 [13184/50048]	Loss: 1.2023
Training Epoch: 20 [13312/50048]	Loss: 1.1938
Training Epoch: 20 [13440/50048]	Loss: 1.1041
Training Epoch: 20 [13568/50048]	Loss: 1.2420
Training Epoch: 20 [13696/50048]	Loss: 1.5161
Training Epoch: 20 [13824/50048]	Loss: 1.1911
Training Epoch: 20 [13952/50048]	Loss: 1.2525
Training Epoch: 20 [14080/50048]	Loss: 1.2138
Training Epoch: 20 [14208/50048]	Loss: 1.3037
Training Epoch: 20 [14336/50048]	Loss: 1.1009
Training Epoch: 20 [14464/50048]	Loss: 1.3651
Training Epoch: 20 [14592/50048]	Loss: 1.1762
Training Epoch: 20 [14720/50048]	Loss: 1.1839
Training Epoch: 20 [14848/50048]	Loss: 0.9878
Training Epoch: 20 [14976/50048]	Loss: 1.3352
Training Epoch: 20 [15104/50048]	Loss: 1.3246
Training Epoch: 20 [15232/50048]	Loss: 1.1938
Training Epoch: 20 [15360/50048]	Loss: 1.3711
Training Epoch: 20 [15488/50048]	Loss: 1.3296
Training Epoch: 20 [15616/50048]	Loss: 1.3994
Training Epoch: 20 [15744/50048]	Loss: 1.2514
Training Epoch: 20 [15872/50048]	Loss: 1.4980
Training Epoch: 20 [16000/50048]	Loss: 1.3314
Training Epoch: 20 [16128/50048]	Loss: 1.3345
Training Epoch: 20 [16256/50048]	Loss: 1.2624
Training Epoch: 20 [16384/50048]	Loss: 1.0342
Training Epoch: 20 [16512/50048]	Loss: 1.2952
Training Epoch: 20 [16640/50048]	Loss: 1.0612
Training Epoch: 20 [16768/50048]	Loss: 1.3789
Training Epoch: 20 [16896/50048]	Loss: 1.2599
Training Epoch: 20 [17024/50048]	Loss: 1.2884
Training Epoch: 20 [17152/50048]	Loss: 1.1315
Training Epoch: 20 [17280/50048]	Loss: 1.3120
Training Epoch: 20 [17408/50048]	Loss: 1.3772
Training Epoch: 20 [17536/50048]	Loss: 1.3107
Training Epoch: 20 [17664/50048]	Loss: 1.1486
Training Epoch: 20 [17792/50048]	Loss: 1.2835
Training Epoch: 20 [17920/50048]	Loss: 1.3614
Training Epoch: 20 [18048/50048]	Loss: 1.4766
Training Epoch: 20 [18176/50048]	Loss: 1.1823
Training Epoch: 20 [18304/50048]	Loss: 1.2539
Training Epoch: 20 [18432/50048]	Loss: 1.1336
Training Epoch: 20 [18560/50048]	Loss: 1.0803
Training Epoch: 20 [18688/50048]	Loss: 1.1741
Training Epoch: 20 [18816/50048]	Loss: 1.3332
Training Epoch: 20 [18944/50048]	Loss: 1.2570
Training Epoch: 20 [19072/50048]	Loss: 1.3363
Training Epoch: 20 [19200/50048]	Loss: 1.3878
Training Epoch: 20 [19328/50048]	Loss: 1.1396
Training Epoch: 20 [19456/50048]	Loss: 1.2230
Training Epoch: 20 [19584/50048]	Loss: 1.3061
Training Epoch: 20 [19712/50048]	Loss: 1.2937
Training Epoch: 20 [19840/50048]	Loss: 1.4589
Training Epoch: 20 [19968/50048]	Loss: 1.4796
Training Epoch: 20 [20096/50048]	Loss: 1.1526
Training Epoch: 20 [20224/50048]	Loss: 1.1277
Training Epoch: 20 [20352/50048]	Loss: 1.1950
Training Epoch: 20 [20480/50048]	Loss: 1.3183
Training Epoch: 20 [20608/50048]	Loss: 1.1024
Training Epoch: 20 [20736/50048]	Loss: 1.3014
Training Epoch: 20 [20864/50048]	Loss: 1.2360
Training Epoch: 20 [20992/50048]	Loss: 1.0229
Training Epoch: 20 [21120/50048]	Loss: 1.2455
Training Epoch: 20 [21248/50048]	Loss: 1.2263
Training Epoch: 20 [21376/50048]	Loss: 1.2500
Training Epoch: 20 [21504/50048]	Loss: 1.1444
Training Epoch: 20 [21632/50048]	Loss: 1.3253
Training Epoch: 20 [21760/50048]	Loss: 1.0491
Training Epoch: 20 [21888/50048]	Loss: 1.2189
Training Epoch: 20 [22016/50048]	Loss: 1.4601
Training Epoch: 20 [22144/50048]	Loss: 1.2893
Training Epoch: 20 [22272/50048]	Loss: 1.3783
Training Epoch: 20 [22400/50048]	Loss: 1.1851
Training Epoch: 20 [22528/50048]	Loss: 1.3786
Training Epoch: 20 [22656/50048]	Loss: 0.9498
Training Epoch: 20 [22784/50048]	Loss: 1.1021
Training Epoch: 20 [22912/50048]	Loss: 1.0851
Training Epoch: 20 [23040/50048]	Loss: 0.9567
Training Epoch: 20 [23168/50048]	Loss: 1.2163
Training Epoch: 20 [23296/50048]	Loss: 1.1309
Training Epoch: 20 [23424/50048]	Loss: 1.3338
Training Epoch: 20 [23552/50048]	Loss: 1.1302
Training Epoch: 20 [23680/50048]	Loss: 1.3729
Training Epoch: 20 [23808/50048]	Loss: 1.3659
Training Epoch: 20 [23936/50048]	Loss: 1.2996
Training Epoch: 20 [24064/50048]	Loss: 1.0213
Training Epoch: 20 [24192/50048]	Loss: 1.3467
Training Epoch: 20 [24320/50048]	Loss: 1.3787
Training Epoch: 20 [24448/50048]	Loss: 1.0400
Training Epoch: 20 [24576/50048]	Loss: 1.3667
Training Epoch: 20 [24704/50048]	Loss: 1.3061
Training Epoch: 20 [24832/50048]	Loss: 1.3275
Training Epoch: 20 [24960/50048]	Loss: 1.6491
Training Epoch: 20 [25088/50048]	Loss: 1.3237
Training Epoch: 20 [25216/50048]	Loss: 1.1447
Training Epoch: 20 [25344/50048]	Loss: 1.3724
Training Epoch: 20 [25472/50048]	Loss: 1.1443
Training Epoch: 20 [25600/50048]	Loss: 1.0854
Training Epoch: 20 [25728/50048]	Loss: 1.4631
Training Epoch: 20 [25856/50048]	Loss: 1.3253
Training Epoch: 20 [25984/50048]	Loss: 1.1510
Training Epoch: 20 [26112/50048]	Loss: 1.1596
Training Epoch: 20 [26240/50048]	Loss: 1.2878
Training Epoch: 20 [26368/50048]	Loss: 1.0586
Training Epoch: 20 [26496/50048]	Loss: 1.2368
Training Epoch: 20 [26624/50048]	Loss: 1.2973
Training Epoch: 20 [26752/50048]	Loss: 1.1069
Training Epoch: 20 [26880/50048]	Loss: 1.1102
Training Epoch: 20 [27008/50048]	Loss: 1.2117
Training Epoch: 20 [27136/50048]	Loss: 1.0901
Training Epoch: 20 [27264/50048]	Loss: 1.3535
Training Epoch: 20 [27392/50048]	Loss: 1.1363
Training Epoch: 20 [27520/50048]	Loss: 0.9740
Training Epoch: 20 [27648/50048]	Loss: 1.3548
Training Epoch: 20 [27776/50048]	Loss: 1.2016
Training Epoch: 20 [27904/50048]	Loss: 1.2363
Training Epoch: 20 [28032/50048]	Loss: 1.3404
Training Epoch: 20 [28160/50048]	Loss: 1.3029
Training Epoch: 20 [28288/50048]	Loss: 1.3077
Training Epoch: 20 [28416/50048]	Loss: 1.0565
Training Epoch: 20 [28544/50048]	Loss: 1.2257
Training Epoch: 20 [28672/50048]	Loss: 1.4183
Training Epoch: 20 [28800/50048]	Loss: 1.1538
Training Epoch: 20 [28928/50048]	Loss: 1.1756
Training Epoch: 20 [29056/50048]	Loss: 1.4519
Training Epoch: 20 [29184/50048]	Loss: 1.3261
Training Epoch: 20 [29312/50048]	Loss: 1.2895
Training Epoch: 20 [29440/50048]	Loss: 1.1184
Training Epoch: 20 [29568/50048]	Loss: 1.2119
Training Epoch: 20 [29696/50048]	Loss: 1.2870
Training Epoch: 20 [29824/50048]	Loss: 1.3579
Training Epoch: 20 [29952/50048]	Loss: 1.2895
Training Epoch: 20 [30080/50048]	Loss: 1.1609
Training Epoch: 20 [30208/50048]	Loss: 1.2029
Training Epoch: 20 [30336/50048]	Loss: 1.1254
Training Epoch: 20 [30464/50048]	Loss: 1.6172
Training Epoch: 20 [30592/50048]	Loss: 1.3215
Training Epoch: 20 [30720/50048]	Loss: 1.1988
Training Epoch: 20 [30848/50048]	Loss: 1.1470
Training Epoch: 20 [30976/50048]	Loss: 1.1708
Training Epoch: 20 [31104/50048]	Loss: 1.2344
Training Epoch: 20 [31232/50048]	Loss: 1.4111
Training Epoch: 20 [31360/50048]	Loss: 1.1725
Training Epoch: 20 [31488/50048]	Loss: 1.3312
Training Epoch: 20 [31616/50048]	Loss: 1.2433
Training Epoch: 20 [31744/50048]	Loss: 1.1678
Training Epoch: 20 [31872/50048]	Loss: 1.3950
Training Epoch: 20 [32000/50048]	Loss: 1.1940
Training Epoch: 20 [32128/50048]	Loss: 1.3151
Training Epoch: 20 [32256/50048]	Loss: 1.3681
Training Epoch: 20 [32384/50048]	Loss: 1.3466
Training Epoch: 20 [32512/50048]	Loss: 1.2138
Training Epoch: 20 [32640/50048]	Loss: 1.4026
Training Epoch: 20 [32768/50048]	Loss: 1.4390
Training Epoch: 20 [32896/50048]	Loss: 1.2557
Training Epoch: 20 [33024/50048]	Loss: 1.2006
Training Epoch: 20 [33152/50048]	Loss: 1.4170
Training Epoch: 20 [33280/50048]	Loss: 1.2160
Training Epoch: 20 [33408/50048]	Loss: 1.1323
Training Epoch: 20 [33536/50048]	Loss: 1.3428
Training Epoch: 20 [33664/50048]	Loss: 1.5617
Training Epoch: 20 [33792/50048]	Loss: 1.4331
Training Epoch: 20 [33920/50048]	Loss: 1.3100
Training Epoch: 20 [34048/50048]	Loss: 1.3617
Training Epoch: 20 [34176/50048]	Loss: 1.3934
Training Epoch: 20 [34304/50048]	Loss: 1.3497
Training Epoch: 20 [34432/50048]	Loss: 1.0952
Training Epoch: 20 [34560/50048]	Loss: 1.2525
Training Epoch: 20 [34688/50048]	Loss: 1.3986
Training Epoch: 20 [34816/50048]	Loss: 1.3856
Training Epoch: 20 [34944/50048]	Loss: 1.1177
Training Epoch: 20 [35072/50048]	Loss: 1.5230
Training Epoch: 20 [35200/50048]	Loss: 1.3355
Training Epoch: 20 [35328/50048]	Loss: 1.5026
Training Epoch: 20 [35456/50048]	Loss: 1.1687
Training Epoch: 20 [35584/50048]	Loss: 1.2956
Training Epoch: 20 [35712/50048]	Loss: 1.3626
Training Epoch: 20 [35840/50048]	Loss: 1.3002
Training Epoch: 20 [35968/50048]	Loss: 1.2461
Training Epoch: 20 [36096/50048]	Loss: 1.2384
Training Epoch: 20 [36224/50048]	Loss: 1.2234
Training Epoch: 20 [36352/50048]	Loss: 1.0258
Training Epoch: 20 [36480/50048]	Loss: 1.3584
Training Epoch: 20 [36608/50048]	Loss: 1.3467
Training Epoch: 20 [36736/50048]	Loss: 1.3515
Training Epoch: 20 [36864/50048]	Loss: 1.1793
Training Epoch: 20 [36992/50048]	Loss: 1.1920
Training Epoch: 20 [37120/50048]	Loss: 1.1124
Training Epoch: 20 [37248/50048]	Loss: 1.0974
Training Epoch: 20 [37376/50048]	Loss: 1.2887
Training Epoch: 20 [37504/50048]	Loss: 1.0584
Training Epoch: 20 [37632/50048]	Loss: 1.3168
Training Epoch: 20 [37760/50048]	Loss: 1.2811
Training Epoch: 20 [37888/50048]	Loss: 1.4426
Training Epoch: 20 [38016/50048]	Loss: 1.2643
Training Epoch: 20 [38144/50048]	Loss: 1.2365
Training Epoch: 20 [38272/50048]	Loss: 1.3438
Training Epoch: 20 [38400/50048]	Loss: 1.3645
Training Epoch: 20 [38528/50048]	Loss: 1.2686
Training Epoch: 20 [38656/50048]	Loss: 1.6201
Training Epoch: 20 [38784/50048]	Loss: 1.0665
Training Epoch: 20 [38912/50048]	Loss: 1.1804
Training Epoch: 20 [39040/50048]	Loss: 1.2646
Training Epoch: 20 [39168/50048]	Loss: 1.3623
Training Epoch: 20 [39296/50048]	Loss: 1.3226
Training Epoch: 20 [39424/50048]	Loss: 1.1991
Training Epoch: 20 [39552/50048]	Loss: 1.2638
Training Epoch: 20 [39680/50048]	Loss: 1.4649
Training Epoch: 20 [39808/50048]	Loss: 1.3721
Training Epoch: 20 [39936/50048]	Loss: 1.3252
Training Epoch: 20 [40064/50048]	Loss: 1.3839
Training Epoch: 20 [40192/50048]	Loss: 1.4173
Training Epoch: 20 [40320/50048]	Loss: 1.1854
Training Epoch: 20 [40448/50048]	Loss: 1.0365
Training Epoch: 20 [40576/50048]	Loss: 1.0295
Training Epoch: 20 [40704/50048]	Loss: 1.1161
Training Epoch: 20 [40832/50048]	Loss: 1.2012
Training Epoch: 20 [40960/50048]	Loss: 1.2640
Training Epoch: 20 [41088/50048]	Loss: 1.3800
Training Epoch: 20 [41216/50048]	Loss: 1.1848
Training Epoch: 20 [41344/50048]	Loss: 1.2058
Training Epoch: 20 [41472/50048]	Loss: 1.1530
Training Epoch: 20 [41600/50048]	Loss: 1.1362
Training Epoch: 20 [41728/50048]	Loss: 1.5066
Training Epoch: 20 [41856/50048]	Loss: 1.3483
Training Epoch: 20 [41984/50048]	Loss: 1.2543
Training Epoch: 20 [42112/50048]	Loss: 1.3333
Training Epoch: 20 [42240/50048]	Loss: 1.2019
Training Epoch: 20 [42368/50048]	Loss: 1.3008
Training Epoch: 20 [42496/50048]	Loss: 1.1223
Training Epoch: 20 [42624/50048]	Loss: 1.1355
Training Epoch: 20 [42752/50048]	Loss: 1.4109
Training Epoch: 20 [42880/50048]	Loss: 1.0914
Training Epoch: 20 [43008/50048]	Loss: 1.3029
Training Epoch: 20 [43136/50048]	Loss: 1.4818
Training Epoch: 20 [43264/50048]	Loss: 1.1888
Training Epoch: 20 [43392/50048]	Loss: 1.3869
Training Epoch: 20 [43520/50048]	Loss: 1.2128
Training Epoch: 20 [43648/50048]	Loss: 1.3356
Training Epoch: 20 [43776/50048]	Loss: 1.3706
Training Epoch: 20 [43904/50048]	Loss: 1.3233
Training Epoch: 20 [44032/50048]	Loss: 1.1487
Training Epoch: 20 [44160/50048]	Loss: 1.4079
Training Epoch: 20 [44288/50048]	Loss: 1.2464
Training Epoch: 20 [44416/50048]	Loss: 1.0678
Training Epoch: 20 [44544/50048]	Loss: 1.2608
Training Epoch: 20 [44672/50048]	Loss: 1.1931
Training Epoch: 20 [44800/50048]	Loss: 1.7601
Training Epoch: 20 [44928/50048]	Loss: 1.2825
Training Epoch: 20 [45056/50048]	Loss: 1.3463
Training Epoch: 20 [45184/50048]	Loss: 1.2857
Training Epoch: 20 [45312/50048]	Loss: 1.2918
Training Epoch: 20 [45440/50048]	Loss: 1.2292
Training Epoch: 20 [45568/50048]	Loss: 1.2809
Training Epoch: 20 [45696/50048]	Loss: 1.4788
Training Epoch: 20 [45824/50048]	Loss: 1.3629
Training Epoch: 20 [45952/50048]	Loss: 1.4946
Training Epoch: 20 [46080/50048]	Loss: 1.4173
Training Epoch: 20 [46208/50048]	Loss: 1.3677
Training Epoch: 20 [46336/50048]	Loss: 1.1603
Training Epoch: 20 [46464/50048]	Loss: 1.1620
Training Epoch: 20 [46592/50048]	Loss: 1.2276
Training Epoch: 20 [46720/50048]	Loss: 1.2753
Training Epoch: 20 [46848/50048]	Loss: 1.7096
Training Epoch: 20 [46976/50048]	Loss: 1.1078
Training Epoch: 20 [47104/50048]	Loss: 1.4315
Training Epoch: 20 [47232/50048]	Loss: 1.3525
Training Epoch: 20 [47360/50048]	Loss: 1.1873
Training Epoch: 20 [47488/50048]	Loss: 1.1118
Training Epoch: 20 [47616/50048]	Loss: 1.4299
Training Epoch: 20 [47744/50048]	Loss: 1.2066
Training Epoch: 20 [47872/50048]	Loss: 1.2783
Training Epoch: 20 [48000/50048]	Loss: 1.1557
Training Epoch: 20 [48128/50048]	Loss: 1.3013
Training Epoch: 20 [48256/50048]	Loss: 1.3662
Training Epoch: 20 [48384/50048]	Loss: 1.1979
Training Epoch: 20 [48512/50048]	Loss: 1.2603
Training Epoch: 20 [48640/50048]	Loss: 1.0473
Training Epoch: 20 [48768/50048]	Loss: 1.2762
Training Epoch: 20 [48896/50048]	Loss: 1.2435
Training Epoch: 20 [49024/50048]	Loss: 1.1790
Training Epoch: 20 [49152/50048]	Loss: 1.1546
Training Epoch: 20 [49280/50048]	Loss: 1.2063
Training Epoch: 20 [49408/50048]	Loss: 1.2862
Training Epoch: 20 [49536/50048]	Loss: 1.3533
Training Epoch: 20 [49664/50048]	Loss: 1.4890
Training Epoch: 20 [49792/50048]	Loss: 1.1493
Training Epoch: 20 [49920/50048]	Loss: 1.3880
Training Epoch: 20 [50048/50048]	Loss: 1.3568
Validation Epoch: 20, Average loss: 0.0117, Accuracy: 0.5921
Training Epoch: 21 [128/50048]	Loss: 1.1343
Training Epoch: 21 [256/50048]	Loss: 1.2185
Training Epoch: 21 [384/50048]	Loss: 1.2962
Training Epoch: 21 [512/50048]	Loss: 1.3835
Training Epoch: 21 [640/50048]	Loss: 1.3288
Training Epoch: 21 [768/50048]	Loss: 1.1428
Training Epoch: 21 [896/50048]	Loss: 1.1734
Training Epoch: 21 [1024/50048]	Loss: 1.3408
Training Epoch: 21 [1152/50048]	Loss: 1.0882
Training Epoch: 21 [1280/50048]	Loss: 1.0909
Training Epoch: 21 [1408/50048]	Loss: 1.3915
Training Epoch: 21 [1536/50048]	Loss: 1.2522
Training Epoch: 21 [1664/50048]	Loss: 1.3185
Training Epoch: 21 [1792/50048]	Loss: 1.2317
Training Epoch: 21 [1920/50048]	Loss: 1.1257
Training Epoch: 21 [2048/50048]	Loss: 1.4577
Training Epoch: 21 [2176/50048]	Loss: 1.2898
Training Epoch: 21 [2304/50048]	Loss: 1.1199
Training Epoch: 21 [2432/50048]	Loss: 1.3712
Training Epoch: 21 [2560/50048]	Loss: 1.3795
Training Epoch: 21 [2688/50048]	Loss: 1.4235
Training Epoch: 21 [2816/50048]	Loss: 1.1614
Training Epoch: 21 [2944/50048]	Loss: 1.1188
Training Epoch: 21 [3072/50048]	Loss: 1.2340
Training Epoch: 21 [3200/50048]	Loss: 1.3197
Training Epoch: 21 [3328/50048]	Loss: 1.1231
Training Epoch: 21 [3456/50048]	Loss: 1.3222
Training Epoch: 21 [3584/50048]	Loss: 1.2283
Training Epoch: 21 [3712/50048]	Loss: 1.2330
Training Epoch: 21 [3840/50048]	Loss: 1.2575
Training Epoch: 21 [3968/50048]	Loss: 1.4296
Training Epoch: 21 [4096/50048]	Loss: 1.1853
Training Epoch: 21 [4224/50048]	Loss: 1.2297
Training Epoch: 21 [4352/50048]	Loss: 1.1419
Training Epoch: 21 [4480/50048]	Loss: 1.1879
Training Epoch: 21 [4608/50048]	Loss: 1.1845
Training Epoch: 21 [4736/50048]	Loss: 1.1382
Training Epoch: 21 [4864/50048]	Loss: 1.3714
Training Epoch: 21 [4992/50048]	Loss: 1.3096
Training Epoch: 21 [5120/50048]	Loss: 1.0616
Training Epoch: 21 [5248/50048]	Loss: 1.2077
Training Epoch: 21 [5376/50048]	Loss: 1.1002
Training Epoch: 21 [5504/50048]	Loss: 1.5182
Training Epoch: 21 [5632/50048]	Loss: 1.0320
Training Epoch: 21 [5760/50048]	Loss: 1.3730
Training Epoch: 21 [5888/50048]	Loss: 1.3387
Training Epoch: 21 [6016/50048]	Loss: 1.3312
Training Epoch: 21 [6144/50048]	Loss: 1.1624
Training Epoch: 21 [6272/50048]	Loss: 1.3370
Training Epoch: 21 [6400/50048]	Loss: 1.1099
Training Epoch: 21 [6528/50048]	Loss: 1.1954
Training Epoch: 21 [6656/50048]	Loss: 1.2301
Training Epoch: 21 [6784/50048]	Loss: 1.1304
Training Epoch: 21 [6912/50048]	Loss: 1.2244
Training Epoch: 21 [7040/50048]	Loss: 1.1214
Training Epoch: 21 [7168/50048]	Loss: 1.3681
Training Epoch: 21 [7296/50048]	Loss: 1.0592
Training Epoch: 21 [7424/50048]	Loss: 1.2394
Training Epoch: 21 [7552/50048]	Loss: 1.0447
Training Epoch: 21 [7680/50048]	Loss: 0.9071
Training Epoch: 21 [7808/50048]	Loss: 1.2019
Training Epoch: 21 [7936/50048]	Loss: 1.3862
Training Epoch: 21 [8064/50048]	Loss: 1.2482
Training Epoch: 21 [8192/50048]	Loss: 1.4495
Training Epoch: 21 [8320/50048]	Loss: 1.2179
Training Epoch: 21 [8448/50048]	Loss: 0.9870
Training Epoch: 21 [8576/50048]	Loss: 1.3831
Training Epoch: 21 [8704/50048]	Loss: 1.2683
Training Epoch: 21 [8832/50048]	Loss: 1.1858
Training Epoch: 21 [8960/50048]	Loss: 1.1766
Training Epoch: 21 [9088/50048]	Loss: 1.1468
Training Epoch: 21 [9216/50048]	Loss: 1.1411
Training Epoch: 21 [9344/50048]	Loss: 1.2147
Training Epoch: 21 [9472/50048]	Loss: 1.0864
Training Epoch: 21 [9600/50048]	Loss: 1.2301
Training Epoch: 21 [9728/50048]	Loss: 1.5169
Training Epoch: 21 [9856/50048]	Loss: 1.0583
Training Epoch: 21 [9984/50048]	Loss: 1.3224
Training Epoch: 21 [10112/50048]	Loss: 1.2165
Training Epoch: 21 [10240/50048]	Loss: 1.2893
Training Epoch: 21 [10368/50048]	Loss: 1.2143
Training Epoch: 21 [10496/50048]	Loss: 1.2197
Training Epoch: 21 [10624/50048]	Loss: 1.0666
Training Epoch: 21 [10752/50048]	Loss: 1.2302
Training Epoch: 21 [10880/50048]	Loss: 1.3283
Training Epoch: 21 [11008/50048]	Loss: 1.2832
Training Epoch: 21 [11136/50048]	Loss: 1.2300
Training Epoch: 21 [11264/50048]	Loss: 1.2247
Training Epoch: 21 [11392/50048]	Loss: 1.2599
Training Epoch: 21 [11520/50048]	Loss: 1.2163
Training Epoch: 21 [11648/50048]	Loss: 1.1565
Training Epoch: 21 [11776/50048]	Loss: 1.2735
Training Epoch: 21 [11904/50048]	Loss: 1.0204
Training Epoch: 21 [12032/50048]	Loss: 1.0197
Training Epoch: 21 [12160/50048]	Loss: 1.1594
Training Epoch: 21 [12288/50048]	Loss: 1.0029
Training Epoch: 21 [12416/50048]	Loss: 1.3235
Training Epoch: 21 [12544/50048]	Loss: 1.0756
Training Epoch: 21 [12672/50048]	Loss: 1.1107
Training Epoch: 21 [12800/50048]	Loss: 1.1648
Training Epoch: 21 [12928/50048]	Loss: 1.1901
Training Epoch: 21 [13056/50048]	Loss: 1.0016
Training Epoch: 21 [13184/50048]	Loss: 1.3524
Training Epoch: 21 [13312/50048]	Loss: 1.2433
Training Epoch: 21 [13440/50048]	Loss: 1.3780
Training Epoch: 21 [13568/50048]	Loss: 1.0690
Training Epoch: 21 [13696/50048]	Loss: 1.3701
Training Epoch: 21 [13824/50048]	Loss: 1.4353
Training Epoch: 21 [13952/50048]	Loss: 0.8455
Training Epoch: 21 [14080/50048]	Loss: 1.3646
Training Epoch: 21 [14208/50048]	Loss: 1.2851
Training Epoch: 21 [14336/50048]	Loss: 1.4141
Training Epoch: 21 [14464/50048]	Loss: 0.9891
Training Epoch: 21 [14592/50048]	Loss: 1.0373
Training Epoch: 21 [14720/50048]	Loss: 1.0106
Training Epoch: 21 [14848/50048]	Loss: 1.3422
Training Epoch: 21 [14976/50048]	Loss: 1.1617
Training Epoch: 21 [15104/50048]	Loss: 1.5687
Training Epoch: 21 [15232/50048]	Loss: 1.0073
Training Epoch: 21 [15360/50048]	Loss: 1.0029
Training Epoch: 21 [15488/50048]	Loss: 1.2368
Training Epoch: 21 [15616/50048]	Loss: 1.2472
Training Epoch: 21 [15744/50048]	Loss: 1.2077
Training Epoch: 21 [15872/50048]	Loss: 1.5200
Training Epoch: 21 [16000/50048]	Loss: 1.0934
Training Epoch: 21 [16128/50048]	Loss: 1.2015
Training Epoch: 21 [16256/50048]	Loss: 1.2474
Training Epoch: 21 [16384/50048]	Loss: 1.1886
Training Epoch: 21 [16512/50048]	Loss: 1.2474
Training Epoch: 21 [16640/50048]	Loss: 1.3394
Training Epoch: 21 [16768/50048]	Loss: 1.5438
Training Epoch: 21 [16896/50048]	Loss: 1.1131
Training Epoch: 21 [17024/50048]	Loss: 1.1569
Training Epoch: 21 [17152/50048]	Loss: 1.0385
Training Epoch: 21 [17280/50048]	Loss: 1.2809
Training Epoch: 21 [17408/50048]	Loss: 1.3251
Training Epoch: 21 [17536/50048]	Loss: 1.3527
Training Epoch: 21 [17664/50048]	Loss: 1.2672
Training Epoch: 21 [17792/50048]	Loss: 1.1763
Training Epoch: 21 [17920/50048]	Loss: 1.1740
Training Epoch: 21 [18048/50048]	Loss: 1.1258
Training Epoch: 21 [18176/50048]	Loss: 1.3071
Training Epoch: 21 [18304/50048]	Loss: 1.0877
Training Epoch: 21 [18432/50048]	Loss: 1.3817
Training Epoch: 21 [18560/50048]	Loss: 1.2141
Training Epoch: 21 [18688/50048]	Loss: 1.4538
Training Epoch: 21 [18816/50048]	Loss: 1.3221
Training Epoch: 21 [18944/50048]	Loss: 1.2972
Training Epoch: 21 [19072/50048]	Loss: 1.1677
Training Epoch: 21 [19200/50048]	Loss: 1.3825
Training Epoch: 21 [19328/50048]	Loss: 1.5009
Training Epoch: 21 [19456/50048]	Loss: 1.0405
Training Epoch: 21 [19584/50048]	Loss: 1.2478
Training Epoch: 21 [19712/50048]	Loss: 1.1466
Training Epoch: 21 [19840/50048]	Loss: 1.1547
Training Epoch: 21 [19968/50048]	Loss: 1.0613
Training Epoch: 21 [20096/50048]	Loss: 1.2262
Training Epoch: 21 [20224/50048]	Loss: 1.7675
Training Epoch: 21 [20352/50048]	Loss: 1.1531
Training Epoch: 21 [20480/50048]	Loss: 1.1036
Training Epoch: 21 [20608/50048]	Loss: 1.2575
Training Epoch: 21 [20736/50048]	Loss: 1.3214
Training Epoch: 21 [20864/50048]	Loss: 1.1398
Training Epoch: 21 [20992/50048]	Loss: 0.9570
Training Epoch: 21 [21120/50048]	Loss: 1.3082
Training Epoch: 21 [21248/50048]	Loss: 1.1917
Training Epoch: 21 [21376/50048]	Loss: 1.0021
Training Epoch: 21 [21504/50048]	Loss: 0.9516
Training Epoch: 21 [21632/50048]	Loss: 1.3575
Training Epoch: 21 [21760/50048]	Loss: 1.0984
Training Epoch: 21 [21888/50048]	Loss: 1.4717
Training Epoch: 21 [22016/50048]	Loss: 1.0794
Training Epoch: 21 [22144/50048]	Loss: 1.2331
Training Epoch: 21 [22272/50048]	Loss: 1.3875
Training Epoch: 21 [22400/50048]	Loss: 1.1532
Training Epoch: 21 [22528/50048]	Loss: 1.1477
Training Epoch: 21 [22656/50048]	Loss: 1.3202
Training Epoch: 21 [22784/50048]	Loss: 1.1338
Training Epoch: 21 [22912/50048]	Loss: 1.1189
Training Epoch: 21 [23040/50048]	Loss: 1.1060
Training Epoch: 21 [23168/50048]	Loss: 1.1082
Training Epoch: 21 [23296/50048]	Loss: 1.0703
Training Epoch: 21 [23424/50048]	Loss: 1.4647
Training Epoch: 21 [23552/50048]	Loss: 1.3581
Training Epoch: 21 [23680/50048]	Loss: 1.0879
Training Epoch: 21 [23808/50048]	Loss: 1.2288
Training Epoch: 21 [23936/50048]	Loss: 1.2661
Training Epoch: 21 [24064/50048]	Loss: 1.2001
Training Epoch: 21 [24192/50048]	Loss: 1.4398
Training Epoch: 21 [24320/50048]	Loss: 1.2060
Training Epoch: 21 [24448/50048]	Loss: 1.0857
Training Epoch: 21 [24576/50048]	Loss: 1.1495
Training Epoch: 21 [24704/50048]	Loss: 1.1115
Training Epoch: 21 [24832/50048]	Loss: 1.3809
Training Epoch: 21 [24960/50048]	Loss: 1.2393
Training Epoch: 21 [25088/50048]	Loss: 1.1208
Training Epoch: 21 [25216/50048]	Loss: 1.2694
Training Epoch: 21 [25344/50048]	Loss: 1.2483
Training Epoch: 21 [25472/50048]	Loss: 1.2830
Training Epoch: 21 [25600/50048]	Loss: 1.2891
Training Epoch: 21 [25728/50048]	Loss: 1.2076
Training Epoch: 21 [25856/50048]	Loss: 1.1989
Training Epoch: 21 [25984/50048]	Loss: 1.3140
Training Epoch: 21 [26112/50048]	Loss: 1.1206
Training Epoch: 21 [26240/50048]	Loss: 1.3243
Training Epoch: 21 [26368/50048]	Loss: 1.3230
Training Epoch: 21 [26496/50048]	Loss: 1.2570
Training Epoch: 21 [26624/50048]	Loss: 1.0994
Training Epoch: 21 [26752/50048]	Loss: 1.1229
Training Epoch: 21 [26880/50048]	Loss: 1.2039
Training Epoch: 21 [27008/50048]	Loss: 1.2660
Training Epoch: 21 [27136/50048]	Loss: 1.4084
Training Epoch: 21 [27264/50048]	Loss: 1.0110
Training Epoch: 21 [27392/50048]	Loss: 1.0583
Training Epoch: 21 [27520/50048]	Loss: 1.4053
Training Epoch: 21 [27648/50048]	Loss: 1.3400
Training Epoch: 21 [27776/50048]	Loss: 1.0413
Training Epoch: 21 [27904/50048]	Loss: 1.1916
Training Epoch: 21 [28032/50048]	Loss: 1.2941
Training Epoch: 21 [28160/50048]	Loss: 1.2768
Training Epoch: 21 [28288/50048]	Loss: 1.4358
Training Epoch: 21 [28416/50048]	Loss: 1.3492
Training Epoch: 21 [28544/50048]	Loss: 1.1989
Training Epoch: 21 [28672/50048]	Loss: 1.2136
Training Epoch: 21 [28800/50048]	Loss: 1.3142
Training Epoch: 21 [28928/50048]	Loss: 1.0145
Training Epoch: 21 [29056/50048]	Loss: 1.3451
Training Epoch: 21 [29184/50048]	Loss: 1.2133
Training Epoch: 21 [29312/50048]	Loss: 1.2973
Training Epoch: 21 [29440/50048]	Loss: 1.1877
Training Epoch: 21 [29568/50048]	Loss: 1.3173
Training Epoch: 21 [29696/50048]	Loss: 1.1794
Training Epoch: 21 [29824/50048]	Loss: 1.2882
Training Epoch: 21 [29952/50048]	Loss: 1.2003
Training Epoch: 21 [30080/50048]	Loss: 1.1796
Training Epoch: 21 [30208/50048]	Loss: 1.3839
Training Epoch: 21 [30336/50048]	Loss: 1.2035
Training Epoch: 21 [30464/50048]	Loss: 1.2996
Training Epoch: 21 [30592/50048]	Loss: 1.3283
Training Epoch: 21 [30720/50048]	Loss: 1.3202
Training Epoch: 21 [30848/50048]	Loss: 1.1941
Training Epoch: 21 [30976/50048]	Loss: 1.1337
Training Epoch: 21 [31104/50048]	Loss: 1.2310
Training Epoch: 21 [31232/50048]	Loss: 1.1283
Training Epoch: 21 [31360/50048]	Loss: 1.2894
Training Epoch: 21 [31488/50048]	Loss: 1.3550
Training Epoch: 21 [31616/50048]	Loss: 1.1214
Training Epoch: 21 [31744/50048]	Loss: 1.3051
Training Epoch: 21 [31872/50048]	Loss: 1.2692
Training Epoch: 21 [32000/50048]	Loss: 1.1854
Training Epoch: 21 [32128/50048]	Loss: 1.0207
Training Epoch: 21 [32256/50048]	Loss: 1.2476
Training Epoch: 21 [32384/50048]	Loss: 1.2495
Training Epoch: 21 [32512/50048]	Loss: 1.2266
Training Epoch: 21 [32640/50048]	Loss: 1.1108
Training Epoch: 21 [32768/50048]	Loss: 1.5482
Training Epoch: 21 [32896/50048]	Loss: 1.3094
Training Epoch: 21 [33024/50048]	Loss: 1.3784
Training Epoch: 21 [33152/50048]	Loss: 1.3889
Training Epoch: 21 [33280/50048]	Loss: 1.1856
Training Epoch: 21 [33408/50048]	Loss: 1.1736
Training Epoch: 21 [33536/50048]	Loss: 1.1972
Training Epoch: 21 [33664/50048]	Loss: 1.3405
Training Epoch: 21 [33792/50048]	Loss: 1.5450
Training Epoch: 21 [33920/50048]	Loss: 1.5770
Training Epoch: 21 [34048/50048]	Loss: 1.3710
Training Epoch: 21 [34176/50048]	Loss: 1.3066
Training Epoch: 21 [34304/50048]	Loss: 1.3098
Training Epoch: 21 [34432/50048]	Loss: 1.4724
Training Epoch: 21 [34560/50048]	Loss: 1.3781
Training Epoch: 21 [34688/50048]	Loss: 1.3310
Training Epoch: 21 [34816/50048]	Loss: 1.0710
Training Epoch: 21 [34944/50048]	Loss: 1.1168
Training Epoch: 21 [35072/50048]	Loss: 1.0261
Training Epoch: 21 [35200/50048]	Loss: 1.0591
Training Epoch: 21 [35328/50048]	Loss: 1.3443
Training Epoch: 21 [35456/50048]	Loss: 1.2229
Training Epoch: 21 [35584/50048]	Loss: 1.1985
Training Epoch: 21 [35712/50048]	Loss: 1.2923
Training Epoch: 21 [35840/50048]	Loss: 1.5556
Training Epoch: 21 [35968/50048]	Loss: 1.4253
Training Epoch: 21 [36096/50048]	Loss: 1.1861
Training Epoch: 21 [36224/50048]	Loss: 1.2319
Training Epoch: 21 [36352/50048]	Loss: 1.2404
Training Epoch: 21 [36480/50048]	Loss: 1.1637
Training Epoch: 21 [36608/50048]	Loss: 1.3334
Training Epoch: 21 [36736/50048]	Loss: 1.3537
Training Epoch: 21 [36864/50048]	Loss: 1.0472
Training Epoch: 21 [36992/50048]	Loss: 1.4848
Training Epoch: 21 [37120/50048]	Loss: 1.1937
Training Epoch: 21 [37248/50048]	Loss: 1.2345
Training Epoch: 21 [37376/50048]	Loss: 1.3720
Training Epoch: 21 [37504/50048]	Loss: 1.1347
Training Epoch: 21 [37632/50048]	Loss: 1.4050
Training Epoch: 21 [37760/50048]	Loss: 1.4002
Training Epoch: 21 [37888/50048]	Loss: 1.1096
Training Epoch: 21 [38016/50048]	Loss: 1.2883
Training Epoch: 21 [38144/50048]	Loss: 1.2937
Training Epoch: 21 [38272/50048]	Loss: 1.2948
Training Epoch: 21 [38400/50048]	Loss: 1.1766
Training Epoch: 21 [38528/50048]	Loss: 1.3233
Training Epoch: 21 [38656/50048]	Loss: 1.2101
Training Epoch: 21 [38784/50048]	Loss: 1.0798
Training Epoch: 21 [38912/50048]	Loss: 1.3153
Training Epoch: 21 [39040/50048]	Loss: 1.3036
Training Epoch: 21 [39168/50048]	Loss: 1.2058
Training Epoch: 21 [39296/50048]	Loss: 1.2811
Training Epoch: 21 [39424/50048]	Loss: 1.5267
Training Epoch: 21 [39552/50048]	Loss: 1.2720
Training Epoch: 21 [39680/50048]	Loss: 1.1615
Training Epoch: 21 [39808/50048]	Loss: 1.2096
Training Epoch: 21 [39936/50048]	Loss: 0.9262
Training Epoch: 21 [40064/50048]	Loss: 1.5153
Training Epoch: 21 [40192/50048]	Loss: 1.1251
Training Epoch: 21 [40320/50048]	Loss: 1.4401
Training Epoch: 21 [40448/50048]	Loss: 1.1393
Training Epoch: 21 [40576/50048]	Loss: 1.0793
Training Epoch: 21 [40704/50048]	Loss: 1.3123
Training Epoch: 21 [40832/50048]	Loss: 1.3026
Training Epoch: 21 [40960/50048]	Loss: 1.1705
Training Epoch: 21 [41088/50048]	Loss: 1.2083
Training Epoch: 21 [41216/50048]	Loss: 1.1439
Training Epoch: 21 [41344/50048]	Loss: 1.1450
Training Epoch: 21 [41472/50048]	Loss: 1.1651
Training Epoch: 21 [41600/50048]	Loss: 1.0806
Training Epoch: 21 [41728/50048]	Loss: 1.2210
Training Epoch: 21 [41856/50048]	Loss: 1.0379
Training Epoch: 21 [41984/50048]	Loss: 1.3012
Training Epoch: 21 [42112/50048]	Loss: 1.1579
Training Epoch: 21 [42240/50048]	Loss: 1.6480
Training Epoch: 21 [42368/50048]	Loss: 1.5208
Training Epoch: 21 [42496/50048]	Loss: 1.2187
Training Epoch: 21 [42624/50048]	Loss: 1.2921
Training Epoch: 21 [42752/50048]	Loss: 1.3912
Training Epoch: 21 [42880/50048]	Loss: 1.0695
Training Epoch: 21 [43008/50048]	Loss: 1.1539
Training Epoch: 21 [43136/50048]	Loss: 1.5935
Training Epoch: 21 [43264/50048]	Loss: 1.3421
Training Epoch: 21 [43392/50048]	Loss: 1.3105
Training Epoch: 21 [43520/50048]	Loss: 1.2135
Training Epoch: 21 [43648/50048]	Loss: 1.0500
Training Epoch: 21 [43776/50048]	Loss: 1.1810
Training Epoch: 21 [43904/50048]	Loss: 1.3975
Training Epoch: 21 [44032/50048]	Loss: 1.2805
Training Epoch: 21 [44160/50048]	Loss: 1.4665
Training Epoch: 21 [44288/50048]	Loss: 1.1018
Training Epoch: 21 [44416/50048]	Loss: 1.1570
Training Epoch: 21 [44544/50048]	Loss: 1.2150
Training Epoch: 21 [44672/50048]	Loss: 1.3158
Training Epoch: 21 [44800/50048]	Loss: 1.1508
Training Epoch: 21 [44928/50048]	Loss: 1.3658
Training Epoch: 21 [45056/50048]	Loss: 1.0005
Training Epoch: 21 [45184/50048]	Loss: 1.3583
Training Epoch: 21 [45312/50048]	Loss: 1.1323
Training Epoch: 21 [45440/50048]	Loss: 1.2661
Training Epoch: 21 [45568/50048]	Loss: 1.3809
Training Epoch: 21 [45696/50048]	Loss: 1.2995
Training Epoch: 21 [45824/50048]	Loss: 0.9795
Training Epoch: 21 [45952/50048]	Loss: 1.0877
Training Epoch: 21 [46080/50048]	Loss: 1.0511
Training Epoch: 21 [46208/50048]	Loss: 1.0841
Training Epoch: 21 [46336/50048]	Loss: 1.1785
Training Epoch: 21 [46464/50048]	Loss: 1.2174
Training Epoch: 21 [46592/50048]	Loss: 1.0689
Training Epoch: 21 [46720/50048]	Loss: 1.3498
Training Epoch: 21 [46848/50048]	Loss: 1.2212
Training Epoch: 21 [46976/50048]	Loss: 1.1078
Training Epoch: 21 [47104/50048]	Loss: 1.3580
Training Epoch: 21 [47232/50048]	Loss: 1.0090
Training Epoch: 21 [47360/50048]	Loss: 1.0858
Training Epoch: 21 [47488/50048]	Loss: 1.1111
Training Epoch: 21 [47616/50048]	Loss: 1.4688
Training Epoch: 21 [47744/50048]	Loss: 1.3851
Training Epoch: 21 [47872/50048]	Loss: 1.2846
Training Epoch: 21 [48000/50048]	Loss: 1.1743
Training Epoch: 21 [48128/50048]	Loss: 1.1516
Training Epoch: 21 [48256/50048]	Loss: 1.3255
Training Epoch: 21 [48384/50048]	Loss: 1.3626
Training Epoch: 21 [48512/50048]	Loss: 1.2964
Training Epoch: 21 [48640/50048]	Loss: 1.3372
Training Epoch: 21 [48768/50048]	Loss: 1.3524
Training Epoch: 21 [48896/50048]	Loss: 1.3772
Training Epoch: 21 [49024/50048]	Loss: 1.4071
Training Epoch: 21 [49152/50048]	Loss: 1.2119
Training Epoch: 21 [49280/50048]	Loss: 1.1788
Training Epoch: 21 [49408/50048]	Loss: 1.0067
Training Epoch: 21 [49536/50048]	Loss: 1.3806
Training Epoch: 21 [49664/50048]	Loss: 1.0081
Training Epoch: 21 [49792/50048]	Loss: 1.2577
Training Epoch: 21 [49920/50048]	Loss: 1.1187
Training Epoch: 21 [50048/50048]	Loss: 1.2833
Validation Epoch: 21, Average loss: 0.0117, Accuracy: 0.5937
Training Epoch: 22 [128/50048]	Loss: 1.2445
Training Epoch: 22 [256/50048]	Loss: 1.1715
Training Epoch: 22 [384/50048]	Loss: 1.2342
Training Epoch: 22 [512/50048]	Loss: 1.1401
Training Epoch: 22 [640/50048]	Loss: 1.0991
Training Epoch: 22 [768/50048]	Loss: 1.2294
Training Epoch: 22 [896/50048]	Loss: 1.2038
Training Epoch: 22 [1024/50048]	Loss: 1.2230
Training Epoch: 22 [1152/50048]	Loss: 1.1168
Training Epoch: 22 [1280/50048]	Loss: 1.1184
Training Epoch: 22 [1408/50048]	Loss: 1.2569
Training Epoch: 22 [1536/50048]	Loss: 1.0474
Training Epoch: 22 [1664/50048]	Loss: 1.1427
Training Epoch: 22 [1792/50048]	Loss: 1.2166
Training Epoch: 22 [1920/50048]	Loss: 1.3715
Training Epoch: 22 [2048/50048]	Loss: 1.1577
Training Epoch: 22 [2176/50048]	Loss: 1.3187
Training Epoch: 22 [2304/50048]	Loss: 1.2476
Training Epoch: 22 [2432/50048]	Loss: 1.1853
Training Epoch: 22 [2560/50048]	Loss: 1.0467
Training Epoch: 22 [2688/50048]	Loss: 1.2575
Training Epoch: 22 [2816/50048]	Loss: 1.1871
Training Epoch: 22 [2944/50048]	Loss: 1.1461
Training Epoch: 22 [3072/50048]	Loss: 1.0283
Training Epoch: 22 [3200/50048]	Loss: 0.8998
Training Epoch: 22 [3328/50048]	Loss: 1.1152
Training Epoch: 22 [3456/50048]	Loss: 1.1653
Training Epoch: 22 [3584/50048]	Loss: 1.1313
Training Epoch: 22 [3712/50048]	Loss: 1.0900
Training Epoch: 22 [3840/50048]	Loss: 1.0709
Training Epoch: 22 [3968/50048]	Loss: 1.1826
Training Epoch: 22 [4096/50048]	Loss: 1.1466
Training Epoch: 22 [4224/50048]	Loss: 1.3157
Training Epoch: 22 [4352/50048]	Loss: 1.2681
Training Epoch: 22 [4480/50048]	Loss: 1.0593
Training Epoch: 22 [4608/50048]	Loss: 1.2675
Training Epoch: 22 [4736/50048]	Loss: 1.1830
Training Epoch: 22 [4864/50048]	Loss: 1.4983
Training Epoch: 22 [4992/50048]	Loss: 1.2315
Training Epoch: 22 [5120/50048]	Loss: 1.2179
Training Epoch: 22 [5248/50048]	Loss: 1.3377
Training Epoch: 22 [5376/50048]	Loss: 1.1436
Training Epoch: 22 [5504/50048]	Loss: 1.2218
Training Epoch: 22 [5632/50048]	Loss: 1.1051
Training Epoch: 22 [5760/50048]	Loss: 1.0346
Training Epoch: 22 [5888/50048]	Loss: 1.0949
Training Epoch: 22 [6016/50048]	Loss: 1.1743
Training Epoch: 22 [6144/50048]	Loss: 1.2090
Training Epoch: 22 [6272/50048]	Loss: 1.1306
Training Epoch: 22 [6400/50048]	Loss: 1.0300
Training Epoch: 22 [6528/50048]	Loss: 1.1880
Training Epoch: 22 [6656/50048]	Loss: 1.4343
Training Epoch: 22 [6784/50048]	Loss: 1.2960
Training Epoch: 22 [6912/50048]	Loss: 0.9362
Training Epoch: 22 [7040/50048]	Loss: 1.0189
Training Epoch: 22 [7168/50048]	Loss: 1.2687
Training Epoch: 22 [7296/50048]	Loss: 1.2597
Training Epoch: 22 [7424/50048]	Loss: 1.1381
Training Epoch: 22 [7552/50048]	Loss: 1.2111
Training Epoch: 22 [7680/50048]	Loss: 1.1460
Training Epoch: 22 [7808/50048]	Loss: 1.1858
Training Epoch: 22 [7936/50048]	Loss: 1.2510
Training Epoch: 22 [8064/50048]	Loss: 1.1173
Training Epoch: 22 [8192/50048]	Loss: 1.2150
Training Epoch: 22 [8320/50048]	Loss: 1.2042
Training Epoch: 22 [8448/50048]	Loss: 1.2105
Training Epoch: 22 [8576/50048]	Loss: 1.3581
Training Epoch: 22 [8704/50048]	Loss: 1.1156
Training Epoch: 22 [8832/50048]	Loss: 1.4215
Training Epoch: 22 [8960/50048]	Loss: 1.1072
Training Epoch: 22 [9088/50048]	Loss: 1.2341
Training Epoch: 22 [9216/50048]	Loss: 1.1307
Training Epoch: 22 [9344/50048]	Loss: 1.3025
Training Epoch: 22 [9472/50048]	Loss: 1.1019
Training Epoch: 22 [9600/50048]	Loss: 1.1849
Training Epoch: 22 [9728/50048]	Loss: 1.2431
Training Epoch: 22 [9856/50048]	Loss: 1.1859
Training Epoch: 22 [9984/50048]	Loss: 1.3026
Training Epoch: 22 [10112/50048]	Loss: 1.2710
Training Epoch: 22 [10240/50048]	Loss: 1.2180
Training Epoch: 22 [10368/50048]	Loss: 1.3297
Training Epoch: 22 [10496/50048]	Loss: 1.1442
Training Epoch: 22 [10624/50048]	Loss: 1.1530
Training Epoch: 22 [10752/50048]	Loss: 1.1038
Training Epoch: 22 [10880/50048]	Loss: 0.9608
Training Epoch: 22 [11008/50048]	Loss: 1.1585
Training Epoch: 22 [11136/50048]	Loss: 1.0637
Training Epoch: 22 [11264/50048]	Loss: 1.1918
Training Epoch: 22 [11392/50048]	Loss: 1.2430
Training Epoch: 22 [11520/50048]	Loss: 1.1401
Training Epoch: 22 [11648/50048]	Loss: 1.4411
Training Epoch: 22 [11776/50048]	Loss: 1.4021
Training Epoch: 22 [11904/50048]	Loss: 1.1575
Training Epoch: 22 [12032/50048]	Loss: 1.1929
Training Epoch: 22 [12160/50048]	Loss: 1.0930
Training Epoch: 22 [12288/50048]	Loss: 1.2283
Training Epoch: 22 [12416/50048]	Loss: 1.2371
Training Epoch: 22 [12544/50048]	Loss: 1.2472
Training Epoch: 22 [12672/50048]	Loss: 1.1512
Training Epoch: 22 [12800/50048]	Loss: 1.1220
Training Epoch: 22 [12928/50048]	Loss: 1.2063
Training Epoch: 22 [13056/50048]	Loss: 1.4040
Training Epoch: 22 [13184/50048]	Loss: 1.2201
Training Epoch: 22 [13312/50048]	Loss: 1.4085
Training Epoch: 22 [13440/50048]	Loss: 1.2835
Training Epoch: 22 [13568/50048]	Loss: 1.4317
Training Epoch: 22 [13696/50048]	Loss: 1.3551
Training Epoch: 22 [13824/50048]	Loss: 1.1725
Training Epoch: 22 [13952/50048]	Loss: 1.2483
Training Epoch: 22 [14080/50048]	Loss: 1.0425
Training Epoch: 22 [14208/50048]	Loss: 0.9687
Training Epoch: 22 [14336/50048]	Loss: 1.0052
Training Epoch: 22 [14464/50048]	Loss: 1.2375
Training Epoch: 22 [14592/50048]	Loss: 1.4286
Training Epoch: 22 [14720/50048]	Loss: 1.2414
Training Epoch: 22 [14848/50048]	Loss: 0.9966
Training Epoch: 22 [14976/50048]	Loss: 1.0675
Training Epoch: 22 [15104/50048]	Loss: 1.1470
Training Epoch: 22 [15232/50048]	Loss: 1.2671
Training Epoch: 22 [15360/50048]	Loss: 1.2887
Training Epoch: 22 [15488/50048]	Loss: 1.2278
Training Epoch: 22 [15616/50048]	Loss: 1.1929
Training Epoch: 22 [15744/50048]	Loss: 1.1621
Training Epoch: 22 [15872/50048]	Loss: 1.2913
Training Epoch: 22 [16000/50048]	Loss: 1.1011
Training Epoch: 22 [16128/50048]	Loss: 1.1015
Training Epoch: 22 [16256/50048]	Loss: 1.0928
Training Epoch: 22 [16384/50048]	Loss: 1.2443
Training Epoch: 22 [16512/50048]	Loss: 1.0806
Training Epoch: 22 [16640/50048]	Loss: 1.1744
Training Epoch: 22 [16768/50048]	Loss: 1.0037
Training Epoch: 22 [16896/50048]	Loss: 1.2016
Training Epoch: 22 [17024/50048]	Loss: 0.9545
Training Epoch: 22 [17152/50048]	Loss: 1.0819
Training Epoch: 22 [17280/50048]	Loss: 1.3260
Training Epoch: 22 [17408/50048]	Loss: 1.0958
Training Epoch: 22 [17536/50048]	Loss: 1.2654
Training Epoch: 22 [17664/50048]	Loss: 1.2550
Training Epoch: 22 [17792/50048]	Loss: 1.0892
Training Epoch: 22 [17920/50048]	Loss: 1.0965
Training Epoch: 22 [18048/50048]	Loss: 1.3222
Training Epoch: 22 [18176/50048]	Loss: 1.2650
Training Epoch: 22 [18304/50048]	Loss: 1.0760
Training Epoch: 22 [18432/50048]	Loss: 1.1933
Training Epoch: 22 [18560/50048]	Loss: 1.2309
Training Epoch: 22 [18688/50048]	Loss: 1.2465
Training Epoch: 22 [18816/50048]	Loss: 1.2385
Training Epoch: 22 [18944/50048]	Loss: 1.1302
Training Epoch: 22 [19072/50048]	Loss: 1.6710
Training Epoch: 22 [19200/50048]	Loss: 1.1513
Training Epoch: 22 [19328/50048]	Loss: 1.5010
Training Epoch: 22 [19456/50048]	Loss: 1.1073
Training Epoch: 22 [19584/50048]	Loss: 1.2963
Training Epoch: 22 [19712/50048]	Loss: 1.0860
Training Epoch: 22 [19840/50048]	Loss: 1.1914
Training Epoch: 22 [19968/50048]	Loss: 1.0877
Training Epoch: 22 [20096/50048]	Loss: 1.0448
Training Epoch: 22 [20224/50048]	Loss: 1.1201
Training Epoch: 22 [20352/50048]	Loss: 1.1493
Training Epoch: 22 [20480/50048]	Loss: 1.3359
Training Epoch: 22 [20608/50048]	Loss: 0.9984
Training Epoch: 22 [20736/50048]	Loss: 1.3626
Training Epoch: 22 [20864/50048]	Loss: 1.2958
Training Epoch: 22 [20992/50048]	Loss: 1.0792
Training Epoch: 22 [21120/50048]	Loss: 1.2371
Training Epoch: 22 [21248/50048]	Loss: 1.3201
Training Epoch: 22 [21376/50048]	Loss: 1.2242
Training Epoch: 22 [21504/50048]	Loss: 1.1359
Training Epoch: 22 [21632/50048]	Loss: 1.2399
Training Epoch: 22 [21760/50048]	Loss: 1.2352
Training Epoch: 22 [21888/50048]	Loss: 1.1274
Training Epoch: 22 [22016/50048]	Loss: 1.0895
Training Epoch: 22 [22144/50048]	Loss: 1.0994
Training Epoch: 22 [22272/50048]	Loss: 1.4768
Training Epoch: 22 [22400/50048]	Loss: 1.0949
Training Epoch: 22 [22528/50048]	Loss: 1.2574
Training Epoch: 22 [22656/50048]	Loss: 1.1444
Training Epoch: 22 [22784/50048]	Loss: 1.2363
Training Epoch: 22 [22912/50048]	Loss: 1.2397
Training Epoch: 22 [23040/50048]	Loss: 1.3830
Training Epoch: 22 [23168/50048]	Loss: 1.2828
Training Epoch: 22 [23296/50048]	Loss: 1.5547
Training Epoch: 22 [23424/50048]	Loss: 1.0865
Training Epoch: 22 [23552/50048]	Loss: 1.2163
Training Epoch: 22 [23680/50048]	Loss: 1.2564
Training Epoch: 22 [23808/50048]	Loss: 1.0844
Training Epoch: 22 [23936/50048]	Loss: 1.1039
Training Epoch: 22 [24064/50048]	Loss: 1.3805
Training Epoch: 22 [24192/50048]	Loss: 1.2272
Training Epoch: 22 [24320/50048]	Loss: 1.0753
Training Epoch: 22 [24448/50048]	Loss: 0.9315
Training Epoch: 22 [24576/50048]	Loss: 1.2374
Training Epoch: 22 [24704/50048]	Loss: 1.4744
Training Epoch: 22 [24832/50048]	Loss: 1.2073
Training Epoch: 22 [24960/50048]	Loss: 1.2044
Training Epoch: 22 [25088/50048]	Loss: 1.2005
Training Epoch: 22 [25216/50048]	Loss: 1.0918
Training Epoch: 22 [25344/50048]	Loss: 1.0809
Training Epoch: 22 [25472/50048]	Loss: 1.1280
Training Epoch: 22 [25600/50048]	Loss: 1.2825
Training Epoch: 22 [25728/50048]	Loss: 1.0698
Training Epoch: 22 [25856/50048]	Loss: 1.4877
Training Epoch: 22 [25984/50048]	Loss: 1.1395
Training Epoch: 22 [26112/50048]	Loss: 1.1339
Training Epoch: 22 [26240/50048]	Loss: 0.9534
Training Epoch: 22 [26368/50048]	Loss: 1.1892
Training Epoch: 22 [26496/50048]	Loss: 1.1265
Training Epoch: 22 [26624/50048]	Loss: 1.3489
Training Epoch: 22 [26752/50048]	Loss: 1.4408
Training Epoch: 22 [26880/50048]	Loss: 1.1525
Training Epoch: 22 [27008/50048]	Loss: 1.2844
Training Epoch: 22 [27136/50048]	Loss: 1.3482
Training Epoch: 22 [27264/50048]	Loss: 1.0608
Training Epoch: 22 [27392/50048]	Loss: 1.1152
Training Epoch: 22 [27520/50048]	Loss: 1.2295
Training Epoch: 22 [27648/50048]	Loss: 1.1528
Training Epoch: 22 [27776/50048]	Loss: 1.1734
Training Epoch: 22 [27904/50048]	Loss: 1.3855
Training Epoch: 22 [28032/50048]	Loss: 1.4071
Training Epoch: 22 [28160/50048]	Loss: 1.1688
Training Epoch: 22 [28288/50048]	Loss: 1.1333
Training Epoch: 22 [28416/50048]	Loss: 1.3266
Training Epoch: 22 [28544/50048]	Loss: 1.3372
Training Epoch: 22 [28672/50048]	Loss: 1.0400
Training Epoch: 22 [28800/50048]	Loss: 1.2422
Training Epoch: 22 [28928/50048]	Loss: 1.2750
Training Epoch: 22 [29056/50048]	Loss: 1.4675
Training Epoch: 22 [29184/50048]	Loss: 1.5282
Training Epoch: 22 [29312/50048]	Loss: 1.0013
Training Epoch: 22 [29440/50048]	Loss: 1.3486
Training Epoch: 22 [29568/50048]	Loss: 1.1111
Training Epoch: 22 [29696/50048]	Loss: 1.0865
Training Epoch: 22 [29824/50048]	Loss: 1.3146
Training Epoch: 22 [29952/50048]	Loss: 0.9866
Training Epoch: 22 [30080/50048]	Loss: 1.2194
Training Epoch: 22 [30208/50048]	Loss: 1.3261
Training Epoch: 22 [30336/50048]	Loss: 1.1556
Training Epoch: 22 [30464/50048]	Loss: 1.3583
Training Epoch: 22 [30592/50048]	Loss: 1.1809
Training Epoch: 22 [30720/50048]	Loss: 1.0828
Training Epoch: 22 [30848/50048]	Loss: 1.1441
Training Epoch: 22 [30976/50048]	Loss: 1.1608
Training Epoch: 22 [31104/50048]	Loss: 1.1822
Training Epoch: 22 [31232/50048]	Loss: 1.0864
Training Epoch: 22 [31360/50048]	Loss: 1.1115
Training Epoch: 22 [31488/50048]	Loss: 0.9544
Training Epoch: 22 [31616/50048]	Loss: 0.9710
Training Epoch: 22 [31744/50048]	Loss: 1.0364
Training Epoch: 22 [31872/50048]	Loss: 1.1958
Training Epoch: 22 [32000/50048]	Loss: 1.4800
Training Epoch: 22 [32128/50048]	Loss: 1.2702
Training Epoch: 22 [32256/50048]	Loss: 1.4473
Training Epoch: 22 [32384/50048]	Loss: 1.0099
Training Epoch: 22 [32512/50048]	Loss: 1.2282
Training Epoch: 22 [32640/50048]	Loss: 1.1807
Training Epoch: 22 [32768/50048]	Loss: 1.2566
Training Epoch: 22 [32896/50048]	Loss: 1.3297
Training Epoch: 22 [33024/50048]	Loss: 1.3971
Training Epoch: 22 [33152/50048]	Loss: 1.3766
Training Epoch: 22 [33280/50048]	Loss: 1.1196
Training Epoch: 22 [33408/50048]	Loss: 1.1711
Training Epoch: 22 [33536/50048]	Loss: 1.0951
Training Epoch: 22 [33664/50048]	Loss: 1.1791
Training Epoch: 22 [33792/50048]	Loss: 1.1240
Training Epoch: 22 [33920/50048]	Loss: 0.9811
Training Epoch: 22 [34048/50048]	Loss: 1.2045
Training Epoch: 22 [34176/50048]	Loss: 1.1153
Training Epoch: 22 [34304/50048]	Loss: 1.1031
Training Epoch: 22 [34432/50048]	Loss: 1.0836
Training Epoch: 22 [34560/50048]	Loss: 1.1244
Training Epoch: 22 [34688/50048]	Loss: 1.1674
Training Epoch: 22 [34816/50048]	Loss: 1.2830
Training Epoch: 22 [34944/50048]	Loss: 1.2038
Training Epoch: 22 [35072/50048]	Loss: 1.3007
Training Epoch: 22 [35200/50048]	Loss: 1.0526
Training Epoch: 22 [35328/50048]	Loss: 1.1260
Training Epoch: 22 [35456/50048]	Loss: 1.2505
Training Epoch: 22 [35584/50048]	Loss: 1.2984
Training Epoch: 22 [35712/50048]	Loss: 1.3074
Training Epoch: 22 [35840/50048]	Loss: 1.1903
Training Epoch: 22 [35968/50048]	Loss: 1.1804
Training Epoch: 22 [36096/50048]	Loss: 1.2553
Training Epoch: 22 [36224/50048]	Loss: 1.2316
Training Epoch: 22 [36352/50048]	Loss: 1.1826
Training Epoch: 22 [36480/50048]	Loss: 1.3039
Training Epoch: 22 [36608/50048]	Loss: 1.2486
Training Epoch: 22 [36736/50048]	Loss: 1.4186
Training Epoch: 22 [36864/50048]	Loss: 1.0606
Training Epoch: 22 [36992/50048]	Loss: 0.9825
Training Epoch: 22 [37120/50048]	Loss: 1.0374
Training Epoch: 22 [37248/50048]	Loss: 1.3709
Training Epoch: 22 [37376/50048]	Loss: 1.0912
Training Epoch: 22 [37504/50048]	Loss: 1.1779
Training Epoch: 22 [37632/50048]	Loss: 1.3391
Training Epoch: 22 [37760/50048]	Loss: 1.2885
Training Epoch: 22 [37888/50048]	Loss: 1.0156
Training Epoch: 22 [38016/50048]	Loss: 1.2992
Training Epoch: 22 [38144/50048]	Loss: 1.1901
Training Epoch: 22 [38272/50048]	Loss: 1.0393
Training Epoch: 22 [38400/50048]	Loss: 1.1676
Training Epoch: 22 [38528/50048]	Loss: 0.9215
Training Epoch: 22 [38656/50048]	Loss: 1.2551
Training Epoch: 22 [38784/50048]	Loss: 1.1970
Training Epoch: 22 [38912/50048]	Loss: 1.2741
Training Epoch: 22 [39040/50048]	Loss: 1.1916
Training Epoch: 22 [39168/50048]	Loss: 1.2019
Training Epoch: 22 [39296/50048]	Loss: 1.1137
Training Epoch: 22 [39424/50048]	Loss: 1.3495
Training Epoch: 22 [39552/50048]	Loss: 1.3538
Training Epoch: 22 [39680/50048]	Loss: 1.2762
Training Epoch: 22 [39808/50048]	Loss: 1.2535
Training Epoch: 22 [39936/50048]	Loss: 1.3076
Training Epoch: 22 [40064/50048]	Loss: 1.1200
Training Epoch: 22 [40192/50048]	Loss: 1.2233
Training Epoch: 22 [40320/50048]	Loss: 1.1772
Training Epoch: 22 [40448/50048]	Loss: 1.2979
Training Epoch: 22 [40576/50048]	Loss: 0.9809
Training Epoch: 22 [40704/50048]	Loss: 1.3372
Training Epoch: 22 [40832/50048]	Loss: 1.3463
Training Epoch: 22 [40960/50048]	Loss: 1.3024
Training Epoch: 22 [41088/50048]	Loss: 1.3972
Training Epoch: 22 [41216/50048]	Loss: 1.1211
Training Epoch: 22 [41344/50048]	Loss: 1.1319
Training Epoch: 22 [41472/50048]	Loss: 1.2173
Training Epoch: 22 [41600/50048]	Loss: 1.0629
Training Epoch: 22 [41728/50048]	Loss: 1.0535
Training Epoch: 22 [41856/50048]	Loss: 1.0962
Training Epoch: 22 [41984/50048]	Loss: 1.0848
Training Epoch: 22 [42112/50048]	Loss: 1.3009
Training Epoch: 22 [42240/50048]	Loss: 1.4299
Training Epoch: 22 [42368/50048]	Loss: 1.3038
Training Epoch: 22 [42496/50048]	Loss: 1.0872
Training Epoch: 22 [42624/50048]	Loss: 1.2608
Training Epoch: 22 [42752/50048]	Loss: 1.3697
Training Epoch: 22 [42880/50048]	Loss: 1.3197
Training Epoch: 22 [43008/50048]	Loss: 1.2826
Training Epoch: 22 [43136/50048]	Loss: 1.3167
Training Epoch: 22 [43264/50048]	Loss: 1.4302
Training Epoch: 22 [43392/50048]	Loss: 1.3357
Training Epoch: 22 [43520/50048]	Loss: 1.3801
Training Epoch: 22 [43648/50048]	Loss: 1.2655
Training Epoch: 22 [43776/50048]	Loss: 1.3447
Training Epoch: 22 [43904/50048]	Loss: 1.2003
Training Epoch: 22 [44032/50048]	Loss: 1.2317
Training Epoch: 22 [44160/50048]	Loss: 1.3082
Training Epoch: 22 [44288/50048]	Loss: 1.2839
Training Epoch: 22 [44416/50048]	Loss: 1.2468
Training Epoch: 22 [44544/50048]	Loss: 1.2705
Training Epoch: 22 [44672/50048]	Loss: 1.0246
Training Epoch: 22 [44800/50048]	Loss: 1.3122
Training Epoch: 22 [44928/50048]	Loss: 1.0380
Training Epoch: 22 [45056/50048]	Loss: 1.2208
Training Epoch: 22 [45184/50048]	Loss: 1.2500
Training Epoch: 22 [45312/50048]	Loss: 1.3442
Training Epoch: 22 [45440/50048]	Loss: 1.0740
Training Epoch: 22 [45568/50048]	Loss: 1.3786
Training Epoch: 22 [45696/50048]	Loss: 0.9810
Training Epoch: 22 [45824/50048]	Loss: 1.3389
Training Epoch: 22 [45952/50048]	Loss: 1.2040
Training Epoch: 22 [46080/50048]	Loss: 1.2042
Training Epoch: 22 [46208/50048]	Loss: 1.1532
Training Epoch: 22 [46336/50048]	Loss: 1.1988
Training Epoch: 22 [46464/50048]	Loss: 1.1681
Training Epoch: 22 [46592/50048]	Loss: 1.1930
Training Epoch: 22 [46720/50048]	Loss: 1.0947
Training Epoch: 22 [46848/50048]	Loss: 1.2912
Training Epoch: 22 [46976/50048]	Loss: 1.2482
Training Epoch: 22 [47104/50048]	Loss: 1.3819
Training Epoch: 22 [47232/50048]	Loss: 1.3736
Training Epoch: 22 [47360/50048]	Loss: 0.9564
Training Epoch: 22 [47488/50048]	Loss: 1.0902
Training Epoch: 22 [47616/50048]	Loss: 1.2676
Training Epoch: 22 [47744/50048]	Loss: 1.4332
Training Epoch: 22 [47872/50048]	Loss: 1.1728
Training Epoch: 22 [48000/50048]	Loss: 1.2701
Training Epoch: 22 [48128/50048]	Loss: 1.3847
Training Epoch: 22 [48256/50048]	Loss: 1.3713
Training Epoch: 22 [48384/50048]	Loss: 1.1024
Training Epoch: 22 [48512/50048]	Loss: 1.1558
Training Epoch: 22 [48640/50048]	Loss: 1.2352
Training Epoch: 22 [48768/50048]	Loss: 1.1917
Training Epoch: 22 [48896/50048]	Loss: 1.2311
Training Epoch: 22 [49024/50048]	Loss: 1.0494
Training Epoch: 22 [49152/50048]	Loss: 1.2982
Training Epoch: 22 [49280/50048]	Loss: 1.0787
Training Epoch: 22 [49408/50048]	Loss: 1.2914
Training Epoch: 22 [49536/50048]	Loss: 1.3538
Training Epoch: 22 [49664/50048]	Loss: 1.2714
Training Epoch: 22 [49792/50048]	Loss: 1.2228
Training Epoch: 22 [49920/50048]	Loss: 1.3782
Training Epoch: 22 [50048/50048]	Loss: 1.3817
Validation Epoch: 22, Average loss: 0.0117, Accuracy: 0.5939
Training Epoch: 23 [128/50048]	Loss: 0.9053
Training Epoch: 23 [256/50048]	Loss: 1.0626
Training Epoch: 23 [384/50048]	Loss: 1.1342
Training Epoch: 23 [512/50048]	Loss: 1.0751
Training Epoch: 23 [640/50048]	Loss: 1.0709
Training Epoch: 23 [768/50048]	Loss: 1.0630
Training Epoch: 23 [896/50048]	Loss: 1.3829
Training Epoch: 23 [1024/50048]	Loss: 1.2652
Training Epoch: 23 [1152/50048]	Loss: 1.0963
Training Epoch: 23 [1280/50048]	Loss: 1.2644
Training Epoch: 23 [1408/50048]	Loss: 1.1060
Training Epoch: 23 [1536/50048]	Loss: 1.0402
Training Epoch: 23 [1664/50048]	Loss: 1.1855
Training Epoch: 23 [1792/50048]	Loss: 1.1404
Training Epoch: 23 [1920/50048]	Loss: 1.3781
Training Epoch: 23 [2048/50048]	Loss: 1.0045
Training Epoch: 23 [2176/50048]	Loss: 1.2952
Training Epoch: 23 [2304/50048]	Loss: 1.0960
Training Epoch: 23 [2432/50048]	Loss: 1.1853
Training Epoch: 23 [2560/50048]	Loss: 1.1915
Training Epoch: 23 [2688/50048]	Loss: 0.8773
Training Epoch: 23 [2816/50048]	Loss: 1.1274
Training Epoch: 23 [2944/50048]	Loss: 1.3622
Training Epoch: 23 [3072/50048]	Loss: 1.3557
Training Epoch: 23 [3200/50048]	Loss: 1.2922
Training Epoch: 23 [3328/50048]	Loss: 1.1877
Training Epoch: 23 [3456/50048]	Loss: 1.1328
Training Epoch: 23 [3584/50048]	Loss: 1.2305
Training Epoch: 23 [3712/50048]	Loss: 1.3086
Training Epoch: 23 [3840/50048]	Loss: 1.0939
Training Epoch: 23 [3968/50048]	Loss: 1.1790
Training Epoch: 23 [4096/50048]	Loss: 1.2685
Training Epoch: 23 [4224/50048]	Loss: 1.0158
Training Epoch: 23 [4352/50048]	Loss: 1.1705
Training Epoch: 23 [4480/50048]	Loss: 1.1518
Training Epoch: 23 [4608/50048]	Loss: 1.2549
Training Epoch: 23 [4736/50048]	Loss: 0.9588
Training Epoch: 23 [4864/50048]	Loss: 1.3689
Training Epoch: 23 [4992/50048]	Loss: 1.2663
Training Epoch: 23 [5120/50048]	Loss: 0.9441
Training Epoch: 23 [5248/50048]	Loss: 1.0400
Training Epoch: 23 [5376/50048]	Loss: 1.2535
Training Epoch: 23 [5504/50048]	Loss: 1.1936
Training Epoch: 23 [5632/50048]	Loss: 1.2943
Training Epoch: 23 [5760/50048]	Loss: 1.2097
Training Epoch: 23 [5888/50048]	Loss: 1.2845
Training Epoch: 23 [6016/50048]	Loss: 1.1015
Training Epoch: 23 [6144/50048]	Loss: 1.2711
Training Epoch: 23 [6272/50048]	Loss: 1.2470
Training Epoch: 23 [6400/50048]	Loss: 0.9925
Training Epoch: 23 [6528/50048]	Loss: 1.0537
Training Epoch: 23 [6656/50048]	Loss: 1.0822
Training Epoch: 23 [6784/50048]	Loss: 0.9924
Training Epoch: 23 [6912/50048]	Loss: 1.2209
Training Epoch: 23 [7040/50048]	Loss: 1.2046
Training Epoch: 23 [7168/50048]	Loss: 0.9919
Training Epoch: 23 [7296/50048]	Loss: 1.2484
Training Epoch: 23 [7424/50048]	Loss: 1.0929
Training Epoch: 23 [7552/50048]	Loss: 1.1388
Training Epoch: 23 [7680/50048]	Loss: 0.9758
Training Epoch: 23 [7808/50048]	Loss: 1.2379
Training Epoch: 23 [7936/50048]	Loss: 1.2357
Training Epoch: 23 [8064/50048]	Loss: 1.0534
Training Epoch: 23 [8192/50048]	Loss: 1.3575
Training Epoch: 23 [8320/50048]	Loss: 1.1173
Training Epoch: 23 [8448/50048]	Loss: 1.1910
Training Epoch: 23 [8576/50048]	Loss: 1.1880
Training Epoch: 23 [8704/50048]	Loss: 1.0654
Training Epoch: 23 [8832/50048]	Loss: 1.0979
Training Epoch: 23 [8960/50048]	Loss: 1.1560
Training Epoch: 23 [9088/50048]	Loss: 1.3138
Training Epoch: 23 [9216/50048]	Loss: 1.0975
Training Epoch: 23 [9344/50048]	Loss: 1.3572
Training Epoch: 23 [9472/50048]	Loss: 1.0912
Training Epoch: 23 [9600/50048]	Loss: 1.3405
Training Epoch: 23 [9728/50048]	Loss: 1.1964
Training Epoch: 23 [9856/50048]	Loss: 1.2357
Training Epoch: 23 [9984/50048]	Loss: 1.1697
Training Epoch: 23 [10112/50048]	Loss: 0.8527
Training Epoch: 23 [10240/50048]	Loss: 1.3088
Training Epoch: 23 [10368/50048]	Loss: 1.0468
Training Epoch: 23 [10496/50048]	Loss: 1.2807
Training Epoch: 23 [10624/50048]	Loss: 1.1317
Training Epoch: 23 [10752/50048]	Loss: 1.1817
Training Epoch: 23 [10880/50048]	Loss: 0.9594
Training Epoch: 23 [11008/50048]	Loss: 1.1639
Training Epoch: 23 [11136/50048]	Loss: 1.0356
Training Epoch: 23 [11264/50048]	Loss: 1.2912
Training Epoch: 23 [11392/50048]	Loss: 1.3040
Training Epoch: 23 [11520/50048]	Loss: 1.3056
Training Epoch: 23 [11648/50048]	Loss: 1.1781
Training Epoch: 23 [11776/50048]	Loss: 1.3627
Training Epoch: 23 [11904/50048]	Loss: 1.0029
Training Epoch: 23 [12032/50048]	Loss: 1.2094
Training Epoch: 23 [12160/50048]	Loss: 0.9744
Training Epoch: 23 [12288/50048]	Loss: 1.1557
Training Epoch: 23 [12416/50048]	Loss: 1.3195
Training Epoch: 23 [12544/50048]	Loss: 1.1090
Training Epoch: 23 [12672/50048]	Loss: 1.4121
Training Epoch: 23 [12800/50048]	Loss: 1.1100
Training Epoch: 23 [12928/50048]	Loss: 1.0939
Training Epoch: 23 [13056/50048]	Loss: 1.4063
Training Epoch: 23 [13184/50048]	Loss: 1.1457
Training Epoch: 23 [13312/50048]	Loss: 1.2482
Training Epoch: 23 [13440/50048]	Loss: 1.4133
Training Epoch: 23 [13568/50048]	Loss: 1.2379
Training Epoch: 23 [13696/50048]	Loss: 1.4256
Training Epoch: 23 [13824/50048]	Loss: 1.0651
Training Epoch: 23 [13952/50048]	Loss: 1.2372
Training Epoch: 23 [14080/50048]	Loss: 1.3420
Training Epoch: 23 [14208/50048]	Loss: 1.2540
Training Epoch: 23 [14336/50048]	Loss: 1.4239
Training Epoch: 23 [14464/50048]	Loss: 1.3106
Training Epoch: 23 [14592/50048]	Loss: 0.9835
Training Epoch: 23 [14720/50048]	Loss: 1.1665
Training Epoch: 23 [14848/50048]	Loss: 1.1929
Training Epoch: 23 [14976/50048]	Loss: 1.2570
Training Epoch: 23 [15104/50048]	Loss: 1.2982
Training Epoch: 23 [15232/50048]	Loss: 0.9837
Training Epoch: 23 [15360/50048]	Loss: 1.3064
Training Epoch: 23 [15488/50048]	Loss: 1.3293
Training Epoch: 23 [15616/50048]	Loss: 1.3460
Training Epoch: 23 [15744/50048]	Loss: 1.0917
Training Epoch: 23 [15872/50048]	Loss: 1.0876
Training Epoch: 23 [16000/50048]	Loss: 1.2563
Training Epoch: 23 [16128/50048]	Loss: 1.1128
Training Epoch: 23 [16256/50048]	Loss: 1.4220
Training Epoch: 23 [16384/50048]	Loss: 1.0844
Training Epoch: 23 [16512/50048]	Loss: 1.2233
Training Epoch: 23 [16640/50048]	Loss: 1.0645
Training Epoch: 23 [16768/50048]	Loss: 1.2060
Training Epoch: 23 [16896/50048]	Loss: 1.0409
Training Epoch: 23 [17024/50048]	Loss: 1.1694
Training Epoch: 23 [17152/50048]	Loss: 1.0342
Training Epoch: 23 [17280/50048]	Loss: 1.0801
Training Epoch: 23 [17408/50048]	Loss: 1.0399
Training Epoch: 23 [17536/50048]	Loss: 1.3785
Training Epoch: 23 [17664/50048]	Loss: 1.4738
Training Epoch: 23 [17792/50048]	Loss: 1.3235
Training Epoch: 23 [17920/50048]	Loss: 1.3557
Training Epoch: 23 [18048/50048]	Loss: 1.0322
Training Epoch: 23 [18176/50048]	Loss: 1.2061
Training Epoch: 23 [18304/50048]	Loss: 1.2647
Training Epoch: 23 [18432/50048]	Loss: 1.2556
Training Epoch: 23 [18560/50048]	Loss: 1.0979
Training Epoch: 23 [18688/50048]	Loss: 1.3867
Training Epoch: 23 [18816/50048]	Loss: 1.0927
Training Epoch: 23 [18944/50048]	Loss: 1.3374
Training Epoch: 23 [19072/50048]	Loss: 1.0225
Training Epoch: 23 [19200/50048]	Loss: 1.1584
Training Epoch: 23 [19328/50048]	Loss: 1.1934
Training Epoch: 23 [19456/50048]	Loss: 1.1612
Training Epoch: 23 [19584/50048]	Loss: 1.3718
Training Epoch: 23 [19712/50048]	Loss: 1.2127
Training Epoch: 23 [19840/50048]	Loss: 1.1287
Training Epoch: 23 [19968/50048]	Loss: 1.2377
Training Epoch: 23 [20096/50048]	Loss: 1.2275
Training Epoch: 23 [20224/50048]	Loss: 1.0443
Training Epoch: 23 [20352/50048]	Loss: 1.1700
Training Epoch: 23 [20480/50048]	Loss: 1.1872
Training Epoch: 23 [20608/50048]	Loss: 1.2714
Training Epoch: 23 [20736/50048]	Loss: 0.9897
Training Epoch: 23 [20864/50048]	Loss: 1.5826
Training Epoch: 23 [20992/50048]	Loss: 1.3032
Training Epoch: 23 [21120/50048]	Loss: 0.8737
Training Epoch: 23 [21248/50048]	Loss: 1.4617
Training Epoch: 23 [21376/50048]	Loss: 1.2365
Training Epoch: 23 [21504/50048]	Loss: 1.1416
Training Epoch: 23 [21632/50048]	Loss: 1.1149
Training Epoch: 23 [21760/50048]	Loss: 1.3486
Training Epoch: 23 [21888/50048]	Loss: 1.1619
Training Epoch: 23 [22016/50048]	Loss: 1.2476
Training Epoch: 23 [22144/50048]	Loss: 1.1824
Training Epoch: 23 [22272/50048]	Loss: 1.1475
Training Epoch: 23 [22400/50048]	Loss: 1.2994
Training Epoch: 23 [22528/50048]	Loss: 1.0021
Training Epoch: 23 [22656/50048]	Loss: 1.3997
Training Epoch: 23 [22784/50048]	Loss: 1.1220
Training Epoch: 23 [22912/50048]	Loss: 1.2699
Training Epoch: 23 [23040/50048]	Loss: 1.2890
Training Epoch: 23 [23168/50048]	Loss: 1.3303
Training Epoch: 23 [23296/50048]	Loss: 1.1166
Training Epoch: 23 [23424/50048]	Loss: 1.0200
Training Epoch: 23 [23552/50048]	Loss: 1.1637
Training Epoch: 23 [23680/50048]	Loss: 1.1039
Training Epoch: 23 [23808/50048]	Loss: 1.3187
Training Epoch: 23 [23936/50048]	Loss: 1.1137
Training Epoch: 23 [24064/50048]	Loss: 1.1885
Training Epoch: 23 [24192/50048]	Loss: 1.1582
Training Epoch: 23 [24320/50048]	Loss: 1.1752
Training Epoch: 23 [24448/50048]	Loss: 1.1759
Training Epoch: 23 [24576/50048]	Loss: 1.2856
Training Epoch: 23 [24704/50048]	Loss: 1.4061
Training Epoch: 23 [24832/50048]	Loss: 1.2431
Training Epoch: 23 [24960/50048]	Loss: 1.4715
Training Epoch: 23 [25088/50048]	Loss: 1.1309
Training Epoch: 23 [25216/50048]	Loss: 1.4281
Training Epoch: 23 [25344/50048]	Loss: 1.1526
Training Epoch: 23 [25472/50048]	Loss: 1.3468
Training Epoch: 23 [25600/50048]	Loss: 1.0316
Training Epoch: 23 [25728/50048]	Loss: 1.1485
Training Epoch: 23 [25856/50048]	Loss: 1.3090
Training Epoch: 23 [25984/50048]	Loss: 1.2297
Training Epoch: 23 [26112/50048]	Loss: 1.1140
Training Epoch: 23 [26240/50048]	Loss: 1.3771
Training Epoch: 23 [26368/50048]	Loss: 1.1569
Training Epoch: 23 [26496/50048]	Loss: 1.1067
Training Epoch: 23 [26624/50048]	Loss: 1.2350
Training Epoch: 23 [26752/50048]	Loss: 1.0826
Training Epoch: 23 [26880/50048]	Loss: 1.2820
Training Epoch: 23 [27008/50048]	Loss: 1.2774
Training Epoch: 23 [27136/50048]	Loss: 1.1364
Training Epoch: 23 [27264/50048]	Loss: 1.0619
Training Epoch: 23 [27392/50048]	Loss: 1.2085
Training Epoch: 23 [27520/50048]	Loss: 1.0428
Training Epoch: 23 [27648/50048]	Loss: 1.1161
Training Epoch: 23 [27776/50048]	Loss: 1.0886
Training Epoch: 23 [27904/50048]	Loss: 1.2382
Training Epoch: 23 [28032/50048]	Loss: 1.2103
Training Epoch: 23 [28160/50048]	Loss: 1.0821
Training Epoch: 23 [28288/50048]	Loss: 1.3280
Training Epoch: 23 [28416/50048]	Loss: 1.1931
Training Epoch: 23 [28544/50048]	Loss: 1.1169
Training Epoch: 23 [28672/50048]	Loss: 1.2119
Training Epoch: 23 [28800/50048]	Loss: 1.0261
Training Epoch: 23 [28928/50048]	Loss: 1.3102
Training Epoch: 23 [29056/50048]	Loss: 1.3493
Training Epoch: 23 [29184/50048]	Loss: 1.1828
Training Epoch: 23 [29312/50048]	Loss: 0.9962
Training Epoch: 23 [29440/50048]	Loss: 1.0812
Training Epoch: 23 [29568/50048]	Loss: 0.9369
Training Epoch: 23 [29696/50048]	Loss: 1.0876
Training Epoch: 23 [29824/50048]	Loss: 1.2012
Training Epoch: 23 [29952/50048]	Loss: 1.1451
Training Epoch: 23 [30080/50048]	Loss: 0.9951
Training Epoch: 23 [30208/50048]	Loss: 1.0454
Training Epoch: 23 [30336/50048]	Loss: 0.8735
Training Epoch: 23 [30464/50048]	Loss: 1.1765
Training Epoch: 23 [30592/50048]	Loss: 1.0280
Training Epoch: 23 [30720/50048]	Loss: 1.2548
Training Epoch: 23 [30848/50048]	Loss: 1.4008
Training Epoch: 23 [30976/50048]	Loss: 1.1220
Training Epoch: 23 [31104/50048]	Loss: 1.3609
Training Epoch: 23 [31232/50048]	Loss: 1.0164
Training Epoch: 23 [31360/50048]	Loss: 1.0774
Training Epoch: 23 [31488/50048]	Loss: 1.4584
Training Epoch: 23 [31616/50048]	Loss: 1.0034
Training Epoch: 23 [31744/50048]	Loss: 1.1593
Training Epoch: 23 [31872/50048]	Loss: 1.0492
Training Epoch: 23 [32000/50048]	Loss: 1.0466
Training Epoch: 23 [32128/50048]	Loss: 1.3017
Training Epoch: 23 [32256/50048]	Loss: 1.2158
Training Epoch: 23 [32384/50048]	Loss: 1.0532
Training Epoch: 23 [32512/50048]	Loss: 0.9266
Training Epoch: 23 [32640/50048]	Loss: 1.0977
Training Epoch: 23 [32768/50048]	Loss: 1.2222
Training Epoch: 23 [32896/50048]	Loss: 0.9977
Training Epoch: 23 [33024/50048]	Loss: 1.2190
Training Epoch: 23 [33152/50048]	Loss: 1.1428
Training Epoch: 23 [33280/50048]	Loss: 1.0203
Training Epoch: 23 [33408/50048]	Loss: 1.1857
Training Epoch: 23 [33536/50048]	Loss: 1.2160
Training Epoch: 23 [33664/50048]	Loss: 1.1575
Training Epoch: 23 [33792/50048]	Loss: 1.1229
Training Epoch: 23 [33920/50048]	Loss: 1.1930
Training Epoch: 23 [34048/50048]	Loss: 1.0103
Training Epoch: 23 [34176/50048]	Loss: 1.2713
Training Epoch: 23 [34304/50048]	Loss: 1.5428
Training Epoch: 23 [34432/50048]	Loss: 1.3195
Training Epoch: 23 [34560/50048]	Loss: 1.0425
Training Epoch: 23 [34688/50048]	Loss: 0.9827
Training Epoch: 23 [34816/50048]	Loss: 1.1852
Training Epoch: 23 [34944/50048]	Loss: 1.2072
Training Epoch: 23 [35072/50048]	Loss: 1.2404
Training Epoch: 23 [35200/50048]	Loss: 1.1830
Training Epoch: 23 [35328/50048]	Loss: 1.3104
Training Epoch: 23 [35456/50048]	Loss: 0.9995
Training Epoch: 23 [35584/50048]	Loss: 1.2383
Training Epoch: 23 [35712/50048]	Loss: 1.1714
Training Epoch: 23 [35840/50048]	Loss: 1.0963
Training Epoch: 23 [35968/50048]	Loss: 1.1055
Training Epoch: 23 [36096/50048]	Loss: 1.0094
Training Epoch: 23 [36224/50048]	Loss: 1.0473
Training Epoch: 23 [36352/50048]	Loss: 1.1560
Training Epoch: 23 [36480/50048]	Loss: 1.1712
Training Epoch: 23 [36608/50048]	Loss: 1.1117
Training Epoch: 23 [36736/50048]	Loss: 1.1465
Training Epoch: 23 [36864/50048]	Loss: 1.1396
Training Epoch: 23 [36992/50048]	Loss: 1.0815
Training Epoch: 23 [37120/50048]	Loss: 1.2153
Training Epoch: 23 [37248/50048]	Loss: 1.2263
Training Epoch: 23 [37376/50048]	Loss: 0.9610
Training Epoch: 23 [37504/50048]	Loss: 1.1086
Training Epoch: 23 [37632/50048]	Loss: 1.1076
Training Epoch: 23 [37760/50048]	Loss: 1.1513
Training Epoch: 23 [37888/50048]	Loss: 1.1713
Training Epoch: 23 [38016/50048]	Loss: 1.2879
Training Epoch: 23 [38144/50048]	Loss: 1.3127
Training Epoch: 23 [38272/50048]	Loss: 1.1638
Training Epoch: 23 [38400/50048]	Loss: 1.1807
Training Epoch: 23 [38528/50048]	Loss: 1.0020
Training Epoch: 23 [38656/50048]	Loss: 1.0973
Training Epoch: 23 [38784/50048]	Loss: 1.3185
Training Epoch: 23 [38912/50048]	Loss: 1.1773
Training Epoch: 23 [39040/50048]	Loss: 1.0537
Training Epoch: 23 [39168/50048]	Loss: 1.0947
Training Epoch: 23 [39296/50048]	Loss: 1.0891
Training Epoch: 23 [39424/50048]	Loss: 1.0930
Training Epoch: 23 [39552/50048]	Loss: 0.9957
Training Epoch: 23 [39680/50048]	Loss: 1.1322
Training Epoch: 23 [39808/50048]	Loss: 1.0689
Training Epoch: 23 [39936/50048]	Loss: 1.2159
Training Epoch: 23 [40064/50048]	Loss: 1.0779
Training Epoch: 23 [40192/50048]	Loss: 1.1373
Training Epoch: 23 [40320/50048]	Loss: 1.1326
Training Epoch: 23 [40448/50048]	Loss: 1.1385
Training Epoch: 23 [40576/50048]	Loss: 1.3226
Training Epoch: 23 [40704/50048]	Loss: 1.3249
Training Epoch: 23 [40832/50048]	Loss: 1.3281
Training Epoch: 23 [40960/50048]	Loss: 1.2347
Training Epoch: 23 [41088/50048]	Loss: 1.2628
Training Epoch: 23 [41216/50048]	Loss: 1.2389
Training Epoch: 23 [41344/50048]	Loss: 1.1135
Training Epoch: 23 [41472/50048]	Loss: 1.4601
Training Epoch: 23 [41600/50048]	Loss: 1.0639
Training Epoch: 23 [41728/50048]	Loss: 1.3509
Training Epoch: 23 [41856/50048]	Loss: 1.2857
Training Epoch: 23 [41984/50048]	Loss: 1.1886
Training Epoch: 23 [42112/50048]	Loss: 1.0583
Training Epoch: 23 [42240/50048]	Loss: 1.4093
Training Epoch: 23 [42368/50048]	Loss: 0.9994
Training Epoch: 23 [42496/50048]	Loss: 1.1784
Training Epoch: 23 [42624/50048]	Loss: 1.0570
Training Epoch: 23 [42752/50048]	Loss: 1.1311
Training Epoch: 23 [42880/50048]	Loss: 1.1087
Training Epoch: 23 [43008/50048]	Loss: 1.0560
Training Epoch: 23 [43136/50048]	Loss: 1.0379
Training Epoch: 23 [43264/50048]	Loss: 1.3421
Training Epoch: 23 [43392/50048]	Loss: 1.3249
Training Epoch: 23 [43520/50048]	Loss: 1.2334
Training Epoch: 23 [43648/50048]	Loss: 1.4327
Training Epoch: 23 [43776/50048]	Loss: 1.2696
Training Epoch: 23 [43904/50048]	Loss: 1.1677
Training Epoch: 23 [44032/50048]	Loss: 1.3587
Training Epoch: 23 [44160/50048]	Loss: 1.1746
Training Epoch: 23 [44288/50048]	Loss: 1.0441
Training Epoch: 23 [44416/50048]	Loss: 1.1969
Training Epoch: 23 [44544/50048]	Loss: 1.1584
Training Epoch: 23 [44672/50048]	Loss: 1.1112
Training Epoch: 23 [44800/50048]	Loss: 1.2768
Training Epoch: 23 [44928/50048]	Loss: 1.2669
Training Epoch: 23 [45056/50048]	Loss: 1.1334
Training Epoch: 23 [45184/50048]	Loss: 1.0929
Training Epoch: 23 [45312/50048]	Loss: 1.0771
Training Epoch: 23 [45440/50048]	Loss: 1.1824
Training Epoch: 23 [45568/50048]	Loss: 1.1265
Training Epoch: 23 [45696/50048]	Loss: 1.1639
Training Epoch: 23 [45824/50048]	Loss: 1.1237
Training Epoch: 23 [45952/50048]	Loss: 1.2853
Training Epoch: 23 [46080/50048]	Loss: 1.1491
Training Epoch: 23 [46208/50048]	Loss: 1.3323
Training Epoch: 23 [46336/50048]	Loss: 1.1684
Training Epoch: 23 [46464/50048]	Loss: 1.1573
Training Epoch: 23 [46592/50048]	Loss: 1.0584
Training Epoch: 23 [46720/50048]	Loss: 1.1936
Training Epoch: 23 [46848/50048]	Loss: 1.3101
Training Epoch: 23 [46976/50048]	Loss: 1.0281
Training Epoch: 23 [47104/50048]	Loss: 1.0031
Training Epoch: 23 [47232/50048]	Loss: 1.2302
Training Epoch: 23 [47360/50048]	Loss: 1.2191
Training Epoch: 23 [47488/50048]	Loss: 1.1190
Training Epoch: 23 [47616/50048]	Loss: 1.0896
Training Epoch: 23 [47744/50048]	Loss: 1.1329
Training Epoch: 23 [47872/50048]	Loss: 1.2200
Training Epoch: 23 [48000/50048]	Loss: 1.1204
Training Epoch: 23 [48128/50048]	Loss: 1.3792
Training Epoch: 23 [48256/50048]	Loss: 1.3877
Training Epoch: 23 [48384/50048]	Loss: 1.1508
Training Epoch: 23 [48512/50048]	Loss: 1.1340
Training Epoch: 23 [48640/50048]	Loss: 1.1846
Training Epoch: 23 [48768/50048]	Loss: 1.1446
Training Epoch: 23 [48896/50048]	Loss: 1.1839
Training Epoch: 23 [49024/50048]	Loss: 1.1868
Training Epoch: 23 [49152/50048]	Loss: 1.1998
Training Epoch: 23 [49280/50048]	Loss: 1.2387
Training Epoch: 23 [49408/50048]	Loss: 1.0978
Training Epoch: 23 [49536/50048]	Loss: 1.1564
Training Epoch: 23 [49664/50048]	Loss: 1.6036
Training Epoch: 23 [49792/50048]	Loss: 1.3975
Training Epoch: 23 [49920/50048]	Loss: 1.2143
Training Epoch: 23 [50048/50048]	Loss: 1.3898
Validation Epoch: 23, Average loss: 0.0117, Accuracy: 0.5948
Training Epoch: 24 [128/50048]	Loss: 1.3869
Training Epoch: 24 [256/50048]	Loss: 1.2372
Training Epoch: 24 [384/50048]	Loss: 1.1931
Training Epoch: 24 [512/50048]	Loss: 1.1227
Training Epoch: 24 [640/50048]	Loss: 1.2403
Training Epoch: 24 [768/50048]	Loss: 1.1794
Training Epoch: 24 [896/50048]	Loss: 1.2507
Training Epoch: 24 [1024/50048]	Loss: 1.1462
Training Epoch: 24 [1152/50048]	Loss: 1.1424
Training Epoch: 24 [1280/50048]	Loss: 0.8772
Training Epoch: 24 [1408/50048]	Loss: 1.0587
Training Epoch: 24 [1536/50048]	Loss: 0.9965
Training Epoch: 24 [1664/50048]	Loss: 1.3136
Training Epoch: 24 [1792/50048]	Loss: 1.0897
Training Epoch: 24 [1920/50048]	Loss: 1.1742
Training Epoch: 24 [2048/50048]	Loss: 1.3084
Training Epoch: 24 [2176/50048]	Loss: 1.0819
Training Epoch: 24 [2304/50048]	Loss: 1.2992
Training Epoch: 24 [2432/50048]	Loss: 1.3250
Training Epoch: 24 [2560/50048]	Loss: 1.1357
Training Epoch: 24 [2688/50048]	Loss: 0.9674
Training Epoch: 24 [2816/50048]	Loss: 1.1588
Training Epoch: 24 [2944/50048]	Loss: 1.1732
Training Epoch: 24 [3072/50048]	Loss: 1.1224
Training Epoch: 24 [3200/50048]	Loss: 0.9910
Training Epoch: 24 [3328/50048]	Loss: 0.9948
Training Epoch: 24 [3456/50048]	Loss: 1.0301
Training Epoch: 24 [3584/50048]	Loss: 1.2056
Training Epoch: 24 [3712/50048]	Loss: 1.2325
Training Epoch: 24 [3840/50048]	Loss: 0.9161
Training Epoch: 24 [3968/50048]	Loss: 0.9801
Training Epoch: 24 [4096/50048]	Loss: 1.0699
Training Epoch: 24 [4224/50048]	Loss: 1.2471
Training Epoch: 24 [4352/50048]	Loss: 1.3323
Training Epoch: 24 [4480/50048]	Loss: 1.1424
Training Epoch: 24 [4608/50048]	Loss: 1.1704
Training Epoch: 24 [4736/50048]	Loss: 0.9933
Training Epoch: 24 [4864/50048]	Loss: 1.2421
Training Epoch: 24 [4992/50048]	Loss: 1.2719
Training Epoch: 24 [5120/50048]	Loss: 1.1507
Training Epoch: 24 [5248/50048]	Loss: 1.2020
Training Epoch: 24 [5376/50048]	Loss: 1.0645
Training Epoch: 24 [5504/50048]	Loss: 1.1328
Training Epoch: 24 [5632/50048]	Loss: 1.0614
Training Epoch: 24 [5760/50048]	Loss: 1.2149
Training Epoch: 24 [5888/50048]	Loss: 1.2419
Training Epoch: 24 [6016/50048]	Loss: 1.4519
Training Epoch: 24 [6144/50048]	Loss: 1.1431
Training Epoch: 24 [6272/50048]	Loss: 1.1198
Training Epoch: 24 [6400/50048]	Loss: 1.2244
Training Epoch: 24 [6528/50048]	Loss: 1.1022
Training Epoch: 24 [6656/50048]	Loss: 0.9600
Training Epoch: 24 [6784/50048]	Loss: 1.1253
Training Epoch: 24 [6912/50048]	Loss: 0.9985
Training Epoch: 24 [7040/50048]	Loss: 0.9765
Training Epoch: 24 [7168/50048]	Loss: 1.0991
Training Epoch: 24 [7296/50048]	Loss: 0.9821
Training Epoch: 24 [7424/50048]	Loss: 1.1389
Training Epoch: 24 [7552/50048]	Loss: 0.8668
Training Epoch: 24 [7680/50048]	Loss: 1.1935
Training Epoch: 24 [7808/50048]	Loss: 0.9870
Training Epoch: 24 [7936/50048]	Loss: 1.1723
Training Epoch: 24 [8064/50048]	Loss: 1.4184
Training Epoch: 24 [8192/50048]	Loss: 1.1016
Training Epoch: 24 [8320/50048]	Loss: 1.2158
Training Epoch: 24 [8448/50048]	Loss: 1.2959
Training Epoch: 24 [8576/50048]	Loss: 1.1885
Training Epoch: 24 [8704/50048]	Loss: 1.2586
Training Epoch: 24 [8832/50048]	Loss: 1.1862
Training Epoch: 24 [8960/50048]	Loss: 1.0305
Training Epoch: 24 [9088/50048]	Loss: 1.0608
Training Epoch: 24 [9216/50048]	Loss: 1.2454
Training Epoch: 24 [9344/50048]	Loss: 1.1230
Training Epoch: 24 [9472/50048]	Loss: 1.0967
Training Epoch: 24 [9600/50048]	Loss: 1.1177
Training Epoch: 24 [9728/50048]	Loss: 1.0479
Training Epoch: 24 [9856/50048]	Loss: 0.9678
Training Epoch: 24 [9984/50048]	Loss: 1.0607
Training Epoch: 24 [10112/50048]	Loss: 1.1450
Training Epoch: 24 [10240/50048]	Loss: 1.3878
Training Epoch: 24 [10368/50048]	Loss: 1.1556
Training Epoch: 24 [10496/50048]	Loss: 1.1550
Training Epoch: 24 [10624/50048]	Loss: 1.2770
Training Epoch: 24 [10752/50048]	Loss: 1.0375
Training Epoch: 24 [10880/50048]	Loss: 1.1055
Training Epoch: 24 [11008/50048]	Loss: 1.1582
Training Epoch: 24 [11136/50048]	Loss: 1.0256
Training Epoch: 24 [11264/50048]	Loss: 1.1098
Training Epoch: 24 [11392/50048]	Loss: 1.0230
Training Epoch: 24 [11520/50048]	Loss: 1.0759
Training Epoch: 24 [11648/50048]	Loss: 1.1497
Training Epoch: 24 [11776/50048]	Loss: 1.0683
Training Epoch: 24 [11904/50048]	Loss: 0.9296
Training Epoch: 24 [12032/50048]	Loss: 1.1707
Training Epoch: 24 [12160/50048]	Loss: 1.1632
Training Epoch: 24 [12288/50048]	Loss: 1.1797
Training Epoch: 24 [12416/50048]	Loss: 1.1276
Training Epoch: 24 [12544/50048]	Loss: 1.1084
Training Epoch: 24 [12672/50048]	Loss: 1.1885
Training Epoch: 24 [12800/50048]	Loss: 1.1081
Training Epoch: 24 [12928/50048]	Loss: 1.1196
Training Epoch: 24 [13056/50048]	Loss: 0.9780
Training Epoch: 24 [13184/50048]	Loss: 1.1947
Training Epoch: 24 [13312/50048]	Loss: 1.0930
Training Epoch: 24 [13440/50048]	Loss: 1.2666
Training Epoch: 24 [13568/50048]	Loss: 1.3225
Training Epoch: 24 [13696/50048]	Loss: 1.2043
Training Epoch: 24 [13824/50048]	Loss: 1.0615
Training Epoch: 24 [13952/50048]	Loss: 1.3301
Training Epoch: 24 [14080/50048]	Loss: 1.1274
Training Epoch: 24 [14208/50048]	Loss: 1.1888
Training Epoch: 24 [14336/50048]	Loss: 1.3417
Training Epoch: 24 [14464/50048]	Loss: 0.9908
Training Epoch: 24 [14592/50048]	Loss: 1.0509
Training Epoch: 24 [14720/50048]	Loss: 1.2494
Training Epoch: 24 [14848/50048]	Loss: 1.0323
Training Epoch: 24 [14976/50048]	Loss: 1.0123
Training Epoch: 24 [15104/50048]	Loss: 1.1692
Training Epoch: 24 [15232/50048]	Loss: 1.1870
Training Epoch: 24 [15360/50048]	Loss: 1.0548
Training Epoch: 24 [15488/50048]	Loss: 1.1603
Training Epoch: 24 [15616/50048]	Loss: 1.1081
Training Epoch: 24 [15744/50048]	Loss: 1.3194
Training Epoch: 24 [15872/50048]	Loss: 1.1690
Training Epoch: 24 [16000/50048]	Loss: 1.2803
Training Epoch: 24 [16128/50048]	Loss: 0.8981
Training Epoch: 24 [16256/50048]	Loss: 0.9188
Training Epoch: 24 [16384/50048]	Loss: 1.2993
Training Epoch: 24 [16512/50048]	Loss: 1.0973
Training Epoch: 24 [16640/50048]	Loss: 1.1823
Training Epoch: 24 [16768/50048]	Loss: 1.1290
Training Epoch: 24 [16896/50048]	Loss: 1.1146
Training Epoch: 24 [17024/50048]	Loss: 0.9309
Training Epoch: 24 [17152/50048]	Loss: 1.1834
Training Epoch: 24 [17280/50048]	Loss: 1.2848
Training Epoch: 24 [17408/50048]	Loss: 1.2772
Training Epoch: 24 [17536/50048]	Loss: 1.2036
Training Epoch: 24 [17664/50048]	Loss: 0.9831
Training Epoch: 24 [17792/50048]	Loss: 1.1437
Training Epoch: 24 [17920/50048]	Loss: 1.0934
Training Epoch: 24 [18048/50048]	Loss: 1.1768
Training Epoch: 24 [18176/50048]	Loss: 1.2353
Training Epoch: 24 [18304/50048]	Loss: 0.9559
Training Epoch: 24 [18432/50048]	Loss: 1.1707
Training Epoch: 24 [18560/50048]	Loss: 1.1654
Training Epoch: 24 [18688/50048]	Loss: 1.0542
Training Epoch: 24 [18816/50048]	Loss: 1.1279
Training Epoch: 24 [18944/50048]	Loss: 1.2812
Training Epoch: 24 [19072/50048]	Loss: 0.9902
Training Epoch: 24 [19200/50048]	Loss: 1.3137
Training Epoch: 24 [19328/50048]	Loss: 1.1816
Training Epoch: 24 [19456/50048]	Loss: 1.1056
Training Epoch: 24 [19584/50048]	Loss: 1.1395
Training Epoch: 24 [19712/50048]	Loss: 1.1016
Training Epoch: 24 [19840/50048]	Loss: 1.2508
Training Epoch: 24 [19968/50048]	Loss: 1.3027
Training Epoch: 24 [20096/50048]	Loss: 1.2892
Training Epoch: 24 [20224/50048]	Loss: 1.0615
Training Epoch: 24 [20352/50048]	Loss: 1.2756
Training Epoch: 24 [20480/50048]	Loss: 1.2266
Training Epoch: 24 [20608/50048]	Loss: 1.2555
Training Epoch: 24 [20736/50048]	Loss: 1.1041
Training Epoch: 24 [20864/50048]	Loss: 1.0799
Training Epoch: 24 [20992/50048]	Loss: 1.2687
Training Epoch: 24 [21120/50048]	Loss: 1.0710
Training Epoch: 24 [21248/50048]	Loss: 1.2314
Training Epoch: 24 [21376/50048]	Loss: 1.1068
Training Epoch: 24 [21504/50048]	Loss: 1.1155
Training Epoch: 24 [21632/50048]	Loss: 1.2545
Training Epoch: 24 [21760/50048]	Loss: 1.1253
Training Epoch: 24 [21888/50048]	Loss: 1.0773
Training Epoch: 24 [22016/50048]	Loss: 0.9440
Training Epoch: 24 [22144/50048]	Loss: 1.3600
Training Epoch: 24 [22272/50048]	Loss: 1.0577
Training Epoch: 24 [22400/50048]	Loss: 1.3142
Training Epoch: 24 [22528/50048]	Loss: 1.3117
Training Epoch: 24 [22656/50048]	Loss: 1.0430
Training Epoch: 24 [22784/50048]	Loss: 1.1829
Training Epoch: 24 [22912/50048]	Loss: 1.0775
Training Epoch: 24 [23040/50048]	Loss: 1.1216
Training Epoch: 24 [23168/50048]	Loss: 1.1509
Training Epoch: 24 [23296/50048]	Loss: 1.2568
Training Epoch: 24 [23424/50048]	Loss: 1.0656
Training Epoch: 24 [23552/50048]	Loss: 1.1187
Training Epoch: 24 [23680/50048]	Loss: 1.2986
Training Epoch: 24 [23808/50048]	Loss: 1.0491
Training Epoch: 24 [23936/50048]	Loss: 1.1519
Training Epoch: 24 [24064/50048]	Loss: 0.9947
Training Epoch: 24 [24192/50048]	Loss: 1.1640
Training Epoch: 24 [24320/50048]	Loss: 1.0799
Training Epoch: 24 [24448/50048]	Loss: 0.9730
Training Epoch: 24 [24576/50048]	Loss: 1.0711
Training Epoch: 24 [24704/50048]	Loss: 1.0312
Training Epoch: 24 [24832/50048]	Loss: 0.8839
Training Epoch: 24 [24960/50048]	Loss: 1.0168
Training Epoch: 24 [25088/50048]	Loss: 1.0878
Training Epoch: 24 [25216/50048]	Loss: 1.2744
Training Epoch: 24 [25344/50048]	Loss: 1.3520
Training Epoch: 24 [25472/50048]	Loss: 1.2432
Training Epoch: 24 [25600/50048]	Loss: 0.9757
Training Epoch: 24 [25728/50048]	Loss: 1.2251
Training Epoch: 24 [25856/50048]	Loss: 1.0409
Training Epoch: 24 [25984/50048]	Loss: 1.2814
Training Epoch: 24 [26112/50048]	Loss: 1.1061
Training Epoch: 24 [26240/50048]	Loss: 1.1914
Training Epoch: 24 [26368/50048]	Loss: 1.2915
Training Epoch: 24 [26496/50048]	Loss: 1.0947
Training Epoch: 24 [26624/50048]	Loss: 0.9605
Training Epoch: 24 [26752/50048]	Loss: 1.0906
Training Epoch: 24 [26880/50048]	Loss: 1.0396
Training Epoch: 24 [27008/50048]	Loss: 1.4257
Training Epoch: 24 [27136/50048]	Loss: 1.2745
Training Epoch: 24 [27264/50048]	Loss: 1.0955
Training Epoch: 24 [27392/50048]	Loss: 1.1191
Training Epoch: 24 [27520/50048]	Loss: 1.1520
Training Epoch: 24 [27648/50048]	Loss: 1.0626
Training Epoch: 24 [27776/50048]	Loss: 1.0333
Training Epoch: 24 [27904/50048]	Loss: 1.2887
Training Epoch: 24 [28032/50048]	Loss: 1.0821
Training Epoch: 24 [28160/50048]	Loss: 1.0333
Training Epoch: 24 [28288/50048]	Loss: 1.2422
Training Epoch: 24 [28416/50048]	Loss: 1.1092
Training Epoch: 24 [28544/50048]	Loss: 1.2453
Training Epoch: 24 [28672/50048]	Loss: 1.1003
Training Epoch: 24 [28800/50048]	Loss: 1.2395
Training Epoch: 24 [28928/50048]	Loss: 1.0848
Training Epoch: 24 [29056/50048]	Loss: 1.0309
Training Epoch: 24 [29184/50048]	Loss: 1.2984
Training Epoch: 24 [29312/50048]	Loss: 1.3723
Training Epoch: 24 [29440/50048]	Loss: 1.0830
Training Epoch: 24 [29568/50048]	Loss: 1.2437
Training Epoch: 24 [29696/50048]	Loss: 1.1483
Training Epoch: 24 [29824/50048]	Loss: 1.0557
Training Epoch: 24 [29952/50048]	Loss: 1.1060
Training Epoch: 24 [30080/50048]	Loss: 1.0595
Training Epoch: 24 [30208/50048]	Loss: 1.1966
Training Epoch: 24 [30336/50048]	Loss: 1.2034
Training Epoch: 24 [30464/50048]	Loss: 1.0087
Training Epoch: 24 [30592/50048]	Loss: 1.0642
Training Epoch: 24 [30720/50048]	Loss: 1.1228
Training Epoch: 24 [30848/50048]	Loss: 1.2797
Training Epoch: 24 [30976/50048]	Loss: 1.2050
Training Epoch: 24 [31104/50048]	Loss: 1.1899
Training Epoch: 24 [31232/50048]	Loss: 1.1339
Training Epoch: 24 [31360/50048]	Loss: 1.2189
Training Epoch: 24 [31488/50048]	Loss: 1.0466
Training Epoch: 24 [31616/50048]	Loss: 1.2522
Training Epoch: 24 [31744/50048]	Loss: 1.3462
Training Epoch: 24 [31872/50048]	Loss: 1.0541
Training Epoch: 24 [32000/50048]	Loss: 0.9908
Training Epoch: 24 [32128/50048]	Loss: 1.2436
Training Epoch: 24 [32256/50048]	Loss: 1.3166
Training Epoch: 24 [32384/50048]	Loss: 1.1118
Training Epoch: 24 [32512/50048]	Loss: 0.9769
Training Epoch: 24 [32640/50048]	Loss: 1.1321
Training Epoch: 24 [32768/50048]	Loss: 1.2571
Training Epoch: 24 [32896/50048]	Loss: 1.0878
Training Epoch: 24 [33024/50048]	Loss: 1.2197
Training Epoch: 24 [33152/50048]	Loss: 1.0376
Training Epoch: 24 [33280/50048]	Loss: 1.2196
Training Epoch: 24 [33408/50048]	Loss: 1.2466
Training Epoch: 24 [33536/50048]	Loss: 1.3061
Training Epoch: 24 [33664/50048]	Loss: 1.1441
Training Epoch: 24 [33792/50048]	Loss: 1.2562
Training Epoch: 24 [33920/50048]	Loss: 1.0515
Training Epoch: 24 [34048/50048]	Loss: 1.0885
Training Epoch: 24 [34176/50048]	Loss: 1.2017
Training Epoch: 24 [34304/50048]	Loss: 1.2403
Training Epoch: 24 [34432/50048]	Loss: 1.1758
Training Epoch: 24 [34560/50048]	Loss: 1.1356
Training Epoch: 24 [34688/50048]	Loss: 1.1905
Training Epoch: 24 [34816/50048]	Loss: 1.1597
Training Epoch: 24 [34944/50048]	Loss: 1.4711
Training Epoch: 24 [35072/50048]	Loss: 1.0917
Training Epoch: 24 [35200/50048]	Loss: 1.1927
Training Epoch: 24 [35328/50048]	Loss: 1.3251
Training Epoch: 24 [35456/50048]	Loss: 1.0959
Training Epoch: 24 [35584/50048]	Loss: 1.3756
Training Epoch: 24 [35712/50048]	Loss: 1.3381
Training Epoch: 24 [35840/50048]	Loss: 1.0800
Training Epoch: 24 [35968/50048]	Loss: 1.1744
Training Epoch: 24 [36096/50048]	Loss: 1.2014
Training Epoch: 24 [36224/50048]	Loss: 1.1575
Training Epoch: 24 [36352/50048]	Loss: 1.0707
Training Epoch: 24 [36480/50048]	Loss: 1.1875
Training Epoch: 24 [36608/50048]	Loss: 1.0773
Training Epoch: 24 [36736/50048]	Loss: 1.3719
Training Epoch: 24 [36864/50048]	Loss: 1.1629
Training Epoch: 24 [36992/50048]	Loss: 1.1919
Training Epoch: 24 [37120/50048]	Loss: 1.0766
Training Epoch: 24 [37248/50048]	Loss: 1.0374
Training Epoch: 24 [37376/50048]	Loss: 1.0820
Training Epoch: 24 [37504/50048]	Loss: 1.1717
Training Epoch: 24 [37632/50048]	Loss: 1.2333
Training Epoch: 24 [37760/50048]	Loss: 1.0127
Training Epoch: 24 [37888/50048]	Loss: 1.2611
Training Epoch: 24 [38016/50048]	Loss: 1.2356
Training Epoch: 24 [38144/50048]	Loss: 1.1403
Training Epoch: 24 [38272/50048]	Loss: 0.9406
Training Epoch: 24 [38400/50048]	Loss: 1.0493
Training Epoch: 24 [38528/50048]	Loss: 1.0933
Training Epoch: 24 [38656/50048]	Loss: 1.1064
Training Epoch: 24 [38784/50048]	Loss: 0.9785
Training Epoch: 24 [38912/50048]	Loss: 1.5408
Training Epoch: 24 [39040/50048]	Loss: 1.2444
Training Epoch: 24 [39168/50048]	Loss: 1.1386
Training Epoch: 24 [39296/50048]	Loss: 1.0701
Training Epoch: 24 [39424/50048]	Loss: 1.0208
Training Epoch: 24 [39552/50048]	Loss: 0.9257
Training Epoch: 24 [39680/50048]	Loss: 1.1531
Training Epoch: 24 [39808/50048]	Loss: 1.1132
Training Epoch: 24 [39936/50048]	Loss: 1.4113
Training Epoch: 24 [40064/50048]	Loss: 1.3810
Training Epoch: 24 [40192/50048]	Loss: 1.2540
Training Epoch: 24 [40320/50048]	Loss: 1.3755
Training Epoch: 24 [40448/50048]	Loss: 1.0854
Training Epoch: 24 [40576/50048]	Loss: 1.1971
Training Epoch: 24 [40704/50048]	Loss: 1.0402
Training Epoch: 24 [40832/50048]	Loss: 1.2792
Training Epoch: 24 [40960/50048]	Loss: 1.0980
Training Epoch: 24 [41088/50048]	Loss: 1.1293
Training Epoch: 24 [41216/50048]	Loss: 1.2272
Training Epoch: 24 [41344/50048]	Loss: 1.2501
Training Epoch: 24 [41472/50048]	Loss: 1.2723
Training Epoch: 24 [41600/50048]	Loss: 1.1990
Training Epoch: 24 [41728/50048]	Loss: 1.0726
Training Epoch: 24 [41856/50048]	Loss: 1.0788
Training Epoch: 24 [41984/50048]	Loss: 0.9079
Training Epoch: 24 [42112/50048]	Loss: 1.1619
Training Epoch: 24 [42240/50048]	Loss: 1.1689
Training Epoch: 24 [42368/50048]	Loss: 1.2279
Training Epoch: 24 [42496/50048]	Loss: 1.1921
Training Epoch: 24 [42624/50048]	Loss: 1.0694
Training Epoch: 24 [42752/50048]	Loss: 1.2169
Training Epoch: 24 [42880/50048]	Loss: 1.3265
Training Epoch: 24 [43008/50048]	Loss: 1.0484
Training Epoch: 24 [43136/50048]	Loss: 1.1132
Training Epoch: 24 [43264/50048]	Loss: 1.1925
Training Epoch: 24 [43392/50048]	Loss: 1.3082
Training Epoch: 24 [43520/50048]	Loss: 1.2772
Training Epoch: 24 [43648/50048]	Loss: 1.1863
Training Epoch: 24 [43776/50048]	Loss: 1.0699
Training Epoch: 24 [43904/50048]	Loss: 1.1291
Training Epoch: 24 [44032/50048]	Loss: 1.2701
Training Epoch: 24 [44160/50048]	Loss: 1.2065
Training Epoch: 24 [44288/50048]	Loss: 1.2431
Training Epoch: 24 [44416/50048]	Loss: 1.0908
Training Epoch: 24 [44544/50048]	Loss: 1.1598
Training Epoch: 24 [44672/50048]	Loss: 1.1459
Training Epoch: 24 [44800/50048]	Loss: 1.0498
Training Epoch: 24 [44928/50048]	Loss: 1.2634
Training Epoch: 24 [45056/50048]	Loss: 1.0998
Training Epoch: 24 [45184/50048]	Loss: 0.8122
Training Epoch: 24 [45312/50048]	Loss: 1.2790
Training Epoch: 24 [45440/50048]	Loss: 1.0764
Training Epoch: 24 [45568/50048]	Loss: 1.1155
Training Epoch: 24 [45696/50048]	Loss: 0.9655
Training Epoch: 24 [45824/50048]	Loss: 1.2629
Training Epoch: 24 [45952/50048]	Loss: 1.1401
Training Epoch: 24 [46080/50048]	Loss: 1.2026
Training Epoch: 24 [46208/50048]	Loss: 1.0562
Training Epoch: 24 [46336/50048]	Loss: 1.2511
Training Epoch: 24 [46464/50048]	Loss: 1.1227
Training Epoch: 24 [46592/50048]	Loss: 1.3388
Training Epoch: 24 [46720/50048]	Loss: 1.1415
Training Epoch: 24 [46848/50048]	Loss: 1.1377
Training Epoch: 24 [46976/50048]	Loss: 1.1018
Training Epoch: 24 [47104/50048]	Loss: 1.2936
Training Epoch: 24 [47232/50048]	Loss: 1.2442
Training Epoch: 24 [47360/50048]	Loss: 1.3511
Training Epoch: 24 [47488/50048]	Loss: 1.2410
Training Epoch: 24 [47616/50048]	Loss: 0.9606
Training Epoch: 24 [47744/50048]	Loss: 0.9664
Training Epoch: 24 [47872/50048]	Loss: 1.2030
Training Epoch: 24 [48000/50048]	Loss: 1.3193
Training Epoch: 24 [48128/50048]	Loss: 1.2580
Training Epoch: 24 [48256/50048]	Loss: 0.8970
Training Epoch: 24 [48384/50048]	Loss: 1.3873
Training Epoch: 24 [48512/50048]	Loss: 1.1744
Training Epoch: 24 [48640/50048]	Loss: 1.1679
Training Epoch: 24 [48768/50048]	Loss: 1.2856
Training Epoch: 24 [48896/50048]	Loss: 1.3042
Training Epoch: 24 [49024/50048]	Loss: 1.1008
Training Epoch: 24 [49152/50048]	Loss: 1.1122
Training Epoch: 24 [49280/50048]	Loss: 1.1648
Training Epoch: 24 [49408/50048]	Loss: 1.0467
Training Epoch: 24 [49536/50048]	Loss: 1.0378
Training Epoch: 24 [49664/50048]	Loss: 1.2000
Training Epoch: 24 [49792/50048]	Loss: 1.0922
Training Epoch: 24 [49920/50048]	Loss: 1.1720
Training Epoch: 24 [50048/50048]	Loss: 1.5330
Validation Epoch: 24, Average loss: 0.0117, Accuracy: 0.5950
Training Epoch: 25 [128/50048]	Loss: 1.0517
Training Epoch: 25 [256/50048]	Loss: 0.8989
Training Epoch: 25 [384/50048]	Loss: 1.1947
Training Epoch: 25 [512/50048]	Loss: 1.2102
Training Epoch: 25 [640/50048]	Loss: 1.1337
Training Epoch: 25 [768/50048]	Loss: 1.0611
Training Epoch: 25 [896/50048]	Loss: 1.2091
Training Epoch: 25 [1024/50048]	Loss: 1.0600
Training Epoch: 25 [1152/50048]	Loss: 1.3239
Training Epoch: 25 [1280/50048]	Loss: 0.9491
Training Epoch: 25 [1408/50048]	Loss: 1.3235
Training Epoch: 25 [1536/50048]	Loss: 1.1025
Training Epoch: 25 [1664/50048]	Loss: 1.0327
Training Epoch: 25 [1792/50048]	Loss: 1.1686
Training Epoch: 25 [1920/50048]	Loss: 1.1901
Training Epoch: 25 [2048/50048]	Loss: 1.2882
Training Epoch: 25 [2176/50048]	Loss: 1.1548
Training Epoch: 25 [2304/50048]	Loss: 0.9390
Training Epoch: 25 [2432/50048]	Loss: 1.0128
Training Epoch: 25 [2560/50048]	Loss: 0.8628
Training Epoch: 25 [2688/50048]	Loss: 1.1064
Training Epoch: 25 [2816/50048]	Loss: 1.0752
Training Epoch: 25 [2944/50048]	Loss: 1.1194
Training Epoch: 25 [3072/50048]	Loss: 1.0071
Training Epoch: 25 [3200/50048]	Loss: 1.3063
Training Epoch: 25 [3328/50048]	Loss: 1.1933
Training Epoch: 25 [3456/50048]	Loss: 1.0162
Training Epoch: 25 [3584/50048]	Loss: 1.1479
Training Epoch: 25 [3712/50048]	Loss: 0.9394
Training Epoch: 25 [3840/50048]	Loss: 1.1351
Training Epoch: 25 [3968/50048]	Loss: 0.9697
Training Epoch: 25 [4096/50048]	Loss: 1.0170
Training Epoch: 25 [4224/50048]	Loss: 1.1439
Training Epoch: 25 [4352/50048]	Loss: 1.2092
Training Epoch: 25 [4480/50048]	Loss: 1.2606
Training Epoch: 25 [4608/50048]	Loss: 1.0084
Training Epoch: 25 [4736/50048]	Loss: 1.0418
Training Epoch: 25 [4864/50048]	Loss: 1.2440
Training Epoch: 25 [4992/50048]	Loss: 1.0190
Training Epoch: 25 [5120/50048]	Loss: 1.0371
Training Epoch: 25 [5248/50048]	Loss: 1.1210
Training Epoch: 25 [5376/50048]	Loss: 1.1968
Training Epoch: 25 [5504/50048]	Loss: 1.1212
Training Epoch: 25 [5632/50048]	Loss: 1.1069
Training Epoch: 25 [5760/50048]	Loss: 1.0751
Training Epoch: 25 [5888/50048]	Loss: 0.9503
Training Epoch: 25 [6016/50048]	Loss: 1.3858
Training Epoch: 25 [6144/50048]	Loss: 1.0424
Training Epoch: 25 [6272/50048]	Loss: 1.1676
Training Epoch: 25 [6400/50048]	Loss: 0.9485
Training Epoch: 25 [6528/50048]	Loss: 1.3151
Training Epoch: 25 [6656/50048]	Loss: 1.1644
Training Epoch: 25 [6784/50048]	Loss: 1.2163
Training Epoch: 25 [6912/50048]	Loss: 0.9918
Training Epoch: 25 [7040/50048]	Loss: 1.1377
Training Epoch: 25 [7168/50048]	Loss: 0.9446
Training Epoch: 25 [7296/50048]	Loss: 0.9847
Training Epoch: 25 [7424/50048]	Loss: 1.1730
Training Epoch: 25 [7552/50048]	Loss: 1.1339
Training Epoch: 25 [7680/50048]	Loss: 1.1401
Training Epoch: 25 [7808/50048]	Loss: 1.1195
Training Epoch: 25 [7936/50048]	Loss: 1.1245
Training Epoch: 25 [8064/50048]	Loss: 1.1231
Training Epoch: 25 [8192/50048]	Loss: 1.1833
Training Epoch: 25 [8320/50048]	Loss: 1.0058
Training Epoch: 25 [8448/50048]	Loss: 1.1345
Training Epoch: 25 [8576/50048]	Loss: 0.9530
Training Epoch: 25 [8704/50048]	Loss: 1.1584
Training Epoch: 25 [8832/50048]	Loss: 1.1348
Training Epoch: 25 [8960/50048]	Loss: 1.1987
Training Epoch: 25 [9088/50048]	Loss: 1.1651
Training Epoch: 25 [9216/50048]	Loss: 1.1703
Training Epoch: 25 [9344/50048]	Loss: 1.1360
Training Epoch: 25 [9472/50048]	Loss: 1.0740
Training Epoch: 25 [9600/50048]	Loss: 0.9833
Training Epoch: 25 [9728/50048]	Loss: 1.2046
Training Epoch: 25 [9856/50048]	Loss: 1.1204
Training Epoch: 25 [9984/50048]	Loss: 1.1923
Training Epoch: 25 [10112/50048]	Loss: 1.0802
Training Epoch: 25 [10240/50048]	Loss: 1.0211
Training Epoch: 25 [10368/50048]	Loss: 1.2075
Training Epoch: 25 [10496/50048]	Loss: 0.8768
Training Epoch: 25 [10624/50048]	Loss: 1.0630
Training Epoch: 25 [10752/50048]	Loss: 0.9770
Training Epoch: 25 [10880/50048]	Loss: 1.1285
Training Epoch: 25 [11008/50048]	Loss: 1.1720
Training Epoch: 25 [11136/50048]	Loss: 1.3967
Training Epoch: 25 [11264/50048]	Loss: 1.4362
Training Epoch: 25 [11392/50048]	Loss: 1.5061
Training Epoch: 25 [11520/50048]	Loss: 1.2700
Training Epoch: 25 [11648/50048]	Loss: 1.1408
Training Epoch: 25 [11776/50048]	Loss: 0.9147
Training Epoch: 25 [11904/50048]	Loss: 1.2164
Training Epoch: 25 [12032/50048]	Loss: 1.2408
Training Epoch: 25 [12160/50048]	Loss: 1.0573
Training Epoch: 25 [12288/50048]	Loss: 1.0381
Training Epoch: 25 [12416/50048]	Loss: 1.1924
Training Epoch: 25 [12544/50048]	Loss: 1.3374
Training Epoch: 25 [12672/50048]	Loss: 1.2662
Training Epoch: 25 [12800/50048]	Loss: 0.9742
Training Epoch: 25 [12928/50048]	Loss: 1.2220
Training Epoch: 25 [13056/50048]	Loss: 1.1742
Training Epoch: 25 [13184/50048]	Loss: 1.0446
Training Epoch: 25 [13312/50048]	Loss: 1.1584
Training Epoch: 25 [13440/50048]	Loss: 1.3796
Training Epoch: 25 [13568/50048]	Loss: 1.3473
Training Epoch: 25 [13696/50048]	Loss: 1.3497
Training Epoch: 25 [13824/50048]	Loss: 1.0995
Training Epoch: 25 [13952/50048]	Loss: 1.0612
Training Epoch: 25 [14080/50048]	Loss: 0.9602
Training Epoch: 25 [14208/50048]	Loss: 1.1308
Training Epoch: 25 [14336/50048]	Loss: 0.9100
Training Epoch: 25 [14464/50048]	Loss: 1.1366
Training Epoch: 25 [14592/50048]	Loss: 1.1231
Training Epoch: 25 [14720/50048]	Loss: 1.0090
Training Epoch: 25 [14848/50048]	Loss: 0.8186
Training Epoch: 25 [14976/50048]	Loss: 1.1288
Training Epoch: 25 [15104/50048]	Loss: 1.1245
Training Epoch: 25 [15232/50048]	Loss: 1.0403
Training Epoch: 25 [15360/50048]	Loss: 1.2449
Training Epoch: 25 [15488/50048]	Loss: 1.1843
Training Epoch: 25 [15616/50048]	Loss: 1.1166
Training Epoch: 25 [15744/50048]	Loss: 1.2951
Training Epoch: 25 [15872/50048]	Loss: 0.9609
Training Epoch: 25 [16000/50048]	Loss: 1.2405
Training Epoch: 25 [16128/50048]	Loss: 1.0710
Training Epoch: 25 [16256/50048]	Loss: 1.2123
Training Epoch: 25 [16384/50048]	Loss: 0.9940
Training Epoch: 25 [16512/50048]	Loss: 0.9912
Training Epoch: 25 [16640/50048]	Loss: 1.1399
Training Epoch: 25 [16768/50048]	Loss: 1.0993
Training Epoch: 25 [16896/50048]	Loss: 1.2293
Training Epoch: 25 [17024/50048]	Loss: 1.2052
Training Epoch: 25 [17152/50048]	Loss: 1.0086
Training Epoch: 25 [17280/50048]	Loss: 1.1455
Training Epoch: 25 [17408/50048]	Loss: 1.1692
Training Epoch: 25 [17536/50048]	Loss: 1.1345
Training Epoch: 25 [17664/50048]	Loss: 1.1934
Training Epoch: 25 [17792/50048]	Loss: 1.1741
Training Epoch: 25 [17920/50048]	Loss: 1.1645
Training Epoch: 25 [18048/50048]	Loss: 1.0082
Training Epoch: 25 [18176/50048]	Loss: 1.2345
Training Epoch: 25 [18304/50048]	Loss: 1.1464
Training Epoch: 25 [18432/50048]	Loss: 0.9787
Training Epoch: 25 [18560/50048]	Loss: 1.1009
Training Epoch: 25 [18688/50048]	Loss: 1.0386
Training Epoch: 25 [18816/50048]	Loss: 1.0584
Training Epoch: 25 [18944/50048]	Loss: 1.2925
Training Epoch: 25 [19072/50048]	Loss: 1.3815
Training Epoch: 25 [19200/50048]	Loss: 1.2272
Training Epoch: 25 [19328/50048]	Loss: 1.0942
Training Epoch: 25 [19456/50048]	Loss: 1.1112
Training Epoch: 25 [19584/50048]	Loss: 0.9380
Training Epoch: 25 [19712/50048]	Loss: 1.0694
Training Epoch: 25 [19840/50048]	Loss: 1.2710
Training Epoch: 25 [19968/50048]	Loss: 0.9280
Training Epoch: 25 [20096/50048]	Loss: 1.0917
Training Epoch: 25 [20224/50048]	Loss: 1.3359
Training Epoch: 25 [20352/50048]	Loss: 1.0896
Training Epoch: 25 [20480/50048]	Loss: 1.1548
Training Epoch: 25 [20608/50048]	Loss: 1.0441
Training Epoch: 25 [20736/50048]	Loss: 1.2666
Training Epoch: 25 [20864/50048]	Loss: 1.1425
Training Epoch: 25 [20992/50048]	Loss: 1.1737
Training Epoch: 25 [21120/50048]	Loss: 1.1099
Training Epoch: 25 [21248/50048]	Loss: 1.0468
Training Epoch: 25 [21376/50048]	Loss: 1.1473
Training Epoch: 25 [21504/50048]	Loss: 1.2421
Training Epoch: 25 [21632/50048]	Loss: 1.0827
Training Epoch: 25 [21760/50048]	Loss: 1.0144
Training Epoch: 25 [21888/50048]	Loss: 1.1175
Training Epoch: 25 [22016/50048]	Loss: 1.1236
Training Epoch: 25 [22144/50048]	Loss: 1.3266
Training Epoch: 25 [22272/50048]	Loss: 1.1241
Training Epoch: 25 [22400/50048]	Loss: 1.0188
Training Epoch: 25 [22528/50048]	Loss: 1.0937
Training Epoch: 25 [22656/50048]	Loss: 1.3216
Training Epoch: 25 [22784/50048]	Loss: 1.1973
Training Epoch: 25 [22912/50048]	Loss: 1.1716
Training Epoch: 25 [23040/50048]	Loss: 1.1712
Training Epoch: 25 [23168/50048]	Loss: 1.1288
Training Epoch: 25 [23296/50048]	Loss: 1.2794
Training Epoch: 25 [23424/50048]	Loss: 1.0287
Training Epoch: 25 [23552/50048]	Loss: 1.0164
Training Epoch: 25 [23680/50048]	Loss: 1.2262
Training Epoch: 25 [23808/50048]	Loss: 1.0147
Training Epoch: 25 [23936/50048]	Loss: 1.2154
Training Epoch: 25 [24064/50048]	Loss: 1.2932
Training Epoch: 25 [24192/50048]	Loss: 1.2494
Training Epoch: 25 [24320/50048]	Loss: 0.9719
Training Epoch: 25 [24448/50048]	Loss: 1.1293
Training Epoch: 25 [24576/50048]	Loss: 1.0454
Training Epoch: 25 [24704/50048]	Loss: 1.0394
Training Epoch: 25 [24832/50048]	Loss: 1.0384
Training Epoch: 25 [24960/50048]	Loss: 1.0264
Training Epoch: 25 [25088/50048]	Loss: 0.9549
Training Epoch: 25 [25216/50048]	Loss: 0.9619
Training Epoch: 25 [25344/50048]	Loss: 1.3794
Training Epoch: 25 [25472/50048]	Loss: 0.9971
Training Epoch: 25 [25600/50048]	Loss: 1.3001
Training Epoch: 25 [25728/50048]	Loss: 0.9906
Training Epoch: 25 [25856/50048]	Loss: 1.3157
Training Epoch: 25 [25984/50048]	Loss: 1.2280
Training Epoch: 25 [26112/50048]	Loss: 1.1297
Training Epoch: 25 [26240/50048]	Loss: 1.1553
Training Epoch: 25 [26368/50048]	Loss: 1.3663
Training Epoch: 25 [26496/50048]	Loss: 1.1611
Training Epoch: 25 [26624/50048]	Loss: 1.3601
Training Epoch: 25 [26752/50048]	Loss: 1.0546
Training Epoch: 25 [26880/50048]	Loss: 1.0127
Training Epoch: 25 [27008/50048]	Loss: 1.5874
Training Epoch: 25 [27136/50048]	Loss: 1.0781
Training Epoch: 25 [27264/50048]	Loss: 0.9590
Training Epoch: 25 [27392/50048]	Loss: 1.0906
Training Epoch: 25 [27520/50048]	Loss: 1.3386
Training Epoch: 25 [27648/50048]	Loss: 1.1640
Training Epoch: 25 [27776/50048]	Loss: 1.2071
Training Epoch: 25 [27904/50048]	Loss: 1.3643
Training Epoch: 25 [28032/50048]	Loss: 1.1330
Training Epoch: 25 [28160/50048]	Loss: 1.2963
Training Epoch: 25 [28288/50048]	Loss: 1.1810
Training Epoch: 25 [28416/50048]	Loss: 1.0884
Training Epoch: 25 [28544/50048]	Loss: 1.2354
Training Epoch: 25 [28672/50048]	Loss: 1.2440
Training Epoch: 25 [28800/50048]	Loss: 1.0240
Training Epoch: 25 [28928/50048]	Loss: 1.2808
Training Epoch: 25 [29056/50048]	Loss: 1.0608
Training Epoch: 25 [29184/50048]	Loss: 1.1352
Training Epoch: 25 [29312/50048]	Loss: 1.0631
Training Epoch: 25 [29440/50048]	Loss: 1.1052
Training Epoch: 25 [29568/50048]	Loss: 1.1042
Training Epoch: 25 [29696/50048]	Loss: 1.1039
Training Epoch: 25 [29824/50048]	Loss: 1.1323
Training Epoch: 25 [29952/50048]	Loss: 1.1539
Training Epoch: 25 [30080/50048]	Loss: 1.0824
Training Epoch: 25 [30208/50048]	Loss: 1.0718
Training Epoch: 25 [30336/50048]	Loss: 1.0765
Training Epoch: 25 [30464/50048]	Loss: 1.3203
Training Epoch: 25 [30592/50048]	Loss: 0.9978
Training Epoch: 25 [30720/50048]	Loss: 1.1948
Training Epoch: 25 [30848/50048]	Loss: 1.0275
Training Epoch: 25 [30976/50048]	Loss: 1.1600
Training Epoch: 25 [31104/50048]	Loss: 0.9847
Training Epoch: 25 [31232/50048]	Loss: 1.1426
Training Epoch: 25 [31360/50048]	Loss: 1.0980
Training Epoch: 25 [31488/50048]	Loss: 1.1811
Training Epoch: 25 [31616/50048]	Loss: 0.9706
Training Epoch: 25 [31744/50048]	Loss: 1.0675
Training Epoch: 25 [31872/50048]	Loss: 1.1920
Training Epoch: 25 [32000/50048]	Loss: 1.1516
Training Epoch: 25 [32128/50048]	Loss: 1.0760
Training Epoch: 25 [32256/50048]	Loss: 1.0963
Training Epoch: 25 [32384/50048]	Loss: 1.0507
Training Epoch: 25 [32512/50048]	Loss: 1.0402
Training Epoch: 25 [32640/50048]	Loss: 1.1964
Training Epoch: 25 [32768/50048]	Loss: 1.0803
Training Epoch: 25 [32896/50048]	Loss: 1.2565
Training Epoch: 25 [33024/50048]	Loss: 0.9627
Training Epoch: 25 [33152/50048]	Loss: 1.1836
Training Epoch: 25 [33280/50048]	Loss: 1.1207
Training Epoch: 25 [33408/50048]	Loss: 1.3466
Training Epoch: 25 [33536/50048]	Loss: 1.1409
Training Epoch: 25 [33664/50048]	Loss: 1.0158
Training Epoch: 25 [33792/50048]	Loss: 1.0405
Training Epoch: 25 [33920/50048]	Loss: 1.2554
Training Epoch: 25 [34048/50048]	Loss: 1.3062
Training Epoch: 25 [34176/50048]	Loss: 1.0684
Training Epoch: 25 [34304/50048]	Loss: 0.9939
Training Epoch: 25 [34432/50048]	Loss: 1.1369
Training Epoch: 25 [34560/50048]	Loss: 1.2668
Training Epoch: 25 [34688/50048]	Loss: 1.4734
Training Epoch: 25 [34816/50048]	Loss: 1.2463
Training Epoch: 25 [34944/50048]	Loss: 1.2748
Training Epoch: 25 [35072/50048]	Loss: 0.9747
Training Epoch: 25 [35200/50048]	Loss: 1.4970
Training Epoch: 25 [35328/50048]	Loss: 1.2341
Training Epoch: 25 [35456/50048]	Loss: 1.1629
Training Epoch: 25 [35584/50048]	Loss: 0.9966
Training Epoch: 25 [35712/50048]	Loss: 0.9053
Training Epoch: 25 [35840/50048]	Loss: 1.4302
Training Epoch: 25 [35968/50048]	Loss: 1.1760
Training Epoch: 25 [36096/50048]	Loss: 0.9144
Training Epoch: 25 [36224/50048]	Loss: 1.2382
Training Epoch: 25 [36352/50048]	Loss: 1.2356
Training Epoch: 25 [36480/50048]	Loss: 0.9980
Training Epoch: 25 [36608/50048]	Loss: 1.1067
Training Epoch: 25 [36736/50048]	Loss: 1.1635
Training Epoch: 25 [36864/50048]	Loss: 1.1467
Training Epoch: 25 [36992/50048]	Loss: 1.1172
Training Epoch: 25 [37120/50048]	Loss: 1.2537
Training Epoch: 25 [37248/50048]	Loss: 1.0528
Training Epoch: 25 [37376/50048]	Loss: 1.4265
Training Epoch: 25 [37504/50048]	Loss: 1.0359
Training Epoch: 25 [37632/50048]	Loss: 1.1424
Training Epoch: 25 [37760/50048]	Loss: 1.1406
Training Epoch: 25 [37888/50048]	Loss: 1.3023
Training Epoch: 25 [38016/50048]	Loss: 0.9955
Training Epoch: 25 [38144/50048]	Loss: 1.2743
Training Epoch: 25 [38272/50048]	Loss: 1.3828
Training Epoch: 25 [38400/50048]	Loss: 0.9797
Training Epoch: 25 [38528/50048]	Loss: 1.0692
Training Epoch: 25 [38656/50048]	Loss: 1.2697
Training Epoch: 25 [38784/50048]	Loss: 1.1120
Training Epoch: 25 [38912/50048]	Loss: 1.1171
Training Epoch: 25 [39040/50048]	Loss: 1.0044
Training Epoch: 25 [39168/50048]	Loss: 1.1060
Training Epoch: 25 [39296/50048]	Loss: 1.3001
Training Epoch: 25 [39424/50048]	Loss: 1.1429
Training Epoch: 25 [39552/50048]	Loss: 1.0738
Training Epoch: 25 [39680/50048]	Loss: 1.0429
Training Epoch: 25 [39808/50048]	Loss: 1.1985
Training Epoch: 25 [39936/50048]	Loss: 1.1478
Training Epoch: 25 [40064/50048]	Loss: 1.1383
Training Epoch: 25 [40192/50048]	Loss: 1.0500
Training Epoch: 25 [40320/50048]	Loss: 1.0066
Training Epoch: 25 [40448/50048]	Loss: 1.2030
Training Epoch: 25 [40576/50048]	Loss: 1.3659
Training Epoch: 25 [40704/50048]	Loss: 1.0900
Training Epoch: 25 [40832/50048]	Loss: 0.8920
Training Epoch: 25 [40960/50048]	Loss: 1.0180
Training Epoch: 25 [41088/50048]	Loss: 1.0491
Training Epoch: 25 [41216/50048]	Loss: 1.3052
Training Epoch: 25 [41344/50048]	Loss: 1.1351
Training Epoch: 25 [41472/50048]	Loss: 1.1684
Training Epoch: 25 [41600/50048]	Loss: 0.9759
Training Epoch: 25 [41728/50048]	Loss: 1.1688
Training Epoch: 25 [41856/50048]	Loss: 1.3674
Training Epoch: 25 [41984/50048]	Loss: 1.1110
Training Epoch: 25 [42112/50048]	Loss: 1.2878
Training Epoch: 25 [42240/50048]	Loss: 0.9789
Training Epoch: 25 [42368/50048]	Loss: 1.2573
Training Epoch: 25 [42496/50048]	Loss: 1.1676
Training Epoch: 25 [42624/50048]	Loss: 1.1983
Training Epoch: 25 [42752/50048]	Loss: 1.1579
Training Epoch: 25 [42880/50048]	Loss: 1.1968
Training Epoch: 25 [43008/50048]	Loss: 1.0935
Training Epoch: 25 [43136/50048]	Loss: 1.1484
Training Epoch: 25 [43264/50048]	Loss: 1.1467
Training Epoch: 25 [43392/50048]	Loss: 1.1993
Training Epoch: 25 [43520/50048]	Loss: 1.1192
Training Epoch: 25 [43648/50048]	Loss: 1.0353
Training Epoch: 25 [43776/50048]	Loss: 1.0749
Training Epoch: 25 [43904/50048]	Loss: 1.0040
Training Epoch: 25 [44032/50048]	Loss: 1.1840
Training Epoch: 25 [44160/50048]	Loss: 1.2902
Training Epoch: 25 [44288/50048]	Loss: 1.2564
Training Epoch: 25 [44416/50048]	Loss: 1.2587
Training Epoch: 25 [44544/50048]	Loss: 1.2911
Training Epoch: 25 [44672/50048]	Loss: 1.0165
Training Epoch: 25 [44800/50048]	Loss: 1.2032
Training Epoch: 25 [44928/50048]	Loss: 1.0887
Training Epoch: 25 [45056/50048]	Loss: 1.1944
Training Epoch: 25 [45184/50048]	Loss: 1.2403
Training Epoch: 25 [45312/50048]	Loss: 1.1432
Training Epoch: 25 [45440/50048]	Loss: 1.1054
Training Epoch: 25 [45568/50048]	Loss: 1.1502
Training Epoch: 25 [45696/50048]	Loss: 1.1606
Training Epoch: 25 [45824/50048]	Loss: 0.9730
Training Epoch: 25 [45952/50048]	Loss: 1.1387
Training Epoch: 25 [46080/50048]	Loss: 1.0387
Training Epoch: 25 [46208/50048]	Loss: 1.1803
Training Epoch: 25 [46336/50048]	Loss: 1.1880
Training Epoch: 25 [46464/50048]	Loss: 1.2879
Training Epoch: 25 [46592/50048]	Loss: 1.2196
Training Epoch: 25 [46720/50048]	Loss: 0.9505
Training Epoch: 25 [46848/50048]	Loss: 1.0225
Training Epoch: 25 [46976/50048]	Loss: 1.2145
Training Epoch: 25 [47104/50048]	Loss: 1.0302
Training Epoch: 25 [47232/50048]	Loss: 1.1911
Training Epoch: 25 [47360/50048]	Loss: 1.2566
Training Epoch: 25 [47488/50048]	Loss: 1.0715
Training Epoch: 25 [47616/50048]	Loss: 1.2099
Training Epoch: 25 [47744/50048]	Loss: 1.1600
Training Epoch: 25 [47872/50048]	Loss: 1.0663
Training Epoch: 25 [48000/50048]	Loss: 1.0826
Training Epoch: 25 [48128/50048]	Loss: 1.0755
Training Epoch: 25 [48256/50048]	Loss: 1.0027
Training Epoch: 25 [48384/50048]	Loss: 1.2482
Training Epoch: 25 [48512/50048]	Loss: 1.0924
Training Epoch: 25 [48640/50048]	Loss: 1.0661
Training Epoch: 25 [48768/50048]	Loss: 1.2514
Training Epoch: 25 [48896/50048]	Loss: 1.1453
Training Epoch: 25 [49024/50048]	Loss: 1.2097
Training Epoch: 25 [49152/50048]	Loss: 1.1081
Training Epoch: 25 [49280/50048]	Loss: 1.0095
Training Epoch: 25 [49408/50048]	Loss: 1.0804
Training Epoch: 25 [49536/50048]	Loss: 1.1549
Training Epoch: 25 [49664/50048]	Loss: 1.1063
Training Epoch: 25 [49792/50048]	Loss: 0.9857
Training Epoch: 25 [49920/50048]	Loss: 1.1459
Training Epoch: 25 [50048/50048]	Loss: 1.1664
Validation Epoch: 25, Average loss: 0.0116, Accuracy: 0.6019
[Training Loop] Target accuracy 0.6 reached!
[Training Loop] Training done
Stopped Zeus monitor 0.
