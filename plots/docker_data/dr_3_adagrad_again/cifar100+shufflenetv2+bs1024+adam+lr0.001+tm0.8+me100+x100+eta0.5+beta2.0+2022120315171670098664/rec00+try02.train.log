2022-12-03 17:47:56,777 [ZeusDataLoader(train)] Distributed data parallel: OFF
2022-12-03 17:47:56,818 [ZeusDataLoader(train)] Power limit range: [175000, 150000, 125000, 100000]
2022-12-03 17:47:56,819 [ZeusDataLoader(train)] Power profiling: OFF
2022-12-03 17:47:56,819 [ZeusDataLoader(train)] Loaded /workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.8+me100+x100+eta0.5+beta2.0+2022120315171670098664/bs128+dr0.3.power.json: {'job_id': 'rec00+try01', 'train_power': {'175000': 128.7114632095464, '150000': 129.0903198276856, '125000': 122.14751317057538, '100000': 98.52768040107193}, 'train_throughput': {'175000': 4.569441891941347, '150000': 4.560482884012167, '125000': 4.447955440044087, '100000': 3.7653759644070846}, 'eval_power': {'175000': 129.05193511768522}, 'eval_throughput': {'175000': 21.23484361556091}, 'optimal_pl': 175000}
2022-12-03 17:47:56,819 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-03 17:47:56,823 [ZeusDataLoader(train)] [GPU_0] Set GPU power limit to 175W.
2022-12-03 17:47:58,872 [ZeusDataLoader(train)] Up to epoch 0: time=0.00, energy=0.00, cost=0.00
2022-12-03 17:47:58,872 [ZeusDataLoader(train)] Epoch 1 begin.
Files already downloaded and verified
Files already downloaded and verified
2022-12-03 17:47:59,117 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-03 17:47:59,118 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-03 22:47:59.120 [ZeusMonitor] Monitor started.
2022-12-03 22:47:59.120 [ZeusMonitor] Running indefinitely. 2022-12-03 22:47:59.120 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-03 22:47:59.120 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.8+me100+x100+eta0.5+beta2.0+2022120315171670098664/bs128+e1+gpu0.power.log
Training Epoch: 0 [128/50048]	Loss: 4.6415
Training Epoch: 0 [256/50048]	Loss: 4.8469
Training Epoch: 0 [384/50048]	Loss: 4.9079
Training Epoch: 0 [512/50048]	Loss: 4.7387
Training Epoch: 0 [640/50048]	Loss: 4.9237
Training Epoch: 0 [768/50048]	Loss: 4.8391
Training Epoch: 0 [896/50048]	Loss: 4.9119
Training Epoch: 0 [1024/50048]	Loss: 4.7867
Training Epoch: 0 [1152/50048]	Loss: 4.8988
Training Epoch: 0 [1280/50048]	Loss: 4.8090
Training Epoch: 0 [1408/50048]	Loss: 4.7800
Training Epoch: 0 [1536/50048]	Loss: 4.6401
Training Epoch: 0 [1664/50048]	Loss: 4.8786
Training Epoch: 0 [1792/50048]	Loss: 4.7454
Training Epoch: 0 [1920/50048]	Loss: 4.8291
Training Epoch: 0 [2048/50048]	Loss: 4.6701
Training Epoch: 0 [2176/50048]	Loss: 4.9174
Training Epoch: 0 [2304/50048]	Loss: 4.8345
Training Epoch: 0 [2432/50048]	Loss: 4.6762
Training Epoch: 0 [2560/50048]	Loss: 4.6156
Training Epoch: 0 [2688/50048]	Loss: 4.6715
Training Epoch: 0 [2816/50048]	Loss: 4.8374
Training Epoch: 0 [2944/50048]	Loss: 4.5950
Training Epoch: 0 [3072/50048]	Loss: 4.6705
Training Epoch: 0 [3200/50048]	Loss: 4.7912
Training Epoch: 0 [3328/50048]	Loss: 4.6594
Training Epoch: 0 [3456/50048]	Loss: 4.6211
Training Epoch: 0 [3584/50048]	Loss: 4.5707
Training Epoch: 0 [3712/50048]	Loss: 4.6172
Training Epoch: 0 [3840/50048]	Loss: 4.5734
Training Epoch: 0 [3968/50048]	Loss: 4.6646
Training Epoch: 0 [4096/50048]	Loss: 4.6890
Training Epoch: 0 [4224/50048]	Loss: 4.6100
Training Epoch: 0 [4352/50048]	Loss: 4.5602
Training Epoch: 0 [4480/50048]	Loss: 4.4841
Training Epoch: 0 [4608/50048]	Loss: 4.4976
Training Epoch: 0 [4736/50048]	Loss: 4.5230
Training Epoch: 0 [4864/50048]	Loss: 4.4597
Training Epoch: 0 [4992/50048]	Loss: 4.5534
Training Epoch: 0 [5120/50048]	Loss: 4.4884
Training Epoch: 0 [5248/50048]	Loss: 4.4290
Training Epoch: 0 [5376/50048]	Loss: 4.4113
Training Epoch: 0 [5504/50048]	Loss: 4.4041
Training Epoch: 0 [5632/50048]	Loss: 4.4741
Training Epoch: 0 [5760/50048]	Loss: 4.5123
Training Epoch: 0 [5888/50048]	Loss: 4.4605
Training Epoch: 0 [6016/50048]	Loss: 4.2443
Training Epoch: 0 [6144/50048]	Loss: 4.0987
Training Epoch: 0 [6272/50048]	Loss: 4.6017
Training Epoch: 0 [6400/50048]	Loss: 4.2791
Training Epoch: 0 [6528/50048]	Loss: 4.3125
Training Epoch: 0 [6656/50048]	Loss: 4.4639
Training Epoch: 0 [6784/50048]	Loss: 4.3268
Training Epoch: 0 [6912/50048]	Loss: 4.4167
Training Epoch: 0 [7040/50048]	Loss: 4.2262
Training Epoch: 0 [7168/50048]	Loss: 4.2904
Training Epoch: 0 [7296/50048]	Loss: 4.3278
Training Epoch: 0 [7424/50048]	Loss: 4.2896
Training Epoch: 0 [7552/50048]	Loss: 4.2676
Training Epoch: 0 [7680/50048]	Loss: 4.4047
Training Epoch: 0 [7808/50048]	Loss: 4.3031
Training Epoch: 0 [7936/50048]	Loss: 4.2962
Training Epoch: 0 [8064/50048]	Loss: 4.1961
Training Epoch: 0 [8192/50048]	Loss: 4.2170
Training Epoch: 0 [8320/50048]	Loss: 4.3181
Training Epoch: 0 [8448/50048]	Loss: 4.2722
Training Epoch: 0 [8576/50048]	Loss: 4.1255
Training Epoch: 0 [8704/50048]	Loss: 4.3046
Training Epoch: 0 [8832/50048]	Loss: 4.1457
Training Epoch: 0 [8960/50048]	Loss: 4.3197
Training Epoch: 0 [9088/50048]	Loss: 4.2677
Training Epoch: 0 [9216/50048]	Loss: 4.3033
Training Epoch: 0 [9344/50048]	Loss: 4.2025
Training Epoch: 0 [9472/50048]	Loss: 4.2480
Training Epoch: 0 [9600/50048]	Loss: 4.1081
Training Epoch: 0 [9728/50048]	Loss: 4.3372
Training Epoch: 0 [9856/50048]	Loss: 4.2097
Training Epoch: 0 [9984/50048]	Loss: 4.2596
Training Epoch: 0 [10112/50048]	Loss: 4.2743
Training Epoch: 0 [10240/50048]	Loss: 4.2927
Training Epoch: 0 [10368/50048]	Loss: 4.1545
Training Epoch: 0 [10496/50048]	Loss: 4.2003
Training Epoch: 0 [10624/50048]	Loss: 4.2364
Training Epoch: 0 [10752/50048]	Loss: 4.1694
Training Epoch: 0 [10880/50048]	Loss: 4.2134
Training Epoch: 0 [11008/50048]	Loss: 4.0759
Training Epoch: 0 [11136/50048]	Loss: 4.4039
Training Epoch: 0 [11264/50048]	Loss: 4.2512
Training Epoch: 0 [11392/50048]	Loss: 4.2191
Training Epoch: 0 [11520/50048]	Loss: 4.1275
Training Epoch: 0 [11648/50048]	Loss: 4.1071
Training Epoch: 0 [11776/50048]	Loss: 4.1241
Training Epoch: 0 [11904/50048]	Loss: 4.2516
Training Epoch: 0 [12032/50048]	Loss: 4.1828
Training Epoch: 0 [12160/50048]	Loss: 4.2418
Training Epoch: 0 [12288/50048]	Loss: 4.0683
Training Epoch: 0 [12416/50048]	Loss: 4.2250
Training Epoch: 0 [12544/50048]	Loss: 4.1201
Training Epoch: 0 [12672/50048]	Loss: 4.1537
Training Epoch: 0 [12800/50048]	Loss: 4.0735
Training Epoch: 0 [12928/50048]	Loss: 4.0313
Training Epoch: 0 [13056/50048]	Loss: 4.0436
Training Epoch: 0 [13184/50048]	Loss: 4.2264
Training Epoch: 0 [13312/50048]	Loss: 4.1812
Training Epoch: 0 [13440/50048]	Loss: 4.1328
Training Epoch: 0 [13568/50048]	Loss: 4.1393
Training Epoch: 0 [13696/50048]	Loss: 3.9211
Training Epoch: 0 [13824/50048]	Loss: 4.2604
Training Epoch: 0 [13952/50048]	Loss: 4.1215
Training Epoch: 0 [14080/50048]	Loss: 4.0320
Training Epoch: 0 [14208/50048]	Loss: 4.2550
Training Epoch: 0 [14336/50048]	Loss: 4.1085
Training Epoch: 0 [14464/50048]	Loss: 4.1256
Training Epoch: 0 [14592/50048]	Loss: 4.0611
Training Epoch: 0 [14720/50048]	Loss: 4.0289
Training Epoch: 0 [14848/50048]	Loss: 3.8880
Training Epoch: 0 [14976/50048]	Loss: 4.1639
Training Epoch: 0 [15104/50048]	Loss: 4.0211
Training Epoch: 0 [15232/50048]	Loss: 4.1637
Training Epoch: 0 [15360/50048]	Loss: 4.2391
Training Epoch: 0 [15488/50048]	Loss: 4.0768
Training Epoch: 0 [15616/50048]	Loss: 4.0881
Training Epoch: 0 [15744/50048]	Loss: 4.0742
Training Epoch: 0 [15872/50048]	Loss: 4.1539
Training Epoch: 0 [16000/50048]	Loss: 4.0496
Training Epoch: 0 [16128/50048]	Loss: 4.0801
Training Epoch: 0 [16256/50048]	Loss: 4.0501
Training Epoch: 0 [16384/50048]	Loss: 3.9033
Training Epoch: 0 [16512/50048]	Loss: 4.0490
Training Epoch: 0 [16640/50048]	Loss: 3.9496
Training Epoch: 0 [16768/50048]	Loss: 3.8091
Training Epoch: 0 [16896/50048]	Loss: 4.1385
Training Epoch: 0 [17024/50048]	Loss: 4.0140
Training Epoch: 0 [17152/50048]	Loss: 4.1231
Training Epoch: 0 [17280/50048]	Loss: 4.2776
Training Epoch: 0 [17408/50048]	Loss: 4.1037
Training Epoch: 0 [17536/50048]	Loss: 4.0239
Training Epoch: 0 [17664/50048]	Loss: 3.8997
Training Epoch: 0 [17792/50048]	Loss: 4.0942
Training Epoch: 0 [17920/50048]	Loss: 4.1559
Training Epoch: 0 [18048/50048]	Loss: 3.9521
Training Epoch: 0 [18176/50048]	Loss: 3.8737
Training Epoch: 0 [18304/50048]	Loss: 4.2151
Training Epoch: 0 [18432/50048]	Loss: 4.0880
Training Epoch: 0 [18560/50048]	Loss: 3.9543
Training Epoch: 0 [18688/50048]	Loss: 3.9234
Training Epoch: 0 [18816/50048]	Loss: 3.9736
Training Epoch: 0 [18944/50048]	Loss: 3.9690
Training Epoch: 0 [19072/50048]	Loss: 4.1307
Training Epoch: 0 [19200/50048]	Loss: 4.0600
Training Epoch: 0 [19328/50048]	Loss: 3.9189
Training Epoch: 0 [19456/50048]	Loss: 3.8538
Training Epoch: 0 [19584/50048]	Loss: 3.9536
Training Epoch: 0 [19712/50048]	Loss: 3.9819
Training Epoch: 0 [19840/50048]	Loss: 4.1161
Training Epoch: 0 [19968/50048]	Loss: 4.1309
Training Epoch: 0 [20096/50048]	Loss: 3.8833
Training Epoch: 0 [20224/50048]	Loss: 3.8654
Training Epoch: 0 [20352/50048]	Loss: 3.7532
Training Epoch: 0 [20480/50048]	Loss: 4.0873
Training Epoch: 0 [20608/50048]	Loss: 3.8739
Training Epoch: 0 [20736/50048]	Loss: 3.9504
Training Epoch: 0 [20864/50048]	Loss: 3.8245
Training Epoch: 0 [20992/50048]	Loss: 3.9587
Training Epoch: 0 [21120/50048]	Loss: 4.1558
Training Epoch: 0 [21248/50048]	Loss: 4.2125
Training Epoch: 0 [21376/50048]	Loss: 4.0570
Training Epoch: 0 [21504/50048]	Loss: 3.8957
Training Epoch: 0 [21632/50048]	Loss: 3.9765
Training Epoch: 0 [21760/50048]	Loss: 3.9942
Training Epoch: 0 [21888/50048]	Loss: 4.0296
Training Epoch: 0 [22016/50048]	Loss: 4.0029
Training Epoch: 0 [22144/50048]	Loss: 4.0647
Training Epoch: 0 [22272/50048]	Loss: 3.7415
Training Epoch: 0 [22400/50048]	Loss: 3.9603
Training Epoch: 0 [22528/50048]	Loss: 3.9671
Training Epoch: 0 [22656/50048]	Loss: 3.7124
Training Epoch: 0 [22784/50048]	Loss: 3.8866
Training Epoch: 0 [22912/50048]	Loss: 3.8485
Training Epoch: 0 [23040/50048]	Loss: 3.9384
Training Epoch: 0 [23168/50048]	Loss: 3.9278
Training Epoch: 0 [23296/50048]	Loss: 3.9245
Training Epoch: 0 [23424/50048]	Loss: 4.0171
Training Epoch: 0 [23552/50048]	Loss: 3.9058
Training Epoch: 0 [23680/50048]	Loss: 3.8521
Training Epoch: 0 [23808/50048]	Loss: 3.8481
Training Epoch: 0 [23936/50048]	Loss: 3.9532
Training Epoch: 0 [24064/50048]	Loss: 3.9923
Training Epoch: 0 [24192/50048]	Loss: 3.8561
Training Epoch: 0 [24320/50048]	Loss: 3.8927
Training Epoch: 0 [24448/50048]	Loss: 3.8078
Training Epoch: 0 [24576/50048]	Loss: 4.0521
Training Epoch: 0 [24704/50048]	Loss: 3.9692
Training Epoch: 0 [24832/50048]	Loss: 4.0178
Training Epoch: 0 [24960/50048]	Loss: 3.8805
Training Epoch: 0 [25088/50048]	Loss: 3.8315
Training Epoch: 0 [25216/50048]	Loss: 3.7940
Training Epoch: 0 [25344/50048]	Loss: 3.8927
Training Epoch: 0 [25472/50048]	Loss: 3.8991
Training Epoch: 0 [25600/50048]	Loss: 3.9402
Training Epoch: 0 [25728/50048]	Loss: 3.8712
Training Epoch: 0 [25856/50048]	Loss: 3.9308
Training Epoch: 0 [25984/50048]	Loss: 3.8551
Training Epoch: 0 [26112/50048]	Loss: 3.7965
Training Epoch: 0 [26240/50048]	Loss: 4.0437
Training Epoch: 0 [26368/50048]	Loss: 3.9334
Training Epoch: 0 [26496/50048]	Loss: 3.8632
Training Epoch: 0 [26624/50048]	Loss: 3.8901
Training Epoch: 0 [26752/50048]	Loss: 3.9672
Training Epoch: 0 [26880/50048]	Loss: 3.8341
Training Epoch: 0 [27008/50048]	Loss: 3.7121
Training Epoch: 0 [27136/50048]	Loss: 3.8075
Training Epoch: 0 [27264/50048]	Loss: 3.8883
Training Epoch: 0 [27392/50048]	Loss: 3.9365
Training Epoch: 0 [27520/50048]	Loss: 3.9347
Training Epoch: 0 [27648/50048]	Loss: 3.8095
Training Epoch: 0 [27776/50048]	Loss: 3.8819
Training Epoch: 0 [27904/50048]	Loss: 4.0664
Training Epoch: 0 [28032/50048]	Loss: 3.8529
Training Epoch: 0 [28160/50048]	Loss: 3.8640
Training Epoch: 0 [28288/50048]	Loss: 3.7419
Training Epoch: 0 [28416/50048]	Loss: 3.7336
Training Epoch: 0 [28544/50048]	Loss: 3.7922
Training Epoch: 0 [28672/50048]	Loss: 3.7946
Training Epoch: 0 [28800/50048]	Loss: 3.9506
Training Epoch: 0 [28928/50048]	Loss: 3.8502
Training Epoch: 0 [29056/50048]	Loss: 3.9027
Training Epoch: 0 [29184/50048]	Loss: 3.9522
Training Epoch: 0 [29312/50048]	Loss: 3.7438
Training Epoch: 0 [29440/50048]	Loss: 3.9474
Training Epoch: 0 [29568/50048]	Loss: 3.9021
Training Epoch: 0 [29696/50048]	Loss: 3.7834
Training Epoch: 0 [29824/50048]	Loss: 3.8856
Training Epoch: 0 [29952/50048]	Loss: 3.8984
Training Epoch: 0 [30080/50048]	Loss: 3.8441
Training Epoch: 0 [30208/50048]	Loss: 4.0457
Training Epoch: 0 [30336/50048]	Loss: 3.8980
Training Epoch: 0 [30464/50048]	Loss: 3.9178
Training Epoch: 0 [30592/50048]	Loss: 3.8683
Training Epoch: 0 [30720/50048]	Loss: 3.7491
Training Epoch: 0 [30848/50048]	Loss: 3.9584
Training Epoch: 0 [30976/50048]	Loss: 3.8044
Training Epoch: 0 [31104/50048]	Loss: 3.8583
Training Epoch: 0 [31232/50048]	Loss: 3.9444
Training Epoch: 0 [31360/50048]	Loss: 3.7546
Training Epoch: 0 [31488/50048]	Loss: 4.0161
Training Epoch: 0 [31616/50048]	Loss: 3.8591
Training Epoch: 0 [31744/50048]	Loss: 3.9084
Training Epoch: 0 [31872/50048]	Loss: 3.8701
Training Epoch: 0 [32000/50048]	Loss: 3.9905
Training Epoch: 0 [32128/50048]	Loss: 3.5582
Training Epoch: 0 [32256/50048]	Loss: 3.9021
Training Epoch: 0 [32384/50048]	Loss: 3.6759
Training Epoch: 0 [32512/50048]	Loss: 3.6197
Training Epoch: 0 [32640/50048]	Loss: 3.8387
Training Epoch: 0 [32768/50048]	Loss: 3.7969
Training Epoch: 0 [32896/50048]	Loss: 3.9054
Training Epoch: 0 [33024/50048]	Loss: 3.8727
Training Epoch: 0 [33152/50048]	Loss: 3.8399
Training Epoch: 0 [33280/50048]	Loss: 3.7620
Training Epoch: 0 [33408/50048]	Loss: 3.7707
Training Epoch: 0 [33536/50048]	Loss: 3.9746
Training Epoch: 0 [33664/50048]	Loss: 3.7176
Training Epoch: 0 [33792/50048]	Loss: 3.6076
Training Epoch: 0 [33920/50048]	Loss: 3.6573
Training Epoch: 0 [34048/50048]	Loss: 3.9132
Training Epoch: 0 [34176/50048]	Loss: 3.8964
Training Epoch: 0 [34304/50048]	Loss: 3.7404
Training Epoch: 0 [34432/50048]	Loss: 3.9448
Training Epoch: 0 [34560/50048]	Loss: 3.8132
Training Epoch: 0 [34688/50048]	Loss: 3.9426
Training Epoch: 0 [34816/50048]	Loss: 3.8819
Training Epoch: 0 [34944/50048]	Loss: 3.8298
Training Epoch: 0 [35072/50048]	Loss: 3.7077
Training Epoch: 0 [35200/50048]	Loss: 3.6395
Training Epoch: 0 [35328/50048]	Loss: 3.7794
Training Epoch: 0 [35456/50048]	Loss: 3.7335
Training Epoch: 0 [35584/50048]	Loss: 3.7276
Training Epoch: 0 [35712/50048]	Loss: 3.8013
Training Epoch: 0 [35840/50048]	Loss: 3.8460
Training Epoch: 0 [35968/50048]	Loss: 3.8088
Training Epoch: 0 [36096/50048]	Loss: 3.7056
Training Epoch: 0 [36224/50048]	Loss: 3.6967
Training Epoch: 0 [36352/50048]	Loss: 3.5917
Training Epoch: 0 [36480/50048]	Loss: 3.7251
Training Epoch: 0 [36608/50048]	Loss: 3.9177
Training Epoch: 0 [36736/50048]	Loss: 3.8434
Training Epoch: 0 [36864/50048]	Loss: 3.7202
Training Epoch: 0 [36992/50048]	Loss: 3.6714
Training Epoch: 0 [37120/50048]	Loss: 3.7613
Training Epoch: 0 [37248/50048]	Loss: 3.5567
Training Epoch: 0 [37376/50048]	Loss: 3.8127
Training Epoch: 0 [37504/50048]	Loss: 3.8610
Training Epoch: 0 [37632/50048]	Loss: 3.7922
Training Epoch: 0 [37760/50048]	Loss: 3.6054
Training Epoch: 0 [37888/50048]	Loss: 4.0631
Training Epoch: 0 [38016/50048]	Loss: 3.7254
Training Epoch: 0 [38144/50048]	Loss: 3.7380
Training Epoch: 0 [38272/50048]	Loss: 3.9176
Training Epoch: 0 [38400/50048]	Loss: 3.7012
Training Epoch: 0 [38528/50048]	Loss: 3.8469
Training Epoch: 0 [38656/50048]	Loss: 3.5971
Training Epoch: 0 [38784/50048]	Loss: 3.5457
Training Epoch: 0 [38912/50048]	Loss: 3.7349
Training Epoch: 0 [39040/50048]	Loss: 3.7475
Training Epoch: 0 [39168/50048]	Loss: 3.9304
Training Epoch: 0 [39296/50048]	Loss: 3.7775
Training Epoch: 0 [39424/50048]	Loss: 3.7314
Training Epoch: 0 [39552/50048]	Loss: 3.4927
Training Epoch: 0 [39680/50048]	Loss: 3.6907
Training Epoch: 0 [39808/50048]	Loss: 3.6642
Training Epoch: 0 [39936/50048]	Loss: 3.8835
Training Epoch: 0 [40064/50048]	Loss: 3.7782
Training Epoch: 0 [40192/50048]	Loss: 3.6706
Training Epoch: 0 [40320/50048]	Loss: 3.6132
Training Epoch: 0 [40448/50048]	Loss: 3.6719
Training Epoch: 0 [40576/50048]	Loss: 3.7782
Training Epoch: 0 [40704/50048]	Loss: 3.6561
Training Epoch: 0 [40832/50048]	Loss: 3.7011
Training Epoch: 0 [40960/50048]	Loss: 3.7914
Training Epoch: 0 [41088/50048]	Loss: 3.6759
Training Epoch: 0 [41216/50048]	Loss: 3.6583
Training Epoch: 0 [41344/50048]	Loss: 3.8508
Training Epoch: 0 [41472/50048]	Loss: 3.7041
Training Epoch: 0 [41600/50048]	Loss: 3.6053
Training Epoch: 0 [41728/50048]	Loss: 3.5911
Training Epoch: 0 [41856/50048]	Loss: 3.7173
Training Epoch: 0 [41984/50048]	Loss: 3.8214
Training Epoch: 0 [42112/50048]	Loss: 3.7660
Training Epoch: 0 [42240/50048]	Loss: 3.7959
Training Epoch: 0 [42368/50048]	Loss: 3.6299
Training Epoch: 0 [42496/50048]	Loss: 4.0236
Training Epoch: 0 [42624/50048]	Loss: 3.7994
Training Epoch: 0 [42752/50048]	Loss: 3.8095
Training Epoch: 0 [42880/50048]	Loss: 3.7944
Training Epoch: 0 [43008/50048]	Loss: 3.6086
Training Epoch: 0 [43136/50048]	Loss: 3.5458
Training Epoch: 0 [43264/50048]	Loss: 3.6888
Training Epoch: 0 [43392/50048]	Loss: 3.5979
Training Epoch: 0 [43520/50048]	Loss: 3.7507
Training Epoch: 0 [43648/50048]	Loss: 3.3786
Training Epoch: 0 [43776/50048]	Loss: 3.4928
Training Epoch: 0 [43904/50048]	Loss: 3.7072
Training Epoch: 0 [44032/50048]	Loss: 3.6569
Training Epoch: 0 [44160/50048]	Loss: 3.6488
Training Epoch: 0 [44288/50048]	Loss: 3.5999
Training Epoch: 0 [44416/50048]	Loss: 3.6715
Training Epoch: 0 [44544/50048]	Loss: 3.6818
Training Epoch: 0 [44672/50048]	Loss: 3.8439
Training Epoch: 0 [44800/50048]	Loss: 3.6105
Training Epoch: 0 [44928/50048]	Loss: 3.5842
Training Epoch: 0 [45056/50048]	Loss: 3.7542
Training Epoch: 0 [45184/50048]	Loss: 4.1439
Training Epoch: 0 [45312/50048]	Loss: 3.7930
Training Epoch: 0 [45440/50048]	Loss: 3.5943
Training Epoch: 0 [45568/50048]	Loss: 3.5374
Training Epoch: 0 [45696/50048]	Loss: 3.5429
Training Epoch: 0 [45824/50048]	Loss: 3.8252
Training Epoch: 0 [45952/50048]	Loss: 3.5781
Training Epoch: 0 [46080/50048]	Loss: 3.6505
Training Epoch: 0 [46208/50048]	Loss: 3.6614
Training Epoch: 0 [46336/50048]	Loss: 3.6203
Training Epoch: 0 [46464/50048]	Loss: 3.5269
Training Epoch: 0 [46592/50048]	Loss: 3.8121
Training Epoch: 0 [46720/50048]	Loss: 3.6974
2022-12-03 17:49:25,459 [ZeusDataLoader(train)] train epoch 1 done: time=86.57 energy=11200.15
2022-12-03 17:49:25,460 [ZeusDataLoader(eval)] Epoch 1 begin.
Training Epoch: 0 [46848/50048]	Loss: 3.7094
Training Epoch: 0 [46976/50048]	Loss: 3.8221
Training Epoch: 0 [47104/50048]	Loss: 3.6956
Training Epoch: 0 [47232/50048]	Loss: 3.5807
Training Epoch: 0 [47360/50048]	Loss: 3.3393
Training Epoch: 0 [47488/50048]	Loss: 3.6392
Training Epoch: 0 [47616/50048]	Loss: 3.6993
Training Epoch: 0 [47744/50048]	Loss: 3.8211
Training Epoch: 0 [47872/50048]	Loss: 3.6782
Training Epoch: 0 [48000/50048]	Loss: 3.6898
Training Epoch: 0 [48128/50048]	Loss: 3.9993
Training Epoch: 0 [48256/50048]	Loss: 3.6708
Training Epoch: 0 [48384/50048]	Loss: 3.6328
Training Epoch: 0 [48512/50048]	Loss: 3.7408
Training Epoch: 0 [48640/50048]	Loss: 3.6096
Training Epoch: 0 [48768/50048]	Loss: 3.7651
Training Epoch: 0 [48896/50048]	Loss: 3.5440
Training Epoch: 0 [49024/50048]	Loss: 3.6577
Training Epoch: 0 [49152/50048]	Loss: 3.6477
Training Epoch: 0 [49280/50048]	Loss: 3.5145
Training Epoch: 0 [49408/50048]	Loss: 3.4048
Training Epoch: 0 [49536/50048]	Loss: 3.6091
Training Epoch: 0 [49664/50048]	Loss: 3.5593
Training Epoch: 0 [49792/50048]	Loss: 3.6135
Training Epoch: 0 [49920/50048]	Loss: 3.6582
Training Epoch: 0 [50048/50048]	Loss: 3.6871
2022-12-03 22:49:29.208 [ZeusMonitor] Caught signal 2, end monitoring.
2022-12-03 17:49:29,267 [ZeusDataLoader(eval)] eval epoch 1 done: time=3.80 energy=493.00
2022-12-03 17:49:29,267 [ZeusDataLoader(train)] Up to epoch 1: time=90.37, energy=11693.15, cost=13754.04
2022-12-03 17:49:29,267 [ZeusDataLoader(train)] Optimal PL train & eval expected time=89.29 energy=11493.75
2022-12-03 17:49:29,267 [ZeusDataLoader(train)] Expected next epoch: time=179.66, energy=23186.90, cost=27313.68
2022-12-03 17:49:29,268 [ZeusDataLoader(train)] Epoch 2 begin.
Validation Epoch: 0, Average loss: 0.0324, Accuracy: 0.1047
2022-12-03 17:49:29,474 [ZeusDataLoader(train)] [GPU_0] Zeus monitor started.
2022-12-03 17:49:29,475 [ZeusDataLoader(train)] Steady state power limit: OPT 175W
2022-12-03 22:49:29.479 [ZeusMonitor] Monitor started.
2022-12-03 22:49:29.479 [ZeusMonitor] Running indefinitely. 2022-12-03 22:49:29.479 [ZeusMonitor] Sleeping 100ms after each poll. 
2022-12-03 22:49:29.479 [ZeusMonitor] Logfile path: /workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.8+me100+x100+eta0.5+beta2.0+2022120315171670098664/bs128+e2+gpu0.power.log
Training Epoch: 1 [128/50048]	Loss: 3.7105
Training Epoch: 1 [256/50048]	Loss: 3.6323
Training Epoch: 1 [384/50048]	Loss: 3.5601
Training Epoch: 1 [512/50048]	Loss: 3.8089
Training Epoch: 1 [640/50048]	Loss: 3.4242
Training Epoch: 1 [768/50048]	Loss: 3.5480
Training Epoch: 1 [896/50048]	Loss: 3.5555
Training Epoch: 1 [1024/50048]	Loss: 3.6048
Training Epoch: 1 [1152/50048]	Loss: 3.6034
Training Epoch: 1 [1280/50048]	Loss: 3.6176
Training Epoch: 1 [1408/50048]	Loss: 3.6887
Training Epoch: 1 [1536/50048]	Loss: 3.7464
Training Epoch: 1 [1664/50048]	Loss: 3.5385
Training Epoch: 1 [1792/50048]	Loss: 3.3619
Training Epoch: 1 [1920/50048]	Loss: 3.5350
Training Epoch: 1 [2048/50048]	Loss: 3.6067
Training Epoch: 1 [2176/50048]	Loss: 3.8483
Training Epoch: 1 [2304/50048]	Loss: 3.5434
Training Epoch: 1 [2432/50048]	Loss: 3.6844
Training Epoch: 1 [2560/50048]	Loss: 3.4355
Training Epoch: 1 [2688/50048]	Loss: 3.5836
Training Epoch: 1 [2816/50048]	Loss: 3.4718
Training Epoch: 1 [2944/50048]	Loss: 3.5947
Training Epoch: 1 [3072/50048]	Loss: 3.5640
Training Epoch: 1 [3200/50048]	Loss: 3.6620
Training Epoch: 1 [3328/50048]	Loss: 3.5075
Training Epoch: 1 [3456/50048]	Loss: 3.4173
Training Epoch: 1 [3584/50048]	Loss: 3.6307
Training Epoch: 1 [3712/50048]	Loss: 3.5453
Training Epoch: 1 [3840/50048]	Loss: 3.6176
Training Epoch: 1 [3968/50048]	Loss: 3.5162
Training Epoch: 1 [4096/50048]	Loss: 3.6388
Training Epoch: 1 [4224/50048]	Loss: 3.6748
Training Epoch: 1 [4352/50048]	Loss: 3.6129
Training Epoch: 1 [4480/50048]	Loss: 3.2157
Training Epoch: 1 [4608/50048]	Loss: 3.5223
Training Epoch: 1 [4736/50048]	Loss: 3.5943
Training Epoch: 1 [4864/50048]	Loss: 3.5666
Training Epoch: 1 [4992/50048]	Loss: 3.6972
Training Epoch: 1 [5120/50048]	Loss: 3.4730
Training Epoch: 1 [5248/50048]	Loss: 3.7109
Training Epoch: 1 [5376/50048]	Loss: 3.4480
Training Epoch: 1 [5504/50048]	Loss: 3.5126
Training Epoch: 1 [5632/50048]	Loss: 3.4459
Training Epoch: 1 [5760/50048]	Loss: 3.6086
Training Epoch: 1 [5888/50048]	Loss: 3.5078
Training Epoch: 1 [6016/50048]	Loss: 3.5751
Training Epoch: 1 [6144/50048]	Loss: 3.6332
Training Epoch: 1 [6272/50048]	Loss: 3.3768
Training Epoch: 1 [6400/50048]	Loss: 3.6486
Training Epoch: 1 [6528/50048]	Loss: 3.5425
Training Epoch: 1 [6656/50048]	Loss: 3.4204
Training Epoch: 1 [6784/50048]	Loss: 3.5345
Training Epoch: 1 [6912/50048]	Loss: 3.4073
Training Epoch: 1 [7040/50048]	Loss: 3.5948
Training Epoch: 1 [7168/50048]	Loss: 3.4551
Training Epoch: 1 [7296/50048]	Loss: 3.6386
Training Epoch: 1 [7424/50048]	Loss: 3.3905
Training Epoch: 1 [7552/50048]	Loss: 3.4843
Training Epoch: 1 [7680/50048]	Loss: 3.5069
Training Epoch: 1 [7808/50048]	Loss: 3.5334
Training Epoch: 1 [7936/50048]	Loss: 3.4603
Training Epoch: 1 [8064/50048]	Loss: 3.2943
Training Epoch: 1 [8192/50048]	Loss: 3.5165
Training Epoch: 1 [8320/50048]	Loss: 3.7194
Training Epoch: 1 [8448/50048]	Loss: 3.5047
Training Epoch: 1 [8576/50048]	Loss: 3.3655
Training Epoch: 1 [8704/50048]	Loss: 3.5661
Training Epoch: 1 [8832/50048]	Loss: 3.5920
Training Epoch: 1 [8960/50048]	Loss: 3.5010
Training Epoch: 1 [9088/50048]	Loss: 3.7226
Training Epoch: 1 [9216/50048]	Loss: 3.9551
Training Epoch: 1 [9344/50048]	Loss: 3.5120
Training Epoch: 1 [9472/50048]	Loss: 3.7317
Training Epoch: 1 [9600/50048]	Loss: 3.6750
Training Epoch: 1 [9728/50048]	Loss: 3.4104
Training Epoch: 1 [9856/50048]	Loss: 3.3965
Training Epoch: 1 [9984/50048]	Loss: 3.4577
Training Epoch: 1 [10112/50048]	Loss: 3.4472
Training Epoch: 1 [10240/50048]	Loss: 3.6624
Training Epoch: 1 [10368/50048]	Loss: 3.4971
Training Epoch: 1 [10496/50048]	Loss: 3.6153
Training Epoch: 1 [10624/50048]	Loss: 3.5703
Training Epoch: 1 [10752/50048]	Loss: 3.6052
Training Epoch: 1 [10880/50048]	Loss: 3.5571
Training Epoch: 1 [11008/50048]	Loss: 3.6464
Training Epoch: 1 [11136/50048]	Loss: 3.4285
Training Epoch: 1 [11264/50048]	Loss: 3.7802
Training Epoch: 1 [11392/50048]	Loss: 3.5506
Training Epoch: 1 [11520/50048]	Loss: 3.5727
Training Epoch: 1 [11648/50048]	Loss: 3.5113
Training Epoch: 1 [11776/50048]	Loss: 3.6291
Training Epoch: 1 [11904/50048]	Loss: 3.4310
Training Epoch: 1 [12032/50048]	Loss: 3.1219
Training Epoch: 1 [12160/50048]	Loss: 3.4995
Training Epoch: 1 [12288/50048]	Loss: 3.5863
Training Epoch: 1 [12416/50048]	Loss: 3.4689
Training Epoch: 1 [12544/50048]	Loss: 3.4525
Training Epoch: 1 [12672/50048]	Loss: 3.4644
Training Epoch: 1 [12800/50048]	Loss: 3.6663
Training Epoch: 1 [12928/50048]	Loss: 3.3769
Training Epoch: 1 [13056/50048]	Loss: 3.3287
Training Epoch: 1 [13184/50048]	Loss: 3.6682
Training Epoch: 1 [13312/50048]	Loss: 3.2795
Training Epoch: 1 [13440/50048]	Loss: 3.6031
Training Epoch: 1 [13568/50048]	Loss: 3.6328
Training Epoch: 1 [13696/50048]	Loss: 3.5622
Training Epoch: 1 [13824/50048]	Loss: 3.6450
Training Epoch: 1 [13952/50048]	Loss: 3.4893
Training Epoch: 1 [14080/50048]	Loss: 3.3561
Training Epoch: 1 [14208/50048]	Loss: 3.6459
Training Epoch: 1 [14336/50048]	Loss: 3.4473
Training Epoch: 1 [14464/50048]	Loss: 3.3934
Training Epoch: 1 [14592/50048]	Loss: 3.7077
Training Epoch: 1 [14720/50048]	Loss: 3.4732
Training Epoch: 1 [14848/50048]	Loss: 3.6537
Training Epoch: 1 [14976/50048]	Loss: 3.5401
Training Epoch: 1 [15104/50048]	Loss: 3.4247
Training Epoch: 1 [15232/50048]	Loss: 3.4492
Training Epoch: 1 [15360/50048]	Loss: 3.4120
Training Epoch: 1 [15488/50048]	Loss: 3.5306
Training Epoch: 1 [15616/50048]	Loss: 3.5442
Training Epoch: 1 [15744/50048]	Loss: 3.4911
Training Epoch: 1 [15872/50048]	Loss: 3.2891
Training Epoch: 1 [16000/50048]	Loss: 3.7222
Training Epoch: 1 [16128/50048]	Loss: 3.2866
Training Epoch: 1 [16256/50048]	Loss: 3.4748
Training Epoch: 1 [16384/50048]	Loss: 3.4894
Training Epoch: 1 [16512/50048]	Loss: 3.3595
Training Epoch: 1 [16640/50048]	Loss: 3.3774
Training Epoch: 1 [16768/50048]	Loss: 3.5188
Training Epoch: 1 [16896/50048]	Loss: 3.3441
Training Epoch: 1 [17024/50048]	Loss: 3.6831
Training Epoch: 1 [17152/50048]	Loss: 3.4418
Training Epoch: 1 [17280/50048]	Loss: 3.4740
Training Epoch: 1 [17408/50048]	Loss: 3.4264
Training Epoch: 1 [17536/50048]	Loss: 3.4366
Training Epoch: 1 [17664/50048]	Loss: 3.2678
Training Epoch: 1 [17792/50048]	Loss: 3.3047
Training Epoch: 1 [17920/50048]	Loss: 3.3705
Training Epoch: 1 [18048/50048]	Loss: 3.4806
Training Epoch: 1 [18176/50048]	Loss: 3.3528
Training Epoch: 1 [18304/50048]	Loss: 3.1867
Training Epoch: 1 [18432/50048]	Loss: 3.3769
Training Epoch: 1 [18560/50048]	Loss: 3.5010
Training Epoch: 1 [18688/50048]	Loss: 3.4868
Training Epoch: 1 [18816/50048]	Loss: 3.4427
Training Epoch: 1 [18944/50048]	Loss: 3.3878
Training Epoch: 1 [19072/50048]	Loss: 3.3421
Training Epoch: 1 [19200/50048]	Loss: 3.5104
Training Epoch: 1 [19328/50048]	Loss: 3.6107
Training Epoch: 1 [19456/50048]	Loss: 3.5412
Training Epoch: 1 [19584/50048]	Loss: 3.3751
Training Epoch: 1 [19712/50048]	Loss: 3.2039
Training Epoch: 1 [19840/50048]	Loss: 3.1747
Training Epoch: 1 [19968/50048]	Loss: 3.3542
Training Epoch: 1 [20096/50048]	Loss: 3.5047
Training Epoch: 1 [20224/50048]	Loss: 3.5916
Training Epoch: 1 [20352/50048]	Loss: 3.4212
Training Epoch: 1 [20480/50048]	Loss: 3.4150
Training Epoch: 1 [20608/50048]	Loss: 3.5558
Training Epoch: 1 [20736/50048]	Loss: 3.3605
Training Epoch: 1 [20864/50048]	Loss: 3.5432
Training Epoch: 1 [20992/50048]	Loss: 3.5523
Training Epoch: 1 [21120/50048]	Loss: 3.2684
Training Epoch: 1 [21248/50048]	Loss: 3.2987
Training Epoch: 1 [21376/50048]	Loss: 3.4248
Training Epoch: 1 [21504/50048]	Loss: 3.4363
Training Epoch: 1 [21632/50048]	Loss: 3.2383
Training Epoch: 1 [21760/50048]	Loss: 3.4321
Training Epoch: 1 [21888/50048]	Loss: 3.4235
Training Epoch: 1 [22016/50048]	Loss: 3.5240
Training Epoch: 1 [22144/50048]	Loss: 3.4264
Training Epoch: 1 [22272/50048]	Loss: 3.2344
Training Epoch: 1 [22400/50048]	Loss: 3.5261
Training Epoch: 1 [22528/50048]	Loss: 3.3615
Training Epoch: 1 [22656/50048]	Loss: 3.2080
Training Epoch: 1 [22784/50048]	Loss: 3.3912
Training Epoch: 1 [22912/50048]	Loss: 3.4687
Training Epoch: 1 [23040/50048]	Loss: 3.2618
Training Epoch: 1 [23168/50048]	Loss: 3.7107
Training Epoch: 1 [23296/50048]	Loss: 3.3271
Training Epoch: 1 [23424/50048]	Loss: 3.1927
Training Epoch: 1 [23552/50048]	Loss: 3.3560
Training Epoch: 1 [23680/50048]	Loss: 3.4742
Training Epoch: 1 [23808/50048]	Loss: 3.4175
Training Epoch: 1 [23936/50048]	Loss: 3.2550
Training Epoch: 1 [24064/50048]	Loss: 3.4749
Training Epoch: 1 [24192/50048]	Loss: 3.1071
Training Epoch: 1 [24320/50048]	Loss: 3.1456
Training Epoch: 1 [24448/50048]	Loss: 3.3721
Training Epoch: 1 [24576/50048]	Loss: 3.3355
Training Epoch: 1 [24704/50048]	Loss: 3.4389
Training Epoch: 1 [24832/50048]	Loss: 3.1510
Training Epoch: 1 [24960/50048]	Loss: 3.3678
Training Epoch: 1 [25088/50048]	Loss: 3.3320
Training Epoch: 1 [25216/50048]	Loss: 3.4438
Training Epoch: 1 [25344/50048]	Loss: 3.3680
Training Epoch: 1 [25472/50048]	Loss: 3.4049
Training Epoch: 1 [25600/50048]	Loss: 3.4219
Training Epoch: 1 [25728/50048]	Loss: 3.5157
Training Epoch: 1 [25856/50048]	Loss: 3.4745
Training Epoch: 1 [25984/50048]	Loss: 3.3580
Training Epoch: 1 [26112/50048]	Loss: 3.2921
Training Epoch: 1 [26240/50048]	Loss: 3.4444
Training Epoch: 1 [26368/50048]	Loss: 3.5661
Training Epoch: 1 [26496/50048]	Loss: 3.2352
Training Epoch: 1 [26624/50048]	Loss: 3.3620
Training Epoch: 1 [26752/50048]	Loss: 3.2217
Training Epoch: 1 [26880/50048]	Loss: 3.2569
Training Epoch: 1 [27008/50048]	Loss: 3.4180
Training Epoch: 1 [27136/50048]	Loss: 3.2323
Training Epoch: 1 [27264/50048]	Loss: 3.2835
Training Epoch: 1 [27392/50048]	Loss: 3.3421
Training Epoch: 1 [27520/50048]	Loss: 3.4264
Training Epoch: 1 [27648/50048]	Loss: 3.2193
Training Epoch: 1 [27776/50048]	Loss: 3.2574
Training Epoch: 1 [27904/50048]	Loss: 3.3945
Training Epoch: 1 [28032/50048]	Loss: 3.2576
Training Epoch: 1 [28160/50048]	Loss: 3.1664
Training Epoch: 1 [28288/50048]	Loss: 3.4393
Training Epoch: 1 [28416/50048]	Loss: 3.3195
Training Epoch: 1 [28544/50048]	Loss: 3.4387
Training Epoch: 1 [28672/50048]	Loss: 3.1473
Training Epoch: 1 [28800/50048]	Loss: 3.6438
Traceback (most recent call last):
  File "/workspace/zeus/examples/cifar100/train_lr.py", line 259, in <module>
    main(parse_args())
  File "/workspace/zeus/examples/cifar100/train_lr.py", line 191, in main
    train(train_loader, model, criterion, optimizer, epoch, args)
  File "/workspace/zeus/examples/cifar100/train_lr.py", line 215, in train
    optimizer.step()
  File "/root/.local/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/root/.local/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/root/.local/miniconda3/lib/python3.9/site-packages/torch/optim/adadelta.py", line 105, in step
    F.adadelta(params_with_grad,
  File "/root/.local/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py", line 202, in adadelta
    std = square_avg.add(eps).sqrt_()
KeyboardInterrupt
2022-12-03 17:50:18,985 [ZeusDataLoader(train)] [GPU_0] Stopped Zeus monitor.
