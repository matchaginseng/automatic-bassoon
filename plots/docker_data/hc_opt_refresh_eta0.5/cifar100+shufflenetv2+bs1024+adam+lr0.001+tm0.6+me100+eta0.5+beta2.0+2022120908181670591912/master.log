Files already downloaded and verified
Files already downloaded and verified
Job(cifar100,shufflenetv2,adam,0.6,bs1024~100)
[Training Loop] Testing batch sizes: [128, 256, 512, 1024]
[Training Loop] Testing power limits: [175, 150, 125, 100]
[Training Loop] Testing learning rates: [0.001, 0.005, 0.01]
[Training Loop] Testing dropout rates: [0.0, 0.25, 0.5]
[Training Loop] Reprofiling at accuracy thresholds [0.5, 0.4, 0.3]
[Training Loop] Model's accuracy 0.0 surpasses threshold 0.0! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
Launching Zeus monitor 0...
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6139
Profiling... [256/50048]	Loss: 4.6801
Profiling... [384/50048]	Loss: 4.8137
Profiling... [512/50048]	Loss: 4.6905
Profiling... [640/50048]	Loss: 4.7436
Profiling... [768/50048]	Loss: 4.7958
Profiling... [896/50048]	Loss: 4.7038
Profiling... [1024/50048]	Loss: 4.7313
Profiling... [1152/50048]	Loss: 4.6415
Profiling... [1280/50048]	Loss: 4.6719
Profiling... [1408/50048]	Loss: 4.6626
Profiling... [1536/50048]	Loss: 4.5990
Profiling... [1664/50048]	Loss: 4.5838
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 126.29960076727478,
                        "time": 2.147053507000237,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19010839.886296526
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6375
Profiling... [256/50048]	Loss: 4.7062
Profiling... [384/50048]	Loss: 4.6800
Profiling... [512/50048]	Loss: 4.7550
Profiling... [640/50048]	Loss: 4.7059
Profiling... [768/50048]	Loss: 4.6791
Profiling... [896/50048]	Loss: 4.6702
Profiling... [1024/50048]	Loss: 4.8617
Profiling... [1152/50048]	Loss: 4.7164
Profiling... [1280/50048]	Loss: 4.7363
Profiling... [1408/50048]	Loss: 4.5882
Profiling... [1536/50048]	Loss: 4.6955
Profiling... [1664/50048]	Loss: 4.5992
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.84479558179913,
                        "time": 2.155597918001149,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19085791.906893566
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6515
Profiling... [256/50048]	Loss: 4.5982
Profiling... [384/50048]	Loss: 4.8092
Profiling... [512/50048]	Loss: 4.7268
Profiling... [640/50048]	Loss: 4.6489
Profiling... [768/50048]	Loss: 4.6896
Profiling... [896/50048]	Loss: 4.7947
Profiling... [1024/50048]	Loss: 4.6448
Profiling... [1152/50048]	Loss: 4.7282
Profiling... [1280/50048]	Loss: 4.6894
Profiling... [1408/50048]	Loss: 4.6524
Profiling... [1536/50048]	Loss: 4.7579
Profiling... [1664/50048]	Loss: 4.6004
Profile done
epoch 1 train time consumed: 3.08s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.31156715631083,
                        "time": 2.18450834900068,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19341707.68447145
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6387
Profiling... [256/50048]	Loss: 4.7089
Profiling... [384/50048]	Loss: 4.6402
Profiling... [512/50048]	Loss: 4.6873
Profiling... [640/50048]	Loss: 4.6898
Profiling... [768/50048]	Loss: 4.7073
Profiling... [896/50048]	Loss: 4.7230
Profiling... [1024/50048]	Loss: 4.7117
Profiling... [1152/50048]	Loss: 4.6896
Profiling... [1280/50048]	Loss: 4.7276
Profiling... [1408/50048]	Loss: 4.7896
Profiling... [1536/50048]	Loss: 4.6406
Profiling... [1664/50048]	Loss: 4.5211
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.75490141440173,
                        "time": 2.5428054769981827,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 22513624.801975705
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6610
Profiling... [256/50048]	Loss: 4.6551
Profiling... [384/50048]	Loss: 4.6888
Profiling... [512/50048]	Loss: 4.7558
Profiling... [640/50048]	Loss: 4.6682
Profiling... [768/50048]	Loss: 4.7847
Profiling... [896/50048]	Loss: 4.7114
Profiling... [1024/50048]	Loss: 4.8056
Profiling... [1152/50048]	Loss: 4.6947
Profiling... [1280/50048]	Loss: 4.6291
Profiling... [1408/50048]	Loss: 4.6564
Profiling... [1536/50048]	Loss: 4.6999
Profiling... [1664/50048]	Loss: 4.6889
Profile done
epoch 1 train time consumed: 3.07s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.66353020040538,
                        "time": 2.1503502539999317,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19038548.018918984
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6631
Profiling... [256/50048]	Loss: 4.6605
Profiling... [384/50048]	Loss: 4.6571
Profiling... [512/50048]	Loss: 4.6801
Profiling... [640/50048]	Loss: 4.7772
Profiling... [768/50048]	Loss: 4.7148
Profiling... [896/50048]	Loss: 4.6383
Profiling... [1024/50048]	Loss: 4.7133
Profiling... [1152/50048]	Loss: 4.6391
Profiling... [1280/50048]	Loss: 4.5279
Profiling... [1408/50048]	Loss: 4.6720
Profiling... [1536/50048]	Loss: 4.5784
Profiling... [1664/50048]	Loss: 4.5911
Profile done
epoch 1 train time consumed: 3.10s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.02296178999899,
                        "time": 2.154373248999036,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19074314.470966518
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6134
Profiling... [256/50048]	Loss: 4.6957
Profiling... [384/50048]	Loss: 4.6823
Profiling... [512/50048]	Loss: 4.7257
Profiling... [640/50048]	Loss: 4.6966
Profiling... [768/50048]	Loss: 4.6148
Profiling... [896/50048]	Loss: 4.7246
Profiling... [1024/50048]	Loss: 4.6544
Profiling... [1152/50048]	Loss: 4.6741
Profiling... [1280/50048]	Loss: 4.6001
Profiling... [1408/50048]	Loss: 4.6488
Profiling... [1536/50048]	Loss: 4.6599
Profiling... [1664/50048]	Loss: 4.6555
Profile done
epoch 1 train time consumed: 3.11s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.76282002706209,
                        "time": 2.194182011997327,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19426853.98180842
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6389
Profiling... [256/50048]	Loss: 4.5879
Profiling... [384/50048]	Loss: 4.6891
Profiling... [512/50048]	Loss: 4.7250
Profiling... [640/50048]	Loss: 4.7412
Profiling... [768/50048]	Loss: 4.8422
Profiling... [896/50048]	Loss: 4.7195
Profiling... [1024/50048]	Loss: 4.8006
Profiling... [1152/50048]	Loss: 4.7488
Profiling... [1280/50048]	Loss: 4.6731
Profiling... [1408/50048]	Loss: 4.6623
Profiling... [1536/50048]	Loss: 4.6491
Profiling... [1664/50048]	Loss: 4.5786
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.79397164095194,
                        "time": 2.5492321380006615,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 22570272.76837053
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6880
Profiling... [256/50048]	Loss: 4.6032
Profiling... [384/50048]	Loss: 4.7217
Profiling... [512/50048]	Loss: 4.8115
Profiling... [640/50048]	Loss: 4.7172
Profiling... [768/50048]	Loss: 4.7008
Profiling... [896/50048]	Loss: 4.6232
Profiling... [1024/50048]	Loss: 4.7171
Profiling... [1152/50048]	Loss: 4.6980
Profiling... [1280/50048]	Loss: 4.6228
Profiling... [1408/50048]	Loss: 4.6021
Profiling... [1536/50048]	Loss: 4.6592
Profiling... [1664/50048]	Loss: 4.6428
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.36835662614448,
                        "time": 2.1514685209986055,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19048416.706807476
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6438
Profiling... [256/50048]	Loss: 4.6254
Profiling... [384/50048]	Loss: 4.7489
Profiling... [512/50048]	Loss: 4.7676
Profiling... [640/50048]	Loss: 4.7234
Profiling... [768/50048]	Loss: 4.7585
Profiling... [896/50048]	Loss: 4.6717
Profiling... [1024/50048]	Loss: 4.6138
Profiling... [1152/50048]	Loss: 4.6065
Profiling... [1280/50048]	Loss: 4.7177
Profiling... [1408/50048]	Loss: 4.6102
Profiling... [1536/50048]	Loss: 4.6011
Profiling... [1664/50048]	Loss: 4.5950
Profile done
epoch 1 train time consumed: 3.09s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.16302847445843,
                        "time": 2.157509389999177,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19101987.321705494
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6534
Profiling... [256/50048]	Loss: 4.6404
Profiling... [384/50048]	Loss: 4.7136
Profiling... [512/50048]	Loss: 4.7382
Profiling... [640/50048]	Loss: 4.8904
Profiling... [768/50048]	Loss: 4.7646
Profiling... [896/50048]	Loss: 4.7720
Profiling... [1024/50048]	Loss: 4.7100
Profiling... [1152/50048]	Loss: 4.7098
Profiling... [1280/50048]	Loss: 4.5762
Profiling... [1408/50048]	Loss: 4.5855
Profiling... [1536/50048]	Loss: 4.5829
Profiling... [1664/50048]	Loss: 4.6411
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.7470648044059,
                        "time": 2.1925773870025296,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19412534.345880374
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6508
Profiling... [256/50048]	Loss: 4.7009
Profiling... [384/50048]	Loss: 4.8186
Profiling... [512/50048]	Loss: 4.6297
Profiling... [640/50048]	Loss: 4.7007
Profiling... [768/50048]	Loss: 4.6615
Profiling... [896/50048]	Loss: 4.6436
Profiling... [1024/50048]	Loss: 4.6556
Profiling... [1152/50048]	Loss: 4.5556
Profiling... [1280/50048]	Loss: 4.6336
Profiling... [1408/50048]	Loss: 4.6058
Profiling... [1536/50048]	Loss: 4.7115
Profiling... [1664/50048]	Loss: 4.6372
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.20515864348138,
                        "time": 2.530928001997381,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 22408137.114871245
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6404
Profiling... [256/50048]	Loss: 5.5796
Profiling... [384/50048]	Loss: 5.4884
Profiling... [512/50048]	Loss: 5.3806
Profiling... [640/50048]	Loss: 5.0365
Profiling... [768/50048]	Loss: 5.1290
Profiling... [896/50048]	Loss: 4.7816
Profiling... [1024/50048]	Loss: 5.2156
Profiling... [1152/50048]	Loss: 4.6766
Profiling... [1280/50048]	Loss: 5.0255
Profiling... [1408/50048]	Loss: 5.0302
Profiling... [1536/50048]	Loss: 4.9159
Profiling... [1664/50048]	Loss: 4.8385
Profile done
epoch 1 train time consumed: 3.11s
Validation Epoch: 0, Average loss: 0.0368, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.34063425197289,
                        "time": 2.1559367749978264,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19087974.182026926
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6421
Profiling... [256/50048]	Loss: 5.2478
Profiling... [384/50048]	Loss: 5.7187
Profiling... [512/50048]	Loss: 5.9206
Profiling... [640/50048]	Loss: 5.3450
Profiling... [768/50048]	Loss: 5.5624
Profiling... [896/50048]	Loss: 5.1205
Profiling... [1024/50048]	Loss: 4.6648
Profiling... [1152/50048]	Loss: 4.8382
Profiling... [1280/50048]	Loss: 5.0864
Profiling... [1408/50048]	Loss: 4.8421
Profiling... [1536/50048]	Loss: 4.6686
Profiling... [1664/50048]	Loss: 4.6152
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.91427878621738,
                        "time": 2.154207370000222,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19072725.063171003
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6763
Profiling... [256/50048]	Loss: 6.0552
Profiling... [384/50048]	Loss: 5.6294
Profiling... [512/50048]	Loss: 5.4368
Profiling... [640/50048]	Loss: 5.4031
Profiling... [768/50048]	Loss: 5.0185
Profiling... [896/50048]	Loss: 5.0469
Profiling... [1024/50048]	Loss: 5.0009
Profiling... [1152/50048]	Loss: 4.7573
Profiling... [1280/50048]	Loss: 4.9542
Profiling... [1408/50048]	Loss: 4.9535
Profiling... [1536/50048]	Loss: 4.7110
Profiling... [1664/50048]	Loss: 4.7437
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0378, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.32135707749153,
                        "time": 2.1932803050003713,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19418710.599154327
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.7143
Profiling... [256/50048]	Loss: 5.6223
Profiling... [384/50048]	Loss: 5.5172
Profiling... [512/50048]	Loss: 5.8789
Profiling... [640/50048]	Loss: 5.3915
Profiling... [768/50048]	Loss: 5.1908
Profiling... [896/50048]	Loss: 5.2563
Profiling... [1024/50048]	Loss: 5.0356
Profiling... [1152/50048]	Loss: 4.8396
Profiling... [1280/50048]	Loss: 4.9464
Profiling... [1408/50048]	Loss: 4.8478
Profiling... [1536/50048]	Loss: 4.7975
Profiling... [1664/50048]	Loss: 4.8711
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 0, Average loss: 0.0376, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.92774936315435,
                        "time": 2.5596152559992333,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 22662090.23347645
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6738
Profiling... [256/50048]	Loss: 5.5646
Profiling... [384/50048]	Loss: 5.8737
Profiling... [512/50048]	Loss: 5.4444
Profiling... [640/50048]	Loss: 5.6083
Profiling... [768/50048]	Loss: 5.3315
Profiling... [896/50048]	Loss: 4.8185
Profiling... [1024/50048]	Loss: 5.0166
Profiling... [1152/50048]	Loss: 4.7601
Profiling... [1280/50048]	Loss: 4.7124
Profiling... [1408/50048]	Loss: 4.6018
Profiling... [1536/50048]	Loss: 4.6769
Profiling... [1664/50048]	Loss: 4.6765
Profile done
epoch 1 train time consumed: 3.18s
Validation Epoch: 0, Average loss: 0.0369, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.27918101487735,
                        "time": 2.149518722999346,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19031144.125060942
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6162
Profiling... [256/50048]	Loss: 5.6867
Profiling... [384/50048]	Loss: 5.8254
Profiling... [512/50048]	Loss: 5.6341
Profiling... [640/50048]	Loss: 4.9367
Profiling... [768/50048]	Loss: 4.9374
Profiling... [896/50048]	Loss: 4.9760
Profiling... [1024/50048]	Loss: 4.6847
Profiling... [1152/50048]	Loss: 4.8236
Profiling... [1280/50048]	Loss: 4.8070
Profiling... [1408/50048]	Loss: 4.6332
Profiling... [1536/50048]	Loss: 4.8083
Profiling... [1664/50048]	Loss: 4.7676
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0373, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.69300438871626,
                        "time": 2.1576993729977403,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19103618.10179949
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6391
Profiling... [256/50048]	Loss: 5.4036
Profiling... [384/50048]	Loss: 5.8762
Profiling... [512/50048]	Loss: 6.2022
Profiling... [640/50048]	Loss: 5.4526
Profiling... [768/50048]	Loss: 5.1926
Profiling... [896/50048]	Loss: 5.0013
Profiling... [1024/50048]	Loss: 4.7611
Profiling... [1152/50048]	Loss: 4.8718
Profiling... [1280/50048]	Loss: 4.8384
Profiling... [1408/50048]	Loss: 4.6987
Profiling... [1536/50048]	Loss: 4.7146
Profiling... [1664/50048]	Loss: 4.7318
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.02875262334636,
                        "time": 2.204710062000231,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19519873.95954004
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6270
Profiling... [256/50048]	Loss: 5.9047
Profiling... [384/50048]	Loss: 5.4051
Profiling... [512/50048]	Loss: 5.3996
Profiling... [640/50048]	Loss: 5.2093
Profiling... [768/50048]	Loss: 4.8897
Profiling... [896/50048]	Loss: 4.8513
Profiling... [1024/50048]	Loss: 4.7609
Profiling... [1152/50048]	Loss: 4.6767
Profiling... [1280/50048]	Loss: 4.7305
Profiling... [1408/50048]	Loss: 4.7019
Profiling... [1536/50048]	Loss: 4.8565
Profiling... [1664/50048]	Loss: 4.7565
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0372, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.73306472564123,
                        "time": 2.556667232998734,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 22635964.127928067
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6552
Profiling... [256/50048]	Loss: 5.8046
Profiling... [384/50048]	Loss: 5.9209
Profiling... [512/50048]	Loss: 5.2202
Profiling... [640/50048]	Loss: 4.8270
Profiling... [768/50048]	Loss: 4.8967
Profiling... [896/50048]	Loss: 5.2520
Profiling... [1024/50048]	Loss: 4.7450
Profiling... [1152/50048]	Loss: 4.7409
Profiling... [1280/50048]	Loss: 4.5856
Profiling... [1408/50048]	Loss: 5.2293
Profiling... [1536/50048]	Loss: 4.6527
Profiling... [1664/50048]	Loss: 4.6389
Profile done
epoch 1 train time consumed: 3.11s
Validation Epoch: 0, Average loss: 0.0372, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.20433995348938,
                        "time": 2.1539852710011473,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19070681.3470358
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6557
Profiling... [256/50048]	Loss: 5.8554
Profiling... [384/50048]	Loss: 6.2162
Profiling... [512/50048]	Loss: 5.7945
Profiling... [640/50048]	Loss: 5.7163
Profiling... [768/50048]	Loss: 5.0352
Profiling... [896/50048]	Loss: 4.8507
Profiling... [1024/50048]	Loss: 4.9849
Profiling... [1152/50048]	Loss: 4.8920
Profiling... [1280/50048]	Loss: 4.7501
Profiling... [1408/50048]	Loss: 4.6455
Profiling... [1536/50048]	Loss: 4.7300
Profiling... [1664/50048]	Loss: 5.0838
Profile done
epoch 1 train time consumed: 3.06s
Validation Epoch: 0, Average loss: 0.0368, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.60347374072107,
                        "time": 2.1502795909982524,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19037915.860938363
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6418
Profiling... [256/50048]	Loss: 5.8405
Profiling... [384/50048]	Loss: 6.1259
Profiling... [512/50048]	Loss: 5.3386
Profiling... [640/50048]	Loss: 5.4868
Profiling... [768/50048]	Loss: 5.1377
Profiling... [896/50048]	Loss: 5.0519
Profiling... [1024/50048]	Loss: 5.1276
Profiling... [1152/50048]	Loss: 4.8792
Profiling... [1280/50048]	Loss: 4.9283
Profiling... [1408/50048]	Loss: 5.0692
Profiling... [1536/50048]	Loss: 4.7326
Profiling... [1664/50048]	Loss: 4.8719
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.0376, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.91734187072292,
                        "time": 2.1955181129997072,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19438478.69804769
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6499
Profiling... [256/50048]	Loss: 5.9505
Profiling... [384/50048]	Loss: 5.6329
Profiling... [512/50048]	Loss: 5.4227
Profiling... [640/50048]	Loss: 5.2450
Profiling... [768/50048]	Loss: 4.9457
Profiling... [896/50048]	Loss: 4.9341
Profiling... [1024/50048]	Loss: 4.5536
Profiling... [1152/50048]	Loss: 5.0462
Profiling... [1280/50048]	Loss: 4.6101
Profiling... [1408/50048]	Loss: 4.5983
Profiling... [1536/50048]	Loss: 4.6010
Profiling... [1664/50048]	Loss: 4.8863
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0371, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.69553933775852,
                        "time": 2.5260852850005904,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 22365195.948849957
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6711
Profiling... [256/50048]	Loss: 8.0192
Profiling... [384/50048]	Loss: 7.7753
Profiling... [512/50048]	Loss: 6.4108
Profiling... [640/50048]	Loss: 4.8488
Profiling... [768/50048]	Loss: 5.0652
Profiling... [896/50048]	Loss: 4.8324
Profiling... [1024/50048]	Loss: 5.0545
Profiling... [1152/50048]	Loss: 4.9171
Profiling... [1280/50048]	Loss: 4.8353
Profiling... [1408/50048]	Loss: 4.7109
Profiling... [1536/50048]	Loss: 5.5573
Profiling... [1664/50048]	Loss: 4.5467
Profile done
epoch 1 train time consumed: 3.11s
Validation Epoch: 0, Average loss: 0.0866, Accuracy: 0.0138
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.26576051000707,
                        "time": 2.1587198529996385,
                        "accuracy": 0.013844936708860759,
                        "total_cost": 13651861.779874735
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6357
Profiling... [256/50048]	Loss: 7.9782
Profiling... [384/50048]	Loss: 7.3307
Profiling... [512/50048]	Loss: 5.4373
Profiling... [640/50048]	Loss: 5.6101
Profiling... [768/50048]	Loss: 4.7479
Profiling... [896/50048]	Loss: 4.9996
Profiling... [1024/50048]	Loss: 4.8110
Profiling... [1152/50048]	Loss: 4.7632
Profiling... [1280/50048]	Loss: 4.8102
Profiling... [1408/50048]	Loss: 4.6301
Profiling... [1536/50048]	Loss: 4.9052
Profiling... [1664/50048]	Loss: 4.8726
Profile done
epoch 1 train time consumed: 3.10s
Validation Epoch: 0, Average loss: 0.0367, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.59126667378679,
                        "time": 2.156396439000673,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19092071.2257915
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6314
Profiling... [256/50048]	Loss: 7.9821
Profiling... [384/50048]	Loss: 6.2788
Profiling... [512/50048]	Loss: 5.5470
Profiling... [640/50048]	Loss: 4.8997
Profiling... [768/50048]	Loss: 6.0677
Profiling... [896/50048]	Loss: 5.5034
Profiling... [1024/50048]	Loss: 5.0388
Profiling... [1152/50048]	Loss: 5.1067
Profiling... [1280/50048]	Loss: 4.9817
Profiling... [1408/50048]	Loss: 4.6876
Profiling... [1536/50048]	Loss: 4.7332
Profiling... [1664/50048]	Loss: 5.2710
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.8571674003448,
                        "time": 2.1924527999981365,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19411332.63806394
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6489
Profiling... [256/50048]	Loss: 8.2319
Profiling... [384/50048]	Loss: 7.2678
Profiling... [512/50048]	Loss: 5.9178
Profiling... [640/50048]	Loss: 4.8685
Profiling... [768/50048]	Loss: 6.0009
Profiling... [896/50048]	Loss: 4.8000
Profiling... [1024/50048]	Loss: 4.7030
Profiling... [1152/50048]	Loss: 4.7632
Profiling... [1280/50048]	Loss: 4.9170
Profiling... [1408/50048]	Loss: 5.1997
Profiling... [1536/50048]	Loss: 5.0045
Profiling... [1664/50048]	Loss: 4.7816
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0384, Accuracy: 0.0102
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.65084667626246,
                        "time": 2.5503631710016634,
                        "accuracy": 0.010185917721518988,
                        "total_cost": 21922465.29501892
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6575
Profiling... [256/50048]	Loss: 7.9290
Profiling... [384/50048]	Loss: 7.0742
Profiling... [512/50048]	Loss: 6.3191
Profiling... [640/50048]	Loss: 4.9676
Profiling... [768/50048]	Loss: 5.4900
Profiling... [896/50048]	Loss: 4.9386
Profiling... [1024/50048]	Loss: 4.7626
Profiling... [1152/50048]	Loss: 4.8121
Profiling... [1280/50048]	Loss: 4.8077
Profiling... [1408/50048]	Loss: 4.7898
Profiling... [1536/50048]	Loss: 4.5661
Profiling... [1664/50048]	Loss: 4.6645
Profile done
epoch 1 train time consumed: 3.11s
Validation Epoch: 0, Average loss: 0.1503, Accuracy: 0.0107
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.28965675168439,
                        "time": 2.1461572840016743,
                        "accuracy": 0.010680379746835443,
                        "total_cost": 17593874.228882276
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6275
Profiling... [256/50048]	Loss: 7.6859
Profiling... [384/50048]	Loss: 7.9311
Profiling... [512/50048]	Loss: 5.6538
Profiling... [640/50048]	Loss: 4.8368
Profiling... [768/50048]	Loss: 5.7070
Profiling... [896/50048]	Loss: 4.8152
Profiling... [1024/50048]	Loss: 4.7558
Profiling... [1152/50048]	Loss: 4.7727
Profiling... [1280/50048]	Loss: 4.5893
Profiling... [1408/50048]	Loss: 4.6734
Profiling... [1536/50048]	Loss: 4.5866
Profiling... [1664/50048]	Loss: 4.7171
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.54324345948702,
                        "time": 2.1520463859997108,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19053551.96838022
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6969
Profiling... [256/50048]	Loss: 7.6060
Profiling... [384/50048]	Loss: 8.2634
Profiling... [512/50048]	Loss: 6.8917
Profiling... [640/50048]	Loss: 5.1045
Profiling... [768/50048]	Loss: 5.1377
Profiling... [896/50048]	Loss: 4.7271
Profiling... [1024/50048]	Loss: 4.8781
Profiling... [1152/50048]	Loss: 4.9266
Profiling... [1280/50048]	Loss: 4.7857
Profiling... [1408/50048]	Loss: 4.6505
Profiling... [1536/50048]	Loss: 4.8309
Profiling... [1664/50048]	Loss: 4.9830
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.7360972292667,
                        "time": 2.195345506999729,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19436930.377183385
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6320
Profiling... [256/50048]	Loss: 7.8672
Profiling... [384/50048]	Loss: 8.0224
Profiling... [512/50048]	Loss: 6.1837
Profiling... [640/50048]	Loss: 4.9914
Profiling... [768/50048]	Loss: 5.0116
Profiling... [896/50048]	Loss: 4.7854
Profiling... [1024/50048]	Loss: 5.4016
Profiling... [1152/50048]	Loss: 4.9262
Profiling... [1280/50048]	Loss: 5.4528
Profiling... [1408/50048]	Loss: 4.8717
Profiling... [1536/50048]	Loss: 4.8911
Profiling... [1664/50048]	Loss: 4.8888
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.57649403224609,
                        "time": 2.5314852369992877,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 22412990.255613785
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6418
Profiling... [256/50048]	Loss: 7.7845
Profiling... [384/50048]	Loss: 6.8270
Profiling... [512/50048]	Loss: 6.1827
Profiling... [640/50048]	Loss: 4.9892
Profiling... [768/50048]	Loss: 4.7293
Profiling... [896/50048]	Loss: 4.9855
Profiling... [1024/50048]	Loss: 4.7943
Profiling... [1152/50048]	Loss: 5.0896
Profiling... [1280/50048]	Loss: 4.9813
Profiling... [1408/50048]	Loss: 4.8958
Profiling... [1536/50048]	Loss: 4.5874
Profiling... [1664/50048]	Loss: 4.7302
Profile done
epoch 1 train time consumed: 3.16s
Validation Epoch: 0, Average loss: 0.0379, Accuracy: 0.0080
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.24321958830092,
                        "time": 2.158342407001328,
                        "accuracy": 0.008010284810126582,
                        "total_cost": 23591681.786904883
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6149
Profiling... [256/50048]	Loss: 7.7722
Profiling... [384/50048]	Loss: 6.9493
Profiling... [512/50048]	Loss: 6.0822
Profiling... [640/50048]	Loss: 5.9538
Profiling... [768/50048]	Loss: 5.7057
Profiling... [896/50048]	Loss: 4.6299
Profiling... [1024/50048]	Loss: 4.8829
Profiling... [1152/50048]	Loss: 4.7253
Profiling... [1280/50048]	Loss: 4.5953
Profiling... [1408/50048]	Loss: 4.5708
Profiling... [1536/50048]	Loss: 4.8406
Profiling... [1664/50048]	Loss: 4.7121
Profile done
epoch 1 train time consumed: 3.09s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0096
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.48178958827793,
                        "time": 2.149747888001002,
                        "accuracy": 0.009592563291139241,
                        "total_cost": 19621850.618254993
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6271
Profiling... [256/50048]	Loss: 7.9574
Profiling... [384/50048]	Loss: 7.8085
Profiling... [512/50048]	Loss: 5.6129
Profiling... [640/50048]	Loss: 5.3690
Profiling... [768/50048]	Loss: 4.9959
Profiling... [896/50048]	Loss: 4.9291
Profiling... [1024/50048]	Loss: 4.7287
Profiling... [1152/50048]	Loss: 4.7133
Profiling... [1280/50048]	Loss: 4.7901
Profiling... [1408/50048]	Loss: 4.6248
Profiling... [1536/50048]	Loss: 4.6220
Profiling... [1664/50048]	Loss: 4.5967
Profile done
epoch 1 train time consumed: 3.16s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.66608680768297,
                        "time": 2.1923787350024213,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 19410655.707715485
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6724
Profiling... [256/50048]	Loss: 7.7731
Profiling... [384/50048]	Loss: 7.1342
Profiling... [512/50048]	Loss: 5.8010
Profiling... [640/50048]	Loss: 5.9681
Profiling... [768/50048]	Loss: 5.1446
Profiling... [896/50048]	Loss: 4.7849
Profiling... [1024/50048]	Loss: 5.1112
Profiling... [1152/50048]	Loss: 5.0771
Profiling... [1280/50048]	Loss: 4.9795
Profiling... [1408/50048]	Loss: 4.9570
Profiling... [1536/50048]	Loss: 4.7028
Profiling... [1664/50048]	Loss: 4.9422
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 0, Average loss: 0.1066, Accuracy: 0.0109
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.53662427376248,
                        "time": 2.5280679729985422,
                        "accuracy": 0.010878164556962026,
                        "total_cost": 20347936.14273076
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6377
Profiling... [512/50176]	Loss: 4.6550
Profiling... [768/50176]	Loss: 4.7262
Profiling... [1024/50176]	Loss: 4.6371
Profiling... [1280/50176]	Loss: 4.6285
Profiling... [1536/50176]	Loss: 4.6835
Profiling... [1792/50176]	Loss: 4.5488
Profiling... [2048/50176]	Loss: 4.5935
Profiling... [2304/50176]	Loss: 4.7007
Profiling... [2560/50176]	Loss: 4.6813
Profiling... [2816/50176]	Loss: 4.5684
Profiling... [3072/50176]	Loss: 4.6964
Profiling... [3328/50176]	Loss: 4.6013
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.35677654220346,
                        "time": 2.367720656999154,
                        "accuracy": 0.009765625,
                        "total_cost": 21228397.79510399
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6620
Profiling... [512/50176]	Loss: 4.6493
Profiling... [768/50176]	Loss: 4.7257
Profiling... [1024/50176]	Loss: 4.7013
Profiling... [1280/50176]	Loss: 4.7088
Profiling... [1536/50176]	Loss: 4.6792
Profiling... [1792/50176]	Loss: 4.6981
Profiling... [2048/50176]	Loss: 4.6760
Profiling... [2304/50176]	Loss: 4.6490
Profiling... [2560/50176]	Loss: 4.6020
Profiling... [2816/50176]	Loss: 4.5712
Profiling... [3072/50176]	Loss: 4.6109
Profiling... [3328/50176]	Loss: 4.6185
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.71432080801485,
                        "time": 2.3728685429996403,
                        "accuracy": 0.009765625,
                        "total_cost": 21274595.90610575
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6940
Profiling... [512/50176]	Loss: 4.6164
Profiling... [768/50176]	Loss: 4.7106
Profiling... [1024/50176]	Loss: 4.6593
Profiling... [1280/50176]	Loss: 4.7467
Profiling... [1536/50176]	Loss: 4.6592
Profiling... [1792/50176]	Loss: 4.6617
Profiling... [2048/50176]	Loss: 4.6069
Profiling... [2304/50176]	Loss: 4.6172
Profiling... [2560/50176]	Loss: 4.5926
Profiling... [2816/50176]	Loss: 4.6605
Profiling... [3072/50176]	Loss: 4.5733
Profiling... [3328/50176]	Loss: 4.5831
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.91858774372626,
                        "time": 2.50449459200172,
                        "accuracy": 0.009765625,
                        "total_cost": 22454751.108743247
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6716
Profiling... [512/50176]	Loss: 4.6237
Profiling... [768/50176]	Loss: 4.6895
Profiling... [1024/50176]	Loss: 4.7147
Profiling... [1280/50176]	Loss: 4.6930
Profiling... [1536/50176]	Loss: 4.6596
Profiling... [1792/50176]	Loss: 4.6705
Profiling... [2048/50176]	Loss: 4.5993
Profiling... [2304/50176]	Loss: 4.6409
Profiling... [2560/50176]	Loss: 4.5879
Profiling... [2816/50176]	Loss: 4.5458
Profiling... [3072/50176]	Loss: 4.5600
Profiling... [3328/50176]	Loss: 4.5735
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.73906634029625,
                        "time": 2.993779248001374,
                        "accuracy": 0.009765625,
                        "total_cost": 26841542.875007443
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6791
Profiling... [512/50176]	Loss: 4.6632
Profiling... [768/50176]	Loss: 4.6626
Profiling... [1024/50176]	Loss: 4.6017
Profiling... [1280/50176]	Loss: 4.6871
Profiling... [1536/50176]	Loss: 4.7005
Profiling... [1792/50176]	Loss: 4.6793
Profiling... [2048/50176]	Loss: 4.6396
Profiling... [2304/50176]	Loss: 4.7042
Profiling... [2560/50176]	Loss: 4.5519
Profiling... [2816/50176]	Loss: 4.6262
Profiling... [3072/50176]	Loss: 4.5991
Profiling... [3328/50176]	Loss: 4.5431
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.54704635534314,
                        "time": 2.3614507140009664,
                        "accuracy": 0.009765625,
                        "total_cost": 21172206.041760955
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6415
Profiling... [512/50176]	Loss: 4.6369
Profiling... [768/50176]	Loss: 4.6183
Profiling... [1024/50176]	Loss: 4.6808
Profiling... [1280/50176]	Loss: 4.6660
Profiling... [1536/50176]	Loss: 4.7549
Profiling... [1792/50176]	Loss: 4.7071
Profiling... [2048/50176]	Loss: 4.6727
Profiling... [2304/50176]	Loss: 4.6011
Profiling... [2560/50176]	Loss: 4.5744
Profiling... [2816/50176]	Loss: 4.5734
Profiling... [3072/50176]	Loss: 4.5799
Profiling... [3328/50176]	Loss: 4.6569
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.85638194899164,
                        "time": 2.3695925259999058,
                        "accuracy": 0.009765625,
                        "total_cost": 21245241.12328505
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6436
Profiling... [512/50176]	Loss: 4.6242
Profiling... [768/50176]	Loss: 4.6698
Profiling... [1024/50176]	Loss: 4.7024
Profiling... [1280/50176]	Loss: 4.6756
Profiling... [1536/50176]	Loss: 4.6471
Profiling... [1792/50176]	Loss: 4.6701
Profiling... [2048/50176]	Loss: 4.6631
Profiling... [2304/50176]	Loss: 4.6039
Profiling... [2560/50176]	Loss: 4.5081
Profiling... [2816/50176]	Loss: 4.5481
Profiling... [3072/50176]	Loss: 4.5723
Profiling... [3328/50176]	Loss: 4.6168
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.05215764674237,
                        "time": 2.506183876997966,
                        "accuracy": 0.009765625,
                        "total_cost": 22469914.008033182
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6535
Profiling... [512/50176]	Loss: 4.6628
Profiling... [768/50176]	Loss: 4.6800
Profiling... [1024/50176]	Loss: 4.7096
Profiling... [1280/50176]	Loss: 4.7143
Profiling... [1536/50176]	Loss: 4.6721
Profiling... [1792/50176]	Loss: 4.6417
Profiling... [2048/50176]	Loss: 4.6080
Profiling... [2304/50176]	Loss: 4.6488
Profiling... [2560/50176]	Loss: 4.6332
Profiling... [2816/50176]	Loss: 4.5741
Profiling... [3072/50176]	Loss: 4.6301
Profiling... [3328/50176]	Loss: 4.6309
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.87818700514556,
                        "time": 3.0030089669999143,
                        "accuracy": 0.009765625,
                        "total_cost": 26924315.823756296
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6409
Profiling... [512/50176]	Loss: 4.6385
Profiling... [768/50176]	Loss: 4.6628
Profiling... [1024/50176]	Loss: 4.7374
Profiling... [1280/50176]	Loss: 4.6862
Profiling... [1536/50176]	Loss: 4.6684
Profiling... [1792/50176]	Loss: 4.5977
Profiling... [2048/50176]	Loss: 4.6254
Profiling... [2304/50176]	Loss: 4.6173
Profiling... [2560/50176]	Loss: 4.6806
Profiling... [2816/50176]	Loss: 4.5809
Profiling... [3072/50176]	Loss: 4.6201
Profiling... [3328/50176]	Loss: 4.5681
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.72066961555649,
                        "time": 2.3763871070004825,
                        "accuracy": 0.009765625,
                        "total_cost": 21306143.317557827
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6357
Profiling... [512/50176]	Loss: 4.6224
Profiling... [768/50176]	Loss: 4.6474
Profiling... [1024/50176]	Loss: 4.7023
Profiling... [1280/50176]	Loss: 4.6339
Profiling... [1536/50176]	Loss: 4.6175
Profiling... [1792/50176]	Loss: 4.6689
Profiling... [2048/50176]	Loss: 4.5899
Profiling... [2304/50176]	Loss: 4.6222
Profiling... [2560/50176]	Loss: 4.6260
Profiling... [2816/50176]	Loss: 4.5860
Profiling... [3072/50176]	Loss: 4.6542
Profiling... [3328/50176]	Loss: 4.5573
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.99376129951416,
                        "time": 2.367178010998032,
                        "accuracy": 0.009765625,
                        "total_cost": 21223609.767515328
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6075
Profiling... [512/50176]	Loss: 4.7011
Profiling... [768/50176]	Loss: 4.6896
Profiling... [1024/50176]	Loss: 4.6810
Profiling... [1280/50176]	Loss: 4.6627
Profiling... [1536/50176]	Loss: 4.6439
Profiling... [1792/50176]	Loss: 4.5978
Profiling... [2048/50176]	Loss: 4.6156
Profiling... [2304/50176]	Loss: 4.6434
Profiling... [2560/50176]	Loss: 4.6088
Profiling... [2816/50176]	Loss: 4.5979
Profiling... [3072/50176]	Loss: 4.6000
Profiling... [3328/50176]	Loss: 4.5986
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.1526255789422,
                        "time": 2.499685162001697,
                        "accuracy": 0.009765625,
                        "total_cost": 22411660.763622306
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6403
Profiling... [512/50176]	Loss: 4.6802
Profiling... [768/50176]	Loss: 4.6322
Profiling... [1024/50176]	Loss: 4.6397
Profiling... [1280/50176]	Loss: 4.6465
Profiling... [1536/50176]	Loss: 4.6258
Profiling... [1792/50176]	Loss: 4.5989
Profiling... [2048/50176]	Loss: 4.6050
Profiling... [2304/50176]	Loss: 4.5950
Profiling... [2560/50176]	Loss: 4.5016
Profiling... [2816/50176]	Loss: 4.6307
Profiling... [3072/50176]	Loss: 4.5464
Profiling... [3328/50176]	Loss: 4.5306
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.9944030943289,
                        "time": 2.991907824998634,
                        "accuracy": 0.009765625,
                        "total_cost": 26824803.236534268
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6690
Profiling... [512/50176]	Loss: 5.5209
Profiling... [768/50176]	Loss: 5.5178
Profiling... [1024/50176]	Loss: 5.3916
Profiling... [1280/50176]	Loss: 5.2753
Profiling... [1536/50176]	Loss: 4.8455
Profiling... [1792/50176]	Loss: 4.7352
Profiling... [2048/50176]	Loss: 4.7710
Profiling... [2304/50176]	Loss: 4.7498
Profiling... [2560/50176]	Loss: 4.7202
Profiling... [2816/50176]	Loss: 4.7726
Profiling... [3072/50176]	Loss: 4.7235
Profiling... [3328/50176]	Loss: 4.7560
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.83860506360907,
                        "time": 2.3625167540012626,
                        "accuracy": 0.009765625,
                        "total_cost": 21181799.170313314
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6081
Profiling... [512/50176]	Loss: 5.5866
Profiling... [768/50176]	Loss: 5.4900
Profiling... [1024/50176]	Loss: 5.3049
Profiling... [1280/50176]	Loss: 4.9791
Profiling... [1536/50176]	Loss: 4.8937
Profiling... [1792/50176]	Loss: 4.8574
Profiling... [2048/50176]	Loss: 5.0299
Profiling... [2304/50176]	Loss: 4.8837
Profiling... [2560/50176]	Loss: 4.8687
Profiling... [2816/50176]	Loss: 4.9093
Profiling... [3072/50176]	Loss: 4.6325
Profiling... [3328/50176]	Loss: 4.6802
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 0, Average loss: 0.0192, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.10344533258869,
                        "time": 2.370807973002229,
                        "accuracy": 0.009765625,
                        "total_cost": 21256168.54145836
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6444
Profiling... [512/50176]	Loss: 5.5595
Profiling... [768/50176]	Loss: 5.4560
Profiling... [1024/50176]	Loss: 5.3414
Profiling... [1280/50176]	Loss: 5.3688
Profiling... [1536/50176]	Loss: 4.9110
Profiling... [1792/50176]	Loss: 4.8234
Profiling... [2048/50176]	Loss: 4.9131
Profiling... [2304/50176]	Loss: 4.7811
Profiling... [2560/50176]	Loss: 4.7252
Profiling... [2816/50176]	Loss: 4.6414
Profiling... [3072/50176]	Loss: 4.5821
Profiling... [3328/50176]	Loss: 4.6848
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.2186275518287,
                        "time": 2.5231554890015104,
                        "accuracy": 0.009765625,
                        "total_cost": 22622099.393373642
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6605
Profiling... [512/50176]	Loss: 5.6222
Profiling... [768/50176]	Loss: 5.4256
Profiling... [1024/50176]	Loss: 5.4654
Profiling... [1280/50176]	Loss: 5.1383
Profiling... [1536/50176]	Loss: 5.1603
Profiling... [1792/50176]	Loss: 4.9697
Profiling... [2048/50176]	Loss: 4.7451
Profiling... [2304/50176]	Loss: 4.7318
Profiling... [2560/50176]	Loss: 4.6541
Profiling... [2816/50176]	Loss: 4.6518
Profiling... [3072/50176]	Loss: 4.8161
Profiling... [3328/50176]	Loss: 4.6049
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.0590800485975,
                        "time": 2.984834648999822,
                        "accuracy": 0.009765625,
                        "total_cost": 26761396.543205477
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6361
Profiling... [512/50176]	Loss: 5.8965
Profiling... [768/50176]	Loss: 5.4434
Profiling... [1024/50176]	Loss: 5.0375
Profiling... [1280/50176]	Loss: 5.0425
Profiling... [1536/50176]	Loss: 5.0384
Profiling... [1792/50176]	Loss: 4.6726
Profiling... [2048/50176]	Loss: 4.6563
Profiling... [2304/50176]	Loss: 4.7074
Profiling... [2560/50176]	Loss: 4.7345
Profiling... [2816/50176]	Loss: 4.7803
Profiling... [3072/50176]	Loss: 4.5592
Profiling... [3328/50176]	Loss: 4.6316
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.93404827553043,
                        "time": 2.37177459999657,
                        "accuracy": 0.009765625,
                        "total_cost": 21264814.54625661
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6455
Profiling... [512/50176]	Loss: 5.5720
Profiling... [768/50176]	Loss: 5.5925
Profiling... [1024/50176]	Loss: 5.1710
Profiling... [1280/50176]	Loss: 5.1898
Profiling... [1536/50176]	Loss: 4.9622
Profiling... [1792/50176]	Loss: 5.0085
Profiling... [2048/50176]	Loss: 4.8057
Profiling... [2304/50176]	Loss: 4.6563
Profiling... [2560/50176]	Loss: 4.6989
Profiling... [2816/50176]	Loss: 4.8281
Profiling... [3072/50176]	Loss: 4.8274
Profiling... [3328/50176]	Loss: 4.7790
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.16194138462984,
                        "time": 2.373779937999643,
                        "accuracy": 0.009765625,
                        "total_cost": 21282821.667642444
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6552
Profiling... [512/50176]	Loss: 5.5657
Profiling... [768/50176]	Loss: 5.5881
Profiling... [1024/50176]	Loss: 5.0886
Profiling... [1280/50176]	Loss: 4.8925
Profiling... [1536/50176]	Loss: 4.9114
Profiling... [1792/50176]	Loss: 5.0525
Profiling... [2048/50176]	Loss: 4.8526
Profiling... [2304/50176]	Loss: 4.7300
Profiling... [2560/50176]	Loss: 4.9189
Profiling... [2816/50176]	Loss: 4.6960
Profiling... [3072/50176]	Loss: 4.5884
Profiling... [3328/50176]	Loss: 4.5785
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.2846479605459,
                        "time": 2.5283149410024635,
                        "accuracy": 0.009765625,
                        "total_cost": 22668366.537904944
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6533
Profiling... [512/50176]	Loss: 5.7266
Profiling... [768/50176]	Loss: 5.7971
Profiling... [1024/50176]	Loss: 5.1858
Profiling... [1280/50176]	Loss: 4.8639
Profiling... [1536/50176]	Loss: 4.7709
Profiling... [1792/50176]	Loss: 4.9026
Profiling... [2048/50176]	Loss: 4.7416
Profiling... [2304/50176]	Loss: 4.7299
Profiling... [2560/50176]	Loss: 4.7311
Profiling... [2816/50176]	Loss: 4.6844
Profiling... [3072/50176]	Loss: 4.6080
Profiling... [3328/50176]	Loss: 4.6847
Profile done
epoch 1 train time consumed: 4.16s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.14124667375451,
                        "time": 2.979534941001475,
                        "accuracy": 0.009765625,
                        "total_cost": 26713893.016217146
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6299
Profiling... [512/50176]	Loss: 5.9123
Profiling... [768/50176]	Loss: 5.6952
Profiling... [1024/50176]	Loss: 4.9161
Profiling... [1280/50176]	Loss: 4.8491
Profiling... [1536/50176]	Loss: 4.9460
Profiling... [1792/50176]	Loss: 4.6396
Profiling... [2048/50176]	Loss: 4.7782
Profiling... [2304/50176]	Loss: 4.7332
Profiling... [2560/50176]	Loss: 4.8291
Profiling... [2816/50176]	Loss: 4.7412
Profiling... [3072/50176]	Loss: 4.6295
Profiling... [3328/50176]	Loss: 4.6954
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.00206910688834,
                        "time": 2.376472965002904,
                        "accuracy": 0.009765625,
                        "total_cost": 21306947.34017178
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6584
Profiling... [512/50176]	Loss: 5.6579
Profiling... [768/50176]	Loss: 5.5328
Profiling... [1024/50176]	Loss: 5.1354
Profiling... [1280/50176]	Loss: 4.9712
Profiling... [1536/50176]	Loss: 4.8820
Profiling... [1792/50176]	Loss: 4.7837
Profiling... [2048/50176]	Loss: 4.7324
Profiling... [2304/50176]	Loss: 4.7350
Profiling... [2560/50176]	Loss: 4.6381
Profiling... [2816/50176]	Loss: 4.5610
Profiling... [3072/50176]	Loss: 4.6230
Profiling... [3328/50176]	Loss: 4.6416
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.23508260121055,
                        "time": 2.3673134700002265,
                        "accuracy": 0.009765625,
                        "total_cost": 21224853.51354153
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6226
Profiling... [512/50176]	Loss: 5.4532
Profiling... [768/50176]	Loss: 5.3987
Profiling... [1024/50176]	Loss: 5.0365
Profiling... [1280/50176]	Loss: 4.8904
Profiling... [1536/50176]	Loss: 4.8033
Profiling... [1792/50176]	Loss: 4.7564
Profiling... [2048/50176]	Loss: 4.6380
Profiling... [2304/50176]	Loss: 4.8560
Profiling... [2560/50176]	Loss: 4.9410
Profiling... [2816/50176]	Loss: 4.8512
Profiling... [3072/50176]	Loss: 4.6317
Profiling... [3328/50176]	Loss: 4.6652
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.34378593694844,
                        "time": 2.5242707780016644,
                        "accuracy": 0.009765625,
                        "total_cost": 22632115.023718398
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6382
Profiling... [512/50176]	Loss: 5.6394
Profiling... [768/50176]	Loss: 5.3082
Profiling... [1024/50176]	Loss: 5.2345
Profiling... [1280/50176]	Loss: 4.8981
Profiling... [1536/50176]	Loss: 4.7714
Profiling... [1792/50176]	Loss: 4.6372
Profiling... [2048/50176]	Loss: 4.8199
Profiling... [2304/50176]	Loss: 4.7201
Profiling... [2560/50176]	Loss: 4.7303
Profiling... [2816/50176]	Loss: 4.7792
Profiling... [3072/50176]	Loss: 4.6707
Profiling... [3328/50176]	Loss: 4.6228
Profile done
epoch 1 train time consumed: 4.16s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.21252587912309,
                        "time": 2.9914047490019584,
                        "accuracy": 0.009765625,
                        "total_cost": 26820326.172820687
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6757
Profiling... [512/50176]	Loss: 7.4283
Profiling... [768/50176]	Loss: 6.9635
Profiling... [1024/50176]	Loss: 5.4566
Profiling... [1280/50176]	Loss: 4.9281
Profiling... [1536/50176]	Loss: 5.1075
Profiling... [1792/50176]	Loss: 5.1576
Profiling... [2048/50176]	Loss: 4.7735
Profiling... [2304/50176]	Loss: 5.0056
Profiling... [2560/50176]	Loss: 5.0753
Profiling... [2816/50176]	Loss: 4.8095
Profiling... [3072/50176]	Loss: 4.6147
Profiling... [3328/50176]	Loss: 4.6951
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.07949681688439,
                        "time": 2.371593424999446,
                        "accuracy": 0.009765625,
                        "total_cost": 21263207.83186208
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6133
Profiling... [512/50176]	Loss: 7.1880
Profiling... [768/50176]	Loss: 5.9876
Profiling... [1024/50176]	Loss: 5.5786
Profiling... [1280/50176]	Loss: 5.1593
Profiling... [1536/50176]	Loss: 4.8309
Profiling... [1792/50176]	Loss: 5.2652
Profiling... [2048/50176]	Loss: 4.8801
Profiling... [2304/50176]	Loss: 5.0627
Profiling... [2560/50176]	Loss: 4.9619
Profiling... [2816/50176]	Loss: 4.9080
Profiling... [3072/50176]	Loss: 4.9574
Profiling... [3328/50176]	Loss: 4.9858
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.27540847512239,
                        "time": 2.3768350439968344,
                        "accuracy": 0.009765625,
                        "total_cost": 21310226.926588558
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6531
Profiling... [512/50176]	Loss: 7.8239
Profiling... [768/50176]	Loss: 6.5652
Profiling... [1024/50176]	Loss: 5.9301
Profiling... [1280/50176]	Loss: 5.5132
Profiling... [1536/50176]	Loss: 5.2896
Profiling... [1792/50176]	Loss: 5.1081
Profiling... [2048/50176]	Loss: 4.7787
Profiling... [2304/50176]	Loss: 4.7756
Profiling... [2560/50176]	Loss: 4.9321
Profiling... [2816/50176]	Loss: 4.7774
Profiling... [3072/50176]	Loss: 4.8640
Profiling... [3328/50176]	Loss: 4.8168
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.39021156856201,
                        "time": 2.5127922399988165,
                        "accuracy": 0.009765625,
                        "total_cost": 22529206.68382793
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6159
Profiling... [512/50176]	Loss: 7.8630
Profiling... [768/50176]	Loss: 6.4521
Profiling... [1024/50176]	Loss: 5.3155
Profiling... [1280/50176]	Loss: 5.3351
Profiling... [1536/50176]	Loss: 5.1200
Profiling... [1792/50176]	Loss: 4.9759
Profiling... [2048/50176]	Loss: 4.9440
Profiling... [2304/50176]	Loss: 4.9927
Profiling... [2560/50176]	Loss: 4.9896
Profiling... [2816/50176]	Loss: 4.6685
Profiling... [3072/50176]	Loss: 4.6081
Profiling... [3328/50176]	Loss: 4.8459
Profile done
epoch 1 train time consumed: 4.22s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.264260716187,
                        "time": 2.9608447369973874,
                        "accuracy": 0.009765625,
                        "total_cost": 26546339.167476438
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6669
Profiling... [512/50176]	Loss: 7.6832
Profiling... [768/50176]	Loss: 6.6320
Profiling... [1024/50176]	Loss: 5.2850
Profiling... [1280/50176]	Loss: 5.1180
Profiling... [1536/50176]	Loss: 5.1485
Profiling... [1792/50176]	Loss: 5.1617
Profiling... [2048/50176]	Loss: 4.7936
Profiling... [2304/50176]	Loss: 4.6932
Profiling... [2560/50176]	Loss: 4.7995
Profiling... [2816/50176]	Loss: 4.6989
Profiling... [3072/50176]	Loss: 4.6739
Profiling... [3328/50176]	Loss: 4.6553
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 0, Average loss: 0.0348, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.15045737381483,
                        "time": 2.362300275999587,
                        "accuracy": 0.009765625,
                        "total_cost": 21179895.995218504
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6399
Profiling... [512/50176]	Loss: 7.6680
Profiling... [768/50176]	Loss: 7.2762
Profiling... [1024/50176]	Loss: 5.3457
Profiling... [1280/50176]	Loss: 4.8193
Profiling... [1536/50176]	Loss: 4.9333
Profiling... [1792/50176]	Loss: 4.6826
Profiling... [2048/50176]	Loss: 4.5707
Profiling... [2304/50176]	Loss: 4.6502
Profiling... [2560/50176]	Loss: 4.6692
Profiling... [2816/50176]	Loss: 4.7171
Profiling... [3072/50176]	Loss: 4.6547
Profiling... [3328/50176]	Loss: 4.8094
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0110
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.30256016986556,
                        "time": 2.3704068119986914,
                        "accuracy": 0.01103515625,
                        "total_cost": 18807607.063117683
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6366
Profiling... [512/50176]	Loss: 7.7751
Profiling... [768/50176]	Loss: 6.1387
Profiling... [1024/50176]	Loss: 4.9793
Profiling... [1280/50176]	Loss: 4.8873
Profiling... [1536/50176]	Loss: 5.1320
Profiling... [1792/50176]	Loss: 4.7096
Profiling... [2048/50176]	Loss: 4.8421
Profiling... [2304/50176]	Loss: 4.7386
Profiling... [2560/50176]	Loss: 4.6950
Profiling... [2816/50176]	Loss: 5.1962
Profiling... [3072/50176]	Loss: 5.0497
Profiling... [3328/50176]	Loss: 5.0483
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 0, Average loss: 0.0474, Accuracy: 0.0126
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.40540644079128,
                        "time": 2.5124139560030017,
                        "accuracy": 0.01259765625,
                        "total_cost": 17461873.657107975
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6593
Profiling... [512/50176]	Loss: 7.7172
Profiling... [768/50176]	Loss: 6.8540
Profiling... [1024/50176]	Loss: 5.5369
Profiling... [1280/50176]	Loss: 4.9700
Profiling... [1536/50176]	Loss: 5.1208
Profiling... [1792/50176]	Loss: 5.1031
Profiling... [2048/50176]	Loss: 5.9957
Profiling... [2304/50176]	Loss: 5.0217
Profiling... [2560/50176]	Loss: 4.8756
Profiling... [2816/50176]	Loss: 4.7186
Profiling... [3072/50176]	Loss: 4.8568
Profiling... [3328/50176]	Loss: 4.7916
Profile done
epoch 1 train time consumed: 4.28s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0105
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.27090162585756,
                        "time": 2.9965949440011173,
                        "accuracy": 0.010546875,
                        "total_cost": 24876730.89001766
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6206
Profiling... [512/50176]	Loss: 7.5190
Profiling... [768/50176]	Loss: 6.1555
Profiling... [1024/50176]	Loss: 5.8940
Profiling... [1280/50176]	Loss: 5.1405
Profiling... [1536/50176]	Loss: 5.1941
Profiling... [1792/50176]	Loss: 4.9034
Profiling... [2048/50176]	Loss: 4.9407
Profiling... [2304/50176]	Loss: 4.7222
Profiling... [2560/50176]	Loss: 4.8700
Profiling... [2816/50176]	Loss: 4.9527
Profiling... [3072/50176]	Loss: 4.6976
Profiling... [3328/50176]	Loss: 4.7009
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.15640865907947,
                        "time": 2.3675791889982065,
                        "accuracy": 0.009765625,
                        "total_cost": 21227226.35944597
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6337
Profiling... [512/50176]	Loss: 7.7220
Profiling... [768/50176]	Loss: 6.8384
Profiling... [1024/50176]	Loss: 5.5714
Profiling... [1280/50176]	Loss: 5.2066
Profiling... [1536/50176]	Loss: 5.0304
Profiling... [1792/50176]	Loss: 4.9614
Profiling... [2048/50176]	Loss: 5.0367
Profiling... [2304/50176]	Loss: 5.0359
Profiling... [2560/50176]	Loss: 4.6276
Profiling... [2816/50176]	Loss: 4.7139
Profiling... [3072/50176]	Loss: 4.7958
Profiling... [3328/50176]	Loss: 4.9088
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.33369239522601,
                        "time": 2.368682225998782,
                        "accuracy": 0.009765625,
                        "total_cost": 21237137.461891595
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6345
Profiling... [512/50176]	Loss: 8.0927
Profiling... [768/50176]	Loss: 7.4672
Profiling... [1024/50176]	Loss: 5.4226
Profiling... [1280/50176]	Loss: 5.0270
Profiling... [1536/50176]	Loss: 5.2813
Profiling... [1792/50176]	Loss: 5.0519
Profiling... [2048/50176]	Loss: 4.9828
Profiling... [2304/50176]	Loss: 4.8687
Profiling... [2560/50176]	Loss: 4.8135
Profiling... [2816/50176]	Loss: 4.6398
Profiling... [3072/50176]	Loss: 4.6307
Profiling... [3328/50176]	Loss: 4.6174
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.43828478916164,
                        "time": 2.516894266998861,
                        "accuracy": 0.009765625,
                        "total_cost": 22565990.855344366
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6370
Profiling... [512/50176]	Loss: 7.3408
Profiling... [768/50176]	Loss: 6.2279
Profiling... [1024/50176]	Loss: 5.9298
Profiling... [1280/50176]	Loss: 5.3211
Profiling... [1536/50176]	Loss: 5.4639
Profiling... [1792/50176]	Loss: 6.0117
Profiling... [2048/50176]	Loss: 5.2972
Profiling... [2304/50176]	Loss: 5.2208
Profiling... [2560/50176]	Loss: 4.9166
Profiling... [2816/50176]	Loss: 4.9744
Profiling... [3072/50176]	Loss: 4.7283
Profiling... [3328/50176]	Loss: 4.8617
Profile done
epoch 1 train time consumed: 4.24s
Validation Epoch: 0, Average loss: 0.0231, Accuracy: 0.0146
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.32853109974172,
                        "time": 2.9669797879978432,
                        "accuracy": 0.01455078125,
                        "total_cost": 17853258.09776705
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6319
Profiling... [1024/50176]	Loss: 4.6601
Profiling... [1536/50176]	Loss: 4.6602
Profiling... [2048/50176]	Loss: 4.6353
Profiling... [2560/50176]	Loss: 4.5969
Profiling... [3072/50176]	Loss: 4.5514
Profiling... [3584/50176]	Loss: 4.5790
Profiling... [4096/50176]	Loss: 4.5553
Profiling... [4608/50176]	Loss: 4.5965
Profiling... [5120/50176]	Loss: 4.5073
Profiling... [5632/50176]	Loss: 4.5361
Profiling... [6144/50176]	Loss: 4.5610
Profiling... [6656/50176]	Loss: 4.5171
Profile done
epoch 1 train time consumed: 6.39s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.35991901231084,
                        "time": 4.469327547998546,
                        "accuracy": 0.009765625,
                        "total_cost": 40071114.93164166
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6614
Profiling... [1024/50176]	Loss: 4.6043
Profiling... [1536/50176]	Loss: 4.6371
Profiling... [2048/50176]	Loss: 4.6967
Profiling... [2560/50176]	Loss: 4.6747
Profiling... [3072/50176]	Loss: 4.6161
Profiling... [3584/50176]	Loss: 4.5839
Profiling... [4096/50176]	Loss: 4.5988
Profiling... [4608/50176]	Loss: 4.5324
Profiling... [5120/50176]	Loss: 4.5489
Profiling... [5632/50176]	Loss: 4.5358
Profiling... [6144/50176]	Loss: 4.4879
Profiling... [6656/50176]	Loss: 4.5033
Profile done
epoch 1 train time consumed: 6.40s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.66251913412113,
                        "time": 4.481536364997737,
                        "accuracy": 0.009765625,
                        "total_cost": 40180646.22527697
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6518
Profiling... [1024/50176]	Loss: 4.6618
Profiling... [1536/50176]	Loss: 4.6241
Profiling... [2048/50176]	Loss: 4.6416
Profiling... [2560/50176]	Loss: 4.6038
Profiling... [3072/50176]	Loss: 4.6017
Profiling... [3584/50176]	Loss: 4.5532
Profiling... [4096/50176]	Loss: 4.5816
Profiling... [4608/50176]	Loss: 4.5299
Profiling... [5120/50176]	Loss: 4.5623
Profiling... [5632/50176]	Loss: 4.5703
Profiling... [6144/50176]	Loss: 4.5253
Profiling... [6656/50176]	Loss: 4.5548
Profile done
epoch 1 train time consumed: 6.88s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.77719810304895,
                        "time": 4.827716731000692,
                        "accuracy": 0.009765625,
                        "total_cost": 43284465.25481009
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6567
Profiling... [1024/50176]	Loss: 4.6953
Profiling... [1536/50176]	Loss: 4.6386
Profiling... [2048/50176]	Loss: 4.6291
Profiling... [2560/50176]	Loss: 4.6482
Profiling... [3072/50176]	Loss: 4.6175
Profiling... [3584/50176]	Loss: 4.5913
Profiling... [4096/50176]	Loss: 4.5970
Profiling... [4608/50176]	Loss: 4.5482
Profiling... [5120/50176]	Loss: 4.5473
Profiling... [5632/50176]	Loss: 4.5729
Profiling... [6144/50176]	Loss: 4.5505
Profiling... [6656/50176]	Loss: 4.5074
Profile done
epoch 1 train time consumed: 8.21s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.5214186647405,
                        "time": 5.871344027000305,
                        "accuracy": 0.009765625,
                        "total_cost": 52641368.47505753
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6541
Profiling... [1024/50176]	Loss: 4.6160
Profiling... [1536/50176]	Loss: 4.6229
Profiling... [2048/50176]	Loss: 4.6536
Profiling... [2560/50176]	Loss: 4.6311
Profiling... [3072/50176]	Loss: 4.6081
Profiling... [3584/50176]	Loss: 4.5892
Profiling... [4096/50176]	Loss: 4.5557
Profiling... [4608/50176]	Loss: 4.5894
Profiling... [5120/50176]	Loss: 4.5657
Profiling... [5632/50176]	Loss: 4.5346
Profiling... [6144/50176]	Loss: 4.5422
Profiling... [6656/50176]	Loss: 4.5801
Profile done
epoch 1 train time consumed: 6.39s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.551962141241,
                        "time": 4.471468875999562,
                        "accuracy": 0.009765625,
                        "total_cost": 40090357.62505967
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6578
Profiling... [1024/50176]	Loss: 4.6522
Profiling... [1536/50176]	Loss: 4.6487
Profiling... [2048/50176]	Loss: 4.6169
Profiling... [2560/50176]	Loss: 4.5857
Profiling... [3072/50176]	Loss: 4.6322
Profiling... [3584/50176]	Loss: 4.5633
Profiling... [4096/50176]	Loss: 4.5490
Profiling... [4608/50176]	Loss: 4.5607
Profiling... [5120/50176]	Loss: 4.5503
Profiling... [5632/50176]	Loss: 4.5395
Profiling... [6144/50176]	Loss: 4.5164
Profiling... [6656/50176]	Loss: 4.5194
Profile done
epoch 1 train time consumed: 6.51s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.8415532885464,
                        "time": 4.476360338001541,
                        "accuracy": 0.009765625,
                        "total_cost": 40134279.93416842
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6226
Profiling... [1024/50176]	Loss: 4.6748
Profiling... [1536/50176]	Loss: 4.6698
Profiling... [2048/50176]	Loss: 4.6159
Profiling... [2560/50176]	Loss: 4.6122
Profiling... [3072/50176]	Loss: 4.6064
Profiling... [3584/50176]	Loss: 4.6154
Profiling... [4096/50176]	Loss: 4.6153
Profiling... [4608/50176]	Loss: 4.5831
Profiling... [5120/50176]	Loss: 4.5639
Profiling... [5632/50176]	Loss: 4.5631
Profiling... [6144/50176]	Loss: 4.5673
Profiling... [6656/50176]	Loss: 4.5525
Profile done
epoch 1 train time consumed: 6.87s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.94273968703773,
                        "time": 4.816666596001596,
                        "accuracy": 0.009765625,
                        "total_cost": 43185432.49860541
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6581
Profiling... [1024/50176]	Loss: 4.6347
Profiling... [1536/50176]	Loss: 4.6674
Profiling... [2048/50176]	Loss: 4.6347
Profiling... [2560/50176]	Loss: 4.6239
Profiling... [3072/50176]	Loss: 4.6516
Profiling... [3584/50176]	Loss: 4.5556
Profiling... [4096/50176]	Loss: 4.5663
Profiling... [4608/50176]	Loss: 4.5766
Profiling... [5120/50176]	Loss: 4.5713
Profiling... [5632/50176]	Loss: 4.5622
Profiling... [6144/50176]	Loss: 4.5684
Profiling... [6656/50176]	Loss: 4.5517
Profile done
epoch 1 train time consumed: 8.17s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.6950971336303,
                        "time": 5.879284777001885,
                        "accuracy": 0.009765625,
                        "total_cost": 52712616.02965133
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6379
Profiling... [1024/50176]	Loss: 4.6574
Profiling... [1536/50176]	Loss: 4.6627
Profiling... [2048/50176]	Loss: 4.6658
Profiling... [2560/50176]	Loss: 4.6444
Profiling... [3072/50176]	Loss: 4.6187
Profiling... [3584/50176]	Loss: 4.5661
Profiling... [4096/50176]	Loss: 4.6019
Profiling... [4608/50176]	Loss: 4.5447
Profiling... [5120/50176]	Loss: 4.5533
Profiling... [5632/50176]	Loss: 4.5566
Profiling... [6144/50176]	Loss: 4.5635
Profiling... [6656/50176]	Loss: 4.6158
Profile done
epoch 1 train time consumed: 6.47s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.71414495143138,
                        "time": 4.470905240999855,
                        "accuracy": 0.009765625,
                        "total_cost": 40085341.30389063
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6538
Profiling... [1024/50176]	Loss: 4.6665
Profiling... [1536/50176]	Loss: 4.6694
Profiling... [2048/50176]	Loss: 4.6180
Profiling... [2560/50176]	Loss: 4.6270
Profiling... [3072/50176]	Loss: 4.6175
Profiling... [3584/50176]	Loss: 4.5972
Profiling... [4096/50176]	Loss: 4.6003
Profiling... [4608/50176]	Loss: 4.5620
Profiling... [5120/50176]	Loss: 4.5739
Profiling... [5632/50176]	Loss: 4.5450
Profiling... [6144/50176]	Loss: 4.5139
Profiling... [6656/50176]	Loss: 4.5715
Profile done
epoch 1 train time consumed: 6.44s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.97104853545012,
                        "time": 4.487812488001509,
                        "accuracy": 0.009765625,
                        "total_cost": 40236987.70407179
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6305
Profiling... [1024/50176]	Loss: 4.6390
Profiling... [1536/50176]	Loss: 4.6759
Profiling... [2048/50176]	Loss: 4.6369
Profiling... [2560/50176]	Loss: 4.6557
Profiling... [3072/50176]	Loss: 4.5709
Profiling... [3584/50176]	Loss: 4.5734
Profiling... [4096/50176]	Loss: 4.5857
Profiling... [4608/50176]	Loss: 4.5387
Profiling... [5120/50176]	Loss: 4.5665
Profiling... [5632/50176]	Loss: 4.5867
Profiling... [6144/50176]	Loss: 4.5579
Profiling... [6656/50176]	Loss: 4.5179
Profile done
epoch 1 train time consumed: 6.87s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.07044644396443,
                        "time": 4.803765568998642,
                        "accuracy": 0.009765625,
                        "total_cost": 43069795.44360091
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6491
Profiling... [1024/50176]	Loss: 4.6453
Profiling... [1536/50176]	Loss: 4.6394
Profiling... [2048/50176]	Loss: 4.6860
Profiling... [2560/50176]	Loss: 4.6160
Profiling... [3072/50176]	Loss: 4.5943
Profiling... [3584/50176]	Loss: 4.6151
Profiling... [4096/50176]	Loss: 4.5833
Profiling... [4608/50176]	Loss: 4.5698
Profiling... [5120/50176]	Loss: 4.5458
Profiling... [5632/50176]	Loss: 4.5210
Profiling... [6144/50176]	Loss: 4.5547
Profiling... [6656/50176]	Loss: 4.5606
Profile done
epoch 1 train time consumed: 8.23s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.84364427046485,
                        "time": 5.865987580000365,
                        "accuracy": 0.009765625,
                        "total_cost": 52593440.35345492
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6408
Profiling... [1024/50176]	Loss: 5.4358
Profiling... [1536/50176]	Loss: 5.4415
Profiling... [2048/50176]	Loss: 5.4184
Profiling... [2560/50176]	Loss: 4.9497
Profiling... [3072/50176]	Loss: 4.7232
Profiling... [3584/50176]	Loss: 4.7349
Profiling... [4096/50176]	Loss: 4.6936
Profiling... [4608/50176]	Loss: 4.6444
Profiling... [5120/50176]	Loss: 4.6261
Profiling... [5632/50176]	Loss: 4.6300
Profiling... [6144/50176]	Loss: 4.5606
Profiling... [6656/50176]	Loss: 4.5984
Profile done
epoch 1 train time consumed: 6.44s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.8521021836163,
                        "time": 4.476147519999358,
                        "accuracy": 0.009765625,
                        "total_cost": 40132374.26200185
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6661
Profiling... [1024/50176]	Loss: 5.5890
Profiling... [1536/50176]	Loss: 5.2154
Profiling... [2048/50176]	Loss: 4.9427
Profiling... [2560/50176]	Loss: 4.8890
Profiling... [3072/50176]	Loss: 4.7312
Profiling... [3584/50176]	Loss: 4.6735
Profiling... [4096/50176]	Loss: 4.7775
Profiling... [4608/50176]	Loss: 4.6079
Profiling... [5120/50176]	Loss: 4.6904
Profiling... [5632/50176]	Loss: 4.5596
Profiling... [6144/50176]	Loss: 4.5738
Profiling... [6656/50176]	Loss: 4.5231
Profile done
epoch 1 train time consumed: 6.44s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.09489069642703,
                        "time": 4.481327407000208,
                        "accuracy": 0.009765625,
                        "total_cost": 40178871.950628504
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6406
Profiling... [1024/50176]	Loss: 5.4341
Profiling... [1536/50176]	Loss: 5.3048
Profiling... [2048/50176]	Loss: 4.7694
Profiling... [2560/50176]	Loss: 4.7861
Profiling... [3072/50176]	Loss: 4.7447
Profiling... [3584/50176]	Loss: 4.8401
Profiling... [4096/50176]	Loss: 4.7290
Profiling... [4608/50176]	Loss: 4.6900
Profiling... [5120/50176]	Loss: 4.7677
Profiling... [5632/50176]	Loss: 4.6555
Profiling... [6144/50176]	Loss: 4.7224
Profiling... [6656/50176]	Loss: 4.5780
Profile done
epoch 1 train time consumed: 6.82s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.18214136249044,
                        "time": 4.821194172000105,
                        "accuracy": 0.009765625,
                        "total_cost": 43226085.08797449
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6684
Profiling... [1024/50176]	Loss: 5.5425
Profiling... [1536/50176]	Loss: 5.3002
Profiling... [2048/50176]	Loss: 4.8127
Profiling... [2560/50176]	Loss: 4.7938
Profiling... [3072/50176]	Loss: 4.9509
Profiling... [3584/50176]	Loss: 4.8068
Profiling... [4096/50176]	Loss: 4.7423
Profiling... [4608/50176]	Loss: 4.7876
Profiling... [5120/50176]	Loss: 4.6755
Profiling... [5632/50176]	Loss: 4.6003
Profiling... [6144/50176]	Loss: 4.6346
Profiling... [6656/50176]	Loss: 4.5773
Profile done
epoch 1 train time consumed: 8.14s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.96415500780414,
                        "time": 5.8913040080005885,
                        "accuracy": 0.009765625,
                        "total_cost": 52820459.46282336
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6653
Profiling... [1024/50176]	Loss: 5.5974
Profiling... [1536/50176]	Loss: 5.0705
Profiling... [2048/50176]	Loss: 4.8693
Profiling... [2560/50176]	Loss: 4.9470
Profiling... [3072/50176]	Loss: 4.8565
Profiling... [3584/50176]	Loss: 4.7159
Profiling... [4096/50176]	Loss: 4.6301
Profiling... [4608/50176]	Loss: 4.6878
Profiling... [5120/50176]	Loss: 4.6664
Profiling... [5632/50176]	Loss: 4.5840
Profiling... [6144/50176]	Loss: 4.6254
Profiling... [6656/50176]	Loss: 4.6448
Profile done
epoch 1 train time consumed: 6.47s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.978581385709,
                        "time": 4.470339108000189,
                        "accuracy": 0.009765625,
                        "total_cost": 40080325.980665654
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6378
Profiling... [1024/50176]	Loss: 5.5311
Profiling... [1536/50176]	Loss: 5.3111
Profiling... [2048/50176]	Loss: 5.0016
Profiling... [2560/50176]	Loss: 4.9532
Profiling... [3072/50176]	Loss: 4.8650
Profiling... [3584/50176]	Loss: 4.6448
Profiling... [4096/50176]	Loss: 4.6588
Profiling... [4608/50176]	Loss: 4.6044
Profiling... [5120/50176]	Loss: 4.6207
Profiling... [5632/50176]	Loss: 4.6532
Profiling... [6144/50176]	Loss: 4.6147
Profiling... [6656/50176]	Loss: 4.5733
Profile done
epoch 1 train time consumed: 6.42s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.20810571394104,
                        "time": 4.474067625000316,
                        "accuracy": 0.009765625,
                        "total_cost": 40113807.82916315
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6376
Profiling... [1024/50176]	Loss: 5.6424
Profiling... [1536/50176]	Loss: 5.0957
Profiling... [2048/50176]	Loss: 5.0154
Profiling... [2560/50176]	Loss: 4.9224
Profiling... [3072/50176]	Loss: 4.7299
Profiling... [3584/50176]	Loss: 4.7017
Profiling... [4096/50176]	Loss: 4.6337
Profiling... [4608/50176]	Loss: 4.5970
Profiling... [5120/50176]	Loss: 4.5966
Profiling... [5632/50176]	Loss: 4.6095
Profiling... [6144/50176]	Loss: 4.5762
Profiling... [6656/50176]	Loss: 4.5555
Profile done
epoch 1 train time consumed: 6.82s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.29935990481961,
                        "time": 4.832274591000896,
                        "accuracy": 0.009765625,
                        "total_cost": 43325459.42107098
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6517
Profiling... [1024/50176]	Loss: 5.6173
Profiling... [1536/50176]	Loss: 5.2807
Profiling... [2048/50176]	Loss: 5.0287
Profiling... [2560/50176]	Loss: 4.8732
Profiling... [3072/50176]	Loss: 4.7745
Profiling... [3584/50176]	Loss: 4.6780
Profiling... [4096/50176]	Loss: 4.6840
Profiling... [4608/50176]	Loss: 4.6843
Profiling... [5120/50176]	Loss: 4.6126
Profiling... [5632/50176]	Loss: 4.7361
Profiling... [6144/50176]	Loss: 4.5911
Profiling... [6656/50176]	Loss: 4.6196
Profile done
epoch 1 train time consumed: 8.15s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.09315422341089,
                        "time": 5.875026981000701,
                        "accuracy": 0.009765625,
                        "total_cost": 52674561.12816807
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6416
Profiling... [1024/50176]	Loss: 5.7079
Profiling... [1536/50176]	Loss: 5.0798
Profiling... [2048/50176]	Loss: 5.0313
Profiling... [2560/50176]	Loss: 4.8689
Profiling... [3072/50176]	Loss: 4.8493
Profiling... [3584/50176]	Loss: 4.7371
Profiling... [4096/50176]	Loss: 4.6840
Profiling... [4608/50176]	Loss: 4.8265
Profiling... [5120/50176]	Loss: 4.6019
Profiling... [5632/50176]	Loss: 4.6271
Profiling... [6144/50176]	Loss: 4.6097
Profiling... [6656/50176]	Loss: 4.5923
Profile done
epoch 1 train time consumed: 6.43s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.12038427862862,
                        "time": 4.475126954999723,
                        "accuracy": 0.009765625,
                        "total_cost": 40123285.521036886
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6270
Profiling... [1024/50176]	Loss: 5.6526
Profiling... [1536/50176]	Loss: 5.4324
Profiling... [2048/50176]	Loss: 4.7835
Profiling... [2560/50176]	Loss: 4.7443
Profiling... [3072/50176]	Loss: 4.7849
Profiling... [3584/50176]	Loss: 4.6769
Profiling... [4096/50176]	Loss: 4.6916
Profiling... [4608/50176]	Loss: 4.6579
Profiling... [5120/50176]	Loss: 4.6705
Profiling... [5632/50176]	Loss: 4.8261
Profiling... [6144/50176]	Loss: 4.6163
Profiling... [6656/50176]	Loss: 4.6092
Profile done
epoch 1 train time consumed: 6.55s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.32463828346239,
                        "time": 4.477608854002028,
                        "accuracy": 0.009765625,
                        "total_cost": 40145584.66370373
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6521
Profiling... [1024/50176]	Loss: 5.6818
Profiling... [1536/50176]	Loss: 5.1773
Profiling... [2048/50176]	Loss: 5.0522
Profiling... [2560/50176]	Loss: 4.8588
Profiling... [3072/50176]	Loss: 4.6748
Profiling... [3584/50176]	Loss: 4.6701
Profiling... [4096/50176]	Loss: 4.7007
Profiling... [4608/50176]	Loss: 4.6061
Profiling... [5120/50176]	Loss: 4.7042
Profiling... [5632/50176]	Loss: 4.7481
Profiling... [6144/50176]	Loss: 4.5983
Profiling... [6656/50176]	Loss: 4.5934
Profile done
epoch 1 train time consumed: 6.87s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.39885245764344,
                        "time": 4.795422893999785,
                        "accuracy": 0.009765625,
                        "total_cost": 42995076.98309555
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6487
Profiling... [1024/50176]	Loss: 5.3165
Profiling... [1536/50176]	Loss: 5.4017
Profiling... [2048/50176]	Loss: 5.1298
Profiling... [2560/50176]	Loss: 4.8762
Profiling... [3072/50176]	Loss: 4.7663
Profiling... [3584/50176]	Loss: 4.7378
Profiling... [4096/50176]	Loss: 4.6953
Profiling... [4608/50176]	Loss: 4.5904
Profiling... [5120/50176]	Loss: 4.6820
Profiling... [5632/50176]	Loss: 4.6516
Profiling... [6144/50176]	Loss: 4.6424
Profiling... [6656/50176]	Loss: 4.5935
Profile done
epoch 1 train time consumed: 8.16s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.20616421445095,
                        "time": 5.84075032599867,
                        "accuracy": 0.009765625,
                        "total_cost": 52367275.86512045
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6259
Profiling... [1024/50176]	Loss: 7.2494
Profiling... [1536/50176]	Loss: 5.4007
Profiling... [2048/50176]	Loss: 5.2739
Profiling... [2560/50176]	Loss: 4.9245
Profiling... [3072/50176]	Loss: 4.7383
Profiling... [3584/50176]	Loss: 4.8058
Profiling... [4096/50176]	Loss: 5.1699
Profiling... [4608/50176]	Loss: 4.8594
Profiling... [5120/50176]	Loss: 4.9574
Profiling... [5632/50176]	Loss: 4.9218
Profiling... [6144/50176]	Loss: 4.7115
Profiling... [6656/50176]	Loss: 4.7973
Profile done
epoch 1 train time consumed: 6.53s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0104
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.20783130491154,
                        "time": 4.475021864000155,
                        "accuracy": 0.0103515625,
                        "total_cost": 37851286.158111714
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6396
Profiling... [1024/50176]	Loss: 7.6587
Profiling... [1536/50176]	Loss: 6.0149
Profiling... [2048/50176]	Loss: 5.3137
Profiling... [2560/50176]	Loss: 4.7125
Profiling... [3072/50176]	Loss: 4.6353
Profiling... [3584/50176]	Loss: 4.6497
Profiling... [4096/50176]	Loss: 4.5858
Profiling... [4608/50176]	Loss: 4.6672
Profiling... [5120/50176]	Loss: 4.7484
Profiling... [5632/50176]	Loss: 4.5986
Profiling... [6144/50176]	Loss: 4.6041
Profiling... [6656/50176]	Loss: 4.5998
Profile done
epoch 1 train time consumed: 6.43s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.40183573419843,
                        "time": 4.47446789499736,
                        "accuracy": 0.009765625,
                        "total_cost": 40117440.97104177
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6312
Profiling... [1024/50176]	Loss: 7.4396
Profiling... [1536/50176]	Loss: 5.7955
Profiling... [2048/50176]	Loss: 5.3261
Profiling... [2560/50176]	Loss: 4.8993
Profiling... [3072/50176]	Loss: 4.6681
Profiling... [3584/50176]	Loss: 4.8256
Profiling... [4096/50176]	Loss: 5.0892
Profiling... [4608/50176]	Loss: 4.7433
Profiling... [5120/50176]	Loss: 5.0291
Profiling... [5632/50176]	Loss: 5.5504
Profiling... [6144/50176]	Loss: 4.8783
Profiling... [6656/50176]	Loss: 5.1505
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 0, Average loss: 0.0155, Accuracy: 0.0107
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.47180588042941,
                        "time": 4.825083152001753,
                        "accuracy": 0.0107421875,
                        "total_cost": 39328204.222009696
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6407
Profiling... [1024/50176]	Loss: 7.6345
Profiling... [1536/50176]	Loss: 6.2795
Profiling... [2048/50176]	Loss: 5.5339
Profiling... [2560/50176]	Loss: 4.9237
Profiling... [3072/50176]	Loss: 4.8667
Profiling... [3584/50176]	Loss: 4.8394
Profiling... [4096/50176]	Loss: 4.8167
Profiling... [4608/50176]	Loss: 4.6615
Profiling... [5120/50176]	Loss: 4.8379
Profiling... [5632/50176]	Loss: 4.6799
Profiling... [6144/50176]	Loss: 4.7003
Profiling... [6656/50176]	Loss: 4.6870
Profile done
epoch 1 train time consumed: 8.16s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.28416262880998,
                        "time": 5.870236987997487,
                        "accuracy": 0.009765625,
                        "total_cost": 52631672.2185301
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6421
Profiling... [1024/50176]	Loss: 7.2469
Profiling... [1536/50176]	Loss: 5.5757
Profiling... [2048/50176]	Loss: 5.2499
Profiling... [2560/50176]	Loss: 5.2022
Profiling... [3072/50176]	Loss: 4.8281
Profiling... [3584/50176]	Loss: 4.9448
Profiling... [4096/50176]	Loss: 4.7342
Profiling... [4608/50176]	Loss: 4.7690
Profiling... [5120/50176]	Loss: 4.9313
Profiling... [5632/50176]	Loss: 4.7145
Profiling... [6144/50176]	Loss: 4.8792
Profiling... [6656/50176]	Loss: 4.6322
Profile done
epoch 1 train time consumed: 6.41s
Validation Epoch: 0, Average loss: 0.0234, Accuracy: 0.0104
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.29497726214464,
                        "time": 4.470026712999243,
                        "accuracy": 0.01044921875,
                        "total_cost": 37455698.607917026
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6520
Profiling... [1024/50176]	Loss: 7.2772
Profiling... [1536/50176]	Loss: 6.0620
Profiling... [2048/50176]	Loss: 4.9264
Profiling... [2560/50176]	Loss: 4.9372
Profiling... [3072/50176]	Loss: 4.7510
Profiling... [3584/50176]	Loss: 4.9982
Profiling... [4096/50176]	Loss: 4.7364
Profiling... [4608/50176]	Loss: 4.7015
Profiling... [5120/50176]	Loss: 4.7530
Profiling... [5632/50176]	Loss: 4.7943
Profiling... [6144/50176]	Loss: 4.6047
Profiling... [6656/50176]	Loss: 4.7955
Profile done
epoch 1 train time consumed: 6.50s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0106
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.47785443378876,
                        "time": 4.477599597998051,
                        "accuracy": 0.01064453125,
                        "total_cost": 36830767.70734508
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6724
Profiling... [1024/50176]	Loss: 7.2190
Profiling... [1536/50176]	Loss: 5.2200
Profiling... [2048/50176]	Loss: 4.9723
Profiling... [2560/50176]	Loss: 4.6940
Profiling... [3072/50176]	Loss: 5.6050
Profiling... [3584/50176]	Loss: 5.1153
Profiling... [4096/50176]	Loss: 5.1452
Profiling... [4608/50176]	Loss: 5.0593
Profiling... [5120/50176]	Loss: 4.7215
Profiling... [5632/50176]	Loss: 4.7336
Profiling... [6144/50176]	Loss: 4.8612
Profiling... [6656/50176]	Loss: 4.8683
Profile done
epoch 1 train time consumed: 6.82s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0114
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.54545050744359,
                        "time": 4.824810833000811,
                        "accuracy": 0.01142578125,
                        "total_cost": 36973163.472109236
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6217
Profiling... [1024/50176]	Loss: 7.5569
Profiling... [1536/50176]	Loss: 5.9474
Profiling... [2048/50176]	Loss: 5.2492
Profiling... [2560/50176]	Loss: 4.8941
Profiling... [3072/50176]	Loss: 4.7740
Profiling... [3584/50176]	Loss: 4.7811
Profiling... [4096/50176]	Loss: 4.6024
Profiling... [4608/50176]	Loss: 4.6940
Profiling... [5120/50176]	Loss: 4.7648
Profiling... [5632/50176]	Loss: 4.8557
Profiling... [6144/50176]	Loss: 4.7068
Profiling... [6656/50176]	Loss: 4.6128
Profile done
epoch 1 train time consumed: 8.08s
Validation Epoch: 0, Average loss: 0.0113, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.36659464836018,
                        "time": 5.85684762800156,
                        "accuracy": 0.009765625,
                        "total_cost": 52511649.92608573
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6594
Profiling... [1024/50176]	Loss: 7.5535
Profiling... [1536/50176]	Loss: 6.1248
Profiling... [2048/50176]	Loss: 5.1497
Profiling... [2560/50176]	Loss: 5.0037
Profiling... [3072/50176]	Loss: 4.8096
Profiling... [3584/50176]	Loss: 4.8142
Profiling... [4096/50176]	Loss: 4.7018
Profiling... [4608/50176]	Loss: 4.6664
Profiling... [5120/50176]	Loss: 4.6833
Profiling... [5632/50176]	Loss: 4.7472
Profiling... [6144/50176]	Loss: 4.7042
Profiling... [6656/50176]	Loss: 4.6496
Profile done
epoch 1 train time consumed: 6.42s
Validation Epoch: 0, Average loss: 0.0113, Accuracy: 0.0168
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.37601420108408,
                        "time": 4.47032311500152,
                        "accuracy": 0.016796875,
                        "total_cost": 23302484.62482905
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6531
Profiling... [1024/50176]	Loss: 7.5370
Profiling... [1536/50176]	Loss: 6.6691
Profiling... [2048/50176]	Loss: 5.1618
Profiling... [2560/50176]	Loss: 4.7974
Profiling... [3072/50176]	Loss: 4.6523
Profiling... [3584/50176]	Loss: 4.7812
Profiling... [4096/50176]	Loss: 4.6594
Profiling... [4608/50176]	Loss: 4.5856
Profiling... [5120/50176]	Loss: 4.8943
Profiling... [5632/50176]	Loss: 4.9554
Profiling... [6144/50176]	Loss: 4.8002
Profiling... [6656/50176]	Loss: 4.7552
Profile done
epoch 1 train time consumed: 6.49s
Validation Epoch: 0, Average loss: 0.0235, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.55766415102288,
                        "time": 4.4851030229983735,
                        "accuracy": 0.0099609375,
                        "total_cost": 39424342.93712348
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6528
Profiling... [1024/50176]	Loss: 7.2208
Profiling... [1536/50176]	Loss: 5.9309
Profiling... [2048/50176]	Loss: 5.3306
Profiling... [2560/50176]	Loss: 5.2724
Profiling... [3072/50176]	Loss: 4.8535
Profiling... [3584/50176]	Loss: 4.7759
Profiling... [4096/50176]	Loss: 4.9684
Profiling... [4608/50176]	Loss: 4.7611
Profiling... [5120/50176]	Loss: 4.6982
Profiling... [5632/50176]	Loss: 4.6265
Profiling... [6144/50176]	Loss: 4.7231
Profiling... [6656/50176]	Loss: 4.7400
Profile done
epoch 1 train time consumed: 6.79s
Validation Epoch: 0, Average loss: 0.0125, Accuracy: 0.0111
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.62967087905282,
                        "time": 4.8090774189986405,
                        "accuracy": 0.0111328125,
                        "total_cost": 37822419.59462327
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6792
Profiling... [1024/50176]	Loss: 7.6351
Profiling... [1536/50176]	Loss: 7.0947
Profiling... [2048/50176]	Loss: 5.3592
Profiling... [2560/50176]	Loss: 5.0849
Profiling... [3072/50176]	Loss: 4.8784
Profiling... [3584/50176]	Loss: 4.7514
Profiling... [4096/50176]	Loss: 5.0941
Profiling... [4608/50176]	Loss: 4.7663
Profiling... [5120/50176]	Loss: 4.6924
Profiling... [5632/50176]	Loss: 4.7069
Profiling... [6144/50176]	Loss: 4.6927
Profiling... [6656/50176]	Loss: 4.5918
Profile done
epoch 1 train time consumed: 8.23s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.46472458676661,
                        "time": 5.882415105999826,
                        "accuracy": 0.009765625,
                        "total_cost": 52740913.795839116
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6626
Profiling... [2048/50176]	Loss: 4.6083
Profiling... [3072/50176]	Loss: 4.6169
Profiling... [4096/50176]	Loss: 4.6390
Profiling... [5120/50176]	Loss: 4.6029
Profiling... [6144/50176]	Loss: 4.5555
Profiling... [7168/50176]	Loss: 4.5436
Profiling... [8192/50176]	Loss: 4.5507
Profiling... [9216/50176]	Loss: 4.5115
Profiling... [10240/50176]	Loss: 4.5319
Profiling... [11264/50176]	Loss: 4.5280
Profiling... [12288/50176]	Loss: 4.5228
Profiling... [13312/50176]	Loss: 4.5113
Profile done
epoch 1 train time consumed: 12.39s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.6037111548308,
                        "time": 8.736845767998602,
                        "accuracy": 0.009765625,
                        "total_cost": 78333403.35864604
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6478
Profiling... [2048/50176]	Loss: 4.6437
Profiling... [3072/50176]	Loss: 4.6348
Profiling... [4096/50176]	Loss: 4.6145
Profiling... [5120/50176]	Loss: 4.5892
Profiling... [6144/50176]	Loss: 4.5740
Profiling... [7168/50176]	Loss: 4.5937
Profiling... [8192/50176]	Loss: 4.5607
Profiling... [9216/50176]	Loss: 4.5463
Profiling... [10240/50176]	Loss: 4.5230
Profiling... [11264/50176]	Loss: 4.5276
Profiling... [12288/50176]	Loss: 4.4836
Profiling... [13312/50176]	Loss: 4.5009
Profile done
epoch 1 train time consumed: 12.48s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.9077174287799,
                        "time": 8.757505445999413,
                        "accuracy": 0.009765625,
                        "total_cost": 78518771.61016664
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6392
Profiling... [2048/50176]	Loss: 4.6368
Profiling... [3072/50176]	Loss: 4.6350
Profiling... [4096/50176]	Loss: 4.6120
Profiling... [5120/50176]	Loss: 4.5827
Profiling... [6144/50176]	Loss: 4.5455
Profiling... [7168/50176]	Loss: 4.5589
Profiling... [8192/50176]	Loss: 4.5631
Profiling... [9216/50176]	Loss: 4.5095
Profiling... [10240/50176]	Loss: 4.5316
Profiling... [11264/50176]	Loss: 4.4951
Profiling... [12288/50176]	Loss: 4.4953
Profiling... [13312/50176]	Loss: 4.4687
Profile done
epoch 1 train time consumed: 13.26s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.9795474634346,
                        "time": 9.484329633000016,
                        "accuracy": 0.009765625,
                        "total_cost": 85035427.312855
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6431
Profiling... [2048/50176]	Loss: 4.6254
Profiling... [3072/50176]	Loss: 4.6138
Profiling... [4096/50176]	Loss: 4.5928
Profiling... [5120/50176]	Loss: 4.6191
Profiling... [6144/50176]	Loss: 4.5885
Profiling... [7168/50176]	Loss: 4.5606
Profiling... [8192/50176]	Loss: 4.5450
Profiling... [9216/50176]	Loss: 4.5126
Profiling... [10240/50176]	Loss: 4.5367
Profiling... [11264/50176]	Loss: 4.5206
Profiling... [12288/50176]	Loss: 4.5133
Profiling... [13312/50176]	Loss: 4.4839
Profile done
epoch 1 train time consumed: 18.14s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.59299514201545,
                        "time": 12.31588667899996,
                        "accuracy": 0.009765625,
                        "total_cost": 110422603.93816929
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6442
Profiling... [2048/50176]	Loss: 4.6548
Profiling... [3072/50176]	Loss: 4.5970
Profiling... [4096/50176]	Loss: 4.6020
Profiling... [5120/50176]	Loss: 4.6048
Profiling... [6144/50176]	Loss: 4.5486
Profiling... [7168/50176]	Loss: 4.5487
Profiling... [8192/50176]	Loss: 4.5777
Profiling... [9216/50176]	Loss: 4.5528
Profiling... [10240/50176]	Loss: 4.5255
Profiling... [11264/50176]	Loss: 4.5290
Profiling... [12288/50176]	Loss: 4.5204
Profiling... [13312/50176]	Loss: 4.5411
Profile done
epoch 1 train time consumed: 12.35s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.73979952072158,
                        "time": 8.735300299998926,
                        "accuracy": 0.009765625,
                        "total_cost": 78319607.76217535
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6606
Profiling... [2048/50176]	Loss: 4.6255
Profiling... [3072/50176]	Loss: 4.6443
Profiling... [4096/50176]	Loss: 4.6147
Profiling... [5120/50176]	Loss: 4.5963
Profiling... [6144/50176]	Loss: 4.5674
Profiling... [7168/50176]	Loss: 4.5601
Profiling... [8192/50176]	Loss: 4.5062
Profiling... [9216/50176]	Loss: 4.5190
Profiling... [10240/50176]	Loss: 4.5286
Profiling... [11264/50176]	Loss: 4.5201
Profiling... [12288/50176]	Loss: 4.4649
Profiling... [13312/50176]	Loss: 4.4492
Profile done
epoch 1 train time consumed: 12.34s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.01219729817856,
                        "time": 8.766036290002376,
                        "accuracy": 0.009765625,
                        "total_cost": 78595305.05450109
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6468
Profiling... [2048/50176]	Loss: 4.6286
Profiling... [3072/50176]	Loss: 4.6348
Profiling... [4096/50176]	Loss: 4.6129
Profiling... [5120/50176]	Loss: 4.5908
Profiling... [6144/50176]	Loss: 4.5762
Profiling... [7168/50176]	Loss: 4.5698
Profiling... [8192/50176]	Loss: 4.5661
Profiling... [9216/50176]	Loss: 4.5412
Profiling... [10240/50176]	Loss: 4.5197
Profiling... [11264/50176]	Loss: 4.5120
Profiling... [12288/50176]	Loss: 4.5308
Profiling... [13312/50176]	Loss: 4.4870
Profile done
epoch 1 train time consumed: 13.29s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.09057755201374,
                        "time": 9.487499634000415,
                        "accuracy": 0.009765625,
                        "total_cost": 85063903.11743867
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6450
Profiling... [2048/50176]	Loss: 4.6235
Profiling... [3072/50176]	Loss: 4.6405
Profiling... [4096/50176]	Loss: 4.5945
Profiling... [5120/50176]	Loss: 4.5882
Profiling... [6144/50176]	Loss: 4.5469
Profiling... [7168/50176]	Loss: 4.5692
Profiling... [8192/50176]	Loss: 4.5389
Profiling... [9216/50176]	Loss: 4.5436
Profiling... [10240/50176]	Loss: 4.5391
Profiling... [11264/50176]	Loss: 4.5169
Profiling... [12288/50176]	Loss: 4.5357
Profiling... [13312/50176]	Loss: 4.5067
Profile done
epoch 1 train time consumed: 18.33s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.71568989927478,
                        "time": 12.356603723001172,
                        "accuracy": 0.009765625,
                        "total_cost": 110787745.16972038
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6422
Profiling... [2048/50176]	Loss: 4.6454
Profiling... [3072/50176]	Loss: 4.6138
Profiling... [4096/50176]	Loss: 4.5630
Profiling... [5120/50176]	Loss: 4.6092
Profiling... [6144/50176]	Loss: 4.5663
Profiling... [7168/50176]	Loss: 4.5535
Profiling... [8192/50176]	Loss: 4.5333
Profiling... [9216/50176]	Loss: 4.5157
Profiling... [10240/50176]	Loss: 4.4998
Profiling... [11264/50176]	Loss: 4.4946
Profiling... [12288/50176]	Loss: 4.4702
Profiling... [13312/50176]	Loss: 4.4689
Profile done
epoch 1 train time consumed: 12.44s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.8462595689297,
                        "time": 8.74776222500077,
                        "accuracy": 0.009765625,
                        "total_cost": 78431387.50188926
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6283
Profiling... [2048/50176]	Loss: 4.6348
Profiling... [3072/50176]	Loss: 4.6243
Profiling... [4096/50176]	Loss: 4.6220
Profiling... [5120/50176]	Loss: 4.5774
Profiling... [6144/50176]	Loss: 4.5615
Profiling... [7168/50176]	Loss: 4.5654
Profiling... [8192/50176]	Loss: 4.5499
Profiling... [9216/50176]	Loss: 4.5253
Profiling... [10240/50176]	Loss: 4.4929
Profiling... [11264/50176]	Loss: 4.4909
Profiling... [12288/50176]	Loss: 4.4969
Profiling... [13312/50176]	Loss: 4.5000
Profile done
epoch 1 train time consumed: 12.39s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.10054363058747,
                        "time": 8.748487742999714,
                        "accuracy": 0.009765625,
                        "total_cost": 78438006.30886988
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6307
Profiling... [2048/50176]	Loss: 4.6398
Profiling... [3072/50176]	Loss: 4.6162
Profiling... [4096/50176]	Loss: 4.6144
Profiling... [5120/50176]	Loss: 4.5902
Profiling... [6144/50176]	Loss: 4.5515
Profiling... [7168/50176]	Loss: 4.5444
Profiling... [8192/50176]	Loss: 4.5130
Profiling... [9216/50176]	Loss: 4.5273
Profiling... [10240/50176]	Loss: 4.5142
Profiling... [11264/50176]	Loss: 4.4994
Profiling... [12288/50176]	Loss: 4.4515
Profiling... [13312/50176]	Loss: 4.4869
Profile done
epoch 1 train time consumed: 13.34s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.16946911090575,
                        "time": 9.487789322000026,
                        "accuracy": 0.009765625,
                        "total_cost": 85066538.75247388
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6471
Profiling... [2048/50176]	Loss: 4.6353
Profiling... [3072/50176]	Loss: 4.6153
Profiling... [4096/50176]	Loss: 4.6341
Profiling... [5120/50176]	Loss: 4.5938
Profiling... [6144/50176]	Loss: 4.5956
Profiling... [7168/50176]	Loss: 4.5698
Profiling... [8192/50176]	Loss: 4.5473
Profiling... [9216/50176]	Loss: 4.5330
Profiling... [10240/50176]	Loss: 4.5138
Profiling... [11264/50176]	Loss: 4.4963
Profiling... [12288/50176]	Loss: 4.5290
Profiling... [13312/50176]	Loss: 4.4876
Profile done
epoch 1 train time consumed: 19.57s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.81166188869685,
                        "time": 14.461583692998829,
                        "accuracy": 0.009765625,
                        "total_cost": 129660800.2422851
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6425
Profiling... [2048/50176]	Loss: 5.4409
Profiling... [3072/50176]	Loss: 5.1374
Profiling... [4096/50176]	Loss: 4.8363
Profiling... [5120/50176]	Loss: 4.6720
Profiling... [6144/50176]	Loss: 4.7567
Profiling... [7168/50176]	Loss: 4.6927
Profiling... [8192/50176]	Loss: 4.5844
Profiling... [9216/50176]	Loss: 4.5971
Profiling... [10240/50176]	Loss: 4.5453
Profiling... [11264/50176]	Loss: 4.5322
Profiling... [12288/50176]	Loss: 4.5157
Profiling... [13312/50176]	Loss: 4.5261
Profile done
epoch 1 train time consumed: 12.35s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.92886456431823,
                        "time": 8.722305998999218,
                        "accuracy": 0.009765625,
                        "total_cost": 78203186.92094518
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6520
Profiling... [2048/50176]	Loss: 5.5415
Profiling... [3072/50176]	Loss: 5.1216
Profiling... [4096/50176]	Loss: 4.7572
Profiling... [5120/50176]	Loss: 4.7643
Profiling... [6144/50176]	Loss: 4.6549
Profiling... [7168/50176]	Loss: 4.6030
Profiling... [8192/50176]	Loss: 4.6569
Profiling... [9216/50176]	Loss: 4.6808
Profiling... [10240/50176]	Loss: 4.5994
Profiling... [11264/50176]	Loss: 4.5768
Profiling... [12288/50176]	Loss: 4.5433
Profiling... [13312/50176]	Loss: 4.5921
Profile done
epoch 1 train time consumed: 12.36s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.16512427757057,
                        "time": 8.759276488999603,
                        "accuracy": 0.009765625,
                        "total_cost": 78534766.01550683
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6311
Profiling... [2048/50176]	Loss: 5.4892
Profiling... [3072/50176]	Loss: 5.1334
Profiling... [4096/50176]	Loss: 4.7655
Profiling... [5120/50176]	Loss: 4.6864
Profiling... [6144/50176]	Loss: 4.7350
Profiling... [7168/50176]	Loss: 4.6411
Profiling... [8192/50176]	Loss: 4.6971
Profiling... [9216/50176]	Loss: 4.6430
Profiling... [10240/50176]	Loss: 4.6105
Profiling... [11264/50176]	Loss: 4.5647
Profiling... [12288/50176]	Loss: 4.5957
Profiling... [13312/50176]	Loss: 4.6442
Profile done
epoch 1 train time consumed: 13.34s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.22438814481956,
                        "time": 9.478365716000553,
                        "accuracy": 0.009765625,
                        "total_cost": 84982074.3265452
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6347
Profiling... [2048/50176]	Loss: 5.5265
Profiling... [3072/50176]	Loss: 4.9253
Profiling... [4096/50176]	Loss: 4.8587
Profiling... [5120/50176]	Loss: 4.6612
Profiling... [6144/50176]	Loss: 4.7966
Profiling... [7168/50176]	Loss: 4.7137
Profiling... [8192/50176]	Loss: 4.6433
Profiling... [9216/50176]	Loss: 4.6352
Profiling... [10240/50176]	Loss: 4.5814
Profiling... [11264/50176]	Loss: 4.5204
Profiling... [12288/50176]	Loss: 4.4849
Profiling... [13312/50176]	Loss: 4.4975
Profile done
epoch 1 train time consumed: 18.19s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.90931876369294,
                        "time": 13.055037564998202,
                        "accuracy": 0.009765625,
                        "total_cost": 117049944.03060296
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6446
Profiling... [2048/50176]	Loss: 5.5952
Profiling... [3072/50176]	Loss: 5.1604
Profiling... [4096/50176]	Loss: 4.8847
Profiling... [5120/50176]	Loss: 4.8904
Profiling... [6144/50176]	Loss: 4.7653
Profiling... [7168/50176]	Loss: 4.6950
Profiling... [8192/50176]	Loss: 4.6301
Profiling... [9216/50176]	Loss: 4.5760
Profiling... [10240/50176]	Loss: 4.5974
Profiling... [11264/50176]	Loss: 4.6363
Profiling... [12288/50176]	Loss: 4.5480
Profiling... [13312/50176]	Loss: 4.5532
Profile done
epoch 1 train time consumed: 12.40s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.01568634692875,
                        "time": 8.735791555001924,
                        "accuracy": 0.009765625,
                        "total_cost": 78324135.68956512
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6404
Profiling... [2048/50176]	Loss: 5.5265
Profiling... [3072/50176]	Loss: 5.0277
Profiling... [4096/50176]	Loss: 4.8288
Profiling... [5120/50176]	Loss: 4.7162
Profiling... [6144/50176]	Loss: 4.7739
Profiling... [7168/50176]	Loss: 4.6358
Profiling... [8192/50176]	Loss: 4.7713
Profiling... [9216/50176]	Loss: 4.6522
Profiling... [10240/50176]	Loss: 4.6324
Profiling... [11264/50176]	Loss: 4.5618
Profiling... [12288/50176]	Loss: 4.5863
Profiling... [13312/50176]	Loss: 4.5105
Profile done
epoch 1 train time consumed: 12.57s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.24531979168992,
                        "time": 8.768685806000576,
                        "accuracy": 0.009765625,
                        "total_cost": 78619164.98175827
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6499
Profiling... [2048/50176]	Loss: 5.5565
Profiling... [3072/50176]	Loss: 5.1718
Profiling... [4096/50176]	Loss: 4.8579
Profiling... [5120/50176]	Loss: 4.8534
Profiling... [6144/50176]	Loss: 4.7334
Profiling... [7168/50176]	Loss: 4.6741
Profiling... [8192/50176]	Loss: 4.5515
Profiling... [9216/50176]	Loss: 4.5520
Profiling... [10240/50176]	Loss: 4.4774
Profiling... [11264/50176]	Loss: 4.5141
Profiling... [12288/50176]	Loss: 4.5107
Profiling... [13312/50176]	Loss: 4.4033
Profile done
epoch 1 train time consumed: 13.29s
Validation Epoch: 0, Average loss: 0.0048, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.2908838570151,
                        "time": 9.481231010002375,
                        "accuracy": 0.009765625,
                        "total_cost": 85007796.54418491
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6233
Profiling... [2048/50176]	Loss: 5.6338
Profiling... [3072/50176]	Loss: 4.9705
Profiling... [4096/50176]	Loss: 4.8818
Profiling... [5120/50176]	Loss: 4.8134
Profiling... [6144/50176]	Loss: 4.7442
Profiling... [7168/50176]	Loss: 4.7336
Profiling... [8192/50176]	Loss: 4.6341
Profiling... [9216/50176]	Loss: 4.6816
Profiling... [10240/50176]	Loss: 4.5867
Profiling... [11264/50176]	Loss: 5.0248
Profiling... [12288/50176]	Loss: 4.5695
Profiling... [13312/50176]	Loss: 4.5524
Profile done
epoch 1 train time consumed: 20.08s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.94418514158971,
                        "time": 13.66158422800072,
                        "accuracy": 0.009765625,
                        "total_cost": 122488195.0498282
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6438
Profiling... [2048/50176]	Loss: 5.5111
Profiling... [3072/50176]	Loss: 5.0307
Profiling... [4096/50176]	Loss: 4.8185
Profiling... [5120/50176]	Loss: 4.7442
Profiling... [6144/50176]	Loss: 4.8231
Profiling... [7168/50176]	Loss: 4.7026
Profiling... [8192/50176]	Loss: 4.7165
Profiling... [9216/50176]	Loss: 4.6351
Profiling... [10240/50176]	Loss: 4.5704
Profiling... [11264/50176]	Loss: 4.6435
Profiling... [12288/50176]	Loss: 4.6543
Profiling... [13312/50176]	Loss: 4.5669
Profile done
epoch 1 train time consumed: 12.43s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.05490253989771,
                        "time": 8.734334024000418,
                        "accuracy": 0.009765625,
                        "total_cost": 78311085.16607772
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6582
Profiling... [2048/50176]	Loss: 5.5559
Profiling... [3072/50176]	Loss: 5.1130
Profiling... [4096/50176]	Loss: 4.8231
Profiling... [5120/50176]	Loss: 4.8256
Profiling... [6144/50176]	Loss: 4.7316
Profiling... [7168/50176]	Loss: 4.6385
Profiling... [8192/50176]	Loss: 4.6279
Profiling... [9216/50176]	Loss: 4.6173
Profiling... [10240/50176]	Loss: 4.5972
Profiling... [11264/50176]	Loss: 4.5568
Profiling... [12288/50176]	Loss: 4.5446
Profiling... [13312/50176]	Loss: 4.5200
Profile done
epoch 1 train time consumed: 12.42s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.26249641032594,
                        "time": 8.749273078999977,
                        "accuracy": 0.009765625,
                        "total_cost": 78445120.09635109
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6415
Profiling... [2048/50176]	Loss: 5.6036
Profiling... [3072/50176]	Loss: 5.1522
Profiling... [4096/50176]	Loss: 4.7579
Profiling... [5120/50176]	Loss: 4.7282
Profiling... [6144/50176]	Loss: 4.7682
Profiling... [7168/50176]	Loss: 4.6606
Profiling... [8192/50176]	Loss: 4.6407
Profiling... [9216/50176]	Loss: 4.6252
Profiling... [10240/50176]	Loss: 4.6702
Profiling... [11264/50176]	Loss: 4.6554
Profiling... [12288/50176]	Loss: 4.5966
Profiling... [13312/50176]	Loss: 4.5680
Profile done
epoch 1 train time consumed: 13.27s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.31589771597758,
                        "time": 9.489275160001853,
                        "accuracy": 0.009765625,
                        "total_cost": 85079931.76494458
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6420
Profiling... [2048/50176]	Loss: 5.5800
Profiling... [3072/50176]	Loss: 5.0699
Profiling... [4096/50176]	Loss: 4.8148
Profiling... [5120/50176]	Loss: 4.8190
Profiling... [6144/50176]	Loss: 4.6954
Profiling... [7168/50176]	Loss: 4.6266
Profiling... [8192/50176]	Loss: 4.6492
Profiling... [9216/50176]	Loss: 4.6332
Profiling... [10240/50176]	Loss: 4.6362
Profiling... [11264/50176]	Loss: 4.6669
Profiling... [12288/50176]	Loss: 4.5281
Profiling... [13312/50176]	Loss: 4.4885
Profile done
epoch 1 train time consumed: 18.90s
Validation Epoch: 0, Average loss: 0.0048, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.02582348458915,
                        "time": 13.715668335000373,
                        "accuracy": 0.009765625,
                        "total_cost": 122973164.27110092
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6351
Profiling... [2048/50176]	Loss: 7.2938
Profiling... [3072/50176]	Loss: 5.6106
Profiling... [4096/50176]	Loss: 4.8229
Profiling... [5120/50176]	Loss: 4.9741
Profiling... [6144/50176]	Loss: 5.1616
Profiling... [7168/50176]	Loss: 4.9546
Profiling... [8192/50176]	Loss: 4.7843
Profiling... [9216/50176]	Loss: 4.6539
Profiling... [10240/50176]	Loss: 4.8201
Profiling... [11264/50176]	Loss: 4.7145
Profiling... [12288/50176]	Loss: 4.6604
Profiling... [13312/50176]	Loss: 4.6925
Profile done
epoch 1 train time consumed: 12.42s
Validation Epoch: 0, Average loss: 0.0052, Accuracy: 0.0110
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.11748708876885,
                        "time": 8.728965105001407,
                        "accuracy": 0.01103515625,
                        "total_cost": 69259270.79206602
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6338
Profiling... [2048/50176]	Loss: 6.8296
Profiling... [3072/50176]	Loss: 6.2045
Profiling... [4096/50176]	Loss: 5.2632
Profiling... [5120/50176]	Loss: 5.0667
Profiling... [6144/50176]	Loss: 4.6709
Profiling... [7168/50176]	Loss: 4.7345
Profiling... [8192/50176]	Loss: 4.7372
Profiling... [9216/50176]	Loss: 4.8347
Profiling... [10240/50176]	Loss: 4.7594
Profiling... [11264/50176]	Loss: 4.6153
Profiling... [12288/50176]	Loss: 4.7275
Profiling... [13312/50176]	Loss: 4.7585
Profile done
epoch 1 train time consumed: 12.40s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.3226647781336,
                        "time": 8.759569551999448,
                        "accuracy": 0.009765625,
                        "total_cost": 78537464.24335101
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6572
Profiling... [2048/50176]	Loss: 7.2312
Profiling... [3072/50176]	Loss: 5.7022
Profiling... [4096/50176]	Loss: 4.9514
Profiling... [5120/50176]	Loss: 4.7300
Profiling... [6144/50176]	Loss: 4.8290
Profiling... [7168/50176]	Loss: 4.7766
Profiling... [8192/50176]	Loss: 4.8154
Profiling... [9216/50176]	Loss: 4.6559
Profiling... [10240/50176]	Loss: 4.6731
Profiling... [11264/50176]	Loss: 4.7614
Profiling... [12288/50176]	Loss: 4.7203
Profiling... [13312/50176]	Loss: 4.6827
Profile done
epoch 1 train time consumed: 13.38s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0104
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.36925398746278,
                        "time": 9.479574740998942,
                        "accuracy": 0.0103515625,
                        "total_cost": 80182060.97585763
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6416
Profiling... [2048/50176]	Loss: 7.2753
Profiling... [3072/50176]	Loss: 5.9968
Profiling... [4096/50176]	Loss: 5.3765
Profiling... [5120/50176]	Loss: 4.9267
Profiling... [6144/50176]	Loss: 4.9182
Profiling... [7168/50176]	Loss: 4.7340
Profiling... [8192/50176]	Loss: 4.7512
Profiling... [9216/50176]	Loss: 4.7467
Profiling... [10240/50176]	Loss: 4.7073
Profiling... [11264/50176]	Loss: 4.6764
Profiling... [12288/50176]	Loss: 4.6614
Profiling... [13312/50176]	Loss: 4.6133
Profile done
epoch 1 train time consumed: 17.58s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.10650496163866,
                        "time": 12.376066074000846,
                        "accuracy": 0.009765625,
                        "total_cost": 110962489.78744836
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6613
Profiling... [2048/50176]	Loss: 7.4125
Profiling... [3072/50176]	Loss: 6.1879
Profiling... [4096/50176]	Loss: 5.3541
Profiling... [5120/50176]	Loss: 4.9206
Profiling... [6144/50176]	Loss: 4.7184
Profiling... [7168/50176]	Loss: 4.7472
Profiling... [8192/50176]	Loss: 4.6260
Profiling... [9216/50176]	Loss: 4.7512
Profiling... [10240/50176]	Loss: 4.7204
Profiling... [11264/50176]	Loss: 4.7652
Profiling... [12288/50176]	Loss: 4.6663
Profiling... [13312/50176]	Loss: 4.6692
Profile done
epoch 1 train time consumed: 12.42s
Validation Epoch: 0, Average loss: 0.0050, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.19163010045125,
                        "time": 8.714051629998721,
                        "accuracy": 0.009765625,
                        "total_cost": 78129296.43836495
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6499
Profiling... [2048/50176]	Loss: 7.4976
Profiling... [3072/50176]	Loss: 5.9121
Profiling... [4096/50176]	Loss: 4.9742
Profiling... [5120/50176]	Loss: 4.7713
Profiling... [6144/50176]	Loss: 4.9160
Profiling... [7168/50176]	Loss: 4.6483
Profiling... [8192/50176]	Loss: 4.8816
Profiling... [9216/50176]	Loss: 4.8559
Profiling... [10240/50176]	Loss: 5.1960
Profiling... [11264/50176]	Loss: 4.9333
Profiling... [12288/50176]	Loss: 4.8344
Profiling... [13312/50176]	Loss: 4.7600
Profile done
epoch 1 train time consumed: 12.48s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0125
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.38133948950502,
                        "time": 8.766554174999328,
                        "accuracy": 0.0125,
                        "total_cost": 61406339.095532045
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6338
Profiling... [2048/50176]	Loss: 7.3183
Profiling... [3072/50176]	Loss: 6.1751
Profiling... [4096/50176]	Loss: 4.9570
Profiling... [5120/50176]	Loss: 4.8637
Profiling... [6144/50176]	Loss: 4.7913
Profiling... [7168/50176]	Loss: 4.7043
Profiling... [8192/50176]	Loss: 4.6633
Profiling... [9216/50176]	Loss: 4.6384
Profiling... [10240/50176]	Loss: 4.7518
Profiling... [11264/50176]	Loss: 4.6834
Profiling... [12288/50176]	Loss: 4.7552
Profiling... [13312/50176]	Loss: 4.7982
Profile done
epoch 1 train time consumed: 13.27s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.41957280681135,
                        "time": 9.488577717002045,
                        "accuracy": 0.00986328125,
                        "total_cost": 84231414.77700427
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6467
Profiling... [2048/50176]	Loss: 7.3407
Profiling... [3072/50176]	Loss: 6.1418
Profiling... [4096/50176]	Loss: 5.4899
Profiling... [5120/50176]	Loss: 5.0292
Profiling... [6144/50176]	Loss: 4.7812
Profiling... [7168/50176]	Loss: 4.6708
Profiling... [8192/50176]	Loss: 4.6088
Profiling... [9216/50176]	Loss: 4.7104
Profiling... [10240/50176]	Loss: 4.6110
Profiling... [11264/50176]	Loss: 4.6106
Profiling... [12288/50176]	Loss: 4.6421
Profiling... [13312/50176]	Loss: 4.6226
Profile done
epoch 1 train time consumed: 18.16s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.16291020441656,
                        "time": 13.04065490100038,
                        "accuracy": 0.009765625,
                        "total_cost": 116921160.06115517
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6498
Profiling... [2048/50176]	Loss: 7.2050
Profiling... [3072/50176]	Loss: 5.9076
Profiling... [4096/50176]	Loss: 5.0987
Profiling... [5120/50176]	Loss: 5.0236
Profiling... [6144/50176]	Loss: 4.8038
Profiling... [7168/50176]	Loss: 4.7293
Profiling... [8192/50176]	Loss: 4.8112
Profiling... [9216/50176]	Loss: 4.7215
Profiling... [10240/50176]	Loss: 4.8067
Profiling... [11264/50176]	Loss: 4.7634
Profiling... [12288/50176]	Loss: 4.8842
Profiling... [13312/50176]	Loss: 4.6397
Profile done
epoch 1 train time consumed: 12.48s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0109
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.24542559155587,
                        "time": 8.71835748599915,
                        "accuracy": 0.0109375,
                        "total_cost": 69792791.35400158
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6380
Profiling... [2048/50176]	Loss: 7.4073
Profiling... [3072/50176]	Loss: 5.3095
Profiling... [4096/50176]	Loss: 5.1690
Profiling... [5120/50176]	Loss: 5.0035
Profiling... [6144/50176]	Loss: 4.7791
Profiling... [7168/50176]	Loss: 4.8563
Profiling... [8192/50176]	Loss: 4.7291
Profiling... [9216/50176]	Loss: 4.6456
Profiling... [10240/50176]	Loss: 4.7656
Profiling... [11264/50176]	Loss: 4.7541
Profiling... [12288/50176]	Loss: 4.6743
Profiling... [13312/50176]	Loss: 4.6110
Profile done
epoch 1 train time consumed: 12.40s
Validation Epoch: 0, Average loss: 0.0049, Accuracy: 0.0108
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.42437309456317,
                        "time": 8.760340484001063,
                        "accuracy": 0.01083984375,
                        "total_cost": 70760740.50922768
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6479
Profiling... [2048/50176]	Loss: 7.1725
Profiling... [3072/50176]	Loss: 5.7381
Profiling... [4096/50176]	Loss: 5.1924
Profiling... [5120/50176]	Loss: 4.8628
Profiling... [6144/50176]	Loss: 4.7648
Profiling... [7168/50176]	Loss: 4.6885
Profiling... [8192/50176]	Loss: 4.7507
Profiling... [9216/50176]	Loss: 4.9228
Profiling... [10240/50176]	Loss: 4.7233
Profiling... [11264/50176]	Loss: 4.7972
Profiling... [12288/50176]	Loss: 4.7381
Profiling... [13312/50176]	Loss: 4.6891
Profile done
epoch 1 train time consumed: 13.37s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0101
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.46128359541918,
                        "time": 9.469301039000129,
                        "accuracy": 0.01005859375,
                        "total_cost": 82428073.97792245
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6504
Profiling... [2048/50176]	Loss: 7.5040
Profiling... [3072/50176]	Loss: 5.3073
Profiling... [4096/50176]	Loss: 5.0444
Profiling... [5120/50176]	Loss: 4.8178
Profiling... [6144/50176]	Loss: 4.7637
Profiling... [7168/50176]	Loss: 4.7269
Profiling... [8192/50176]	Loss: 4.6150
Profiling... [9216/50176]	Loss: 5.2051
Profiling... [10240/50176]	Loss: 5.0129
Profiling... [11264/50176]	Loss: 4.7298
Profiling... [12288/50176]	Loss: 4.7631
Profiling... [13312/50176]	Loss: 4.6872
Profile done
epoch 1 train time consumed: 17.52s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0095
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.21726801910931,
                        "time": 12.374659426997823,
                        "accuracy": 0.00947265625,
                        "total_cost": 114381389.81220101
                    },
                    
[Training Loop] The optimal parameters are lr: 0.01 dr: 0.5 bs: 256 pl: 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[GPU_0] Set GPU power limit to 100W.
[GPU_0] Set GPU power limit to 100W.
Training Epoch: 0 [256/50176]	Loss: 4.6760
Training Epoch: 0 [512/50176]	Loss: 7.4164
Training Epoch: 0 [768/50176]	Loss: 6.7747
Training Epoch: 0 [1024/50176]	Loss: 5.6055
Training Epoch: 0 [1280/50176]	Loss: 4.9155
Training Epoch: 0 [1536/50176]	Loss: 4.9554
Training Epoch: 0 [1792/50176]	Loss: 5.1352
Training Epoch: 0 [2048/50176]	Loss: 5.9413
Training Epoch: 0 [2304/50176]	Loss: 4.8774
Training Epoch: 0 [2560/50176]	Loss: 4.8660
Training Epoch: 0 [2816/50176]	Loss: 5.1200
Training Epoch: 0 [3072/50176]	Loss: 5.2695
Training Epoch: 0 [3328/50176]	Loss: 4.9159
Training Epoch: 0 [3584/50176]	Loss: 4.8838
Training Epoch: 0 [3840/50176]	Loss: 5.5126
Training Epoch: 0 [4096/50176]	Loss: 4.8168
Training Epoch: 0 [4352/50176]	Loss: 4.6804
Training Epoch: 0 [4608/50176]	Loss: 4.7592
Training Epoch: 0 [4864/50176]	Loss: 5.2859
Training Epoch: 0 [5120/50176]	Loss: 4.8183
Training Epoch: 0 [5376/50176]	Loss: 4.6510
Training Epoch: 0 [5632/50176]	Loss: 4.6973
Training Epoch: 0 [5888/50176]	Loss: 4.6646
Training Epoch: 0 [6144/50176]	Loss: 4.6649
Training Epoch: 0 [6400/50176]	Loss: 4.6853
Training Epoch: 0 [6656/50176]	Loss: 4.6967
Training Epoch: 0 [6912/50176]	Loss: 4.6335
Training Epoch: 0 [7168/50176]	Loss: 4.6455
Training Epoch: 0 [7424/50176]	Loss: 4.6285
Training Epoch: 0 [7680/50176]	Loss: 4.6106
Training Epoch: 0 [7936/50176]	Loss: 4.6181
Training Epoch: 0 [8192/50176]	Loss: 4.6039
Training Epoch: 0 [8448/50176]	Loss: 4.6174
Training Epoch: 0 [8704/50176]	Loss: 4.6082
Training Epoch: 0 [8960/50176]	Loss: 4.6039
Training Epoch: 0 [9216/50176]	Loss: 4.6162
Training Epoch: 0 [9472/50176]	Loss: 4.6002
Training Epoch: 0 [9728/50176]	Loss: 4.6059
Training Epoch: 0 [9984/50176]	Loss: 4.6047
Training Epoch: 0 [10240/50176]	Loss: 4.6036
Training Epoch: 0 [10496/50176]	Loss: 4.5987
Training Epoch: 0 [10752/50176]	Loss: 4.6047
Training Epoch: 0 [11008/50176]	Loss: 4.5836
Training Epoch: 0 [11264/50176]	Loss: 4.5925
Training Epoch: 0 [11520/50176]	Loss: 4.5816
Training Epoch: 0 [11776/50176]	Loss: 4.5906
Training Epoch: 0 [12032/50176]	Loss: 4.5872
Training Epoch: 0 [12288/50176]	Loss: 4.5717
Training Epoch: 0 [12544/50176]	Loss: 4.5970
Training Epoch: 0 [12800/50176]	Loss: 4.5778
Training Epoch: 0 [13056/50176]	Loss: 4.5511
Training Epoch: 0 [13312/50176]	Loss: 4.5751
Training Epoch: 0 [13568/50176]	Loss: 4.5609
Training Epoch: 0 [13824/50176]	Loss: 4.5641
Training Epoch: 0 [14080/50176]	Loss: 4.5755
Training Epoch: 0 [14336/50176]	Loss: 4.5468
Training Epoch: 0 [14592/50176]	Loss: 4.5672
Training Epoch: 0 [14848/50176]	Loss: 4.5655
Training Epoch: 0 [15104/50176]	Loss: 4.5457
Training Epoch: 0 [15360/50176]	Loss: 4.5672
Training Epoch: 0 [15616/50176]	Loss: 4.5524
Training Epoch: 0 [15872/50176]	Loss: 4.5713
Training Epoch: 0 [16128/50176]	Loss: 4.5143
Training Epoch: 0 [16384/50176]	Loss: 4.5749
Training Epoch: 0 [16640/50176]	Loss: 4.5606
Training Epoch: 0 [16896/50176]	Loss: 4.5465
Training Epoch: 0 [17152/50176]	Loss: 4.5074
Training Epoch: 0 [17408/50176]	Loss: 4.5603
Training Epoch: 0 [17664/50176]	Loss: 4.5244
Training Epoch: 0 [17920/50176]	Loss: 4.5423
Training Epoch: 0 [18176/50176]	Loss: 4.4794
Training Epoch: 0 [18432/50176]	Loss: 4.4984
Training Epoch: 0 [18688/50176]	Loss: 4.5601
Training Epoch: 0 [18944/50176]	Loss: 4.5050
Training Epoch: 0 [19200/50176]	Loss: 4.5275
Training Epoch: 0 [19456/50176]	Loss: 4.5238
Training Epoch: 0 [19712/50176]	Loss: 4.5150
Training Epoch: 0 [19968/50176]	Loss: 4.5019
Training Epoch: 0 [20224/50176]	Loss: 4.4885
Training Epoch: 0 [20480/50176]	Loss: 4.4618
Training Epoch: 0 [20736/50176]	Loss: 4.4821
Training Epoch: 0 [20992/50176]	Loss: 4.4731
Training Epoch: 0 [21248/50176]	Loss: 4.5109
Training Epoch: 0 [21504/50176]	Loss: 4.4881
Training Epoch: 0 [21760/50176]	Loss: 4.4936
Training Epoch: 0 [22016/50176]	Loss: 4.4683
Training Epoch: 0 [22272/50176]	Loss: 4.4346
Training Epoch: 0 [22528/50176]	Loss: 4.4709
Training Epoch: 0 [22784/50176]	Loss: 4.3543
Training Epoch: 0 [23040/50176]	Loss: 4.4958
Training Epoch: 0 [23296/50176]	Loss: 4.4054
Training Epoch: 0 [23552/50176]	Loss: 4.4013
Training Epoch: 0 [23808/50176]	Loss: 4.4166
Training Epoch: 0 [24064/50176]	Loss: 4.3911
Training Epoch: 0 [24320/50176]	Loss: 4.3309
Training Epoch: 0 [24576/50176]	Loss: 4.3299
Training Epoch: 0 [24832/50176]	Loss: 4.3219
Training Epoch: 0 [25088/50176]	Loss: 4.3187
Training Epoch: 0 [25344/50176]	Loss: 4.3878
Training Epoch: 0 [25600/50176]	Loss: 4.2889
Training Epoch: 0 [25856/50176]	Loss: 4.2942
Training Epoch: 0 [26112/50176]	Loss: 4.2425
Training Epoch: 0 [26368/50176]	Loss: 4.3709
Training Epoch: 0 [26624/50176]	Loss: 4.2905
Training Epoch: 0 [26880/50176]	Loss: 4.2781
Training Epoch: 0 [27136/50176]	Loss: 4.2545
Training Epoch: 0 [27392/50176]	Loss: 4.2434
Training Epoch: 0 [27648/50176]	Loss: 4.2025
Training Epoch: 0 [27904/50176]	Loss: 4.1701
Training Epoch: 0 [28160/50176]	Loss: 4.1862
Training Epoch: 0 [28416/50176]	Loss: 4.1080
Training Epoch: 0 [28672/50176]	Loss: 4.1991
Training Epoch: 0 [28928/50176]	Loss: 4.1850
Training Epoch: 0 [29184/50176]	Loss: 4.2120
Training Epoch: 0 [29440/50176]	Loss: 4.3121
Training Epoch: 0 [29696/50176]	Loss: 4.2171
Training Epoch: 0 [29952/50176]	Loss: 4.1856
Training Epoch: 0 [30208/50176]	Loss: 4.0789
Training Epoch: 0 [30464/50176]	Loss: 4.0688
Training Epoch: 0 [30720/50176]	Loss: 4.3346
Training Epoch: 0 [30976/50176]	Loss: 4.2533
Training Epoch: 0 [31232/50176]	Loss: 4.1922
Training Epoch: 0 [31488/50176]	Loss: 4.2742
Training Epoch: 0 [31744/50176]	Loss: 4.1707
Training Epoch: 0 [32000/50176]	Loss: 4.1350
Training Epoch: 0 [32256/50176]	Loss: 4.1364
Training Epoch: 0 [32512/50176]	Loss: 4.0523
Training Epoch: 0 [32768/50176]	Loss: 4.1011
Training Epoch: 0 [33024/50176]	Loss: 4.1917
Training Epoch: 0 [33280/50176]	Loss: 4.0448
Training Epoch: 0 [33536/50176]	Loss: 4.1482
Training Epoch: 0 [33792/50176]	Loss: 4.1262
Training Epoch: 0 [34048/50176]	Loss: 4.1772
Training Epoch: 0 [34304/50176]	Loss: 4.1168
Training Epoch: 0 [34560/50176]	Loss: 3.9932
Training Epoch: 0 [34816/50176]	Loss: 4.0013
Training Epoch: 0 [35072/50176]	Loss: 4.1160
Training Epoch: 0 [35328/50176]	Loss: 4.1686
Training Epoch: 0 [35584/50176]	Loss: 4.2328
Training Epoch: 0 [35840/50176]	Loss: 4.1364
Training Epoch: 0 [36096/50176]	Loss: 4.1373
Training Epoch: 0 [36352/50176]	Loss: 4.1376
Training Epoch: 0 [36608/50176]	Loss: 4.1971
Training Epoch: 0 [36864/50176]	Loss: 3.9881
Training Epoch: 0 [37120/50176]	Loss: 4.0446
Training Epoch: 0 [37376/50176]	Loss: 4.0986
Training Epoch: 0 [37632/50176]	Loss: 4.1167
Training Epoch: 0 [37888/50176]	Loss: 4.0219
Training Epoch: 0 [38144/50176]	Loss: 4.2897
Training Epoch: 0 [38400/50176]	Loss: 4.0140
Training Epoch: 0 [38656/50176]	Loss: 4.0600
Training Epoch: 0 [38912/50176]	Loss: 3.9839
Training Epoch: 0 [39168/50176]	Loss: 4.1018
Training Epoch: 0 [39424/50176]	Loss: 3.9675
Training Epoch: 0 [39680/50176]	Loss: 4.1291
Training Epoch: 0 [39936/50176]	Loss: 4.0863
Training Epoch: 0 [40192/50176]	Loss: 4.0058
Training Epoch: 0 [40448/50176]	Loss: 4.1066
Training Epoch: 0 [40704/50176]	Loss: 4.0534
Training Epoch: 0 [40960/50176]	Loss: 4.0170
Training Epoch: 0 [41216/50176]	Loss: 4.0504
Training Epoch: 0 [41472/50176]	Loss: 3.9571
Training Epoch: 0 [41728/50176]	Loss: 3.9807
Training Epoch: 0 [41984/50176]	Loss: 4.0245
Training Epoch: 0 [42240/50176]	Loss: 4.0234
Training Epoch: 0 [42496/50176]	Loss: 3.8955
Training Epoch: 0 [42752/50176]	Loss: 4.0343
Training Epoch: 0 [43008/50176]	Loss: 4.0112
Training Epoch: 0 [43264/50176]	Loss: 4.0208
Training Epoch: 0 [43520/50176]	Loss: 4.0479
Training Epoch: 0 [43776/50176]	Loss: 3.9389
Training Epoch: 0 [44032/50176]	Loss: 3.9690
Training Epoch: 0 [44288/50176]	Loss: 3.9945
Training Epoch: 0 [44544/50176]	Loss: 4.0491
Training Epoch: 0 [44800/50176]	Loss: 4.0732
Training Epoch: 0 [45056/50176]	Loss: 3.9804
Training Epoch: 0 [45312/50176]	Loss: 4.0263
Training Epoch: 0 [45568/50176]	Loss: 3.9212
Training Epoch: 0 [45824/50176]	Loss: 3.9612
Training Epoch: 0 [46080/50176]	Loss: 4.0453
Training Epoch: 0 [46336/50176]	Loss: 4.0598
Training Epoch: 0 [46592/50176]	Loss: 3.9156
Training Epoch: 0 [46848/50176]	Loss: 4.1498
Training Epoch: 0 [47104/50176]	Loss: 4.0470
Training Epoch: 0 [47360/50176]	Loss: 3.9586
Training Epoch: 0 [47616/50176]	Loss: 3.9826
Training Epoch: 0 [47872/50176]	Loss: 3.9910
Training Epoch: 0 [48128/50176]	Loss: 3.8858
Training Epoch: 0 [48384/50176]	Loss: 3.9434
Training Epoch: 0 [48640/50176]	Loss: 3.9510
Training Epoch: 0 [48896/50176]	Loss: 3.9252
Training Epoch: 0 [49152/50176]	Loss: 3.8577
Training Epoch: 0 [49408/50176]	Loss: 3.9821
Training Epoch: 0 [49664/50176]	Loss: 3.9115
Training Epoch: 0 [49920/50176]	Loss: 3.9134
Training Epoch: 0 [50176/50176]	Loss: 3.9491
Validation Epoch: 0, Average loss: 0.0162, Accuracy: 0.0733
Training Epoch: 1 [256/50176]	Loss: 3.8592
Training Epoch: 1 [512/50176]	Loss: 4.0765
Training Epoch: 1 [768/50176]	Loss: 3.9217
Training Epoch: 1 [1024/50176]	Loss: 3.8651
Training Epoch: 1 [1280/50176]	Loss: 3.7651
Training Epoch: 1 [1536/50176]	Loss: 3.9086
Training Epoch: 1 [1792/50176]	Loss: 3.9663
Training Epoch: 1 [2048/50176]	Loss: 4.0070
Training Epoch: 1 [2304/50176]	Loss: 3.9096
Training Epoch: 1 [2560/50176]	Loss: 3.9748
Training Epoch: 1 [2816/50176]	Loss: 3.9502
Training Epoch: 1 [3072/50176]	Loss: 3.7916
Training Epoch: 1 [3328/50176]	Loss: 3.9232
Training Epoch: 1 [3584/50176]	Loss: 3.8087
Training Epoch: 1 [3840/50176]	Loss: 3.9307
Training Epoch: 1 [4096/50176]	Loss: 3.8667
Training Epoch: 1 [4352/50176]	Loss: 3.9126
Training Epoch: 1 [4608/50176]	Loss: 3.8865
Training Epoch: 1 [4864/50176]	Loss: 3.8804
Training Epoch: 1 [5120/50176]	Loss: 3.8300
Training Epoch: 1 [5376/50176]	Loss: 3.7519
Training Epoch: 1 [5632/50176]	Loss: 3.8176
Training Epoch: 1 [5888/50176]	Loss: 3.8344
Training Epoch: 1 [6144/50176]	Loss: 3.7193
Training Epoch: 1 [6400/50176]	Loss: 3.9770
Training Epoch: 1 [6656/50176]	Loss: 3.7949
Training Epoch: 1 [6912/50176]	Loss: 3.9989
Training Epoch: 1 [7168/50176]	Loss: 3.8480
Training Epoch: 1 [7424/50176]	Loss: 3.8637
Training Epoch: 1 [7680/50176]	Loss: 3.8372
Training Epoch: 1 [7936/50176]	Loss: 3.8696
Training Epoch: 1 [8192/50176]	Loss: 3.8699
Training Epoch: 1 [8448/50176]	Loss: 3.6971
Training Epoch: 1 [8704/50176]	Loss: 3.9278
Training Epoch: 1 [8960/50176]	Loss: 3.9761
Training Epoch: 1 [9216/50176]	Loss: 3.9029
Training Epoch: 1 [9472/50176]	Loss: 3.9858
Training Epoch: 1 [9728/50176]	Loss: 3.8017
Training Epoch: 1 [9984/50176]	Loss: 3.8102
Training Epoch: 1 [10240/50176]	Loss: 3.8268
Training Epoch: 1 [10496/50176]	Loss: 3.7886
Training Epoch: 1 [10752/50176]	Loss: 3.8088
Training Epoch: 1 [11008/50176]	Loss: 3.8597
Training Epoch: 1 [11264/50176]	Loss: 3.7407
Training Epoch: 1 [11520/50176]	Loss: 3.8010
Training Epoch: 1 [11776/50176]	Loss: 3.6511
Training Epoch: 1 [12032/50176]	Loss: 3.7756
Training Epoch: 1 [12288/50176]	Loss: 3.6681
Training Epoch: 1 [12544/50176]	Loss: 3.8706
Training Epoch: 1 [12800/50176]	Loss: 3.8728
Training Epoch: 1 [13056/50176]	Loss: 3.7930
Training Epoch: 1 [13312/50176]	Loss: 3.7761
Training Epoch: 1 [13568/50176]	Loss: 3.7304
Training Epoch: 1 [13824/50176]	Loss: 3.8073
Training Epoch: 1 [14080/50176]	Loss: 3.8115
Training Epoch: 1 [14336/50176]	Loss: 3.8871
Training Epoch: 1 [14592/50176]	Loss: 3.8253
Training Epoch: 1 [14848/50176]	Loss: 3.8049
Training Epoch: 1 [15104/50176]	Loss: 3.8123
Training Epoch: 1 [15360/50176]	Loss: 3.8496
Training Epoch: 1 [15616/50176]	Loss: 3.7063
Training Epoch: 1 [15872/50176]	Loss: 3.6954
Training Epoch: 1 [16128/50176]	Loss: 3.8842
Training Epoch: 1 [16384/50176]	Loss: 3.7457
Training Epoch: 1 [16640/50176]	Loss: 3.7322
Training Epoch: 1 [16896/50176]	Loss: 3.8062
Training Epoch: 1 [17152/50176]	Loss: 3.6420
Training Epoch: 1 [17408/50176]	Loss: 3.6676
Training Epoch: 1 [17664/50176]	Loss: 3.6517
Training Epoch: 1 [17920/50176]	Loss: 3.7442
Training Epoch: 1 [18176/50176]	Loss: 3.8261
Training Epoch: 1 [18432/50176]	Loss: 3.7502
Training Epoch: 1 [18688/50176]	Loss: 3.6987
Training Epoch: 1 [18944/50176]	Loss: 3.6840
Training Epoch: 1 [19200/50176]	Loss: 3.7246
Training Epoch: 1 [19456/50176]	Loss: 3.7023
Training Epoch: 1 [19712/50176]	Loss: 3.8135
Training Epoch: 1 [19968/50176]	Loss: 3.7860
Training Epoch: 1 [20224/50176]	Loss: 3.7225
Training Epoch: 1 [20480/50176]	Loss: 3.6733
Training Epoch: 1 [20736/50176]	Loss: 3.8453
Training Epoch: 1 [20992/50176]	Loss: 3.7572
Training Epoch: 1 [21248/50176]	Loss: 3.7515
Training Epoch: 1 [21504/50176]	Loss: 3.6739
Training Epoch: 1 [21760/50176]	Loss: 3.8025
Training Epoch: 1 [22016/50176]	Loss: 3.6950
Training Epoch: 1 [22272/50176]	Loss: 3.6713
Training Epoch: 1 [22528/50176]	Loss: 3.6759
Training Epoch: 1 [22784/50176]	Loss: 3.7549
Training Epoch: 1 [23040/50176]	Loss: 3.7748
Training Epoch: 1 [23296/50176]	Loss: 3.7277
Training Epoch: 1 [23552/50176]	Loss: 3.8236
Training Epoch: 1 [23808/50176]	Loss: 3.6971
Training Epoch: 1 [24064/50176]	Loss: 3.8410
Training Epoch: 1 [24320/50176]	Loss: 3.7369
Training Epoch: 1 [24576/50176]	Loss: 3.6488
Training Epoch: 1 [24832/50176]	Loss: 3.8640
Training Epoch: 1 [25088/50176]	Loss: 3.6350
Training Epoch: 1 [25344/50176]	Loss: 3.6382
Training Epoch: 1 [25600/50176]	Loss: 3.7553
Training Epoch: 1 [25856/50176]	Loss: 3.6387
Training Epoch: 1 [26112/50176]	Loss: 3.7178
Training Epoch: 1 [26368/50176]	Loss: 3.5940
Training Epoch: 1 [26624/50176]	Loss: 3.6147
Training Epoch: 1 [26880/50176]	Loss: 3.7455
Training Epoch: 1 [27136/50176]	Loss: 3.7027
Training Epoch: 1 [27392/50176]	Loss: 3.6659
Training Epoch: 1 [27648/50176]	Loss: 3.7710
Training Epoch: 1 [27904/50176]	Loss: 3.7218
Training Epoch: 1 [28160/50176]	Loss: 3.7092
Training Epoch: 1 [28416/50176]	Loss: 3.5074
Training Epoch: 1 [28672/50176]	Loss: 3.5784
Training Epoch: 1 [28928/50176]	Loss: 3.6440
Training Epoch: 1 [29184/50176]	Loss: 3.8296
Training Epoch: 1 [29440/50176]	Loss: 3.5479
Training Epoch: 1 [29696/50176]	Loss: 3.5772
Training Epoch: 1 [29952/50176]	Loss: 3.6519
Training Epoch: 1 [30208/50176]	Loss: 3.6275
Training Epoch: 1 [30464/50176]	Loss: 3.7456
Training Epoch: 1 [30720/50176]	Loss: 3.6801
Training Epoch: 1 [30976/50176]	Loss: 3.7400
Training Epoch: 1 [31232/50176]	Loss: 3.6375
Training Epoch: 1 [31488/50176]	Loss: 3.8240
Training Epoch: 1 [31744/50176]	Loss: 3.6899
Training Epoch: 1 [32000/50176]	Loss: 3.6838
Training Epoch: 1 [32256/50176]	Loss: 3.7034
Training Epoch: 1 [32512/50176]	Loss: 3.5435
Training Epoch: 1 [32768/50176]	Loss: 3.7286
Training Epoch: 1 [33024/50176]	Loss: 3.7857
Training Epoch: 1 [33280/50176]	Loss: 3.6897
Training Epoch: 1 [33536/50176]	Loss: 3.8105
Training Epoch: 1 [33792/50176]	Loss: 3.6464
Training Epoch: 1 [34048/50176]	Loss: 3.5432
Training Epoch: 1 [34304/50176]	Loss: 3.5644
Training Epoch: 1 [34560/50176]	Loss: 3.6476
Training Epoch: 1 [34816/50176]	Loss: 3.6105
Training Epoch: 1 [35072/50176]	Loss: 3.5172
Training Epoch: 1 [35328/50176]	Loss: 3.6043
Training Epoch: 1 [35584/50176]	Loss: 3.6439
Training Epoch: 1 [35840/50176]	Loss: 3.6107
Training Epoch: 1 [36096/50176]	Loss: 3.5439
Training Epoch: 1 [36352/50176]	Loss: 3.4974
Training Epoch: 1 [36608/50176]	Loss: 3.6408
Training Epoch: 1 [36864/50176]	Loss: 3.6830
Training Epoch: 1 [37120/50176]	Loss: 3.7733
Training Epoch: 1 [37376/50176]	Loss: 3.5851
Training Epoch: 1 [37632/50176]	Loss: 3.5807
Training Epoch: 1 [37888/50176]	Loss: 3.5971
Training Epoch: 1 [38144/50176]	Loss: 3.4722
Training Epoch: 1 [38400/50176]	Loss: 3.4388
Training Epoch: 1 [38656/50176]	Loss: 3.4561
Training Epoch: 1 [38912/50176]	Loss: 3.6436
Training Epoch: 1 [39168/50176]	Loss: 3.6330
Training Epoch: 1 [39424/50176]	Loss: 3.5144
Training Epoch: 1 [39680/50176]	Loss: 3.4330
Training Epoch: 1 [39936/50176]	Loss: 3.5842
Training Epoch: 1 [40192/50176]	Loss: 3.8156
Training Epoch: 1 [40448/50176]	Loss: 3.5680
Training Epoch: 1 [40704/50176]	Loss: 3.5591
Training Epoch: 1 [40960/50176]	Loss: 3.5776
Training Epoch: 1 [41216/50176]	Loss: 3.5794
Training Epoch: 1 [41472/50176]	Loss: 3.5088
Training Epoch: 1 [41728/50176]	Loss: 3.5402
Training Epoch: 1 [41984/50176]	Loss: 3.5438
Training Epoch: 1 [42240/50176]	Loss: 3.5869
Training Epoch: 1 [42496/50176]	Loss: 3.5772
Training Epoch: 1 [42752/50176]	Loss: 3.5652
Training Epoch: 1 [43008/50176]	Loss: 3.5118
Training Epoch: 1 [43264/50176]	Loss: 3.6248
Training Epoch: 1 [43520/50176]	Loss: 3.6430
Training Epoch: 1 [43776/50176]	Loss: 3.6627
Training Epoch: 1 [44032/50176]	Loss: 3.6798
Training Epoch: 1 [44288/50176]	Loss: 3.5603
Training Epoch: 1 [44544/50176]	Loss: 3.6361
Training Epoch: 1 [44800/50176]	Loss: 3.5346
Training Epoch: 1 [45056/50176]	Loss: 3.6907
Training Epoch: 1 [45312/50176]	Loss: 3.5106
Training Epoch: 1 [45568/50176]	Loss: 3.5217
Training Epoch: 1 [45824/50176]	Loss: 3.4985
Training Epoch: 1 [46080/50176]	Loss: 3.6643
Training Epoch: 1 [46336/50176]	Loss: 3.4585
Training Epoch: 1 [46592/50176]	Loss: 3.4520
Training Epoch: 1 [46848/50176]	Loss: 3.5571
Training Epoch: 1 [47104/50176]	Loss: 3.4138
Training Epoch: 1 [47360/50176]	Loss: 3.5019
Training Epoch: 1 [47616/50176]	Loss: 3.6402
Training Epoch: 1 [47872/50176]	Loss: 3.6024
Training Epoch: 1 [48128/50176]	Loss: 3.6695
Training Epoch: 1 [48384/50176]	Loss: 3.4797
Training Epoch: 1 [48640/50176]	Loss: 3.4474
Training Epoch: 1 [48896/50176]	Loss: 3.4722
Training Epoch: 1 [49152/50176]	Loss: 3.5183
Training Epoch: 1 [49408/50176]	Loss: 3.6759
Training Epoch: 1 [49664/50176]	Loss: 3.5707
Training Epoch: 1 [49920/50176]	Loss: 3.4119
Training Epoch: 1 [50176/50176]	Loss: 3.3498
Validation Epoch: 1, Average loss: 0.0136, Accuracy: 0.1557
Training Epoch: 2 [256/50176]	Loss: 3.4403
Training Epoch: 2 [512/50176]	Loss: 3.5078
Training Epoch: 2 [768/50176]	Loss: 3.4899
Training Epoch: 2 [1024/50176]	Loss: 3.5666
Training Epoch: 2 [1280/50176]	Loss: 3.5494
Training Epoch: 2 [1536/50176]	Loss: 3.4596
Training Epoch: 2 [1792/50176]	Loss: 3.4158
Training Epoch: 2 [2048/50176]	Loss: 3.5164
Training Epoch: 2 [2304/50176]	Loss: 3.4121
Training Epoch: 2 [2560/50176]	Loss: 3.3941
Training Epoch: 2 [2816/50176]	Loss: 3.4113
Training Epoch: 2 [3072/50176]	Loss: 3.4868
Training Epoch: 2 [3328/50176]	Loss: 3.3962
Training Epoch: 2 [3584/50176]	Loss: 3.4437
Training Epoch: 2 [3840/50176]	Loss: 3.5814
Training Epoch: 2 [4096/50176]	Loss: 3.5111
Training Epoch: 2 [4352/50176]	Loss: 3.4910
Training Epoch: 2 [4608/50176]	Loss: 3.4906
Training Epoch: 2 [4864/50176]	Loss: 3.3942
Training Epoch: 2 [5120/50176]	Loss: 3.3572
Training Epoch: 2 [5376/50176]	Loss: 3.5154
Training Epoch: 2 [5632/50176]	Loss: 3.3329
Training Epoch: 2 [5888/50176]	Loss: 3.3792
Training Epoch: 2 [6144/50176]	Loss: 3.3083
Training Epoch: 2 [6400/50176]	Loss: 3.2723
Training Epoch: 2 [6656/50176]	Loss: 3.5715
Training Epoch: 2 [6912/50176]	Loss: 3.3013
Training Epoch: 2 [7168/50176]	Loss: 3.4457
Training Epoch: 2 [7424/50176]	Loss: 3.5767
Training Epoch: 2 [7680/50176]	Loss: 3.4512
Training Epoch: 2 [7936/50176]	Loss: 3.4042
Training Epoch: 2 [8192/50176]	Loss: 3.4932
Training Epoch: 2 [8448/50176]	Loss: 3.4743
Training Epoch: 2 [8704/50176]	Loss: 3.4284
Training Epoch: 2 [8960/50176]	Loss: 3.4362
Training Epoch: 2 [9216/50176]	Loss: 3.4923
Training Epoch: 2 [9472/50176]	Loss: 3.3743
Training Epoch: 2 [9728/50176]	Loss: 3.3149
Training Epoch: 2 [9984/50176]	Loss: 3.5879
Training Epoch: 2 [10240/50176]	Loss: 3.5861
Training Epoch: 2 [10496/50176]	Loss: 3.4901
Training Epoch: 2 [10752/50176]	Loss: 3.6079
Training Epoch: 2 [11008/50176]	Loss: 3.4732
Training Epoch: 2 [11264/50176]	Loss: 3.4928
Training Epoch: 2 [11520/50176]	Loss: 3.2405
Training Epoch: 2 [11776/50176]	Loss: 3.4771
Training Epoch: 2 [12032/50176]	Loss: 3.5258
Training Epoch: 2 [12288/50176]	Loss: 3.4637
Training Epoch: 2 [12544/50176]	Loss: 3.3909
Training Epoch: 2 [12800/50176]	Loss: 3.4387
Training Epoch: 2 [13056/50176]	Loss: 3.3683
Training Epoch: 2 [13312/50176]	Loss: 3.3966
Training Epoch: 2 [13568/50176]	Loss: 3.4786
Training Epoch: 2 [13824/50176]	Loss: 3.4866
Training Epoch: 2 [14080/50176]	Loss: 3.3756
Training Epoch: 2 [14336/50176]	Loss: 3.3304
Training Epoch: 2 [14592/50176]	Loss: 3.3120
Training Epoch: 2 [14848/50176]	Loss: 3.4916
Training Epoch: 2 [15104/50176]	Loss: 3.3945
Training Epoch: 2 [15360/50176]	Loss: 3.3989
Training Epoch: 2 [15616/50176]	Loss: 3.3465
Training Epoch: 2 [15872/50176]	Loss: 3.3619
Training Epoch: 2 [16128/50176]	Loss: 3.4047
Training Epoch: 2 [16384/50176]	Loss: 3.3439
Training Epoch: 2 [16640/50176]	Loss: 3.4208
Training Epoch: 2 [16896/50176]	Loss: 3.3698
Training Epoch: 2 [17152/50176]	Loss: 3.2838
Training Epoch: 2 [17408/50176]	Loss: 3.4528
Training Epoch: 2 [17664/50176]	Loss: 3.2888
Training Epoch: 2 [17920/50176]	Loss: 3.2827
Training Epoch: 2 [18176/50176]	Loss: 3.3559
Training Epoch: 2 [18432/50176]	Loss: 3.2295
Training Epoch: 2 [18688/50176]	Loss: 3.4901
Training Epoch: 2 [18944/50176]	Loss: 3.2707
Training Epoch: 2 [19200/50176]	Loss: 3.4197
Training Epoch: 2 [19456/50176]	Loss: 3.2159
Training Epoch: 2 [19712/50176]	Loss: 3.3797
Training Epoch: 2 [19968/50176]	Loss: 3.3142
Training Epoch: 2 [20224/50176]	Loss: 3.1027
Training Epoch: 2 [20480/50176]	Loss: 3.3940
Training Epoch: 2 [20736/50176]	Loss: 3.3044
Training Epoch: 2 [20992/50176]	Loss: 3.3267
Training Epoch: 2 [21248/50176]	Loss: 3.4198
Training Epoch: 2 [21504/50176]	Loss: 3.2968
Training Epoch: 2 [21760/50176]	Loss: 3.3443
Training Epoch: 2 [22016/50176]	Loss: 3.1940
Training Epoch: 2 [22272/50176]	Loss: 3.3198
Training Epoch: 2 [22528/50176]	Loss: 3.3325
Training Epoch: 2 [22784/50176]	Loss: 3.1922
Training Epoch: 2 [23040/50176]	Loss: 3.2090
Training Epoch: 2 [23296/50176]	Loss: 3.3586
Training Epoch: 2 [23552/50176]	Loss: 3.2575
Training Epoch: 2 [23808/50176]	Loss: 3.3364
Training Epoch: 2 [24064/50176]	Loss: 3.2723
Training Epoch: 2 [24320/50176]	Loss: 3.2980
Training Epoch: 2 [24576/50176]	Loss: 3.4169
Training Epoch: 2 [24832/50176]	Loss: 3.1915
Training Epoch: 2 [25088/50176]	Loss: 3.2281
Training Epoch: 2 [25344/50176]	Loss: 3.2216
Training Epoch: 2 [25600/50176]	Loss: 3.1239
Training Epoch: 2 [25856/50176]	Loss: 3.2266
Training Epoch: 2 [26112/50176]	Loss: 3.2142
Training Epoch: 2 [26368/50176]	Loss: 3.1636
Training Epoch: 2 [26624/50176]	Loss: 3.2868
Training Epoch: 2 [26880/50176]	Loss: 3.2987
Training Epoch: 2 [27136/50176]	Loss: 3.1915
Training Epoch: 2 [27392/50176]	Loss: 3.2850
Training Epoch: 2 [27648/50176]	Loss: 3.1901
Training Epoch: 2 [27904/50176]	Loss: 3.1760
Training Epoch: 2 [28160/50176]	Loss: 3.3631
Training Epoch: 2 [28416/50176]	Loss: 3.0934
Training Epoch: 2 [28672/50176]	Loss: 3.2167
Training Epoch: 2 [28928/50176]	Loss: 3.2696
Training Epoch: 2 [29184/50176]	Loss: 3.2674
Training Epoch: 2 [29440/50176]	Loss: 3.2157
Training Epoch: 2 [29696/50176]	Loss: 3.2248
Training Epoch: 2 [29952/50176]	Loss: 3.3464
Training Epoch: 2 [30208/50176]	Loss: 3.3106
Training Epoch: 2 [30464/50176]	Loss: 3.2827
Training Epoch: 2 [30720/50176]	Loss: 3.2230
Training Epoch: 2 [30976/50176]	Loss: 3.3517
Training Epoch: 2 [31232/50176]	Loss: 3.1866
Training Epoch: 2 [31488/50176]	Loss: 3.2157
Training Epoch: 2 [31744/50176]	Loss: 3.2096
Training Epoch: 2 [32000/50176]	Loss: 3.1994
Training Epoch: 2 [32256/50176]	Loss: 3.3881
Training Epoch: 2 [32512/50176]	Loss: 3.3873
Training Epoch: 2 [32768/50176]	Loss: 3.3560
Training Epoch: 2 [33024/50176]	Loss: 3.3066
Training Epoch: 2 [33280/50176]	Loss: 3.2770
Training Epoch: 2 [33536/50176]	Loss: 3.3055
Training Epoch: 2 [33792/50176]	Loss: 3.3102
Training Epoch: 2 [34048/50176]	Loss: 3.2776
Training Epoch: 2 [34304/50176]	Loss: 3.2298
Training Epoch: 2 [34560/50176]	Loss: 3.2250
Training Epoch: 2 [34816/50176]	Loss: 3.2953
Training Epoch: 2 [35072/50176]	Loss: 3.5078
Training Epoch: 2 [35328/50176]	Loss: 3.2754
Training Epoch: 2 [35584/50176]	Loss: 3.1048
Training Epoch: 2 [35840/50176]	Loss: 3.0868
Training Epoch: 2 [36096/50176]	Loss: 3.4082
Training Epoch: 2 [36352/50176]	Loss: 3.2320
Training Epoch: 2 [36608/50176]	Loss: 3.2430
Training Epoch: 2 [36864/50176]	Loss: 3.2885
Training Epoch: 2 [37120/50176]	Loss: 3.1985
Training Epoch: 2 [37376/50176]	Loss: 3.1752
Training Epoch: 2 [37632/50176]	Loss: 3.3895
Training Epoch: 2 [37888/50176]	Loss: 3.1415
Training Epoch: 2 [38144/50176]	Loss: 2.9761
Training Epoch: 2 [38400/50176]	Loss: 3.0851
Training Epoch: 2 [38656/50176]	Loss: 3.1624
Training Epoch: 2 [38912/50176]	Loss: 3.1405
Training Epoch: 2 [39168/50176]	Loss: 3.1643
Training Epoch: 2 [39424/50176]	Loss: 3.2154
Training Epoch: 2 [39680/50176]	Loss: 3.1394
Training Epoch: 2 [39936/50176]	Loss: 3.1446
Training Epoch: 2 [40192/50176]	Loss: 3.3653
Training Epoch: 2 [40448/50176]	Loss: 3.2834
Training Epoch: 2 [40704/50176]	Loss: 3.2362
Training Epoch: 2 [40960/50176]	Loss: 3.2078
Training Epoch: 2 [41216/50176]	Loss: 3.1634
Training Epoch: 2 [41472/50176]	Loss: 3.2489
Training Epoch: 2 [41728/50176]	Loss: 3.2632
Training Epoch: 2 [41984/50176]	Loss: 3.1867
Training Epoch: 2 [42240/50176]	Loss: 3.1442
Training Epoch: 2 [42496/50176]	Loss: 3.2545
Training Epoch: 2 [42752/50176]	Loss: 3.1599
Training Epoch: 2 [43008/50176]	Loss: 3.3053
Training Epoch: 2 [43264/50176]	Loss: 3.2566
Training Epoch: 2 [43520/50176]	Loss: 3.1468
Training Epoch: 2 [43776/50176]	Loss: 3.0315
Training Epoch: 2 [44032/50176]	Loss: 3.2689
Training Epoch: 2 [44288/50176]	Loss: 3.2170
Training Epoch: 2 [44544/50176]	Loss: 3.0924
Training Epoch: 2 [44800/50176]	Loss: 3.2237
Training Epoch: 2 [45056/50176]	Loss: 3.3132
Training Epoch: 2 [45312/50176]	Loss: 3.1838
Training Epoch: 2 [45568/50176]	Loss: 3.2558
Training Epoch: 2 [45824/50176]	Loss: 3.0800
Training Epoch: 2 [46080/50176]	Loss: 3.2319
Training Epoch: 2 [46336/50176]	Loss: 3.1601
Training Epoch: 2 [46592/50176]	Loss: 3.3338
Training Epoch: 2 [46848/50176]	Loss: 3.1486
Training Epoch: 2 [47104/50176]	Loss: 3.1101
Training Epoch: 2 [47360/50176]	Loss: 3.0852
Training Epoch: 2 [47616/50176]	Loss: 3.0929
Training Epoch: 2 [47872/50176]	Loss: 3.3112
Training Epoch: 2 [48128/50176]	Loss: 3.1259
Training Epoch: 2 [48384/50176]	Loss: 3.1483
Training Epoch: 2 [48640/50176]	Loss: 3.2596
Training Epoch: 2 [48896/50176]	Loss: 3.1395
Training Epoch: 2 [49152/50176]	Loss: 3.0471
Training Epoch: 2 [49408/50176]	Loss: 3.0722
Training Epoch: 2 [49664/50176]	Loss: 3.0644
Training Epoch: 2 [49920/50176]	Loss: 3.1237
Training Epoch: 2 [50176/50176]	Loss: 2.9214
Validation Epoch: 2, Average loss: 0.0129, Accuracy: 0.2040
Training Epoch: 3 [256/50176]	Loss: 3.0362
Training Epoch: 3 [512/50176]	Loss: 3.0657
Training Epoch: 3 [768/50176]	Loss: 3.2842
Training Epoch: 3 [1024/50176]	Loss: 3.1052
Training Epoch: 3 [1280/50176]	Loss: 3.1698
Training Epoch: 3 [1536/50176]	Loss: 3.1549
Training Epoch: 3 [1792/50176]	Loss: 3.2165
Training Epoch: 3 [2048/50176]	Loss: 3.0256
Training Epoch: 3 [2304/50176]	Loss: 3.0069
Training Epoch: 3 [2560/50176]	Loss: 3.0570
Training Epoch: 3 [2816/50176]	Loss: 3.1420
Training Epoch: 3 [3072/50176]	Loss: 3.0903
Training Epoch: 3 [3328/50176]	Loss: 3.1039
Training Epoch: 3 [3584/50176]	Loss: 3.0969
Training Epoch: 3 [3840/50176]	Loss: 2.9650
Training Epoch: 3 [4096/50176]	Loss: 3.0646
Training Epoch: 3 [4352/50176]	Loss: 2.9556
Training Epoch: 3 [4608/50176]	Loss: 3.2938
Training Epoch: 3 [4864/50176]	Loss: 3.2275
Training Epoch: 3 [5120/50176]	Loss: 2.9963
Training Epoch: 3 [5376/50176]	Loss: 3.0934
Training Epoch: 3 [5632/50176]	Loss: 3.0452
Training Epoch: 3 [5888/50176]	Loss: 3.1078
Training Epoch: 3 [6144/50176]	Loss: 2.9840
Training Epoch: 3 [6400/50176]	Loss: 3.0751
Training Epoch: 3 [6656/50176]	Loss: 3.2322
Training Epoch: 3 [6912/50176]	Loss: 3.1205
Training Epoch: 3 [7168/50176]	Loss: 3.0063
Training Epoch: 3 [7424/50176]	Loss: 2.9703
Training Epoch: 3 [7680/50176]	Loss: 3.1027
Training Epoch: 3 [7936/50176]	Loss: 3.1089
Training Epoch: 3 [8192/50176]	Loss: 3.0551
Training Epoch: 3 [8448/50176]	Loss: 3.1549
Training Epoch: 3 [8704/50176]	Loss: 3.0397
Training Epoch: 3 [8960/50176]	Loss: 3.1157
Training Epoch: 3 [9216/50176]	Loss: 3.0936
Training Epoch: 3 [9472/50176]	Loss: 3.0539
Training Epoch: 3 [9728/50176]	Loss: 3.0453
Training Epoch: 3 [9984/50176]	Loss: 3.0648
Training Epoch: 3 [10240/50176]	Loss: 3.0953
Training Epoch: 3 [10496/50176]	Loss: 3.0143
Training Epoch: 3 [10752/50176]	Loss: 2.9676
Training Epoch: 3 [11008/50176]	Loss: 2.9592
Training Epoch: 3 [11264/50176]	Loss: 3.0568
Training Epoch: 3 [11520/50176]	Loss: 3.0236
Training Epoch: 3 [11776/50176]	Loss: 3.0683
Training Epoch: 3 [12032/50176]	Loss: 3.0763
Training Epoch: 3 [12288/50176]	Loss: 3.1200
Training Epoch: 3 [12544/50176]	Loss: 3.0841
Training Epoch: 3 [12800/50176]	Loss: 3.0787
Training Epoch: 3 [13056/50176]	Loss: 3.0111
Training Epoch: 3 [13312/50176]	Loss: 3.0373
Training Epoch: 3 [13568/50176]	Loss: 2.9717
Training Epoch: 3 [13824/50176]	Loss: 2.8982
Training Epoch: 3 [14080/50176]	Loss: 3.0305
Training Epoch: 3 [14336/50176]	Loss: 2.8123
Training Epoch: 3 [14592/50176]	Loss: 2.9254
Training Epoch: 3 [14848/50176]	Loss: 2.9084
Training Epoch: 3 [15104/50176]	Loss: 3.0477
Training Epoch: 3 [15360/50176]	Loss: 3.0448
Training Epoch: 3 [15616/50176]	Loss: 3.0011
Training Epoch: 3 [15872/50176]	Loss: 3.1124
Training Epoch: 3 [16128/50176]	Loss: 2.9646
Training Epoch: 3 [16384/50176]	Loss: 3.0201
Training Epoch: 3 [16640/50176]	Loss: 3.0270
Training Epoch: 3 [16896/50176]	Loss: 3.0202
Training Epoch: 3 [17152/50176]	Loss: 3.0227
Training Epoch: 3 [17408/50176]	Loss: 2.9522
Training Epoch: 3 [17664/50176]	Loss: 3.0572
Training Epoch: 3 [17920/50176]	Loss: 2.8905
Training Epoch: 3 [18176/50176]	Loss: 3.0240
Training Epoch: 3 [18432/50176]	Loss: 3.0001
Training Epoch: 3 [18688/50176]	Loss: 2.8985
Training Epoch: 3 [18944/50176]	Loss: 3.0350
Training Epoch: 3 [19200/50176]	Loss: 2.8848
Training Epoch: 3 [19456/50176]	Loss: 2.9274
Training Epoch: 3 [19712/50176]	Loss: 3.0393
Training Epoch: 3 [19968/50176]	Loss: 2.9224
Training Epoch: 3 [20224/50176]	Loss: 3.0228
Training Epoch: 3 [20480/50176]	Loss: 2.8236
Training Epoch: 3 [20736/50176]	Loss: 3.0057
Training Epoch: 3 [20992/50176]	Loss: 2.8054
Training Epoch: 3 [21248/50176]	Loss: 3.1193
Training Epoch: 3 [21504/50176]	Loss: 3.1813
Training Epoch: 3 [21760/50176]	Loss: 2.8474
Training Epoch: 3 [22016/50176]	Loss: 2.9333
Training Epoch: 3 [22272/50176]	Loss: 2.7819
Training Epoch: 3 [22528/50176]	Loss: 3.0202
Training Epoch: 3 [22784/50176]	Loss: 2.9594
Training Epoch: 3 [23040/50176]	Loss: 2.8151
Training Epoch: 3 [23296/50176]	Loss: 3.0690
Training Epoch: 3 [23552/50176]	Loss: 3.0939
Training Epoch: 3 [23808/50176]	Loss: 2.8662
Training Epoch: 3 [24064/50176]	Loss: 3.1017
Training Epoch: 3 [24320/50176]	Loss: 2.9287
Training Epoch: 3 [24576/50176]	Loss: 3.0689
Training Epoch: 3 [24832/50176]	Loss: 2.8534
Training Epoch: 3 [25088/50176]	Loss: 2.8993
Training Epoch: 3 [25344/50176]	Loss: 3.0680
Training Epoch: 3 [25600/50176]	Loss: 2.9919
Training Epoch: 3 [25856/50176]	Loss: 3.0357
Training Epoch: 3 [26112/50176]	Loss: 2.9143
Training Epoch: 3 [26368/50176]	Loss: 2.9521
Training Epoch: 3 [26624/50176]	Loss: 2.8900
Training Epoch: 3 [26880/50176]	Loss: 3.0329
Training Epoch: 3 [27136/50176]	Loss: 3.0244
Training Epoch: 3 [27392/50176]	Loss: 2.9216
Training Epoch: 3 [27648/50176]	Loss: 3.0441
Training Epoch: 3 [27904/50176]	Loss: 2.8408
Training Epoch: 3 [28160/50176]	Loss: 2.8360
Training Epoch: 3 [28416/50176]	Loss: 3.1040
Training Epoch: 3 [28672/50176]	Loss: 3.0420
Training Epoch: 3 [28928/50176]	Loss: 2.9834
Training Epoch: 3 [29184/50176]	Loss: 2.8378
Training Epoch: 3 [29440/50176]	Loss: 2.9113
Training Epoch: 3 [29696/50176]	Loss: 3.0240
Training Epoch: 3 [29952/50176]	Loss: 3.1097
Training Epoch: 3 [30208/50176]	Loss: 2.9898
Training Epoch: 3 [30464/50176]	Loss: 2.8552
Training Epoch: 3 [30720/50176]	Loss: 3.0995
Training Epoch: 3 [30976/50176]	Loss: 2.8974
Training Epoch: 3 [31232/50176]	Loss: 2.7550
Training Epoch: 3 [31488/50176]	Loss: 2.8139
Training Epoch: 3 [31744/50176]	Loss: 2.8932
Training Epoch: 3 [32000/50176]	Loss: 2.9619
Training Epoch: 3 [32256/50176]	Loss: 3.0372
Training Epoch: 3 [32512/50176]	Loss: 3.0780
Training Epoch: 3 [32768/50176]	Loss: 3.1268
Training Epoch: 3 [33024/50176]	Loss: 2.9613
Training Epoch: 3 [33280/50176]	Loss: 2.9101
Training Epoch: 3 [33536/50176]	Loss: 2.8376
Training Epoch: 3 [33792/50176]	Loss: 2.7280
Training Epoch: 3 [34048/50176]	Loss: 2.9714
Training Epoch: 3 [34304/50176]	Loss: 2.7663
Training Epoch: 3 [34560/50176]	Loss: 2.7311
Training Epoch: 3 [34816/50176]	Loss: 2.8370
Training Epoch: 3 [35072/50176]	Loss: 3.0497
Training Epoch: 3 [35328/50176]	Loss: 2.8529
Training Epoch: 3 [35584/50176]	Loss: 2.7010
Training Epoch: 3 [35840/50176]	Loss: 2.9723
Training Epoch: 3 [36096/50176]	Loss: 3.0618
Training Epoch: 3 [36352/50176]	Loss: 3.0569
Training Epoch: 3 [36608/50176]	Loss: 2.9327
Training Epoch: 3 [36864/50176]	Loss: 2.9270
Training Epoch: 3 [37120/50176]	Loss: 2.9689
Training Epoch: 3 [37376/50176]	Loss: 2.9547
Training Epoch: 3 [37632/50176]	Loss: 2.8755
Training Epoch: 3 [37888/50176]	Loss: 2.9035
Training Epoch: 3 [38144/50176]	Loss: 2.9353
Training Epoch: 3 [38400/50176]	Loss: 2.7775
Training Epoch: 3 [38656/50176]	Loss: 2.8543
Training Epoch: 3 [38912/50176]	Loss: 2.9277
Training Epoch: 3 [39168/50176]	Loss: 2.9934
Training Epoch: 3 [39424/50176]	Loss: 2.7910
Training Epoch: 3 [39680/50176]	Loss: 2.8450
Training Epoch: 3 [39936/50176]	Loss: 2.7401
Training Epoch: 3 [40192/50176]	Loss: 2.7742
Training Epoch: 3 [40448/50176]	Loss: 2.9440
Training Epoch: 3 [40704/50176]	Loss: 2.8520
Training Epoch: 3 [40960/50176]	Loss: 2.9834
Training Epoch: 3 [41216/50176]	Loss: 2.7933
Training Epoch: 3 [41472/50176]	Loss: 2.9499
Training Epoch: 3 [41728/50176]	Loss: 2.9301
Training Epoch: 3 [41984/50176]	Loss: 2.8659
Training Epoch: 3 [42240/50176]	Loss: 2.9702
Training Epoch: 3 [42496/50176]	Loss: 2.7566
Training Epoch: 3 [42752/50176]	Loss: 2.8350
Training Epoch: 3 [43008/50176]	Loss: 2.9343
Training Epoch: 3 [43264/50176]	Loss: 3.0088
Training Epoch: 3 [43520/50176]	Loss: 2.8443
Training Epoch: 3 [43776/50176]	Loss: 3.0523
Training Epoch: 3 [44032/50176]	Loss: 2.7626
Training Epoch: 3 [44288/50176]	Loss: 2.8487
Training Epoch: 3 [44544/50176]	Loss: 2.8798
Training Epoch: 3 [44800/50176]	Loss: 2.9415
Training Epoch: 3 [45056/50176]	Loss: 2.8804
Training Epoch: 3 [45312/50176]	Loss: 2.9722
Training Epoch: 3 [45568/50176]	Loss: 2.8452
Training Epoch: 3 [45824/50176]	Loss: 2.7557
Training Epoch: 3 [46080/50176]	Loss: 2.8536
Training Epoch: 3 [46336/50176]	Loss: 2.8740
Training Epoch: 3 [46592/50176]	Loss: 2.7696
Training Epoch: 3 [46848/50176]	Loss: 2.9016
Training Epoch: 3 [47104/50176]	Loss: 2.8356
Training Epoch: 3 [47360/50176]	Loss: 2.8500
Training Epoch: 3 [47616/50176]	Loss: 2.8377
Training Epoch: 3 [47872/50176]	Loss: 2.8995
Training Epoch: 3 [48128/50176]	Loss: 2.9904
Training Epoch: 3 [48384/50176]	Loss: 2.9509
Training Epoch: 3 [48640/50176]	Loss: 2.9145
Training Epoch: 3 [48896/50176]	Loss: 3.0780
Training Epoch: 3 [49152/50176]	Loss: 2.7548
Training Epoch: 3 [49408/50176]	Loss: 2.6805
Training Epoch: 3 [49664/50176]	Loss: 2.7508
Training Epoch: 3 [49920/50176]	Loss: 2.8761
Training Epoch: 3 [50176/50176]	Loss: 2.8775
Validation Epoch: 3, Average loss: 0.0120, Accuracy: 0.2469
Training Epoch: 4 [256/50176]	Loss: 2.9067
Training Epoch: 4 [512/50176]	Loss: 2.7665
Training Epoch: 4 [768/50176]	Loss: 2.7330
Training Epoch: 4 [1024/50176]	Loss: 2.8250
Training Epoch: 4 [1280/50176]	Loss: 2.8021
Training Epoch: 4 [1536/50176]	Loss: 2.6637
Training Epoch: 4 [1792/50176]	Loss: 2.8058
Training Epoch: 4 [2048/50176]	Loss: 2.9212
Training Epoch: 4 [2304/50176]	Loss: 2.8059
Training Epoch: 4 [2560/50176]	Loss: 2.8168
Training Epoch: 4 [2816/50176]	Loss: 2.6840
Training Epoch: 4 [3072/50176]	Loss: 2.6494
Training Epoch: 4 [3328/50176]	Loss: 2.5580
Training Epoch: 4 [3584/50176]	Loss: 2.8083
Training Epoch: 4 [3840/50176]	Loss: 2.8972
Training Epoch: 4 [4096/50176]	Loss: 2.6236
Training Epoch: 4 [4352/50176]	Loss: 2.7971
Training Epoch: 4 [4608/50176]	Loss: 2.8341
Training Epoch: 4 [4864/50176]	Loss: 2.6994
Training Epoch: 4 [5120/50176]	Loss: 2.8651
Training Epoch: 4 [5376/50176]	Loss: 2.7559
Training Epoch: 4 [5632/50176]	Loss: 2.7702
Training Epoch: 4 [5888/50176]	Loss: 2.7166
Training Epoch: 4 [6144/50176]	Loss: 2.7287
Training Epoch: 4 [6400/50176]	Loss: 2.8087
Training Epoch: 4 [6656/50176]	Loss: 2.7553
Training Epoch: 4 [6912/50176]	Loss: 2.6863
Training Epoch: 4 [7168/50176]	Loss: 2.7217
Training Epoch: 4 [7424/50176]	Loss: 2.7608
Training Epoch: 4 [7680/50176]	Loss: 2.8814
Training Epoch: 4 [7936/50176]	Loss: 2.7356
Training Epoch: 4 [8192/50176]	Loss: 2.7319
Training Epoch: 4 [8448/50176]	Loss: 2.7954
Training Epoch: 4 [8704/50176]	Loss: 2.7345
Training Epoch: 4 [8960/50176]	Loss: 2.6174
Training Epoch: 4 [9216/50176]	Loss: 2.5808
Training Epoch: 4 [9472/50176]	Loss: 2.8631
Training Epoch: 4 [9728/50176]	Loss: 2.8022
Training Epoch: 4 [9984/50176]	Loss: 2.8817
Training Epoch: 4 [10240/50176]	Loss: 2.6941
Training Epoch: 4 [10496/50176]	Loss: 2.6935
Training Epoch: 4 [10752/50176]	Loss: 2.7008
Training Epoch: 4 [11008/50176]	Loss: 2.7972
Training Epoch: 4 [11264/50176]	Loss: 2.5484
Training Epoch: 4 [11520/50176]	Loss: 2.8292
Training Epoch: 4 [11776/50176]	Loss: 2.7176
Training Epoch: 4 [12032/50176]	Loss: 2.8644
Training Epoch: 4 [12288/50176]	Loss: 2.8717
Training Epoch: 4 [12544/50176]	Loss: 2.8057
Training Epoch: 4 [12800/50176]	Loss: 2.6403
Training Epoch: 4 [13056/50176]	Loss: 2.7662
Training Epoch: 4 [13312/50176]	Loss: 3.0047
Training Epoch: 4 [13568/50176]	Loss: 2.5389
Training Epoch: 4 [13824/50176]	Loss: 2.7179
Training Epoch: 4 [14080/50176]	Loss: 2.5929
Training Epoch: 4 [14336/50176]	Loss: 2.7540
Training Epoch: 4 [14592/50176]	Loss: 2.7363
Training Epoch: 4 [14848/50176]	Loss: 2.8107
Training Epoch: 4 [15104/50176]	Loss: 2.7337
Training Epoch: 4 [15360/50176]	Loss: 2.8506
Training Epoch: 4 [15616/50176]	Loss: 2.9320
Training Epoch: 4 [15872/50176]	Loss: 2.8136
Training Epoch: 4 [16128/50176]	Loss: 2.5543
Training Epoch: 4 [16384/50176]	Loss: 2.8242
Training Epoch: 4 [16640/50176]	Loss: 2.7868
Training Epoch: 4 [16896/50176]	Loss: 2.5625
Training Epoch: 4 [17152/50176]	Loss: 2.6329
Training Epoch: 4 [17408/50176]	Loss: 2.7586
Training Epoch: 4 [17664/50176]	Loss: 2.6616
Training Epoch: 4 [17920/50176]	Loss: 2.6008
Training Epoch: 4 [18176/50176]	Loss: 2.5291
Training Epoch: 4 [18432/50176]	Loss: 2.7859
Training Epoch: 4 [18688/50176]	Loss: 2.7196
Training Epoch: 4 [18944/50176]	Loss: 2.7359
Training Epoch: 4 [19200/50176]	Loss: 2.5339
Training Epoch: 4 [19456/50176]	Loss: 2.5810
Training Epoch: 4 [19712/50176]	Loss: 2.7177
Training Epoch: 4 [19968/50176]	Loss: 2.7365
Training Epoch: 4 [20224/50176]	Loss: 2.6639
Training Epoch: 4 [20480/50176]	Loss: 2.7193
Training Epoch: 4 [20736/50176]	Loss: 2.5773
Training Epoch: 4 [20992/50176]	Loss: 2.7577
Training Epoch: 4 [21248/50176]	Loss: 2.7091
Training Epoch: 4 [21504/50176]	Loss: 2.8649
Training Epoch: 4 [21760/50176]	Loss: 2.7119
Training Epoch: 4 [22016/50176]	Loss: 2.7348
Training Epoch: 4 [22272/50176]	Loss: 2.6399
Training Epoch: 4 [22528/50176]	Loss: 2.6867
Training Epoch: 4 [22784/50176]	Loss: 2.8511
Training Epoch: 4 [23040/50176]	Loss: 2.7275
Training Epoch: 4 [23296/50176]	Loss: 2.7304
Training Epoch: 4 [23552/50176]	Loss: 2.7222
Training Epoch: 4 [23808/50176]	Loss: 2.6829
Training Epoch: 4 [24064/50176]	Loss: 2.6998
Training Epoch: 4 [24320/50176]	Loss: 2.7101
Training Epoch: 4 [24576/50176]	Loss: 2.5724
Training Epoch: 4 [24832/50176]	Loss: 2.6470
Training Epoch: 4 [25088/50176]	Loss: 2.8896
Training Epoch: 4 [25344/50176]	Loss: 2.9560
Training Epoch: 4 [25600/50176]	Loss: 2.5589
Training Epoch: 4 [25856/50176]	Loss: 2.5732
Training Epoch: 4 [26112/50176]	Loss: 2.5801
Training Epoch: 4 [26368/50176]	Loss: 2.6376
Training Epoch: 4 [26624/50176]	Loss: 2.6096
Training Epoch: 4 [26880/50176]	Loss: 2.6013
Training Epoch: 4 [27136/50176]	Loss: 2.5309
Training Epoch: 4 [27392/50176]	Loss: 2.7165
Training Epoch: 4 [27648/50176]	Loss: 2.6320
Training Epoch: 4 [27904/50176]	Loss: 2.6650
Training Epoch: 4 [28160/50176]	Loss: 2.8705
Training Epoch: 4 [28416/50176]	Loss: 2.8692
Training Epoch: 4 [28672/50176]	Loss: 2.5983
Training Epoch: 4 [28928/50176]	Loss: 2.7290
Training Epoch: 4 [29184/50176]	Loss: 2.5929
Training Epoch: 4 [29440/50176]	Loss: 2.7335
Training Epoch: 4 [29696/50176]	Loss: 2.7187
Training Epoch: 4 [29952/50176]	Loss: 2.6832
Training Epoch: 4 [30208/50176]	Loss: 2.5214
Training Epoch: 4 [30464/50176]	Loss: 2.8767
Training Epoch: 4 [30720/50176]	Loss: 2.6932
Training Epoch: 4 [30976/50176]	Loss: 2.5887
Training Epoch: 4 [31232/50176]	Loss: 2.5406
Training Epoch: 4 [31488/50176]	Loss: 2.7188
Training Epoch: 4 [31744/50176]	Loss: 2.6823
Training Epoch: 4 [32000/50176]	Loss: 2.6776
Training Epoch: 4 [32256/50176]	Loss: 2.5297
Training Epoch: 4 [32512/50176]	Loss: 2.6760
Training Epoch: 4 [32768/50176]	Loss: 2.6452
Training Epoch: 4 [33024/50176]	Loss: 2.5970
Training Epoch: 4 [33280/50176]	Loss: 2.7719
Training Epoch: 4 [33536/50176]	Loss: 2.7222
Training Epoch: 4 [33792/50176]	Loss: 2.6394
Training Epoch: 4 [34048/50176]	Loss: 2.9500
Training Epoch: 4 [34304/50176]	Loss: 2.7077
Training Epoch: 4 [34560/50176]	Loss: 2.5175
Training Epoch: 4 [34816/50176]	Loss: 2.6435
Training Epoch: 4 [35072/50176]	Loss: 2.6262
Training Epoch: 4 [35328/50176]	Loss: 2.5369
Training Epoch: 4 [35584/50176]	Loss: 2.7747
Training Epoch: 4 [35840/50176]	Loss: 2.5614
Training Epoch: 4 [36096/50176]	Loss: 2.6110
Training Epoch: 4 [36352/50176]	Loss: 2.7057
Training Epoch: 4 [36608/50176]	Loss: 2.6639
Training Epoch: 4 [36864/50176]	Loss: 2.6138
Training Epoch: 4 [37120/50176]	Loss: 2.5056
Training Epoch: 4 [37376/50176]	Loss: 2.3707
Training Epoch: 4 [37632/50176]	Loss: 2.7532
Training Epoch: 4 [37888/50176]	Loss: 2.5979
Training Epoch: 4 [38144/50176]	Loss: 2.3467
Training Epoch: 4 [38400/50176]	Loss: 2.5988
Training Epoch: 4 [38656/50176]	Loss: 2.5709
Training Epoch: 4 [38912/50176]	Loss: 2.6383
Training Epoch: 4 [39168/50176]	Loss: 2.7019
Training Epoch: 4 [39424/50176]	Loss: 2.6801
Training Epoch: 4 [39680/50176]	Loss: 2.5994
Training Epoch: 4 [39936/50176]	Loss: 2.7486
Training Epoch: 4 [40192/50176]	Loss: 2.5580
Training Epoch: 4 [40448/50176]	Loss: 2.6562
Training Epoch: 4 [40704/50176]	Loss: 2.5899
Training Epoch: 4 [40960/50176]	Loss: 2.5601
Training Epoch: 4 [41216/50176]	Loss: 2.6044
Training Epoch: 4 [41472/50176]	Loss: 2.7851
Training Epoch: 4 [41728/50176]	Loss: 2.6104
Training Epoch: 4 [41984/50176]	Loss: 2.6661
Training Epoch: 4 [42240/50176]	Loss: 2.6561
Training Epoch: 4 [42496/50176]	Loss: 2.5227
Training Epoch: 4 [42752/50176]	Loss: 2.4832
Training Epoch: 4 [43008/50176]	Loss: 2.4371
Training Epoch: 4 [43264/50176]	Loss: 2.5691
Training Epoch: 4 [43520/50176]	Loss: 2.6308
Training Epoch: 4 [43776/50176]	Loss: 2.5779
Training Epoch: 4 [44032/50176]	Loss: 2.6098
Training Epoch: 4 [44288/50176]	Loss: 2.6141
Training Epoch: 4 [44544/50176]	Loss: 2.6402
Training Epoch: 4 [44800/50176]	Loss: 2.6008
Training Epoch: 4 [45056/50176]	Loss: 2.5194
Training Epoch: 4 [45312/50176]	Loss: 2.5921
Training Epoch: 4 [45568/50176]	Loss: 2.6207
Training Epoch: 4 [45824/50176]	Loss: 2.4982
Training Epoch: 4 [46080/50176]	Loss: 2.6668
Training Epoch: 4 [46336/50176]	Loss: 2.6047
Training Epoch: 4 [46592/50176]	Loss: 2.7343
Training Epoch: 4 [46848/50176]	Loss: 2.5238
Training Epoch: 4 [47104/50176]	Loss: 2.5170
Training Epoch: 4 [47360/50176]	Loss: 2.5511
Training Epoch: 4 [47616/50176]	Loss: 2.7170
Training Epoch: 4 [47872/50176]	Loss: 2.4218
Training Epoch: 4 [48128/50176]	Loss: 2.4400
Training Epoch: 4 [48384/50176]	Loss: 2.3922
Training Epoch: 4 [48640/50176]	Loss: 2.5902
Training Epoch: 4 [48896/50176]	Loss: 2.4357
Training Epoch: 4 [49152/50176]	Loss: 2.6820
Training Epoch: 4 [49408/50176]	Loss: 2.5954
Training Epoch: 4 [49664/50176]	Loss: 2.4963
Training Epoch: 4 [49920/50176]	Loss: 2.6600
Training Epoch: 4 [50176/50176]	Loss: 2.8904
Validation Epoch: 4, Average loss: 0.0110, Accuracy: 0.3063
[Training Loop] Model's accuracy 0.30634765625 surpasses threshold 0.3! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.5351
Profiling... [256/50048]	Loss: 2.4004
Profiling... [384/50048]	Loss: 2.4364
Profiling... [512/50048]	Loss: 2.5002
Profiling... [640/50048]	Loss: 2.5478
Profiling... [768/50048]	Loss: 2.4604
Profiling... [896/50048]	Loss: 2.2633
Profiling... [1024/50048]	Loss: 2.5533
Profiling... [1152/50048]	Loss: 2.3991
Profiling... [1280/50048]	Loss: 2.6725
Profiling... [1408/50048]	Loss: 2.4029
Profiling... [1536/50048]	Loss: 2.4400
Profiling... [1664/50048]	Loss: 2.5221
Profile done
epoch 1 train time consumed: 3.20s
Validation Epoch: 5, Average loss: 0.0190, Accuracy: 0.3702
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.33165544651855,
                        "time": 2.159091764002369,
                        "accuracy": 0.3701542721518987,
                        "total_cost": 510710.8326138315
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.8258
Profiling... [256/50048]	Loss: 2.3670
Profiling... [384/50048]	Loss: 2.4663
Profiling... [512/50048]	Loss: 2.3717
Profiling... [640/50048]	Loss: 2.4905
Profiling... [768/50048]	Loss: 2.4856
Profiling... [896/50048]	Loss: 2.2796
Profiling... [1024/50048]	Loss: 2.4327
Profiling... [1152/50048]	Loss: 2.4516
Profiling... [1280/50048]	Loss: 2.6571
Profiling... [1408/50048]	Loss: 2.4913
Profiling... [1536/50048]	Loss: 2.3448
Profiling... [1664/50048]	Loss: 2.2263
Profile done
epoch 1 train time consumed: 3.33s
Validation Epoch: 5, Average loss: 0.0188, Accuracy: 0.3762
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.36174542255748,
                        "time": 2.159732399999484,
                        "accuracy": 0.3761867088607595,
                        "total_cost": 502670.3926453758
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.2366
Profiling... [256/50048]	Loss: 2.4548
Profiling... [384/50048]	Loss: 2.2693
Profiling... [512/50048]	Loss: 2.2900
Profiling... [640/50048]	Loss: 2.4165
Profiling... [768/50048]	Loss: 2.4527
Profiling... [896/50048]	Loss: 2.1727
Profiling... [1024/50048]	Loss: 2.4332
Profiling... [1152/50048]	Loss: 2.3332
Profiling... [1280/50048]	Loss: 2.4443
Profiling... [1408/50048]	Loss: 2.5039
Profiling... [1536/50048]	Loss: 2.4095
Profiling... [1664/50048]	Loss: 2.2245
Profile done
epoch 1 train time consumed: 3.22s
Validation Epoch: 5, Average loss: 0.0188, Accuracy: 0.3773
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.38563309138621,
                        "time": 2.2023240589987836,
                        "accuracy": 0.3772745253164557,
                        "total_cost": 511105.5664107529
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.5266
Profiling... [256/50048]	Loss: 2.4170
Profiling... [384/50048]	Loss: 2.6990
Profiling... [512/50048]	Loss: 2.4945
Profiling... [640/50048]	Loss: 2.4011
Profiling... [768/50048]	Loss: 2.2590
Profiling... [896/50048]	Loss: 2.0878
Profiling... [1024/50048]	Loss: 2.3581
Profiling... [1152/50048]	Loss: 2.7400
Profiling... [1280/50048]	Loss: 2.4256
Profiling... [1408/50048]	Loss: 2.3325
Profiling... [1536/50048]	Loss: 2.4736
Profiling... [1664/50048]	Loss: 2.4548
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 5, Average loss: 0.0189, Accuracy: 0.3742
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.36541865329168,
                        "time": 2.5534881969979324,
                        "accuracy": 0.37420886075949367,
                        "total_cost": 597456.9353293647
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.3994
Profiling... [256/50048]	Loss: 2.4581
Profiling... [384/50048]	Loss: 2.8779
Profiling... [512/50048]	Loss: 2.3472
Profiling... [640/50048]	Loss: 2.6938
Profiling... [768/50048]	Loss: 2.3136
Profiling... [896/50048]	Loss: 2.3459
Profiling... [1024/50048]	Loss: 2.3790
Profiling... [1152/50048]	Loss: 2.2945
Profiling... [1280/50048]	Loss: 2.4474
Profiling... [1408/50048]	Loss: 2.0978
Profiling... [1536/50048]	Loss: 2.3633
Profiling... [1664/50048]	Loss: 2.3373
Profile done
epoch 1 train time consumed: 3.21s
Validation Epoch: 5, Average loss: 0.0191, Accuracy: 0.3680
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.32497173536663,
                        "time": 2.1643283320008777,
                        "accuracy": 0.36797863924050633,
                        "total_cost": 514976.3135723233
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1760
Profiling... [256/50048]	Loss: 2.4910
Profiling... [384/50048]	Loss: 2.6165
Profiling... [512/50048]	Loss: 2.9759
Profiling... [640/50048]	Loss: 2.5085
Profiling... [768/50048]	Loss: 2.6293
Profiling... [896/50048]	Loss: 2.7946
Profiling... [1024/50048]	Loss: 2.7330
Profiling... [1152/50048]	Loss: 2.5125
Profiling... [1280/50048]	Loss: 2.2969
Profiling... [1408/50048]	Loss: 2.4678
Profiling... [1536/50048]	Loss: 2.4391
Profiling... [1664/50048]	Loss: 2.3754
Profile done
epoch 1 train time consumed: 3.34s
Validation Epoch: 5, Average loss: 0.0190, Accuracy: 0.3698
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.3505471700904,
                        "time": 2.1606111399996735,
                        "accuracy": 0.36975870253164556,
                        "total_cost": 511617.0258512885
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.4936
Profiling... [256/50048]	Loss: 2.4211
Profiling... [384/50048]	Loss: 2.2872
Profiling... [512/50048]	Loss: 2.4679
Profiling... [640/50048]	Loss: 2.5257
Profiling... [768/50048]	Loss: 2.7438
Profiling... [896/50048]	Loss: 2.4590
Profiling... [1024/50048]	Loss: 2.6369
Profiling... [1152/50048]	Loss: 2.6663
Profiling... [1280/50048]	Loss: 2.5701
Profiling... [1408/50048]	Loss: 2.5412
Profiling... [1536/50048]	Loss: 2.4225
Profiling... [1664/50048]	Loss: 2.5207
Profile done
epoch 1 train time consumed: 3.22s
Validation Epoch: 5, Average loss: 0.0190, Accuracy: 0.3691
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.37266223592192,
                        "time": 2.194384739999805,
                        "accuracy": 0.36906645569620256,
                        "total_cost": 520589.0598080753
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.6154
Profiling... [256/50048]	Loss: 2.4163
Profiling... [384/50048]	Loss: 2.5590
Profiling... [512/50048]	Loss: 2.3266
Profiling... [640/50048]	Loss: 2.4164
Profiling... [768/50048]	Loss: 2.5491
Profiling... [896/50048]	Loss: 2.4892
Profiling... [1024/50048]	Loss: 2.5170
Profiling... [1152/50048]	Loss: 2.4765
Profiling... [1280/50048]	Loss: 2.2571
Profiling... [1408/50048]	Loss: 2.7037
Profiling... [1536/50048]	Loss: 2.5570
Profiling... [1664/50048]	Loss: 2.2786
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 5, Average loss: 0.0190, Accuracy: 0.3740
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.35653570151773,
                        "time": 2.5270518399993307,
                        "accuracy": 0.3740110759493671,
                        "total_cost": 591584.0883413712
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.7204
Profiling... [256/50048]	Loss: 2.5212
Profiling... [384/50048]	Loss: 2.5804
Profiling... [512/50048]	Loss: 2.6797
Profiling... [640/50048]	Loss: 2.4349
Profiling... [768/50048]	Loss: 2.2479
Profiling... [896/50048]	Loss: 2.6734
Profiling... [1024/50048]	Loss: 2.4725
Profiling... [1152/50048]	Loss: 2.4106
Profiling... [1280/50048]	Loss: 2.5532
Profiling... [1408/50048]	Loss: 2.3371
Profiling... [1536/50048]	Loss: 2.4228
Profiling... [1664/50048]	Loss: 2.4961
Profile done
epoch 1 train time consumed: 3.29s
Validation Epoch: 5, Average loss: 0.0191, Accuracy: 0.3685
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.31045168457172,
                        "time": 2.1598707039993315,
                        "accuracy": 0.3684731012658228,
                        "total_cost": 513225.9966262437
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.5199
Profiling... [256/50048]	Loss: 2.2273
Profiling... [384/50048]	Loss: 2.2925
Profiling... [512/50048]	Loss: 2.5302
Profiling... [640/50048]	Loss: 2.4635
Profiling... [768/50048]	Loss: 2.3500
Profiling... [896/50048]	Loss: 2.4722
Profiling... [1024/50048]	Loss: 2.2725
Profiling... [1152/50048]	Loss: 2.2782
Profiling... [1280/50048]	Loss: 2.5627
Profiling... [1408/50048]	Loss: 2.4053
Profiling... [1536/50048]	Loss: 2.4103
Profiling... [1664/50048]	Loss: 2.5901
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 5, Average loss: 0.0190, Accuracy: 0.3694
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.33801242287217,
                        "time": 2.1595813280000584,
                        "accuracy": 0.3693631329113924,
                        "total_cost": 511920.79254534515
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.6004
Profiling... [256/50048]	Loss: 2.2336
Profiling... [384/50048]	Loss: 2.4521
Profiling... [512/50048]	Loss: 2.5404
Profiling... [640/50048]	Loss: 2.4787
Profiling... [768/50048]	Loss: 2.6566
Profiling... [896/50048]	Loss: 2.4370
Profiling... [1024/50048]	Loss: 2.5882
Profiling... [1152/50048]	Loss: 2.6087
Profiling... [1280/50048]	Loss: 2.5058
Profiling... [1408/50048]	Loss: 2.3654
Profiling... [1536/50048]	Loss: 2.4361
Profiling... [1664/50048]	Loss: 2.4166
Profile done
epoch 1 train time consumed: 3.28s
Validation Epoch: 5, Average loss: 0.0187, Accuracy: 0.3787
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.36176523312838,
                        "time": 2.202152894002211,
                        "accuracy": 0.3786590189873418,
                        "total_cost": 509197.1601101112
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.4522
Profiling... [256/50048]	Loss: 2.6573
Profiling... [384/50048]	Loss: 2.4847
Profiling... [512/50048]	Loss: 2.5769
Profiling... [640/50048]	Loss: 2.5455
Profiling... [768/50048]	Loss: 2.5051
Profiling... [896/50048]	Loss: 2.2391
Profiling... [1024/50048]	Loss: 2.7474
Profiling... [1152/50048]	Loss: 2.4416
Profiling... [1280/50048]	Loss: 2.2055
Profiling... [1408/50048]	Loss: 2.6246
Profiling... [1536/50048]	Loss: 2.2174
Profiling... [1664/50048]	Loss: 2.6751
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 5, Average loss: 0.0190, Accuracy: 0.3659
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.34440398721587,
                        "time": 2.524463844998536,
                        "accuracy": 0.3659018987341772,
                        "total_cost": 604075.5511109708
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.3954
Profiling... [256/50048]	Loss: 2.7786
Profiling... [384/50048]	Loss: 2.5817
Profiling... [512/50048]	Loss: 2.6476
Profiling... [640/50048]	Loss: 2.6099
Profiling... [768/50048]	Loss: 2.6919
Profiling... [896/50048]	Loss: 2.3698
Profiling... [1024/50048]	Loss: 2.4231
Profiling... [1152/50048]	Loss: 2.3824
Profiling... [1280/50048]	Loss: 2.5815
Profiling... [1408/50048]	Loss: 2.5362
Profiling... [1536/50048]	Loss: 2.6861
Profiling... [1664/50048]	Loss: 2.8048
Profile done
epoch 1 train time consumed: 3.35s
Validation Epoch: 5, Average loss: 0.0213, Accuracy: 0.3200
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.30216294239773,
                        "time": 2.159880974999396,
                        "accuracy": 0.3200158227848101,
                        "total_cost": 590942.233166412
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.6616
Profiling... [256/50048]	Loss: 2.7245
Profiling... [384/50048]	Loss: 2.6209
Profiling... [512/50048]	Loss: 2.7176
Profiling... [640/50048]	Loss: 2.3291
Profiling... [768/50048]	Loss: 2.8972
Profiling... [896/50048]	Loss: 2.8483
Profiling... [1024/50048]	Loss: 2.6083
Profiling... [1152/50048]	Loss: 2.7313
Profiling... [1280/50048]	Loss: 2.5694
Profiling... [1408/50048]	Loss: 2.5025
Profiling... [1536/50048]	Loss: 2.2019
Profiling... [1664/50048]	Loss: 2.5351
Profile done
epoch 1 train time consumed: 3.24s
Validation Epoch: 5, Average loss: 0.0207, Accuracy: 0.3398
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.32808727253543,
                        "time": 2.158983089000685,
                        "accuracy": 0.33979430379746833,
                        "total_cost": 556313.8504542198
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.3150
Profiling... [256/50048]	Loss: 2.4758
Profiling... [384/50048]	Loss: 2.6173
Profiling... [512/50048]	Loss: 2.5105
Profiling... [640/50048]	Loss: 2.4241
Profiling... [768/50048]	Loss: 2.4920
Profiling... [896/50048]	Loss: 2.6575
Profiling... [1024/50048]	Loss: 2.5693
Profiling... [1152/50048]	Loss: 2.5513
Profiling... [1280/50048]	Loss: 2.7049
Profiling... [1408/50048]	Loss: 2.6129
Profiling... [1536/50048]	Loss: 2.6367
Profiling... [1664/50048]	Loss: 2.6129
Profile done
epoch 1 train time consumed: 3.33s
Validation Epoch: 5, Average loss: 0.0212, Accuracy: 0.3187
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.35209889153145,
                        "time": 2.197182520001661,
                        "accuracy": 0.31873022151898733,
                        "total_cost": 603572.8228632065
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.4439
Profiling... [256/50048]	Loss: 2.4105
Profiling... [384/50048]	Loss: 2.6771
Profiling... [512/50048]	Loss: 2.5247
Profiling... [640/50048]	Loss: 2.5128
Profiling... [768/50048]	Loss: 2.5354
Profiling... [896/50048]	Loss: 2.4420
Profiling... [1024/50048]	Loss: 2.5307
Profiling... [1152/50048]	Loss: 2.5614
Profiling... [1280/50048]	Loss: 2.4705
Profiling... [1408/50048]	Loss: 2.4018
Profiling... [1536/50048]	Loss: 2.6150
Profiling... [1664/50048]	Loss: 2.7044
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 5, Average loss: 0.0217, Accuracy: 0.3127
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.32898675247945,
                        "time": 2.5379305969981942,
                        "accuracy": 0.31269778481012656,
                        "total_cost": 710626.936351582
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.3435
Profiling... [256/50048]	Loss: 2.5275
Profiling... [384/50048]	Loss: 2.5000
Profiling... [512/50048]	Loss: 2.6934
Profiling... [640/50048]	Loss: 2.6738
Profiling... [768/50048]	Loss: 2.6981
Profiling... [896/50048]	Loss: 2.5332
Profiling... [1024/50048]	Loss: 2.3883
Profiling... [1152/50048]	Loss: 2.6358
Profiling... [1280/50048]	Loss: 2.5598
Profiling... [1408/50048]	Loss: 2.5762
Profiling... [1536/50048]	Loss: 2.5889
Profiling... [1664/50048]	Loss: 2.7637
Profile done
epoch 1 train time consumed: 3.28s
Validation Epoch: 5, Average loss: 0.0206, Accuracy: 0.3379
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.29009301761116,
                        "time": 2.1527984099993773,
                        "accuracy": 0.33791534810126583,
                        "total_cost": 557804.5830144197
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.4780
Profiling... [256/50048]	Loss: 2.4272
Profiling... [384/50048]	Loss: 2.6759
Profiling... [512/50048]	Loss: 2.7736
Profiling... [640/50048]	Loss: 2.2424
Profiling... [768/50048]	Loss: 2.5010
Profiling... [896/50048]	Loss: 2.3972
Profiling... [1024/50048]	Loss: 2.6453
Profiling... [1152/50048]	Loss: 2.5543
Profiling... [1280/50048]	Loss: 2.3973
Profiling... [1408/50048]	Loss: 2.6359
Profiling... [1536/50048]	Loss: 2.4485
Profiling... [1664/50048]	Loss: 2.2742
Profile done
epoch 1 train time consumed: 3.21s
Validation Epoch: 5, Average loss: 0.0218, Accuracy: 0.3170
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.31685827723747,
                        "time": 2.1591593060002197,
                        "accuracy": 0.3169501582278481,
                        "total_cost": 596458.7471005528
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.9040
Profiling... [256/50048]	Loss: 2.3865
Profiling... [384/50048]	Loss: 2.5328
Profiling... [512/50048]	Loss: 2.5608
Profiling... [640/50048]	Loss: 2.7384
Profiling... [768/50048]	Loss: 2.5578
Profiling... [896/50048]	Loss: 2.4430
Profiling... [1024/50048]	Loss: 2.7654
Profiling... [1152/50048]	Loss: 2.8010
Profiling... [1280/50048]	Loss: 2.6549
Profiling... [1408/50048]	Loss: 2.2958
Profiling... [1536/50048]	Loss: 2.3956
Profiling... [1664/50048]	Loss: 2.8679
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 5, Average loss: 0.0217, Accuracy: 0.3174
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.33897898181388,
                        "time": 2.1976341160006996,
                        "accuracy": 0.31744462025316456,
                        "total_cost": 606141.7105855856
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.6843
Profiling... [256/50048]	Loss: 2.4951
Profiling... [384/50048]	Loss: 2.7144
Profiling... [512/50048]	Loss: 2.6828
Profiling... [640/50048]	Loss: 2.4861
Profiling... [768/50048]	Loss: 2.3140
Profiling... [896/50048]	Loss: 2.7891
Profiling... [1024/50048]	Loss: 2.4125
Profiling... [1152/50048]	Loss: 2.7220
Profiling... [1280/50048]	Loss: 2.6217
Profiling... [1408/50048]	Loss: 2.4729
Profiling... [1536/50048]	Loss: 2.6607
Profiling... [1664/50048]	Loss: 2.2499
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 5, Average loss: 0.0211, Accuracy: 0.3223
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.32205988590701,
                        "time": 2.5382482620007067,
                        "accuracy": 0.32229034810126583,
                        "total_cost": 689562.2995569127
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.7814
Profiling... [256/50048]	Loss: 2.4509
Profiling... [384/50048]	Loss: 2.1248
Profiling... [512/50048]	Loss: 2.8096
Profiling... [640/50048]	Loss: 2.5834
Profiling... [768/50048]	Loss: 2.4951
Profiling... [896/50048]	Loss: 2.4760
Profiling... [1024/50048]	Loss: 2.7719
Profiling... [1152/50048]	Loss: 2.5021
Profiling... [1280/50048]	Loss: 2.5681
Profiling... [1408/50048]	Loss: 2.5117
Profiling... [1536/50048]	Loss: 2.6773
Profiling... [1664/50048]	Loss: 2.6644
Profile done
epoch 1 train time consumed: 3.29s
Validation Epoch: 5, Average loss: 0.0213, Accuracy: 0.3278
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.28321203320282,
                        "time": 2.152650296000502,
                        "accuracy": 0.3278283227848101,
                        "total_cost": 574928.2201848442
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.6707
Profiling... [256/50048]	Loss: 2.3224
Profiling... [384/50048]	Loss: 2.4850
Profiling... [512/50048]	Loss: 2.5644
Profiling... [640/50048]	Loss: 2.5260
Profiling... [768/50048]	Loss: 2.6844
Profiling... [896/50048]	Loss: 2.2873
Profiling... [1024/50048]	Loss: 2.7565
Profiling... [1152/50048]	Loss: 2.6348
Profiling... [1280/50048]	Loss: 2.7631
Profiling... [1408/50048]	Loss: 2.6972
Profiling... [1536/50048]	Loss: 2.4882
Profiling... [1664/50048]	Loss: 2.6057
Profile done
epoch 1 train time consumed: 3.22s
Validation Epoch: 5, Average loss: 0.0226, Accuracy: 0.3029
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.31235363211202,
                        "time": 2.161409444997844,
                        "accuracy": 0.3029074367088608,
                        "total_cost": 624760.8344795078
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.4586
Profiling... [256/50048]	Loss: 2.5748
Profiling... [384/50048]	Loss: 2.2808
Profiling... [512/50048]	Loss: 2.5168
Profiling... [640/50048]	Loss: 2.5465
Profiling... [768/50048]	Loss: 2.5056
Profiling... [896/50048]	Loss: 2.4220
Profiling... [1024/50048]	Loss: 2.6970
Profiling... [1152/50048]	Loss: 2.4214
Profiling... [1280/50048]	Loss: 2.3878
Profiling... [1408/50048]	Loss: 2.7610
Profiling... [1536/50048]	Loss: 2.2872
Profiling... [1664/50048]	Loss: 2.6276
Profile done
epoch 1 train time consumed: 3.30s
Validation Epoch: 5, Average loss: 0.0207, Accuracy: 0.3312
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.331290914886,
                        "time": 2.2036667610009317,
                        "accuracy": 0.331190664556962,
                        "total_cost": 582578.6551432261
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.5658
Profiling... [256/50048]	Loss: 2.8434
Profiling... [384/50048]	Loss: 2.5133
Profiling... [512/50048]	Loss: 2.7457
Profiling... [640/50048]	Loss: 2.3989
Profiling... [768/50048]	Loss: 2.6906
Profiling... [896/50048]	Loss: 2.5637
Profiling... [1024/50048]	Loss: 2.6720
Profiling... [1152/50048]	Loss: 2.6174
Profiling... [1280/50048]	Loss: 2.7441
Profiling... [1408/50048]	Loss: 2.7251
Profiling... [1536/50048]	Loss: 2.7164
Profiling... [1664/50048]	Loss: 2.3607
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 5, Average loss: 0.0221, Accuracy: 0.3171
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.31305099855913,
                        "time": 2.567638974000147,
                        "accuracy": 0.31714794303797467,
                        "total_cost": 708857.1906065007
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.5898
Profiling... [256/50048]	Loss: 2.3481
Profiling... [384/50048]	Loss: 2.8701
Profiling... [512/50048]	Loss: 2.8027
Profiling... [640/50048]	Loss: 2.9875
Profiling... [768/50048]	Loss: 2.8034
Profiling... [896/50048]	Loss: 2.7942
Profiling... [1024/50048]	Loss: 2.5937
Profiling... [1152/50048]	Loss: 2.7853
Profiling... [1280/50048]	Loss: 3.0330
Profiling... [1408/50048]	Loss: 3.0769
Profiling... [1536/50048]	Loss: 2.9010
Profiling... [1664/50048]	Loss: 2.5983
Profile done
epoch 1 train time consumed: 3.28s
Validation Epoch: 5, Average loss: 0.0337, Accuracy: 0.2295
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.27317496388284,
                        "time": 2.157532966000872,
                        "accuracy": 0.22952927215189872,
                        "total_cost": 823011.5892937306
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.7432
Profiling... [256/50048]	Loss: 2.5630
Profiling... [384/50048]	Loss: 2.8638
Profiling... [512/50048]	Loss: 2.8676
Profiling... [640/50048]	Loss: 2.9055
Profiling... [768/50048]	Loss: 2.6578
Profiling... [896/50048]	Loss: 2.7227
Profiling... [1024/50048]	Loss: 3.0984
Profiling... [1152/50048]	Loss: 2.6897
Profiling... [1280/50048]	Loss: 2.9908
Profiling... [1408/50048]	Loss: 2.8985
Profiling... [1536/50048]	Loss: 3.0042
Profiling... [1664/50048]	Loss: 2.9971
Profile done
epoch 1 train time consumed: 3.22s
Validation Epoch: 5, Average loss: 0.0513, Accuracy: 0.1151
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.30054907974427,
                        "time": 2.1590319800016005,
                        "accuracy": 0.11511075949367089,
                        "total_cost": 1642214.2406153723
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.2809
Profiling... [256/50048]	Loss: 2.9511
Profiling... [384/50048]	Loss: 2.4913
Profiling... [512/50048]	Loss: 2.9451
Profiling... [640/50048]	Loss: 2.8294
Profiling... [768/50048]	Loss: 2.8443
Profiling... [896/50048]	Loss: 2.9966
Profiling... [1024/50048]	Loss: 2.7930
Profiling... [1152/50048]	Loss: 2.6976
Profiling... [1280/50048]	Loss: 2.8129
Profiling... [1408/50048]	Loss: 2.7971
Profiling... [1536/50048]	Loss: 3.0404
Profiling... [1664/50048]	Loss: 2.7218
Profile done
epoch 1 train time consumed: 3.30s
Validation Epoch: 5, Average loss: 0.0238, Accuracy: 0.2578
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.32250683294262,
                        "time": 2.1997145470013493,
                        "accuracy": 0.2578125,
                        "total_cost": 747048.9661623704
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.3200
Profiling... [256/50048]	Loss: 2.7248
Profiling... [384/50048]	Loss: 2.8647
Profiling... [512/50048]	Loss: 2.8954
Profiling... [640/50048]	Loss: 2.7316
Profiling... [768/50048]	Loss: 2.6907
Profiling... [896/50048]	Loss: 2.8754
Profiling... [1024/50048]	Loss: 2.7959
Profiling... [1152/50048]	Loss: 2.9109
Profiling... [1280/50048]	Loss: 2.8377
Profiling... [1408/50048]	Loss: 2.9179
Profiling... [1536/50048]	Loss: 2.6192
Profiling... [1664/50048]	Loss: 2.9149
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 5, Average loss: 0.0499, Accuracy: 0.2182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.30424436321891,
                        "time": 2.5403025790001266,
                        "accuracy": 0.21815664556962025,
                        "total_cost": 1019538.5910090226
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.4759
Profiling... [256/50048]	Loss: 2.4074
Profiling... [384/50048]	Loss: 3.0754
Profiling... [512/50048]	Loss: 2.9076
Profiling... [640/50048]	Loss: 2.7939
Profiling... [768/50048]	Loss: 2.8106
Profiling... [896/50048]	Loss: 2.9768
Profiling... [1024/50048]	Loss: 2.9073
Profiling... [1152/50048]	Loss: 2.7018
Profiling... [1280/50048]	Loss: 2.6913
Profiling... [1408/50048]	Loss: 2.8310
Profiling... [1536/50048]	Loss: 3.1738
Profiling... [1664/50048]	Loss: 3.1323
Profile done
epoch 1 train time consumed: 3.27s
Validation Epoch: 5, Average loss: 0.0699, Accuracy: 0.2123
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.26719814850034,
                        "time": 2.161459307997575,
                        "accuracy": 0.21232199367088608,
                        "total_cost": 891330.2699735735
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.6100
Profiling... [256/50048]	Loss: 2.5533
Profiling... [384/50048]	Loss: 2.8583
Profiling... [512/50048]	Loss: 2.7799
Profiling... [640/50048]	Loss: 2.7301
Profiling... [768/50048]	Loss: 2.6849
Profiling... [896/50048]	Loss: 2.9739
Profiling... [1024/50048]	Loss: 2.7954
Profiling... [1152/50048]	Loss: 2.8411
Profiling... [1280/50048]	Loss: 2.8433
Profiling... [1408/50048]	Loss: 2.8821
Profiling... [1536/50048]	Loss: 2.8349
Profiling... [1664/50048]	Loss: 3.0669
Profile done
epoch 1 train time consumed: 3.20s
Validation Epoch: 5, Average loss: 0.1290, Accuracy: 0.1794
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.29047308852866,
                        "time": 2.154246696001792,
                        "accuracy": 0.17939082278481014,
                        "total_cost": 1051433.5887557436
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.4169
Profiling... [256/50048]	Loss: 2.6109
Profiling... [384/50048]	Loss: 2.9436
Profiling... [512/50048]	Loss: 2.8957
Profiling... [640/50048]	Loss: 3.0101
Profiling... [768/50048]	Loss: 2.8074
Profiling... [896/50048]	Loss: 2.8768
Profiling... [1024/50048]	Loss: 2.9411
Profiling... [1152/50048]	Loss: 2.7917
Profiling... [1280/50048]	Loss: 2.8190
Profiling... [1408/50048]	Loss: 2.8519
Profiling... [1536/50048]	Loss: 2.9399
Profiling... [1664/50048]	Loss: 2.8158
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 5, Average loss: 0.0643, Accuracy: 0.2046
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.31468484982152,
                        "time": 2.1982620649978344,
                        "accuracy": 0.20460838607594936,
                        "total_cost": 940681.7723071728
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.2232
Profiling... [256/50048]	Loss: 2.5002
Profiling... [384/50048]	Loss: 2.5627
Profiling... [512/50048]	Loss: 2.3660
Profiling... [640/50048]	Loss: 2.6448
Profiling... [768/50048]	Loss: 2.6350
Profiling... [896/50048]	Loss: 2.9225
Profiling... [1024/50048]	Loss: 3.1654
Profiling... [1152/50048]	Loss: 2.9868
Profiling... [1280/50048]	Loss: 2.6497
Profiling... [1408/50048]	Loss: 2.9471
Profiling... [1536/50048]	Loss: 2.9125
Profiling... [1664/50048]	Loss: 2.8340
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 5, Average loss: 0.0284, Accuracy: 0.2219
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.29401151006832,
                        "time": 2.529557461002696,
                        "accuracy": 0.2219145569620253,
                        "total_cost": 998034.1440735523
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.5841
Profiling... [256/50048]	Loss: 2.7536
Profiling... [384/50048]	Loss: 2.6972
Profiling... [512/50048]	Loss: 3.1797
Profiling... [640/50048]	Loss: 2.9361
Profiling... [768/50048]	Loss: 2.8311
Profiling... [896/50048]	Loss: 2.9425
Profiling... [1024/50048]	Loss: 3.1158
Profiling... [1152/50048]	Loss: 2.5459
Profiling... [1280/50048]	Loss: 2.8733
Profiling... [1408/50048]	Loss: 3.0945
Profiling... [1536/50048]	Loss: 2.7264
Profiling... [1664/50048]	Loss: 2.6814
Profile done
epoch 1 train time consumed: 3.17s
Validation Epoch: 5, Average loss: 0.0281, Accuracy: 0.2323
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.25815312356089,
                        "time": 2.1611978969995107,
                        "accuracy": 0.2322982594936709,
                        "total_cost": 814582.6079030033
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.5070
Profiling... [256/50048]	Loss: 2.4203
Profiling... [384/50048]	Loss: 2.8562
Profiling... [512/50048]	Loss: 2.7441
Profiling... [640/50048]	Loss: 2.8058
Profiling... [768/50048]	Loss: 3.0200
Profiling... [896/50048]	Loss: 2.6967
Profiling... [1024/50048]	Loss: 3.0591
Profiling... [1152/50048]	Loss: 3.0910
Profiling... [1280/50048]	Loss: 3.0225
Profiling... [1408/50048]	Loss: 3.2206
Profiling... [1536/50048]	Loss: 2.9538
Profiling... [1664/50048]	Loss: 3.0234
Profile done
epoch 1 train time consumed: 3.18s
Validation Epoch: 5, Average loss: 0.0495, Accuracy: 0.1858
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.28259730627757,
                        "time": 2.1576438510019216,
                        "accuracy": 0.18581882911392406,
                        "total_cost": 1016662.1477023304
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.4079
Profiling... [256/50048]	Loss: 2.8462
Profiling... [384/50048]	Loss: 2.8272
Profiling... [512/50048]	Loss: 2.8204
Profiling... [640/50048]	Loss: 2.7367
Profiling... [768/50048]	Loss: 3.1468
Profiling... [896/50048]	Loss: 2.9303
Profiling... [1024/50048]	Loss: 2.9965
Profiling... [1152/50048]	Loss: 2.9459
Profiling... [1280/50048]	Loss: 3.0772
Profiling... [1408/50048]	Loss: 2.9120
Profiling... [1536/50048]	Loss: 2.8074
Profiling... [1664/50048]	Loss: 2.7679
Profile done
epoch 1 train time consumed: 3.30s
Validation Epoch: 5, Average loss: 0.0372, Accuracy: 0.1847
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.30874686052658,
                        "time": 2.20425431500189,
                        "accuracy": 0.18473101265822786,
                        "total_cost": 1044740.8277876362
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.3885
Profiling... [256/50048]	Loss: 2.4560
Profiling... [384/50048]	Loss: 2.8870
Profiling... [512/50048]	Loss: 2.7902
Profiling... [640/50048]	Loss: 2.7535
Profiling... [768/50048]	Loss: 2.9003
Profiling... [896/50048]	Loss: 2.7707
Profiling... [1024/50048]	Loss: 2.8639
Profiling... [1152/50048]	Loss: 2.8879
Profiling... [1280/50048]	Loss: 3.0080
Profiling... [1408/50048]	Loss: 2.4928
Profiling... [1536/50048]	Loss: 3.0090
Profiling... [1664/50048]	Loss: 2.7474
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 5, Average loss: 0.0432, Accuracy: 0.2248
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.29014137044089,
                        "time": 2.541893433997757,
                        "accuracy": 0.22478243670886075,
                        "total_cost": 990105.7819280983
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.4644
Profiling... [512/50176]	Loss: 2.5292
Profiling... [768/50176]	Loss: 2.3792
Profiling... [1024/50176]	Loss: 2.3763
Profiling... [1280/50176]	Loss: 2.2391
Profiling... [1536/50176]	Loss: 2.6456
Profiling... [1792/50176]	Loss: 2.4565
Profiling... [2048/50176]	Loss: 2.4427
Profiling... [2304/50176]	Loss: 2.2568
Profiling... [2560/50176]	Loss: 2.4107
Profiling... [2816/50176]	Loss: 2.3979
Profiling... [3072/50176]	Loss: 2.4716
Profiling... [3328/50176]	Loss: 2.2270
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 5, Average loss: 0.0094, Accuracy: 0.3731
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.2660110464252,
                        "time": 2.380007829000533,
                        "accuracy": 0.37314453125,
                        "total_cost": 558454.6055976995
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.5820
Profiling... [512/50176]	Loss: 2.1956
Profiling... [768/50176]	Loss: 2.4051
Profiling... [1024/50176]	Loss: 2.6091
Profiling... [1280/50176]	Loss: 2.6583
Profiling... [1536/50176]	Loss: 2.4531
Profiling... [1792/50176]	Loss: 2.4449
Profiling... [2048/50176]	Loss: 2.5518
Profiling... [2304/50176]	Loss: 2.5663
Profiling... [2560/50176]	Loss: 2.2989
Profiling... [2816/50176]	Loss: 2.3238
Profiling... [3072/50176]	Loss: 2.2323
Profiling... [3328/50176]	Loss: 2.4223
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 5, Average loss: 0.0093, Accuracy: 0.3791
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.30863224432379,
                        "time": 2.3782826470014697,
                        "accuracy": 0.3791015625,
                        "total_cost": 549280.9923415079
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.7107
Profiling... [512/50176]	Loss: 2.3529
Profiling... [768/50176]	Loss: 2.5099
Profiling... [1024/50176]	Loss: 2.2655
Profiling... [1280/50176]	Loss: 2.3455
Profiling... [1536/50176]	Loss: 2.5524
Profiling... [1792/50176]	Loss: 2.3549
Profiling... [2048/50176]	Loss: 2.4028
Profiling... [2304/50176]	Loss: 2.3225
Profiling... [2560/50176]	Loss: 2.3589
Profiling... [2816/50176]	Loss: 2.3884
Profiling... [3072/50176]	Loss: 2.3530
Profiling... [3328/50176]	Loss: 2.3388
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 5, Average loss: 0.0093, Accuracy: 0.3752
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.33522200647062,
                        "time": 2.521788956000819,
                        "accuracy": 0.3751953125,
                        "total_cost": 588488.6328668741
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.6900
Profiling... [512/50176]	Loss: 2.4866
Profiling... [768/50176]	Loss: 2.4915
Profiling... [1024/50176]	Loss: 2.3782
Profiling... [1280/50176]	Loss: 2.4307
Profiling... [1536/50176]	Loss: 2.2546
Profiling... [1792/50176]	Loss: 2.4854
Profiling... [2048/50176]	Loss: 2.4187
Profiling... [2304/50176]	Loss: 2.4597
Profiling... [2560/50176]	Loss: 2.4643
Profiling... [2816/50176]	Loss: 2.3129
Profiling... [3072/50176]	Loss: 2.3697
Profiling... [3328/50176]	Loss: 2.4090
Profile done
epoch 1 train time consumed: 4.25s
Validation Epoch: 5, Average loss: 0.0093, Accuracy: 0.3763
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.30783707933547,
                        "time": 3.005121223999595,
                        "accuracy": 0.37626953125,
                        "total_cost": 699277.6044296806
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.6999
Profiling... [512/50176]	Loss: 2.3835
Profiling... [768/50176]	Loss: 2.3613
Profiling... [1024/50176]	Loss: 2.5691
Profiling... [1280/50176]	Loss: 2.3445
Profiling... [1536/50176]	Loss: 2.4718
Profiling... [1792/50176]	Loss: 2.3280
Profiling... [2048/50176]	Loss: 2.4224
Profiling... [2304/50176]	Loss: 2.3042
Profiling... [2560/50176]	Loss: 2.2636
Profiling... [2816/50176]	Loss: 2.4641
Profiling... [3072/50176]	Loss: 2.4574
Profiling... [3328/50176]	Loss: 2.3669
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 5, Average loss: 0.0093, Accuracy: 0.3737
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.28598318075638,
                        "time": 2.3724275050008146,
                        "accuracy": 0.37373046875,
                        "total_cost": 555803.2304933214
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.5817
Profiling... [512/50176]	Loss: 2.3159
Profiling... [768/50176]	Loss: 2.4226
Profiling... [1024/50176]	Loss: 2.5795
Profiling... [1280/50176]	Loss: 2.4710
Profiling... [1536/50176]	Loss: 2.2443
Profiling... [1792/50176]	Loss: 2.4142
Profiling... [2048/50176]	Loss: 2.3030
Profiling... [2304/50176]	Loss: 2.4030
Profiling... [2560/50176]	Loss: 2.2350
Profiling... [2816/50176]	Loss: 2.4339
Profiling... [3072/50176]	Loss: 2.3817
Profiling... [3328/50176]	Loss: 2.4962
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 5, Average loss: 0.0093, Accuracy: 0.3754
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.3308553113855,
                        "time": 2.3733404030026577,
                        "accuracy": 0.375390625,
                        "total_cost": 553558.2699792776
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.3062
Profiling... [512/50176]	Loss: 2.4318
Profiling... [768/50176]	Loss: 2.4377
Profiling... [1024/50176]	Loss: 2.5679
Profiling... [1280/50176]	Loss: 2.3578
Profiling... [1536/50176]	Loss: 2.4294
Profiling... [1792/50176]	Loss: 2.4835
Profiling... [2048/50176]	Loss: 2.5438
Profiling... [2304/50176]	Loss: 2.5370
Profiling... [2560/50176]	Loss: 2.2037
Profiling... [2816/50176]	Loss: 2.2119
Profiling... [3072/50176]	Loss: 2.2921
Profiling... [3328/50176]	Loss: 2.4451
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 5, Average loss: 0.0094, Accuracy: 0.3720
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.35872316870896,
                        "time": 2.5184391619986854,
                        "accuracy": 0.37197265625,
                        "total_cost": 592798.7105347749
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.5273
Profiling... [512/50176]	Loss: 2.4831
Profiling... [768/50176]	Loss: 2.3905
Profiling... [1024/50176]	Loss: 2.4070
Profiling... [1280/50176]	Loss: 2.3611
Profiling... [1536/50176]	Loss: 2.3769
Profiling... [1792/50176]	Loss: 2.5166
Profiling... [2048/50176]	Loss: 2.3124
Profiling... [2304/50176]	Loss: 2.1385
Profiling... [2560/50176]	Loss: 2.3972
Profiling... [2816/50176]	Loss: 2.4531
Profiling... [3072/50176]	Loss: 2.4815
Profiling... [3328/50176]	Loss: 2.3625
Profile done
epoch 1 train time consumed: 4.33s
Validation Epoch: 5, Average loss: 0.0093, Accuracy: 0.3744
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.3305777364617,
                        "time": 3.0045931880013086,
                        "accuracy": 0.3744140625,
                        "total_cost": 702619.5972392199
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.4651
Profiling... [512/50176]	Loss: 2.4531
Profiling... [768/50176]	Loss: 2.6329
Profiling... [1024/50176]	Loss: 2.7533
Profiling... [1280/50176]	Loss: 2.3120
Profiling... [1536/50176]	Loss: 2.3182
Profiling... [1792/50176]	Loss: 2.3989
Profiling... [2048/50176]	Loss: 2.2652
Profiling... [2304/50176]	Loss: 2.4870
Profiling... [2560/50176]	Loss: 2.4149
Profiling... [2816/50176]	Loss: 2.4241
Profiling... [3072/50176]	Loss: 2.3609
Profiling... [3328/50176]	Loss: 2.3237
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 5, Average loss: 0.0093, Accuracy: 0.3722
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.30684441735943,
                        "time": 2.375731629999791,
                        "accuracy": 0.37216796875,
                        "total_cost": 558914.0940444128
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.3532
Profiling... [512/50176]	Loss: 2.6116
Profiling... [768/50176]	Loss: 2.6486
Profiling... [1024/50176]	Loss: 2.5466
Profiling... [1280/50176]	Loss: 2.4629
Profiling... [1536/50176]	Loss: 2.4014
Profiling... [1792/50176]	Loss: 2.4041
Profiling... [2048/50176]	Loss: 2.4140
Profiling... [2304/50176]	Loss: 2.2688
Profiling... [2560/50176]	Loss: 2.3967
Profiling... [2816/50176]	Loss: 2.3792
Profiling... [3072/50176]	Loss: 2.2919
Profiling... [3328/50176]	Loss: 2.3238
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 5, Average loss: 0.0094, Accuracy: 0.3712
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.3482797345332,
                        "time": 2.3696663590017124,
                        "accuracy": 0.37119140625,
                        "total_cost": 558953.9975567231
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.5016
Profiling... [512/50176]	Loss: 2.5707
Profiling... [768/50176]	Loss: 2.4544
Profiling... [1024/50176]	Loss: 2.3074
Profiling... [1280/50176]	Loss: 2.2949
Profiling... [1536/50176]	Loss: 2.4950
Profiling... [1792/50176]	Loss: 2.5183
Profiling... [2048/50176]	Loss: 2.5006
Profiling... [2304/50176]	Loss: 2.6488
Profiling... [2560/50176]	Loss: 2.3474
Profiling... [2816/50176]	Loss: 2.3320
Profiling... [3072/50176]	Loss: 2.5862
Profiling... [3328/50176]	Loss: 2.4570
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 5, Average loss: 0.0093, Accuracy: 0.3748
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.37534916019435,
                        "time": 2.531043298997247,
                        "accuracy": 0.3748046875,
                        "total_cost": 591263.9555755057
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.6580
Profiling... [512/50176]	Loss: 2.4944
Profiling... [768/50176]	Loss: 2.4772
Profiling... [1024/50176]	Loss: 2.4958
Profiling... [1280/50176]	Loss: 2.4280
Profiling... [1536/50176]	Loss: 2.4841
Profiling... [1792/50176]	Loss: 2.4075
Profiling... [2048/50176]	Loss: 2.3554
Profiling... [2304/50176]	Loss: 2.3532
Profiling... [2560/50176]	Loss: 2.5174
Profiling... [2816/50176]	Loss: 2.4359
Profiling... [3072/50176]	Loss: 2.3806
Profiling... [3328/50176]	Loss: 2.2989
Profile done
epoch 1 train time consumed: 4.31s
Validation Epoch: 5, Average loss: 0.0093, Accuracy: 0.3751
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.34717041446751,
                        "time": 2.978967831997579,
                        "accuracy": 0.37509765625,
                        "total_cost": 695357.6495537714
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.5871
Profiling... [512/50176]	Loss: 2.4449
Profiling... [768/50176]	Loss: 2.3788
Profiling... [1024/50176]	Loss: 2.5495
Profiling... [1280/50176]	Loss: 2.5256
Profiling... [1536/50176]	Loss: 2.7314
Profiling... [1792/50176]	Loss: 2.4070
Profiling... [2048/50176]	Loss: 2.5164
Profiling... [2304/50176]	Loss: 2.4049
Profiling... [2560/50176]	Loss: 2.4392
Profiling... [2816/50176]	Loss: 2.5721
Profiling... [3072/50176]	Loss: 2.3463
Profiling... [3328/50176]	Loss: 2.3812
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 5, Average loss: 0.0102, Accuracy: 0.3361
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.32165465090465,
                        "time": 2.3729360580000503,
                        "accuracy": 0.3361328125,
                        "total_cost": 618104.4021913573
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.5262
Profiling... [512/50176]	Loss: 2.4923
Profiling... [768/50176]	Loss: 2.5439
Profiling... [1024/50176]	Loss: 2.4788
Profiling... [1280/50176]	Loss: 2.4653
Profiling... [1536/50176]	Loss: 2.4336
Profiling... [1792/50176]	Loss: 2.6616
Profiling... [2048/50176]	Loss: 2.5085
Profiling... [2304/50176]	Loss: 2.7788
Profiling... [2560/50176]	Loss: 2.5338
Profiling... [2816/50176]	Loss: 2.3199
Profiling... [3072/50176]	Loss: 2.6475
Profiling... [3328/50176]	Loss: 2.5538
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 5, Average loss: 0.0102, Accuracy: 0.3386
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.36453406331638,
                        "time": 2.3832247009995626,
                        "accuracy": 0.33857421875,
                        "total_cost": 616308.1674511278
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.4452
Profiling... [512/50176]	Loss: 2.5736
Profiling... [768/50176]	Loss: 2.3240
Profiling... [1024/50176]	Loss: 2.5334
Profiling... [1280/50176]	Loss: 2.4345
Profiling... [1536/50176]	Loss: 2.4392
Profiling... [1792/50176]	Loss: 2.5435
Profiling... [2048/50176]	Loss: 2.5235
Profiling... [2304/50176]	Loss: 2.5861
Profiling... [2560/50176]	Loss: 2.3908
Profiling... [2816/50176]	Loss: 2.4480
Profiling... [3072/50176]	Loss: 2.3890
Profiling... [3328/50176]	Loss: 2.3512
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 5, Average loss: 0.0100, Accuracy: 0.3400
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.38966409381906,
                        "time": 2.5185221760002605,
                        "accuracy": 0.3400390625,
                        "total_cost": 648490.8431092665
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.5379
Profiling... [512/50176]	Loss: 2.5306
Profiling... [768/50176]	Loss: 2.4870
Profiling... [1024/50176]	Loss: 2.6274
Profiling... [1280/50176]	Loss: 2.3852
Profiling... [1536/50176]	Loss: 2.4582
Profiling... [1792/50176]	Loss: 2.5288
Profiling... [2048/50176]	Loss: 2.5940
Profiling... [2304/50176]	Loss: 2.2926
Profiling... [2560/50176]	Loss: 2.4780
Profiling... [2816/50176]	Loss: 2.4419
Profiling... [3072/50176]	Loss: 2.4614
Profiling... [3328/50176]	Loss: 2.6028
Profile done
epoch 1 train time consumed: 4.36s
Validation Epoch: 5, Average loss: 0.0097, Accuracy: 0.3539
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.3608854444021,
                        "time": 2.99422980899908,
                        "accuracy": 0.35390625,
                        "total_cost": 740770.5443000825
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.4628
Profiling... [512/50176]	Loss: 2.5041
Profiling... [768/50176]	Loss: 2.4910
Profiling... [1024/50176]	Loss: 2.5311
Profiling... [1280/50176]	Loss: 2.3963
Profiling... [1536/50176]	Loss: 2.3596
Profiling... [1792/50176]	Loss: 2.5204
Profiling... [2048/50176]	Loss: 2.5722
Profiling... [2304/50176]	Loss: 2.4934
Profiling... [2560/50176]	Loss: 2.5711
Profiling... [2816/50176]	Loss: 2.5464
Profiling... [3072/50176]	Loss: 2.7085
Profiling... [3328/50176]	Loss: 2.6375
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 5, Average loss: 0.0098, Accuracy: 0.3517
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.3391773009116,
                        "time": 2.373401135999302,
                        "accuracy": 0.35166015625,
                        "total_cost": 590928.2262211089
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.4585
Profiling... [512/50176]	Loss: 2.5129
Profiling... [768/50176]	Loss: 2.3956
Profiling... [1024/50176]	Loss: 2.4180
Profiling... [1280/50176]	Loss: 2.3218
Profiling... [1536/50176]	Loss: 2.5030
Profiling... [1792/50176]	Loss: 2.4454
Profiling... [2048/50176]	Loss: 2.4396
Profiling... [2304/50176]	Loss: 2.5197
Profiling... [2560/50176]	Loss: 2.4159
Profiling... [2816/50176]	Loss: 2.4030
Profiling... [3072/50176]	Loss: 2.4342
Profiling... [3328/50176]	Loss: 2.3968
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 5, Average loss: 0.0100, Accuracy: 0.3473
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.37822265646018,
                        "time": 2.380713882001146,
                        "accuracy": 0.347265625,
                        "total_cost": 600250.1251670293
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.5480
Profiling... [512/50176]	Loss: 2.4344
Profiling... [768/50176]	Loss: 2.5458
Profiling... [1024/50176]	Loss: 2.4355
Profiling... [1280/50176]	Loss: 2.5367
Profiling... [1536/50176]	Loss: 2.5640
Profiling... [1792/50176]	Loss: 2.6096
Profiling... [2048/50176]	Loss: 2.6812
Profiling... [2304/50176]	Loss: 2.4833
Profiling... [2560/50176]	Loss: 2.4961
Profiling... [2816/50176]	Loss: 2.5650
Profiling... [3072/50176]	Loss: 2.4091
Profiling... [3328/50176]	Loss: 2.5203
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 5, Average loss: 0.0101, Accuracy: 0.3340
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.40827946450952,
                        "time": 2.5208331149988226,
                        "accuracy": 0.333984375,
                        "total_cost": 660853.0079259968
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.5289
Profiling... [512/50176]	Loss: 2.3653
Profiling... [768/50176]	Loss: 2.3379
Profiling... [1024/50176]	Loss: 2.4490
Profiling... [1280/50176]	Loss: 2.6894
Profiling... [1536/50176]	Loss: 2.4432
Profiling... [1792/50176]	Loss: 2.3968
Profiling... [2048/50176]	Loss: 2.3697
Profiling... [2304/50176]	Loss: 2.4246
Profiling... [2560/50176]	Loss: 2.5563
Profiling... [2816/50176]	Loss: 2.4707
Profiling... [3072/50176]	Loss: 2.6655
Profiling... [3328/50176]	Loss: 2.6966
Profile done
epoch 1 train time consumed: 4.31s
Validation Epoch: 5, Average loss: 0.0104, Accuracy: 0.3312
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.38147698376025,
                        "time": 2.995120699997642,
                        "accuracy": 0.33125,
                        "total_cost": 791672.0280567512
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.4609
Profiling... [512/50176]	Loss: 2.6360
Profiling... [768/50176]	Loss: 2.3478
Profiling... [1024/50176]	Loss: 2.4341
Profiling... [1280/50176]	Loss: 2.6339
Profiling... [1536/50176]	Loss: 2.4306
Profiling... [1792/50176]	Loss: 2.3036
Profiling... [2048/50176]	Loss: 2.6925
Profiling... [2304/50176]	Loss: 2.4482
Profiling... [2560/50176]	Loss: 2.4030
Profiling... [2816/50176]	Loss: 2.4408
Profiling... [3072/50176]	Loss: 2.5461
Profiling... [3328/50176]	Loss: 2.4542
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 5, Average loss: 0.0099, Accuracy: 0.3404
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.35717516900009,
                        "time": 2.378373264000402,
                        "accuracy": 0.3404296875,
                        "total_cost": 611701.2760550015
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.6285
Profiling... [512/50176]	Loss: 2.2994
Profiling... [768/50176]	Loss: 2.4451
Profiling... [1024/50176]	Loss: 2.4324
Profiling... [1280/50176]	Loss: 2.6283
Profiling... [1536/50176]	Loss: 2.3613
Profiling... [1792/50176]	Loss: 2.5629
Profiling... [2048/50176]	Loss: 2.6089
Profiling... [2304/50176]	Loss: 2.6080
Profiling... [2560/50176]	Loss: 2.5731
Profiling... [2816/50176]	Loss: 2.5031
Profiling... [3072/50176]	Loss: 2.5842
Profiling... [3328/50176]	Loss: 2.5036
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 5, Average loss: 0.0102, Accuracy: 0.3315
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.39449854317863,
                        "time": 2.3774545029991714,
                        "accuracy": 0.33154296875,
                        "total_cost": 627854.893742983
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.3532
Profiling... [512/50176]	Loss: 2.5060
Profiling... [768/50176]	Loss: 2.4041
Profiling... [1024/50176]	Loss: 2.5649
Profiling... [1280/50176]	Loss: 2.5501
Profiling... [1536/50176]	Loss: 2.4708
Profiling... [1792/50176]	Loss: 2.5041
Profiling... [2048/50176]	Loss: 2.3765
Profiling... [2304/50176]	Loss: 2.5650
Profiling... [2560/50176]	Loss: 2.4854
Profiling... [2816/50176]	Loss: 2.4118
Profiling... [3072/50176]	Loss: 2.5086
Profiling... [3328/50176]	Loss: 2.4700
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 5, Average loss: 0.0100, Accuracy: 0.3402
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.41997999862541,
                        "time": 2.508368206999876,
                        "accuracy": 0.340234375,
                        "total_cost": 645505.6561063803
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.4854
Profiling... [512/50176]	Loss: 2.3047
Profiling... [768/50176]	Loss: 2.5470
Profiling... [1024/50176]	Loss: 2.4984
Profiling... [1280/50176]	Loss: 2.5287
Profiling... [1536/50176]	Loss: 2.3436
Profiling... [1792/50176]	Loss: 2.3089
Profiling... [2048/50176]	Loss: 2.4537
Profiling... [2304/50176]	Loss: 2.2611
Profiling... [2560/50176]	Loss: 2.3458
Profiling... [2816/50176]	Loss: 2.5674
Profiling... [3072/50176]	Loss: 2.2888
Profiling... [3328/50176]	Loss: 2.4720
Profile done
epoch 1 train time consumed: 4.41s
Validation Epoch: 5, Average loss: 0.0098, Accuracy: 0.3472
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.39550385868091,
                        "time": 2.9848302569989755,
                        "accuracy": 0.34716796875,
                        "total_cost": 752777.9396777785
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.6081
Profiling... [512/50176]	Loss: 2.5522
Profiling... [768/50176]	Loss: 2.9002
Profiling... [1024/50176]	Loss: 2.7058
Profiling... [1280/50176]	Loss: 2.5139
Profiling... [1536/50176]	Loss: 2.9469
Profiling... [1792/50176]	Loss: 2.5118
Profiling... [2048/50176]	Loss: 2.7082
Profiling... [2304/50176]	Loss: 2.8328
Profiling... [2560/50176]	Loss: 2.8081
Profiling... [2816/50176]	Loss: 2.7797
Profiling... [3072/50176]	Loss: 2.8027
Profiling... [3328/50176]	Loss: 2.5291
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 5, Average loss: 0.0114, Accuracy: 0.2765
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.37007527446335,
                        "time": 2.37996471400038,
                        "accuracy": 0.27646484375,
                        "total_cost": 753732.8365356205
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.4965
Profiling... [512/50176]	Loss: 2.5164
Profiling... [768/50176]	Loss: 2.6526
Profiling... [1024/50176]	Loss: 2.5641
Profiling... [1280/50176]	Loss: 2.6199
Profiling... [1536/50176]	Loss: 2.9291
Profiling... [1792/50176]	Loss: 2.6024
Profiling... [2048/50176]	Loss: 2.8676
Profiling... [2304/50176]	Loss: 2.6948
Profiling... [2560/50176]	Loss: 2.7399
Profiling... [2816/50176]	Loss: 2.5532
Profiling... [3072/50176]	Loss: 2.8435
Profiling... [3328/50176]	Loss: 2.6909
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 5, Average loss: 0.0151, Accuracy: 0.2431
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.40879470639625,
                        "time": 2.3779548259990406,
                        "accuracy": 0.24306640625,
                        "total_cost": 856575.3779183315
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.4578
Profiling... [512/50176]	Loss: 2.7120
Profiling... [768/50176]	Loss: 2.5681
Profiling... [1024/50176]	Loss: 2.5151
Profiling... [1280/50176]	Loss: 2.4904
Profiling... [1536/50176]	Loss: 2.5238
Profiling... [1792/50176]	Loss: 2.7415
Profiling... [2048/50176]	Loss: 2.5451
Profiling... [2304/50176]	Loss: 2.6746
Profiling... [2560/50176]	Loss: 2.6427
Profiling... [2816/50176]	Loss: 2.7004
Profiling... [3072/50176]	Loss: 2.6373
Profiling... [3328/50176]	Loss: 2.7913
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 5, Average loss: 0.0137, Accuracy: 0.2432
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.43486559952942,
                        "time": 2.5366586320014903,
                        "accuracy": 0.2431640625,
                        "total_cost": 913376.0657429011
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.5566
Profiling... [512/50176]	Loss: 2.5294
Profiling... [768/50176]	Loss: 2.6509
Profiling... [1024/50176]	Loss: 2.7711
Profiling... [1280/50176]	Loss: 2.6901
Profiling... [1536/50176]	Loss: 2.7174
Profiling... [1792/50176]	Loss: 2.8267
Profiling... [2048/50176]	Loss: 2.6405
Profiling... [2304/50176]	Loss: 2.5931
Profiling... [2560/50176]	Loss: 2.7612
Profiling... [2816/50176]	Loss: 2.7361
Profiling... [3072/50176]	Loss: 2.4880
Profiling... [3328/50176]	Loss: 2.7474
Profile done
epoch 1 train time consumed: 4.33s
Validation Epoch: 5, Average loss: 0.0142, Accuracy: 0.2473
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.41046917935257,
                        "time": 2.9852729689991975,
                        "accuracy": 0.247265625,
                        "total_cost": 1057078.4869711131
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.6431
Profiling... [512/50176]	Loss: 2.6315
Profiling... [768/50176]	Loss: 2.6200
Profiling... [1024/50176]	Loss: 2.7725
Profiling... [1280/50176]	Loss: 2.7998
Profiling... [1536/50176]	Loss: 2.6549
Profiling... [1792/50176]	Loss: 2.7855
Profiling... [2048/50176]	Loss: 2.7487
Profiling... [2304/50176]	Loss: 2.5726
Profiling... [2560/50176]	Loss: 2.7463
Profiling... [2816/50176]	Loss: 2.7082
Profiling... [3072/50176]	Loss: 2.6182
Profiling... [3328/50176]	Loss: 2.5787
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 5, Average loss: 0.0172, Accuracy: 0.2252
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.38666219982133,
                        "time": 2.3801509119984985,
                        "accuracy": 0.2251953125,
                        "total_cost": 925405.3785339529
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.5368
Profiling... [512/50176]	Loss: 2.7822
Profiling... [768/50176]	Loss: 2.7487
Profiling... [1024/50176]	Loss: 2.7755
Profiling... [1280/50176]	Loss: 2.7163
Profiling... [1536/50176]	Loss: 2.6628
Profiling... [1792/50176]	Loss: 2.6445
Profiling... [2048/50176]	Loss: 2.7408
Profiling... [2304/50176]	Loss: 2.4655
Profiling... [2560/50176]	Loss: 2.7214
Profiling... [2816/50176]	Loss: 2.5773
Profiling... [3072/50176]	Loss: 2.7707
Profiling... [3328/50176]	Loss: 2.6226
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 5, Average loss: 0.0122, Accuracy: 0.2693
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.42358181989052,
                        "time": 2.3876660940004513,
                        "accuracy": 0.2693359375,
                        "total_cost": 776186.79539294
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.4781
Profiling... [512/50176]	Loss: 2.4071
Profiling... [768/50176]	Loss: 2.5446
Profiling... [1024/50176]	Loss: 2.8641
Profiling... [1280/50176]	Loss: 2.6064
Profiling... [1536/50176]	Loss: 2.7656
Profiling... [1792/50176]	Loss: 2.6758
Profiling... [2048/50176]	Loss: 2.8191
Profiling... [2304/50176]	Loss: 2.9580
Profiling... [2560/50176]	Loss: 2.7495
Profiling... [2816/50176]	Loss: 2.7952
Profiling... [3072/50176]	Loss: 2.6743
Profiling... [3328/50176]	Loss: 2.6046
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 5, Average loss: 0.0127, Accuracy: 0.2508
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.44702857452218,
                        "time": 2.5269800139976724,
                        "accuracy": 0.25078125,
                        "total_cost": 882254.2631146347
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.6151
Profiling... [512/50176]	Loss: 2.7407
Profiling... [768/50176]	Loss: 2.5804
Profiling... [1024/50176]	Loss: 2.6175
Profiling... [1280/50176]	Loss: 2.6896
Profiling... [1536/50176]	Loss: 2.7515
Profiling... [1792/50176]	Loss: 2.8404
Profiling... [2048/50176]	Loss: 2.5273
Profiling... [2304/50176]	Loss: 2.5187
Profiling... [2560/50176]	Loss: 2.5015
Profiling... [2816/50176]	Loss: 2.7050
Profiling... [3072/50176]	Loss: 2.5058
Profiling... [3328/50176]	Loss: 2.6426
Profile done
epoch 1 train time consumed: 4.30s
Validation Epoch: 5, Average loss: 0.0125, Accuracy: 0.2896
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.42409185942853,
                        "time": 2.9745952730008867,
                        "accuracy": 0.28955078125,
                        "total_cost": 899477.0912008577
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.4847
Profiling... [512/50176]	Loss: 2.5532
Profiling... [768/50176]	Loss: 2.6018
Profiling... [1024/50176]	Loss: 2.8183
Profiling... [1280/50176]	Loss: 2.8040
Profiling... [1536/50176]	Loss: 2.7283
Profiling... [1792/50176]	Loss: 2.7485
Profiling... [2048/50176]	Loss: 2.6340
Profiling... [2304/50176]	Loss: 2.6592
Profiling... [2560/50176]	Loss: 2.6711
Profiling... [2816/50176]	Loss: 2.8285
Profiling... [3072/50176]	Loss: 2.6993
Profiling... [3328/50176]	Loss: 2.5156
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 5, Average loss: 0.0130, Accuracy: 0.2403
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.40081351954302,
                        "time": 2.377456368998537,
                        "accuracy": 0.24033203125,
                        "total_cost": 866139.4206160912
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.7249
Profiling... [512/50176]	Loss: 2.7267
Profiling... [768/50176]	Loss: 2.6367
Profiling... [1024/50176]	Loss: 2.5040
Profiling... [1280/50176]	Loss: 2.7547
Profiling... [1536/50176]	Loss: 2.7317
Profiling... [1792/50176]	Loss: 2.7262
Profiling... [2048/50176]	Loss: 2.7130
Profiling... [2304/50176]	Loss: 2.7296
Profiling... [2560/50176]	Loss: 2.6377
Profiling... [2816/50176]	Loss: 2.8257
Profiling... [3072/50176]	Loss: 2.4244
Profiling... [3328/50176]	Loss: 2.6747
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 5, Average loss: 0.0118, Accuracy: 0.2770
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.43734289781953,
                        "time": 2.378676579999592,
                        "accuracy": 0.276953125,
                        "total_cost": 751997.0276092695
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.5084
Profiling... [512/50176]	Loss: 2.5482
Profiling... [768/50176]	Loss: 2.6192
Profiling... [1024/50176]	Loss: 2.7058
Profiling... [1280/50176]	Loss: 2.8416
Profiling... [1536/50176]	Loss: 2.6476
Profiling... [1792/50176]	Loss: 2.7383
Profiling... [2048/50176]	Loss: 2.6268
Profiling... [2304/50176]	Loss: 2.7963
Profiling... [2560/50176]	Loss: 2.8626
Profiling... [2816/50176]	Loss: 2.7579
Profiling... [3072/50176]	Loss: 2.5085
Profiling... [3328/50176]	Loss: 2.5885
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 5, Average loss: 0.0154, Accuracy: 0.2840
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.46118388972543,
                        "time": 2.5142296430021815,
                        "accuracy": 0.283984375,
                        "total_cost": 775171.0649003214
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.4476
Profiling... [512/50176]	Loss: 2.6661
Profiling... [768/50176]	Loss: 2.5930
Profiling... [1024/50176]	Loss: 2.8066
Profiling... [1280/50176]	Loss: 2.7309
Profiling... [1536/50176]	Loss: 2.5502
Profiling... [1792/50176]	Loss: 2.6769
Profiling... [2048/50176]	Loss: 2.6759
Profiling... [2304/50176]	Loss: 2.7668
Profiling... [2560/50176]	Loss: 2.7645
Profiling... [2816/50176]	Loss: 2.7108
Profiling... [3072/50176]	Loss: 2.5840
Profiling... [3328/50176]	Loss: 2.6757
Profile done
epoch 1 train time consumed: 4.28s
Validation Epoch: 5, Average loss: 0.0139, Accuracy: 0.2068
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.44124501537769,
                        "time": 2.992372169999726,
                        "accuracy": 0.2068359375,
                        "total_cost": 1266708.2958015846
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.5520
Profiling... [1024/50176]	Loss: 2.5286
Profiling... [1536/50176]	Loss: 2.4666
Profiling... [2048/50176]	Loss: 2.3952
Profiling... [2560/50176]	Loss: 2.4253
Profiling... [3072/50176]	Loss: 2.4237
Profiling... [3584/50176]	Loss: 2.3389
Profiling... [4096/50176]	Loss: 2.3277
Profiling... [4608/50176]	Loss: 2.3896
Profiling... [5120/50176]	Loss: 2.4208
Profiling... [5632/50176]	Loss: 2.3473
Profiling... [6144/50176]	Loss: 2.3195
Profiling... [6656/50176]	Loss: 2.2925
Profile done
epoch 1 train time consumed: 6.64s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3829
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.45207281165625,
                        "time": 4.497462925999571,
                        "accuracy": 0.38291015625,
                        "total_cost": 1028389.7517778977
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.5327
Profiling... [1024/50176]	Loss: 2.3675
Profiling... [1536/50176]	Loss: 2.4380
Profiling... [2048/50176]	Loss: 2.2403
Profiling... [2560/50176]	Loss: 2.3737
Profiling... [3072/50176]	Loss: 2.3499
Profiling... [3584/50176]	Loss: 2.4160
Profiling... [4096/50176]	Loss: 2.4831
Profiling... [4608/50176]	Loss: 2.4185
Profiling... [5120/50176]	Loss: 2.3757
Profiling... [5632/50176]	Loss: 2.2689
Profiling... [6144/50176]	Loss: 2.3675
Profiling... [6656/50176]	Loss: 2.4327
Profile done
epoch 1 train time consumed: 6.52s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3794
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.52469489381515,
                        "time": 4.4834719030004635,
                        "accuracy": 0.37939453125,
                        "total_cost": 1034690.8292883192
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.5700
Profiling... [1024/50176]	Loss: 2.4637
Profiling... [1536/50176]	Loss: 2.3400
Profiling... [2048/50176]	Loss: 2.3889
Profiling... [2560/50176]	Loss: 2.3761
Profiling... [3072/50176]	Loss: 2.4065
Profiling... [3584/50176]	Loss: 2.4094
Profiling... [4096/50176]	Loss: 2.5242
Profiling... [4608/50176]	Loss: 2.3125
Profiling... [5120/50176]	Loss: 2.3062
Profiling... [5632/50176]	Loss: 2.4733
Profiling... [6144/50176]	Loss: 2.3506
Profiling... [6656/50176]	Loss: 2.3841
Profile done
epoch 1 train time consumed: 6.98s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3761
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.56178854278956,
                        "time": 4.824624371998652,
                        "accuracy": 0.37607421875,
                        "total_cost": 1123252.1286041006
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.5360
Profiling... [1024/50176]	Loss: 2.5193
Profiling... [1536/50176]	Loss: 2.4199
Profiling... [2048/50176]	Loss: 2.4180
Profiling... [2560/50176]	Loss: 2.3996
Profiling... [3072/50176]	Loss: 2.4069
Profiling... [3584/50176]	Loss: 2.3349
Profiling... [4096/50176]	Loss: 2.2797
Profiling... [4608/50176]	Loss: 2.3766
Profiling... [5120/50176]	Loss: 2.3546
Profiling... [5632/50176]	Loss: 2.4093
Profiling... [6144/50176]	Loss: 2.3869
Profiling... [6656/50176]	Loss: 2.3297
Profile done
epoch 1 train time consumed: 8.23s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3797
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.50567480668195,
                        "time": 5.880359835999116,
                        "accuracy": 0.3796875,
                        "total_cost": 1356015.8619276385
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.5212
Profiling... [1024/50176]	Loss: 2.4823
Profiling... [1536/50176]	Loss: 2.3506
Profiling... [2048/50176]	Loss: 2.3778
Profiling... [2560/50176]	Loss: 2.4991
Profiling... [3072/50176]	Loss: 2.3870
Profiling... [3584/50176]	Loss: 2.3309
Profiling... [4096/50176]	Loss: 2.4283
Profiling... [4608/50176]	Loss: 2.3140
Profiling... [5120/50176]	Loss: 2.3180
Profiling... [5632/50176]	Loss: 2.4569
Profiling... [6144/50176]	Loss: 2.3995
Profiling... [6656/50176]	Loss: 2.3672
Profile done
epoch 1 train time consumed: 6.77s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3787
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.51008171904343,
                        "time": 4.483013748998928,
                        "accuracy": 0.3787109375,
                        "total_cost": 1036452.4926324051
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.6133
Profiling... [1024/50176]	Loss: 2.4463
Profiling... [1536/50176]	Loss: 2.4967
Profiling... [2048/50176]	Loss: 2.2551
Profiling... [2560/50176]	Loss: 2.3562
Profiling... [3072/50176]	Loss: 2.3023
Profiling... [3584/50176]	Loss: 2.4481
Profiling... [4096/50176]	Loss: 2.4477
Profiling... [4608/50176]	Loss: 2.4224
Profiling... [5120/50176]	Loss: 2.4991
Profiling... [5632/50176]	Loss: 2.3893
Profiling... [6144/50176]	Loss: 2.4174
Profiling... [6656/50176]	Loss: 2.4283
Profile done
epoch 1 train time consumed: 6.69s
Validation Epoch: 5, Average loss: 0.0047, Accuracy: 0.3746
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.58484827861699,
                        "time": 4.493579286001477,
                        "accuracy": 0.374609375,
                        "total_cost": 1050270.4103339657
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.4720
Profiling... [1024/50176]	Loss: 2.5746
Profiling... [1536/50176]	Loss: 2.3714
Profiling... [2048/50176]	Loss: 2.4374
Profiling... [2560/50176]	Loss: 2.4912
Profiling... [3072/50176]	Loss: 2.4877
Profiling... [3584/50176]	Loss: 2.3620
Profiling... [4096/50176]	Loss: 2.4031
Profiling... [4608/50176]	Loss: 2.3547
Profiling... [5120/50176]	Loss: 2.4001
Profiling... [5632/50176]	Loss: 2.3847
Profiling... [6144/50176]	Loss: 2.2479
Profiling... [6656/50176]	Loss: 2.3718
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3803
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.61814990373567,
                        "time": 4.829445653998846,
                        "accuracy": 0.3802734375,
                        "total_cost": 1111958.9081007165
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.6574
Profiling... [1024/50176]	Loss: 2.3911
Profiling... [1536/50176]	Loss: 2.4030
Profiling... [2048/50176]	Loss: 2.4213
Profiling... [2560/50176]	Loss: 2.4097
Profiling... [3072/50176]	Loss: 2.4675
Profiling... [3584/50176]	Loss: 2.4018
Profiling... [4096/50176]	Loss: 2.3280
Profiling... [4608/50176]	Loss: 2.2736
Profiling... [5120/50176]	Loss: 2.4586
Profiling... [5632/50176]	Loss: 2.3139
Profiling... [6144/50176]	Loss: 2.3178
Profiling... [6656/50176]	Loss: 2.3044
Profile done
epoch 1 train time consumed: 8.36s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3771
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.56225920114866,
                        "time": 5.8701991669986455,
                        "accuracy": 0.3771484375,
                        "total_cost": 1362786.5250070947
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.3479
Profiling... [1024/50176]	Loss: 2.5915
Profiling... [1536/50176]	Loss: 2.6309
Profiling... [2048/50176]	Loss: 2.2991
Profiling... [2560/50176]	Loss: 2.3153
Profiling... [3072/50176]	Loss: 2.3381
Profiling... [3584/50176]	Loss: 2.4111
Profiling... [4096/50176]	Loss: 2.2846
Profiling... [4608/50176]	Loss: 2.4216
Profiling... [5120/50176]	Loss: 2.4534
Profiling... [5632/50176]	Loss: 2.2367
Profiling... [6144/50176]	Loss: 2.3922
Profiling... [6656/50176]	Loss: 2.2900
Profile done
epoch 1 train time consumed: 6.57s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3757
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.56730668821136,
                        "time": 4.495133093001641,
                        "accuracy": 0.37568359375,
                        "total_cost": 1047629.3207850142
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.5314
Profiling... [1024/50176]	Loss: 2.7314
Profiling... [1536/50176]	Loss: 2.4685
Profiling... [2048/50176]	Loss: 2.2724
Profiling... [2560/50176]	Loss: 2.3209
Profiling... [3072/50176]	Loss: 2.3772
Profiling... [3584/50176]	Loss: 2.4169
Profiling... [4096/50176]	Loss: 2.3492
Profiling... [4608/50176]	Loss: 2.3584
Profiling... [5120/50176]	Loss: 2.4306
Profiling... [5632/50176]	Loss: 2.3815
Profiling... [6144/50176]	Loss: 2.3955
Profiling... [6656/50176]	Loss: 2.2684
Profile done
epoch 1 train time consumed: 6.57s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3830
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.64420951324664,
                        "time": 4.490805184999772,
                        "accuracy": 0.3830078125,
                        "total_cost": 1026606.6969261917
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.4602
Profiling... [1024/50176]	Loss: 2.4807
Profiling... [1536/50176]	Loss: 2.4891
Profiling... [2048/50176]	Loss: 2.3846
Profiling... [2560/50176]	Loss: 2.3592
Profiling... [3072/50176]	Loss: 2.3081
Profiling... [3584/50176]	Loss: 2.4166
Profiling... [4096/50176]	Loss: 2.3458
Profiling... [4608/50176]	Loss: 2.3516
Profiling... [5120/50176]	Loss: 2.3213
Profiling... [5632/50176]	Loss: 2.3364
Profiling... [6144/50176]	Loss: 2.3084
Profiling... [6656/50176]	Loss: 2.3436
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3803
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.67335192587277,
                        "time": 4.830887137002719,
                        "accuracy": 0.3802734375,
                        "total_cost": 1112291.1539436379
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.5606
Profiling... [1024/50176]	Loss: 2.4632
Profiling... [1536/50176]	Loss: 2.4523
Profiling... [2048/50176]	Loss: 2.4066
Profiling... [2560/50176]	Loss: 2.3859
Profiling... [3072/50176]	Loss: 2.3989
Profiling... [3584/50176]	Loss: 2.3576
Profiling... [4096/50176]	Loss: 2.3689
Profiling... [4608/50176]	Loss: 2.4474
Profiling... [5120/50176]	Loss: 2.1953
Profiling... [5632/50176]	Loss: 2.2215
Profiling... [6144/50176]	Loss: 2.4266
Profiling... [6656/50176]	Loss: 2.5529
Profile done
epoch 1 train time consumed: 8.20s
Validation Epoch: 5, Average loss: 0.0046, Accuracy: 0.3793
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.61938507495029,
                        "time": 5.870774351998989,
                        "accuracy": 0.379296875,
                        "total_cost": 1355200.559716263
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.5075
Profiling... [1024/50176]	Loss: 2.4727
Profiling... [1536/50176]	Loss: 2.3659
Profiling... [2048/50176]	Loss: 2.4727
Profiling... [2560/50176]	Loss: 2.3106
Profiling... [3072/50176]	Loss: 2.4169
Profiling... [3584/50176]	Loss: 2.4311
Profiling... [4096/50176]	Loss: 2.4082
Profiling... [4608/50176]	Loss: 2.3673
Profiling... [5120/50176]	Loss: 2.4545
Profiling... [5632/50176]	Loss: 2.4815
Profiling... [6144/50176]	Loss: 2.3913
Profiling... [6656/50176]	Loss: 2.3274
Profile done
epoch 1 train time consumed: 6.60s
Validation Epoch: 5, Average loss: 0.0050, Accuracy: 0.3442
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.62877360982262,
                        "time": 4.4943094109985395,
                        "accuracy": 0.34423828125,
                        "total_cost": 1143118.5582035382
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.4563
Profiling... [1024/50176]	Loss: 2.5118
Profiling... [1536/50176]	Loss: 2.5480
Profiling... [2048/50176]	Loss: 2.4952
Profiling... [2560/50176]	Loss: 2.3750
Profiling... [3072/50176]	Loss: 2.4732
Profiling... [3584/50176]	Loss: 2.3308
Profiling... [4096/50176]	Loss: 2.2713
Profiling... [4608/50176]	Loss: 2.5429
Profiling... [5120/50176]	Loss: 2.3130
Profiling... [5632/50176]	Loss: 2.3939
Profiling... [6144/50176]	Loss: 2.5585
Profiling... [6656/50176]	Loss: 2.3621
Profile done
epoch 1 train time consumed: 6.52s
Validation Epoch: 5, Average loss: 0.0048, Accuracy: 0.3593
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.69684670775464,
                        "time": 4.502345709999645,
                        "accuracy": 0.35927734375,
                        "total_cost": 1097227.410981496
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.5472
Profiling... [1024/50176]	Loss: 2.4425
Profiling... [1536/50176]	Loss: 2.4380
Profiling... [2048/50176]	Loss: 2.4060
Profiling... [2560/50176]	Loss: 2.4634
Profiling... [3072/50176]	Loss: 2.4479
Profiling... [3584/50176]	Loss: 2.4195
Profiling... [4096/50176]	Loss: 2.4028
Profiling... [4608/50176]	Loss: 2.4205
Profiling... [5120/50176]	Loss: 2.4971
Profiling... [5632/50176]	Loss: 2.3321
Profiling... [6144/50176]	Loss: 2.6796
Profiling... [6656/50176]	Loss: 2.4035
Profile done
epoch 1 train time consumed: 6.96s
Validation Epoch: 5, Average loss: 0.0047, Accuracy: 0.3697
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.72944750501664,
                        "time": 4.840995487000328,
                        "accuracy": 0.3697265625,
                        "total_cost": 1146414.7006907046
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.5201
Profiling... [1024/50176]	Loss: 2.4374
Profiling... [1536/50176]	Loss: 2.6831
Profiling... [2048/50176]	Loss: 2.3133
Profiling... [2560/50176]	Loss: 2.5422
Profiling... [3072/50176]	Loss: 2.4834
Profiling... [3584/50176]	Loss: 2.4474
Profiling... [4096/50176]	Loss: 2.4691
Profiling... [4608/50176]	Loss: 2.6489
Profiling... [5120/50176]	Loss: 2.3869
Profiling... [5632/50176]	Loss: 2.2660
Profiling... [6144/50176]	Loss: 2.5010
Profiling... [6656/50176]	Loss: 2.4842
Profile done
epoch 1 train time consumed: 8.21s
Validation Epoch: 5, Average loss: 0.0049, Accuracy: 0.3466
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.6736800043546,
                        "time": 5.866732973998296,
                        "accuracy": 0.34658203125,
                        "total_cost": 1482101.2115634386
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.4662
Profiling... [1024/50176]	Loss: 2.3683
Profiling... [1536/50176]	Loss: 2.5627
Profiling... [2048/50176]	Loss: 2.4745
Profiling... [2560/50176]	Loss: 2.4912
Profiling... [3072/50176]	Loss: 2.4796
Profiling... [3584/50176]	Loss: 2.4064
Profiling... [4096/50176]	Loss: 2.4490
Profiling... [4608/50176]	Loss: 2.4935
Profiling... [5120/50176]	Loss: 2.4027
Profiling... [5632/50176]	Loss: 2.4339
Profiling... [6144/50176]	Loss: 2.4455
Profiling... [6656/50176]	Loss: 2.3520
Profile done
epoch 1 train time consumed: 6.50s
Validation Epoch: 5, Average loss: 0.0047, Accuracy: 0.3696
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.6821124828144,
                        "time": 4.488624054003594,
                        "accuracy": 0.36962890625,
                        "total_cost": 1063248.8203716823
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.4299
Profiling... [1024/50176]	Loss: 2.4867
Profiling... [1536/50176]	Loss: 2.4838
Profiling... [2048/50176]	Loss: 2.5105
Profiling... [2560/50176]	Loss: 2.4213
Profiling... [3072/50176]	Loss: 2.4038
Profiling... [3584/50176]	Loss: 2.5161
Profiling... [4096/50176]	Loss: 2.4032
Profiling... [4608/50176]	Loss: 2.3612
Profiling... [5120/50176]	Loss: 2.5107
Profiling... [5632/50176]	Loss: 2.4661
Profiling... [6144/50176]	Loss: 2.4380
Profiling... [6656/50176]	Loss: 2.4617
Profile done
epoch 1 train time consumed: 6.69s
Validation Epoch: 5, Average loss: 0.0048, Accuracy: 0.3588
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.74997531491854,
                        "time": 4.4923639920016285,
                        "accuracy": 0.3587890625,
                        "total_cost": 1096285.108369892
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.4104
Profiling... [1024/50176]	Loss: 2.4324
Profiling... [1536/50176]	Loss: 2.2794
Profiling... [2048/50176]	Loss: 2.4848
Profiling... [2560/50176]	Loss: 2.4744
Profiling... [3072/50176]	Loss: 2.4325
Profiling... [3584/50176]	Loss: 2.3631
Profiling... [4096/50176]	Loss: 2.4784
Profiling... [4608/50176]	Loss: 2.3894
Profiling... [5120/50176]	Loss: 2.4311
Profiling... [5632/50176]	Loss: 2.4238
Profiling... [6144/50176]	Loss: 2.4920
Profiling... [6656/50176]	Loss: 2.4201
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 5, Average loss: 0.0049, Accuracy: 0.3513
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.77722058890171,
                        "time": 4.843272415000683,
                        "accuracy": 0.35126953125,
                        "total_cost": 1207219.539378479
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.5064
Profiling... [1024/50176]	Loss: 2.4503
Profiling... [1536/50176]	Loss: 2.4342
Profiling... [2048/50176]	Loss: 2.4663
Profiling... [2560/50176]	Loss: 2.3531
Profiling... [3072/50176]	Loss: 2.4286
Profiling... [3584/50176]	Loss: 2.4603
Profiling... [4096/50176]	Loss: 2.5939
Profiling... [4608/50176]	Loss: 2.4613
Profiling... [5120/50176]	Loss: 2.4912
Profiling... [5632/50176]	Loss: 2.5320
Profiling... [6144/50176]	Loss: 2.4350
Profiling... [6656/50176]	Loss: 2.3151
Profile done
epoch 1 train time consumed: 8.37s
Validation Epoch: 5, Average loss: 0.0050, Accuracy: 0.3484
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.72529005169118,
                        "time": 5.89480665299925,
                        "accuracy": 0.3484375,
                        "total_cost": 1481263.7245770427
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.5727
Profiling... [1024/50176]	Loss: 2.4215
Profiling... [1536/50176]	Loss: 2.4146
Profiling... [2048/50176]	Loss: 2.5323
Profiling... [2560/50176]	Loss: 2.4199
Profiling... [3072/50176]	Loss: 2.4018
Profiling... [3584/50176]	Loss: 2.4682
Profiling... [4096/50176]	Loss: 2.5087
Profiling... [4608/50176]	Loss: 2.4190
Profiling... [5120/50176]	Loss: 2.5109
Profiling... [5632/50176]	Loss: 2.4201
Profiling... [6144/50176]	Loss: 2.3810
Profiling... [6656/50176]	Loss: 2.3534
Profile done
epoch 1 train time consumed: 6.50s
Validation Epoch: 5, Average loss: 0.0048, Accuracy: 0.3623
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.73657115196102,
                        "time": 4.492407972000365,
                        "accuracy": 0.3623046875,
                        "total_cost": 1085657.8467136764
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.3988
Profiling... [1024/50176]	Loss: 2.5510
Profiling... [1536/50176]	Loss: 2.3822
Profiling... [2048/50176]	Loss: 2.4435
Profiling... [2560/50176]	Loss: 2.4771
Profiling... [3072/50176]	Loss: 2.5628
Profiling... [3584/50176]	Loss: 2.5402
Profiling... [4096/50176]	Loss: 2.3056
Profiling... [4608/50176]	Loss: 2.5779
Profiling... [5120/50176]	Loss: 2.3208
Profiling... [5632/50176]	Loss: 2.4115
Profiling... [6144/50176]	Loss: 2.3517
Profiling... [6656/50176]	Loss: 2.3499
Profile done
epoch 1 train time consumed: 6.54s
Validation Epoch: 5, Average loss: 0.0048, Accuracy: 0.3515
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.8005215313716,
                        "time": 4.489980258000287,
                        "accuracy": 0.35146484375,
                        "total_cost": 1118537.2182261376
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.5265
Profiling... [1024/50176]	Loss: 2.4243
Profiling... [1536/50176]	Loss: 2.4143
Profiling... [2048/50176]	Loss: 2.4106
Profiling... [2560/50176]	Loss: 2.5816
Profiling... [3072/50176]	Loss: 2.3980
Profiling... [3584/50176]	Loss: 2.4751
Profiling... [4096/50176]	Loss: 2.4684
Profiling... [4608/50176]	Loss: 2.3848
Profiling... [5120/50176]	Loss: 2.4338
Profiling... [5632/50176]	Loss: 2.3773
Profiling... [6144/50176]	Loss: 2.4398
Profiling... [6656/50176]	Loss: 2.4523
Profile done
epoch 1 train time consumed: 6.99s
Validation Epoch: 5, Average loss: 0.0049, Accuracy: 0.3521
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.82973268702796,
                        "time": 4.834417076002865,
                        "accuracy": 0.3521484375,
                        "total_cost": 1202005.1264417216
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.4504
Profiling... [1024/50176]	Loss: 2.5152
Profiling... [1536/50176]	Loss: 2.4247
Profiling... [2048/50176]	Loss: 2.4527
Profiling... [2560/50176]	Loss: 2.3860
Profiling... [3072/50176]	Loss: 2.4490
Profiling... [3584/50176]	Loss: 2.5339
Profiling... [4096/50176]	Loss: 2.3648
Profiling... [4608/50176]	Loss: 2.5692
Profiling... [5120/50176]	Loss: 2.3668
Profiling... [5632/50176]	Loss: 2.2754
Profiling... [6144/50176]	Loss: 2.4509
Profiling... [6656/50176]	Loss: 2.4051
Profile done
epoch 1 train time consumed: 8.23s
Validation Epoch: 5, Average loss: 0.0048, Accuracy: 0.3594
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.77688714474093,
                        "time": 5.862338660997921,
                        "accuracy": 0.359375,
                        "total_cost": 1428271.8636246424
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.4242
Profiling... [1024/50176]	Loss: 2.5038
Profiling... [1536/50176]	Loss: 2.6327
Profiling... [2048/50176]	Loss: 2.5289
Profiling... [2560/50176]	Loss: 2.5107
Profiling... [3072/50176]	Loss: 2.6410
Profiling... [3584/50176]	Loss: 2.6106
Profiling... [4096/50176]	Loss: 2.6889
Profiling... [4608/50176]	Loss: 2.5574
Profiling... [5120/50176]	Loss: 2.5816
Profiling... [5632/50176]	Loss: 2.6486
Profiling... [6144/50176]	Loss: 2.4359
Profiling... [6656/50176]	Loss: 2.5081
Profile done
epoch 1 train time consumed: 6.61s
Validation Epoch: 5, Average loss: 0.0055, Accuracy: 0.2942
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.78632478421646,
                        "time": 4.488286492000043,
                        "accuracy": 0.29423828125,
                        "total_cost": 1335578.0051784455
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.4961
Profiling... [1024/50176]	Loss: 2.6625
Profiling... [1536/50176]	Loss: 2.6003
Profiling... [2048/50176]	Loss: 2.7878
Profiling... [2560/50176]	Loss: 2.5986
Profiling... [3072/50176]	Loss: 2.7313
Profiling... [3584/50176]	Loss: 2.5214
Profiling... [4096/50176]	Loss: 2.6309
Profiling... [4608/50176]	Loss: 2.4520
Profiling... [5120/50176]	Loss: 2.5942
Profiling... [5632/50176]	Loss: 2.5691
Profiling... [6144/50176]	Loss: 2.5069
Profiling... [6656/50176]	Loss: 2.3908
Profile done
epoch 1 train time consumed: 6.62s
Validation Epoch: 5, Average loss: 0.0063, Accuracy: 0.2554
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.84738357382348,
                        "time": 4.487007237999933,
                        "accuracy": 0.25537109375,
                        "total_cost": 1538413.377447645
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.3877
Profiling... [1024/50176]	Loss: 2.7825
Profiling... [1536/50176]	Loss: 2.5449
Profiling... [2048/50176]	Loss: 2.5220
Profiling... [2560/50176]	Loss: 2.5505
Profiling... [3072/50176]	Loss: 2.6715
Profiling... [3584/50176]	Loss: 2.4896
Profiling... [4096/50176]	Loss: 2.4916
Profiling... [4608/50176]	Loss: 2.5590
Profiling... [5120/50176]	Loss: 2.5183
Profiling... [5632/50176]	Loss: 2.6047
Profiling... [6144/50176]	Loss: 2.5907
Profiling... [6656/50176]	Loss: 2.6007
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 5, Average loss: 0.0063, Accuracy: 0.2584
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.87416245412737,
                        "time": 4.826730289001716,
                        "accuracy": 0.2583984375,
                        "total_cost": 1635502.5631957687
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.4654
Profiling... [1024/50176]	Loss: 2.6680
Profiling... [1536/50176]	Loss: 2.4522
Profiling... [2048/50176]	Loss: 2.5604
Profiling... [2560/50176]	Loss: 2.6461
Profiling... [3072/50176]	Loss: 2.5994
Profiling... [3584/50176]	Loss: 2.6151
Profiling... [4096/50176]	Loss: 2.5136
Profiling... [4608/50176]	Loss: 2.6038
Profiling... [5120/50176]	Loss: 2.5883
Profiling... [5632/50176]	Loss: 2.6022
Profiling... [6144/50176]	Loss: 2.5687
Profiling... [6656/50176]	Loss: 2.5167
Profile done
epoch 1 train time consumed: 8.28s
Validation Epoch: 5, Average loss: 0.0063, Accuracy: 0.2750
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.82294674200146,
                        "time": 5.863357584999903,
                        "accuracy": 0.275,
                        "total_cost": 1866816.543010045
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.3897
Profiling... [1024/50176]	Loss: 2.8387
Profiling... [1536/50176]	Loss: 2.5842
Profiling... [2048/50176]	Loss: 2.6527
Profiling... [2560/50176]	Loss: 2.5500
Profiling... [3072/50176]	Loss: 2.6850
Profiling... [3584/50176]	Loss: 2.4299
Profiling... [4096/50176]	Loss: 2.4719
Profiling... [4608/50176]	Loss: 2.4483
Profiling... [5120/50176]	Loss: 2.4937
Profiling... [5632/50176]	Loss: 2.4618
Profiling... [6144/50176]	Loss: 2.4308
Profiling... [6656/50176]	Loss: 2.5964
Profile done
epoch 1 train time consumed: 6.54s
Validation Epoch: 5, Average loss: 0.0058, Accuracy: 0.2895
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.83040731004941,
                        "time": 4.485908154998469,
                        "accuracy": 0.289453125,
                        "total_cost": 1356938.3194066675
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.5545
Profiling... [1024/50176]	Loss: 2.6998
Profiling... [1536/50176]	Loss: 2.5022
Profiling... [2048/50176]	Loss: 2.5258
Profiling... [2560/50176]	Loss: 2.6326
Profiling... [3072/50176]	Loss: 2.6257
Profiling... [3584/50176]	Loss: 2.6067
Profiling... [4096/50176]	Loss: 2.5587
Profiling... [4608/50176]	Loss: 2.7187
Profiling... [5120/50176]	Loss: 2.4882
Profiling... [5632/50176]	Loss: 2.5612
Profiling... [6144/50176]	Loss: 2.4815
Profiling... [6656/50176]	Loss: 2.4896
Profile done
epoch 1 train time consumed: 6.61s
Validation Epoch: 5, Average loss: 0.0060, Accuracy: 0.2821
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.89312367582991,
                        "time": 4.497862783002347,
                        "accuracy": 0.28212890625,
                        "total_cost": 1395875.692558265
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.4876
Profiling... [1024/50176]	Loss: 2.6868
Profiling... [1536/50176]	Loss: 2.5957
Profiling... [2048/50176]	Loss: 2.5501
Profiling... [2560/50176]	Loss: 2.5599
Profiling... [3072/50176]	Loss: 2.5405
Profiling... [3584/50176]	Loss: 2.5203
Profiling... [4096/50176]	Loss: 2.5065
Profiling... [4608/50176]	Loss: 2.5342
Profiling... [5120/50176]	Loss: 2.4962
Profiling... [5632/50176]	Loss: 2.6312
Profiling... [6144/50176]	Loss: 2.5292
Profiling... [6656/50176]	Loss: 2.5509
Profile done
epoch 1 train time consumed: 7.01s
Validation Epoch: 5, Average loss: 0.0056, Accuracy: 0.3089
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.92006670023117,
                        "time": 4.834246586000518,
                        "accuracy": 0.30888671875,
                        "total_cost": 1370306.627981277
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.5024
Profiling... [1024/50176]	Loss: 2.9143
Profiling... [1536/50176]	Loss: 2.6917
Profiling... [2048/50176]	Loss: 2.6836
Profiling... [2560/50176]	Loss: 2.7417
Profiling... [3072/50176]	Loss: 2.7073
Profiling... [3584/50176]	Loss: 2.5463
Profiling... [4096/50176]	Loss: 2.6521
Profiling... [4608/50176]	Loss: 2.5979
Profiling... [5120/50176]	Loss: 2.6198
Profiling... [5632/50176]	Loss: 2.5460
Profiling... [6144/50176]	Loss: 2.4824
Profiling... [6656/50176]	Loss: 2.6179
Profile done
epoch 1 train time consumed: 8.24s
Validation Epoch: 5, Average loss: 0.0059, Accuracy: 0.2771
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.87118927922974,
                        "time": 5.8733734320012445,
                        "accuracy": 0.27705078125,
                        "total_cost": 1856163.8422461022
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.4828
Profiling... [1024/50176]	Loss: 2.7066
Profiling... [1536/50176]	Loss: 2.7291
Profiling... [2048/50176]	Loss: 2.8002
Profiling... [2560/50176]	Loss: 2.5325
Profiling... [3072/50176]	Loss: 2.6982
Profiling... [3584/50176]	Loss: 2.4859
Profiling... [4096/50176]	Loss: 2.7613
Profiling... [4608/50176]	Loss: 2.5741
Profiling... [5120/50176]	Loss: 2.5358
Profiling... [5632/50176]	Loss: 2.4680
Profiling... [6144/50176]	Loss: 2.4594
Profiling... [6656/50176]	Loss: 2.4810
Profile done
epoch 1 train time consumed: 6.61s
Validation Epoch: 5, Average loss: 0.0062, Accuracy: 0.2700
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.87976247075281,
                        "time": 4.493168796998361,
                        "accuracy": 0.27001953125,
                        "total_cost": 1456953.3612233086
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.4638
Profiling... [1024/50176]	Loss: 2.6939
Profiling... [1536/50176]	Loss: 2.6337
Profiling... [2048/50176]	Loss: 2.5851
Profiling... [2560/50176]	Loss: 2.6680
Profiling... [3072/50176]	Loss: 2.5551
Profiling... [3584/50176]	Loss: 2.5658
Profiling... [4096/50176]	Loss: 2.5217
Profiling... [4608/50176]	Loss: 2.7134
Profiling... [5120/50176]	Loss: 2.5282
Profiling... [5632/50176]	Loss: 2.4439
Profiling... [6144/50176]	Loss: 2.5670
Profiling... [6656/50176]	Loss: 2.3982
Profile done
epoch 1 train time consumed: 6.69s
Validation Epoch: 5, Average loss: 0.0056, Accuracy: 0.2945
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.93743483647663,
                        "time": 4.489912490000279,
                        "accuracy": 0.29453125,
                        "total_cost": 1334734.030683178
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.4452
Profiling... [1024/50176]	Loss: 2.4930
Profiling... [1536/50176]	Loss: 2.5282
Profiling... [2048/50176]	Loss: 2.5426
Profiling... [2560/50176]	Loss: 2.6985
Profiling... [3072/50176]	Loss: 2.6075
Profiling... [3584/50176]	Loss: 2.6252
Profiling... [4096/50176]	Loss: 2.6457
Profiling... [4608/50176]	Loss: 2.5707
Profiling... [5120/50176]	Loss: 2.4850
Profiling... [5632/50176]	Loss: 2.5473
Profiling... [6144/50176]	Loss: 2.4730
Profiling... [6656/50176]	Loss: 2.5214
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 5, Average loss: 0.0057, Accuracy: 0.2784
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.96330548899583,
                        "time": 4.83808001700163,
                        "accuracy": 0.27841796875,
                        "total_cost": 1521472.4327777892
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.3614
Profiling... [1024/50176]	Loss: 2.6645
Profiling... [1536/50176]	Loss: 2.5328
Profiling... [2048/50176]	Loss: 2.6546
Profiling... [2560/50176]	Loss: 2.5288
Profiling... [3072/50176]	Loss: 2.4332
Profiling... [3584/50176]	Loss: 2.5317
Profiling... [4096/50176]	Loss: 2.7268
Profiling... [4608/50176]	Loss: 2.4190
Profiling... [5120/50176]	Loss: 2.6450
Profiling... [5632/50176]	Loss: 2.6091
Profiling... [6144/50176]	Loss: 2.6975
Profiling... [6656/50176]	Loss: 2.6066
Profile done
epoch 1 train time consumed: 8.32s
Validation Epoch: 5, Average loss: 0.0059, Accuracy: 0.2858
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.91392673868125,
                        "time": 5.879566269999486,
                        "accuracy": 0.28583984375,
                        "total_cost": 1800987.519195312
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.5602
Profiling... [2048/50176]	Loss: 2.4914
Profiling... [3072/50176]	Loss: 2.4856
Profiling... [4096/50176]	Loss: 2.3978
Profiling... [5120/50176]	Loss: 2.4172
Profiling... [6144/50176]	Loss: 2.3327
Profiling... [7168/50176]	Loss: 2.3870
Profiling... [8192/50176]	Loss: 2.3633
Profiling... [9216/50176]	Loss: 2.3619
Profiling... [10240/50176]	Loss: 2.3456
Profiling... [11264/50176]	Loss: 2.3894
Profiling... [12288/50176]	Loss: 2.2531
Profiling... [13312/50176]	Loss: 2.2994
Profile done
epoch 1 train time consumed: 12.63s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3839
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.96689391117827,
                        "time": 8.772871749999467,
                        "accuracy": 0.38388671875,
                        "total_cost": 2000907.4621342123
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.5280
Profiling... [2048/50176]	Loss: 2.4770
Profiling... [3072/50176]	Loss: 2.3963
Profiling... [4096/50176]	Loss: 2.3620
Profiling... [5120/50176]	Loss: 2.3924
Profiling... [6144/50176]	Loss: 2.4353
Profiling... [7168/50176]	Loss: 2.3610
Profiling... [8192/50176]	Loss: 2.3350
Profiling... [9216/50176]	Loss: 2.2881
Profiling... [10240/50176]	Loss: 2.3597
Profiling... [11264/50176]	Loss: 2.4037
Profiling... [12288/50176]	Loss: 2.4359
Profiling... [13312/50176]	Loss: 2.3520
Profile done
epoch 1 train time consumed: 12.55s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3842
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.07778195179867,
                        "time": 8.801505703002476,
                        "accuracy": 0.3841796875,
                        "total_cost": 2005908.6970444855
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.4654
Profiling... [2048/50176]	Loss: 2.4574
Profiling... [3072/50176]	Loss: 2.3512
Profiling... [4096/50176]	Loss: 2.5087
Profiling... [5120/50176]	Loss: 2.3665
Profiling... [6144/50176]	Loss: 2.3106
Profiling... [7168/50176]	Loss: 2.4077
Profiling... [8192/50176]	Loss: 2.3470
Profiling... [9216/50176]	Loss: 2.2912
Profiling... [10240/50176]	Loss: 2.3357
Profiling... [11264/50176]	Loss: 2.2729
Profiling... [12288/50176]	Loss: 2.3795
Profiling... [13312/50176]	Loss: 2.3095
Profile done
epoch 1 train time consumed: 13.54s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3857
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.11389528067997,
                        "time": 9.512002272000245,
                        "accuracy": 0.3857421875,
                        "total_cost": 2159053.8852182818
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.5531
Profiling... [2048/50176]	Loss: 2.4717
Profiling... [3072/50176]	Loss: 2.5345
Profiling... [4096/50176]	Loss: 2.5059
Profiling... [5120/50176]	Loss: 2.3033
Profiling... [6144/50176]	Loss: 2.3627
Profiling... [7168/50176]	Loss: 2.3333
Profiling... [8192/50176]	Loss: 2.2892
Profiling... [9216/50176]	Loss: 2.4323
Profiling... [10240/50176]	Loss: 2.3658
Profiling... [11264/50176]	Loss: 2.4131
Profiling... [12288/50176]	Loss: 2.3300
Profiling... [13312/50176]	Loss: 2.3176
Profile done
epoch 1 train time consumed: 19.75s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3830
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.9790160230629,
                        "time": 13.718994101000135,
                        "accuracy": 0.3830078125,
                        "total_cost": 3136194.4165686974
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.3972
Profiling... [2048/50176]	Loss: 2.4428
Profiling... [3072/50176]	Loss: 2.5317
Profiling... [4096/50176]	Loss: 2.3660
Profiling... [5120/50176]	Loss: 2.3900
Profiling... [6144/50176]	Loss: 2.4270
Profiling... [7168/50176]	Loss: 2.3691
Profiling... [8192/50176]	Loss: 2.3802
Profiling... [9216/50176]	Loss: 2.4311
Profiling... [10240/50176]	Loss: 2.3452
Profiling... [11264/50176]	Loss: 2.3099
Profiling... [12288/50176]	Loss: 2.2848
Profiling... [13312/50176]	Loss: 2.3704
Profile done
epoch 1 train time consumed: 12.50s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3851
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.03436596687072,
                        "time": 8.775893686000927,
                        "accuracy": 0.38505859375,
                        "total_cost": 1995505.875693434
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.5276
Profiling... [2048/50176]	Loss: 2.4857
Profiling... [3072/50176]	Loss: 2.3747
Profiling... [4096/50176]	Loss: 2.4208
Profiling... [5120/50176]	Loss: 2.3700
Profiling... [6144/50176]	Loss: 2.4013
Profiling... [7168/50176]	Loss: 2.2993
Profiling... [8192/50176]	Loss: 2.3469
Profiling... [9216/50176]	Loss: 2.3364
Profiling... [10240/50176]	Loss: 2.2840
Profiling... [11264/50176]	Loss: 2.3889
Profiling... [12288/50176]	Loss: 2.2843
Profiling... [13312/50176]	Loss: 2.4065
Profile done
epoch 1 train time consumed: 12.58s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3842
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.14413950839922,
                        "time": 8.792197857001156,
                        "accuracy": 0.3841796875,
                        "total_cost": 2003788.150090731
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.4997
Profiling... [2048/50176]	Loss: 2.4659
Profiling... [3072/50176]	Loss: 2.4004
Profiling... [4096/50176]	Loss: 2.3869
Profiling... [5120/50176]	Loss: 2.3900
Profiling... [6144/50176]	Loss: 2.3337
Profiling... [7168/50176]	Loss: 2.3959
Profiling... [8192/50176]	Loss: 2.3455
Profiling... [9216/50176]	Loss: 2.3101
Profiling... [10240/50176]	Loss: 2.2214
Profiling... [11264/50176]	Loss: 2.4012
Profiling... [12288/50176]	Loss: 2.3954
Profiling... [13312/50176]	Loss: 2.3020
Profile done
epoch 1 train time consumed: 13.49s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3831
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.17457433694459,
                        "time": 9.523242216000654,
                        "accuracy": 0.38310546875,
                        "total_cost": 2176483.1263378016
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.4817
Profiling... [2048/50176]	Loss: 2.4604
Profiling... [3072/50176]	Loss: 2.4326
Profiling... [4096/50176]	Loss: 2.4311
Profiling... [5120/50176]	Loss: 2.4653
Profiling... [6144/50176]	Loss: 2.4329
Profiling... [7168/50176]	Loss: 2.4159
Profiling... [8192/50176]	Loss: 2.3301
Profiling... [9216/50176]	Loss: 2.3317
Profiling... [10240/50176]	Loss: 2.2167
Profiling... [11264/50176]	Loss: 2.3293
Profiling... [12288/50176]	Loss: 2.3222
Profiling... [13312/50176]	Loss: 2.3060
Profile done
epoch 1 train time consumed: 16.29s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3824
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.0749795874503,
                        "time": 11.714086377000058,
                        "accuracy": 0.382421875,
                        "total_cost": 2681972.214655575
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.4233
Profiling... [2048/50176]	Loss: 2.4307
Profiling... [3072/50176]	Loss: 2.4062
Profiling... [4096/50176]	Loss: 2.3556
Profiling... [5120/50176]	Loss: 2.3621
Profiling... [6144/50176]	Loss: 2.2832
Profiling... [7168/50176]	Loss: 2.3801
Profiling... [8192/50176]	Loss: 2.4283
Profiling... [9216/50176]	Loss: 2.4503
Profiling... [10240/50176]	Loss: 2.3510
Profiling... [11264/50176]	Loss: 2.2741
Profiling... [12288/50176]	Loss: 2.3470
Profiling... [13312/50176]	Loss: 2.3747
Profile done
epoch 1 train time consumed: 12.60s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3829
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.12609407431607,
                        "time": 8.768861378997826,
                        "accuracy": 0.38291015625,
                        "total_cost": 2005095.3249714235
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.5223
Profiling... [2048/50176]	Loss: 2.4170
Profiling... [3072/50176]	Loss: 2.4550
Profiling... [4096/50176]	Loss: 2.4491
Profiling... [5120/50176]	Loss: 2.3929
Profiling... [6144/50176]	Loss: 2.3908
Profiling... [7168/50176]	Loss: 2.3712
Profiling... [8192/50176]	Loss: 2.3428
Profiling... [9216/50176]	Loss: 2.4308
Profiling... [10240/50176]	Loss: 2.3166
Profiling... [11264/50176]	Loss: 2.3463
Profiling... [12288/50176]	Loss: 2.2693
Profiling... [13312/50176]	Loss: 2.3772
Profile done
epoch 1 train time consumed: 12.59s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3848
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.23071923454252,
                        "time": 8.787211327002296,
                        "accuracy": 0.384765625,
                        "total_cost": 1999602.9589234018
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.5480
Profiling... [2048/50176]	Loss: 2.4351
Profiling... [3072/50176]	Loss: 2.4285
Profiling... [4096/50176]	Loss: 2.4056
Profiling... [5120/50176]	Loss: 2.4157
Profiling... [6144/50176]	Loss: 2.3087
Profiling... [7168/50176]	Loss: 2.4137
Profiling... [8192/50176]	Loss: 2.4374
Profiling... [9216/50176]	Loss: 2.3209
Profiling... [10240/50176]	Loss: 2.2496
Profiling... [11264/50176]	Loss: 2.3401
Profiling... [12288/50176]	Loss: 2.4300
Profiling... [13312/50176]	Loss: 2.2565
Profile done
epoch 1 train time consumed: 13.52s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3816
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.26438163796296,
                        "time": 9.625388956999814,
                        "accuracy": 0.381640625,
                        "total_cost": 2208272.8760901787
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.4391
Profiling... [2048/50176]	Loss: 2.3735
Profiling... [3072/50176]	Loss: 2.3864
Profiling... [4096/50176]	Loss: 2.4176
Profiling... [5120/50176]	Loss: 2.3976
Profiling... [6144/50176]	Loss: 2.4371
Profiling... [7168/50176]	Loss: 2.4183
Profiling... [8192/50176]	Loss: 2.3775
Profiling... [9216/50176]	Loss: 2.4207
Profiling... [10240/50176]	Loss: 2.3689
Profiling... [11264/50176]	Loss: 2.3431
Profiling... [12288/50176]	Loss: 2.3451
Profiling... [13312/50176]	Loss: 2.3211
Profile done
epoch 1 train time consumed: 20.63s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3832
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.12801308324747,
                        "time": 13.971896625997033,
                        "accuracy": 0.383203125,
                        "total_cost": 3192383.3116624304
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.4180
Profiling... [2048/50176]	Loss: 2.3806
Profiling... [3072/50176]	Loss: 2.3897
Profiling... [4096/50176]	Loss: 2.3813
Profiling... [5120/50176]	Loss: 2.3461
Profiling... [6144/50176]	Loss: 2.3289
Profiling... [7168/50176]	Loss: 2.3877
Profiling... [8192/50176]	Loss: 2.4826
Profiling... [9216/50176]	Loss: 2.3974
Profiling... [10240/50176]	Loss: 2.2633
Profiling... [11264/50176]	Loss: 2.2626
Profiling... [12288/50176]	Loss: 2.3607
Profiling... [13312/50176]	Loss: 2.3632
Profile done
epoch 1 train time consumed: 12.78s
Validation Epoch: 5, Average loss: 0.0024, Accuracy: 0.3640
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.18953424332715,
                        "time": 8.982139277999522,
                        "accuracy": 0.36396484375,
                        "total_cost": 2160773.3340471867
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.4981
Profiling... [2048/50176]	Loss: 2.3368
Profiling... [3072/50176]	Loss: 2.4341
Profiling... [4096/50176]	Loss: 2.4527
Profiling... [5120/50176]	Loss: 2.2975
Profiling... [6144/50176]	Loss: 2.3047
Profiling... [7168/50176]	Loss: 2.4349
Profiling... [8192/50176]	Loss: 2.3220
Profiling... [9216/50176]	Loss: 2.4872
Profiling... [10240/50176]	Loss: 2.4428
Profiling... [11264/50176]	Loss: 2.3774
Profiling... [12288/50176]	Loss: 2.3491
Profiling... [13312/50176]	Loss: 2.3114
Profile done
epoch 1 train time consumed: 12.76s
Validation Epoch: 5, Average loss: 0.0024, Accuracy: 0.3646
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.29151761741504,
                        "time": 9.016811178000353,
                        "accuracy": 0.3646484375,
                        "total_cost": 2165049.0198144442
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.4724
Profiling... [2048/50176]	Loss: 2.3997
Profiling... [3072/50176]	Loss: 2.3936
Profiling... [4096/50176]	Loss: 2.3606
Profiling... [5120/50176]	Loss: 2.3017
Profiling... [6144/50176]	Loss: 2.4182
Profiling... [7168/50176]	Loss: 2.3925
Profiling... [8192/50176]	Loss: 2.3407
Profiling... [9216/50176]	Loss: 2.3132
Profiling... [10240/50176]	Loss: 2.3718
Profiling... [11264/50176]	Loss: 2.3747
Profiling... [12288/50176]	Loss: 2.2847
Profiling... [13312/50176]	Loss: 2.3637
Profile done
epoch 1 train time consumed: 13.52s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3509
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.32329764350496,
                        "time": 9.547619425000448,
                        "accuracy": 0.35087890625,
                        "total_cost": 2382467.764964091
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.5247
Profiling... [2048/50176]	Loss: 2.4523
Profiling... [3072/50176]	Loss: 2.4086
Profiling... [4096/50176]	Loss: 2.4216
Profiling... [5120/50176]	Loss: 2.4049
Profiling... [6144/50176]	Loss: 2.3408
Profiling... [7168/50176]	Loss: 2.4028
Profiling... [8192/50176]	Loss: 2.4125
Profiling... [9216/50176]	Loss: 2.3733
Profiling... [10240/50176]	Loss: 2.3367
Profiling... [11264/50176]	Loss: 2.3443
Profiling... [12288/50176]	Loss: 2.3144
Profiling... [13312/50176]	Loss: 2.3516
Profile done
epoch 1 train time consumed: 22.63s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3776
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.17912345026573,
                        "time": 16.63365105900084,
                        "accuracy": 0.37763671875,
                        "total_cost": 3856578.7869003173
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.5175
Profiling... [2048/50176]	Loss: 2.3591
Profiling... [3072/50176]	Loss: 2.5087
Profiling... [4096/50176]	Loss: 2.4507
Profiling... [5120/50176]	Loss: 2.3230
Profiling... [6144/50176]	Loss: 2.3734
Profiling... [7168/50176]	Loss: 2.3389
Profiling... [8192/50176]	Loss: 2.3867
Profiling... [9216/50176]	Loss: 2.3494
Profiling... [10240/50176]	Loss: 2.3746
Profiling... [11264/50176]	Loss: 2.3084
Profiling... [12288/50176]	Loss: 2.3667
Profiling... [13312/50176]	Loss: 2.3052
Profile done
epoch 1 train time consumed: 12.63s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3772
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.2343890176437,
                        "time": 8.882734811999399,
                        "accuracy": 0.37724609375,
                        "total_cost": 2061630.9206636248
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.3960
Profiling... [2048/50176]	Loss: 2.4015
Profiling... [3072/50176]	Loss: 2.4165
Profiling... [4096/50176]	Loss: 2.3388
Profiling... [5120/50176]	Loss: 2.3447
Profiling... [6144/50176]	Loss: 2.3814
Profiling... [7168/50176]	Loss: 2.3952
Profiling... [8192/50176]	Loss: 2.3798
Profiling... [9216/50176]	Loss: 2.3623
Profiling... [10240/50176]	Loss: 2.3553
Profiling... [11264/50176]	Loss: 2.2356
Profiling... [12288/50176]	Loss: 2.4055
Profiling... [13312/50176]	Loss: 2.2818
Profile done
epoch 1 train time consumed: 12.69s
Validation Epoch: 5, Average loss: 0.0024, Accuracy: 0.3641
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.33776417921513,
                        "time": 8.818980559997726,
                        "accuracy": 0.3640625,
                        "total_cost": 2120956.0467483085
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.5000
Profiling... [2048/50176]	Loss: 2.4002
Profiling... [3072/50176]	Loss: 2.4178
Profiling... [4096/50176]	Loss: 2.3563
Profiling... [5120/50176]	Loss: 2.3246
Profiling... [6144/50176]	Loss: 2.3326
Profiling... [7168/50176]	Loss: 2.3770
Profiling... [8192/50176]	Loss: 2.4540
Profiling... [9216/50176]	Loss: 2.3593
Profiling... [10240/50176]	Loss: 2.3423
Profiling... [11264/50176]	Loss: 2.2863
Profiling... [12288/50176]	Loss: 2.3376
Profiling... [13312/50176]	Loss: 2.3119
Profile done
epoch 1 train time consumed: 13.61s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3722
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.36751717018872,
                        "time": 9.578046837999864,
                        "accuracy": 0.37216796875,
                        "total_cost": 2253342.8140964103
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.5925
Profiling... [2048/50176]	Loss: 2.4852
Profiling... [3072/50176]	Loss: 2.4944
Profiling... [4096/50176]	Loss: 2.4372
Profiling... [5120/50176]	Loss: 2.3501
Profiling... [6144/50176]	Loss: 2.4005
Profiling... [7168/50176]	Loss: 2.4332
Profiling... [8192/50176]	Loss: 2.3839
Profiling... [9216/50176]	Loss: 2.3779
Profiling... [10240/50176]	Loss: 2.3071
Profiling... [11264/50176]	Loss: 2.2923
Profiling... [12288/50176]	Loss: 2.3468
Profiling... [13312/50176]	Loss: 2.4404
Profile done
epoch 1 train time consumed: 21.87s
Validation Epoch: 5, Average loss: 0.0024, Accuracy: 0.3701
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.23295945931775,
                        "time": 15.270497837998846,
                        "accuracy": 0.3701171875,
                        "total_cost": 3612458.8854880077
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.4818
Profiling... [2048/50176]	Loss: 2.4944
Profiling... [3072/50176]	Loss: 2.4230
Profiling... [4096/50176]	Loss: 2.3803
Profiling... [5120/50176]	Loss: 2.4429
Profiling... [6144/50176]	Loss: 2.4483
Profiling... [7168/50176]	Loss: 2.3671
Profiling... [8192/50176]	Loss: 2.4486
Profiling... [9216/50176]	Loss: 2.2619
Profiling... [10240/50176]	Loss: 2.4058
Profiling... [11264/50176]	Loss: 2.3523
Profiling... [12288/50176]	Loss: 2.3900
Profiling... [13312/50176]	Loss: 2.3142
Profile done
epoch 1 train time consumed: 12.52s
Validation Epoch: 5, Average loss: 0.0024, Accuracy: 0.3609
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.29114759784602,
                        "time": 8.785448454000289,
                        "accuracy": 0.3609375,
                        "total_cost": 2131184.4751342903
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.4199
Profiling... [2048/50176]	Loss: 2.3744
Profiling... [3072/50176]	Loss: 2.4782
Profiling... [4096/50176]	Loss: 2.4531
Profiling... [5120/50176]	Loss: 2.3346
Profiling... [6144/50176]	Loss: 2.3849
Profiling... [7168/50176]	Loss: 2.4529
Profiling... [8192/50176]	Loss: 2.3583
Profiling... [9216/50176]	Loss: 2.3159
Profiling... [10240/50176]	Loss: 2.3170
Profiling... [11264/50176]	Loss: 2.3725
Profiling... [12288/50176]	Loss: 2.2961
Profiling... [13312/50176]	Loss: 2.3556
Profile done
epoch 1 train time consumed: 12.82s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3740
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.38850999688498,
                        "time": 8.912854869999137,
                        "accuracy": 0.3740234375,
                        "total_cost": 2086447.0793803886
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.5186
Profiling... [2048/50176]	Loss: 2.4530
Profiling... [3072/50176]	Loss: 2.4820
Profiling... [4096/50176]	Loss: 2.4324
Profiling... [5120/50176]	Loss: 2.3034
Profiling... [6144/50176]	Loss: 2.3457
Profiling... [7168/50176]	Loss: 2.3655
Profiling... [8192/50176]	Loss: 2.2723
Profiling... [9216/50176]	Loss: 2.3794
Profiling... [10240/50176]	Loss: 2.2779
Profiling... [11264/50176]	Loss: 2.2964
Profiling... [12288/50176]	Loss: 2.4136
Profiling... [13312/50176]	Loss: 2.3377
Profile done
epoch 1 train time consumed: 13.58s
Validation Epoch: 5, Average loss: 0.0024, Accuracy: 0.3658
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.41587643346084,
                        "time": 9.58174184499876,
                        "accuracy": 0.3658203125,
                        "total_cost": 2293327.4714261424
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.4833
Profiling... [2048/50176]	Loss: 2.3617
Profiling... [3072/50176]	Loss: 2.4508
Profiling... [4096/50176]	Loss: 2.3399
Profiling... [5120/50176]	Loss: 2.4140
Profiling... [6144/50176]	Loss: 2.3254
Profiling... [7168/50176]	Loss: 2.4159
Profiling... [8192/50176]	Loss: 2.3636
Profiling... [9216/50176]	Loss: 2.3652
Profiling... [10240/50176]	Loss: 2.3115
Profiling... [11264/50176]	Loss: 2.4132
Profiling... [12288/50176]	Loss: 2.4370
Profiling... [13312/50176]	Loss: 2.3024
Profile done
epoch 1 train time consumed: 23.47s
Validation Epoch: 5, Average loss: 0.0023, Accuracy: 0.3675
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.26912702219714,
                        "time": 16.68891144599911,
                        "accuracy": 0.36748046875,
                        "total_cost": 3976333.5605578125
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.5412
Profiling... [2048/50176]	Loss: 2.6978
Profiling... [3072/50176]	Loss: 2.5221
Profiling... [4096/50176]	Loss: 2.4346
Profiling... [5120/50176]	Loss: 2.4036
Profiling... [6144/50176]	Loss: 2.5314
Profiling... [7168/50176]	Loss: 2.4886
Profiling... [8192/50176]	Loss: 2.4566
Profiling... [9216/50176]	Loss: 2.4578
Profiling... [10240/50176]	Loss: 2.4680
Profiling... [11264/50176]	Loss: 2.4308
Profiling... [12288/50176]	Loss: 2.4982
Profiling... [13312/50176]	Loss: 2.4438
Profile done
epoch 1 train time consumed: 12.76s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3315
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.32369193449549,
                        "time": 8.998803561000386,
                        "accuracy": 0.33154296875,
                        "total_cost": 2376479.8975511277
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.3745
Profiling... [2048/50176]	Loss: 2.6969
Profiling... [3072/50176]	Loss: 2.5967
Profiling... [4096/50176]	Loss: 2.6486
Profiling... [5120/50176]	Loss: 2.4643
Profiling... [6144/50176]	Loss: 2.6478
Profiling... [7168/50176]	Loss: 2.5134
Profiling... [8192/50176]	Loss: 2.4223
Profiling... [9216/50176]	Loss: 2.4581
Profiling... [10240/50176]	Loss: 2.4644
Profiling... [11264/50176]	Loss: 2.4002
Profiling... [12288/50176]	Loss: 2.4633
Profiling... [13312/50176]	Loss: 2.3473
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 5, Average loss: 0.0027, Accuracy: 0.3286
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.42064976164333,
                        "time": 8.915773754997645,
                        "accuracy": 0.32861328125,
                        "total_cost": 2375545.5562205883
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.5562
Profiling... [2048/50176]	Loss: 2.6133
Profiling... [3072/50176]	Loss: 2.5897
Profiling... [4096/50176]	Loss: 2.4831
Profiling... [5120/50176]	Loss: 2.5251
Profiling... [6144/50176]	Loss: 2.4491
Profiling... [7168/50176]	Loss: 2.4652
Profiling... [8192/50176]	Loss: 2.5434
Profiling... [9216/50176]	Loss: 2.4081
Profiling... [10240/50176]	Loss: 2.4220
Profiling... [11264/50176]	Loss: 2.3650
Profiling... [12288/50176]	Loss: 2.4451
Profiling... [13312/50176]	Loss: 2.4055
Profile done
epoch 1 train time consumed: 13.58s
Validation Epoch: 5, Average loss: 0.0026, Accuracy: 0.3137
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.44811979034822,
                        "time": 9.580507329999818,
                        "accuracy": 0.313671875,
                        "total_cost": 2674252.6299069603
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.4225
Profiling... [2048/50176]	Loss: 2.6681
Profiling... [3072/50176]	Loss: 2.4756
Profiling... [4096/50176]	Loss: 2.5619
Profiling... [5120/50176]	Loss: 2.5172
Profiling... [6144/50176]	Loss: 2.5053
Profiling... [7168/50176]	Loss: 2.5834
Profiling... [8192/50176]	Loss: 2.4882
Profiling... [9216/50176]	Loss: 2.5774
Profiling... [10240/50176]	Loss: 2.3813
Profiling... [11264/50176]	Loss: 2.4061
Profiling... [12288/50176]	Loss: 2.5083
Profiling... [13312/50176]	Loss: 2.5654
Profile done
epoch 1 train time consumed: 21.85s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3327
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.32045686305133,
                        "time": 15.978860238999914,
                        "accuracy": 0.33271484375,
                        "total_cost": 4204969.099109243
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.3865
Profiling... [2048/50176]	Loss: 2.6582
Profiling... [3072/50176]	Loss: 2.4894
Profiling... [4096/50176]	Loss: 2.5049
Profiling... [5120/50176]	Loss: 2.4790
Profiling... [6144/50176]	Loss: 2.4730
Profiling... [7168/50176]	Loss: 2.4487
Profiling... [8192/50176]	Loss: 2.5390
Profiling... [9216/50176]	Loss: 2.5175
Profiling... [10240/50176]	Loss: 2.3896
Profiling... [11264/50176]	Loss: 2.4480
Profiling... [12288/50176]	Loss: 2.4412
Profiling... [13312/50176]	Loss: 2.4644
Profile done
epoch 1 train time consumed: 12.51s
Validation Epoch: 5, Average loss: 0.0028, Accuracy: 0.2996
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.3726796630133,
                        "time": 8.779280942002515,
                        "accuracy": 0.299609375,
                        "total_cost": 2565623.1475673104
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.4663
Profiling... [2048/50176]	Loss: 2.7069
Profiling... [3072/50176]	Loss: 2.4202
Profiling... [4096/50176]	Loss: 2.5475
Profiling... [5120/50176]	Loss: 2.5773
Profiling... [6144/50176]	Loss: 2.5436
Profiling... [7168/50176]	Loss: 2.4096
Profiling... [8192/50176]	Loss: 2.4739
Profiling... [9216/50176]	Loss: 2.4496
Profiling... [10240/50176]	Loss: 2.4504
Profiling... [11264/50176]	Loss: 2.3987
Profiling... [12288/50176]	Loss: 2.4077
Profiling... [13312/50176]	Loss: 2.4494
Profile done
epoch 1 train time consumed: 12.75s
Validation Epoch: 5, Average loss: 0.0026, Accuracy: 0.3242
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.46728908224937,
                        "time": 8.913024554000003,
                        "accuracy": 0.32421875,
                        "total_cost": 2407002.4230302325
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.4539
Profiling... [2048/50176]	Loss: 2.5454
Profiling... [3072/50176]	Loss: 2.5153
Profiling... [4096/50176]	Loss: 2.5483
Profiling... [5120/50176]	Loss: 2.4703
Profiling... [6144/50176]	Loss: 2.4928
Profiling... [7168/50176]	Loss: 2.5650
Profiling... [8192/50176]	Loss: 2.4339
Profiling... [9216/50176]	Loss: 2.3683
Profiling... [10240/50176]	Loss: 2.5490
Profiling... [11264/50176]	Loss: 2.4520
Profiling... [12288/50176]	Loss: 2.3380
Profiling... [13312/50176]	Loss: 2.4037
Profile done
epoch 1 train time consumed: 13.59s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3477
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.49594629414695,
                        "time": 9.566549317001773,
                        "accuracy": 0.34765625,
                        "total_cost": 2409322.2760166298
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.5371
Profiling... [2048/50176]	Loss: 2.5615
Profiling... [3072/50176]	Loss: 2.4810
Profiling... [4096/50176]	Loss: 2.4109
Profiling... [5120/50176]	Loss: 2.4823
Profiling... [6144/50176]	Loss: 2.4701
Profiling... [7168/50176]	Loss: 2.4162
Profiling... [8192/50176]	Loss: 2.5584
Profiling... [9216/50176]	Loss: 2.4969
Profiling... [10240/50176]	Loss: 2.4463
Profiling... [11264/50176]	Loss: 2.4768
Profiling... [12288/50176]	Loss: 2.3525
Profiling... [13312/50176]	Loss: 2.5015
Profile done
epoch 1 train time consumed: 22.04s
Validation Epoch: 5, Average loss: 0.0029, Accuracy: 0.3001
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.36615564810006,
                        "time": 16.007085844998073,
                        "accuracy": 0.30009765625,
                        "total_cost": 4670237.548148194
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.4723
Profiling... [2048/50176]	Loss: 2.7599
Profiling... [3072/50176]	Loss: 2.3769
Profiling... [4096/50176]	Loss: 2.5811
Profiling... [5120/50176]	Loss: 2.5601
Profiling... [6144/50176]	Loss: 2.5829
Profiling... [7168/50176]	Loss: 2.5041
Profiling... [8192/50176]	Loss: 2.4301
Profiling... [9216/50176]	Loss: 2.5386
Profiling... [10240/50176]	Loss: 2.4606
Profiling... [11264/50176]	Loss: 2.4723
Profiling... [12288/50176]	Loss: 2.3782
Profiling... [13312/50176]	Loss: 2.5068
Profile done
epoch 1 train time consumed: 12.55s
Validation Epoch: 5, Average loss: 0.0032, Accuracy: 0.2556
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.41616391459924,
                        "time": 8.799140178998641,
                        "accuracy": 0.25556640625,
                        "total_cost": 3014573.6262032897
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.5366
Profiling... [2048/50176]	Loss: 2.6453
Profiling... [3072/50176]	Loss: 2.5072
Profiling... [4096/50176]	Loss: 2.5934
Profiling... [5120/50176]	Loss: 2.4645
Profiling... [6144/50176]	Loss: 2.4965
Profiling... [7168/50176]	Loss: 2.4958
Profiling... [8192/50176]	Loss: 2.4618
Profiling... [9216/50176]	Loss: 2.5039
Profiling... [10240/50176]	Loss: 2.4574
Profiling... [11264/50176]	Loss: 2.4141
Profiling... [12288/50176]	Loss: 2.4579
Profiling... [13312/50176]	Loss: 2.4810
Profile done
epoch 1 train time consumed: 12.76s
Validation Epoch: 5, Average loss: 0.0027, Accuracy: 0.3097
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.51005244217122,
                        "time": 8.920655991001695,
                        "accuracy": 0.30966796875,
                        "total_cost": 2522261.8097381983
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.5052
Profiling... [2048/50176]	Loss: 2.6126
Profiling... [3072/50176]	Loss: 2.5616
Profiling... [4096/50176]	Loss: 2.4704
Profiling... [5120/50176]	Loss: 2.4843
Profiling... [6144/50176]	Loss: 2.5352
Profiling... [7168/50176]	Loss: 2.5444
Profiling... [8192/50176]	Loss: 2.5385
Profiling... [9216/50176]	Loss: 2.5187
Profiling... [10240/50176]	Loss: 2.4720
Profiling... [11264/50176]	Loss: 2.3957
Profiling... [12288/50176]	Loss: 2.3118
Profiling... [13312/50176]	Loss: 2.4567
Profile done
epoch 1 train time consumed: 13.56s
Validation Epoch: 5, Average loss: 0.0026, Accuracy: 0.3235
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.53868190614793,
                        "time": 9.569866903999355,
                        "accuracy": 0.32353515625,
                        "total_cost": 2589847.232829407
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.4848
Profiling... [2048/50176]	Loss: 2.6057
Profiling... [3072/50176]	Loss: 2.5199
Profiling... [4096/50176]	Loss: 2.5320
Profiling... [5120/50176]	Loss: 2.5269
Profiling... [6144/50176]	Loss: 2.5186
Profiling... [7168/50176]	Loss: 2.5097
Profiling... [8192/50176]	Loss: 2.4803
Profiling... [9216/50176]	Loss: 2.5190
Profiling... [10240/50176]	Loss: 2.4418
Profiling... [11264/50176]	Loss: 2.3687
Profiling... [12288/50176]	Loss: 2.5020
Profiling... [13312/50176]	Loss: 2.4348
Profile done
epoch 1 train time consumed: 21.99s
Validation Epoch: 5, Average loss: 0.0025, Accuracy: 0.3311
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.41095694670977,
                        "time": 15.23720499400224,
                        "accuracy": 0.3310546875,
                        "total_cost": 4029906.599570433
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.5 bs: 128 pl: 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[GPU_0] Set GPU power limit to 125W.
[GPU_0] Set GPU power limit to 125W.
Training Epoch: 5 [128/50048]	Loss: 2.5970
Training Epoch: 5 [256/50048]	Loss: 2.4249
Training Epoch: 5 [384/50048]	Loss: 2.6378
Training Epoch: 5 [512/50048]	Loss: 2.4174
Training Epoch: 5 [640/50048]	Loss: 2.5604
Training Epoch: 5 [768/50048]	Loss: 2.3110
Training Epoch: 5 [896/50048]	Loss: 2.5918
Training Epoch: 5 [1024/50048]	Loss: 2.5570
Training Epoch: 5 [1152/50048]	Loss: 2.7163
Training Epoch: 5 [1280/50048]	Loss: 2.5267
Training Epoch: 5 [1408/50048]	Loss: 2.5133
Training Epoch: 5 [1536/50048]	Loss: 2.4288
Training Epoch: 5 [1664/50048]	Loss: 2.4537
Training Epoch: 5 [1792/50048]	Loss: 2.5535
Training Epoch: 5 [1920/50048]	Loss: 2.4061
Training Epoch: 5 [2048/50048]	Loss: 2.5623
Training Epoch: 5 [2176/50048]	Loss: 2.2354
Training Epoch: 5 [2304/50048]	Loss: 2.3828
Training Epoch: 5 [2432/50048]	Loss: 2.4264
Training Epoch: 5 [2560/50048]	Loss: 2.4646
Training Epoch: 5 [2688/50048]	Loss: 2.3515
Training Epoch: 5 [2816/50048]	Loss: 2.3029
Training Epoch: 5 [2944/50048]	Loss: 2.4884
Training Epoch: 5 [3072/50048]	Loss: 2.1378
Training Epoch: 5 [3200/50048]	Loss: 2.4658
Training Epoch: 5 [3328/50048]	Loss: 2.2869
Training Epoch: 5 [3456/50048]	Loss: 2.3130
Training Epoch: 5 [3584/50048]	Loss: 2.3426
Training Epoch: 5 [3712/50048]	Loss: 2.3920
Training Epoch: 5 [3840/50048]	Loss: 2.5945
Training Epoch: 5 [3968/50048]	Loss: 2.1811
Training Epoch: 5 [4096/50048]	Loss: 2.3810
Training Epoch: 5 [4224/50048]	Loss: 2.4390
Training Epoch: 5 [4352/50048]	Loss: 2.4124
Training Epoch: 5 [4480/50048]	Loss: 2.2581
Training Epoch: 5 [4608/50048]	Loss: 2.0993
Training Epoch: 5 [4736/50048]	Loss: 2.5359
Training Epoch: 5 [4864/50048]	Loss: 2.2946
Training Epoch: 5 [4992/50048]	Loss: 2.1689
Training Epoch: 5 [5120/50048]	Loss: 2.2577
Training Epoch: 5 [5248/50048]	Loss: 2.6103
Training Epoch: 5 [5376/50048]	Loss: 2.3972
Training Epoch: 5 [5504/50048]	Loss: 2.3004
Training Epoch: 5 [5632/50048]	Loss: 2.1819
Training Epoch: 5 [5760/50048]	Loss: 2.3032
Training Epoch: 5 [5888/50048]	Loss: 2.4396
Training Epoch: 5 [6016/50048]	Loss: 2.4334
Training Epoch: 5 [6144/50048]	Loss: 2.3775
Training Epoch: 5 [6272/50048]	Loss: 2.5060
Training Epoch: 5 [6400/50048]	Loss: 2.2285
Training Epoch: 5 [6528/50048]	Loss: 2.5489
Training Epoch: 5 [6656/50048]	Loss: 2.1579
Training Epoch: 5 [6784/50048]	Loss: 2.5303
Training Epoch: 5 [6912/50048]	Loss: 2.5643
Training Epoch: 5 [7040/50048]	Loss: 2.2653
Training Epoch: 5 [7168/50048]	Loss: 2.2663
Training Epoch: 5 [7296/50048]	Loss: 2.2199
Training Epoch: 5 [7424/50048]	Loss: 2.2568
Training Epoch: 5 [7552/50048]	Loss: 2.4538
Training Epoch: 5 [7680/50048]	Loss: 2.4555
Training Epoch: 5 [7808/50048]	Loss: 2.0902
Training Epoch: 5 [7936/50048]	Loss: 2.0479
Training Epoch: 5 [8064/50048]	Loss: 2.2693
Training Epoch: 5 [8192/50048]	Loss: 2.3640
Training Epoch: 5 [8320/50048]	Loss: 2.0805
Training Epoch: 5 [8448/50048]	Loss: 2.3644
Training Epoch: 5 [8576/50048]	Loss: 2.2741
Training Epoch: 5 [8704/50048]	Loss: 2.2027
Training Epoch: 5 [8832/50048]	Loss: 2.2170
Training Epoch: 5 [8960/50048]	Loss: 2.3627
Training Epoch: 5 [9088/50048]	Loss: 2.3235
Training Epoch: 5 [9216/50048]	Loss: 2.2868
Training Epoch: 5 [9344/50048]	Loss: 2.1722
Training Epoch: 5 [9472/50048]	Loss: 2.2472
Training Epoch: 5 [9600/50048]	Loss: 2.4112
Training Epoch: 5 [9728/50048]	Loss: 2.4246
Training Epoch: 5 [9856/50048]	Loss: 2.2784
Training Epoch: 5 [9984/50048]	Loss: 2.2098
Training Epoch: 5 [10112/50048]	Loss: 2.2868
Training Epoch: 5 [10240/50048]	Loss: 1.9708
Training Epoch: 5 [10368/50048]	Loss: 2.2912
Training Epoch: 5 [10496/50048]	Loss: 2.3215
Training Epoch: 5 [10624/50048]	Loss: 2.3406
Training Epoch: 5 [10752/50048]	Loss: 2.1806
Training Epoch: 5 [10880/50048]	Loss: 2.1745
Training Epoch: 5 [11008/50048]	Loss: 2.2424
Training Epoch: 5 [11136/50048]	Loss: 2.4500
Training Epoch: 5 [11264/50048]	Loss: 2.5133
Training Epoch: 5 [11392/50048]	Loss: 1.9928
Training Epoch: 5 [11520/50048]	Loss: 2.3057
Training Epoch: 5 [11648/50048]	Loss: 2.4217
Training Epoch: 5 [11776/50048]	Loss: 2.5935
Training Epoch: 5 [11904/50048]	Loss: 2.2755
Training Epoch: 5 [12032/50048]	Loss: 2.3607
Training Epoch: 5 [12160/50048]	Loss: 2.2268
Training Epoch: 5 [12288/50048]	Loss: 2.2905
Training Epoch: 5 [12416/50048]	Loss: 2.3208
Training Epoch: 5 [12544/50048]	Loss: 1.9888
Training Epoch: 5 [12672/50048]	Loss: 2.5029
Training Epoch: 5 [12800/50048]	Loss: 2.5450
Training Epoch: 5 [12928/50048]	Loss: 2.4337
Training Epoch: 5 [13056/50048]	Loss: 2.2795
Training Epoch: 5 [13184/50048]	Loss: 2.2890
Training Epoch: 5 [13312/50048]	Loss: 2.3629
Training Epoch: 5 [13440/50048]	Loss: 2.5640
Training Epoch: 5 [13568/50048]	Loss: 2.1807
Training Epoch: 5 [13696/50048]	Loss: 2.4136
Training Epoch: 5 [13824/50048]	Loss: 2.2639
Training Epoch: 5 [13952/50048]	Loss: 2.2374
Training Epoch: 5 [14080/50048]	Loss: 2.3925
Training Epoch: 5 [14208/50048]	Loss: 2.1644
Training Epoch: 5 [14336/50048]	Loss: 2.3904
Training Epoch: 5 [14464/50048]	Loss: 2.4462
Training Epoch: 5 [14592/50048]	Loss: 2.6577
Training Epoch: 5 [14720/50048]	Loss: 2.2581
Training Epoch: 5 [14848/50048]	Loss: 2.3361
Training Epoch: 5 [14976/50048]	Loss: 2.5384
Training Epoch: 5 [15104/50048]	Loss: 2.3329
Training Epoch: 5 [15232/50048]	Loss: 2.2230
Training Epoch: 5 [15360/50048]	Loss: 2.3767
Training Epoch: 5 [15488/50048]	Loss: 2.4258
Training Epoch: 5 [15616/50048]	Loss: 2.4378
Training Epoch: 5 [15744/50048]	Loss: 2.3418
Training Epoch: 5 [15872/50048]	Loss: 2.4729
Training Epoch: 5 [16000/50048]	Loss: 2.0606
Training Epoch: 5 [16128/50048]	Loss: 2.3206
Training Epoch: 5 [16256/50048]	Loss: 2.4148
Training Epoch: 5 [16384/50048]	Loss: 2.1392
Training Epoch: 5 [16512/50048]	Loss: 2.4245
Training Epoch: 5 [16640/50048]	Loss: 2.3949
Training Epoch: 5 [16768/50048]	Loss: 2.0528
Training Epoch: 5 [16896/50048]	Loss: 2.4693
Training Epoch: 5 [17024/50048]	Loss: 2.1808
Training Epoch: 5 [17152/50048]	Loss: 2.3245
Training Epoch: 5 [17280/50048]	Loss: 2.6301
Training Epoch: 5 [17408/50048]	Loss: 2.4760
Training Epoch: 5 [17536/50048]	Loss: 2.3299
Training Epoch: 5 [17664/50048]	Loss: 2.2022
Training Epoch: 5 [17792/50048]	Loss: 2.3769
Training Epoch: 5 [17920/50048]	Loss: 2.3557
Training Epoch: 5 [18048/50048]	Loss: 2.3132
Training Epoch: 5 [18176/50048]	Loss: 2.2829
Training Epoch: 5 [18304/50048]	Loss: 2.2771
Training Epoch: 5 [18432/50048]	Loss: 2.0894
Training Epoch: 5 [18560/50048]	Loss: 2.2980
Training Epoch: 5 [18688/50048]	Loss: 2.5943
Training Epoch: 5 [18816/50048]	Loss: 2.0186
Training Epoch: 5 [18944/50048]	Loss: 2.1778
Training Epoch: 5 [19072/50048]	Loss: 2.3016
Training Epoch: 5 [19200/50048]	Loss: 2.3945
Training Epoch: 5 [19328/50048]	Loss: 2.5987
Training Epoch: 5 [19456/50048]	Loss: 2.2706
Training Epoch: 5 [19584/50048]	Loss: 2.4555
Training Epoch: 5 [19712/50048]	Loss: 2.4220
Training Epoch: 5 [19840/50048]	Loss: 2.5532
Training Epoch: 5 [19968/50048]	Loss: 2.1738
Training Epoch: 5 [20096/50048]	Loss: 2.2859
Training Epoch: 5 [20224/50048]	Loss: 2.2714
Training Epoch: 5 [20352/50048]	Loss: 2.1671
Training Epoch: 5 [20480/50048]	Loss: 2.3640
Training Epoch: 5 [20608/50048]	Loss: 2.3862
Training Epoch: 5 [20736/50048]	Loss: 2.3010
Training Epoch: 5 [20864/50048]	Loss: 2.2387
Training Epoch: 5 [20992/50048]	Loss: 2.2349
Training Epoch: 5 [21120/50048]	Loss: 2.0966
Training Epoch: 5 [21248/50048]	Loss: 2.4649
Training Epoch: 5 [21376/50048]	Loss: 2.3240
Training Epoch: 5 [21504/50048]	Loss: 2.1887
Training Epoch: 5 [21632/50048]	Loss: 2.1745
Training Epoch: 5 [21760/50048]	Loss: 2.2353
Training Epoch: 5 [21888/50048]	Loss: 2.1985
Training Epoch: 5 [22016/50048]	Loss: 2.5771
Training Epoch: 5 [22144/50048]	Loss: 2.2703
Training Epoch: 5 [22272/50048]	Loss: 2.2532
Training Epoch: 5 [22400/50048]	Loss: 2.0525
Training Epoch: 5 [22528/50048]	Loss: 2.1513
Training Epoch: 5 [22656/50048]	Loss: 2.0638
Training Epoch: 5 [22784/50048]	Loss: 2.2711
Training Epoch: 5 [22912/50048]	Loss: 2.2673
Training Epoch: 5 [23040/50048]	Loss: 2.1018
Training Epoch: 5 [23168/50048]	Loss: 2.4416
Training Epoch: 5 [23296/50048]	Loss: 2.0576
Training Epoch: 5 [23424/50048]	Loss: 2.4268
Training Epoch: 5 [23552/50048]	Loss: 2.5244
Training Epoch: 5 [23680/50048]	Loss: 2.4177
Training Epoch: 5 [23808/50048]	Loss: 2.5807
Training Epoch: 5 [23936/50048]	Loss: 2.4298
Training Epoch: 5 [24064/50048]	Loss: 2.3412
Training Epoch: 5 [24192/50048]	Loss: 2.4368
Training Epoch: 5 [24320/50048]	Loss: 2.3799
Training Epoch: 5 [24448/50048]	Loss: 2.4345
Training Epoch: 5 [24576/50048]	Loss: 2.0218
Training Epoch: 5 [24704/50048]	Loss: 2.6378
Training Epoch: 5 [24832/50048]	Loss: 2.0613
Training Epoch: 5 [24960/50048]	Loss: 2.4882
Training Epoch: 5 [25088/50048]	Loss: 2.1456
Training Epoch: 5 [25216/50048]	Loss: 2.2218
Training Epoch: 5 [25344/50048]	Loss: 2.2762
Training Epoch: 5 [25472/50048]	Loss: 2.0141
Training Epoch: 5 [25600/50048]	Loss: 2.2929
Training Epoch: 5 [25728/50048]	Loss: 2.0562
Training Epoch: 5 [25856/50048]	Loss: 2.3591
Training Epoch: 5 [25984/50048]	Loss: 2.3087
Training Epoch: 5 [26112/50048]	Loss: 2.4424
Training Epoch: 5 [26240/50048]	Loss: 2.4592
Training Epoch: 5 [26368/50048]	Loss: 2.4536
Training Epoch: 5 [26496/50048]	Loss: 2.3961
Training Epoch: 5 [26624/50048]	Loss: 2.4683
Training Epoch: 5 [26752/50048]	Loss: 2.4063
Training Epoch: 5 [26880/50048]	Loss: 2.4141
Training Epoch: 5 [27008/50048]	Loss: 2.1321
Training Epoch: 5 [27136/50048]	Loss: 2.1723
Training Epoch: 5 [27264/50048]	Loss: 2.2188
Training Epoch: 5 [27392/50048]	Loss: 2.2274
Training Epoch: 5 [27520/50048]	Loss: 2.4115
Training Epoch: 5 [27648/50048]	Loss: 2.1350
Training Epoch: 5 [27776/50048]	Loss: 2.4417
Training Epoch: 5 [27904/50048]	Loss: 2.2576
Training Epoch: 5 [28032/50048]	Loss: 2.7477
Training Epoch: 5 [28160/50048]	Loss: 2.3192
Training Epoch: 5 [28288/50048]	Loss: 2.1818
Training Epoch: 5 [28416/50048]	Loss: 2.3605
Training Epoch: 5 [28544/50048]	Loss: 2.0867
Training Epoch: 5 [28672/50048]	Loss: 2.1123
Training Epoch: 5 [28800/50048]	Loss: 2.2501
Training Epoch: 5 [28928/50048]	Loss: 2.2093
Training Epoch: 5 [29056/50048]	Loss: 2.1030
Training Epoch: 5 [29184/50048]	Loss: 2.3991
Training Epoch: 5 [29312/50048]	Loss: 1.9830
Training Epoch: 5 [29440/50048]	Loss: 2.2603
Training Epoch: 5 [29568/50048]	Loss: 2.2808
Training Epoch: 5 [29696/50048]	Loss: 2.4634
Training Epoch: 5 [29824/50048]	Loss: 2.0989
Training Epoch: 5 [29952/50048]	Loss: 2.3270
Training Epoch: 5 [30080/50048]	Loss: 2.3208
Training Epoch: 5 [30208/50048]	Loss: 2.1340
Training Epoch: 5 [30336/50048]	Loss: 2.4946
Training Epoch: 5 [30464/50048]	Loss: 2.4116
Training Epoch: 5 [30592/50048]	Loss: 2.1299
Training Epoch: 5 [30720/50048]	Loss: 2.2543
Training Epoch: 5 [30848/50048]	Loss: 2.6566
Training Epoch: 5 [30976/50048]	Loss: 2.0695
Training Epoch: 5 [31104/50048]	Loss: 2.0025
Training Epoch: 5 [31232/50048]	Loss: 2.1901
Training Epoch: 5 [31360/50048]	Loss: 2.4381
Training Epoch: 5 [31488/50048]	Loss: 2.3415
Training Epoch: 5 [31616/50048]	Loss: 2.0549
Training Epoch: 5 [31744/50048]	Loss: 2.4065
Training Epoch: 5 [31872/50048]	Loss: 2.4605
Training Epoch: 5 [32000/50048]	Loss: 2.1615
Training Epoch: 5 [32128/50048]	Loss: 2.1264
Training Epoch: 5 [32256/50048]	Loss: 2.4801
Training Epoch: 5 [32384/50048]	Loss: 2.2831
Training Epoch: 5 [32512/50048]	Loss: 2.3007
Training Epoch: 5 [32640/50048]	Loss: 2.1324
Training Epoch: 5 [32768/50048]	Loss: 2.0801
Training Epoch: 5 [32896/50048]	Loss: 2.4229
Training Epoch: 5 [33024/50048]	Loss: 2.3321
Training Epoch: 5 [33152/50048]	Loss: 2.2437
Training Epoch: 5 [33280/50048]	Loss: 2.2406
Training Epoch: 5 [33408/50048]	Loss: 2.1248
Training Epoch: 5 [33536/50048]	Loss: 2.1682
Training Epoch: 5 [33664/50048]	Loss: 1.9118
Training Epoch: 5 [33792/50048]	Loss: 2.3021
Training Epoch: 5 [33920/50048]	Loss: 2.2828
Training Epoch: 5 [34048/50048]	Loss: 2.0411
Training Epoch: 5 [34176/50048]	Loss: 2.1836
Training Epoch: 5 [34304/50048]	Loss: 2.0917
Training Epoch: 5 [34432/50048]	Loss: 2.3188
Training Epoch: 5 [34560/50048]	Loss: 2.1435
Training Epoch: 5 [34688/50048]	Loss: 2.3660
Training Epoch: 5 [34816/50048]	Loss: 2.1436
Training Epoch: 5 [34944/50048]	Loss: 2.1302
Training Epoch: 5 [35072/50048]	Loss: 2.4917
Training Epoch: 5 [35200/50048]	Loss: 2.3401
Training Epoch: 5 [35328/50048]	Loss: 2.2898
Training Epoch: 5 [35456/50048]	Loss: 2.0872
Training Epoch: 5 [35584/50048]	Loss: 2.0611
Training Epoch: 5 [35712/50048]	Loss: 2.3845
Training Epoch: 5 [35840/50048]	Loss: 2.1051
Training Epoch: 5 [35968/50048]	Loss: 2.6455
Training Epoch: 5 [36096/50048]	Loss: 2.4538
Training Epoch: 5 [36224/50048]	Loss: 2.1627
Training Epoch: 5 [36352/50048]	Loss: 2.1683
Training Epoch: 5 [36480/50048]	Loss: 2.2032
Training Epoch: 5 [36608/50048]	Loss: 2.2511
Training Epoch: 5 [36736/50048]	Loss: 2.5133
Training Epoch: 5 [36864/50048]	Loss: 2.2083
Training Epoch: 5 [36992/50048]	Loss: 2.0043
Training Epoch: 5 [37120/50048]	Loss: 2.1157
Training Epoch: 5 [37248/50048]	Loss: 2.0709
Training Epoch: 5 [37376/50048]	Loss: 2.2465
Training Epoch: 5 [37504/50048]	Loss: 2.4338
Training Epoch: 5 [37632/50048]	Loss: 2.2220
Training Epoch: 5 [37760/50048]	Loss: 2.1471
Training Epoch: 5 [37888/50048]	Loss: 2.1462
Training Epoch: 5 [38016/50048]	Loss: 2.1404
Training Epoch: 5 [38144/50048]	Loss: 2.2838
Training Epoch: 5 [38272/50048]	Loss: 2.1695
Training Epoch: 5 [38400/50048]	Loss: 2.5123
Training Epoch: 5 [38528/50048]	Loss: 2.2114
Training Epoch: 5 [38656/50048]	Loss: 2.2182
Training Epoch: 5 [38784/50048]	Loss: 2.4757
Training Epoch: 5 [38912/50048]	Loss: 2.1507
Training Epoch: 5 [39040/50048]	Loss: 2.0655
Training Epoch: 5 [39168/50048]	Loss: 2.0859
Training Epoch: 5 [39296/50048]	Loss: 2.0871
Training Epoch: 5 [39424/50048]	Loss: 2.2841
Training Epoch: 5 [39552/50048]	Loss: 2.3340
Training Epoch: 5 [39680/50048]	Loss: 2.2327
Training Epoch: 5 [39808/50048]	Loss: 2.2055
Training Epoch: 5 [39936/50048]	Loss: 2.1408
Training Epoch: 5 [40064/50048]	Loss: 2.2173
Training Epoch: 5 [40192/50048]	Loss: 2.2032
Training Epoch: 5 [40320/50048]	Loss: 2.2315
Training Epoch: 5 [40448/50048]	Loss: 2.5175
Training Epoch: 5 [40576/50048]	Loss: 2.3041
Training Epoch: 5 [40704/50048]	Loss: 2.4144
Training Epoch: 5 [40832/50048]	Loss: 2.2710
Training Epoch: 5 [40960/50048]	Loss: 2.1513
Training Epoch: 5 [41088/50048]	Loss: 2.4167
Training Epoch: 5 [41216/50048]	Loss: 2.2180
Training Epoch: 5 [41344/50048]	Loss: 2.3880
Training Epoch: 5 [41472/50048]	Loss: 2.3989
Training Epoch: 5 [41600/50048]	Loss: 2.1612
Training Epoch: 5 [41728/50048]	Loss: 2.2951
Training Epoch: 5 [41856/50048]	Loss: 2.2313
Training Epoch: 5 [41984/50048]	Loss: 2.4124
Training Epoch: 5 [42112/50048]	Loss: 2.1409
Training Epoch: 5 [42240/50048]	Loss: 2.1235
Training Epoch: 5 [42368/50048]	Loss: 2.1317
Training Epoch: 5 [42496/50048]	Loss: 2.1126
Training Epoch: 5 [42624/50048]	Loss: 2.4205
Training Epoch: 5 [42752/50048]	Loss: 2.3908
Training Epoch: 5 [42880/50048]	Loss: 2.4474
Training Epoch: 5 [43008/50048]	Loss: 2.2531
Training Epoch: 5 [43136/50048]	Loss: 2.2176
Training Epoch: 5 [43264/50048]	Loss: 1.9691
Training Epoch: 5 [43392/50048]	Loss: 2.3927
Training Epoch: 5 [43520/50048]	Loss: 2.1655
Training Epoch: 5 [43648/50048]	Loss: 2.2248
Training Epoch: 5 [43776/50048]	Loss: 2.3697
Training Epoch: 5 [43904/50048]	Loss: 2.3659
Training Epoch: 5 [44032/50048]	Loss: 2.5749
Training Epoch: 5 [44160/50048]	Loss: 2.2358
Training Epoch: 5 [44288/50048]	Loss: 2.1914
Training Epoch: 5 [44416/50048]	Loss: 2.3130
Training Epoch: 5 [44544/50048]	Loss: 2.3830
Training Epoch: 5 [44672/50048]	Loss: 2.4599
Training Epoch: 5 [44800/50048]	Loss: 2.4149
Training Epoch: 5 [44928/50048]	Loss: 2.2081
Training Epoch: 5 [45056/50048]	Loss: 2.3138
Training Epoch: 5 [45184/50048]	Loss: 2.5331
Training Epoch: 5 [45312/50048]	Loss: 2.1975
Training Epoch: 5 [45440/50048]	Loss: 2.4394
Training Epoch: 5 [45568/50048]	Loss: 2.0783
Training Epoch: 5 [45696/50048]	Loss: 2.0724
Training Epoch: 5 [45824/50048]	Loss: 2.0174
Training Epoch: 5 [45952/50048]	Loss: 2.3684
Training Epoch: 5 [46080/50048]	Loss: 2.3322
Training Epoch: 5 [46208/50048]	Loss: 2.0912
Training Epoch: 5 [46336/50048]	Loss: 2.1019
Training Epoch: 5 [46464/50048]	Loss: 2.0280
Training Epoch: 5 [46592/50048]	Loss: 2.1937
Training Epoch: 5 [46720/50048]	Loss: 2.4111
Training Epoch: 5 [46848/50048]	Loss: 2.1241
Training Epoch: 5 [46976/50048]	Loss: 2.2045
Training Epoch: 5 [47104/50048]	Loss: 2.2361
Training Epoch: 5 [47232/50048]	Loss: 2.1753
Training Epoch: 5 [47360/50048]	Loss: 2.0757
Training Epoch: 5 [47488/50048]	Loss: 2.4822
Training Epoch: 5 [47616/50048]	Loss: 2.6017
Training Epoch: 5 [47744/50048]	Loss: 2.4105
Training Epoch: 5 [47872/50048]	Loss: 2.4955
Training Epoch: 5 [48000/50048]	Loss: 1.9758
Training Epoch: 5 [48128/50048]	Loss: 2.4205
Training Epoch: 5 [48256/50048]	Loss: 2.2094
Training Epoch: 5 [48384/50048]	Loss: 2.0495
Training Epoch: 5 [48512/50048]	Loss: 2.1836
Training Epoch: 5 [48640/50048]	Loss: 2.2137
Training Epoch: 5 [48768/50048]	Loss: 2.2022
Training Epoch: 5 [48896/50048]	Loss: 1.9735
Training Epoch: 5 [49024/50048]	Loss: 2.4172
Training Epoch: 5 [49152/50048]	Loss: 2.1381
Training Epoch: 5 [49280/50048]	Loss: 2.4252
Training Epoch: 5 [49408/50048]	Loss: 2.2141
Training Epoch: 5 [49536/50048]	Loss: 2.1942
Training Epoch: 5 [49664/50048]	Loss: 2.1641
Training Epoch: 5 [49792/50048]	Loss: 2.4526
Training Epoch: 5 [49920/50048]	Loss: 2.5667
Training Epoch: 5 [50048/50048]	Loss: 2.4259
Validation Epoch: 5, Average loss: 0.0169, Accuracy: 0.4206
[Training Loop] Model's accuracy 0.4205893987341772 surpasses threshold 0.4! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0823
Profiling... [256/50048]	Loss: 2.2008
Profiling... [384/50048]	Loss: 1.9871
Profiling... [512/50048]	Loss: 2.3748
Profiling... [640/50048]	Loss: 2.3728
Profiling... [768/50048]	Loss: 1.9020
Profiling... [896/50048]	Loss: 2.3247
Profiling... [1024/50048]	Loss: 2.5305
Profiling... [1152/50048]	Loss: 2.3050
Profiling... [1280/50048]	Loss: 1.9401
Profiling... [1408/50048]	Loss: 2.3147
Profiling... [1536/50048]	Loss: 2.2228
Profiling... [1664/50048]	Loss: 2.1850
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 6, Average loss: 0.0169, Accuracy: 0.4214
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.60997794752323,
                        "time": 2.1676904449996073,
                        "accuracy": 0.42138053797468356,
                        "total_cost": 450414.8446948332
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.3631
Profiling... [256/50048]	Loss: 1.9327
Profiling... [384/50048]	Loss: 2.2592
Profiling... [512/50048]	Loss: 2.2004
Profiling... [640/50048]	Loss: 2.1305
Profiling... [768/50048]	Loss: 2.1323
Profiling... [896/50048]	Loss: 2.0461
Profiling... [1024/50048]	Loss: 2.2324
Profiling... [1152/50048]	Loss: 1.9404
Profiling... [1280/50048]	Loss: 2.3616
Profiling... [1408/50048]	Loss: 2.1078
Profiling... [1536/50048]	Loss: 2.0397
Profiling... [1664/50048]	Loss: 2.4179
Profile done
epoch 1 train time consumed: 3.24s
Validation Epoch: 6, Average loss: 0.0172, Accuracy: 0.4183
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.62534226265318,
                        "time": 2.162999891996151,
                        "accuracy": 0.4183148734177215,
                        "total_cost": 452734.0249794741
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9982
Profiling... [256/50048]	Loss: 2.1929
Profiling... [384/50048]	Loss: 2.3252
Profiling... [512/50048]	Loss: 2.0903
Profiling... [640/50048]	Loss: 2.0630
Profiling... [768/50048]	Loss: 2.3586
Profiling... [896/50048]	Loss: 2.0277
Profiling... [1024/50048]	Loss: 2.2609
Profiling... [1152/50048]	Loss: 2.3449
Profiling... [1280/50048]	Loss: 2.0776
Profiling... [1408/50048]	Loss: 2.1380
Profiling... [1536/50048]	Loss: 2.2018
Profiling... [1664/50048]	Loss: 2.2621
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 6, Average loss: 0.0172, Accuracy: 0.4185
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.63646002594415,
                        "time": 2.2206372220025514,
                        "accuracy": 0.4185126582278481,
                        "total_cost": 464578.3724319876
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0769
Profiling... [256/50048]	Loss: 1.9328
Profiling... [384/50048]	Loss: 2.0338
Profiling... [512/50048]	Loss: 2.2246
Profiling... [640/50048]	Loss: 2.1576
Profiling... [768/50048]	Loss: 2.1155
Profiling... [896/50048]	Loss: 2.0948
Profiling... [1024/50048]	Loss: 2.3486
Profiling... [1152/50048]	Loss: 2.0585
Profiling... [1280/50048]	Loss: 2.3014
Profiling... [1408/50048]	Loss: 2.4610
Profiling... [1536/50048]	Loss: 2.1714
Profiling... [1664/50048]	Loss: 2.2675
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 6, Average loss: 0.0171, Accuracy: 0.4170
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.6225810982447,
                        "time": 2.5614354579956853,
                        "accuracy": 0.4170292721518987,
                        "total_cost": 537782.6833866977
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1484
Profiling... [256/50048]	Loss: 2.1203
Profiling... [384/50048]	Loss: 2.0571
Profiling... [512/50048]	Loss: 2.1769
Profiling... [640/50048]	Loss: 2.3916
Profiling... [768/50048]	Loss: 2.2078
Profiling... [896/50048]	Loss: 2.2431
Profiling... [1024/50048]	Loss: 2.3102
Profiling... [1152/50048]	Loss: 1.8395
Profiling... [1280/50048]	Loss: 2.4692
Profiling... [1408/50048]	Loss: 2.1526
Profiling... [1536/50048]	Loss: 2.3194
Profiling... [1664/50048]	Loss: 2.2393
Profile done
epoch 1 train time consumed: 3.40s
Validation Epoch: 6, Average loss: 0.0170, Accuracy: 0.4220
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.59730870719244,
                        "time": 2.1664604319958016,
                        "accuracy": 0.4219738924050633,
                        "total_cost": 449526.2461754239
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1699
Profiling... [256/50048]	Loss: 2.0538
Profiling... [384/50048]	Loss: 2.1642
Profiling... [512/50048]	Loss: 2.0248
Profiling... [640/50048]	Loss: 2.3225
Profiling... [768/50048]	Loss: 2.1322
Profiling... [896/50048]	Loss: 2.2103
Profiling... [1024/50048]	Loss: 2.4133
Profiling... [1152/50048]	Loss: 1.9151
Profiling... [1280/50048]	Loss: 2.2017
Profiling... [1408/50048]	Loss: 2.2167
Profiling... [1536/50048]	Loss: 2.0886
Profiling... [1664/50048]	Loss: 1.9881
Profile done
epoch 1 train time consumed: 3.33s
Validation Epoch: 6, Average loss: 0.0171, Accuracy: 0.4178
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.61096574891751,
                        "time": 2.166812633004156,
                        "accuracy": 0.41782041139240506,
                        "total_cost": 454068.7507188371
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0145
Profiling... [256/50048]	Loss: 2.1651
Profiling... [384/50048]	Loss: 2.1932
Profiling... [512/50048]	Loss: 1.9750
Profiling... [640/50048]	Loss: 1.8702
Profiling... [768/50048]	Loss: 2.4647
Profiling... [896/50048]	Loss: 2.4904
Profiling... [1024/50048]	Loss: 2.0858
Profiling... [1152/50048]	Loss: 2.0515
Profiling... [1280/50048]	Loss: 2.0741
Profiling... [1408/50048]	Loss: 2.3013
Profiling... [1536/50048]	Loss: 2.6168
Profiling... [1664/50048]	Loss: 2.2325
Profile done
epoch 1 train time consumed: 3.33s
Validation Epoch: 6, Average loss: 0.0170, Accuracy: 0.4234
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.61851681381219,
                        "time": 2.215885817000526,
                        "accuracy": 0.4233583860759494,
                        "total_cost": 458278.1354961002
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.2913
Profiling... [256/50048]	Loss: 2.4072
Profiling... [384/50048]	Loss: 2.2573
Profiling... [512/50048]	Loss: 2.1500
Profiling... [640/50048]	Loss: 2.2925
Profiling... [768/50048]	Loss: 2.1484
Profiling... [896/50048]	Loss: 2.3567
Profiling... [1024/50048]	Loss: 2.3756
Profiling... [1152/50048]	Loss: 2.2083
Profiling... [1280/50048]	Loss: 2.1798
Profiling... [1408/50048]	Loss: 2.0959
Profiling... [1536/50048]	Loss: 2.2787
Profiling... [1664/50048]	Loss: 2.2371
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 6, Average loss: 0.0171, Accuracy: 0.4211
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.60557140803016,
                        "time": 2.5563380509993294,
                        "accuracy": 0.42108386075949367,
                        "total_cost": 531544.4438578924
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1490
Profiling... [256/50048]	Loss: 2.1276
Profiling... [384/50048]	Loss: 2.0797
Profiling... [512/50048]	Loss: 2.1260
Profiling... [640/50048]	Loss: 2.4013
Profiling... [768/50048]	Loss: 2.2035
Profiling... [896/50048]	Loss: 2.1660
Profiling... [1024/50048]	Loss: 2.2792
Profiling... [1152/50048]	Loss: 2.5259
Profiling... [1280/50048]	Loss: 2.3502
Profiling... [1408/50048]	Loss: 2.2174
Profiling... [1536/50048]	Loss: 2.0570
Profiling... [1664/50048]	Loss: 2.3648
Profile done
epoch 1 train time consumed: 3.35s
Validation Epoch: 6, Average loss: 0.0170, Accuracy: 0.4217
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.58061905545232,
                        "time": 2.1659708110018983,
                        "accuracy": 0.42167721518987344,
                        "total_cost": 449740.8095195684
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9994
Profiling... [256/50048]	Loss: 2.0930
Profiling... [384/50048]	Loss: 1.8672
Profiling... [512/50048]	Loss: 2.1877
Profiling... [640/50048]	Loss: 2.0354
Profiling... [768/50048]	Loss: 1.9302
Profiling... [896/50048]	Loss: 2.0780
Profiling... [1024/50048]	Loss: 2.0224
Profiling... [1152/50048]	Loss: 2.2797
Profiling... [1280/50048]	Loss: 2.1316
Profiling... [1408/50048]	Loss: 2.3059
Profiling... [1536/50048]	Loss: 2.0041
Profiling... [1664/50048]	Loss: 1.8717
Profile done
epoch 1 train time consumed: 3.35s
Validation Epoch: 6, Average loss: 0.0172, Accuracy: 0.4181
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.59613633916226,
                        "time": 2.160621341994556,
                        "accuracy": 0.41811708860759494,
                        "total_cost": 452450.02344388416
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1252
Profiling... [256/50048]	Loss: 2.0433
Profiling... [384/50048]	Loss: 2.0666
Profiling... [512/50048]	Loss: 2.2295
Profiling... [640/50048]	Loss: 2.3158
Profiling... [768/50048]	Loss: 2.0402
Profiling... [896/50048]	Loss: 2.1327
Profiling... [1024/50048]	Loss: 2.2173
Profiling... [1152/50048]	Loss: 2.3222
Profiling... [1280/50048]	Loss: 1.9332
Profiling... [1408/50048]	Loss: 2.1612
Profiling... [1536/50048]	Loss: 2.2482
Profiling... [1664/50048]	Loss: 2.2011
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 6, Average loss: 0.0172, Accuracy: 0.4208
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.6041557768467,
                        "time": 2.22042307099764,
                        "accuracy": 0.4207871835443038,
                        "total_cost": 462022.49250789796
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0597
Profiling... [256/50048]	Loss: 2.1820
Profiling... [384/50048]	Loss: 2.1443
Profiling... [512/50048]	Loss: 2.2566
Profiling... [640/50048]	Loss: 2.3131
Profiling... [768/50048]	Loss: 2.2583
Profiling... [896/50048]	Loss: 2.3031
Profiling... [1024/50048]	Loss: 2.2003
Profiling... [1152/50048]	Loss: 2.0385
Profiling... [1280/50048]	Loss: 2.0017
Profiling... [1408/50048]	Loss: 2.0094
Profiling... [1536/50048]	Loss: 1.8764
Profiling... [1664/50048]	Loss: 2.1354
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 6, Average loss: 0.0172, Accuracy: 0.4150
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.58853231352695,
                        "time": 2.576107709995995,
                        "accuracy": 0.4149525316455696,
                        "total_cost": 543569.964200718
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.2179
Profiling... [256/50048]	Loss: 2.2583
Profiling... [384/50048]	Loss: 2.5885
Profiling... [512/50048]	Loss: 2.2767
Profiling... [640/50048]	Loss: 2.7395
Profiling... [768/50048]	Loss: 2.4687
Profiling... [896/50048]	Loss: 2.3947
Profiling... [1024/50048]	Loss: 2.5310
Profiling... [1152/50048]	Loss: 2.1853
Profiling... [1280/50048]	Loss: 2.4217
Profiling... [1408/50048]	Loss: 2.4899
Profiling... [1536/50048]	Loss: 2.4955
Profiling... [1664/50048]	Loss: 2.2774
Profile done
epoch 1 train time consumed: 3.33s
Validation Epoch: 6, Average loss: 0.0203, Accuracy: 0.3426
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.56182609977641,
                        "time": 2.1618678439990617,
                        "accuracy": 0.3425632911392405,
                        "total_cost": 552558.2982067243
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.3122
Profiling... [256/50048]	Loss: 2.4373
Profiling... [384/50048]	Loss: 2.0949
Profiling... [512/50048]	Loss: 2.3180
Profiling... [640/50048]	Loss: 2.6017
Profiling... [768/50048]	Loss: 2.5439
Profiling... [896/50048]	Loss: 2.4095
Profiling... [1024/50048]	Loss: 2.5499
Profiling... [1152/50048]	Loss: 2.6388
Profiling... [1280/50048]	Loss: 2.3376
Profiling... [1408/50048]	Loss: 2.3641
Profiling... [1536/50048]	Loss: 2.2364
Profiling... [1664/50048]	Loss: 2.3443
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 6, Average loss: 0.0193, Accuracy: 0.3652
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.57586826392887,
                        "time": 2.2773329799965722,
                        "accuracy": 0.36520965189873417,
                        "total_cost": 545976.7006383869
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.2034
Profiling... [256/50048]	Loss: 2.1145
Profiling... [384/50048]	Loss: 2.5764
Profiling... [512/50048]	Loss: 2.1887
Profiling... [640/50048]	Loss: 2.2901
Profiling... [768/50048]	Loss: 2.3925
Profiling... [896/50048]	Loss: 2.6331
Profiling... [1024/50048]	Loss: 2.5779
Profiling... [1152/50048]	Loss: 2.3716
Profiling... [1280/50048]	Loss: 2.3921
Profiling... [1408/50048]	Loss: 2.3931
Profiling... [1536/50048]	Loss: 2.3098
Profiling... [1664/50048]	Loss: 2.5342
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 6, Average loss: 0.0199, Accuracy: 0.3476
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.58311375024147,
                        "time": 2.218606563001231,
                        "accuracy": 0.34760680379746833,
                        "total_cost": 558832.7681197956
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0443
Profiling... [256/50048]	Loss: 2.1477
Profiling... [384/50048]	Loss: 2.1616
Profiling... [512/50048]	Loss: 2.6010
Profiling... [640/50048]	Loss: 2.9425
Profiling... [768/50048]	Loss: 2.1835
Profiling... [896/50048]	Loss: 2.5330
Profiling... [1024/50048]	Loss: 2.3760
Profiling... [1152/50048]	Loss: 2.3615
Profiling... [1280/50048]	Loss: 2.4085
Profiling... [1408/50048]	Loss: 2.4003
Profiling... [1536/50048]	Loss: 2.6923
Profiling... [1664/50048]	Loss: 2.5875
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 6, Average loss: 0.0200, Accuracy: 0.3542
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.56689556384097,
                        "time": 2.5826209350052522,
                        "accuracy": 0.35423259493670883,
                        "total_cost": 638354.5307973851
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1673
Profiling... [256/50048]	Loss: 2.3149
Profiling... [384/50048]	Loss: 2.2938
Profiling... [512/50048]	Loss: 2.2262
Profiling... [640/50048]	Loss: 2.3604
Profiling... [768/50048]	Loss: 2.2722
Profiling... [896/50048]	Loss: 2.4727
Profiling... [1024/50048]	Loss: 2.5429
Profiling... [1152/50048]	Loss: 2.7879
Profiling... [1280/50048]	Loss: 2.3874
Profiling... [1408/50048]	Loss: 2.6121
Profiling... [1536/50048]	Loss: 2.5373
Profiling... [1664/50048]	Loss: 2.1863
Profile done
epoch 1 train time consumed: 3.36s
Validation Epoch: 6, Average loss: 0.0206, Accuracy: 0.3432
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.54229512881247,
                        "time": 2.1542076029945747,
                        "accuracy": 0.3431566455696203,
                        "total_cost": 549648.2860958396
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.2330
Profiling... [256/50048]	Loss: 2.0191
Profiling... [384/50048]	Loss: 2.0392
Profiling... [512/50048]	Loss: 2.4455
Profiling... [640/50048]	Loss: 2.4661
Profiling... [768/50048]	Loss: 2.4775
Profiling... [896/50048]	Loss: 2.8170
Profiling... [1024/50048]	Loss: 2.4394
Profiling... [1152/50048]	Loss: 2.6571
Profiling... [1280/50048]	Loss: 2.3769
Profiling... [1408/50048]	Loss: 2.3238
Profiling... [1536/50048]	Loss: 2.5522
Profiling... [1664/50048]	Loss: 2.3898
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 6, Average loss: 0.0200, Accuracy: 0.3489
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.55544553675081,
                        "time": 2.1646731389992055,
                        "accuracy": 0.34889240506329117,
                        "total_cost": 543238.5518378324
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0466
Profiling... [256/50048]	Loss: 2.2582
Profiling... [384/50048]	Loss: 2.1011
Profiling... [512/50048]	Loss: 2.6442
Profiling... [640/50048]	Loss: 2.3316
Profiling... [768/50048]	Loss: 2.5101
Profiling... [896/50048]	Loss: 2.3696
Profiling... [1024/50048]	Loss: 2.3432
Profiling... [1152/50048]	Loss: 2.4068
Profiling... [1280/50048]	Loss: 2.3347
Profiling... [1408/50048]	Loss: 2.2916
Profiling... [1536/50048]	Loss: 2.1259
Profiling... [1664/50048]	Loss: 2.4857
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 6, Average loss: 0.0210, Accuracy: 0.3337
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.56328114346186,
                        "time": 2.213688612995611,
                        "accuracy": 0.3336629746835443,
                        "total_cost": 580895.8896086827
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9918
Profiling... [256/50048]	Loss: 2.3662
Profiling... [384/50048]	Loss: 2.5896
Profiling... [512/50048]	Loss: 2.4863
Profiling... [640/50048]	Loss: 2.3496
Profiling... [768/50048]	Loss: 2.4345
Profiling... [896/50048]	Loss: 2.4354
Profiling... [1024/50048]	Loss: 2.5102
Profiling... [1152/50048]	Loss: 2.4020
Profiling... [1280/50048]	Loss: 2.4256
Profiling... [1408/50048]	Loss: 2.5233
Profiling... [1536/50048]	Loss: 2.3808
Profiling... [1664/50048]	Loss: 2.1537
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 6, Average loss: 0.0208, Accuracy: 0.3294
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.54911361837411,
                        "time": 2.571040032999008,
                        "accuracy": 0.3294106012658228,
                        "total_cost": 683378.0445461983
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1725
Profiling... [256/50048]	Loss: 2.3181
Profiling... [384/50048]	Loss: 2.2115
Profiling... [512/50048]	Loss: 2.2850
Profiling... [640/50048]	Loss: 2.2233
Profiling... [768/50048]	Loss: 2.5779
Profiling... [896/50048]	Loss: 2.4764
Profiling... [1024/50048]	Loss: 2.1909
Profiling... [1152/50048]	Loss: 2.4995
Profiling... [1280/50048]	Loss: 2.4507
Profiling... [1408/50048]	Loss: 2.3219
Profiling... [1536/50048]	Loss: 2.5695
Profiling... [1664/50048]	Loss: 2.7028
Profile done
epoch 1 train time consumed: 3.35s
Validation Epoch: 6, Average loss: 0.0200, Accuracy: 0.3497
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.52497773768782,
                        "time": 2.1661462410047534,
                        "accuracy": 0.34968354430379744,
                        "total_cost": 542378.258940993
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.4355
Profiling... [256/50048]	Loss: 2.3918
Profiling... [384/50048]	Loss: 2.4996
Profiling... [512/50048]	Loss: 2.4010
Profiling... [640/50048]	Loss: 2.2856
Profiling... [768/50048]	Loss: 2.5913
Profiling... [896/50048]	Loss: 2.3530
Profiling... [1024/50048]	Loss: 2.3464
Profiling... [1152/50048]	Loss: 2.7279
Profiling... [1280/50048]	Loss: 2.5066
Profiling... [1408/50048]	Loss: 2.3059
Profiling... [1536/50048]	Loss: 2.2532
Profiling... [1664/50048]	Loss: 2.3547
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 6, Average loss: 0.0214, Accuracy: 0.3254
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.53780694074001,
                        "time": 2.1645330329993158,
                        "accuracy": 0.32535601265822783,
                        "total_cost": 582498.8971491078
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1421
Profiling... [256/50048]	Loss: 2.5112
Profiling... [384/50048]	Loss: 2.5554
Profiling... [512/50048]	Loss: 2.4584
Profiling... [640/50048]	Loss: 2.5000
Profiling... [768/50048]	Loss: 2.4325
Profiling... [896/50048]	Loss: 2.6369
Profiling... [1024/50048]	Loss: 2.3729
Profiling... [1152/50048]	Loss: 2.4207
Profiling... [1280/50048]	Loss: 2.5806
Profiling... [1408/50048]	Loss: 2.4866
Profiling... [1536/50048]	Loss: 2.2121
Profiling... [1664/50048]	Loss: 2.7418
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 6, Average loss: 0.0209, Accuracy: 0.3230
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.54743949435961,
                        "time": 2.212699618001352,
                        "accuracy": 0.32298259493670883,
                        "total_cost": 599836.7800626525
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.2906
Profiling... [256/50048]	Loss: 2.1139
Profiling... [384/50048]	Loss: 2.6781
Profiling... [512/50048]	Loss: 2.6372
Profiling... [640/50048]	Loss: 2.5332
Profiling... [768/50048]	Loss: 2.3614
Profiling... [896/50048]	Loss: 2.6525
Profiling... [1024/50048]	Loss: 2.2959
Profiling... [1152/50048]	Loss: 2.2967
Profiling... [1280/50048]	Loss: 2.4073
Profiling... [1408/50048]	Loss: 2.4070
Profiling... [1536/50048]	Loss: 2.6368
Profiling... [1664/50048]	Loss: 2.4799
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 6, Average loss: 0.0202, Accuracy: 0.3519
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.53524483144433,
                        "time": 2.561913320001622,
                        "accuracy": 0.3518591772151899,
                        "total_cost": 637507.4568282993
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.2307
Profiling... [256/50048]	Loss: 2.7265
Profiling... [384/50048]	Loss: 2.7732
Profiling... [512/50048]	Loss: 2.8324
Profiling... [640/50048]	Loss: 2.7225
Profiling... [768/50048]	Loss: 2.8665
Profiling... [896/50048]	Loss: 2.6286
Profiling... [1024/50048]	Loss: 2.8802
Profiling... [1152/50048]	Loss: 2.4675
Profiling... [1280/50048]	Loss: 2.8046
Profiling... [1408/50048]	Loss: 2.8351
Profiling... [1536/50048]	Loss: 2.8428
Profiling... [1664/50048]	Loss: 2.8396
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 6, Average loss: 0.0318, Accuracy: 0.2203
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.51049400239322,
                        "time": 2.1606163089963957,
                        "accuracy": 0.22033227848101267,
                        "total_cost": 858596.6371049851
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.0873
Profiling... [256/50048]	Loss: 2.6532
Profiling... [384/50048]	Loss: 2.9026
Profiling... [512/50048]	Loss: 2.5862
Profiling... [640/50048]	Loss: 2.9745
Profiling... [768/50048]	Loss: 2.8404
Profiling... [896/50048]	Loss: 2.8046
Profiling... [1024/50048]	Loss: 2.5800
Profiling... [1152/50048]	Loss: 2.8057
Profiling... [1280/50048]	Loss: 2.8162
Profiling... [1408/50048]	Loss: 2.8212
Profiling... [1536/50048]	Loss: 2.9938
Profiling... [1664/50048]	Loss: 2.8820
Profile done
epoch 1 train time consumed: 3.30s
Validation Epoch: 6, Average loss: 0.0461, Accuracy: 0.2132
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.52443206170881,
                        "time": 2.1687130159989465,
                        "accuracy": 0.2132120253164557,
                        "total_cost": 890594.6537246065
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0247
Profiling... [256/50048]	Loss: 2.3936
Profiling... [384/50048]	Loss: 2.8239
Profiling... [512/50048]	Loss: 2.7642
Profiling... [640/50048]	Loss: 2.8244
Profiling... [768/50048]	Loss: 2.8451
Profiling... [896/50048]	Loss: 2.9863
Profiling... [1024/50048]	Loss: 2.8099
Profiling... [1152/50048]	Loss: 2.6490
Profiling... [1280/50048]	Loss: 2.9692
Profiling... [1408/50048]	Loss: 2.6567
Profiling... [1536/50048]	Loss: 2.9304
Profiling... [1664/50048]	Loss: 2.4810
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 6, Average loss: 0.0365, Accuracy: 0.2235
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.5348403473509,
                        "time": 2.207975173005252,
                        "accuracy": 0.22349683544303797,
                        "total_cost": 864992.8680605876
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.1598
Profiling... [256/50048]	Loss: 2.8457
Profiling... [384/50048]	Loss: 2.7435
Profiling... [512/50048]	Loss: 2.6307
Profiling... [640/50048]	Loss: 2.5299
Profiling... [768/50048]	Loss: 2.6448
Profiling... [896/50048]	Loss: 2.8251
Profiling... [1024/50048]	Loss: 2.6133
Profiling... [1152/50048]	Loss: 2.6348
Profiling... [1280/50048]	Loss: 2.7011
Profiling... [1408/50048]	Loss: 2.8731
Profiling... [1536/50048]	Loss: 2.7541
Profiling... [1664/50048]	Loss: 3.0162
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 6, Average loss: 0.0344, Accuracy: 0.2434
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.51913863017906,
                        "time": 2.587036866003473,
                        "accuracy": 0.2433742088607595,
                        "total_cost": 930717.2108907152
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0369
Profiling... [256/50048]	Loss: 2.4425
Profiling... [384/50048]	Loss: 3.0836
Profiling... [512/50048]	Loss: 2.5424
Profiling... [640/50048]	Loss: 2.8370
Profiling... [768/50048]	Loss: 3.0334
Profiling... [896/50048]	Loss: 2.7518
Profiling... [1024/50048]	Loss: 2.7337
Profiling... [1152/50048]	Loss: 2.6900
Profiling... [1280/50048]	Loss: 2.7229
Profiling... [1408/50048]	Loss: 2.9274
Profiling... [1536/50048]	Loss: 3.0644
Profiling... [1664/50048]	Loss: 2.8429
Profile done
epoch 1 train time consumed: 3.40s
Validation Epoch: 6, Average loss: 0.0565, Accuracy: 0.2323
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.49489574626413,
                        "time": 2.167734449998534,
                        "accuracy": 0.2322982594936709,
                        "total_cost": 817052.0872014842
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1901
Profiling... [256/50048]	Loss: 2.5250
Profiling... [384/50048]	Loss: 2.9231
Profiling... [512/50048]	Loss: 2.9970
Profiling... [640/50048]	Loss: 2.6681
Profiling... [768/50048]	Loss: 2.6376
Profiling... [896/50048]	Loss: 2.3780
Profiling... [1024/50048]	Loss: 2.6186
Profiling... [1152/50048]	Loss: 2.6259
Profiling... [1280/50048]	Loss: 2.8414
Profiling... [1408/50048]	Loss: 2.7808
Profiling... [1536/50048]	Loss: 2.9883
Profiling... [1664/50048]	Loss: 2.7819
Profile done
epoch 1 train time consumed: 3.35s
Validation Epoch: 6, Average loss: 0.0415, Accuracy: 0.2174
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.50890196417141,
                        "time": 2.166848637003568,
                        "accuracy": 0.21736550632911392,
                        "total_cost": 872825.8556134822
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1457
Profiling... [256/50048]	Loss: 2.5896
Profiling... [384/50048]	Loss: 2.7163
Profiling... [512/50048]	Loss: 2.8358
Profiling... [640/50048]	Loss: 2.9613
Profiling... [768/50048]	Loss: 2.6821
Profiling... [896/50048]	Loss: 2.7353
Profiling... [1024/50048]	Loss: 2.6887
Profiling... [1152/50048]	Loss: 2.7524
Profiling... [1280/50048]	Loss: 2.9695
Profiling... [1408/50048]	Loss: 3.0450
Profiling... [1536/50048]	Loss: 2.5792
Profiling... [1664/50048]	Loss: 2.8949
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 6, Average loss: 0.0797, Accuracy: 0.1636
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.51756349634987,
                        "time": 2.2090599380026106,
                        "accuracy": 0.16356803797468356,
                        "total_cost": 1182493.4169354935
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.4644
Profiling... [256/50048]	Loss: 2.2512
Profiling... [384/50048]	Loss: 3.0768
Profiling... [512/50048]	Loss: 2.7642
Profiling... [640/50048]	Loss: 2.6923
Profiling... [768/50048]	Loss: 2.6225
Profiling... [896/50048]	Loss: 2.9347
Profiling... [1024/50048]	Loss: 2.9655
Profiling... [1152/50048]	Loss: 2.8221
Profiling... [1280/50048]	Loss: 2.7291
Profiling... [1408/50048]	Loss: 2.8711
Profiling... [1536/50048]	Loss: 2.6772
Profiling... [1664/50048]	Loss: 2.8901
Profile done
epoch 1 train time consumed: 3.81s
Validation Epoch: 6, Average loss: 0.0303, Accuracy: 0.1920
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.50269266202417,
                        "time": 2.5859278100033407,
                        "accuracy": 0.1920490506329114,
                        "total_cost": 1178945.8865527143
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8785
Profiling... [256/50048]	Loss: 2.7933
Profiling... [384/50048]	Loss: 2.8859
Profiling... [512/50048]	Loss: 2.6926
Profiling... [640/50048]	Loss: 2.6744
Profiling... [768/50048]	Loss: 3.1494
Profiling... [896/50048]	Loss: 2.5955
Profiling... [1024/50048]	Loss: 3.0244
Profiling... [1152/50048]	Loss: 2.8760
Profiling... [1280/50048]	Loss: 2.8310
Profiling... [1408/50048]	Loss: 2.6411
Profiling... [1536/50048]	Loss: 2.5424
Profiling... [1664/50048]	Loss: 2.6098
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 6, Average loss: 0.0376, Accuracy: 0.2399
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.47775598120961,
                        "time": 2.170078424998792,
                        "accuracy": 0.2399129746835443,
                        "total_cost": 791974.6326892292
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.2234
Profiling... [256/50048]	Loss: 2.6185
Profiling... [384/50048]	Loss: 2.5583
Profiling... [512/50048]	Loss: 2.8518
Profiling... [640/50048]	Loss: 2.5585
Profiling... [768/50048]	Loss: 2.7276
Profiling... [896/50048]	Loss: 2.8592
Profiling... [1024/50048]	Loss: 2.7365
Profiling... [1152/50048]	Loss: 2.7966
Profiling... [1280/50048]	Loss: 2.5751
Profiling... [1408/50048]	Loss: 2.5980
Profiling... [1536/50048]	Loss: 2.7651
Profiling... [1664/50048]	Loss: 2.7061
Profile done
epoch 1 train time consumed: 3.41s
Validation Epoch: 6, Average loss: 0.0260, Accuracy: 0.2495
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.49252383897291,
                        "time": 2.1685560690020793,
                        "accuracy": 0.24950553797468356,
                        "total_cost": 760991.9804971473
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1368
Profiling... [256/50048]	Loss: 2.2869
Profiling... [384/50048]	Loss: 2.8533
Profiling... [512/50048]	Loss: 2.7149
Profiling... [640/50048]	Loss: 2.9286
Profiling... [768/50048]	Loss: 2.8198
Profiling... [896/50048]	Loss: 2.7480
Profiling... [1024/50048]	Loss: 2.7299
Profiling... [1152/50048]	Loss: 2.9541
Profiling... [1280/50048]	Loss: 2.7062
Profiling... [1408/50048]	Loss: 2.5584
Profiling... [1536/50048]	Loss: 2.6739
Profiling... [1664/50048]	Loss: 2.6691
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 6, Average loss: 0.0299, Accuracy: 0.2461
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.50005210029492,
                        "time": 2.215238122000301,
                        "accuracy": 0.24614319620253164,
                        "total_cost": 787992.7354829815
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.1304
Profiling... [256/50048]	Loss: 2.4360
Profiling... [384/50048]	Loss: 2.7772
Profiling... [512/50048]	Loss: 3.0124
Profiling... [640/50048]	Loss: 2.8331
Profiling... [768/50048]	Loss: 3.2715
Profiling... [896/50048]	Loss: 2.8693
Profiling... [1024/50048]	Loss: 2.7401
Profiling... [1152/50048]	Loss: 2.6983
Profiling... [1280/50048]	Loss: 2.9827
Profiling... [1408/50048]	Loss: 2.8694
Profiling... [1536/50048]	Loss: 2.8431
Profiling... [1664/50048]	Loss: 2.7794
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 6, Average loss: 0.0368, Accuracy: 0.2005
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.48713530549435,
                        "time": 2.5566450059995987,
                        "accuracy": 0.20045490506329114,
                        "total_cost": 1116717.553572204
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8848
Profiling... [512/50176]	Loss: 2.2797
Profiling... [768/50176]	Loss: 2.1507
Profiling... [1024/50176]	Loss: 2.1788
Profiling... [1280/50176]	Loss: 2.0750
Profiling... [1536/50176]	Loss: 2.1573
Profiling... [1792/50176]	Loss: 2.1533
Profiling... [2048/50176]	Loss: 2.2185
Profiling... [2304/50176]	Loss: 2.1255
Profiling... [2560/50176]	Loss: 1.9444
Profiling... [2816/50176]	Loss: 1.9584
Profiling... [3072/50176]	Loss: 2.1841
Profiling... [3328/50176]	Loss: 2.3146
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.47211945758828,
                        "time": 2.3815273020009045,
                        "accuracy": 0.4181640625,
                        "total_cost": 498652.98360097886
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1171
Profiling... [512/50176]	Loss: 2.2056
Profiling... [768/50176]	Loss: 2.0221
Profiling... [1024/50176]	Loss: 2.1578
Profiling... [1280/50176]	Loss: 2.0732
Profiling... [1536/50176]	Loss: 2.2236
Profiling... [1792/50176]	Loss: 2.2191
Profiling... [2048/50176]	Loss: 2.2516
Profiling... [2304/50176]	Loss: 2.1552
Profiling... [2560/50176]	Loss: 2.2550
Profiling... [2816/50176]	Loss: 2.0302
Profiling... [3072/50176]	Loss: 2.0855
Profiling... [3328/50176]	Loss: 2.0756
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4224
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.49308293374804,
                        "time": 2.384231518000888,
                        "accuracy": 0.42236328125,
                        "total_cost": 494255.9260832539
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.2326
Profiling... [512/50176]	Loss: 2.3037
Profiling... [768/50176]	Loss: 2.2944
Profiling... [1024/50176]	Loss: 2.2480
Profiling... [1280/50176]	Loss: 1.9862
Profiling... [1536/50176]	Loss: 2.1411
Profiling... [1792/50176]	Loss: 2.2479
Profiling... [2048/50176]	Loss: 2.1282
Profiling... [2304/50176]	Loss: 2.4467
Profiling... [2560/50176]	Loss: 2.1347
Profiling... [2816/50176]	Loss: 2.1879
Profiling... [3072/50176]	Loss: 2.2017
Profiling... [3328/50176]	Loss: 2.1640
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4187
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.50549728468843,
                        "time": 2.5267476520020864,
                        "accuracy": 0.41865234375,
                        "total_cost": 528442.806369836
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.2222
Profiling... [512/50176]	Loss: 2.2603
Profiling... [768/50176]	Loss: 2.0424
Profiling... [1024/50176]	Loss: 2.2680
Profiling... [1280/50176]	Loss: 2.1365
Profiling... [1536/50176]	Loss: 2.1923
Profiling... [1792/50176]	Loss: 2.1202
Profiling... [2048/50176]	Loss: 2.0572
Profiling... [2304/50176]	Loss: 2.1603
Profiling... [2560/50176]	Loss: 2.2302
Profiling... [2816/50176]	Loss: 2.1863
Profiling... [3072/50176]	Loss: 2.0115
Profiling... [3328/50176]	Loss: 2.1723
Profile done
epoch 1 train time consumed: 4.39s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4159
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.48626772006689,
                        "time": 3.0198491900009685,
                        "accuracy": 0.41591796875,
                        "total_cost": 635721.8963791404
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0346
Profiling... [512/50176]	Loss: 2.1413
Profiling... [768/50176]	Loss: 2.1847
Profiling... [1024/50176]	Loss: 2.1077
Profiling... [1280/50176]	Loss: 2.1137
Profiling... [1536/50176]	Loss: 2.2156
Profiling... [1792/50176]	Loss: 2.0860
Profiling... [2048/50176]	Loss: 2.1944
Profiling... [2304/50176]	Loss: 2.0011
Profiling... [2560/50176]	Loss: 2.3236
Profiling... [2816/50176]	Loss: 2.2040
Profiling... [3072/50176]	Loss: 2.1084
Profiling... [3328/50176]	Loss: 2.1947
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4167
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.46889529481678,
                        "time": 2.383279378002044,
                        "accuracy": 0.41669921875,
                        "total_cost": 500774.06010082486
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9694
Profiling... [512/50176]	Loss: 2.2408
Profiling... [768/50176]	Loss: 2.2849
Profiling... [1024/50176]	Loss: 2.3441
Profiling... [1280/50176]	Loss: 2.2499
Profiling... [1536/50176]	Loss: 2.3045
Profiling... [1792/50176]	Loss: 2.0956
Profiling... [2048/50176]	Loss: 2.2840
Profiling... [2304/50176]	Loss: 2.2393
Profiling... [2560/50176]	Loss: 2.1961
Profiling... [2816/50176]	Loss: 1.9194
Profiling... [3072/50176]	Loss: 2.4052
Profiling... [3328/50176]	Loss: 2.1966
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4190
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.49136606617276,
                        "time": 2.4855848459992558,
                        "accuracy": 0.41904296875,
                        "total_cost": 519349.41393705894
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1843
Profiling... [512/50176]	Loss: 2.1069
Profiling... [768/50176]	Loss: 2.1080
Profiling... [1024/50176]	Loss: 2.1584
Profiling... [1280/50176]	Loss: 2.1144
Profiling... [1536/50176]	Loss: 2.1565
Profiling... [1792/50176]	Loss: 2.1961
Profiling... [2048/50176]	Loss: 2.2168
Profiling... [2304/50176]	Loss: 2.2258
Profiling... [2560/50176]	Loss: 2.2090
Profiling... [2816/50176]	Loss: 2.2675
Profiling... [3072/50176]	Loss: 2.1192
Profiling... [3328/50176]	Loss: 2.2149
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4186
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.50344380268167,
                        "time": 2.5314699500013376,
                        "accuracy": 0.4185546875,
                        "total_cost": 529553.9448563005
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1497
Profiling... [512/50176]	Loss: 2.2339
Profiling... [768/50176]	Loss: 2.1422
Profiling... [1024/50176]	Loss: 2.2433
Profiling... [1280/50176]	Loss: 2.4092
Profiling... [1536/50176]	Loss: 2.3335
Profiling... [1792/50176]	Loss: 2.1397
Profiling... [2048/50176]	Loss: 2.2933
Profiling... [2304/50176]	Loss: 2.0944
Profiling... [2560/50176]	Loss: 2.3028
Profiling... [2816/50176]	Loss: 2.2886
Profiling... [3072/50176]	Loss: 1.9296
Profiling... [3328/50176]	Loss: 2.3437
Profile done
epoch 1 train time consumed: 4.54s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4180
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.48299187255661,
                        "time": 3.0321332690000418,
                        "accuracy": 0.41796875,
                        "total_cost": 635175.9762304355
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.2574
Profiling... [512/50176]	Loss: 2.1970
Profiling... [768/50176]	Loss: 2.1473
Profiling... [1024/50176]	Loss: 2.0957
Profiling... [1280/50176]	Loss: 2.0304
Profiling... [1536/50176]	Loss: 2.1181
Profiling... [1792/50176]	Loss: 2.2030
Profiling... [2048/50176]	Loss: 2.1680
Profiling... [2304/50176]	Loss: 2.0790
Profiling... [2560/50176]	Loss: 2.1540
Profiling... [2816/50176]	Loss: 2.1768
Profiling... [3072/50176]	Loss: 2.0220
Profiling... [3328/50176]	Loss: 2.1175
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4193
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.46537365016519,
                        "time": 2.3831876819967874,
                        "accuracy": 0.4193359375,
                        "total_cost": 497606.11518092704
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1539
Profiling... [512/50176]	Loss: 2.2510
Profiling... [768/50176]	Loss: 2.1026
Profiling... [1024/50176]	Loss: 1.9434
Profiling... [1280/50176]	Loss: 2.1651
Profiling... [1536/50176]	Loss: 2.1723
Profiling... [1792/50176]	Loss: 2.1903
Profiling... [2048/50176]	Loss: 2.2981
Profiling... [2304/50176]	Loss: 2.1515
Profiling... [2560/50176]	Loss: 2.1261
Profiling... [2816/50176]	Loss: 2.2145
Profiling... [3072/50176]	Loss: 2.1597
Profiling... [3328/50176]	Loss: 2.3064
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4144
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.48809089470129,
                        "time": 2.382541159997345,
                        "accuracy": 0.41435546875,
                        "total_cost": 503450.6897011499
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.2267
Profiling... [512/50176]	Loss: 2.4344
Profiling... [768/50176]	Loss: 2.2389
Profiling... [1024/50176]	Loss: 2.1589
Profiling... [1280/50176]	Loss: 2.2125
Profiling... [1536/50176]	Loss: 2.1032
Profiling... [1792/50176]	Loss: 2.2644
Profiling... [2048/50176]	Loss: 2.2053
Profiling... [2304/50176]	Loss: 2.1440
Profiling... [2560/50176]	Loss: 2.1704
Profiling... [2816/50176]	Loss: 2.1018
Profiling... [3072/50176]	Loss: 2.1654
Profiling... [3328/50176]	Loss: 2.3139
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 6, Average loss: 0.0084, Accuracy: 0.4196
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.49996785811624,
                        "time": 2.529708998998103,
                        "accuracy": 0.41962890625,
                        "total_cost": 527830.8883359304
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9885
Profiling... [512/50176]	Loss: 1.8704
Profiling... [768/50176]	Loss: 2.1794
Profiling... [1024/50176]	Loss: 2.0020
Profiling... [1280/50176]	Loss: 2.2066
Profiling... [1536/50176]	Loss: 2.2355
Profiling... [1792/50176]	Loss: 1.9764
Profiling... [2048/50176]	Loss: 2.0963
Profiling... [2304/50176]	Loss: 2.0117
Profiling... [2560/50176]	Loss: 2.1646
Profiling... [2816/50176]	Loss: 2.0712
Profiling... [3072/50176]	Loss: 2.1294
Profiling... [3328/50176]	Loss: 2.1604
Profile done
epoch 1 train time consumed: 4.49s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4175
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.48134350165134,
                        "time": 3.0223462610010756,
                        "accuracy": 0.41748046875,
                        "total_cost": 633866.2706474384
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.2107
Profiling... [512/50176]	Loss: 2.1230
Profiling... [768/50176]	Loss: 2.2568
Profiling... [1024/50176]	Loss: 2.2857
Profiling... [1280/50176]	Loss: 2.3864
Profiling... [1536/50176]	Loss: 2.5765
Profiling... [1792/50176]	Loss: 2.4087
Profiling... [2048/50176]	Loss: 2.2262
Profiling... [2304/50176]	Loss: 2.2079
Profiling... [2560/50176]	Loss: 2.3360
Profiling... [2816/50176]	Loss: 2.3249
Profiling... [3072/50176]	Loss: 2.3006
Profiling... [3328/50176]	Loss: 2.3515
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 6, Average loss: 0.0092, Accuracy: 0.3775
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.4647861298968,
                        "time": 2.389357444997586,
                        "accuracy": 0.3775390625,
                        "total_cost": 554126.3174669006
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0477
Profiling... [512/50176]	Loss: 2.2418
Profiling... [768/50176]	Loss: 2.2925
Profiling... [1024/50176]	Loss: 2.2925
Profiling... [1280/50176]	Loss: 2.3602
Profiling... [1536/50176]	Loss: 2.3254
Profiling... [1792/50176]	Loss: 2.4667
Profiling... [2048/50176]	Loss: 2.3792
Profiling... [2304/50176]	Loss: 2.2657
Profiling... [2560/50176]	Loss: 2.2889
Profiling... [2816/50176]	Loss: 2.3793
Profiling... [3072/50176]	Loss: 2.3342
Profiling... [3328/50176]	Loss: 2.3433
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 6, Average loss: 0.0101, Accuracy: 0.3494
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.48894373613375,
                        "time": 2.4836060059969896,
                        "accuracy": 0.3494140625,
                        "total_cost": 622346.0351881952
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.3398
Profiling... [512/50176]	Loss: 2.1840
Profiling... [768/50176]	Loss: 2.3397
Profiling... [1024/50176]	Loss: 2.5323
Profiling... [1280/50176]	Loss: 2.5306
Profiling... [1536/50176]	Loss: 2.4136
Profiling... [1792/50176]	Loss: 2.1943
Profiling... [2048/50176]	Loss: 2.4265
Profiling... [2304/50176]	Loss: 2.3499
Profiling... [2560/50176]	Loss: 2.4671
Profiling... [2816/50176]	Loss: 2.3412
Profiling... [3072/50176]	Loss: 2.4236
Profiling... [3328/50176]	Loss: 2.5057
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 6, Average loss: 0.0094, Accuracy: 0.3714
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.50001152959831,
                        "time": 2.5277375809964724,
                        "accuracy": 0.37138671875,
                        "total_cost": 595930.0004168089
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0956
Profiling... [512/50176]	Loss: 2.2699
Profiling... [768/50176]	Loss: 2.3700
Profiling... [1024/50176]	Loss: 2.3355
Profiling... [1280/50176]	Loss: 2.2478
Profiling... [1536/50176]	Loss: 2.4285
Profiling... [1792/50176]	Loss: 2.4336
Profiling... [2048/50176]	Loss: 2.2446
Profiling... [2304/50176]	Loss: 2.3365
Profiling... [2560/50176]	Loss: 2.4626
Profiling... [2816/50176]	Loss: 2.1607
Profiling... [3072/50176]	Loss: 2.4200
Profiling... [3328/50176]	Loss: 2.3670
Profile done
epoch 1 train time consumed: 4.48s
Validation Epoch: 6, Average loss: 0.0096, Accuracy: 0.3672
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.48230660346168,
                        "time": 3.0617438180051977,
                        "accuracy": 0.3671875,
                        "total_cost": 730080.1659936759
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.2162
Profiling... [512/50176]	Loss: 2.1812
Profiling... [768/50176]	Loss: 2.0561
Profiling... [1024/50176]	Loss: 2.1898
Profiling... [1280/50176]	Loss: 2.3648
Profiling... [1536/50176]	Loss: 2.4827
Profiling... [1792/50176]	Loss: 2.2801
Profiling... [2048/50176]	Loss: 2.2262
Profiling... [2304/50176]	Loss: 2.4113
Profiling... [2560/50176]	Loss: 2.2277
Profiling... [2816/50176]	Loss: 2.5026
Profiling... [3072/50176]	Loss: 2.2705
Profiling... [3328/50176]	Loss: 2.4877
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 6, Average loss: 0.0101, Accuracy: 0.3491
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.46625220737985,
                        "time": 2.385702758001571,
                        "accuracy": 0.34912109375,
                        "total_cost": 598314.8639255003
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1545
Profiling... [512/50176]	Loss: 2.2056
Profiling... [768/50176]	Loss: 2.4961
Profiling... [1024/50176]	Loss: 2.3329
Profiling... [1280/50176]	Loss: 2.2397
Profiling... [1536/50176]	Loss: 2.3393
Profiling... [1792/50176]	Loss: 2.2846
Profiling... [2048/50176]	Loss: 2.4578
Profiling... [2304/50176]	Loss: 2.2758
Profiling... [2560/50176]	Loss: 2.2359
Profiling... [2816/50176]	Loss: 2.3030
Profiling... [3072/50176]	Loss: 2.4118
Profiling... [3328/50176]	Loss: 2.4177
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 6, Average loss: 0.0097, Accuracy: 0.3681
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.48587453193983,
                        "time": 2.498929576999217,
                        "accuracy": 0.36806640625,
                        "total_cost": 594452.8782750089
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.2310
Profiling... [512/50176]	Loss: 2.3217
Profiling... [768/50176]	Loss: 2.2901
Profiling... [1024/50176]	Loss: 2.3022
Profiling... [1280/50176]	Loss: 2.4851
Profiling... [1536/50176]	Loss: 2.4270
Profiling... [1792/50176]	Loss: 2.2053
Profiling... [2048/50176]	Loss: 2.4352
Profiling... [2304/50176]	Loss: 2.5383
Profiling... [2560/50176]	Loss: 2.2284
Profiling... [2816/50176]	Loss: 2.4423
Profiling... [3072/50176]	Loss: 2.4030
Profiling... [3328/50176]	Loss: 2.2474
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 6, Average loss: 0.0097, Accuracy: 0.3742
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.49698023579514,
                        "time": 2.528432740997232,
                        "accuracy": 0.37421875,
                        "total_cost": 591582.7295603816
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.2953
Profiling... [512/50176]	Loss: 2.3025
Profiling... [768/50176]	Loss: 2.3952
Profiling... [1024/50176]	Loss: 2.4608
Profiling... [1280/50176]	Loss: 2.3045
Profiling... [1536/50176]	Loss: 2.2554
Profiling... [1792/50176]	Loss: 2.2650
Profiling... [2048/50176]	Loss: 2.2609
Profiling... [2304/50176]	Loss: 2.2828
Profiling... [2560/50176]	Loss: 2.4224
Profiling... [2816/50176]	Loss: 2.5606
Profiling... [3072/50176]	Loss: 2.2943
Profiling... [3328/50176]	Loss: 2.2866
Profile done
epoch 1 train time consumed: 4.52s
Validation Epoch: 6, Average loss: 0.0100, Accuracy: 0.3481
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.47794149103025,
                        "time": 3.0116879580018576,
                        "accuracy": 0.34814453125,
                        "total_cost": 757425.5883133487
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9620
Profiling... [512/50176]	Loss: 2.3693
Profiling... [768/50176]	Loss: 2.3593
Profiling... [1024/50176]	Loss: 2.1918
Profiling... [1280/50176]	Loss: 2.2354
Profiling... [1536/50176]	Loss: 2.1505
Profiling... [1792/50176]	Loss: 2.3065
Profiling... [2048/50176]	Loss: 2.4693
Profiling... [2304/50176]	Loss: 2.2015
Profiling... [2560/50176]	Loss: 2.2004
Profiling... [2816/50176]	Loss: 2.5243
Profiling... [3072/50176]	Loss: 2.3540
Profiling... [3328/50176]	Loss: 2.3666
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 6, Average loss: 0.0095, Accuracy: 0.3650
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.46076172142877,
                        "time": 2.4921841960021993,
                        "accuracy": 0.3650390625,
                        "total_cost": 597764.7932097861
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1665
Profiling... [512/50176]	Loss: 2.4763
Profiling... [768/50176]	Loss: 2.1665
Profiling... [1024/50176]	Loss: 2.2280
Profiling... [1280/50176]	Loss: 2.1883
Profiling... [1536/50176]	Loss: 2.4380
Profiling... [1792/50176]	Loss: 2.3402
Profiling... [2048/50176]	Loss: 2.3105
Profiling... [2304/50176]	Loss: 2.4439
Profiling... [2560/50176]	Loss: 2.2598
Profiling... [2816/50176]	Loss: 2.3726
Profiling... [3072/50176]	Loss: 2.3768
Profiling... [3328/50176]	Loss: 2.3836
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 6, Average loss: 0.0098, Accuracy: 0.3590
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.48083352573856,
                        "time": 2.3843291259981925,
                        "accuracy": 0.358984375,
                        "total_cost": 581540.8716692775
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1215
Profiling... [512/50176]	Loss: 2.4068
Profiling... [768/50176]	Loss: 2.3490
Profiling... [1024/50176]	Loss: 2.3439
Profiling... [1280/50176]	Loss: 2.3889
Profiling... [1536/50176]	Loss: 2.4213
Profiling... [1792/50176]	Loss: 2.3383
Profiling... [2048/50176]	Loss: 2.2823
Profiling... [2304/50176]	Loss: 2.4252
Profiling... [2560/50176]	Loss: 2.3967
Profiling... [2816/50176]	Loss: 2.3731
Profiling... [3072/50176]	Loss: 2.3684
Profiling... [3328/50176]	Loss: 2.3283
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 6, Average loss: 0.0100, Accuracy: 0.3495
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.49292763328673,
                        "time": 2.523598812003911,
                        "accuracy": 0.34951171875,
                        "total_cost": 632190.8236703875
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.3566
Profiling... [512/50176]	Loss: 2.3240
Profiling... [768/50176]	Loss: 2.1063
Profiling... [1024/50176]	Loss: 2.3364
Profiling... [1280/50176]	Loss: 2.2842
Profiling... [1536/50176]	Loss: 2.2677
Profiling... [1792/50176]	Loss: 2.1042
Profiling... [2048/50176]	Loss: 2.4713
Profiling... [2304/50176]	Loss: 2.2994
Profiling... [2560/50176]	Loss: 2.5012
Profiling... [2816/50176]	Loss: 2.1782
Profiling... [3072/50176]	Loss: 2.3690
Profiling... [3328/50176]	Loss: 2.5116
Profile done
epoch 1 train time consumed: 4.45s
Validation Epoch: 6, Average loss: 0.0101, Accuracy: 0.3460
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.4759217344909,
                        "time": 3.0253950799960876,
                        "accuracy": 0.34599609375,
                        "total_cost": 765597.4417986169
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0867
Profiling... [512/50176]	Loss: 2.4303
Profiling... [768/50176]	Loss: 2.3275
Profiling... [1024/50176]	Loss: 2.5702
Profiling... [1280/50176]	Loss: 2.7646
Profiling... [1536/50176]	Loss: 2.5462
Profiling... [1792/50176]	Loss: 2.7422
Profiling... [2048/50176]	Loss: 2.7058
Profiling... [2304/50176]	Loss: 2.4926
Profiling... [2560/50176]	Loss: 2.6598
Profiling... [2816/50176]	Loss: 2.5012
Profiling... [3072/50176]	Loss: 2.7537
Profiling... [3328/50176]	Loss: 2.6594
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 6, Average loss: 0.0130, Accuracy: 0.2444
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.45942279447404,
                        "time": 2.3809171920001972,
                        "accuracy": 0.24443359375,
                        "total_cost": 852850.5425420074
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9783
Profiling... [512/50176]	Loss: 2.4919
Profiling... [768/50176]	Loss: 2.4364
Profiling... [1024/50176]	Loss: 2.5304
Profiling... [1280/50176]	Loss: 2.6447
Profiling... [1536/50176]	Loss: 2.7291
Profiling... [1792/50176]	Loss: 2.7034
Profiling... [2048/50176]	Loss: 2.4882
Profiling... [2304/50176]	Loss: 2.5969
Profiling... [2560/50176]	Loss: 2.6966
Profiling... [2816/50176]	Loss: 2.6413
Profiling... [3072/50176]	Loss: 2.5415
Profiling... [3328/50176]	Loss: 2.4897
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 6, Average loss: 0.0118, Accuracy: 0.2921
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.48001649785944,
                        "time": 2.382164583999838,
                        "accuracy": 0.29208984375,
                        "total_cost": 714076.7459092193
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1452
Profiling... [512/50176]	Loss: 2.5232
Profiling... [768/50176]	Loss: 2.8113
Profiling... [1024/50176]	Loss: 2.5727
Profiling... [1280/50176]	Loss: 2.6765
Profiling... [1536/50176]	Loss: 2.6505
Profiling... [1792/50176]	Loss: 2.7014
Profiling... [2048/50176]	Loss: 2.5661
Profiling... [2304/50176]	Loss: 2.6413
Profiling... [2560/50176]	Loss: 2.7526
Profiling... [2816/50176]	Loss: 2.6216
Profiling... [3072/50176]	Loss: 2.8202
Profiling... [3328/50176]	Loss: 2.7684
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 6, Average loss: 0.0267, Accuracy: 0.2522
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.48934117102048,
                        "time": 2.554792296003143,
                        "accuracy": 0.25224609375,
                        "total_cost": 886789.9336797008
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0923
Profiling... [512/50176]	Loss: 2.5805
Profiling... [768/50176]	Loss: 2.6795
Profiling... [1024/50176]	Loss: 2.4590
Profiling... [1280/50176]	Loss: 2.6525
Profiling... [1536/50176]	Loss: 2.5753
Profiling... [1792/50176]	Loss: 2.6444
Profiling... [2048/50176]	Loss: 2.6771
Profiling... [2304/50176]	Loss: 2.4549
Profiling... [2560/50176]	Loss: 2.4180
Profiling... [2816/50176]	Loss: 2.6798
Profiling... [3072/50176]	Loss: 2.6286
Profiling... [3328/50176]	Loss: 2.7381
Profile done
epoch 1 train time consumed: 4.47s
Validation Epoch: 6, Average loss: 0.0139, Accuracy: 0.2345
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.47161161299321,
                        "time": 3.014747259003343,
                        "accuracy": 0.23447265625,
                        "total_cost": 1125766.3622678178
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9303
Profiling... [512/50176]	Loss: 2.4634
Profiling... [768/50176]	Loss: 2.3754
Profiling... [1024/50176]	Loss: 2.7212
Profiling... [1280/50176]	Loss: 2.6955
Profiling... [1536/50176]	Loss: 2.6897
Profiling... [1792/50176]	Loss: 2.6927
Profiling... [2048/50176]	Loss: 2.6055
Profiling... [2304/50176]	Loss: 2.5791
Profiling... [2560/50176]	Loss: 2.6641
Profiling... [2816/50176]	Loss: 2.7803
Profiling... [3072/50176]	Loss: 2.5448
Profiling... [3328/50176]	Loss: 2.6092
Profile done
epoch 1 train time consumed: 3.84s
Validation Epoch: 6, Average loss: 0.0127, Accuracy: 0.2878
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.45442355014328,
                        "time": 2.3867159240035107,
                        "accuracy": 0.28779296875,
                        "total_cost": 726122.795833507
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.3164
Profiling... [512/50176]	Loss: 2.5809
Profiling... [768/50176]	Loss: 2.4121
Profiling... [1024/50176]	Loss: 2.3687
Profiling... [1280/50176]	Loss: 2.5405
Profiling... [1536/50176]	Loss: 2.8234
Profiling... [1792/50176]	Loss: 2.6316
Profiling... [2048/50176]	Loss: 2.6035
Profiling... [2304/50176]	Loss: 2.6469
Profiling... [2560/50176]	Loss: 2.6831
Profiling... [2816/50176]	Loss: 2.4985
Profiling... [3072/50176]	Loss: 2.6640
Profiling... [3328/50176]	Loss: 2.6546
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 6, Average loss: 0.0311, Accuracy: 0.2783
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.47589851044057,
                        "time": 2.4934844210001756,
                        "accuracy": 0.2783203125,
                        "total_cost": 784424.8235746815
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1618
Profiling... [512/50176]	Loss: 2.4498
Profiling... [768/50176]	Loss: 2.6002
Profiling... [1024/50176]	Loss: 2.6887
Profiling... [1280/50176]	Loss: 2.5017
Profiling... [1536/50176]	Loss: 2.5563
Profiling... [1792/50176]	Loss: 2.5941
Profiling... [2048/50176]	Loss: 2.7166
Profiling... [2304/50176]	Loss: 2.4703
Profiling... [2560/50176]	Loss: 2.5441
Profiling... [2816/50176]	Loss: 2.4845
Profiling... [3072/50176]	Loss: 2.5539
Profiling... [3328/50176]	Loss: 2.6223
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 6, Average loss: 0.0141, Accuracy: 0.2687
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.48702422867477,
                        "time": 2.5450453819939867,
                        "accuracy": 0.26865234375,
                        "total_cost": 829458.1860983245
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.2446
Profiling... [512/50176]	Loss: 2.4755
Profiling... [768/50176]	Loss: 2.5657
Profiling... [1024/50176]	Loss: 2.6298
Profiling... [1280/50176]	Loss: 2.5884
Profiling... [1536/50176]	Loss: 2.6157
Profiling... [1792/50176]	Loss: 2.6974
Profiling... [2048/50176]	Loss: 2.5062
Profiling... [2304/50176]	Loss: 2.4211
Profiling... [2560/50176]	Loss: 2.6077
Profiling... [2816/50176]	Loss: 2.3391
Profiling... [3072/50176]	Loss: 2.4247
Profiling... [3328/50176]	Loss: 2.8049
Profile done
epoch 1 train time consumed: 4.46s
Validation Epoch: 6, Average loss: 0.0244, Accuracy: 0.2401
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.46887308240855,
                        "time": 3.028427368000848,
                        "accuracy": 0.24013671875,
                        "total_cost": 1104201.0243192087
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9911
Profiling... [512/50176]	Loss: 2.5257
Profiling... [768/50176]	Loss: 2.3368
Profiling... [1024/50176]	Loss: 2.6249
Profiling... [1280/50176]	Loss: 2.7880
Profiling... [1536/50176]	Loss: 2.6226
Profiling... [1792/50176]	Loss: 2.4818
Profiling... [2048/50176]	Loss: 2.4214
Profiling... [2304/50176]	Loss: 2.5903
Profiling... [2560/50176]	Loss: 2.6503
Profiling... [2816/50176]	Loss: 2.5774
Profiling... [3072/50176]	Loss: 2.5230
Profiling... [3328/50176]	Loss: 2.6930
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 6, Average loss: 0.0160, Accuracy: 0.2540
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.45149301214126,
                        "time": 2.39138779300265,
                        "accuracy": 0.25400390625,
                        "total_cost": 824326.2406342439
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.2733
Profiling... [512/50176]	Loss: 2.2547
Profiling... [768/50176]	Loss: 2.6615
Profiling... [1024/50176]	Loss: 2.5646
Profiling... [1280/50176]	Loss: 2.7308
Profiling... [1536/50176]	Loss: 2.4442
Profiling... [1792/50176]	Loss: 2.5607
Profiling... [2048/50176]	Loss: 2.5311
Profiling... [2304/50176]	Loss: 2.6628
Profiling... [2560/50176]	Loss: 2.4948
Profiling... [2816/50176]	Loss: 2.5699
Profiling... [3072/50176]	Loss: 2.5564
Profiling... [3328/50176]	Loss: 2.6367
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 6, Average loss: 0.0142, Accuracy: 0.2466
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.47168133120097,
                        "time": 2.377612356998725,
                        "accuracy": 0.24658203125,
                        "total_cost": 844246.3387049403
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1187
Profiling... [512/50176]	Loss: 2.4373
Profiling... [768/50176]	Loss: 2.5057
Profiling... [1024/50176]	Loss: 2.6325
Profiling... [1280/50176]	Loss: 2.6504
Profiling... [1536/50176]	Loss: 2.6280
Profiling... [1792/50176]	Loss: 2.5987
Profiling... [2048/50176]	Loss: 2.6206
Profiling... [2304/50176]	Loss: 2.6134
Profiling... [2560/50176]	Loss: 2.6919
Profiling... [2816/50176]	Loss: 2.6320
Profiling... [3072/50176]	Loss: 2.6111
Profiling... [3328/50176]	Loss: 2.5675
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 6, Average loss: 0.0114, Accuracy: 0.2880
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.48454001099915,
                        "time": 2.5439312050002627,
                        "accuracy": 0.28798828125,
                        "total_cost": 773428.44612304
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.2606
Profiling... [512/50176]	Loss: 2.5158
Profiling... [768/50176]	Loss: 2.3712
Profiling... [1024/50176]	Loss: 2.6119
Profiling... [1280/50176]	Loss: 2.5912
Profiling... [1536/50176]	Loss: 2.4915
Profiling... [1792/50176]	Loss: 2.6049
Profiling... [2048/50176]	Loss: 2.5133
Profiling... [2304/50176]	Loss: 2.7186
Profiling... [2560/50176]	Loss: 2.6005
Profiling... [2816/50176]	Loss: 2.4852
Profiling... [3072/50176]	Loss: 2.8725
Profiling... [3328/50176]	Loss: 2.5283
Profile done
epoch 1 train time consumed: 4.58s
Validation Epoch: 6, Average loss: 0.0129, Accuracy: 0.2444
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.46618794917484,
                        "time": 3.040624547000334,
                        "accuracy": 0.24443359375,
                        "total_cost": 1089159.422877796
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1092
Profiling... [1024/50176]	Loss: 2.0756
Profiling... [1536/50176]	Loss: 2.2367
Profiling... [2048/50176]	Loss: 2.1865
Profiling... [2560/50176]	Loss: 2.3030
Profiling... [3072/50176]	Loss: 2.0391
Profiling... [3584/50176]	Loss: 2.1502
Profiling... [4096/50176]	Loss: 2.1053
Profiling... [4608/50176]	Loss: 2.1181
Profiling... [5120/50176]	Loss: 2.2342
Profiling... [5632/50176]	Loss: 2.0828
Profiling... [6144/50176]	Loss: 2.1328
Profiling... [6656/50176]	Loss: 2.2015
Profile done
epoch 1 train time consumed: 7.06s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4202
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.46547651021056,
                        "time": 4.8092872819979675,
                        "accuracy": 0.42021484375,
                        "total_cost": 1002071.886498858
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.2304
Profiling... [1024/50176]	Loss: 2.0544
Profiling... [1536/50176]	Loss: 2.1543
Profiling... [2048/50176]	Loss: 2.1021
Profiling... [2560/50176]	Loss: 2.0063
Profiling... [3072/50176]	Loss: 2.0834
Profiling... [3584/50176]	Loss: 2.0413
Profiling... [4096/50176]	Loss: 2.1242
Profiling... [4608/50176]	Loss: 2.1399
Profiling... [5120/50176]	Loss: 2.1965
Profiling... [5632/50176]	Loss: 2.0910
Profiling... [6144/50176]	Loss: 2.1314
Profiling... [6656/50176]	Loss: 2.1879
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4214
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.50418610064568,
                        "time": 4.607756088000315,
                        "accuracy": 0.42138671875,
                        "total_cost": 957410.70980844
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1185
Profiling... [1024/50176]	Loss: 2.1372
Profiling... [1536/50176]	Loss: 2.1176
Profiling... [2048/50176]	Loss: 2.0989
Profiling... [2560/50176]	Loss: 2.0775
Profiling... [3072/50176]	Loss: 2.1419
Profiling... [3584/50176]	Loss: 2.0716
Profiling... [4096/50176]	Loss: 2.0335
Profiling... [4608/50176]	Loss: 2.0546
Profiling... [5120/50176]	Loss: 2.0631
Profiling... [5632/50176]	Loss: 2.1788
Profiling... [6144/50176]	Loss: 2.0837
Profiling... [6656/50176]	Loss: 2.1848
Profile done
epoch 1 train time consumed: 7.19s
Validation Epoch: 6, Average loss: 0.0043, Accuracy: 0.4169
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.52035679257534,
                        "time": 4.844478349994461,
                        "accuracy": 0.41689453125,
                        "total_cost": 1017443.9751178626
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0527
Profiling... [1024/50176]	Loss: 2.1382
Profiling... [1536/50176]	Loss: 2.1681
Profiling... [2048/50176]	Loss: 2.1103
Profiling... [2560/50176]	Loss: 2.1899
Profiling... [3072/50176]	Loss: 1.9844
Profiling... [3584/50176]	Loss: 2.1560
Profiling... [4096/50176]	Loss: 2.1130
Profiling... [4608/50176]	Loss: 2.2427
Profiling... [5120/50176]	Loss: 2.1722
Profiling... [5632/50176]	Loss: 2.0674
Profiling... [6144/50176]	Loss: 2.2021
Profiling... [6656/50176]	Loss: 2.0660
Profile done
epoch 1 train time consumed: 8.63s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4223
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.48127572155339,
                        "time": 6.051487915996404,
                        "accuracy": 0.422265625,
                        "total_cost": 1254775.4933498215
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1257
Profiling... [1024/50176]	Loss: 2.2159
Profiling... [1536/50176]	Loss: 2.0785
Profiling... [2048/50176]	Loss: 2.1533
Profiling... [2560/50176]	Loss: 2.2359
Profiling... [3072/50176]	Loss: 2.0588
Profiling... [3584/50176]	Loss: 2.1131
Profiling... [4096/50176]	Loss: 2.1453
Profiling... [4608/50176]	Loss: 2.2273
Profiling... [5120/50176]	Loss: 2.1104
Profiling... [5632/50176]	Loss: 2.0688
Profiling... [6144/50176]	Loss: 2.1199
Profiling... [6656/50176]	Loss: 2.1772
Profile done
epoch 1 train time consumed: 6.77s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4206
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.48537184630963,
                        "time": 4.504345646004367,
                        "accuracy": 0.42060546875,
                        "total_cost": 937662.1608552557
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.2389
Profiling... [1024/50176]	Loss: 2.1519
Profiling... [1536/50176]	Loss: 2.1235
Profiling... [2048/50176]	Loss: 2.1862
Profiling... [2560/50176]	Loss: 2.0883
Profiling... [3072/50176]	Loss: 2.1281
Profiling... [3584/50176]	Loss: 2.2474
Profiling... [4096/50176]	Loss: 2.1502
Profiling... [4608/50176]	Loss: 2.0268
Profiling... [5120/50176]	Loss: 2.2257
Profiling... [5632/50176]	Loss: 2.1056
Profiling... [6144/50176]	Loss: 2.0926
Profiling... [6656/50176]	Loss: 2.1765
Profile done
epoch 1 train time consumed: 6.69s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4242
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.52841239743506,
                        "time": 4.509687683996162,
                        "accuracy": 0.42421875,
                        "total_cost": 930778.4278541446
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1648
Profiling... [1024/50176]	Loss: 2.2373
Profiling... [1536/50176]	Loss: 2.1475
Profiling... [2048/50176]	Loss: 2.1864
Profiling... [2560/50176]	Loss: 2.0614
Profiling... [3072/50176]	Loss: 2.1753
Profiling... [3584/50176]	Loss: 2.0708
Profiling... [4096/50176]	Loss: 2.0946
Profiling... [4608/50176]	Loss: 2.2884
Profiling... [5120/50176]	Loss: 1.9566
Profiling... [5632/50176]	Loss: 2.1832
Profiling... [6144/50176]	Loss: 2.2010
Profiling... [6656/50176]	Loss: 2.1173
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4250
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.54299434461608,
                        "time": 4.873114169000473,
                        "accuracy": 0.425,
                        "total_cost": 1003939.1618230757
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1403
Profiling... [1024/50176]	Loss: 2.3076
Profiling... [1536/50176]	Loss: 2.1992
Profiling... [2048/50176]	Loss: 2.1349
Profiling... [2560/50176]	Loss: 2.2197
Profiling... [3072/50176]	Loss: 2.2223
Profiling... [3584/50176]	Loss: 2.0614
Profiling... [4096/50176]	Loss: 2.1140
Profiling... [4608/50176]	Loss: 2.0937
Profiling... [5120/50176]	Loss: 2.0066
Profiling... [5632/50176]	Loss: 2.0807
Profiling... [6144/50176]	Loss: 2.0835
Profiling... [6656/50176]	Loss: 2.2503
Profile done
epoch 1 train time consumed: 8.62s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4255
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.50460208473156,
                        "time": 6.013383311998041,
                        "accuracy": 0.42548828125,
                        "total_cost": 1237430.8208748915
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.2176
Profiling... [1024/50176]	Loss: 2.1544
Profiling... [1536/50176]	Loss: 2.1383
Profiling... [2048/50176]	Loss: 2.1957
Profiling... [2560/50176]	Loss: 2.1891
Profiling... [3072/50176]	Loss: 2.0470
Profiling... [3584/50176]	Loss: 2.2395
Profiling... [4096/50176]	Loss: 2.1467
Profiling... [4608/50176]	Loss: 2.1377
Profiling... [5120/50176]	Loss: 2.1009
Profiling... [5632/50176]	Loss: 2.2020
Profiling... [6144/50176]	Loss: 2.2520
Profiling... [6656/50176]	Loss: 2.0458
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4244
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.50511000389542,
                        "time": 4.50181505799992,
                        "accuracy": 0.4244140625,
                        "total_cost": 928725.840891949
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0261
Profiling... [1024/50176]	Loss: 2.0101
Profiling... [1536/50176]	Loss: 2.0604
Profiling... [2048/50176]	Loss: 2.1700
Profiling... [2560/50176]	Loss: 2.1966
Profiling... [3072/50176]	Loss: 2.1485
Profiling... [3584/50176]	Loss: 2.1913
Profiling... [4096/50176]	Loss: 2.2190
Profiling... [4608/50176]	Loss: 2.2342
Profiling... [5120/50176]	Loss: 2.0651
Profiling... [5632/50176]	Loss: 2.2192
Profiling... [6144/50176]	Loss: 2.1835
Profiling... [6656/50176]	Loss: 2.1925
Profile done
epoch 1 train time consumed: 6.81s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4208
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.54427751462534,
                        "time": 4.6129205969991744,
                        "accuracy": 0.42080078125,
                        "total_cost": 959818.6495896319
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.2168
Profiling... [1024/50176]	Loss: 2.1298
Profiling... [1536/50176]	Loss: 2.1245
Profiling... [2048/50176]	Loss: 2.2432
Profiling... [2560/50176]	Loss: 1.9748
Profiling... [3072/50176]	Loss: 2.1746
Profiling... [3584/50176]	Loss: 2.1281
Profiling... [4096/50176]	Loss: 2.1601
Profiling... [4608/50176]	Loss: 2.1858
Profiling... [5120/50176]	Loss: 2.1469
Profiling... [5632/50176]	Loss: 2.0657
Profiling... [6144/50176]	Loss: 2.0487
Profiling... [6656/50176]	Loss: 2.1115
Profile done
epoch 1 train time consumed: 7.14s
Validation Epoch: 6, Average loss: 0.0043, Accuracy: 0.4218
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.55858270144368,
                        "time": 4.875705310994817,
                        "accuracy": 0.42177734375,
                        "total_cost": 1012147.9025138835
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1777
Profiling... [1024/50176]	Loss: 2.1310
Profiling... [1536/50176]	Loss: 2.1057
Profiling... [2048/50176]	Loss: 2.0915
Profiling... [2560/50176]	Loss: 2.2404
Profiling... [3072/50176]	Loss: 2.1705
Profiling... [3584/50176]	Loss: 2.2597
Profiling... [4096/50176]	Loss: 2.0947
Profiling... [4608/50176]	Loss: 2.2355
Profiling... [5120/50176]	Loss: 2.1949
Profiling... [5632/50176]	Loss: 2.1238
Profiling... [6144/50176]	Loss: 2.2210
Profiling... [6656/50176]	Loss: 2.1530
Profile done
epoch 1 train time consumed: 8.63s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4230
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.52329242080462,
                        "time": 6.081290044996422,
                        "accuracy": 0.423046875,
                        "total_cost": 1258626.6308461062
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1652
Profiling... [1024/50176]	Loss: 2.1685
Profiling... [1536/50176]	Loss: 2.2820
Profiling... [2048/50176]	Loss: 2.3243
Profiling... [2560/50176]	Loss: 2.2303
Profiling... [3072/50176]	Loss: 2.2802
Profiling... [3584/50176]	Loss: 2.1686
Profiling... [4096/50176]	Loss: 2.4106
Profiling... [4608/50176]	Loss: 2.2708
Profiling... [5120/50176]	Loss: 2.3888
Profiling... [5632/50176]	Loss: 2.2975
Profiling... [6144/50176]	Loss: 2.3417
Profiling... [6656/50176]	Loss: 2.2830
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 6, Average loss: 0.0048, Accuracy: 0.3666
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.5248439341204,
                        "time": 4.5975307839980815,
                        "accuracy": 0.3666015625,
                        "total_cost": 1098044.7214602365
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.2749
Profiling... [1024/50176]	Loss: 2.2517
Profiling... [1536/50176]	Loss: 2.2356
Profiling... [2048/50176]	Loss: 2.3663
Profiling... [2560/50176]	Loss: 2.2849
Profiling... [3072/50176]	Loss: 2.2285
Profiling... [3584/50176]	Loss: 2.3305
Profiling... [4096/50176]	Loss: 2.4770
Profiling... [4608/50176]	Loss: 2.2164
Profiling... [5120/50176]	Loss: 2.3211
Profiling... [5632/50176]	Loss: 2.3724
Profiling... [6144/50176]	Loss: 2.2273
Profiling... [6656/50176]	Loss: 2.2898
Profile done
epoch 1 train time consumed: 6.77s
Validation Epoch: 6, Average loss: 0.0046, Accuracy: 0.3865
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.56158826317684,
                        "time": 4.508968658999947,
                        "accuracy": 0.3865234375,
                        "total_cost": 1021388.9823528951
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0504
Profiling... [1024/50176]	Loss: 2.2884
Profiling... [1536/50176]	Loss: 2.2405
Profiling... [2048/50176]	Loss: 2.2656
Profiling... [2560/50176]	Loss: 2.1787
Profiling... [3072/50176]	Loss: 2.3352
Profiling... [3584/50176]	Loss: 2.2830
Profiling... [4096/50176]	Loss: 2.3732
Profiling... [4608/50176]	Loss: 2.2174
Profiling... [5120/50176]	Loss: 2.3140
Profiling... [5632/50176]	Loss: 2.2047
Profiling... [6144/50176]	Loss: 2.3088
Profiling... [6656/50176]	Loss: 2.1890
Profile done
epoch 1 train time consumed: 7.19s
Validation Epoch: 6, Average loss: 0.0049, Accuracy: 0.3583
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.57507132338496,
                        "time": 4.866067083996313,
                        "accuracy": 0.35830078125,
                        "total_cost": 1189104.8641349901
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1460
Profiling... [1024/50176]	Loss: 2.2802
Profiling... [1536/50176]	Loss: 2.2768
Profiling... [2048/50176]	Loss: 2.2808
Profiling... [2560/50176]	Loss: 2.1184
Profiling... [3072/50176]	Loss: 2.3175
Profiling... [3584/50176]	Loss: 2.3457
Profiling... [4096/50176]	Loss: 2.3444
Profiling... [4608/50176]	Loss: 2.3701
Profiling... [5120/50176]	Loss: 2.2433
Profiling... [5632/50176]	Loss: 2.2053
Profiling... [6144/50176]	Loss: 2.3049
Profiling... [6656/50176]	Loss: 2.2036
Profile done
epoch 1 train time consumed: 8.66s
Validation Epoch: 6, Average loss: 0.0047, Accuracy: 0.3712
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.53645543169553,
                        "time": 6.045970166000188,
                        "accuracy": 0.37119140625,
                        "total_cost": 1426125.7120258645
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1180
Profiling... [1024/50176]	Loss: 2.3397
Profiling... [1536/50176]	Loss: 2.3197
Profiling... [2048/50176]	Loss: 2.3726
Profiling... [2560/50176]	Loss: 2.2766
Profiling... [3072/50176]	Loss: 2.2157
Profiling... [3584/50176]	Loss: 2.3375
Profiling... [4096/50176]	Loss: 2.3850
Profiling... [4608/50176]	Loss: 2.2641
Profiling... [5120/50176]	Loss: 2.2106
Profiling... [5632/50176]	Loss: 2.2855
Profiling... [6144/50176]	Loss: 2.3670
Profiling... [6656/50176]	Loss: 2.2640
Profile done
epoch 1 train time consumed: 6.69s
Validation Epoch: 6, Average loss: 0.0049, Accuracy: 0.3594
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.54112505315808,
                        "time": 4.495747856002708,
                        "accuracy": 0.359375,
                        "total_cost": 1095327.063749565
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0917
Profiling... [1024/50176]	Loss: 2.1108
Profiling... [1536/50176]	Loss: 2.2012
Profiling... [2048/50176]	Loss: 2.3657
Profiling... [2560/50176]	Loss: 2.2801
Profiling... [3072/50176]	Loss: 2.2736
Profiling... [3584/50176]	Loss: 2.2488
Profiling... [4096/50176]	Loss: 2.3273
Profiling... [4608/50176]	Loss: 2.2332
Profiling... [5120/50176]	Loss: 2.3776
Profiling... [5632/50176]	Loss: 2.2601
Profiling... [6144/50176]	Loss: 2.3063
Profiling... [6656/50176]	Loss: 2.2312
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 6, Average loss: 0.0046, Accuracy: 0.3803
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.57882508278745,
                        "time": 4.613680300004489,
                        "accuracy": 0.3802734375,
                        "total_cost": 1062285.6992063345
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.2172
Profiling... [1024/50176]	Loss: 2.3449
Profiling... [1536/50176]	Loss: 2.2070
Profiling... [2048/50176]	Loss: 2.2640
Profiling... [2560/50176]	Loss: 2.2852
Profiling... [3072/50176]	Loss: 2.3358
Profiling... [3584/50176]	Loss: 2.2348
Profiling... [4096/50176]	Loss: 2.2306
Profiling... [4608/50176]	Loss: 2.3114
Profiling... [5120/50176]	Loss: 2.3344
Profiling... [5632/50176]	Loss: 2.2647
Profiling... [6144/50176]	Loss: 2.3015
Profiling... [6656/50176]	Loss: 2.3979
Profile done
epoch 1 train time consumed: 7.13s
Validation Epoch: 6, Average loss: 0.0045, Accuracy: 0.3889
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.5908849151636,
                        "time": 4.870893274004629,
                        "accuracy": 0.3888671875,
                        "total_cost": 1096723.5594133688
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1314
Profiling... [1024/50176]	Loss: 2.2220
Profiling... [1536/50176]	Loss: 2.2269
Profiling... [2048/50176]	Loss: 2.1448
Profiling... [2560/50176]	Loss: 2.1996
Profiling... [3072/50176]	Loss: 2.2917
Profiling... [3584/50176]	Loss: 2.2633
Profiling... [4096/50176]	Loss: 2.3137
Profiling... [4608/50176]	Loss: 2.3152
Profiling... [5120/50176]	Loss: 2.4171
Profiling... [5632/50176]	Loss: 2.3100
Profiling... [6144/50176]	Loss: 2.3767
Profiling... [6656/50176]	Loss: 2.1886
Profile done
epoch 1 train time consumed: 8.65s
Validation Epoch: 6, Average loss: 0.0048, Accuracy: 0.3686
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.55488839266903,
                        "time": 6.035376809006266,
                        "accuracy": 0.3685546875,
                        "total_cost": 1433812.028392735
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.3056
Profiling... [1024/50176]	Loss: 2.2772
Profiling... [1536/50176]	Loss: 2.1102
Profiling... [2048/50176]	Loss: 2.2703
Profiling... [2560/50176]	Loss: 2.2063
Profiling... [3072/50176]	Loss: 2.1373
Profiling... [3584/50176]	Loss: 2.3353
Profiling... [4096/50176]	Loss: 2.3691
Profiling... [4608/50176]	Loss: 2.2399
Profiling... [5120/50176]	Loss: 2.2788
Profiling... [5632/50176]	Loss: 2.3362
Profiling... [6144/50176]	Loss: 2.1776
Profiling... [6656/50176]	Loss: 2.3231
Profile done
epoch 1 train time consumed: 6.66s
Validation Epoch: 6, Average loss: 0.0046, Accuracy: 0.3807
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.55879403653522,
                        "time": 4.498158729998977,
                        "accuracy": 0.3806640625,
                        "total_cost": 1034624.3063469966
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.2761
Profiling... [1024/50176]	Loss: 2.2638
Profiling... [1536/50176]	Loss: 2.3486
Profiling... [2048/50176]	Loss: 2.3044
Profiling... [2560/50176]	Loss: 2.2538
Profiling... [3072/50176]	Loss: 2.2917
Profiling... [3584/50176]	Loss: 2.3290
Profiling... [4096/50176]	Loss: 2.2894
Profiling... [4608/50176]	Loss: 2.2337
Profiling... [5120/50176]	Loss: 2.3767
Profiling... [5632/50176]	Loss: 2.2157
Profiling... [6144/50176]	Loss: 2.1519
Profiling... [6656/50176]	Loss: 2.2730
Profile done
epoch 1 train time consumed: 6.75s
Validation Epoch: 6, Average loss: 0.0048, Accuracy: 0.3749
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.59804539823547,
                        "time": 4.513455956002872,
                        "accuracy": 0.37490234375,
                        "total_cost": 1054097.8540829616
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0542
Profiling... [1024/50176]	Loss: 2.2712
Profiling... [1536/50176]	Loss: 2.2451
Profiling... [2048/50176]	Loss: 2.3700
Profiling... [2560/50176]	Loss: 2.3502
Profiling... [3072/50176]	Loss: 2.3631
Profiling... [3584/50176]	Loss: 2.3067
Profiling... [4096/50176]	Loss: 2.1966
Profiling... [4608/50176]	Loss: 2.2502
Profiling... [5120/50176]	Loss: 2.1733
Profiling... [5632/50176]	Loss: 2.2471
Profiling... [6144/50176]	Loss: 2.2523
Profiling... [6656/50176]	Loss: 2.1943
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 6, Average loss: 0.0048, Accuracy: 0.3690
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.61369566861464,
                        "time": 4.85153943199839,
                        "accuracy": 0.36904296875,
                        "total_cost": 1151045.6421943002
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.2004
Profiling... [1024/50176]	Loss: 2.1838
Profiling... [1536/50176]	Loss: 2.2999
Profiling... [2048/50176]	Loss: 2.3102
Profiling... [2560/50176]	Loss: 2.4335
Profiling... [3072/50176]	Loss: 2.2379
Profiling... [3584/50176]	Loss: 2.2736
Profiling... [4096/50176]	Loss: 2.3369
Profiling... [4608/50176]	Loss: 2.2214
Profiling... [5120/50176]	Loss: 2.2396
Profiling... [5632/50176]	Loss: 2.3549
Profiling... [6144/50176]	Loss: 2.1896
Profiling... [6656/50176]	Loss: 2.2363
Profile done
epoch 1 train time consumed: 8.63s
Validation Epoch: 6, Average loss: 0.0047, Accuracy: 0.3755
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.57650999409753,
                        "time": 6.018096779996995,
                        "accuracy": 0.37548828125,
                        "total_cost": 1403306.7122897767
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1751
Profiling... [1024/50176]	Loss: 2.6677
Profiling... [1536/50176]	Loss: 2.3707
Profiling... [2048/50176]	Loss: 2.6302
Profiling... [2560/50176]	Loss: 2.3653
Profiling... [3072/50176]	Loss: 2.4529
Profiling... [3584/50176]	Loss: 2.4434
Profiling... [4096/50176]	Loss: 2.5282
Profiling... [4608/50176]	Loss: 2.4433
Profiling... [5120/50176]	Loss: 2.4207
Profiling... [5632/50176]	Loss: 2.3575
Profiling... [6144/50176]	Loss: 2.3413
Profiling... [6656/50176]	Loss: 2.4401
Profile done
epoch 1 train time consumed: 6.71s
Validation Epoch: 6, Average loss: 0.0062, Accuracy: 0.2702
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.58003486325045,
                        "time": 4.515109701998881,
                        "accuracy": 0.27021484375,
                        "total_cost": 1463015.5271904243
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.1046
Profiling... [1024/50176]	Loss: 2.3150
Profiling... [1536/50176]	Loss: 2.5213
Profiling... [2048/50176]	Loss: 2.6193
Profiling... [2560/50176]	Loss: 2.4616
Profiling... [3072/50176]	Loss: 2.5133
Profiling... [3584/50176]	Loss: 2.4823
Profiling... [4096/50176]	Loss: 2.5570
Profiling... [4608/50176]	Loss: 2.4483
Profiling... [5120/50176]	Loss: 2.4366
Profiling... [5632/50176]	Loss: 2.4766
Profiling... [6144/50176]	Loss: 2.5473
Profiling... [6656/50176]	Loss: 2.4952
Profile done
epoch 1 train time consumed: 6.71s
Validation Epoch: 6, Average loss: 0.0054, Accuracy: 0.3247
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.61754613655982,
                        "time": 4.510678371996619,
                        "accuracy": 0.32470703125,
                        "total_cost": 1216298.280124548
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1370
Profiling... [1024/50176]	Loss: 2.5927
Profiling... [1536/50176]	Loss: 2.4431
Profiling... [2048/50176]	Loss: 2.3926
Profiling... [2560/50176]	Loss: 2.5456
Profiling... [3072/50176]	Loss: 2.4003
Profiling... [3584/50176]	Loss: 2.3693
Profiling... [4096/50176]	Loss: 2.4505
Profiling... [4608/50176]	Loss: 2.5301
Profiling... [5120/50176]	Loss: 2.3552
Profiling... [5632/50176]	Loss: 2.5115
Profiling... [6144/50176]	Loss: 2.4963
Profiling... [6656/50176]	Loss: 2.4794
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 6, Average loss: 0.0061, Accuracy: 0.2882
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.63352540146361,
                        "time": 4.855129480994947,
                        "accuracy": 0.28818359375,
                        "total_cost": 1475100.2192561869
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.2958
Profiling... [1024/50176]	Loss: 2.5719
Profiling... [1536/50176]	Loss: 2.4217
Profiling... [2048/50176]	Loss: 2.4281
Profiling... [2560/50176]	Loss: 2.5233
Profiling... [3072/50176]	Loss: 2.4775
Profiling... [3584/50176]	Loss: 2.3299
Profiling... [4096/50176]	Loss: 2.5259
Profiling... [4608/50176]	Loss: 2.4411
Profiling... [5120/50176]	Loss: 2.4620
Profiling... [5632/50176]	Loss: 2.5607
Profiling... [6144/50176]	Loss: 2.4637
Profiling... [6656/50176]	Loss: 2.4584
Profile done
epoch 1 train time consumed: 8.59s
Validation Epoch: 6, Average loss: 0.0067, Accuracy: 0.2613
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.59668433388252,
                        "time": 6.027826065997942,
                        "accuracy": 0.261328125,
                        "total_cost": 2019595.6761341293
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1749
Profiling... [1024/50176]	Loss: 2.5806
Profiling... [1536/50176]	Loss: 2.5756
Profiling... [2048/50176]	Loss: 2.3727
Profiling... [2560/50176]	Loss: 2.4433
Profiling... [3072/50176]	Loss: 2.4383
Profiling... [3584/50176]	Loss: 2.5137
Profiling... [4096/50176]	Loss: 2.4206
Profiling... [4608/50176]	Loss: 2.3332
Profiling... [5120/50176]	Loss: 2.3060
Profiling... [5632/50176]	Loss: 2.4449
Profiling... [6144/50176]	Loss: 2.3710
Profiling... [6656/50176]	Loss: 2.5373
Profile done
epoch 1 train time consumed: 6.81s
Validation Epoch: 6, Average loss: 0.0057, Accuracy: 0.3057
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.59892363676266,
                        "time": 4.503197607002221,
                        "accuracy": 0.3056640625,
                        "total_cost": 1289931.06545927
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.1130
Profiling... [1024/50176]	Loss: 2.4328
Profiling... [1536/50176]	Loss: 2.3738
Profiling... [2048/50176]	Loss: 2.5515
Profiling... [2560/50176]	Loss: 2.4225
Profiling... [3072/50176]	Loss: 2.6724
Profiling... [3584/50176]	Loss: 2.5074
Profiling... [4096/50176]	Loss: 2.5171
Profiling... [4608/50176]	Loss: 2.5312
Profiling... [5120/50176]	Loss: 2.5650
Profiling... [5632/50176]	Loss: 2.4834
Profiling... [6144/50176]	Loss: 2.4633
Profiling... [6656/50176]	Loss: 2.4846
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 6, Average loss: 0.0060, Accuracy: 0.2938
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.63635888302818,
                        "time": 4.512488387998019,
                        "accuracy": 0.29384765625,
                        "total_cost": 1344571.3005403075
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.2739
Profiling... [1024/50176]	Loss: 2.4893
Profiling... [1536/50176]	Loss: 2.5579
Profiling... [2048/50176]	Loss: 2.5184
Profiling... [2560/50176]	Loss: 2.6686
Profiling... [3072/50176]	Loss: 2.5411
Profiling... [3584/50176]	Loss: 2.5578
Profiling... [4096/50176]	Loss: 2.4344
Profiling... [4608/50176]	Loss: 2.4181
Profiling... [5120/50176]	Loss: 2.4279
Profiling... [5632/50176]	Loss: 2.3863
Profiling... [6144/50176]	Loss: 2.1924
Profiling... [6656/50176]	Loss: 2.5187
Profile done
epoch 1 train time consumed: 7.10s
Validation Epoch: 6, Average loss: 0.0059, Accuracy: 0.3032
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.65075656915445,
                        "time": 4.867847435998556,
                        "accuracy": 0.30322265625,
                        "total_cost": 1405611.550247924
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1398
Profiling... [1024/50176]	Loss: 2.3250
Profiling... [1536/50176]	Loss: 2.5719
Profiling... [2048/50176]	Loss: 2.4876
Profiling... [2560/50176]	Loss: 2.5174
Profiling... [3072/50176]	Loss: 2.4256
Profiling... [3584/50176]	Loss: 2.5406
Profiling... [4096/50176]	Loss: 2.6458
Profiling... [4608/50176]	Loss: 2.4258
Profiling... [5120/50176]	Loss: 2.3519
Profiling... [5632/50176]	Loss: 2.4392
Profiling... [6144/50176]	Loss: 2.4507
Profiling... [6656/50176]	Loss: 2.4344
Profile done
epoch 1 train time consumed: 8.58s
Validation Epoch: 6, Average loss: 0.0073, Accuracy: 0.2788
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.61674562428773,
                        "time": 6.034913883006084,
                        "accuracy": 0.27880859375,
                        "total_cost": 1895199.1088000217
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.2539
Profiling... [1024/50176]	Loss: 2.4285
Profiling... [1536/50176]	Loss: 2.3991
Profiling... [2048/50176]	Loss: 2.4207
Profiling... [2560/50176]	Loss: 2.5123
Profiling... [3072/50176]	Loss: 2.4377
Profiling... [3584/50176]	Loss: 2.5578
Profiling... [4096/50176]	Loss: 2.4822
Profiling... [4608/50176]	Loss: 2.4478
Profiling... [5120/50176]	Loss: 2.4769
Profiling... [5632/50176]	Loss: 2.4805
Profiling... [6144/50176]	Loss: 2.4690
Profiling... [6656/50176]	Loss: 2.4688
Profile done
epoch 1 train time consumed: 6.82s
Validation Epoch: 6, Average loss: 0.0058, Accuracy: 0.2743
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.61883448049974,
                        "time": 4.6074482389958575,
                        "accuracy": 0.27431640625,
                        "total_cost": 1470613.7080036916
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.1377
Profiling... [1024/50176]	Loss: 2.4520
Profiling... [1536/50176]	Loss: 2.3238
Profiling... [2048/50176]	Loss: 2.4700
Profiling... [2560/50176]	Loss: 2.6997
Profiling... [3072/50176]	Loss: 2.4939
Profiling... [3584/50176]	Loss: 2.3368
Profiling... [4096/50176]	Loss: 2.4104
Profiling... [4608/50176]	Loss: 2.5706
Profiling... [5120/50176]	Loss: 2.4183
Profiling... [5632/50176]	Loss: 2.5139
Profiling... [6144/50176]	Loss: 2.3535
Profiling... [6656/50176]	Loss: 2.5512
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 6, Average loss: 0.0069, Accuracy: 0.2811
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.6564850684046,
                        "time": 4.619058194002719,
                        "accuracy": 0.2810546875,
                        "total_cost": 1438972.9220743428
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.2422
Profiling... [1024/50176]	Loss: 2.4995
Profiling... [1536/50176]	Loss: 2.4458
Profiling... [2048/50176]	Loss: 2.5219
Profiling... [2560/50176]	Loss: 2.5003
Profiling... [3072/50176]	Loss: 2.4894
Profiling... [3584/50176]	Loss: 2.6978
Profiling... [4096/50176]	Loss: 2.4773
Profiling... [4608/50176]	Loss: 2.5212
Profiling... [5120/50176]	Loss: 2.5817
Profiling... [5632/50176]	Loss: 2.5532
Profiling... [6144/50176]	Loss: 2.3813
Profiling... [6656/50176]	Loss: 2.4579
Profile done
epoch 1 train time consumed: 7.16s
Validation Epoch: 6, Average loss: 0.0051, Accuracy: 0.3229
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.67044066306082,
                        "time": 4.867295659001684,
                        "accuracy": 0.32294921875,
                        "total_cost": 1319603.76195784
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.2354
Profiling... [1024/50176]	Loss: 2.3817
Profiling... [1536/50176]	Loss: 2.3392
Profiling... [2048/50176]	Loss: 2.3617
Profiling... [2560/50176]	Loss: 2.4258
Profiling... [3072/50176]	Loss: 2.6965
Profiling... [3584/50176]	Loss: 2.4854
Profiling... [4096/50176]	Loss: 2.5683
Profiling... [4608/50176]	Loss: 2.5305
Profiling... [5120/50176]	Loss: 2.4588
Profiling... [5632/50176]	Loss: 2.5118
Profiling... [6144/50176]	Loss: 2.3855
Profiling... [6656/50176]	Loss: 2.4544
Profile done
epoch 1 train time consumed: 8.57s
Validation Epoch: 6, Average loss: 0.0064, Accuracy: 0.2735
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.63451642850671,
                        "time": 6.01246891199844,
                        "accuracy": 0.27353515625,
                        "total_cost": 1924552.035707628
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1365
Profiling... [2048/50176]	Loss: 2.0765
Profiling... [3072/50176]	Loss: 2.1848
Profiling... [4096/50176]	Loss: 2.1204
Profiling... [5120/50176]	Loss: 2.1686
Profiling... [6144/50176]	Loss: 2.1285
Profiling... [7168/50176]	Loss: 2.1119
Profiling... [8192/50176]	Loss: 2.1139
Profiling... [9216/50176]	Loss: 2.2344
Profiling... [10240/50176]	Loss: 2.0687
Profiling... [11264/50176]	Loss: 2.0937
Profiling... [12288/50176]	Loss: 2.0865
Profiling... [13312/50176]	Loss: 2.1514
Profile done
epoch 1 train time consumed: 12.90s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4279
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.66501599289074,
                        "time": 9.004533393999736,
                        "accuracy": 0.4279296875,
                        "total_cost": 1842378.4215510772
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1344
Profiling... [2048/50176]	Loss: 2.1817
Profiling... [3072/50176]	Loss: 2.2286
Profiling... [4096/50176]	Loss: 2.1430
Profiling... [5120/50176]	Loss: 2.0417
Profiling... [6144/50176]	Loss: 2.0693
Profiling... [7168/50176]	Loss: 2.0418
Profiling... [8192/50176]	Loss: 2.1406
Profiling... [9216/50176]	Loss: 2.1021
Profiling... [10240/50176]	Loss: 2.1212
Profiling... [11264/50176]	Loss: 2.0279
Profiling... [12288/50176]	Loss: 2.1384
Profiling... [13312/50176]	Loss: 2.1056
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4238
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.73114122487533,
                        "time": 8.915390658999968,
                        "accuracy": 0.423828125,
                        "total_cost": 1841792.9707698235
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.1891
Profiling... [2048/50176]	Loss: 2.0799
Profiling... [3072/50176]	Loss: 2.1378
Profiling... [4096/50176]	Loss: 2.2142
Profiling... [5120/50176]	Loss: 2.1254
Profiling... [6144/50176]	Loss: 2.1318
Profiling... [7168/50176]	Loss: 2.0797
Profiling... [8192/50176]	Loss: 2.0665
Profiling... [9216/50176]	Loss: 2.1327
Profiling... [10240/50176]	Loss: 2.2046
Profiling... [11264/50176]	Loss: 2.0812
Profiling... [12288/50176]	Loss: 2.1466
Profiling... [13312/50176]	Loss: 2.0723
Profile done
epoch 1 train time consumed: 13.68s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4230
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.74863999187296,
                        "time": 9.588413268000295,
                        "accuracy": 0.423046875,
                        "total_cost": 1984488.1147851131
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1922
Profiling... [2048/50176]	Loss: 2.1365
Profiling... [3072/50176]	Loss: 2.1874
Profiling... [4096/50176]	Loss: 2.0868
Profiling... [5120/50176]	Loss: 2.1541
Profiling... [6144/50176]	Loss: 2.0975
Profiling... [7168/50176]	Loss: 2.0975
Profiling... [8192/50176]	Loss: 2.1127
Profiling... [9216/50176]	Loss: 2.0978
Profiling... [10240/50176]	Loss: 2.1774
Profiling... [11264/50176]	Loss: 2.1357
Profiling... [12288/50176]	Loss: 2.2207
Profiling... [13312/50176]	Loss: 2.0609
Profile done
epoch 1 train time consumed: 23.28s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4285
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.6450483563411,
                        "time": 16.54040263799834,
                        "accuracy": 0.428515625,
                        "total_cost": 3379631.952169
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1820
Profiling... [2048/50176]	Loss: 2.1421
Profiling... [3072/50176]	Loss: 2.0980
Profiling... [4096/50176]	Loss: 2.1641
Profiling... [5120/50176]	Loss: 2.1863
Profiling... [6144/50176]	Loss: 2.2222
Profiling... [7168/50176]	Loss: 2.1628
Profiling... [8192/50176]	Loss: 2.1763
Profiling... [9216/50176]	Loss: 2.1912
Profiling... [10240/50176]	Loss: 2.1678
Profiling... [11264/50176]	Loss: 2.1568
Profiling... [12288/50176]	Loss: 2.2603
Profiling... [13312/50176]	Loss: 2.1358
Profile done
epoch 1 train time consumed: 12.86s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4235
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.68209651219973,
                        "time": 8.894331965006131,
                        "accuracy": 0.42353515625,
                        "total_cost": 1838713.0291275913
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1057
Profiling... [2048/50176]	Loss: 2.1451
Profiling... [3072/50176]	Loss: 2.2064
Profiling... [4096/50176]	Loss: 2.2738
Profiling... [5120/50176]	Loss: 2.2306
Profiling... [6144/50176]	Loss: 2.1786
Profiling... [7168/50176]	Loss: 2.0370
Profiling... [8192/50176]	Loss: 2.1596
Profiling... [9216/50176]	Loss: 2.2027
Profiling... [10240/50176]	Loss: 2.0505
Profiling... [11264/50176]	Loss: 2.0563
Profiling... [12288/50176]	Loss: 2.1180
Profiling... [13312/50176]	Loss: 2.1953
Profile done
epoch 1 train time consumed: 13.00s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4247
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.74309364213794,
                        "time": 8.924247798000579,
                        "accuracy": 0.42470703125,
                        "total_cost": 1839807.5864244064
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.1195
Profiling... [2048/50176]	Loss: 2.1570
Profiling... [3072/50176]	Loss: 2.1172
Profiling... [4096/50176]	Loss: 2.1689
Profiling... [5120/50176]	Loss: 2.0110
Profiling... [6144/50176]	Loss: 2.0795
Profiling... [7168/50176]	Loss: 2.1141
Profiling... [8192/50176]	Loss: 2.1281
Profiling... [9216/50176]	Loss: 2.1536
Profiling... [10240/50176]	Loss: 2.1914
Profiling... [11264/50176]	Loss: 2.1418
Profiling... [12288/50176]	Loss: 2.2138
Profiling... [13312/50176]	Loss: 2.2189
Profile done
epoch 1 train time consumed: 13.68s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4230
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.75980235135481,
                        "time": 9.593418925003789,
                        "accuracy": 0.423046875,
                        "total_cost": 1985524.248721191
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1833
Profiling... [2048/50176]	Loss: 2.1886
Profiling... [3072/50176]	Loss: 2.1634
Profiling... [4096/50176]	Loss: 2.1220
Profiling... [5120/50176]	Loss: 2.0968
Profiling... [6144/50176]	Loss: 2.0872
Profiling... [7168/50176]	Loss: 2.1245
Profiling... [8192/50176]	Loss: 2.1643
Profiling... [9216/50176]	Loss: 2.1638
Profiling... [10240/50176]	Loss: 2.1070
Profiling... [11264/50176]	Loss: 2.1173
Profiling... [12288/50176]	Loss: 2.0935
Profiling... [13312/50176]	Loss: 2.1816
Profile done
epoch 1 train time consumed: 19.42s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4259
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.67794143507373,
                        "time": 12.613054938999994,
                        "accuracy": 0.42587890625,
                        "total_cost": 2593129.65262138
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0223
Profiling... [2048/50176]	Loss: 2.1832
Profiling... [3072/50176]	Loss: 2.1443
Profiling... [4096/50176]	Loss: 2.1195
Profiling... [5120/50176]	Loss: 2.1098
Profiling... [6144/50176]	Loss: 2.0568
Profiling... [7168/50176]	Loss: 2.0992
Profiling... [8192/50176]	Loss: 2.1489
Profiling... [9216/50176]	Loss: 2.1748
Profiling... [10240/50176]	Loss: 2.1529
Profiling... [11264/50176]	Loss: 2.1607
Profiling... [12288/50176]	Loss: 2.1315
Profiling... [13312/50176]	Loss: 2.0898
Profile done
epoch 1 train time consumed: 12.72s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4276
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.71331709869898,
                        "time": 8.78980471700197,
                        "accuracy": 0.42763671875,
                        "total_cost": 1799676.3091644188
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1075
Profiling... [2048/50176]	Loss: 2.1438
Profiling... [3072/50176]	Loss: 2.1383
Profiling... [4096/50176]	Loss: 2.1514
Profiling... [5120/50176]	Loss: 2.1387
Profiling... [6144/50176]	Loss: 2.1679
Profiling... [7168/50176]	Loss: 2.1517
Profiling... [8192/50176]	Loss: 2.1100
Profiling... [9216/50176]	Loss: 2.0003
Profiling... [10240/50176]	Loss: 2.1539
Profiling... [11264/50176]	Loss: 2.0361
Profiling... [12288/50176]	Loss: 2.1205
Profiling... [13312/50176]	Loss: 2.1339
Profile done
epoch 1 train time consumed: 12.78s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4263
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.77876404305374,
                        "time": 8.813121482999122,
                        "accuracy": 0.42626953125,
                        "total_cost": 1810238.4670433132
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0725
Profiling... [2048/50176]	Loss: 2.1539
Profiling... [3072/50176]	Loss: 2.1172
Profiling... [4096/50176]	Loss: 2.2099
Profiling... [5120/50176]	Loss: 2.1386
Profiling... [6144/50176]	Loss: 2.1646
Profiling... [7168/50176]	Loss: 2.2095
Profiling... [8192/50176]	Loss: 2.1088
Profiling... [9216/50176]	Loss: 2.1239
Profiling... [10240/50176]	Loss: 2.0534
Profiling... [11264/50176]	Loss: 2.0622
Profiling... [12288/50176]	Loss: 2.1489
Profiling... [13312/50176]	Loss: 2.2172
Profile done
epoch 1 train time consumed: 13.68s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4231
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.7966693684679,
                        "time": 9.575701043002482,
                        "accuracy": 0.42314453125,
                        "total_cost": 1981400.2563822526
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1808
Profiling... [2048/50176]	Loss: 2.1904
Profiling... [3072/50176]	Loss: 2.1649
Profiling... [4096/50176]	Loss: 2.1641
Profiling... [5120/50176]	Loss: 2.1436
Profiling... [6144/50176]	Loss: 2.2093
Profiling... [7168/50176]	Loss: 2.1113
Profiling... [8192/50176]	Loss: 2.1497
Profiling... [9216/50176]	Loss: 2.1055
Profiling... [10240/50176]	Loss: 2.1561
Profiling... [11264/50176]	Loss: 2.1731
Profiling... [12288/50176]	Loss: 2.1056
Profiling... [13312/50176]	Loss: 2.1906
Profile done
epoch 1 train time consumed: 21.29s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4265
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.70647242254938,
                        "time": 15.256213388005563,
                        "accuracy": 0.42646484375,
                        "total_cost": 3132230.138381541
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1075
Profiling... [2048/50176]	Loss: 2.1493
Profiling... [3072/50176]	Loss: 2.2981
Profiling... [4096/50176]	Loss: 2.2903
Profiling... [5120/50176]	Loss: 2.2384
Profiling... [6144/50176]	Loss: 2.2058
Profiling... [7168/50176]	Loss: 2.2210
Profiling... [8192/50176]	Loss: 2.2736
Profiling... [9216/50176]	Loss: 2.2421
Profiling... [10240/50176]	Loss: 2.2645
Profiling... [11264/50176]	Loss: 2.1335
Profiling... [12288/50176]	Loss: 2.1991
Profiling... [13312/50176]	Loss: 2.2267
Profile done
epoch 1 train time consumed: 13.13s
Validation Epoch: 6, Average loss: 0.0023, Accuracy: 0.3917
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.73729475090705,
                        "time": 9.11713647400029,
                        "accuracy": 0.39169921875,
                        "total_cost": 2037961.4828992793
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1711
Profiling... [2048/50176]	Loss: 2.2510
Profiling... [3072/50176]	Loss: 2.2373
Profiling... [4096/50176]	Loss: 2.2290
Profiling... [5120/50176]	Loss: 2.2832
Profiling... [6144/50176]	Loss: 2.2771
Profiling... [7168/50176]	Loss: 2.2233
Profiling... [8192/50176]	Loss: 2.2191
Profiling... [9216/50176]	Loss: 2.2568
Profiling... [10240/50176]	Loss: 2.1486
Profiling... [11264/50176]	Loss: 2.3052
Profiling... [12288/50176]	Loss: 2.2354
Profiling... [13312/50176]	Loss: 2.1920
Profile done
epoch 1 train time consumed: 13.03s
Validation Epoch: 6, Average loss: 0.0023, Accuracy: 0.3965
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.79722246088653,
                        "time": 9.024623694000184,
                        "accuracy": 0.396484375,
                        "total_cost": 1992936.1952285813
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.1458
Profiling... [2048/50176]	Loss: 2.2112
Profiling... [3072/50176]	Loss: 2.2497
Profiling... [4096/50176]	Loss: 2.2621
Profiling... [5120/50176]	Loss: 2.1105
Profiling... [6144/50176]	Loss: 2.1989
Profiling... [7168/50176]	Loss: 2.0941
Profiling... [8192/50176]	Loss: 2.2569
Profiling... [9216/50176]	Loss: 2.2239
Profiling... [10240/50176]	Loss: 2.2634
Profiling... [11264/50176]	Loss: 2.1764
Profiling... [12288/50176]	Loss: 2.1907
Profiling... [13312/50176]	Loss: 2.1789
Profile done
epoch 1 train time consumed: 13.77s
Validation Epoch: 6, Average loss: 0.0024, Accuracy: 0.3649
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.81561409545138,
                        "time": 9.674807169001724,
                        "accuracy": 0.36494140625,
                        "total_cost": 2321184.1266566296
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1932
Profiling... [2048/50176]	Loss: 2.2299
Profiling... [3072/50176]	Loss: 2.1469
Profiling... [4096/50176]	Loss: 2.2308
Profiling... [5120/50176]	Loss: 2.2739
Profiling... [6144/50176]	Loss: 2.2412
Profiling... [7168/50176]	Loss: 2.2199
Profiling... [8192/50176]	Loss: 2.2596
Profiling... [9216/50176]	Loss: 2.2988
Profiling... [10240/50176]	Loss: 2.1997
Profiling... [11264/50176]	Loss: 2.2391
Profiling... [12288/50176]	Loss: 2.1508
Profiling... [13312/50176]	Loss: 2.1968
Profile done
epoch 1 train time consumed: 22.15s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.3930
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.71885149541562,
                        "time": 15.355179015998146,
                        "accuracy": 0.39296875,
                        "total_cost": 3421267.5958608435
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0706
Profiling... [2048/50176]	Loss: 2.1719
Profiling... [3072/50176]	Loss: 2.2690
Profiling... [4096/50176]	Loss: 2.2258
Profiling... [5120/50176]	Loss: 2.2390
Profiling... [6144/50176]	Loss: 2.2341
Profiling... [7168/50176]	Loss: 2.2504
Profiling... [8192/50176]	Loss: 2.2568
Profiling... [9216/50176]	Loss: 2.0842
Profiling... [10240/50176]	Loss: 2.2732
Profiling... [11264/50176]	Loss: 2.2221
Profiling... [12288/50176]	Loss: 2.2759
Profiling... [13312/50176]	Loss: 2.2207
Profile done
epoch 1 train time consumed: 13.00s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.3978
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.75339642582624,
                        "time": 8.998460358001466,
                        "accuracy": 0.39775390625,
                        "total_cost": 1980815.453121125
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1962
Profiling... [2048/50176]	Loss: 2.2008
Profiling... [3072/50176]	Loss: 2.1971
Profiling... [4096/50176]	Loss: 2.2373
Profiling... [5120/50176]	Loss: 2.2323
Profiling... [6144/50176]	Loss: 2.2190
Profiling... [7168/50176]	Loss: 2.1365
Profiling... [8192/50176]	Loss: 2.1786
Profiling... [9216/50176]	Loss: 2.2715
Profiling... [10240/50176]	Loss: 2.2774
Profiling... [11264/50176]	Loss: 2.1961
Profiling... [12288/50176]	Loss: 2.2534
Profiling... [13312/50176]	Loss: 2.1636
Profile done
epoch 1 train time consumed: 12.95s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.3991
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.81200868994186,
                        "time": 9.025320291999378,
                        "accuracy": 0.39912109375,
                        "total_cost": 1979923.2183921547
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.1201
Profiling... [2048/50176]	Loss: 2.2297
Profiling... [3072/50176]	Loss: 2.1928
Profiling... [4096/50176]	Loss: 2.2677
Profiling... [5120/50176]	Loss: 2.1820
Profiling... [6144/50176]	Loss: 2.1670
Profiling... [7168/50176]	Loss: 2.2640
Profiling... [8192/50176]	Loss: 2.2522
Profiling... [9216/50176]	Loss: 2.2697
Profiling... [10240/50176]	Loss: 2.2327
Profiling... [11264/50176]	Loss: 2.2655
Profiling... [12288/50176]	Loss: 2.1720
Profiling... [13312/50176]	Loss: 2.2049
Profile done
epoch 1 train time consumed: 13.74s
Validation Epoch: 6, Average loss: 0.0023, Accuracy: 0.3808
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.83017046698951,
                        "time": 9.547526119000395,
                        "accuracy": 0.38076171875,
                        "total_cost": 2195472.634223071
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1520
Profiling... [2048/50176]	Loss: 2.2488
Profiling... [3072/50176]	Loss: 2.2850
Profiling... [4096/50176]	Loss: 2.2888
Profiling... [5120/50176]	Loss: 2.1357
Profiling... [6144/50176]	Loss: 2.2921
Profiling... [7168/50176]	Loss: 2.1842
Profiling... [8192/50176]	Loss: 2.2161
Profiling... [9216/50176]	Loss: 2.2447
Profiling... [10240/50176]	Loss: 2.2484
Profiling... [11264/50176]	Loss: 2.2210
Profiling... [12288/50176]	Loss: 2.1611
Profiling... [13312/50176]	Loss: 2.1207
Profile done
epoch 1 train time consumed: 23.86s
Validation Epoch: 6, Average loss: 0.0026, Accuracy: 0.3442
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.73213154008906,
                        "time": 17.87189143100113,
                        "accuracy": 0.34423828125,
                        "total_cost": 4545708.277080672
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0684
Profiling... [2048/50176]	Loss: 2.1663
Profiling... [3072/50176]	Loss: 2.2485
Profiling... [4096/50176]	Loss: 2.1835
Profiling... [5120/50176]	Loss: 2.2164
Profiling... [6144/50176]	Loss: 2.2229
Profiling... [7168/50176]	Loss: 2.2054
Profiling... [8192/50176]	Loss: 2.2299
Profiling... [9216/50176]	Loss: 2.3244
Profiling... [10240/50176]	Loss: 2.1893
Profiling... [11264/50176]	Loss: 2.2356
Profiling... [12288/50176]	Loss: 2.1983
Profiling... [13312/50176]	Loss: 2.1594
Profile done
epoch 1 train time consumed: 12.91s
Validation Epoch: 6, Average loss: 0.0023, Accuracy: 0.3940
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.76451039455647,
                        "time": 8.797372277993418,
                        "accuracy": 0.39404296875,
                        "total_cost": 1954788.0555841194
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1314
Profiling... [2048/50176]	Loss: 2.1886
Profiling... [3072/50176]	Loss: 2.2112
Profiling... [4096/50176]	Loss: 2.2203
Profiling... [5120/50176]	Loss: 2.1646
Profiling... [6144/50176]	Loss: 2.2144
Profiling... [7168/50176]	Loss: 2.2672
Profiling... [8192/50176]	Loss: 2.1970
Profiling... [9216/50176]	Loss: 2.1357
Profiling... [10240/50176]	Loss: 2.2190
Profiling... [11264/50176]	Loss: 2.1961
Profiling... [12288/50176]	Loss: 2.1935
Profiling... [13312/50176]	Loss: 2.1777
Profile done
epoch 1 train time consumed: 12.75s
Validation Epoch: 6, Average loss: 0.0023, Accuracy: 0.3838
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.8234708777861,
                        "time": 8.81865800399828,
                        "accuracy": 0.3837890625,
                        "total_cost": 2011871.9784545687
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0937
Profiling... [2048/50176]	Loss: 2.1203
Profiling... [3072/50176]	Loss: 2.3345
Profiling... [4096/50176]	Loss: 2.2002
Profiling... [5120/50176]	Loss: 2.2617
Profiling... [6144/50176]	Loss: 2.2272
Profiling... [7168/50176]	Loss: 2.3001
Profiling... [8192/50176]	Loss: 2.3127
Profiling... [9216/50176]	Loss: 2.1726
Profiling... [10240/50176]	Loss: 2.2861
Profiling... [11264/50176]	Loss: 2.1941
Profiling... [12288/50176]	Loss: 2.2208
Profiling... [13312/50176]	Loss: 2.1384
Profile done
epoch 1 train time consumed: 13.69s
Validation Epoch: 6, Average loss: 0.0023, Accuracy: 0.3824
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.84383195156575,
                        "time": 9.583239252002386,
                        "accuracy": 0.382421875,
                        "total_cost": 2194118.552683962
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.2038
Profiling... [2048/50176]	Loss: 2.1846
Profiling... [3072/50176]	Loss: 2.2068
Profiling... [4096/50176]	Loss: 2.1838
Profiling... [5120/50176]	Loss: 2.1183
Profiling... [6144/50176]	Loss: 2.2600
Profiling... [7168/50176]	Loss: 2.2918
Profiling... [8192/50176]	Loss: 2.1888
Profiling... [9216/50176]	Loss: 2.2422
Profiling... [10240/50176]	Loss: 2.1952
Profiling... [11264/50176]	Loss: 2.2313
Profiling... [12288/50176]	Loss: 2.1896
Profiling... [13312/50176]	Loss: 2.2275
Profile done
epoch 1 train time consumed: 22.04s
Validation Epoch: 6, Average loss: 0.0025, Accuracy: 0.3563
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.75524939697745,
                        "time": 15.92690749099711,
                        "accuracy": 0.35634765625,
                        "total_cost": 3913342.1131603913
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1434
Profiling... [2048/50176]	Loss: 2.4065
Profiling... [3072/50176]	Loss: 2.4847
Profiling... [4096/50176]	Loss: 2.5050
Profiling... [5120/50176]	Loss: 2.3784
Profiling... [6144/50176]	Loss: 2.5177
Profiling... [7168/50176]	Loss: 2.3274
Profiling... [8192/50176]	Loss: 2.3458
Profiling... [9216/50176]	Loss: 2.3608
Profiling... [10240/50176]	Loss: 2.4136
Profiling... [11264/50176]	Loss: 2.4022
Profiling... [12288/50176]	Loss: 2.4017
Profiling... [13312/50176]	Loss: 2.3559
Profile done
epoch 1 train time consumed: 12.71s
Validation Epoch: 6, Average loss: 0.0026, Accuracy: 0.3240
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.78771541034494,
                        "time": 8.781834032997722,
                        "accuracy": 0.3240234375,
                        "total_cost": 2373007.6942448476
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1345
Profiling... [2048/50176]	Loss: 2.5989
Profiling... [3072/50176]	Loss: 2.3353
Profiling... [4096/50176]	Loss: 2.3982
Profiling... [5120/50176]	Loss: 2.4437
Profiling... [6144/50176]	Loss: 2.3521
Profiling... [7168/50176]	Loss: 2.5347
Profiling... [8192/50176]	Loss: 2.3379
Profiling... [9216/50176]	Loss: 2.3784
Profiling... [10240/50176]	Loss: 2.4587
Profiling... [11264/50176]	Loss: 2.3287
Profiling... [12288/50176]	Loss: 2.1922
Profiling... [13312/50176]	Loss: 2.3587
Profile done
epoch 1 train time consumed: 12.70s
Validation Epoch: 6, Average loss: 0.0025, Accuracy: 0.3296
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.84943367728195,
                        "time": 8.803395180002553,
                        "accuracy": 0.32958984375,
                        "total_cost": 2338658.862354777
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.1060
Profiling... [2048/50176]	Loss: 2.5116
Profiling... [3072/50176]	Loss: 2.3708
Profiling... [4096/50176]	Loss: 2.3183
Profiling... [5120/50176]	Loss: 2.3805
Profiling... [6144/50176]	Loss: 2.3886
Profiling... [7168/50176]	Loss: 2.4519
Profiling... [8192/50176]	Loss: 2.3429
Profiling... [9216/50176]	Loss: 2.3376
Profiling... [10240/50176]	Loss: 2.3299
Profiling... [11264/50176]	Loss: 2.4013
Profiling... [12288/50176]	Loss: 2.3687
Profiling... [13312/50176]	Loss: 2.3693
Profile done
epoch 1 train time consumed: 13.82s
Validation Epoch: 6, Average loss: 0.0033, Accuracy: 0.2363
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.86605231996953,
                        "time": 9.660368584001844,
                        "accuracy": 0.236328125,
                        "total_cost": 3579058.75619954
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1381
Profiling... [2048/50176]	Loss: 2.4723
Profiling... [3072/50176]	Loss: 2.5157
Profiling... [4096/50176]	Loss: 2.4065
Profiling... [5120/50176]	Loss: 2.3760
Profiling... [6144/50176]	Loss: 2.4366
Profiling... [7168/50176]	Loss: 2.4414
Profiling... [8192/50176]	Loss: 2.3432
Profiling... [9216/50176]	Loss: 2.3661
Profiling... [10240/50176]	Loss: 2.3506
Profiling... [11264/50176]	Loss: 2.4500
Profiling... [12288/50176]	Loss: 2.3481
Profiling... [13312/50176]	Loss: 2.3976
Profile done
epoch 1 train time consumed: 22.67s
Validation Epoch: 6, Average loss: 0.0030, Accuracy: 0.2750
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.77360915548742,
                        "time": 16.56410991300072,
                        "accuracy": 0.275,
                        "total_cost": 5273825.078804321
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1755
Profiling... [2048/50176]	Loss: 2.4377
Profiling... [3072/50176]	Loss: 2.2974
Profiling... [4096/50176]	Loss: 2.3766
Profiling... [5120/50176]	Loss: 2.5052
Profiling... [6144/50176]	Loss: 2.3965
Profiling... [7168/50176]	Loss: 2.3821
Profiling... [8192/50176]	Loss: 2.3682
Profiling... [9216/50176]	Loss: 2.4059
Profiling... [10240/50176]	Loss: 2.2874
Profiling... [11264/50176]	Loss: 2.3742
Profiling... [12288/50176]	Loss: 2.3393
Profiling... [13312/50176]	Loss: 2.3416
Profile done
epoch 1 train time consumed: 13.11s
Validation Epoch: 6, Average loss: 0.0026, Accuracy: 0.3263
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.80243432412757,
                        "time": 9.199815546999162,
                        "accuracy": 0.32626953125,
                        "total_cost": 2468840.2192465444
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1112
Profiling... [2048/50176]	Loss: 2.5334
Profiling... [3072/50176]	Loss: 2.3773
Profiling... [4096/50176]	Loss: 2.4380
Profiling... [5120/50176]	Loss: 2.3770
Profiling... [6144/50176]	Loss: 2.4479
Profiling... [7168/50176]	Loss: 2.3242
Profiling... [8192/50176]	Loss: 2.4229
Profiling... [9216/50176]	Loss: 2.3241
Profiling... [10240/50176]	Loss: 2.3883
Profiling... [11264/50176]	Loss: 2.3841
Profiling... [12288/50176]	Loss: 2.3952
Profiling... [13312/50176]	Loss: 2.3590
Profile done
epoch 1 train time consumed: 12.93s
Validation Epoch: 6, Average loss: 0.0026, Accuracy: 0.3271
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.85912105581596,
                        "time": 8.913306631999149,
                        "accuracy": 0.3271484375,
                        "total_cost": 2385528.009527283
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.1462
Profiling... [2048/50176]	Loss: 2.5247
Profiling... [3072/50176]	Loss: 2.3042
Profiling... [4096/50176]	Loss: 2.3650
Profiling... [5120/50176]	Loss: 2.3870
Profiling... [6144/50176]	Loss: 2.2944
Profiling... [7168/50176]	Loss: 2.4027
Profiling... [8192/50176]	Loss: 2.3065
Profiling... [9216/50176]	Loss: 2.3113
Profiling... [10240/50176]	Loss: 2.3129
Profiling... [11264/50176]	Loss: 2.3446
Profiling... [12288/50176]	Loss: 2.3432
Profiling... [13312/50176]	Loss: 2.3078
Profile done
epoch 1 train time consumed: 13.67s
Validation Epoch: 6, Average loss: 0.0027, Accuracy: 0.3027
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.87599708218858,
                        "time": 9.568858478000038,
                        "accuracy": 0.302734375,
                        "total_cost": 2767508.474962791
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1671
Profiling... [2048/50176]	Loss: 2.5326
Profiling... [3072/50176]	Loss: 2.3905
Profiling... [4096/50176]	Loss: 2.3470
Profiling... [5120/50176]	Loss: 2.2963
Profiling... [6144/50176]	Loss: 2.5281
Profiling... [7168/50176]	Loss: 2.4913
Profiling... [8192/50176]	Loss: 2.4470
Profiling... [9216/50176]	Loss: 2.3642
Profiling... [10240/50176]	Loss: 2.3355
Profiling... [11264/50176]	Loss: 2.4795
Profiling... [12288/50176]	Loss: 2.3956
Profiling... [13312/50176]	Loss: 2.4448
Profile done
epoch 1 train time consumed: 21.43s
Validation Epoch: 6, Average loss: 0.0025, Accuracy: 0.3412
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.78882576736169,
                        "time": 14.696126560003904,
                        "accuracy": 0.3412109375,
                        "total_cost": 3771119.44570252
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1265
Profiling... [2048/50176]	Loss: 2.5524
Profiling... [3072/50176]	Loss: 2.4843
Profiling... [4096/50176]	Loss: 2.3960
Profiling... [5120/50176]	Loss: 2.4077
Profiling... [6144/50176]	Loss: 2.4316
Profiling... [7168/50176]	Loss: 2.3425
Profiling... [8192/50176]	Loss: 2.3262
Profiling... [9216/50176]	Loss: 2.3745
Profiling... [10240/50176]	Loss: 2.3192
Profiling... [11264/50176]	Loss: 2.3580
Profiling... [12288/50176]	Loss: 2.3697
Profiling... [13312/50176]	Loss: 2.3596
Profile done
epoch 1 train time consumed: 12.83s
Validation Epoch: 6, Average loss: 0.0027, Accuracy: 0.3165
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.82289867928036,
                        "time": 8.801930404995801,
                        "accuracy": 0.31650390625,
                        "total_cost": 2434945.7489625136
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1590
Profiling... [2048/50176]	Loss: 2.5153
Profiling... [3072/50176]	Loss: 2.3727
Profiling... [4096/50176]	Loss: 2.3598
Profiling... [5120/50176]	Loss: 2.4123
Profiling... [6144/50176]	Loss: 2.3451
Profiling... [7168/50176]	Loss: 2.4022
Profiling... [8192/50176]	Loss: 2.4138
Profiling... [9216/50176]	Loss: 2.3354
Profiling... [10240/50176]	Loss: 2.4262
Profiling... [11264/50176]	Loss: 2.2620
Profiling... [12288/50176]	Loss: 2.4807
Profiling... [13312/50176]	Loss: 2.4170
Profile done
epoch 1 train time consumed: 12.96s
Validation Epoch: 6, Average loss: 0.0029, Accuracy: 0.2868
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.87820307257898,
                        "time": 9.03343539100024,
                        "accuracy": 0.28681640625,
                        "total_cost": 2757652.4047165476
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0886
Profiling... [2048/50176]	Loss: 2.4354
Profiling... [3072/50176]	Loss: 2.3942
Profiling... [4096/50176]	Loss: 2.3419
Profiling... [5120/50176]	Loss: 2.3851
Profiling... [6144/50176]	Loss: 2.3794
Profiling... [7168/50176]	Loss: 2.3876
Profiling... [8192/50176]	Loss: 2.3336
Profiling... [9216/50176]	Loss: 2.4024
Profiling... [10240/50176]	Loss: 2.4503
Profiling... [11264/50176]	Loss: 2.4226
Profiling... [12288/50176]	Loss: 2.3645
Profiling... [13312/50176]	Loss: 2.3418
Profile done
epoch 1 train time consumed: 13.72s
Validation Epoch: 6, Average loss: 0.0025, Accuracy: 0.3582
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.89337998952205,
                        "time": 9.586758751000161,
                        "accuracy": 0.358203125,
                        "total_cost": 2343327.755421346
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1146
Profiling... [2048/50176]	Loss: 2.5610
Profiling... [3072/50176]	Loss: 2.3177
Profiling... [4096/50176]	Loss: 2.4400
Profiling... [5120/50176]	Loss: 2.4599
Profiling... [6144/50176]	Loss: 2.4347
Profiling... [7168/50176]	Loss: 2.3648
Profiling... [8192/50176]	Loss: 2.4727
Profiling... [9216/50176]	Loss: 2.3858
Profiling... [10240/50176]	Loss: 2.3223
Profiling... [11264/50176]	Loss: 2.4533
Profiling... [12288/50176]	Loss: 2.3524
Profiling... [13312/50176]	Loss: 2.3511
Profile done
epoch 1 train time consumed: 23.43s
Validation Epoch: 6, Average loss: 0.0027, Accuracy: 0.3180
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.80124582103309,
                        "time": 16.587194359999557,
                        "accuracy": 0.31796875,
                        "total_cost": 4567503.341732114
                    },
                    
[Training Loop] The optimal parameters are lr: 0.005 dr: 0.25 bs: 256 pl: 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[GPU_0] Set GPU power limit to 125W.
[GPU_0] Set GPU power limit to 125W.
Training Epoch: 6 [256/50176]	Loss: 2.2040
Training Epoch: 6 [512/50176]	Loss: 2.1127
Training Epoch: 6 [768/50176]	Loss: 2.4212
Training Epoch: 6 [1024/50176]	Loss: 2.2606
Training Epoch: 6 [1280/50176]	Loss: 2.4700
Training Epoch: 6 [1536/50176]	Loss: 2.2106
Training Epoch: 6 [1792/50176]	Loss: 2.4036
Training Epoch: 6 [2048/50176]	Loss: 2.2305
Training Epoch: 6 [2304/50176]	Loss: 2.3889
Training Epoch: 6 [2560/50176]	Loss: 2.5466
Training Epoch: 6 [2816/50176]	Loss: 2.3090
Training Epoch: 6 [3072/50176]	Loss: 2.3938
Training Epoch: 6 [3328/50176]	Loss: 2.3386
Training Epoch: 6 [3584/50176]	Loss: 2.4193
Training Epoch: 6 [3840/50176]	Loss: 2.3318
Training Epoch: 6 [4096/50176]	Loss: 2.3854
Training Epoch: 6 [4352/50176]	Loss: 2.1793
Training Epoch: 6 [4608/50176]	Loss: 2.4564
Training Epoch: 6 [4864/50176]	Loss: 2.2995
Training Epoch: 6 [5120/50176]	Loss: 2.2467
Training Epoch: 6 [5376/50176]	Loss: 2.2149
Training Epoch: 6 [5632/50176]	Loss: 2.3275
Training Epoch: 6 [5888/50176]	Loss: 2.4088
Training Epoch: 6 [6144/50176]	Loss: 2.1404
Training Epoch: 6 [6400/50176]	Loss: 2.3846
Training Epoch: 6 [6656/50176]	Loss: 2.4654
Training Epoch: 6 [6912/50176]	Loss: 2.2327
Training Epoch: 6 [7168/50176]	Loss: 2.1840
Training Epoch: 6 [7424/50176]	Loss: 2.3912
Training Epoch: 6 [7680/50176]	Loss: 2.3421
Training Epoch: 6 [7936/50176]	Loss: 2.3136
Training Epoch: 6 [8192/50176]	Loss: 2.2176
Training Epoch: 6 [8448/50176]	Loss: 2.4495
Training Epoch: 6 [8704/50176]	Loss: 2.2849
Training Epoch: 6 [8960/50176]	Loss: 2.1211
Training Epoch: 6 [9216/50176]	Loss: 2.2658
Training Epoch: 6 [9472/50176]	Loss: 2.4441
Training Epoch: 6 [9728/50176]	Loss: 2.3043
Training Epoch: 6 [9984/50176]	Loss: 2.4653
Training Epoch: 6 [10240/50176]	Loss: 2.1983
Training Epoch: 6 [10496/50176]	Loss: 2.3253
Training Epoch: 6 [10752/50176]	Loss: 2.1999
Training Epoch: 6 [11008/50176]	Loss: 2.4976
Training Epoch: 6 [11264/50176]	Loss: 2.3479
Training Epoch: 6 [11520/50176]	Loss: 2.2335
Training Epoch: 6 [11776/50176]	Loss: 2.2335
Training Epoch: 6 [12032/50176]	Loss: 2.2792
Training Epoch: 6 [12288/50176]	Loss: 2.2350
Training Epoch: 6 [12544/50176]	Loss: 2.3674
Training Epoch: 6 [12800/50176]	Loss: 2.2883
Training Epoch: 6 [13056/50176]	Loss: 2.1211
Training Epoch: 6 [13312/50176]	Loss: 2.1821
Training Epoch: 6 [13568/50176]	Loss: 2.2878
Training Epoch: 6 [13824/50176]	Loss: 2.1718
Training Epoch: 6 [14080/50176]	Loss: 2.2209
Training Epoch: 6 [14336/50176]	Loss: 2.4270
Training Epoch: 6 [14592/50176]	Loss: 2.4005
Training Epoch: 6 [14848/50176]	Loss: 2.4467
Training Epoch: 6 [15104/50176]	Loss: 2.1520
Training Epoch: 6 [15360/50176]	Loss: 2.3833
Training Epoch: 6 [15616/50176]	Loss: 2.0977
Training Epoch: 6 [15872/50176]	Loss: 2.4439
Training Epoch: 6 [16128/50176]	Loss: 2.1955
Training Epoch: 6 [16384/50176]	Loss: 2.1100
Training Epoch: 6 [16640/50176]	Loss: 2.2264
Training Epoch: 6 [16896/50176]	Loss: 2.2003
Training Epoch: 6 [17152/50176]	Loss: 2.0933
Training Epoch: 6 [17408/50176]	Loss: 2.0379
Training Epoch: 6 [17664/50176]	Loss: 2.1614
Training Epoch: 6 [17920/50176]	Loss: 2.3861
Training Epoch: 6 [18176/50176]	Loss: 2.2130
Training Epoch: 6 [18432/50176]	Loss: 2.2730
Training Epoch: 6 [18688/50176]	Loss: 2.1665
Training Epoch: 6 [18944/50176]	Loss: 2.2438
Training Epoch: 6 [19200/50176]	Loss: 2.2304
Training Epoch: 6 [19456/50176]	Loss: 2.3968
Training Epoch: 6 [19712/50176]	Loss: 2.1790
Training Epoch: 6 [19968/50176]	Loss: 2.2906
Training Epoch: 6 [20224/50176]	Loss: 2.0501
Training Epoch: 6 [20480/50176]	Loss: 2.1648
Training Epoch: 6 [20736/50176]	Loss: 2.2761
Training Epoch: 6 [20992/50176]	Loss: 2.1731
Training Epoch: 6 [21248/50176]	Loss: 2.1486
Training Epoch: 6 [21504/50176]	Loss: 2.1987
Training Epoch: 6 [21760/50176]	Loss: 2.0472
Training Epoch: 6 [22016/50176]	Loss: 2.1792
Training Epoch: 6 [22272/50176]	Loss: 2.4120
Training Epoch: 6 [22528/50176]	Loss: 2.1562
Training Epoch: 6 [22784/50176]	Loss: 2.3447
Training Epoch: 6 [23040/50176]	Loss: 2.4522
Training Epoch: 6 [23296/50176]	Loss: 2.0815
Training Epoch: 6 [23552/50176]	Loss: 2.2345
Training Epoch: 6 [23808/50176]	Loss: 2.3803
Training Epoch: 6 [24064/50176]	Loss: 2.3778
Training Epoch: 6 [24320/50176]	Loss: 2.3231
Training Epoch: 6 [24576/50176]	Loss: 2.2041
Training Epoch: 6 [24832/50176]	Loss: 2.2357
Training Epoch: 6 [25088/50176]	Loss: 2.2951
Training Epoch: 6 [25344/50176]	Loss: 2.1927
Training Epoch: 6 [25600/50176]	Loss: 2.3092
Training Epoch: 6 [25856/50176]	Loss: 2.4349
Training Epoch: 6 [26112/50176]	Loss: 2.2963
Training Epoch: 6 [26368/50176]	Loss: 2.2987
Training Epoch: 6 [26624/50176]	Loss: 2.3567
Training Epoch: 6 [26880/50176]	Loss: 2.1233
Training Epoch: 6 [27136/50176]	Loss: 2.4129
Training Epoch: 6 [27392/50176]	Loss: 2.2699
Training Epoch: 6 [27648/50176]	Loss: 2.4285
Training Epoch: 6 [27904/50176]	Loss: 2.3003
Training Epoch: 6 [28160/50176]	Loss: 2.3967
Training Epoch: 6 [28416/50176]	Loss: 2.2521
Training Epoch: 6 [28672/50176]	Loss: 2.3308
Training Epoch: 6 [28928/50176]	Loss: 2.3687
Training Epoch: 6 [29184/50176]	Loss: 2.3004
Training Epoch: 6 [29440/50176]	Loss: 2.2768
Training Epoch: 6 [29696/50176]	Loss: 2.2027
Training Epoch: 6 [29952/50176]	Loss: 2.1128
Training Epoch: 6 [30208/50176]	Loss: 2.1680
Training Epoch: 6 [30464/50176]	Loss: 2.2079
Training Epoch: 6 [30720/50176]	Loss: 2.1436
Training Epoch: 6 [30976/50176]	Loss: 2.2877
Training Epoch: 6 [31232/50176]	Loss: 2.2643
Training Epoch: 6 [31488/50176]	Loss: 2.2465
Training Epoch: 6 [31744/50176]	Loss: 2.0779
Training Epoch: 6 [32000/50176]	Loss: 2.2727
Training Epoch: 6 [32256/50176]	Loss: 2.2901
Training Epoch: 6 [32512/50176]	Loss: 2.3750
Training Epoch: 6 [32768/50176]	Loss: 2.2429
Training Epoch: 6 [33024/50176]	Loss: 2.2553
Training Epoch: 6 [33280/50176]	Loss: 1.9617
Training Epoch: 6 [33536/50176]	Loss: 2.1839
Training Epoch: 6 [33792/50176]	Loss: 2.2842
Training Epoch: 6 [34048/50176]	Loss: 2.0935
Training Epoch: 6 [34304/50176]	Loss: 2.1827
Training Epoch: 6 [34560/50176]	Loss: 2.3339
Training Epoch: 6 [34816/50176]	Loss: 2.1614
Training Epoch: 6 [35072/50176]	Loss: 2.3559
Training Epoch: 6 [35328/50176]	Loss: 2.2545
Training Epoch: 6 [35584/50176]	Loss: 2.2636
Training Epoch: 6 [35840/50176]	Loss: 2.4229
Training Epoch: 6 [36096/50176]	Loss: 2.1447
Training Epoch: 6 [36352/50176]	Loss: 2.1834
Training Epoch: 6 [36608/50176]	Loss: 2.3125
Training Epoch: 6 [36864/50176]	Loss: 2.2778
Training Epoch: 6 [37120/50176]	Loss: 2.2530
Training Epoch: 6 [37376/50176]	Loss: 2.2122
Training Epoch: 6 [37632/50176]	Loss: 2.1540
Training Epoch: 6 [37888/50176]	Loss: 2.1804
Training Epoch: 6 [38144/50176]	Loss: 2.4607
Training Epoch: 6 [38400/50176]	Loss: 2.3750
Training Epoch: 6 [38656/50176]	Loss: 2.1023
Training Epoch: 6 [38912/50176]	Loss: 2.1734
Training Epoch: 6 [39168/50176]	Loss: 2.2738
Training Epoch: 6 [39424/50176]	Loss: 2.2733
Training Epoch: 6 [39680/50176]	Loss: 2.1093
Training Epoch: 6 [39936/50176]	Loss: 2.2584
Training Epoch: 6 [40192/50176]	Loss: 2.0398
Training Epoch: 6 [40448/50176]	Loss: 2.2114
Training Epoch: 6 [40704/50176]	Loss: 2.4050
Training Epoch: 6 [40960/50176]	Loss: 2.3096
Training Epoch: 6 [41216/50176]	Loss: 2.3178
Training Epoch: 6 [41472/50176]	Loss: 2.1972
Training Epoch: 6 [41728/50176]	Loss: 2.2921
Training Epoch: 6 [41984/50176]	Loss: 2.2500
Training Epoch: 6 [42240/50176]	Loss: 2.2645
Training Epoch: 6 [42496/50176]	Loss: 2.1327
Training Epoch: 6 [42752/50176]	Loss: 2.1580
Training Epoch: 6 [43008/50176]	Loss: 1.9776
Training Epoch: 6 [43264/50176]	Loss: 2.1269
Training Epoch: 6 [43520/50176]	Loss: 2.0291
Training Epoch: 6 [43776/50176]	Loss: 2.2270
Training Epoch: 6 [44032/50176]	Loss: 2.1306
Training Epoch: 6 [44288/50176]	Loss: 2.2331
Training Epoch: 6 [44544/50176]	Loss: 2.1257
Training Epoch: 6 [44800/50176]	Loss: 2.2213
Training Epoch: 6 [45056/50176]	Loss: 2.2259
Training Epoch: 6 [45312/50176]	Loss: 2.3031
Training Epoch: 6 [45568/50176]	Loss: 2.1306
Training Epoch: 6 [45824/50176]	Loss: 2.3595
Training Epoch: 6 [46080/50176]	Loss: 1.9517
Training Epoch: 6 [46336/50176]	Loss: 2.4319
Training Epoch: 6 [46592/50176]	Loss: 2.2269
Training Epoch: 6 [46848/50176]	Loss: 2.1893
Training Epoch: 6 [47104/50176]	Loss: 2.1270
Training Epoch: 6 [47360/50176]	Loss: 2.1486
Training Epoch: 6 [47616/50176]	Loss: 2.2918
Training Epoch: 6 [47872/50176]	Loss: 2.1512
Training Epoch: 6 [48128/50176]	Loss: 2.1539
Training Epoch: 6 [48384/50176]	Loss: 2.1156
Training Epoch: 6 [48640/50176]	Loss: 2.1100
Training Epoch: 6 [48896/50176]	Loss: 2.4601
Training Epoch: 6 [49152/50176]	Loss: 2.1576
Training Epoch: 6 [49408/50176]	Loss: 2.1113
Training Epoch: 6 [49664/50176]	Loss: 2.1486
Training Epoch: 6 [49920/50176]	Loss: 2.1374
Training Epoch: 6 [50176/50176]	Loss: 2.2988
Validation Epoch: 6, Average loss: 0.0088, Accuracy: 0.3997
Training Epoch: 7 [256/50176]	Loss: 2.1601
Training Epoch: 7 [512/50176]	Loss: 1.9735
Training Epoch: 7 [768/50176]	Loss: 2.1752
Training Epoch: 7 [1024/50176]	Loss: 2.2192
Training Epoch: 7 [1280/50176]	Loss: 1.9252
Training Epoch: 7 [1536/50176]	Loss: 2.0244
Training Epoch: 7 [1792/50176]	Loss: 2.2062
Training Epoch: 7 [2048/50176]	Loss: 2.1848
Training Epoch: 7 [2304/50176]	Loss: 2.1805
Training Epoch: 7 [2560/50176]	Loss: 1.9440
Training Epoch: 7 [2816/50176]	Loss: 2.0252
Training Epoch: 7 [3072/50176]	Loss: 2.1137
Training Epoch: 7 [3328/50176]	Loss: 2.2055
Training Epoch: 7 [3584/50176]	Loss: 2.1365
Training Epoch: 7 [3840/50176]	Loss: 2.2403
Training Epoch: 7 [4096/50176]	Loss: 1.9742
Training Epoch: 7 [4352/50176]	Loss: 2.3392
Training Epoch: 7 [4608/50176]	Loss: 2.1576
Training Epoch: 7 [4864/50176]	Loss: 2.0370
Training Epoch: 7 [5120/50176]	Loss: 1.9246
Training Epoch: 7 [5376/50176]	Loss: 2.0613
Training Epoch: 7 [5632/50176]	Loss: 2.0868
Training Epoch: 7 [5888/50176]	Loss: 2.1515
Training Epoch: 7 [6144/50176]	Loss: 2.0729
Training Epoch: 7 [6400/50176]	Loss: 1.9690
Training Epoch: 7 [6656/50176]	Loss: 2.0293
Training Epoch: 7 [6912/50176]	Loss: 2.0892
Training Epoch: 7 [7168/50176]	Loss: 2.0568
Training Epoch: 7 [7424/50176]	Loss: 2.0545
Training Epoch: 7 [7680/50176]	Loss: 2.3075
Training Epoch: 7 [7936/50176]	Loss: 2.1697
Training Epoch: 7 [8192/50176]	Loss: 2.3023
Training Epoch: 7 [8448/50176]	Loss: 2.1613
Training Epoch: 7 [8704/50176]	Loss: 1.9670
Training Epoch: 7 [8960/50176]	Loss: 2.2364
Training Epoch: 7 [9216/50176]	Loss: 2.1132
Training Epoch: 7 [9472/50176]	Loss: 2.1145
Training Epoch: 7 [9728/50176]	Loss: 2.3138
Training Epoch: 7 [9984/50176]	Loss: 2.0891
Training Epoch: 7 [10240/50176]	Loss: 2.0746
Training Epoch: 7 [10496/50176]	Loss: 2.0057
Training Epoch: 7 [10752/50176]	Loss: 2.1994
Training Epoch: 7 [11008/50176]	Loss: 2.0905
Training Epoch: 7 [11264/50176]	Loss: 2.1773
Training Epoch: 7 [11520/50176]	Loss: 2.1015
Training Epoch: 7 [11776/50176]	Loss: 2.2734
Training Epoch: 7 [12032/50176]	Loss: 2.1796
Training Epoch: 7 [12288/50176]	Loss: 1.9276
Training Epoch: 7 [12544/50176]	Loss: 1.8752
Training Epoch: 7 [12800/50176]	Loss: 2.1957
Training Epoch: 7 [13056/50176]	Loss: 2.3071
Training Epoch: 7 [13312/50176]	Loss: 2.0593
Training Epoch: 7 [13568/50176]	Loss: 2.0549
Training Epoch: 7 [13824/50176]	Loss: 2.1279
Training Epoch: 7 [14080/50176]	Loss: 1.9454
Training Epoch: 7 [14336/50176]	Loss: 1.9811
Training Epoch: 7 [14592/50176]	Loss: 2.0338
Training Epoch: 7 [14848/50176]	Loss: 2.0612
Training Epoch: 7 [15104/50176]	Loss: 1.8832
Training Epoch: 7 [15360/50176]	Loss: 2.1345
Training Epoch: 7 [15616/50176]	Loss: 1.9890
Training Epoch: 7 [15872/50176]	Loss: 2.0678
Training Epoch: 7 [16128/50176]	Loss: 1.9811
Training Epoch: 7 [16384/50176]	Loss: 1.9936
Training Epoch: 7 [16640/50176]	Loss: 2.1026
Training Epoch: 7 [16896/50176]	Loss: 2.0229
Training Epoch: 7 [17152/50176]	Loss: 2.1031
Training Epoch: 7 [17408/50176]	Loss: 1.9603
Training Epoch: 7 [17664/50176]	Loss: 2.3611
Training Epoch: 7 [17920/50176]	Loss: 2.0572
Training Epoch: 7 [18176/50176]	Loss: 2.0263
Training Epoch: 7 [18432/50176]	Loss: 2.3449
Training Epoch: 7 [18688/50176]	Loss: 2.0435
Training Epoch: 7 [18944/50176]	Loss: 2.0579
Training Epoch: 7 [19200/50176]	Loss: 2.1638
Training Epoch: 7 [19456/50176]	Loss: 2.1442
Training Epoch: 7 [19712/50176]	Loss: 2.1948
Training Epoch: 7 [19968/50176]	Loss: 2.1280
Training Epoch: 7 [20224/50176]	Loss: 2.0876
Training Epoch: 7 [20480/50176]	Loss: 2.0155
Training Epoch: 7 [20736/50176]	Loss: 2.1103
Training Epoch: 7 [20992/50176]	Loss: 2.0719
Training Epoch: 7 [21248/50176]	Loss: 2.2859
Training Epoch: 7 [21504/50176]	Loss: 2.0887
Training Epoch: 7 [21760/50176]	Loss: 2.0826
Training Epoch: 7 [22016/50176]	Loss: 2.2352
Training Epoch: 7 [22272/50176]	Loss: 1.9652
Training Epoch: 7 [22528/50176]	Loss: 2.3017
Training Epoch: 7 [22784/50176]	Loss: 2.0537
Training Epoch: 7 [23040/50176]	Loss: 2.1482
Training Epoch: 7 [23296/50176]	Loss: 2.2006
Training Epoch: 7 [23552/50176]	Loss: 2.2648
Training Epoch: 7 [23808/50176]	Loss: 2.0065
Training Epoch: 7 [24064/50176]	Loss: 2.1410
Training Epoch: 7 [24320/50176]	Loss: 2.0168
Training Epoch: 7 [24576/50176]	Loss: 2.0073
Training Epoch: 7 [24832/50176]	Loss: 2.0460
Training Epoch: 7 [25088/50176]	Loss: 2.1312
Training Epoch: 7 [25344/50176]	Loss: 2.0277
Training Epoch: 7 [25600/50176]	Loss: 2.0913
Training Epoch: 7 [25856/50176]	Loss: 1.9351
Training Epoch: 7 [26112/50176]	Loss: 2.0025
Training Epoch: 7 [26368/50176]	Loss: 1.9515
Training Epoch: 7 [26624/50176]	Loss: 2.2670
Training Epoch: 7 [26880/50176]	Loss: 2.1514
Training Epoch: 7 [27136/50176]	Loss: 2.1437
Training Epoch: 7 [27392/50176]	Loss: 2.1343
Training Epoch: 7 [27648/50176]	Loss: 2.1017
Training Epoch: 7 [27904/50176]	Loss: 2.1091
Training Epoch: 7 [28160/50176]	Loss: 1.9037
Training Epoch: 7 [28416/50176]	Loss: 2.1217
Training Epoch: 7 [28672/50176]	Loss: 2.0653
Training Epoch: 7 [28928/50176]	Loss: 2.0634
Training Epoch: 7 [29184/50176]	Loss: 2.1011
Training Epoch: 7 [29440/50176]	Loss: 2.1940
Training Epoch: 7 [29696/50176]	Loss: 2.0970
Training Epoch: 7 [29952/50176]	Loss: 1.9067
Training Epoch: 7 [30208/50176]	Loss: 2.3390
Training Epoch: 7 [30464/50176]	Loss: 1.8303
Training Epoch: 7 [30720/50176]	Loss: 2.0442
Training Epoch: 7 [30976/50176]	Loss: 1.9106
Training Epoch: 7 [31232/50176]	Loss: 2.1014
Training Epoch: 7 [31488/50176]	Loss: 1.9904
Training Epoch: 7 [31744/50176]	Loss: 2.0571
Training Epoch: 7 [32000/50176]	Loss: 2.2172
Training Epoch: 7 [32256/50176]	Loss: 2.1906
Training Epoch: 7 [32512/50176]	Loss: 2.1233
Training Epoch: 7 [32768/50176]	Loss: 2.1149
Training Epoch: 7 [33024/50176]	Loss: 2.1251
Training Epoch: 7 [33280/50176]	Loss: 1.9743
Training Epoch: 7 [33536/50176]	Loss: 2.1563
Training Epoch: 7 [33792/50176]	Loss: 2.0405
Training Epoch: 7 [34048/50176]	Loss: 2.0046
Training Epoch: 7 [34304/50176]	Loss: 1.7778
Training Epoch: 7 [34560/50176]	Loss: 1.8861
Training Epoch: 7 [34816/50176]	Loss: 2.1363
Training Epoch: 7 [35072/50176]	Loss: 2.0429
Training Epoch: 7 [35328/50176]	Loss: 2.1009
Training Epoch: 7 [35584/50176]	Loss: 1.9898
Training Epoch: 7 [35840/50176]	Loss: 1.9721
Training Epoch: 7 [36096/50176]	Loss: 2.0992
Training Epoch: 7 [36352/50176]	Loss: 2.0662
Training Epoch: 7 [36608/50176]	Loss: 2.3395
Training Epoch: 7 [36864/50176]	Loss: 1.9984
Training Epoch: 7 [37120/50176]	Loss: 2.0696
Training Epoch: 7 [37376/50176]	Loss: 2.0416
Training Epoch: 7 [37632/50176]	Loss: 2.2860
Training Epoch: 7 [37888/50176]	Loss: 2.0795
Training Epoch: 7 [38144/50176]	Loss: 2.0519
Training Epoch: 7 [38400/50176]	Loss: 2.2965
Training Epoch: 7 [38656/50176]	Loss: 2.2293
Training Epoch: 7 [38912/50176]	Loss: 1.9435
Training Epoch: 7 [39168/50176]	Loss: 2.0968
Training Epoch: 7 [39424/50176]	Loss: 2.2089
Training Epoch: 7 [39680/50176]	Loss: 2.1622
Training Epoch: 7 [39936/50176]	Loss: 2.1759
Training Epoch: 7 [40192/50176]	Loss: 1.9705
Training Epoch: 7 [40448/50176]	Loss: 2.0947
Training Epoch: 7 [40704/50176]	Loss: 1.9818
Training Epoch: 7 [40960/50176]	Loss: 2.1553
Training Epoch: 7 [41216/50176]	Loss: 2.2378
Training Epoch: 7 [41472/50176]	Loss: 1.8778
Training Epoch: 7 [41728/50176]	Loss: 2.0454
Training Epoch: 7 [41984/50176]	Loss: 1.8678
Training Epoch: 7 [42240/50176]	Loss: 2.0655
Training Epoch: 7 [42496/50176]	Loss: 1.9532
Training Epoch: 7 [42752/50176]	Loss: 1.9700
Training Epoch: 7 [43008/50176]	Loss: 1.9142
Training Epoch: 7 [43264/50176]	Loss: 1.9662
Training Epoch: 7 [43520/50176]	Loss: 1.9868
Training Epoch: 7 [43776/50176]	Loss: 1.9067
Training Epoch: 7 [44032/50176]	Loss: 2.1225
Training Epoch: 7 [44288/50176]	Loss: 2.0108
Training Epoch: 7 [44544/50176]	Loss: 2.0841
Training Epoch: 7 [44800/50176]	Loss: 1.7620
Training Epoch: 7 [45056/50176]	Loss: 2.2974
Training Epoch: 7 [45312/50176]	Loss: 1.8234
Training Epoch: 7 [45568/50176]	Loss: 2.1350
Training Epoch: 7 [45824/50176]	Loss: 2.1317
Training Epoch: 7 [46080/50176]	Loss: 2.1008
Training Epoch: 7 [46336/50176]	Loss: 1.8110
Training Epoch: 7 [46592/50176]	Loss: 1.9891
Training Epoch: 7 [46848/50176]	Loss: 1.9535
Training Epoch: 7 [47104/50176]	Loss: 2.1083
Training Epoch: 7 [47360/50176]	Loss: 2.3361
Training Epoch: 7 [47616/50176]	Loss: 2.0240
Training Epoch: 7 [47872/50176]	Loss: 2.0318
Training Epoch: 7 [48128/50176]	Loss: 2.0811
Training Epoch: 7 [48384/50176]	Loss: 1.9516
Training Epoch: 7 [48640/50176]	Loss: 2.0071
Training Epoch: 7 [48896/50176]	Loss: 1.9354
Training Epoch: 7 [49152/50176]	Loss: 1.9091
Training Epoch: 7 [49408/50176]	Loss: 2.1437
Training Epoch: 7 [49664/50176]	Loss: 2.2053
Training Epoch: 7 [49920/50176]	Loss: 2.1411
Training Epoch: 7 [50176/50176]	Loss: 2.2600
Validation Epoch: 7, Average loss: 0.0084, Accuracy: 0.4290
Training Epoch: 8 [256/50176]	Loss: 1.9768
Training Epoch: 8 [512/50176]	Loss: 2.1533
Training Epoch: 8 [768/50176]	Loss: 2.0041
Training Epoch: 8 [1024/50176]	Loss: 2.0408
Training Epoch: 8 [1280/50176]	Loss: 2.1328
Training Epoch: 8 [1536/50176]	Loss: 1.9141
Training Epoch: 8 [1792/50176]	Loss: 2.2068
Training Epoch: 8 [2048/50176]	Loss: 1.8634
Training Epoch: 8 [2304/50176]	Loss: 1.8523
Training Epoch: 8 [2560/50176]	Loss: 1.9641
Training Epoch: 8 [2816/50176]	Loss: 1.8901
Training Epoch: 8 [3072/50176]	Loss: 1.9994
Training Epoch: 8 [3328/50176]	Loss: 1.7126
Training Epoch: 8 [3584/50176]	Loss: 1.8833
Training Epoch: 8 [3840/50176]	Loss: 1.9543
Training Epoch: 8 [4096/50176]	Loss: 1.9405
Training Epoch: 8 [4352/50176]	Loss: 2.0008
Training Epoch: 8 [4608/50176]	Loss: 2.0338
Training Epoch: 8 [4864/50176]	Loss: 1.9047
Training Epoch: 8 [5120/50176]	Loss: 2.0419
Training Epoch: 8 [5376/50176]	Loss: 1.9771
Training Epoch: 8 [5632/50176]	Loss: 1.9852
Training Epoch: 8 [5888/50176]	Loss: 1.9805
Training Epoch: 8 [6144/50176]	Loss: 1.7918
Training Epoch: 8 [6400/50176]	Loss: 1.8752
Training Epoch: 8 [6656/50176]	Loss: 2.0686
Training Epoch: 8 [6912/50176]	Loss: 1.7812
Training Epoch: 8 [7168/50176]	Loss: 1.9980
Training Epoch: 8 [7424/50176]	Loss: 1.9639
Training Epoch: 8 [7680/50176]	Loss: 1.8117
Training Epoch: 8 [7936/50176]	Loss: 1.8698
Training Epoch: 8 [8192/50176]	Loss: 1.9452
Training Epoch: 8 [8448/50176]	Loss: 1.9322
Training Epoch: 8 [8704/50176]	Loss: 1.7197
Training Epoch: 8 [8960/50176]	Loss: 1.9774
Training Epoch: 8 [9216/50176]	Loss: 1.9330
Training Epoch: 8 [9472/50176]	Loss: 2.0714
Training Epoch: 8 [9728/50176]	Loss: 2.0923
Training Epoch: 8 [9984/50176]	Loss: 1.9994
Training Epoch: 8 [10240/50176]	Loss: 1.8445
Training Epoch: 8 [10496/50176]	Loss: 2.1992
Training Epoch: 8 [10752/50176]	Loss: 1.9130
Training Epoch: 8 [11008/50176]	Loss: 1.9122
Training Epoch: 8 [11264/50176]	Loss: 2.0594
Training Epoch: 8 [11520/50176]	Loss: 1.7910
Training Epoch: 8 [11776/50176]	Loss: 2.0528
Training Epoch: 8 [12032/50176]	Loss: 1.9042
Training Epoch: 8 [12288/50176]	Loss: 1.8262
Training Epoch: 8 [12544/50176]	Loss: 1.9878
Training Epoch: 8 [12800/50176]	Loss: 2.0381
Training Epoch: 8 [13056/50176]	Loss: 1.7433
Training Epoch: 8 [13312/50176]	Loss: 2.1122
Training Epoch: 8 [13568/50176]	Loss: 2.0278
Training Epoch: 8 [13824/50176]	Loss: 2.1754
Training Epoch: 8 [14080/50176]	Loss: 2.1973
Training Epoch: 8 [14336/50176]	Loss: 1.8433
Training Epoch: 8 [14592/50176]	Loss: 1.9176
Training Epoch: 8 [14848/50176]	Loss: 2.0003
Training Epoch: 8 [15104/50176]	Loss: 2.0445
Training Epoch: 8 [15360/50176]	Loss: 2.0299
Training Epoch: 8 [15616/50176]	Loss: 1.9957
Training Epoch: 8 [15872/50176]	Loss: 2.0790
Training Epoch: 8 [16128/50176]	Loss: 2.0135
Training Epoch: 8 [16384/50176]	Loss: 1.8018
Training Epoch: 8 [16640/50176]	Loss: 1.9084
Training Epoch: 8 [16896/50176]	Loss: 2.0119
Training Epoch: 8 [17152/50176]	Loss: 2.1149
Training Epoch: 8 [17408/50176]	Loss: 1.8176
Training Epoch: 8 [17664/50176]	Loss: 1.7789
Training Epoch: 8 [17920/50176]	Loss: 2.0765
Training Epoch: 8 [18176/50176]	Loss: 1.8260
Training Epoch: 8 [18432/50176]	Loss: 1.9996
Training Epoch: 8 [18688/50176]	Loss: 2.1003
Training Epoch: 8 [18944/50176]	Loss: 1.8818
Training Epoch: 8 [19200/50176]	Loss: 1.9089
Training Epoch: 8 [19456/50176]	Loss: 1.8797
Training Epoch: 8 [19712/50176]	Loss: 1.9921
Training Epoch: 8 [19968/50176]	Loss: 1.9032
Training Epoch: 8 [20224/50176]	Loss: 2.0285
Training Epoch: 8 [20480/50176]	Loss: 1.9925
Training Epoch: 8 [20736/50176]	Loss: 1.8326
Training Epoch: 8 [20992/50176]	Loss: 1.8759
Training Epoch: 8 [21248/50176]	Loss: 2.0198
Training Epoch: 8 [21504/50176]	Loss: 2.0040
Training Epoch: 8 [21760/50176]	Loss: 1.9071
Training Epoch: 8 [22016/50176]	Loss: 1.9525
Training Epoch: 8 [22272/50176]	Loss: 2.0144
Training Epoch: 8 [22528/50176]	Loss: 1.9562
Training Epoch: 8 [22784/50176]	Loss: 1.8530
Training Epoch: 8 [23040/50176]	Loss: 1.9579
Training Epoch: 8 [23296/50176]	Loss: 1.8671
Training Epoch: 8 [23552/50176]	Loss: 2.0531
Training Epoch: 8 [23808/50176]	Loss: 1.9676
Training Epoch: 8 [24064/50176]	Loss: 1.8764
Training Epoch: 8 [24320/50176]	Loss: 1.9942
Training Epoch: 8 [24576/50176]	Loss: 1.9775
Training Epoch: 8 [24832/50176]	Loss: 1.9640
Training Epoch: 8 [25088/50176]	Loss: 1.8405
Training Epoch: 8 [25344/50176]	Loss: 1.9257
Training Epoch: 8 [25600/50176]	Loss: 2.0521
Training Epoch: 8 [25856/50176]	Loss: 2.0244
Training Epoch: 8 [26112/50176]	Loss: 1.8948
Training Epoch: 8 [26368/50176]	Loss: 2.0104
Training Epoch: 8 [26624/50176]	Loss: 1.8637
Training Epoch: 8 [26880/50176]	Loss: 2.0569
Training Epoch: 8 [27136/50176]	Loss: 1.8915
Training Epoch: 8 [27392/50176]	Loss: 1.9475
Training Epoch: 8 [27648/50176]	Loss: 1.9825
Training Epoch: 8 [27904/50176]	Loss: 1.9912
Training Epoch: 8 [28160/50176]	Loss: 2.0379
Training Epoch: 8 [28416/50176]	Loss: 1.9826
Training Epoch: 8 [28672/50176]	Loss: 1.8431
Training Epoch: 8 [28928/50176]	Loss: 1.8444
Training Epoch: 8 [29184/50176]	Loss: 1.8018
Training Epoch: 8 [29440/50176]	Loss: 2.0506
Training Epoch: 8 [29696/50176]	Loss: 2.1072
Training Epoch: 8 [29952/50176]	Loss: 1.8734
Training Epoch: 8 [30208/50176]	Loss: 1.9643
Training Epoch: 8 [30464/50176]	Loss: 1.9178
Training Epoch: 8 [30720/50176]	Loss: 1.9033
Training Epoch: 8 [30976/50176]	Loss: 1.9786
Training Epoch: 8 [31232/50176]	Loss: 1.9182
Training Epoch: 8 [31488/50176]	Loss: 1.8938
Training Epoch: 8 [31744/50176]	Loss: 2.0003
Training Epoch: 8 [32000/50176]	Loss: 2.0784
Training Epoch: 8 [32256/50176]	Loss: 2.0173
Training Epoch: 8 [32512/50176]	Loss: 1.9981
Training Epoch: 8 [32768/50176]	Loss: 2.0706
Training Epoch: 8 [33024/50176]	Loss: 1.9693
Training Epoch: 8 [33280/50176]	Loss: 1.9066
Training Epoch: 8 [33536/50176]	Loss: 1.9081
Training Epoch: 8 [33792/50176]	Loss: 1.9423
Training Epoch: 8 [34048/50176]	Loss: 1.9637
Training Epoch: 8 [34304/50176]	Loss: 1.9772
Training Epoch: 8 [34560/50176]	Loss: 1.9592
Training Epoch: 8 [34816/50176]	Loss: 1.9712
Training Epoch: 8 [35072/50176]	Loss: 2.0318
Training Epoch: 8 [35328/50176]	Loss: 2.0482
Training Epoch: 8 [35584/50176]	Loss: 1.9697
Training Epoch: 8 [35840/50176]	Loss: 2.0259
Training Epoch: 8 [36096/50176]	Loss: 1.7943
Training Epoch: 8 [36352/50176]	Loss: 1.9045
Training Epoch: 8 [36608/50176]	Loss: 1.9357
Training Epoch: 8 [36864/50176]	Loss: 2.0032
Training Epoch: 8 [37120/50176]	Loss: 1.9448
Training Epoch: 8 [37376/50176]	Loss: 2.2051
Training Epoch: 8 [37632/50176]	Loss: 2.0327
Training Epoch: 8 [37888/50176]	Loss: 1.9566
Training Epoch: 8 [38144/50176]	Loss: 2.0604
Training Epoch: 8 [38400/50176]	Loss: 1.8970
Training Epoch: 8 [38656/50176]	Loss: 1.8849
Training Epoch: 8 [38912/50176]	Loss: 1.8900
Training Epoch: 8 [39168/50176]	Loss: 1.9276
Training Epoch: 8 [39424/50176]	Loss: 1.7741
Training Epoch: 8 [39680/50176]	Loss: 1.9919
Training Epoch: 8 [39936/50176]	Loss: 2.1008
Training Epoch: 8 [40192/50176]	Loss: 2.0139
Training Epoch: 8 [40448/50176]	Loss: 1.8347
Training Epoch: 8 [40704/50176]	Loss: 1.9467
Training Epoch: 8 [40960/50176]	Loss: 1.7523
Training Epoch: 8 [41216/50176]	Loss: 1.9378
Training Epoch: 8 [41472/50176]	Loss: 1.8729
Training Epoch: 8 [41728/50176]	Loss: 1.8660
Training Epoch: 8 [41984/50176]	Loss: 1.7947
Training Epoch: 8 [42240/50176]	Loss: 1.9696
Training Epoch: 8 [42496/50176]	Loss: 1.9639
Training Epoch: 8 [42752/50176]	Loss: 1.8722
Training Epoch: 8 [43008/50176]	Loss: 1.8357
Training Epoch: 8 [43264/50176]	Loss: 1.8870
Training Epoch: 8 [43520/50176]	Loss: 1.9059
Training Epoch: 8 [43776/50176]	Loss: 1.8770
Training Epoch: 8 [44032/50176]	Loss: 1.9538
Training Epoch: 8 [44288/50176]	Loss: 1.8837
Training Epoch: 8 [44544/50176]	Loss: 2.1770
Training Epoch: 8 [44800/50176]	Loss: 1.8527
Training Epoch: 8 [45056/50176]	Loss: 1.7096
Training Epoch: 8 [45312/50176]	Loss: 1.9425
Training Epoch: 8 [45568/50176]	Loss: 1.8942
Training Epoch: 8 [45824/50176]	Loss: 1.8999
Training Epoch: 8 [46080/50176]	Loss: 1.9206
Training Epoch: 8 [46336/50176]	Loss: 1.8442
Training Epoch: 8 [46592/50176]	Loss: 1.9976
Training Epoch: 8 [46848/50176]	Loss: 1.9091
Training Epoch: 8 [47104/50176]	Loss: 1.8342
Training Epoch: 8 [47360/50176]	Loss: 2.0544
Training Epoch: 8 [47616/50176]	Loss: 2.0843
Training Epoch: 8 [47872/50176]	Loss: 1.9382
Training Epoch: 8 [48128/50176]	Loss: 1.7784
Training Epoch: 8 [48384/50176]	Loss: 1.7812
Training Epoch: 8 [48640/50176]	Loss: 1.9105
Training Epoch: 8 [48896/50176]	Loss: 1.9830
Training Epoch: 8 [49152/50176]	Loss: 2.0796
Training Epoch: 8 [49408/50176]	Loss: 1.9775
Training Epoch: 8 [49664/50176]	Loss: 1.9803
Training Epoch: 8 [49920/50176]	Loss: 1.9729
Training Epoch: 8 [50176/50176]	Loss: 2.4038
Validation Epoch: 8, Average loss: 0.0077, Accuracy: 0.4581
Training Epoch: 9 [256/50176]	Loss: 2.0003
Training Epoch: 9 [512/50176]	Loss: 1.8588
Training Epoch: 9 [768/50176]	Loss: 1.7670
Training Epoch: 9 [1024/50176]	Loss: 1.8359
Training Epoch: 9 [1280/50176]	Loss: 1.7304
Training Epoch: 9 [1536/50176]	Loss: 1.8390
Training Epoch: 9 [1792/50176]	Loss: 1.6056
Training Epoch: 9 [2048/50176]	Loss: 1.8718
Training Epoch: 9 [2304/50176]	Loss: 1.7362
Training Epoch: 9 [2560/50176]	Loss: 1.9003
Training Epoch: 9 [2816/50176]	Loss: 1.7793
Training Epoch: 9 [3072/50176]	Loss: 1.8953
Training Epoch: 9 [3328/50176]	Loss: 1.9321
Training Epoch: 9 [3584/50176]	Loss: 1.9071
Training Epoch: 9 [3840/50176]	Loss: 2.0496
Training Epoch: 9 [4096/50176]	Loss: 1.7870
Training Epoch: 9 [4352/50176]	Loss: 2.0334
Training Epoch: 9 [4608/50176]	Loss: 1.8967
Training Epoch: 9 [4864/50176]	Loss: 1.7632
Training Epoch: 9 [5120/50176]	Loss: 1.9285
Training Epoch: 9 [5376/50176]	Loss: 1.8211
Training Epoch: 9 [5632/50176]	Loss: 2.0055
Training Epoch: 9 [5888/50176]	Loss: 1.8525
Training Epoch: 9 [6144/50176]	Loss: 1.8593
Training Epoch: 9 [6400/50176]	Loss: 1.8821
Training Epoch: 9 [6656/50176]	Loss: 1.7328
Training Epoch: 9 [6912/50176]	Loss: 1.7358
Training Epoch: 9 [7168/50176]	Loss: 1.8227
Training Epoch: 9 [7424/50176]	Loss: 1.7656
Training Epoch: 9 [7680/50176]	Loss: 1.9403
Training Epoch: 9 [7936/50176]	Loss: 1.8729
Training Epoch: 9 [8192/50176]	Loss: 1.7200
Training Epoch: 9 [8448/50176]	Loss: 1.7338
Training Epoch: 9 [8704/50176]	Loss: 1.6500
Training Epoch: 9 [8960/50176]	Loss: 1.8440
Training Epoch: 9 [9216/50176]	Loss: 1.7857
Training Epoch: 9 [9472/50176]	Loss: 1.7015
Training Epoch: 9 [9728/50176]	Loss: 1.8873
Training Epoch: 9 [9984/50176]	Loss: 1.6362
Training Epoch: 9 [10240/50176]	Loss: 1.8075
Training Epoch: 9 [10496/50176]	Loss: 1.9652
Training Epoch: 9 [10752/50176]	Loss: 1.8162
Training Epoch: 9 [11008/50176]	Loss: 1.9395
Training Epoch: 9 [11264/50176]	Loss: 1.8137
Training Epoch: 9 [11520/50176]	Loss: 1.6885
Training Epoch: 9 [11776/50176]	Loss: 1.7538
Training Epoch: 9 [12032/50176]	Loss: 1.9231
Training Epoch: 9 [12288/50176]	Loss: 1.8408
Training Epoch: 9 [12544/50176]	Loss: 1.7877
Training Epoch: 9 [12800/50176]	Loss: 1.7066
Training Epoch: 9 [13056/50176]	Loss: 1.8439
Training Epoch: 9 [13312/50176]	Loss: 1.7743
Training Epoch: 9 [13568/50176]	Loss: 1.9233
Training Epoch: 9 [13824/50176]	Loss: 1.8138
Training Epoch: 9 [14080/50176]	Loss: 1.6157
Training Epoch: 9 [14336/50176]	Loss: 1.9364
Training Epoch: 9 [14592/50176]	Loss: 1.9186
Training Epoch: 9 [14848/50176]	Loss: 1.9110
Training Epoch: 9 [15104/50176]	Loss: 1.8873
Training Epoch: 9 [15360/50176]	Loss: 2.0759
Training Epoch: 9 [15616/50176]	Loss: 1.8078
Training Epoch: 9 [15872/50176]	Loss: 1.7191
Training Epoch: 9 [16128/50176]	Loss: 1.6048
Training Epoch: 9 [16384/50176]	Loss: 1.8501
Training Epoch: 9 [16640/50176]	Loss: 1.8855
Training Epoch: 9 [16896/50176]	Loss: 1.8597
Training Epoch: 9 [17152/50176]	Loss: 1.8453
Training Epoch: 9 [17408/50176]	Loss: 1.7671
Training Epoch: 9 [17664/50176]	Loss: 1.9418
Training Epoch: 9 [17920/50176]	Loss: 1.8942
Training Epoch: 9 [18176/50176]	Loss: 1.7317
Training Epoch: 9 [18432/50176]	Loss: 1.8137
Training Epoch: 9 [18688/50176]	Loss: 1.6473
Training Epoch: 9 [18944/50176]	Loss: 1.8371
Training Epoch: 9 [19200/50176]	Loss: 1.7641
Training Epoch: 9 [19456/50176]	Loss: 1.8605
Training Epoch: 9 [19712/50176]	Loss: 1.7689
Training Epoch: 9 [19968/50176]	Loss: 1.7841
Training Epoch: 9 [20224/50176]	Loss: 1.7565
Training Epoch: 9 [20480/50176]	Loss: 1.8934
Training Epoch: 9 [20736/50176]	Loss: 1.8574
Training Epoch: 9 [20992/50176]	Loss: 1.8021
Training Epoch: 9 [21248/50176]	Loss: 1.6781
Training Epoch: 9 [21504/50176]	Loss: 1.7352
Training Epoch: 9 [21760/50176]	Loss: 1.5611
Training Epoch: 9 [22016/50176]	Loss: 1.8997
Training Epoch: 9 [22272/50176]	Loss: 1.8904
Training Epoch: 9 [22528/50176]	Loss: 1.8162
Training Epoch: 9 [22784/50176]	Loss: 1.5339
Training Epoch: 9 [23040/50176]	Loss: 1.8290
Training Epoch: 9 [23296/50176]	Loss: 1.8683
Training Epoch: 9 [23552/50176]	Loss: 1.9828
Training Epoch: 9 [23808/50176]	Loss: 1.8802
Training Epoch: 9 [24064/50176]	Loss: 1.8306
Training Epoch: 9 [24320/50176]	Loss: 1.8893
Training Epoch: 9 [24576/50176]	Loss: 1.8423
Training Epoch: 9 [24832/50176]	Loss: 1.6296
Training Epoch: 9 [25088/50176]	Loss: 1.7099
Training Epoch: 9 [25344/50176]	Loss: 1.8299
Training Epoch: 9 [25600/50176]	Loss: 1.7781
Training Epoch: 9 [25856/50176]	Loss: 1.7830
Training Epoch: 9 [26112/50176]	Loss: 1.8834
Training Epoch: 9 [26368/50176]	Loss: 1.7922
Training Epoch: 9 [26624/50176]	Loss: 1.8253
Training Epoch: 9 [26880/50176]	Loss: 1.7049
Training Epoch: 9 [27136/50176]	Loss: 1.7784
Training Epoch: 9 [27392/50176]	Loss: 1.6257
Training Epoch: 9 [27648/50176]	Loss: 1.7336
Training Epoch: 9 [27904/50176]	Loss: 1.9881
Training Epoch: 9 [28160/50176]	Loss: 1.6803
Training Epoch: 9 [28416/50176]	Loss: 1.9709
Training Epoch: 9 [28672/50176]	Loss: 1.8464
Training Epoch: 9 [28928/50176]	Loss: 2.0820
Training Epoch: 9 [29184/50176]	Loss: 1.7975
Training Epoch: 9 [29440/50176]	Loss: 1.8487
Training Epoch: 9 [29696/50176]	Loss: 1.7247
Training Epoch: 9 [29952/50176]	Loss: 1.8702
Training Epoch: 9 [30208/50176]	Loss: 1.9678
Training Epoch: 9 [30464/50176]	Loss: 1.8377
Training Epoch: 9 [30720/50176]	Loss: 1.7865
Training Epoch: 9 [30976/50176]	Loss: 1.9120
Training Epoch: 9 [31232/50176]	Loss: 1.6480
Training Epoch: 9 [31488/50176]	Loss: 1.9835
Training Epoch: 9 [31744/50176]	Loss: 1.9317
Training Epoch: 9 [32000/50176]	Loss: 2.0306
Training Epoch: 9 [32256/50176]	Loss: 1.8365
Training Epoch: 9 [32512/50176]	Loss: 1.8175
Training Epoch: 9 [32768/50176]	Loss: 1.9154
Training Epoch: 9 [33024/50176]	Loss: 1.9524
Training Epoch: 9 [33280/50176]	Loss: 1.9437
Training Epoch: 9 [33536/50176]	Loss: 1.8164
Training Epoch: 9 [33792/50176]	Loss: 1.8148
Training Epoch: 9 [34048/50176]	Loss: 1.8621
Training Epoch: 9 [34304/50176]	Loss: 1.9143
Training Epoch: 9 [34560/50176]	Loss: 1.8558
Training Epoch: 9 [34816/50176]	Loss: 1.9226
Training Epoch: 9 [35072/50176]	Loss: 1.8010
Training Epoch: 9 [35328/50176]	Loss: 1.8575
Training Epoch: 9 [35584/50176]	Loss: 1.7605
Training Epoch: 9 [35840/50176]	Loss: 1.8952
Training Epoch: 9 [36096/50176]	Loss: 1.9163
Training Epoch: 9 [36352/50176]	Loss: 1.8475
Training Epoch: 9 [36608/50176]	Loss: 1.8164
Training Epoch: 9 [36864/50176]	Loss: 1.9737
Training Epoch: 9 [37120/50176]	Loss: 1.5663
Training Epoch: 9 [37376/50176]	Loss: 1.8467
Training Epoch: 9 [37632/50176]	Loss: 1.8769
Training Epoch: 9 [37888/50176]	Loss: 1.8629
Training Epoch: 9 [38144/50176]	Loss: 1.7580
Training Epoch: 9 [38400/50176]	Loss: 1.6867
Training Epoch: 9 [38656/50176]	Loss: 1.7038
Training Epoch: 9 [38912/50176]	Loss: 1.8800
Training Epoch: 9 [39168/50176]	Loss: 1.9207
Training Epoch: 9 [39424/50176]	Loss: 1.6521
Training Epoch: 9 [39680/50176]	Loss: 2.0692
Training Epoch: 9 [39936/50176]	Loss: 2.0095
Training Epoch: 9 [40192/50176]	Loss: 1.6821
Training Epoch: 9 [40448/50176]	Loss: 1.8530
Training Epoch: 9 [40704/50176]	Loss: 1.8103
Training Epoch: 9 [40960/50176]	Loss: 1.7575
Training Epoch: 9 [41216/50176]	Loss: 1.7861
Training Epoch: 9 [41472/50176]	Loss: 1.8138
Training Epoch: 9 [41728/50176]	Loss: 1.7426
Training Epoch: 9 [41984/50176]	Loss: 1.9685
Training Epoch: 9 [42240/50176]	Loss: 1.9294
Training Epoch: 9 [42496/50176]	Loss: 1.8439
Training Epoch: 9 [42752/50176]	Loss: 1.8147
Training Epoch: 9 [43008/50176]	Loss: 1.7770
Training Epoch: 9 [43264/50176]	Loss: 1.7634
Training Epoch: 9 [43520/50176]	Loss: 1.9904
Training Epoch: 9 [43776/50176]	Loss: 1.9882
Training Epoch: 9 [44032/50176]	Loss: 1.7286
Training Epoch: 9 [44288/50176]	Loss: 1.8957
Training Epoch: 9 [44544/50176]	Loss: 1.8855
Training Epoch: 9 [44800/50176]	Loss: 1.9274
Training Epoch: 9 [45056/50176]	Loss: 1.7483
Training Epoch: 9 [45312/50176]	Loss: 1.7335
Training Epoch: 9 [45568/50176]	Loss: 1.7693
Training Epoch: 9 [45824/50176]	Loss: 1.7173
Training Epoch: 9 [46080/50176]	Loss: 1.9494
Training Epoch: 9 [46336/50176]	Loss: 2.0068
Training Epoch: 9 [46592/50176]	Loss: 1.8851
Training Epoch: 9 [46848/50176]	Loss: 1.9275
Training Epoch: 9 [47104/50176]	Loss: 1.7376
Training Epoch: 9 [47360/50176]	Loss: 1.9275
Training Epoch: 9 [47616/50176]	Loss: 1.8251
Training Epoch: 9 [47872/50176]	Loss: 1.8304
Training Epoch: 9 [48128/50176]	Loss: 1.9016
Training Epoch: 9 [48384/50176]	Loss: 1.7281
Training Epoch: 9 [48640/50176]	Loss: 1.7321
Training Epoch: 9 [48896/50176]	Loss: 2.0037
Training Epoch: 9 [49152/50176]	Loss: 1.8126
Training Epoch: 9 [49408/50176]	Loss: 1.9715
Training Epoch: 9 [49664/50176]	Loss: 1.9440
Training Epoch: 9 [49920/50176]	Loss: 1.9083
Training Epoch: 9 [50176/50176]	Loss: 1.9521
Validation Epoch: 9, Average loss: 0.0075, Accuracy: 0.4812
Training Epoch: 10 [256/50176]	Loss: 1.7174
Training Epoch: 10 [512/50176]	Loss: 1.5683
Training Epoch: 10 [768/50176]	Loss: 1.6852
Training Epoch: 10 [1024/50176]	Loss: 1.7622
Training Epoch: 10 [1280/50176]	Loss: 1.6295
Training Epoch: 10 [1536/50176]	Loss: 1.8958
Training Epoch: 10 [1792/50176]	Loss: 1.6759
Training Epoch: 10 [2048/50176]	Loss: 1.5948
Training Epoch: 10 [2304/50176]	Loss: 1.7353
Training Epoch: 10 [2560/50176]	Loss: 1.5695
Training Epoch: 10 [2816/50176]	Loss: 1.8438
Training Epoch: 10 [3072/50176]	Loss: 1.7530
Training Epoch: 10 [3328/50176]	Loss: 1.7093
Training Epoch: 10 [3584/50176]	Loss: 1.8036
Training Epoch: 10 [3840/50176]	Loss: 1.6804
Training Epoch: 10 [4096/50176]	Loss: 1.6305
Training Epoch: 10 [4352/50176]	Loss: 1.5852
Training Epoch: 10 [4608/50176]	Loss: 1.7023
Training Epoch: 10 [4864/50176]	Loss: 1.7933
Training Epoch: 10 [5120/50176]	Loss: 1.9326
Training Epoch: 10 [5376/50176]	Loss: 1.8957
Training Epoch: 10 [5632/50176]	Loss: 1.6777
Training Epoch: 10 [5888/50176]	Loss: 1.7245
Training Epoch: 10 [6144/50176]	Loss: 1.7892
Training Epoch: 10 [6400/50176]	Loss: 1.6643
Training Epoch: 10 [6656/50176]	Loss: 1.7701
Training Epoch: 10 [6912/50176]	Loss: 1.6381
Training Epoch: 10 [7168/50176]	Loss: 1.6747
Training Epoch: 10 [7424/50176]	Loss: 1.5721
Training Epoch: 10 [7680/50176]	Loss: 1.8956
Training Epoch: 10 [7936/50176]	Loss: 1.6964
Training Epoch: 10 [8192/50176]	Loss: 1.8847
Training Epoch: 10 [8448/50176]	Loss: 1.6374
Training Epoch: 10 [8704/50176]	Loss: 1.7555
Training Epoch: 10 [8960/50176]	Loss: 1.7031
Training Epoch: 10 [9216/50176]	Loss: 1.6843
Training Epoch: 10 [9472/50176]	Loss: 1.6997
Training Epoch: 10 [9728/50176]	Loss: 1.8718
Training Epoch: 10 [9984/50176]	Loss: 1.6586
Training Epoch: 10 [10240/50176]	Loss: 1.5345
Training Epoch: 10 [10496/50176]	Loss: 1.7555
Training Epoch: 10 [10752/50176]	Loss: 1.9503
Training Epoch: 10 [11008/50176]	Loss: 1.6468
Training Epoch: 10 [11264/50176]	Loss: 1.7886
Training Epoch: 10 [11520/50176]	Loss: 1.6856
Training Epoch: 10 [11776/50176]	Loss: 1.6271
Training Epoch: 10 [12032/50176]	Loss: 1.8377
Training Epoch: 10 [12288/50176]	Loss: 1.6828
Training Epoch: 10 [12544/50176]	Loss: 1.7257
Training Epoch: 10 [12800/50176]	Loss: 1.7844
Training Epoch: 10 [13056/50176]	Loss: 1.8588
Training Epoch: 10 [13312/50176]	Loss: 1.6254
Training Epoch: 10 [13568/50176]	Loss: 1.8126
Training Epoch: 10 [13824/50176]	Loss: 1.5947
Training Epoch: 10 [14080/50176]	Loss: 1.7540
Training Epoch: 10 [14336/50176]	Loss: 1.7294
Training Epoch: 10 [14592/50176]	Loss: 1.7747
Training Epoch: 10 [14848/50176]	Loss: 1.6215
Training Epoch: 10 [15104/50176]	Loss: 1.4705
Training Epoch: 10 [15360/50176]	Loss: 1.5997
Training Epoch: 10 [15616/50176]	Loss: 1.7507
Training Epoch: 10 [15872/50176]	Loss: 1.4954
Training Epoch: 10 [16128/50176]	Loss: 2.0188
Training Epoch: 10 [16384/50176]	Loss: 1.8656
Training Epoch: 10 [16640/50176]	Loss: 1.6331
Training Epoch: 10 [16896/50176]	Loss: 1.7061
Training Epoch: 10 [17152/50176]	Loss: 1.7830
Training Epoch: 10 [17408/50176]	Loss: 1.5550
Training Epoch: 10 [17664/50176]	Loss: 1.6676
Training Epoch: 10 [17920/50176]	Loss: 1.7108
Training Epoch: 10 [18176/50176]	Loss: 1.8223
Training Epoch: 10 [18432/50176]	Loss: 1.6474
Training Epoch: 10 [18688/50176]	Loss: 1.8291
Training Epoch: 10 [18944/50176]	Loss: 1.6796
Training Epoch: 10 [19200/50176]	Loss: 1.7516
Training Epoch: 10 [19456/50176]	Loss: 1.6636
Training Epoch: 10 [19712/50176]	Loss: 1.6715
Training Epoch: 10 [19968/50176]	Loss: 1.7438
Training Epoch: 10 [20224/50176]	Loss: 1.6240
Training Epoch: 10 [20480/50176]	Loss: 1.7828
Training Epoch: 10 [20736/50176]	Loss: 1.7477
Training Epoch: 10 [20992/50176]	Loss: 1.6294
Training Epoch: 10 [21248/50176]	Loss: 1.7426
Training Epoch: 10 [21504/50176]	Loss: 1.6104
Training Epoch: 10 [21760/50176]	Loss: 1.7500
Training Epoch: 10 [22016/50176]	Loss: 1.8371
Training Epoch: 10 [22272/50176]	Loss: 1.7548
Training Epoch: 10 [22528/50176]	Loss: 1.4325
Training Epoch: 10 [22784/50176]	Loss: 1.6918
Training Epoch: 10 [23040/50176]	Loss: 1.8079
Training Epoch: 10 [23296/50176]	Loss: 2.0028
Training Epoch: 10 [23552/50176]	Loss: 1.7870
Training Epoch: 10 [23808/50176]	Loss: 1.7820
Training Epoch: 10 [24064/50176]	Loss: 1.7113
Training Epoch: 10 [24320/50176]	Loss: 1.6643
Training Epoch: 10 [24576/50176]	Loss: 1.6762
Training Epoch: 10 [24832/50176]	Loss: 1.8330
Training Epoch: 10 [25088/50176]	Loss: 1.7370
Training Epoch: 10 [25344/50176]	Loss: 1.7447
Training Epoch: 10 [25600/50176]	Loss: 1.7469
Training Epoch: 10 [25856/50176]	Loss: 1.7305
Training Epoch: 10 [26112/50176]	Loss: 1.5188
Training Epoch: 10 [26368/50176]	Loss: 1.8303
Training Epoch: 10 [26624/50176]	Loss: 1.6620
Training Epoch: 10 [26880/50176]	Loss: 1.6520
Training Epoch: 10 [27136/50176]	Loss: 1.7673
Training Epoch: 10 [27392/50176]	Loss: 1.6649
Training Epoch: 10 [27648/50176]	Loss: 1.8608
Training Epoch: 10 [27904/50176]	Loss: 1.7157
Training Epoch: 10 [28160/50176]	Loss: 1.6950
Training Epoch: 10 [28416/50176]	Loss: 1.6940
Training Epoch: 10 [28672/50176]	Loss: 1.9306
Training Epoch: 10 [28928/50176]	Loss: 1.7877
Training Epoch: 10 [29184/50176]	Loss: 1.7323
Training Epoch: 10 [29440/50176]	Loss: 1.6603
Training Epoch: 10 [29696/50176]	Loss: 1.8750
Training Epoch: 10 [29952/50176]	Loss: 1.7735
Training Epoch: 10 [30208/50176]	Loss: 1.6694
Training Epoch: 10 [30464/50176]	Loss: 1.6017
Training Epoch: 10 [30720/50176]	Loss: 1.7758
Training Epoch: 10 [30976/50176]	Loss: 1.9428
Training Epoch: 10 [31232/50176]	Loss: 1.6955
Training Epoch: 10 [31488/50176]	Loss: 1.6424
Training Epoch: 10 [31744/50176]	Loss: 1.9235
Training Epoch: 10 [32000/50176]	Loss: 1.6662
Training Epoch: 10 [32256/50176]	Loss: 1.7787
Training Epoch: 10 [32512/50176]	Loss: 1.6312
Training Epoch: 10 [32768/50176]	Loss: 1.6192
Training Epoch: 10 [33024/50176]	Loss: 1.7346
Training Epoch: 10 [33280/50176]	Loss: 1.8669
Training Epoch: 10 [33536/50176]	Loss: 1.6883
Training Epoch: 10 [33792/50176]	Loss: 1.6579
Training Epoch: 10 [34048/50176]	Loss: 1.7557
Training Epoch: 10 [34304/50176]	Loss: 1.7493
Training Epoch: 10 [34560/50176]	Loss: 1.5244
Training Epoch: 10 [34816/50176]	Loss: 1.7935
Training Epoch: 10 [35072/50176]	Loss: 1.8897
Training Epoch: 10 [35328/50176]	Loss: 1.7073
Training Epoch: 10 [35584/50176]	Loss: 1.7486
Training Epoch: 10 [35840/50176]	Loss: 1.6720
Training Epoch: 10 [36096/50176]	Loss: 1.7601
Training Epoch: 10 [36352/50176]	Loss: 1.7322
Training Epoch: 10 [36608/50176]	Loss: 1.7665
Training Epoch: 10 [36864/50176]	Loss: 1.6349
Training Epoch: 10 [37120/50176]	Loss: 1.8613
Training Epoch: 10 [37376/50176]	Loss: 1.7293
Training Epoch: 10 [37632/50176]	Loss: 1.7208
Training Epoch: 10 [37888/50176]	Loss: 1.7117
Training Epoch: 10 [38144/50176]	Loss: 1.7477
Training Epoch: 10 [38400/50176]	Loss: 1.8954
Training Epoch: 10 [38656/50176]	Loss: 1.8123
Training Epoch: 10 [38912/50176]	Loss: 1.6114
Training Epoch: 10 [39168/50176]	Loss: 1.7650
Training Epoch: 10 [39424/50176]	Loss: 1.6496
Training Epoch: 10 [39680/50176]	Loss: 1.5880
Training Epoch: 10 [39936/50176]	Loss: 1.7568
Training Epoch: 10 [40192/50176]	Loss: 1.8913
Training Epoch: 10 [40448/50176]	Loss: 1.6057
Training Epoch: 10 [40704/50176]	Loss: 1.7969
Training Epoch: 10 [40960/50176]	Loss: 1.7034
Training Epoch: 10 [41216/50176]	Loss: 1.5808
Training Epoch: 10 [41472/50176]	Loss: 1.8784
Training Epoch: 10 [41728/50176]	Loss: 1.8322
Training Epoch: 10 [41984/50176]	Loss: 1.7519
Training Epoch: 10 [42240/50176]	Loss: 1.6050
Training Epoch: 10 [42496/50176]	Loss: 1.7869
Training Epoch: 10 [42752/50176]	Loss: 1.7397
Training Epoch: 10 [43008/50176]	Loss: 1.6131
Training Epoch: 10 [43264/50176]	Loss: 1.7992
Training Epoch: 10 [43520/50176]	Loss: 1.5970
Training Epoch: 10 [43776/50176]	Loss: 1.4975
Training Epoch: 10 [44032/50176]	Loss: 1.6946
Training Epoch: 10 [44288/50176]	Loss: 1.6076
Training Epoch: 10 [44544/50176]	Loss: 1.8367
Training Epoch: 10 [44800/50176]	Loss: 1.8725
Training Epoch: 10 [45056/50176]	Loss: 1.7791
Training Epoch: 10 [45312/50176]	Loss: 1.8248
Training Epoch: 10 [45568/50176]	Loss: 1.7316
Training Epoch: 10 [45824/50176]	Loss: 1.7938
Training Epoch: 10 [46080/50176]	Loss: 1.7693
Training Epoch: 10 [46336/50176]	Loss: 1.7389
Training Epoch: 10 [46592/50176]	Loss: 1.7516
Training Epoch: 10 [46848/50176]	Loss: 1.8986
Training Epoch: 10 [47104/50176]	Loss: 1.5531
Training Epoch: 10 [47360/50176]	Loss: 1.7191
Training Epoch: 10 [47616/50176]	Loss: 1.7644
Training Epoch: 10 [47872/50176]	Loss: 1.7393
Training Epoch: 10 [48128/50176]	Loss: 1.7423
Training Epoch: 10 [48384/50176]	Loss: 1.5244
Training Epoch: 10 [48640/50176]	Loss: 1.8102
Training Epoch: 10 [48896/50176]	Loss: 1.8870
Training Epoch: 10 [49152/50176]	Loss: 1.6412
Training Epoch: 10 [49408/50176]	Loss: 1.7653
Training Epoch: 10 [49664/50176]	Loss: 1.8995
Training Epoch: 10 [49920/50176]	Loss: 1.7950
Training Epoch: 10 [50176/50176]	Loss: 2.0887
Validation Epoch: 10, Average loss: 0.0071, Accuracy: 0.4962
Training Epoch: 11 [256/50176]	Loss: 1.6483
Training Epoch: 11 [512/50176]	Loss: 1.6607
Training Epoch: 11 [768/50176]	Loss: 1.5872
Training Epoch: 11 [1024/50176]	Loss: 1.6041
Training Epoch: 11 [1280/50176]	Loss: 1.5780
Training Epoch: 11 [1536/50176]	Loss: 1.6581
Training Epoch: 11 [1792/50176]	Loss: 1.6071
Training Epoch: 11 [2048/50176]	Loss: 1.6751
Training Epoch: 11 [2304/50176]	Loss: 1.7170
Training Epoch: 11 [2560/50176]	Loss: 1.5193
Training Epoch: 11 [2816/50176]	Loss: 1.5517
Training Epoch: 11 [3072/50176]	Loss: 1.4278
Training Epoch: 11 [3328/50176]	Loss: 1.5810
Training Epoch: 11 [3584/50176]	Loss: 1.5807
Training Epoch: 11 [3840/50176]	Loss: 1.6575
Training Epoch: 11 [4096/50176]	Loss: 1.7379
Training Epoch: 11 [4352/50176]	Loss: 1.6904
Training Epoch: 11 [4608/50176]	Loss: 1.6618
Training Epoch: 11 [4864/50176]	Loss: 1.5890
Training Epoch: 11 [5120/50176]	Loss: 1.6864
Training Epoch: 11 [5376/50176]	Loss: 1.8348
Training Epoch: 11 [5632/50176]	Loss: 1.5541
Training Epoch: 11 [5888/50176]	Loss: 1.7926
Training Epoch: 11 [6144/50176]	Loss: 1.5662
Training Epoch: 11 [6400/50176]	Loss: 1.4780
Training Epoch: 11 [6656/50176]	Loss: 1.5212
Training Epoch: 11 [6912/50176]	Loss: 1.6433
Training Epoch: 11 [7168/50176]	Loss: 1.4828
Training Epoch: 11 [7424/50176]	Loss: 1.5149
Training Epoch: 11 [7680/50176]	Loss: 1.5247
Training Epoch: 11 [7936/50176]	Loss: 1.5429
Training Epoch: 11 [8192/50176]	Loss: 1.4637
Training Epoch: 11 [8448/50176]	Loss: 1.6131
Training Epoch: 11 [8704/50176]	Loss: 1.5176
Training Epoch: 11 [8960/50176]	Loss: 1.7892
Training Epoch: 11 [9216/50176]	Loss: 1.6490
Training Epoch: 11 [9472/50176]	Loss: 1.5390
Training Epoch: 11 [9728/50176]	Loss: 1.5626
Training Epoch: 11 [9984/50176]	Loss: 1.7029
Training Epoch: 11 [10240/50176]	Loss: 1.8380
Training Epoch: 11 [10496/50176]	Loss: 1.6269
Training Epoch: 11 [10752/50176]	Loss: 1.5278
Training Epoch: 11 [11008/50176]	Loss: 1.5686
Training Epoch: 11 [11264/50176]	Loss: 1.6458
Training Epoch: 11 [11520/50176]	Loss: 1.6362
Training Epoch: 11 [11776/50176]	Loss: 1.8475
Training Epoch: 11 [12032/50176]	Loss: 1.6818
Training Epoch: 11 [12288/50176]	Loss: 1.7874
Training Epoch: 11 [12544/50176]	Loss: 1.4867
Training Epoch: 11 [12800/50176]	Loss: 1.6698
Training Epoch: 11 [13056/50176]	Loss: 1.6660
Training Epoch: 11 [13312/50176]	Loss: 1.6699
Training Epoch: 11 [13568/50176]	Loss: 1.8008
Training Epoch: 11 [13824/50176]	Loss: 1.5807
Training Epoch: 11 [14080/50176]	Loss: 1.5683
Training Epoch: 11 [14336/50176]	Loss: 1.4720
Training Epoch: 11 [14592/50176]	Loss: 1.6569
Training Epoch: 11 [14848/50176]	Loss: 1.6572
Training Epoch: 11 [15104/50176]	Loss: 1.6807
Training Epoch: 11 [15360/50176]	Loss: 1.6961
Training Epoch: 11 [15616/50176]	Loss: 1.6634
Training Epoch: 11 [15872/50176]	Loss: 1.8101
Training Epoch: 11 [16128/50176]	Loss: 1.6793
Training Epoch: 11 [16384/50176]	Loss: 1.5449
Training Epoch: 11 [16640/50176]	Loss: 1.4939
Training Epoch: 11 [16896/50176]	Loss: 1.5433
Training Epoch: 11 [17152/50176]	Loss: 1.7748
Training Epoch: 11 [17408/50176]	Loss: 1.8003
Training Epoch: 11 [17664/50176]	Loss: 1.5568
Training Epoch: 11 [17920/50176]	Loss: 1.6756
Training Epoch: 11 [18176/50176]	Loss: 1.5075
Training Epoch: 11 [18432/50176]	Loss: 1.7886
Training Epoch: 11 [18688/50176]	Loss: 1.7806
Training Epoch: 11 [18944/50176]	Loss: 1.7374
Training Epoch: 11 [19200/50176]	Loss: 1.7599
Training Epoch: 11 [19456/50176]	Loss: 1.7229
Training Epoch: 11 [19712/50176]	Loss: 1.6994
Training Epoch: 11 [19968/50176]	Loss: 1.5890
Training Epoch: 11 [20224/50176]	Loss: 1.5870
Training Epoch: 11 [20480/50176]	Loss: 1.6079
Training Epoch: 11 [20736/50176]	Loss: 1.6243
Training Epoch: 11 [20992/50176]	Loss: 1.6484
Training Epoch: 11 [21248/50176]	Loss: 1.5301
Training Epoch: 11 [21504/50176]	Loss: 1.7180
Training Epoch: 11 [21760/50176]	Loss: 1.5812
Training Epoch: 11 [22016/50176]	Loss: 1.8094
Training Epoch: 11 [22272/50176]	Loss: 1.6499
Training Epoch: 11 [22528/50176]	Loss: 1.5083
Training Epoch: 11 [22784/50176]	Loss: 1.7317
Training Epoch: 11 [23040/50176]	Loss: 1.6591
Training Epoch: 11 [23296/50176]	Loss: 1.5381
Training Epoch: 11 [23552/50176]	Loss: 1.5468
Training Epoch: 11 [23808/50176]	Loss: 1.7619
Training Epoch: 11 [24064/50176]	Loss: 1.5695
Training Epoch: 11 [24320/50176]	Loss: 1.6389
Training Epoch: 11 [24576/50176]	Loss: 1.6027
Training Epoch: 11 [24832/50176]	Loss: 1.7717
Training Epoch: 11 [25088/50176]	Loss: 1.8461
Training Epoch: 11 [25344/50176]	Loss: 1.7157
Training Epoch: 11 [25600/50176]	Loss: 1.3732
Training Epoch: 11 [25856/50176]	Loss: 1.8381
Training Epoch: 11 [26112/50176]	Loss: 1.6503
Training Epoch: 11 [26368/50176]	Loss: 1.6976
Training Epoch: 11 [26624/50176]	Loss: 1.8562
Training Epoch: 11 [26880/50176]	Loss: 1.6201
Training Epoch: 11 [27136/50176]	Loss: 1.6697
Training Epoch: 11 [27392/50176]	Loss: 1.5384
Training Epoch: 11 [27648/50176]	Loss: 1.5661
Training Epoch: 11 [27904/50176]	Loss: 1.5605
Training Epoch: 11 [28160/50176]	Loss: 1.7521
Training Epoch: 11 [28416/50176]	Loss: 1.6854
Training Epoch: 11 [28672/50176]	Loss: 1.5891
Training Epoch: 11 [28928/50176]	Loss: 1.6367
Training Epoch: 11 [29184/50176]	Loss: 1.5114
Training Epoch: 11 [29440/50176]	Loss: 1.7060
Training Epoch: 11 [29696/50176]	Loss: 1.5796
Training Epoch: 11 [29952/50176]	Loss: 1.7653
Training Epoch: 11 [30208/50176]	Loss: 1.6654
Training Epoch: 11 [30464/50176]	Loss: 1.5992
Training Epoch: 11 [30720/50176]	Loss: 1.6352
Training Epoch: 11 [30976/50176]	Loss: 1.5118
Training Epoch: 11 [31232/50176]	Loss: 1.7016
Training Epoch: 11 [31488/50176]	Loss: 1.6794
Training Epoch: 11 [31744/50176]	Loss: 1.5069
Training Epoch: 11 [32000/50176]	Loss: 1.5721
Training Epoch: 11 [32256/50176]	Loss: 1.8132
Training Epoch: 11 [32512/50176]	Loss: 1.6166
Training Epoch: 11 [32768/50176]	Loss: 1.5629
Training Epoch: 11 [33024/50176]	Loss: 1.6669
Training Epoch: 11 [33280/50176]	Loss: 1.7049
Training Epoch: 11 [33536/50176]	Loss: 1.7857
Training Epoch: 11 [33792/50176]	Loss: 1.5684
Training Epoch: 11 [34048/50176]	Loss: 1.5344
Training Epoch: 11 [34304/50176]	Loss: 1.6385
Training Epoch: 11 [34560/50176]	Loss: 1.6678
Training Epoch: 11 [34816/50176]	Loss: 1.7032
Training Epoch: 11 [35072/50176]	Loss: 1.5585
Training Epoch: 11 [35328/50176]	Loss: 1.5943
Training Epoch: 11 [35584/50176]	Loss: 1.6976
Training Epoch: 11 [35840/50176]	Loss: 1.6227
Training Epoch: 11 [36096/50176]	Loss: 1.5891
Training Epoch: 11 [36352/50176]	Loss: 1.6702
Training Epoch: 11 [36608/50176]	Loss: 1.6169
Training Epoch: 11 [36864/50176]	Loss: 1.4478
Training Epoch: 11 [37120/50176]	Loss: 1.8178
Training Epoch: 11 [37376/50176]	Loss: 1.7602
Training Epoch: 11 [37632/50176]	Loss: 1.5653
Training Epoch: 11 [37888/50176]	Loss: 1.6334
Training Epoch: 11 [38144/50176]	Loss: 1.6708
Training Epoch: 11 [38400/50176]	Loss: 1.4760
Training Epoch: 11 [38656/50176]	Loss: 1.6567
Training Epoch: 11 [38912/50176]	Loss: 1.7458
Training Epoch: 11 [39168/50176]	Loss: 1.6195
Training Epoch: 11 [39424/50176]	Loss: 1.5955
Training Epoch: 11 [39680/50176]	Loss: 1.7939
Training Epoch: 11 [39936/50176]	Loss: 1.6469
Training Epoch: 11 [40192/50176]	Loss: 1.7228
Training Epoch: 11 [40448/50176]	Loss: 1.6003
Training Epoch: 11 [40704/50176]	Loss: 1.5738
Training Epoch: 11 [40960/50176]	Loss: 1.5525
Training Epoch: 11 [41216/50176]	Loss: 1.6533
Training Epoch: 11 [41472/50176]	Loss: 1.7160
Training Epoch: 11 [41728/50176]	Loss: 1.6651
Training Epoch: 11 [41984/50176]	Loss: 1.7572
Training Epoch: 11 [42240/50176]	Loss: 1.6890
Training Epoch: 11 [42496/50176]	Loss: 1.7146
Training Epoch: 11 [42752/50176]	Loss: 1.4988
Training Epoch: 11 [43008/50176]	Loss: 1.5729
Training Epoch: 11 [43264/50176]	Loss: 1.5287
Training Epoch: 11 [43520/50176]	Loss: 1.6660
Training Epoch: 11 [43776/50176]	Loss: 1.5106
Training Epoch: 11 [44032/50176]	Loss: 1.7663
Training Epoch: 11 [44288/50176]	Loss: 1.4220
Training Epoch: 11 [44544/50176]	Loss: 1.6677
Training Epoch: 11 [44800/50176]	Loss: 1.7795
Training Epoch: 11 [45056/50176]	Loss: 1.3997
Training Epoch: 11 [45312/50176]	Loss: 1.6336
Training Epoch: 11 [45568/50176]	Loss: 1.5150
Training Epoch: 11 [45824/50176]	Loss: 1.6246
Training Epoch: 11 [46080/50176]	Loss: 1.3988
Training Epoch: 11 [46336/50176]	Loss: 1.6571
Training Epoch: 11 [46592/50176]	Loss: 1.6071
Training Epoch: 11 [46848/50176]	Loss: 1.6318
Training Epoch: 11 [47104/50176]	Loss: 1.4595
Training Epoch: 11 [47360/50176]	Loss: 1.4752
Training Epoch: 11 [47616/50176]	Loss: 1.7417
Training Epoch: 11 [47872/50176]	Loss: 1.6326
Training Epoch: 11 [48128/50176]	Loss: 1.6312
Training Epoch: 11 [48384/50176]	Loss: 1.6943
Training Epoch: 11 [48640/50176]	Loss: 1.4431
Training Epoch: 11 [48896/50176]	Loss: 1.5233
Training Epoch: 11 [49152/50176]	Loss: 1.7338
Training Epoch: 11 [49408/50176]	Loss: 1.7449
Training Epoch: 11 [49664/50176]	Loss: 1.5595
Training Epoch: 11 [49920/50176]	Loss: 1.7371
Training Epoch: 11 [50176/50176]	Loss: 1.3241
Validation Epoch: 11, Average loss: 0.0072, Accuracy: 0.5061
[Training Loop] Model's accuracy 0.5060546875 surpasses threshold 0.5! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4140
Profiling... [256/50048]	Loss: 1.5588
Profiling... [384/50048]	Loss: 1.4109
Profiling... [512/50048]	Loss: 1.5992
Profiling... [640/50048]	Loss: 1.6979
Profiling... [768/50048]	Loss: 1.6049
Profiling... [896/50048]	Loss: 1.6146
Profiling... [1024/50048]	Loss: 1.3259
Profiling... [1152/50048]	Loss: 1.2340
Profiling... [1280/50048]	Loss: 1.3550
Profiling... [1408/50048]	Loss: 1.5540
Profiling... [1536/50048]	Loss: 1.3335
Profiling... [1664/50048]	Loss: 1.7889
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 12, Average loss: 0.0133, Accuracy: 0.5390
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.25203054362267,
                        "time": 2.1726485199978924,
                        "accuracy": 0.5389636075949367,
                        "total_cost": 352956.78144435893
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5772
Profiling... [256/50048]	Loss: 1.5137
Profiling... [384/50048]	Loss: 1.5087
Profiling... [512/50048]	Loss: 1.4815
Profiling... [640/50048]	Loss: 1.7070
Profiling... [768/50048]	Loss: 1.7043
Profiling... [896/50048]	Loss: 1.5264
Profiling... [1024/50048]	Loss: 1.5429
Profiling... [1152/50048]	Loss: 1.4795
Profiling... [1280/50048]	Loss: 1.4264
Profiling... [1408/50048]	Loss: 1.4591
Profiling... [1536/50048]	Loss: 1.4865
Profiling... [1664/50048]	Loss: 1.5535
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 12, Average loss: 0.0131, Accuracy: 0.5381
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.25884757133542,
                        "time": 2.1677777639997657,
                        "accuracy": 0.5380735759493671,
                        "total_cost": 352748.0380534294
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.3458
Profiling... [256/50048]	Loss: 1.3847
Profiling... [384/50048]	Loss: 1.4664
Profiling... [512/50048]	Loss: 1.6950
Profiling... [640/50048]	Loss: 1.6222
Profiling... [768/50048]	Loss: 1.4495
Profiling... [896/50048]	Loss: 1.3769
Profiling... [1024/50048]	Loss: 1.4772
Profiling... [1152/50048]	Loss: 1.5256
Profiling... [1280/50048]	Loss: 1.4587
Profiling... [1408/50048]	Loss: 1.6870
Profiling... [1536/50048]	Loss: 1.3768
Profiling... [1664/50048]	Loss: 1.4843
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 12, Average loss: 0.0130, Accuracy: 0.5417
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.2642630508657,
                        "time": 2.2196585869969567,
                        "accuracy": 0.5417325949367089,
                        "total_cost": 358750.68623344204
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5316
Profiling... [256/50048]	Loss: 1.4326
Profiling... [384/50048]	Loss: 1.6960
Profiling... [512/50048]	Loss: 1.3713
Profiling... [640/50048]	Loss: 1.6026
Profiling... [768/50048]	Loss: 1.4892
Profiling... [896/50048]	Loss: 1.5804
Profiling... [1024/50048]	Loss: 1.6717
Profiling... [1152/50048]	Loss: 1.5217
Profiling... [1280/50048]	Loss: 1.5299
Profiling... [1408/50048]	Loss: 1.4092
Profiling... [1536/50048]	Loss: 1.5774
Profiling... [1664/50048]	Loss: 1.3930
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 12, Average loss: 0.0133, Accuracy: 0.5334
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.25163527352663,
                        "time": 2.5676342020015,
                        "accuracy": 0.5334256329113924,
                        "total_cost": 421454.5702487019
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6380
Profiling... [256/50048]	Loss: 1.7032
Profiling... [384/50048]	Loss: 1.3659
Profiling... [512/50048]	Loss: 1.7360
Profiling... [640/50048]	Loss: 1.4632
Profiling... [768/50048]	Loss: 1.8206
Profiling... [896/50048]	Loss: 1.6025
Profiling... [1024/50048]	Loss: 1.5058
Profiling... [1152/50048]	Loss: 1.6161
Profiling... [1280/50048]	Loss: 1.6642
Profiling... [1408/50048]	Loss: 1.3446
Profiling... [1536/50048]	Loss: 1.3092
Profiling... [1664/50048]	Loss: 1.3533
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 12, Average loss: 0.0132, Accuracy: 0.5361
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.23386100413023,
                        "time": 2.158001690004312,
                        "accuracy": 0.5360957278481012,
                        "total_cost": 352452.7365035555
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4780
Profiling... [256/50048]	Loss: 1.5632
Profiling... [384/50048]	Loss: 1.4114
Profiling... [512/50048]	Loss: 1.3718
Profiling... [640/50048]	Loss: 1.5688
Profiling... [768/50048]	Loss: 1.5566
Profiling... [896/50048]	Loss: 1.5867
Profiling... [1024/50048]	Loss: 1.5048
Profiling... [1152/50048]	Loss: 1.5780
Profiling... [1280/50048]	Loss: 1.5277
Profiling... [1408/50048]	Loss: 1.5492
Profiling... [1536/50048]	Loss: 1.4090
Profiling... [1664/50048]	Loss: 1.5971
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 12, Average loss: 0.0130, Accuracy: 0.5420
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.24067524075376,
                        "time": 2.15819111699966,
                        "accuracy": 0.5420292721518988,
                        "total_cost": 348625.081801425
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6226
Profiling... [256/50048]	Loss: 1.4856
Profiling... [384/50048]	Loss: 1.4144
Profiling... [512/50048]	Loss: 1.6214
Profiling... [640/50048]	Loss: 1.6868
Profiling... [768/50048]	Loss: 1.7793
Profiling... [896/50048]	Loss: 1.5769
Profiling... [1024/50048]	Loss: 1.6156
Profiling... [1152/50048]	Loss: 1.1868
Profiling... [1280/50048]	Loss: 1.7710
Profiling... [1408/50048]	Loss: 1.5566
Profiling... [1536/50048]	Loss: 1.5159
Profiling... [1664/50048]	Loss: 1.5913
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 12, Average loss: 0.0131, Accuracy: 0.5387
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.2461822348128,
                        "time": 2.2113622429969837,
                        "accuracy": 0.5386669303797469,
                        "total_cost": 359443.8514587088
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5726
Profiling... [256/50048]	Loss: 1.6449
Profiling... [384/50048]	Loss: 1.5256
Profiling... [512/50048]	Loss: 1.4358
Profiling... [640/50048]	Loss: 1.6315
Profiling... [768/50048]	Loss: 1.4992
Profiling... [896/50048]	Loss: 1.3722
Profiling... [1024/50048]	Loss: 1.2196
Profiling... [1152/50048]	Loss: 1.7296
Profiling... [1280/50048]	Loss: 1.4400
Profiling... [1408/50048]	Loss: 1.5473
Profiling... [1536/50048]	Loss: 1.4711
Profiling... [1664/50048]	Loss: 1.3670
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 12, Average loss: 0.0130, Accuracy: 0.5408
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.23421133957069,
                        "time": 2.590670837998914,
                        "accuracy": 0.5408425632911392,
                        "total_cost": 419404.25059114577
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5439
Profiling... [256/50048]	Loss: 1.4317
Profiling... [384/50048]	Loss: 1.6588
Profiling... [512/50048]	Loss: 1.7503
Profiling... [640/50048]	Loss: 1.7278
Profiling... [768/50048]	Loss: 1.5386
Profiling... [896/50048]	Loss: 1.4811
Profiling... [1024/50048]	Loss: 1.3138
Profiling... [1152/50048]	Loss: 1.6497
Profiling... [1280/50048]	Loss: 1.5409
Profiling... [1408/50048]	Loss: 1.7277
Profiling... [1536/50048]	Loss: 1.5333
Profiling... [1664/50048]	Loss: 1.2648
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 12, Average loss: 0.0130, Accuracy: 0.5406
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.21637242598165,
                        "time": 2.1647957840032177,
                        "accuracy": 0.5406447784810127,
                        "total_cost": 350587.4211780337
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6469
Profiling... [256/50048]	Loss: 1.5980
Profiling... [384/50048]	Loss: 1.5289
Profiling... [512/50048]	Loss: 1.3460
Profiling... [640/50048]	Loss: 1.4530
Profiling... [768/50048]	Loss: 1.7210
Profiling... [896/50048]	Loss: 1.7262
Profiling... [1024/50048]	Loss: 1.5982
Profiling... [1152/50048]	Loss: 1.4863
Profiling... [1280/50048]	Loss: 1.3785
Profiling... [1408/50048]	Loss: 1.8488
Profiling... [1536/50048]	Loss: 1.6408
Profiling... [1664/50048]	Loss: 1.5174
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 12, Average loss: 0.0132, Accuracy: 0.5350
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.22160019923544,
                        "time": 2.15958493600192,
                        "accuracy": 0.5350079113924051,
                        "total_cost": 353428.4512384697
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6062
Profiling... [256/50048]	Loss: 1.3260
Profiling... [384/50048]	Loss: 1.5849
Profiling... [512/50048]	Loss: 1.7890
Profiling... [640/50048]	Loss: 1.6249
Profiling... [768/50048]	Loss: 1.4728
Profiling... [896/50048]	Loss: 1.4583
Profiling... [1024/50048]	Loss: 1.8231
Profiling... [1152/50048]	Loss: 1.5087
Profiling... [1280/50048]	Loss: 1.5566
Profiling... [1408/50048]	Loss: 1.4845
Profiling... [1536/50048]	Loss: 1.5237
Profiling... [1664/50048]	Loss: 1.5463
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 12, Average loss: 0.0131, Accuracy: 0.5370
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.22585735342051,
                        "time": 2.2094892029999755,
                        "accuracy": 0.5369857594936709,
                        "total_cost": 360263.7355675368
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.4561
Profiling... [256/50048]	Loss: 1.6191
Profiling... [384/50048]	Loss: 1.6412
Profiling... [512/50048]	Loss: 1.0795
Profiling... [640/50048]	Loss: 1.5340
Profiling... [768/50048]	Loss: 1.4626
Profiling... [896/50048]	Loss: 1.7041
Profiling... [1024/50048]	Loss: 1.5065
Profiling... [1152/50048]	Loss: 1.5047
Profiling... [1280/50048]	Loss: 1.4158
Profiling... [1408/50048]	Loss: 1.5833
Profiling... [1536/50048]	Loss: 1.3603
Profiling... [1664/50048]	Loss: 1.4991
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 12, Average loss: 0.0131, Accuracy: 0.5365
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.21311897753486,
                        "time": 2.567344368006161,
                        "accuracy": 0.5364912974683544,
                        "total_cost": 418998.86440875253
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8163
Profiling... [256/50048]	Loss: 1.4045
Profiling... [384/50048]	Loss: 1.5736
Profiling... [512/50048]	Loss: 1.4417
Profiling... [640/50048]	Loss: 1.9312
Profiling... [768/50048]	Loss: 2.0663
Profiling... [896/50048]	Loss: 2.1180
Profiling... [1024/50048]	Loss: 1.8207
Profiling... [1152/50048]	Loss: 1.6274
Profiling... [1280/50048]	Loss: 1.6649
Profiling... [1408/50048]	Loss: 1.5208
Profiling... [1536/50048]	Loss: 1.7543
Profiling... [1664/50048]	Loss: 1.7595
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 12, Average loss: 0.0148, Accuracy: 0.4919
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.19439584244415,
                        "time": 2.162341759998526,
                        "accuracy": 0.4918908227848101,
                        "total_cost": 384899.16641144856
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6014
Profiling... [256/50048]	Loss: 1.4285
Profiling... [384/50048]	Loss: 1.5369
Profiling... [512/50048]	Loss: 1.8087
Profiling... [640/50048]	Loss: 1.5114
Profiling... [768/50048]	Loss: 2.0042
Profiling... [896/50048]	Loss: 1.7627
Profiling... [1024/50048]	Loss: 1.6754
Profiling... [1152/50048]	Loss: 1.6307
Profiling... [1280/50048]	Loss: 1.8088
Profiling... [1408/50048]	Loss: 1.9279
Profiling... [1536/50048]	Loss: 1.6133
Profiling... [1664/50048]	Loss: 1.7276
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 12, Average loss: 0.0167, Accuracy: 0.4591
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.20074449290242,
                        "time": 2.162260782002704,
                        "accuracy": 0.45905854430379744,
                        "total_cost": 412412.0695932263
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5830
Profiling... [256/50048]	Loss: 1.6292
Profiling... [384/50048]	Loss: 1.7183
Profiling... [512/50048]	Loss: 1.5653
Profiling... [640/50048]	Loss: 1.7019
Profiling... [768/50048]	Loss: 1.8751
Profiling... [896/50048]	Loss: 1.7350
Profiling... [1024/50048]	Loss: 1.9445
Profiling... [1152/50048]	Loss: 1.7111
Profiling... [1280/50048]	Loss: 1.5657
Profiling... [1408/50048]	Loss: 1.5334
Profiling... [1536/50048]	Loss: 1.6499
Profiling... [1664/50048]	Loss: 1.5691
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 12, Average loss: 0.0153, Accuracy: 0.4836
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.20484122095203,
                        "time": 2.2131025060007232,
                        "accuracy": 0.48358386075949367,
                        "total_cost": 400701.6331787514
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5610
Profiling... [256/50048]	Loss: 1.5481
Profiling... [384/50048]	Loss: 1.7829
Profiling... [512/50048]	Loss: 1.5705
Profiling... [640/50048]	Loss: 1.6882
Profiling... [768/50048]	Loss: 1.6402
Profiling... [896/50048]	Loss: 1.6012
Profiling... [1024/50048]	Loss: 1.8921
Profiling... [1152/50048]	Loss: 1.5179
Profiling... [1280/50048]	Loss: 1.4577
Profiling... [1408/50048]	Loss: 2.0650
Profiling... [1536/50048]	Loss: 1.6308
Profiling... [1664/50048]	Loss: 1.6396
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 12, Average loss: 0.0155, Accuracy: 0.4763
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.19304301350552,
                        "time": 2.5865048459963873,
                        "accuracy": 0.4762658227848101,
                        "total_cost": 475505.1561963004
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6095
Profiling... [256/50048]	Loss: 1.6374
Profiling... [384/50048]	Loss: 1.5101
Profiling... [512/50048]	Loss: 1.6010
Profiling... [640/50048]	Loss: 1.6192
Profiling... [768/50048]	Loss: 1.8680
Profiling... [896/50048]	Loss: 1.8716
Profiling... [1024/50048]	Loss: 1.6688
Profiling... [1152/50048]	Loss: 1.6481
Profiling... [1280/50048]	Loss: 1.7906
Profiling... [1408/50048]	Loss: 1.7660
Profiling... [1536/50048]	Loss: 1.6995
Profiling... [1664/50048]	Loss: 1.2826
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 12, Average loss: 0.0163, Accuracy: 0.4641
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.17360122513405,
                        "time": 2.162977903994033,
                        "accuracy": 0.46410205696202533,
                        "total_cost": 408065.51327849156
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5253
Profiling... [256/50048]	Loss: 1.6945
Profiling... [384/50048]	Loss: 1.4123
Profiling... [512/50048]	Loss: 1.8531
Profiling... [640/50048]	Loss: 1.6242
Profiling... [768/50048]	Loss: 1.5739
Profiling... [896/50048]	Loss: 1.8064
Profiling... [1024/50048]	Loss: 1.9022
Profiling... [1152/50048]	Loss: 1.7156
Profiling... [1280/50048]	Loss: 1.7977
Profiling... [1408/50048]	Loss: 1.6789
Profiling... [1536/50048]	Loss: 1.5520
Profiling... [1664/50048]	Loss: 1.6038
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 12, Average loss: 0.0170, Accuracy: 0.4544
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.18183826543371,
                        "time": 2.1659252980025485,
                        "accuracy": 0.4544106012658228,
                        "total_cost": 417336.47435377777
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4666
Profiling... [256/50048]	Loss: 1.9115
Profiling... [384/50048]	Loss: 1.4599
Profiling... [512/50048]	Loss: 1.5464
Profiling... [640/50048]	Loss: 1.7180
Profiling... [768/50048]	Loss: 1.6197
Profiling... [896/50048]	Loss: 1.5118
Profiling... [1024/50048]	Loss: 1.7248
Profiling... [1152/50048]	Loss: 1.7069
Profiling... [1280/50048]	Loss: 1.7431
Profiling... [1408/50048]	Loss: 1.7179
Profiling... [1536/50048]	Loss: 2.0700
Profiling... [1664/50048]	Loss: 1.7534
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 12, Average loss: 0.0167, Accuracy: 0.4617
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.18482606619176,
                        "time": 2.2158915549953235,
                        "accuracy": 0.46172863924050633,
                        "total_cost": 420197.070680134
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7505
Profiling... [256/50048]	Loss: 1.8460
Profiling... [384/50048]	Loss: 1.4909
Profiling... [512/50048]	Loss: 1.5357
Profiling... [640/50048]	Loss: 1.6545
Profiling... [768/50048]	Loss: 1.6566
Profiling... [896/50048]	Loss: 1.6178
Profiling... [1024/50048]	Loss: 1.7416
Profiling... [1152/50048]	Loss: 1.8418
Profiling... [1280/50048]	Loss: 1.6558
Profiling... [1408/50048]	Loss: 1.5610
Profiling... [1536/50048]	Loss: 1.8309
Profiling... [1664/50048]	Loss: 1.5796
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 12, Average loss: 0.0165, Accuracy: 0.4626
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.17426651302809,
                        "time": 2.5581036520015914,
                        "accuracy": 0.46261867088607594,
                        "total_cost": 484157.07892897
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4524
Profiling... [256/50048]	Loss: 1.7329
Profiling... [384/50048]	Loss: 1.8147
Profiling... [512/50048]	Loss: 1.7887
Profiling... [640/50048]	Loss: 1.6923
Profiling... [768/50048]	Loss: 1.7897
Profiling... [896/50048]	Loss: 1.7737
Profiling... [1024/50048]	Loss: 1.5950
Profiling... [1152/50048]	Loss: 1.8921
Profiling... [1280/50048]	Loss: 1.6687
Profiling... [1408/50048]	Loss: 1.8077
Profiling... [1536/50048]	Loss: 1.8836
Profiling... [1664/50048]	Loss: 1.8565
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 12, Average loss: 0.0163, Accuracy: 0.4706
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.15450290321054,
                        "time": 2.163210833001358,
                        "accuracy": 0.47062895569620256,
                        "total_cost": 402449.563979952
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5318
Profiling... [256/50048]	Loss: 1.3423
Profiling... [384/50048]	Loss: 1.6742
Profiling... [512/50048]	Loss: 1.6623
Profiling... [640/50048]	Loss: 1.5913
Profiling... [768/50048]	Loss: 1.6277
Profiling... [896/50048]	Loss: 1.9146
Profiling... [1024/50048]	Loss: 1.5789
Profiling... [1152/50048]	Loss: 1.8341
Profiling... [1280/50048]	Loss: 1.5576
Profiling... [1408/50048]	Loss: 1.7343
Profiling... [1536/50048]	Loss: 1.8111
Profiling... [1664/50048]	Loss: 1.8087
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 12, Average loss: 0.0159, Accuracy: 0.4827
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.16070879294637,
                        "time": 2.1661873109987937,
                        "accuracy": 0.48269382911392406,
                        "total_cost": 392930.31112488324
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6480
Profiling... [256/50048]	Loss: 1.6048
Profiling... [384/50048]	Loss: 1.5491
Profiling... [512/50048]	Loss: 1.8609
Profiling... [640/50048]	Loss: 1.6411
Profiling... [768/50048]	Loss: 1.7352
Profiling... [896/50048]	Loss: 1.6273
Profiling... [1024/50048]	Loss: 1.7459
Profiling... [1152/50048]	Loss: 1.5941
Profiling... [1280/50048]	Loss: 1.9450
Profiling... [1408/50048]	Loss: 1.8201
Profiling... [1536/50048]	Loss: 1.7187
Profiling... [1664/50048]	Loss: 1.6800
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 12, Average loss: 0.0164, Accuracy: 0.4640
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.16567612586302,
                        "time": 2.211609646001307,
                        "accuracy": 0.464003164556962,
                        "total_cost": 417329.24206516595
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.3850
Profiling... [256/50048]	Loss: 1.3789
Profiling... [384/50048]	Loss: 1.5152
Profiling... [512/50048]	Loss: 1.6959
Profiling... [640/50048]	Loss: 1.7330
Profiling... [768/50048]	Loss: 1.6771
Profiling... [896/50048]	Loss: 1.4554
Profiling... [1024/50048]	Loss: 1.6913
Profiling... [1152/50048]	Loss: 1.6428
Profiling... [1280/50048]	Loss: 1.8632
Profiling... [1408/50048]	Loss: 1.6975
Profiling... [1536/50048]	Loss: 1.7193
Profiling... [1664/50048]	Loss: 1.7750
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 12, Average loss: 0.0164, Accuracy: 0.4590
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.15421182814701,
                        "time": 2.560669673002849,
                        "accuracy": 0.45895965189873417,
                        "total_cost": 488506.4538338878
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.2857
Profiling... [256/50048]	Loss: 1.7816
Profiling... [384/50048]	Loss: 1.9227
Profiling... [512/50048]	Loss: 2.4096
Profiling... [640/50048]	Loss: 2.0568
Profiling... [768/50048]	Loss: 2.0303
Profiling... [896/50048]	Loss: 2.0440
Profiling... [1024/50048]	Loss: 2.1844
Profiling... [1152/50048]	Loss: 2.3206
Profiling... [1280/50048]	Loss: 2.0076
Profiling... [1408/50048]	Loss: 2.1901
Profiling... [1536/50048]	Loss: 1.7042
Profiling... [1664/50048]	Loss: 2.1087
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 12, Average loss: 0.0203, Accuracy: 0.3699
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.1369777891493,
                        "time": 2.1691537279984914,
                        "accuracy": 0.36985759493670883,
                        "total_cost": 513507.7503486319
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4701
Profiling... [256/50048]	Loss: 1.7480
Profiling... [384/50048]	Loss: 1.8772
Profiling... [512/50048]	Loss: 2.2089
Profiling... [640/50048]	Loss: 1.8792
Profiling... [768/50048]	Loss: 1.9743
Profiling... [896/50048]	Loss: 1.8777
Profiling... [1024/50048]	Loss: 1.8876
Profiling... [1152/50048]	Loss: 1.9078
Profiling... [1280/50048]	Loss: 2.3161
Profiling... [1408/50048]	Loss: 2.0773
Profiling... [1536/50048]	Loss: 2.2626
Profiling... [1664/50048]	Loss: 1.9476
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 12, Average loss: 0.0218, Accuracy: 0.3543
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.14266047402677,
                        "time": 2.1718436909941374,
                        "accuracy": 0.35433148734177217,
                        "total_cost": 536673.3687629598
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5333
Profiling... [256/50048]	Loss: 1.6206
Profiling... [384/50048]	Loss: 2.0253
Profiling... [512/50048]	Loss: 1.8908
Profiling... [640/50048]	Loss: 2.2402
Profiling... [768/50048]	Loss: 2.1893
Profiling... [896/50048]	Loss: 1.9485
Profiling... [1024/50048]	Loss: 2.1012
Profiling... [1152/50048]	Loss: 2.0833
Profiling... [1280/50048]	Loss: 2.2838
Profiling... [1408/50048]	Loss: 2.0417
Profiling... [1536/50048]	Loss: 2.0148
Profiling... [1664/50048]	Loss: 2.2273
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 12, Average loss: 0.0275, Accuracy: 0.2793
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.14787296294215,
                        "time": 2.2176431289990433,
                        "accuracy": 0.27927215189873417,
                        "total_cost": 695272.8443934028
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5836
Profiling... [256/50048]	Loss: 1.9466
Profiling... [384/50048]	Loss: 2.0441
Profiling... [512/50048]	Loss: 2.2491
Profiling... [640/50048]	Loss: 2.1052
Profiling... [768/50048]	Loss: 2.3293
Profiling... [896/50048]	Loss: 2.1018
Profiling... [1024/50048]	Loss: 2.0287
Profiling... [1152/50048]	Loss: 1.9511
Profiling... [1280/50048]	Loss: 2.2674
Profiling... [1408/50048]	Loss: 1.8426
Profiling... [1536/50048]	Loss: 1.9736
Profiling... [1664/50048]	Loss: 1.9907
Profile done
epoch 1 train time consumed: 4.04s
Validation Epoch: 12, Average loss: 0.0226, Accuracy: 0.3195
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.13427721470354,
                        "time": 2.5846104609954637,
                        "accuracy": 0.31952136075949367,
                        "total_cost": 708249.7117645454
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5065
Profiling... [256/50048]	Loss: 1.6818
Profiling... [384/50048]	Loss: 2.1565
Profiling... [512/50048]	Loss: 2.1699
Profiling... [640/50048]	Loss: 2.2915
Profiling... [768/50048]	Loss: 1.9459
Profiling... [896/50048]	Loss: 2.1600
Profiling... [1024/50048]	Loss: 2.2100
Profiling... [1152/50048]	Loss: 1.9886
Profiling... [1280/50048]	Loss: 2.2349
Profiling... [1408/50048]	Loss: 2.5280
Profiling... [1536/50048]	Loss: 2.2284
Profiling... [1664/50048]	Loss: 1.9502
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 12, Average loss: 0.0726, Accuracy: 0.0689
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.11616429155082,
                        "time": 2.1606221530018956,
                        "accuracy": 0.06892800632911393,
                        "total_cost": 2744569.7245424693
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5283
Profiling... [256/50048]	Loss: 1.7653
Profiling... [384/50048]	Loss: 2.1019
Profiling... [512/50048]	Loss: 1.9443
Profiling... [640/50048]	Loss: 1.6680
Profiling... [768/50048]	Loss: 1.9030
Profiling... [896/50048]	Loss: 2.1532
Profiling... [1024/50048]	Loss: 2.3196
Profiling... [1152/50048]	Loss: 1.9400
Profiling... [1280/50048]	Loss: 1.9305
Profiling... [1408/50048]	Loss: 1.8835
Profiling... [1536/50048]	Loss: 2.0752
Profiling... [1664/50048]	Loss: 2.0654
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 12, Average loss: 0.0195, Accuracy: 0.3868
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.12232169460259,
                        "time": 2.1650818640046054,
                        "accuracy": 0.38676819620253167,
                        "total_cost": 490133.9020275206
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.2372
Profiling... [256/50048]	Loss: 2.0450
Profiling... [384/50048]	Loss: 1.8716
Profiling... [512/50048]	Loss: 2.0441
Profiling... [640/50048]	Loss: 2.0956
Profiling... [768/50048]	Loss: 2.1291
Profiling... [896/50048]	Loss: 1.7113
Profiling... [1024/50048]	Loss: 2.1817
Profiling... [1152/50048]	Loss: 1.9944
Profiling... [1280/50048]	Loss: 2.2166
Profiling... [1408/50048]	Loss: 2.0540
Profiling... [1536/50048]	Loss: 2.1572
Profiling... [1664/50048]	Loss: 2.1233
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 12, Average loss: 0.0243, Accuracy: 0.3151
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.12539006275277,
                        "time": 2.2174759699992137,
                        "accuracy": 0.31507120253164556,
                        "total_cost": 616227.9540937919
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6307
Profiling... [256/50048]	Loss: 1.6286
Profiling... [384/50048]	Loss: 1.8391
Profiling... [512/50048]	Loss: 2.1525
Profiling... [640/50048]	Loss: 1.8320
Profiling... [768/50048]	Loss: 2.0829
Profiling... [896/50048]	Loss: 1.8382
Profiling... [1024/50048]	Loss: 2.0518
Profiling... [1152/50048]	Loss: 2.0873
Profiling... [1280/50048]	Loss: 2.2420
Profiling... [1408/50048]	Loss: 2.0946
Profiling... [1536/50048]	Loss: 2.1195
Profiling... [1664/50048]	Loss: 1.7865
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 12, Average loss: 0.0263, Accuracy: 0.3194
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.111966620062,
                        "time": 2.577629767001781,
                        "accuracy": 0.3194224683544304,
                        "total_cost": 706555.4122610956
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4208
Profiling... [256/50048]	Loss: 1.7832
Profiling... [384/50048]	Loss: 1.8442
Profiling... [512/50048]	Loss: 1.9400
Profiling... [640/50048]	Loss: 1.9388
Profiling... [768/50048]	Loss: 2.2238
Profiling... [896/50048]	Loss: 1.9654
Profiling... [1024/50048]	Loss: 2.1908
Profiling... [1152/50048]	Loss: 2.2341
Profiling... [1280/50048]	Loss: 2.2636
Profiling... [1408/50048]	Loss: 2.1246
Profiling... [1536/50048]	Loss: 2.2450
Profiling... [1664/50048]	Loss: 1.8520
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 12, Average loss: 0.0234, Accuracy: 0.3378
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.09468908557632,
                        "time": 2.1620305969991023,
                        "accuracy": 0.33781645569620256,
                        "total_cost": 560366.471644112
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6113
Profiling... [256/50048]	Loss: 1.8734
Profiling... [384/50048]	Loss: 2.2239
Profiling... [512/50048]	Loss: 2.0793
Profiling... [640/50048]	Loss: 1.9161
Profiling... [768/50048]	Loss: 2.3298
Profiling... [896/50048]	Loss: 2.1175
Profiling... [1024/50048]	Loss: 1.9980
Profiling... [1152/50048]	Loss: 1.8693
Profiling... [1280/50048]	Loss: 1.9406
Profiling... [1408/50048]	Loss: 2.0189
Profiling... [1536/50048]	Loss: 1.8566
Profiling... [1664/50048]	Loss: 1.9536
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 12, Average loss: 0.0230, Accuracy: 0.3250
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.10176461214871,
                        "time": 2.159575597004732,
                        "accuracy": 0.32496044303797467,
                        "total_cost": 581874.1157028561
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4803
Profiling... [256/50048]	Loss: 1.4386
Profiling... [384/50048]	Loss: 1.9599
Profiling... [512/50048]	Loss: 2.0581
Profiling... [640/50048]	Loss: 2.0333
Profiling... [768/50048]	Loss: 2.1739
Profiling... [896/50048]	Loss: 2.3429
Profiling... [1024/50048]	Loss: 2.0925
Profiling... [1152/50048]	Loss: 2.0455
Profiling... [1280/50048]	Loss: 2.0486
Profiling... [1408/50048]	Loss: 2.2692
Profiling... [1536/50048]	Loss: 2.4015
Profiling... [1664/50048]	Loss: 2.1482
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 12, Average loss: 0.0252, Accuracy: 0.2921
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.10554046539649,
                        "time": 2.2170375870045973,
                        "accuracy": 0.292128164556962,
                        "total_cost": 664493.5358880811
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5733
Profiling... [256/50048]	Loss: 1.5526
Profiling... [384/50048]	Loss: 2.2148
Profiling... [512/50048]	Loss: 2.1157
Profiling... [640/50048]	Loss: 1.9366
Profiling... [768/50048]	Loss: 2.1879
Profiling... [896/50048]	Loss: 1.9913
Profiling... [1024/50048]	Loss: 1.7483
Profiling... [1152/50048]	Loss: 2.0341
Profiling... [1280/50048]	Loss: 2.1259
Profiling... [1408/50048]	Loss: 1.8821
Profiling... [1536/50048]	Loss: 2.0477
Profiling... [1664/50048]	Loss: 2.3144
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 12, Average loss: 0.0251, Accuracy: 0.3132
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.09334758504926,
                        "time": 2.575913426000625,
                        "accuracy": 0.31319224683544306,
                        "total_cost": 720130.7642410703
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.4737
Profiling... [512/50176]	Loss: 1.4715
Profiling... [768/50176]	Loss: 1.5517
Profiling... [1024/50176]	Loss: 1.2186
Profiling... [1280/50176]	Loss: 1.4502
Profiling... [1536/50176]	Loss: 1.2871
Profiling... [1792/50176]	Loss: 1.4520
Profiling... [2048/50176]	Loss: 1.3908
Profiling... [2304/50176]	Loss: 1.4146
Profiling... [2560/50176]	Loss: 1.5660
Profiling... [2816/50176]	Loss: 1.3694
Profiling... [3072/50176]	Loss: 1.4528
Profiling... [3328/50176]	Loss: 1.3291
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 12, Average loss: 0.0064, Accuracy: 0.5355
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.08098683372373,
                        "time": 2.4711685459988075,
                        "accuracy": 0.535546875,
                        "total_cost": 404013.5692101194
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6447
Profiling... [512/50176]	Loss: 1.7564
Profiling... [768/50176]	Loss: 1.6831
Profiling... [1024/50176]	Loss: 1.4734
Profiling... [1280/50176]	Loss: 1.4965
Profiling... [1536/50176]	Loss: 1.2627
Profiling... [1792/50176]	Loss: 1.3556
Profiling... [2048/50176]	Loss: 1.6159
Profiling... [2304/50176]	Loss: 1.5268
Profiling... [2560/50176]	Loss: 1.4818
Profiling... [2816/50176]	Loss: 1.3521
Profiling... [3072/50176]	Loss: 1.4479
Profiling... [3328/50176]	Loss: 1.3243
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 12, Average loss: 0.0064, Accuracy: 0.5373
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.09356027544686,
                        "time": 2.381717354997818,
                        "accuracy": 0.5373046875,
                        "total_cost": 388115.2402352898
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6138
Profiling... [512/50176]	Loss: 1.5455
Profiling... [768/50176]	Loss: 1.6729
Profiling... [1024/50176]	Loss: 1.5653
Profiling... [1280/50176]	Loss: 1.5907
Profiling... [1536/50176]	Loss: 1.4240
Profiling... [1792/50176]	Loss: 1.2886
Profiling... [2048/50176]	Loss: 1.4539
Profiling... [2304/50176]	Loss: 1.4683
Profiling... [2560/50176]	Loss: 1.4139
Profiling... [2816/50176]	Loss: 1.4367
Profiling... [3072/50176]	Loss: 1.3965
Profiling... [3328/50176]	Loss: 1.4822
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 12, Average loss: 0.0065, Accuracy: 0.5367
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.0991409701346,
                        "time": 2.5215295359957963,
                        "accuracy": 0.53671875,
                        "total_cost": 411347.0725154024
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.3033
Profiling... [512/50176]	Loss: 1.5439
Profiling... [768/50176]	Loss: 1.4083
Profiling... [1024/50176]	Loss: 1.5448
Profiling... [1280/50176]	Loss: 1.4580
Profiling... [1536/50176]	Loss: 1.6125
Profiling... [1792/50176]	Loss: 1.5598
Profiling... [2048/50176]	Loss: 1.6632
Profiling... [2304/50176]	Loss: 1.5418
Profiling... [2560/50176]	Loss: 1.4449
Profiling... [2816/50176]	Loss: 1.4025
Profiling... [3072/50176]	Loss: 1.4878
Profiling... [3328/50176]	Loss: 1.3787
Profile done
epoch 1 train time consumed: 4.63s
Validation Epoch: 12, Average loss: 0.0064, Accuracy: 0.5375
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.08354494416128,
                        "time": 3.044654014003754,
                        "accuracy": 0.5375,
                        "total_cost": 495964.4626732121
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.4770
Profiling... [512/50176]	Loss: 1.4095
Profiling... [768/50176]	Loss: 1.6226
Profiling... [1024/50176]	Loss: 1.4148
Profiling... [1280/50176]	Loss: 1.5406
Profiling... [1536/50176]	Loss: 1.3852
Profiling... [1792/50176]	Loss: 1.4725
Profiling... [2048/50176]	Loss: 1.4035
Profiling... [2304/50176]	Loss: 1.5161
Profiling... [2560/50176]	Loss: 1.5827
Profiling... [2816/50176]	Loss: 1.2721
Profiling... [3072/50176]	Loss: 1.4233
Profiling... [3328/50176]	Loss: 1.3768
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 12, Average loss: 0.0065, Accuracy: 0.5308
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.0707103451114,
                        "time": 2.3790944259963,
                        "accuracy": 0.53076171875,
                        "total_cost": 392466.9911402724
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4237
Profiling... [512/50176]	Loss: 1.6127
Profiling... [768/50176]	Loss: 1.7615
Profiling... [1024/50176]	Loss: 1.5187
Profiling... [1280/50176]	Loss: 1.5107
Profiling... [1536/50176]	Loss: 1.4406
Profiling... [1792/50176]	Loss: 1.5237
Profiling... [2048/50176]	Loss: 1.4295
Profiling... [2304/50176]	Loss: 1.3720
Profiling... [2560/50176]	Loss: 1.3791
Profiling... [2816/50176]	Loss: 1.5364
Profiling... [3072/50176]	Loss: 1.4456
Profiling... [3328/50176]	Loss: 1.3227
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 12, Average loss: 0.0065, Accuracy: 0.5396
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.08354030373066,
                        "time": 2.3824633749973145,
                        "accuracy": 0.53955078125,
                        "total_cost": 386620.59715161764
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5128
Profiling... [512/50176]	Loss: 1.5427
Profiling... [768/50176]	Loss: 1.4414
Profiling... [1024/50176]	Loss: 1.5522
Profiling... [1280/50176]	Loss: 1.4244
Profiling... [1536/50176]	Loss: 1.4003
Profiling... [1792/50176]	Loss: 1.5246
Profiling... [2048/50176]	Loss: 1.4052
Profiling... [2304/50176]	Loss: 1.4418
Profiling... [2560/50176]	Loss: 1.4528
Profiling... [2816/50176]	Loss: 1.4430
Profiling... [3072/50176]	Loss: 1.4568
Profiling... [3328/50176]	Loss: 1.6165
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 12, Average loss: 0.0065, Accuracy: 0.5364
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.08953661377952,
                        "time": 2.5381381990009686,
                        "accuracy": 0.53642578125,
                        "total_cost": 414282.6234045358
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.3351
Profiling... [512/50176]	Loss: 1.3065
Profiling... [768/50176]	Loss: 1.6545
Profiling... [1024/50176]	Loss: 1.5983
Profiling... [1280/50176]	Loss: 1.4847
Profiling... [1536/50176]	Loss: 1.4933
Profiling... [1792/50176]	Loss: 1.4993
Profiling... [2048/50176]	Loss: 1.4086
Profiling... [2304/50176]	Loss: 1.6756
Profiling... [2560/50176]	Loss: 1.4107
Profiling... [2816/50176]	Loss: 1.4940
Profiling... [3072/50176]	Loss: 1.3821
Profiling... [3328/50176]	Loss: 1.4760
Profile done
epoch 1 train time consumed: 4.56s
Validation Epoch: 12, Average loss: 0.0065, Accuracy: 0.5385
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.07559829285543,
                        "time": 3.0334194600000046,
                        "accuracy": 0.5384765625,
                        "total_cost": 493238.2220811826
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5829
Profiling... [512/50176]	Loss: 1.5621
Profiling... [768/50176]	Loss: 1.5449
Profiling... [1024/50176]	Loss: 1.4989
Profiling... [1280/50176]	Loss: 1.4448
Profiling... [1536/50176]	Loss: 1.5347
Profiling... [1792/50176]	Loss: 1.4743
Profiling... [2048/50176]	Loss: 1.4538
Profiling... [2304/50176]	Loss: 1.3117
Profiling... [2560/50176]	Loss: 1.5291
Profiling... [2816/50176]	Loss: 1.5549
Profiling... [3072/50176]	Loss: 1.2765
Profiling... [3328/50176]	Loss: 1.3020
Profile done
epoch 1 train time consumed: 4.04s
Validation Epoch: 12, Average loss: 0.0065, Accuracy: 0.5370
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.06138478710609,
                        "time": 2.4783287509999354,
                        "accuracy": 0.53701171875,
                        "total_cost": 404078.9035707477
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4761
Profiling... [512/50176]	Loss: 1.5270
Profiling... [768/50176]	Loss: 1.4108
Profiling... [1024/50176]	Loss: 1.3949
Profiling... [1280/50176]	Loss: 1.6544
Profiling... [1536/50176]	Loss: 1.4699
Profiling... [1792/50176]	Loss: 1.5108
Profiling... [2048/50176]	Loss: 1.6096
Profiling... [2304/50176]	Loss: 1.3694
Profiling... [2560/50176]	Loss: 1.4702
Profiling... [2816/50176]	Loss: 1.3640
Profiling... [3072/50176]	Loss: 1.5235
Profiling... [3328/50176]	Loss: 1.4085
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 12, Average loss: 0.0066, Accuracy: 0.5359
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.0709264080623,
                        "time": 2.4864984970045043,
                        "accuracy": 0.5359375,
                        "total_cost": 406223.556070301
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6679
Profiling... [512/50176]	Loss: 1.6012
Profiling... [768/50176]	Loss: 1.5245
Profiling... [1024/50176]	Loss: 1.5432
Profiling... [1280/50176]	Loss: 1.4573
Profiling... [1536/50176]	Loss: 1.5670
Profiling... [1792/50176]	Loss: 1.4975
Profiling... [2048/50176]	Loss: 1.3314
Profiling... [2304/50176]	Loss: 1.4499
Profiling... [2560/50176]	Loss: 1.4749
Profiling... [2816/50176]	Loss: 1.5165
Profiling... [3072/50176]	Loss: 1.3654
Profiling... [3328/50176]	Loss: 1.4223
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 12, Average loss: 0.0064, Accuracy: 0.5387
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.0789024164403,
                        "time": 2.536736538000696,
                        "accuracy": 0.538671875,
                        "total_cost": 412327.33959805913
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6758
Profiling... [512/50176]	Loss: 1.6511
Profiling... [768/50176]	Loss: 1.6123
Profiling... [1024/50176]	Loss: 1.3737
Profiling... [1280/50176]	Loss: 1.5301
Profiling... [1536/50176]	Loss: 1.5154
Profiling... [1792/50176]	Loss: 1.4212
Profiling... [2048/50176]	Loss: 1.4359
Profiling... [2304/50176]	Loss: 1.6172
Profiling... [2560/50176]	Loss: 1.3311
Profiling... [2816/50176]	Loss: 1.5740
Profiling... [3072/50176]	Loss: 1.6806
Profiling... [3328/50176]	Loss: 1.5458
Profile done
epoch 1 train time consumed: 4.69s
Validation Epoch: 12, Average loss: 0.0064, Accuracy: 0.5362
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.06394723416736,
                        "time": 3.0366552080013207,
                        "accuracy": 0.53623046875,
                        "total_cost": 495832.543345799
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6492
Profiling... [512/50176]	Loss: 1.4004
Profiling... [768/50176]	Loss: 1.6836
Profiling... [1024/50176]	Loss: 1.3673
Profiling... [1280/50176]	Loss: 1.6146
Profiling... [1536/50176]	Loss: 1.7285
Profiling... [1792/50176]	Loss: 1.5150
Profiling... [2048/50176]	Loss: 1.7023
Profiling... [2304/50176]	Loss: 1.5297
Profiling... [2560/50176]	Loss: 1.5320
Profiling... [2816/50176]	Loss: 1.4873
Profiling... [3072/50176]	Loss: 1.5633
Profiling... [3328/50176]	Loss: 1.4556
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 12, Average loss: 0.0075, Accuracy: 0.4816
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.05051026538109,
                        "time": 2.3812264269945445,
                        "accuracy": 0.481640625,
                        "total_cost": 432881.0560914597
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4162
Profiling... [512/50176]	Loss: 1.6776
Profiling... [768/50176]	Loss: 1.5994
Profiling... [1024/50176]	Loss: 1.5141
Profiling... [1280/50176]	Loss: 1.6078
Profiling... [1536/50176]	Loss: 1.8328
Profiling... [1792/50176]	Loss: 1.6249
Profiling... [2048/50176]	Loss: 1.6597
Profiling... [2304/50176]	Loss: 1.6620
Profiling... [2560/50176]	Loss: 1.4801
Profiling... [2816/50176]	Loss: 1.4073
Profiling... [3072/50176]	Loss: 1.6667
Profiling... [3328/50176]	Loss: 1.7172
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 12, Average loss: 0.0074, Accuracy: 0.4840
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.0635227316685,
                        "time": 2.3792932420037687,
                        "accuracy": 0.483984375,
                        "total_cost": 430435.08162784606
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.4932
Profiling... [512/50176]	Loss: 1.5219
Profiling... [768/50176]	Loss: 1.6633
Profiling... [1024/50176]	Loss: 1.3667
Profiling... [1280/50176]	Loss: 1.6987
Profiling... [1536/50176]	Loss: 1.6319
Profiling... [1792/50176]	Loss: 1.5538
Profiling... [2048/50176]	Loss: 1.6231
Profiling... [2304/50176]	Loss: 1.8647
Profiling... [2560/50176]	Loss: 1.3834
Profiling... [2816/50176]	Loss: 1.5946
Profiling... [3072/50176]	Loss: 1.4479
Profiling... [3328/50176]	Loss: 1.5822
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 12, Average loss: 0.0077, Accuracy: 0.4771
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.06900447115217,
                        "time": 2.5526121990042157,
                        "accuracy": 0.4771484375,
                        "total_cost": 468405.923217635
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.4924
Profiling... [512/50176]	Loss: 1.5947
Profiling... [768/50176]	Loss: 1.5584
Profiling... [1024/50176]	Loss: 1.7856
Profiling... [1280/50176]	Loss: 1.4808
Profiling... [1536/50176]	Loss: 1.6154
Profiling... [1792/50176]	Loss: 1.6243
Profiling... [2048/50176]	Loss: 1.5583
Profiling... [2304/50176]	Loss: 1.3604
Profiling... [2560/50176]	Loss: 1.6598
Profiling... [2816/50176]	Loss: 1.5035
Profiling... [3072/50176]	Loss: 1.4991
Profiling... [3328/50176]	Loss: 1.4078
Profile done
epoch 1 train time consumed: 4.60s
Validation Epoch: 12, Average loss: 0.0076, Accuracy: 0.4878
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.05384850257266,
                        "time": 3.0406202789963572,
                        "accuracy": 0.48779296875,
                        "total_cost": 545780.0515590325
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.4275
Profiling... [512/50176]	Loss: 1.4631
Profiling... [768/50176]	Loss: 1.6289
Profiling... [1024/50176]	Loss: 1.7003
Profiling... [1280/50176]	Loss: 1.6017
Profiling... [1536/50176]	Loss: 1.8045
Profiling... [1792/50176]	Loss: 1.5766
Profiling... [2048/50176]	Loss: 1.3824
Profiling... [2304/50176]	Loss: 1.4394
Profiling... [2560/50176]	Loss: 1.6299
Profiling... [2816/50176]	Loss: 1.4871
Profiling... [3072/50176]	Loss: 1.5625
Profiling... [3328/50176]	Loss: 1.7402
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 12, Average loss: 0.0075, Accuracy: 0.4883
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.04204911095829,
                        "time": 2.385284611998941,
                        "accuracy": 0.48828125,
                        "total_cost": 427721.5537609522
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6211
Profiling... [512/50176]	Loss: 1.5413
Profiling... [768/50176]	Loss: 1.5473
Profiling... [1024/50176]	Loss: 1.5151
Profiling... [1280/50176]	Loss: 1.4005
Profiling... [1536/50176]	Loss: 1.5145
Profiling... [1792/50176]	Loss: 1.6128
Profiling... [2048/50176]	Loss: 1.6139
Profiling... [2304/50176]	Loss: 1.6456
Profiling... [2560/50176]	Loss: 1.7544
Profiling... [2816/50176]	Loss: 1.5781
Profiling... [3072/50176]	Loss: 1.4785
Profiling... [3328/50176]	Loss: 1.7286
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 12, Average loss: 0.0077, Accuracy: 0.4784
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.05442176997612,
                        "time": 2.3860622640058864,
                        "accuracy": 0.47841796875,
                        "total_cost": 436682.00657739036
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5318
Profiling... [512/50176]	Loss: 1.4983
Profiling... [768/50176]	Loss: 1.5959
Profiling... [1024/50176]	Loss: 1.7556
Profiling... [1280/50176]	Loss: 1.6923
Profiling... [1536/50176]	Loss: 1.6127
Profiling... [1792/50176]	Loss: 1.5290
Profiling... [2048/50176]	Loss: 1.6223
Profiling... [2304/50176]	Loss: 1.5404
Profiling... [2560/50176]	Loss: 1.4520
Profiling... [2816/50176]	Loss: 1.4780
Profiling... [3072/50176]	Loss: 1.7134
Profiling... [3328/50176]	Loss: 1.6988
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 12, Average loss: 0.0079, Accuracy: 0.4738
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.06094316699404,
                        "time": 2.5337571909985854,
                        "accuracy": 0.473828125,
                        "total_cost": 468204.0678354772
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5094
Profiling... [512/50176]	Loss: 1.4769
Profiling... [768/50176]	Loss: 1.4077
Profiling... [1024/50176]	Loss: 1.5152
Profiling... [1280/50176]	Loss: 1.6521
Profiling... [1536/50176]	Loss: 1.5845
Profiling... [1792/50176]	Loss: 1.4640
Profiling... [2048/50176]	Loss: 1.5540
Profiling... [2304/50176]	Loss: 1.6161
Profiling... [2560/50176]	Loss: 1.5939
Profiling... [2816/50176]	Loss: 1.4296
Profiling... [3072/50176]	Loss: 1.6036
Profiling... [3328/50176]	Loss: 1.8557
Profile done
epoch 1 train time consumed: 4.57s
Validation Epoch: 12, Average loss: 0.0076, Accuracy: 0.4870
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.04593292242151,
                        "time": 3.0352544039997156,
                        "accuracy": 0.48701171875,
                        "total_cost": 545690.8516332404
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.3846
Profiling... [512/50176]	Loss: 1.4192
Profiling... [768/50176]	Loss: 1.5130
Profiling... [1024/50176]	Loss: 1.5979
Profiling... [1280/50176]	Loss: 1.6252
Profiling... [1536/50176]	Loss: 1.7317
Profiling... [1792/50176]	Loss: 1.5153
Profiling... [2048/50176]	Loss: 1.4658
Profiling... [2304/50176]	Loss: 1.5859
Profiling... [2560/50176]	Loss: 1.5851
Profiling... [2816/50176]	Loss: 1.5139
Profiling... [3072/50176]	Loss: 1.6107
Profiling... [3328/50176]	Loss: 1.7186
Profile done
epoch 1 train time consumed: 4.12s
Validation Epoch: 12, Average loss: 0.0075, Accuracy: 0.4893
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.03195963682539,
                        "time": 2.484743729000911,
                        "accuracy": 0.4892578125,
                        "total_cost": 444666.8828326301
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5168
Profiling... [512/50176]	Loss: 1.4550
Profiling... [768/50176]	Loss: 1.4211
Profiling... [1024/50176]	Loss: 1.5148
Profiling... [1280/50176]	Loss: 1.6306
Profiling... [1536/50176]	Loss: 1.6291
Profiling... [1792/50176]	Loss: 1.6600
Profiling... [2048/50176]	Loss: 1.5385
Profiling... [2304/50176]	Loss: 1.4021
Profiling... [2560/50176]	Loss: 1.5661
Profiling... [2816/50176]	Loss: 1.6092
Profiling... [3072/50176]	Loss: 1.5576
Profiling... [3328/50176]	Loss: 1.4244
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 12, Average loss: 0.0076, Accuracy: 0.4847
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.04307603518409,
                        "time": 2.391463890999148,
                        "accuracy": 0.48466796875,
                        "total_cost": 432026.60153431847
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.4356
Profiling... [512/50176]	Loss: 1.4468
Profiling... [768/50176]	Loss: 1.6234
Profiling... [1024/50176]	Loss: 1.6308
Profiling... [1280/50176]	Loss: 1.5301
Profiling... [1536/50176]	Loss: 1.5833
Profiling... [1792/50176]	Loss: 1.4937
Profiling... [2048/50176]	Loss: 1.6102
Profiling... [2304/50176]	Loss: 1.6827
Profiling... [2560/50176]	Loss: 1.3509
Profiling... [2816/50176]	Loss: 1.6461
Profiling... [3072/50176]	Loss: 1.7403
Profiling... [3328/50176]	Loss: 1.5652
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 12, Average loss: 0.0084, Accuracy: 0.4639
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.04858544710847,
                        "time": 2.549165411001013,
                        "accuracy": 0.4638671875,
                        "total_cost": 481166.471420629
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6394
Profiling... [512/50176]	Loss: 1.4775
Profiling... [768/50176]	Loss: 1.4783
Profiling... [1024/50176]	Loss: 1.6621
Profiling... [1280/50176]	Loss: 1.5594
Profiling... [1536/50176]	Loss: 1.7669
Profiling... [1792/50176]	Loss: 1.6993
Profiling... [2048/50176]	Loss: 1.6649
Profiling... [2304/50176]	Loss: 1.6516
Profiling... [2560/50176]	Loss: 1.6446
Profiling... [2816/50176]	Loss: 1.6531
Profiling... [3072/50176]	Loss: 1.4988
Profiling... [3328/50176]	Loss: 1.6233
Profile done
epoch 1 train time consumed: 4.66s
Validation Epoch: 12, Average loss: 0.0074, Accuracy: 0.4851
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.03344481618828,
                        "time": 3.0266765630003647,
                        "accuracy": 0.48505859375,
                        "total_cost": 546339.7079333647
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.4189
Profiling... [512/50176]	Loss: 1.6279
Profiling... [768/50176]	Loss: 1.7529
Profiling... [1024/50176]	Loss: 1.7683
Profiling... [1280/50176]	Loss: 1.7839
Profiling... [1536/50176]	Loss: 1.8393
Profiling... [1792/50176]	Loss: 1.7704
Profiling... [2048/50176]	Loss: 1.8189
Profiling... [2304/50176]	Loss: 1.8969
Profiling... [2560/50176]	Loss: 1.8905
Profiling... [2816/50176]	Loss: 1.7145
Profiling... [3072/50176]	Loss: 1.9135
Profiling... [3328/50176]	Loss: 1.8671
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 12, Average loss: 0.0100, Accuracy: 0.3768
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.02067983736585,
                        "time": 2.3788366000007954,
                        "accuracy": 0.3767578125,
                        "total_cost": 552832.1215190372
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6575
Profiling... [512/50176]	Loss: 1.7254
Profiling... [768/50176]	Loss: 1.7547
Profiling... [1024/50176]	Loss: 1.7342
Profiling... [1280/50176]	Loss: 1.8724
Profiling... [1536/50176]	Loss: 1.8627
Profiling... [1792/50176]	Loss: 1.8074
Profiling... [2048/50176]	Loss: 1.8998
Profiling... [2304/50176]	Loss: 1.8650
Profiling... [2560/50176]	Loss: 1.7997
Profiling... [2816/50176]	Loss: 1.6835
Profiling... [3072/50176]	Loss: 1.8348
Profiling... [3328/50176]	Loss: 1.7352
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 12, Average loss: 0.0107, Accuracy: 0.3509
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.03466804093529,
                        "time": 2.3865070050014765,
                        "accuracy": 0.35087890625,
                        "total_cost": 595520.0825204223
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.4952
Profiling... [512/50176]	Loss: 1.6578
Profiling... [768/50176]	Loss: 1.8136
Profiling... [1024/50176]	Loss: 1.8668
Profiling... [1280/50176]	Loss: 1.7040
Profiling... [1536/50176]	Loss: 1.7197
Profiling... [1792/50176]	Loss: 1.8883
Profiling... [2048/50176]	Loss: 1.8628
Profiling... [2304/50176]	Loss: 1.9447
Profiling... [2560/50176]	Loss: 1.8779
Profiling... [2816/50176]	Loss: 2.0613
Profiling... [3072/50176]	Loss: 1.9969
Profiling... [3328/50176]	Loss: 1.8910
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 12, Average loss: 0.0112, Accuracy: 0.3328
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.04052477061863,
                        "time": 2.528816670004744,
                        "accuracy": 0.3328125,
                        "total_cost": 665286.4673516263
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5598
Profiling... [512/50176]	Loss: 1.6800
Profiling... [768/50176]	Loss: 1.7350
Profiling... [1024/50176]	Loss: 1.7352
Profiling... [1280/50176]	Loss: 1.9816
Profiling... [1536/50176]	Loss: 2.0009
Profiling... [1792/50176]	Loss: 2.0900
Profiling... [2048/50176]	Loss: 1.5606
Profiling... [2304/50176]	Loss: 1.8487
Profiling... [2560/50176]	Loss: 1.9131
Profiling... [2816/50176]	Loss: 1.8214
Profiling... [3072/50176]	Loss: 1.8086
Profiling... [3328/50176]	Loss: 1.9677
Profile done
epoch 1 train time consumed: 4.60s
Validation Epoch: 12, Average loss: 0.0101, Accuracy: 0.3644
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.02626970262655,
                        "time": 3.027900178996788,
                        "accuracy": 0.36435546875,
                        "total_cost": 727624.3078029561
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5451
Profiling... [512/50176]	Loss: 1.8562
Profiling... [768/50176]	Loss: 1.7722
Profiling... [1024/50176]	Loss: 1.9249
Profiling... [1280/50176]	Loss: 1.6318
Profiling... [1536/50176]	Loss: 1.9077
Profiling... [1792/50176]	Loss: 1.8485
Profiling... [2048/50176]	Loss: 1.8020
Profiling... [2304/50176]	Loss: 1.9512
Profiling... [2560/50176]	Loss: 1.9548
Profiling... [2816/50176]	Loss: 1.7274
Profiling... [3072/50176]	Loss: 1.8280
Profiling... [3328/50176]	Loss: 1.8668
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 12, Average loss: 0.0098, Accuracy: 0.3734
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.0132324405888,
                        "time": 2.3850677560039912,
                        "accuracy": 0.3734375,
                        "total_cost": 559208.4171851386
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.3780
Profiling... [512/50176]	Loss: 1.9233
Profiling... [768/50176]	Loss: 1.7285
Profiling... [1024/50176]	Loss: 1.8994
Profiling... [1280/50176]	Loss: 1.8747
Profiling... [1536/50176]	Loss: 1.7160
Profiling... [1792/50176]	Loss: 1.7851
Profiling... [2048/50176]	Loss: 1.7415
Profiling... [2304/50176]	Loss: 1.8369
Profiling... [2560/50176]	Loss: 2.0019
Profiling... [2816/50176]	Loss: 1.8426
Profiling... [3072/50176]	Loss: 1.8119
Profiling... [3328/50176]	Loss: 1.8677
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 12, Average loss: 0.0111, Accuracy: 0.3744
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.02601407876858,
                        "time": 2.3882375600005616,
                        "accuracy": 0.3744140625,
                        "total_cost": 558491.1680630831
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.2245
Profiling... [512/50176]	Loss: 1.7347
Profiling... [768/50176]	Loss: 1.8372
Profiling... [1024/50176]	Loss: 2.1438
Profiling... [1280/50176]	Loss: 1.8895
Profiling... [1536/50176]	Loss: 1.7185
Profiling... [1792/50176]	Loss: 1.9915
Profiling... [2048/50176]	Loss: 1.9962
Profiling... [2304/50176]	Loss: 1.9207
Profiling... [2560/50176]	Loss: 1.8521
Profiling... [2816/50176]	Loss: 1.8370
Profiling... [3072/50176]	Loss: 1.9133
Profiling... [3328/50176]	Loss: 1.8911
Profile done
epoch 1 train time consumed: 4.09s
Validation Epoch: 12, Average loss: 0.0101, Accuracy: 0.3816
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.0313658435785,
                        "time": 2.530259038998338,
                        "accuracy": 0.381640625,
                        "total_cost": 580498.8144525549
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6489
Profiling... [512/50176]	Loss: 1.8148
Profiling... [768/50176]	Loss: 1.7434
Profiling... [1024/50176]	Loss: 1.8046
Profiling... [1280/50176]	Loss: 1.9386
Profiling... [1536/50176]	Loss: 1.8616
Profiling... [1792/50176]	Loss: 1.9175
Profiling... [2048/50176]	Loss: 1.9332
Profiling... [2304/50176]	Loss: 1.9476
Profiling... [2560/50176]	Loss: 2.0198
Profiling... [2816/50176]	Loss: 1.9006
Profiling... [3072/50176]	Loss: 1.9275
Profiling... [3328/50176]	Loss: 2.0858
Profile done
epoch 1 train time consumed: 4.61s
Validation Epoch: 12, Average loss: 0.0118, Accuracy: 0.3354
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.01711934607715,
                        "time": 3.051378577001742,
                        "accuracy": 0.33544921875,
                        "total_cost": 796453.1298683921
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5857
Profiling... [512/50176]	Loss: 1.7350
Profiling... [768/50176]	Loss: 1.7716
Profiling... [1024/50176]	Loss: 1.8321
Profiling... [1280/50176]	Loss: 1.9540
Profiling... [1536/50176]	Loss: 1.7832
Profiling... [1792/50176]	Loss: 1.8387
Profiling... [2048/50176]	Loss: 1.8253
Profiling... [2304/50176]	Loss: 1.7872
Profiling... [2560/50176]	Loss: 1.6027
Profiling... [2816/50176]	Loss: 1.8114
Profiling... [3072/50176]	Loss: 1.9679
Profiling... [3328/50176]	Loss: 1.6743
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 12, Average loss: 0.0126, Accuracy: 0.2850
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.00523466494536,
                        "time": 2.3801906350054196,
                        "accuracy": 0.2849609375,
                        "total_cost": 731336.5806810618
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7547
Profiling... [512/50176]	Loss: 1.8163
Profiling... [768/50176]	Loss: 1.8558
Profiling... [1024/50176]	Loss: 1.7664
Profiling... [1280/50176]	Loss: 1.8589
Profiling... [1536/50176]	Loss: 1.7989
Profiling... [1792/50176]	Loss: 1.9114
Profiling... [2048/50176]	Loss: 1.7999
Profiling... [2304/50176]	Loss: 2.0629
Profiling... [2560/50176]	Loss: 1.7218
Profiling... [2816/50176]	Loss: 1.7944
Profiling... [3072/50176]	Loss: 1.9959
Profiling... [3328/50176]	Loss: 2.0214
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 12, Average loss: 0.0100, Accuracy: 0.3638
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.01638327846584,
                        "time": 2.4874623340001563,
                        "accuracy": 0.36376953125,
                        "total_cost": 598716.3334049729
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6079
Profiling... [512/50176]	Loss: 1.6188
Profiling... [768/50176]	Loss: 1.9557
Profiling... [1024/50176]	Loss: 1.7963
Profiling... [1280/50176]	Loss: 1.9299
Profiling... [1536/50176]	Loss: 1.7970
Profiling... [1792/50176]	Loss: 1.8815
Profiling... [2048/50176]	Loss: 1.7787
Profiling... [2304/50176]	Loss: 1.8552
Profiling... [2560/50176]	Loss: 1.6463
Profiling... [2816/50176]	Loss: 1.9335
Profiling... [3072/50176]	Loss: 1.9088
Profiling... [3328/50176]	Loss: 1.8143
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 12, Average loss: 0.0111, Accuracy: 0.3522
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.02111282005227,
                        "time": 2.5340783779975027,
                        "accuracy": 0.35224609375,
                        "total_cost": 629890.1002166125
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5704
Profiling... [512/50176]	Loss: 1.5434
Profiling... [768/50176]	Loss: 1.7564
Profiling... [1024/50176]	Loss: 1.8519
Profiling... [1280/50176]	Loss: 1.7778
Profiling... [1536/50176]	Loss: 2.0463
Profiling... [1792/50176]	Loss: 1.7272
Profiling... [2048/50176]	Loss: 1.8778
Profiling... [2304/50176]	Loss: 2.0263
Profiling... [2560/50176]	Loss: 2.0100
Profiling... [2816/50176]	Loss: 1.8409
Profiling... [3072/50176]	Loss: 1.9165
Profiling... [3328/50176]	Loss: 1.7797
Profile done
epoch 1 train time consumed: 4.67s
Validation Epoch: 12, Average loss: 0.0103, Accuracy: 0.3895
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.00764844213673,
                        "time": 3.020157992003078,
                        "accuracy": 0.389453125,
                        "total_cost": 678993.1005318428
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.5512
Profiling... [1024/50176]	Loss: 1.4256
Profiling... [1536/50176]	Loss: 1.5529
Profiling... [2048/50176]	Loss: 1.4516
Profiling... [2560/50176]	Loss: 1.5728
Profiling... [3072/50176]	Loss: 1.3749
Profiling... [3584/50176]	Loss: 1.3457
Profiling... [4096/50176]	Loss: 1.2881
Profiling... [4608/50176]	Loss: 1.4434
Profiling... [5120/50176]	Loss: 1.3675
Profiling... [5632/50176]	Loss: 1.3801
Profiling... [6144/50176]	Loss: 1.3520
Profiling... [6656/50176]	Loss: 1.4278
Profile done
epoch 1 train time consumed: 6.93s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5382
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.00680576951751,
                        "time": 4.606867735004926,
                        "accuracy": 0.53818359375,
                        "total_cost": 749490.5802309517
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4473
Profiling... [1024/50176]	Loss: 1.5049
Profiling... [1536/50176]	Loss: 1.6222
Profiling... [2048/50176]	Loss: 1.4486
Profiling... [2560/50176]	Loss: 1.4368
Profiling... [3072/50176]	Loss: 1.5165
Profiling... [3584/50176]	Loss: 1.4631
Profiling... [4096/50176]	Loss: 1.4256
Profiling... [4608/50176]	Loss: 1.4712
Profiling... [5120/50176]	Loss: 1.3987
Profiling... [5632/50176]	Loss: 1.3624
Profiling... [6144/50176]	Loss: 1.4395
Profiling... [6656/50176]	Loss: 1.3778
Profile done
epoch 1 train time consumed: 6.93s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5392
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.0323647102722,
                        "time": 4.6201187770057,
                        "accuracy": 0.53916015625,
                        "total_cost": 750285.0680515042
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4769
Profiling... [1024/50176]	Loss: 1.5576
Profiling... [1536/50176]	Loss: 1.5083
Profiling... [2048/50176]	Loss: 1.3235
Profiling... [2560/50176]	Loss: 1.3770
Profiling... [3072/50176]	Loss: 1.4954
Profiling... [3584/50176]	Loss: 1.3649
Profiling... [4096/50176]	Loss: 1.4489
Profiling... [4608/50176]	Loss: 1.4098
Profiling... [5120/50176]	Loss: 1.4152
Profiling... [5632/50176]	Loss: 1.4371
Profiling... [6144/50176]	Loss: 1.4263
Profiling... [6656/50176]	Loss: 1.3790
Profile done
epoch 1 train time consumed: 7.25s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5397
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.04091313746036,
                        "time": 4.850038421005593,
                        "accuracy": 0.53974609375,
                        "total_cost": 786767.9232150645
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5225
Profiling... [1024/50176]	Loss: 1.5104
Profiling... [1536/50176]	Loss: 1.4771
Profiling... [2048/50176]	Loss: 1.5081
Profiling... [2560/50176]	Loss: 1.5556
Profiling... [3072/50176]	Loss: 1.3292
Profiling... [3584/50176]	Loss: 1.4361
Profiling... [4096/50176]	Loss: 1.3648
Profiling... [4608/50176]	Loss: 1.5027
Profiling... [5120/50176]	Loss: 1.4144
Profiling... [5632/50176]	Loss: 1.4109
Profiling... [6144/50176]	Loss: 1.3144
Profiling... [6656/50176]	Loss: 1.5023
Profile done
epoch 1 train time consumed: 8.65s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5371
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.01304273020757,
                        "time": 6.0393273969966685,
                        "accuracy": 0.537109375,
                        "total_cost": 984502.3246587276
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.5660
Profiling... [1024/50176]	Loss: 1.4486
Profiling... [1536/50176]	Loss: 1.3774
Profiling... [2048/50176]	Loss: 1.4025
Profiling... [2560/50176]	Loss: 1.5319
Profiling... [3072/50176]	Loss: 1.4140
Profiling... [3584/50176]	Loss: 1.4985
Profiling... [4096/50176]	Loss: 1.5054
Profiling... [4608/50176]	Loss: 1.4178
Profiling... [5120/50176]	Loss: 1.4887
Profiling... [5632/50176]	Loss: 1.4161
Profiling... [6144/50176]	Loss: 1.3640
Profiling... [6656/50176]	Loss: 1.4005
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5383
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.01200867963996,
                        "time": 4.498911337002937,
                        "accuracy": 0.53828125,
                        "total_cost": 731794.4047780944
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5458
Profiling... [1024/50176]	Loss: 1.4747
Profiling... [1536/50176]	Loss: 1.4295
Profiling... [2048/50176]	Loss: 1.3940
Profiling... [2560/50176]	Loss: 1.4787
Profiling... [3072/50176]	Loss: 1.4323
Profiling... [3584/50176]	Loss: 1.5228
Profiling... [4096/50176]	Loss: 1.4635
Profiling... [4608/50176]	Loss: 1.4562
Profiling... [5120/50176]	Loss: 1.4422
Profiling... [5632/50176]	Loss: 1.5596
Profiling... [6144/50176]	Loss: 1.4750
Profiling... [6656/50176]	Loss: 1.5043
Profile done
epoch 1 train time consumed: 7.07s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5371
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.03703232689168,
                        "time": 4.614076080993982,
                        "accuracy": 0.537109375,
                        "total_cost": 752164.761336696
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4553
Profiling... [1024/50176]	Loss: 1.4171
Profiling... [1536/50176]	Loss: 1.3848
Profiling... [2048/50176]	Loss: 1.3504
Profiling... [2560/50176]	Loss: 1.4480
Profiling... [3072/50176]	Loss: 1.3948
Profiling... [3584/50176]	Loss: 1.4920
Profiling... [4096/50176]	Loss: 1.4430
Profiling... [4608/50176]	Loss: 1.5744
Profiling... [5120/50176]	Loss: 1.4346
Profiling... [5632/50176]	Loss: 1.4527
Profiling... [6144/50176]	Loss: 1.4626
Profiling... [6656/50176]	Loss: 1.3231
Profile done
epoch 1 train time consumed: 7.22s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5409
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.04506548072264,
                        "time": 4.86611102000461,
                        "accuracy": 0.54091796875,
                        "total_cost": 787665.0746321871
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5913
Profiling... [1024/50176]	Loss: 1.5146
Profiling... [1536/50176]	Loss: 1.4208
Profiling... [2048/50176]	Loss: 1.3363
Profiling... [2560/50176]	Loss: 1.4352
Profiling... [3072/50176]	Loss: 1.5375
Profiling... [3584/50176]	Loss: 1.4695
Profiling... [4096/50176]	Loss: 1.4093
Profiling... [4608/50176]	Loss: 1.3160
Profiling... [5120/50176]	Loss: 1.4536
Profiling... [5632/50176]	Loss: 1.3583
Profiling... [6144/50176]	Loss: 1.3595
Profiling... [6656/50176]	Loss: 1.3857
Profile done
epoch 1 train time consumed: 8.76s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5438
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.01675709106091,
                        "time": 6.045808878996468,
                        "accuracy": 0.54384765625,
                        "total_cost": 973347.8348025494
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4777
Profiling... [1024/50176]	Loss: 1.4573
Profiling... [1536/50176]	Loss: 1.5065
Profiling... [2048/50176]	Loss: 1.4973
Profiling... [2560/50176]	Loss: 1.4866
Profiling... [3072/50176]	Loss: 1.4006
Profiling... [3584/50176]	Loss: 1.4889
Profiling... [4096/50176]	Loss: 1.4422
Profiling... [4608/50176]	Loss: 1.4464
Profiling... [5120/50176]	Loss: 1.3937
Profiling... [5632/50176]	Loss: 1.5552
Profiling... [6144/50176]	Loss: 1.3070
Profiling... [6656/50176]	Loss: 1.5187
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5434
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.01688360175226,
                        "time": 4.495785100996727,
                        "accuracy": 0.543359375,
                        "total_cost": 724451.4627919929
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5246
Profiling... [1024/50176]	Loss: 1.4542
Profiling... [1536/50176]	Loss: 1.5705
Profiling... [2048/50176]	Loss: 1.4288
Profiling... [2560/50176]	Loss: 1.5287
Profiling... [3072/50176]	Loss: 1.5716
Profiling... [3584/50176]	Loss: 1.4680
Profiling... [4096/50176]	Loss: 1.3544
Profiling... [4608/50176]	Loss: 1.3882
Profiling... [5120/50176]	Loss: 1.4068
Profiling... [5632/50176]	Loss: 1.4045
Profiling... [6144/50176]	Loss: 1.4732
Profiling... [6656/50176]	Loss: 1.4499
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5367
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.04256113009701,
                        "time": 4.615723022005113,
                        "accuracy": 0.53671875,
                        "total_cost": 752980.884053139
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5313
Profiling... [1024/50176]	Loss: 1.5914
Profiling... [1536/50176]	Loss: 1.4906
Profiling... [2048/50176]	Loss: 1.5934
Profiling... [2560/50176]	Loss: 1.4145
Profiling... [3072/50176]	Loss: 1.4691
Profiling... [3584/50176]	Loss: 1.4834
Profiling... [4096/50176]	Loss: 1.4345
Profiling... [4608/50176]	Loss: 1.4731
Profiling... [5120/50176]	Loss: 1.2893
Profiling... [5632/50176]	Loss: 1.4279
Profiling... [6144/50176]	Loss: 1.4057
Profiling... [6656/50176]	Loss: 1.5002
Profile done
epoch 1 train time consumed: 7.30s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5437
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.0507850835583,
                        "time": 4.883123774001433,
                        "accuracy": 0.54375,
                        "total_cost": 786302.1467129525
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5876
Profiling... [1024/50176]	Loss: 1.5555
Profiling... [1536/50176]	Loss: 1.4681
Profiling... [2048/50176]	Loss: 1.4623
Profiling... [2560/50176]	Loss: 1.3359
Profiling... [3072/50176]	Loss: 1.4359
Profiling... [3584/50176]	Loss: 1.4349
Profiling... [4096/50176]	Loss: 1.4070
Profiling... [4608/50176]	Loss: 1.4008
Profiling... [5120/50176]	Loss: 1.3046
Profiling... [5632/50176]	Loss: 1.5220
Profiling... [6144/50176]	Loss: 1.4209
Profiling... [6656/50176]	Loss: 1.4473
Profile done
epoch 1 train time consumed: 8.67s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.5403
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.02395904557581,
                        "time": 6.049795797000115,
                        "accuracy": 0.54033203125,
                        "total_cost": 980326.9331381245
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.5006
Profiling... [1024/50176]	Loss: 1.3965
Profiling... [1536/50176]	Loss: 1.3569
Profiling... [2048/50176]	Loss: 1.5114
Profiling... [2560/50176]	Loss: 1.4730
Profiling... [3072/50176]	Loss: 1.5889
Profiling... [3584/50176]	Loss: 1.6196
Profiling... [4096/50176]	Loss: 1.4627
Profiling... [4608/50176]	Loss: 1.5585
Profiling... [5120/50176]	Loss: 1.4442
Profiling... [5632/50176]	Loss: 1.5486
Profiling... [6144/50176]	Loss: 1.4596
Profiling... [6656/50176]	Loss: 1.5666
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 12, Average loss: 0.0035, Accuracy: 0.5035
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.02367144212171,
                        "time": 4.504895985002804,
                        "accuracy": 0.503515625,
                        "total_cost": 783362.4449640124
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5864
Profiling... [1024/50176]	Loss: 1.5701
Profiling... [1536/50176]	Loss: 1.3647
Profiling... [2048/50176]	Loss: 1.4462
Profiling... [2560/50176]	Loss: 1.4950
Profiling... [3072/50176]	Loss: 1.6475
Profiling... [3584/50176]	Loss: 1.5098
Profiling... [4096/50176]	Loss: 1.4727
Profiling... [4608/50176]	Loss: 1.6291
Profiling... [5120/50176]	Loss: 1.5379
Profiling... [5632/50176]	Loss: 1.6254
Profiling... [6144/50176]	Loss: 1.5531
Profiling... [6656/50176]	Loss: 1.4765
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 12, Average loss: 0.0037, Accuracy: 0.4945
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.04977665025714,
                        "time": 4.5290723880025325,
                        "accuracy": 0.49453125,
                        "total_cost": 801874.7122600724
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5461
Profiling... [1024/50176]	Loss: 1.3607
Profiling... [1536/50176]	Loss: 1.4966
Profiling... [2048/50176]	Loss: 1.4673
Profiling... [2560/50176]	Loss: 1.5681
Profiling... [3072/50176]	Loss: 1.4537
Profiling... [3584/50176]	Loss: 1.5613
Profiling... [4096/50176]	Loss: 1.4320
Profiling... [4608/50176]	Loss: 1.4406
Profiling... [5120/50176]	Loss: 1.4408
Profiling... [5632/50176]	Loss: 1.5295
Profiling... [6144/50176]	Loss: 1.5125
Profiling... [6656/50176]	Loss: 1.5503
Profile done
epoch 1 train time consumed: 7.33s
Validation Epoch: 12, Average loss: 0.0036, Accuracy: 0.5066
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.05656757878519,
                        "time": 4.85516491800081,
                        "accuracy": 0.506640625,
                        "total_cost": 839063.8078970847
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5544
Profiling... [1024/50176]	Loss: 1.4263
Profiling... [1536/50176]	Loss: 1.4217
Profiling... [2048/50176]	Loss: 1.5063
Profiling... [2560/50176]	Loss: 1.4140
Profiling... [3072/50176]	Loss: 1.5648
Profiling... [3584/50176]	Loss: 1.4867
Profiling... [4096/50176]	Loss: 1.6491
Profiling... [4608/50176]	Loss: 1.4732
Profiling... [5120/50176]	Loss: 1.4636
Profiling... [5632/50176]	Loss: 1.6116
Profiling... [6144/50176]	Loss: 1.4484
Profiling... [6656/50176]	Loss: 1.4095
Profile done
epoch 1 train time consumed: 8.72s
Validation Epoch: 12, Average loss: 0.0038, Accuracy: 0.4833
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.02843315397334,
                        "time": 6.020177063001029,
                        "accuracy": 0.48330078125,
                        "total_cost": 1090643.2373814662
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4789
Profiling... [1024/50176]	Loss: 1.4105
Profiling... [1536/50176]	Loss: 1.5083
Profiling... [2048/50176]	Loss: 1.5859
Profiling... [2560/50176]	Loss: 1.4911
Profiling... [3072/50176]	Loss: 1.3994
Profiling... [3584/50176]	Loss: 1.5646
Profiling... [4096/50176]	Loss: 1.4605
Profiling... [4608/50176]	Loss: 1.4880
Profiling... [5120/50176]	Loss: 1.5598
Profiling... [5632/50176]	Loss: 1.5609
Profiling... [6144/50176]	Loss: 1.4135
Profiling... [6656/50176]	Loss: 1.4216
Profile done
epoch 1 train time consumed: 6.83s
Validation Epoch: 12, Average loss: 0.0035, Accuracy: 0.5125
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.02726495792508,
                        "time": 4.5031359779968625,
                        "accuracy": 0.5125,
                        "total_cost": 769329.0501743958
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5708
Profiling... [1024/50176]	Loss: 1.4712
Profiling... [1536/50176]	Loss: 1.5137
Profiling... [2048/50176]	Loss: 1.5481
Profiling... [2560/50176]	Loss: 1.5518
Profiling... [3072/50176]	Loss: 1.4397
Profiling... [3584/50176]	Loss: 1.4695
Profiling... [4096/50176]	Loss: 1.5023
Profiling... [4608/50176]	Loss: 1.5926
Profiling... [5120/50176]	Loss: 1.5210
Profiling... [5632/50176]	Loss: 1.5204
Profiling... [6144/50176]	Loss: 1.6056
Profiling... [6656/50176]	Loss: 1.5452
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 12, Average loss: 0.0035, Accuracy: 0.5077
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.05289991108667,
                        "time": 4.516098965003039,
                        "accuracy": 0.50771484375,
                        "total_cost": 778815.513071924
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5036
Profiling... [1024/50176]	Loss: 1.5568
Profiling... [1536/50176]	Loss: 1.5878
Profiling... [2048/50176]	Loss: 1.4967
Profiling... [2560/50176]	Loss: 1.6864
Profiling... [3072/50176]	Loss: 1.5167
Profiling... [3584/50176]	Loss: 1.5313
Profiling... [4096/50176]	Loss: 1.5172
Profiling... [4608/50176]	Loss: 1.5670
Profiling... [5120/50176]	Loss: 1.4081
Profiling... [5632/50176]	Loss: 1.5446
Profiling... [6144/50176]	Loss: 1.5133
Profiling... [6656/50176]	Loss: 1.4558
Profile done
epoch 1 train time consumed: 7.37s
Validation Epoch: 12, Average loss: 0.0038, Accuracy: 0.4842
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.06115610612247,
                        "time": 4.883747421998123,
                        "accuracy": 0.4841796875,
                        "total_cost": 883156.4673257332
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5365
Profiling... [1024/50176]	Loss: 1.6275
Profiling... [1536/50176]	Loss: 1.4098
Profiling... [2048/50176]	Loss: 1.4803
Profiling... [2560/50176]	Loss: 1.3580
Profiling... [3072/50176]	Loss: 1.5464
Profiling... [3584/50176]	Loss: 1.5590
Profiling... [4096/50176]	Loss: 1.4866
Profiling... [4608/50176]	Loss: 1.4725
Profiling... [5120/50176]	Loss: 1.4859
Profiling... [5632/50176]	Loss: 1.5011
Profiling... [6144/50176]	Loss: 1.5096
Profiling... [6656/50176]	Loss: 1.5161
Profile done
epoch 1 train time consumed: 8.70s
Validation Epoch: 12, Average loss: 0.0038, Accuracy: 0.4843
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.0337868108931,
                        "time": 6.072411239998473,
                        "accuracy": 0.48427734375,
                        "total_cost": 1097887.8536979905
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4406
Profiling... [1024/50176]	Loss: 1.4711
Profiling... [1536/50176]	Loss: 1.5998
Profiling... [2048/50176]	Loss: 1.5060
Profiling... [2560/50176]	Loss: 1.4908
Profiling... [3072/50176]	Loss: 1.4379
Profiling... [3584/50176]	Loss: 1.4672
Profiling... [4096/50176]	Loss: 1.4515
Profiling... [4608/50176]	Loss: 1.4833
Profiling... [5120/50176]	Loss: 1.4601
Profiling... [5632/50176]	Loss: 1.3874
Profiling... [6144/50176]	Loss: 1.6255
Profiling... [6656/50176]	Loss: 1.5179
Profile done
epoch 1 train time consumed: 7.00s
Validation Epoch: 12, Average loss: 0.0037, Accuracy: 0.4946
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.03349290257913,
                        "time": 4.609470452996902,
                        "accuracy": 0.49462890625,
                        "total_cost": 815948.030019368
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6667
Profiling... [1024/50176]	Loss: 1.4637
Profiling... [1536/50176]	Loss: 1.4929
Profiling... [2048/50176]	Loss: 1.5374
Profiling... [2560/50176]	Loss: 1.4711
Profiling... [3072/50176]	Loss: 1.5039
Profiling... [3584/50176]	Loss: 1.5650
Profiling... [4096/50176]	Loss: 1.4177
Profiling... [4608/50176]	Loss: 1.3870
Profiling... [5120/50176]	Loss: 1.4175
Profiling... [5632/50176]	Loss: 1.5306
Profiling... [6144/50176]	Loss: 1.4803
Profiling... [6656/50176]	Loss: 1.3852
Profile done
epoch 1 train time consumed: 6.83s
Validation Epoch: 12, Average loss: 0.0040, Accuracy: 0.4808
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.05903989154513,
                        "time": 4.525603299000068,
                        "accuracy": 0.48076171875,
                        "total_cost": 824209.5120975414
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4530
Profiling... [1024/50176]	Loss: 1.5643
Profiling... [1536/50176]	Loss: 1.6245
Profiling... [2048/50176]	Loss: 1.5423
Profiling... [2560/50176]	Loss: 1.6757
Profiling... [3072/50176]	Loss: 1.5223
Profiling... [3584/50176]	Loss: 1.4727
Profiling... [4096/50176]	Loss: 1.5119
Profiling... [4608/50176]	Loss: 1.5489
Profiling... [5120/50176]	Loss: 1.4279
Profiling... [5632/50176]	Loss: 1.5510
Profiling... [6144/50176]	Loss: 1.4740
Profiling... [6656/50176]	Loss: 1.5361
Profile done
epoch 1 train time consumed: 7.28s
Validation Epoch: 12, Average loss: 0.0036, Accuracy: 0.4972
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.0671629007486,
                        "time": 4.87902038100583,
                        "accuracy": 0.49716796875,
                        "total_cost": 859251.9594904177
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4657
Profiling... [1024/50176]	Loss: 1.6109
Profiling... [1536/50176]	Loss: 1.5843
Profiling... [2048/50176]	Loss: 1.5607
Profiling... [2560/50176]	Loss: 1.3989
Profiling... [3072/50176]	Loss: 1.4786
Profiling... [3584/50176]	Loss: 1.5032
Profiling... [4096/50176]	Loss: 1.5139
Profiling... [4608/50176]	Loss: 1.5216
Profiling... [5120/50176]	Loss: 1.4230
Profiling... [5632/50176]	Loss: 1.5171
Profiling... [6144/50176]	Loss: 1.5275
Profiling... [6656/50176]	Loss: 1.5490
Profile done
epoch 1 train time consumed: 8.84s
Validation Epoch: 12, Average loss: 0.0039, Accuracy: 0.4813
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.03923691755756,
                        "time": 6.0417479389943765,
                        "accuracy": 0.48125,
                        "total_cost": 1099215.4656089633
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4889
Profiling... [1024/50176]	Loss: 1.6831
Profiling... [1536/50176]	Loss: 1.5697
Profiling... [2048/50176]	Loss: 1.6651
Profiling... [2560/50176]	Loss: 1.8549
Profiling... [3072/50176]	Loss: 1.7244
Profiling... [3584/50176]	Loss: 1.7013
Profiling... [4096/50176]	Loss: 1.6969
Profiling... [4608/50176]	Loss: 1.6827
Profiling... [5120/50176]	Loss: 1.6095
Profiling... [5632/50176]	Loss: 1.7527
Profiling... [6144/50176]	Loss: 1.7322
Profiling... [6656/50176]	Loss: 1.8149
Profile done
epoch 1 train time consumed: 6.95s
Validation Epoch: 12, Average loss: 0.0053, Accuracy: 0.3523
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.03814153678817,
                        "time": 4.602652180001314,
                        "accuracy": 0.35234375,
                        "total_cost": 1143753.804914913
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4275
Profiling... [1024/50176]	Loss: 1.6352
Profiling... [1536/50176]	Loss: 1.5356
Profiling... [2048/50176]	Loss: 1.5919
Profiling... [2560/50176]	Loss: 1.8180
Profiling... [3072/50176]	Loss: 1.7804
Profiling... [3584/50176]	Loss: 1.7199
Profiling... [4096/50176]	Loss: 1.7920
Profiling... [4608/50176]	Loss: 1.6684
Profiling... [5120/50176]	Loss: 1.7579
Profiling... [5632/50176]	Loss: 1.7058
Profiling... [6144/50176]	Loss: 1.6025
Profiling... [6656/50176]	Loss: 1.6993
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 12, Average loss: 0.0049, Accuracy: 0.3860
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.06273824134327,
                        "time": 4.51676788199984,
                        "accuracy": 0.38603515625,
                        "total_cost": 1024452.7751643001
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5577
Profiling... [1024/50176]	Loss: 1.7526
Profiling... [1536/50176]	Loss: 1.6181
Profiling... [2048/50176]	Loss: 1.6690
Profiling... [2560/50176]	Loss: 1.8342
Profiling... [3072/50176]	Loss: 1.7140
Profiling... [3584/50176]	Loss: 1.7211
Profiling... [4096/50176]	Loss: 1.6974
Profiling... [4608/50176]	Loss: 1.6293
Profiling... [5120/50176]	Loss: 1.6638
Profiling... [5632/50176]	Loss: 1.7209
Profiling... [6144/50176]	Loss: 1.7252
Profiling... [6656/50176]	Loss: 1.6306
Profile done
epoch 1 train time consumed: 7.22s
Validation Epoch: 12, Average loss: 0.0051, Accuracy: 0.3862
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.07098888572102,
                        "time": 4.854703557997709,
                        "accuracy": 0.38623046875,
                        "total_cost": 1100543.5514144776
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6281
Profiling... [1024/50176]	Loss: 1.5962
Profiling... [1536/50176]	Loss: 1.7361
Profiling... [2048/50176]	Loss: 1.7705
Profiling... [2560/50176]	Loss: 1.5860
Profiling... [3072/50176]	Loss: 1.7029
Profiling... [3584/50176]	Loss: 1.9994
Profiling... [4096/50176]	Loss: 1.7604
Profiling... [4608/50176]	Loss: 1.8164
Profiling... [5120/50176]	Loss: 1.7734
Profiling... [5632/50176]	Loss: 1.7536
Profiling... [6144/50176]	Loss: 1.7155
Profiling... [6656/50176]	Loss: 1.7981
Profile done
epoch 1 train time consumed: 8.73s
Validation Epoch: 12, Average loss: 0.0050, Accuracy: 0.3746
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.0456831151987,
                        "time": 6.06777385299938,
                        "accuracy": 0.374609375,
                        "total_cost": 1418213.876373149
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4530
Profiling... [1024/50176]	Loss: 1.7128
Profiling... [1536/50176]	Loss: 1.7442
Profiling... [2048/50176]	Loss: 1.6508
Profiling... [2560/50176]	Loss: 1.6313
Profiling... [3072/50176]	Loss: 1.7535
Profiling... [3584/50176]	Loss: 1.7439
Profiling... [4096/50176]	Loss: 1.7289
Profiling... [4608/50176]	Loss: 1.5267
Profiling... [5120/50176]	Loss: 1.7071
Profiling... [5632/50176]	Loss: 1.6830
Profiling... [6144/50176]	Loss: 1.7805
Profiling... [6656/50176]	Loss: 1.7217
Profile done
epoch 1 train time consumed: 6.98s
Validation Epoch: 12, Average loss: 0.0052, Accuracy: 0.3729
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.04446539659536,
                        "time": 4.60628112500126,
                        "accuracy": 0.3728515625,
                        "total_cost": 1081696.5769636377
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5687
Profiling... [1024/50176]	Loss: 1.6671
Profiling... [1536/50176]	Loss: 1.6088
Profiling... [2048/50176]	Loss: 1.6652
Profiling... [2560/50176]	Loss: 1.7305
Profiling... [3072/50176]	Loss: 1.8654
Profiling... [3584/50176]	Loss: 1.7285
Profiling... [4096/50176]	Loss: 1.7643
Profiling... [4608/50176]	Loss: 1.6629
Profiling... [5120/50176]	Loss: 1.6993
Profiling... [5632/50176]	Loss: 1.6950
Profiling... [6144/50176]	Loss: 1.6583
Profiling... [6656/50176]	Loss: 1.6871
Profile done
epoch 1 train time consumed: 6.91s
Validation Epoch: 12, Average loss: 0.0057, Accuracy: 0.3438
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.06847670670403,
                        "time": 4.520029237995914,
                        "accuracy": 0.34375,
                        "total_cost": 1151302.850180557
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6167
Profiling... [1024/50176]	Loss: 1.6092
Profiling... [1536/50176]	Loss: 1.6457
Profiling... [2048/50176]	Loss: 1.6416
Profiling... [2560/50176]	Loss: 1.7527
Profiling... [3072/50176]	Loss: 1.7447
Profiling... [3584/50176]	Loss: 1.6214
Profiling... [4096/50176]	Loss: 1.8843
Profiling... [4608/50176]	Loss: 1.6742
Profiling... [5120/50176]	Loss: 1.6225
Profiling... [5632/50176]	Loss: 1.8749
Profiling... [6144/50176]	Loss: 1.6785
Profiling... [6656/50176]	Loss: 1.7352
Profile done
epoch 1 train time consumed: 7.30s
Validation Epoch: 12, Average loss: 0.0048, Accuracy: 0.3968
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.07660225459287,
                        "time": 4.886963952005317,
                        "accuracy": 0.39677734375,
                        "total_cost": 1078408.57514173
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4695
Profiling... [1024/50176]	Loss: 1.7218
Profiling... [1536/50176]	Loss: 1.6452
Profiling... [2048/50176]	Loss: 1.6909
Profiling... [2560/50176]	Loss: 1.7615
Profiling... [3072/50176]	Loss: 1.7888
Profiling... [3584/50176]	Loss: 1.6018
Profiling... [4096/50176]	Loss: 1.6954
Profiling... [4608/50176]	Loss: 1.6965
Profiling... [5120/50176]	Loss: 1.6898
Profiling... [5632/50176]	Loss: 1.7564
Profiling... [6144/50176]	Loss: 1.6750
Profiling... [6656/50176]	Loss: 1.8013
Profile done
epoch 1 train time consumed: 8.79s
Validation Epoch: 12, Average loss: 0.0043, Accuracy: 0.4346
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.04905719255741,
                        "time": 6.040986751999299,
                        "accuracy": 0.4345703125,
                        "total_cost": 1217135.2023079756
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.5981
Profiling... [1024/50176]	Loss: 1.7291
Profiling... [1536/50176]	Loss: 1.6268
Profiling... [2048/50176]	Loss: 1.6694
Profiling... [2560/50176]	Loss: 1.6524
Profiling... [3072/50176]	Loss: 1.7719
Profiling... [3584/50176]	Loss: 1.8066
Profiling... [4096/50176]	Loss: 1.7031
Profiling... [4608/50176]	Loss: 1.7505
Profiling... [5120/50176]	Loss: 1.7492
Profiling... [5632/50176]	Loss: 1.6912
Profiling... [6144/50176]	Loss: 1.7178
Profiling... [6656/50176]	Loss: 1.5752
Profile done
epoch 1 train time consumed: 7.08s
Validation Epoch: 12, Average loss: 0.0044, Accuracy: 0.4275
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.04706159402181,
                        "time": 4.615249304995814,
                        "accuracy": 0.4275390625,
                        "total_cost": 945170.9269208893
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5467
Profiling... [1024/50176]	Loss: 1.5638
Profiling... [1536/50176]	Loss: 1.7256
Profiling... [2048/50176]	Loss: 1.6087
Profiling... [2560/50176]	Loss: 1.7742
Profiling... [3072/50176]	Loss: 1.6090
Profiling... [3584/50176]	Loss: 1.6628
Profiling... [4096/50176]	Loss: 1.7197
Profiling... [4608/50176]	Loss: 1.7631
Profiling... [5120/50176]	Loss: 1.6838
Profiling... [5632/50176]	Loss: 1.7048
Profiling... [6144/50176]	Loss: 1.7232
Profiling... [6656/50176]	Loss: 1.6255
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 12, Average loss: 0.0050, Accuracy: 0.3967
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.07141083159921,
                        "time": 4.5145050729988725,
                        "accuracy": 0.3966796875,
                        "total_cost": 996463.1271139716
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5054
Profiling... [1024/50176]	Loss: 1.7634
Profiling... [1536/50176]	Loss: 1.7606
Profiling... [2048/50176]	Loss: 1.5613
Profiling... [2560/50176]	Loss: 1.7996
Profiling... [3072/50176]	Loss: 1.6237
Profiling... [3584/50176]	Loss: 1.7722
Profiling... [4096/50176]	Loss: 1.6239
Profiling... [4608/50176]	Loss: 1.6376
Profiling... [5120/50176]	Loss: 1.8408
Profiling... [5632/50176]	Loss: 1.6118
Profiling... [6144/50176]	Loss: 1.7446
Profiling... [6656/50176]	Loss: 1.7151
Profile done
epoch 1 train time consumed: 7.24s
Validation Epoch: 12, Average loss: 0.0062, Accuracy: 0.3143
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.08015895538783,
                        "time": 4.851435582000704,
                        "accuracy": 0.3142578125,
                        "total_cost": 1351684.2630483208
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5376
Profiling... [1024/50176]	Loss: 1.7003
Profiling... [1536/50176]	Loss: 1.8299
Profiling... [2048/50176]	Loss: 1.6249
Profiling... [2560/50176]	Loss: 1.6882
Profiling... [3072/50176]	Loss: 1.7485
Profiling... [3584/50176]	Loss: 1.7298
Profiling... [4096/50176]	Loss: 1.7199
Profiling... [4608/50176]	Loss: 1.7363
Profiling... [5120/50176]	Loss: 1.6292
Profiling... [5632/50176]	Loss: 1.7792
Profiling... [6144/50176]	Loss: 1.7550
Profiling... [6656/50176]	Loss: 1.7880
Profile done
epoch 1 train time consumed: 8.79s
Validation Epoch: 12, Average loss: 0.0057, Accuracy: 0.3587
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.05263353767981,
                        "time": 6.036743585995282,
                        "accuracy": 0.35869140625,
                        "total_cost": 1473576.750980703
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5340
Profiling... [2048/50176]	Loss: 1.5467
Profiling... [3072/50176]	Loss: 1.4499
Profiling... [4096/50176]	Loss: 1.4723
Profiling... [5120/50176]	Loss: 1.3596
Profiling... [6144/50176]	Loss: 1.4901
Profiling... [7168/50176]	Loss: 1.3604
Profiling... [8192/50176]	Loss: 1.3967
Profiling... [9216/50176]	Loss: 1.4133
Profiling... [10240/50176]	Loss: 1.4419
Profiling... [11264/50176]	Loss: 1.4142
Profiling... [12288/50176]	Loss: 1.3880
Profiling... [13312/50176]	Loss: 1.3238
Profile done
epoch 1 train time consumed: 12.76s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5468
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.07404099037248,
                        "time": 8.784470319995307,
                        "accuracy": 0.54677734375,
                        "total_cost": 1406682.6319799765
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6159
Profiling... [2048/50176]	Loss: 1.5066
Profiling... [3072/50176]	Loss: 1.4394
Profiling... [4096/50176]	Loss: 1.5309
Profiling... [5120/50176]	Loss: 1.4235
Profiling... [6144/50176]	Loss: 1.4631
Profiling... [7168/50176]	Loss: 1.3917
Profiling... [8192/50176]	Loss: 1.4523
Profiling... [9216/50176]	Loss: 1.3126
Profiling... [10240/50176]	Loss: 1.3964
Profiling... [11264/50176]	Loss: 1.5338
Profiling... [12288/50176]	Loss: 1.3390
Profiling... [13312/50176]	Loss: 1.3551
Profile done
epoch 1 train time consumed: 13.03s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5454
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.1185480536852,
                        "time": 8.927811718996963,
                        "accuracy": 0.54541015625,
                        "total_cost": 1433220.3588619344
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4853
Profiling... [2048/50176]	Loss: 1.4288
Profiling... [3072/50176]	Loss: 1.4543
Profiling... [4096/50176]	Loss: 1.4921
Profiling... [5120/50176]	Loss: 1.3915
Profiling... [6144/50176]	Loss: 1.4631
Profiling... [7168/50176]	Loss: 1.4998
Profiling... [8192/50176]	Loss: 1.3048
Profiling... [9216/50176]	Loss: 1.4287
Profiling... [10240/50176]	Loss: 1.4089
Profiling... [11264/50176]	Loss: 1.3210
Profiling... [12288/50176]	Loss: 1.4610
Profiling... [13312/50176]	Loss: 1.4573
Profile done
epoch 1 train time consumed: 13.78s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5422
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.12990912742882,
                        "time": 9.582451100999606,
                        "accuracy": 0.5421875,
                        "total_cost": 1547455.9879638555
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5227
Profiling... [2048/50176]	Loss: 1.4254
Profiling... [3072/50176]	Loss: 1.4118
Profiling... [4096/50176]	Loss: 1.4229
Profiling... [5120/50176]	Loss: 1.4409
Profiling... [6144/50176]	Loss: 1.4539
Profiling... [7168/50176]	Loss: 1.4138
Profiling... [8192/50176]	Loss: 1.4279
Profiling... [9216/50176]	Loss: 1.4027
Profiling... [10240/50176]	Loss: 1.3965
Profiling... [11264/50176]	Loss: 1.3376
Profiling... [12288/50176]	Loss: 1.4434
Profiling... [13312/50176]	Loss: 1.3676
Profile done
epoch 1 train time consumed: 21.41s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5444
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.06405799442496,
                        "time": 15.27134263899643,
                        "accuracy": 0.54443359375,
                        "total_cost": 2455971.5857328125
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5438
Profiling... [2048/50176]	Loss: 1.4661
Profiling... [3072/50176]	Loss: 1.5125
Profiling... [4096/50176]	Loss: 1.4001
Profiling... [5120/50176]	Loss: 1.4568
Profiling... [6144/50176]	Loss: 1.4095
Profiling... [7168/50176]	Loss: 1.4154
Profiling... [8192/50176]	Loss: 1.3205
Profiling... [9216/50176]	Loss: 1.4121
Profiling... [10240/50176]	Loss: 1.3919
Profiling... [11264/50176]	Loss: 1.5228
Profiling... [12288/50176]	Loss: 1.3166
Profiling... [13312/50176]	Loss: 1.4107
Profile done
epoch 1 train time consumed: 12.89s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5431
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.08628896661736,
                        "time": 8.884086982005101,
                        "accuracy": 0.54306640625,
                        "total_cost": 1432355.9296442177
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5721
Profiling... [2048/50176]	Loss: 1.5512
Profiling... [3072/50176]	Loss: 1.4688
Profiling... [4096/50176]	Loss: 1.4864
Profiling... [5120/50176]	Loss: 1.4709
Profiling... [6144/50176]	Loss: 1.3771
Profiling... [7168/50176]	Loss: 1.3871
Profiling... [8192/50176]	Loss: 1.3978
Profiling... [9216/50176]	Loss: 1.4392
Profiling... [10240/50176]	Loss: 1.3155
Profiling... [11264/50176]	Loss: 1.3675
Profiling... [12288/50176]	Loss: 1.3570
Profiling... [13312/50176]	Loss: 1.3996
Profile done
epoch 1 train time consumed: 13.02s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5435
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.13033575391059,
                        "time": 8.926528944000893,
                        "accuracy": 0.54345703125,
                        "total_cost": 1438164.6230155863
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5294
Profiling... [2048/50176]	Loss: 1.4806
Profiling... [3072/50176]	Loss: 1.4965
Profiling... [4096/50176]	Loss: 1.4573
Profiling... [5120/50176]	Loss: 1.4599
Profiling... [6144/50176]	Loss: 1.3771
Profiling... [7168/50176]	Loss: 1.4164
Profiling... [8192/50176]	Loss: 1.3263
Profiling... [9216/50176]	Loss: 1.3552
Profiling... [10240/50176]	Loss: 1.3589
Profiling... [11264/50176]	Loss: 1.3479
Profiling... [12288/50176]	Loss: 1.4511
Profiling... [13312/50176]	Loss: 1.3933
Profile done
epoch 1 train time consumed: 13.80s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5429
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.14192043125453,
                        "time": 9.574827379001363,
                        "accuracy": 0.54287109375,
                        "total_cost": 1544277.9140513737
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5269
Profiling... [2048/50176]	Loss: 1.4678
Profiling... [3072/50176]	Loss: 1.4583
Profiling... [4096/50176]	Loss: 1.4229
Profiling... [5120/50176]	Loss: 1.4733
Profiling... [6144/50176]	Loss: 1.3998
Profiling... [7168/50176]	Loss: 1.4259
Profiling... [8192/50176]	Loss: 1.3993
Profiling... [9216/50176]	Loss: 1.3380
Profiling... [10240/50176]	Loss: 1.3999
Profiling... [11264/50176]	Loss: 1.3450
Profiling... [12288/50176]	Loss: 1.4514
Profiling... [13312/50176]	Loss: 1.4317
Profile done
epoch 1 train time consumed: 21.40s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5468
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.07525601610392,
                        "time": 15.263847986003384,
                        "accuracy": 0.54677734375,
                        "total_cost": 2444244.1292332592
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5138
Profiling... [2048/50176]	Loss: 1.5745
Profiling... [3072/50176]	Loss: 1.4921
Profiling... [4096/50176]	Loss: 1.4209
Profiling... [5120/50176]	Loss: 1.4618
Profiling... [6144/50176]	Loss: 1.4710
Profiling... [7168/50176]	Loss: 1.4496
Profiling... [8192/50176]	Loss: 1.3747
Profiling... [9216/50176]	Loss: 1.3544
Profiling... [10240/50176]	Loss: 1.4331
Profiling... [11264/50176]	Loss: 1.3978
Profiling... [12288/50176]	Loss: 1.3525
Profiling... [13312/50176]	Loss: 1.3573
Profile done
epoch 1 train time consumed: 13.10s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5430
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.09745930692168,
                        "time": 8.894496816996252,
                        "accuracy": 0.54296875,
                        "total_cost": 1434292.2888867701
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5387
Profiling... [2048/50176]	Loss: 1.4576
Profiling... [3072/50176]	Loss: 1.4745
Profiling... [4096/50176]	Loss: 1.4318
Profiling... [5120/50176]	Loss: 1.4318
Profiling... [6144/50176]	Loss: 1.5463
Profiling... [7168/50176]	Loss: 1.3988
Profiling... [8192/50176]	Loss: 1.3904
Profiling... [9216/50176]	Loss: 1.3834
Profiling... [10240/50176]	Loss: 1.3687
Profiling... [11264/50176]	Loss: 1.3849
Profiling... [12288/50176]	Loss: 1.3819
Profiling... [13312/50176]	Loss: 1.4313
Profile done
epoch 1 train time consumed: 12.85s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5404
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.14104116964899,
                        "time": 8.825363568001194,
                        "accuracy": 0.5404296875,
                        "total_cost": 1429830.7405499085
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4718
Profiling... [2048/50176]	Loss: 1.4809
Profiling... [3072/50176]	Loss: 1.4805
Profiling... [4096/50176]	Loss: 1.3657
Profiling... [5120/50176]	Loss: 1.4695
Profiling... [6144/50176]	Loss: 1.4467
Profiling... [7168/50176]	Loss: 1.4706
Profiling... [8192/50176]	Loss: 1.3305
Profiling... [9216/50176]	Loss: 1.4026
Profiling... [10240/50176]	Loss: 1.4130
Profiling... [11264/50176]	Loss: 1.4137
Profiling... [12288/50176]	Loss: 1.4975
Profiling... [13312/50176]	Loss: 1.3774
Profile done
epoch 1 train time consumed: 13.81s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5445
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.15420515107341,
                        "time": 9.568618406003225,
                        "accuracy": 0.54453125,
                        "total_cost": 1538571.4952806616
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5117
Profiling... [2048/50176]	Loss: 1.4941
Profiling... [3072/50176]	Loss: 1.4853
Profiling... [4096/50176]	Loss: 1.4003
Profiling... [5120/50176]	Loss: 1.4222
Profiling... [6144/50176]	Loss: 1.3917
Profiling... [7168/50176]	Loss: 1.3017
Profiling... [8192/50176]	Loss: 1.3544
Profiling... [9216/50176]	Loss: 1.3964
Profiling... [10240/50176]	Loss: 1.4583
Profiling... [11264/50176]	Loss: 1.4119
Profiling... [12288/50176]	Loss: 1.4514
Profiling... [13312/50176]	Loss: 1.3138
Profile done
epoch 1 train time consumed: 23.43s
Validation Epoch: 12, Average loss: 0.0016, Accuracy: 0.5412
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.07961888606191,
                        "time": 16.580393721997098,
                        "accuracy": 0.5412109375,
                        "total_cost": 2682374.0848236997
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5237
Profiling... [2048/50176]	Loss: 1.4142
Profiling... [3072/50176]	Loss: 1.4738
Profiling... [4096/50176]	Loss: 1.5159
Profiling... [5120/50176]	Loss: 1.4312
Profiling... [6144/50176]	Loss: 1.3970
Profiling... [7168/50176]	Loss: 1.4412
Profiling... [8192/50176]	Loss: 1.4827
Profiling... [9216/50176]	Loss: 1.4963
Profiling... [10240/50176]	Loss: 1.3778
Profiling... [11264/50176]	Loss: 1.4119
Profiling... [12288/50176]	Loss: 1.4318
Profiling... [13312/50176]	Loss: 1.4280
Profile done
epoch 1 train time consumed: 13.11s
Validation Epoch: 12, Average loss: 0.0019, Accuracy: 0.4798
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.10258586847175,
                        "time": 8.998642981998273,
                        "accuracy": 0.47978515625,
                        "total_cost": 1642182.2035925596
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.4863
Profiling... [2048/50176]	Loss: 1.4425
Profiling... [3072/50176]	Loss: 1.4190
Profiling... [4096/50176]	Loss: 1.4425
Profiling... [5120/50176]	Loss: 1.4039
Profiling... [6144/50176]	Loss: 1.4648
Profiling... [7168/50176]	Loss: 1.4229
Profiling... [8192/50176]	Loss: 1.4590
Profiling... [9216/50176]	Loss: 1.4843
Profiling... [10240/50176]	Loss: 1.4363
Profiling... [11264/50176]	Loss: 1.5020
Profiling... [12288/50176]	Loss: 1.4099
Profiling... [13312/50176]	Loss: 1.5411
Profile done
epoch 1 train time consumed: 12.99s
Validation Epoch: 12, Average loss: 0.0017, Accuracy: 0.5176
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.14526383355657,
                        "time": 9.025578934000805,
                        "accuracy": 0.517578125,
                        "total_cost": 1526828.9599167402
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5504
Profiling... [2048/50176]	Loss: 1.4582
Profiling... [3072/50176]	Loss: 1.5724
Profiling... [4096/50176]	Loss: 1.4836
Profiling... [5120/50176]	Loss: 1.5953
Profiling... [6144/50176]	Loss: 1.4833
Profiling... [7168/50176]	Loss: 1.4808
Profiling... [8192/50176]	Loss: 1.5005
Profiling... [9216/50176]	Loss: 1.4076
Profiling... [10240/50176]	Loss: 1.4808
Profiling... [11264/50176]	Loss: 1.4255
Profiling... [12288/50176]	Loss: 1.3944
Profiling... [13312/50176]	Loss: 1.4652
Profile done
epoch 1 train time consumed: 13.78s
Validation Epoch: 12, Average loss: 0.0017, Accuracy: 0.5177
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.15773390086898,
                        "time": 9.557968798006186,
                        "accuracy": 0.51767578125,
                        "total_cost": 1616586.7868768086
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5040
Profiling... [2048/50176]	Loss: 1.4776
Profiling... [3072/50176]	Loss: 1.4466
Profiling... [4096/50176]	Loss: 1.4468
Profiling... [5120/50176]	Loss: 1.3133
Profiling... [6144/50176]	Loss: 1.4955
Profiling... [7168/50176]	Loss: 1.4988
Profiling... [8192/50176]	Loss: 1.3684
Profiling... [9216/50176]	Loss: 1.4140
Profiling... [10240/50176]	Loss: 1.3713
Profiling... [11264/50176]	Loss: 1.3935
Profiling... [12288/50176]	Loss: 1.4786
Profiling... [13312/50176]	Loss: 1.3491
Profile done
epoch 1 train time consumed: 24.07s
Validation Epoch: 12, Average loss: 0.0017, Accuracy: 0.5157
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.0838171808302,
                        "time": 17.96146499200404,
                        "accuracy": 0.51572265625,
                        "total_cost": 3049415.6577876294
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6011
Profiling... [2048/50176]	Loss: 1.4973
Profiling... [3072/50176]	Loss: 1.4201
Profiling... [4096/50176]	Loss: 1.4872
Profiling... [5120/50176]	Loss: 1.3932
Profiling... [6144/50176]	Loss: 1.5179
Profiling... [7168/50176]	Loss: 1.4358
Profiling... [8192/50176]	Loss: 1.4170
Profiling... [9216/50176]	Loss: 1.4938
Profiling... [10240/50176]	Loss: 1.4926
Profiling... [11264/50176]	Loss: 1.5370
Profiling... [12288/50176]	Loss: 1.4678
Profiling... [13312/50176]	Loss: 1.4712
Profile done
epoch 1 train time consumed: 12.93s
Validation Epoch: 12, Average loss: 0.0019, Accuracy: 0.4893
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.10551737154593,
                        "time": 8.794308041004115,
                        "accuracy": 0.4892578125,
                        "total_cost": 1573819.9236672013
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5177
Profiling... [2048/50176]	Loss: 1.4717
Profiling... [3072/50176]	Loss: 1.5356
Profiling... [4096/50176]	Loss: 1.5190
Profiling... [5120/50176]	Loss: 1.4553
Profiling... [6144/50176]	Loss: 1.4943
Profiling... [7168/50176]	Loss: 1.4739
Profiling... [8192/50176]	Loss: 1.4077
Profiling... [9216/50176]	Loss: 1.4359
Profiling... [10240/50176]	Loss: 1.3907
Profiling... [11264/50176]	Loss: 1.4171
Profiling... [12288/50176]	Loss: 1.5219
Profiling... [13312/50176]	Loss: 1.4618
Profile done
epoch 1 train time consumed: 12.95s
Validation Epoch: 12, Average loss: 0.0017, Accuracy: 0.5165
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.14851887291884,
                        "time": 8.936060649000865,
                        "accuracy": 0.51650390625,
                        "total_cost": 1514829.4453608436
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4976
Profiling... [2048/50176]	Loss: 1.4684
Profiling... [3072/50176]	Loss: 1.5066
Profiling... [4096/50176]	Loss: 1.4616
Profiling... [5120/50176]	Loss: 1.5207
Profiling... [6144/50176]	Loss: 1.4007
Profiling... [7168/50176]	Loss: 1.3746
Profiling... [8192/50176]	Loss: 1.4821
Profiling... [9216/50176]	Loss: 1.4910
Profiling... [10240/50176]	Loss: 1.4967
Profiling... [11264/50176]	Loss: 1.4111
Profiling... [12288/50176]	Loss: 1.3863
Profiling... [13312/50176]	Loss: 1.4445
Profile done
epoch 1 train time consumed: 13.83s
Validation Epoch: 12, Average loss: 0.0018, Accuracy: 0.4985
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.15958000250767,
                        "time": 9.565115470999444,
                        "accuracy": 0.49853515625,
                        "total_cost": 1679908.7646988262
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.4674
Profiling... [2048/50176]	Loss: 1.5683
Profiling... [3072/50176]	Loss: 1.4314
Profiling... [4096/50176]	Loss: 1.4711
Profiling... [5120/50176]	Loss: 1.4137
Profiling... [6144/50176]	Loss: 1.4599
Profiling... [7168/50176]	Loss: 1.4316
Profiling... [8192/50176]	Loss: 1.4149
Profiling... [9216/50176]	Loss: 1.5111
Profiling... [10240/50176]	Loss: 1.5277
Profiling... [11264/50176]	Loss: 1.3648
Profiling... [12288/50176]	Loss: 1.4467
Profiling... [13312/50176]	Loss: 1.3328
Profile done
epoch 1 train time consumed: 20.09s
Validation Epoch: 12, Average loss: 0.0018, Accuracy: 0.5079
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.10126268652411,
                        "time": 14.579122504001134,
                        "accuracy": 0.50791015625,
                        "total_cost": 2513249.5413520946
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4570
Profiling... [2048/50176]	Loss: 1.4072
Profiling... [3072/50176]	Loss: 1.4752
Profiling... [4096/50176]	Loss: 1.4187
Profiling... [5120/50176]	Loss: 1.4937
Profiling... [6144/50176]	Loss: 1.4614
Profiling... [7168/50176]	Loss: 1.4625
Profiling... [8192/50176]	Loss: 1.4381
Profiling... [9216/50176]	Loss: 1.4814
Profiling... [10240/50176]	Loss: 1.4940
Profiling... [11264/50176]	Loss: 1.4443
Profiling... [12288/50176]	Loss: 1.4134
Profiling... [13312/50176]	Loss: 1.5038
Profile done
epoch 1 train time consumed: 12.84s
Validation Epoch: 12, Average loss: 0.0018, Accuracy: 0.5138
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.12221060787411,
                        "time": 8.784056217999023,
                        "accuracy": 0.51376953125,
                        "total_cost": 1496986.684205527
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5164
Profiling... [2048/50176]	Loss: 1.4878
Profiling... [3072/50176]	Loss: 1.4693
Profiling... [4096/50176]	Loss: 1.4471
Profiling... [5120/50176]	Loss: 1.5199
Profiling... [6144/50176]	Loss: 1.4395
Profiling... [7168/50176]	Loss: 1.4444
Profiling... [8192/50176]	Loss: 1.4765
Profiling... [9216/50176]	Loss: 1.4373
Profiling... [10240/50176]	Loss: 1.4684
Profiling... [11264/50176]	Loss: 1.3995
Profiling... [12288/50176]	Loss: 1.4312
Profiling... [13312/50176]	Loss: 1.5180
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 12, Average loss: 0.0019, Accuracy: 0.4852
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.16446055425952,
                        "time": 8.81476860299881,
                        "accuracy": 0.48515625,
                        "total_cost": 1590818.2557962139
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5056
Profiling... [2048/50176]	Loss: 1.5933
Profiling... [3072/50176]	Loss: 1.4946
Profiling... [4096/50176]	Loss: 1.5046
Profiling... [5120/50176]	Loss: 1.4532
Profiling... [6144/50176]	Loss: 1.4849
Profiling... [7168/50176]	Loss: 1.4409
Profiling... [8192/50176]	Loss: 1.5098
Profiling... [9216/50176]	Loss: 1.4750
Profiling... [10240/50176]	Loss: 1.4759
Profiling... [11264/50176]	Loss: 1.3799
Profiling... [12288/50176]	Loss: 1.4868
Profiling... [13312/50176]	Loss: 1.3752
Profile done
epoch 1 train time consumed: 13.88s
Validation Epoch: 12, Average loss: 0.0017, Accuracy: 0.5150
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.17759234525369,
                        "time": 9.588300467003137,
                        "accuracy": 0.5150390625,
                        "total_cost": 1630019.4228350925
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.4586
Profiling... [2048/50176]	Loss: 1.4411
Profiling... [3072/50176]	Loss: 1.4384
Profiling... [4096/50176]	Loss: 1.4165
Profiling... [5120/50176]	Loss: 1.5439
Profiling... [6144/50176]	Loss: 1.4644
Profiling... [7168/50176]	Loss: 1.4716
Profiling... [8192/50176]	Loss: 1.4809
Profiling... [9216/50176]	Loss: 1.5114
Profiling... [10240/50176]	Loss: 1.4086
Profiling... [11264/50176]	Loss: 1.4810
Profiling... [12288/50176]	Loss: 1.4507
Profiling... [13312/50176]	Loss: 1.4909
Profile done
epoch 1 train time consumed: 22.12s
Validation Epoch: 12, Average loss: 0.0018, Accuracy: 0.5051
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.10863709130318,
                        "time": 15.266916553999181,
                        "accuracy": 0.505078125,
                        "total_cost": 2646573.2246772912
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4536
Profiling... [2048/50176]	Loss: 1.6387
Profiling... [3072/50176]	Loss: 1.6848
Profiling... [4096/50176]	Loss: 1.6475
Profiling... [5120/50176]	Loss: 1.5839
Profiling... [6144/50176]	Loss: 1.6624
Profiling... [7168/50176]	Loss: 1.7501
Profiling... [8192/50176]	Loss: 1.5586
Profiling... [9216/50176]	Loss: 1.5609
Profiling... [10240/50176]	Loss: 1.6120
Profiling... [11264/50176]	Loss: 1.6556
Profiling... [12288/50176]	Loss: 1.5420
Profiling... [13312/50176]	Loss: 1.5978
Profile done
epoch 1 train time consumed: 13.01s
Validation Epoch: 12, Average loss: 0.0037, Accuracy: 0.2924
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.12888488389771,
                        "time": 8.982004495999718,
                        "accuracy": 0.2923828125,
                        "total_cost": 2689754.366045544
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.4879
Profiling... [2048/50176]	Loss: 1.5792
Profiling... [3072/50176]	Loss: 1.7043
Profiling... [4096/50176]	Loss: 1.6376
Profiling... [5120/50176]	Loss: 1.6074
Profiling... [6144/50176]	Loss: 1.6315
Profiling... [7168/50176]	Loss: 1.5872
Profiling... [8192/50176]	Loss: 1.6387
Profiling... [9216/50176]	Loss: 1.6402
Profiling... [10240/50176]	Loss: 1.5358
Profiling... [11264/50176]	Loss: 1.5728
Profiling... [12288/50176]	Loss: 1.5982
Profiling... [13312/50176]	Loss: 1.5901
Profile done
epoch 1 train time consumed: 12.99s
Validation Epoch: 12, Average loss: 0.0032, Accuracy: 0.3217
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.1704642775396,
                        "time": 8.904244284000015,
                        "accuracy": 0.3216796875,
                        "total_cost": 2423621.0926497304
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4663
Profiling... [2048/50176]	Loss: 1.6498
Profiling... [3072/50176]	Loss: 1.6486
Profiling... [4096/50176]	Loss: 1.5822
Profiling... [5120/50176]	Loss: 1.5626
Profiling... [6144/50176]	Loss: 1.7137
Profiling... [7168/50176]	Loss: 1.6020
Profiling... [8192/50176]	Loss: 1.6225
Profiling... [9216/50176]	Loss: 1.5093
Profiling... [10240/50176]	Loss: 1.6012
Profiling... [11264/50176]	Loss: 1.6480
Profiling... [12288/50176]	Loss: 1.5729
Profiling... [13312/50176]	Loss: 1.5640
Profile done
epoch 1 train time consumed: 13.76s
Validation Epoch: 12, Average loss: 0.0023, Accuracy: 0.4294
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.18018644570218,
                        "time": 9.56071927800076,
                        "accuracy": 0.42939453125,
                        "total_cost": 1949509.5960887957
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5079
Profiling... [2048/50176]	Loss: 1.7286
Profiling... [3072/50176]	Loss: 1.6243
Profiling... [4096/50176]	Loss: 1.6696
Profiling... [5120/50176]	Loss: 1.5917
Profiling... [6144/50176]	Loss: 1.6123
Profiling... [7168/50176]	Loss: 1.6485
Profiling... [8192/50176]	Loss: 1.5308
Profiling... [9216/50176]	Loss: 1.6814
Profiling... [10240/50176]	Loss: 1.6404
Profiling... [11264/50176]	Loss: 1.5379
Profiling... [12288/50176]	Loss: 1.5864
Profiling... [13312/50176]	Loss: 1.5719
Profile done
epoch 1 train time consumed: 23.54s
Validation Epoch: 12, Average loss: 0.0022, Accuracy: 0.4327
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.11024986080407,
                        "time": 17.319481135993556,
                        "accuracy": 0.43271484375,
                        "total_cost": 3504485.1972665424
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5251
Profiling... [2048/50176]	Loss: 1.6425
Profiling... [3072/50176]	Loss: 1.6580
Profiling... [4096/50176]	Loss: 1.6530
Profiling... [5120/50176]	Loss: 1.6033
Profiling... [6144/50176]	Loss: 1.5110
Profiling... [7168/50176]	Loss: 1.5914
Profiling... [8192/50176]	Loss: 1.6646
Profiling... [9216/50176]	Loss: 1.6824
Profiling... [10240/50176]	Loss: 1.5861
Profiling... [11264/50176]	Loss: 1.5825
Profiling... [12288/50176]	Loss: 1.5983
Profiling... [13312/50176]	Loss: 1.5894
Profile done
epoch 1 train time consumed: 12.97s
Validation Epoch: 12, Average loss: 0.0025, Accuracy: 0.3611
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.13042037484678,
                        "time": 8.896220848000667,
                        "accuracy": 0.3611328125,
                        "total_cost": 2156898.9633491784
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5750
Profiling... [2048/50176]	Loss: 1.6125
Profiling... [3072/50176]	Loss: 1.6140
Profiling... [4096/50176]	Loss: 1.4622
Profiling... [5120/50176]	Loss: 1.5960
Profiling... [6144/50176]	Loss: 1.6960
Profiling... [7168/50176]	Loss: 1.6031
Profiling... [8192/50176]	Loss: 1.6342
Profiling... [9216/50176]	Loss: 1.6082
Profiling... [10240/50176]	Loss: 1.6039
Profiling... [11264/50176]	Loss: 1.6866
Profiling... [12288/50176]	Loss: 1.5622
Profiling... [13312/50176]	Loss: 1.5714
Profile done
epoch 1 train time consumed: 12.93s
Validation Epoch: 12, Average loss: 0.0020, Accuracy: 0.4593
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.17209288144907,
                        "time": 8.815435001000878,
                        "accuracy": 0.45927734375,
                        "total_cost": 1680583.228027867
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4873
Profiling... [2048/50176]	Loss: 1.6303
Profiling... [3072/50176]	Loss: 1.6992
Profiling... [4096/50176]	Loss: 1.5853
Profiling... [5120/50176]	Loss: 1.5592
Profiling... [6144/50176]	Loss: 1.5098
Profiling... [7168/50176]	Loss: 1.7762
Profiling... [8192/50176]	Loss: 1.6541
Profiling... [9216/50176]	Loss: 1.5854
Profiling... [10240/50176]	Loss: 1.5861
Profiling... [11264/50176]	Loss: 1.5597
Profiling... [12288/50176]	Loss: 1.6317
Profiling... [13312/50176]	Loss: 1.5558
Profile done
epoch 1 train time consumed: 13.84s
Validation Epoch: 12, Average loss: 0.0024, Accuracy: 0.3913
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.18029084985471,
                        "time": 9.578943661996163,
                        "accuracy": 0.39130859375,
                        "total_cost": 2143332.5183938537
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5868
Profiling... [2048/50176]	Loss: 1.5983
Profiling... [3072/50176]	Loss: 1.5098
Profiling... [4096/50176]	Loss: 1.6621
Profiling... [5120/50176]	Loss: 1.5897
Profiling... [6144/50176]	Loss: 1.5641
Profiling... [7168/50176]	Loss: 1.5498
Profiling... [8192/50176]	Loss: 1.6538
Profiling... [9216/50176]	Loss: 1.6131
Profiling... [10240/50176]	Loss: 1.5213
Profiling... [11264/50176]	Loss: 1.5030
Profiling... [12288/50176]	Loss: 1.5321
Profiling... [13312/50176]	Loss: 1.6487
Profile done
epoch 1 train time consumed: 24.25s
Validation Epoch: 12, Average loss: 0.0023, Accuracy: 0.4203
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.10831540919749,
                        "time": 17.944262848002836,
                        "accuracy": 0.4203125,
                        "total_cost": 3738044.4169580275
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.5231
Profiling... [2048/50176]	Loss: 1.5660
Profiling... [3072/50176]	Loss: 1.6313
Profiling... [4096/50176]	Loss: 1.6051
Profiling... [5120/50176]	Loss: 1.5915
Profiling... [6144/50176]	Loss: 1.5913
Profiling... [7168/50176]	Loss: 1.6338
Profiling... [8192/50176]	Loss: 1.5997
Profiling... [9216/50176]	Loss: 1.6107
Profiling... [10240/50176]	Loss: 1.6573
Profiling... [11264/50176]	Loss: 1.6680
Profiling... [12288/50176]	Loss: 1.6570
Profiling... [13312/50176]	Loss: 1.5537
Profile done
epoch 1 train time consumed: 13.10s
Validation Epoch: 12, Average loss: 0.0021, Accuracy: 0.4354
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.12806776225494,
                        "time": 8.988575789000606,
                        "accuracy": 0.4353515625,
                        "total_cost": 1807764.9736950689
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.4757
Profiling... [2048/50176]	Loss: 1.6200
Profiling... [3072/50176]	Loss: 1.6028
Profiling... [4096/50176]	Loss: 1.5277
Profiling... [5120/50176]	Loss: 1.6102
Profiling... [6144/50176]	Loss: 1.6352
Profiling... [7168/50176]	Loss: 1.6624
Profiling... [8192/50176]	Loss: 1.6042
Profiling... [9216/50176]	Loss: 1.5924
Profiling... [10240/50176]	Loss: 1.6618
Profiling... [11264/50176]	Loss: 1.5755
Profiling... [12288/50176]	Loss: 1.6964
Profiling... [13312/50176]	Loss: 1.5438
Profile done
epoch 1 train time consumed: 12.93s
Validation Epoch: 12, Average loss: 0.0029, Accuracy: 0.3437
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.16754683826338,
                        "time": 8.919617270999879,
                        "accuracy": 0.34365234375,
                        "total_cost": 2272574.858944993
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.5174
Profiling... [2048/50176]	Loss: 1.5917
Profiling... [3072/50176]	Loss: 1.6306
Profiling... [4096/50176]	Loss: 1.7085
Profiling... [5120/50176]	Loss: 1.5984
Profiling... [6144/50176]	Loss: 1.5483
Profiling... [7168/50176]	Loss: 1.5421
Profiling... [8192/50176]	Loss: 1.6309
Profiling... [9216/50176]	Loss: 1.5693
Profiling... [10240/50176]	Loss: 1.5844
Profiling... [11264/50176]	Loss: 1.6240
Profiling... [12288/50176]	Loss: 1.5697
Profiling... [13312/50176]	Loss: 1.5882
Profile done
epoch 1 train time consumed: 13.83s
Validation Epoch: 12, Average loss: 0.0028, Accuracy: 0.3588
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.17744924909773,
                        "time": 9.577601645003597,
                        "accuracy": 0.3587890625,
                        "total_cost": 2337270.0136328433
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5242
Profiling... [2048/50176]	Loss: 1.5295
Profiling... [3072/50176]	Loss: 1.6398
Profiling... [4096/50176]	Loss: 1.6088
Profiling... [5120/50176]	Loss: 1.6675
Profiling... [6144/50176]	Loss: 1.5843
Profiling... [7168/50176]	Loss: 1.5675
Profiling... [8192/50176]	Loss: 1.5743
Profiling... [9216/50176]	Loss: 1.5587
Profiling... [10240/50176]	Loss: 1.5910
Profiling... [11264/50176]	Loss: 1.5765
Profiling... [12288/50176]	Loss: 1.6690
Profiling... [13312/50176]	Loss: 1.6163
Profile done
epoch 1 train time consumed: 22.15s
Validation Epoch: 12, Average loss: 0.0025, Accuracy: 0.3666
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.11217523693819,
                        "time": 15.255763009001384,
                        "accuracy": 0.3666015625,
                        "total_cost": 3643600.6664280095
                    },
                    
[Training Loop] The optimal parameters are lr: 0.01 dr: 0.25 bs: 512 pl: 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 0.5
[GPU_0] Set GPU power limit to 125W.
[GPU_0] Set GPU power limit to 125W.
Training Epoch: 12 [512/50176]	Loss: 1.6122
Training Epoch: 12 [1024/50176]	Loss: 1.6127
Training Epoch: 12 [1536/50176]	Loss: 1.7002
Training Epoch: 12 [2048/50176]	Loss: 1.6651
Training Epoch: 12 [2560/50176]	Loss: 1.7813
Training Epoch: 12 [3072/50176]	Loss: 1.8202
Training Epoch: 12 [3584/50176]	Loss: 1.6074
Training Epoch: 12 [4096/50176]	Loss: 1.6655
Training Epoch: 12 [4608/50176]	Loss: 1.6315
Training Epoch: 12 [5120/50176]	Loss: 1.7259
Training Epoch: 12 [5632/50176]	Loss: 1.6492
Training Epoch: 12 [6144/50176]	Loss: 1.6804
Training Epoch: 12 [6656/50176]	Loss: 1.7137
Training Epoch: 12 [7168/50176]	Loss: 1.6451
Training Epoch: 12 [7680/50176]	Loss: 1.7440
Training Epoch: 12 [8192/50176]	Loss: 1.6022
Training Epoch: 12 [8704/50176]	Loss: 1.5274
Training Epoch: 12 [9216/50176]	Loss: 1.6026
Training Epoch: 12 [9728/50176]	Loss: 1.7697
Training Epoch: 12 [10240/50176]	Loss: 1.6541
Training Epoch: 12 [10752/50176]	Loss: 1.6868
Training Epoch: 12 [11264/50176]	Loss: 1.4857
Training Epoch: 12 [11776/50176]	Loss: 1.6181
Training Epoch: 12 [12288/50176]	Loss: 1.6522
Training Epoch: 12 [12800/50176]	Loss: 1.7388
Training Epoch: 12 [13312/50176]	Loss: 1.5907
Training Epoch: 12 [13824/50176]	Loss: 1.7260
Training Epoch: 12 [14336/50176]	Loss: 1.7633
Training Epoch: 12 [14848/50176]	Loss: 1.6685
Training Epoch: 12 [15360/50176]	Loss: 1.6019
Training Epoch: 12 [15872/50176]	Loss: 1.7598
Training Epoch: 12 [16384/50176]	Loss: 1.7149
Training Epoch: 12 [16896/50176]	Loss: 1.7774
Training Epoch: 12 [17408/50176]	Loss: 1.9097
Training Epoch: 12 [17920/50176]	Loss: 1.6974
Training Epoch: 12 [18432/50176]	Loss: 1.7030
Training Epoch: 12 [18944/50176]	Loss: 1.6425
Training Epoch: 12 [19456/50176]	Loss: 1.7202
Training Epoch: 12 [19968/50176]	Loss: 1.5995
Training Epoch: 12 [20480/50176]	Loss: 1.7524
Training Epoch: 12 [20992/50176]	Loss: 1.7256
Training Epoch: 12 [21504/50176]	Loss: 1.7300
Training Epoch: 12 [22016/50176]	Loss: 1.6083
Training Epoch: 12 [22528/50176]	Loss: 1.7682
Training Epoch: 12 [23040/50176]	Loss: 1.6391
Training Epoch: 12 [23552/50176]	Loss: 1.6226
Training Epoch: 12 [24064/50176]	Loss: 1.6268
Training Epoch: 12 [24576/50176]	Loss: 1.5939
Training Epoch: 12 [25088/50176]	Loss: 1.6639
Training Epoch: 12 [25600/50176]	Loss: 1.7543
Training Epoch: 12 [26112/50176]	Loss: 1.6022
Training Epoch: 12 [26624/50176]	Loss: 1.5098
Training Epoch: 12 [27136/50176]	Loss: 1.6696
Training Epoch: 12 [27648/50176]	Loss: 1.5447
Training Epoch: 12 [28160/50176]	Loss: 1.6114
Training Epoch: 12 [28672/50176]	Loss: 1.5941
Training Epoch: 12 [29184/50176]	Loss: 1.7410
Training Epoch: 12 [29696/50176]	Loss: 1.6317
Training Epoch: 12 [30208/50176]	Loss: 1.5753
Training Epoch: 12 [30720/50176]	Loss: 1.5812
Training Epoch: 12 [31232/50176]	Loss: 1.6265
Training Epoch: 12 [31744/50176]	Loss: 1.6443
Training Epoch: 12 [32256/50176]	Loss: 1.6446
Training Epoch: 12 [32768/50176]	Loss: 1.6247
Training Epoch: 12 [33280/50176]	Loss: 1.5195
Training Epoch: 12 [33792/50176]	Loss: 1.7950
Training Epoch: 12 [34304/50176]	Loss: 1.6367
Training Epoch: 12 [34816/50176]	Loss: 1.6199
Training Epoch: 12 [35328/50176]	Loss: 1.7682
Training Epoch: 12 [35840/50176]	Loss: 1.6795
Training Epoch: 12 [36352/50176]	Loss: 1.6739
Training Epoch: 12 [36864/50176]	Loss: 1.6041
Training Epoch: 12 [37376/50176]	Loss: 1.6217
Training Epoch: 12 [37888/50176]	Loss: 1.6622
Training Epoch: 12 [38400/50176]	Loss: 1.6954
Training Epoch: 12 [38912/50176]	Loss: 1.7085
Training Epoch: 12 [39424/50176]	Loss: 1.7648
Training Epoch: 12 [39936/50176]	Loss: 1.6283
Training Epoch: 12 [40448/50176]	Loss: 1.6746
Training Epoch: 12 [40960/50176]	Loss: 1.6039
Training Epoch: 12 [41472/50176]	Loss: 1.7558
Training Epoch: 12 [41984/50176]	Loss: 1.6697
Training Epoch: 12 [42496/50176]	Loss: 1.6606
Training Epoch: 12 [43008/50176]	Loss: 1.6521
Training Epoch: 12 [43520/50176]	Loss: 1.5869
Training Epoch: 12 [44032/50176]	Loss: 1.4915
Training Epoch: 12 [44544/50176]	Loss: 1.6659
Training Epoch: 12 [45056/50176]	Loss: 1.6025
Training Epoch: 12 [45568/50176]	Loss: 1.6428
Training Epoch: 12 [46080/50176]	Loss: 1.6889
Training Epoch: 12 [46592/50176]	Loss: 1.7138
Training Epoch: 12 [47104/50176]	Loss: 1.5008
Training Epoch: 12 [47616/50176]	Loss: 1.6532
Training Epoch: 12 [48128/50176]	Loss: 1.5326
Training Epoch: 12 [48640/50176]	Loss: 1.7457
Training Epoch: 12 [49152/50176]	Loss: 1.5136
Training Epoch: 12 [49664/50176]	Loss: 1.6430
Training Epoch: 12 [50176/50176]	Loss: 1.6903
Validation Epoch: 12, Average loss: 0.0041, Accuracy: 0.4637
Training Epoch: 13 [512/50176]	Loss: 1.3977
Training Epoch: 13 [1024/50176]	Loss: 1.5774
Training Epoch: 13 [1536/50176]	Loss: 1.5225
Training Epoch: 13 [2048/50176]	Loss: 1.5376
Training Epoch: 13 [2560/50176]	Loss: 1.5528
Training Epoch: 13 [3072/50176]	Loss: 1.6046
Training Epoch: 13 [3584/50176]	Loss: 1.5761
Training Epoch: 13 [4096/50176]	Loss: 1.5081
Training Epoch: 13 [4608/50176]	Loss: 1.5674
Training Epoch: 13 [5120/50176]	Loss: 1.5063
Training Epoch: 13 [5632/50176]	Loss: 1.4555
Training Epoch: 13 [6144/50176]	Loss: 1.3847
Training Epoch: 13 [6656/50176]	Loss: 1.4852
Training Epoch: 13 [7168/50176]	Loss: 1.4953
Training Epoch: 13 [7680/50176]	Loss: 1.6389
Training Epoch: 13 [8192/50176]	Loss: 1.5007
Training Epoch: 13 [8704/50176]	Loss: 1.5793
Training Epoch: 13 [9216/50176]	Loss: 1.5314
Training Epoch: 13 [9728/50176]	Loss: 1.5140
Training Epoch: 13 [10240/50176]	Loss: 1.5352
Training Epoch: 13 [10752/50176]	Loss: 1.5338
Training Epoch: 13 [11264/50176]	Loss: 1.4915
Training Epoch: 13 [11776/50176]	Loss: 1.5396
Training Epoch: 13 [12288/50176]	Loss: 1.3758
Training Epoch: 13 [12800/50176]	Loss: 1.5702
Training Epoch: 13 [13312/50176]	Loss: 1.5530
Training Epoch: 13 [13824/50176]	Loss: 1.6379
Training Epoch: 13 [14336/50176]	Loss: 1.5071
Training Epoch: 13 [14848/50176]	Loss: 1.5259
Training Epoch: 13 [15360/50176]	Loss: 1.4670
Training Epoch: 13 [15872/50176]	Loss: 1.3325
Training Epoch: 13 [16384/50176]	Loss: 1.5332
Training Epoch: 13 [16896/50176]	Loss: 1.6738
Training Epoch: 13 [17408/50176]	Loss: 1.5136
Training Epoch: 13 [17920/50176]	Loss: 1.5434
Training Epoch: 13 [18432/50176]	Loss: 1.6675
Training Epoch: 13 [18944/50176]	Loss: 1.5928
Training Epoch: 13 [19456/50176]	Loss: 1.4875
Training Epoch: 13 [19968/50176]	Loss: 1.5970
Training Epoch: 13 [20480/50176]	Loss: 1.5394
Training Epoch: 13 [20992/50176]	Loss: 1.4380
Training Epoch: 13 [21504/50176]	Loss: 1.5405
Training Epoch: 13 [22016/50176]	Loss: 1.4196
Training Epoch: 13 [22528/50176]	Loss: 1.6183
Training Epoch: 13 [23040/50176]	Loss: 1.4368
Training Epoch: 13 [23552/50176]	Loss: 1.5185
Training Epoch: 13 [24064/50176]	Loss: 1.4794
Training Epoch: 13 [24576/50176]	Loss: 1.5602
Training Epoch: 13 [25088/50176]	Loss: 1.5122
Training Epoch: 13 [25600/50176]	Loss: 1.6629
Training Epoch: 13 [26112/50176]	Loss: 1.6260
Training Epoch: 13 [26624/50176]	Loss: 1.5549
Training Epoch: 13 [27136/50176]	Loss: 1.3949
Training Epoch: 13 [27648/50176]	Loss: 1.6223
Training Epoch: 13 [28160/50176]	Loss: 1.5195
Training Epoch: 13 [28672/50176]	Loss: 1.4194
Training Epoch: 13 [29184/50176]	Loss: 1.4696
Training Epoch: 13 [29696/50176]	Loss: 1.5679
Training Epoch: 13 [30208/50176]	Loss: 1.6269
Training Epoch: 13 [30720/50176]	Loss: 1.5373
Training Epoch: 13 [31232/50176]	Loss: 1.5198
Training Epoch: 13 [31744/50176]	Loss: 1.6018
Training Epoch: 13 [32256/50176]	Loss: 1.5352
Training Epoch: 13 [32768/50176]	Loss: 1.4713
Training Epoch: 13 [33280/50176]	Loss: 1.5726
Training Epoch: 13 [33792/50176]	Loss: 1.6313
Training Epoch: 13 [34304/50176]	Loss: 1.4664
Training Epoch: 13 [34816/50176]	Loss: 1.4917
Training Epoch: 13 [35328/50176]	Loss: 1.4823
Training Epoch: 13 [35840/50176]	Loss: 1.5334
Training Epoch: 13 [36352/50176]	Loss: 1.5132
Training Epoch: 13 [36864/50176]	Loss: 1.6149
Training Epoch: 13 [37376/50176]	Loss: 1.6408
Training Epoch: 13 [37888/50176]	Loss: 1.7193
Training Epoch: 13 [38400/50176]	Loss: 1.5502
Training Epoch: 13 [38912/50176]	Loss: 1.6166
Training Epoch: 13 [39424/50176]	Loss: 1.5473
Training Epoch: 13 [39936/50176]	Loss: 1.5109
Training Epoch: 13 [40448/50176]	Loss: 1.5510
Training Epoch: 13 [40960/50176]	Loss: 1.6883
Training Epoch: 13 [41472/50176]	Loss: 1.5346
Training Epoch: 13 [41984/50176]	Loss: 1.5238
Training Epoch: 13 [42496/50176]	Loss: 1.6450
Training Epoch: 13 [43008/50176]	Loss: 1.5529
Training Epoch: 13 [43520/50176]	Loss: 1.5761
Training Epoch: 13 [44032/50176]	Loss: 1.4259
Training Epoch: 13 [44544/50176]	Loss: 1.5555
Training Epoch: 13 [45056/50176]	Loss: 1.6691
Training Epoch: 13 [45568/50176]	Loss: 1.6577
Training Epoch: 13 [46080/50176]	Loss: 1.5884
Training Epoch: 13 [46592/50176]	Loss: 1.5049
Training Epoch: 13 [47104/50176]	Loss: 1.5900
Training Epoch: 13 [47616/50176]	Loss: 1.4994
Training Epoch: 13 [48128/50176]	Loss: 1.7223
Training Epoch: 13 [48640/50176]	Loss: 1.4792
Training Epoch: 13 [49152/50176]	Loss: 1.6485
Training Epoch: 13 [49664/50176]	Loss: 1.5733
Training Epoch: 13 [50176/50176]	Loss: 1.5481
Validation Epoch: 13, Average loss: 0.0039, Accuracy: 0.4863
Training Epoch: 14 [512/50176]	Loss: 1.4209
Training Epoch: 14 [1024/50176]	Loss: 1.3995
Training Epoch: 14 [1536/50176]	Loss: 1.3785
Training Epoch: 14 [2048/50176]	Loss: 1.4513
Training Epoch: 14 [2560/50176]	Loss: 1.3980
Training Epoch: 14 [3072/50176]	Loss: 1.2954
Training Epoch: 14 [3584/50176]	Loss: 1.4861
Training Epoch: 14 [4096/50176]	Loss: 1.4737
Training Epoch: 14 [4608/50176]	Loss: 1.3235
Training Epoch: 14 [5120/50176]	Loss: 1.4291
Training Epoch: 14 [5632/50176]	Loss: 1.4429
Training Epoch: 14 [6144/50176]	Loss: 1.4113
Training Epoch: 14 [6656/50176]	Loss: 1.4113
Training Epoch: 14 [7168/50176]	Loss: 1.2421
Training Epoch: 14 [7680/50176]	Loss: 1.3221
Training Epoch: 14 [8192/50176]	Loss: 1.3597
Training Epoch: 14 [8704/50176]	Loss: 1.3541
Training Epoch: 14 [9216/50176]	Loss: 1.4350
Training Epoch: 14 [9728/50176]	Loss: 1.3438
Training Epoch: 14 [10240/50176]	Loss: 1.4105
Training Epoch: 14 [10752/50176]	Loss: 1.4150
Training Epoch: 14 [11264/50176]	Loss: 1.4139
Training Epoch: 14 [11776/50176]	Loss: 1.4271
Training Epoch: 14 [12288/50176]	Loss: 1.3480
Training Epoch: 14 [12800/50176]	Loss: 1.4010
Training Epoch: 14 [13312/50176]	Loss: 1.4388
Training Epoch: 14 [13824/50176]	Loss: 1.4246
Training Epoch: 14 [14336/50176]	Loss: 1.5474
Training Epoch: 14 [14848/50176]	Loss: 1.4771
Training Epoch: 14 [15360/50176]	Loss: 1.3863
Training Epoch: 14 [15872/50176]	Loss: 1.3206
Training Epoch: 14 [16384/50176]	Loss: 1.3814
Training Epoch: 14 [16896/50176]	Loss: 1.4099
Training Epoch: 14 [17408/50176]	Loss: 1.4788
Training Epoch: 14 [17920/50176]	Loss: 1.4723
Training Epoch: 14 [18432/50176]	Loss: 1.3762
Training Epoch: 14 [18944/50176]	Loss: 1.4907
Training Epoch: 14 [19456/50176]	Loss: 1.3928
Training Epoch: 14 [19968/50176]	Loss: 1.4375
Training Epoch: 14 [20480/50176]	Loss: 1.3704
Training Epoch: 14 [20992/50176]	Loss: 1.4594
Training Epoch: 14 [21504/50176]	Loss: 1.4326
Training Epoch: 14 [22016/50176]	Loss: 1.3974
Training Epoch: 14 [22528/50176]	Loss: 1.4504
Training Epoch: 14 [23040/50176]	Loss: 1.4972
Training Epoch: 14 [23552/50176]	Loss: 1.4302
Training Epoch: 14 [24064/50176]	Loss: 1.4889
Training Epoch: 14 [24576/50176]	Loss: 1.4156
Training Epoch: 14 [25088/50176]	Loss: 1.4815
Training Epoch: 14 [25600/50176]	Loss: 1.4626
Training Epoch: 14 [26112/50176]	Loss: 1.5089
Training Epoch: 14 [26624/50176]	Loss: 1.5055
Training Epoch: 14 [27136/50176]	Loss: 1.5083
Training Epoch: 14 [27648/50176]	Loss: 1.5038
Training Epoch: 14 [28160/50176]	Loss: 1.4799
Training Epoch: 14 [28672/50176]	Loss: 1.6055
Training Epoch: 14 [29184/50176]	Loss: 1.4740
Training Epoch: 14 [29696/50176]	Loss: 1.5101
Training Epoch: 14 [30208/50176]	Loss: 1.4131
Training Epoch: 14 [30720/50176]	Loss: 1.5756
Training Epoch: 14 [31232/50176]	Loss: 1.5320
Training Epoch: 14 [31744/50176]	Loss: 1.4748
Training Epoch: 14 [32256/50176]	Loss: 1.3559
Training Epoch: 14 [32768/50176]	Loss: 1.4312
Training Epoch: 14 [33280/50176]	Loss: 1.4592
Training Epoch: 14 [33792/50176]	Loss: 1.4612
Training Epoch: 14 [34304/50176]	Loss: 1.4066
Training Epoch: 14 [34816/50176]	Loss: 1.4797
Training Epoch: 14 [35328/50176]	Loss: 1.3495
Training Epoch: 14 [35840/50176]	Loss: 1.5303
Training Epoch: 14 [36352/50176]	Loss: 1.4353
Training Epoch: 14 [36864/50176]	Loss: 1.4335
Training Epoch: 14 [37376/50176]	Loss: 1.5828
Training Epoch: 14 [37888/50176]	Loss: 1.4826
Training Epoch: 14 [38400/50176]	Loss: 1.4472
Training Epoch: 14 [38912/50176]	Loss: 1.3579
Training Epoch: 14 [39424/50176]	Loss: 1.4803
Training Epoch: 14 [39936/50176]	Loss: 1.4499
Training Epoch: 14 [40448/50176]	Loss: 1.5060
Training Epoch: 14 [40960/50176]	Loss: 1.4788
Training Epoch: 14 [41472/50176]	Loss: 1.4452
Training Epoch: 14 [41984/50176]	Loss: 1.6600
Training Epoch: 14 [42496/50176]	Loss: 1.5391
Training Epoch: 14 [43008/50176]	Loss: 1.4421
Training Epoch: 14 [43520/50176]	Loss: 1.5414
Training Epoch: 14 [44032/50176]	Loss: 1.5993
Training Epoch: 14 [44544/50176]	Loss: 1.5187
Training Epoch: 14 [45056/50176]	Loss: 1.4907
Training Epoch: 14 [45568/50176]	Loss: 1.4001
Training Epoch: 14 [46080/50176]	Loss: 1.5642
Training Epoch: 14 [46592/50176]	Loss: 1.4652
Training Epoch: 14 [47104/50176]	Loss: 1.6146
Training Epoch: 14 [47616/50176]	Loss: 1.4580
Training Epoch: 14 [48128/50176]	Loss: 1.5310
Training Epoch: 14 [48640/50176]	Loss: 1.4179
Training Epoch: 14 [49152/50176]	Loss: 1.4685
Training Epoch: 14 [49664/50176]	Loss: 1.4764
Training Epoch: 14 [50176/50176]	Loss: 1.4678
Validation Epoch: 14, Average loss: 0.0038, Accuracy: 0.4905
Training Epoch: 15 [512/50176]	Loss: 1.2603
Training Epoch: 15 [1024/50176]	Loss: 1.3262
Training Epoch: 15 [1536/50176]	Loss: 1.3105
Training Epoch: 15 [2048/50176]	Loss: 1.4541
Training Epoch: 15 [2560/50176]	Loss: 1.3017
Training Epoch: 15 [3072/50176]	Loss: 1.3548
Training Epoch: 15 [3584/50176]	Loss: 1.3092
Training Epoch: 15 [4096/50176]	Loss: 1.4256
Training Epoch: 15 [4608/50176]	Loss: 1.2456
Training Epoch: 15 [5120/50176]	Loss: 1.3949
Training Epoch: 15 [5632/50176]	Loss: 1.4071
Training Epoch: 15 [6144/50176]	Loss: 1.3798
Training Epoch: 15 [6656/50176]	Loss: 1.3690
Training Epoch: 15 [7168/50176]	Loss: 1.3954
Training Epoch: 15 [7680/50176]	Loss: 1.4423
Training Epoch: 15 [8192/50176]	Loss: 1.2745
Training Epoch: 15 [8704/50176]	Loss: 1.4004
Training Epoch: 15 [9216/50176]	Loss: 1.3861
Training Epoch: 15 [9728/50176]	Loss: 1.3706
Training Epoch: 15 [10240/50176]	Loss: 1.4020
Training Epoch: 15 [10752/50176]	Loss: 1.4067
Training Epoch: 15 [11264/50176]	Loss: 1.3471
Training Epoch: 15 [11776/50176]	Loss: 1.4705
Training Epoch: 15 [12288/50176]	Loss: 1.2617
Training Epoch: 15 [12800/50176]	Loss: 1.3764
Training Epoch: 15 [13312/50176]	Loss: 1.2507
Training Epoch: 15 [13824/50176]	Loss: 1.3198
Training Epoch: 15 [14336/50176]	Loss: 1.3238
Training Epoch: 15 [14848/50176]	Loss: 1.4424
Training Epoch: 15 [15360/50176]	Loss: 1.3526
Training Epoch: 15 [15872/50176]	Loss: 1.2882
Training Epoch: 15 [16384/50176]	Loss: 1.2628
Training Epoch: 15 [16896/50176]	Loss: 1.5257
Training Epoch: 15 [17408/50176]	Loss: 1.3087
Training Epoch: 15 [17920/50176]	Loss: 1.2513
Training Epoch: 15 [18432/50176]	Loss: 1.3473
Training Epoch: 15 [18944/50176]	Loss: 1.4011
Training Epoch: 15 [19456/50176]	Loss: 1.2711
Training Epoch: 15 [19968/50176]	Loss: 1.2301
Training Epoch: 15 [20480/50176]	Loss: 1.2541
Training Epoch: 15 [20992/50176]	Loss: 1.4776
Training Epoch: 15 [21504/50176]	Loss: 1.4084
Training Epoch: 15 [22016/50176]	Loss: 1.3010
Training Epoch: 15 [22528/50176]	Loss: 1.4569
Training Epoch: 15 [23040/50176]	Loss: 1.3627
Training Epoch: 15 [23552/50176]	Loss: 1.3280
Training Epoch: 15 [24064/50176]	Loss: 1.3203
Training Epoch: 15 [24576/50176]	Loss: 1.4101
Training Epoch: 15 [25088/50176]	Loss: 1.4924
Training Epoch: 15 [25600/50176]	Loss: 1.4268
Training Epoch: 15 [26112/50176]	Loss: 1.4603
Training Epoch: 15 [26624/50176]	Loss: 1.3041
Training Epoch: 15 [27136/50176]	Loss: 1.3959
Training Epoch: 15 [27648/50176]	Loss: 1.3100
Training Epoch: 15 [28160/50176]	Loss: 1.4574
Training Epoch: 15 [28672/50176]	Loss: 1.4453
Training Epoch: 15 [29184/50176]	Loss: 1.3773
Training Epoch: 15 [29696/50176]	Loss: 1.3061
Training Epoch: 15 [30208/50176]	Loss: 1.3272
Training Epoch: 15 [30720/50176]	Loss: 1.3036
Training Epoch: 15 [31232/50176]	Loss: 1.4236
Training Epoch: 15 [31744/50176]	Loss: 1.4586
Training Epoch: 15 [32256/50176]	Loss: 1.4166
Training Epoch: 15 [32768/50176]	Loss: 1.4293
Training Epoch: 15 [33280/50176]	Loss: 1.4901
Training Epoch: 15 [33792/50176]	Loss: 1.4055
Training Epoch: 15 [34304/50176]	Loss: 1.4515
Training Epoch: 15 [34816/50176]	Loss: 1.4732
Training Epoch: 15 [35328/50176]	Loss: 1.4953
Training Epoch: 15 [35840/50176]	Loss: 1.4596
Training Epoch: 15 [36352/50176]	Loss: 1.3652
Training Epoch: 15 [36864/50176]	Loss: 1.3455
Training Epoch: 15 [37376/50176]	Loss: 1.4354
Training Epoch: 15 [37888/50176]	Loss: 1.3735
Training Epoch: 15 [38400/50176]	Loss: 1.4430
Training Epoch: 15 [38912/50176]	Loss: 1.3148
Training Epoch: 15 [39424/50176]	Loss: 1.4365
Training Epoch: 15 [39936/50176]	Loss: 1.4464
Training Epoch: 15 [40448/50176]	Loss: 1.4352
Training Epoch: 15 [40960/50176]	Loss: 1.4790
Training Epoch: 15 [41472/50176]	Loss: 1.4811
Training Epoch: 15 [41984/50176]	Loss: 1.4985
Training Epoch: 15 [42496/50176]	Loss: 1.4033
Training Epoch: 15 [43008/50176]	Loss: 1.4343
Training Epoch: 15 [43520/50176]	Loss: 1.4190
Training Epoch: 15 [44032/50176]	Loss: 1.3803
Training Epoch: 15 [44544/50176]	Loss: 1.3388
Training Epoch: 15 [45056/50176]	Loss: 1.4048
Training Epoch: 15 [45568/50176]	Loss: 1.3298
Training Epoch: 15 [46080/50176]	Loss: 1.3254
Training Epoch: 15 [46592/50176]	Loss: 1.4442
Training Epoch: 15 [47104/50176]	Loss: 1.1897
Training Epoch: 15 [47616/50176]	Loss: 1.3856
Training Epoch: 15 [48128/50176]	Loss: 1.3628
Training Epoch: 15 [48640/50176]	Loss: 1.5019
Training Epoch: 15 [49152/50176]	Loss: 1.3553
Training Epoch: 15 [49664/50176]	Loss: 1.4738
Training Epoch: 15 [50176/50176]	Loss: 1.4072
Validation Epoch: 15, Average loss: 0.0036, Accuracy: 0.5064
Training Epoch: 16 [512/50176]	Loss: 1.3383
Training Epoch: 16 [1024/50176]	Loss: 1.3258
Training Epoch: 16 [1536/50176]	Loss: 1.1664
Training Epoch: 16 [2048/50176]	Loss: 1.1695
Training Epoch: 16 [2560/50176]	Loss: 1.3825
Training Epoch: 16 [3072/50176]	Loss: 1.2631
Training Epoch: 16 [3584/50176]	Loss: 1.2648
Training Epoch: 16 [4096/50176]	Loss: 1.3117
Training Epoch: 16 [4608/50176]	Loss: 1.3347
Training Epoch: 16 [5120/50176]	Loss: 1.3040
Training Epoch: 16 [5632/50176]	Loss: 1.2663
Training Epoch: 16 [6144/50176]	Loss: 1.3284
Training Epoch: 16 [6656/50176]	Loss: 1.2818
Training Epoch: 16 [7168/50176]	Loss: 1.2292
Training Epoch: 16 [7680/50176]	Loss: 1.1615
Training Epoch: 16 [8192/50176]	Loss: 1.2023
Training Epoch: 16 [8704/50176]	Loss: 1.2619
Training Epoch: 16 [9216/50176]	Loss: 1.2236
Training Epoch: 16 [9728/50176]	Loss: 1.3327
Training Epoch: 16 [10240/50176]	Loss: 1.2771
Training Epoch: 16 [10752/50176]	Loss: 1.2663
Training Epoch: 16 [11264/50176]	Loss: 1.3219
Training Epoch: 16 [11776/50176]	Loss: 1.3330
Training Epoch: 16 [12288/50176]	Loss: 1.4040
Training Epoch: 16 [12800/50176]	Loss: 1.1646
Training Epoch: 16 [13312/50176]	Loss: 1.2990
Training Epoch: 16 [13824/50176]	Loss: 1.3744
Training Epoch: 16 [14336/50176]	Loss: 1.1663
Training Epoch: 16 [14848/50176]	Loss: 1.2858
Training Epoch: 16 [15360/50176]	Loss: 1.3236
Training Epoch: 16 [15872/50176]	Loss: 1.2355
Training Epoch: 16 [16384/50176]	Loss: 1.2781
Training Epoch: 16 [16896/50176]	Loss: 1.2779
Training Epoch: 16 [17408/50176]	Loss: 1.3869
Training Epoch: 16 [17920/50176]	Loss: 1.2541
Training Epoch: 16 [18432/50176]	Loss: 1.3579
Training Epoch: 16 [18944/50176]	Loss: 1.3595
Training Epoch: 16 [19456/50176]	Loss: 1.4330
Training Epoch: 16 [19968/50176]	Loss: 1.2469
Training Epoch: 16 [20480/50176]	Loss: 1.3703
Training Epoch: 16 [20992/50176]	Loss: 1.3769
Training Epoch: 16 [21504/50176]	Loss: 1.1725
Training Epoch: 16 [22016/50176]	Loss: 1.3018
Training Epoch: 16 [22528/50176]	Loss: 1.3715
Training Epoch: 16 [23040/50176]	Loss: 1.3561
Training Epoch: 16 [23552/50176]	Loss: 1.2561
Training Epoch: 16 [24064/50176]	Loss: 1.4479
Training Epoch: 16 [24576/50176]	Loss: 1.2764
Training Epoch: 16 [25088/50176]	Loss: 1.3246
Training Epoch: 16 [25600/50176]	Loss: 1.1745
Training Epoch: 16 [26112/50176]	Loss: 1.4198
Training Epoch: 16 [26624/50176]	Loss: 1.3764
Training Epoch: 16 [27136/50176]	Loss: 1.2008
Training Epoch: 16 [27648/50176]	Loss: 1.1695
Training Epoch: 16 [28160/50176]	Loss: 1.3830
Training Epoch: 16 [28672/50176]	Loss: 1.2084
Training Epoch: 16 [29184/50176]	Loss: 1.5484
Training Epoch: 16 [29696/50176]	Loss: 1.3711
Training Epoch: 16 [30208/50176]	Loss: 1.3692
Training Epoch: 16 [30720/50176]	Loss: 1.3755
Training Epoch: 16 [31232/50176]	Loss: 1.2179
Training Epoch: 16 [31744/50176]	Loss: 1.2057
Training Epoch: 16 [32256/50176]	Loss: 1.4050
Training Epoch: 16 [32768/50176]	Loss: 1.2850
Training Epoch: 16 [33280/50176]	Loss: 1.3569
Training Epoch: 16 [33792/50176]	Loss: 1.2976
Training Epoch: 16 [34304/50176]	Loss: 1.3797
Training Epoch: 16 [34816/50176]	Loss: 1.1513
Training Epoch: 16 [35328/50176]	Loss: 1.3988
Training Epoch: 16 [35840/50176]	Loss: 1.3561
Training Epoch: 16 [36352/50176]	Loss: 1.2786
Training Epoch: 16 [36864/50176]	Loss: 1.3330
Training Epoch: 16 [37376/50176]	Loss: 1.2427
Training Epoch: 16 [37888/50176]	Loss: 1.2937
Training Epoch: 16 [38400/50176]	Loss: 1.2133
Training Epoch: 16 [38912/50176]	Loss: 1.3090
Training Epoch: 16 [39424/50176]	Loss: 1.2414
Training Epoch: 16 [39936/50176]	Loss: 1.2834
Training Epoch: 16 [40448/50176]	Loss: 1.2108
Training Epoch: 16 [40960/50176]	Loss: 1.2418
Training Epoch: 16 [41472/50176]	Loss: 1.3055
Training Epoch: 16 [41984/50176]	Loss: 1.4048
Training Epoch: 16 [42496/50176]	Loss: 1.5350
Training Epoch: 16 [43008/50176]	Loss: 1.3764
Training Epoch: 16 [43520/50176]	Loss: 1.3966
Training Epoch: 16 [44032/50176]	Loss: 1.2637
Training Epoch: 16 [44544/50176]	Loss: 1.3896
Training Epoch: 16 [45056/50176]	Loss: 1.3425
Training Epoch: 16 [45568/50176]	Loss: 1.3607
Training Epoch: 16 [46080/50176]	Loss: 1.4273
Training Epoch: 16 [46592/50176]	Loss: 1.3783
Training Epoch: 16 [47104/50176]	Loss: 1.3767
Training Epoch: 16 [47616/50176]	Loss: 1.2283
Training Epoch: 16 [48128/50176]	Loss: 1.3213
Training Epoch: 16 [48640/50176]	Loss: 1.3203
Training Epoch: 16 [49152/50176]	Loss: 1.5590
Training Epoch: 16 [49664/50176]	Loss: 1.2523
Training Epoch: 16 [50176/50176]	Loss: 1.2350
Validation Epoch: 16, Average loss: 0.0039, Accuracy: 0.4971
Training Epoch: 17 [512/50176]	Loss: 1.1765
Training Epoch: 17 [1024/50176]	Loss: 1.1591
Training Epoch: 17 [1536/50176]	Loss: 1.1587
Training Epoch: 17 [2048/50176]	Loss: 1.2225
Training Epoch: 17 [2560/50176]	Loss: 1.2297
Training Epoch: 17 [3072/50176]	Loss: 1.1904
Training Epoch: 17 [3584/50176]	Loss: 1.3050
Training Epoch: 17 [4096/50176]	Loss: 1.1324
Training Epoch: 17 [4608/50176]	Loss: 1.2731
Training Epoch: 17 [5120/50176]	Loss: 1.2033
Training Epoch: 17 [5632/50176]	Loss: 1.1874
Training Epoch: 17 [6144/50176]	Loss: 1.1239
Training Epoch: 17 [6656/50176]	Loss: 1.2846
Training Epoch: 17 [7168/50176]	Loss: 1.1524
Training Epoch: 17 [7680/50176]	Loss: 1.2052
Training Epoch: 17 [8192/50176]	Loss: 1.0896
Training Epoch: 17 [8704/50176]	Loss: 1.1813
Training Epoch: 17 [9216/50176]	Loss: 1.1995
Training Epoch: 17 [9728/50176]	Loss: 1.1583
Training Epoch: 17 [10240/50176]	Loss: 1.2113
Training Epoch: 17 [10752/50176]	Loss: 1.2337
Training Epoch: 17 [11264/50176]	Loss: 1.2534
Training Epoch: 17 [11776/50176]	Loss: 1.1736
Training Epoch: 17 [12288/50176]	Loss: 1.2193
Training Epoch: 17 [12800/50176]	Loss: 1.3308
Training Epoch: 17 [13312/50176]	Loss: 1.2684
Training Epoch: 17 [13824/50176]	Loss: 1.1912
Training Epoch: 17 [14336/50176]	Loss: 1.1091
Training Epoch: 17 [14848/50176]	Loss: 1.2134
Training Epoch: 17 [15360/50176]	Loss: 1.1681
Training Epoch: 17 [15872/50176]	Loss: 1.2312
Training Epoch: 17 [16384/50176]	Loss: 1.2258
Training Epoch: 17 [16896/50176]	Loss: 1.1948
Training Epoch: 17 [17408/50176]	Loss: 1.1565
Training Epoch: 17 [17920/50176]	Loss: 1.2563
Training Epoch: 17 [18432/50176]	Loss: 1.2015
Training Epoch: 17 [18944/50176]	Loss: 1.2389
Training Epoch: 17 [19456/50176]	Loss: 1.1621
Training Epoch: 17 [19968/50176]	Loss: 1.2592
Training Epoch: 17 [20480/50176]	Loss: 1.2875
Training Epoch: 17 [20992/50176]	Loss: 1.3546
Training Epoch: 17 [21504/50176]	Loss: 1.2503
Training Epoch: 17 [22016/50176]	Loss: 1.1769
Training Epoch: 17 [22528/50176]	Loss: 1.1954
Training Epoch: 17 [23040/50176]	Loss: 1.1706
Training Epoch: 17 [23552/50176]	Loss: 1.1689
Training Epoch: 17 [24064/50176]	Loss: 1.2734
Training Epoch: 17 [24576/50176]	Loss: 1.2988
Training Epoch: 17 [25088/50176]	Loss: 1.2696
Training Epoch: 17 [25600/50176]	Loss: 1.2147
Training Epoch: 17 [26112/50176]	Loss: 1.2178
Training Epoch: 17 [26624/50176]	Loss: 1.2746
Training Epoch: 17 [27136/50176]	Loss: 1.2307
Training Epoch: 17 [27648/50176]	Loss: 1.2725
Training Epoch: 17 [28160/50176]	Loss: 1.2746
Training Epoch: 17 [28672/50176]	Loss: 1.2428
Training Epoch: 17 [29184/50176]	Loss: 1.3366
Training Epoch: 17 [29696/50176]	Loss: 1.1394
Training Epoch: 17 [30208/50176]	Loss: 1.2204
Training Epoch: 17 [30720/50176]	Loss: 1.2922
Training Epoch: 17 [31232/50176]	Loss: 1.2781
Training Epoch: 17 [31744/50176]	Loss: 1.2646
Training Epoch: 17 [32256/50176]	Loss: 1.3318
Training Epoch: 17 [32768/50176]	Loss: 1.2140
Training Epoch: 17 [33280/50176]	Loss: 1.3887
Training Epoch: 17 [33792/50176]	Loss: 1.3025
Training Epoch: 17 [34304/50176]	Loss: 1.3891
Training Epoch: 17 [34816/50176]	Loss: 1.1703
Training Epoch: 17 [35328/50176]	Loss: 1.2503
Training Epoch: 17 [35840/50176]	Loss: 1.3538
Training Epoch: 17 [36352/50176]	Loss: 1.2058
Training Epoch: 17 [36864/50176]	Loss: 1.3151
Training Epoch: 17 [37376/50176]	Loss: 1.3891
Training Epoch: 17 [37888/50176]	Loss: 1.1876
Training Epoch: 17 [38400/50176]	Loss: 1.3290
Training Epoch: 17 [38912/50176]	Loss: 1.3268
Training Epoch: 17 [39424/50176]	Loss: 1.2939
Training Epoch: 17 [39936/50176]	Loss: 1.2828
Training Epoch: 17 [40448/50176]	Loss: 1.2388
Training Epoch: 17 [40960/50176]	Loss: 1.3340
Training Epoch: 17 [41472/50176]	Loss: 1.2879
Training Epoch: 17 [41984/50176]	Loss: 1.2745
Training Epoch: 17 [42496/50176]	Loss: 1.2567
Training Epoch: 17 [43008/50176]	Loss: 1.2004
Training Epoch: 17 [43520/50176]	Loss: 1.2980
Training Epoch: 17 [44032/50176]	Loss: 1.2131
Training Epoch: 17 [44544/50176]	Loss: 1.2654
Training Epoch: 17 [45056/50176]	Loss: 1.2339
Training Epoch: 17 [45568/50176]	Loss: 1.3244
Training Epoch: 17 [46080/50176]	Loss: 1.2453
Training Epoch: 17 [46592/50176]	Loss: 1.2686
Training Epoch: 17 [47104/50176]	Loss: 1.3353
Training Epoch: 17 [47616/50176]	Loss: 1.2206
Training Epoch: 17 [48128/50176]	Loss: 1.1754
Training Epoch: 17 [48640/50176]	Loss: 1.2618
Training Epoch: 17 [49152/50176]	Loss: 1.3619
Training Epoch: 17 [49664/50176]	Loss: 1.3019
Training Epoch: 17 [50176/50176]	Loss: 1.3962
Validation Epoch: 17, Average loss: 0.0034, Accuracy: 0.5180
Training Epoch: 18 [512/50176]	Loss: 1.1330
Training Epoch: 18 [1024/50176]	Loss: 1.2682
Training Epoch: 18 [1536/50176]	Loss: 1.0740
Training Epoch: 18 [2048/50176]	Loss: 1.1211
Training Epoch: 18 [2560/50176]	Loss: 1.2459
Training Epoch: 18 [3072/50176]	Loss: 1.1222
Training Epoch: 18 [3584/50176]	Loss: 1.2235
Training Epoch: 18 [4096/50176]	Loss: 1.1597
Training Epoch: 18 [4608/50176]	Loss: 1.1327
Training Epoch: 18 [5120/50176]	Loss: 1.0885
Training Epoch: 18 [5632/50176]	Loss: 1.3065
Training Epoch: 18 [6144/50176]	Loss: 1.2231
Training Epoch: 18 [6656/50176]	Loss: 1.1192
Training Epoch: 18 [7168/50176]	Loss: 1.0584
Training Epoch: 18 [7680/50176]	Loss: 1.1591
Training Epoch: 18 [8192/50176]	Loss: 1.1842
Training Epoch: 18 [8704/50176]	Loss: 1.0841
Training Epoch: 18 [9216/50176]	Loss: 1.1797
Training Epoch: 18 [9728/50176]	Loss: 1.2556
Training Epoch: 18 [10240/50176]	Loss: 1.2129
Training Epoch: 18 [10752/50176]	Loss: 1.1428
Training Epoch: 18 [11264/50176]	Loss: 1.3016
Training Epoch: 18 [11776/50176]	Loss: 1.2914
Training Epoch: 18 [12288/50176]	Loss: 1.1571
Training Epoch: 18 [12800/50176]	Loss: 1.1749
Training Epoch: 18 [13312/50176]	Loss: 1.1800
Training Epoch: 18 [13824/50176]	Loss: 1.1566
Training Epoch: 18 [14336/50176]	Loss: 1.0383
Training Epoch: 18 [14848/50176]	Loss: 1.3074
Training Epoch: 18 [15360/50176]	Loss: 1.2164
Training Epoch: 18 [15872/50176]	Loss: 1.2463
Training Epoch: 18 [16384/50176]	Loss: 1.1709
Training Epoch: 18 [16896/50176]	Loss: 1.1747
Training Epoch: 18 [17408/50176]	Loss: 1.2132
Training Epoch: 18 [17920/50176]	Loss: 1.0842
Training Epoch: 18 [18432/50176]	Loss: 1.0162
Training Epoch: 18 [18944/50176]	Loss: 1.1989
Training Epoch: 18 [19456/50176]	Loss: 1.2312
Training Epoch: 18 [19968/50176]	Loss: 1.1151
Training Epoch: 18 [20480/50176]	Loss: 1.1724
Training Epoch: 18 [20992/50176]	Loss: 1.1720
Training Epoch: 18 [21504/50176]	Loss: 1.1396
Training Epoch: 18 [22016/50176]	Loss: 1.1931
Training Epoch: 18 [22528/50176]	Loss: 1.1423
Training Epoch: 18 [23040/50176]	Loss: 1.2671
Training Epoch: 18 [23552/50176]	Loss: 1.1588
Training Epoch: 18 [24064/50176]	Loss: 1.1114
Training Epoch: 18 [24576/50176]	Loss: 1.2936
Training Epoch: 18 [25088/50176]	Loss: 1.2549
Training Epoch: 18 [25600/50176]	Loss: 1.2006
Training Epoch: 18 [26112/50176]	Loss: 1.1153
Training Epoch: 18 [26624/50176]	Loss: 1.1364
Training Epoch: 18 [27136/50176]	Loss: 1.2298
Training Epoch: 18 [27648/50176]	Loss: 1.1780
Training Epoch: 18 [28160/50176]	Loss: 1.2277
Training Epoch: 18 [28672/50176]	Loss: 1.2093
Training Epoch: 18 [29184/50176]	Loss: 1.2186
Training Epoch: 18 [29696/50176]	Loss: 1.1123
Training Epoch: 18 [30208/50176]	Loss: 1.2801
Training Epoch: 18 [30720/50176]	Loss: 1.2724
Training Epoch: 18 [31232/50176]	Loss: 1.2212
Training Epoch: 18 [31744/50176]	Loss: 1.2951
Training Epoch: 18 [32256/50176]	Loss: 1.2879
Training Epoch: 18 [32768/50176]	Loss: 1.2010
Training Epoch: 18 [33280/50176]	Loss: 1.2917
Training Epoch: 18 [33792/50176]	Loss: 1.1159
Training Epoch: 18 [34304/50176]	Loss: 1.1849
Training Epoch: 18 [34816/50176]	Loss: 1.2554
Training Epoch: 18 [35328/50176]	Loss: 1.2726
Training Epoch: 18 [35840/50176]	Loss: 1.1185
Training Epoch: 18 [36352/50176]	Loss: 1.1677
Training Epoch: 18 [36864/50176]	Loss: 1.2742
Training Epoch: 18 [37376/50176]	Loss: 1.2017
Training Epoch: 18 [37888/50176]	Loss: 1.2004
Training Epoch: 18 [38400/50176]	Loss: 1.1403
Training Epoch: 18 [38912/50176]	Loss: 1.2446
Training Epoch: 18 [39424/50176]	Loss: 1.1133
Training Epoch: 18 [39936/50176]	Loss: 1.1860
Training Epoch: 18 [40448/50176]	Loss: 1.2257
Training Epoch: 18 [40960/50176]	Loss: 1.2620
Training Epoch: 18 [41472/50176]	Loss: 1.2946
Training Epoch: 18 [41984/50176]	Loss: 1.1960
Training Epoch: 18 [42496/50176]	Loss: 1.3093
Training Epoch: 18 [43008/50176]	Loss: 1.2072
Training Epoch: 18 [43520/50176]	Loss: 1.2335
Training Epoch: 18 [44032/50176]	Loss: 1.1153
Training Epoch: 18 [44544/50176]	Loss: 1.2095
Training Epoch: 18 [45056/50176]	Loss: 1.2241
Training Epoch: 18 [45568/50176]	Loss: 1.1823
Training Epoch: 18 [46080/50176]	Loss: 1.2397
Training Epoch: 18 [46592/50176]	Loss: 1.2204
Training Epoch: 18 [47104/50176]	Loss: 1.2748
Training Epoch: 18 [47616/50176]	Loss: 1.2689
Training Epoch: 18 [48128/50176]	Loss: 1.0873
Training Epoch: 18 [48640/50176]	Loss: 1.1785
Training Epoch: 18 [49152/50176]	Loss: 1.1927
Training Epoch: 18 [49664/50176]	Loss: 1.1295
Training Epoch: 18 [50176/50176]	Loss: 1.2834
Validation Epoch: 18, Average loss: 0.0038, Accuracy: 0.5046
Training Epoch: 19 [512/50176]	Loss: 1.1522
Training Epoch: 19 [1024/50176]	Loss: 1.0244
Training Epoch: 19 [1536/50176]	Loss: 1.1258
Training Epoch: 19 [2048/50176]	Loss: 0.9975
Training Epoch: 19 [2560/50176]	Loss: 1.0767
Training Epoch: 19 [3072/50176]	Loss: 0.9904
Training Epoch: 19 [3584/50176]	Loss: 1.1791
Training Epoch: 19 [4096/50176]	Loss: 1.0895
Training Epoch: 19 [4608/50176]	Loss: 1.0993
Training Epoch: 19 [5120/50176]	Loss: 1.1749
Training Epoch: 19 [5632/50176]	Loss: 0.9918
Training Epoch: 19 [6144/50176]	Loss: 0.9688
Training Epoch: 19 [6656/50176]	Loss: 1.1842
Training Epoch: 19 [7168/50176]	Loss: 1.1064
Training Epoch: 19 [7680/50176]	Loss: 1.1398
Training Epoch: 19 [8192/50176]	Loss: 1.1408
Training Epoch: 19 [8704/50176]	Loss: 1.1272
Training Epoch: 19 [9216/50176]	Loss: 1.1620
Training Epoch: 19 [9728/50176]	Loss: 0.9960
Training Epoch: 19 [10240/50176]	Loss: 0.9872
Training Epoch: 19 [10752/50176]	Loss: 1.0712
Training Epoch: 19 [11264/50176]	Loss: 1.0708
Training Epoch: 19 [11776/50176]	Loss: 0.9866
Training Epoch: 19 [12288/50176]	Loss: 0.9435
Training Epoch: 19 [12800/50176]	Loss: 1.1494
Training Epoch: 19 [13312/50176]	Loss: 1.1102
Training Epoch: 19 [13824/50176]	Loss: 1.1431
Training Epoch: 19 [14336/50176]	Loss: 1.1197
Training Epoch: 19 [14848/50176]	Loss: 1.1342
Training Epoch: 19 [15360/50176]	Loss: 1.1460
Training Epoch: 19 [15872/50176]	Loss: 1.0546
Training Epoch: 19 [16384/50176]	Loss: 1.1435
Training Epoch: 19 [16896/50176]	Loss: 1.2967
Training Epoch: 19 [17408/50176]	Loss: 1.2594
Training Epoch: 19 [17920/50176]	Loss: 1.1977
Training Epoch: 19 [18432/50176]	Loss: 1.1841
Training Epoch: 19 [18944/50176]	Loss: 1.1475
Training Epoch: 19 [19456/50176]	Loss: 1.2051
Training Epoch: 19 [19968/50176]	Loss: 1.0987
Training Epoch: 19 [20480/50176]	Loss: 1.0797
Training Epoch: 19 [20992/50176]	Loss: 1.1245
Training Epoch: 19 [21504/50176]	Loss: 1.0738
Training Epoch: 19 [22016/50176]	Loss: 1.2224
Training Epoch: 19 [22528/50176]	Loss: 1.0286
Training Epoch: 19 [23040/50176]	Loss: 1.1436
Training Epoch: 19 [23552/50176]	Loss: 1.0163
Training Epoch: 19 [24064/50176]	Loss: 1.0739
Training Epoch: 19 [24576/50176]	Loss: 1.0996
Training Epoch: 19 [25088/50176]	Loss: 1.2886
Training Epoch: 19 [25600/50176]	Loss: 1.1129
Training Epoch: 19 [26112/50176]	Loss: 1.1553
Training Epoch: 19 [26624/50176]	Loss: 1.0689
Training Epoch: 19 [27136/50176]	Loss: 1.1535
Training Epoch: 19 [27648/50176]	Loss: 1.0939
Training Epoch: 19 [28160/50176]	Loss: 1.0795
Training Epoch: 19 [28672/50176]	Loss: 1.1461
Training Epoch: 19 [29184/50176]	Loss: 1.1532
Training Epoch: 19 [29696/50176]	Loss: 1.1944
Training Epoch: 19 [30208/50176]	Loss: 1.1801
Training Epoch: 19 [30720/50176]	Loss: 1.1365
Training Epoch: 19 [31232/50176]	Loss: 1.0696
Training Epoch: 19 [31744/50176]	Loss: 1.0357
Training Epoch: 19 [32256/50176]	Loss: 1.0773
Training Epoch: 19 [32768/50176]	Loss: 1.1587
Training Epoch: 19 [33280/50176]	Loss: 1.1354
Training Epoch: 19 [33792/50176]	Loss: 1.1600
Training Epoch: 19 [34304/50176]	Loss: 1.1296
Training Epoch: 19 [34816/50176]	Loss: 1.1445
Training Epoch: 19 [35328/50176]	Loss: 1.1602
Training Epoch: 19 [35840/50176]	Loss: 1.1846
Training Epoch: 19 [36352/50176]	Loss: 1.1668
Training Epoch: 19 [36864/50176]	Loss: 1.2588
Training Epoch: 19 [37376/50176]	Loss: 1.1469
Training Epoch: 19 [37888/50176]	Loss: 1.2733
Training Epoch: 19 [38400/50176]	Loss: 1.0742
Training Epoch: 19 [38912/50176]	Loss: 1.3197
Training Epoch: 19 [39424/50176]	Loss: 1.1264
Training Epoch: 19 [39936/50176]	Loss: 1.2680
Training Epoch: 19 [40448/50176]	Loss: 1.1617
Training Epoch: 19 [40960/50176]	Loss: 1.2353
Training Epoch: 19 [41472/50176]	Loss: 1.2305
Training Epoch: 19 [41984/50176]	Loss: 1.1040
Training Epoch: 19 [42496/50176]	Loss: 1.2811
Training Epoch: 19 [43008/50176]	Loss: 1.1830
Training Epoch: 19 [43520/50176]	Loss: 1.2153
Training Epoch: 19 [44032/50176]	Loss: 1.2313
Training Epoch: 19 [44544/50176]	Loss: 1.0665
Training Epoch: 19 [45056/50176]	Loss: 1.1726
Training Epoch: 19 [45568/50176]	Loss: 1.1520
Training Epoch: 19 [46080/50176]	Loss: 1.2516
Training Epoch: 19 [46592/50176]	Loss: 1.2656
Training Epoch: 19 [47104/50176]	Loss: 1.1039
Training Epoch: 19 [47616/50176]	Loss: 1.1731
Training Epoch: 19 [48128/50176]	Loss: 1.2343
Training Epoch: 19 [48640/50176]	Loss: 1.1983
Training Epoch: 19 [49152/50176]	Loss: 1.1979
Training Epoch: 19 [49664/50176]	Loss: 1.0840
Training Epoch: 19 [50176/50176]	Loss: 1.2361
Validation Epoch: 19, Average loss: 0.0042, Accuracy: 0.4896
Training Epoch: 20 [512/50176]	Loss: 0.9253
Training Epoch: 20 [1024/50176]	Loss: 1.0263
Training Epoch: 20 [1536/50176]	Loss: 1.0960
Training Epoch: 20 [2048/50176]	Loss: 1.0901
Training Epoch: 20 [2560/50176]	Loss: 1.0090
Training Epoch: 20 [3072/50176]	Loss: 1.0459
Training Epoch: 20 [3584/50176]	Loss: 1.1027
Training Epoch: 20 [4096/50176]	Loss: 1.0318
Training Epoch: 20 [4608/50176]	Loss: 0.9719
Training Epoch: 20 [5120/50176]	Loss: 1.0492
Training Epoch: 20 [5632/50176]	Loss: 0.9376
Training Epoch: 20 [6144/50176]	Loss: 1.0108
Training Epoch: 20 [6656/50176]	Loss: 0.9739
Training Epoch: 20 [7168/50176]	Loss: 0.9638
Training Epoch: 20 [7680/50176]	Loss: 0.9530
Training Epoch: 20 [8192/50176]	Loss: 0.9567
Training Epoch: 20 [8704/50176]	Loss: 0.9800
Training Epoch: 20 [9216/50176]	Loss: 1.1500
Training Epoch: 20 [9728/50176]	Loss: 1.0352
Training Epoch: 20 [10240/50176]	Loss: 1.0464
Training Epoch: 20 [10752/50176]	Loss: 0.9715
Training Epoch: 20 [11264/50176]	Loss: 0.9457
Training Epoch: 20 [11776/50176]	Loss: 1.0913
Training Epoch: 20 [12288/50176]	Loss: 1.0012
Training Epoch: 20 [12800/50176]	Loss: 1.0833
Training Epoch: 20 [13312/50176]	Loss: 1.0103
Training Epoch: 20 [13824/50176]	Loss: 1.0865
Training Epoch: 20 [14336/50176]	Loss: 1.0656
Training Epoch: 20 [14848/50176]	Loss: 1.0684
Training Epoch: 20 [15360/50176]	Loss: 1.0434
Training Epoch: 20 [15872/50176]	Loss: 1.0555
Training Epoch: 20 [16384/50176]	Loss: 1.1395
Training Epoch: 20 [16896/50176]	Loss: 1.0360
Training Epoch: 20 [17408/50176]	Loss: 1.0636
Training Epoch: 20 [17920/50176]	Loss: 1.1541
Training Epoch: 20 [18432/50176]	Loss: 0.9827
Training Epoch: 20 [18944/50176]	Loss: 0.9912
Training Epoch: 20 [19456/50176]	Loss: 1.1083
Training Epoch: 20 [19968/50176]	Loss: 1.0893
Training Epoch: 20 [20480/50176]	Loss: 1.0197
Training Epoch: 20 [20992/50176]	Loss: 1.0807
Training Epoch: 20 [21504/50176]	Loss: 1.1522
Training Epoch: 20 [22016/50176]	Loss: 1.1383
Training Epoch: 20 [22528/50176]	Loss: 1.0863
Training Epoch: 20 [23040/50176]	Loss: 1.0658
Training Epoch: 20 [23552/50176]	Loss: 1.2295
Training Epoch: 20 [24064/50176]	Loss: 1.0739
Training Epoch: 20 [24576/50176]	Loss: 1.1622
Training Epoch: 20 [25088/50176]	Loss: 1.1146
Training Epoch: 20 [25600/50176]	Loss: 1.0967
Training Epoch: 20 [26112/50176]	Loss: 1.0877
Training Epoch: 20 [26624/50176]	Loss: 1.0564
Training Epoch: 20 [27136/50176]	Loss: 1.1493
Training Epoch: 20 [27648/50176]	Loss: 1.0546
Training Epoch: 20 [28160/50176]	Loss: 1.0513
Training Epoch: 20 [28672/50176]	Loss: 1.1318
Training Epoch: 20 [29184/50176]	Loss: 1.0577
Training Epoch: 20 [29696/50176]	Loss: 1.0609
Training Epoch: 20 [30208/50176]	Loss: 1.0795
Training Epoch: 20 [30720/50176]	Loss: 1.1071
Training Epoch: 20 [31232/50176]	Loss: 1.1372
Training Epoch: 20 [31744/50176]	Loss: 1.0093
Training Epoch: 20 [32256/50176]	Loss: 1.0565
Training Epoch: 20 [32768/50176]	Loss: 0.9762
Training Epoch: 20 [33280/50176]	Loss: 1.1348
Training Epoch: 20 [33792/50176]	Loss: 1.0563
Training Epoch: 20 [34304/50176]	Loss: 1.1070
Training Epoch: 20 [34816/50176]	Loss: 1.0688
Training Epoch: 20 [35328/50176]	Loss: 1.0459
Training Epoch: 20 [35840/50176]	Loss: 1.0303
Training Epoch: 20 [36352/50176]	Loss: 1.0264
Training Epoch: 20 [36864/50176]	Loss: 0.9709
Training Epoch: 20 [37376/50176]	Loss: 1.2662
Training Epoch: 20 [37888/50176]	Loss: 1.1948
Training Epoch: 20 [38400/50176]	Loss: 1.1885
Training Epoch: 20 [38912/50176]	Loss: 1.1631
Training Epoch: 20 [39424/50176]	Loss: 1.1025
Training Epoch: 20 [39936/50176]	Loss: 1.1147
Training Epoch: 20 [40448/50176]	Loss: 1.0689
Training Epoch: 20 [40960/50176]	Loss: 1.3141
Training Epoch: 20 [41472/50176]	Loss: 1.1012
Training Epoch: 20 [41984/50176]	Loss: 1.1258
Training Epoch: 20 [42496/50176]	Loss: 1.1153
Training Epoch: 20 [43008/50176]	Loss: 1.1265
Training Epoch: 20 [43520/50176]	Loss: 1.2095
Training Epoch: 20 [44032/50176]	Loss: 1.0859
Training Epoch: 20 [44544/50176]	Loss: 1.2315
Training Epoch: 20 [45056/50176]	Loss: 1.0752
Training Epoch: 20 [45568/50176]	Loss: 1.1078
Training Epoch: 20 [46080/50176]	Loss: 1.0342
Training Epoch: 20 [46592/50176]	Loss: 1.0379
Training Epoch: 20 [47104/50176]	Loss: 1.0565
Training Epoch: 20 [47616/50176]	Loss: 1.2440
Training Epoch: 20 [48128/50176]	Loss: 1.1161
Training Epoch: 20 [48640/50176]	Loss: 1.0942
Training Epoch: 20 [49152/50176]	Loss: 1.1790
Training Epoch: 20 [49664/50176]	Loss: 1.1249
Training Epoch: 20 [50176/50176]	Loss: 1.2158
Validation Epoch: 20, Average loss: 0.0032, Accuracy: 0.5474
Training Epoch: 21 [512/50176]	Loss: 0.9494
Training Epoch: 21 [1024/50176]	Loss: 1.0115
Training Epoch: 21 [1536/50176]	Loss: 0.8906
Training Epoch: 21 [2048/50176]	Loss: 0.9945
Training Epoch: 21 [2560/50176]	Loss: 0.9781
Training Epoch: 21 [3072/50176]	Loss: 0.9790
Training Epoch: 21 [3584/50176]	Loss: 0.9460
Training Epoch: 21 [4096/50176]	Loss: 0.9019
Training Epoch: 21 [4608/50176]	Loss: 1.0306
Training Epoch: 21 [5120/50176]	Loss: 0.9469
Training Epoch: 21 [5632/50176]	Loss: 1.0970
Training Epoch: 21 [6144/50176]	Loss: 1.0292
Training Epoch: 21 [6656/50176]	Loss: 0.9945
Training Epoch: 21 [7168/50176]	Loss: 1.0386
Training Epoch: 21 [7680/50176]	Loss: 0.9622
Training Epoch: 21 [8192/50176]	Loss: 0.9754
Training Epoch: 21 [8704/50176]	Loss: 0.8999
Training Epoch: 21 [9216/50176]	Loss: 1.0345
Training Epoch: 21 [9728/50176]	Loss: 0.9443
Training Epoch: 21 [10240/50176]	Loss: 0.9148
Training Epoch: 21 [10752/50176]	Loss: 0.9185
Training Epoch: 21 [11264/50176]	Loss: 1.1075
Training Epoch: 21 [11776/50176]	Loss: 1.0303
Training Epoch: 21 [12288/50176]	Loss: 0.9319
Training Epoch: 21 [12800/50176]	Loss: 0.9597
Training Epoch: 21 [13312/50176]	Loss: 0.9626
Training Epoch: 21 [13824/50176]	Loss: 1.0397
Training Epoch: 21 [14336/50176]	Loss: 1.0533
Training Epoch: 21 [14848/50176]	Loss: 1.1031
Training Epoch: 21 [15360/50176]	Loss: 1.0890
Training Epoch: 21 [15872/50176]	Loss: 1.0710
Training Epoch: 21 [16384/50176]	Loss: 1.0929
Training Epoch: 21 [16896/50176]	Loss: 0.9912
Training Epoch: 21 [17408/50176]	Loss: 0.9948
Training Epoch: 21 [17920/50176]	Loss: 0.9562
Training Epoch: 21 [18432/50176]	Loss: 0.9447
Training Epoch: 21 [18944/50176]	Loss: 1.0609
Training Epoch: 21 [19456/50176]	Loss: 0.9199
Training Epoch: 21 [19968/50176]	Loss: 0.9381
Training Epoch: 21 [20480/50176]	Loss: 1.0885
Training Epoch: 21 [20992/50176]	Loss: 1.1368
Training Epoch: 21 [21504/50176]	Loss: 0.9640
Training Epoch: 21 [22016/50176]	Loss: 0.9754
Training Epoch: 21 [22528/50176]	Loss: 0.9937
Training Epoch: 21 [23040/50176]	Loss: 1.0165
Training Epoch: 21 [23552/50176]	Loss: 1.0204
Training Epoch: 21 [24064/50176]	Loss: 1.0970
Training Epoch: 21 [24576/50176]	Loss: 1.0682
Training Epoch: 21 [25088/50176]	Loss: 0.9430
Training Epoch: 21 [25600/50176]	Loss: 0.9806
Training Epoch: 21 [26112/50176]	Loss: 1.1605
Training Epoch: 21 [26624/50176]	Loss: 1.0573
Training Epoch: 21 [27136/50176]	Loss: 1.1244
Training Epoch: 21 [27648/50176]	Loss: 1.0301
Training Epoch: 21 [28160/50176]	Loss: 1.0715
Training Epoch: 21 [28672/50176]	Loss: 0.8916
Training Epoch: 21 [29184/50176]	Loss: 1.0733
Training Epoch: 21 [29696/50176]	Loss: 1.0537
Training Epoch: 21 [30208/50176]	Loss: 1.0539
Training Epoch: 21 [30720/50176]	Loss: 1.0811
Training Epoch: 21 [31232/50176]	Loss: 1.0351
Training Epoch: 21 [31744/50176]	Loss: 1.1190
Training Epoch: 21 [32256/50176]	Loss: 1.0559
Training Epoch: 21 [32768/50176]	Loss: 0.9559
Training Epoch: 21 [33280/50176]	Loss: 0.9886
Training Epoch: 21 [33792/50176]	Loss: 0.9397
Training Epoch: 21 [34304/50176]	Loss: 1.0344
Training Epoch: 21 [34816/50176]	Loss: 0.9860
Training Epoch: 21 [35328/50176]	Loss: 0.9615
Training Epoch: 21 [35840/50176]	Loss: 0.9833
Training Epoch: 21 [36352/50176]	Loss: 1.0808
Training Epoch: 21 [36864/50176]	Loss: 1.0385
Training Epoch: 21 [37376/50176]	Loss: 1.0640
Training Epoch: 21 [37888/50176]	Loss: 1.1391
Training Epoch: 21 [38400/50176]	Loss: 1.0751
Training Epoch: 21 [38912/50176]	Loss: 1.0844
Training Epoch: 21 [39424/50176]	Loss: 1.0948
Training Epoch: 21 [39936/50176]	Loss: 1.0940
Training Epoch: 21 [40448/50176]	Loss: 1.0374
Training Epoch: 21 [40960/50176]	Loss: 0.9789
Training Epoch: 21 [41472/50176]	Loss: 1.1432
Training Epoch: 21 [41984/50176]	Loss: 1.1530
Training Epoch: 21 [42496/50176]	Loss: 1.0867
Training Epoch: 21 [43008/50176]	Loss: 1.1446
Training Epoch: 21 [43520/50176]	Loss: 1.1267
Training Epoch: 21 [44032/50176]	Loss: 1.1172
Training Epoch: 21 [44544/50176]	Loss: 1.1426
Training Epoch: 21 [45056/50176]	Loss: 1.0156
Training Epoch: 21 [45568/50176]	Loss: 1.0996
Training Epoch: 21 [46080/50176]	Loss: 1.0197
Training Epoch: 21 [46592/50176]	Loss: 1.1047
Training Epoch: 21 [47104/50176]	Loss: 1.0615
Training Epoch: 21 [47616/50176]	Loss: 1.0517
Training Epoch: 21 [48128/50176]	Loss: 1.0540
Training Epoch: 21 [48640/50176]	Loss: 1.0727
Training Epoch: 21 [49152/50176]	Loss: 1.1342
Training Epoch: 21 [49664/50176]	Loss: 1.1400
Training Epoch: 21 [50176/50176]	Loss: 0.9932
Validation Epoch: 21, Average loss: 0.0041, Accuracy: 0.5112
Training Epoch: 22 [512/50176]	Loss: 0.8707
Training Epoch: 22 [1024/50176]	Loss: 0.8505
Training Epoch: 22 [1536/50176]	Loss: 0.9326
Training Epoch: 22 [2048/50176]	Loss: 0.9394
Training Epoch: 22 [2560/50176]	Loss: 0.8828
Training Epoch: 22 [3072/50176]	Loss: 0.9275
Training Epoch: 22 [3584/50176]	Loss: 0.8886
Training Epoch: 22 [4096/50176]	Loss: 0.9024
Training Epoch: 22 [4608/50176]	Loss: 0.9601
Training Epoch: 22 [5120/50176]	Loss: 0.8450
Training Epoch: 22 [5632/50176]	Loss: 0.9361
Training Epoch: 22 [6144/50176]	Loss: 0.8913
Training Epoch: 22 [6656/50176]	Loss: 0.8494
Training Epoch: 22 [7168/50176]	Loss: 0.7918
Training Epoch: 22 [7680/50176]	Loss: 0.9417
Training Epoch: 22 [8192/50176]	Loss: 0.9092
Training Epoch: 22 [8704/50176]	Loss: 1.0075
Training Epoch: 22 [9216/50176]	Loss: 0.9230
Training Epoch: 22 [9728/50176]	Loss: 0.9074
Training Epoch: 22 [10240/50176]	Loss: 0.8161
Training Epoch: 22 [10752/50176]	Loss: 0.9539
Training Epoch: 22 [11264/50176]	Loss: 0.9477
Training Epoch: 22 [11776/50176]	Loss: 1.0054
Training Epoch: 22 [12288/50176]	Loss: 0.9246
Training Epoch: 22 [12800/50176]	Loss: 0.9572
Training Epoch: 22 [13312/50176]	Loss: 0.9312
Training Epoch: 22 [13824/50176]	Loss: 0.9566
Training Epoch: 22 [14336/50176]	Loss: 0.9481
Training Epoch: 22 [14848/50176]	Loss: 0.9320
Training Epoch: 22 [15360/50176]	Loss: 0.8912
Training Epoch: 22 [15872/50176]	Loss: 0.9840
Training Epoch: 22 [16384/50176]	Loss: 0.8954
Training Epoch: 22 [16896/50176]	Loss: 0.9488
Training Epoch: 22 [17408/50176]	Loss: 1.0190
Training Epoch: 22 [17920/50176]	Loss: 1.0496
Training Epoch: 22 [18432/50176]	Loss: 1.0014
Training Epoch: 22 [18944/50176]	Loss: 0.9441
Training Epoch: 22 [19456/50176]	Loss: 0.9416
Training Epoch: 22 [19968/50176]	Loss: 1.0645
Training Epoch: 22 [20480/50176]	Loss: 1.0030
Training Epoch: 22 [20992/50176]	Loss: 1.0341
Training Epoch: 22 [21504/50176]	Loss: 1.0114
Training Epoch: 22 [22016/50176]	Loss: 1.0371
Training Epoch: 22 [22528/50176]	Loss: 0.9775
Training Epoch: 22 [23040/50176]	Loss: 1.0007
Training Epoch: 22 [23552/50176]	Loss: 1.0247
Training Epoch: 22 [24064/50176]	Loss: 1.0624
Training Epoch: 22 [24576/50176]	Loss: 0.9904
Training Epoch: 22 [25088/50176]	Loss: 1.0333
Training Epoch: 22 [25600/50176]	Loss: 1.0021
Training Epoch: 22 [26112/50176]	Loss: 0.9933
Training Epoch: 22 [26624/50176]	Loss: 1.0070
Training Epoch: 22 [27136/50176]	Loss: 0.9825
Training Epoch: 22 [27648/50176]	Loss: 1.0031
Training Epoch: 22 [28160/50176]	Loss: 1.0192
Training Epoch: 22 [28672/50176]	Loss: 0.9553
Training Epoch: 22 [29184/50176]	Loss: 0.9499
Training Epoch: 22 [29696/50176]	Loss: 1.0091
Training Epoch: 22 [30208/50176]	Loss: 1.0900
Training Epoch: 22 [30720/50176]	Loss: 1.0239
Training Epoch: 22 [31232/50176]	Loss: 0.9654
Training Epoch: 22 [31744/50176]	Loss: 1.1472
Training Epoch: 22 [32256/50176]	Loss: 1.0138
Training Epoch: 22 [32768/50176]	Loss: 1.0312
Training Epoch: 22 [33280/50176]	Loss: 1.0765
Training Epoch: 22 [33792/50176]	Loss: 0.9813
Training Epoch: 22 [34304/50176]	Loss: 0.9577
Training Epoch: 22 [34816/50176]	Loss: 0.9945
Training Epoch: 22 [35328/50176]	Loss: 1.1051
Training Epoch: 22 [35840/50176]	Loss: 0.9547
Training Epoch: 22 [36352/50176]	Loss: 1.0397
Training Epoch: 22 [36864/50176]	Loss: 1.0545
Training Epoch: 22 [37376/50176]	Loss: 0.9161
Training Epoch: 22 [37888/50176]	Loss: 1.1614
Training Epoch: 22 [38400/50176]	Loss: 0.8949
Training Epoch: 22 [38912/50176]	Loss: 1.1026
Training Epoch: 22 [39424/50176]	Loss: 1.0199
Training Epoch: 22 [39936/50176]	Loss: 1.0884
Training Epoch: 22 [40448/50176]	Loss: 1.0309
Training Epoch: 22 [40960/50176]	Loss: 0.9729
Training Epoch: 22 [41472/50176]	Loss: 1.0497
Training Epoch: 22 [41984/50176]	Loss: 1.1370
Training Epoch: 22 [42496/50176]	Loss: 1.0164
Training Epoch: 22 [43008/50176]	Loss: 0.9473
Training Epoch: 22 [43520/50176]	Loss: 1.0252
Training Epoch: 22 [44032/50176]	Loss: 1.0086
Training Epoch: 22 [44544/50176]	Loss: 0.9360
Training Epoch: 22 [45056/50176]	Loss: 0.8777
Training Epoch: 22 [45568/50176]	Loss: 0.9868
Training Epoch: 22 [46080/50176]	Loss: 0.9339
Training Epoch: 22 [46592/50176]	Loss: 0.9765
Training Epoch: 22 [47104/50176]	Loss: 0.9096
Training Epoch: 22 [47616/50176]	Loss: 0.9961
Training Epoch: 22 [48128/50176]	Loss: 1.0264
Training Epoch: 22 [48640/50176]	Loss: 0.9437
Training Epoch: 22 [49152/50176]	Loss: 1.1013
Training Epoch: 22 [49664/50176]	Loss: 1.0096
Training Epoch: 22 [50176/50176]	Loss: 0.8644
Validation Epoch: 22, Average loss: 0.0038, Accuracy: 0.5247
Training Epoch: 23 [512/50176]	Loss: 0.8447
Training Epoch: 23 [1024/50176]	Loss: 0.8326
Training Epoch: 23 [1536/50176]	Loss: 0.8880
Training Epoch: 23 [2048/50176]	Loss: 0.9001
Training Epoch: 23 [2560/50176]	Loss: 0.9116
Training Epoch: 23 [3072/50176]	Loss: 0.9581
Training Epoch: 23 [3584/50176]	Loss: 0.8787
Training Epoch: 23 [4096/50176]	Loss: 0.8250
Training Epoch: 23 [4608/50176]	Loss: 0.8999
Training Epoch: 23 [5120/50176]	Loss: 0.8111
Training Epoch: 23 [5632/50176]	Loss: 0.9616
Training Epoch: 23 [6144/50176]	Loss: 0.8693
Training Epoch: 23 [6656/50176]	Loss: 0.8485
Training Epoch: 23 [7168/50176]	Loss: 0.8110
Training Epoch: 23 [7680/50176]	Loss: 0.7732
Training Epoch: 23 [8192/50176]	Loss: 0.8055
Training Epoch: 23 [8704/50176]	Loss: 0.9223
Training Epoch: 23 [9216/50176]	Loss: 0.8314
Training Epoch: 23 [9728/50176]	Loss: 0.9090
Training Epoch: 23 [10240/50176]	Loss: 0.8960
Training Epoch: 23 [10752/50176]	Loss: 0.8643
Training Epoch: 23 [11264/50176]	Loss: 0.8611
Training Epoch: 23 [11776/50176]	Loss: 0.9228
Training Epoch: 23 [12288/50176]	Loss: 0.8139
Training Epoch: 23 [12800/50176]	Loss: 0.9119
Training Epoch: 23 [13312/50176]	Loss: 0.9635
Training Epoch: 23 [13824/50176]	Loss: 0.8977
Training Epoch: 23 [14336/50176]	Loss: 0.8519
Training Epoch: 23 [14848/50176]	Loss: 0.9054
Training Epoch: 23 [15360/50176]	Loss: 0.8025
Training Epoch: 23 [15872/50176]	Loss: 0.8303
Training Epoch: 23 [16384/50176]	Loss: 1.0011
Training Epoch: 23 [16896/50176]	Loss: 0.9936
Training Epoch: 23 [17408/50176]	Loss: 0.9778
Training Epoch: 23 [17920/50176]	Loss: 0.9383
Training Epoch: 23 [18432/50176]	Loss: 0.8939
Training Epoch: 23 [18944/50176]	Loss: 0.9488
Training Epoch: 23 [19456/50176]	Loss: 0.9200
Training Epoch: 23 [19968/50176]	Loss: 0.9169
Training Epoch: 23 [20480/50176]	Loss: 0.9552
Training Epoch: 23 [20992/50176]	Loss: 0.9834
Training Epoch: 23 [21504/50176]	Loss: 0.9183
Training Epoch: 23 [22016/50176]	Loss: 0.8886
Training Epoch: 23 [22528/50176]	Loss: 0.9347
Training Epoch: 23 [23040/50176]	Loss: 0.8898
Training Epoch: 23 [23552/50176]	Loss: 0.8775
Training Epoch: 23 [24064/50176]	Loss: 0.8476
Training Epoch: 23 [24576/50176]	Loss: 0.9634
Training Epoch: 23 [25088/50176]	Loss: 0.9912
Training Epoch: 23 [25600/50176]	Loss: 0.8779
Training Epoch: 23 [26112/50176]	Loss: 1.0045
Training Epoch: 23 [26624/50176]	Loss: 0.8074
Training Epoch: 23 [27136/50176]	Loss: 0.8942
Training Epoch: 23 [27648/50176]	Loss: 0.9310
Training Epoch: 23 [28160/50176]	Loss: 0.9655
Training Epoch: 23 [28672/50176]	Loss: 0.9399
Training Epoch: 23 [29184/50176]	Loss: 0.9560
Training Epoch: 23 [29696/50176]	Loss: 0.9603
Training Epoch: 23 [30208/50176]	Loss: 0.9849
Training Epoch: 23 [30720/50176]	Loss: 0.9911
Training Epoch: 23 [31232/50176]	Loss: 0.9790
Training Epoch: 23 [31744/50176]	Loss: 0.9150
Training Epoch: 23 [32256/50176]	Loss: 1.0194
Training Epoch: 23 [32768/50176]	Loss: 0.9171
Training Epoch: 23 [33280/50176]	Loss: 0.9566
Training Epoch: 23 [33792/50176]	Loss: 1.0610
Training Epoch: 23 [34304/50176]	Loss: 0.9166
Training Epoch: 23 [34816/50176]	Loss: 1.0646
Training Epoch: 23 [35328/50176]	Loss: 0.9984
Training Epoch: 23 [35840/50176]	Loss: 0.9808
Training Epoch: 23 [36352/50176]	Loss: 0.9552
Training Epoch: 23 [36864/50176]	Loss: 0.9541
Training Epoch: 23 [37376/50176]	Loss: 0.9936
Training Epoch: 23 [37888/50176]	Loss: 1.0620
Training Epoch: 23 [38400/50176]	Loss: 0.9172
Training Epoch: 23 [38912/50176]	Loss: 0.8899
Training Epoch: 23 [39424/50176]	Loss: 0.9872
Training Epoch: 23 [39936/50176]	Loss: 0.9937
Training Epoch: 23 [40448/50176]	Loss: 0.9675
Training Epoch: 23 [40960/50176]	Loss: 1.0101
Training Epoch: 23 [41472/50176]	Loss: 1.1126
Training Epoch: 23 [41984/50176]	Loss: 0.9987
Training Epoch: 23 [42496/50176]	Loss: 0.7901
Training Epoch: 23 [43008/50176]	Loss: 1.0153
Training Epoch: 23 [43520/50176]	Loss: 0.9051
Training Epoch: 23 [44032/50176]	Loss: 0.9036
Training Epoch: 23 [44544/50176]	Loss: 0.9885
Training Epoch: 23 [45056/50176]	Loss: 0.9533
Training Epoch: 23 [45568/50176]	Loss: 0.9494
Training Epoch: 23 [46080/50176]	Loss: 1.0884
Training Epoch: 23 [46592/50176]	Loss: 0.9147
Training Epoch: 23 [47104/50176]	Loss: 0.9949
Training Epoch: 23 [47616/50176]	Loss: 1.0066
Training Epoch: 23 [48128/50176]	Loss: 1.0789
Training Epoch: 23 [48640/50176]	Loss: 1.0578
Training Epoch: 23 [49152/50176]	Loss: 1.0040
Training Epoch: 23 [49664/50176]	Loss: 0.9238
Training Epoch: 23 [50176/50176]	Loss: 1.0334
Validation Epoch: 23, Average loss: 0.0045, Accuracy: 0.4885
Training Epoch: 24 [512/50176]	Loss: 0.8730
Training Epoch: 24 [1024/50176]	Loss: 0.8747
Training Epoch: 24 [1536/50176]	Loss: 0.8853
Training Epoch: 24 [2048/50176]	Loss: 0.8834
Training Epoch: 24 [2560/50176]	Loss: 0.8745
Training Epoch: 24 [3072/50176]	Loss: 0.8581
Training Epoch: 24 [3584/50176]	Loss: 0.8653
Training Epoch: 24 [4096/50176]	Loss: 0.8558
Training Epoch: 24 [4608/50176]	Loss: 0.8382
Training Epoch: 24 [5120/50176]	Loss: 0.8528
Training Epoch: 24 [5632/50176]	Loss: 0.8176
Training Epoch: 24 [6144/50176]	Loss: 0.8898
Training Epoch: 24 [6656/50176]	Loss: 0.9001
Training Epoch: 24 [7168/50176]	Loss: 0.8727
Training Epoch: 24 [7680/50176]	Loss: 0.8396
Training Epoch: 24 [8192/50176]	Loss: 0.8453
Training Epoch: 24 [8704/50176]	Loss: 0.8561
Training Epoch: 24 [9216/50176]	Loss: 0.8030
Training Epoch: 24 [9728/50176]	Loss: 0.8657
Training Epoch: 24 [10240/50176]	Loss: 0.8093
Training Epoch: 24 [10752/50176]	Loss: 0.8200
Training Epoch: 24 [11264/50176]	Loss: 0.8531
Training Epoch: 24 [11776/50176]	Loss: 0.8397
Training Epoch: 24 [12288/50176]	Loss: 0.8373
Training Epoch: 24 [12800/50176]	Loss: 0.8803
Training Epoch: 24 [13312/50176]	Loss: 0.9419
Training Epoch: 24 [13824/50176]	Loss: 0.8335
Training Epoch: 24 [14336/50176]	Loss: 0.8996
Training Epoch: 24 [14848/50176]	Loss: 0.7860
Training Epoch: 24 [15360/50176]	Loss: 0.8698
Training Epoch: 24 [15872/50176]	Loss: 0.8518
Training Epoch: 24 [16384/50176]	Loss: 0.8740
Training Epoch: 24 [16896/50176]	Loss: 0.9409
Training Epoch: 24 [17408/50176]	Loss: 0.8239
Training Epoch: 24 [17920/50176]	Loss: 0.8585
Training Epoch: 24 [18432/50176]	Loss: 0.8623
Training Epoch: 24 [18944/50176]	Loss: 0.8317
Training Epoch: 24 [19456/50176]	Loss: 0.7903
Training Epoch: 24 [19968/50176]	Loss: 0.8399
Training Epoch: 24 [20480/50176]	Loss: 0.8651
Training Epoch: 24 [20992/50176]	Loss: 0.9025
Training Epoch: 24 [21504/50176]	Loss: 0.8171
Training Epoch: 24 [22016/50176]	Loss: 0.8599
Training Epoch: 24 [22528/50176]	Loss: 0.9131
Training Epoch: 24 [23040/50176]	Loss: 0.9131
Training Epoch: 24 [23552/50176]	Loss: 0.7903
Training Epoch: 24 [24064/50176]	Loss: 0.9325
Training Epoch: 24 [24576/50176]	Loss: 0.8849
Training Epoch: 24 [25088/50176]	Loss: 1.0353
Training Epoch: 24 [25600/50176]	Loss: 0.7817
Training Epoch: 24 [26112/50176]	Loss: 0.9131
Training Epoch: 24 [26624/50176]	Loss: 0.8409
Training Epoch: 24 [27136/50176]	Loss: 0.8162
Training Epoch: 24 [27648/50176]	Loss: 0.8431
Training Epoch: 24 [28160/50176]	Loss: 0.8329
Training Epoch: 24 [28672/50176]	Loss: 0.9010
Training Epoch: 24 [29184/50176]	Loss: 0.9221
Training Epoch: 24 [29696/50176]	Loss: 0.9347
Training Epoch: 24 [30208/50176]	Loss: 1.0110
Training Epoch: 24 [30720/50176]	Loss: 0.9149
Training Epoch: 24 [31232/50176]	Loss: 0.9698
Training Epoch: 24 [31744/50176]	Loss: 0.8109
Training Epoch: 24 [32256/50176]	Loss: 0.8377
Training Epoch: 24 [32768/50176]	Loss: 0.9762
Training Epoch: 24 [33280/50176]	Loss: 0.8175
Training Epoch: 24 [33792/50176]	Loss: 0.8645
Training Epoch: 24 [34304/50176]	Loss: 1.0204
Training Epoch: 24 [34816/50176]	Loss: 0.9469
Training Epoch: 24 [35328/50176]	Loss: 0.8723
Training Epoch: 24 [35840/50176]	Loss: 0.8836
Training Epoch: 24 [36352/50176]	Loss: 0.9254
Training Epoch: 24 [36864/50176]	Loss: 0.8594
Training Epoch: 24 [37376/50176]	Loss: 0.7995
Training Epoch: 24 [37888/50176]	Loss: 0.9878
Training Epoch: 24 [38400/50176]	Loss: 1.0149
Training Epoch: 24 [38912/50176]	Loss: 0.9229
Training Epoch: 24 [39424/50176]	Loss: 0.9511
Training Epoch: 24 [39936/50176]	Loss: 1.0503
Training Epoch: 24 [40448/50176]	Loss: 0.8694
Training Epoch: 24 [40960/50176]	Loss: 0.9888
Training Epoch: 24 [41472/50176]	Loss: 0.9073
Training Epoch: 24 [41984/50176]	Loss: 0.8607
Training Epoch: 24 [42496/50176]	Loss: 0.8731
Training Epoch: 24 [43008/50176]	Loss: 0.9410
Training Epoch: 24 [43520/50176]	Loss: 0.8844
Training Epoch: 24 [44032/50176]	Loss: 0.8345
Training Epoch: 24 [44544/50176]	Loss: 0.9803
Training Epoch: 24 [45056/50176]	Loss: 0.9196
Training Epoch: 24 [45568/50176]	Loss: 0.8831
Training Epoch: 24 [46080/50176]	Loss: 0.9199
Training Epoch: 24 [46592/50176]	Loss: 0.9417
Training Epoch: 24 [47104/50176]	Loss: 0.8982
Training Epoch: 24 [47616/50176]	Loss: 0.8633
Training Epoch: 24 [48128/50176]	Loss: 1.0370
Training Epoch: 24 [48640/50176]	Loss: 0.9702
Training Epoch: 24 [49152/50176]	Loss: 1.0245
Training Epoch: 24 [49664/50176]	Loss: 0.9201
Training Epoch: 24 [50176/50176]	Loss: 1.0397
Validation Epoch: 24, Average loss: 0.0036, Accuracy: 0.5308
Training Epoch: 25 [512/50176]	Loss: 0.8005
Training Epoch: 25 [1024/50176]	Loss: 0.7832
Training Epoch: 25 [1536/50176]	Loss: 0.8094
Training Epoch: 25 [2048/50176]	Loss: 0.7866
Training Epoch: 25 [2560/50176]	Loss: 0.8236
Training Epoch: 25 [3072/50176]	Loss: 0.7570
Training Epoch: 25 [3584/50176]	Loss: 0.7985
Training Epoch: 25 [4096/50176]	Loss: 0.7850
Training Epoch: 25 [4608/50176]	Loss: 0.8428
Training Epoch: 25 [5120/50176]	Loss: 0.8100
Training Epoch: 25 [5632/50176]	Loss: 0.8075
Training Epoch: 25 [6144/50176]	Loss: 0.7555
Training Epoch: 25 [6656/50176]	Loss: 0.8136
Training Epoch: 25 [7168/50176]	Loss: 0.7974
Training Epoch: 25 [7680/50176]	Loss: 0.8873
Training Epoch: 25 [8192/50176]	Loss: 0.7814
Training Epoch: 25 [8704/50176]	Loss: 0.8036
Training Epoch: 25 [9216/50176]	Loss: 0.9812
Training Epoch: 25 [9728/50176]	Loss: 0.7791
Training Epoch: 25 [10240/50176]	Loss: 0.7479
Training Epoch: 25 [10752/50176]	Loss: 0.8099
Training Epoch: 25 [11264/50176]	Loss: 0.9125
Training Epoch: 25 [11776/50176]	Loss: 0.8832
Training Epoch: 25 [12288/50176]	Loss: 0.9069
Training Epoch: 25 [12800/50176]	Loss: 0.9061
Training Epoch: 25 [13312/50176]	Loss: 0.8635
Training Epoch: 25 [13824/50176]	Loss: 0.7236
Training Epoch: 25 [14336/50176]	Loss: 0.9516
Training Epoch: 25 [14848/50176]	Loss: 0.8050
Training Epoch: 25 [15360/50176]	Loss: 0.8716
Training Epoch: 25 [15872/50176]	Loss: 0.8215
Training Epoch: 25 [16384/50176]	Loss: 0.8082
Training Epoch: 25 [16896/50176]	Loss: 0.7610
Training Epoch: 25 [17408/50176]	Loss: 0.7460
Training Epoch: 25 [17920/50176]	Loss: 0.8092
Training Epoch: 25 [18432/50176]	Loss: 0.9081
Training Epoch: 25 [18944/50176]	Loss: 0.9191
Training Epoch: 25 [19456/50176]	Loss: 0.6744
Training Epoch: 25 [19968/50176]	Loss: 0.8501
Training Epoch: 25 [20480/50176]	Loss: 0.8484
Training Epoch: 25 [20992/50176]	Loss: 0.8755
Training Epoch: 25 [21504/50176]	Loss: 0.7476
Training Epoch: 25 [22016/50176]	Loss: 0.8750
Training Epoch: 25 [22528/50176]	Loss: 0.8198
Training Epoch: 25 [23040/50176]	Loss: 0.9505
Training Epoch: 25 [23552/50176]	Loss: 0.8972
Training Epoch: 25 [24064/50176]	Loss: 0.8417
Training Epoch: 25 [24576/50176]	Loss: 0.9583
Training Epoch: 25 [25088/50176]	Loss: 0.8480
Training Epoch: 25 [25600/50176]	Loss: 0.8582
Training Epoch: 25 [26112/50176]	Loss: 0.8711
Training Epoch: 25 [26624/50176]	Loss: 0.9650
Training Epoch: 25 [27136/50176]	Loss: 0.8561
Training Epoch: 25 [27648/50176]	Loss: 0.8149
Training Epoch: 25 [28160/50176]	Loss: 0.8111
Training Epoch: 25 [28672/50176]	Loss: 0.8817
Training Epoch: 25 [29184/50176]	Loss: 0.9036
Training Epoch: 25 [29696/50176]	Loss: 0.7734
Training Epoch: 25 [30208/50176]	Loss: 0.8926
Training Epoch: 25 [30720/50176]	Loss: 0.7568
Training Epoch: 25 [31232/50176]	Loss: 0.7817
Training Epoch: 25 [31744/50176]	Loss: 0.8768
Training Epoch: 25 [32256/50176]	Loss: 0.9730
Training Epoch: 25 [32768/50176]	Loss: 0.9190
Training Epoch: 25 [33280/50176]	Loss: 1.0157
Training Epoch: 25 [33792/50176]	Loss: 0.9316
Training Epoch: 25 [34304/50176]	Loss: 0.8880
Training Epoch: 25 [34816/50176]	Loss: 0.9001
Training Epoch: 25 [35328/50176]	Loss: 0.8716
Training Epoch: 25 [35840/50176]	Loss: 0.9096
Training Epoch: 25 [36352/50176]	Loss: 0.9154
Training Epoch: 25 [36864/50176]	Loss: 0.9204
Training Epoch: 25 [37376/50176]	Loss: 0.9571
Training Epoch: 25 [37888/50176]	Loss: 0.8957
Training Epoch: 25 [38400/50176]	Loss: 0.8867
Training Epoch: 25 [38912/50176]	Loss: 0.7820
Training Epoch: 25 [39424/50176]	Loss: 0.9987
Training Epoch: 25 [39936/50176]	Loss: 0.8446
Training Epoch: 25 [40448/50176]	Loss: 0.8898
Training Epoch: 25 [40960/50176]	Loss: 0.8878
Training Epoch: 25 [41472/50176]	Loss: 0.8537
Training Epoch: 25 [41984/50176]	Loss: 0.9556
Training Epoch: 25 [42496/50176]	Loss: 0.8671
Training Epoch: 25 [43008/50176]	Loss: 0.9257
Training Epoch: 25 [43520/50176]	Loss: 0.8985
Training Epoch: 25 [44032/50176]	Loss: 0.8992
Training Epoch: 25 [44544/50176]	Loss: 0.8975
Training Epoch: 25 [45056/50176]	Loss: 0.8760
Training Epoch: 25 [45568/50176]	Loss: 0.8861
Training Epoch: 25 [46080/50176]	Loss: 0.8237
Training Epoch: 25 [46592/50176]	Loss: 0.8886
Training Epoch: 25 [47104/50176]	Loss: 0.8668
Training Epoch: 25 [47616/50176]	Loss: 0.9980
Training Epoch: 25 [48128/50176]	Loss: 0.8460
Training Epoch: 25 [48640/50176]	Loss: 0.8297
Training Epoch: 25 [49152/50176]	Loss: 0.8966
Training Epoch: 25 [49664/50176]	Loss: 0.9399
Training Epoch: 25 [50176/50176]	Loss: 0.9407
Validation Epoch: 25, Average loss: 0.0040, Accuracy: 0.5232
Training Epoch: 26 [512/50176]	Loss: 0.6888
Training Epoch: 26 [1024/50176]	Loss: 0.7540
Training Epoch: 26 [1536/50176]	Loss: 0.7740
Training Epoch: 26 [2048/50176]	Loss: 0.7750
Training Epoch: 26 [2560/50176]	Loss: 0.7397
Training Epoch: 26 [3072/50176]	Loss: 0.7107
Training Epoch: 26 [3584/50176]	Loss: 0.6377
Training Epoch: 26 [4096/50176]	Loss: 0.8124
Training Epoch: 26 [4608/50176]	Loss: 0.7216
Training Epoch: 26 [5120/50176]	Loss: 0.7465
Training Epoch: 26 [5632/50176]	Loss: 0.7890
Training Epoch: 26 [6144/50176]	Loss: 0.7678
Training Epoch: 26 [6656/50176]	Loss: 0.7386
Training Epoch: 26 [7168/50176]	Loss: 0.8283
Training Epoch: 26 [7680/50176]	Loss: 0.7729
Training Epoch: 26 [8192/50176]	Loss: 0.6948
Training Epoch: 26 [8704/50176]	Loss: 0.6754
Training Epoch: 26 [9216/50176]	Loss: 0.7226
Training Epoch: 26 [9728/50176]	Loss: 0.7119
Training Epoch: 26 [10240/50176]	Loss: 0.7624
Training Epoch: 26 [10752/50176]	Loss: 0.7828
Training Epoch: 26 [11264/50176]	Loss: 0.7790
Training Epoch: 26 [11776/50176]	Loss: 0.7811
Training Epoch: 26 [12288/50176]	Loss: 0.7925
Training Epoch: 26 [12800/50176]	Loss: 0.7611
Training Epoch: 26 [13312/50176]	Loss: 0.7664
Training Epoch: 26 [13824/50176]	Loss: 0.7320
Training Epoch: 26 [14336/50176]	Loss: 0.7829
Training Epoch: 26 [14848/50176]	Loss: 0.7273
Training Epoch: 26 [15360/50176]	Loss: 0.8476
Training Epoch: 26 [15872/50176]	Loss: 0.7956
Training Epoch: 26 [16384/50176]	Loss: 0.8350
Training Epoch: 26 [16896/50176]	Loss: 0.6961
Training Epoch: 26 [17408/50176]	Loss: 0.6365
Training Epoch: 26 [17920/50176]	Loss: 0.7340
Training Epoch: 26 [18432/50176]	Loss: 0.7905
Training Epoch: 26 [18944/50176]	Loss: 0.7519
Training Epoch: 26 [19456/50176]	Loss: 0.8206
Training Epoch: 26 [19968/50176]	Loss: 0.7461
Training Epoch: 26 [20480/50176]	Loss: 0.8556
Training Epoch: 26 [20992/50176]	Loss: 0.8392
Training Epoch: 26 [21504/50176]	Loss: 0.8576
Training Epoch: 26 [22016/50176]	Loss: 0.7887
Training Epoch: 26 [22528/50176]	Loss: 0.7664
Training Epoch: 26 [23040/50176]	Loss: 0.7618
Training Epoch: 26 [23552/50176]	Loss: 0.8428
Training Epoch: 26 [24064/50176]	Loss: 0.9037
Training Epoch: 26 [24576/50176]	Loss: 0.6921
Training Epoch: 26 [25088/50176]	Loss: 0.7159
Training Epoch: 26 [25600/50176]	Loss: 0.9400
Training Epoch: 26 [26112/50176]	Loss: 0.9212
Training Epoch: 26 [26624/50176]	Loss: 0.7583
Training Epoch: 26 [27136/50176]	Loss: 0.8603
Training Epoch: 26 [27648/50176]	Loss: 0.7307
Training Epoch: 26 [28160/50176]	Loss: 0.7591
Training Epoch: 26 [28672/50176]	Loss: 0.7637
Training Epoch: 26 [29184/50176]	Loss: 0.7319
Training Epoch: 26 [29696/50176]	Loss: 0.7055
Training Epoch: 26 [30208/50176]	Loss: 0.9205
Training Epoch: 26 [30720/50176]	Loss: 0.8442
Training Epoch: 26 [31232/50176]	Loss: 0.9432
Training Epoch: 26 [31744/50176]	Loss: 0.8555
Training Epoch: 26 [32256/50176]	Loss: 0.8703
Training Epoch: 26 [32768/50176]	Loss: 0.8607
Training Epoch: 26 [33280/50176]	Loss: 0.8381
Training Epoch: 26 [33792/50176]	Loss: 0.7552
Training Epoch: 26 [34304/50176]	Loss: 0.8480
Training Epoch: 26 [34816/50176]	Loss: 0.8063
Training Epoch: 26 [35328/50176]	Loss: 0.7961
Training Epoch: 26 [35840/50176]	Loss: 0.8472
Training Epoch: 26 [36352/50176]	Loss: 0.8172
Training Epoch: 26 [36864/50176]	Loss: 0.8420
Training Epoch: 26 [37376/50176]	Loss: 0.9662
Training Epoch: 26 [37888/50176]	Loss: 0.8027
Training Epoch: 26 [38400/50176]	Loss: 0.8026
Training Epoch: 26 [38912/50176]	Loss: 0.7919
Training Epoch: 26 [39424/50176]	Loss: 0.8545
Training Epoch: 26 [39936/50176]	Loss: 0.8024
Training Epoch: 26 [40448/50176]	Loss: 0.9122
Training Epoch: 26 [40960/50176]	Loss: 0.8892
Training Epoch: 26 [41472/50176]	Loss: 0.9684
Training Epoch: 26 [41984/50176]	Loss: 0.8311
Training Epoch: 26 [42496/50176]	Loss: 0.8818
Training Epoch: 26 [43008/50176]	Loss: 0.8410
Training Epoch: 26 [43520/50176]	Loss: 0.8297
Training Epoch: 26 [44032/50176]	Loss: 0.9164
Training Epoch: 26 [44544/50176]	Loss: 0.8433
Training Epoch: 26 [45056/50176]	Loss: 0.9297
Training Epoch: 26 [45568/50176]	Loss: 0.8228
Training Epoch: 26 [46080/50176]	Loss: 0.7996
Training Epoch: 26 [46592/50176]	Loss: 0.9521
Training Epoch: 26 [47104/50176]	Loss: 0.9483
Training Epoch: 26 [47616/50176]	Loss: 0.8820
Training Epoch: 26 [48128/50176]	Loss: 0.8796
Training Epoch: 26 [48640/50176]	Loss: 0.8012
Training Epoch: 26 [49152/50176]	Loss: 0.8601
Training Epoch: 26 [49664/50176]	Loss: 0.8768
Training Epoch: 26 [50176/50176]	Loss: 0.8764
Validation Epoch: 26, Average loss: 0.0037, Accuracy: 0.5443
Training Epoch: 27 [512/50176]	Loss: 0.7474
Training Epoch: 27 [1024/50176]	Loss: 0.7866
Training Epoch: 27 [1536/50176]	Loss: 0.7372
Training Epoch: 27 [2048/50176]	Loss: 0.5659
Training Epoch: 27 [2560/50176]	Loss: 0.6762
Training Epoch: 27 [3072/50176]	Loss: 0.7107
Training Epoch: 27 [3584/50176]	Loss: 0.6691
Training Epoch: 27 [4096/50176]	Loss: 0.7157
Training Epoch: 27 [4608/50176]	Loss: 0.6758
Training Epoch: 27 [5120/50176]	Loss: 0.7235
Training Epoch: 27 [5632/50176]	Loss: 0.7684
Training Epoch: 27 [6144/50176]	Loss: 0.7928
Training Epoch: 27 [6656/50176]	Loss: 0.7940
Training Epoch: 27 [7168/50176]	Loss: 0.6684
Training Epoch: 27 [7680/50176]	Loss: 0.7224
Training Epoch: 27 [8192/50176]	Loss: 0.6804
Training Epoch: 27 [8704/50176]	Loss: 0.7165
Training Epoch: 27 [9216/50176]	Loss: 0.7284
Training Epoch: 27 [9728/50176]	Loss: 0.7504
Training Epoch: 27 [10240/50176]	Loss: 0.7273
Training Epoch: 27 [10752/50176]	Loss: 0.5797
Training Epoch: 27 [11264/50176]	Loss: 0.7325
Training Epoch: 27 [11776/50176]	Loss: 0.7180
Training Epoch: 27 [12288/50176]	Loss: 0.7758
Training Epoch: 27 [12800/50176]	Loss: 0.8033
Training Epoch: 27 [13312/50176]	Loss: 0.6711
Training Epoch: 27 [13824/50176]	Loss: 0.7081
Training Epoch: 27 [14336/50176]	Loss: 0.8097
Training Epoch: 27 [14848/50176]	Loss: 0.7084
Training Epoch: 27 [15360/50176]	Loss: 0.7785
Training Epoch: 27 [15872/50176]	Loss: 0.8177
Training Epoch: 27 [16384/50176]	Loss: 0.8286
Training Epoch: 27 [16896/50176]	Loss: 0.7443
Training Epoch: 27 [17408/50176]	Loss: 0.7320
Training Epoch: 27 [17920/50176]	Loss: 0.7539
Training Epoch: 27 [18432/50176]	Loss: 0.7068
Training Epoch: 27 [18944/50176]	Loss: 0.7490
Training Epoch: 27 [19456/50176]	Loss: 0.7633
Training Epoch: 27 [19968/50176]	Loss: 0.6932
Training Epoch: 27 [20480/50176]	Loss: 0.7339
Training Epoch: 27 [20992/50176]	Loss: 0.7258
Training Epoch: 27 [21504/50176]	Loss: 0.7718
Training Epoch: 27 [22016/50176]	Loss: 0.7118
Training Epoch: 27 [22528/50176]	Loss: 0.7874
Training Epoch: 27 [23040/50176]	Loss: 0.7849
Training Epoch: 27 [23552/50176]	Loss: 0.7144
Training Epoch: 27 [24064/50176]	Loss: 0.7520
Training Epoch: 27 [24576/50176]	Loss: 0.7950
Training Epoch: 27 [25088/50176]	Loss: 0.9082
Training Epoch: 27 [25600/50176]	Loss: 0.8028
Training Epoch: 27 [26112/50176]	Loss: 0.8352
Training Epoch: 27 [26624/50176]	Loss: 0.7466
Training Epoch: 27 [27136/50176]	Loss: 0.7865
Training Epoch: 27 [27648/50176]	Loss: 0.7982
Training Epoch: 27 [28160/50176]	Loss: 0.8584
Training Epoch: 27 [28672/50176]	Loss: 0.7388
Training Epoch: 27 [29184/50176]	Loss: 0.8084
Training Epoch: 27 [29696/50176]	Loss: 0.7380
Training Epoch: 27 [30208/50176]	Loss: 0.8029
Training Epoch: 27 [30720/50176]	Loss: 0.8009
Training Epoch: 27 [31232/50176]	Loss: 0.7478
Training Epoch: 27 [31744/50176]	Loss: 0.8209
Training Epoch: 27 [32256/50176]	Loss: 0.7953
Training Epoch: 27 [32768/50176]	Loss: 0.8091
Training Epoch: 27 [33280/50176]	Loss: 0.8122
Training Epoch: 27 [33792/50176]	Loss: 0.7696
Training Epoch: 27 [34304/50176]	Loss: 0.6825
Training Epoch: 27 [34816/50176]	Loss: 0.8802
Training Epoch: 27 [35328/50176]	Loss: 0.7516
Training Epoch: 27 [35840/50176]	Loss: 0.7816
Training Epoch: 27 [36352/50176]	Loss: 0.8857
Training Epoch: 27 [36864/50176]	Loss: 0.8116
Training Epoch: 27 [37376/50176]	Loss: 0.8278
Training Epoch: 27 [37888/50176]	Loss: 0.7816
Training Epoch: 27 [38400/50176]	Loss: 0.8824
Training Epoch: 27 [38912/50176]	Loss: 0.7775
Training Epoch: 27 [39424/50176]	Loss: 0.7913
Training Epoch: 27 [39936/50176]	Loss: 0.7580
Training Epoch: 27 [40448/50176]	Loss: 0.8124
Training Epoch: 27 [40960/50176]	Loss: 0.8241
Training Epoch: 27 [41472/50176]	Loss: 0.7702
Training Epoch: 27 [41984/50176]	Loss: 0.8122
Training Epoch: 27 [42496/50176]	Loss: 0.8008
Training Epoch: 27 [43008/50176]	Loss: 0.7831
Training Epoch: 27 [43520/50176]	Loss: 0.8804
Training Epoch: 27 [44032/50176]	Loss: 0.8275
Training Epoch: 27 [44544/50176]	Loss: 0.8272
Training Epoch: 27 [45056/50176]	Loss: 0.8006
Training Epoch: 27 [45568/50176]	Loss: 0.8652
Training Epoch: 27 [46080/50176]	Loss: 0.7150
Training Epoch: 27 [46592/50176]	Loss: 0.7790
Training Epoch: 27 [47104/50176]	Loss: 0.7027
Training Epoch: 27 [47616/50176]	Loss: 0.8288
Training Epoch: 27 [48128/50176]	Loss: 0.7974
Training Epoch: 27 [48640/50176]	Loss: 0.7689
Training Epoch: 27 [49152/50176]	Loss: 0.8237
Training Epoch: 27 [49664/50176]	Loss: 0.8725
Training Epoch: 27 [50176/50176]	Loss: 0.7410
Validation Epoch: 27, Average loss: 0.0039, Accuracy: 0.5447
Training Epoch: 28 [512/50176]	Loss: 0.6435
Training Epoch: 28 [1024/50176]	Loss: 0.6310
Training Epoch: 28 [1536/50176]	Loss: 0.7310
Training Epoch: 28 [2048/50176]	Loss: 0.5626
Training Epoch: 28 [2560/50176]	Loss: 0.6508
Training Epoch: 28 [3072/50176]	Loss: 0.6791
Training Epoch: 28 [3584/50176]	Loss: 0.6595
Training Epoch: 28 [4096/50176]	Loss: 0.6671
Training Epoch: 28 [4608/50176]	Loss: 0.5336
Training Epoch: 28 [5120/50176]	Loss: 0.6632
Training Epoch: 28 [5632/50176]	Loss: 0.7248
Training Epoch: 28 [6144/50176]	Loss: 0.6009
Training Epoch: 28 [6656/50176]	Loss: 0.6224
Training Epoch: 28 [7168/50176]	Loss: 0.6236
Training Epoch: 28 [7680/50176]	Loss: 0.6347
Training Epoch: 28 [8192/50176]	Loss: 0.7000
Training Epoch: 28 [8704/50176]	Loss: 0.7099
Training Epoch: 28 [9216/50176]	Loss: 0.6530
Training Epoch: 28 [9728/50176]	Loss: 0.7515
Training Epoch: 28 [10240/50176]	Loss: 0.6356
Training Epoch: 28 [10752/50176]	Loss: 0.6637
Training Epoch: 28 [11264/50176]	Loss: 0.6040
Training Epoch: 28 [11776/50176]	Loss: 0.7090
Training Epoch: 28 [12288/50176]	Loss: 0.6065
Training Epoch: 28 [12800/50176]	Loss: 0.6798
Training Epoch: 28 [13312/50176]	Loss: 0.6854
Training Epoch: 28 [13824/50176]	Loss: 0.6673
Training Epoch: 28 [14336/50176]	Loss: 0.6820
Training Epoch: 28 [14848/50176]	Loss: 0.6665
Training Epoch: 28 [15360/50176]	Loss: 0.6592
Training Epoch: 28 [15872/50176]	Loss: 0.6642
Training Epoch: 28 [16384/50176]	Loss: 0.7074
Training Epoch: 28 [16896/50176]	Loss: 0.7136
Training Epoch: 28 [17408/50176]	Loss: 0.6258
Training Epoch: 28 [17920/50176]	Loss: 0.7418
Training Epoch: 28 [18432/50176]	Loss: 0.7368
Training Epoch: 28 [18944/50176]	Loss: 0.6776
Training Epoch: 28 [19456/50176]	Loss: 0.6547
Training Epoch: 28 [19968/50176]	Loss: 0.7342
Training Epoch: 28 [20480/50176]	Loss: 0.6858
Training Epoch: 28 [20992/50176]	Loss: 0.8041
Training Epoch: 28 [21504/50176]	Loss: 0.6605
Training Epoch: 28 [22016/50176]	Loss: 0.6472
Training Epoch: 28 [22528/50176]	Loss: 0.8010
Training Epoch: 28 [23040/50176]	Loss: 0.8359
Training Epoch: 28 [23552/50176]	Loss: 0.8298
Training Epoch: 28 [24064/50176]	Loss: 0.6313
Training Epoch: 28 [24576/50176]	Loss: 0.7480
Training Epoch: 28 [25088/50176]	Loss: 0.7125
Training Epoch: 28 [25600/50176]	Loss: 0.7034
Training Epoch: 28 [26112/50176]	Loss: 0.7461
Training Epoch: 28 [26624/50176]	Loss: 0.7123
Training Epoch: 28 [27136/50176]	Loss: 0.8308
Training Epoch: 28 [27648/50176]	Loss: 0.7954
Training Epoch: 28 [28160/50176]	Loss: 0.7636
Training Epoch: 28 [28672/50176]	Loss: 0.8089
Training Epoch: 28 [29184/50176]	Loss: 0.7617
Training Epoch: 28 [29696/50176]	Loss: 0.7135
Training Epoch: 28 [30208/50176]	Loss: 0.7294
Training Epoch: 28 [30720/50176]	Loss: 0.7284
Training Epoch: 28 [31232/50176]	Loss: 0.7484
Training Epoch: 28 [31744/50176]	Loss: 0.7157
Training Epoch: 28 [32256/50176]	Loss: 0.6655
Training Epoch: 28 [32768/50176]	Loss: 0.7753
Training Epoch: 28 [33280/50176]	Loss: 0.7811
Training Epoch: 28 [33792/50176]	Loss: 0.8253
Training Epoch: 28 [34304/50176]	Loss: 0.8406
Training Epoch: 28 [34816/50176]	Loss: 0.7344
Training Epoch: 28 [35328/50176]	Loss: 0.7434
Training Epoch: 28 [35840/50176]	Loss: 0.7770
Training Epoch: 28 [36352/50176]	Loss: 0.7626
Training Epoch: 28 [36864/50176]	Loss: 0.7482
Training Epoch: 28 [37376/50176]	Loss: 0.7350
Training Epoch: 28 [37888/50176]	Loss: 0.7665
Training Epoch: 28 [38400/50176]	Loss: 0.7158
Training Epoch: 28 [38912/50176]	Loss: 0.8076
Training Epoch: 28 [39424/50176]	Loss: 0.7520
Training Epoch: 28 [39936/50176]	Loss: 0.7738
Training Epoch: 28 [40448/50176]	Loss: 0.7916
Training Epoch: 28 [40960/50176]	Loss: 0.7782
Training Epoch: 28 [41472/50176]	Loss: 0.7893
Training Epoch: 28 [41984/50176]	Loss: 0.8840
Training Epoch: 28 [42496/50176]	Loss: 0.8446
Training Epoch: 28 [43008/50176]	Loss: 0.8056
Training Epoch: 28 [43520/50176]	Loss: 0.8322
Training Epoch: 28 [44032/50176]	Loss: 0.7914
Training Epoch: 28 [44544/50176]	Loss: 0.8125
Training Epoch: 28 [45056/50176]	Loss: 0.8215
Training Epoch: 28 [45568/50176]	Loss: 0.7572
Training Epoch: 28 [46080/50176]	Loss: 0.7910
Training Epoch: 28 [46592/50176]	Loss: 0.7481
Training Epoch: 28 [47104/50176]	Loss: 0.7021
Training Epoch: 28 [47616/50176]	Loss: 0.7735
Training Epoch: 28 [48128/50176]	Loss: 0.6790
Training Epoch: 28 [48640/50176]	Loss: 0.8103
Training Epoch: 28 [49152/50176]	Loss: 0.9312
Training Epoch: 28 [49664/50176]	Loss: 0.8648
Training Epoch: 28 [50176/50176]	Loss: 0.7054
Validation Epoch: 28, Average loss: 0.0037, Accuracy: 0.5430
Training Epoch: 29 [512/50176]	Loss: 0.6146
Training Epoch: 29 [1024/50176]	Loss: 0.6806
Training Epoch: 29 [1536/50176]	Loss: 0.5821
Training Epoch: 29 [2048/50176]	Loss: 0.6444
Training Epoch: 29 [2560/50176]	Loss: 0.6419
Training Epoch: 29 [3072/50176]	Loss: 0.5874
Training Epoch: 29 [3584/50176]	Loss: 0.6402
Training Epoch: 29 [4096/50176]	Loss: 0.6172
Training Epoch: 29 [4608/50176]	Loss: 0.6537
Training Epoch: 29 [5120/50176]	Loss: 0.6824
Training Epoch: 29 [5632/50176]	Loss: 0.6549
Training Epoch: 29 [6144/50176]	Loss: 0.6440
Training Epoch: 29 [6656/50176]	Loss: 0.6778
Training Epoch: 29 [7168/50176]	Loss: 0.6580
Training Epoch: 29 [7680/50176]	Loss: 0.6824
Training Epoch: 29 [8192/50176]	Loss: 0.6181
Training Epoch: 29 [8704/50176]	Loss: 0.6881
Training Epoch: 29 [9216/50176]	Loss: 0.6620
Training Epoch: 29 [9728/50176]	Loss: 0.5981
Training Epoch: 29 [10240/50176]	Loss: 0.6484
Training Epoch: 29 [10752/50176]	Loss: 0.6288
Training Epoch: 29 [11264/50176]	Loss: 0.6637
Training Epoch: 29 [11776/50176]	Loss: 0.6914
Training Epoch: 29 [12288/50176]	Loss: 0.6197
Training Epoch: 29 [12800/50176]	Loss: 0.6705
Training Epoch: 29 [13312/50176]	Loss: 0.6002
Training Epoch: 29 [13824/50176]	Loss: 0.7089
Training Epoch: 29 [14336/50176]	Loss: 0.7036
Training Epoch: 29 [14848/50176]	Loss: 0.7099
Training Epoch: 29 [15360/50176]	Loss: 0.6846
Training Epoch: 29 [15872/50176]	Loss: 0.7036
Training Epoch: 29 [16384/50176]	Loss: 0.6918
Training Epoch: 29 [16896/50176]	Loss: 0.6549
Training Epoch: 29 [17408/50176]	Loss: 0.6299
Training Epoch: 29 [17920/50176]	Loss: 0.5916
Training Epoch: 29 [18432/50176]	Loss: 0.6786
Training Epoch: 29 [18944/50176]	Loss: 0.7428
Training Epoch: 29 [19456/50176]	Loss: 0.6408
Training Epoch: 29 [19968/50176]	Loss: 0.6907
Training Epoch: 29 [20480/50176]	Loss: 0.6142
Training Epoch: 29 [20992/50176]	Loss: 0.6377
Training Epoch: 29 [21504/50176]	Loss: 0.7119
Training Epoch: 29 [22016/50176]	Loss: 0.7106
Training Epoch: 29 [22528/50176]	Loss: 0.6724
Training Epoch: 29 [23040/50176]	Loss: 0.7383
Training Epoch: 29 [23552/50176]	Loss: 0.6717
Training Epoch: 29 [24064/50176]	Loss: 0.7517
Training Epoch: 29 [24576/50176]	Loss: 0.6100
Training Epoch: 29 [25088/50176]	Loss: 0.7284
Training Epoch: 29 [25600/50176]	Loss: 0.7689
Training Epoch: 29 [26112/50176]	Loss: 0.6487
Training Epoch: 29 [26624/50176]	Loss: 0.7508
Training Epoch: 29 [27136/50176]	Loss: 0.7396
Training Epoch: 29 [27648/50176]	Loss: 0.6503
Training Epoch: 29 [28160/50176]	Loss: 0.7153
Training Epoch: 29 [28672/50176]	Loss: 0.6881
Training Epoch: 29 [29184/50176]	Loss: 0.7071
Training Epoch: 29 [29696/50176]	Loss: 0.7676
Training Epoch: 29 [30208/50176]	Loss: 0.6861
Training Epoch: 29 [30720/50176]	Loss: 0.6681
Training Epoch: 29 [31232/50176]	Loss: 0.7695
Training Epoch: 29 [31744/50176]	Loss: 0.6853
Training Epoch: 29 [32256/50176]	Loss: 0.7422
Training Epoch: 29 [32768/50176]	Loss: 0.7077
Training Epoch: 29 [33280/50176]	Loss: 0.7186
Training Epoch: 29 [33792/50176]	Loss: 0.7471
Training Epoch: 29 [34304/50176]	Loss: 0.7103
Training Epoch: 29 [34816/50176]	Loss: 0.7507
Training Epoch: 29 [35328/50176]	Loss: 0.8234
Training Epoch: 29 [35840/50176]	Loss: 0.7374
Training Epoch: 29 [36352/50176]	Loss: 0.6686
Training Epoch: 29 [36864/50176]	Loss: 0.6216
Training Epoch: 29 [37376/50176]	Loss: 0.7609
Training Epoch: 29 [37888/50176]	Loss: 0.7571
Training Epoch: 29 [38400/50176]	Loss: 0.7351
Training Epoch: 29 [38912/50176]	Loss: 0.7609
Training Epoch: 29 [39424/50176]	Loss: 0.7434
Training Epoch: 29 [39936/50176]	Loss: 0.7159
Training Epoch: 29 [40448/50176]	Loss: 0.7109
Training Epoch: 29 [40960/50176]	Loss: 0.7418
Training Epoch: 29 [41472/50176]	Loss: 0.6893
Training Epoch: 29 [41984/50176]	Loss: 0.7328
Training Epoch: 29 [42496/50176]	Loss: 0.6600
Training Epoch: 29 [43008/50176]	Loss: 0.7324
Training Epoch: 29 [43520/50176]	Loss: 0.7796
Training Epoch: 29 [44032/50176]	Loss: 0.7544
Training Epoch: 29 [44544/50176]	Loss: 0.7124
Training Epoch: 29 [45056/50176]	Loss: 0.8332
Training Epoch: 29 [45568/50176]	Loss: 0.6743
Training Epoch: 29 [46080/50176]	Loss: 0.7820
Training Epoch: 29 [46592/50176]	Loss: 0.7114
Training Epoch: 29 [47104/50176]	Loss: 0.6975
Training Epoch: 29 [47616/50176]	Loss: 0.7568
Training Epoch: 29 [48128/50176]	Loss: 0.7525
Training Epoch: 29 [48640/50176]	Loss: 0.7791
Training Epoch: 29 [49152/50176]	Loss: 0.7222
Training Epoch: 29 [49664/50176]	Loss: 0.6974
Training Epoch: 29 [50176/50176]	Loss: 0.7535
Validation Epoch: 29, Average loss: 0.0041, Accuracy: 0.5371
Training Epoch: 30 [512/50176]	Loss: 0.6664
Training Epoch: 30 [1024/50176]	Loss: 0.6021
Training Epoch: 30 [1536/50176]	Loss: 0.5530
Training Epoch: 30 [2048/50176]	Loss: 0.5753
Training Epoch: 30 [2560/50176]	Loss: 0.5893
Training Epoch: 30 [3072/50176]	Loss: 0.6299
Training Epoch: 30 [3584/50176]	Loss: 0.6031
Training Epoch: 30 [4096/50176]	Loss: 0.6334
Training Epoch: 30 [4608/50176]	Loss: 0.5210
Training Epoch: 30 [5120/50176]	Loss: 0.6014
Training Epoch: 30 [5632/50176]	Loss: 0.6703
Training Epoch: 30 [6144/50176]	Loss: 0.6249
Training Epoch: 30 [6656/50176]	Loss: 0.6269
Training Epoch: 30 [7168/50176]	Loss: 0.5397
Training Epoch: 30 [7680/50176]	Loss: 0.5203
Training Epoch: 30 [8192/50176]	Loss: 0.6616
Training Epoch: 30 [8704/50176]	Loss: 0.6077
Training Epoch: 30 [9216/50176]	Loss: 0.6508
Training Epoch: 30 [9728/50176]	Loss: 0.6516
Training Epoch: 30 [10240/50176]	Loss: 0.6132
Training Epoch: 30 [10752/50176]	Loss: 0.5787
Training Epoch: 30 [11264/50176]	Loss: 0.6698
Training Epoch: 30 [11776/50176]	Loss: 0.5498
Training Epoch: 30 [12288/50176]	Loss: 0.6672
Training Epoch: 30 [12800/50176]	Loss: 0.6403
Training Epoch: 30 [13312/50176]	Loss: 0.6857
Training Epoch: 30 [13824/50176]	Loss: 0.5469
Training Epoch: 30 [14336/50176]	Loss: 0.6770
Training Epoch: 30 [14848/50176]	Loss: 0.5838
Training Epoch: 30 [15360/50176]	Loss: 0.6302
Training Epoch: 30 [15872/50176]	Loss: 0.6342
Training Epoch: 30 [16384/50176]	Loss: 0.5494
Training Epoch: 30 [16896/50176]	Loss: 0.6003
Training Epoch: 30 [17408/50176]	Loss: 0.6576
Training Epoch: 30 [17920/50176]	Loss: 0.6466
Training Epoch: 30 [18432/50176]	Loss: 0.5270
Training Epoch: 30 [18944/50176]	Loss: 0.6983
Training Epoch: 30 [19456/50176]	Loss: 0.6447
Training Epoch: 30 [19968/50176]	Loss: 0.6336
Training Epoch: 30 [20480/50176]	Loss: 0.5677
Training Epoch: 30 [20992/50176]	Loss: 0.5782
Training Epoch: 30 [21504/50176]	Loss: 0.6275
Training Epoch: 30 [22016/50176]	Loss: 0.6921
Training Epoch: 30 [22528/50176]	Loss: 0.5670
Training Epoch: 30 [23040/50176]	Loss: 0.7050
Training Epoch: 30 [23552/50176]	Loss: 0.5932
Training Epoch: 30 [24064/50176]	Loss: 0.6942
Training Epoch: 30 [24576/50176]	Loss: 0.7084
Training Epoch: 30 [25088/50176]	Loss: 0.6115
Training Epoch: 30 [25600/50176]	Loss: 0.6250
Training Epoch: 30 [26112/50176]	Loss: 0.6778
Training Epoch: 30 [26624/50176]	Loss: 0.7050
Training Epoch: 30 [27136/50176]	Loss: 0.6916
Training Epoch: 30 [27648/50176]	Loss: 0.6365
Training Epoch: 30 [28160/50176]	Loss: 0.7389
Training Epoch: 30 [28672/50176]	Loss: 0.6216
Training Epoch: 30 [29184/50176]	Loss: 0.6455
Training Epoch: 30 [29696/50176]	Loss: 0.6445
Training Epoch: 30 [30208/50176]	Loss: 0.6535
Training Epoch: 30 [30720/50176]	Loss: 0.6966
Training Epoch: 30 [31232/50176]	Loss: 0.7251
Training Epoch: 30 [31744/50176]	Loss: 0.6789
Training Epoch: 30 [32256/50176]	Loss: 0.6343
Training Epoch: 30 [32768/50176]	Loss: 0.6805
Training Epoch: 30 [33280/50176]	Loss: 0.7068
Training Epoch: 30 [33792/50176]	Loss: 0.6813
Training Epoch: 30 [34304/50176]	Loss: 0.6246
Training Epoch: 30 [34816/50176]	Loss: 0.6900
Training Epoch: 30 [35328/50176]	Loss: 0.6683
Training Epoch: 30 [35840/50176]	Loss: 0.6238
Training Epoch: 30 [36352/50176]	Loss: 0.6226
Training Epoch: 30 [36864/50176]	Loss: 0.6741
Training Epoch: 30 [37376/50176]	Loss: 0.6910
Training Epoch: 30 [37888/50176]	Loss: 0.6895
Training Epoch: 30 [38400/50176]	Loss: 0.6628
Training Epoch: 30 [38912/50176]	Loss: 0.6882
Training Epoch: 30 [39424/50176]	Loss: 0.6422
Training Epoch: 30 [39936/50176]	Loss: 0.7311
Training Epoch: 30 [40448/50176]	Loss: 0.6333
Training Epoch: 30 [40960/50176]	Loss: 0.7092
Training Epoch: 30 [41472/50176]	Loss: 0.6461
Training Epoch: 30 [41984/50176]	Loss: 0.6988
Training Epoch: 30 [42496/50176]	Loss: 0.6457
Training Epoch: 30 [43008/50176]	Loss: 0.7561
Training Epoch: 30 [43520/50176]	Loss: 0.7057
Training Epoch: 30 [44032/50176]	Loss: 0.7754
Training Epoch: 30 [44544/50176]	Loss: 0.5964
Training Epoch: 30 [45056/50176]	Loss: 0.6802
Training Epoch: 30 [45568/50176]	Loss: 0.7224
Training Epoch: 30 [46080/50176]	Loss: 0.7294
Training Epoch: 30 [46592/50176]	Loss: 0.6431
Training Epoch: 30 [47104/50176]	Loss: 0.6574
Training Epoch: 30 [47616/50176]	Loss: 0.7027
Training Epoch: 30 [48128/50176]	Loss: 0.6704
Training Epoch: 30 [48640/50176]	Loss: 0.7739
Training Epoch: 30 [49152/50176]	Loss: 0.6692
Training Epoch: 30 [49664/50176]	Loss: 0.7354
Training Epoch: 30 [50176/50176]	Loss: 0.6251
Validation Epoch: 30, Average loss: 0.0039, Accuracy: 0.5508
Training Epoch: 31 [512/50176]	Loss: 0.5671
Training Epoch: 31 [1024/50176]	Loss: 0.5527
Training Epoch: 31 [1536/50176]	Loss: 0.6600
Training Epoch: 31 [2048/50176]	Loss: 0.5825
Training Epoch: 31 [2560/50176]	Loss: 0.5357
Training Epoch: 31 [3072/50176]	Loss: 0.6125
Training Epoch: 31 [3584/50176]	Loss: 0.6462
Training Epoch: 31 [4096/50176]	Loss: 0.5582
Training Epoch: 31 [4608/50176]	Loss: 0.5977
Training Epoch: 31 [5120/50176]	Loss: 0.5829
Training Epoch: 31 [5632/50176]	Loss: 0.5109
Training Epoch: 31 [6144/50176]	Loss: 0.6178
Training Epoch: 31 [6656/50176]	Loss: 0.5859
Training Epoch: 31 [7168/50176]	Loss: 0.5506
Training Epoch: 31 [7680/50176]	Loss: 0.6435
Training Epoch: 31 [8192/50176]	Loss: 0.6163
Training Epoch: 31 [8704/50176]	Loss: 0.5792
Training Epoch: 31 [9216/50176]	Loss: 0.5476
Training Epoch: 31 [9728/50176]	Loss: 0.6301
Training Epoch: 31 [10240/50176]	Loss: 0.5971
Training Epoch: 31 [10752/50176]	Loss: 0.5356
Training Epoch: 31 [11264/50176]	Loss: 0.5957
Training Epoch: 31 [11776/50176]	Loss: 0.4868
Training Epoch: 31 [12288/50176]	Loss: 0.5348
Training Epoch: 31 [12800/50176]	Loss: 0.6810
Training Epoch: 31 [13312/50176]	Loss: 0.5412
Training Epoch: 31 [13824/50176]	Loss: 0.5566
Training Epoch: 31 [14336/50176]	Loss: 0.5746
Training Epoch: 31 [14848/50176]	Loss: 0.6441
Training Epoch: 31 [15360/50176]	Loss: 0.6242
Training Epoch: 31 [15872/50176]	Loss: 0.5318
Training Epoch: 31 [16384/50176]	Loss: 0.6291
Training Epoch: 31 [16896/50176]	Loss: 0.5272
Training Epoch: 31 [17408/50176]	Loss: 0.5848
Training Epoch: 31 [17920/50176]	Loss: 0.6497
Training Epoch: 31 [18432/50176]	Loss: 0.6931
Training Epoch: 31 [18944/50176]	Loss: 0.5629
Training Epoch: 31 [19456/50176]	Loss: 0.6433
Training Epoch: 31 [19968/50176]	Loss: 0.5455
Training Epoch: 31 [20480/50176]	Loss: 0.6677
Training Epoch: 31 [20992/50176]	Loss: 0.6663
Training Epoch: 31 [21504/50176]	Loss: 0.5747
Training Epoch: 31 [22016/50176]	Loss: 0.6508
Training Epoch: 31 [22528/50176]	Loss: 0.6279
Training Epoch: 31 [23040/50176]	Loss: 0.5201
Training Epoch: 31 [23552/50176]	Loss: 0.6333
Training Epoch: 31 [24064/50176]	Loss: 0.6675
Training Epoch: 31 [24576/50176]	Loss: 0.6035
Training Epoch: 31 [25088/50176]	Loss: 0.5997
Training Epoch: 31 [25600/50176]	Loss: 0.6755
Training Epoch: 31 [26112/50176]	Loss: 0.5929
Training Epoch: 31 [26624/50176]	Loss: 0.6998
Training Epoch: 31 [27136/50176]	Loss: 0.6250
Training Epoch: 31 [27648/50176]	Loss: 0.5680
Training Epoch: 31 [28160/50176]	Loss: 0.7008
Training Epoch: 31 [28672/50176]	Loss: 0.5971
Training Epoch: 31 [29184/50176]	Loss: 0.6068
Training Epoch: 31 [29696/50176]	Loss: 0.6063
Training Epoch: 31 [30208/50176]	Loss: 0.6602
Training Epoch: 31 [30720/50176]	Loss: 0.6981
Training Epoch: 31 [31232/50176]	Loss: 0.6952
Training Epoch: 31 [31744/50176]	Loss: 0.7338
Training Epoch: 31 [32256/50176]	Loss: 0.6412
Training Epoch: 31 [32768/50176]	Loss: 0.6707
Training Epoch: 31 [33280/50176]	Loss: 0.7298
Training Epoch: 31 [33792/50176]	Loss: 0.7168
Training Epoch: 31 [34304/50176]	Loss: 0.7084
Training Epoch: 31 [34816/50176]	Loss: 0.6675
Training Epoch: 31 [35328/50176]	Loss: 0.6187
Training Epoch: 31 [35840/50176]	Loss: 0.5827
Training Epoch: 31 [36352/50176]	Loss: 0.6834
Training Epoch: 31 [36864/50176]	Loss: 0.6783
Training Epoch: 31 [37376/50176]	Loss: 0.6564
Training Epoch: 31 [37888/50176]	Loss: 0.6896
Training Epoch: 31 [38400/50176]	Loss: 0.6704
Training Epoch: 31 [38912/50176]	Loss: 0.6749
Training Epoch: 31 [39424/50176]	Loss: 0.6254
Training Epoch: 31 [39936/50176]	Loss: 0.7354
Training Epoch: 31 [40448/50176]	Loss: 0.6408
Training Epoch: 31 [40960/50176]	Loss: 0.5870
Training Epoch: 31 [41472/50176]	Loss: 0.6682
Training Epoch: 31 [41984/50176]	Loss: 0.6399
Training Epoch: 31 [42496/50176]	Loss: 0.7108
Training Epoch: 31 [43008/50176]	Loss: 0.6009
Training Epoch: 31 [43520/50176]	Loss: 0.6911
Training Epoch: 31 [44032/50176]	Loss: 0.6532
Training Epoch: 31 [44544/50176]	Loss: 0.7308
Training Epoch: 31 [45056/50176]	Loss: 0.5899
Training Epoch: 31 [45568/50176]	Loss: 0.6806
Training Epoch: 31 [46080/50176]	Loss: 0.6359
Training Epoch: 31 [46592/50176]	Loss: 0.5994
Training Epoch: 31 [47104/50176]	Loss: 0.5565
Training Epoch: 31 [47616/50176]	Loss: 0.6513
Training Epoch: 31 [48128/50176]	Loss: 0.7781
Training Epoch: 31 [48640/50176]	Loss: 0.6455
Training Epoch: 31 [49152/50176]	Loss: 0.6917
Training Epoch: 31 [49664/50176]	Loss: 0.7577
Training Epoch: 31 [50176/50176]	Loss: 0.6055
Validation Epoch: 31, Average loss: 0.0040, Accuracy: 0.5530
Training Epoch: 32 [512/50176]	Loss: 0.5004
Training Epoch: 32 [1024/50176]	Loss: 0.6035
Training Epoch: 32 [1536/50176]	Loss: 0.5432
Training Epoch: 32 [2048/50176]	Loss: 0.5480
Training Epoch: 32 [2560/50176]	Loss: 0.5826
Training Epoch: 32 [3072/50176]	Loss: 0.6206
Training Epoch: 32 [3584/50176]	Loss: 0.4915
Training Epoch: 32 [4096/50176]	Loss: 0.4745
Training Epoch: 32 [4608/50176]	Loss: 0.5606
Training Epoch: 32 [5120/50176]	Loss: 0.5471
Training Epoch: 32 [5632/50176]	Loss: 0.6035
Training Epoch: 32 [6144/50176]	Loss: 0.5222
Training Epoch: 32 [6656/50176]	Loss: 0.4952
Training Epoch: 32 [7168/50176]	Loss: 0.5133
Training Epoch: 32 [7680/50176]	Loss: 0.4928
Training Epoch: 32 [8192/50176]	Loss: 0.5819
Training Epoch: 32 [8704/50176]	Loss: 0.5770
Training Epoch: 32 [9216/50176]	Loss: 0.5928
Training Epoch: 32 [9728/50176]	Loss: 0.5202
Training Epoch: 32 [10240/50176]	Loss: 0.5132
Training Epoch: 32 [10752/50176]	Loss: 0.5681
Training Epoch: 32 [11264/50176]	Loss: 0.5817
Training Epoch: 32 [11776/50176]	Loss: 0.5536
Training Epoch: 32 [12288/50176]	Loss: 0.5511
Training Epoch: 32 [12800/50176]	Loss: 0.5020
Training Epoch: 32 [13312/50176]	Loss: 0.5123
Training Epoch: 32 [13824/50176]	Loss: 0.6422
Training Epoch: 32 [14336/50176]	Loss: 0.4840
Training Epoch: 32 [14848/50176]	Loss: 0.5294
Training Epoch: 32 [15360/50176]	Loss: 0.5044
Training Epoch: 32 [15872/50176]	Loss: 0.5713
Training Epoch: 32 [16384/50176]	Loss: 0.4952
Training Epoch: 32 [16896/50176]	Loss: 0.5906
Training Epoch: 32 [17408/50176]	Loss: 0.5304
Training Epoch: 32 [17920/50176]	Loss: 0.5396
Training Epoch: 32 [18432/50176]	Loss: 0.5434
Training Epoch: 32 [18944/50176]	Loss: 0.5721
Training Epoch: 32 [19456/50176]	Loss: 0.5146
Training Epoch: 32 [19968/50176]	Loss: 0.6039
Training Epoch: 32 [20480/50176]	Loss: 0.5725
Training Epoch: 32 [20992/50176]	Loss: 0.5592
Training Epoch: 32 [21504/50176]	Loss: 0.5548
Training Epoch: 32 [22016/50176]	Loss: 0.6339
Training Epoch: 32 [22528/50176]	Loss: 0.5691
Training Epoch: 32 [23040/50176]	Loss: 0.5447
Training Epoch: 32 [23552/50176]	Loss: 0.5010
Training Epoch: 32 [24064/50176]	Loss: 0.5148
Training Epoch: 32 [24576/50176]	Loss: 0.5746
Training Epoch: 32 [25088/50176]	Loss: 0.6364
Training Epoch: 32 [25600/50176]	Loss: 0.5524
Training Epoch: 32 [26112/50176]	Loss: 0.6301
Training Epoch: 32 [26624/50176]	Loss: 0.6232
Training Epoch: 32 [27136/50176]	Loss: 0.7330
Training Epoch: 32 [27648/50176]	Loss: 0.6469
Training Epoch: 32 [28160/50176]	Loss: 0.5401
Training Epoch: 32 [28672/50176]	Loss: 0.6713
Training Epoch: 32 [29184/50176]	Loss: 0.6285
Training Epoch: 32 [29696/50176]	Loss: 0.6177
Training Epoch: 32 [30208/50176]	Loss: 0.5008
Training Epoch: 32 [30720/50176]	Loss: 0.6257
Training Epoch: 32 [31232/50176]	Loss: 0.5824
Training Epoch: 32 [31744/50176]	Loss: 0.5458
Training Epoch: 32 [32256/50176]	Loss: 0.6520
Training Epoch: 32 [32768/50176]	Loss: 0.6393
Training Epoch: 32 [33280/50176]	Loss: 0.5743
Training Epoch: 32 [33792/50176]	Loss: 0.5965
Training Epoch: 32 [34304/50176]	Loss: 0.5757
Training Epoch: 32 [34816/50176]	Loss: 0.5711
Training Epoch: 32 [35328/50176]	Loss: 0.6258
Training Epoch: 32 [35840/50176]	Loss: 0.5536
Training Epoch: 32 [36352/50176]	Loss: 0.5719
Training Epoch: 32 [36864/50176]	Loss: 0.6391
Training Epoch: 32 [37376/50176]	Loss: 0.5915
Training Epoch: 32 [37888/50176]	Loss: 0.5858
Training Epoch: 32 [38400/50176]	Loss: 0.6097
Training Epoch: 32 [38912/50176]	Loss: 0.7049
Training Epoch: 32 [39424/50176]	Loss: 0.6388
Training Epoch: 32 [39936/50176]	Loss: 0.6043
Training Epoch: 32 [40448/50176]	Loss: 0.6626
Training Epoch: 32 [40960/50176]	Loss: 0.6519
Training Epoch: 32 [41472/50176]	Loss: 0.5946
Training Epoch: 32 [41984/50176]	Loss: 0.7388
Training Epoch: 32 [42496/50176]	Loss: 0.6769
Training Epoch: 32 [43008/50176]	Loss: 0.6490
Training Epoch: 32 [43520/50176]	Loss: 0.6328
Training Epoch: 32 [44032/50176]	Loss: 0.5476
Training Epoch: 32 [44544/50176]	Loss: 0.6276
Training Epoch: 32 [45056/50176]	Loss: 0.6569
Training Epoch: 32 [45568/50176]	Loss: 0.5525
Training Epoch: 32 [46080/50176]	Loss: 0.6467
Training Epoch: 32 [46592/50176]	Loss: 0.5831
Training Epoch: 32 [47104/50176]	Loss: 0.6208
Training Epoch: 32 [47616/50176]	Loss: 0.5719
Training Epoch: 32 [48128/50176]	Loss: 0.5793
Training Epoch: 32 [48640/50176]	Loss: 0.7066
Training Epoch: 32 [49152/50176]	Loss: 0.5974
Training Epoch: 32 [49664/50176]	Loss: 0.7067
Training Epoch: 32 [50176/50176]	Loss: 0.5502
Validation Epoch: 32, Average loss: 0.0035, Accuracy: 0.5763
Training Epoch: 33 [512/50176]	Loss: 0.5014
Training Epoch: 33 [1024/50176]	Loss: 0.5163
Training Epoch: 33 [1536/50176]	Loss: 0.5157
Training Epoch: 33 [2048/50176]	Loss: 0.5002
Training Epoch: 33 [2560/50176]	Loss: 0.5414
Training Epoch: 33 [3072/50176]	Loss: 0.5862
Training Epoch: 33 [3584/50176]	Loss: 0.4919
Training Epoch: 33 [4096/50176]	Loss: 0.4352
Training Epoch: 33 [4608/50176]	Loss: 0.5716
Training Epoch: 33 [5120/50176]	Loss: 0.5106
Training Epoch: 33 [5632/50176]	Loss: 0.5269
Training Epoch: 33 [6144/50176]	Loss: 0.5113
Training Epoch: 33 [6656/50176]	Loss: 0.4230
Training Epoch: 33 [7168/50176]	Loss: 0.4837
Training Epoch: 33 [7680/50176]	Loss: 0.5759
Training Epoch: 33 [8192/50176]	Loss: 0.5159
Training Epoch: 33 [8704/50176]	Loss: 0.4756
Training Epoch: 33 [9216/50176]	Loss: 0.5337
Training Epoch: 33 [9728/50176]	Loss: 0.4998
Training Epoch: 33 [10240/50176]	Loss: 0.6206
Training Epoch: 33 [10752/50176]	Loss: 0.5525
Training Epoch: 33 [11264/50176]	Loss: 0.3875
Training Epoch: 33 [11776/50176]	Loss: 0.5110
Training Epoch: 33 [12288/50176]	Loss: 0.5655
Training Epoch: 33 [12800/50176]	Loss: 0.5110
Training Epoch: 33 [13312/50176]	Loss: 0.5454
Training Epoch: 33 [13824/50176]	Loss: 0.4452
Training Epoch: 33 [14336/50176]	Loss: 0.5487
Training Epoch: 33 [14848/50176]	Loss: 0.6703
Training Epoch: 33 [15360/50176]	Loss: 0.5451
Training Epoch: 33 [15872/50176]	Loss: 0.5751
Training Epoch: 33 [16384/50176]	Loss: 0.5896
Training Epoch: 33 [16896/50176]	Loss: 0.5211
Training Epoch: 33 [17408/50176]	Loss: 0.5170
Training Epoch: 33 [17920/50176]	Loss: 0.5162
Training Epoch: 33 [18432/50176]	Loss: 0.5584
Training Epoch: 33 [18944/50176]	Loss: 0.5715
Training Epoch: 33 [19456/50176]	Loss: 0.5736
Training Epoch: 33 [19968/50176]	Loss: 0.5611
Training Epoch: 33 [20480/50176]	Loss: 0.6032
Training Epoch: 33 [20992/50176]	Loss: 0.4938
Training Epoch: 33 [21504/50176]	Loss: 0.5156
Training Epoch: 33 [22016/50176]	Loss: 0.5707
Training Epoch: 33 [22528/50176]	Loss: 0.5590
Training Epoch: 33 [23040/50176]	Loss: 0.5420
Training Epoch: 33 [23552/50176]	Loss: 0.6067
Training Epoch: 33 [24064/50176]	Loss: 0.5946
Training Epoch: 33 [24576/50176]	Loss: 0.6333
Training Epoch: 33 [25088/50176]	Loss: 0.5985
Training Epoch: 33 [25600/50176]	Loss: 0.5435
Training Epoch: 33 [26112/50176]	Loss: 0.4634
Training Epoch: 33 [26624/50176]	Loss: 0.4783
Training Epoch: 33 [27136/50176]	Loss: 0.5362
Training Epoch: 33 [27648/50176]	Loss: 0.5815
Training Epoch: 33 [28160/50176]	Loss: 0.5506
Training Epoch: 33 [28672/50176]	Loss: 0.5887
Training Epoch: 33 [29184/50176]	Loss: 0.4902
Training Epoch: 33 [29696/50176]	Loss: 0.4935
Training Epoch: 33 [30208/50176]	Loss: 0.5446
Training Epoch: 33 [30720/50176]	Loss: 0.6088
Training Epoch: 33 [31232/50176]	Loss: 0.5610
Training Epoch: 33 [31744/50176]	Loss: 0.5671
Training Epoch: 33 [32256/50176]	Loss: 0.5624
Training Epoch: 33 [32768/50176]	Loss: 0.4830
Training Epoch: 33 [33280/50176]	Loss: 0.6184
Training Epoch: 33 [33792/50176]	Loss: 0.5736
Training Epoch: 33 [34304/50176]	Loss: 0.5739
Training Epoch: 33 [34816/50176]	Loss: 0.6574
Training Epoch: 33 [35328/50176]	Loss: 0.6185
Training Epoch: 33 [35840/50176]	Loss: 0.5683
Training Epoch: 33 [36352/50176]	Loss: 0.5616
Training Epoch: 33 [36864/50176]	Loss: 0.5603
Training Epoch: 33 [37376/50176]	Loss: 0.6428
Training Epoch: 33 [37888/50176]	Loss: 0.5715
Training Epoch: 33 [38400/50176]	Loss: 0.6580
Training Epoch: 33 [38912/50176]	Loss: 0.6276
Training Epoch: 33 [39424/50176]	Loss: 0.5867
Training Epoch: 33 [39936/50176]	Loss: 0.6795
Training Epoch: 33 [40448/50176]	Loss: 0.6231
Training Epoch: 33 [40960/50176]	Loss: 0.6546
Training Epoch: 33 [41472/50176]	Loss: 0.6153
Training Epoch: 33 [41984/50176]	Loss: 0.5708
Training Epoch: 33 [42496/50176]	Loss: 0.6772
Training Epoch: 33 [43008/50176]	Loss: 0.5917
Training Epoch: 33 [43520/50176]	Loss: 0.5571
Training Epoch: 33 [44032/50176]	Loss: 0.6644
Training Epoch: 33 [44544/50176]	Loss: 0.7013
Training Epoch: 33 [45056/50176]	Loss: 0.6165
Training Epoch: 33 [45568/50176]	Loss: 0.5289
Training Epoch: 33 [46080/50176]	Loss: 0.6105
Training Epoch: 33 [46592/50176]	Loss: 0.5794
Training Epoch: 33 [47104/50176]	Loss: 0.5253
Training Epoch: 33 [47616/50176]	Loss: 0.6941
Training Epoch: 33 [48128/50176]	Loss: 0.6282
Training Epoch: 33 [48640/50176]	Loss: 0.7013
Training Epoch: 33 [49152/50176]	Loss: 0.5752
Training Epoch: 33 [49664/50176]	Loss: 0.6173
Training Epoch: 33 [50176/50176]	Loss: 0.6039
Validation Epoch: 33, Average loss: 0.0038, Accuracy: 0.5749
Training Epoch: 34 [512/50176]	Loss: 0.4690
Training Epoch: 34 [1024/50176]	Loss: 0.4728
Training Epoch: 34 [1536/50176]	Loss: 0.4065
Training Epoch: 34 [2048/50176]	Loss: 0.4949
Training Epoch: 34 [2560/50176]	Loss: 0.5693
Training Epoch: 34 [3072/50176]	Loss: 0.5740
Training Epoch: 34 [3584/50176]	Loss: 0.5195
Training Epoch: 34 [4096/50176]	Loss: 0.4865
Training Epoch: 34 [4608/50176]	Loss: 0.5230
Training Epoch: 34 [5120/50176]	Loss: 0.4859
Training Epoch: 34 [5632/50176]	Loss: 0.4355
Training Epoch: 34 [6144/50176]	Loss: 0.4949
Training Epoch: 34 [6656/50176]	Loss: 0.4748
Training Epoch: 34 [7168/50176]	Loss: 0.5119
Training Epoch: 34 [7680/50176]	Loss: 0.6263
Training Epoch: 34 [8192/50176]	Loss: 0.3854
Training Epoch: 34 [8704/50176]	Loss: 0.4670
Training Epoch: 34 [9216/50176]	Loss: 0.5774
Training Epoch: 34 [9728/50176]	Loss: 0.5238
Training Epoch: 34 [10240/50176]	Loss: 0.4744
Training Epoch: 34 [10752/50176]	Loss: 0.4912
Training Epoch: 34 [11264/50176]	Loss: 0.4723
Training Epoch: 34 [11776/50176]	Loss: 0.5214
Training Epoch: 34 [12288/50176]	Loss: 0.5012
Training Epoch: 34 [12800/50176]	Loss: 0.4082
Training Epoch: 34 [13312/50176]	Loss: 0.4677
Training Epoch: 34 [13824/50176]	Loss: 0.5732
Training Epoch: 34 [14336/50176]	Loss: 0.5567
Training Epoch: 34 [14848/50176]	Loss: 0.4267
Training Epoch: 34 [15360/50176]	Loss: 0.4750
Training Epoch: 34 [15872/50176]	Loss: 0.4706
Training Epoch: 34 [16384/50176]	Loss: 0.5010
Training Epoch: 34 [16896/50176]	Loss: 0.5112
Training Epoch: 34 [17408/50176]	Loss: 0.5564
Training Epoch: 34 [17920/50176]	Loss: 0.4905
Training Epoch: 34 [18432/50176]	Loss: 0.5384
Training Epoch: 34 [18944/50176]	Loss: 0.4396
Training Epoch: 34 [19456/50176]	Loss: 0.5630
Training Epoch: 34 [19968/50176]	Loss: 0.5125
Training Epoch: 34 [20480/50176]	Loss: 0.5554
Training Epoch: 34 [20992/50176]	Loss: 0.5082
Training Epoch: 34 [21504/50176]	Loss: 0.5386
Training Epoch: 34 [22016/50176]	Loss: 0.5214
Training Epoch: 34 [22528/50176]	Loss: 0.5897
Training Epoch: 34 [23040/50176]	Loss: 0.5511
Training Epoch: 34 [23552/50176]	Loss: 0.5239
Training Epoch: 34 [24064/50176]	Loss: 0.6180
Training Epoch: 34 [24576/50176]	Loss: 0.4467
Training Epoch: 34 [25088/50176]	Loss: 0.5505
Training Epoch: 34 [25600/50176]	Loss: 0.6033
Training Epoch: 34 [26112/50176]	Loss: 0.5636
Training Epoch: 34 [26624/50176]	Loss: 0.6355
Training Epoch: 34 [27136/50176]	Loss: 0.5331
Training Epoch: 34 [27648/50176]	Loss: 0.5455
Training Epoch: 34 [28160/50176]	Loss: 0.5311
Training Epoch: 34 [28672/50176]	Loss: 0.5420
Training Epoch: 34 [29184/50176]	Loss: 0.5835
Training Epoch: 34 [29696/50176]	Loss: 0.5366
Training Epoch: 34 [30208/50176]	Loss: 0.5528
Training Epoch: 34 [30720/50176]	Loss: 0.5634
Training Epoch: 34 [31232/50176]	Loss: 0.6127
Training Epoch: 34 [31744/50176]	Loss: 0.6175
Training Epoch: 34 [32256/50176]	Loss: 0.5003
Training Epoch: 34 [32768/50176]	Loss: 0.5627
Training Epoch: 34 [33280/50176]	Loss: 0.5344
Training Epoch: 34 [33792/50176]	Loss: 0.5469
Training Epoch: 34 [34304/50176]	Loss: 0.6268
Training Epoch: 34 [34816/50176]	Loss: 0.5404
Training Epoch: 34 [35328/50176]	Loss: 0.5186
Training Epoch: 34 [35840/50176]	Loss: 0.5537
Training Epoch: 34 [36352/50176]	Loss: 0.5614
Training Epoch: 34 [36864/50176]	Loss: 0.6077
Training Epoch: 34 [37376/50176]	Loss: 0.5547
Training Epoch: 34 [37888/50176]	Loss: 0.5220
Training Epoch: 34 [38400/50176]	Loss: 0.6355
Training Epoch: 34 [38912/50176]	Loss: 0.6188
Training Epoch: 34 [39424/50176]	Loss: 0.6552
Training Epoch: 34 [39936/50176]	Loss: 0.5488
Training Epoch: 34 [40448/50176]	Loss: 0.5059
Training Epoch: 34 [40960/50176]	Loss: 0.5217
Training Epoch: 34 [41472/50176]	Loss: 0.5630
Training Epoch: 34 [41984/50176]	Loss: 0.5943
Training Epoch: 34 [42496/50176]	Loss: 0.6366
Training Epoch: 34 [43008/50176]	Loss: 0.5651
Training Epoch: 34 [43520/50176]	Loss: 0.5274
Training Epoch: 34 [44032/50176]	Loss: 0.6232
Training Epoch: 34 [44544/50176]	Loss: 0.5630
Training Epoch: 34 [45056/50176]	Loss: 0.6685
Training Epoch: 34 [45568/50176]	Loss: 0.5353
Training Epoch: 34 [46080/50176]	Loss: 0.5697
Training Epoch: 34 [46592/50176]	Loss: 0.6437
Training Epoch: 34 [47104/50176]	Loss: 0.7002
Training Epoch: 34 [47616/50176]	Loss: 0.6721
Training Epoch: 34 [48128/50176]	Loss: 0.5642
Training Epoch: 34 [48640/50176]	Loss: 0.6040
Training Epoch: 34 [49152/50176]	Loss: 0.6364
Training Epoch: 34 [49664/50176]	Loss: 0.5871
Training Epoch: 34 [50176/50176]	Loss: 0.6790
Validation Epoch: 34, Average loss: 0.0043, Accuracy: 0.5640
Training Epoch: 35 [512/50176]	Loss: 0.4456
Training Epoch: 35 [1024/50176]	Loss: 0.4678
Training Epoch: 35 [1536/50176]	Loss: 0.4869
Training Epoch: 35 [2048/50176]	Loss: 0.4314
Training Epoch: 35 [2560/50176]	Loss: 0.4231
Training Epoch: 35 [3072/50176]	Loss: 0.4641
Training Epoch: 35 [3584/50176]	Loss: 0.4225
Training Epoch: 35 [4096/50176]	Loss: 0.5048
Training Epoch: 35 [4608/50176]	Loss: 0.4446
Training Epoch: 35 [5120/50176]	Loss: 0.4835
Training Epoch: 35 [5632/50176]	Loss: 0.4172
Training Epoch: 35 [6144/50176]	Loss: 0.4777
Training Epoch: 35 [6656/50176]	Loss: 0.4775
Training Epoch: 35 [7168/50176]	Loss: 0.4025
Training Epoch: 35 [7680/50176]	Loss: 0.4619
Training Epoch: 35 [8192/50176]	Loss: 0.4579
Training Epoch: 35 [8704/50176]	Loss: 0.4523
Training Epoch: 35 [9216/50176]	Loss: 0.5427
Training Epoch: 35 [9728/50176]	Loss: 0.4968
Training Epoch: 35 [10240/50176]	Loss: 0.5210
Training Epoch: 35 [10752/50176]	Loss: 0.4773
Training Epoch: 35 [11264/50176]	Loss: 0.4830
Training Epoch: 35 [11776/50176]	Loss: 0.5141
Training Epoch: 35 [12288/50176]	Loss: 0.4823
Training Epoch: 35 [12800/50176]	Loss: 0.5021
Training Epoch: 35 [13312/50176]	Loss: 0.4300
Training Epoch: 35 [13824/50176]	Loss: 0.4939
Training Epoch: 35 [14336/50176]	Loss: 0.4914
Training Epoch: 35 [14848/50176]	Loss: 0.5872
Training Epoch: 35 [15360/50176]	Loss: 0.4993
Training Epoch: 35 [15872/50176]	Loss: 0.5050
Training Epoch: 35 [16384/50176]	Loss: 0.4837
Training Epoch: 35 [16896/50176]	Loss: 0.4974
Training Epoch: 35 [17408/50176]	Loss: 0.4450
Training Epoch: 35 [17920/50176]	Loss: 0.4650
Training Epoch: 35 [18432/50176]	Loss: 0.4595
Training Epoch: 35 [18944/50176]	Loss: 0.4925
Training Epoch: 35 [19456/50176]	Loss: 0.4908
Training Epoch: 35 [19968/50176]	Loss: 0.4965
Training Epoch: 35 [20480/50176]	Loss: 0.5231
Training Epoch: 35 [20992/50176]	Loss: 0.5337
Training Epoch: 35 [21504/50176]	Loss: 0.4542
Training Epoch: 35 [22016/50176]	Loss: 0.4964
Training Epoch: 35 [22528/50176]	Loss: 0.4998
Training Epoch: 35 [23040/50176]	Loss: 0.4699
Training Epoch: 35 [23552/50176]	Loss: 0.4414
Training Epoch: 35 [24064/50176]	Loss: 0.5430
Training Epoch: 35 [24576/50176]	Loss: 0.5126
Training Epoch: 35 [25088/50176]	Loss: 0.4970
Training Epoch: 35 [25600/50176]	Loss: 0.4632
Training Epoch: 35 [26112/50176]	Loss: 0.5767
Training Epoch: 35 [26624/50176]	Loss: 0.6090
Training Epoch: 35 [27136/50176]	Loss: 0.4712
Training Epoch: 35 [27648/50176]	Loss: 0.4550
Training Epoch: 35 [28160/50176]	Loss: 0.5558
Training Epoch: 35 [28672/50176]	Loss: 0.4818
Training Epoch: 35 [29184/50176]	Loss: 0.5383
Training Epoch: 35 [29696/50176]	Loss: 0.4597
Training Epoch: 35 [30208/50176]	Loss: 0.5634
Training Epoch: 35 [30720/50176]	Loss: 0.5021
Training Epoch: 35 [31232/50176]	Loss: 0.4504
Training Epoch: 35 [31744/50176]	Loss: 0.5582
Training Epoch: 35 [32256/50176]	Loss: 0.5292
Training Epoch: 35 [32768/50176]	Loss: 0.5142
Training Epoch: 35 [33280/50176]	Loss: 0.5014
Training Epoch: 35 [33792/50176]	Loss: 0.5640
Training Epoch: 35 [34304/50176]	Loss: 0.5486
Training Epoch: 35 [34816/50176]	Loss: 0.5108
Training Epoch: 35 [35328/50176]	Loss: 0.5985
Training Epoch: 35 [35840/50176]	Loss: 0.5602
Training Epoch: 35 [36352/50176]	Loss: 0.4589
Training Epoch: 35 [36864/50176]	Loss: 0.4649
Training Epoch: 35 [37376/50176]	Loss: 0.5774
Training Epoch: 35 [37888/50176]	Loss: 0.5514
Training Epoch: 35 [38400/50176]	Loss: 0.4812
Training Epoch: 35 [38912/50176]	Loss: 0.5109
Training Epoch: 35 [39424/50176]	Loss: 0.5011
Training Epoch: 35 [39936/50176]	Loss: 0.5922
Training Epoch: 35 [40448/50176]	Loss: 0.4799
Training Epoch: 35 [40960/50176]	Loss: 0.5745
Training Epoch: 35 [41472/50176]	Loss: 0.5108
Training Epoch: 35 [41984/50176]	Loss: 0.5886
Training Epoch: 35 [42496/50176]	Loss: 0.5824
Training Epoch: 35 [43008/50176]	Loss: 0.5028
Training Epoch: 35 [43520/50176]	Loss: 0.4910
Training Epoch: 35 [44032/50176]	Loss: 0.5181
Training Epoch: 35 [44544/50176]	Loss: 0.5700
Training Epoch: 35 [45056/50176]	Loss: 0.6076
Training Epoch: 35 [45568/50176]	Loss: 0.5879
Training Epoch: 35 [46080/50176]	Loss: 0.6496
Training Epoch: 35 [46592/50176]	Loss: 0.5399
Training Epoch: 35 [47104/50176]	Loss: 0.5193
Training Epoch: 35 [47616/50176]	Loss: 0.4449
Training Epoch: 35 [48128/50176]	Loss: 0.5252
Training Epoch: 35 [48640/50176]	Loss: 0.5488
Training Epoch: 35 [49152/50176]	Loss: 0.5173
Training Epoch: 35 [49664/50176]	Loss: 0.6099
Training Epoch: 35 [50176/50176]	Loss: 0.5012
Validation Epoch: 35, Average loss: 0.0039, Accuracy: 0.5686
Training Epoch: 36 [512/50176]	Loss: 0.4700
Training Epoch: 36 [1024/50176]	Loss: 0.4374
Training Epoch: 36 [1536/50176]	Loss: 0.4689
Training Epoch: 36 [2048/50176]	Loss: 0.4839
Training Epoch: 36 [2560/50176]	Loss: 0.4811
Training Epoch: 36 [3072/50176]	Loss: 0.4446
Training Epoch: 36 [3584/50176]	Loss: 0.3912
Training Epoch: 36 [4096/50176]	Loss: 0.4411
Training Epoch: 36 [4608/50176]	Loss: 0.4654
Training Epoch: 36 [5120/50176]	Loss: 0.4756
Training Epoch: 36 [5632/50176]	Loss: 0.4661
Training Epoch: 36 [6144/50176]	Loss: 0.5125
Training Epoch: 36 [6656/50176]	Loss: 0.4147
Training Epoch: 36 [7168/50176]	Loss: 0.4234
Training Epoch: 36 [7680/50176]	Loss: 0.3995
Training Epoch: 36 [8192/50176]	Loss: 0.5442
Training Epoch: 36 [8704/50176]	Loss: 0.4838
Training Epoch: 36 [9216/50176]	Loss: 0.4908
Training Epoch: 36 [9728/50176]	Loss: 0.4852
Training Epoch: 36 [10240/50176]	Loss: 0.4263
Training Epoch: 36 [10752/50176]	Loss: 0.4220
Training Epoch: 36 [11264/50176]	Loss: 0.4215
Training Epoch: 36 [11776/50176]	Loss: 0.4277
Training Epoch: 36 [12288/50176]	Loss: 0.4468
Training Epoch: 36 [12800/50176]	Loss: 0.4511
Training Epoch: 36 [13312/50176]	Loss: 0.4472
Training Epoch: 36 [13824/50176]	Loss: 0.4893
Training Epoch: 36 [14336/50176]	Loss: 0.4668
Training Epoch: 36 [14848/50176]	Loss: 0.3931
Training Epoch: 36 [15360/50176]	Loss: 0.3892
Training Epoch: 36 [15872/50176]	Loss: 0.5196
Training Epoch: 36 [16384/50176]	Loss: 0.5238
Training Epoch: 36 [16896/50176]	Loss: 0.4378
Training Epoch: 36 [17408/50176]	Loss: 0.5070
Training Epoch: 36 [17920/50176]	Loss: 0.4761
Training Epoch: 36 [18432/50176]	Loss: 0.4233
Training Epoch: 36 [18944/50176]	Loss: 0.5015
Training Epoch: 36 [19456/50176]	Loss: 0.4749
Training Epoch: 36 [19968/50176]	Loss: 0.4325
Training Epoch: 36 [20480/50176]	Loss: 0.5001
Training Epoch: 36 [20992/50176]	Loss: 0.4860
Training Epoch: 36 [21504/50176]	Loss: 0.4257
Training Epoch: 36 [22016/50176]	Loss: 0.5843
Training Epoch: 36 [22528/50176]	Loss: 0.4592
Training Epoch: 36 [23040/50176]	Loss: 0.4737
Training Epoch: 36 [23552/50176]	Loss: 0.4804
Training Epoch: 36 [24064/50176]	Loss: 0.5205
Training Epoch: 36 [24576/50176]	Loss: 0.4982
Training Epoch: 36 [25088/50176]	Loss: 0.4679
Training Epoch: 36 [25600/50176]	Loss: 0.5051
Training Epoch: 36 [26112/50176]	Loss: 0.5416
Training Epoch: 36 [26624/50176]	Loss: 0.4403
Training Epoch: 36 [27136/50176]	Loss: 0.4948
Training Epoch: 36 [27648/50176]	Loss: 0.4687
Training Epoch: 36 [28160/50176]	Loss: 0.4705
Training Epoch: 36 [28672/50176]	Loss: 0.4710
Training Epoch: 36 [29184/50176]	Loss: 0.4321
Training Epoch: 36 [29696/50176]	Loss: 0.6147
Training Epoch: 36 [30208/50176]	Loss: 0.5026
Training Epoch: 36 [30720/50176]	Loss: 0.4930
Training Epoch: 36 [31232/50176]	Loss: 0.5186
Training Epoch: 36 [31744/50176]	Loss: 0.4857
Training Epoch: 36 [32256/50176]	Loss: 0.5620
Training Epoch: 36 [32768/50176]	Loss: 0.5448
Training Epoch: 36 [33280/50176]	Loss: 0.4109
Training Epoch: 36 [33792/50176]	Loss: 0.5314
Training Epoch: 36 [34304/50176]	Loss: 0.5074
Training Epoch: 36 [34816/50176]	Loss: 0.5134
Training Epoch: 36 [35328/50176]	Loss: 0.5126
Training Epoch: 36 [35840/50176]	Loss: 0.5058
Training Epoch: 36 [36352/50176]	Loss: 0.5246
Training Epoch: 36 [36864/50176]	Loss: 0.4755
Training Epoch: 36 [37376/50176]	Loss: 0.5339
Training Epoch: 36 [37888/50176]	Loss: 0.5089
Training Epoch: 36 [38400/50176]	Loss: 0.5290
Training Epoch: 36 [38912/50176]	Loss: 0.4986
Training Epoch: 36 [39424/50176]	Loss: 0.5302
Training Epoch: 36 [39936/50176]	Loss: 0.5722
Training Epoch: 36 [40448/50176]	Loss: 0.4814
Training Epoch: 36 [40960/50176]	Loss: 0.5387
Training Epoch: 36 [41472/50176]	Loss: 0.5774
Training Epoch: 36 [41984/50176]	Loss: 0.5571
Training Epoch: 36 [42496/50176]	Loss: 0.4686
Training Epoch: 36 [43008/50176]	Loss: 0.6041
Training Epoch: 36 [43520/50176]	Loss: 0.5770
Training Epoch: 36 [44032/50176]	Loss: 0.5121
Training Epoch: 36 [44544/50176]	Loss: 0.4813
Training Epoch: 36 [45056/50176]	Loss: 0.5540
Training Epoch: 36 [45568/50176]	Loss: 0.5603
Training Epoch: 36 [46080/50176]	Loss: 0.4726
Training Epoch: 36 [46592/50176]	Loss: 0.4840
Training Epoch: 36 [47104/50176]	Loss: 0.5335
Training Epoch: 36 [47616/50176]	Loss: 0.4735
Training Epoch: 36 [48128/50176]	Loss: 0.5733
Training Epoch: 36 [48640/50176]	Loss: 0.5229
Training Epoch: 36 [49152/50176]	Loss: 0.4903
Training Epoch: 36 [49664/50176]	Loss: 0.5512
Training Epoch: 36 [50176/50176]	Loss: 0.6005
Validation Epoch: 36, Average loss: 0.0048, Accuracy: 0.5430
Training Epoch: 37 [512/50176]	Loss: 0.4192
Training Epoch: 37 [1024/50176]	Loss: 0.4153
Training Epoch: 37 [1536/50176]	Loss: 0.4029
Training Epoch: 37 [2048/50176]	Loss: 0.4030
Training Epoch: 37 [2560/50176]	Loss: 0.3665
Training Epoch: 37 [3072/50176]	Loss: 0.3981
Training Epoch: 37 [3584/50176]	Loss: 0.4098
Training Epoch: 37 [4096/50176]	Loss: 0.3865
Training Epoch: 37 [4608/50176]	Loss: 0.4795
Training Epoch: 37 [5120/50176]	Loss: 0.3818
Training Epoch: 37 [5632/50176]	Loss: 0.4902
Training Epoch: 37 [6144/50176]	Loss: 0.4303
Training Epoch: 37 [6656/50176]	Loss: 0.3986
Training Epoch: 37 [7168/50176]	Loss: 0.3789
Training Epoch: 37 [7680/50176]	Loss: 0.4028
Training Epoch: 37 [8192/50176]	Loss: 0.4274
Training Epoch: 37 [8704/50176]	Loss: 0.4221
Training Epoch: 37 [9216/50176]	Loss: 0.4470
Training Epoch: 37 [9728/50176]	Loss: 0.4514
Training Epoch: 37 [10240/50176]	Loss: 0.4226
Training Epoch: 37 [10752/50176]	Loss: 0.4997
Training Epoch: 37 [11264/50176]	Loss: 0.4162
Training Epoch: 37 [11776/50176]	Loss: 0.4768
Training Epoch: 37 [12288/50176]	Loss: 0.4205
Training Epoch: 37 [12800/50176]	Loss: 0.4521
Training Epoch: 37 [13312/50176]	Loss: 0.4433
Training Epoch: 37 [13824/50176]	Loss: 0.3882
Training Epoch: 37 [14336/50176]	Loss: 0.3869
Training Epoch: 37 [14848/50176]	Loss: 0.3778
Training Epoch: 37 [15360/50176]	Loss: 0.3743
Training Epoch: 37 [15872/50176]	Loss: 0.3956
Training Epoch: 37 [16384/50176]	Loss: 0.4102
Training Epoch: 37 [16896/50176]	Loss: 0.5050
Training Epoch: 37 [17408/50176]	Loss: 0.4560
Training Epoch: 37 [17920/50176]	Loss: 0.4916
Training Epoch: 37 [18432/50176]	Loss: 0.4410
Training Epoch: 37 [18944/50176]	Loss: 0.4261
Training Epoch: 37 [19456/50176]	Loss: 0.5260
Training Epoch: 37 [19968/50176]	Loss: 0.4943
Training Epoch: 37 [20480/50176]	Loss: 0.4024
Training Epoch: 37 [20992/50176]	Loss: 0.4719
Training Epoch: 37 [21504/50176]	Loss: 0.5106
Training Epoch: 37 [22016/50176]	Loss: 0.4848
Training Epoch: 37 [22528/50176]	Loss: 0.5145
Training Epoch: 37 [23040/50176]	Loss: 0.3975
Training Epoch: 37 [23552/50176]	Loss: 0.4543
Training Epoch: 37 [24064/50176]	Loss: 0.4657
Training Epoch: 37 [24576/50176]	Loss: 0.4087
Training Epoch: 37 [25088/50176]	Loss: 0.4747
Training Epoch: 37 [25600/50176]	Loss: 0.4072
Training Epoch: 37 [26112/50176]	Loss: 0.4073
Training Epoch: 37 [26624/50176]	Loss: 0.5061
Training Epoch: 37 [27136/50176]	Loss: 0.5268
Training Epoch: 37 [27648/50176]	Loss: 0.4236
Training Epoch: 37 [28160/50176]	Loss: 0.5474
Training Epoch: 37 [28672/50176]	Loss: 0.4063
Training Epoch: 37 [29184/50176]	Loss: 0.4706
Training Epoch: 37 [29696/50176]	Loss: 0.4451
Training Epoch: 37 [30208/50176]	Loss: 0.4476
Training Epoch: 37 [30720/50176]	Loss: 0.4869
Training Epoch: 37 [31232/50176]	Loss: 0.4450
Training Epoch: 37 [31744/50176]	Loss: 0.4412
Training Epoch: 37 [32256/50176]	Loss: 0.5278
Training Epoch: 37 [32768/50176]	Loss: 0.4615
Training Epoch: 37 [33280/50176]	Loss: 0.4357
Training Epoch: 37 [33792/50176]	Loss: 0.4936
Training Epoch: 37 [34304/50176]	Loss: 0.4543
Training Epoch: 37 [34816/50176]	Loss: 0.5348
Training Epoch: 37 [35328/50176]	Loss: 0.4931
Training Epoch: 37 [35840/50176]	Loss: 0.4347
Training Epoch: 37 [36352/50176]	Loss: 0.5198
Training Epoch: 37 [36864/50176]	Loss: 0.4761
Training Epoch: 37 [37376/50176]	Loss: 0.5243
Training Epoch: 37 [37888/50176]	Loss: 0.5173
Training Epoch: 37 [38400/50176]	Loss: 0.4693
Training Epoch: 37 [38912/50176]	Loss: 0.4887
Training Epoch: 37 [39424/50176]	Loss: 0.5246
Training Epoch: 37 [39936/50176]	Loss: 0.4276
Training Epoch: 37 [40448/50176]	Loss: 0.4305
Training Epoch: 37 [40960/50176]	Loss: 0.4441
Training Epoch: 37 [41472/50176]	Loss: 0.5769
Training Epoch: 37 [41984/50176]	Loss: 0.5338
Training Epoch: 37 [42496/50176]	Loss: 0.5006
Training Epoch: 37 [43008/50176]	Loss: 0.4416
Training Epoch: 37 [43520/50176]	Loss: 0.4472
Training Epoch: 37 [44032/50176]	Loss: 0.4720
Training Epoch: 37 [44544/50176]	Loss: 0.5095
Training Epoch: 37 [45056/50176]	Loss: 0.5196
Training Epoch: 37 [45568/50176]	Loss: 0.4760
Training Epoch: 37 [46080/50176]	Loss: 0.4507
Training Epoch: 37 [46592/50176]	Loss: 0.5012
Training Epoch: 37 [47104/50176]	Loss: 0.5121
Training Epoch: 37 [47616/50176]	Loss: 0.5180
Training Epoch: 37 [48128/50176]	Loss: 0.5497
Training Epoch: 37 [48640/50176]	Loss: 0.4777
Training Epoch: 37 [49152/50176]	Loss: 0.5768
Training Epoch: 37 [49664/50176]	Loss: 0.4084
Training Epoch: 37 [50176/50176]	Loss: 0.5639
Validation Epoch: 37, Average loss: 0.0044, Accuracy: 0.5562
Training Epoch: 38 [512/50176]	Loss: 0.4630
Training Epoch: 38 [1024/50176]	Loss: 0.3990
Training Epoch: 38 [1536/50176]	Loss: 0.3739
Training Epoch: 38 [2048/50176]	Loss: 0.3841
Training Epoch: 38 [2560/50176]	Loss: 0.3903
Training Epoch: 38 [3072/50176]	Loss: 0.4034
Training Epoch: 38 [3584/50176]	Loss: 0.4467
Training Epoch: 38 [4096/50176]	Loss: 0.4594
Training Epoch: 38 [4608/50176]	Loss: 0.4737
Training Epoch: 38 [5120/50176]	Loss: 0.3326
Training Epoch: 38 [5632/50176]	Loss: 0.4217
Training Epoch: 38 [6144/50176]	Loss: 0.4139
Training Epoch: 38 [6656/50176]	Loss: 0.4260
Training Epoch: 38 [7168/50176]	Loss: 0.3634
Training Epoch: 38 [7680/50176]	Loss: 0.3848
Training Epoch: 38 [8192/50176]	Loss: 0.4352
Training Epoch: 38 [8704/50176]	Loss: 0.4122
Training Epoch: 38 [9216/50176]	Loss: 0.4133
Training Epoch: 38 [9728/50176]	Loss: 0.4282
Training Epoch: 38 [10240/50176]	Loss: 0.4081
Training Epoch: 38 [10752/50176]	Loss: 0.4696
Training Epoch: 38 [11264/50176]	Loss: 0.4831
Training Epoch: 38 [11776/50176]	Loss: 0.3866
Training Epoch: 38 [12288/50176]	Loss: 0.4551
Training Epoch: 38 [12800/50176]	Loss: 0.3947
Training Epoch: 38 [13312/50176]	Loss: 0.3851
Training Epoch: 38 [13824/50176]	Loss: 0.4183
Training Epoch: 38 [14336/50176]	Loss: 0.3462
Training Epoch: 38 [14848/50176]	Loss: 0.3525
Training Epoch: 38 [15360/50176]	Loss: 0.3812
Training Epoch: 38 [15872/50176]	Loss: 0.3717
Training Epoch: 38 [16384/50176]	Loss: 0.4236
Training Epoch: 38 [16896/50176]	Loss: 0.3750
Training Epoch: 38 [17408/50176]	Loss: 0.4442
Training Epoch: 38 [17920/50176]	Loss: 0.4341
Training Epoch: 38 [18432/50176]	Loss: 0.4451
Training Epoch: 38 [18944/50176]	Loss: 0.3822
Training Epoch: 38 [19456/50176]	Loss: 0.3988
Training Epoch: 38 [19968/50176]	Loss: 0.3782
Training Epoch: 38 [20480/50176]	Loss: 0.3939
Training Epoch: 38 [20992/50176]	Loss: 0.5565
Training Epoch: 38 [21504/50176]	Loss: 0.4859
Training Epoch: 38 [22016/50176]	Loss: 0.4806
Training Epoch: 38 [22528/50176]	Loss: 0.4575
Training Epoch: 38 [23040/50176]	Loss: 0.4732
Training Epoch: 38 [23552/50176]	Loss: 0.4449
Training Epoch: 38 [24064/50176]	Loss: 0.4681
Training Epoch: 38 [24576/50176]	Loss: 0.3880
Training Epoch: 38 [25088/50176]	Loss: 0.3573
Training Epoch: 38 [25600/50176]	Loss: 0.4647
Training Epoch: 38 [26112/50176]	Loss: 0.3907
Training Epoch: 38 [26624/50176]	Loss: 0.3923
Training Epoch: 38 [27136/50176]	Loss: 0.4588
Training Epoch: 38 [27648/50176]	Loss: 0.5481
Training Epoch: 38 [28160/50176]	Loss: 0.4319
Training Epoch: 38 [28672/50176]	Loss: 0.4030
Training Epoch: 38 [29184/50176]	Loss: 0.4268
Training Epoch: 38 [29696/50176]	Loss: 0.4501
Training Epoch: 38 [30208/50176]	Loss: 0.4829
Training Epoch: 38 [30720/50176]	Loss: 0.4790
Training Epoch: 38 [31232/50176]	Loss: 0.4197
Training Epoch: 38 [31744/50176]	Loss: 0.4232
Training Epoch: 38 [32256/50176]	Loss: 0.5119
Training Epoch: 38 [32768/50176]	Loss: 0.4952
Training Epoch: 38 [33280/50176]	Loss: 0.4980
Training Epoch: 38 [33792/50176]	Loss: 0.4107
Training Epoch: 38 [34304/50176]	Loss: 0.4491
Training Epoch: 38 [34816/50176]	Loss: 0.4464
Training Epoch: 38 [35328/50176]	Loss: 0.4643
Training Epoch: 38 [35840/50176]	Loss: 0.4755
Training Epoch: 38 [36352/50176]	Loss: 0.4477
Training Epoch: 38 [36864/50176]	Loss: 0.4630
Training Epoch: 38 [37376/50176]	Loss: 0.5081
Training Epoch: 38 [37888/50176]	Loss: 0.4893
Training Epoch: 38 [38400/50176]	Loss: 0.3964
Training Epoch: 38 [38912/50176]	Loss: 0.4814
Training Epoch: 38 [39424/50176]	Loss: 0.5255
Training Epoch: 38 [39936/50176]	Loss: 0.5053
Training Epoch: 38 [40448/50176]	Loss: 0.4904
Training Epoch: 38 [40960/50176]	Loss: 0.5198
Training Epoch: 38 [41472/50176]	Loss: 0.5442
Training Epoch: 38 [41984/50176]	Loss: 0.4829
Training Epoch: 38 [42496/50176]	Loss: 0.4862
Training Epoch: 38 [43008/50176]	Loss: 0.4621
Training Epoch: 38 [43520/50176]	Loss: 0.4786
Training Epoch: 38 [44032/50176]	Loss: 0.5101
Training Epoch: 38 [44544/50176]	Loss: 0.5014
Training Epoch: 38 [45056/50176]	Loss: 0.5005
Training Epoch: 38 [45568/50176]	Loss: 0.4588
Training Epoch: 38 [46080/50176]	Loss: 0.4261
Training Epoch: 38 [46592/50176]	Loss: 0.4736
Training Epoch: 38 [47104/50176]	Loss: 0.5107
Training Epoch: 38 [47616/50176]	Loss: 0.4380
Training Epoch: 38 [48128/50176]	Loss: 0.5048
Training Epoch: 38 [48640/50176]	Loss: 0.5013
Training Epoch: 38 [49152/50176]	Loss: 0.4561
Training Epoch: 38 [49664/50176]	Loss: 0.4323
Training Epoch: 38 [50176/50176]	Loss: 0.5364
Validation Epoch: 38, Average loss: 0.0043, Accuracy: 0.5606
Training Epoch: 39 [512/50176]	Loss: 0.3556
Training Epoch: 39 [1024/50176]	Loss: 0.3619
Training Epoch: 39 [1536/50176]	Loss: 0.4272
Training Epoch: 39 [2048/50176]	Loss: 0.4166
Training Epoch: 39 [2560/50176]	Loss: 0.4125
Training Epoch: 39 [3072/50176]	Loss: 0.3489
Training Epoch: 39 [3584/50176]	Loss: 0.3798
Training Epoch: 39 [4096/50176]	Loss: 0.3376
Training Epoch: 39 [4608/50176]	Loss: 0.3803
Training Epoch: 39 [5120/50176]	Loss: 0.3328
Training Epoch: 39 [5632/50176]	Loss: 0.3823
Training Epoch: 39 [6144/50176]	Loss: 0.3484
Training Epoch: 39 [6656/50176]	Loss: 0.4341
Training Epoch: 39 [7168/50176]	Loss: 0.3541
Training Epoch: 39 [7680/50176]	Loss: 0.4186
Training Epoch: 39 [8192/50176]	Loss: 0.3088
Training Epoch: 39 [8704/50176]	Loss: 0.3062
Training Epoch: 39 [9216/50176]	Loss: 0.4016
Training Epoch: 39 [9728/50176]	Loss: 0.3595
Training Epoch: 39 [10240/50176]	Loss: 0.3951
Training Epoch: 39 [10752/50176]	Loss: 0.3913
Training Epoch: 39 [11264/50176]	Loss: 0.3644
Training Epoch: 39 [11776/50176]	Loss: 0.4111
Training Epoch: 39 [12288/50176]	Loss: 0.3202
Training Epoch: 39 [12800/50176]	Loss: 0.3708
Training Epoch: 39 [13312/50176]	Loss: 0.4050
Training Epoch: 39 [13824/50176]	Loss: 0.4402
Training Epoch: 39 [14336/50176]	Loss: 0.3328
Training Epoch: 39 [14848/50176]	Loss: 0.3678
Training Epoch: 39 [15360/50176]	Loss: 0.3527
Training Epoch: 39 [15872/50176]	Loss: 0.3947
Training Epoch: 39 [16384/50176]	Loss: 0.4209
Training Epoch: 39 [16896/50176]	Loss: 0.3661
Training Epoch: 39 [17408/50176]	Loss: 0.4520
Training Epoch: 39 [17920/50176]	Loss: 0.4226
Training Epoch: 39 [18432/50176]	Loss: 0.4176
Training Epoch: 39 [18944/50176]	Loss: 0.4028
Training Epoch: 39 [19456/50176]	Loss: 0.3530
Training Epoch: 39 [19968/50176]	Loss: 0.3989
Training Epoch: 39 [20480/50176]	Loss: 0.4181
Training Epoch: 39 [20992/50176]	Loss: 0.3275
Training Epoch: 39 [21504/50176]	Loss: 0.4008
Training Epoch: 39 [22016/50176]	Loss: 0.3520
Training Epoch: 39 [22528/50176]	Loss: 0.4256
Training Epoch: 39 [23040/50176]	Loss: 0.4090
Training Epoch: 39 [23552/50176]	Loss: 0.3450
Training Epoch: 39 [24064/50176]	Loss: 0.3645
Training Epoch: 39 [24576/50176]	Loss: 0.4220
Training Epoch: 39 [25088/50176]	Loss: 0.5277
Training Epoch: 39 [25600/50176]	Loss: 0.3970
Training Epoch: 39 [26112/50176]	Loss: 0.4171
Training Epoch: 39 [26624/50176]	Loss: 0.3835
Training Epoch: 39 [27136/50176]	Loss: 0.4390
Training Epoch: 39 [27648/50176]	Loss: 0.3633
Training Epoch: 39 [28160/50176]	Loss: 0.4035
Training Epoch: 39 [28672/50176]	Loss: 0.4660
Training Epoch: 39 [29184/50176]	Loss: 0.3951
Training Epoch: 39 [29696/50176]	Loss: 0.4396
Training Epoch: 39 [30208/50176]	Loss: 0.4063
Training Epoch: 39 [30720/50176]	Loss: 0.4611
Training Epoch: 39 [31232/50176]	Loss: 0.4165
Training Epoch: 39 [31744/50176]	Loss: 0.4160
Training Epoch: 39 [32256/50176]	Loss: 0.4965
Training Epoch: 39 [32768/50176]	Loss: 0.4546
Training Epoch: 39 [33280/50176]	Loss: 0.3603
Training Epoch: 39 [33792/50176]	Loss: 0.5268
Training Epoch: 39 [34304/50176]	Loss: 0.4267
Training Epoch: 39 [34816/50176]	Loss: 0.4823
Training Epoch: 39 [35328/50176]	Loss: 0.4638
Training Epoch: 39 [35840/50176]	Loss: 0.3748
Training Epoch: 39 [36352/50176]	Loss: 0.4491
Training Epoch: 39 [36864/50176]	Loss: 0.4570
Training Epoch: 39 [37376/50176]	Loss: 0.3885
Training Epoch: 39 [37888/50176]	Loss: 0.4777
Training Epoch: 39 [38400/50176]	Loss: 0.4101
Training Epoch: 39 [38912/50176]	Loss: 0.4238
Training Epoch: 39 [39424/50176]	Loss: 0.4050
Training Epoch: 39 [39936/50176]	Loss: 0.4129
Training Epoch: 39 [40448/50176]	Loss: 0.4160
Training Epoch: 39 [40960/50176]	Loss: 0.4468
Training Epoch: 39 [41472/50176]	Loss: 0.4580
Training Epoch: 39 [41984/50176]	Loss: 0.4558
Training Epoch: 39 [42496/50176]	Loss: 0.4363
Training Epoch: 39 [43008/50176]	Loss: 0.4954
Training Epoch: 39 [43520/50176]	Loss: 0.3919
Training Epoch: 39 [44032/50176]	Loss: 0.4868
Training Epoch: 39 [44544/50176]	Loss: 0.4933
Training Epoch: 39 [45056/50176]	Loss: 0.4874
Training Epoch: 39 [45568/50176]	Loss: 0.5277
Training Epoch: 39 [46080/50176]	Loss: 0.4723
Training Epoch: 39 [46592/50176]	Loss: 0.5191
Training Epoch: 39 [47104/50176]	Loss: 0.4273
Training Epoch: 39 [47616/50176]	Loss: 0.4804
Training Epoch: 39 [48128/50176]	Loss: 0.4910
Training Epoch: 39 [48640/50176]	Loss: 0.4343
Training Epoch: 39 [49152/50176]	Loss: 0.3658
Training Epoch: 39 [49664/50176]	Loss: 0.4692
Training Epoch: 39 [50176/50176]	Loss: 0.4332
Validation Epoch: 39, Average loss: 0.0041, Accuracy: 0.5807
Training Epoch: 40 [512/50176]	Loss: 0.3639
Training Epoch: 40 [1024/50176]	Loss: 0.3964
Training Epoch: 40 [1536/50176]	Loss: 0.3491
Training Epoch: 40 [2048/50176]	Loss: 0.3941
Training Epoch: 40 [2560/50176]	Loss: 0.3932
Training Epoch: 40 [3072/50176]	Loss: 0.3253
Training Epoch: 40 [3584/50176]	Loss: 0.3831
Training Epoch: 40 [4096/50176]	Loss: 0.4288
Training Epoch: 40 [4608/50176]	Loss: 0.3682
Training Epoch: 40 [5120/50176]	Loss: 0.4772
Training Epoch: 40 [5632/50176]	Loss: 0.4417
Training Epoch: 40 [6144/50176]	Loss: 0.3299
Training Epoch: 40 [6656/50176]	Loss: 0.3890
Training Epoch: 40 [7168/50176]	Loss: 0.3779
Training Epoch: 40 [7680/50176]	Loss: 0.4156
Training Epoch: 40 [8192/50176]	Loss: 0.3345
Training Epoch: 40 [8704/50176]	Loss: 0.3451
Training Epoch: 40 [9216/50176]	Loss: 0.3610
Training Epoch: 40 [9728/50176]	Loss: 0.3359
Training Epoch: 40 [10240/50176]	Loss: 0.3501
Training Epoch: 40 [10752/50176]	Loss: 0.3666
Training Epoch: 40 [11264/50176]	Loss: 0.3934
Training Epoch: 40 [11776/50176]	Loss: 0.3661
Training Epoch: 40 [12288/50176]	Loss: 0.4553
Training Epoch: 40 [12800/50176]	Loss: 0.3818
Training Epoch: 40 [13312/50176]	Loss: 0.3648
Training Epoch: 40 [13824/50176]	Loss: 0.3254
Training Epoch: 40 [14336/50176]	Loss: 0.3246
Training Epoch: 40 [14848/50176]	Loss: 0.4055
Training Epoch: 40 [15360/50176]	Loss: 0.3857
Training Epoch: 40 [15872/50176]	Loss: 0.4071
Training Epoch: 40 [16384/50176]	Loss: 0.3291
Training Epoch: 40 [16896/50176]	Loss: 0.3494
Training Epoch: 40 [17408/50176]	Loss: 0.4131
Training Epoch: 40 [17920/50176]	Loss: 0.4304
Training Epoch: 40 [18432/50176]	Loss: 0.4204
Training Epoch: 40 [18944/50176]	Loss: 0.4286
Training Epoch: 40 [19456/50176]	Loss: 0.3939
Training Epoch: 40 [19968/50176]	Loss: 0.4188
Training Epoch: 40 [20480/50176]	Loss: 0.3500
Training Epoch: 40 [20992/50176]	Loss: 0.3777
Training Epoch: 40 [21504/50176]	Loss: 0.4081
Training Epoch: 40 [22016/50176]	Loss: 0.4029
Training Epoch: 40 [22528/50176]	Loss: 0.3771
Training Epoch: 40 [23040/50176]	Loss: 0.4322
Training Epoch: 40 [23552/50176]	Loss: 0.4635
Training Epoch: 40 [24064/50176]	Loss: 0.4299
Training Epoch: 40 [24576/50176]	Loss: 0.3811
Training Epoch: 40 [25088/50176]	Loss: 0.3542
Training Epoch: 40 [25600/50176]	Loss: 0.3711
Training Epoch: 40 [26112/50176]	Loss: 0.3726
Training Epoch: 40 [26624/50176]	Loss: 0.3905
Training Epoch: 40 [27136/50176]	Loss: 0.3923
Training Epoch: 40 [27648/50176]	Loss: 0.4679
Training Epoch: 40 [28160/50176]	Loss: 0.3568
Training Epoch: 40 [28672/50176]	Loss: 0.3577
Training Epoch: 40 [29184/50176]	Loss: 0.3757
Training Epoch: 40 [29696/50176]	Loss: 0.4169
Training Epoch: 40 [30208/50176]	Loss: 0.3745
Training Epoch: 40 [30720/50176]	Loss: 0.4768
Training Epoch: 40 [31232/50176]	Loss: 0.3679
Training Epoch: 40 [31744/50176]	Loss: 0.3622
Training Epoch: 40 [32256/50176]	Loss: 0.3885
Training Epoch: 40 [32768/50176]	Loss: 0.3709
Training Epoch: 40 [33280/50176]	Loss: 0.3795
Training Epoch: 40 [33792/50176]	Loss: 0.3873
Training Epoch: 40 [34304/50176]	Loss: 0.3930
Training Epoch: 40 [34816/50176]	Loss: 0.3936
Training Epoch: 40 [35328/50176]	Loss: 0.4206
Training Epoch: 40 [35840/50176]	Loss: 0.4446
Training Epoch: 40 [36352/50176]	Loss: 0.4125
Training Epoch: 40 [36864/50176]	Loss: 0.3928
Training Epoch: 40 [37376/50176]	Loss: 0.3647
Training Epoch: 40 [37888/50176]	Loss: 0.3596
Training Epoch: 40 [38400/50176]	Loss: 0.4449
Training Epoch: 40 [38912/50176]	Loss: 0.4394
Training Epoch: 40 [39424/50176]	Loss: 0.4431
Training Epoch: 40 [39936/50176]	Loss: 0.4166
Training Epoch: 40 [40448/50176]	Loss: 0.4903
Training Epoch: 40 [40960/50176]	Loss: 0.5103
Training Epoch: 40 [41472/50176]	Loss: 0.3870
Training Epoch: 40 [41984/50176]	Loss: 0.4355
Training Epoch: 40 [42496/50176]	Loss: 0.3846
Training Epoch: 40 [43008/50176]	Loss: 0.4322
Training Epoch: 40 [43520/50176]	Loss: 0.4066
Training Epoch: 40 [44032/50176]	Loss: 0.4870
Training Epoch: 40 [44544/50176]	Loss: 0.5068
Training Epoch: 40 [45056/50176]	Loss: 0.3950
Training Epoch: 40 [45568/50176]	Loss: 0.5039
Training Epoch: 40 [46080/50176]	Loss: 0.4089
Training Epoch: 40 [46592/50176]	Loss: 0.4723
Training Epoch: 40 [47104/50176]	Loss: 0.4969
Training Epoch: 40 [47616/50176]	Loss: 0.4499
Training Epoch: 40 [48128/50176]	Loss: 0.4594
Training Epoch: 40 [48640/50176]	Loss: 0.4456
Training Epoch: 40 [49152/50176]	Loss: 0.4983
Training Epoch: 40 [49664/50176]	Loss: 0.4106
Training Epoch: 40 [50176/50176]	Loss: 0.4263
Validation Epoch: 40, Average loss: 0.0053, Accuracy: 0.5372
Training Epoch: 41 [512/50176]	Loss: 0.3340
Training Epoch: 41 [1024/50176]	Loss: 0.3498
Training Epoch: 41 [1536/50176]	Loss: 0.3564
Training Epoch: 41 [2048/50176]	Loss: 0.3997
Training Epoch: 41 [2560/50176]	Loss: 0.3502
Training Epoch: 41 [3072/50176]	Loss: 0.3584
Training Epoch: 41 [3584/50176]	Loss: 0.3171
Training Epoch: 41 [4096/50176]	Loss: 0.3471
Training Epoch: 41 [4608/50176]	Loss: 0.3721
Training Epoch: 41 [5120/50176]	Loss: 0.2917
Training Epoch: 41 [5632/50176]	Loss: 0.3180
Training Epoch: 41 [6144/50176]	Loss: 0.3583
Training Epoch: 41 [6656/50176]	Loss: 0.3710
Training Epoch: 41 [7168/50176]	Loss: 0.3161
Training Epoch: 41 [7680/50176]	Loss: 0.3546
Training Epoch: 41 [8192/50176]	Loss: 0.3566
Training Epoch: 41 [8704/50176]	Loss: 0.3313
Training Epoch: 41 [9216/50176]	Loss: 0.3115
Training Epoch: 41 [9728/50176]	Loss: 0.3920
Training Epoch: 41 [10240/50176]	Loss: 0.3706
Training Epoch: 41 [10752/50176]	Loss: 0.3551
Training Epoch: 41 [11264/50176]	Loss: 0.3323
Training Epoch: 41 [11776/50176]	Loss: 0.4066
Training Epoch: 41 [12288/50176]	Loss: 0.3205
Training Epoch: 41 [12800/50176]	Loss: 0.2994
Training Epoch: 41 [13312/50176]	Loss: 0.3615
Training Epoch: 41 [13824/50176]	Loss: 0.3874
Training Epoch: 41 [14336/50176]	Loss: 0.3493
Training Epoch: 41 [14848/50176]	Loss: 0.3799
Training Epoch: 41 [15360/50176]	Loss: 0.3851
Training Epoch: 41 [15872/50176]	Loss: 0.3299
Training Epoch: 41 [16384/50176]	Loss: 0.3601
Training Epoch: 41 [16896/50176]	Loss: 0.3330
Training Epoch: 41 [17408/50176]	Loss: 0.3565
Training Epoch: 41 [17920/50176]	Loss: 0.3878
Training Epoch: 41 [18432/50176]	Loss: 0.3750
Training Epoch: 41 [18944/50176]	Loss: 0.3598
Training Epoch: 41 [19456/50176]	Loss: 0.3832
Training Epoch: 41 [19968/50176]	Loss: 0.4040
Training Epoch: 41 [20480/50176]	Loss: 0.3617
Training Epoch: 41 [20992/50176]	Loss: 0.3655
Training Epoch: 41 [21504/50176]	Loss: 0.3880
Training Epoch: 41 [22016/50176]	Loss: 0.3631
Training Epoch: 41 [22528/50176]	Loss: 0.3871
Training Epoch: 41 [23040/50176]	Loss: 0.3487
Training Epoch: 41 [23552/50176]	Loss: 0.3403
Training Epoch: 41 [24064/50176]	Loss: 0.3710
Training Epoch: 41 [24576/50176]	Loss: 0.3671
Training Epoch: 41 [25088/50176]	Loss: 0.3594
Training Epoch: 41 [25600/50176]	Loss: 0.3676
Training Epoch: 41 [26112/50176]	Loss: 0.3443
Training Epoch: 41 [26624/50176]	Loss: 0.3369
Training Epoch: 41 [27136/50176]	Loss: 0.3680
Training Epoch: 41 [27648/50176]	Loss: 0.3932
Training Epoch: 41 [28160/50176]	Loss: 0.4077
Training Epoch: 41 [28672/50176]	Loss: 0.3428
Training Epoch: 41 [29184/50176]	Loss: 0.2946
Training Epoch: 41 [29696/50176]	Loss: 0.4176
Training Epoch: 41 [30208/50176]	Loss: 0.3709
Training Epoch: 41 [30720/50176]	Loss: 0.3836
Training Epoch: 41 [31232/50176]	Loss: 0.3648
Training Epoch: 41 [31744/50176]	Loss: 0.4150
Training Epoch: 41 [32256/50176]	Loss: 0.4481
Training Epoch: 41 [32768/50176]	Loss: 0.3806
Training Epoch: 41 [33280/50176]	Loss: 0.3652
Training Epoch: 41 [33792/50176]	Loss: 0.4342
Training Epoch: 41 [34304/50176]	Loss: 0.4344
Training Epoch: 41 [34816/50176]	Loss: 0.3956
Training Epoch: 41 [35328/50176]	Loss: 0.4181
Training Epoch: 41 [35840/50176]	Loss: 0.4474
Training Epoch: 41 [36352/50176]	Loss: 0.3773
Training Epoch: 41 [36864/50176]	Loss: 0.4280
Training Epoch: 41 [37376/50176]	Loss: 0.3159
Training Epoch: 41 [37888/50176]	Loss: 0.4451
Training Epoch: 41 [38400/50176]	Loss: 0.4416
Training Epoch: 41 [38912/50176]	Loss: 0.4163
Training Epoch: 41 [39424/50176]	Loss: 0.3796
Training Epoch: 41 [39936/50176]	Loss: 0.4247
Training Epoch: 41 [40448/50176]	Loss: 0.4080
Training Epoch: 41 [40960/50176]	Loss: 0.3828
Training Epoch: 41 [41472/50176]	Loss: 0.3501
Training Epoch: 41 [41984/50176]	Loss: 0.3929
Training Epoch: 41 [42496/50176]	Loss: 0.4538
Training Epoch: 41 [43008/50176]	Loss: 0.4702
Training Epoch: 41 [43520/50176]	Loss: 0.3747
Training Epoch: 41 [44032/50176]	Loss: 0.3932
Training Epoch: 41 [44544/50176]	Loss: 0.3252
Training Epoch: 41 [45056/50176]	Loss: 0.4245
Training Epoch: 41 [45568/50176]	Loss: 0.4120
Training Epoch: 41 [46080/50176]	Loss: 0.4644
Training Epoch: 41 [46592/50176]	Loss: 0.3726
Training Epoch: 41 [47104/50176]	Loss: 0.4709
Training Epoch: 41 [47616/50176]	Loss: 0.4435
Training Epoch: 41 [48128/50176]	Loss: 0.4650
Training Epoch: 41 [48640/50176]	Loss: 0.3924
Training Epoch: 41 [49152/50176]	Loss: 0.4053
Training Epoch: 41 [49664/50176]	Loss: 0.4576
Training Epoch: 41 [50176/50176]	Loss: 0.4985
Validation Epoch: 41, Average loss: 0.0046, Accuracy: 0.5657
Training Epoch: 42 [512/50176]	Loss: 0.3181
Training Epoch: 42 [1024/50176]	Loss: 0.3412
Training Epoch: 42 [1536/50176]	Loss: 0.3448
Training Epoch: 42 [2048/50176]	Loss: 0.3557
Training Epoch: 42 [2560/50176]	Loss: 0.3805
Training Epoch: 42 [3072/50176]	Loss: 0.4255
Training Epoch: 42 [3584/50176]	Loss: 0.3629
Training Epoch: 42 [4096/50176]	Loss: 0.3484
Training Epoch: 42 [4608/50176]	Loss: 0.3724
Training Epoch: 42 [5120/50176]	Loss: 0.3504
Training Epoch: 42 [5632/50176]	Loss: 0.3234
Training Epoch: 42 [6144/50176]	Loss: 0.3800
Training Epoch: 42 [6656/50176]	Loss: 0.2688
Training Epoch: 42 [7168/50176]	Loss: 0.3501
Training Epoch: 42 [7680/50176]	Loss: 0.4071
Training Epoch: 42 [8192/50176]	Loss: 0.2697
Training Epoch: 42 [8704/50176]	Loss: 0.3449
Training Epoch: 42 [9216/50176]	Loss: 0.2985
Training Epoch: 42 [9728/50176]	Loss: 0.3296
Training Epoch: 42 [10240/50176]	Loss: 0.3221
Training Epoch: 42 [10752/50176]	Loss: 0.3261
Training Epoch: 42 [11264/50176]	Loss: 0.3499
Training Epoch: 42 [11776/50176]	Loss: 0.3126
Training Epoch: 42 [12288/50176]	Loss: 0.3849
Training Epoch: 42 [12800/50176]	Loss: 0.3827
Training Epoch: 42 [13312/50176]	Loss: 0.3244
Training Epoch: 42 [13824/50176]	Loss: 0.3906
Training Epoch: 42 [14336/50176]	Loss: 0.3833
Training Epoch: 42 [14848/50176]	Loss: 0.3419
Training Epoch: 42 [15360/50176]	Loss: 0.3183
Training Epoch: 42 [15872/50176]	Loss: 0.3727
Training Epoch: 42 [16384/50176]	Loss: 0.3644
Training Epoch: 42 [16896/50176]	Loss: 0.3346
Training Epoch: 42 [17408/50176]	Loss: 0.3521
Training Epoch: 42 [17920/50176]	Loss: 0.3634
Training Epoch: 42 [18432/50176]	Loss: 0.3851
Training Epoch: 42 [18944/50176]	Loss: 0.4457
Training Epoch: 42 [19456/50176]	Loss: 0.3524
Training Epoch: 42 [19968/50176]	Loss: 0.3656
Training Epoch: 42 [20480/50176]	Loss: 0.3890
Training Epoch: 42 [20992/50176]	Loss: 0.3191
Training Epoch: 42 [21504/50176]	Loss: 0.3648
Training Epoch: 42 [22016/50176]	Loss: 0.3556
Training Epoch: 42 [22528/50176]	Loss: 0.3260
Training Epoch: 42 [23040/50176]	Loss: 0.4297
Training Epoch: 42 [23552/50176]	Loss: 0.3850
Training Epoch: 42 [24064/50176]	Loss: 0.3961
Training Epoch: 42 [24576/50176]	Loss: 0.3789
Training Epoch: 42 [25088/50176]	Loss: 0.3965
Training Epoch: 42 [25600/50176]	Loss: 0.3655
Training Epoch: 42 [26112/50176]	Loss: 0.3770
Training Epoch: 42 [26624/50176]	Loss: 0.4326
Training Epoch: 42 [27136/50176]	Loss: 0.3603
Training Epoch: 42 [27648/50176]	Loss: 0.3791
Training Epoch: 42 [28160/50176]	Loss: 0.3710
Training Epoch: 42 [28672/50176]	Loss: 0.3879
Training Epoch: 42 [29184/50176]	Loss: 0.3517
Training Epoch: 42 [29696/50176]	Loss: 0.3910
Training Epoch: 42 [30208/50176]	Loss: 0.3847
Training Epoch: 42 [30720/50176]	Loss: 0.3620
Training Epoch: 42 [31232/50176]	Loss: 0.3700
Training Epoch: 42 [31744/50176]	Loss: 0.3579
Training Epoch: 42 [32256/50176]	Loss: 0.3320
Training Epoch: 42 [32768/50176]	Loss: 0.3339
Training Epoch: 42 [33280/50176]	Loss: 0.4072
Training Epoch: 42 [33792/50176]	Loss: 0.3445
Training Epoch: 42 [34304/50176]	Loss: 0.4756
Training Epoch: 42 [34816/50176]	Loss: 0.4088
Training Epoch: 42 [35328/50176]	Loss: 0.3869
Training Epoch: 42 [35840/50176]	Loss: 0.4785
Training Epoch: 42 [36352/50176]	Loss: 0.3732
Training Epoch: 42 [36864/50176]	Loss: 0.4039
Training Epoch: 42 [37376/50176]	Loss: 0.3664
Training Epoch: 42 [37888/50176]	Loss: 0.4443
Training Epoch: 42 [38400/50176]	Loss: 0.3264
Training Epoch: 42 [38912/50176]	Loss: 0.3855
Training Epoch: 42 [39424/50176]	Loss: 0.4360
Training Epoch: 42 [39936/50176]	Loss: 0.3841
Training Epoch: 42 [40448/50176]	Loss: 0.4098
Training Epoch: 42 [40960/50176]	Loss: 0.3583
Training Epoch: 42 [41472/50176]	Loss: 0.3991
Training Epoch: 42 [41984/50176]	Loss: 0.3890
Training Epoch: 42 [42496/50176]	Loss: 0.4872
Training Epoch: 42 [43008/50176]	Loss: 0.4438
Training Epoch: 42 [43520/50176]	Loss: 0.3832
Training Epoch: 42 [44032/50176]	Loss: 0.2917
Training Epoch: 42 [44544/50176]	Loss: 0.4166
Training Epoch: 42 [45056/50176]	Loss: 0.3639
Training Epoch: 42 [45568/50176]	Loss: 0.3770
Training Epoch: 42 [46080/50176]	Loss: 0.4598
Training Epoch: 42 [46592/50176]	Loss: 0.3786
Training Epoch: 42 [47104/50176]	Loss: 0.4288
Training Epoch: 42 [47616/50176]	Loss: 0.4215
Training Epoch: 42 [48128/50176]	Loss: 0.4026
Training Epoch: 42 [48640/50176]	Loss: 0.4495
Training Epoch: 42 [49152/50176]	Loss: 0.3868
Training Epoch: 42 [49664/50176]	Loss: 0.5080
Training Epoch: 42 [50176/50176]	Loss: 0.4026
Validation Epoch: 42, Average loss: 0.0047, Accuracy: 0.5543
Training Epoch: 43 [512/50176]	Loss: 0.3985
Training Epoch: 43 [1024/50176]	Loss: 0.4355
Training Epoch: 43 [1536/50176]	Loss: 0.3083
Training Epoch: 43 [2048/50176]	Loss: 0.3386
Training Epoch: 43 [2560/50176]	Loss: 0.3276
Training Epoch: 43 [3072/50176]	Loss: 0.2617
Training Epoch: 43 [3584/50176]	Loss: 0.3601
Training Epoch: 43 [4096/50176]	Loss: 0.3275
Training Epoch: 43 [4608/50176]	Loss: 0.3222
Training Epoch: 43 [5120/50176]	Loss: 0.3338
Training Epoch: 43 [5632/50176]	Loss: 0.2980
Training Epoch: 43 [6144/50176]	Loss: 0.3831
Training Epoch: 43 [6656/50176]	Loss: 0.3107
Training Epoch: 43 [7168/50176]	Loss: 0.3728
Training Epoch: 43 [7680/50176]	Loss: 0.2748
Training Epoch: 43 [8192/50176]	Loss: 0.3179
Training Epoch: 43 [8704/50176]	Loss: 0.2790
Training Epoch: 43 [9216/50176]	Loss: 0.4254
Training Epoch: 43 [9728/50176]	Loss: 0.3636
Training Epoch: 43 [10240/50176]	Loss: 0.3477
Training Epoch: 43 [10752/50176]	Loss: 0.2643
Training Epoch: 43 [11264/50176]	Loss: 0.3189
Training Epoch: 43 [11776/50176]	Loss: 0.3178
Training Epoch: 43 [12288/50176]	Loss: 0.3362
Training Epoch: 43 [12800/50176]	Loss: 0.3401
Training Epoch: 43 [13312/50176]	Loss: 0.3205
Training Epoch: 43 [13824/50176]	Loss: 0.3149
Training Epoch: 43 [14336/50176]	Loss: 0.2930
Training Epoch: 43 [14848/50176]	Loss: 0.3340
Training Epoch: 43 [15360/50176]	Loss: 0.3051
Training Epoch: 43 [15872/50176]	Loss: 0.3484
Training Epoch: 43 [16384/50176]	Loss: 0.3507
Training Epoch: 43 [16896/50176]	Loss: 0.2702
Training Epoch: 43 [17408/50176]	Loss: 0.3798
Training Epoch: 43 [17920/50176]	Loss: 0.3097
Training Epoch: 43 [18432/50176]	Loss: 0.3324
Training Epoch: 43 [18944/50176]	Loss: 0.3042
Training Epoch: 43 [19456/50176]	Loss: 0.3445
Training Epoch: 43 [19968/50176]	Loss: 0.3178
Training Epoch: 43 [20480/50176]	Loss: 0.3180
Training Epoch: 43 [20992/50176]	Loss: 0.3145
Training Epoch: 43 [21504/50176]	Loss: 0.3070
Training Epoch: 43 [22016/50176]	Loss: 0.3730
Training Epoch: 43 [22528/50176]	Loss: 0.2848
Training Epoch: 43 [23040/50176]	Loss: 0.3048
Training Epoch: 43 [23552/50176]	Loss: 0.2854
Training Epoch: 43 [24064/50176]	Loss: 0.3751
Training Epoch: 43 [24576/50176]	Loss: 0.3231
Training Epoch: 43 [25088/50176]	Loss: 0.3347
Training Epoch: 43 [25600/50176]	Loss: 0.3104
Training Epoch: 43 [26112/50176]	Loss: 0.3329
Training Epoch: 43 [26624/50176]	Loss: 0.3359
Training Epoch: 43 [27136/50176]	Loss: 0.3407
Training Epoch: 43 [27648/50176]	Loss: 0.3835
Training Epoch: 43 [28160/50176]	Loss: 0.3029
Training Epoch: 43 [28672/50176]	Loss: 0.2359
Training Epoch: 43 [29184/50176]	Loss: 0.3246
Training Epoch: 43 [29696/50176]	Loss: 0.3534
Training Epoch: 43 [30208/50176]	Loss: 0.3710
Training Epoch: 43 [30720/50176]	Loss: 0.3313
Training Epoch: 43 [31232/50176]	Loss: 0.3329
Training Epoch: 43 [31744/50176]	Loss: 0.3546
Training Epoch: 43 [32256/50176]	Loss: 0.2516
Training Epoch: 43 [32768/50176]	Loss: 0.3465
Training Epoch: 43 [33280/50176]	Loss: 0.4132
Training Epoch: 43 [33792/50176]	Loss: 0.3885
Training Epoch: 43 [34304/50176]	Loss: 0.3311
Training Epoch: 43 [34816/50176]	Loss: 0.3234
Training Epoch: 43 [35328/50176]	Loss: 0.3408
Training Epoch: 43 [35840/50176]	Loss: 0.3650
Training Epoch: 43 [36352/50176]	Loss: 0.3327
Training Epoch: 43 [36864/50176]	Loss: 0.3556
Training Epoch: 43 [37376/50176]	Loss: 0.3769
Training Epoch: 43 [37888/50176]	Loss: 0.3684
Training Epoch: 43 [38400/50176]	Loss: 0.3586
Training Epoch: 43 [38912/50176]	Loss: 0.3744
Training Epoch: 43 [39424/50176]	Loss: 0.3346
Training Epoch: 43 [39936/50176]	Loss: 0.3758
Training Epoch: 43 [40448/50176]	Loss: 0.3571
Training Epoch: 43 [40960/50176]	Loss: 0.3996
Training Epoch: 43 [41472/50176]	Loss: 0.3625
Training Epoch: 43 [41984/50176]	Loss: 0.4247
Training Epoch: 43 [42496/50176]	Loss: 0.4006
Training Epoch: 43 [43008/50176]	Loss: 0.3655
Training Epoch: 43 [43520/50176]	Loss: 0.3341
Training Epoch: 43 [44032/50176]	Loss: 0.3371
Training Epoch: 43 [44544/50176]	Loss: 0.2985
Training Epoch: 43 [45056/50176]	Loss: 0.4047
Training Epoch: 43 [45568/50176]	Loss: 0.3738
Training Epoch: 43 [46080/50176]	Loss: 0.3621
Training Epoch: 43 [46592/50176]	Loss: 0.3662
Training Epoch: 43 [47104/50176]	Loss: 0.3775
Training Epoch: 43 [47616/50176]	Loss: 0.4128
Training Epoch: 43 [48128/50176]	Loss: 0.3690
Training Epoch: 43 [48640/50176]	Loss: 0.3900
Training Epoch: 43 [49152/50176]	Loss: 0.3761
Training Epoch: 43 [49664/50176]	Loss: 0.3802
Training Epoch: 43 [50176/50176]	Loss: 0.4264
Validation Epoch: 43, Average loss: 0.0045, Accuracy: 0.5559
Training Epoch: 44 [512/50176]	Loss: 0.2830
Training Epoch: 44 [1024/50176]	Loss: 0.3264
Training Epoch: 44 [1536/50176]	Loss: 0.2953
Training Epoch: 44 [2048/50176]	Loss: 0.2809
Training Epoch: 44 [2560/50176]	Loss: 0.3170
Training Epoch: 44 [3072/50176]	Loss: 0.2590
Training Epoch: 44 [3584/50176]	Loss: 0.3147
Training Epoch: 44 [4096/50176]	Loss: 0.4120
Training Epoch: 44 [4608/50176]	Loss: 0.3150
Training Epoch: 44 [5120/50176]	Loss: 0.2893
Training Epoch: 44 [5632/50176]	Loss: 0.3832
Training Epoch: 44 [6144/50176]	Loss: 0.2844
Training Epoch: 44 [6656/50176]	Loss: 0.2678
Training Epoch: 44 [7168/50176]	Loss: 0.3207
Training Epoch: 44 [7680/50176]	Loss: 0.2869
Training Epoch: 44 [8192/50176]	Loss: 0.3534
Training Epoch: 44 [8704/50176]	Loss: 0.3203
Training Epoch: 44 [9216/50176]	Loss: 0.2754
Training Epoch: 44 [9728/50176]	Loss: 0.3635
Training Epoch: 44 [10240/50176]	Loss: 0.2641
Training Epoch: 44 [10752/50176]	Loss: 0.3508
Training Epoch: 44 [11264/50176]	Loss: 0.2656
Training Epoch: 44 [11776/50176]	Loss: 0.3241
Training Epoch: 44 [12288/50176]	Loss: 0.3030
Training Epoch: 44 [12800/50176]	Loss: 0.3116
Training Epoch: 44 [13312/50176]	Loss: 0.2605
Training Epoch: 44 [13824/50176]	Loss: 0.3123
Training Epoch: 44 [14336/50176]	Loss: 0.2980
Training Epoch: 44 [14848/50176]	Loss: 0.3508
Training Epoch: 44 [15360/50176]	Loss: 0.3624
Training Epoch: 44 [15872/50176]	Loss: 0.3281
Training Epoch: 44 [16384/50176]	Loss: 0.3344
Training Epoch: 44 [16896/50176]	Loss: 0.2180
Training Epoch: 44 [17408/50176]	Loss: 0.3016
Training Epoch: 44 [17920/50176]	Loss: 0.3142
Training Epoch: 44 [18432/50176]	Loss: 0.3500
Training Epoch: 44 [18944/50176]	Loss: 0.3411
Training Epoch: 44 [19456/50176]	Loss: 0.3028
Training Epoch: 44 [19968/50176]	Loss: 0.3246
Training Epoch: 44 [20480/50176]	Loss: 0.3376
Training Epoch: 44 [20992/50176]	Loss: 0.3490
Training Epoch: 44 [21504/50176]	Loss: 0.3325
Training Epoch: 44 [22016/50176]	Loss: 0.3512
Training Epoch: 44 [22528/50176]	Loss: 0.3408
Training Epoch: 44 [23040/50176]	Loss: 0.3751
Training Epoch: 44 [23552/50176]	Loss: 0.2879
Training Epoch: 44 [24064/50176]	Loss: 0.3765
Training Epoch: 44 [24576/50176]	Loss: 0.3024
Training Epoch: 44 [25088/50176]	Loss: 0.3866
Training Epoch: 44 [25600/50176]	Loss: 0.3461
Training Epoch: 44 [26112/50176]	Loss: 0.3772
Training Epoch: 44 [26624/50176]	Loss: 0.3947
Training Epoch: 44 [27136/50176]	Loss: 0.3962
Training Epoch: 44 [27648/50176]	Loss: 0.3630
Training Epoch: 44 [28160/50176]	Loss: 0.3258
Training Epoch: 44 [28672/50176]	Loss: 0.3210
Training Epoch: 44 [29184/50176]	Loss: 0.3509
Training Epoch: 44 [29696/50176]	Loss: 0.3460
Training Epoch: 44 [30208/50176]	Loss: 0.3624
Training Epoch: 44 [30720/50176]	Loss: 0.3423
Training Epoch: 44 [31232/50176]	Loss: 0.3444
Training Epoch: 44 [31744/50176]	Loss: 0.3620
Training Epoch: 44 [32256/50176]	Loss: 0.3299
Training Epoch: 44 [32768/50176]	Loss: 0.3314
Training Epoch: 44 [33280/50176]	Loss: 0.3450
Training Epoch: 44 [33792/50176]	Loss: 0.3587
Training Epoch: 44 [34304/50176]	Loss: 0.3587
Training Epoch: 44 [34816/50176]	Loss: 0.2819
Training Epoch: 44 [35328/50176]	Loss: 0.2922
Training Epoch: 44 [35840/50176]	Loss: 0.3613
Training Epoch: 44 [36352/50176]	Loss: 0.3861
Training Epoch: 44 [36864/50176]	Loss: 0.3097
Training Epoch: 44 [37376/50176]	Loss: 0.3806
Training Epoch: 44 [37888/50176]	Loss: 0.3074
Training Epoch: 44 [38400/50176]	Loss: 0.4339
Training Epoch: 44 [38912/50176]	Loss: 0.4166
Training Epoch: 44 [39424/50176]	Loss: 0.3927
Training Epoch: 44 [39936/50176]	Loss: 0.3457
Training Epoch: 44 [40448/50176]	Loss: 0.3516
Training Epoch: 44 [40960/50176]	Loss: 0.2700
Training Epoch: 44 [41472/50176]	Loss: 0.3408
Training Epoch: 44 [41984/50176]	Loss: 0.4316
Training Epoch: 44 [42496/50176]	Loss: 0.4246
Training Epoch: 44 [43008/50176]	Loss: 0.3603
Training Epoch: 44 [43520/50176]	Loss: 0.4676
Training Epoch: 44 [44032/50176]	Loss: 0.4112
Training Epoch: 44 [44544/50176]	Loss: 0.3693
Training Epoch: 44 [45056/50176]	Loss: 0.3645
Training Epoch: 44 [45568/50176]	Loss: 0.4263
Training Epoch: 44 [46080/50176]	Loss: 0.4181
Training Epoch: 44 [46592/50176]	Loss: 0.3655
Training Epoch: 44 [47104/50176]	Loss: 0.4396
Training Epoch: 44 [47616/50176]	Loss: 0.3831
Training Epoch: 44 [48128/50176]	Loss: 0.3860
Training Epoch: 44 [48640/50176]	Loss: 0.3923
Training Epoch: 44 [49152/50176]	Loss: 0.3654
Training Epoch: 44 [49664/50176]	Loss: 0.3667
Training Epoch: 44 [50176/50176]	Loss: 0.3935
Validation Epoch: 44, Average loss: 0.0052, Accuracy: 0.5570
Training Epoch: 45 [512/50176]	Loss: 0.2922
Training Epoch: 45 [1024/50176]	Loss: 0.2694
Training Epoch: 45 [1536/50176]	Loss: 0.3229
Training Epoch: 45 [2048/50176]	Loss: 0.3194
Training Epoch: 45 [2560/50176]	Loss: 0.3167
Training Epoch: 45 [3072/50176]	Loss: 0.2788
Training Epoch: 45 [3584/50176]	Loss: 0.3553
Training Epoch: 45 [4096/50176]	Loss: 0.2941
Training Epoch: 45 [4608/50176]	Loss: 0.3235
Training Epoch: 45 [5120/50176]	Loss: 0.2528
Training Epoch: 45 [5632/50176]	Loss: 0.3395
Training Epoch: 45 [6144/50176]	Loss: 0.2919
Training Epoch: 45 [6656/50176]	Loss: 0.3532
Training Epoch: 45 [7168/50176]	Loss: 0.2938
Training Epoch: 45 [7680/50176]	Loss: 0.2686
Training Epoch: 45 [8192/50176]	Loss: 0.2967
Training Epoch: 45 [8704/50176]	Loss: 0.2929
Training Epoch: 45 [9216/50176]	Loss: 0.3288
Training Epoch: 45 [9728/50176]	Loss: 0.2890
Training Epoch: 45 [10240/50176]	Loss: 0.3173
Training Epoch: 45 [10752/50176]	Loss: 0.3630
Training Epoch: 45 [11264/50176]	Loss: 0.2960
Training Epoch: 45 [11776/50176]	Loss: 0.3165
Training Epoch: 45 [12288/50176]	Loss: 0.3289
Training Epoch: 45 [12800/50176]	Loss: 0.2997
Training Epoch: 45 [13312/50176]	Loss: 0.3584
Training Epoch: 45 [13824/50176]	Loss: 0.2639
Training Epoch: 45 [14336/50176]	Loss: 0.2895
Training Epoch: 45 [14848/50176]	Loss: 0.3697
Training Epoch: 45 [15360/50176]	Loss: 0.3550
Training Epoch: 45 [15872/50176]	Loss: 0.2724
Training Epoch: 45 [16384/50176]	Loss: 0.3436
Training Epoch: 45 [16896/50176]	Loss: 0.3167
Training Epoch: 45 [17408/50176]	Loss: 0.2417
Training Epoch: 45 [17920/50176]	Loss: 0.3674
Training Epoch: 45 [18432/50176]	Loss: 0.2998
Training Epoch: 45 [18944/50176]	Loss: 0.3670
Training Epoch: 45 [19456/50176]	Loss: 0.3273
Training Epoch: 45 [19968/50176]	Loss: 0.2875
Training Epoch: 45 [20480/50176]	Loss: 0.2966
Training Epoch: 45 [20992/50176]	Loss: 0.3297
Training Epoch: 45 [21504/50176]	Loss: 0.3057
Training Epoch: 45 [22016/50176]	Loss: 0.3131
Training Epoch: 45 [22528/50176]	Loss: 0.2796
Training Epoch: 45 [23040/50176]	Loss: 0.3272
Training Epoch: 45 [23552/50176]	Loss: 0.3392
Training Epoch: 45 [24064/50176]	Loss: 0.2953
Training Epoch: 45 [24576/50176]	Loss: 0.3843
Training Epoch: 45 [25088/50176]	Loss: 0.3245
Training Epoch: 45 [25600/50176]	Loss: 0.3689
Training Epoch: 45 [26112/50176]	Loss: 0.2971
Training Epoch: 45 [26624/50176]	Loss: 0.3227
Training Epoch: 45 [27136/50176]	Loss: 0.3113
Training Epoch: 45 [27648/50176]	Loss: 0.2837
Training Epoch: 45 [28160/50176]	Loss: 0.3325
Training Epoch: 45 [28672/50176]	Loss: 0.3560
Training Epoch: 45 [29184/50176]	Loss: 0.3371
Training Epoch: 45 [29696/50176]	Loss: 0.2821
Training Epoch: 45 [30208/50176]	Loss: 0.3049
Training Epoch: 45 [30720/50176]	Loss: 0.3598
Training Epoch: 45 [31232/50176]	Loss: 0.3005
Training Epoch: 45 [31744/50176]	Loss: 0.3656
Training Epoch: 45 [32256/50176]	Loss: 0.3517
Training Epoch: 45 [32768/50176]	Loss: 0.3430
Training Epoch: 45 [33280/50176]	Loss: 0.3418
Training Epoch: 45 [33792/50176]	Loss: 0.3249
Training Epoch: 45 [34304/50176]	Loss: 0.3493
Training Epoch: 45 [34816/50176]	Loss: 0.3078
Training Epoch: 45 [35328/50176]	Loss: 0.3072
Training Epoch: 45 [35840/50176]	Loss: 0.3401
Training Epoch: 45 [36352/50176]	Loss: 0.3601
Training Epoch: 45 [36864/50176]	Loss: 0.2955
Training Epoch: 45 [37376/50176]	Loss: 0.3163
Training Epoch: 45 [37888/50176]	Loss: 0.3737
Training Epoch: 45 [38400/50176]	Loss: 0.3691
Training Epoch: 45 [38912/50176]	Loss: 0.3400
Training Epoch: 45 [39424/50176]	Loss: 0.3256
Training Epoch: 45 [39936/50176]	Loss: 0.2902
Training Epoch: 45 [40448/50176]	Loss: 0.4066
Training Epoch: 45 [40960/50176]	Loss: 0.3384
Training Epoch: 45 [41472/50176]	Loss: 0.3601
Training Epoch: 45 [41984/50176]	Loss: 0.4007
Training Epoch: 45 [42496/50176]	Loss: 0.3067
Training Epoch: 45 [43008/50176]	Loss: 0.4085
Training Epoch: 45 [43520/50176]	Loss: 0.3452
Training Epoch: 45 [44032/50176]	Loss: 0.4103
Training Epoch: 45 [44544/50176]	Loss: 0.4383
Training Epoch: 45 [45056/50176]	Loss: 0.3203
Training Epoch: 45 [45568/50176]	Loss: 0.2926
Training Epoch: 45 [46080/50176]	Loss: 0.3233
Training Epoch: 45 [46592/50176]	Loss: 0.4356
Training Epoch: 45 [47104/50176]	Loss: 0.3101
Training Epoch: 45 [47616/50176]	Loss: 0.4539
Training Epoch: 45 [48128/50176]	Loss: 0.3844
Training Epoch: 45 [48640/50176]	Loss: 0.4027
Training Epoch: 45 [49152/50176]	Loss: 0.3005
Training Epoch: 45 [49664/50176]	Loss: 0.4119
Training Epoch: 45 [50176/50176]	Loss: 0.3839
Validation Epoch: 45, Average loss: 0.0048, Accuracy: 0.5595
Training Epoch: 46 [512/50176]	Loss: 0.3090
Training Epoch: 46 [1024/50176]	Loss: 0.2765
Training Epoch: 46 [1536/50176]	Loss: 0.2967
Training Epoch: 46 [2048/50176]	Loss: 0.2584
Training Epoch: 46 [2560/50176]	Loss: 0.2708
Training Epoch: 46 [3072/50176]	Loss: 0.2865
Training Epoch: 46 [3584/50176]	Loss: 0.3395
Training Epoch: 46 [4096/50176]	Loss: 0.2811
Training Epoch: 46 [4608/50176]	Loss: 0.3056
Training Epoch: 46 [5120/50176]	Loss: 0.2757
Training Epoch: 46 [5632/50176]	Loss: 0.2974
Training Epoch: 46 [6144/50176]	Loss: 0.2559
Training Epoch: 46 [6656/50176]	Loss: 0.3053
Training Epoch: 46 [7168/50176]	Loss: 0.3066
Training Epoch: 46 [7680/50176]	Loss: 0.2861
Training Epoch: 46 [8192/50176]	Loss: 0.3303
Training Epoch: 46 [8704/50176]	Loss: 0.3527
Training Epoch: 46 [9216/50176]	Loss: 0.3235
Training Epoch: 46 [9728/50176]	Loss: 0.2869
Training Epoch: 46 [10240/50176]	Loss: 0.3177
Training Epoch: 46 [10752/50176]	Loss: 0.3271
Training Epoch: 46 [11264/50176]	Loss: 0.2922
Training Epoch: 46 [11776/50176]	Loss: 0.2602
Training Epoch: 46 [12288/50176]	Loss: 0.2929
Training Epoch: 46 [12800/50176]	Loss: 0.2827
Training Epoch: 46 [13312/50176]	Loss: 0.2616
Training Epoch: 46 [13824/50176]	Loss: 0.2964
Training Epoch: 46 [14336/50176]	Loss: 0.3030
Training Epoch: 46 [14848/50176]	Loss: 0.3411
Training Epoch: 46 [15360/50176]	Loss: 0.3092
Training Epoch: 46 [15872/50176]	Loss: 0.2575
Training Epoch: 46 [16384/50176]	Loss: 0.2882
Training Epoch: 46 [16896/50176]	Loss: 0.3488
Training Epoch: 46 [17408/50176]	Loss: 0.3659
Training Epoch: 46 [17920/50176]	Loss: 0.3125
Training Epoch: 46 [18432/50176]	Loss: 0.3405
Training Epoch: 46 [18944/50176]	Loss: 0.2914
Training Epoch: 46 [19456/50176]	Loss: 0.3105
Training Epoch: 46 [19968/50176]	Loss: 0.2693
Training Epoch: 46 [20480/50176]	Loss: 0.3252
Training Epoch: 46 [20992/50176]	Loss: 0.3340
Training Epoch: 46 [21504/50176]	Loss: 0.2968
Training Epoch: 46 [22016/50176]	Loss: 0.3002
Training Epoch: 46 [22528/50176]	Loss: 0.2841
Training Epoch: 46 [23040/50176]	Loss: 0.3063
Training Epoch: 46 [23552/50176]	Loss: 0.3509
Training Epoch: 46 [24064/50176]	Loss: 0.2815
Training Epoch: 46 [24576/50176]	Loss: 0.3296
Training Epoch: 46 [25088/50176]	Loss: 0.3589
Training Epoch: 46 [25600/50176]	Loss: 0.2897
Training Epoch: 46 [26112/50176]	Loss: 0.3521
Training Epoch: 46 [26624/50176]	Loss: 0.3700
Training Epoch: 46 [27136/50176]	Loss: 0.2866
Training Epoch: 46 [27648/50176]	Loss: 0.3004
Training Epoch: 46 [28160/50176]	Loss: 0.3415
Training Epoch: 46 [28672/50176]	Loss: 0.3160
Training Epoch: 46 [29184/50176]	Loss: 0.2685
Training Epoch: 46 [29696/50176]	Loss: 0.3226
Training Epoch: 46 [30208/50176]	Loss: 0.3259
Training Epoch: 46 [30720/50176]	Loss: 0.2785
Training Epoch: 46 [31232/50176]	Loss: 0.2922
Training Epoch: 46 [31744/50176]	Loss: 0.3328
Training Epoch: 46 [32256/50176]	Loss: 0.2915
Training Epoch: 46 [32768/50176]	Loss: 0.3599
Training Epoch: 46 [33280/50176]	Loss: 0.3113
Training Epoch: 46 [33792/50176]	Loss: 0.3488
Training Epoch: 46 [34304/50176]	Loss: 0.3269
Training Epoch: 46 [34816/50176]	Loss: 0.3399
Training Epoch: 46 [35328/50176]	Loss: 0.3257
Training Epoch: 46 [35840/50176]	Loss: 0.2488
Training Epoch: 46 [36352/50176]	Loss: 0.3241
Training Epoch: 46 [36864/50176]	Loss: 0.3330
Training Epoch: 46 [37376/50176]	Loss: 0.3094
Training Epoch: 46 [37888/50176]	Loss: 0.3418
Training Epoch: 46 [38400/50176]	Loss: 0.2867
Training Epoch: 46 [38912/50176]	Loss: 0.3062
Training Epoch: 46 [39424/50176]	Loss: 0.3325
Training Epoch: 46 [39936/50176]	Loss: 0.2870
Training Epoch: 46 [40448/50176]	Loss: 0.3309
Training Epoch: 46 [40960/50176]	Loss: 0.3774
Training Epoch: 46 [41472/50176]	Loss: 0.3708
Training Epoch: 46 [41984/50176]	Loss: 0.3791
Training Epoch: 46 [42496/50176]	Loss: 0.3064
Training Epoch: 46 [43008/50176]	Loss: 0.3965
Training Epoch: 46 [43520/50176]	Loss: 0.3729
Training Epoch: 46 [44032/50176]	Loss: 0.3206
Training Epoch: 46 [44544/50176]	Loss: 0.3357
Training Epoch: 46 [45056/50176]	Loss: 0.3236
Training Epoch: 46 [45568/50176]	Loss: 0.3752
Training Epoch: 46 [46080/50176]	Loss: 0.3323
Training Epoch: 46 [46592/50176]	Loss: 0.4110
Training Epoch: 46 [47104/50176]	Loss: 0.3806
Training Epoch: 46 [47616/50176]	Loss: 0.2924
Training Epoch: 46 [48128/50176]	Loss: 0.3118
Training Epoch: 46 [48640/50176]	Loss: 0.3400
Training Epoch: 46 [49152/50176]	Loss: 0.3184
Training Epoch: 46 [49664/50176]	Loss: 0.3522
Training Epoch: 46 [50176/50176]	Loss: 0.3178
Validation Epoch: 46, Average loss: 0.0046, Accuracy: 0.5797
Training Epoch: 47 [512/50176]	Loss: 0.2832
Training Epoch: 47 [1024/50176]	Loss: 0.2608
Training Epoch: 47 [1536/50176]	Loss: 0.2909
Training Epoch: 47 [2048/50176]	Loss: 0.2792
Training Epoch: 47 [2560/50176]	Loss: 0.2233
Training Epoch: 47 [3072/50176]	Loss: 0.3325
Training Epoch: 47 [3584/50176]	Loss: 0.3142
Training Epoch: 47 [4096/50176]	Loss: 0.2759
Training Epoch: 47 [4608/50176]	Loss: 0.2263
Training Epoch: 47 [5120/50176]	Loss: 0.2431
Training Epoch: 47 [5632/50176]	Loss: 0.2814
Training Epoch: 47 [6144/50176]	Loss: 0.2986
Training Epoch: 47 [6656/50176]	Loss: 0.2648
Training Epoch: 47 [7168/50176]	Loss: 0.2762
Training Epoch: 47 [7680/50176]	Loss: 0.2764
Training Epoch: 47 [8192/50176]	Loss: 0.2467
Training Epoch: 47 [8704/50176]	Loss: 0.2466
Training Epoch: 47 [9216/50176]	Loss: 0.2601
Training Epoch: 47 [9728/50176]	Loss: 0.3341
Training Epoch: 47 [10240/50176]	Loss: 0.2838
Training Epoch: 47 [10752/50176]	Loss: 0.2728
Training Epoch: 47 [11264/50176]	Loss: 0.2720
Training Epoch: 47 [11776/50176]	Loss: 0.2966
Training Epoch: 47 [12288/50176]	Loss: 0.2437
Training Epoch: 47 [12800/50176]	Loss: 0.2870
Training Epoch: 47 [13312/50176]	Loss: 0.3286
Training Epoch: 47 [13824/50176]	Loss: 0.3146
Training Epoch: 47 [14336/50176]	Loss: 0.2488
Training Epoch: 47 [14848/50176]	Loss: 0.2901
Training Epoch: 47 [15360/50176]	Loss: 0.2692
Training Epoch: 47 [15872/50176]	Loss: 0.2970
Training Epoch: 47 [16384/50176]	Loss: 0.2389
Training Epoch: 47 [16896/50176]	Loss: 0.2071
Training Epoch: 47 [17408/50176]	Loss: 0.3236
Training Epoch: 47 [17920/50176]	Loss: 0.3195
Training Epoch: 47 [18432/50176]	Loss: 0.2326
Training Epoch: 47 [18944/50176]	Loss: 0.2770
Training Epoch: 47 [19456/50176]	Loss: 0.2749
Training Epoch: 47 [19968/50176]	Loss: 0.2422
Training Epoch: 47 [20480/50176]	Loss: 0.2687
Training Epoch: 47 [20992/50176]	Loss: 0.2858
Training Epoch: 47 [21504/50176]	Loss: 0.2535
Training Epoch: 47 [22016/50176]	Loss: 0.2953
Training Epoch: 47 [22528/50176]	Loss: 0.2804
Training Epoch: 47 [23040/50176]	Loss: 0.3019
Training Epoch: 47 [23552/50176]	Loss: 0.3184
Training Epoch: 47 [24064/50176]	Loss: 0.3203
Training Epoch: 47 [24576/50176]	Loss: 0.3036
Training Epoch: 47 [25088/50176]	Loss: 0.3269
Training Epoch: 47 [25600/50176]	Loss: 0.2646
Training Epoch: 47 [26112/50176]	Loss: 0.2739
Training Epoch: 47 [26624/50176]	Loss: 0.3227
Training Epoch: 47 [27136/50176]	Loss: 0.2728
Training Epoch: 47 [27648/50176]	Loss: 0.2859
Training Epoch: 47 [28160/50176]	Loss: 0.2651
Training Epoch: 47 [28672/50176]	Loss: 0.3176
Training Epoch: 47 [29184/50176]	Loss: 0.3168
Training Epoch: 47 [29696/50176]	Loss: 0.3172
Training Epoch: 47 [30208/50176]	Loss: 0.3213
Training Epoch: 47 [30720/50176]	Loss: 0.3091
Training Epoch: 47 [31232/50176]	Loss: 0.2808
Training Epoch: 47 [31744/50176]	Loss: 0.2834
Training Epoch: 47 [32256/50176]	Loss: 0.3601
Training Epoch: 47 [32768/50176]	Loss: 0.3158
Training Epoch: 47 [33280/50176]	Loss: 0.3361
Training Epoch: 47 [33792/50176]	Loss: 0.3087
Training Epoch: 47 [34304/50176]	Loss: 0.2646
Training Epoch: 47 [34816/50176]	Loss: 0.3777
Training Epoch: 47 [35328/50176]	Loss: 0.2461
Training Epoch: 47 [35840/50176]	Loss: 0.2800
Training Epoch: 47 [36352/50176]	Loss: 0.2783
Training Epoch: 47 [36864/50176]	Loss: 0.3438
Training Epoch: 47 [37376/50176]	Loss: 0.2551
Training Epoch: 47 [37888/50176]	Loss: 0.3359
Training Epoch: 47 [38400/50176]	Loss: 0.2841
Training Epoch: 47 [38912/50176]	Loss: 0.2905
Training Epoch: 47 [39424/50176]	Loss: 0.3096
Training Epoch: 47 [39936/50176]	Loss: 0.3377
Training Epoch: 47 [40448/50176]	Loss: 0.3306
Training Epoch: 47 [40960/50176]	Loss: 0.3492
Training Epoch: 47 [41472/50176]	Loss: 0.3450
Training Epoch: 47 [41984/50176]	Loss: 0.3291
Training Epoch: 47 [42496/50176]	Loss: 0.3027
Training Epoch: 47 [43008/50176]	Loss: 0.2921
Training Epoch: 47 [43520/50176]	Loss: 0.3043
Training Epoch: 47 [44032/50176]	Loss: 0.4023
Training Epoch: 47 [44544/50176]	Loss: 0.3683
Training Epoch: 47 [45056/50176]	Loss: 0.3693
Training Epoch: 47 [45568/50176]	Loss: 0.3637
Training Epoch: 47 [46080/50176]	Loss: 0.3689
Training Epoch: 47 [46592/50176]	Loss: 0.2785
Training Epoch: 47 [47104/50176]	Loss: 0.3496
Training Epoch: 47 [47616/50176]	Loss: 0.3458
Training Epoch: 47 [48128/50176]	Loss: 0.3900
Training Epoch: 47 [48640/50176]	Loss: 0.4063
Training Epoch: 47 [49152/50176]	Loss: 0.4091
Training Epoch: 47 [49664/50176]	Loss: 0.3634
Training Epoch: 47 [50176/50176]	Loss: 0.3577
Validation Epoch: 47, Average loss: 0.0046, Accuracy: 0.5877
Training Epoch: 48 [512/50176]	Loss: 0.2475
Training Epoch: 48 [1024/50176]	Loss: 0.3647
Training Epoch: 48 [1536/50176]	Loss: 0.2802
Training Epoch: 48 [2048/50176]	Loss: 0.2511
Training Epoch: 48 [2560/50176]	Loss: 0.2943
Training Epoch: 48 [3072/50176]	Loss: 0.2402
Training Epoch: 48 [3584/50176]	Loss: 0.3668
Training Epoch: 48 [4096/50176]	Loss: 0.3332
Training Epoch: 48 [4608/50176]	Loss: 0.2978
Training Epoch: 48 [5120/50176]	Loss: 0.2283
Training Epoch: 48 [5632/50176]	Loss: 0.2571
Training Epoch: 48 [6144/50176]	Loss: 0.2999
Training Epoch: 48 [6656/50176]	Loss: 0.3263
Training Epoch: 48 [7168/50176]	Loss: 0.2770
Training Epoch: 48 [7680/50176]	Loss: 0.2785
Training Epoch: 48 [8192/50176]	Loss: 0.2972
Training Epoch: 48 [8704/50176]	Loss: 0.3191
Training Epoch: 48 [9216/50176]	Loss: 0.2716
Training Epoch: 48 [9728/50176]	Loss: 0.2042
Training Epoch: 48 [10240/50176]	Loss: 0.2648
Training Epoch: 48 [10752/50176]	Loss: 0.2658
Training Epoch: 48 [11264/50176]	Loss: 0.2662
Training Epoch: 48 [11776/50176]	Loss: 0.3129
Training Epoch: 48 [12288/50176]	Loss: 0.2591
Training Epoch: 48 [12800/50176]	Loss: 0.2334
Training Epoch: 48 [13312/50176]	Loss: 0.2974
Training Epoch: 48 [13824/50176]	Loss: 0.2571
Training Epoch: 48 [14336/50176]	Loss: 0.2628
Training Epoch: 48 [14848/50176]	Loss: 0.2522
Training Epoch: 48 [15360/50176]	Loss: 0.2809
Training Epoch: 48 [15872/50176]	Loss: 0.3099
Training Epoch: 48 [16384/50176]	Loss: 0.2850
Training Epoch: 48 [16896/50176]	Loss: 0.2588
Training Epoch: 48 [17408/50176]	Loss: 0.3216
Training Epoch: 48 [17920/50176]	Loss: 0.2868
Training Epoch: 48 [18432/50176]	Loss: 0.2564
Training Epoch: 48 [18944/50176]	Loss: 0.2513
Training Epoch: 48 [19456/50176]	Loss: 0.2545
Training Epoch: 48 [19968/50176]	Loss: 0.2752
Training Epoch: 48 [20480/50176]	Loss: 0.2941
Training Epoch: 48 [20992/50176]	Loss: 0.2914
Training Epoch: 48 [21504/50176]	Loss: 0.2856
Training Epoch: 48 [22016/50176]	Loss: 0.2719
Training Epoch: 48 [22528/50176]	Loss: 0.3069
Training Epoch: 48 [23040/50176]	Loss: 0.2878
Training Epoch: 48 [23552/50176]	Loss: 0.2261
Training Epoch: 48 [24064/50176]	Loss: 0.2607
Training Epoch: 48 [24576/50176]	Loss: 0.2789
Training Epoch: 48 [25088/50176]	Loss: 0.2902
Training Epoch: 48 [25600/50176]	Loss: 0.2759
Training Epoch: 48 [26112/50176]	Loss: 0.2731
Training Epoch: 48 [26624/50176]	Loss: 0.3124
Training Epoch: 48 [27136/50176]	Loss: 0.2717
Training Epoch: 48 [27648/50176]	Loss: 0.3181
Training Epoch: 48 [28160/50176]	Loss: 0.3062
Training Epoch: 48 [28672/50176]	Loss: 0.3011
Training Epoch: 48 [29184/50176]	Loss: 0.2953
Training Epoch: 48 [29696/50176]	Loss: 0.3154
Training Epoch: 48 [30208/50176]	Loss: 0.3091
Training Epoch: 48 [30720/50176]	Loss: 0.2939
Training Epoch: 48 [31232/50176]	Loss: 0.2276
Training Epoch: 48 [31744/50176]	Loss: 0.3097
Training Epoch: 48 [32256/50176]	Loss: 0.3090
Training Epoch: 48 [32768/50176]	Loss: 0.2636
Training Epoch: 48 [33280/50176]	Loss: 0.3154
Training Epoch: 48 [33792/50176]	Loss: 0.2994
Training Epoch: 48 [34304/50176]	Loss: 0.3396
Training Epoch: 48 [34816/50176]	Loss: 0.3002
Training Epoch: 48 [35328/50176]	Loss: 0.2756
Training Epoch: 48 [35840/50176]	Loss: 0.2738
Training Epoch: 48 [36352/50176]	Loss: 0.2983
Training Epoch: 48 [36864/50176]	Loss: 0.2858
Training Epoch: 48 [37376/50176]	Loss: 0.3033
Training Epoch: 48 [37888/50176]	Loss: 0.3653
Training Epoch: 48 [38400/50176]	Loss: 0.2219
Training Epoch: 48 [38912/50176]	Loss: 0.2954
Training Epoch: 48 [39424/50176]	Loss: 0.2901
Training Epoch: 48 [39936/50176]	Loss: 0.3499
Training Epoch: 48 [40448/50176]	Loss: 0.3020
Training Epoch: 48 [40960/50176]	Loss: 0.3326
Training Epoch: 48 [41472/50176]	Loss: 0.3123
Training Epoch: 48 [41984/50176]	Loss: 0.2841
Training Epoch: 48 [42496/50176]	Loss: 0.2884
Training Epoch: 48 [43008/50176]	Loss: 0.3434
Training Epoch: 48 [43520/50176]	Loss: 0.3586
Training Epoch: 48 [44032/50176]	Loss: 0.2863
Training Epoch: 48 [44544/50176]	Loss: 0.2704
Training Epoch: 48 [45056/50176]	Loss: 0.3307
Training Epoch: 48 [45568/50176]	Loss: 0.2561
Training Epoch: 48 [46080/50176]	Loss: 0.2615
Training Epoch: 48 [46592/50176]	Loss: 0.2452
Training Epoch: 48 [47104/50176]	Loss: 0.3384
Training Epoch: 48 [47616/50176]	Loss: 0.3136
Training Epoch: 48 [48128/50176]	Loss: 0.3839
Training Epoch: 48 [48640/50176]	Loss: 0.3743
Training Epoch: 48 [49152/50176]	Loss: 0.3623
Training Epoch: 48 [49664/50176]	Loss: 0.3195
Training Epoch: 48 [50176/50176]	Loss: 0.2682
Validation Epoch: 48, Average loss: 0.0047, Accuracy: 0.5727
Training Epoch: 49 [512/50176]	Loss: 0.2399
Training Epoch: 49 [1024/50176]	Loss: 0.2365
Training Epoch: 49 [1536/50176]	Loss: 0.2613
Training Epoch: 49 [2048/50176]	Loss: 0.2815
Training Epoch: 49 [2560/50176]	Loss: 0.2545
Training Epoch: 49 [3072/50176]	Loss: 0.2441
Training Epoch: 49 [3584/50176]	Loss: 0.2131
Training Epoch: 49 [4096/50176]	Loss: 0.2218
Training Epoch: 49 [4608/50176]	Loss: 0.2133
Training Epoch: 49 [5120/50176]	Loss: 0.2341
Training Epoch: 49 [5632/50176]	Loss: 0.2578
Training Epoch: 49 [6144/50176]	Loss: 0.2647
Training Epoch: 49 [6656/50176]	Loss: 0.2274
Training Epoch: 49 [7168/50176]	Loss: 0.2570
Training Epoch: 49 [7680/50176]	Loss: 0.3172
Training Epoch: 49 [8192/50176]	Loss: 0.2902
Training Epoch: 49 [8704/50176]	Loss: 0.2634
Training Epoch: 49 [9216/50176]	Loss: 0.2796
Training Epoch: 49 [9728/50176]	Loss: 0.2624
Training Epoch: 49 [10240/50176]	Loss: 0.2545
Training Epoch: 49 [10752/50176]	Loss: 0.2369
Training Epoch: 49 [11264/50176]	Loss: 0.2535
Training Epoch: 49 [11776/50176]	Loss: 0.2596
Training Epoch: 49 [12288/50176]	Loss: 0.2403
Training Epoch: 49 [12800/50176]	Loss: 0.2306
Training Epoch: 49 [13312/50176]	Loss: 0.2229
Training Epoch: 49 [13824/50176]	Loss: 0.2749
Training Epoch: 49 [14336/50176]	Loss: 0.2806
Training Epoch: 49 [14848/50176]	Loss: 0.2306
Training Epoch: 49 [15360/50176]	Loss: 0.1975
Training Epoch: 49 [15872/50176]	Loss: 0.2664
Training Epoch: 49 [16384/50176]	Loss: 0.2762
Training Epoch: 49 [16896/50176]	Loss: 0.2759
Training Epoch: 49 [17408/50176]	Loss: 0.2776
Training Epoch: 49 [17920/50176]	Loss: 0.2518
Training Epoch: 49 [18432/50176]	Loss: 0.3424
Training Epoch: 49 [18944/50176]	Loss: 0.2091
Training Epoch: 49 [19456/50176]	Loss: 0.3124
Training Epoch: 49 [19968/50176]	Loss: 0.2149
Training Epoch: 49 [20480/50176]	Loss: 0.2257
Training Epoch: 49 [20992/50176]	Loss: 0.2091
Training Epoch: 49 [21504/50176]	Loss: 0.2745
Training Epoch: 49 [22016/50176]	Loss: 0.2760
Training Epoch: 49 [22528/50176]	Loss: 0.3141
Training Epoch: 49 [23040/50176]	Loss: 0.2074
Training Epoch: 49 [23552/50176]	Loss: 0.3040
Training Epoch: 49 [24064/50176]	Loss: 0.2838
Training Epoch: 49 [24576/50176]	Loss: 0.2788
Training Epoch: 49 [25088/50176]	Loss: 0.3021
Training Epoch: 49 [25600/50176]	Loss: 0.2735
Training Epoch: 49 [26112/50176]	Loss: 0.2050
Training Epoch: 49 [26624/50176]	Loss: 0.2788
Training Epoch: 49 [27136/50176]	Loss: 0.3170
Training Epoch: 49 [27648/50176]	Loss: 0.3302
Training Epoch: 49 [28160/50176]	Loss: 0.2266
Training Epoch: 49 [28672/50176]	Loss: 0.2966
Training Epoch: 49 [29184/50176]	Loss: 0.3090
Training Epoch: 49 [29696/50176]	Loss: 0.2753
Training Epoch: 49 [30208/50176]	Loss: 0.3238
Training Epoch: 49 [30720/50176]	Loss: 0.2591
Training Epoch: 49 [31232/50176]	Loss: 0.2536
Training Epoch: 49 [31744/50176]	Loss: 0.3035
Training Epoch: 49 [32256/50176]	Loss: 0.2949
Training Epoch: 49 [32768/50176]	Loss: 0.2951
Training Epoch: 49 [33280/50176]	Loss: 0.2955
Training Epoch: 49 [33792/50176]	Loss: 0.2616
Training Epoch: 49 [34304/50176]	Loss: 0.2911
Training Epoch: 49 [34816/50176]	Loss: 0.3770
Training Epoch: 49 [35328/50176]	Loss: 0.2538
Training Epoch: 49 [35840/50176]	Loss: 0.2949
Training Epoch: 49 [36352/50176]	Loss: 0.3114
Training Epoch: 49 [36864/50176]	Loss: 0.3361
Training Epoch: 49 [37376/50176]	Loss: 0.2560
Training Epoch: 49 [37888/50176]	Loss: 0.2928
Training Epoch: 49 [38400/50176]	Loss: 0.2968
Training Epoch: 49 [38912/50176]	Loss: 0.2941
Training Epoch: 49 [39424/50176]	Loss: 0.2523
Training Epoch: 49 [39936/50176]	Loss: 0.2470
Training Epoch: 49 [40448/50176]	Loss: 0.3316
Training Epoch: 49 [40960/50176]	Loss: 0.2873
Training Epoch: 49 [41472/50176]	Loss: 0.3361
Training Epoch: 49 [41984/50176]	Loss: 0.2786
Training Epoch: 49 [42496/50176]	Loss: 0.3321
Training Epoch: 49 [43008/50176]	Loss: 0.2780
Training Epoch: 49 [43520/50176]	Loss: 0.3027
Training Epoch: 49 [44032/50176]	Loss: 0.2908
Training Epoch: 49 [44544/50176]	Loss: 0.2904
Training Epoch: 49 [45056/50176]	Loss: 0.2991
Training Epoch: 49 [45568/50176]	Loss: 0.3790
Training Epoch: 49 [46080/50176]	Loss: 0.3529
Training Epoch: 49 [46592/50176]	Loss: 0.3411
Training Epoch: 49 [47104/50176]	Loss: 0.3846
Training Epoch: 49 [47616/50176]	Loss: 0.3180
Training Epoch: 49 [48128/50176]	Loss: 0.2683
Training Epoch: 49 [48640/50176]	Loss: 0.2669
Training Epoch: 49 [49152/50176]	Loss: 0.3725
Training Epoch: 49 [49664/50176]	Loss: 0.3604
Training Epoch: 49 [50176/50176]	Loss: 0.3622
Validation Epoch: 49, Average loss: 0.0052, Accuracy: 0.5437
Training Epoch: 50 [512/50176]	Loss: 0.2164
Training Epoch: 50 [1024/50176]	Loss: 0.2615
Training Epoch: 50 [1536/50176]	Loss: 0.2594
Training Epoch: 50 [2048/50176]	Loss: 0.2280
Training Epoch: 50 [2560/50176]	Loss: 0.2319
Training Epoch: 50 [3072/50176]	Loss: 0.2807
Training Epoch: 50 [3584/50176]	Loss: 0.2678
Training Epoch: 50 [4096/50176]	Loss: 0.2739
Training Epoch: 50 [4608/50176]	Loss: 0.2732
Training Epoch: 50 [5120/50176]	Loss: 0.2533
Training Epoch: 50 [5632/50176]	Loss: 0.2758
Training Epoch: 50 [6144/50176]	Loss: 0.2402
Training Epoch: 50 [6656/50176]	Loss: 0.2861
Training Epoch: 50 [7168/50176]	Loss: 0.2576
Training Epoch: 50 [7680/50176]	Loss: 0.3148
Training Epoch: 50 [8192/50176]	Loss: 0.2613
Training Epoch: 50 [8704/50176]	Loss: 0.2787
Training Epoch: 50 [9216/50176]	Loss: 0.2570
Training Epoch: 50 [9728/50176]	Loss: 0.2226
Training Epoch: 50 [10240/50176]	Loss: 0.2479
Training Epoch: 50 [10752/50176]	Loss: 0.2587
Training Epoch: 50 [11264/50176]	Loss: 0.2280
Training Epoch: 50 [11776/50176]	Loss: 0.2325
Training Epoch: 50 [12288/50176]	Loss: 0.2376
Training Epoch: 50 [12800/50176]	Loss: 0.2249
Training Epoch: 50 [13312/50176]	Loss: 0.2201
Training Epoch: 50 [13824/50176]	Loss: 0.2931
Training Epoch: 50 [14336/50176]	Loss: 0.2817
Training Epoch: 50 [14848/50176]	Loss: 0.2857
Training Epoch: 50 [15360/50176]	Loss: 0.2877
Training Epoch: 50 [15872/50176]	Loss: 0.2607
Training Epoch: 50 [16384/50176]	Loss: 0.2799
Training Epoch: 50 [16896/50176]	Loss: 0.2523
Training Epoch: 50 [17408/50176]	Loss: 0.2126
Training Epoch: 50 [17920/50176]	Loss: 0.2969
Training Epoch: 50 [18432/50176]	Loss: 0.2423
Training Epoch: 50 [18944/50176]	Loss: 0.2972
Training Epoch: 50 [19456/50176]	Loss: 0.3061
Training Epoch: 50 [19968/50176]	Loss: 0.2646
Training Epoch: 50 [20480/50176]	Loss: 0.2535
Training Epoch: 50 [20992/50176]	Loss: 0.2735
Training Epoch: 50 [21504/50176]	Loss: 0.2788
Training Epoch: 50 [22016/50176]	Loss: 0.2513
Training Epoch: 50 [22528/50176]	Loss: 0.2407
Training Epoch: 50 [23040/50176]	Loss: 0.2858
Training Epoch: 50 [23552/50176]	Loss: 0.2684
Training Epoch: 50 [24064/50176]	Loss: 0.3091
Training Epoch: 50 [24576/50176]	Loss: 0.2609
Training Epoch: 50 [25088/50176]	Loss: 0.2142
Training Epoch: 50 [25600/50176]	Loss: 0.2801
Training Epoch: 50 [26112/50176]	Loss: 0.2624
Training Epoch: 50 [26624/50176]	Loss: 0.2795
Training Epoch: 50 [27136/50176]	Loss: 0.2573
Training Epoch: 50 [27648/50176]	Loss: 0.3613
Training Epoch: 50 [28160/50176]	Loss: 0.2794
Training Epoch: 50 [28672/50176]	Loss: 0.2154
Training Epoch: 50 [29184/50176]	Loss: 0.2833
Training Epoch: 50 [29696/50176]	Loss: 0.2932
Training Epoch: 50 [30208/50176]	Loss: 0.2772
Training Epoch: 50 [30720/50176]	Loss: 0.2120
Training Epoch: 50 [31232/50176]	Loss: 0.2637
Training Epoch: 50 [31744/50176]	Loss: 0.2854
Training Epoch: 50 [32256/50176]	Loss: 0.3011
Training Epoch: 50 [32768/50176]	Loss: 0.3374
Training Epoch: 50 [33280/50176]	Loss: 0.2914
Training Epoch: 50 [33792/50176]	Loss: 0.2767
Training Epoch: 50 [34304/50176]	Loss: 0.2768
Training Epoch: 50 [34816/50176]	Loss: 0.2865
Training Epoch: 50 [35328/50176]	Loss: 0.2915
Training Epoch: 50 [35840/50176]	Loss: 0.3047
Training Epoch: 50 [36352/50176]	Loss: 0.3002
Training Epoch: 50 [36864/50176]	Loss: 0.3234
Training Epoch: 50 [37376/50176]	Loss: 0.2986
Training Epoch: 50 [37888/50176]	Loss: 0.2624
Training Epoch: 50 [38400/50176]	Loss: 0.2792
Training Epoch: 50 [38912/50176]	Loss: 0.2767
Training Epoch: 50 [39424/50176]	Loss: 0.2759
Training Epoch: 50 [39936/50176]	Loss: 0.3257
Training Epoch: 50 [40448/50176]	Loss: 0.2625
Training Epoch: 50 [40960/50176]	Loss: 0.2500
Training Epoch: 50 [41472/50176]	Loss: 0.2999
Training Epoch: 50 [41984/50176]	Loss: 0.3175
Training Epoch: 50 [42496/50176]	Loss: 0.2814
Training Epoch: 50 [43008/50176]	Loss: 0.2643
Training Epoch: 50 [43520/50176]	Loss: 0.2727
Training Epoch: 50 [44032/50176]	Loss: 0.3074
Training Epoch: 50 [44544/50176]	Loss: 0.3000
Training Epoch: 50 [45056/50176]	Loss: 0.2587
Training Epoch: 50 [45568/50176]	Loss: 0.2727
Training Epoch: 50 [46080/50176]	Loss: 0.3153
Training Epoch: 50 [46592/50176]	Loss: 0.3338
Training Epoch: 50 [47104/50176]	Loss: 0.3199
Training Epoch: 50 [47616/50176]	Loss: 0.2877
Training Epoch: 50 [48128/50176]	Loss: 0.2824
Training Epoch: 50 [48640/50176]	Loss: 0.2977
Training Epoch: 50 [49152/50176]	Loss: 0.3290
Training Epoch: 50 [49664/50176]	Loss: 0.3130
Training Epoch: 50 [50176/50176]	Loss: 0.3394
Validation Epoch: 50, Average loss: 0.0049, Accuracy: 0.5654
Training Epoch: 51 [512/50176]	Loss: 0.2200
Training Epoch: 51 [1024/50176]	Loss: 0.2147
Training Epoch: 51 [1536/50176]	Loss: 0.2625
Training Epoch: 51 [2048/50176]	Loss: 0.2997
Training Epoch: 51 [2560/50176]	Loss: 0.2565
Training Epoch: 51 [3072/50176]	Loss: 0.2542
Training Epoch: 51 [3584/50176]	Loss: 0.2122
Training Epoch: 51 [4096/50176]	Loss: 0.2706
Training Epoch: 51 [4608/50176]	Loss: 0.2707
Training Epoch: 51 [5120/50176]	Loss: 0.2557
Training Epoch: 51 [5632/50176]	Loss: 0.2398
Training Epoch: 51 [6144/50176]	Loss: 0.2735
Training Epoch: 51 [6656/50176]	Loss: 0.2137
Training Epoch: 51 [7168/50176]	Loss: 0.2692
Training Epoch: 51 [7680/50176]	Loss: 0.2422
Training Epoch: 51 [8192/50176]	Loss: 0.2826
Training Epoch: 51 [8704/50176]	Loss: 0.3100
Training Epoch: 51 [9216/50176]	Loss: 0.2413
Training Epoch: 51 [9728/50176]	Loss: 0.2468
Training Epoch: 51 [10240/50176]	Loss: 0.1887
Training Epoch: 51 [10752/50176]	Loss: 0.3187
Training Epoch: 51 [11264/50176]	Loss: 0.2500
Training Epoch: 51 [11776/50176]	Loss: 0.2900
Training Epoch: 51 [12288/50176]	Loss: 0.2437
Training Epoch: 51 [12800/50176]	Loss: 0.2613
Training Epoch: 51 [13312/50176]	Loss: 0.2592
Training Epoch: 51 [13824/50176]	Loss: 0.2608
Training Epoch: 51 [14336/50176]	Loss: 0.2423
Training Epoch: 51 [14848/50176]	Loss: 0.2289
Training Epoch: 51 [15360/50176]	Loss: 0.2767
Training Epoch: 51 [15872/50176]	Loss: 0.1763
Training Epoch: 51 [16384/50176]	Loss: 0.2436
Training Epoch: 51 [16896/50176]	Loss: 0.2582
Training Epoch: 51 [17408/50176]	Loss: 0.2518
Training Epoch: 51 [17920/50176]	Loss: 0.2812
Training Epoch: 51 [18432/50176]	Loss: 0.2176
Training Epoch: 51 [18944/50176]	Loss: 0.2490
Training Epoch: 51 [19456/50176]	Loss: 0.2578
Training Epoch: 51 [19968/50176]	Loss: 0.2210
Training Epoch: 51 [20480/50176]	Loss: 0.2437
Training Epoch: 51 [20992/50176]	Loss: 0.3106
Training Epoch: 51 [21504/50176]	Loss: 0.2401
Training Epoch: 51 [22016/50176]	Loss: 0.2959
Training Epoch: 51 [22528/50176]	Loss: 0.2183
Training Epoch: 51 [23040/50176]	Loss: 0.2720
Training Epoch: 51 [23552/50176]	Loss: 0.2701
Training Epoch: 51 [24064/50176]	Loss: 0.2637
Training Epoch: 51 [24576/50176]	Loss: 0.2470
Training Epoch: 51 [25088/50176]	Loss: 0.2281
Training Epoch: 51 [25600/50176]	Loss: 0.2593
Training Epoch: 51 [26112/50176]	Loss: 0.2641
Training Epoch: 51 [26624/50176]	Loss: 0.2057
Training Epoch: 51 [27136/50176]	Loss: 0.2937
Training Epoch: 51 [27648/50176]	Loss: 0.2650
Training Epoch: 51 [28160/50176]	Loss: 0.2363
Training Epoch: 51 [28672/50176]	Loss: 0.2025
Training Epoch: 51 [29184/50176]	Loss: 0.2662
Training Epoch: 51 [29696/50176]	Loss: 0.2485
Training Epoch: 51 [30208/50176]	Loss: 0.2821
Training Epoch: 51 [30720/50176]	Loss: 0.2937
Training Epoch: 51 [31232/50176]	Loss: 0.2301
Training Epoch: 51 [31744/50176]	Loss: 0.2720
Training Epoch: 51 [32256/50176]	Loss: 0.2383
Training Epoch: 51 [32768/50176]	Loss: 0.2690
Training Epoch: 51 [33280/50176]	Loss: 0.2757
Training Epoch: 51 [33792/50176]	Loss: 0.2830
Training Epoch: 51 [34304/50176]	Loss: 0.2979
Training Epoch: 51 [34816/50176]	Loss: 0.2796
Training Epoch: 51 [35328/50176]	Loss: 0.2765
Training Epoch: 51 [35840/50176]	Loss: 0.2579
Training Epoch: 51 [36352/50176]	Loss: 0.3185
Training Epoch: 51 [36864/50176]	Loss: 0.3165
Training Epoch: 51 [37376/50176]	Loss: 0.2857
Training Epoch: 51 [37888/50176]	Loss: 0.3408
Training Epoch: 51 [38400/50176]	Loss: 0.2999
Training Epoch: 51 [38912/50176]	Loss: 0.2478
Training Epoch: 51 [39424/50176]	Loss: 0.2759
Training Epoch: 51 [39936/50176]	Loss: 0.2808
Training Epoch: 51 [40448/50176]	Loss: 0.2571
Training Epoch: 51 [40960/50176]	Loss: 0.2949
Training Epoch: 51 [41472/50176]	Loss: 0.2650
Training Epoch: 51 [41984/50176]	Loss: 0.3157
Training Epoch: 51 [42496/50176]	Loss: 0.3211
Training Epoch: 51 [43008/50176]	Loss: 0.2999
Training Epoch: 51 [43520/50176]	Loss: 0.2685
Training Epoch: 51 [44032/50176]	Loss: 0.2876
Training Epoch: 51 [44544/50176]	Loss: 0.2761
Training Epoch: 51 [45056/50176]	Loss: 0.2503
Training Epoch: 51 [45568/50176]	Loss: 0.2895
Training Epoch: 51 [46080/50176]	Loss: 0.2414
Training Epoch: 51 [46592/50176]	Loss: 0.2424
Training Epoch: 51 [47104/50176]	Loss: 0.2827
Training Epoch: 51 [47616/50176]	Loss: 0.2939
Training Epoch: 51 [48128/50176]	Loss: 0.3061
Training Epoch: 51 [48640/50176]	Loss: 0.3351
Training Epoch: 51 [49152/50176]	Loss: 0.2859
Training Epoch: 51 [49664/50176]	Loss: 0.3302
Training Epoch: 51 [50176/50176]	Loss: 0.2713
Validation Epoch: 51, Average loss: 0.0050, Accuracy: 0.5658
Training Epoch: 52 [512/50176]	Loss: 0.2162
Training Epoch: 52 [1024/50176]	Loss: 0.2285
Training Epoch: 52 [1536/50176]	Loss: 0.1974
Training Epoch: 52 [2048/50176]	Loss: 0.2264
Training Epoch: 52 [2560/50176]	Loss: 0.1868
Training Epoch: 52 [3072/50176]	Loss: 0.2703
Training Epoch: 52 [3584/50176]	Loss: 0.2925
Training Epoch: 52 [4096/50176]	Loss: 0.2544
Training Epoch: 52 [4608/50176]	Loss: 0.2652
Training Epoch: 52 [5120/50176]	Loss: 0.3228
Training Epoch: 52 [5632/50176]	Loss: 0.2232
Training Epoch: 52 [6144/50176]	Loss: 0.2762
Training Epoch: 52 [6656/50176]	Loss: 0.2233
Training Epoch: 52 [7168/50176]	Loss: 0.2542
Training Epoch: 52 [7680/50176]	Loss: 0.2606
Training Epoch: 52 [8192/50176]	Loss: 0.2765
Training Epoch: 52 [8704/50176]	Loss: 0.1843
Training Epoch: 52 [9216/50176]	Loss: 0.2512
Training Epoch: 52 [9728/50176]	Loss: 0.2956
Training Epoch: 52 [10240/50176]	Loss: 0.2185
Training Epoch: 52 [10752/50176]	Loss: 0.2471
Training Epoch: 52 [11264/50176]	Loss: 0.2184
Training Epoch: 52 [11776/50176]	Loss: 0.2523
Training Epoch: 52 [12288/50176]	Loss: 0.2067
Training Epoch: 52 [12800/50176]	Loss: 0.2364
Training Epoch: 52 [13312/50176]	Loss: 0.2269
Training Epoch: 52 [13824/50176]	Loss: 0.2139
Training Epoch: 52 [14336/50176]	Loss: 0.1591
Training Epoch: 52 [14848/50176]	Loss: 0.2290
Training Epoch: 52 [15360/50176]	Loss: 0.2507
Training Epoch: 52 [15872/50176]	Loss: 0.3011
Training Epoch: 52 [16384/50176]	Loss: 0.1851
Training Epoch: 52 [16896/50176]	Loss: 0.2534
Training Epoch: 52 [17408/50176]	Loss: 0.2823
Training Epoch: 52 [17920/50176]	Loss: 0.2614
Training Epoch: 52 [18432/50176]	Loss: 0.2672
Training Epoch: 52 [18944/50176]	Loss: 0.2423
Training Epoch: 52 [19456/50176]	Loss: 0.2283
Training Epoch: 52 [19968/50176]	Loss: 0.2250
Training Epoch: 52 [20480/50176]	Loss: 0.2493
Training Epoch: 52 [20992/50176]	Loss: 0.2709
Training Epoch: 52 [21504/50176]	Loss: 0.2402
Training Epoch: 52 [22016/50176]	Loss: 0.3007
Training Epoch: 52 [22528/50176]	Loss: 0.2682
Training Epoch: 52 [23040/50176]	Loss: 0.3099
Training Epoch: 52 [23552/50176]	Loss: 0.2952
Training Epoch: 52 [24064/50176]	Loss: 0.2347
Training Epoch: 52 [24576/50176]	Loss: 0.2375
Training Epoch: 52 [25088/50176]	Loss: 0.2449
Training Epoch: 52 [25600/50176]	Loss: 0.3158
Training Epoch: 52 [26112/50176]	Loss: 0.2498
Training Epoch: 52 [26624/50176]	Loss: 0.2721
Training Epoch: 52 [27136/50176]	Loss: 0.2743
Training Epoch: 52 [27648/50176]	Loss: 0.2964
Training Epoch: 52 [28160/50176]	Loss: 0.2473
Training Epoch: 52 [28672/50176]	Loss: 0.2368
Training Epoch: 52 [29184/50176]	Loss: 0.2030
Training Epoch: 52 [29696/50176]	Loss: 0.2275
Training Epoch: 52 [30208/50176]	Loss: 0.2345
Training Epoch: 52 [30720/50176]	Loss: 0.2637
Training Epoch: 52 [31232/50176]	Loss: 0.2343
Training Epoch: 52 [31744/50176]	Loss: 0.2579
Training Epoch: 52 [32256/50176]	Loss: 0.2546
Training Epoch: 52 [32768/50176]	Loss: 0.2491
Training Epoch: 52 [33280/50176]	Loss: 0.3439
Training Epoch: 52 [33792/50176]	Loss: 0.2576
Training Epoch: 52 [34304/50176]	Loss: 0.2369
Training Epoch: 52 [34816/50176]	Loss: 0.2887
Training Epoch: 52 [35328/50176]	Loss: 0.2827
Training Epoch: 52 [35840/50176]	Loss: 0.2873
Training Epoch: 52 [36352/50176]	Loss: 0.3218
Training Epoch: 52 [36864/50176]	Loss: 0.2646
Training Epoch: 52 [37376/50176]	Loss: 0.2459
Training Epoch: 52 [37888/50176]	Loss: 0.2299
Training Epoch: 52 [38400/50176]	Loss: 0.2898
Training Epoch: 52 [38912/50176]	Loss: 0.2810
Training Epoch: 52 [39424/50176]	Loss: 0.2627
Training Epoch: 52 [39936/50176]	Loss: 0.2392
Training Epoch: 52 [40448/50176]	Loss: 0.2424
Training Epoch: 52 [40960/50176]	Loss: 0.2530
Training Epoch: 52 [41472/50176]	Loss: 0.2995
Training Epoch: 52 [41984/50176]	Loss: 0.3137
Training Epoch: 52 [42496/50176]	Loss: 0.2902
Training Epoch: 52 [43008/50176]	Loss: 0.2738
Training Epoch: 52 [43520/50176]	Loss: 0.3047
Training Epoch: 52 [44032/50176]	Loss: 0.3198
Training Epoch: 52 [44544/50176]	Loss: 0.2745
Training Epoch: 52 [45056/50176]	Loss: 0.2777
Training Epoch: 52 [45568/50176]	Loss: 0.2605
Training Epoch: 52 [46080/50176]	Loss: 0.3207
Training Epoch: 52 [46592/50176]	Loss: 0.2967
Training Epoch: 52 [47104/50176]	Loss: 0.2881
Training Epoch: 52 [47616/50176]	Loss: 0.2553
Training Epoch: 52 [48128/50176]	Loss: 0.3136
Training Epoch: 52 [48640/50176]	Loss: 0.2918
Training Epoch: 52 [49152/50176]	Loss: 0.2844
Training Epoch: 52 [49664/50176]	Loss: 0.2994
Training Epoch: 52 [50176/50176]	Loss: 0.3331
Validation Epoch: 52, Average loss: 0.0050, Accuracy: 0.5719
Training Epoch: 53 [512/50176]	Loss: 0.2085
Training Epoch: 53 [1024/50176]	Loss: 0.1980
Training Epoch: 53 [1536/50176]	Loss: 0.2408
Training Epoch: 53 [2048/50176]	Loss: 0.2523
Training Epoch: 53 [2560/50176]	Loss: 0.2852
Training Epoch: 53 [3072/50176]	Loss: 0.2372
Training Epoch: 53 [3584/50176]	Loss: 0.2154
Training Epoch: 53 [4096/50176]	Loss: 0.2219
Training Epoch: 53 [4608/50176]	Loss: 0.2375
Training Epoch: 53 [5120/50176]	Loss: 0.2986
Training Epoch: 53 [5632/50176]	Loss: 0.1964
Training Epoch: 53 [6144/50176]	Loss: 0.1879
Training Epoch: 53 [6656/50176]	Loss: 0.2315
Training Epoch: 53 [7168/50176]	Loss: 0.2859
Training Epoch: 53 [7680/50176]	Loss: 0.2079
Training Epoch: 53 [8192/50176]	Loss: 0.2490
Training Epoch: 53 [8704/50176]	Loss: 0.2384
Training Epoch: 53 [9216/50176]	Loss: 0.2388
Training Epoch: 53 [9728/50176]	Loss: 0.1894
Training Epoch: 53 [10240/50176]	Loss: 0.2140
Training Epoch: 53 [10752/50176]	Loss: 0.2624
Training Epoch: 53 [11264/50176]	Loss: 0.2727
Training Epoch: 53 [11776/50176]	Loss: 0.2528
Training Epoch: 53 [12288/50176]	Loss: 0.2071
Training Epoch: 53 [12800/50176]	Loss: 0.1994
Training Epoch: 53 [13312/50176]	Loss: 0.2124
Training Epoch: 53 [13824/50176]	Loss: 0.2362
Training Epoch: 53 [14336/50176]	Loss: 0.2626
Training Epoch: 53 [14848/50176]	Loss: 0.2322
Training Epoch: 53 [15360/50176]	Loss: 0.2358
Training Epoch: 53 [15872/50176]	Loss: 0.2562
Training Epoch: 53 [16384/50176]	Loss: 0.2822
Training Epoch: 53 [16896/50176]	Loss: 0.2480
Training Epoch: 53 [17408/50176]	Loss: 0.2483
Training Epoch: 53 [17920/50176]	Loss: 0.2373
Training Epoch: 53 [18432/50176]	Loss: 0.2718
Training Epoch: 53 [18944/50176]	Loss: 0.2348
Training Epoch: 53 [19456/50176]	Loss: 0.2350
Training Epoch: 53 [19968/50176]	Loss: 0.2237
Training Epoch: 53 [20480/50176]	Loss: 0.2120
Training Epoch: 53 [20992/50176]	Loss: 0.2644
Training Epoch: 53 [21504/50176]	Loss: 0.2162
Training Epoch: 53 [22016/50176]	Loss: 0.2494
Training Epoch: 53 [22528/50176]	Loss: 0.3086
Training Epoch: 53 [23040/50176]	Loss: 0.2156
Training Epoch: 53 [23552/50176]	Loss: 0.2955
Training Epoch: 53 [24064/50176]	Loss: 0.2591
Training Epoch: 53 [24576/50176]	Loss: 0.2286
Training Epoch: 53 [25088/50176]	Loss: 0.2481
Training Epoch: 53 [25600/50176]	Loss: 0.2600
Training Epoch: 53 [26112/50176]	Loss: 0.2547
Training Epoch: 53 [26624/50176]	Loss: 0.3037
Training Epoch: 53 [27136/50176]	Loss: 0.2856
Training Epoch: 53 [27648/50176]	Loss: 0.2490
Training Epoch: 53 [28160/50176]	Loss: 0.2338
Training Epoch: 53 [28672/50176]	Loss: 0.2289
Training Epoch: 53 [29184/50176]	Loss: 0.2611
Training Epoch: 53 [29696/50176]	Loss: 0.2594
Training Epoch: 53 [30208/50176]	Loss: 0.2469
Training Epoch: 53 [30720/50176]	Loss: 0.2780
Training Epoch: 53 [31232/50176]	Loss: 0.2777
Training Epoch: 53 [31744/50176]	Loss: 0.2659
Training Epoch: 53 [32256/50176]	Loss: 0.2653
Training Epoch: 53 [32768/50176]	Loss: 0.2227
Training Epoch: 53 [33280/50176]	Loss: 0.2955
Training Epoch: 53 [33792/50176]	Loss: 0.2547
Training Epoch: 53 [34304/50176]	Loss: 0.2536
Training Epoch: 53 [34816/50176]	Loss: 0.2894
Training Epoch: 53 [35328/50176]	Loss: 0.2442
Training Epoch: 53 [35840/50176]	Loss: 0.2378
Training Epoch: 53 [36352/50176]	Loss: 0.3451
Training Epoch: 53 [36864/50176]	Loss: 0.2461
Training Epoch: 53 [37376/50176]	Loss: 0.2523
Training Epoch: 53 [37888/50176]	Loss: 0.2764
Training Epoch: 53 [38400/50176]	Loss: 0.2918
Training Epoch: 53 [38912/50176]	Loss: 0.3738
Training Epoch: 53 [39424/50176]	Loss: 0.2248
Training Epoch: 53 [39936/50176]	Loss: 0.3548
Training Epoch: 53 [40448/50176]	Loss: 0.2683
Training Epoch: 53 [40960/50176]	Loss: 0.3053
Training Epoch: 53 [41472/50176]	Loss: 0.2783
Training Epoch: 53 [41984/50176]	Loss: 0.2768
Training Epoch: 53 [42496/50176]	Loss: 0.2284
Training Epoch: 53 [43008/50176]	Loss: 0.2326
Training Epoch: 53 [43520/50176]	Loss: 0.2645
Training Epoch: 53 [44032/50176]	Loss: 0.2653
Training Epoch: 53 [44544/50176]	Loss: 0.2601
Training Epoch: 53 [45056/50176]	Loss: 0.2519
Training Epoch: 53 [45568/50176]	Loss: 0.2725
Training Epoch: 53 [46080/50176]	Loss: 0.3137
Training Epoch: 53 [46592/50176]	Loss: 0.2605
Training Epoch: 53 [47104/50176]	Loss: 0.2906
Training Epoch: 53 [47616/50176]	Loss: 0.2609
Training Epoch: 53 [48128/50176]	Loss: 0.3169
Training Epoch: 53 [48640/50176]	Loss: 0.2879
Training Epoch: 53 [49152/50176]	Loss: 0.2929
Training Epoch: 53 [49664/50176]	Loss: 0.3418
Training Epoch: 53 [50176/50176]	Loss: 0.2786
Validation Epoch: 53, Average loss: 0.0048, Accuracy: 0.5764
Training Epoch: 54 [512/50176]	Loss: 0.2663
Training Epoch: 54 [1024/50176]	Loss: 0.2395
Training Epoch: 54 [1536/50176]	Loss: 0.2113
Training Epoch: 54 [2048/50176]	Loss: 0.1836
Training Epoch: 54 [2560/50176]	Loss: 0.1856
Training Epoch: 54 [3072/50176]	Loss: 0.1883
Training Epoch: 54 [3584/50176]	Loss: 0.1443
Training Epoch: 54 [4096/50176]	Loss: 0.1826
Training Epoch: 54 [4608/50176]	Loss: 0.2158
Training Epoch: 54 [5120/50176]	Loss: 0.2368
Training Epoch: 54 [5632/50176]	Loss: 0.2203
Training Epoch: 54 [6144/50176]	Loss: 0.2370
Training Epoch: 54 [6656/50176]	Loss: 0.2169
Training Epoch: 54 [7168/50176]	Loss: 0.2068
Training Epoch: 54 [7680/50176]	Loss: 0.2157
Training Epoch: 54 [8192/50176]	Loss: 0.2326
Training Epoch: 54 [8704/50176]	Loss: 0.1800
Training Epoch: 54 [9216/50176]	Loss: 0.2291
Training Epoch: 54 [9728/50176]	Loss: 0.2359
Training Epoch: 54 [10240/50176]	Loss: 0.2557
Training Epoch: 54 [10752/50176]	Loss: 0.2087
Training Epoch: 54 [11264/50176]	Loss: 0.2426
Training Epoch: 54 [11776/50176]	Loss: 0.2280
Training Epoch: 54 [12288/50176]	Loss: 0.2059
Training Epoch: 54 [12800/50176]	Loss: 0.2145
Training Epoch: 54 [13312/50176]	Loss: 0.1644
Training Epoch: 54 [13824/50176]	Loss: 0.2024
Training Epoch: 54 [14336/50176]	Loss: 0.2748
Training Epoch: 54 [14848/50176]	Loss: 0.2450
Training Epoch: 54 [15360/50176]	Loss: 0.2566
Training Epoch: 54 [15872/50176]	Loss: 0.1774
Training Epoch: 54 [16384/50176]	Loss: 0.2222
Training Epoch: 54 [16896/50176]	Loss: 0.1792
Training Epoch: 54 [17408/50176]	Loss: 0.2154
Training Epoch: 54 [17920/50176]	Loss: 0.2054
Training Epoch: 54 [18432/50176]	Loss: 0.2509
Training Epoch: 54 [18944/50176]	Loss: 0.2679
Training Epoch: 54 [19456/50176]	Loss: 0.2197
Training Epoch: 54 [19968/50176]	Loss: 0.2290
Training Epoch: 54 [20480/50176]	Loss: 0.1998
Training Epoch: 54 [20992/50176]	Loss: 0.2372
Training Epoch: 54 [21504/50176]	Loss: 0.2044
Training Epoch: 54 [22016/50176]	Loss: 0.2139
Training Epoch: 54 [22528/50176]	Loss: 0.2773
Training Epoch: 54 [23040/50176]	Loss: 0.1854
Training Epoch: 54 [23552/50176]	Loss: 0.2071
Training Epoch: 54 [24064/50176]	Loss: 0.2556
Training Epoch: 54 [24576/50176]	Loss: 0.2481
Training Epoch: 54 [25088/50176]	Loss: 0.2224
Training Epoch: 54 [25600/50176]	Loss: 0.2188
Training Epoch: 54 [26112/50176]	Loss: 0.1966
Training Epoch: 54 [26624/50176]	Loss: 0.2085
Training Epoch: 54 [27136/50176]	Loss: 0.2264
Training Epoch: 54 [27648/50176]	Loss: 0.2810
Training Epoch: 54 [28160/50176]	Loss: 0.2315
Training Epoch: 54 [28672/50176]	Loss: 0.2618
Training Epoch: 54 [29184/50176]	Loss: 0.2446
Training Epoch: 54 [29696/50176]	Loss: 0.2423
Training Epoch: 54 [30208/50176]	Loss: 0.2946
Training Epoch: 54 [30720/50176]	Loss: 0.2110
Training Epoch: 54 [31232/50176]	Loss: 0.2082
Training Epoch: 54 [31744/50176]	Loss: 0.2813
Training Epoch: 54 [32256/50176]	Loss: 0.2359
Training Epoch: 54 [32768/50176]	Loss: 0.2558
Training Epoch: 54 [33280/50176]	Loss: 0.2754
Training Epoch: 54 [33792/50176]	Loss: 0.2554
Training Epoch: 54 [34304/50176]	Loss: 0.2194
Training Epoch: 54 [34816/50176]	Loss: 0.2218
Training Epoch: 54 [35328/50176]	Loss: 0.2893
Training Epoch: 54 [35840/50176]	Loss: 0.2715
Training Epoch: 54 [36352/50176]	Loss: 0.2344
Training Epoch: 54 [36864/50176]	Loss: 0.2826
Training Epoch: 54 [37376/50176]	Loss: 0.2343
Training Epoch: 54 [37888/50176]	Loss: 0.2476
Training Epoch: 54 [38400/50176]	Loss: 0.2736
Training Epoch: 54 [38912/50176]	Loss: 0.2074
Training Epoch: 54 [39424/50176]	Loss: 0.2785
Training Epoch: 54 [39936/50176]	Loss: 0.1663
Training Epoch: 54 [40448/50176]	Loss: 0.2448
Training Epoch: 54 [40960/50176]	Loss: 0.2488
Training Epoch: 54 [41472/50176]	Loss: 0.2192
Training Epoch: 54 [41984/50176]	Loss: 0.2023
Training Epoch: 54 [42496/50176]	Loss: 0.2647
Training Epoch: 54 [43008/50176]	Loss: 0.2804
Training Epoch: 54 [43520/50176]	Loss: 0.2656
Training Epoch: 54 [44032/50176]	Loss: 0.2410
Training Epoch: 54 [44544/50176]	Loss: 0.2496
Training Epoch: 54 [45056/50176]	Loss: 0.1923
Training Epoch: 54 [45568/50176]	Loss: 0.2031
Training Epoch: 54 [46080/50176]	Loss: 0.2309
Training Epoch: 54 [46592/50176]	Loss: 0.2623
Training Epoch: 54 [47104/50176]	Loss: 0.3241
Training Epoch: 54 [47616/50176]	Loss: 0.3011
Training Epoch: 54 [48128/50176]	Loss: 0.2361
Training Epoch: 54 [48640/50176]	Loss: 0.2788
Training Epoch: 54 [49152/50176]	Loss: 0.2557
Training Epoch: 54 [49664/50176]	Loss: 0.2828
Training Epoch: 54 [50176/50176]	Loss: 0.2469
Validation Epoch: 54, Average loss: 0.0048, Accuracy: 0.5877
Training Epoch: 55 [512/50176]	Loss: 0.1772
Training Epoch: 55 [1024/50176]	Loss: 0.2060
Training Epoch: 55 [1536/50176]	Loss: 0.2024
Training Epoch: 55 [2048/50176]	Loss: 0.1910
Training Epoch: 55 [2560/50176]	Loss: 0.2284
Training Epoch: 55 [3072/50176]	Loss: 0.2088
Training Epoch: 55 [3584/50176]	Loss: 0.2202
Training Epoch: 55 [4096/50176]	Loss: 0.1504
Training Epoch: 55 [4608/50176]	Loss: 0.2073
Training Epoch: 55 [5120/50176]	Loss: 0.2562
Training Epoch: 55 [5632/50176]	Loss: 0.2509
Training Epoch: 55 [6144/50176]	Loss: 0.2041
Training Epoch: 55 [6656/50176]	Loss: 0.1837
Training Epoch: 55 [7168/50176]	Loss: 0.2443
Training Epoch: 55 [7680/50176]	Loss: 0.2083
Training Epoch: 55 [8192/50176]	Loss: 0.2411
Training Epoch: 55 [8704/50176]	Loss: 0.1810
Training Epoch: 55 [9216/50176]	Loss: 0.1907
Training Epoch: 55 [9728/50176]	Loss: 0.2146
Training Epoch: 55 [10240/50176]	Loss: 0.1669
Training Epoch: 55 [10752/50176]	Loss: 0.2059
Training Epoch: 55 [11264/50176]	Loss: 0.2341
Training Epoch: 55 [11776/50176]	Loss: 0.1767
Training Epoch: 55 [12288/50176]	Loss: 0.1446
Training Epoch: 55 [12800/50176]	Loss: 0.1850
Training Epoch: 55 [13312/50176]	Loss: 0.1979
Training Epoch: 55 [13824/50176]	Loss: 0.2609
Training Epoch: 55 [14336/50176]	Loss: 0.2023
Training Epoch: 55 [14848/50176]	Loss: 0.2039
Training Epoch: 55 [15360/50176]	Loss: 0.2396
Training Epoch: 55 [15872/50176]	Loss: 0.2468
Training Epoch: 55 [16384/50176]	Loss: 0.1640
Training Epoch: 55 [16896/50176]	Loss: 0.2143
Training Epoch: 55 [17408/50176]	Loss: 0.1725
Training Epoch: 55 [17920/50176]	Loss: 0.2142
Training Epoch: 55 [18432/50176]	Loss: 0.2157
Training Epoch: 55 [18944/50176]	Loss: 0.1734
Training Epoch: 55 [19456/50176]	Loss: 0.2006
Training Epoch: 55 [19968/50176]	Loss: 0.1931
Training Epoch: 55 [20480/50176]	Loss: 0.2213
Training Epoch: 55 [20992/50176]	Loss: 0.2519
Training Epoch: 55 [21504/50176]	Loss: 0.1809
Training Epoch: 55 [22016/50176]	Loss: 0.2111
Training Epoch: 55 [22528/50176]	Loss: 0.2184
Training Epoch: 55 [23040/50176]	Loss: 0.1407
Training Epoch: 55 [23552/50176]	Loss: 0.2541
Training Epoch: 55 [24064/50176]	Loss: 0.2109
Training Epoch: 55 [24576/50176]	Loss: 0.2211
Training Epoch: 55 [25088/50176]	Loss: 0.1949
Training Epoch: 55 [25600/50176]	Loss: 0.2464
Training Epoch: 55 [26112/50176]	Loss: 0.1971
Training Epoch: 55 [26624/50176]	Loss: 0.2238
Training Epoch: 55 [27136/50176]	Loss: 0.2729
Training Epoch: 55 [27648/50176]	Loss: 0.2247
Training Epoch: 55 [28160/50176]	Loss: 0.2536
Training Epoch: 55 [28672/50176]	Loss: 0.2053
Training Epoch: 55 [29184/50176]	Loss: 0.2576
Training Epoch: 55 [29696/50176]	Loss: 0.2229
Training Epoch: 55 [30208/50176]	Loss: 0.2185
Training Epoch: 55 [30720/50176]	Loss: 0.2072
Training Epoch: 55 [31232/50176]	Loss: 0.2619
Training Epoch: 55 [31744/50176]	Loss: 0.2422
Training Epoch: 55 [32256/50176]	Loss: 0.2802
Training Epoch: 55 [32768/50176]	Loss: 0.2457
Training Epoch: 55 [33280/50176]	Loss: 0.2610
Training Epoch: 55 [33792/50176]	Loss: 0.2452
Training Epoch: 55 [34304/50176]	Loss: 0.2285
Training Epoch: 55 [34816/50176]	Loss: 0.2337
Training Epoch: 55 [35328/50176]	Loss: 0.1985
Training Epoch: 55 [35840/50176]	Loss: 0.2769
Training Epoch: 55 [36352/50176]	Loss: 0.2998
Training Epoch: 55 [36864/50176]	Loss: 0.2737
Training Epoch: 55 [37376/50176]	Loss: 0.2775
Training Epoch: 55 [37888/50176]	Loss: 0.2511
Training Epoch: 55 [38400/50176]	Loss: 0.2933
Training Epoch: 55 [38912/50176]	Loss: 0.2566
Training Epoch: 55 [39424/50176]	Loss: 0.2257
Training Epoch: 55 [39936/50176]	Loss: 0.2462
Training Epoch: 55 [40448/50176]	Loss: 0.2873
Training Epoch: 55 [40960/50176]	Loss: 0.2820
Training Epoch: 55 [41472/50176]	Loss: 0.2719
Training Epoch: 55 [41984/50176]	Loss: 0.2626
Training Epoch: 55 [42496/50176]	Loss: 0.2904
Training Epoch: 55 [43008/50176]	Loss: 0.2237
Training Epoch: 55 [43520/50176]	Loss: 0.2538
Training Epoch: 55 [44032/50176]	Loss: 0.2260
Training Epoch: 55 [44544/50176]	Loss: 0.2198
Training Epoch: 55 [45056/50176]	Loss: 0.2572
Training Epoch: 55 [45568/50176]	Loss: 0.2914
Training Epoch: 55 [46080/50176]	Loss: 0.2371
Training Epoch: 55 [46592/50176]	Loss: 0.2502
Training Epoch: 55 [47104/50176]	Loss: 0.2629
Training Epoch: 55 [47616/50176]	Loss: 0.2715
Training Epoch: 55 [48128/50176]	Loss: 0.2706
Training Epoch: 55 [48640/50176]	Loss: 0.2495
Training Epoch: 55 [49152/50176]	Loss: 0.3010
Training Epoch: 55 [49664/50176]	Loss: 0.2402
Training Epoch: 55 [50176/50176]	Loss: 0.2883
Validation Epoch: 55, Average loss: 0.0047, Accuracy: 0.5854
Training Epoch: 56 [512/50176]	Loss: 0.2332
Training Epoch: 56 [1024/50176]	Loss: 0.1652
Training Epoch: 56 [1536/50176]	Loss: 0.2332
Training Epoch: 56 [2048/50176]	Loss: 0.2096
Training Epoch: 56 [2560/50176]	Loss: 0.2397
Training Epoch: 56 [3072/50176]	Loss: 0.2375
Training Epoch: 56 [3584/50176]	Loss: 0.2133
Training Epoch: 56 [4096/50176]	Loss: 0.2239
Training Epoch: 56 [4608/50176]	Loss: 0.2600
Training Epoch: 56 [5120/50176]	Loss: 0.2190
Training Epoch: 56 [5632/50176]	Loss: 0.2413
Training Epoch: 56 [6144/50176]	Loss: 0.1871
Training Epoch: 56 [6656/50176]	Loss: 0.2007
Training Epoch: 56 [7168/50176]	Loss: 0.1981
Training Epoch: 56 [7680/50176]	Loss: 0.1868
Training Epoch: 56 [8192/50176]	Loss: 0.2118
Training Epoch: 56 [8704/50176]	Loss: 0.2167
Training Epoch: 56 [9216/50176]	Loss: 0.2244
Training Epoch: 56 [9728/50176]	Loss: 0.2233
Training Epoch: 56 [10240/50176]	Loss: 0.2182
Training Epoch: 56 [10752/50176]	Loss: 0.2022
Training Epoch: 56 [11264/50176]	Loss: 0.2207
Training Epoch: 56 [11776/50176]	Loss: 0.1897
Training Epoch: 56 [12288/50176]	Loss: 0.2042
Training Epoch: 56 [12800/50176]	Loss: 0.2584
Training Epoch: 56 [13312/50176]	Loss: 0.2372
Training Epoch: 56 [13824/50176]	Loss: 0.2405
Training Epoch: 56 [14336/50176]	Loss: 0.1753
Training Epoch: 56 [14848/50176]	Loss: 0.1959
Training Epoch: 56 [15360/50176]	Loss: 0.2144
Training Epoch: 56 [15872/50176]	Loss: 0.2921
Training Epoch: 56 [16384/50176]	Loss: 0.2526
Training Epoch: 56 [16896/50176]	Loss: 0.2025
Training Epoch: 56 [17408/50176]	Loss: 0.2444
Training Epoch: 56 [17920/50176]	Loss: 0.2387
Training Epoch: 56 [18432/50176]	Loss: 0.1903
Training Epoch: 56 [18944/50176]	Loss: 0.2308
Training Epoch: 56 [19456/50176]	Loss: 0.2175
Training Epoch: 56 [19968/50176]	Loss: 0.2569
Training Epoch: 56 [20480/50176]	Loss: 0.2633
Training Epoch: 56 [20992/50176]	Loss: 0.1765
Training Epoch: 56 [21504/50176]	Loss: 0.2097
Training Epoch: 56 [22016/50176]	Loss: 0.2252
Training Epoch: 56 [22528/50176]	Loss: 0.1929
Training Epoch: 56 [23040/50176]	Loss: 0.2186
Training Epoch: 56 [23552/50176]	Loss: 0.2058
Training Epoch: 56 [24064/50176]	Loss: 0.1915
Training Epoch: 56 [24576/50176]	Loss: 0.1905
Training Epoch: 56 [25088/50176]	Loss: 0.2579
Training Epoch: 56 [25600/50176]	Loss: 0.2843
Training Epoch: 56 [26112/50176]	Loss: 0.2101
Training Epoch: 56 [26624/50176]	Loss: 0.1929
Training Epoch: 56 [27136/50176]	Loss: 0.2184
Training Epoch: 56 [27648/50176]	Loss: 0.2597
Training Epoch: 56 [28160/50176]	Loss: 0.2089
Training Epoch: 56 [28672/50176]	Loss: 0.2338
Training Epoch: 56 [29184/50176]	Loss: 0.1936
Training Epoch: 56 [29696/50176]	Loss: 0.1989
Training Epoch: 56 [30208/50176]	Loss: 0.2358
Training Epoch: 56 [30720/50176]	Loss: 0.2315
Training Epoch: 56 [31232/50176]	Loss: 0.2733
Training Epoch: 56 [31744/50176]	Loss: 0.2065
Training Epoch: 56 [32256/50176]	Loss: 0.1950
Training Epoch: 56 [32768/50176]	Loss: 0.1918
Training Epoch: 56 [33280/50176]	Loss: 0.2540
Training Epoch: 56 [33792/50176]	Loss: 0.2052
Training Epoch: 56 [34304/50176]	Loss: 0.1715
Training Epoch: 56 [34816/50176]	Loss: 0.2267
Training Epoch: 56 [35328/50176]	Loss: 0.2339
Training Epoch: 56 [35840/50176]	Loss: 0.1807
Training Epoch: 56 [36352/50176]	Loss: 0.2087
Training Epoch: 56 [36864/50176]	Loss: 0.2301
Training Epoch: 56 [37376/50176]	Loss: 0.2464
Training Epoch: 56 [37888/50176]	Loss: 0.2415
Training Epoch: 56 [38400/50176]	Loss: 0.1911
Training Epoch: 56 [38912/50176]	Loss: 0.2014
Training Epoch: 56 [39424/50176]	Loss: 0.2055
Training Epoch: 56 [39936/50176]	Loss: 0.2194
Training Epoch: 56 [40448/50176]	Loss: 0.2075
Training Epoch: 56 [40960/50176]	Loss: 0.2141
Training Epoch: 56 [41472/50176]	Loss: 0.2963
Training Epoch: 56 [41984/50176]	Loss: 0.2762
Training Epoch: 56 [42496/50176]	Loss: 0.2742
Training Epoch: 56 [43008/50176]	Loss: 0.2015
Training Epoch: 56 [43520/50176]	Loss: 0.2978
Training Epoch: 56 [44032/50176]	Loss: 0.2441
Training Epoch: 56 [44544/50176]	Loss: 0.2594
Training Epoch: 56 [45056/50176]	Loss: 0.2663
Training Epoch: 56 [45568/50176]	Loss: 0.2425
Training Epoch: 56 [46080/50176]	Loss: 0.2820
Training Epoch: 56 [46592/50176]	Loss: 0.2351
Training Epoch: 56 [47104/50176]	Loss: 0.1826
Training Epoch: 56 [47616/50176]	Loss: 0.2096
Training Epoch: 56 [48128/50176]	Loss: 0.2240
Training Epoch: 56 [48640/50176]	Loss: 0.3186
Training Epoch: 56 [49152/50176]	Loss: 0.2144
Training Epoch: 56 [49664/50176]	Loss: 0.1896
Training Epoch: 56 [50176/50176]	Loss: 0.2850
Validation Epoch: 56, Average loss: 0.0052, Accuracy: 0.5744
Training Epoch: 57 [512/50176]	Loss: 0.2210
Training Epoch: 57 [1024/50176]	Loss: 0.2276
Training Epoch: 57 [1536/50176]	Loss: 0.2155
Training Epoch: 57 [2048/50176]	Loss: 0.1961
Training Epoch: 57 [2560/50176]	Loss: 0.1793
Training Epoch: 57 [3072/50176]	Loss: 0.1966
Training Epoch: 57 [3584/50176]	Loss: 0.2003
Training Epoch: 57 [4096/50176]	Loss: 0.2501
Training Epoch: 57 [4608/50176]	Loss: 0.1693
Training Epoch: 57 [5120/50176]	Loss: 0.2166
Training Epoch: 57 [5632/50176]	Loss: 0.2273
Training Epoch: 57 [6144/50176]	Loss: 0.2400
Training Epoch: 57 [6656/50176]	Loss: 0.2611
Training Epoch: 57 [7168/50176]	Loss: 0.1776
Training Epoch: 57 [7680/50176]	Loss: 0.2122
Training Epoch: 57 [8192/50176]	Loss: 0.1972
Training Epoch: 57 [8704/50176]	Loss: 0.2297
Training Epoch: 57 [9216/50176]	Loss: 0.2040
Training Epoch: 57 [9728/50176]	Loss: 0.2440
Training Epoch: 57 [10240/50176]	Loss: 0.1756
Training Epoch: 57 [10752/50176]	Loss: 0.2273
Training Epoch: 57 [11264/50176]	Loss: 0.2198
Training Epoch: 57 [11776/50176]	Loss: 0.2024
Training Epoch: 57 [12288/50176]	Loss: 0.1786
Training Epoch: 57 [12800/50176]	Loss: 0.2286
Training Epoch: 57 [13312/50176]	Loss: 0.1726
Training Epoch: 57 [13824/50176]	Loss: 0.1854
Training Epoch: 57 [14336/50176]	Loss: 0.2042
Training Epoch: 57 [14848/50176]	Loss: 0.2147
Training Epoch: 57 [15360/50176]	Loss: 0.2213
Training Epoch: 57 [15872/50176]	Loss: 0.2167
Training Epoch: 57 [16384/50176]	Loss: 0.2639
Training Epoch: 57 [16896/50176]	Loss: 0.2149
Training Epoch: 57 [17408/50176]	Loss: 0.2216
Training Epoch: 57 [17920/50176]	Loss: 0.2564
Training Epoch: 57 [18432/50176]	Loss: 0.1675
Training Epoch: 57 [18944/50176]	Loss: 0.2681
Training Epoch: 57 [19456/50176]	Loss: 0.2492
Training Epoch: 57 [19968/50176]	Loss: 0.1946
Training Epoch: 57 [20480/50176]	Loss: 0.2401
Training Epoch: 57 [20992/50176]	Loss: 0.2424
Training Epoch: 57 [21504/50176]	Loss: 0.2103
Training Epoch: 57 [22016/50176]	Loss: 0.1602
Training Epoch: 57 [22528/50176]	Loss: 0.1722
Training Epoch: 57 [23040/50176]	Loss: 0.2232
Training Epoch: 57 [23552/50176]	Loss: 0.1874
Training Epoch: 57 [24064/50176]	Loss: 0.1704
Training Epoch: 57 [24576/50176]	Loss: 0.1964
Training Epoch: 57 [25088/50176]	Loss: 0.2742
Training Epoch: 57 [25600/50176]	Loss: 0.2587
Training Epoch: 57 [26112/50176]	Loss: 0.2509
Training Epoch: 57 [26624/50176]	Loss: 0.2094
Training Epoch: 57 [27136/50176]	Loss: 0.2510
Training Epoch: 57 [27648/50176]	Loss: 0.1831
Training Epoch: 57 [28160/50176]	Loss: 0.2537
Training Epoch: 57 [28672/50176]	Loss: 0.2355
Training Epoch: 57 [29184/50176]	Loss: 0.2068
Training Epoch: 57 [29696/50176]	Loss: 0.2575
Training Epoch: 57 [30208/50176]	Loss: 0.2230
Training Epoch: 57 [30720/50176]	Loss: 0.2472
Training Epoch: 57 [31232/50176]	Loss: 0.2300
Training Epoch: 57 [31744/50176]	Loss: 0.2395
Training Epoch: 57 [32256/50176]	Loss: 0.2632
Training Epoch: 57 [32768/50176]	Loss: 0.2332
Training Epoch: 57 [33280/50176]	Loss: 0.2149
Training Epoch: 57 [33792/50176]	Loss: 0.3107
Training Epoch: 57 [34304/50176]	Loss: 0.2210
Training Epoch: 57 [34816/50176]	Loss: 0.2293
Training Epoch: 57 [35328/50176]	Loss: 0.2277
Training Epoch: 57 [35840/50176]	Loss: 0.2435
Training Epoch: 57 [36352/50176]	Loss: 0.2067
Training Epoch: 57 [36864/50176]	Loss: 0.2617
Training Epoch: 57 [37376/50176]	Loss: 0.2420
Training Epoch: 57 [37888/50176]	Loss: 0.2201
Training Epoch: 57 [38400/50176]	Loss: 0.1894
Training Epoch: 57 [38912/50176]	Loss: 0.1908
Training Epoch: 57 [39424/50176]	Loss: 0.1976
Training Epoch: 57 [39936/50176]	Loss: 0.2366
Training Epoch: 57 [40448/50176]	Loss: 0.2570
Training Epoch: 57 [40960/50176]	Loss: 0.2424
Training Epoch: 57 [41472/50176]	Loss: 0.2643
Training Epoch: 57 [41984/50176]	Loss: 0.2338
Training Epoch: 57 [42496/50176]	Loss: 0.2228
Training Epoch: 57 [43008/50176]	Loss: 0.2667
Training Epoch: 57 [43520/50176]	Loss: 0.2701
Training Epoch: 57 [44032/50176]	Loss: 0.2220
Training Epoch: 57 [44544/50176]	Loss: 0.2685
Training Epoch: 57 [45056/50176]	Loss: 0.2141
Training Epoch: 57 [45568/50176]	Loss: 0.2266
Training Epoch: 57 [46080/50176]	Loss: 0.1973
Training Epoch: 57 [46592/50176]	Loss: 0.2680
Training Epoch: 57 [47104/50176]	Loss: 0.2514
Training Epoch: 57 [47616/50176]	Loss: 0.2466
Training Epoch: 57 [48128/50176]	Loss: 0.2566
Training Epoch: 57 [48640/50176]	Loss: 0.2759
Training Epoch: 57 [49152/50176]	Loss: 0.2395
Training Epoch: 57 [49664/50176]	Loss: 0.2801
Training Epoch: 57 [50176/50176]	Loss: 0.1932
Validation Epoch: 57, Average loss: 0.0055, Accuracy: 0.5611
Training Epoch: 58 [512/50176]	Loss: 0.2038
Training Epoch: 58 [1024/50176]	Loss: 0.1845
Training Epoch: 58 [1536/50176]	Loss: 0.1880
Training Epoch: 58 [2048/50176]	Loss: 0.2123
Training Epoch: 58 [2560/50176]	Loss: 0.1717
Training Epoch: 58 [3072/50176]	Loss: 0.2194
Training Epoch: 58 [3584/50176]	Loss: 0.2647
Training Epoch: 58 [4096/50176]	Loss: 0.2225
Training Epoch: 58 [4608/50176]	Loss: 0.2003
Training Epoch: 58 [5120/50176]	Loss: 0.1930
Training Epoch: 58 [5632/50176]	Loss: 0.2241
Training Epoch: 58 [6144/50176]	Loss: 0.2386
Training Epoch: 58 [6656/50176]	Loss: 0.2270
Training Epoch: 58 [7168/50176]	Loss: 0.1706
Training Epoch: 58 [7680/50176]	Loss: 0.2733
Training Epoch: 58 [8192/50176]	Loss: 0.2058
Training Epoch: 58 [8704/50176]	Loss: 0.2019
Training Epoch: 58 [9216/50176]	Loss: 0.1787
Training Epoch: 58 [9728/50176]	Loss: 0.2064
Training Epoch: 58 [10240/50176]	Loss: 0.2164
Training Epoch: 58 [10752/50176]	Loss: 0.2157
Training Epoch: 58 [11264/50176]	Loss: 0.2108
Training Epoch: 58 [11776/50176]	Loss: 0.1669
Training Epoch: 58 [12288/50176]	Loss: 0.2211
Training Epoch: 58 [12800/50176]	Loss: 0.2217
Training Epoch: 58 [13312/50176]	Loss: 0.2253
Training Epoch: 58 [13824/50176]	Loss: 0.1864
Training Epoch: 58 [14336/50176]	Loss: 0.2145
Training Epoch: 58 [14848/50176]	Loss: 0.1977
Training Epoch: 58 [15360/50176]	Loss: 0.1788
Training Epoch: 58 [15872/50176]	Loss: 0.2494
Training Epoch: 58 [16384/50176]	Loss: 0.1957
Training Epoch: 58 [16896/50176]	Loss: 0.1913
Training Epoch: 58 [17408/50176]	Loss: 0.2117
Training Epoch: 58 [17920/50176]	Loss: 0.2247
Training Epoch: 58 [18432/50176]	Loss: 0.1971
Training Epoch: 58 [18944/50176]	Loss: 0.2285
Training Epoch: 58 [19456/50176]	Loss: 0.2284
Training Epoch: 58 [19968/50176]	Loss: 0.2080
Training Epoch: 58 [20480/50176]	Loss: 0.1947
Training Epoch: 58 [20992/50176]	Loss: 0.2392
Training Epoch: 58 [21504/50176]	Loss: 0.2698
Training Epoch: 58 [22016/50176]	Loss: 0.1952
Training Epoch: 58 [22528/50176]	Loss: 0.1973
Training Epoch: 58 [23040/50176]	Loss: 0.2052
Training Epoch: 58 [23552/50176]	Loss: 0.1835
Training Epoch: 58 [24064/50176]	Loss: 0.2035
Training Epoch: 58 [24576/50176]	Loss: 0.2013
Training Epoch: 58 [25088/50176]	Loss: 0.2017
Training Epoch: 58 [25600/50176]	Loss: 0.2244
Training Epoch: 58 [26112/50176]	Loss: 0.2018
Training Epoch: 58 [26624/50176]	Loss: 0.1787
Training Epoch: 58 [27136/50176]	Loss: 0.2103
Training Epoch: 58 [27648/50176]	Loss: 0.2794
Training Epoch: 58 [28160/50176]	Loss: 0.1575
Training Epoch: 58 [28672/50176]	Loss: 0.1887
Training Epoch: 58 [29184/50176]	Loss: 0.2275
Training Epoch: 58 [29696/50176]	Loss: 0.1830
Training Epoch: 58 [30208/50176]	Loss: 0.2030
Training Epoch: 58 [30720/50176]	Loss: 0.2758
Training Epoch: 58 [31232/50176]	Loss: 0.2161
Training Epoch: 58 [31744/50176]	Loss: 0.1954
Training Epoch: 58 [32256/50176]	Loss: 0.2634
Training Epoch: 58 [32768/50176]	Loss: 0.2189
Training Epoch: 58 [33280/50176]	Loss: 0.1756
Training Epoch: 58 [33792/50176]	Loss: 0.2395
Training Epoch: 58 [34304/50176]	Loss: 0.2360
Training Epoch: 58 [34816/50176]	Loss: 0.2292
Training Epoch: 58 [35328/50176]	Loss: 0.1981
Training Epoch: 58 [35840/50176]	Loss: 0.1883
Training Epoch: 58 [36352/50176]	Loss: 0.2296
Training Epoch: 58 [36864/50176]	Loss: 0.2641
Training Epoch: 58 [37376/50176]	Loss: 0.2309
Training Epoch: 58 [37888/50176]	Loss: 0.2112
Training Epoch: 58 [38400/50176]	Loss: 0.2672
Training Epoch: 58 [38912/50176]	Loss: 0.2221
Training Epoch: 58 [39424/50176]	Loss: 0.2122
Training Epoch: 58 [39936/50176]	Loss: 0.1803
Training Epoch: 58 [40448/50176]	Loss: 0.2442
Training Epoch: 58 [40960/50176]	Loss: 0.2519
Training Epoch: 58 [41472/50176]	Loss: 0.2449
Training Epoch: 58 [41984/50176]	Loss: 0.2203
Training Epoch: 58 [42496/50176]	Loss: 0.2453
Training Epoch: 58 [43008/50176]	Loss: 0.2626
Training Epoch: 58 [43520/50176]	Loss: 0.2249
Training Epoch: 58 [44032/50176]	Loss: 0.2281
Training Epoch: 58 [44544/50176]	Loss: 0.2149
Training Epoch: 58 [45056/50176]	Loss: 0.2272
Training Epoch: 58 [45568/50176]	Loss: 0.2806
Training Epoch: 58 [46080/50176]	Loss: 0.2646
Training Epoch: 58 [46592/50176]	Loss: 0.2384
Training Epoch: 58 [47104/50176]	Loss: 0.2737
Training Epoch: 58 [47616/50176]	Loss: 0.1804
Training Epoch: 58 [48128/50176]	Loss: 0.2714
Training Epoch: 58 [48640/50176]	Loss: 0.2260
Training Epoch: 58 [49152/50176]	Loss: 0.2700
Training Epoch: 58 [49664/50176]	Loss: 0.2380
Training Epoch: 58 [50176/50176]	Loss: 0.2195
Validation Epoch: 58, Average loss: 0.0051, Accuracy: 0.5685
Training Epoch: 59 [512/50176]	Loss: 0.2162
Training Epoch: 59 [1024/50176]	Loss: 0.1999
Training Epoch: 59 [1536/50176]	Loss: 0.2018
Training Epoch: 59 [2048/50176]	Loss: 0.1628
Training Epoch: 59 [2560/50176]	Loss: 0.2213
Training Epoch: 59 [3072/50176]	Loss: 0.1722
Training Epoch: 59 [3584/50176]	Loss: 0.2003
Training Epoch: 59 [4096/50176]	Loss: 0.1527
Training Epoch: 59 [4608/50176]	Loss: 0.2173
Training Epoch: 59 [5120/50176]	Loss: 0.1912
Training Epoch: 59 [5632/50176]	Loss: 0.1943
Training Epoch: 59 [6144/50176]	Loss: 0.1990
Training Epoch: 59 [6656/50176]	Loss: 0.2474
Training Epoch: 59 [7168/50176]	Loss: 0.1647
Training Epoch: 59 [7680/50176]	Loss: 0.2084
Training Epoch: 59 [8192/50176]	Loss: 0.1956
Training Epoch: 59 [8704/50176]	Loss: 0.1926
Training Epoch: 59 [9216/50176]	Loss: 0.2130
Training Epoch: 59 [9728/50176]	Loss: 0.1526
Training Epoch: 59 [10240/50176]	Loss: 0.1762
Training Epoch: 59 [10752/50176]	Loss: 0.1986
Training Epoch: 59 [11264/50176]	Loss: 0.1921
Training Epoch: 59 [11776/50176]	Loss: 0.2298
Training Epoch: 59 [12288/50176]	Loss: 0.2027
Training Epoch: 59 [12800/50176]	Loss: 0.1744
Training Epoch: 59 [13312/50176]	Loss: 0.1743
Training Epoch: 59 [13824/50176]	Loss: 0.1961
Training Epoch: 59 [14336/50176]	Loss: 0.2201
Training Epoch: 59 [14848/50176]	Loss: 0.1860
Training Epoch: 59 [15360/50176]	Loss: 0.1786
Training Epoch: 59 [15872/50176]	Loss: 0.1762
Training Epoch: 59 [16384/50176]	Loss: 0.1763
Training Epoch: 59 [16896/50176]	Loss: 0.2217
Training Epoch: 59 [17408/50176]	Loss: 0.2015
Training Epoch: 59 [17920/50176]	Loss: 0.1714
Training Epoch: 59 [18432/50176]	Loss: 0.1849
Training Epoch: 59 [18944/50176]	Loss: 0.1911
Training Epoch: 59 [19456/50176]	Loss: 0.1954
Training Epoch: 59 [19968/50176]	Loss: 0.1857
Training Epoch: 59 [20480/50176]	Loss: 0.1843
Training Epoch: 59 [20992/50176]	Loss: 0.1679
Training Epoch: 59 [21504/50176]	Loss: 0.2183
Training Epoch: 59 [22016/50176]	Loss: 0.1843
Training Epoch: 59 [22528/50176]	Loss: 0.2172
Training Epoch: 59 [23040/50176]	Loss: 0.1849
Training Epoch: 59 [23552/50176]	Loss: 0.2557
Training Epoch: 59 [24064/50176]	Loss: 0.1733
Training Epoch: 59 [24576/50176]	Loss: 0.2510
Training Epoch: 59 [25088/50176]	Loss: 0.2236
Training Epoch: 59 [25600/50176]	Loss: 0.1998
Training Epoch: 59 [26112/50176]	Loss: 0.2158
Training Epoch: 59 [26624/50176]	Loss: 0.2301
Training Epoch: 59 [27136/50176]	Loss: 0.2221
Training Epoch: 59 [27648/50176]	Loss: 0.2125
Training Epoch: 59 [28160/50176]	Loss: 0.2216
Training Epoch: 59 [28672/50176]	Loss: 0.2418
Training Epoch: 59 [29184/50176]	Loss: 0.2157
Training Epoch: 59 [29696/50176]	Loss: 0.2474
Training Epoch: 59 [30208/50176]	Loss: 0.2326
Training Epoch: 59 [30720/50176]	Loss: 0.2169
Training Epoch: 59 [31232/50176]	Loss: 0.2376
Training Epoch: 59 [31744/50176]	Loss: 0.2090
Training Epoch: 59 [32256/50176]	Loss: 0.2001
Training Epoch: 59 [32768/50176]	Loss: 0.2124
Training Epoch: 59 [33280/50176]	Loss: 0.1860
Training Epoch: 59 [33792/50176]	Loss: 0.2456
Training Epoch: 59 [34304/50176]	Loss: 0.2224
Training Epoch: 59 [34816/50176]	Loss: 0.2634
Training Epoch: 59 [35328/50176]	Loss: 0.2707
Training Epoch: 59 [35840/50176]	Loss: 0.1614
Training Epoch: 59 [36352/50176]	Loss: 0.2158
Training Epoch: 59 [36864/50176]	Loss: 0.2488
Training Epoch: 59 [37376/50176]	Loss: 0.2381
Training Epoch: 59 [37888/50176]	Loss: 0.2472
Training Epoch: 59 [38400/50176]	Loss: 0.2920
Training Epoch: 59 [38912/50176]	Loss: 0.1930
Training Epoch: 59 [39424/50176]	Loss: 0.2525
Training Epoch: 59 [39936/50176]	Loss: 0.1917
Training Epoch: 59 [40448/50176]	Loss: 0.2580
Training Epoch: 59 [40960/50176]	Loss: 0.2090
Training Epoch: 59 [41472/50176]	Loss: 0.1835
Training Epoch: 59 [41984/50176]	Loss: 0.2081
Training Epoch: 59 [42496/50176]	Loss: 0.2827
Training Epoch: 59 [43008/50176]	Loss: 0.2201
Training Epoch: 59 [43520/50176]	Loss: 0.2929
Training Epoch: 59 [44032/50176]	Loss: 0.2148
Training Epoch: 59 [44544/50176]	Loss: 0.2968
Training Epoch: 59 [45056/50176]	Loss: 0.2113
Training Epoch: 59 [45568/50176]	Loss: 0.2335
Training Epoch: 59 [46080/50176]	Loss: 0.1996
Training Epoch: 59 [46592/50176]	Loss: 0.2615
Training Epoch: 59 [47104/50176]	Loss: 0.2356
Training Epoch: 59 [47616/50176]	Loss: 0.2398
Training Epoch: 59 [48128/50176]	Loss: 0.2557
Training Epoch: 59 [48640/50176]	Loss: 0.2441
Training Epoch: 59 [49152/50176]	Loss: 0.3001
Training Epoch: 59 [49664/50176]	Loss: 0.2598
Training Epoch: 59 [50176/50176]	Loss: 0.2216
Validation Epoch: 59, Average loss: 0.0053, Accuracy: 0.5772
Training Epoch: 60 [512/50176]	Loss: 0.2207
Training Epoch: 60 [1024/50176]	Loss: 0.1816
Training Epoch: 60 [1536/50176]	Loss: 0.1884
Training Epoch: 60 [2048/50176]	Loss: 0.1763
Training Epoch: 60 [2560/50176]	Loss: 0.2001
Training Epoch: 60 [3072/50176]	Loss: 0.2962
Training Epoch: 60 [3584/50176]	Loss: 0.2074
Training Epoch: 60 [4096/50176]	Loss: 0.2145
Training Epoch: 60 [4608/50176]	Loss: 0.1919
Training Epoch: 60 [5120/50176]	Loss: 0.2001
Training Epoch: 60 [5632/50176]	Loss: 0.2042
Training Epoch: 60 [6144/50176]	Loss: 0.1980
Training Epoch: 60 [6656/50176]	Loss: 0.2374
Training Epoch: 60 [7168/50176]	Loss: 0.1910
Training Epoch: 60 [7680/50176]	Loss: 0.2342
Training Epoch: 60 [8192/50176]	Loss: 0.1837
Training Epoch: 60 [8704/50176]	Loss: 0.1850
Training Epoch: 60 [9216/50176]	Loss: 0.2247
Training Epoch: 60 [9728/50176]	Loss: 0.1881
Training Epoch: 60 [10240/50176]	Loss: 0.2505
Training Epoch: 60 [10752/50176]	Loss: 0.1660
Training Epoch: 60 [11264/50176]	Loss: 0.2063
Training Epoch: 60 [11776/50176]	Loss: 0.2269
Training Epoch: 60 [12288/50176]	Loss: 0.1891
Training Epoch: 60 [12800/50176]	Loss: 0.1841
Training Epoch: 60 [13312/50176]	Loss: 0.2090
Training Epoch: 60 [13824/50176]	Loss: 0.1815
Training Epoch: 60 [14336/50176]	Loss: 0.2044
Training Epoch: 60 [14848/50176]	Loss: 0.1748
Training Epoch: 60 [15360/50176]	Loss: 0.2303
Training Epoch: 60 [15872/50176]	Loss: 0.2155
Training Epoch: 60 [16384/50176]	Loss: 0.1916
Training Epoch: 60 [16896/50176]	Loss: 0.1879
Training Epoch: 60 [17408/50176]	Loss: 0.2586
Training Epoch: 60 [17920/50176]	Loss: 0.2323
Training Epoch: 60 [18432/50176]	Loss: 0.1562
Training Epoch: 60 [18944/50176]	Loss: 0.2007
Training Epoch: 60 [19456/50176]	Loss: 0.2159
Training Epoch: 60 [19968/50176]	Loss: 0.1583
Training Epoch: 60 [20480/50176]	Loss: 0.1716
Training Epoch: 60 [20992/50176]	Loss: 0.1683
Training Epoch: 60 [21504/50176]	Loss: 0.2668
Training Epoch: 60 [22016/50176]	Loss: 0.2244
Training Epoch: 60 [22528/50176]	Loss: 0.2581
Training Epoch: 60 [23040/50176]	Loss: 0.1893
Training Epoch: 60 [23552/50176]	Loss: 0.1858
Training Epoch: 60 [24064/50176]	Loss: 0.2018
Training Epoch: 60 [24576/50176]	Loss: 0.1916
Training Epoch: 60 [25088/50176]	Loss: 0.1746
Training Epoch: 60 [25600/50176]	Loss: 0.2364
Training Epoch: 60 [26112/50176]	Loss: 0.2396
Training Epoch: 60 [26624/50176]	Loss: 0.1823
Training Epoch: 60 [27136/50176]	Loss: 0.1717
Training Epoch: 60 [27648/50176]	Loss: 0.1731
Training Epoch: 60 [28160/50176]	Loss: 0.2345
Training Epoch: 60 [28672/50176]	Loss: 0.2204
Training Epoch: 60 [29184/50176]	Loss: 0.1938
Training Epoch: 60 [29696/50176]	Loss: 0.2215
Training Epoch: 60 [30208/50176]	Loss: 0.1752
Training Epoch: 60 [30720/50176]	Loss: 0.2162
Training Epoch: 60 [31232/50176]	Loss: 0.2133
Training Epoch: 60 [31744/50176]	Loss: 0.2070
Training Epoch: 60 [32256/50176]	Loss: 0.2411
Training Epoch: 60 [32768/50176]	Loss: 0.2135
Training Epoch: 60 [33280/50176]	Loss: 0.2486
Training Epoch: 60 [33792/50176]	Loss: 0.2187
Training Epoch: 60 [34304/50176]	Loss: 0.2231
Training Epoch: 60 [34816/50176]	Loss: 0.2325
Training Epoch: 60 [35328/50176]	Loss: 0.2294
Training Epoch: 60 [35840/50176]	Loss: 0.1725
Training Epoch: 60 [36352/50176]	Loss: 0.2456
Training Epoch: 60 [36864/50176]	Loss: 0.2188
Training Epoch: 60 [37376/50176]	Loss: 0.2241
Training Epoch: 60 [37888/50176]	Loss: 0.2330
Training Epoch: 60 [38400/50176]	Loss: 0.2390
Training Epoch: 60 [38912/50176]	Loss: 0.2265
Training Epoch: 60 [39424/50176]	Loss: 0.2400
Training Epoch: 60 [39936/50176]	Loss: 0.2576
Training Epoch: 60 [40448/50176]	Loss: 0.2223
Training Epoch: 60 [40960/50176]	Loss: 0.2548
Training Epoch: 60 [41472/50176]	Loss: 0.1779
Training Epoch: 60 [41984/50176]	Loss: 0.2076
Training Epoch: 60 [42496/50176]	Loss: 0.2340
Training Epoch: 60 [43008/50176]	Loss: 0.2536
Training Epoch: 60 [43520/50176]	Loss: 0.1926
Training Epoch: 60 [44032/50176]	Loss: 0.2427
Training Epoch: 60 [44544/50176]	Loss: 0.2044
Training Epoch: 60 [45056/50176]	Loss: 0.2445
Training Epoch: 60 [45568/50176]	Loss: 0.2443
Training Epoch: 60 [46080/50176]	Loss: 0.2241
Training Epoch: 60 [46592/50176]	Loss: 0.2900
Training Epoch: 60 [47104/50176]	Loss: 0.2809
Training Epoch: 60 [47616/50176]	Loss: 0.2202
Training Epoch: 60 [48128/50176]	Loss: 0.2001
Training Epoch: 60 [48640/50176]	Loss: 0.3057
Training Epoch: 60 [49152/50176]	Loss: 0.2372
Training Epoch: 60 [49664/50176]	Loss: 0.2395
Training Epoch: 60 [50176/50176]	Loss: 0.2850
Validation Epoch: 60, Average loss: 0.0053, Accuracy: 0.5651
Training Epoch: 61 [512/50176]	Loss: 0.1670
Training Epoch: 61 [1024/50176]	Loss: 0.1986
Training Epoch: 61 [1536/50176]	Loss: 0.2243
Training Epoch: 61 [2048/50176]	Loss: 0.2036
Training Epoch: 61 [2560/50176]	Loss: 0.1796
Training Epoch: 61 [3072/50176]	Loss: 0.2338
Training Epoch: 61 [3584/50176]	Loss: 0.1948
Training Epoch: 61 [4096/50176]	Loss: 0.1987
Training Epoch: 61 [4608/50176]	Loss: 0.2029
Training Epoch: 61 [5120/50176]	Loss: 0.2588
Training Epoch: 61 [5632/50176]	Loss: 0.1954
Training Epoch: 61 [6144/50176]	Loss: 0.1665
Training Epoch: 61 [6656/50176]	Loss: 0.1535
Training Epoch: 61 [7168/50176]	Loss: 0.2354
Training Epoch: 61 [7680/50176]	Loss: 0.1778
Training Epoch: 61 [8192/50176]	Loss: 0.1794
Training Epoch: 61 [8704/50176]	Loss: 0.1952
Training Epoch: 61 [9216/50176]	Loss: 0.1947
Training Epoch: 61 [9728/50176]	Loss: 0.1973
Training Epoch: 61 [10240/50176]	Loss: 0.1893
Training Epoch: 61 [10752/50176]	Loss: 0.1908
Training Epoch: 61 [11264/50176]	Loss: 0.2084
Training Epoch: 61 [11776/50176]	Loss: 0.1772
Training Epoch: 61 [12288/50176]	Loss: 0.1780
Training Epoch: 61 [12800/50176]	Loss: 0.1615
Training Epoch: 61 [13312/50176]	Loss: 0.2644
Training Epoch: 61 [13824/50176]	Loss: 0.1458
Training Epoch: 61 [14336/50176]	Loss: 0.2218
Training Epoch: 61 [14848/50176]	Loss: 0.1835
Training Epoch: 61 [15360/50176]	Loss: 0.1793
Training Epoch: 61 [15872/50176]	Loss: 0.1931
Training Epoch: 61 [16384/50176]	Loss: 0.2162
Training Epoch: 61 [16896/50176]	Loss: 0.2155
Training Epoch: 61 [17408/50176]	Loss: 0.2197
Training Epoch: 61 [17920/50176]	Loss: 0.2090
Training Epoch: 61 [18432/50176]	Loss: 0.1759
Training Epoch: 61 [18944/50176]	Loss: 0.2194
Training Epoch: 61 [19456/50176]	Loss: 0.1565
Training Epoch: 61 [19968/50176]	Loss: 0.2190
Training Epoch: 61 [20480/50176]	Loss: 0.1812
Training Epoch: 61 [20992/50176]	Loss: 0.1691
Training Epoch: 61 [21504/50176]	Loss: 0.2091
Training Epoch: 61 [22016/50176]	Loss: 0.1683
Training Epoch: 61 [22528/50176]	Loss: 0.2238
Training Epoch: 61 [23040/50176]	Loss: 0.1944
Training Epoch: 61 [23552/50176]	Loss: 0.1947
Training Epoch: 61 [24064/50176]	Loss: 0.2201
Training Epoch: 61 [24576/50176]	Loss: 0.2183
Training Epoch: 61 [25088/50176]	Loss: 0.2327
Training Epoch: 61 [25600/50176]	Loss: 0.1745
Training Epoch: 61 [26112/50176]	Loss: 0.2157
Training Epoch: 61 [26624/50176]	Loss: 0.1895
Training Epoch: 61 [27136/50176]	Loss: 0.2065
Training Epoch: 61 [27648/50176]	Loss: 0.1979
Training Epoch: 61 [28160/50176]	Loss: 0.2743
Training Epoch: 61 [28672/50176]	Loss: 0.1628
Training Epoch: 61 [29184/50176]	Loss: 0.1787
Training Epoch: 61 [29696/50176]	Loss: 0.2570
Training Epoch: 61 [30208/50176]	Loss: 0.2547
Training Epoch: 61 [30720/50176]	Loss: 0.2239
Training Epoch: 61 [31232/50176]	Loss: 0.2270
Training Epoch: 61 [31744/50176]	Loss: 0.2149
Training Epoch: 61 [32256/50176]	Loss: 0.1858
Training Epoch: 61 [32768/50176]	Loss: 0.1568
Training Epoch: 61 [33280/50176]	Loss: 0.2382
Training Epoch: 61 [33792/50176]	Loss: 0.2127
Training Epoch: 61 [34304/50176]	Loss: 0.2184
Training Epoch: 61 [34816/50176]	Loss: 0.1955
Training Epoch: 61 [35328/50176]	Loss: 0.2074
Training Epoch: 61 [35840/50176]	Loss: 0.1777
Training Epoch: 61 [36352/50176]	Loss: 0.1950
Training Epoch: 61 [36864/50176]	Loss: 0.2416
Training Epoch: 61 [37376/50176]	Loss: 0.1738
Training Epoch: 61 [37888/50176]	Loss: 0.2763
Training Epoch: 61 [38400/50176]	Loss: 0.2073
Training Epoch: 61 [38912/50176]	Loss: 0.2674
Training Epoch: 61 [39424/50176]	Loss: 0.2234
Training Epoch: 61 [39936/50176]	Loss: 0.2103
Training Epoch: 61 [40448/50176]	Loss: 0.2410
Training Epoch: 61 [40960/50176]	Loss: 0.1695
Training Epoch: 61 [41472/50176]	Loss: 0.2223
Training Epoch: 61 [41984/50176]	Loss: 0.2300
Training Epoch: 61 [42496/50176]	Loss: 0.2197
Training Epoch: 61 [43008/50176]	Loss: 0.1882
Training Epoch: 61 [43520/50176]	Loss: 0.1931
Training Epoch: 61 [44032/50176]	Loss: 0.2208
Training Epoch: 61 [44544/50176]	Loss: 0.2265
Training Epoch: 61 [45056/50176]	Loss: 0.2037
Training Epoch: 61 [45568/50176]	Loss: 0.2219
Training Epoch: 61 [46080/50176]	Loss: 0.1951
Training Epoch: 61 [46592/50176]	Loss: 0.1839
Training Epoch: 61 [47104/50176]	Loss: 0.1685
Training Epoch: 61 [47616/50176]	Loss: 0.2194
Training Epoch: 61 [48128/50176]	Loss: 0.2249
Training Epoch: 61 [48640/50176]	Loss: 0.2419
Training Epoch: 61 [49152/50176]	Loss: 0.2151
Training Epoch: 61 [49664/50176]	Loss: 0.2273
Training Epoch: 61 [50176/50176]	Loss: 0.1563
Validation Epoch: 61, Average loss: 0.0055, Accuracy: 0.5765
Training Epoch: 62 [512/50176]	Loss: 0.1432
Training Epoch: 62 [1024/50176]	Loss: 0.1517
Training Epoch: 62 [1536/50176]	Loss: 0.1925
Training Epoch: 62 [2048/50176]	Loss: 0.2087
Training Epoch: 62 [2560/50176]	Loss: 0.1614
Training Epoch: 62 [3072/50176]	Loss: 0.1613
Training Epoch: 62 [3584/50176]	Loss: 0.2060
Training Epoch: 62 [4096/50176]	Loss: 0.1475
Training Epoch: 62 [4608/50176]	Loss: 0.1648
Training Epoch: 62 [5120/50176]	Loss: 0.2123
Training Epoch: 62 [5632/50176]	Loss: 0.1680
Training Epoch: 62 [6144/50176]	Loss: 0.2263
Training Epoch: 62 [6656/50176]	Loss: 0.1801
Training Epoch: 62 [7168/50176]	Loss: 0.1764
Training Epoch: 62 [7680/50176]	Loss: 0.1461
Training Epoch: 62 [8192/50176]	Loss: 0.1947
Training Epoch: 62 [8704/50176]	Loss: 0.1961
Training Epoch: 62 [9216/50176]	Loss: 0.1726
Training Epoch: 62 [9728/50176]	Loss: 0.2312
Training Epoch: 62 [10240/50176]	Loss: 0.1847
Training Epoch: 62 [10752/50176]	Loss: 0.2003
Training Epoch: 62 [11264/50176]	Loss: 0.1665
Training Epoch: 62 [11776/50176]	Loss: 0.2189
Training Epoch: 62 [12288/50176]	Loss: 0.1886
Training Epoch: 62 [12800/50176]	Loss: 0.1677
Training Epoch: 62 [13312/50176]	Loss: 0.1518
Training Epoch: 62 [13824/50176]	Loss: 0.1976
Training Epoch: 62 [14336/50176]	Loss: 0.1865
Training Epoch: 62 [14848/50176]	Loss: 0.1557
Training Epoch: 62 [15360/50176]	Loss: 0.1688
Training Epoch: 62 [15872/50176]	Loss: 0.2116
Training Epoch: 62 [16384/50176]	Loss: 0.1635
Training Epoch: 62 [16896/50176]	Loss: 0.2276
Training Epoch: 62 [17408/50176]	Loss: 0.1503
Training Epoch: 62 [17920/50176]	Loss: 0.1940
Training Epoch: 62 [18432/50176]	Loss: 0.1838
Training Epoch: 62 [18944/50176]	Loss: 0.2379
Training Epoch: 62 [19456/50176]	Loss: 0.2357
Training Epoch: 62 [19968/50176]	Loss: 0.1903
Training Epoch: 62 [20480/50176]	Loss: 0.1773
Training Epoch: 62 [20992/50176]	Loss: 0.1604
Training Epoch: 62 [21504/50176]	Loss: 0.1770
Training Epoch: 62 [22016/50176]	Loss: 0.1637
Training Epoch: 62 [22528/50176]	Loss: 0.1995
Training Epoch: 62 [23040/50176]	Loss: 0.2015
Training Epoch: 62 [23552/50176]	Loss: 0.2221
Training Epoch: 62 [24064/50176]	Loss: 0.2648
Training Epoch: 62 [24576/50176]	Loss: 0.1899
Training Epoch: 62 [25088/50176]	Loss: 0.1978
Training Epoch: 62 [25600/50176]	Loss: 0.1876
Training Epoch: 62 [26112/50176]	Loss: 0.1960
Training Epoch: 62 [26624/50176]	Loss: 0.1773
Training Epoch: 62 [27136/50176]	Loss: 0.1804
Training Epoch: 62 [27648/50176]	Loss: 0.1410
Training Epoch: 62 [28160/50176]	Loss: 0.1855
Training Epoch: 62 [28672/50176]	Loss: 0.1650
Training Epoch: 62 [29184/50176]	Loss: 0.1693
Training Epoch: 62 [29696/50176]	Loss: 0.1853
Training Epoch: 62 [30208/50176]	Loss: 0.1951
Training Epoch: 62 [30720/50176]	Loss: 0.1904
Training Epoch: 62 [31232/50176]	Loss: 0.2080
Training Epoch: 62 [31744/50176]	Loss: 0.2348
Training Epoch: 62 [32256/50176]	Loss: 0.1712
Training Epoch: 62 [32768/50176]	Loss: 0.2054
Training Epoch: 62 [33280/50176]	Loss: 0.1787
Training Epoch: 62 [33792/50176]	Loss: 0.2131
Training Epoch: 62 [34304/50176]	Loss: 0.2392
Training Epoch: 62 [34816/50176]	Loss: 0.2066
Training Epoch: 62 [35328/50176]	Loss: 0.2392
Training Epoch: 62 [35840/50176]	Loss: 0.2133
Training Epoch: 62 [36352/50176]	Loss: 0.1961
Training Epoch: 62 [36864/50176]	Loss: 0.2266
Training Epoch: 62 [37376/50176]	Loss: 0.1900
Training Epoch: 62 [37888/50176]	Loss: 0.2223
Training Epoch: 62 [38400/50176]	Loss: 0.3027
Training Epoch: 62 [38912/50176]	Loss: 0.1948
Training Epoch: 62 [39424/50176]	Loss: 0.2792
Training Epoch: 62 [39936/50176]	Loss: 0.1940
Training Epoch: 62 [40448/50176]	Loss: 0.2316
Training Epoch: 62 [40960/50176]	Loss: 0.1399
Training Epoch: 62 [41472/50176]	Loss: 0.2211
Training Epoch: 62 [41984/50176]	Loss: 0.2549
Training Epoch: 62 [42496/50176]	Loss: 0.2104
Training Epoch: 62 [43008/50176]	Loss: 0.2614
Training Epoch: 62 [43520/50176]	Loss: 0.1715
Training Epoch: 62 [44032/50176]	Loss: 0.2348
Training Epoch: 62 [44544/50176]	Loss: 0.2383
Training Epoch: 62 [45056/50176]	Loss: 0.2041
Training Epoch: 62 [45568/50176]	Loss: 0.1723
Training Epoch: 62 [46080/50176]	Loss: 0.2166
Training Epoch: 62 [46592/50176]	Loss: 0.2187
Training Epoch: 62 [47104/50176]	Loss: 0.1843
Training Epoch: 62 [47616/50176]	Loss: 0.2290
Training Epoch: 62 [48128/50176]	Loss: 0.2236
Training Epoch: 62 [48640/50176]	Loss: 0.2108
Training Epoch: 62 [49152/50176]	Loss: 0.2004
Training Epoch: 62 [49664/50176]	Loss: 0.2209
Training Epoch: 62 [50176/50176]	Loss: 0.2659
Validation Epoch: 62, Average loss: 0.0054, Accuracy: 0.5814
Training Epoch: 63 [512/50176]	Loss: 0.1651
Training Epoch: 63 [1024/50176]	Loss: 0.1501
Training Epoch: 63 [1536/50176]	Loss: 0.1904
Training Epoch: 63 [2048/50176]	Loss: 0.1597
Training Epoch: 63 [2560/50176]	Loss: 0.1419
Training Epoch: 63 [3072/50176]	Loss: 0.1615
Training Epoch: 63 [3584/50176]	Loss: 0.1712
Training Epoch: 63 [4096/50176]	Loss: 0.1719
Training Epoch: 63 [4608/50176]	Loss: 0.1674
Training Epoch: 63 [5120/50176]	Loss: 0.2048
Training Epoch: 63 [5632/50176]	Loss: 0.1219
Training Epoch: 63 [6144/50176]	Loss: 0.2226
Training Epoch: 63 [6656/50176]	Loss: 0.1908
Training Epoch: 63 [7168/50176]	Loss: 0.1466
Training Epoch: 63 [7680/50176]	Loss: 0.1664
Training Epoch: 63 [8192/50176]	Loss: 0.1693
Training Epoch: 63 [8704/50176]	Loss: 0.1825
Training Epoch: 63 [9216/50176]	Loss: 0.2264
Training Epoch: 63 [9728/50176]	Loss: 0.1769
Training Epoch: 63 [10240/50176]	Loss: 0.1916
Training Epoch: 63 [10752/50176]	Loss: 0.1770
Training Epoch: 63 [11264/50176]	Loss: 0.1940
Training Epoch: 63 [11776/50176]	Loss: 0.1803
Training Epoch: 63 [12288/50176]	Loss: 0.1577
Training Epoch: 63 [12800/50176]	Loss: 0.1786
Training Epoch: 63 [13312/50176]	Loss: 0.1780
Training Epoch: 63 [13824/50176]	Loss: 0.1734
Training Epoch: 63 [14336/50176]	Loss: 0.1251
Training Epoch: 63 [14848/50176]	Loss: 0.1917
Training Epoch: 63 [15360/50176]	Loss: 0.1615
Training Epoch: 63 [15872/50176]	Loss: 0.1619
Training Epoch: 63 [16384/50176]	Loss: 0.1908
Training Epoch: 63 [16896/50176]	Loss: 0.2326
Training Epoch: 63 [17408/50176]	Loss: 0.1860
Training Epoch: 63 [17920/50176]	Loss: 0.1445
Training Epoch: 63 [18432/50176]	Loss: 0.2418
Training Epoch: 63 [18944/50176]	Loss: 0.1812
Training Epoch: 63 [19456/50176]	Loss: 0.1698
Training Epoch: 63 [19968/50176]	Loss: 0.1792
Training Epoch: 63 [20480/50176]	Loss: 0.1821
Training Epoch: 63 [20992/50176]	Loss: 0.2081
Training Epoch: 63 [21504/50176]	Loss: 0.2136
Training Epoch: 63 [22016/50176]	Loss: 0.1762
Training Epoch: 63 [22528/50176]	Loss: 0.2065
Training Epoch: 63 [23040/50176]	Loss: 0.1912
Training Epoch: 63 [23552/50176]	Loss: 0.1523
Training Epoch: 63 [24064/50176]	Loss: 0.1945
Training Epoch: 63 [24576/50176]	Loss: 0.1889
Training Epoch: 63 [25088/50176]	Loss: 0.1860
Training Epoch: 63 [25600/50176]	Loss: 0.1589
Training Epoch: 63 [26112/50176]	Loss: 0.1619
Training Epoch: 63 [26624/50176]	Loss: 0.1895
Training Epoch: 63 [27136/50176]	Loss: 0.1863
Training Epoch: 63 [27648/50176]	Loss: 0.1665
Training Epoch: 63 [28160/50176]	Loss: 0.1768
Training Epoch: 63 [28672/50176]	Loss: 0.1805
Training Epoch: 63 [29184/50176]	Loss: 0.1737
Training Epoch: 63 [29696/50176]	Loss: 0.1691
Training Epoch: 63 [30208/50176]	Loss: 0.2026
Training Epoch: 63 [30720/50176]	Loss: 0.2030
Training Epoch: 63 [31232/50176]	Loss: 0.1992
Training Epoch: 63 [31744/50176]	Loss: 0.2597
Training Epoch: 63 [32256/50176]	Loss: 0.1903
Training Epoch: 63 [32768/50176]	Loss: 0.1788
Training Epoch: 63 [33280/50176]	Loss: 0.1505
Training Epoch: 63 [33792/50176]	Loss: 0.1952
Training Epoch: 63 [34304/50176]	Loss: 0.2183
Training Epoch: 63 [34816/50176]	Loss: 0.2246
Training Epoch: 63 [35328/50176]	Loss: 0.2001
Training Epoch: 63 [35840/50176]	Loss: 0.2129
Training Epoch: 63 [36352/50176]	Loss: 0.1404
Training Epoch: 63 [36864/50176]	Loss: 0.2171
Training Epoch: 63 [37376/50176]	Loss: 0.2610
Training Epoch: 63 [37888/50176]	Loss: 0.2492
Training Epoch: 63 [38400/50176]	Loss: 0.1935
Training Epoch: 63 [38912/50176]	Loss: 0.2317
Training Epoch: 63 [39424/50176]	Loss: 0.2192
Training Epoch: 63 [39936/50176]	Loss: 0.1814
Training Epoch: 63 [40448/50176]	Loss: 0.1812
Training Epoch: 63 [40960/50176]	Loss: 0.2318
Training Epoch: 63 [41472/50176]	Loss: 0.2371
Training Epoch: 63 [41984/50176]	Loss: 0.2545
Training Epoch: 63 [42496/50176]	Loss: 0.1859
Training Epoch: 63 [43008/50176]	Loss: 0.1820
Training Epoch: 63 [43520/50176]	Loss: 0.2360
Training Epoch: 63 [44032/50176]	Loss: 0.2124
Training Epoch: 63 [44544/50176]	Loss: 0.2005
Training Epoch: 63 [45056/50176]	Loss: 0.2079
Training Epoch: 63 [45568/50176]	Loss: 0.1833
Training Epoch: 63 [46080/50176]	Loss: 0.2107
Training Epoch: 63 [46592/50176]	Loss: 0.2006
Training Epoch: 63 [47104/50176]	Loss: 0.2502
Training Epoch: 63 [47616/50176]	Loss: 0.2345
Training Epoch: 63 [48128/50176]	Loss: 0.2049
Training Epoch: 63 [48640/50176]	Loss: 0.2400
Training Epoch: 63 [49152/50176]	Loss: 0.2092
Training Epoch: 63 [49664/50176]	Loss: 0.2064
Training Epoch: 63 [50176/50176]	Loss: 0.1940
Validation Epoch: 63, Average loss: 0.0050, Accuracy: 0.5801
Training Epoch: 64 [512/50176]	Loss: 0.1716
Training Epoch: 64 [1024/50176]	Loss: 0.1424
Training Epoch: 64 [1536/50176]	Loss: 0.1320
Training Epoch: 64 [2048/50176]	Loss: 0.1260
Training Epoch: 64 [2560/50176]	Loss: 0.2128
Training Epoch: 64 [3072/50176]	Loss: 0.1252
Training Epoch: 64 [3584/50176]	Loss: 0.1756
Training Epoch: 64 [4096/50176]	Loss: 0.1376
Training Epoch: 64 [4608/50176]	Loss: 0.2116
Training Epoch: 64 [5120/50176]	Loss: 0.1155
Training Epoch: 64 [5632/50176]	Loss: 0.2541
Training Epoch: 64 [6144/50176]	Loss: 0.1737
Training Epoch: 64 [6656/50176]	Loss: 0.1683
Training Epoch: 64 [7168/50176]	Loss: 0.1761
Training Epoch: 64 [7680/50176]	Loss: 0.1527
Training Epoch: 64 [8192/50176]	Loss: 0.1874
Training Epoch: 64 [8704/50176]	Loss: 0.2294
Training Epoch: 64 [9216/50176]	Loss: 0.1612
Training Epoch: 64 [9728/50176]	Loss: 0.1477
Training Epoch: 64 [10240/50176]	Loss: 0.1708
Training Epoch: 64 [10752/50176]	Loss: 0.1598
Training Epoch: 64 [11264/50176]	Loss: 0.1680
Training Epoch: 64 [11776/50176]	Loss: 0.1198
Training Epoch: 64 [12288/50176]	Loss: 0.2203
Training Epoch: 64 [12800/50176]	Loss: 0.1733
Training Epoch: 64 [13312/50176]	Loss: 0.1576
Training Epoch: 64 [13824/50176]	Loss: 0.1757
Training Epoch: 64 [14336/50176]	Loss: 0.1604
Training Epoch: 64 [14848/50176]	Loss: 0.2032
Training Epoch: 64 [15360/50176]	Loss: 0.1810
Training Epoch: 64 [15872/50176]	Loss: 0.2021
Training Epoch: 64 [16384/50176]	Loss: 0.2044
Training Epoch: 64 [16896/50176]	Loss: 0.1770
Training Epoch: 64 [17408/50176]	Loss: 0.2304
Training Epoch: 64 [17920/50176]	Loss: 0.1065
Training Epoch: 64 [18432/50176]	Loss: 0.1745
Training Epoch: 64 [18944/50176]	Loss: 0.1949
Training Epoch: 64 [19456/50176]	Loss: 0.1587
Training Epoch: 64 [19968/50176]	Loss: 0.1956
Training Epoch: 64 [20480/50176]	Loss: 0.1731
Training Epoch: 64 [20992/50176]	Loss: 0.1949
Training Epoch: 64 [21504/50176]	Loss: 0.1389
Training Epoch: 64 [22016/50176]	Loss: 0.1643
Training Epoch: 64 [22528/50176]	Loss: 0.1762
Training Epoch: 64 [23040/50176]	Loss: 0.1377
Training Epoch: 64 [23552/50176]	Loss: 0.1962
Training Epoch: 64 [24064/50176]	Loss: 0.1768
Training Epoch: 64 [24576/50176]	Loss: 0.2197
Training Epoch: 64 [25088/50176]	Loss: 0.1760
Training Epoch: 64 [25600/50176]	Loss: 0.1887
Training Epoch: 64 [26112/50176]	Loss: 0.1910
Training Epoch: 64 [26624/50176]	Loss: 0.1607
Training Epoch: 64 [27136/50176]	Loss: 0.1621
Training Epoch: 64 [27648/50176]	Loss: 0.2314
Training Epoch: 64 [28160/50176]	Loss: 0.1864
Training Epoch: 64 [28672/50176]	Loss: 0.1834
Training Epoch: 64 [29184/50176]	Loss: 0.1562
Training Epoch: 64 [29696/50176]	Loss: 0.2093
Training Epoch: 64 [30208/50176]	Loss: 0.2501
Training Epoch: 64 [30720/50176]	Loss: 0.1988
Training Epoch: 64 [31232/50176]	Loss: 0.1539
Training Epoch: 64 [31744/50176]	Loss: 0.2310
Training Epoch: 64 [32256/50176]	Loss: 0.1552
Training Epoch: 64 [32768/50176]	Loss: 0.2271
Training Epoch: 64 [33280/50176]	Loss: 0.2112
Training Epoch: 64 [33792/50176]	Loss: 0.1960
Training Epoch: 64 [34304/50176]	Loss: 0.2001
Training Epoch: 64 [34816/50176]	Loss: 0.2216
Training Epoch: 64 [35328/50176]	Loss: 0.1530
Training Epoch: 64 [35840/50176]	Loss: 0.2032
Training Epoch: 64 [36352/50176]	Loss: 0.2179
Training Epoch: 64 [36864/50176]	Loss: 0.1611
Training Epoch: 64 [37376/50176]	Loss: 0.2012
Training Epoch: 64 [37888/50176]	Loss: 0.2032
Training Epoch: 64 [38400/50176]	Loss: 0.2536
Training Epoch: 64 [38912/50176]	Loss: 0.2126
Training Epoch: 64 [39424/50176]	Loss: 0.2130
Training Epoch: 64 [39936/50176]	Loss: 0.1908
Training Epoch: 64 [40448/50176]	Loss: 0.1982
Training Epoch: 64 [40960/50176]	Loss: 0.2090
Training Epoch: 64 [41472/50176]	Loss: 0.2283
Training Epoch: 64 [41984/50176]	Loss: 0.1849
Training Epoch: 64 [42496/50176]	Loss: 0.1983
Training Epoch: 64 [43008/50176]	Loss: 0.2270
Training Epoch: 64 [43520/50176]	Loss: 0.1983
Training Epoch: 64 [44032/50176]	Loss: 0.2649
Training Epoch: 64 [44544/50176]	Loss: 0.2599
Training Epoch: 64 [45056/50176]	Loss: 0.1626
Training Epoch: 64 [45568/50176]	Loss: 0.2074
Training Epoch: 64 [46080/50176]	Loss: 0.1990
Training Epoch: 64 [46592/50176]	Loss: 0.1956
Training Epoch: 64 [47104/50176]	Loss: 0.2929
Training Epoch: 64 [47616/50176]	Loss: 0.2207
Training Epoch: 64 [48128/50176]	Loss: 0.2491
Training Epoch: 64 [48640/50176]	Loss: 0.1203
Training Epoch: 64 [49152/50176]	Loss: 0.1744
Training Epoch: 64 [49664/50176]	Loss: 0.2611
Training Epoch: 64 [50176/50176]	Loss: 0.1869
Validation Epoch: 64, Average loss: 0.0054, Accuracy: 0.5775
Training Epoch: 65 [512/50176]	Loss: 0.1885
Training Epoch: 65 [1024/50176]	Loss: 0.1489
Training Epoch: 65 [1536/50176]	Loss: 0.1536
Training Epoch: 65 [2048/50176]	Loss: 0.1419
Training Epoch: 65 [2560/50176]	Loss: 0.2027
Training Epoch: 65 [3072/50176]	Loss: 0.2129
Training Epoch: 65 [3584/50176]	Loss: 0.1707
Training Epoch: 65 [4096/50176]	Loss: 0.1654
Training Epoch: 65 [4608/50176]	Loss: 0.1565
Training Epoch: 65 [5120/50176]	Loss: 0.1623
Training Epoch: 65 [5632/50176]	Loss: 0.1430
Training Epoch: 65 [6144/50176]	Loss: 0.1499
Training Epoch: 65 [6656/50176]	Loss: 0.1258
Training Epoch: 65 [7168/50176]	Loss: 0.1605
Training Epoch: 65 [7680/50176]	Loss: 0.1877
Training Epoch: 65 [8192/50176]	Loss: 0.1920
Training Epoch: 65 [8704/50176]	Loss: 0.1742
Training Epoch: 65 [9216/50176]	Loss: 0.2003
Training Epoch: 65 [9728/50176]	Loss: 0.1792
Training Epoch: 65 [10240/50176]	Loss: 0.1793
Training Epoch: 65 [10752/50176]	Loss: 0.1616
Training Epoch: 65 [11264/50176]	Loss: 0.1608
Training Epoch: 65 [11776/50176]	Loss: 0.1950
Training Epoch: 65 [12288/50176]	Loss: 0.2026
Training Epoch: 65 [12800/50176]	Loss: 0.1765
Training Epoch: 65 [13312/50176]	Loss: 0.1509
Training Epoch: 65 [13824/50176]	Loss: 0.1435
Training Epoch: 65 [14336/50176]	Loss: 0.1972
Training Epoch: 65 [14848/50176]	Loss: 0.1971
Training Epoch: 65 [15360/50176]	Loss: 0.1963
Training Epoch: 65 [15872/50176]	Loss: 0.1934
Training Epoch: 65 [16384/50176]	Loss: 0.2124
Training Epoch: 65 [16896/50176]	Loss: 0.1526
Training Epoch: 65 [17408/50176]	Loss: 0.1585
Training Epoch: 65 [17920/50176]	Loss: 0.2219
Training Epoch: 65 [18432/50176]	Loss: 0.1967
Training Epoch: 65 [18944/50176]	Loss: 0.2048
Training Epoch: 65 [19456/50176]	Loss: 0.2098
Training Epoch: 65 [19968/50176]	Loss: 0.2058
Training Epoch: 65 [20480/50176]	Loss: 0.1976
Training Epoch: 65 [20992/50176]	Loss: 0.1880
Training Epoch: 65 [21504/50176]	Loss: 0.1507
Training Epoch: 65 [22016/50176]	Loss: 0.2127
Training Epoch: 65 [22528/50176]	Loss: 0.1583
Training Epoch: 65 [23040/50176]	Loss: 0.1611
Training Epoch: 65 [23552/50176]	Loss: 0.2394
Training Epoch: 65 [24064/50176]	Loss: 0.1756
Training Epoch: 65 [24576/50176]	Loss: 0.1750
Training Epoch: 65 [25088/50176]	Loss: 0.1769
Training Epoch: 65 [25600/50176]	Loss: 0.1660
Training Epoch: 65 [26112/50176]	Loss: 0.1767
Training Epoch: 65 [26624/50176]	Loss: 0.1760
Training Epoch: 65 [27136/50176]	Loss: 0.1956
Training Epoch: 65 [27648/50176]	Loss: 0.2072
Training Epoch: 65 [28160/50176]	Loss: 0.2193
Training Epoch: 65 [28672/50176]	Loss: 0.1503
Training Epoch: 65 [29184/50176]	Loss: 0.1816
Training Epoch: 65 [29696/50176]	Loss: 0.1861
Training Epoch: 65 [30208/50176]	Loss: 0.2395
Training Epoch: 65 [30720/50176]	Loss: 0.1906
Training Epoch: 65 [31232/50176]	Loss: 0.2644
Training Epoch: 65 [31744/50176]	Loss: 0.1966
Training Epoch: 65 [32256/50176]	Loss: 0.2077
Training Epoch: 65 [32768/50176]	Loss: 0.1642
Training Epoch: 65 [33280/50176]	Loss: 0.2052
Training Epoch: 65 [33792/50176]	Loss: 0.2093
Training Epoch: 65 [34304/50176]	Loss: 0.1827
Training Epoch: 65 [34816/50176]	Loss: 0.2229
Training Epoch: 65 [35328/50176]	Loss: 0.2091
Training Epoch: 65 [35840/50176]	Loss: 0.1993
Training Epoch: 65 [36352/50176]	Loss: 0.1957
Training Epoch: 65 [36864/50176]	Loss: 0.1545
Training Epoch: 65 [37376/50176]	Loss: 0.2190
Training Epoch: 65 [37888/50176]	Loss: 0.1788
Training Epoch: 65 [38400/50176]	Loss: 0.1914
Training Epoch: 65 [38912/50176]	Loss: 0.1875
Training Epoch: 65 [39424/50176]	Loss: 0.2072
Training Epoch: 65 [39936/50176]	Loss: 0.1534
Training Epoch: 65 [40448/50176]	Loss: 0.1949
Training Epoch: 65 [40960/50176]	Loss: 0.1976
Training Epoch: 65 [41472/50176]	Loss: 0.1956
Training Epoch: 65 [41984/50176]	Loss: 0.2144
Training Epoch: 65 [42496/50176]	Loss: 0.1751
Training Epoch: 65 [43008/50176]	Loss: 0.1756
Training Epoch: 65 [43520/50176]	Loss: 0.2061
Training Epoch: 65 [44032/50176]	Loss: 0.2423
Training Epoch: 65 [44544/50176]	Loss: 0.2405
Training Epoch: 65 [45056/50176]	Loss: 0.2179
Training Epoch: 65 [45568/50176]	Loss: 0.1814
Training Epoch: 65 [46080/50176]	Loss: 0.1939
Training Epoch: 65 [46592/50176]	Loss: 0.2204
Training Epoch: 65 [47104/50176]	Loss: 0.2378
Training Epoch: 65 [47616/50176]	Loss: 0.2035
Training Epoch: 65 [48128/50176]	Loss: 0.2069
Training Epoch: 65 [48640/50176]	Loss: 0.2458
Training Epoch: 65 [49152/50176]	Loss: 0.2214
Training Epoch: 65 [49664/50176]	Loss: 0.2150
Training Epoch: 65 [50176/50176]	Loss: 0.1739
Validation Epoch: 65, Average loss: 0.0052, Accuracy: 0.5870
Training Epoch: 66 [512/50176]	Loss: 0.1786
Training Epoch: 66 [1024/50176]	Loss: 0.1585
Training Epoch: 66 [1536/50176]	Loss: 0.1836
Training Epoch: 66 [2048/50176]	Loss: 0.1837
Training Epoch: 66 [2560/50176]	Loss: 0.1723
Training Epoch: 66 [3072/50176]	Loss: 0.1835
Training Epoch: 66 [3584/50176]	Loss: 0.1956
Training Epoch: 66 [4096/50176]	Loss: 0.1931
Training Epoch: 66 [4608/50176]	Loss: 0.2177
Training Epoch: 66 [5120/50176]	Loss: 0.1820
Training Epoch: 66 [5632/50176]	Loss: 0.2035
Training Epoch: 66 [6144/50176]	Loss: 0.1933
Training Epoch: 66 [6656/50176]	Loss: 0.2168
Training Epoch: 66 [7168/50176]	Loss: 0.1928
Training Epoch: 66 [7680/50176]	Loss: 0.1987
Training Epoch: 66 [8192/50176]	Loss: 0.1670
Training Epoch: 66 [8704/50176]	Loss: 0.1842
Training Epoch: 66 [9216/50176]	Loss: 0.1905
Training Epoch: 66 [9728/50176]	Loss: 0.1955
Training Epoch: 66 [10240/50176]	Loss: 0.1902
Training Epoch: 66 [10752/50176]	Loss: 0.1422
Training Epoch: 66 [11264/50176]	Loss: 0.1768
Training Epoch: 66 [11776/50176]	Loss: 0.1497
Training Epoch: 66 [12288/50176]	Loss: 0.1666
Training Epoch: 66 [12800/50176]	Loss: 0.1586
Training Epoch: 66 [13312/50176]	Loss: 0.1696
Training Epoch: 66 [13824/50176]	Loss: 0.1794
Training Epoch: 66 [14336/50176]	Loss: 0.1670
Training Epoch: 66 [14848/50176]	Loss: 0.1743
Training Epoch: 66 [15360/50176]	Loss: 0.1631
Training Epoch: 66 [15872/50176]	Loss: 0.1633
Training Epoch: 66 [16384/50176]	Loss: 0.1867
Training Epoch: 66 [16896/50176]	Loss: 0.1870
Training Epoch: 66 [17408/50176]	Loss: 0.1876
Training Epoch: 66 [17920/50176]	Loss: 0.1808
Training Epoch: 66 [18432/50176]	Loss: 0.1911
Training Epoch: 66 [18944/50176]	Loss: 0.1723
Training Epoch: 66 [19456/50176]	Loss: 0.1364
Training Epoch: 66 [19968/50176]	Loss: 0.1780
Training Epoch: 66 [20480/50176]	Loss: 0.1779
Training Epoch: 66 [20992/50176]	Loss: 0.1476
Training Epoch: 66 [21504/50176]	Loss: 0.2182
Training Epoch: 66 [22016/50176]	Loss: 0.1734
Training Epoch: 66 [22528/50176]	Loss: 0.1675
Training Epoch: 66 [23040/50176]	Loss: 0.1623
Training Epoch: 66 [23552/50176]	Loss: 0.2354
Training Epoch: 66 [24064/50176]	Loss: 0.1582
Training Epoch: 66 [24576/50176]	Loss: 0.1834
Training Epoch: 66 [25088/50176]	Loss: 0.1952
Training Epoch: 66 [25600/50176]	Loss: 0.1519
Training Epoch: 66 [26112/50176]	Loss: 0.1535
Training Epoch: 66 [26624/50176]	Loss: 0.2367
Training Epoch: 66 [27136/50176]	Loss: 0.2517
Training Epoch: 66 [27648/50176]	Loss: 0.2026
Training Epoch: 66 [28160/50176]	Loss: 0.2658
Training Epoch: 66 [28672/50176]	Loss: 0.2102
Training Epoch: 66 [29184/50176]	Loss: 0.1421
Training Epoch: 66 [29696/50176]	Loss: 0.1880
Training Epoch: 66 [30208/50176]	Loss: 0.1856
Training Epoch: 66 [30720/50176]	Loss: 0.2214
Training Epoch: 66 [31232/50176]	Loss: 0.2435
Training Epoch: 66 [31744/50176]	Loss: 0.1759
Training Epoch: 66 [32256/50176]	Loss: 0.1897
Training Epoch: 66 [32768/50176]	Loss: 0.1995
Training Epoch: 66 [33280/50176]	Loss: 0.1995
Training Epoch: 66 [33792/50176]	Loss: 0.1922
Training Epoch: 66 [34304/50176]	Loss: 0.1668
Training Epoch: 66 [34816/50176]	Loss: 0.1778
Training Epoch: 66 [35328/50176]	Loss: 0.1866
Training Epoch: 66 [35840/50176]	Loss: 0.2196
Training Epoch: 66 [36352/50176]	Loss: 0.1888
Training Epoch: 66 [36864/50176]	Loss: 0.2211
Training Epoch: 66 [37376/50176]	Loss: 0.2500
Training Epoch: 66 [37888/50176]	Loss: 0.1752
Training Epoch: 66 [38400/50176]	Loss: 0.2504
Training Epoch: 66 [38912/50176]	Loss: 0.1461
Training Epoch: 66 [39424/50176]	Loss: 0.2458
Training Epoch: 66 [39936/50176]	Loss: 0.2980
Training Epoch: 66 [40448/50176]	Loss: 0.2136
Training Epoch: 66 [40960/50176]	Loss: 0.2242
Training Epoch: 66 [41472/50176]	Loss: 0.2657
Training Epoch: 66 [41984/50176]	Loss: 0.2361
Training Epoch: 66 [42496/50176]	Loss: 0.1736
Training Epoch: 66 [43008/50176]	Loss: 0.1936
Training Epoch: 66 [43520/50176]	Loss: 0.1925
Training Epoch: 66 [44032/50176]	Loss: 0.2271
Training Epoch: 66 [44544/50176]	Loss: 0.1818
Training Epoch: 66 [45056/50176]	Loss: 0.2348
Training Epoch: 66 [45568/50176]	Loss: 0.2162
Training Epoch: 66 [46080/50176]	Loss: 0.2109
Training Epoch: 66 [46592/50176]	Loss: 0.1788
Training Epoch: 66 [47104/50176]	Loss: 0.1496
Training Epoch: 66 [47616/50176]	Loss: 0.2103
Training Epoch: 66 [48128/50176]	Loss: 0.1656
Training Epoch: 66 [48640/50176]	Loss: 0.2682
Training Epoch: 66 [49152/50176]	Loss: 0.2141
Training Epoch: 66 [49664/50176]	Loss: 0.2458
Training Epoch: 66 [50176/50176]	Loss: 0.1740
Validation Epoch: 66, Average loss: 0.0053, Accuracy: 0.5828
Training Epoch: 67 [512/50176]	Loss: 0.1568
Training Epoch: 67 [1024/50176]	Loss: 0.1509
Training Epoch: 67 [1536/50176]	Loss: 0.1661
Training Epoch: 67 [2048/50176]	Loss: 0.1346
Training Epoch: 67 [2560/50176]	Loss: 0.1680
Training Epoch: 67 [3072/50176]	Loss: 0.1750
Training Epoch: 67 [3584/50176]	Loss: 0.1657
Training Epoch: 67 [4096/50176]	Loss: 0.1350
Training Epoch: 67 [4608/50176]	Loss: 0.1543
Training Epoch: 67 [5120/50176]	Loss: 0.1582
Training Epoch: 67 [5632/50176]	Loss: 0.1811
Training Epoch: 67 [6144/50176]	Loss: 0.1893
Training Epoch: 67 [6656/50176]	Loss: 0.1842
Training Epoch: 67 [7168/50176]	Loss: 0.1325
Training Epoch: 67 [7680/50176]	Loss: 0.1981
Training Epoch: 67 [8192/50176]	Loss: 0.1699
Training Epoch: 67 [8704/50176]	Loss: 0.1384
Training Epoch: 67 [9216/50176]	Loss: 0.1707
Training Epoch: 67 [9728/50176]	Loss: 0.1599
Training Epoch: 67 [10240/50176]	Loss: 0.1437
Training Epoch: 67 [10752/50176]	Loss: 0.1603
Training Epoch: 67 [11264/50176]	Loss: 0.1522
Training Epoch: 67 [11776/50176]	Loss: 0.1662
Training Epoch: 67 [12288/50176]	Loss: 0.1651
Training Epoch: 67 [12800/50176]	Loss: 0.1932
Training Epoch: 67 [13312/50176]	Loss: 0.1320
Training Epoch: 67 [13824/50176]	Loss: 0.1883
Training Epoch: 67 [14336/50176]	Loss: 0.1474
Training Epoch: 67 [14848/50176]	Loss: 0.1887
Training Epoch: 67 [15360/50176]	Loss: 0.1615
Training Epoch: 67 [15872/50176]	Loss: 0.1681
Training Epoch: 67 [16384/50176]	Loss: 0.1526
Training Epoch: 67 [16896/50176]	Loss: 0.1828
Training Epoch: 67 [17408/50176]	Loss: 0.2383
Training Epoch: 67 [17920/50176]	Loss: 0.1638
Training Epoch: 67 [18432/50176]	Loss: 0.2181
Training Epoch: 67 [18944/50176]	Loss: 0.1541
Training Epoch: 67 [19456/50176]	Loss: 0.1800
Training Epoch: 67 [19968/50176]	Loss: 0.1819
Training Epoch: 67 [20480/50176]	Loss: 0.2202
Training Epoch: 67 [20992/50176]	Loss: 0.1548
Training Epoch: 67 [21504/50176]	Loss: 0.1671
Training Epoch: 67 [22016/50176]	Loss: 0.1297
Training Epoch: 67 [22528/50176]	Loss: 0.1818
Training Epoch: 67 [23040/50176]	Loss: 0.1399
Training Epoch: 67 [23552/50176]	Loss: 0.2141
Training Epoch: 67 [24064/50176]	Loss: 0.1332
Training Epoch: 67 [24576/50176]	Loss: 0.1436
Training Epoch: 67 [25088/50176]	Loss: 0.2012
Training Epoch: 67 [25600/50176]	Loss: 0.1620
Training Epoch: 67 [26112/50176]	Loss: 0.1447
Training Epoch: 67 [26624/50176]	Loss: 0.1516
Training Epoch: 67 [27136/50176]	Loss: 0.1547
Training Epoch: 67 [27648/50176]	Loss: 0.2072
Training Epoch: 67 [28160/50176]	Loss: 0.1290
Training Epoch: 67 [28672/50176]	Loss: 0.1699
Training Epoch: 67 [29184/50176]	Loss: 0.2285
Training Epoch: 67 [29696/50176]	Loss: 0.1975
Training Epoch: 67 [30208/50176]	Loss: 0.1802
Training Epoch: 67 [30720/50176]	Loss: 0.1324
Training Epoch: 67 [31232/50176]	Loss: 0.2033
Training Epoch: 67 [31744/50176]	Loss: 0.1776
Training Epoch: 67 [32256/50176]	Loss: 0.2025
Training Epoch: 67 [32768/50176]	Loss: 0.2197
Training Epoch: 67 [33280/50176]	Loss: 0.1530
Training Epoch: 67 [33792/50176]	Loss: 0.1643
Training Epoch: 67 [34304/50176]	Loss: 0.1985
Training Epoch: 67 [34816/50176]	Loss: 0.1837
Training Epoch: 67 [35328/50176]	Loss: 0.2333
Training Epoch: 67 [35840/50176]	Loss: 0.1467
Training Epoch: 67 [36352/50176]	Loss: 0.1615
Training Epoch: 67 [36864/50176]	Loss: 0.1587
Training Epoch: 67 [37376/50176]	Loss: 0.1988
Training Epoch: 67 [37888/50176]	Loss: 0.1998
Training Epoch: 67 [38400/50176]	Loss: 0.1749
Training Epoch: 67 [38912/50176]	Loss: 0.1653
Training Epoch: 67 [39424/50176]	Loss: 0.2391
Training Epoch: 67 [39936/50176]	Loss: 0.2106
Training Epoch: 67 [40448/50176]	Loss: 0.1710
Training Epoch: 67 [40960/50176]	Loss: 0.1273
Training Epoch: 67 [41472/50176]	Loss: 0.1681
Training Epoch: 67 [41984/50176]	Loss: 0.1738
Training Epoch: 67 [42496/50176]	Loss: 0.1663
Training Epoch: 67 [43008/50176]	Loss: 0.2516
Training Epoch: 67 [43520/50176]	Loss: 0.1800
Training Epoch: 67 [44032/50176]	Loss: 0.1468
Training Epoch: 67 [44544/50176]	Loss: 0.1590
Training Epoch: 67 [45056/50176]	Loss: 0.1905
Training Epoch: 67 [45568/50176]	Loss: 0.1423
Training Epoch: 67 [46080/50176]	Loss: 0.1755
Training Epoch: 67 [46592/50176]	Loss: 0.1949
Training Epoch: 67 [47104/50176]	Loss: 0.1837
Training Epoch: 67 [47616/50176]	Loss: 0.2166
Training Epoch: 67 [48128/50176]	Loss: 0.2199
Training Epoch: 67 [48640/50176]	Loss: 0.1614
Training Epoch: 67 [49152/50176]	Loss: 0.2203
Training Epoch: 67 [49664/50176]	Loss: 0.2090
Training Epoch: 67 [50176/50176]	Loss: 0.1869
Validation Epoch: 67, Average loss: 0.0051, Accuracy: 0.5845
Training Epoch: 68 [512/50176]	Loss: 0.1664
Training Epoch: 68 [1024/50176]	Loss: 0.1159
Training Epoch: 68 [1536/50176]	Loss: 0.1691
Training Epoch: 68 [2048/50176]	Loss: 0.2157
Training Epoch: 68 [2560/50176]	Loss: 0.1688
Training Epoch: 68 [3072/50176]	Loss: 0.1590
Training Epoch: 68 [3584/50176]	Loss: 0.1291
Training Epoch: 68 [4096/50176]	Loss: 0.1697
Training Epoch: 68 [4608/50176]	Loss: 0.1699
Training Epoch: 68 [5120/50176]	Loss: 0.1940
Training Epoch: 68 [5632/50176]	Loss: 0.1460
Training Epoch: 68 [6144/50176]	Loss: 0.1734
Training Epoch: 68 [6656/50176]	Loss: 0.1026
Training Epoch: 68 [7168/50176]	Loss: 0.1541
Training Epoch: 68 [7680/50176]	Loss: 0.2312
Training Epoch: 68 [8192/50176]	Loss: 0.2028
Training Epoch: 68 [8704/50176]	Loss: 0.1202
Training Epoch: 68 [9216/50176]	Loss: 0.1485
Training Epoch: 68 [9728/50176]	Loss: 0.1589
Training Epoch: 68 [10240/50176]	Loss: 0.1354
Training Epoch: 68 [10752/50176]	Loss: 0.1590
Training Epoch: 68 [11264/50176]	Loss: 0.1406
Training Epoch: 68 [11776/50176]	Loss: 0.1646
Training Epoch: 68 [12288/50176]	Loss: 0.1518
Training Epoch: 68 [12800/50176]	Loss: 0.1325
Training Epoch: 68 [13312/50176]	Loss: 0.2129
Training Epoch: 68 [13824/50176]	Loss: 0.1358
Training Epoch: 68 [14336/50176]	Loss: 0.1396
Training Epoch: 68 [14848/50176]	Loss: 0.1407
Training Epoch: 68 [15360/50176]	Loss: 0.1677
Training Epoch: 68 [15872/50176]	Loss: 0.1518
Training Epoch: 68 [16384/50176]	Loss: 0.1789
Training Epoch: 68 [16896/50176]	Loss: 0.1932
Training Epoch: 68 [17408/50176]	Loss: 0.1443
Training Epoch: 68 [17920/50176]	Loss: 0.1802
Training Epoch: 68 [18432/50176]	Loss: 0.2013
Training Epoch: 68 [18944/50176]	Loss: 0.1678
Training Epoch: 68 [19456/50176]	Loss: 0.1227
Training Epoch: 68 [19968/50176]	Loss: 0.1725
Training Epoch: 68 [20480/50176]	Loss: 0.1686
Training Epoch: 68 [20992/50176]	Loss: 0.1874
Training Epoch: 68 [21504/50176]	Loss: 0.1506
Training Epoch: 68 [22016/50176]	Loss: 0.1703
Training Epoch: 68 [22528/50176]	Loss: 0.1768
Training Epoch: 68 [23040/50176]	Loss: 0.1738
Training Epoch: 68 [23552/50176]	Loss: 0.1828
Training Epoch: 68 [24064/50176]	Loss: 0.1738
Training Epoch: 68 [24576/50176]	Loss: 0.1756
Training Epoch: 68 [25088/50176]	Loss: 0.2052
Training Epoch: 68 [25600/50176]	Loss: 0.1867
Training Epoch: 68 [26112/50176]	Loss: 0.2027
Training Epoch: 68 [26624/50176]	Loss: 0.1326
Training Epoch: 68 [27136/50176]	Loss: 0.1649
Training Epoch: 68 [27648/50176]	Loss: 0.1674
Training Epoch: 68 [28160/50176]	Loss: 0.1194
Training Epoch: 68 [28672/50176]	Loss: 0.1726
Training Epoch: 68 [29184/50176]	Loss: 0.1841
Training Epoch: 68 [29696/50176]	Loss: 0.2223
Training Epoch: 68 [30208/50176]	Loss: 0.2357
Training Epoch: 68 [30720/50176]	Loss: 0.1946
Training Epoch: 68 [31232/50176]	Loss: 0.1596
Training Epoch: 68 [31744/50176]	Loss: 0.1451
Training Epoch: 68 [32256/50176]	Loss: 0.1276
Training Epoch: 68 [32768/50176]	Loss: 0.1631
Training Epoch: 68 [33280/50176]	Loss: 0.1592
Training Epoch: 68 [33792/50176]	Loss: 0.1775
Training Epoch: 68 [34304/50176]	Loss: 0.1611
Training Epoch: 68 [34816/50176]	Loss: 0.2269
Training Epoch: 68 [35328/50176]	Loss: 0.2273
Training Epoch: 68 [35840/50176]	Loss: 0.2117
Training Epoch: 68 [36352/50176]	Loss: 0.2063
Training Epoch: 68 [36864/50176]	Loss: 0.1524
Training Epoch: 68 [37376/50176]	Loss: 0.1525
Training Epoch: 68 [37888/50176]	Loss: 0.1879
Training Epoch: 68 [38400/50176]	Loss: 0.1715
Training Epoch: 68 [38912/50176]	Loss: 0.1553
Training Epoch: 68 [39424/50176]	Loss: 0.2515
Training Epoch: 68 [39936/50176]	Loss: 0.1578
Training Epoch: 68 [40448/50176]	Loss: 0.2435
Training Epoch: 68 [40960/50176]	Loss: 0.2153
Training Epoch: 68 [41472/50176]	Loss: 0.1412
Training Epoch: 68 [41984/50176]	Loss: 0.1749
Training Epoch: 68 [42496/50176]	Loss: 0.2392
Training Epoch: 68 [43008/50176]	Loss: 0.2044
Training Epoch: 68 [43520/50176]	Loss: 0.2225
Training Epoch: 68 [44032/50176]	Loss: 0.1807
Training Epoch: 68 [44544/50176]	Loss: 0.1758
Training Epoch: 68 [45056/50176]	Loss: 0.2160
Training Epoch: 68 [45568/50176]	Loss: 0.2471
Training Epoch: 68 [46080/50176]	Loss: 0.1958
Training Epoch: 68 [46592/50176]	Loss: 0.2101
Training Epoch: 68 [47104/50176]	Loss: 0.2073
Training Epoch: 68 [47616/50176]	Loss: 0.1811
Training Epoch: 68 [48128/50176]	Loss: 0.2066
Training Epoch: 68 [48640/50176]	Loss: 0.2421
Training Epoch: 68 [49152/50176]	Loss: 0.1645
Training Epoch: 68 [49664/50176]	Loss: 0.1798
Training Epoch: 68 [50176/50176]	Loss: 0.1573
Validation Epoch: 68, Average loss: 0.0059, Accuracy: 0.5686
Training Epoch: 69 [512/50176]	Loss: 0.1872
Training Epoch: 69 [1024/50176]	Loss: 0.1789
Training Epoch: 69 [1536/50176]	Loss: 0.1434
Training Epoch: 69 [2048/50176]	Loss: 0.1163
Training Epoch: 69 [2560/50176]	Loss: 0.1799
Training Epoch: 69 [3072/50176]	Loss: 0.1593
Training Epoch: 69 [3584/50176]	Loss: 0.1578
Training Epoch: 69 [4096/50176]	Loss: 0.2193
Training Epoch: 69 [4608/50176]	Loss: 0.1853
Training Epoch: 69 [5120/50176]	Loss: 0.1047
Training Epoch: 69 [5632/50176]	Loss: 0.1754
Training Epoch: 69 [6144/50176]	Loss: 0.1982
Training Epoch: 69 [6656/50176]	Loss: 0.1603
Training Epoch: 69 [7168/50176]	Loss: 0.1587
Training Epoch: 69 [7680/50176]	Loss: 0.2484
Training Epoch: 69 [8192/50176]	Loss: 0.1279
Training Epoch: 69 [8704/50176]	Loss: 0.1930
Training Epoch: 69 [9216/50176]	Loss: 0.1705
Training Epoch: 69 [9728/50176]	Loss: 0.1517
Training Epoch: 69 [10240/50176]	Loss: 0.1542
Training Epoch: 69 [10752/50176]	Loss: 0.1545
Training Epoch: 69 [11264/50176]	Loss: 0.1670
Training Epoch: 69 [11776/50176]	Loss: 0.1374
Training Epoch: 69 [12288/50176]	Loss: 0.1668
Training Epoch: 69 [12800/50176]	Loss: 0.1588
Training Epoch: 69 [13312/50176]	Loss: 0.1324
Training Epoch: 69 [13824/50176]	Loss: 0.1749
Training Epoch: 69 [14336/50176]	Loss: 0.1625
Training Epoch: 69 [14848/50176]	Loss: 0.1609
Training Epoch: 69 [15360/50176]	Loss: 0.1257
Training Epoch: 69 [15872/50176]	Loss: 0.2024
Training Epoch: 69 [16384/50176]	Loss: 0.1748
Training Epoch: 69 [16896/50176]	Loss: 0.1489
Training Epoch: 69 [17408/50176]	Loss: 0.1711
Training Epoch: 69 [17920/50176]	Loss: 0.1659
Training Epoch: 69 [18432/50176]	Loss: 0.1822
Training Epoch: 69 [18944/50176]	Loss: 0.1708
Training Epoch: 69 [19456/50176]	Loss: 0.1729
Training Epoch: 69 [19968/50176]	Loss: 0.2024
Training Epoch: 69 [20480/50176]	Loss: 0.1271
Training Epoch: 69 [20992/50176]	Loss: 0.1865
Training Epoch: 69 [21504/50176]	Loss: 0.1562
Training Epoch: 69 [22016/50176]	Loss: 0.1595
Training Epoch: 69 [22528/50176]	Loss: 0.1603
Training Epoch: 69 [23040/50176]	Loss: 0.2071
Training Epoch: 69 [23552/50176]	Loss: 0.1279
Training Epoch: 69 [24064/50176]	Loss: 0.1867
Training Epoch: 69 [24576/50176]	Loss: 0.1501
Training Epoch: 69 [25088/50176]	Loss: 0.1529
Training Epoch: 69 [25600/50176]	Loss: 0.2235
Training Epoch: 69 [26112/50176]	Loss: 0.1915
Training Epoch: 69 [26624/50176]	Loss: 0.1681
Training Epoch: 69 [27136/50176]	Loss: 0.1933
Training Epoch: 69 [27648/50176]	Loss: 0.2081
Training Epoch: 69 [28160/50176]	Loss: 0.1590
Training Epoch: 69 [28672/50176]	Loss: 0.1521
Training Epoch: 69 [29184/50176]	Loss: 0.1724
Training Epoch: 69 [29696/50176]	Loss: 0.1641
Training Epoch: 69 [30208/50176]	Loss: 0.2333
Training Epoch: 69 [30720/50176]	Loss: 0.1921
Training Epoch: 69 [31232/50176]	Loss: 0.2176
Training Epoch: 69 [31744/50176]	Loss: 0.2203
Training Epoch: 69 [32256/50176]	Loss: 0.1582
Training Epoch: 69 [32768/50176]	Loss: 0.1764
Training Epoch: 69 [33280/50176]	Loss: 0.1841
Training Epoch: 69 [33792/50176]	Loss: 0.2210
Training Epoch: 69 [34304/50176]	Loss: 0.2163
Training Epoch: 69 [34816/50176]	Loss: 0.1957
Training Epoch: 69 [35328/50176]	Loss: 0.2114
Training Epoch: 69 [35840/50176]	Loss: 0.1339
Training Epoch: 69 [36352/50176]	Loss: 0.1790
Training Epoch: 69 [36864/50176]	Loss: 0.1711
Training Epoch: 69 [37376/50176]	Loss: 0.1697
Training Epoch: 69 [37888/50176]	Loss: 0.1694
Training Epoch: 69 [38400/50176]	Loss: 0.2192
Training Epoch: 69 [38912/50176]	Loss: 0.1704
Training Epoch: 69 [39424/50176]	Loss: 0.2112
Training Epoch: 69 [39936/50176]	Loss: 0.1439
Training Epoch: 69 [40448/50176]	Loss: 0.2041
Training Epoch: 69 [40960/50176]	Loss: 0.1966
Training Epoch: 69 [41472/50176]	Loss: 0.1533
Training Epoch: 69 [41984/50176]	Loss: 0.1759
Training Epoch: 69 [42496/50176]	Loss: 0.1753
Training Epoch: 69 [43008/50176]	Loss: 0.1981
Training Epoch: 69 [43520/50176]	Loss: 0.1663
Training Epoch: 69 [44032/50176]	Loss: 0.1716
Training Epoch: 69 [44544/50176]	Loss: 0.2048
Training Epoch: 69 [45056/50176]	Loss: 0.1694
Training Epoch: 69 [45568/50176]	Loss: 0.2048
Training Epoch: 69 [46080/50176]	Loss: 0.1768
Training Epoch: 69 [46592/50176]	Loss: 0.2024
Training Epoch: 69 [47104/50176]	Loss: 0.2350
Training Epoch: 69 [47616/50176]	Loss: 0.1765
Training Epoch: 69 [48128/50176]	Loss: 0.1588
Training Epoch: 69 [48640/50176]	Loss: 0.1679
Training Epoch: 69 [49152/50176]	Loss: 0.1958
Training Epoch: 69 [49664/50176]	Loss: 0.1932
Training Epoch: 69 [50176/50176]	Loss: 0.1775
Validation Epoch: 69, Average loss: 0.0052, Accuracy: 0.5913
Training Epoch: 70 [512/50176]	Loss: 0.1629
Training Epoch: 70 [1024/50176]	Loss: 0.1531
Training Epoch: 70 [1536/50176]	Loss: 0.1758
Training Epoch: 70 [2048/50176]	Loss: 0.1260
Training Epoch: 70 [2560/50176]	Loss: 0.1512
Training Epoch: 70 [3072/50176]	Loss: 0.1434
Training Epoch: 70 [3584/50176]	Loss: 0.1264
Training Epoch: 70 [4096/50176]	Loss: 0.1375
Training Epoch: 70 [4608/50176]	Loss: 0.1567
Training Epoch: 70 [5120/50176]	Loss: 0.1563
Training Epoch: 70 [5632/50176]	Loss: 0.1368
Training Epoch: 70 [6144/50176]	Loss: 0.1352
Training Epoch: 70 [6656/50176]	Loss: 0.1340
Training Epoch: 70 [7168/50176]	Loss: 0.1623
Training Epoch: 70 [7680/50176]	Loss: 0.1722
Training Epoch: 70 [8192/50176]	Loss: 0.1965
Training Epoch: 70 [8704/50176]	Loss: 0.1274
Training Epoch: 70 [9216/50176]	Loss: 0.1659
Training Epoch: 70 [9728/50176]	Loss: 0.1295
Training Epoch: 70 [10240/50176]	Loss: 0.1927
Training Epoch: 70 [10752/50176]	Loss: 0.1418
Training Epoch: 70 [11264/50176]	Loss: 0.1103
Training Epoch: 70 [11776/50176]	Loss: 0.1407
Training Epoch: 70 [12288/50176]	Loss: 0.1279
Training Epoch: 70 [12800/50176]	Loss: 0.1950
Training Epoch: 70 [13312/50176]	Loss: 0.1519
Training Epoch: 70 [13824/50176]	Loss: 0.1698
Training Epoch: 70 [14336/50176]	Loss: 0.1825
Training Epoch: 70 [14848/50176]	Loss: 0.1763
Training Epoch: 70 [15360/50176]	Loss: 0.1320
Training Epoch: 70 [15872/50176]	Loss: 0.1689
Training Epoch: 70 [16384/50176]	Loss: 0.1247
Training Epoch: 70 [16896/50176]	Loss: 0.1731
Training Epoch: 70 [17408/50176]	Loss: 0.1296
Training Epoch: 70 [17920/50176]	Loss: 0.1448
Training Epoch: 70 [18432/50176]	Loss: 0.1699
Training Epoch: 70 [18944/50176]	Loss: 0.1673
Training Epoch: 70 [19456/50176]	Loss: 0.1737
Training Epoch: 70 [19968/50176]	Loss: 0.2038
Training Epoch: 70 [20480/50176]	Loss: 0.1274
Training Epoch: 70 [20992/50176]	Loss: 0.1985
Training Epoch: 70 [21504/50176]	Loss: 0.1169
Training Epoch: 70 [22016/50176]	Loss: 0.1575
Training Epoch: 70 [22528/50176]	Loss: 0.1838
Training Epoch: 70 [23040/50176]	Loss: 0.1692
Training Epoch: 70 [23552/50176]	Loss: 0.1385
Training Epoch: 70 [24064/50176]	Loss: 0.1951
Training Epoch: 70 [24576/50176]	Loss: 0.1404
Training Epoch: 70 [25088/50176]	Loss: 0.1512
Training Epoch: 70 [25600/50176]	Loss: 0.1919
Training Epoch: 70 [26112/50176]	Loss: 0.1773
Training Epoch: 70 [26624/50176]	Loss: 0.1859
Training Epoch: 70 [27136/50176]	Loss: 0.1615
Training Epoch: 70 [27648/50176]	Loss: 0.1554
Training Epoch: 70 [28160/50176]	Loss: 0.1617
Training Epoch: 70 [28672/50176]	Loss: 0.1406
Training Epoch: 70 [29184/50176]	Loss: 0.2335
Training Epoch: 70 [29696/50176]	Loss: 0.1876
Training Epoch: 70 [30208/50176]	Loss: 0.1664
Training Epoch: 70 [30720/50176]	Loss: 0.1743
Training Epoch: 70 [31232/50176]	Loss: 0.1891
Training Epoch: 70 [31744/50176]	Loss: 0.1693
Training Epoch: 70 [32256/50176]	Loss: 0.1120
Training Epoch: 70 [32768/50176]	Loss: 0.1966
Training Epoch: 70 [33280/50176]	Loss: 0.1684
Training Epoch: 70 [33792/50176]	Loss: 0.1820
Training Epoch: 70 [34304/50176]	Loss: 0.1810
Training Epoch: 70 [34816/50176]	Loss: 0.1564
Training Epoch: 70 [35328/50176]	Loss: 0.1547
Training Epoch: 70 [35840/50176]	Loss: 0.2315
Training Epoch: 70 [36352/50176]	Loss: 0.1697
Training Epoch: 70 [36864/50176]	Loss: 0.1741
Training Epoch: 70 [37376/50176]	Loss: 0.1709
Training Epoch: 70 [37888/50176]	Loss: 0.2022
Training Epoch: 70 [38400/50176]	Loss: 0.1566
Training Epoch: 70 [38912/50176]	Loss: 0.1804
Training Epoch: 70 [39424/50176]	Loss: 0.1743
Training Epoch: 70 [39936/50176]	Loss: 0.1522
Training Epoch: 70 [40448/50176]	Loss: 0.1616
Training Epoch: 70 [40960/50176]	Loss: 0.1713
Training Epoch: 70 [41472/50176]	Loss: 0.2294
Training Epoch: 70 [41984/50176]	Loss: 0.1662
Training Epoch: 70 [42496/50176]	Loss: 0.2111
Training Epoch: 70 [43008/50176]	Loss: 0.1983
Training Epoch: 70 [43520/50176]	Loss: 0.1918
Training Epoch: 70 [44032/50176]	Loss: 0.1782
Training Epoch: 70 [44544/50176]	Loss: 0.1965
Training Epoch: 70 [45056/50176]	Loss: 0.1479
Training Epoch: 70 [45568/50176]	Loss: 0.1707
Training Epoch: 70 [46080/50176]	Loss: 0.1883
Training Epoch: 70 [46592/50176]	Loss: 0.2040
Training Epoch: 70 [47104/50176]	Loss: 0.2354
Training Epoch: 70 [47616/50176]	Loss: 0.1973
Training Epoch: 70 [48128/50176]	Loss: 0.1665
Training Epoch: 70 [48640/50176]	Loss: 0.2408
Training Epoch: 70 [49152/50176]	Loss: 0.1972
Training Epoch: 70 [49664/50176]	Loss: 0.2164
Training Epoch: 70 [50176/50176]	Loss: 0.1860
Validation Epoch: 70, Average loss: 0.0053, Accuracy: 0.5872
Training Epoch: 71 [512/50176]	Loss: 0.1293
Training Epoch: 71 [1024/50176]	Loss: 0.1467
Training Epoch: 71 [1536/50176]	Loss: 0.1644
Training Epoch: 71 [2048/50176]	Loss: 0.1408
Training Epoch: 71 [2560/50176]	Loss: 0.1918
Training Epoch: 71 [3072/50176]	Loss: 0.1392
Training Epoch: 71 [3584/50176]	Loss: 0.1360
Training Epoch: 71 [4096/50176]	Loss: 0.1898
Training Epoch: 71 [4608/50176]	Loss: 0.1872
Training Epoch: 71 [5120/50176]	Loss: 0.1841
Training Epoch: 71 [5632/50176]	Loss: 0.1751
Training Epoch: 71 [6144/50176]	Loss: 0.1327
Training Epoch: 71 [6656/50176]	Loss: 0.1643
Training Epoch: 71 [7168/50176]	Loss: 0.1520
Training Epoch: 71 [7680/50176]	Loss: 0.0919
Training Epoch: 71 [8192/50176]	Loss: 0.1636
Training Epoch: 71 [8704/50176]	Loss: 0.1562
Training Epoch: 71 [9216/50176]	Loss: 0.1451
Training Epoch: 71 [9728/50176]	Loss: 0.1507
Training Epoch: 71 [10240/50176]	Loss: 0.1549
Training Epoch: 71 [10752/50176]	Loss: 0.1571
Training Epoch: 71 [11264/50176]	Loss: 0.1510
Training Epoch: 71 [11776/50176]	Loss: 0.1437
Training Epoch: 71 [12288/50176]	Loss: 0.1702
Training Epoch: 71 [12800/50176]	Loss: 0.1063
Training Epoch: 71 [13312/50176]	Loss: 0.1454
Training Epoch: 71 [13824/50176]	Loss: 0.1518
Training Epoch: 71 [14336/50176]	Loss: 0.1381
Training Epoch: 71 [14848/50176]	Loss: 0.1567
Training Epoch: 71 [15360/50176]	Loss: 0.1438
Training Epoch: 71 [15872/50176]	Loss: 0.1575
Training Epoch: 71 [16384/50176]	Loss: 0.1321
Training Epoch: 71 [16896/50176]	Loss: 0.1617
Training Epoch: 71 [17408/50176]	Loss: 0.1949
Training Epoch: 71 [17920/50176]	Loss: 0.1858
Training Epoch: 71 [18432/50176]	Loss: 0.2164
Training Epoch: 71 [18944/50176]	Loss: 0.1382
Training Epoch: 71 [19456/50176]	Loss: 0.1556
Training Epoch: 71 [19968/50176]	Loss: 0.1724
Training Epoch: 71 [20480/50176]	Loss: 0.1669
Training Epoch: 71 [20992/50176]	Loss: 0.1548
Training Epoch: 71 [21504/50176]	Loss: 0.2114
Training Epoch: 71 [22016/50176]	Loss: 0.1420
Training Epoch: 71 [22528/50176]	Loss: 0.1963
Training Epoch: 71 [23040/50176]	Loss: 0.1375
Training Epoch: 71 [23552/50176]	Loss: 0.1848
Training Epoch: 71 [24064/50176]	Loss: 0.1377
Training Epoch: 71 [24576/50176]	Loss: 0.1912
Training Epoch: 71 [25088/50176]	Loss: 0.1643
Training Epoch: 71 [25600/50176]	Loss: 0.1940
Training Epoch: 71 [26112/50176]	Loss: 0.1931
Training Epoch: 71 [26624/50176]	Loss: 0.1517
Training Epoch: 71 [27136/50176]	Loss: 0.1549
Training Epoch: 71 [27648/50176]	Loss: 0.2008
Training Epoch: 71 [28160/50176]	Loss: 0.1721
Training Epoch: 71 [28672/50176]	Loss: 0.1630
Training Epoch: 71 [29184/50176]	Loss: 0.1994
Training Epoch: 71 [29696/50176]	Loss: 0.1924
Training Epoch: 71 [30208/50176]	Loss: 0.2115
Training Epoch: 71 [30720/50176]	Loss: 0.1749
Training Epoch: 71 [31232/50176]	Loss: 0.1848
Training Epoch: 71 [31744/50176]	Loss: 0.1584
Training Epoch: 71 [32256/50176]	Loss: 0.1501
Training Epoch: 71 [32768/50176]	Loss: 0.2338
Training Epoch: 71 [33280/50176]	Loss: 0.1699
Training Epoch: 71 [33792/50176]	Loss: 0.1886
Training Epoch: 71 [34304/50176]	Loss: 0.1322
Training Epoch: 71 [34816/50176]	Loss: 0.1799
Training Epoch: 71 [35328/50176]	Loss: 0.1692
Training Epoch: 71 [35840/50176]	Loss: 0.1562
Training Epoch: 71 [36352/50176]	Loss: 0.1977
Training Epoch: 71 [36864/50176]	Loss: 0.1635
Training Epoch: 71 [37376/50176]	Loss: 0.1526
Training Epoch: 71 [37888/50176]	Loss: 0.2158
Training Epoch: 71 [38400/50176]	Loss: 0.1650
Training Epoch: 71 [38912/50176]	Loss: 0.2099
Training Epoch: 71 [39424/50176]	Loss: 0.1550
Training Epoch: 71 [39936/50176]	Loss: 0.1780
Training Epoch: 71 [40448/50176]	Loss: 0.1324
Training Epoch: 71 [40960/50176]	Loss: 0.1650
Training Epoch: 71 [41472/50176]	Loss: 0.1456
Training Epoch: 71 [41984/50176]	Loss: 0.2129
Training Epoch: 71 [42496/50176]	Loss: 0.1824
Training Epoch: 71 [43008/50176]	Loss: 0.1730
Training Epoch: 71 [43520/50176]	Loss: 0.1890
Training Epoch: 71 [44032/50176]	Loss: 0.1885
Training Epoch: 71 [44544/50176]	Loss: 0.1752
Training Epoch: 71 [45056/50176]	Loss: 0.1907
Training Epoch: 71 [45568/50176]	Loss: 0.1970
Training Epoch: 71 [46080/50176]	Loss: 0.1955
Training Epoch: 71 [46592/50176]	Loss: 0.1571
Training Epoch: 71 [47104/50176]	Loss: 0.2307
Training Epoch: 71 [47616/50176]	Loss: 0.2205
Training Epoch: 71 [48128/50176]	Loss: 0.1828
Training Epoch: 71 [48640/50176]	Loss: 0.2317
Training Epoch: 71 [49152/50176]	Loss: 0.2169
Training Epoch: 71 [49664/50176]	Loss: 0.2273
Training Epoch: 71 [50176/50176]	Loss: 0.1943
Validation Epoch: 71, Average loss: 0.0055, Accuracy: 0.5835
Training Epoch: 72 [512/50176]	Loss: 0.1467
Training Epoch: 72 [1024/50176]	Loss: 0.1523
Training Epoch: 72 [1536/50176]	Loss: 0.1528
Training Epoch: 72 [2048/50176]	Loss: 0.1317
Training Epoch: 72 [2560/50176]	Loss: 0.1729
Training Epoch: 72 [3072/50176]	Loss: 0.1710
Training Epoch: 72 [3584/50176]	Loss: 0.1103
Training Epoch: 72 [4096/50176]	Loss: 0.1365
Training Epoch: 72 [4608/50176]	Loss: 0.1675
Training Epoch: 72 [5120/50176]	Loss: 0.1158
Training Epoch: 72 [5632/50176]	Loss: 0.1451
Training Epoch: 72 [6144/50176]	Loss: 0.1323
Training Epoch: 72 [6656/50176]	Loss: 0.1582
Training Epoch: 72 [7168/50176]	Loss: 0.1444
Training Epoch: 72 [7680/50176]	Loss: 0.1765
Training Epoch: 72 [8192/50176]	Loss: 0.1814
Training Epoch: 72 [8704/50176]	Loss: 0.1565
Training Epoch: 72 [9216/50176]	Loss: 0.1135
Training Epoch: 72 [9728/50176]	Loss: 0.1341
Training Epoch: 72 [10240/50176]	Loss: 0.1525
Training Epoch: 72 [10752/50176]	Loss: 0.1542
Training Epoch: 72 [11264/50176]	Loss: 0.1690
Training Epoch: 72 [11776/50176]	Loss: 0.1446
Training Epoch: 72 [12288/50176]	Loss: 0.1294
Training Epoch: 72 [12800/50176]	Loss: 0.2163
Training Epoch: 72 [13312/50176]	Loss: 0.1621
Training Epoch: 72 [13824/50176]	Loss: 0.1547
Training Epoch: 72 [14336/50176]	Loss: 0.1746
Training Epoch: 72 [14848/50176]	Loss: 0.1564
Training Epoch: 72 [15360/50176]	Loss: 0.1658
Training Epoch: 72 [15872/50176]	Loss: 0.1499
Training Epoch: 72 [16384/50176]	Loss: 0.1655
Training Epoch: 72 [16896/50176]	Loss: 0.1384
Training Epoch: 72 [17408/50176]	Loss: 0.1782
Training Epoch: 72 [17920/50176]	Loss: 0.2110
Training Epoch: 72 [18432/50176]	Loss: 0.1705
Training Epoch: 72 [18944/50176]	Loss: 0.1457
Training Epoch: 72 [19456/50176]	Loss: 0.1478
Training Epoch: 72 [19968/50176]	Loss: 0.1126
Training Epoch: 72 [20480/50176]	Loss: 0.1415
Training Epoch: 72 [20992/50176]	Loss: 0.1318
Training Epoch: 72 [21504/50176]	Loss: 0.1661
Training Epoch: 72 [22016/50176]	Loss: 0.1561
Training Epoch: 72 [22528/50176]	Loss: 0.1303
Training Epoch: 72 [23040/50176]	Loss: 0.1708
Training Epoch: 72 [23552/50176]	Loss: 0.1385
Training Epoch: 72 [24064/50176]	Loss: 0.1786
Training Epoch: 72 [24576/50176]	Loss: 0.1647
Training Epoch: 72 [25088/50176]	Loss: 0.1616
Training Epoch: 72 [25600/50176]	Loss: 0.1867
Training Epoch: 72 [26112/50176]	Loss: 0.1537
Training Epoch: 72 [26624/50176]	Loss: 0.2020
Training Epoch: 72 [27136/50176]	Loss: 0.1210
Training Epoch: 72 [27648/50176]	Loss: 0.1278
Training Epoch: 72 [28160/50176]	Loss: 0.1886
Training Epoch: 72 [28672/50176]	Loss: 0.1502
Training Epoch: 72 [29184/50176]	Loss: 0.1722
Training Epoch: 72 [29696/50176]	Loss: 0.1759
Training Epoch: 72 [30208/50176]	Loss: 0.1493
Training Epoch: 72 [30720/50176]	Loss: 0.1334
Training Epoch: 72 [31232/50176]	Loss: 0.1713
Training Epoch: 72 [31744/50176]	Loss: 0.1576
Training Epoch: 72 [32256/50176]	Loss: 0.1616
Training Epoch: 72 [32768/50176]	Loss: 0.1630
Training Epoch: 72 [33280/50176]	Loss: 0.2300
Training Epoch: 72 [33792/50176]	Loss: 0.1871
Training Epoch: 72 [34304/50176]	Loss: 0.1390
Training Epoch: 72 [34816/50176]	Loss: 0.1852
Training Epoch: 72 [35328/50176]	Loss: 0.1489
Training Epoch: 72 [35840/50176]	Loss: 0.1329
Training Epoch: 72 [36352/50176]	Loss: 0.2348
Training Epoch: 72 [36864/50176]	Loss: 0.1434
Training Epoch: 72 [37376/50176]	Loss: 0.2200
Training Epoch: 72 [37888/50176]	Loss: 0.1813
Training Epoch: 72 [38400/50176]	Loss: 0.2016
Training Epoch: 72 [38912/50176]	Loss: 0.1512
Training Epoch: 72 [39424/50176]	Loss: 0.1842
Training Epoch: 72 [39936/50176]	Loss: 0.1588
Training Epoch: 72 [40448/50176]	Loss: 0.1353
Training Epoch: 72 [40960/50176]	Loss: 0.1459
Training Epoch: 72 [41472/50176]	Loss: 0.1844
Training Epoch: 72 [41984/50176]	Loss: 0.2178
Training Epoch: 72 [42496/50176]	Loss: 0.1391
Training Epoch: 72 [43008/50176]	Loss: 0.2240
Training Epoch: 72 [43520/50176]	Loss: 0.1666
Training Epoch: 72 [44032/50176]	Loss: 0.2259
Training Epoch: 72 [44544/50176]	Loss: 0.2072
Training Epoch: 72 [45056/50176]	Loss: 0.2315
Training Epoch: 72 [45568/50176]	Loss: 0.1661
Training Epoch: 72 [46080/50176]	Loss: 0.1715
Training Epoch: 72 [46592/50176]	Loss: 0.1972
Training Epoch: 72 [47104/50176]	Loss: 0.1961
Training Epoch: 72 [47616/50176]	Loss: 0.2333
Training Epoch: 72 [48128/50176]	Loss: 0.1882
Training Epoch: 72 [48640/50176]	Loss: 0.1680
Training Epoch: 72 [49152/50176]	Loss: 0.1985
Training Epoch: 72 [49664/50176]	Loss: 0.1721
Training Epoch: 72 [50176/50176]	Loss: 0.1823
Validation Epoch: 72, Average loss: 0.0056, Accuracy: 0.5762
Training Epoch: 73 [512/50176]	Loss: 0.1768
Training Epoch: 73 [1024/50176]	Loss: 0.1734
Training Epoch: 73 [1536/50176]	Loss: 0.1508
Training Epoch: 73 [2048/50176]	Loss: 0.1518
Training Epoch: 73 [2560/50176]	Loss: 0.1736
Training Epoch: 73 [3072/50176]	Loss: 0.1685
Training Epoch: 73 [3584/50176]	Loss: 0.1296
Training Epoch: 73 [4096/50176]	Loss: 0.1730
Training Epoch: 73 [4608/50176]	Loss: 0.1286
Training Epoch: 73 [5120/50176]	Loss: 0.1407
Training Epoch: 73 [5632/50176]	Loss: 0.1484
Training Epoch: 73 [6144/50176]	Loss: 0.1671
Training Epoch: 73 [6656/50176]	Loss: 0.1651
Training Epoch: 73 [7168/50176]	Loss: 0.1725
Training Epoch: 73 [7680/50176]	Loss: 0.1728
Training Epoch: 73 [8192/50176]	Loss: 0.1789
Training Epoch: 73 [8704/50176]	Loss: 0.1220
Training Epoch: 73 [9216/50176]	Loss: 0.1512
Training Epoch: 73 [9728/50176]	Loss: 0.1047
Training Epoch: 73 [10240/50176]	Loss: 0.2091
Training Epoch: 73 [10752/50176]	Loss: 0.1378
Training Epoch: 73 [11264/50176]	Loss: 0.1533
Training Epoch: 73 [11776/50176]	Loss: 0.1462
Training Epoch: 73 [12288/50176]	Loss: 0.1546
Training Epoch: 73 [12800/50176]	Loss: 0.1042
Training Epoch: 73 [13312/50176]	Loss: 0.1699
Training Epoch: 73 [13824/50176]	Loss: 0.1101
Training Epoch: 73 [14336/50176]	Loss: 0.1503
Training Epoch: 73 [14848/50176]	Loss: 0.1570
Training Epoch: 73 [15360/50176]	Loss: 0.1619
Training Epoch: 73 [15872/50176]	Loss: 0.1411
Training Epoch: 73 [16384/50176]	Loss: 0.1590
Training Epoch: 73 [16896/50176]	Loss: 0.1157
Training Epoch: 73 [17408/50176]	Loss: 0.1461
Training Epoch: 73 [17920/50176]	Loss: 0.1878
Training Epoch: 73 [18432/50176]	Loss: 0.1841
Training Epoch: 73 [18944/50176]	Loss: 0.1689
Training Epoch: 73 [19456/50176]	Loss: 0.1878
Training Epoch: 73 [19968/50176]	Loss: 0.1704
Training Epoch: 73 [20480/50176]	Loss: 0.1832
Training Epoch: 73 [20992/50176]	Loss: 0.1412
Training Epoch: 73 [21504/50176]	Loss: 0.1399
Training Epoch: 73 [22016/50176]	Loss: 0.1924
Training Epoch: 73 [22528/50176]	Loss: 0.1241
Training Epoch: 73 [23040/50176]	Loss: 0.1545
Training Epoch: 73 [23552/50176]	Loss: 0.1507
Training Epoch: 73 [24064/50176]	Loss: 0.1493
Training Epoch: 73 [24576/50176]	Loss: 0.1244
Training Epoch: 73 [25088/50176]	Loss: 0.1683
Training Epoch: 73 [25600/50176]	Loss: 0.1840
Training Epoch: 73 [26112/50176]	Loss: 0.1651
Training Epoch: 73 [26624/50176]	Loss: 0.1880
Training Epoch: 73 [27136/50176]	Loss: 0.1639
Training Epoch: 73 [27648/50176]	Loss: 0.1593
Training Epoch: 73 [28160/50176]	Loss: 0.1221
Training Epoch: 73 [28672/50176]	Loss: 0.2147
Training Epoch: 73 [29184/50176]	Loss: 0.1701
Training Epoch: 73 [29696/50176]	Loss: 0.1172
Training Epoch: 73 [30208/50176]	Loss: 0.1468
Training Epoch: 73 [30720/50176]	Loss: 0.1253
Training Epoch: 73 [31232/50176]	Loss: 0.1449
Training Epoch: 73 [31744/50176]	Loss: 0.1951
Training Epoch: 73 [32256/50176]	Loss: 0.1748
Training Epoch: 73 [32768/50176]	Loss: 0.1331
Training Epoch: 73 [33280/50176]	Loss: 0.1554
Training Epoch: 73 [33792/50176]	Loss: 0.1360
Training Epoch: 73 [34304/50176]	Loss: 0.1526
Training Epoch: 73 [34816/50176]	Loss: 0.2237
Training Epoch: 73 [35328/50176]	Loss: 0.1535
Training Epoch: 73 [35840/50176]	Loss: 0.1532
Training Epoch: 73 [36352/50176]	Loss: 0.1506
Training Epoch: 73 [36864/50176]	Loss: 0.1248
Training Epoch: 73 [37376/50176]	Loss: 0.1448
Training Epoch: 73 [37888/50176]	Loss: 0.1759
Training Epoch: 73 [38400/50176]	Loss: 0.1738
Training Epoch: 73 [38912/50176]	Loss: 0.1430
Training Epoch: 73 [39424/50176]	Loss: 0.1817
Training Epoch: 73 [39936/50176]	Loss: 0.1251
Training Epoch: 73 [40448/50176]	Loss: 0.1113
Training Epoch: 73 [40960/50176]	Loss: 0.1781
Training Epoch: 73 [41472/50176]	Loss: 0.2026
Training Epoch: 73 [41984/50176]	Loss: 0.1833
Training Epoch: 73 [42496/50176]	Loss: 0.1659
Training Epoch: 73 [43008/50176]	Loss: 0.2176
Training Epoch: 73 [43520/50176]	Loss: 0.1569
Training Epoch: 73 [44032/50176]	Loss: 0.1655
Training Epoch: 73 [44544/50176]	Loss: 0.1816
Training Epoch: 73 [45056/50176]	Loss: 0.1613
Training Epoch: 73 [45568/50176]	Loss: 0.2243
Training Epoch: 73 [46080/50176]	Loss: 0.1633
Training Epoch: 73 [46592/50176]	Loss: 0.1787
Training Epoch: 73 [47104/50176]	Loss: 0.1663
Training Epoch: 73 [47616/50176]	Loss: 0.1744
Training Epoch: 73 [48128/50176]	Loss: 0.1921
Training Epoch: 73 [48640/50176]	Loss: 0.1545
Training Epoch: 73 [49152/50176]	Loss: 0.1702
Training Epoch: 73 [49664/50176]	Loss: 0.2243
Training Epoch: 73 [50176/50176]	Loss: 0.1561
Validation Epoch: 73, Average loss: 0.0055, Accuracy: 0.5880
Training Epoch: 74 [512/50176]	Loss: 0.1390
Training Epoch: 74 [1024/50176]	Loss: 0.1419
Training Epoch: 74 [1536/50176]	Loss: 0.1780
Training Epoch: 74 [2048/50176]	Loss: 0.1950
Training Epoch: 74 [2560/50176]	Loss: 0.1402
Training Epoch: 74 [3072/50176]	Loss: 0.1530
Training Epoch: 74 [3584/50176]	Loss: 0.1756
Training Epoch: 74 [4096/50176]	Loss: 0.1452
Training Epoch: 74 [4608/50176]	Loss: 0.1609
Training Epoch: 74 [5120/50176]	Loss: 0.1256
Training Epoch: 74 [5632/50176]	Loss: 0.1471
Training Epoch: 74 [6144/50176]	Loss: 0.1169
Training Epoch: 74 [6656/50176]	Loss: 0.2122
Training Epoch: 74 [7168/50176]	Loss: 0.1456
Training Epoch: 74 [7680/50176]	Loss: 0.1471
Training Epoch: 74 [8192/50176]	Loss: 0.1608
Training Epoch: 74 [8704/50176]	Loss: 0.1448
Training Epoch: 74 [9216/50176]	Loss: 0.1526
Training Epoch: 74 [9728/50176]	Loss: 0.1308
Training Epoch: 74 [10240/50176]	Loss: 0.1433
Training Epoch: 74 [10752/50176]	Loss: 0.1641
Training Epoch: 74 [11264/50176]	Loss: 0.1365
Training Epoch: 74 [11776/50176]	Loss: 0.1956
Training Epoch: 74 [12288/50176]	Loss: 0.1391
Training Epoch: 74 [12800/50176]	Loss: 0.1477
Training Epoch: 74 [13312/50176]	Loss: 0.1415
Training Epoch: 74 [13824/50176]	Loss: 0.1273
Training Epoch: 74 [14336/50176]	Loss: 0.1542
Training Epoch: 74 [14848/50176]	Loss: 0.1401
Training Epoch: 74 [15360/50176]	Loss: 0.1376
Training Epoch: 74 [15872/50176]	Loss: 0.1882
Training Epoch: 74 [16384/50176]	Loss: 0.1364
Training Epoch: 74 [16896/50176]	Loss: 0.1156
Training Epoch: 74 [17408/50176]	Loss: 0.1507
Training Epoch: 74 [17920/50176]	Loss: 0.1339
Training Epoch: 74 [18432/50176]	Loss: 0.1581
Training Epoch: 74 [18944/50176]	Loss: 0.1531
Training Epoch: 74 [19456/50176]	Loss: 0.1488
Training Epoch: 74 [19968/50176]	Loss: 0.1411
Training Epoch: 74 [20480/50176]	Loss: 0.1553
Training Epoch: 74 [20992/50176]	Loss: 0.1761
Training Epoch: 74 [21504/50176]	Loss: 0.2007
Training Epoch: 74 [22016/50176]	Loss: 0.1212
Training Epoch: 74 [22528/50176]	Loss: 0.1216
Training Epoch: 74 [23040/50176]	Loss: 0.1592
Training Epoch: 74 [23552/50176]	Loss: 0.1600
Training Epoch: 74 [24064/50176]	Loss: 0.1258
Training Epoch: 74 [24576/50176]	Loss: 0.1384
Training Epoch: 74 [25088/50176]	Loss: 0.1584
Training Epoch: 74 [25600/50176]	Loss: 0.1610
Training Epoch: 74 [26112/50176]	Loss: 0.1105
Training Epoch: 74 [26624/50176]	Loss: 0.1478
Training Epoch: 74 [27136/50176]	Loss: 0.1764
Training Epoch: 74 [27648/50176]	Loss: 0.1628
Training Epoch: 74 [28160/50176]	Loss: 0.1979
Training Epoch: 74 [28672/50176]	Loss: 0.1175
Training Epoch: 74 [29184/50176]	Loss: 0.1289
Training Epoch: 74 [29696/50176]	Loss: 0.1177
Training Epoch: 74 [30208/50176]	Loss: 0.1681
Training Epoch: 74 [30720/50176]	Loss: 0.1686
Training Epoch: 74 [31232/50176]	Loss: 0.1535
Training Epoch: 74 [31744/50176]	Loss: 0.1639
Training Epoch: 74 [32256/50176]	Loss: 0.1420
Training Epoch: 74 [32768/50176]	Loss: 0.1598
Training Epoch: 74 [33280/50176]	Loss: 0.1554
Training Epoch: 74 [33792/50176]	Loss: 0.1614
Training Epoch: 74 [34304/50176]	Loss: 0.1448
Training Epoch: 74 [34816/50176]	Loss: 0.1752
Training Epoch: 74 [35328/50176]	Loss: 0.1331
Training Epoch: 74 [35840/50176]	Loss: 0.1590
Training Epoch: 74 [36352/50176]	Loss: 0.1846
Training Epoch: 74 [36864/50176]	Loss: 0.1299
Training Epoch: 74 [37376/50176]	Loss: 0.1408
Training Epoch: 74 [37888/50176]	Loss: 0.1768
Training Epoch: 74 [38400/50176]	Loss: 0.1904
Training Epoch: 74 [38912/50176]	Loss: 0.1317
Training Epoch: 74 [39424/50176]	Loss: 0.1464
Training Epoch: 74 [39936/50176]	Loss: 0.1413
Training Epoch: 74 [40448/50176]	Loss: 0.1587
Training Epoch: 74 [40960/50176]	Loss: 0.2627
Training Epoch: 74 [41472/50176]	Loss: 0.0837
Training Epoch: 74 [41984/50176]	Loss: 0.1438
Training Epoch: 74 [42496/50176]	Loss: 0.1744
Training Epoch: 74 [43008/50176]	Loss: 0.1894
Training Epoch: 74 [43520/50176]	Loss: 0.1372
Training Epoch: 74 [44032/50176]	Loss: 0.1355
Training Epoch: 74 [44544/50176]	Loss: 0.1172
Training Epoch: 74 [45056/50176]	Loss: 0.1556
Training Epoch: 74 [45568/50176]	Loss: 0.1478
Training Epoch: 74 [46080/50176]	Loss: 0.1582
Training Epoch: 74 [46592/50176]	Loss: 0.1441
Training Epoch: 74 [47104/50176]	Loss: 0.1659
Training Epoch: 74 [47616/50176]	Loss: 0.1413
Training Epoch: 74 [48128/50176]	Loss: 0.1743
Training Epoch: 74 [48640/50176]	Loss: 0.1832
Training Epoch: 74 [49152/50176]	Loss: 0.1446
Training Epoch: 74 [49664/50176]	Loss: 0.1466
Training Epoch: 74 [50176/50176]	Loss: 0.1643
Validation Epoch: 74, Average loss: 0.0054, Accuracy: 0.5940
Training Epoch: 75 [512/50176]	Loss: 0.1622
Training Epoch: 75 [1024/50176]	Loss: 0.0988
Training Epoch: 75 [1536/50176]	Loss: 0.1229
Training Epoch: 75 [2048/50176]	Loss: 0.1432
Training Epoch: 75 [2560/50176]	Loss: 0.1156
Training Epoch: 75 [3072/50176]	Loss: 0.1177
Training Epoch: 75 [3584/50176]	Loss: 0.1360
Training Epoch: 75 [4096/50176]	Loss: 0.1330
Training Epoch: 75 [4608/50176]	Loss: 0.1050
Training Epoch: 75 [5120/50176]	Loss: 0.1504
Training Epoch: 75 [5632/50176]	Loss: 0.1227
Training Epoch: 75 [6144/50176]	Loss: 0.1042
Training Epoch: 75 [6656/50176]	Loss: 0.1470
Training Epoch: 75 [7168/50176]	Loss: 0.1673
Training Epoch: 75 [7680/50176]	Loss: 0.1271
Training Epoch: 75 [8192/50176]	Loss: 0.1360
Training Epoch: 75 [8704/50176]	Loss: 0.1446
Training Epoch: 75 [9216/50176]	Loss: 0.1394
Training Epoch: 75 [9728/50176]	Loss: 0.1536
Training Epoch: 75 [10240/50176]	Loss: 0.1620
Training Epoch: 75 [10752/50176]	Loss: 0.1154
Training Epoch: 75 [11264/50176]	Loss: 0.1123
Training Epoch: 75 [11776/50176]	Loss: 0.1158
Training Epoch: 75 [12288/50176]	Loss: 0.0959
Training Epoch: 75 [12800/50176]	Loss: 0.1298
Training Epoch: 75 [13312/50176]	Loss: 0.1308
Training Epoch: 75 [13824/50176]	Loss: 0.1367
Training Epoch: 75 [14336/50176]	Loss: 0.1262
Training Epoch: 75 [14848/50176]	Loss: 0.1244
Training Epoch: 75 [15360/50176]	Loss: 0.1510
Training Epoch: 75 [15872/50176]	Loss: 0.1360
Training Epoch: 75 [16384/50176]	Loss: 0.1201
Training Epoch: 75 [16896/50176]	Loss: 0.1514
Training Epoch: 75 [17408/50176]	Loss: 0.1240
Training Epoch: 75 [17920/50176]	Loss: 0.1137
Training Epoch: 75 [18432/50176]	Loss: 0.1277
Training Epoch: 75 [18944/50176]	Loss: 0.1074
Training Epoch: 75 [19456/50176]	Loss: 0.1518
Training Epoch: 75 [19968/50176]	Loss: 0.1825
Training Epoch: 75 [20480/50176]	Loss: 0.1520
Training Epoch: 75 [20992/50176]	Loss: 0.1614
Training Epoch: 75 [21504/50176]	Loss: 0.1101
Training Epoch: 75 [22016/50176]	Loss: 0.1431
Training Epoch: 75 [22528/50176]	Loss: 0.1546
Training Epoch: 75 [23040/50176]	Loss: 0.1619
Training Epoch: 75 [23552/50176]	Loss: 0.1353
Training Epoch: 75 [24064/50176]	Loss: 0.1314
Training Epoch: 75 [24576/50176]	Loss: 0.1248
Training Epoch: 75 [25088/50176]	Loss: 0.1761
Training Epoch: 75 [25600/50176]	Loss: 0.1474
Training Epoch: 75 [26112/50176]	Loss: 0.1608
Training Epoch: 75 [26624/50176]	Loss: 0.1647
Training Epoch: 75 [27136/50176]	Loss: 0.1365
Training Epoch: 75 [27648/50176]	Loss: 0.1884
Training Epoch: 75 [28160/50176]	Loss: 0.1500
Training Epoch: 75 [28672/50176]	Loss: 0.1359
Training Epoch: 75 [29184/50176]	Loss: 0.1370
Training Epoch: 75 [29696/50176]	Loss: 0.1327
Training Epoch: 75 [30208/50176]	Loss: 0.1556
Training Epoch: 75 [30720/50176]	Loss: 0.1640
Training Epoch: 75 [31232/50176]	Loss: 0.1561
Training Epoch: 75 [31744/50176]	Loss: 0.1912
Training Epoch: 75 [32256/50176]	Loss: 0.1766
Training Epoch: 75 [32768/50176]	Loss: 0.1861
Training Epoch: 75 [33280/50176]	Loss: 0.1483
Training Epoch: 75 [33792/50176]	Loss: 0.1536
Training Epoch: 75 [34304/50176]	Loss: 0.1740
Training Epoch: 75 [34816/50176]	Loss: 0.1479
Training Epoch: 75 [35328/50176]	Loss: 0.1761
Training Epoch: 75 [35840/50176]	Loss: 0.1789
Training Epoch: 75 [36352/50176]	Loss: 0.1817
Training Epoch: 75 [36864/50176]	Loss: 0.1682
Training Epoch: 75 [37376/50176]	Loss: 0.1619
Training Epoch: 75 [37888/50176]	Loss: 0.1574
Training Epoch: 75 [38400/50176]	Loss: 0.1695
Training Epoch: 75 [38912/50176]	Loss: 0.1501
Training Epoch: 75 [39424/50176]	Loss: 0.1817
Training Epoch: 75 [39936/50176]	Loss: 0.2025
Training Epoch: 75 [40448/50176]	Loss: 0.1840
Training Epoch: 75 [40960/50176]	Loss: 0.1548
Training Epoch: 75 [41472/50176]	Loss: 0.1736
Training Epoch: 75 [41984/50176]	Loss: 0.1554
Training Epoch: 75 [42496/50176]	Loss: 0.1827
Training Epoch: 75 [43008/50176]	Loss: 0.1979
Training Epoch: 75 [43520/50176]	Loss: 0.1591
Training Epoch: 75 [44032/50176]	Loss: 0.1658
Training Epoch: 75 [44544/50176]	Loss: 0.1539
Training Epoch: 75 [45056/50176]	Loss: 0.1866
Training Epoch: 75 [45568/50176]	Loss: 0.1565
Training Epoch: 75 [46080/50176]	Loss: 0.1253
Training Epoch: 75 [46592/50176]	Loss: 0.1702
Training Epoch: 75 [47104/50176]	Loss: 0.1547
Training Epoch: 75 [47616/50176]	Loss: 0.1778
Training Epoch: 75 [48128/50176]	Loss: 0.1648
Training Epoch: 75 [48640/50176]	Loss: 0.1774
Training Epoch: 75 [49152/50176]	Loss: 0.1851
Training Epoch: 75 [49664/50176]	Loss: 0.1762
Training Epoch: 75 [50176/50176]	Loss: 0.2062
Validation Epoch: 75, Average loss: 0.0055, Accuracy: 0.5803
Training Epoch: 76 [512/50176]	Loss: 0.1378
Training Epoch: 76 [1024/50176]	Loss: 0.1625
Training Epoch: 76 [1536/50176]	Loss: 0.1434
Training Epoch: 76 [2048/50176]	Loss: 0.1433
Training Epoch: 76 [2560/50176]	Loss: 0.1960
Training Epoch: 76 [3072/50176]	Loss: 0.0963
Training Epoch: 76 [3584/50176]	Loss: 0.1560
Training Epoch: 76 [4096/50176]	Loss: 0.1556
Training Epoch: 76 [4608/50176]	Loss: 0.1389
Training Epoch: 76 [5120/50176]	Loss: 0.1607
Training Epoch: 76 [5632/50176]	Loss: 0.1431
Training Epoch: 76 [6144/50176]	Loss: 0.1303
Training Epoch: 76 [6656/50176]	Loss: 0.1133
Training Epoch: 76 [7168/50176]	Loss: 0.1394
Training Epoch: 76 [7680/50176]	Loss: 0.1723
Training Epoch: 76 [8192/50176]	Loss: 0.1565
Training Epoch: 76 [8704/50176]	Loss: 0.1172
Training Epoch: 76 [9216/50176]	Loss: 0.1474
Training Epoch: 76 [9728/50176]	Loss: 0.1475
Training Epoch: 76 [10240/50176]	Loss: 0.1477
Training Epoch: 76 [10752/50176]	Loss: 0.1129
Training Epoch: 76 [11264/50176]	Loss: 0.1467
Training Epoch: 76 [11776/50176]	Loss: 0.1325
Training Epoch: 76 [12288/50176]	Loss: 0.1181
Training Epoch: 76 [12800/50176]	Loss: 0.1373
Training Epoch: 76 [13312/50176]	Loss: 0.1587
Training Epoch: 76 [13824/50176]	Loss: 0.1473
Training Epoch: 76 [14336/50176]	Loss: 0.1590
Training Epoch: 76 [14848/50176]	Loss: 0.1718
Training Epoch: 76 [15360/50176]	Loss: 0.1548
Training Epoch: 76 [15872/50176]	Loss: 0.1277
Training Epoch: 76 [16384/50176]	Loss: 0.1866
Training Epoch: 76 [16896/50176]	Loss: 0.1505
Training Epoch: 76 [17408/50176]	Loss: 0.1525
Training Epoch: 76 [17920/50176]	Loss: 0.1426
Training Epoch: 76 [18432/50176]	Loss: 0.1384
Training Epoch: 76 [18944/50176]	Loss: 0.1169
Training Epoch: 76 [19456/50176]	Loss: 0.1747
Training Epoch: 76 [19968/50176]	Loss: 0.1794
Training Epoch: 76 [20480/50176]	Loss: 0.1514
Training Epoch: 76 [20992/50176]	Loss: 0.1579
Training Epoch: 76 [21504/50176]	Loss: 0.1390
Training Epoch: 76 [22016/50176]	Loss: 0.1606
Training Epoch: 76 [22528/50176]	Loss: 0.1916
Training Epoch: 76 [23040/50176]	Loss: 0.1359
Training Epoch: 76 [23552/50176]	Loss: 0.1409
Training Epoch: 76 [24064/50176]	Loss: 0.1375
Training Epoch: 76 [24576/50176]	Loss: 0.1781
Training Epoch: 76 [25088/50176]	Loss: 0.1610
Training Epoch: 76 [25600/50176]	Loss: 0.1407
Training Epoch: 76 [26112/50176]	Loss: 0.1753
Training Epoch: 76 [26624/50176]	Loss: 0.1538
Training Epoch: 76 [27136/50176]	Loss: 0.1686
Training Epoch: 76 [27648/50176]	Loss: 0.1855
Training Epoch: 76 [28160/50176]	Loss: 0.1896
Training Epoch: 76 [28672/50176]	Loss: 0.1379
Training Epoch: 76 [29184/50176]	Loss: 0.1463
Training Epoch: 76 [29696/50176]	Loss: 0.1815
Training Epoch: 76 [30208/50176]	Loss: 0.1575
Training Epoch: 76 [30720/50176]	Loss: 0.1638
Training Epoch: 76 [31232/50176]	Loss: 0.1973
Training Epoch: 76 [31744/50176]	Loss: 0.1749
Training Epoch: 76 [32256/50176]	Loss: 0.1623
Training Epoch: 76 [32768/50176]	Loss: 0.1460
Training Epoch: 76 [33280/50176]	Loss: 0.1400
Training Epoch: 76 [33792/50176]	Loss: 0.1722
Training Epoch: 76 [34304/50176]	Loss: 0.1850
Training Epoch: 76 [34816/50176]	Loss: 0.1667
Training Epoch: 76 [35328/50176]	Loss: 0.1647
Training Epoch: 76 [35840/50176]	Loss: 0.1775
Training Epoch: 76 [36352/50176]	Loss: 0.1207
Training Epoch: 76 [36864/50176]	Loss: 0.1332
Training Epoch: 76 [37376/50176]	Loss: 0.1179
Training Epoch: 76 [37888/50176]	Loss: 0.1679
Training Epoch: 76 [38400/50176]	Loss: 0.1637
Training Epoch: 76 [38912/50176]	Loss: 0.1928
Training Epoch: 76 [39424/50176]	Loss: 0.1772
Training Epoch: 76 [39936/50176]	Loss: 0.1661
Training Epoch: 76 [40448/50176]	Loss: 0.1546
Training Epoch: 76 [40960/50176]	Loss: 0.1285
Training Epoch: 76 [41472/50176]	Loss: 0.1953
Training Epoch: 76 [41984/50176]	Loss: 0.1673
Training Epoch: 76 [42496/50176]	Loss: 0.1650
Training Epoch: 76 [43008/50176]	Loss: 0.1380
Training Epoch: 76 [43520/50176]	Loss: 0.1420
Training Epoch: 76 [44032/50176]	Loss: 0.1706
Training Epoch: 76 [44544/50176]	Loss: 0.1423
Training Epoch: 76 [45056/50176]	Loss: 0.1192
Training Epoch: 76 [45568/50176]	Loss: 0.2258
Training Epoch: 76 [46080/50176]	Loss: 0.1425
Training Epoch: 76 [46592/50176]	Loss: 0.1081
Training Epoch: 76 [47104/50176]	Loss: 0.1279
Training Epoch: 76 [47616/50176]	Loss: 0.1845
Training Epoch: 76 [48128/50176]	Loss: 0.1933
Training Epoch: 76 [48640/50176]	Loss: 0.2093
Training Epoch: 76 [49152/50176]	Loss: 0.2261
Training Epoch: 76 [49664/50176]	Loss: 0.1679
Training Epoch: 76 [50176/50176]	Loss: 0.1405
Validation Epoch: 76, Average loss: 0.0055, Accuracy: 0.5920
Training Epoch: 77 [512/50176]	Loss: 0.1735
Training Epoch: 77 [1024/50176]	Loss: 0.1789
Training Epoch: 77 [1536/50176]	Loss: 0.1246
Training Epoch: 77 [2048/50176]	Loss: 0.1524
Training Epoch: 77 [2560/50176]	Loss: 0.1321
Training Epoch: 77 [3072/50176]	Loss: 0.1199
Training Epoch: 77 [3584/50176]	Loss: 0.1660
Training Epoch: 77 [4096/50176]	Loss: 0.1607
Training Epoch: 77 [4608/50176]	Loss: 0.1531
Training Epoch: 77 [5120/50176]	Loss: 0.0992
Training Epoch: 77 [5632/50176]	Loss: 0.1357
Training Epoch: 77 [6144/50176]	Loss: 0.1136
Training Epoch: 77 [6656/50176]	Loss: 0.1441
Training Epoch: 77 [7168/50176]	Loss: 0.1675
Training Epoch: 77 [7680/50176]	Loss: 0.1508
Training Epoch: 77 [8192/50176]	Loss: 0.1835
Training Epoch: 77 [8704/50176]	Loss: 0.1373
Training Epoch: 77 [9216/50176]	Loss: 0.1047
Training Epoch: 77 [9728/50176]	Loss: 0.1513
Training Epoch: 77 [10240/50176]	Loss: 0.1600
Training Epoch: 77 [10752/50176]	Loss: 0.1560
Training Epoch: 77 [11264/50176]	Loss: 0.1170
Training Epoch: 77 [11776/50176]	Loss: 0.1272
Training Epoch: 77 [12288/50176]	Loss: 0.1536
Training Epoch: 77 [12800/50176]	Loss: 0.1267
Training Epoch: 77 [13312/50176]	Loss: 0.1293
Training Epoch: 77 [13824/50176]	Loss: 0.1481
Training Epoch: 77 [14336/50176]	Loss: 0.1808
Training Epoch: 77 [14848/50176]	Loss: 0.1918
Training Epoch: 77 [15360/50176]	Loss: 0.1488
Training Epoch: 77 [15872/50176]	Loss: 0.1501
Training Epoch: 77 [16384/50176]	Loss: 0.1346
Training Epoch: 77 [16896/50176]	Loss: 0.1377
Training Epoch: 77 [17408/50176]	Loss: 0.1362
Training Epoch: 77 [17920/50176]	Loss: 0.1943
Training Epoch: 77 [18432/50176]	Loss: 0.1793
Training Epoch: 77 [18944/50176]	Loss: 0.0988
Training Epoch: 77 [19456/50176]	Loss: 0.1768
Training Epoch: 77 [19968/50176]	Loss: 0.1382
Training Epoch: 77 [20480/50176]	Loss: 0.1544
Training Epoch: 77 [20992/50176]	Loss: 0.1477
Training Epoch: 77 [21504/50176]	Loss: 0.1472
Training Epoch: 77 [22016/50176]	Loss: 0.1585
Training Epoch: 77 [22528/50176]	Loss: 0.1593
Training Epoch: 77 [23040/50176]	Loss: 0.1721
Training Epoch: 77 [23552/50176]	Loss: 0.1320
Training Epoch: 77 [24064/50176]	Loss: 0.1447
Training Epoch: 77 [24576/50176]	Loss: 0.1986
Training Epoch: 77 [25088/50176]	Loss: 0.1413
Training Epoch: 77 [25600/50176]	Loss: 0.1326
Training Epoch: 77 [26112/50176]	Loss: 0.1701
Training Epoch: 77 [26624/50176]	Loss: 0.2243
Training Epoch: 77 [27136/50176]	Loss: 0.1483
Training Epoch: 77 [27648/50176]	Loss: 0.1425
Training Epoch: 77 [28160/50176]	Loss: 0.2103
Training Epoch: 77 [28672/50176]	Loss: 0.1860
Training Epoch: 77 [29184/50176]	Loss: 0.1851
Training Epoch: 77 [29696/50176]	Loss: 0.1653
Training Epoch: 77 [30208/50176]	Loss: 0.1481
Training Epoch: 77 [30720/50176]	Loss: 0.1792
Training Epoch: 77 [31232/50176]	Loss: 0.1407
Training Epoch: 77 [31744/50176]	Loss: 0.1746
Training Epoch: 77 [32256/50176]	Loss: 0.1310
Training Epoch: 77 [32768/50176]	Loss: 0.1562
Training Epoch: 77 [33280/50176]	Loss: 0.1666
Training Epoch: 77 [33792/50176]	Loss: 0.1342
Training Epoch: 77 [34304/50176]	Loss: 0.1618
Training Epoch: 77 [34816/50176]	Loss: 0.1610
Training Epoch: 77 [35328/50176]	Loss: 0.1544
Training Epoch: 77 [35840/50176]	Loss: 0.1777
Training Epoch: 77 [36352/50176]	Loss: 0.1768
Training Epoch: 77 [36864/50176]	Loss: 0.1844
Training Epoch: 77 [37376/50176]	Loss: 0.1248
Training Epoch: 77 [37888/50176]	Loss: 0.2092
Training Epoch: 77 [38400/50176]	Loss: 0.1643
Training Epoch: 77 [38912/50176]	Loss: 0.1078
Training Epoch: 77 [39424/50176]	Loss: 0.1791
Training Epoch: 77 [39936/50176]	Loss: 0.1675
Training Epoch: 77 [40448/50176]	Loss: 0.1448
Training Epoch: 77 [40960/50176]	Loss: 0.1876
Training Epoch: 77 [41472/50176]	Loss: 0.1781
Training Epoch: 77 [41984/50176]	Loss: 0.2127
Training Epoch: 77 [42496/50176]	Loss: 0.1649
Training Epoch: 77 [43008/50176]	Loss: 0.1935
Training Epoch: 77 [43520/50176]	Loss: 0.1778
Training Epoch: 77 [44032/50176]	Loss: 0.1648
Training Epoch: 77 [44544/50176]	Loss: 0.1746
Training Epoch: 77 [45056/50176]	Loss: 0.1698
Training Epoch: 77 [45568/50176]	Loss: 0.1761
Training Epoch: 77 [46080/50176]	Loss: 0.1313
Training Epoch: 77 [46592/50176]	Loss: 0.1983
Training Epoch: 77 [47104/50176]	Loss: 0.1484
Training Epoch: 77 [47616/50176]	Loss: 0.1876
Training Epoch: 77 [48128/50176]	Loss: 0.1383
Training Epoch: 77 [48640/50176]	Loss: 0.1660
Training Epoch: 77 [49152/50176]	Loss: 0.1814
Training Epoch: 77 [49664/50176]	Loss: 0.1861
Training Epoch: 77 [50176/50176]	Loss: 0.1263
Validation Epoch: 77, Average loss: 0.0057, Accuracy: 0.5898
Training Epoch: 78 [512/50176]	Loss: 0.1620
Training Epoch: 78 [1024/50176]	Loss: 0.1351
Training Epoch: 78 [1536/50176]	Loss: 0.1419
Training Epoch: 78 [2048/50176]	Loss: 0.1585
Training Epoch: 78 [2560/50176]	Loss: 0.1273
Training Epoch: 78 [3072/50176]	Loss: 0.1374
Training Epoch: 78 [3584/50176]	Loss: 0.1321
Training Epoch: 78 [4096/50176]	Loss: 0.1289
Training Epoch: 78 [4608/50176]	Loss: 0.2024
Training Epoch: 78 [5120/50176]	Loss: 0.2024
Training Epoch: 78 [5632/50176]	Loss: 0.1209
Training Epoch: 78 [6144/50176]	Loss: 0.1581
Training Epoch: 78 [6656/50176]	Loss: 0.1304
Training Epoch: 78 [7168/50176]	Loss: 0.1395
Training Epoch: 78 [7680/50176]	Loss: 0.1657
Training Epoch: 78 [8192/50176]	Loss: 0.1549
Training Epoch: 78 [8704/50176]	Loss: 0.1677
Training Epoch: 78 [9216/50176]	Loss: 0.1758
Training Epoch: 78 [9728/50176]	Loss: 0.1828
Training Epoch: 78 [10240/50176]	Loss: 0.1682
Training Epoch: 78 [10752/50176]	Loss: 0.1248
Training Epoch: 78 [11264/50176]	Loss: 0.1450
Training Epoch: 78 [11776/50176]	Loss: 0.1821
Training Epoch: 78 [12288/50176]	Loss: 0.1390
Training Epoch: 78 [12800/50176]	Loss: 0.1018
Training Epoch: 78 [13312/50176]	Loss: 0.1497
Training Epoch: 78 [13824/50176]	Loss: 0.1267
Training Epoch: 78 [14336/50176]	Loss: 0.1576
Training Epoch: 78 [14848/50176]	Loss: 0.1493
Training Epoch: 78 [15360/50176]	Loss: 0.1307
Training Epoch: 78 [15872/50176]	Loss: 0.1340
Training Epoch: 78 [16384/50176]	Loss: 0.1499
Training Epoch: 78 [16896/50176]	Loss: 0.1508
Training Epoch: 78 [17408/50176]	Loss: 0.1452
Training Epoch: 78 [17920/50176]	Loss: 0.1103
Training Epoch: 78 [18432/50176]	Loss: 0.1355
Training Epoch: 78 [18944/50176]	Loss: 0.1878
Training Epoch: 78 [19456/50176]	Loss: 0.1960
Training Epoch: 78 [19968/50176]	Loss: 0.1710
Training Epoch: 78 [20480/50176]	Loss: 0.1287
Training Epoch: 78 [20992/50176]	Loss: 0.1934
Training Epoch: 78 [21504/50176]	Loss: 0.1646
Training Epoch: 78 [22016/50176]	Loss: 0.1205
Training Epoch: 78 [22528/50176]	Loss: 0.1324
Training Epoch: 78 [23040/50176]	Loss: 0.1630
Training Epoch: 78 [23552/50176]	Loss: 0.1413
Training Epoch: 78 [24064/50176]	Loss: 0.1423
Training Epoch: 78 [24576/50176]	Loss: 0.1590
Training Epoch: 78 [25088/50176]	Loss: 0.1889
Training Epoch: 78 [25600/50176]	Loss: 0.1436
Training Epoch: 78 [26112/50176]	Loss: 0.1804
Training Epoch: 78 [26624/50176]	Loss: 0.1636
Training Epoch: 78 [27136/50176]	Loss: 0.1330
Training Epoch: 78 [27648/50176]	Loss: 0.1598
Training Epoch: 78 [28160/50176]	Loss: 0.1713
Training Epoch: 78 [28672/50176]	Loss: 0.1657
Training Epoch: 78 [29184/50176]	Loss: 0.1163
Training Epoch: 78 [29696/50176]	Loss: 0.1720
Training Epoch: 78 [30208/50176]	Loss: 0.1276
Training Epoch: 78 [30720/50176]	Loss: 0.1246
Training Epoch: 78 [31232/50176]	Loss: 0.1533
Training Epoch: 78 [31744/50176]	Loss: 0.1985
Training Epoch: 78 [32256/50176]	Loss: 0.1268
Training Epoch: 78 [32768/50176]	Loss: 0.1419
Training Epoch: 78 [33280/50176]	Loss: 0.1575
Training Epoch: 78 [33792/50176]	Loss: 0.1550
Training Epoch: 78 [34304/50176]	Loss: 0.1795
Training Epoch: 78 [34816/50176]	Loss: 0.1265
Training Epoch: 78 [35328/50176]	Loss: 0.1952
Training Epoch: 78 [35840/50176]	Loss: 0.1372
Training Epoch: 78 [36352/50176]	Loss: 0.2333
Training Epoch: 78 [36864/50176]	Loss: 0.1992
Training Epoch: 78 [37376/50176]	Loss: 0.1671
Training Epoch: 78 [37888/50176]	Loss: 0.1942
Training Epoch: 78 [38400/50176]	Loss: 0.1551
Training Epoch: 78 [38912/50176]	Loss: 0.1171
Training Epoch: 78 [39424/50176]	Loss: 0.1297
Training Epoch: 78 [39936/50176]	Loss: 0.1475
Training Epoch: 78 [40448/50176]	Loss: 0.1638
Training Epoch: 78 [40960/50176]	Loss: 0.1786
Training Epoch: 78 [41472/50176]	Loss: 0.1672
Training Epoch: 78 [41984/50176]	Loss: 0.1765
Training Epoch: 78 [42496/50176]	Loss: 0.1825
Training Epoch: 78 [43008/50176]	Loss: 0.1954
Training Epoch: 78 [43520/50176]	Loss: 0.2011
Training Epoch: 78 [44032/50176]	Loss: 0.1657
Training Epoch: 78 [44544/50176]	Loss: 0.1733
Training Epoch: 78 [45056/50176]	Loss: 0.1320
Training Epoch: 78 [45568/50176]	Loss: 0.1211
Training Epoch: 78 [46080/50176]	Loss: 0.1825
Training Epoch: 78 [46592/50176]	Loss: 0.1645
Training Epoch: 78 [47104/50176]	Loss: 0.1188
Training Epoch: 78 [47616/50176]	Loss: 0.1702
Training Epoch: 78 [48128/50176]	Loss: 0.1906
Training Epoch: 78 [48640/50176]	Loss: 0.1662
Training Epoch: 78 [49152/50176]	Loss: 0.2102
Training Epoch: 78 [49664/50176]	Loss: 0.1963
Training Epoch: 78 [50176/50176]	Loss: 0.1700
Validation Epoch: 78, Average loss: 0.0062, Accuracy: 0.5780
Training Epoch: 79 [512/50176]	Loss: 0.1216
Training Epoch: 79 [1024/50176]	Loss: 0.1169
Training Epoch: 79 [1536/50176]	Loss: 0.1405
Training Epoch: 79 [2048/50176]	Loss: 0.1498
Training Epoch: 79 [2560/50176]	Loss: 0.1334
Training Epoch: 79 [3072/50176]	Loss: 0.1243
Training Epoch: 79 [3584/50176]	Loss: 0.1280
Training Epoch: 79 [4096/50176]	Loss: 0.1511
Training Epoch: 79 [4608/50176]	Loss: 0.1491
Training Epoch: 79 [5120/50176]	Loss: 0.1597
Training Epoch: 79 [5632/50176]	Loss: 0.1329
Training Epoch: 79 [6144/50176]	Loss: 0.1144
Training Epoch: 79 [6656/50176]	Loss: 0.1058
Training Epoch: 79 [7168/50176]	Loss: 0.1185
Training Epoch: 79 [7680/50176]	Loss: 0.1568
Training Epoch: 79 [8192/50176]	Loss: 0.1741
Training Epoch: 79 [8704/50176]	Loss: 0.1047
Training Epoch: 79 [9216/50176]	Loss: 0.1361
Training Epoch: 79 [9728/50176]	Loss: 0.1648
Training Epoch: 79 [10240/50176]	Loss: 0.1606
Training Epoch: 79 [10752/50176]	Loss: 0.1200
Training Epoch: 79 [11264/50176]	Loss: 0.1727
Training Epoch: 79 [11776/50176]	Loss: 0.1390
Training Epoch: 79 [12288/50176]	Loss: 0.1692
Training Epoch: 79 [12800/50176]	Loss: 0.1207
Training Epoch: 79 [13312/50176]	Loss: 0.1379
Training Epoch: 79 [13824/50176]	Loss: 0.1423
Training Epoch: 79 [14336/50176]	Loss: 0.1459
Training Epoch: 79 [14848/50176]	Loss: 0.1662
Training Epoch: 79 [15360/50176]	Loss: 0.1374
Training Epoch: 79 [15872/50176]	Loss: 0.1020
Training Epoch: 79 [16384/50176]	Loss: 0.0885
Training Epoch: 79 [16896/50176]	Loss: 0.1821
Training Epoch: 79 [17408/50176]	Loss: 0.1364
Training Epoch: 79 [17920/50176]	Loss: 0.0924
Training Epoch: 79 [18432/50176]	Loss: 0.1377
Training Epoch: 79 [18944/50176]	Loss: 0.1650
Training Epoch: 79 [19456/50176]	Loss: 0.2315
Training Epoch: 79 [19968/50176]	Loss: 0.1081
Training Epoch: 79 [20480/50176]	Loss: 0.1267
Training Epoch: 79 [20992/50176]	Loss: 0.1314
Training Epoch: 79 [21504/50176]	Loss: 0.1290
Training Epoch: 79 [22016/50176]	Loss: 0.1417
Training Epoch: 79 [22528/50176]	Loss: 0.1304
Training Epoch: 79 [23040/50176]	Loss: 0.0859
Training Epoch: 79 [23552/50176]	Loss: 0.1326
Training Epoch: 79 [24064/50176]	Loss: 0.1584
Training Epoch: 79 [24576/50176]	Loss: 0.1502
Training Epoch: 79 [25088/50176]	Loss: 0.1448
Training Epoch: 79 [25600/50176]	Loss: 0.1712
Training Epoch: 79 [26112/50176]	Loss: 0.1343
Training Epoch: 79 [26624/50176]	Loss: 0.1900
Training Epoch: 79 [27136/50176]	Loss: 0.1831
Training Epoch: 79 [27648/50176]	Loss: 0.1073
Training Epoch: 79 [28160/50176]	Loss: 0.1535
Training Epoch: 79 [28672/50176]	Loss: 0.1517
Training Epoch: 79 [29184/50176]	Loss: 0.1304
Training Epoch: 79 [29696/50176]	Loss: 0.1224
Training Epoch: 79 [30208/50176]	Loss: 0.1576
Training Epoch: 79 [30720/50176]	Loss: 0.1452
Training Epoch: 79 [31232/50176]	Loss: 0.1362
Training Epoch: 79 [31744/50176]	Loss: 0.1417
Training Epoch: 79 [32256/50176]	Loss: 0.1778
Training Epoch: 79 [32768/50176]	Loss: 0.1650
Training Epoch: 79 [33280/50176]	Loss: 0.2131
Training Epoch: 79 [33792/50176]	Loss: 0.1098
Training Epoch: 79 [34304/50176]	Loss: 0.1577
Training Epoch: 79 [34816/50176]	Loss: 0.1196
Training Epoch: 79 [35328/50176]	Loss: 0.1119
Training Epoch: 79 [35840/50176]	Loss: 0.1410
Training Epoch: 79 [36352/50176]	Loss: 0.1377
Training Epoch: 79 [36864/50176]	Loss: 0.1484
Training Epoch: 79 [37376/50176]	Loss: 0.1047
Training Epoch: 79 [37888/50176]	Loss: 0.1357
Training Epoch: 79 [38400/50176]	Loss: 0.1296
Training Epoch: 79 [38912/50176]	Loss: 0.1815
Training Epoch: 79 [39424/50176]	Loss: 0.1522
Training Epoch: 79 [39936/50176]	Loss: 0.1457
Training Epoch: 79 [40448/50176]	Loss: 0.1823
Training Epoch: 79 [40960/50176]	Loss: 0.1439
Training Epoch: 79 [41472/50176]	Loss: 0.1788
Training Epoch: 79 [41984/50176]	Loss: 0.1559
Training Epoch: 79 [42496/50176]	Loss: 0.1817
Training Epoch: 79 [43008/50176]	Loss: 0.1577
Training Epoch: 79 [43520/50176]	Loss: 0.1292
Training Epoch: 79 [44032/50176]	Loss: 0.1686
Training Epoch: 79 [44544/50176]	Loss: 0.1512
Training Epoch: 79 [45056/50176]	Loss: 0.1694
Training Epoch: 79 [45568/50176]	Loss: 0.1353
Training Epoch: 79 [46080/50176]	Loss: 0.1152
Training Epoch: 79 [46592/50176]	Loss: 0.1463
Training Epoch: 79 [47104/50176]	Loss: 0.1292
Training Epoch: 79 [47616/50176]	Loss: 0.1724
Training Epoch: 79 [48128/50176]	Loss: 0.1418
Training Epoch: 79 [48640/50176]	Loss: 0.1514
Training Epoch: 79 [49152/50176]	Loss: 0.1405
Training Epoch: 79 [49664/50176]	Loss: 0.1075
Training Epoch: 79 [50176/50176]	Loss: 0.1890
Validation Epoch: 79, Average loss: 0.0059, Accuracy: 0.5835
Training Epoch: 80 [512/50176]	Loss: 0.1079
Training Epoch: 80 [1024/50176]	Loss: 0.1247
Training Epoch: 80 [1536/50176]	Loss: 0.1411
Training Epoch: 80 [2048/50176]	Loss: 0.1134
Training Epoch: 80 [2560/50176]	Loss: 0.1438
Training Epoch: 80 [3072/50176]	Loss: 0.1005
Training Epoch: 80 [3584/50176]	Loss: 0.1315
Training Epoch: 80 [4096/50176]	Loss: 0.1340
Training Epoch: 80 [4608/50176]	Loss: 0.1215
Training Epoch: 80 [5120/50176]	Loss: 0.1165
Training Epoch: 80 [5632/50176]	Loss: 0.1157
Training Epoch: 80 [6144/50176]	Loss: 0.1294
Training Epoch: 80 [6656/50176]	Loss: 0.0953
Training Epoch: 80 [7168/50176]	Loss: 0.1069
Training Epoch: 80 [7680/50176]	Loss: 0.1146
Training Epoch: 80 [8192/50176]	Loss: 0.1297
Training Epoch: 80 [8704/50176]	Loss: 0.1086
Training Epoch: 80 [9216/50176]	Loss: 0.1327
Training Epoch: 80 [9728/50176]	Loss: 0.1144
Training Epoch: 80 [10240/50176]	Loss: 0.1078
Training Epoch: 80 [10752/50176]	Loss: 0.1275
Training Epoch: 80 [11264/50176]	Loss: 0.1196
Training Epoch: 80 [11776/50176]	Loss: 0.1800
Training Epoch: 80 [12288/50176]	Loss: 0.1325
Training Epoch: 80 [12800/50176]	Loss: 0.1128
Training Epoch: 80 [13312/50176]	Loss: 0.1388
Training Epoch: 80 [13824/50176]	Loss: 0.1250
Training Epoch: 80 [14336/50176]	Loss: 0.1583
Training Epoch: 80 [14848/50176]	Loss: 0.1045
Training Epoch: 80 [15360/50176]	Loss: 0.1107
Training Epoch: 80 [15872/50176]	Loss: 0.1321
Training Epoch: 80 [16384/50176]	Loss: 0.1106
Training Epoch: 80 [16896/50176]	Loss: 0.1129
Training Epoch: 80 [17408/50176]	Loss: 0.1540
Training Epoch: 80 [17920/50176]	Loss: 0.1224
Training Epoch: 80 [18432/50176]	Loss: 0.1529
Training Epoch: 80 [18944/50176]	Loss: 0.1292
Training Epoch: 80 [19456/50176]	Loss: 0.1336
Training Epoch: 80 [19968/50176]	Loss: 0.1234
Training Epoch: 80 [20480/50176]	Loss: 0.1022
Training Epoch: 80 [20992/50176]	Loss: 0.1434
Training Epoch: 80 [21504/50176]	Loss: 0.1221
Training Epoch: 80 [22016/50176]	Loss: 0.1559
Training Epoch: 80 [22528/50176]	Loss: 0.1036
Training Epoch: 80 [23040/50176]	Loss: 0.1633
Training Epoch: 80 [23552/50176]	Loss: 0.1386
Training Epoch: 80 [24064/50176]	Loss: 0.1530
Training Epoch: 80 [24576/50176]	Loss: 0.1318
Training Epoch: 80 [25088/50176]	Loss: 0.1344
Training Epoch: 80 [25600/50176]	Loss: 0.1002
Training Epoch: 80 [26112/50176]	Loss: 0.2259
Training Epoch: 80 [26624/50176]	Loss: 0.0903
Training Epoch: 80 [27136/50176]	Loss: 0.1388
Training Epoch: 80 [27648/50176]	Loss: 0.1269
Training Epoch: 80 [28160/50176]	Loss: 0.1442
Training Epoch: 80 [28672/50176]	Loss: 0.2000
Training Epoch: 80 [29184/50176]	Loss: 0.1498
Training Epoch: 80 [29696/50176]	Loss: 0.1365
Training Epoch: 80 [30208/50176]	Loss: 0.1749
Training Epoch: 80 [30720/50176]	Loss: 0.1369
Training Epoch: 80 [31232/50176]	Loss: 0.1582
Training Epoch: 80 [31744/50176]	Loss: 0.1456
Training Epoch: 80 [32256/50176]	Loss: 0.1440
Training Epoch: 80 [32768/50176]	Loss: 0.1513
Training Epoch: 80 [33280/50176]	Loss: 0.1209
Training Epoch: 80 [33792/50176]	Loss: 0.1963
Training Epoch: 80 [34304/50176]	Loss: 0.1869
Training Epoch: 80 [34816/50176]	Loss: 0.1827
Training Epoch: 80 [35328/50176]	Loss: 0.1453
Training Epoch: 80 [35840/50176]	Loss: 0.1936
Training Epoch: 80 [36352/50176]	Loss: 0.1406
Training Epoch: 80 [36864/50176]	Loss: 0.1191
Training Epoch: 80 [37376/50176]	Loss: 0.1639
Training Epoch: 80 [37888/50176]	Loss: 0.1383
Training Epoch: 80 [38400/50176]	Loss: 0.1365
Training Epoch: 80 [38912/50176]	Loss: 0.1458
Training Epoch: 80 [39424/50176]	Loss: 0.1758
Training Epoch: 80 [39936/50176]	Loss: 0.1748
Training Epoch: 80 [40448/50176]	Loss: 0.1818
Training Epoch: 80 [40960/50176]	Loss: 0.1165
Training Epoch: 80 [41472/50176]	Loss: 0.1573
Training Epoch: 80 [41984/50176]	Loss: 0.1319
Training Epoch: 80 [42496/50176]	Loss: 0.1370
Training Epoch: 80 [43008/50176]	Loss: 0.1354
Training Epoch: 80 [43520/50176]	Loss: 0.1145
Training Epoch: 80 [44032/50176]	Loss: 0.2168
Training Epoch: 80 [44544/50176]	Loss: 0.1354
Training Epoch: 80 [45056/50176]	Loss: 0.1022
Training Epoch: 80 [45568/50176]	Loss: 0.1661
Training Epoch: 80 [46080/50176]	Loss: 0.1721
Training Epoch: 80 [46592/50176]	Loss: 0.1476
Training Epoch: 80 [47104/50176]	Loss: 0.1857
Training Epoch: 80 [47616/50176]	Loss: 0.1219
Training Epoch: 80 [48128/50176]	Loss: 0.1727
Training Epoch: 80 [48640/50176]	Loss: 0.1540
Training Epoch: 80 [49152/50176]	Loss: 0.1703
Training Epoch: 80 [49664/50176]	Loss: 0.1351
Training Epoch: 80 [50176/50176]	Loss: 0.2115
Validation Epoch: 80, Average loss: 0.0057, Accuracy: 0.5945
Training Epoch: 81 [512/50176]	Loss: 0.1682
Training Epoch: 81 [1024/50176]	Loss: 0.1318
Training Epoch: 81 [1536/50176]	Loss: 0.1725
Training Epoch: 81 [2048/50176]	Loss: 0.1543
Training Epoch: 81 [2560/50176]	Loss: 0.1186
Training Epoch: 81 [3072/50176]	Loss: 0.1366
Training Epoch: 81 [3584/50176]	Loss: 0.1740
Training Epoch: 81 [4096/50176]	Loss: 0.1662
Training Epoch: 81 [4608/50176]	Loss: 0.1850
Training Epoch: 81 [5120/50176]	Loss: 0.1850
Training Epoch: 81 [5632/50176]	Loss: 0.1547
Training Epoch: 81 [6144/50176]	Loss: 0.1398
Training Epoch: 81 [6656/50176]	Loss: 0.1411
Training Epoch: 81 [7168/50176]	Loss: 0.2039
Training Epoch: 81 [7680/50176]	Loss: 0.1724
Training Epoch: 81 [8192/50176]	Loss: 0.1844
Training Epoch: 81 [8704/50176]	Loss: 0.1490
Training Epoch: 81 [9216/50176]	Loss: 0.1366
Training Epoch: 81 [9728/50176]	Loss: 0.1650
Training Epoch: 81 [10240/50176]	Loss: 0.1362
Training Epoch: 81 [10752/50176]	Loss: 0.1455
Training Epoch: 81 [11264/50176]	Loss: 0.1467
Training Epoch: 81 [11776/50176]	Loss: 0.1648
Training Epoch: 81 [12288/50176]	Loss: 0.1197
Training Epoch: 81 [12800/50176]	Loss: 0.1447
Training Epoch: 81 [13312/50176]	Loss: 0.1325
Training Epoch: 81 [13824/50176]	Loss: 0.1647
Training Epoch: 81 [14336/50176]	Loss: 0.1615
Training Epoch: 81 [14848/50176]	Loss: 0.1594
Training Epoch: 81 [15360/50176]	Loss: 0.1322
Training Epoch: 81 [15872/50176]	Loss: 0.1348
Training Epoch: 81 [16384/50176]	Loss: 0.1660
Training Epoch: 81 [16896/50176]	Loss: 0.1574
Training Epoch: 81 [17408/50176]	Loss: 0.1252
Training Epoch: 81 [17920/50176]	Loss: 0.1531
Training Epoch: 81 [18432/50176]	Loss: 0.1679
Training Epoch: 81 [18944/50176]	Loss: 0.1390
Training Epoch: 81 [19456/50176]	Loss: 0.1656
Training Epoch: 81 [19968/50176]	Loss: 0.1253
Training Epoch: 81 [20480/50176]	Loss: 0.1866
Training Epoch: 81 [20992/50176]	Loss: 0.1466
Training Epoch: 81 [21504/50176]	Loss: 0.1436
Training Epoch: 81 [22016/50176]	Loss: 0.2120
Training Epoch: 81 [22528/50176]	Loss: 0.1487
Training Epoch: 81 [23040/50176]	Loss: 0.1744
Training Epoch: 81 [23552/50176]	Loss: 0.1082
Training Epoch: 81 [24064/50176]	Loss: 0.1443
Training Epoch: 81 [24576/50176]	Loss: 0.1815
Training Epoch: 81 [25088/50176]	Loss: 0.1576
Training Epoch: 81 [25600/50176]	Loss: 0.1758
Training Epoch: 81 [26112/50176]	Loss: 0.1583
Training Epoch: 81 [26624/50176]	Loss: 0.1412
Training Epoch: 81 [27136/50176]	Loss: 0.1821
Training Epoch: 81 [27648/50176]	Loss: 0.1601
Training Epoch: 81 [28160/50176]	Loss: 0.1883
Training Epoch: 81 [28672/50176]	Loss: 0.1626
Training Epoch: 81 [29184/50176]	Loss: 0.1285
Training Epoch: 81 [29696/50176]	Loss: 0.1727
Training Epoch: 81 [30208/50176]	Loss: 0.1050
Training Epoch: 81 [30720/50176]	Loss: 0.1876
Training Epoch: 81 [31232/50176]	Loss: 0.1749
Training Epoch: 81 [31744/50176]	Loss: 0.1463
Training Epoch: 81 [32256/50176]	Loss: 0.1690
Training Epoch: 81 [32768/50176]	Loss: 0.1586
Training Epoch: 81 [33280/50176]	Loss: 0.1649
Training Epoch: 81 [33792/50176]	Loss: 0.1678
Training Epoch: 81 [34304/50176]	Loss: 0.1292
Training Epoch: 81 [34816/50176]	Loss: 0.1722
Training Epoch: 81 [35328/50176]	Loss: 0.1981
Training Epoch: 81 [35840/50176]	Loss: 0.1242
Training Epoch: 81 [36352/50176]	Loss: 0.2310
Training Epoch: 81 [36864/50176]	Loss: 0.1455
Training Epoch: 81 [37376/50176]	Loss: 0.1385
Training Epoch: 81 [37888/50176]	Loss: 0.1390
Training Epoch: 81 [38400/50176]	Loss: 0.1740
Training Epoch: 81 [38912/50176]	Loss: 0.1907
Training Epoch: 81 [39424/50176]	Loss: 0.1347
Training Epoch: 81 [39936/50176]	Loss: 0.1655
Training Epoch: 81 [40448/50176]	Loss: 0.1165
Training Epoch: 81 [40960/50176]	Loss: 0.1593
Training Epoch: 81 [41472/50176]	Loss: 0.1646
Training Epoch: 81 [41984/50176]	Loss: 0.1123
Training Epoch: 81 [42496/50176]	Loss: 0.1616
Training Epoch: 81 [43008/50176]	Loss: 0.1373
Training Epoch: 81 [43520/50176]	Loss: 0.1878
Training Epoch: 81 [44032/50176]	Loss: 0.1795
Training Epoch: 81 [44544/50176]	Loss: 0.1577
Training Epoch: 81 [45056/50176]	Loss: 0.1294
Training Epoch: 81 [45568/50176]	Loss: 0.1872
Training Epoch: 81 [46080/50176]	Loss: 0.1564
Training Epoch: 81 [46592/50176]	Loss: 0.2013
Training Epoch: 81 [47104/50176]	Loss: 0.1755
Training Epoch: 81 [47616/50176]	Loss: 0.1520
Training Epoch: 81 [48128/50176]	Loss: 0.1327
Training Epoch: 81 [48640/50176]	Loss: 0.1879
Training Epoch: 81 [49152/50176]	Loss: 0.1442
Training Epoch: 81 [49664/50176]	Loss: 0.1635
Training Epoch: 81 [50176/50176]	Loss: 0.2047
Validation Epoch: 81, Average loss: 0.0061, Accuracy: 0.5811
Training Epoch: 82 [512/50176]	Loss: 0.1223
Training Epoch: 82 [1024/50176]	Loss: 0.1419
Training Epoch: 82 [1536/50176]	Loss: 0.1003
Training Epoch: 82 [2048/50176]	Loss: 0.1531
Training Epoch: 82 [2560/50176]	Loss: 0.1688
Training Epoch: 82 [3072/50176]	Loss: 0.1334
Training Epoch: 82 [3584/50176]	Loss: 0.1212
Training Epoch: 82 [4096/50176]	Loss: 0.1856
Training Epoch: 82 [4608/50176]	Loss: 0.1117
Training Epoch: 82 [5120/50176]	Loss: 0.1798
Training Epoch: 82 [5632/50176]	Loss: 0.1157
Training Epoch: 82 [6144/50176]	Loss: 0.0936
Training Epoch: 82 [6656/50176]	Loss: 0.1056
Training Epoch: 82 [7168/50176]	Loss: 0.1533
Training Epoch: 82 [7680/50176]	Loss: 0.1378
Training Epoch: 82 [8192/50176]	Loss: 0.1539
Training Epoch: 82 [8704/50176]	Loss: 0.1226
Training Epoch: 82 [9216/50176]	Loss: 0.2412
Training Epoch: 82 [9728/50176]	Loss: 0.1628
Training Epoch: 82 [10240/50176]	Loss: 0.1415
Training Epoch: 82 [10752/50176]	Loss: 0.1229
Training Epoch: 82 [11264/50176]	Loss: 0.1528
Training Epoch: 82 [11776/50176]	Loss: 0.1341
Training Epoch: 82 [12288/50176]	Loss: 0.1323
Training Epoch: 82 [12800/50176]	Loss: 0.1396
Training Epoch: 82 [13312/50176]	Loss: 0.2173
Training Epoch: 82 [13824/50176]	Loss: 0.1088
Training Epoch: 82 [14336/50176]	Loss: 0.1426
Training Epoch: 82 [14848/50176]	Loss: 0.1075
Training Epoch: 82 [15360/50176]	Loss: 0.1392
Training Epoch: 82 [15872/50176]	Loss: 0.1582
Training Epoch: 82 [16384/50176]	Loss: 0.1553
Training Epoch: 82 [16896/50176]	Loss: 0.1550
Training Epoch: 82 [17408/50176]	Loss: 0.1483
Training Epoch: 82 [17920/50176]	Loss: 0.1347
Training Epoch: 82 [18432/50176]	Loss: 0.1404
Training Epoch: 82 [18944/50176]	Loss: 0.1378
Training Epoch: 82 [19456/50176]	Loss: 0.1224
Training Epoch: 82 [19968/50176]	Loss: 0.1837
Training Epoch: 82 [20480/50176]	Loss: 0.1363
Training Epoch: 82 [20992/50176]	Loss: 0.1485
Training Epoch: 82 [21504/50176]	Loss: 0.1307
Training Epoch: 82 [22016/50176]	Loss: 0.1436
Training Epoch: 82 [22528/50176]	Loss: 0.1603
Training Epoch: 82 [23040/50176]	Loss: 0.1650
Training Epoch: 82 [23552/50176]	Loss: 0.1470
Training Epoch: 82 [24064/50176]	Loss: 0.1584
Training Epoch: 82 [24576/50176]	Loss: 0.1722
Training Epoch: 82 [25088/50176]	Loss: 0.2038
Training Epoch: 82 [25600/50176]	Loss: 0.1422
Training Epoch: 82 [26112/50176]	Loss: 0.2401
Training Epoch: 82 [26624/50176]	Loss: 0.1381
Training Epoch: 82 [27136/50176]	Loss: 0.1363
Training Epoch: 82 [27648/50176]	Loss: 0.1485
Training Epoch: 82 [28160/50176]	Loss: 0.1770
Training Epoch: 82 [28672/50176]	Loss: 0.1216
Training Epoch: 82 [29184/50176]	Loss: 0.1797
Training Epoch: 82 [29696/50176]	Loss: 0.1957
Training Epoch: 82 [30208/50176]	Loss: 0.1037
Training Epoch: 82 [30720/50176]	Loss: 0.1270
Training Epoch: 82 [31232/50176]	Loss: 0.1605
Training Epoch: 82 [31744/50176]	Loss: 0.1407
Training Epoch: 82 [32256/50176]	Loss: 0.1945
Training Epoch: 82 [32768/50176]	Loss: 0.1092
Training Epoch: 82 [33280/50176]	Loss: 0.1569
Training Epoch: 82 [33792/50176]	Loss: 0.1137
Training Epoch: 82 [34304/50176]	Loss: 0.1856
Training Epoch: 82 [34816/50176]	Loss: 0.1771
Training Epoch: 82 [35328/50176]	Loss: 0.1037
Training Epoch: 82 [35840/50176]	Loss: 0.1859
Training Epoch: 82 [36352/50176]	Loss: 0.1544
Training Epoch: 82 [36864/50176]	Loss: 0.2267
Training Epoch: 82 [37376/50176]	Loss: 0.1450
Training Epoch: 82 [37888/50176]	Loss: 0.1839
Training Epoch: 82 [38400/50176]	Loss: 0.1800
Training Epoch: 82 [38912/50176]	Loss: 0.2432
Training Epoch: 82 [39424/50176]	Loss: 0.1255
Training Epoch: 82 [39936/50176]	Loss: 0.1272
Training Epoch: 82 [40448/50176]	Loss: 0.1330
Training Epoch: 82 [40960/50176]	Loss: 0.1554
Training Epoch: 82 [41472/50176]	Loss: 0.1667
Training Epoch: 82 [41984/50176]	Loss: 0.1510
Training Epoch: 82 [42496/50176]	Loss: 0.1837
Training Epoch: 82 [43008/50176]	Loss: 0.1200
Training Epoch: 82 [43520/50176]	Loss: 0.2081
Training Epoch: 82 [44032/50176]	Loss: 0.1402
Training Epoch: 82 [44544/50176]	Loss: 0.1348
Training Epoch: 82 [45056/50176]	Loss: 0.1385
Training Epoch: 82 [45568/50176]	Loss: 0.1538
Training Epoch: 82 [46080/50176]	Loss: 0.1925
Training Epoch: 82 [46592/50176]	Loss: 0.1188
Training Epoch: 82 [47104/50176]	Loss: 0.1343
Training Epoch: 82 [47616/50176]	Loss: 0.1490
Training Epoch: 82 [48128/50176]	Loss: 0.1421
Training Epoch: 82 [48640/50176]	Loss: 0.1537
Training Epoch: 82 [49152/50176]	Loss: 0.1513
Training Epoch: 82 [49664/50176]	Loss: 0.1770
Training Epoch: 82 [50176/50176]	Loss: 0.1863
Validation Epoch: 82, Average loss: 0.0057, Accuracy: 0.6006
[Training Loop] Target accuracy 0.6 reached!
[Training Loop] Training done
Stopped Zeus monitor 0.
