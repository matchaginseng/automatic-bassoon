Files already downloaded and verified
Files already downloaded and verified
Job(cifar100,shufflenetv2,adam,0.6,bs1024~100)
[Training Loop] Testing batch sizes: [128, 256, 512, 1024]
[Training Loop] Testing power limits: [175, 150, 125, 100]
[Training Loop] Testing learning rates: [0.001, 0.005, 0.01]
[Training Loop] Testing dropout rates: [0.0, 0.25, 0.5]
[Training Loop] Reprofiling at accuracy thresholds [0.5, 0.4, 0.3]
[Training Loop] Model's accuracy 0.0 surpasses threshold 0.0! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
Launching Zeus monitor 0...
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6330
Profiling... [256/50048]	Loss: 4.6702
Profiling... [384/50048]	Loss: 4.7363
Profiling... [512/50048]	Loss: 4.7920
Profiling... [640/50048]	Loss: 4.7589
Profiling... [768/50048]	Loss: 4.6696
Profiling... [896/50048]	Loss: 4.6461
Profiling... [1024/50048]	Loss: 4.7112
Profiling... [1152/50048]	Loss: 4.5984
Profiling... [1280/50048]	Loss: 4.6082
Profiling... [1408/50048]	Loss: 4.5664
Profiling... [1536/50048]	Loss: 4.6222
Profiling... [1664/50048]	Loss: 4.6756
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 126.42747934932417,
                        "time": 2.1643350000013015,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 27669.609040508214
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6402
Profiling... [256/50048]	Loss: 4.7019
Profiling... [384/50048]	Loss: 4.6599
Profiling... [512/50048]	Loss: 4.8030
Profiling... [640/50048]	Loss: 4.7827
Profiling... [768/50048]	Loss: 4.8077
Profiling... [896/50048]	Loss: 4.7217
Profiling... [1024/50048]	Loss: 4.7030
Profiling... [1152/50048]	Loss: 4.7011
Profiling... [1280/50048]	Loss: 4.6922
Profiling... [1408/50048]	Loss: 4.6771
Profiling... [1536/50048]	Loss: 4.7696
Profiling... [1664/50048]	Loss: 4.7399
Profile done
epoch 1 train time consumed: 3.11s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.75389013353987,
                        "time": 2.161243551003281,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26171.607677356555
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6087
Profiling... [256/50048]	Loss: 4.6517
Profiling... [384/50048]	Loss: 4.7252
Profiling... [512/50048]	Loss: 4.7836
Profiling... [640/50048]	Loss: 4.8498
Profiling... [768/50048]	Loss: 4.6467
Profiling... [896/50048]	Loss: 4.7609
Profiling... [1024/50048]	Loss: 4.5723
Profiling... [1152/50048]	Loss: 4.6691
Profiling... [1280/50048]	Loss: 4.6289
Profiling... [1408/50048]	Loss: 4.6364
Profiling... [1536/50048]	Loss: 4.6063
Profiling... [1664/50048]	Loss: 4.6801
Profile done
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.98258833738083,
                        "time": 2.220898608000425,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26720.784538070217
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6262
Profiling... [256/50048]	Loss: 4.6509
Profiling... [384/50048]	Loss: 4.7612
Profiling... [512/50048]	Loss: 4.6846
Profiling... [640/50048]	Loss: 4.8140
Profiling... [768/50048]	Loss: 4.7688
Profiling... [896/50048]	Loss: 4.6145
Profiling... [1024/50048]	Loss: 4.6881
Profiling... [1152/50048]	Loss: 4.7082
Profiling... [1280/50048]	Loss: 4.6843
Profiling... [1408/50048]	Loss: 4.6803
Profiling... [1536/50048]	Loss: 4.6385
Profiling... [1664/50048]	Loss: 4.5990
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.40659336576225,
                        "time": 2.575150778000534,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 30051.78997335869
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6358
Profiling... [256/50048]	Loss: 4.7356
Profiling... [384/50048]	Loss: 4.7264
Profiling... [512/50048]	Loss: 4.6863
Profiling... [640/50048]	Loss: 4.7383
Profiling... [768/50048]	Loss: 4.6240
Profiling... [896/50048]	Loss: 4.7825
Profiling... [1024/50048]	Loss: 4.6254
Profiling... [1152/50048]	Loss: 4.6663
Profiling... [1280/50048]	Loss: 4.6403
Profiling... [1408/50048]	Loss: 4.7824
Profiling... [1536/50048]	Loss: 4.6111
Profiling... [1664/50048]	Loss: 4.6981
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.36992899178925,
                        "time": 2.169152860005852,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24647.75254431275
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6010
Profiling... [256/50048]	Loss: 4.6896
Profiling... [384/50048]	Loss: 4.6786
Profiling... [512/50048]	Loss: 4.7289
Profiling... [640/50048]	Loss: 4.7943
Profiling... [768/50048]	Loss: 4.7597
Profiling... [896/50048]	Loss: 4.7628
Profiling... [1024/50048]	Loss: 4.6979
Profiling... [1152/50048]	Loss: 4.7015
Profiling... [1280/50048]	Loss: 4.7049
Profiling... [1408/50048]	Loss: 4.6580
Profiling... [1536/50048]	Loss: 4.6233
Profiling... [1664/50048]	Loss: 4.6146
Profile done
epoch 1 train time consumed: 3.20s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.8095970240819,
                        "time": 2.1748030469971127,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25028.56091194043
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6309
Profiling... [256/50048]	Loss: 4.6792
Profiling... [384/50048]	Loss: 4.6747
Profiling... [512/50048]	Loss: 4.7275
Profiling... [640/50048]	Loss: 4.7294
Profiling... [768/50048]	Loss: 4.8078
Profiling... [896/50048]	Loss: 4.8086
Profiling... [1024/50048]	Loss: 4.7512
Profiling... [1152/50048]	Loss: 4.7406
Profiling... [1280/50048]	Loss: 4.6803
Profiling... [1408/50048]	Loss: 4.6782
Profiling... [1536/50048]	Loss: 4.5734
Profiling... [1664/50048]	Loss: 4.6530
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.5128755005248,
                        "time": 2.220227004996559,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25709.211788646204
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.5870
Profiling... [256/50048]	Loss: 4.6807
Profiling... [384/50048]	Loss: 4.6799
Profiling... [512/50048]	Loss: 4.7014
Profiling... [640/50048]	Loss: 4.8494
Profiling... [768/50048]	Loss: 4.7448
Profiling... [896/50048]	Loss: 4.7004
Profiling... [1024/50048]	Loss: 4.7871
Profiling... [1152/50048]	Loss: 4.6780
Profiling... [1280/50048]	Loss: 4.6381
Profiling... [1408/50048]	Loss: 4.6716
Profiling... [1536/50048]	Loss: 4.6616
Profiling... [1664/50048]	Loss: 4.6949
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.5993468052461,
                        "time": 2.5877644089996465,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 29726.079202956364
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6711
Profiling... [256/50048]	Loss: 4.6648
Profiling... [384/50048]	Loss: 4.6783
Profiling... [512/50048]	Loss: 4.7038
Profiling... [640/50048]	Loss: 4.8243
Profiling... [768/50048]	Loss: 4.6646
Profiling... [896/50048]	Loss: 4.6598
Profiling... [1024/50048]	Loss: 4.6993
Profiling... [1152/50048]	Loss: 4.6923
Profiling... [1280/50048]	Loss: 4.7154
Profiling... [1408/50048]	Loss: 4.6647
Profiling... [1536/50048]	Loss: 4.6552
Profiling... [1664/50048]	Loss: 4.8234
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.13058574100215,
                        "time": 2.1771177890041145,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24685.565362875855
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6645
Profiling... [256/50048]	Loss: 4.6716
Profiling... [384/50048]	Loss: 4.7079
Profiling... [512/50048]	Loss: 4.7133
Profiling... [640/50048]	Loss: 4.8449
Profiling... [768/50048]	Loss: 4.7388
Profiling... [896/50048]	Loss: 4.7171
Profiling... [1024/50048]	Loss: 4.6463
Profiling... [1152/50048]	Loss: 4.8120
Profiling... [1280/50048]	Loss: 4.7184
Profiling... [1408/50048]	Loss: 4.6008
Profiling... [1536/50048]	Loss: 4.6513
Profiling... [1664/50048]	Loss: 4.6529
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.90784050210195,
                        "time": 2.1768193940006313,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24853.27126814302
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6366
Profiling... [256/50048]	Loss: 4.5902
Profiling... [384/50048]	Loss: 4.7404
Profiling... [512/50048]	Loss: 4.7842
Profiling... [640/50048]	Loss: 4.7526
Profiling... [768/50048]	Loss: 4.7063
Profiling... [896/50048]	Loss: 4.7117
Profiling... [1024/50048]	Loss: 4.7548
Profiling... [1152/50048]	Loss: 4.5950
Profiling... [1280/50048]	Loss: 4.6350
Profiling... [1408/50048]	Loss: 4.6237
Profiling... [1536/50048]	Loss: 4.6465
Profiling... [1664/50048]	Loss: 4.5845
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.4407539834631,
                        "time": 2.212114513000415,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25375.451036095666
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6631
Profiling... [256/50048]	Loss: 4.6798
Profiling... [384/50048]	Loss: 4.7390
Profiling... [512/50048]	Loss: 4.6870
Profiling... [640/50048]	Loss: 4.7051
Profiling... [768/50048]	Loss: 4.5860
Profiling... [896/50048]	Loss: 4.7486
Profiling... [1024/50048]	Loss: 4.6671
Profiling... [1152/50048]	Loss: 4.6809
Profiling... [1280/50048]	Loss: 4.6672
Profiling... [1408/50048]	Loss: 4.7439
Profiling... [1536/50048]	Loss: 4.7324
Profiling... [1664/50048]	Loss: 4.6331
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.9730927323927,
                        "time": 2.5802105919938185,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 29475.910742777778
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6161
Profiling... [256/50048]	Loss: 5.8867
Profiling... [384/50048]	Loss: 5.8297
Profiling... [512/50048]	Loss: 5.5117
Profiling... [640/50048]	Loss: 5.4395
Profiling... [768/50048]	Loss: 5.2887
Profiling... [896/50048]	Loss: 5.0928
Profiling... [1024/50048]	Loss: 4.9629
Profiling... [1152/50048]	Loss: 4.7109
Profiling... [1280/50048]	Loss: 4.9179
Profiling... [1408/50048]	Loss: 5.0266
Profiling... [1536/50048]	Loss: 4.8584
Profiling... [1664/50048]	Loss: 4.6707
Profile done
epoch 1 train time consumed: 3.18s
Validation Epoch: 0, Average loss: 0.0368, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.073270927258,
                        "time": 2.1700102780014277,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24592.3991479164
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.5715
Profiling... [256/50048]	Loss: 5.8230
Profiling... [384/50048]	Loss: 5.5348
Profiling... [512/50048]	Loss: 5.2533
Profiling... [640/50048]	Loss: 5.1837
Profiling... [768/50048]	Loss: 4.9951
Profiling... [896/50048]	Loss: 5.0210
Profiling... [1024/50048]	Loss: 4.8961
Profiling... [1152/50048]	Loss: 4.6946
Profiling... [1280/50048]	Loss: 5.0565
Profiling... [1408/50048]	Loss: 4.7515
Profiling... [1536/50048]	Loss: 4.7573
Profiling... [1664/50048]	Loss: 4.8452
Profile done
epoch 1 train time consumed: 3.08s
Validation Epoch: 0, Average loss: 0.0371, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.66250559525966,
                        "time": 2.173895384003117,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24765.956567343444
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6482
Profiling... [256/50048]	Loss: 5.6556
Profiling... [384/50048]	Loss: 6.1044
Profiling... [512/50048]	Loss: 5.8589
Profiling... [640/50048]	Loss: 5.1135
Profiling... [768/50048]	Loss: 4.9503
Profiling... [896/50048]	Loss: 4.9538
Profiling... [1024/50048]	Loss: 4.7748
Profiling... [1152/50048]	Loss: 4.8773
Profiling... [1280/50048]	Loss: 4.7375
Profiling... [1408/50048]	Loss: 4.6769
Profiling... [1536/50048]	Loss: 4.6301
Profiling... [1664/50048]	Loss: 4.6856
Profile done
epoch 1 train time consumed: 3.18s
Validation Epoch: 0, Average loss: 0.0372, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.1059972846543,
                        "time": 2.2260389489965746,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25459.827290033354
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6377
Profiling... [256/50048]	Loss: 5.6418
Profiling... [384/50048]	Loss: 6.1441
Profiling... [512/50048]	Loss: 5.4799
Profiling... [640/50048]	Loss: 5.3516
Profiling... [768/50048]	Loss: 5.0984
Profiling... [896/50048]	Loss: 4.6905
Profiling... [1024/50048]	Loss: 4.6811
Profiling... [1152/50048]	Loss: 4.8418
Profiling... [1280/50048]	Loss: 4.7023
Profiling... [1408/50048]	Loss: 4.6385
Profiling... [1536/50048]	Loss: 4.6797
Profiling... [1664/50048]	Loss: 4.7133
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0368, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.74054640988612,
                        "time": 2.5651700290036388,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 29243.769581601995
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6509
Profiling... [256/50048]	Loss: 5.8487
Profiling... [384/50048]	Loss: 6.0471
Profiling... [512/50048]	Loss: 5.6172
Profiling... [640/50048]	Loss: 5.1334
Profiling... [768/50048]	Loss: 4.9658
Profiling... [896/50048]	Loss: 4.7835
Profiling... [1024/50048]	Loss: 4.7881
Profiling... [1152/50048]	Loss: 5.0823
Profiling... [1280/50048]	Loss: 4.6018
Profiling... [1408/50048]	Loss: 4.9344
Profiling... [1536/50048]	Loss: 4.7592
Profiling... [1664/50048]	Loss: 4.7928
Profile done
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.0374, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.07197657783722,
                        "time": 2.1828658290032763,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24737.80346468953
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6377
Profiling... [256/50048]	Loss: 5.6043
Profiling... [384/50048]	Loss: 5.9723
Profiling... [512/50048]	Loss: 5.6012
Profiling... [640/50048]	Loss: 5.2289
Profiling... [768/50048]	Loss: 5.0194
Profiling... [896/50048]	Loss: 4.9567
Profiling... [1024/50048]	Loss: 4.8350
Profiling... [1152/50048]	Loss: 4.8775
Profiling... [1280/50048]	Loss: 4.8147
Profiling... [1408/50048]	Loss: 4.5752
Profiling... [1536/50048]	Loss: 4.7342
Profiling... [1664/50048]	Loss: 4.5292
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0376, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.51462452330364,
                        "time": 2.189836916004424,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24914.823156926785
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6327
Profiling... [256/50048]	Loss: 5.7817
Profiling... [384/50048]	Loss: 5.9480
Profiling... [512/50048]	Loss: 5.4457
Profiling... [640/50048]	Loss: 5.0765
Profiling... [768/50048]	Loss: 5.4174
Profiling... [896/50048]	Loss: 4.8889
Profiling... [1024/50048]	Loss: 4.8518
Profiling... [1152/50048]	Loss: 4.8803
Profiling... [1280/50048]	Loss: 4.7420
Profiling... [1408/50048]	Loss: 4.8253
Profiling... [1536/50048]	Loss: 4.7482
Profiling... [1664/50048]	Loss: 4.6622
Profile done
epoch 1 train time consumed: 3.20s
Validation Epoch: 0, Average loss: 0.0380, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.88283085621289,
                        "time": 2.220173445995897,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25342.64015678592
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.5916
Profiling... [256/50048]	Loss: 5.6676
Profiling... [384/50048]	Loss: 6.1382
Profiling... [512/50048]	Loss: 5.4152
Profiling... [640/50048]	Loss: 5.2424
Profiling... [768/50048]	Loss: 4.9208
Profiling... [896/50048]	Loss: 4.9945
Profiling... [1024/50048]	Loss: 4.9858
Profiling... [1152/50048]	Loss: 4.7047
Profiling... [1280/50048]	Loss: 4.9340
Profiling... [1408/50048]	Loss: 4.8242
Profiling... [1536/50048]	Loss: 4.6780
Profiling... [1664/50048]	Loss: 4.7489
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0371, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.61338903394937,
                        "time": 2.572510443998908,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 29294.374956149502
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6299
Profiling... [256/50048]	Loss: 5.5193
Profiling... [384/50048]	Loss: 5.7470
Profiling... [512/50048]	Loss: 5.5562
Profiling... [640/50048]	Loss: 5.8962
Profiling... [768/50048]	Loss: 5.2246
Profiling... [896/50048]	Loss: 5.2790
Profiling... [1024/50048]	Loss: 4.9540
Profiling... [1152/50048]	Loss: 4.9143
Profiling... [1280/50048]	Loss: 4.8385
Profiling... [1408/50048]	Loss: 4.6098
Profiling... [1536/50048]	Loss: 4.7783
Profiling... [1664/50048]	Loss: 4.8752
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.0367, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.06663897957803,
                        "time": 2.1678242470006808,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24566.171396511065
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6576
Profiling... [256/50048]	Loss: 5.7396
Profiling... [384/50048]	Loss: 5.9261
Profiling... [512/50048]	Loss: 5.6162
Profiling... [640/50048]	Loss: 5.0281
Profiling... [768/50048]	Loss: 4.8580
Profiling... [896/50048]	Loss: 5.0352
Profiling... [1024/50048]	Loss: 4.9370
Profiling... [1152/50048]	Loss: 4.6463
Profiling... [1280/50048]	Loss: 4.9196
Profiling... [1408/50048]	Loss: 4.7166
Profiling... [1536/50048]	Loss: 4.5985
Profiling... [1664/50048]	Loss: 4.9082
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0367, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.465820673388,
                        "time": 2.1722928499948466,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24704.495553652047
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6804
Profiling... [256/50048]	Loss: 5.7503
Profiling... [384/50048]	Loss: 5.9100
Profiling... [512/50048]	Loss: 5.4654
Profiling... [640/50048]	Loss: 5.4732
Profiling... [768/50048]	Loss: 5.1585
Profiling... [896/50048]	Loss: 4.9070
Profiling... [1024/50048]	Loss: 4.8572
Profiling... [1152/50048]	Loss: 5.0341
Profiling... [1280/50048]	Loss: 4.6452
Profiling... [1408/50048]	Loss: 4.8439
Profiling... [1536/50048]	Loss: 4.8100
Profiling... [1664/50048]	Loss: 4.8318
Profile done
epoch 1 train time consumed: 3.18s
Validation Epoch: 0, Average loss: 0.0366, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.75265424647138,
                        "time": 2.221007970001665,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25322.929860992183
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6115
Profiling... [256/50048]	Loss: 5.7588
Profiling... [384/50048]	Loss: 6.0288
Profiling... [512/50048]	Loss: 5.2882
Profiling... [640/50048]	Loss: 5.2804
Profiling... [768/50048]	Loss: 5.2846
Profiling... [896/50048]	Loss: 4.9881
Profiling... [1024/50048]	Loss: 4.9148
Profiling... [1152/50048]	Loss: 4.9446
Profiling... [1280/50048]	Loss: 4.7615
Profiling... [1408/50048]	Loss: 4.7559
Profiling... [1536/50048]	Loss: 4.7961
Profiling... [1664/50048]	Loss: 4.7675
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 0, Average loss: 0.0371, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.52054694866607,
                        "time": 2.5772992740021436,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 29324.711414948575
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6586
Profiling... [256/50048]	Loss: 8.1302
Profiling... [384/50048]	Loss: 6.6758
Profiling... [512/50048]	Loss: 4.9971
Profiling... [640/50048]	Loss: 6.4089
Profiling... [768/50048]	Loss: 5.4088
Profiling... [896/50048]	Loss: 4.9486
Profiling... [1024/50048]	Loss: 4.9485
Profiling... [1152/50048]	Loss: 4.7732
Profiling... [1280/50048]	Loss: 4.6950
Profiling... [1408/50048]	Loss: 4.5839
Profiling... [1536/50048]	Loss: 4.8474
Profiling... [1664/50048]	Loss: 4.7573
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.0379, Accuracy: 0.0134
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.09635763322,
                        "time": 2.1704759709973587,
                        "accuracy": 0.013449367088607595,
                        "total_cost": 18090.25280344395
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6292
Profiling... [256/50048]	Loss: 7.6371
Profiling... [384/50048]	Loss: 7.4257
Profiling... [512/50048]	Loss: 5.8459
Profiling... [640/50048]	Loss: 5.4260
Profiling... [768/50048]	Loss: 5.0504
Profiling... [896/50048]	Loss: 5.2086
Profiling... [1024/50048]	Loss: 4.9655
Profiling... [1152/50048]	Loss: 4.6872
Profiling... [1280/50048]	Loss: 4.9227
Profiling... [1408/50048]	Loss: 4.8664
Profiling... [1536/50048]	Loss: 4.8401
Profiling... [1664/50048]	Loss: 4.7593
Profile done
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.44034999113275,
                        "time": 2.2019385530002182,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25035.971306376603
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.5927
Profiling... [256/50048]	Loss: 7.5358
Profiling... [384/50048]	Loss: 7.5105
Profiling... [512/50048]	Loss: 5.5920
Profiling... [640/50048]	Loss: 5.1508
Profiling... [768/50048]	Loss: 4.7786
Profiling... [896/50048]	Loss: 5.2310
Profiling... [1024/50048]	Loss: 5.3158
Profiling... [1152/50048]	Loss: 4.9688
Profiling... [1280/50048]	Loss: 4.9378
Profiling... [1408/50048]	Loss: 5.1054
Profiling... [1536/50048]	Loss: 4.8937
Profiling... [1664/50048]	Loss: 4.8677
Profile done
epoch 1 train time consumed: 3.17s
Validation Epoch: 0, Average loss: 0.0974, Accuracy: 0.0106
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.69060118267541,
                        "time": 2.2263320019992534,
                        "accuracy": 0.010581487341772151,
                        "total_cost": 23709.96473691453
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6457
Profiling... [256/50048]	Loss: 8.1189
Profiling... [384/50048]	Loss: 7.3844
Profiling... [512/50048]	Loss: 5.6885
Profiling... [640/50048]	Loss: 5.1182
Profiling... [768/50048]	Loss: 4.6002
Profiling... [896/50048]	Loss: 4.9098
Profiling... [1024/50048]	Loss: 5.0249
Profiling... [1152/50048]	Loss: 4.5558
Profiling... [1280/50048]	Loss: 4.6162
Profiling... [1408/50048]	Loss: 4.6567
Profiling... [1536/50048]	Loss: 4.6441
Profiling... [1664/50048]	Loss: 5.2121
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 112.48990068593228,
                        "time": 2.577098185000068,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 29314.437110010575
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6291
Profiling... [256/50048]	Loss: 7.9420
Profiling... [384/50048]	Loss: 7.5634
Profiling... [512/50048]	Loss: 6.5720
Profiling... [640/50048]	Loss: 5.8592
Profiling... [768/50048]	Loss: 5.0659
Profiling... [896/50048]	Loss: 4.9855
Profiling... [1024/50048]	Loss: 4.9152
Profiling... [1152/50048]	Loss: 4.7720
Profiling... [1280/50048]	Loss: 4.6665
Profiling... [1408/50048]	Loss: 5.0279
Profiling... [1536/50048]	Loss: 4.5920
Profiling... [1664/50048]	Loss: 4.8387
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.11809364356974,
                        "time": 2.1684305800008588,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24584.32502197914
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6261
Profiling... [256/50048]	Loss: 7.6633
Profiling... [384/50048]	Loss: 6.4777
Profiling... [512/50048]	Loss: 5.9995
Profiling... [640/50048]	Loss: 5.3966
Profiling... [768/50048]	Loss: 4.9180
Profiling... [896/50048]	Loss: 4.8436
Profiling... [1024/50048]	Loss: 5.2464
Profiling... [1152/50048]	Loss: 4.8390
Profiling... [1280/50048]	Loss: 5.4550
Profiling... [1408/50048]	Loss: 4.7086
Profiling... [1536/50048]	Loss: 5.0478
Profiling... [1664/50048]	Loss: 4.7641
Profile done
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.42155516198814,
                        "time": 2.1715035540037206,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24685.799321458842
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6370
Profiling... [256/50048]	Loss: 8.0547
Profiling... [384/50048]	Loss: 6.7236
Profiling... [512/50048]	Loss: 6.3075
Profiling... [640/50048]	Loss: 5.2869
Profiling... [768/50048]	Loss: 4.7179
Profiling... [896/50048]	Loss: 4.9336
Profiling... [1024/50048]	Loss: 5.0574
Profiling... [1152/50048]	Loss: 4.8573
Profiling... [1280/50048]	Loss: 4.6835
Profiling... [1408/50048]	Loss: 4.9089
Profiling... [1536/50048]	Loss: 5.2895
Profiling... [1664/50048]	Loss: 4.8341
Profile done
epoch 1 train time consumed: 3.27s
Validation Epoch: 0, Average loss: 12.6783, Accuracy: 0.0103
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.60422281782856,
                        "time": 2.240942406002432,
                        "accuracy": 0.010284810126582278,
                        "total_cost": 24535.171277029018
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.5985
Profiling... [256/50048]	Loss: 7.6750
Profiling... [384/50048]	Loss: 8.9865
Profiling... [512/50048]	Loss: 6.6475
Profiling... [640/50048]	Loss: 5.8669
Profiling... [768/50048]	Loss: 5.4516
Profiling... [896/50048]	Loss: 5.5511
Profiling... [1024/50048]	Loss: 4.8287
Profiling... [1152/50048]	Loss: 4.7288
Profiling... [1280/50048]	Loss: 4.8646
Profiling... [1408/50048]	Loss: 4.5664
Profiling... [1536/50048]	Loss: 4.8214
Profiling... [1664/50048]	Loss: 4.6330
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 112.45146400939265,
                        "time": 2.5939933600020595,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 29496.53724936593
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6319
Profiling... [256/50048]	Loss: 7.3964
Profiling... [384/50048]	Loss: 7.0208
Profiling... [512/50048]	Loss: 6.2221
Profiling... [640/50048]	Loss: 5.7550
Profiling... [768/50048]	Loss: 5.5530
Profiling... [896/50048]	Loss: 4.9423
Profiling... [1024/50048]	Loss: 4.8735
Profiling... [1152/50048]	Loss: 5.4577
Profiling... [1280/50048]	Loss: 4.8679
Profiling... [1408/50048]	Loss: 4.8069
Profiling... [1536/50048]	Loss: 4.7863
Profiling... [1664/50048]	Loss: 4.6937
Profile done
epoch 1 train time consumed: 3.12s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.11916378357336,
                        "time": 2.186052887002006,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 24784.35240001174
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6258
Profiling... [256/50048]	Loss: 7.6547
Profiling... [384/50048]	Loss: 6.2747
Profiling... [512/50048]	Loss: 5.8397
Profiling... [640/50048]	Loss: 5.2302
Profiling... [768/50048]	Loss: 4.9580
Profiling... [896/50048]	Loss: 5.1670
Profiling... [1024/50048]	Loss: 5.1844
Profiling... [1152/50048]	Loss: 5.2267
Profiling... [1280/50048]	Loss: 4.8492
Profiling... [1408/50048]	Loss: 5.0317
Profiling... [1536/50048]	Loss: 5.0030
Profiling... [1664/50048]	Loss: 5.0602
Profile done
epoch 1 train time consumed: 3.14s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0111
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.40910929353655,
                        "time": 2.178193972998997,
                        "accuracy": 0.011075949367088608,
                        "total_cost": 22106.352806281124
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6340
Profiling... [256/50048]	Loss: 8.0792
Profiling... [384/50048]	Loss: 7.5188
Profiling... [512/50048]	Loss: 6.4173
Profiling... [640/50048]	Loss: 6.1995
Profiling... [768/50048]	Loss: 5.2311
Profiling... [896/50048]	Loss: 4.7975
Profiling... [1024/50048]	Loss: 4.8254
Profiling... [1152/50048]	Loss: 4.8342
Profiling... [1280/50048]	Loss: 4.9134
Profiling... [1408/50048]	Loss: 4.6857
Profiling... [1536/50048]	Loss: 4.6584
Profiling... [1664/50048]	Loss: 4.8613
Profile done
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.59217998388333,
                        "time": 2.215513650000503,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25224.33461679481
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6651
Profiling... [256/50048]	Loss: 7.4183
Profiling... [384/50048]	Loss: 7.3810
Profiling... [512/50048]	Loss: 5.7456
Profiling... [640/50048]	Loss: 5.2455
Profiling... [768/50048]	Loss: 5.6305
Profiling... [896/50048]	Loss: 5.3309
Profiling... [1024/50048]	Loss: 5.1944
Profiling... [1152/50048]	Loss: 4.9506
Profiling... [1280/50048]	Loss: 4.7790
Profiling... [1408/50048]	Loss: 4.6662
Profiling... [1536/50048]	Loss: 4.5884
Profiling... [1664/50048]	Loss: 5.0588
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 112.48141558939174,
                        "time": 2.572213829997054,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 29256.670682800246
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6048
Profiling... [512/50176]	Loss: 4.6884
Profiling... [768/50176]	Loss: 4.6867
Profiling... [1024/50176]	Loss: 4.7396
Profiling... [1280/50176]	Loss: 4.6419
Profiling... [1536/50176]	Loss: 4.6732
Profiling... [1792/50176]	Loss: 4.5902
Profiling... [2048/50176]	Loss: 4.6660
Profiling... [2304/50176]	Loss: 4.5899
Profiling... [2560/50176]	Loss: 4.6014
Profiling... [2816/50176]	Loss: 4.5607
Profiling... [3072/50176]	Loss: 4.6119
Profiling... [3328/50176]	Loss: 4.5825
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.3317882237111,
                        "time": 2.380179637002584,
                        "accuracy": 0.009765625,
                        "total_cost": 27378.67109561998
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6468
Profiling... [512/50176]	Loss: 4.6063
Profiling... [768/50176]	Loss: 4.6231
Profiling... [1024/50176]	Loss: 4.7213
Profiling... [1280/50176]	Loss: 4.7501
Profiling... [1536/50176]	Loss: 4.6885
Profiling... [1792/50176]	Loss: 4.5854
Profiling... [2048/50176]	Loss: 4.6276
Profiling... [2304/50176]	Loss: 4.6407
Profiling... [2560/50176]	Loss: 4.5993
Profiling... [2816/50176]	Loss: 4.6708
Profiling... [3072/50176]	Loss: 4.6015
Profiling... [3328/50176]	Loss: 4.5834
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.71910162856852,
                        "time": 2.395339637005236,
                        "accuracy": 0.009765625,
                        "total_cost": 27648.05447460164
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6444
Profiling... [512/50176]	Loss: 4.6423
Profiling... [768/50176]	Loss: 4.6454
Profiling... [1024/50176]	Loss: 4.6460
Profiling... [1280/50176]	Loss: 4.6615
Profiling... [1536/50176]	Loss: 4.6855
Profiling... [1792/50176]	Loss: 4.6353
Profiling... [2048/50176]	Loss: 4.6223
Profiling... [2304/50176]	Loss: 4.5683
Profiling... [2560/50176]	Loss: 4.6121
Profiling... [2816/50176]	Loss: 4.5994
Profiling... [3072/50176]	Loss: 4.5897
Profiling... [3328/50176]	Loss: 4.6155
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.90502809254862,
                        "time": 2.5519773830019403,
                        "accuracy": 0.009765625,
                        "total_cost": 29504.622399424796
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6112
Profiling... [512/50176]	Loss: 4.6584
Profiling... [768/50176]	Loss: 4.7502
Profiling... [1024/50176]	Loss: 4.7412
Profiling... [1280/50176]	Loss: 4.6278
Profiling... [1536/50176]	Loss: 4.6031
Profiling... [1792/50176]	Loss: 4.5973
Profiling... [2048/50176]	Loss: 4.6071
Profiling... [2304/50176]	Loss: 4.7436
Profiling... [2560/50176]	Loss: 4.5736
Profiling... [2816/50176]	Loss: 4.5258
Profiling... [3072/50176]	Loss: 4.5812
Profiling... [3328/50176]	Loss: 4.6088
Profile done
epoch 1 train time consumed: 4.30s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 112.69587498459998,
                        "time": 3.036501059003058,
                        "accuracy": 0.009765625,
                        "total_cost": 35041.39711856784
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6411
Profiling... [512/50176]	Loss: 4.6678
Profiling... [768/50176]	Loss: 4.7528
Profiling... [1024/50176]	Loss: 4.6760
Profiling... [1280/50176]	Loss: 4.6594
Profiling... [1536/50176]	Loss: 4.6178
Profiling... [1792/50176]	Loss: 4.6437
Profiling... [2048/50176]	Loss: 4.5578
Profiling... [2304/50176]	Loss: 4.6252
Profiling... [2560/50176]	Loss: 4.6125
Profiling... [2816/50176]	Loss: 4.6209
Profiling... [3072/50176]	Loss: 4.6503
Profiling... [3328/50176]	Loss: 4.5196
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.54404310170949,
                        "time": 2.3954380020004464,
                        "accuracy": 0.009765625,
                        "total_cost": 27606.24924104818
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6375
Profiling... [512/50176]	Loss: 4.6211
Profiling... [768/50176]	Loss: 4.6903
Profiling... [1024/50176]	Loss: 4.6905
Profiling... [1280/50176]	Loss: 4.6807
Profiling... [1536/50176]	Loss: 4.6202
Profiling... [1792/50176]	Loss: 4.6812
Profiling... [2048/50176]	Loss: 4.6046
Profiling... [2304/50176]	Loss: 4.5635
Profiling... [2560/50176]	Loss: 4.5321
Profiling... [2816/50176]	Loss: 4.5302
Profiling... [3072/50176]	Loss: 4.5468
Profiling... [3328/50176]	Loss: 4.5520
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.82950940307121,
                        "time": 2.3921074960016995,
                        "accuracy": 0.009765625,
                        "total_cost": 27637.79227783996
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6257
Profiling... [512/50176]	Loss: 4.6723
Profiling... [768/50176]	Loss: 4.6586
Profiling... [1024/50176]	Loss: 4.6798
Profiling... [1280/50176]	Loss: 4.6678
Profiling... [1536/50176]	Loss: 4.6809
Profiling... [1792/50176]	Loss: 4.6609
Profiling... [2048/50176]	Loss: 4.6179
Profiling... [2304/50176]	Loss: 4.6126
Profiling... [2560/50176]	Loss: 4.5793
Profiling... [2816/50176]	Loss: 4.6251
Profiling... [3072/50176]	Loss: 4.6188
Profiling... [3328/50176]	Loss: 4.5937
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.02742316046883,
                        "time": 2.5412050779996207,
                        "accuracy": 0.009765625,
                        "total_cost": 29411.92823691216
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6461
Profiling... [512/50176]	Loss: 4.6353
Profiling... [768/50176]	Loss: 4.7045
Profiling... [1024/50176]	Loss: 4.6483
Profiling... [1280/50176]	Loss: 4.6753
Profiling... [1536/50176]	Loss: 4.6666
Profiling... [1792/50176]	Loss: 4.6559
Profiling... [2048/50176]	Loss: 4.6010
Profiling... [2304/50176]	Loss: 4.6327
Profiling... [2560/50176]	Loss: 4.5493
Profiling... [2816/50176]	Loss: 4.5908
Profiling... [3072/50176]	Loss: 4.5580
Profiling... [3328/50176]	Loss: 4.4992
Profile done
epoch 1 train time consumed: 4.25s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 112.85291270253434,
                        "time": 3.0293705110016163,
                        "accuracy": 0.009765625,
                        "total_cost": 35007.8244681418
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6329
Profiling... [512/50176]	Loss: 4.6843
Profiling... [768/50176]	Loss: 4.7030
Profiling... [1024/50176]	Loss: 4.6526
Profiling... [1280/50176]	Loss: 4.6708
Profiling... [1536/50176]	Loss: 4.6329
Profiling... [1792/50176]	Loss: 4.6270
Profiling... [2048/50176]	Loss: 4.6071
Profiling... [2304/50176]	Loss: 4.6404
Profiling... [2560/50176]	Loss: 4.5897
Profiling... [2816/50176]	Loss: 4.5520
Profiling... [3072/50176]	Loss: 4.6377
Profiling... [3328/50176]	Loss: 4.5500
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.68147931879685,
                        "time": 2.38878120300069,
                        "accuracy": 0.009765625,
                        "total_cost": 27563.151331640616
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6391
Profiling... [512/50176]	Loss: 4.6157
Profiling... [768/50176]	Loss: 4.6965
Profiling... [1024/50176]	Loss: 4.6625
Profiling... [1280/50176]	Loss: 4.6368
Profiling... [1536/50176]	Loss: 4.6697
Profiling... [1792/50176]	Loss: 4.6232
Profiling... [2048/50176]	Loss: 4.5983
Profiling... [2304/50176]	Loss: 4.6132
Profiling... [2560/50176]	Loss: 4.6165
Profiling... [2816/50176]	Loss: 4.5203
Profiling... [3072/50176]	Loss: 4.5204
Profiling... [3328/50176]	Loss: 4.5301
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.97290593817176,
                        "time": 2.3963530660039396,
                        "accuracy": 0.009765625,
                        "total_cost": 27722.03207888001
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6325
Profiling... [512/50176]	Loss: 4.6392
Profiling... [768/50176]	Loss: 4.7197
Profiling... [1024/50176]	Loss: 4.6810
Profiling... [1280/50176]	Loss: 4.6845
Profiling... [1536/50176]	Loss: 4.5697
Profiling... [1792/50176]	Loss: 4.6171
Profiling... [2048/50176]	Loss: 4.6061
Profiling... [2304/50176]	Loss: 4.6264
Profiling... [2560/50176]	Loss: 4.5458
Profiling... [2816/50176]	Loss: 4.5417
Profiling... [3072/50176]	Loss: 4.6184
Profiling... [3328/50176]	Loss: 4.5705
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.13524866156965,
                        "time": 2.539816376003728,
                        "accuracy": 0.009765625,
                        "total_cost": 29423.89834280022
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6607
Profiling... [512/50176]	Loss: 4.6255
Profiling... [768/50176]	Loss: 4.6347
Profiling... [1024/50176]	Loss: 4.7184
Profiling... [1280/50176]	Loss: 4.7265
Profiling... [1536/50176]	Loss: 4.6927
Profiling... [1792/50176]	Loss: 4.6693
Profiling... [2048/50176]	Loss: 4.6053
Profiling... [2304/50176]	Loss: 4.5993
Profiling... [2560/50176]	Loss: 4.5529
Profiling... [2816/50176]	Loss: 4.5422
Profiling... [3072/50176]	Loss: 4.6088
Profiling... [3328/50176]	Loss: 4.5978
Profile done
epoch 1 train time consumed: 4.32s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 112.96274388437584,
                        "time": 3.040737862997048,
                        "accuracy": 0.009765625,
                        "total_cost": 35173.38546557541
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6329
Profiling... [512/50176]	Loss: 5.6708
Profiling... [768/50176]	Loss: 5.3033
Profiling... [1024/50176]	Loss: 5.1200
Profiling... [1280/50176]	Loss: 4.9265
Profiling... [1536/50176]	Loss: 4.8809
Profiling... [1792/50176]	Loss: 4.8235
Profiling... [2048/50176]	Loss: 4.7523
Profiling... [2304/50176]	Loss: 4.8184
Profiling... [2560/50176]	Loss: 4.6992
Profiling... [2816/50176]	Loss: 4.5972
Profiling... [3072/50176]	Loss: 4.5761
Profiling... [3328/50176]	Loss: 4.6864
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 112.78909406592274,
                        "time": 2.3915725909973844,
                        "accuracy": 0.009765625,
                        "total_cost": 27621.71452738422
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6098
Profiling... [512/50176]	Loss: 5.4133
Profiling... [768/50176]	Loss: 5.5653
Profiling... [1024/50176]	Loss: 5.1580
Profiling... [1280/50176]	Loss: 5.0008
Profiling... [1536/50176]	Loss: 4.8572
Profiling... [1792/50176]	Loss: 4.8363
Profiling... [2048/50176]	Loss: 4.7701
Profiling... [2304/50176]	Loss: 4.6850
Profiling... [2560/50176]	Loss: 4.6801
Profiling... [2816/50176]	Loss: 4.5857
Profiling... [3072/50176]	Loss: 4.6310
Profiling... [3328/50176]	Loss: 4.6132
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 0, Average loss: 0.0187, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.06380957633739,
                        "time": 2.4163758990034694,
                        "accuracy": 0.009765625,
                        "total_cost": 27976.157645801402
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6522
Profiling... [512/50176]	Loss: 5.5923
Profiling... [768/50176]	Loss: 5.6202
Profiling... [1024/50176]	Loss: 5.1861
Profiling... [1280/50176]	Loss: 4.7011
Profiling... [1536/50176]	Loss: 4.8840
Profiling... [1792/50176]	Loss: 4.9562
Profiling... [2048/50176]	Loss: 4.6254
Profiling... [2304/50176]	Loss: 4.8817
Profiling... [2560/50176]	Loss: 4.6244
Profiling... [2816/50176]	Loss: 4.6542
Profiling... [3072/50176]	Loss: 4.7721
Profiling... [3328/50176]	Loss: 4.6178
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.18853653235571,
                        "time": 2.5648922160035,
                        "accuracy": 0.009765625,
                        "total_cost": 29728.398980369093
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6378
Profiling... [512/50176]	Loss: 5.5729
Profiling... [768/50176]	Loss: 5.4230
Profiling... [1024/50176]	Loss: 5.2350
Profiling... [1280/50176]	Loss: 4.9104
Profiling... [1536/50176]	Loss: 4.8406
Profiling... [1792/50176]	Loss: 4.8856
Profiling... [2048/50176]	Loss: 4.8227
Profiling... [2304/50176]	Loss: 4.6737
Profiling... [2560/50176]	Loss: 4.6457
Profiling... [2816/50176]	Loss: 4.7861
Profiling... [3072/50176]	Loss: 4.5575
Profiling... [3328/50176]	Loss: 4.6502
Profile done
epoch 1 train time consumed: 4.24s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.04003921431115,
                        "time": 3.059172725996177,
                        "accuracy": 0.009765625,
                        "total_cost": 35410.8421027798
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6483
Profiling... [512/50176]	Loss: 5.7401
Profiling... [768/50176]	Loss: 5.4180
Profiling... [1024/50176]	Loss: 5.1444
Profiling... [1280/50176]	Loss: 4.9928
Profiling... [1536/50176]	Loss: 4.9616
Profiling... [1792/50176]	Loss: 4.8504
Profiling... [2048/50176]	Loss: 4.7116
Profiling... [2304/50176]	Loss: 4.6804
Profiling... [2560/50176]	Loss: 4.5876
Profiling... [2816/50176]	Loss: 4.5714
Profiling... [3072/50176]	Loss: 4.6127
Profiling... [3328/50176]	Loss: 4.5899
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 0, Average loss: 0.0189, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 112.87343319435014,
                        "time": 2.407151626997802,
                        "accuracy": 0.009765625,
                        "total_cost": 27822.435159921424
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6335
Profiling... [512/50176]	Loss: 5.6408
Profiling... [768/50176]	Loss: 5.2562
Profiling... [1024/50176]	Loss: 5.0559
Profiling... [1280/50176]	Loss: 5.0979
Profiling... [1536/50176]	Loss: 4.8554
Profiling... [1792/50176]	Loss: 4.6675
Profiling... [2048/50176]	Loss: 4.7112
Profiling... [2304/50176]	Loss: 4.6750
Profiling... [2560/50176]	Loss: 4.7017
Profiling... [2816/50176]	Loss: 4.6988
Profiling... [3072/50176]	Loss: 4.6285
Profiling... [3328/50176]	Loss: 4.5588
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.09959789308198,
                        "time": 2.488000536999607,
                        "accuracy": 0.009765625,
                        "total_cost": 28814.526493944584
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6212
Profiling... [512/50176]	Loss: 5.7218
Profiling... [768/50176]	Loss: 5.6261
Profiling... [1024/50176]	Loss: 4.9652
Profiling... [1280/50176]	Loss: 4.9968
Profiling... [1536/50176]	Loss: 5.0898
Profiling... [1792/50176]	Loss: 5.0471
Profiling... [2048/50176]	Loss: 4.8880
Profiling... [2304/50176]	Loss: 4.6635
Profiling... [2560/50176]	Loss: 4.7362
Profiling... [2816/50176]	Loss: 4.8155
Profiling... [3072/50176]	Loss: 4.6292
Profiling... [3328/50176]	Loss: 4.6988
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.22711054452368,
                        "time": 2.5440026099968236,
                        "accuracy": 0.009765625,
                        "total_cost": 29496.326630161115
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6427
Profiling... [512/50176]	Loss: 5.5886
Profiling... [768/50176]	Loss: 5.3799
Profiling... [1024/50176]	Loss: 5.1483
Profiling... [1280/50176]	Loss: 4.7077
Profiling... [1536/50176]	Loss: 4.6245
Profiling... [1792/50176]	Loss: 4.6375
Profiling... [2048/50176]	Loss: 5.0577
Profiling... [2304/50176]	Loss: 4.9615
Profiling... [2560/50176]	Loss: 4.8437
Profiling... [2816/50176]	Loss: 4.9414
Profiling... [3072/50176]	Loss: 5.0607
Profiling... [3328/50176]	Loss: 4.6475
Profile done
epoch 1 train time consumed: 4.30s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.07437269654001,
                        "time": 3.033453058997111,
                        "accuracy": 0.009765625,
                        "total_cost": 35123.79409925106
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6370
Profiling... [512/50176]	Loss: 5.6121
Profiling... [768/50176]	Loss: 5.2694
Profiling... [1024/50176]	Loss: 5.1172
Profiling... [1280/50176]	Loss: 5.0198
Profiling... [1536/50176]	Loss: 4.9297
Profiling... [1792/50176]	Loss: 4.6948
Profiling... [2048/50176]	Loss: 4.7248
Profiling... [2304/50176]	Loss: 4.7056
Profiling... [2560/50176]	Loss: 4.7285
Profiling... [2816/50176]	Loss: 4.6835
Profiling... [3072/50176]	Loss: 4.5973
Profiling... [3328/50176]	Loss: 4.6091
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 0, Average loss: 0.0189, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 112.91953572933693,
                        "time": 2.496151341001678,
                        "accuracy": 0.009765625,
                        "total_cost": 28862.8992548937
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6228
Profiling... [512/50176]	Loss: 5.7566
Profiling... [768/50176]	Loss: 5.3896
Profiling... [1024/50176]	Loss: 4.9219
Profiling... [1280/50176]	Loss: 4.7624
Profiling... [1536/50176]	Loss: 4.6818
Profiling... [1792/50176]	Loss: 4.8503
Profiling... [2048/50176]	Loss: 4.6350
Profiling... [2304/50176]	Loss: 4.7678
Profiling... [2560/50176]	Loss: 4.7253
Profiling... [2816/50176]	Loss: 4.7891
Profiling... [3072/50176]	Loss: 4.5810
Profiling... [3328/50176]	Loss: 4.7079
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.1271767530561,
                        "time": 2.4032022440005676,
                        "accuracy": 0.009765625,
                        "total_cost": 27839.23046711227
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6267
Profiling... [512/50176]	Loss: 5.6678
Profiling... [768/50176]	Loss: 5.2973
Profiling... [1024/50176]	Loss: 5.1260
Profiling... [1280/50176]	Loss: 4.8941
Profiling... [1536/50176]	Loss: 4.9273
Profiling... [1792/50176]	Loss: 4.9765
Profiling... [2048/50176]	Loss: 4.7887
Profiling... [2304/50176]	Loss: 4.6992
Profiling... [2560/50176]	Loss: 4.8295
Profiling... [2816/50176]	Loss: 4.6750
Profiling... [3072/50176]	Loss: 4.6438
Profiling... [3328/50176]	Loss: 4.8120
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.26927002444505,
                        "time": 2.5343017389968736,
                        "accuracy": 0.009765625,
                        "total_cost": 29394.791218980612
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6383
Profiling... [512/50176]	Loss: 5.6723
Profiling... [768/50176]	Loss: 5.4374
Profiling... [1024/50176]	Loss: 5.3031
Profiling... [1280/50176]	Loss: 4.9837
Profiling... [1536/50176]	Loss: 4.8279
Profiling... [1792/50176]	Loss: 4.7494
Profiling... [2048/50176]	Loss: 4.8756
Profiling... [2304/50176]	Loss: 4.7230
Profiling... [2560/50176]	Loss: 4.6330
Profiling... [2816/50176]	Loss: 4.6156
Profiling... [3072/50176]	Loss: 4.6091
Profiling... [3328/50176]	Loss: 4.5884
Profile done
epoch 1 train time consumed: 4.19s
Validation Epoch: 0, Average loss: 0.0188, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 113.13462192617784,
                        "time": 3.0107352359991637,
                        "accuracy": 0.009765625,
                        "total_cost": 34879.323406805735
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6234
Profiling... [512/50176]	Loss: 7.5385
Profiling... [768/50176]	Loss: 6.2007
Profiling... [1024/50176]	Loss: 5.0514
Profiling... [1280/50176]	Loss: 4.9207
Profiling... [1536/50176]	Loss: 5.2162
Profiling... [1792/50176]	Loss: 5.2015
Profiling... [2048/50176]	Loss: 4.8703
Profiling... [2304/50176]	Loss: 4.7164
Profiling... [2560/50176]	Loss: 5.0476
Profiling... [2816/50176]	Loss: 4.8539
Profiling... [3072/50176]	Loss: 4.6051
Profiling... [3328/50176]	Loss: 4.6841
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.00604555862411,
                        "time": 2.4921656160004204,
                        "accuracy": 0.00986328125,
                        "total_cost": 28553.356028591403
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6208
Profiling... [512/50176]	Loss: 8.1575
Profiling... [768/50176]	Loss: 6.5801
Profiling... [1024/50176]	Loss: 5.1937
Profiling... [1280/50176]	Loss: 5.1214
Profiling... [1536/50176]	Loss: 5.0180
Profiling... [1792/50176]	Loss: 4.8276
Profiling... [2048/50176]	Loss: 5.0689
Profiling... [2304/50176]	Loss: 4.8772
Profiling... [2560/50176]	Loss: 4.7187
Profiling... [2816/50176]	Loss: 4.6004
Profiling... [3072/50176]	Loss: 4.6271
Profiling... [3328/50176]	Loss: 4.9375
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.21252549276802,
                        "time": 2.3916740649947315,
                        "accuracy": 0.009765625,
                        "total_cost": 27726.588011889475
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6471
Profiling... [512/50176]	Loss: 7.3421
Profiling... [768/50176]	Loss: 6.0373
Profiling... [1024/50176]	Loss: 5.0098
Profiling... [1280/50176]	Loss: 5.5018
Profiling... [1536/50176]	Loss: 4.7538
Profiling... [1792/50176]	Loss: 4.6293
Profiling... [2048/50176]	Loss: 4.7921
Profiling... [2304/50176]	Loss: 5.7200
Profiling... [2560/50176]	Loss: 5.8046
Profiling... [2816/50176]	Loss: 5.0177
Profiling... [3072/50176]	Loss: 7.0469
Profiling... [3328/50176]	Loss: 5.1677
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.32390370458494,
                        "time": 2.5559646580004483,
                        "accuracy": 0.009765625,
                        "total_cost": 29660.35382021787
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6491
Profiling... [512/50176]	Loss: 7.5316
Profiling... [768/50176]	Loss: 6.8875
Profiling... [1024/50176]	Loss: 5.5791
Profiling... [1280/50176]	Loss: 5.1524
Profiling... [1536/50176]	Loss: 4.9644
Profiling... [1792/50176]	Loss: 4.7551
Profiling... [2048/50176]	Loss: 4.7035
Profiling... [2304/50176]	Loss: 4.8714
Profiling... [2560/50176]	Loss: 4.6602
Profiling... [2816/50176]	Loss: 4.8401
Profiling... [3072/50176]	Loss: 4.8411
Profiling... [3328/50176]	Loss: 4.7174
Profile done
epoch 1 train time consumed: 4.30s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 113.17988538903069,
                        "time": 3.01163750699925,
                        "accuracy": 0.009765625,
                        "total_cost": 34903.73507844928
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6176
Profiling... [512/50176]	Loss: 7.4514
Profiling... [768/50176]	Loss: 7.1791
Profiling... [1024/50176]	Loss: 5.7389
Profiling... [1280/50176]	Loss: 5.1211
Profiling... [1536/50176]	Loss: 4.7645
Profiling... [1792/50176]	Loss: 4.7951
Profiling... [2048/50176]	Loss: 4.7948
Profiling... [2304/50176]	Loss: 4.7462
Profiling... [2560/50176]	Loss: 4.7909
Profiling... [2816/50176]	Loss: 4.5991
Profiling... [3072/50176]	Loss: 4.9583
Profiling... [3328/50176]	Loss: 4.7770
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.05818874464073,
                        "time": 2.3992130879996694,
                        "accuracy": 0.009765625,
                        "total_cost": 27776.070260907924
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6471
Profiling... [512/50176]	Loss: 7.1427
Profiling... [768/50176]	Loss: 6.9383
Profiling... [1024/50176]	Loss: 5.9870
Profiling... [1280/50176]	Loss: 5.4445
Profiling... [1536/50176]	Loss: 5.3657
Profiling... [1792/50176]	Loss: 5.1135
Profiling... [2048/50176]	Loss: 5.1176
Profiling... [2304/50176]	Loss: 4.7784
Profiling... [2560/50176]	Loss: 4.7211
Profiling... [2816/50176]	Loss: 4.8088
Profiling... [3072/50176]	Loss: 4.7540
Profiling... [3328/50176]	Loss: 4.6608
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.24475914825167,
                        "time": 2.4016570410021814,
                        "accuracy": 0.009765625,
                        "total_cost": 27850.247492095466
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6356
Profiling... [512/50176]	Loss: 7.1520
Profiling... [768/50176]	Loss: 6.5937
Profiling... [1024/50176]	Loss: 5.7590
Profiling... [1280/50176]	Loss: 5.3338
Profiling... [1536/50176]	Loss: 5.3104
Profiling... [1792/50176]	Loss: 5.1905
Profiling... [2048/50176]	Loss: 5.0705
Profiling... [2304/50176]	Loss: 4.9459
Profiling... [2560/50176]	Loss: 4.7505
Profiling... [2816/50176]	Loss: 4.8176
Profiling... [3072/50176]	Loss: 4.7196
Profiling... [3328/50176]	Loss: 5.1147
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.35320712710357,
                        "time": 2.5605780180048896,
                        "accuracy": 0.009765625,
                        "total_cost": 29721.572397057702
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6317
Profiling... [512/50176]	Loss: 7.2837
Profiling... [768/50176]	Loss: 5.6335
Profiling... [1024/50176]	Loss: 5.3265
Profiling... [1280/50176]	Loss: 4.9485
Profiling... [1536/50176]	Loss: 4.8783
Profiling... [1792/50176]	Loss: 4.6785
Profiling... [2048/50176]	Loss: 4.6444
Profiling... [2304/50176]	Loss: 4.8930
Profiling... [2560/50176]	Loss: 5.0784
Profiling... [2816/50176]	Loss: 4.7638
Profiling... [3072/50176]	Loss: 4.8430
Profiling... [3328/50176]	Loss: 4.8304
Profile done
epoch 1 train time consumed: 4.36s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0101
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 113.23503713029152,
                        "time": 3.0475044669947238,
                        "accuracy": 0.01005859375,
                        "total_cost": 34307.40817769648
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6389
Profiling... [512/50176]	Loss: 7.9074
Profiling... [768/50176]	Loss: 6.6223
Profiling... [1024/50176]	Loss: 5.6712
Profiling... [1280/50176]	Loss: 5.2230
Profiling... [1536/50176]	Loss: 4.9103
Profiling... [1792/50176]	Loss: 4.6860
Profiling... [2048/50176]	Loss: 4.9184
Profiling... [2304/50176]	Loss: 4.9149
Profiling... [2560/50176]	Loss: 4.8469
Profiling... [2816/50176]	Loss: 4.6240
Profiling... [3072/50176]	Loss: 4.7407
Profiling... [3328/50176]	Loss: 4.7994
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 0, Average loss: 0.0181, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.10634045973637,
                        "time": 2.4977187359982054,
                        "accuracy": 0.009765625,
                        "total_cost": 28928.801354391082
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6390
Profiling... [512/50176]	Loss: 7.5409
Profiling... [768/50176]	Loss: 6.9413
Profiling... [1024/50176]	Loss: 5.4644
Profiling... [1280/50176]	Loss: 4.8878
Profiling... [1536/50176]	Loss: 4.9291
Profiling... [1792/50176]	Loss: 4.9218
Profiling... [2048/50176]	Loss: 4.6258
Profiling... [2304/50176]	Loss: 4.6516
Profiling... [2560/50176]	Loss: 4.6625
Profiling... [2816/50176]	Loss: 4.7273
Profiling... [3072/50176]	Loss: 4.6342
Profiling... [3328/50176]	Loss: 4.6238
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.27507259723642,
                        "time": 2.416464645000815,
                        "accuracy": 0.009765625,
                        "total_cost": 28029.46130853094
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6338
Profiling... [512/50176]	Loss: 7.6265
Profiling... [768/50176]	Loss: 6.4089
Profiling... [1024/50176]	Loss: 5.7492
Profiling... [1280/50176]	Loss: 4.8728
Profiling... [1536/50176]	Loss: 4.9524
Profiling... [1792/50176]	Loss: 4.6089
Profiling... [2048/50176]	Loss: 5.0042
Profiling... [2304/50176]	Loss: 4.9812
Profiling... [2560/50176]	Loss: 5.1853
Profiling... [2816/50176]	Loss: 4.7918
Profiling... [3072/50176]	Loss: 4.8705
Profiling... [3328/50176]	Loss: 4.8274
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.38124753423475,
                        "time": 2.532747962999565,
                        "accuracy": 0.009765625,
                        "total_cost": 29405.81107043149
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6309
Profiling... [512/50176]	Loss: 7.5102
Profiling... [768/50176]	Loss: 7.1108
Profiling... [1024/50176]	Loss: 5.8508
Profiling... [1280/50176]	Loss: 4.9975
Profiling... [1536/50176]	Loss: 4.7538
Profiling... [1792/50176]	Loss: 4.8121
Profiling... [2048/50176]	Loss: 4.7042
Profiling... [2304/50176]	Loss: 4.8503
Profiling... [2560/50176]	Loss: 4.8201
Profiling... [2816/50176]	Loss: 4.8410
Profiling... [3072/50176]	Loss: 4.7705
Profiling... [3328/50176]	Loss: 4.7880
Profile done
epoch 1 train time consumed: 4.42s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 113.26346386141267,
                        "time": 3.0393128109935788,
                        "accuracy": 0.009765625,
                        "total_cost": 35250.49310530558
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6159
Profiling... [1024/50176]	Loss: 4.6251
Profiling... [1536/50176]	Loss: 4.6287
Profiling... [2048/50176]	Loss: 4.6390
Profiling... [2560/50176]	Loss: 4.6252
Profiling... [3072/50176]	Loss: 4.6046
Profiling... [3584/50176]	Loss: 4.6143
Profiling... [4096/50176]	Loss: 4.5817
Profiling... [4608/50176]	Loss: 4.5570
Profiling... [5120/50176]	Loss: 4.5338
Profiling... [5632/50176]	Loss: 4.5069
Profiling... [6144/50176]	Loss: 4.4928
Profiling... [6656/50176]	Loss: 4.4917
Profile done
epoch 1 train time consumed: 6.54s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.31307926161597,
                        "time": 4.59806388799916,
                        "accuracy": 0.009765625,
                        "total_cost": 53352.52764578026
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6349
Profiling... [1024/50176]	Loss: 4.6485
Profiling... [1536/50176]	Loss: 4.6564
Profiling... [2048/50176]	Loss: 4.6725
Profiling... [2560/50176]	Loss: 4.6766
Profiling... [3072/50176]	Loss: 4.6382
Profiling... [3584/50176]	Loss: 4.6100
Profiling... [4096/50176]	Loss: 4.5370
Profiling... [4608/50176]	Loss: 4.5712
Profiling... [5120/50176]	Loss: 4.5313
Profiling... [5632/50176]	Loss: 4.5092
Profiling... [6144/50176]	Loss: 4.4911
Profiling... [6656/50176]	Loss: 4.5340
Profile done
epoch 1 train time consumed: 6.54s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.62130115675696,
                        "time": 4.543472899997141,
                        "accuracy": 0.009765625,
                        "total_cost": 52862.49499321744
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6390
Profiling... [1024/50176]	Loss: 4.6503
Profiling... [1536/50176]	Loss: 4.6333
Profiling... [2048/50176]	Loss: 4.6326
Profiling... [2560/50176]	Loss: 4.5983
Profiling... [3072/50176]	Loss: 4.5428
Profiling... [3584/50176]	Loss: 4.6146
Profiling... [4096/50176]	Loss: 4.5344
Profiling... [4608/50176]	Loss: 4.5562
Profiling... [5120/50176]	Loss: 4.5460
Profiling... [5632/50176]	Loss: 4.5101
Profiling... [6144/50176]	Loss: 4.5395
Profiling... [6656/50176]	Loss: 4.5157
Profile done
epoch 1 train time consumed: 6.99s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.72654773109286,
                        "time": 4.886258432998147,
                        "accuracy": 0.009765625,
                        "total_cost": 56903.40381765824
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6303
Profiling... [1024/50176]	Loss: 4.6373
Profiling... [1536/50176]	Loss: 4.6365
Profiling... [2048/50176]	Loss: 4.6401
Profiling... [2560/50176]	Loss: 4.5820
Profiling... [3072/50176]	Loss: 4.5739
Profiling... [3584/50176]	Loss: 4.5437
Profiling... [4096/50176]	Loss: 4.5890
Profiling... [4608/50176]	Loss: 4.5481
Profiling... [5120/50176]	Loss: 4.5016
Profiling... [5632/50176]	Loss: 4.5034
Profiling... [6144/50176]	Loss: 4.5336
Profiling... [6656/50176]	Loss: 4.5252
Profile done
epoch 1 train time consumed: 8.32s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 113.48216007780006,
                        "time": 6.034870861003583,
                        "accuracy": 0.009765625,
                        "total_cost": 70128.6585443594
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6310
Profiling... [1024/50176]	Loss: 4.6365
Profiling... [1536/50176]	Loss: 4.6537
Profiling... [2048/50176]	Loss: 4.6264
Profiling... [2560/50176]	Loss: 4.6177
Profiling... [3072/50176]	Loss: 4.6175
Profiling... [3584/50176]	Loss: 4.5868
Profiling... [4096/50176]	Loss: 4.5582
Profiling... [4608/50176]	Loss: 4.5673
Profiling... [5120/50176]	Loss: 4.5254
Profiling... [5632/50176]	Loss: 4.5378
Profiling... [6144/50176]	Loss: 4.4989
Profiling... [6656/50176]	Loss: 4.5627
Profile done
epoch 1 train time consumed: 6.51s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.50512628379381,
                        "time": 4.501754775003064,
                        "accuracy": 0.009765625,
                        "total_cost": 52323.557809704405
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6345
Profiling... [1024/50176]	Loss: 4.6357
Profiling... [1536/50176]	Loss: 4.6671
Profiling... [2048/50176]	Loss: 4.6735
Profiling... [2560/50176]	Loss: 4.6063
Profiling... [3072/50176]	Loss: 4.6230
Profiling... [3584/50176]	Loss: 4.5894
Profiling... [4096/50176]	Loss: 4.5963
Profiling... [4608/50176]	Loss: 4.5204
Profiling... [5120/50176]	Loss: 4.5403
Profiling... [5632/50176]	Loss: 4.5168
Profiling... [6144/50176]	Loss: 4.5290
Profiling... [6656/50176]	Loss: 4.5019
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.77610055802432,
                        "time": 4.636175364001247,
                        "accuracy": 0.009765625,
                        "total_cost": 54014.56173253027
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6422
Profiling... [1024/50176]	Loss: 4.6178
Profiling... [1536/50176]	Loss: 4.6940
Profiling... [2048/50176]	Loss: 4.6008
Profiling... [2560/50176]	Loss: 4.6300
Profiling... [3072/50176]	Loss: 4.6080
Profiling... [3584/50176]	Loss: 4.5840
Profiling... [4096/50176]	Loss: 4.5454
Profiling... [4608/50176]	Loss: 4.5276
Profiling... [5120/50176]	Loss: 4.5684
Profiling... [5632/50176]	Loss: 4.4942
Profiling... [6144/50176]	Loss: 4.5370
Profiling... [6656/50176]	Loss: 4.5420
Profile done
epoch 1 train time consumed: 7.01s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.86229453965113,
                        "time": 4.863131215999601,
                        "accuracy": 0.009765625,
                        "total_cost": 56701.67335947452
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6500
Profiling... [1024/50176]	Loss: 4.6346
Profiling... [1536/50176]	Loss: 4.6640
Profiling... [2048/50176]	Loss: 4.6370
Profiling... [2560/50176]	Loss: 4.5707
Profiling... [3072/50176]	Loss: 4.6099
Profiling... [3584/50176]	Loss: 4.5825
Profiling... [4096/50176]	Loss: 4.5516
Profiling... [4608/50176]	Loss: 4.5472
Profiling... [5120/50176]	Loss: 4.5542
Profiling... [5632/50176]	Loss: 4.4949
Profiling... [6144/50176]	Loss: 4.5394
Profiling... [6656/50176]	Loss: 4.5104
Profile done
epoch 1 train time consumed: 8.35s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 113.62514126764663,
                        "time": 6.022585190999962,
                        "accuracy": 0.009765625,
                        "total_cost": 70074.07033587786
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6361
Profiling... [1024/50176]	Loss: 4.6104
Profiling... [1536/50176]	Loss: 4.6193
Profiling... [2048/50176]	Loss: 4.6727
Profiling... [2560/50176]	Loss: 4.6235
Profiling... [3072/50176]	Loss: 4.6384
Profiling... [3584/50176]	Loss: 4.6121
Profiling... [4096/50176]	Loss: 4.6090
Profiling... [4608/50176]	Loss: 4.5938
Profiling... [5120/50176]	Loss: 4.5723
Profiling... [5632/50176]	Loss: 4.5038
Profiling... [6144/50176]	Loss: 4.5296
Profiling... [6656/50176]	Loss: 4.5000
Profile done
epoch 1 train time consumed: 6.68s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.64503780388024,
                        "time": 4.704656706999231,
                        "accuracy": 0.009765625,
                        "total_cost": 54749.27506649153
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6402
Profiling... [1024/50176]	Loss: 4.6551
Profiling... [1536/50176]	Loss: 4.6321
Profiling... [2048/50176]	Loss: 4.6095
Profiling... [2560/50176]	Loss: 4.6076
Profiling... [3072/50176]	Loss: 4.5860
Profiling... [3584/50176]	Loss: 4.5530
Profiling... [4096/50176]	Loss: 4.5333
Profiling... [4608/50176]	Loss: 4.5808
Profiling... [5120/50176]	Loss: 4.5680
Profiling... [5632/50176]	Loss: 4.5922
Profiling... [6144/50176]	Loss: 4.5200
Profiling... [6656/50176]	Loss: 4.5315
Profile done
epoch 1 train time consumed: 6.58s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.90719972206514,
                        "time": 4.6215574579982786,
                        "accuracy": 0.009765625,
                        "total_cost": 53906.29564366946
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6338
Profiling... [1024/50176]	Loss: 4.6670
Profiling... [1536/50176]	Loss: 4.6756
Profiling... [2048/50176]	Loss: 4.6595
Profiling... [2560/50176]	Loss: 4.6124
Profiling... [3072/50176]	Loss: 4.6170
Profiling... [3584/50176]	Loss: 4.5867
Profiling... [4096/50176]	Loss: 4.5452
Profiling... [4608/50176]	Loss: 4.5728
Profiling... [5120/50176]	Loss: 4.5324
Profiling... [5632/50176]	Loss: 4.5385
Profiling... [6144/50176]	Loss: 4.5253
Profiling... [6656/50176]	Loss: 4.5889
Profile done
epoch 1 train time consumed: 6.95s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.02166158726575,
                        "time": 4.887059256994689,
                        "accuracy": 0.009765625,
                        "total_cost": 57060.415156015406
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6440
Profiling... [1024/50176]	Loss: 4.5826
Profiling... [1536/50176]	Loss: 4.6452
Profiling... [2048/50176]	Loss: 4.6942
Profiling... [2560/50176]	Loss: 4.6401
Profiling... [3072/50176]	Loss: 4.6301
Profiling... [3584/50176]	Loss: 4.5743
Profiling... [4096/50176]	Loss: 4.5736
Profiling... [4608/50176]	Loss: 4.5144
Profiling... [5120/50176]	Loss: 4.5373
Profiling... [5632/50176]	Loss: 4.5310
Profiling... [6144/50176]	Loss: 4.5379
Profiling... [6656/50176]	Loss: 4.5367
Profile done
epoch 1 train time consumed: 8.33s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 113.78294943666243,
                        "time": 6.038090250993264,
                        "accuracy": 0.009765625,
                        "total_cost": 70352.04789481175
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6452
Profiling... [1024/50176]	Loss: 5.3651
Profiling... [1536/50176]	Loss: 5.2000
Profiling... [2048/50176]	Loss: 4.9316
Profiling... [2560/50176]	Loss: 4.9303
Profiling... [3072/50176]	Loss: 4.6929
Profiling... [3584/50176]	Loss: 4.6760
Profiling... [4096/50176]	Loss: 4.6981
Profiling... [4608/50176]	Loss: 4.8359
Profiling... [5120/50176]	Loss: 4.7432
Profiling... [5632/50176]	Loss: 4.5748
Profiling... [6144/50176]	Loss: 4.6087
Profiling... [6656/50176]	Loss: 4.6264
Profile done
epoch 1 train time consumed: 6.72s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.78972221800461,
                        "time": 4.719835010000679,
                        "accuracy": 0.009765625,
                        "total_cost": 54995.83638556573
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6376
Profiling... [1024/50176]	Loss: 5.6078
Profiling... [1536/50176]	Loss: 5.4783
Profiling... [2048/50176]	Loss: 4.9876
Profiling... [2560/50176]	Loss: 4.7525
Profiling... [3072/50176]	Loss: 4.7068
Profiling... [3584/50176]	Loss: 4.7685
Profiling... [4096/50176]	Loss: 4.7660
Profiling... [4608/50176]	Loss: 4.6819
Profiling... [5120/50176]	Loss: 4.6700
Profiling... [5632/50176]	Loss: 4.6946
Profiling... [6144/50176]	Loss: 4.6175
Profiling... [6656/50176]	Loss: 4.5384
Profile done
epoch 1 train time consumed: 6.57s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.02246271092443,
                        "time": 4.52710248400399,
                        "accuracy": 0.009765625,
                        "total_cost": 52857.996715097936
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6351
Profiling... [1024/50176]	Loss: 5.7793
Profiling... [1536/50176]	Loss: 5.1845
Profiling... [2048/50176]	Loss: 4.8263
Profiling... [2560/50176]	Loss: 4.8375
Profiling... [3072/50176]	Loss: 4.8441
Profiling... [3584/50176]	Loss: 4.6616
Profiling... [4096/50176]	Loss: 4.7222
Profiling... [4608/50176]	Loss: 4.5935
Profiling... [5120/50176]	Loss: 4.6034
Profiling... [5632/50176]	Loss: 4.6629
Profiling... [6144/50176]	Loss: 4.6273
Profiling... [6656/50176]	Loss: 4.5306
Profile done
epoch 1 train time consumed: 6.91s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.106426469869,
                        "time": 4.870590522994462,
                        "accuracy": 0.009765625,
                        "total_cost": 56910.40556819543
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6351
Profiling... [1024/50176]	Loss: 5.4585
Profiling... [1536/50176]	Loss: 5.3390
Profiling... [2048/50176]	Loss: 4.8905
Profiling... [2560/50176]	Loss: 4.7220
Profiling... [3072/50176]	Loss: 4.8045
Profiling... [3584/50176]	Loss: 4.8160
Profiling... [4096/50176]	Loss: 4.9775
Profiling... [4608/50176]	Loss: 4.8211
Profiling... [5120/50176]	Loss: 4.6347
Profiling... [5632/50176]	Loss: 4.5769
Profiling... [6144/50176]	Loss: 4.7181
Profiling... [6656/50176]	Loss: 4.6465
Profile done
epoch 1 train time consumed: 8.35s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 113.89279848070194,
                        "time": 6.033031868995749,
                        "accuracy": 0.009765625,
                        "total_cost": 70360.9736072382
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6393
Profiling... [1024/50176]	Loss: 5.6534
Profiling... [1536/50176]	Loss: 5.2182
Profiling... [2048/50176]	Loss: 4.8463
Profiling... [2560/50176]	Loss: 4.7804
Profiling... [3072/50176]	Loss: 4.7705
Profiling... [3584/50176]	Loss: 4.6339
Profiling... [4096/50176]	Loss: 4.6079
Profiling... [4608/50176]	Loss: 4.7589
Profiling... [5120/50176]	Loss: 4.5807
Profiling... [5632/50176]	Loss: 4.6165
Profiling... [6144/50176]	Loss: 4.5779
Profiling... [6656/50176]	Loss: 4.6504
Profile done
epoch 1 train time consumed: 6.70s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.8839971605654,
                        "time": 4.604236031002074,
                        "accuracy": 0.009765625,
                        "total_cost": 53693.31743551622
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6258
Profiling... [1024/50176]	Loss: 5.6687
Profiling... [1536/50176]	Loss: 5.1910
Profiling... [2048/50176]	Loss: 5.0382
Profiling... [2560/50176]	Loss: 4.8446
Profiling... [3072/50176]	Loss: 4.7357
Profiling... [3584/50176]	Loss: 4.8061
Profiling... [4096/50176]	Loss: 4.7634
Profiling... [4608/50176]	Loss: 4.6888
Profiling... [5120/50176]	Loss: 4.6589
Profiling... [5632/50176]	Loss: 4.6212
Profiling... [6144/50176]	Loss: 4.6018
Profiling... [6656/50176]	Loss: 4.6016
Profile done
epoch 1 train time consumed: 6.68s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.11183876000936,
                        "time": 4.616819638002198,
                        "accuracy": 0.009765625,
                        "total_cost": 53947.778879052974
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6376
Profiling... [1024/50176]	Loss: 5.4964
Profiling... [1536/50176]	Loss: 5.1301
Profiling... [2048/50176]	Loss: 4.9786
Profiling... [2560/50176]	Loss: 4.9385
Profiling... [3072/50176]	Loss: 4.7843
Profiling... [3584/50176]	Loss: 4.7450
Profiling... [4096/50176]	Loss: 4.6965
Profiling... [4608/50176]	Loss: 4.6303
Profiling... [5120/50176]	Loss: 4.7174
Profiling... [5632/50176]	Loss: 4.6394
Profiling... [6144/50176]	Loss: 4.5711
Profiling... [6656/50176]	Loss: 4.5368
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.20033284717651,
                        "time": 4.841607034002664,
                        "accuracy": 0.009765625,
                        "total_cost": 56618.30500334953
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6314
Profiling... [1024/50176]	Loss: 5.6359
Profiling... [1536/50176]	Loss: 5.1527
Profiling... [2048/50176]	Loss: 4.9024
Profiling... [2560/50176]	Loss: 4.8345
Profiling... [3072/50176]	Loss: 4.7400
Profiling... [3584/50176]	Loss: 4.7315
Profiling... [4096/50176]	Loss: 4.8440
Profiling... [4608/50176]	Loss: 4.6997
Profiling... [5120/50176]	Loss: 4.7660
Profiling... [5632/50176]	Loss: 4.5786
Profiling... [6144/50176]	Loss: 4.6157
Profiling... [6656/50176]	Loss: 4.6264
Profile done
epoch 1 train time consumed: 8.37s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 113.98892109222547,
                        "time": 6.0766534930007765,
                        "accuracy": 0.009765625,
                        "total_cost": 70929.52837309048
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6370
Profiling... [1024/50176]	Loss: 5.4454
Profiling... [1536/50176]	Loss: 5.2556
Profiling... [2048/50176]	Loss: 4.8756
Profiling... [2560/50176]	Loss: 4.9199
Profiling... [3072/50176]	Loss: 4.8352
Profiling... [3584/50176]	Loss: 4.6948
Profiling... [4096/50176]	Loss: 4.6877
Profiling... [4608/50176]	Loss: 4.5974
Profiling... [5120/50176]	Loss: 4.6831
Profiling... [5632/50176]	Loss: 4.6281
Profiling... [6144/50176]	Loss: 4.6356
Profiling... [6656/50176]	Loss: 4.5562
Profile done
epoch 1 train time consumed: 6.46s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.00648591041157,
                        "time": 4.506597115003387,
                        "accuracy": 0.009765625,
                        "total_cost": 52611.205170742796
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6588
Profiling... [1024/50176]	Loss: 5.4865
Profiling... [1536/50176]	Loss: 5.3216
Profiling... [2048/50176]	Loss: 5.0052
Profiling... [2560/50176]	Loss: 4.7012
Profiling... [3072/50176]	Loss: 4.7039
Profiling... [3584/50176]	Loss: 4.6408
Profiling... [4096/50176]	Loss: 4.7174
Profiling... [4608/50176]	Loss: 4.6987
Profiling... [5120/50176]	Loss: 4.6155
Profiling... [5632/50176]	Loss: 4.6179
Profiling... [6144/50176]	Loss: 4.6518
Profiling... [6656/50176]	Loss: 4.8291
Profile done
epoch 1 train time consumed: 6.73s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.1919523178546,
                        "time": 4.709563570002501,
                        "accuracy": 0.009765625,
                        "total_cost": 55070.13208305979
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6401
Profiling... [1024/50176]	Loss: 5.6253
Profiling... [1536/50176]	Loss: 5.4398
Profiling... [2048/50176]	Loss: 4.8400
Profiling... [2560/50176]	Loss: 4.7864
Profiling... [3072/50176]	Loss: 4.7509
Profiling... [3584/50176]	Loss: 4.7422
Profiling... [4096/50176]	Loss: 4.7176
Profiling... [4608/50176]	Loss: 4.5998
Profiling... [5120/50176]	Loss: 4.7744
Profiling... [5632/50176]	Loss: 4.6474
Profiling... [6144/50176]	Loss: 4.6246
Profiling... [6656/50176]	Loss: 4.6094
Profile done
epoch 1 train time consumed: 6.90s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.27818335258881,
                        "time": 4.861953747997177,
                        "accuracy": 0.009765625,
                        "total_cost": 56895.000767019796
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6326
Profiling... [1024/50176]	Loss: 5.6750
Profiling... [1536/50176]	Loss: 5.2334
Profiling... [2048/50176]	Loss: 5.0037
Profiling... [2560/50176]	Loss: 4.8677
Profiling... [3072/50176]	Loss: 4.9739
Profiling... [3584/50176]	Loss: 4.8292
Profiling... [4096/50176]	Loss: 4.9140
Profiling... [4608/50176]	Loss: 4.8612
Profiling... [5120/50176]	Loss: 4.6692
Profiling... [5632/50176]	Loss: 4.6782
Profiling... [6144/50176]	Loss: 4.6727
Profiling... [6656/50176]	Loss: 4.6701
Profile done
epoch 1 train time consumed: 8.36s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.08706147586888,
                        "time": 6.044943159999093,
                        "accuracy": 0.009765625,
                        "total_cost": 70620.13971588603
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6196
Profiling... [1024/50176]	Loss: 7.2734
Profiling... [1536/50176]	Loss: 5.9800
Profiling... [2048/50176]	Loss: 5.0992
Profiling... [2560/50176]	Loss: 4.6658
Profiling... [3072/50176]	Loss: 4.6861
Profiling... [3584/50176]	Loss: 4.7027
Profiling... [4096/50176]	Loss: 6.0831
Profiling... [4608/50176]	Loss: 4.6967
Profiling... [5120/50176]	Loss: 4.8391
Profiling... [5632/50176]	Loss: 4.9176
Profiling... [6144/50176]	Loss: 4.7852
Profiling... [6656/50176]	Loss: 4.5786
Profile done
epoch 1 train time consumed: 6.56s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.09545516041369,
                        "time": 4.60424562299886,
                        "accuracy": 0.009765625,
                        "total_cost": 53793.1264027031
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6455
Profiling... [1024/50176]	Loss: 7.3866
Profiling... [1536/50176]	Loss: 5.5798
Profiling... [2048/50176]	Loss: 4.9373
Profiling... [2560/50176]	Loss: 4.8981
Profiling... [3072/50176]	Loss: 4.9417
Profiling... [3584/50176]	Loss: 4.8231
Profiling... [4096/50176]	Loss: 4.9247
Profiling... [4608/50176]	Loss: 4.7097
Profiling... [5120/50176]	Loss: 4.5845
Profiling... [5632/50176]	Loss: 4.7103
Profiling... [6144/50176]	Loss: 4.6535
Profiling... [6656/50176]	Loss: 4.5866
Profile done
epoch 1 train time consumed: 6.60s
Validation Epoch: 0, Average loss: 0.0725, Accuracy: 0.0179
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.29904840013697,
                        "time": 4.528094257002522,
                        "accuracy": 0.01787109375,
                        "total_cost": 28960.558983219114
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6359
Profiling... [1024/50176]	Loss: 7.0535
Profiling... [1536/50176]	Loss: 6.3731
Profiling... [2048/50176]	Loss: 5.1613
Profiling... [2560/50176]	Loss: 4.8288
Profiling... [3072/50176]	Loss: 4.7237
Profiling... [3584/50176]	Loss: 4.7065
Profiling... [4096/50176]	Loss: 4.6120
Profiling... [4608/50176]	Loss: 4.8686
Profiling... [5120/50176]	Loss: 4.6821
Profiling... [5632/50176]	Loss: 4.6386
Profiling... [6144/50176]	Loss: 4.8300
Profiling... [6656/50176]	Loss: 4.8985
Profile done
epoch 1 train time consumed: 6.90s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.37513969672278,
                        "time": 4.874823919999471,
                        "accuracy": 0.009765625,
                        "total_cost": 57094.00748511901
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6309
Profiling... [1024/50176]	Loss: 7.3746
Profiling... [1536/50176]	Loss: 6.4765
Profiling... [2048/50176]	Loss: 5.4410
Profiling... [2560/50176]	Loss: 5.0325
Profiling... [3072/50176]	Loss: 4.9861
Profiling... [3584/50176]	Loss: 4.6407
Profiling... [4096/50176]	Loss: 4.8234
Profiling... [4608/50176]	Loss: 4.6307
Profiling... [5120/50176]	Loss: 4.7309
Profiling... [5632/50176]	Loss: 4.6803
Profiling... [6144/50176]	Loss: 4.5924
Profiling... [6656/50176]	Loss: 4.6759
Profile done
epoch 1 train time consumed: 8.36s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0111
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.19067057391813,
                        "time": 6.014716596997459,
                        "accuracy": 0.0111328125,
                        "total_cost": 61693.711406997536
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6373
Profiling... [1024/50176]	Loss: 7.4260
Profiling... [1536/50176]	Loss: 6.3682
Profiling... [2048/50176]	Loss: 5.2134
Profiling... [2560/50176]	Loss: 5.0060
Profiling... [3072/50176]	Loss: 4.6718
Profiling... [3584/50176]	Loss: 4.8912
Profiling... [4096/50176]	Loss: 5.0420
Profiling... [4608/50176]	Loss: 5.4981
Profiling... [5120/50176]	Loss: 4.8313
Profiling... [5632/50176]	Loss: 4.7017
Profiling... [6144/50176]	Loss: 4.9732
Profiling... [6656/50176]	Loss: 4.7562
Profile done
epoch 1 train time consumed: 6.58s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.19235581569686,
                        "time": 4.604925268002262,
                        "accuracy": 0.009765625,
                        "total_cost": 53846.75990614093
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6434
Profiling... [1024/50176]	Loss: 7.5295
Profiling... [1536/50176]	Loss: 6.6395
Profiling... [2048/50176]	Loss: 5.2436
Profiling... [2560/50176]	Loss: 5.1874
Profiling... [3072/50176]	Loss: 4.9910
Profiling... [3584/50176]	Loss: 5.2362
Profiling... [4096/50176]	Loss: 5.0420
Profiling... [4608/50176]	Loss: 4.8279
Profiling... [5120/50176]	Loss: 4.7303
Profiling... [5632/50176]	Loss: 4.8308
Profiling... [6144/50176]	Loss: 4.6486
Profiling... [6656/50176]	Loss: 4.6626
Profile done
epoch 1 train time consumed: 6.71s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.37597144690592,
                        "time": 4.6326255690000835,
                        "accuracy": 0.009765625,
                        "total_cost": 54257.77149994597
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6257
Profiling... [1024/50176]	Loss: 7.5338
Profiling... [1536/50176]	Loss: 6.0778
Profiling... [2048/50176]	Loss: 5.4458
Profiling... [2560/50176]	Loss: 4.9137
Profiling... [3072/50176]	Loss: 4.9115
Profiling... [3584/50176]	Loss: 4.7020
Profiling... [4096/50176]	Loss: 4.7968
Profiling... [4608/50176]	Loss: 4.8353
Profiling... [5120/50176]	Loss: 4.7420
Profiling... [5632/50176]	Loss: 4.9085
Profiling... [6144/50176]	Loss: 4.8192
Profiling... [6656/50176]	Loss: 4.7224
Profile done
epoch 1 train time consumed: 6.95s
Validation Epoch: 0, Average loss: 0.0123, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.43690843548944,
                        "time": 4.875932905997615,
                        "accuracy": 0.009765625,
                        "total_cost": 57137.83680012688
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6299
Profiling... [1024/50176]	Loss: 7.2793
Profiling... [1536/50176]	Loss: 6.1261
Profiling... [2048/50176]	Loss: 5.4416
Profiling... [2560/50176]	Loss: 5.2610
Profiling... [3072/50176]	Loss: 4.8273
Profiling... [3584/50176]	Loss: 4.9518
Profiling... [4096/50176]	Loss: 4.8132
Profiling... [4608/50176]	Loss: 4.6804
Profiling... [5120/50176]	Loss: 4.6123
Profiling... [5632/50176]	Loss: 4.7206
Profiling... [6144/50176]	Loss: 4.7527
Profiling... [6656/50176]	Loss: 4.6835
Profile done
epoch 1 train time consumed: 8.30s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.25319273635647,
                        "time": 6.029468795997673,
                        "accuracy": 0.009765625,
                        "total_cost": 70541.93258976965
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6379
Profiling... [1024/50176]	Loss: 7.1618
Profiling... [1536/50176]	Loss: 5.2451
Profiling... [2048/50176]	Loss: 4.8811
Profiling... [2560/50176]	Loss: 5.0546
Profiling... [3072/50176]	Loss: 5.1598
Profiling... [3584/50176]	Loss: 5.1737
Profiling... [4096/50176]	Loss: 4.7455
Profiling... [4608/50176]	Loss: 4.8169
Profiling... [5120/50176]	Loss: 4.7507
Profiling... [5632/50176]	Loss: 4.7895
Profiling... [6144/50176]	Loss: 4.6259
Profiling... [6656/50176]	Loss: 4.7168
Profile done
epoch 1 train time consumed: 6.69s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.24971814049562,
                        "time": 4.6217551629961235,
                        "accuracy": 0.00986328125,
                        "total_cost": 53535.35109694733
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6177
Profiling... [1024/50176]	Loss: 7.7489
Profiling... [1536/50176]	Loss: 5.8073
Profiling... [2048/50176]	Loss: 5.1353
Profiling... [2560/50176]	Loss: 5.0545
Profiling... [3072/50176]	Loss: 4.7495
Profiling... [3584/50176]	Loss: 4.6591
Profiling... [4096/50176]	Loss: 5.0873
Profiling... [4608/50176]	Loss: 4.7765
Profiling... [5120/50176]	Loss: 4.8541
Profiling... [5632/50176]	Loss: 4.8283
Profiling... [6144/50176]	Loss: 4.6606
Profiling... [6656/50176]	Loss: 4.7212
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.425687052745,
                        "time": 4.839862025997718,
                        "accuracy": 0.009765625,
                        "total_cost": 56709.5846466846
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6189
Profiling... [1024/50176]	Loss: 7.3545
Profiling... [1536/50176]	Loss: 5.4849
Profiling... [2048/50176]	Loss: 5.1637
Profiling... [2560/50176]	Loss: 4.7975
Profiling... [3072/50176]	Loss: 5.3255
Profiling... [3584/50176]	Loss: 4.6191
Profiling... [4096/50176]	Loss: 4.8074
Profiling... [4608/50176]	Loss: 4.8780
Profiling... [5120/50176]	Loss: 4.8219
Profiling... [5632/50176]	Loss: 4.6877
Profiling... [6144/50176]	Loss: 4.8372
Profiling... [6656/50176]	Loss: 5.0430
Profile done
epoch 1 train time consumed: 7.00s
Validation Epoch: 0, Average loss: 0.0102, Accuracy: 0.0108
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.47887600455944,
                        "time": 4.871710147999693,
                        "accuracy": 0.01083984375,
                        "total_cost": 51449.81005496604
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6220
Profiling... [1024/50176]	Loss: 7.4872
Profiling... [1536/50176]	Loss: 5.9063
Profiling... [2048/50176]	Loss: 5.4495
Profiling... [2560/50176]	Loss: 5.4743
Profiling... [3072/50176]	Loss: 4.9116
Profiling... [3584/50176]	Loss: 4.8220
Profiling... [4096/50176]	Loss: 4.7047
Profiling... [4608/50176]	Loss: 4.7638
Profiling... [5120/50176]	Loss: 4.6235
Profiling... [5632/50176]	Loss: 4.6485
Profiling... [6144/50176]	Loss: 4.6152
Profiling... [6656/50176]	Loss: 4.6186
Profile done
epoch 1 train time consumed: 8.43s
Validation Epoch: 0, Average loss: 0.0147, Accuracy: 0.0130
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.30740706335736,
                        "time": 6.026581272002659,
                        "accuracy": 0.01298828125,
                        "total_cost": 53038.80208624324
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6353
Profiling... [2048/50176]	Loss: 4.6256
Profiling... [3072/50176]	Loss: 4.6137
Profiling... [4096/50176]	Loss: 4.6193
Profiling... [5120/50176]	Loss: 4.5911
Profiling... [6144/50176]	Loss: 4.5682
Profiling... [7168/50176]	Loss: 4.5267
Profiling... [8192/50176]	Loss: 4.5605
Profiling... [9216/50176]	Loss: 4.5348
Profiling... [10240/50176]	Loss: 4.5214
Profiling... [11264/50176]	Loss: 4.5100
Profiling... [12288/50176]	Loss: 4.5047
Profiling... [13312/50176]	Loss: 4.5013
Profile done
epoch 1 train time consumed: 12.78s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.4363520172735,
                        "time": 9.06480837900017,
                        "accuracy": 0.009765625,
                        "total_cost": 106223.98490914753
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6367
Profiling... [2048/50176]	Loss: 4.6126
Profiling... [3072/50176]	Loss: 4.6457
Profiling... [4096/50176]	Loss: 4.5993
Profiling... [5120/50176]	Loss: 4.5678
Profiling... [6144/50176]	Loss: 4.5315
Profiling... [7168/50176]	Loss: 4.5035
Profiling... [8192/50176]	Loss: 4.5099
Profiling... [9216/50176]	Loss: 4.4738
Profiling... [10240/50176]	Loss: 4.5250
Profiling... [11264/50176]	Loss: 4.4911
Profiling... [12288/50176]	Loss: 4.4535
Profiling... [13312/50176]	Loss: 4.4559
Profile done
epoch 1 train time consumed: 12.61s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.72998026334302,
                        "time": 8.960438112997508,
                        "accuracy": 0.009765625,
                        "total_cost": 105270.36291636333
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6307
Profiling... [2048/50176]	Loss: 4.6160
Profiling... [3072/50176]	Loss: 4.6337
Profiling... [4096/50176]	Loss: 4.6274
Profiling... [5120/50176]	Loss: 4.5486
Profiling... [6144/50176]	Loss: 4.5832
Profiling... [7168/50176]	Loss: 4.5349
Profiling... [8192/50176]	Loss: 4.5129
Profiling... [9216/50176]	Loss: 4.4979
Profiling... [10240/50176]	Loss: 4.4745
Profiling... [11264/50176]	Loss: 4.5251
Profiling... [12288/50176]	Loss: 4.4957
Profiling... [13312/50176]	Loss: 4.4694
Profile done
epoch 1 train time consumed: 13.47s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.80888211792829,
                        "time": 9.572728790997644,
                        "accuracy": 0.009765625,
                        "total_cost": 112541.11143040478
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6328
Profiling... [2048/50176]	Loss: 4.6413
Profiling... [3072/50176]	Loss: 4.6217
Profiling... [4096/50176]	Loss: 4.5919
Profiling... [5120/50176]	Loss: 4.5712
Profiling... [6144/50176]	Loss: 4.5482
Profiling... [7168/50176]	Loss: 4.5325
Profiling... [8192/50176]	Loss: 4.5065
Profiling... [9216/50176]	Loss: 4.4889
Profiling... [10240/50176]	Loss: 4.4895
Profiling... [11264/50176]	Loss: 4.4644
Profiling... [12288/50176]	Loss: 4.4681
Profiling... [13312/50176]	Loss: 4.4739
Profile done
epoch 1 train time consumed: 18.45s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 114.43213428330536,
                        "time": 12.692368005002209,
                        "accuracy": 0.009765625,
                        "total_cost": 148727.2714159658
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6419
Profiling... [2048/50176]	Loss: 4.6131
Profiling... [3072/50176]	Loss: 4.6092
Profiling... [4096/50176]	Loss: 4.6022
Profiling... [5120/50176]	Loss: 4.5439
Profiling... [6144/50176]	Loss: 4.5404
Profiling... [7168/50176]	Loss: 4.5289
Profiling... [8192/50176]	Loss: 4.5319
Profiling... [9216/50176]	Loss: 4.4995
Profiling... [10240/50176]	Loss: 4.5144
Profiling... [11264/50176]	Loss: 4.4540
Profiling... [12288/50176]	Loss: 4.4678
Profiling... [13312/50176]	Loss: 4.4191
Profile done
epoch 1 train time consumed: 12.50s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.57225151192468,
                        "time": 8.809124612002051,
                        "accuracy": 0.009765625,
                        "total_cost": 103350.39904216933
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6305
Profiling... [2048/50176]	Loss: 4.6267
Profiling... [3072/50176]	Loss: 4.6073
Profiling... [4096/50176]	Loss: 4.5923
Profiling... [5120/50176]	Loss: 4.5758
Profiling... [6144/50176]	Loss: 4.5216
Profiling... [7168/50176]	Loss: 4.5337
Profiling... [8192/50176]	Loss: 4.5002
Profiling... [9216/50176]	Loss: 4.4984
Profiling... [10240/50176]	Loss: 4.5175
Profiling... [11264/50176]	Loss: 4.5290
Profiling... [12288/50176]	Loss: 4.4811
Profiling... [13312/50176]	Loss: 4.4644
Profile done
epoch 1 train time consumed: 12.71s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.85067237372515,
                        "time": 8.95440954300284,
                        "accuracy": 0.009765625,
                        "total_cost": 105310.20356849427
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6405
Profiling... [2048/50176]	Loss: 4.6181
Profiling... [3072/50176]	Loss: 4.6204
Profiling... [4096/50176]	Loss: 4.5952
Profiling... [5120/50176]	Loss: 4.5682
Profiling... [6144/50176]	Loss: 4.5407
Profiling... [7168/50176]	Loss: 4.5391
Profiling... [8192/50176]	Loss: 4.5263
Profiling... [9216/50176]	Loss: 4.5125
Profiling... [10240/50176]	Loss: 4.5050
Profiling... [11264/50176]	Loss: 4.4767
Profiling... [12288/50176]	Loss: 4.4442
Profiling... [13312/50176]	Loss: 4.4932
Profile done
epoch 1 train time consumed: 13.62s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.91250451437878,
                        "time": 9.587024717999157,
                        "accuracy": 0.009765625,
                        "total_cost": 112810.9077695016
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6336
Profiling... [2048/50176]	Loss: 4.6120
Profiling... [3072/50176]	Loss: 4.6050
Profiling... [4096/50176]	Loss: 4.5796
Profiling... [5120/50176]	Loss: 4.6018
Profiling... [6144/50176]	Loss: 4.5745
Profiling... [7168/50176]	Loss: 4.5499
Profiling... [8192/50176]	Loss: 4.5448
Profiling... [9216/50176]	Loss: 4.5369
Profiling... [10240/50176]	Loss: 4.5151
Profiling... [11264/50176]	Loss: 4.4975
Profiling... [12288/50176]	Loss: 4.5089
Profiling... [13312/50176]	Loss: 4.5064
Profile done
epoch 1 train time consumed: 23.70s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 114.44404966191918,
                        "time": 17.23570797299908,
                        "accuracy": 0.009765625,
                        "total_cost": 201986.4800481529
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6311
Profiling... [2048/50176]	Loss: 4.6162
Profiling... [3072/50176]	Loss: 4.6251
Profiling... [4096/50176]	Loss: 4.5731
Profiling... [5120/50176]	Loss: 4.5877
Profiling... [6144/50176]	Loss: 4.5575
Profiling... [7168/50176]	Loss: 4.5365
Profiling... [8192/50176]	Loss: 4.5160
Profiling... [9216/50176]	Loss: 4.5331
Profiling... [10240/50176]	Loss: 4.4982
Profiling... [11264/50176]	Loss: 4.4602
Profiling... [12288/50176]	Loss: 4.5026
Profiling... [13312/50176]	Loss: 4.4761
Profile done
epoch 1 train time consumed: 12.57s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.59222119840311,
                        "time": 8.900498057002551,
                        "accuracy": 0.009765625,
                        "total_cost": 104440.61103349693
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6245
Profiling... [2048/50176]	Loss: 4.6266
Profiling... [3072/50176]	Loss: 4.6316
Profiling... [4096/50176]	Loss: 4.6083
Profiling... [5120/50176]	Loss: 4.6294
Profiling... [6144/50176]	Loss: 4.5488
Profiling... [7168/50176]	Loss: 4.5312
Profiling... [8192/50176]	Loss: 4.5252
Profiling... [9216/50176]	Loss: 4.4990
Profiling... [10240/50176]	Loss: 4.4817
Profiling... [11264/50176]	Loss: 4.4802
Profiling... [12288/50176]	Loss: 4.4936
Profiling... [13312/50176]	Loss: 4.4662
Profile done
epoch 1 train time consumed: 12.67s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.84623767729668,
                        "time": 8.923521409000386,
                        "accuracy": 0.009765625,
                        "total_cost": 104942.88493122598
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6264
Profiling... [2048/50176]	Loss: 4.6461
Profiling... [3072/50176]	Loss: 4.6224
Profiling... [4096/50176]	Loss: 4.6044
Profiling... [5120/50176]	Loss: 4.5732
Profiling... [6144/50176]	Loss: 4.5574
Profiling... [7168/50176]	Loss: 4.5376
Profiling... [8192/50176]	Loss: 4.5063
Profiling... [9216/50176]	Loss: 4.5457
Profiling... [10240/50176]	Loss: 4.5082
Profiling... [11264/50176]	Loss: 4.5052
Profiling... [12288/50176]	Loss: 4.4865
Profiling... [13312/50176]	Loss: 4.4679
Profile done
epoch 1 train time consumed: 13.41s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.90713675878712,
                        "time": 9.566505818002042,
                        "accuracy": 0.009765625,
                        "total_cost": 112564.20273488827
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6383
Profiling... [2048/50176]	Loss: 4.6202
Profiling... [3072/50176]	Loss: 4.6172
Profiling... [4096/50176]	Loss: 4.6390
Profiling... [5120/50176]	Loss: 4.6088
Profiling... [6144/50176]	Loss: 4.5517
Profiling... [7168/50176]	Loss: 4.5265
Profiling... [8192/50176]	Loss: 4.5419
Profiling... [9216/50176]	Loss: 4.5279
Profiling... [10240/50176]	Loss: 4.5150
Profiling... [11264/50176]	Loss: 4.4925
Profiling... [12288/50176]	Loss: 4.4646
Profiling... [13312/50176]	Loss: 4.4732
Profile done
epoch 1 train time consumed: 22.96s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 114.47429623301487,
                        "time": 16.474470638997445,
                        "accuracy": 0.009765625,
                        "total_cost": 193116.5114583756
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6368
Profiling... [2048/50176]	Loss: 5.4938
Profiling... [3072/50176]	Loss: 5.1153
Profiling... [4096/50176]	Loss: 4.6687
Profiling... [5120/50176]	Loss: 4.7658
Profiling... [6144/50176]	Loss: 4.6796
Profiling... [7168/50176]	Loss: 4.6663
Profiling... [8192/50176]	Loss: 4.7545
Profiling... [9216/50176]	Loss: 4.7769
Profiling... [10240/50176]	Loss: 4.6193
Profiling... [11264/50176]	Loss: 4.6485
Profiling... [12288/50176]	Loss: 4.5793
Profiling... [13312/50176]	Loss: 4.6026
Profile done
epoch 1 train time consumed: 12.78s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.59527192600513,
                        "time": 9.11230927399447,
                        "accuracy": 0.009765625,
                        "total_cost": 106928.90205463087
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6379
Profiling... [2048/50176]	Loss: 5.5713
Profiling... [3072/50176]	Loss: 5.0652
Profiling... [4096/50176]	Loss: 4.7618
Profiling... [5120/50176]	Loss: 4.6706
Profiling... [6144/50176]	Loss: 4.7736
Profiling... [7168/50176]	Loss: 4.7678
Profiling... [8192/50176]	Loss: 4.6074
Profiling... [9216/50176]	Loss: 4.6143
Profiling... [10240/50176]	Loss: 4.5482
Profiling... [11264/50176]	Loss: 4.5830
Profiling... [12288/50176]	Loss: 4.6376
Profiling... [13312/50176]	Loss: 4.5945
Profile done
epoch 1 train time consumed: 12.55s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.84874109080003,
                        "time": 8.811824081996747,
                        "accuracy": 0.009765625,
                        "total_cost": 103631.55481916631
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6300
Profiling... [2048/50176]	Loss: 5.4733
Profiling... [3072/50176]	Loss: 5.1182
Profiling... [4096/50176]	Loss: 4.7333
Profiling... [5120/50176]	Loss: 4.7187
Profiling... [6144/50176]	Loss: 4.6436
Profiling... [7168/50176]	Loss: 4.7774
Profiling... [8192/50176]	Loss: 4.6931
Profiling... [9216/50176]	Loss: 4.6116
Profiling... [10240/50176]	Loss: 4.7008
Profiling... [11264/50176]	Loss: 4.6151
Profiling... [12288/50176]	Loss: 4.5740
Profiling... [13312/50176]	Loss: 4.5521
Profile done
epoch 1 train time consumed: 13.45s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.90115856653026,
                        "time": 9.601761489000637,
                        "accuracy": 0.009765625,
                        "total_cost": 112973.16038304419
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6505
Profiling... [2048/50176]	Loss: 5.4759
Profiling... [3072/50176]	Loss: 5.1103
Profiling... [4096/50176]	Loss: 4.7693
Profiling... [5120/50176]	Loss: 4.8365
Profiling... [6144/50176]	Loss: 4.7771
Profiling... [7168/50176]	Loss: 4.6856
Profiling... [8192/50176]	Loss: 4.6432
Profiling... [9216/50176]	Loss: 4.6049
Profiling... [10240/50176]	Loss: 4.7387
Profiling... [11264/50176]	Loss: 4.6074
Profiling... [12288/50176]	Loss: 4.6613
Profiling... [13312/50176]	Loss: 4.6615
Profile done
epoch 1 train time consumed: 23.89s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 114.48357238949947,
                        "time": 17.279513467998186,
                        "accuracy": 0.009765625,
                        "total_cost": 202569.7721312155
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6346
Profiling... [2048/50176]	Loss: 5.5029
Profiling... [3072/50176]	Loss: 4.8956
Profiling... [4096/50176]	Loss: 4.8492
Profiling... [5120/50176]	Loss: 4.7307
Profiling... [6144/50176]	Loss: 4.7174
Profiling... [7168/50176]	Loss: 4.6211
Profiling... [8192/50176]	Loss: 4.6114
Profiling... [9216/50176]	Loss: 4.5885
Profiling... [10240/50176]	Loss: 4.5913
Profiling... [11264/50176]	Loss: 4.5817
Profiling... [12288/50176]	Loss: 4.5094
Profiling... [13312/50176]	Loss: 4.4978
Profile done
epoch 1 train time consumed: 13.11s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.58827743659147,
                        "time": 9.236575798000558,
                        "accuracy": 0.009765625,
                        "total_cost": 108380.49895479236
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6303
Profiling... [2048/50176]	Loss: 5.4953
Profiling... [3072/50176]	Loss: 4.8894
Profiling... [4096/50176]	Loss: 4.7677
Profiling... [5120/50176]	Loss: 4.9430
Profiling... [6144/50176]	Loss: 4.6086
Profiling... [7168/50176]	Loss: 4.7651
Profiling... [8192/50176]	Loss: 4.6778
Profiling... [9216/50176]	Loss: 4.5694
Profiling... [10240/50176]	Loss: 4.6322
Profiling... [11264/50176]	Loss: 4.5530
Profiling... [12288/50176]	Loss: 4.5524
Profiling... [13312/50176]	Loss: 4.5012
Profile done
epoch 1 train time consumed: 12.66s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.81727804929085,
                        "time": 8.9449652780022,
                        "accuracy": 0.009765625,
                        "total_cost": 105168.54430368061
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6402
Profiling... [2048/50176]	Loss: 5.5160
Profiling... [3072/50176]	Loss: 5.0399
Profiling... [4096/50176]	Loss: 4.7824
Profiling... [5120/50176]	Loss: 4.6691
Profiling... [6144/50176]	Loss: 4.7052
Profiling... [7168/50176]	Loss: 4.8150
Profiling... [8192/50176]	Loss: 4.8062
Profiling... [9216/50176]	Loss: 4.7420
Profiling... [10240/50176]	Loss: 4.6022
Profiling... [11264/50176]	Loss: 4.5721
Profiling... [12288/50176]	Loss: 4.6676
Profiling... [13312/50176]	Loss: 4.6298
Profile done
epoch 1 train time consumed: 13.46s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.86210259286223,
                        "time": 9.595833557999867,
                        "accuracy": 0.009765625,
                        "total_cost": 112865.03614494833
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6327
Profiling... [2048/50176]	Loss: 5.3663
Profiling... [3072/50176]	Loss: 4.9482
Profiling... [4096/50176]	Loss: 4.6806
Profiling... [5120/50176]	Loss: 4.7196
Profiling... [6144/50176]	Loss: 4.7058
Profiling... [7168/50176]	Loss: 4.7732
Profiling... [8192/50176]	Loss: 4.7274
Profiling... [9216/50176]	Loss: 4.6941
Profiling... [10240/50176]	Loss: 4.6755
Profiling... [11264/50176]	Loss: 4.6938
Profiling... [12288/50176]	Loss: 4.6369
Profiling... [13312/50176]	Loss: 4.5872
Profile done
epoch 1 train time consumed: 23.90s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 114.47045138482562,
                        "time": 17.251679253000475,
                        "accuracy": 0.009765625,
                        "total_cost": 202220.28915068886
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6402
Profiling... [2048/50176]	Loss: 5.5317
Profiling... [3072/50176]	Loss: 5.2110
Profiling... [4096/50176]	Loss: 4.7275
Profiling... [5120/50176]	Loss: 4.6667
Profiling... [6144/50176]	Loss: 4.7013
Profiling... [7168/50176]	Loss: 4.7535
Profiling... [8192/50176]	Loss: 4.6786
Profiling... [9216/50176]	Loss: 4.7312
Profiling... [10240/50176]	Loss: 4.5547
Profiling... [11264/50176]	Loss: 4.5771
Profiling... [12288/50176]	Loss: 4.5668
Profiling... [13312/50176]	Loss: 4.5258
Profile done
epoch 1 train time consumed: 12.47s
Validation Epoch: 0, Average loss: 0.0049, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.5863430038838,
                        "time": 8.791586383995309,
                        "accuracy": 0.009765625,
                        "total_cost": 103157.32305354351
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6328
Profiling... [2048/50176]	Loss: 5.4866
Profiling... [3072/50176]	Loss: 4.8793
Profiling... [4096/50176]	Loss: 4.8247
Profiling... [5120/50176]	Loss: 4.7890
Profiling... [6144/50176]	Loss: 4.8033
Profiling... [7168/50176]	Loss: 4.7566
Profiling... [8192/50176]	Loss: 4.6389
Profiling... [9216/50176]	Loss: 4.5905
Profiling... [10240/50176]	Loss: 4.5777
Profiling... [11264/50176]	Loss: 4.5614
Profiling... [12288/50176]	Loss: 4.6512
Profiling... [13312/50176]	Loss: 4.6038
Profile done
epoch 1 train time consumed: 12.75s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.78806682205064,
                        "time": 9.036879711995425,
                        "accuracy": 0.009765625,
                        "total_cost": 106222.17750972052
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6257
Profiling... [2048/50176]	Loss: 5.5965
Profiling... [3072/50176]	Loss: 4.9607
Profiling... [4096/50176]	Loss: 4.8479
Profiling... [5120/50176]	Loss: 4.7833
Profiling... [6144/50176]	Loss: 4.8134
Profiling... [7168/50176]	Loss: 4.7601
Profiling... [8192/50176]	Loss: 4.6891
Profiling... [9216/50176]	Loss: 4.6861
Profiling... [10240/50176]	Loss: 4.6208
Profiling... [11264/50176]	Loss: 4.5839
Profiling... [12288/50176]	Loss: 4.7942
Profiling... [13312/50176]	Loss: 4.7059
Profile done
epoch 1 train time consumed: 13.57s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.8379299750348,
                        "time": 9.548836545000086,
                        "accuracy": 0.009765625,
                        "total_cost": 112288.62694377196
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6254
Profiling... [2048/50176]	Loss: 5.6155
Profiling... [3072/50176]	Loss: 5.0321
Profiling... [4096/50176]	Loss: 4.9396
Profiling... [5120/50176]	Loss: 4.8687
Profiling... [6144/50176]	Loss: 4.7114
Profiling... [7168/50176]	Loss: 4.6604
Profiling... [8192/50176]	Loss: 4.6993
Profiling... [9216/50176]	Loss: 4.6049
Profiling... [10240/50176]	Loss: 4.5527
Profiling... [11264/50176]	Loss: 4.5802
Profiling... [12288/50176]	Loss: 4.5063
Profiling... [13312/50176]	Loss: 4.5084
Profile done
epoch 1 train time consumed: 21.82s
Validation Epoch: 0, Average loss: 0.0048, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 114.50177518460728,
                        "time": 15.274691193000763,
                        "accuracy": 0.009765625,
                        "total_cost": 179095.4759163161
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6301
Profiling... [2048/50176]	Loss: 7.2018
Profiling... [3072/50176]	Loss: 5.7888
Profiling... [4096/50176]	Loss: 5.0145
Profiling... [5120/50176]	Loss: 4.9664
Profiling... [6144/50176]	Loss: 4.7771
Profiling... [7168/50176]	Loss: 4.7442
Profiling... [8192/50176]	Loss: 4.8076
Profiling... [9216/50176]	Loss: 4.7432
Profiling... [10240/50176]	Loss: 4.6864
Profiling... [11264/50176]	Loss: 4.7075
Profiling... [12288/50176]	Loss: 4.6519
Profiling... [13312/50176]	Loss: 4.6798
Profile done
epoch 1 train time consumed: 12.74s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.61206802118649,
                        "time": 8.914094914995076,
                        "accuracy": 0.009765625,
                        "total_cost": 104618.27612106016
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6398
Profiling... [2048/50176]	Loss: 6.9931
Profiling... [3072/50176]	Loss: 5.8094
Profiling... [4096/50176]	Loss: 5.1943
Profiling... [5120/50176]	Loss: 4.9056
Profiling... [6144/50176]	Loss: 5.0243
Profiling... [7168/50176]	Loss: 4.7214
Profiling... [8192/50176]	Loss: 4.6640
Profiling... [9216/50176]	Loss: 4.8394
Profiling... [10240/50176]	Loss: 4.7467
Profiling... [11264/50176]	Loss: 4.6390
Profiling... [12288/50176]	Loss: 4.6475
Profiling... [13312/50176]	Loss: 4.6053
Profile done
epoch 1 train time consumed: 12.53s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.8194482085653,
                        "time": 8.797455434003496,
                        "accuracy": 0.009765625,
                        "total_cost": 103436.18340574471
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6411
Profiling... [2048/50176]	Loss: 7.2555
Profiling... [3072/50176]	Loss: 5.5280
Profiling... [4096/50176]	Loss: 4.8776
Profiling... [5120/50176]	Loss: 4.7290
Profiling... [6144/50176]	Loss: 4.6699
Profiling... [7168/50176]	Loss: 4.7412
Profiling... [8192/50176]	Loss: 4.6562
Profiling... [9216/50176]	Loss: 4.7875
Profiling... [10240/50176]	Loss: 4.9883
Profiling... [11264/50176]	Loss: 4.8793
Profiling... [12288/50176]	Loss: 4.7836
Profiling... [13312/50176]	Loss: 4.6494
Profile done
epoch 1 train time consumed: 13.44s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0124
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.86827241996922,
                        "time": 9.585626673004299,
                        "accuracy": 0.01240234375,
                        "total_cost": 88780.34653657951
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6409
Profiling... [2048/50176]	Loss: 7.3005
Profiling... [3072/50176]	Loss: 5.6774
Profiling... [4096/50176]	Loss: 5.1096
Profiling... [5120/50176]	Loss: 5.1248
Profiling... [6144/50176]	Loss: 4.8923
Profiling... [7168/50176]	Loss: 4.6983
Profiling... [8192/50176]	Loss: 4.6905
Profiling... [9216/50176]	Loss: 4.8352
Profiling... [10240/50176]	Loss: 4.8882
Profiling... [11264/50176]	Loss: 4.7676
Profiling... [12288/50176]	Loss: 4.7613
Profiling... [13312/50176]	Loss: 4.7399
Profile done
epoch 1 train time consumed: 22.42s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 114.54371380984597,
                        "time": 15.908553546003532,
                        "accuracy": 0.009765625,
                        "total_cost": 186595.81998100877
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6430
Profiling... [2048/50176]	Loss: 7.0687
Profiling... [3072/50176]	Loss: 5.3522
Profiling... [4096/50176]	Loss: 4.7939
Profiling... [5120/50176]	Loss: 5.0093
Profiling... [6144/50176]	Loss: 5.7502
Profiling... [7168/50176]	Loss: 5.2887
Profiling... [8192/50176]	Loss: 5.1876
Profiling... [9216/50176]	Loss: 4.8732
Profiling... [10240/50176]	Loss: 4.7843
Profiling... [11264/50176]	Loss: 4.8495
Profiling... [12288/50176]	Loss: 4.6786
Profiling... [13312/50176]	Loss: 4.6412
Profile done
epoch 1 train time consumed: 12.67s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.64760815373874,
                        "time": 8.918373667998821,
                        "accuracy": 0.009765625,
                        "total_cost": 104700.9494689127
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6386
Profiling... [2048/50176]	Loss: 7.4023
Profiling... [3072/50176]	Loss: 6.1716
Profiling... [4096/50176]	Loss: 5.1032
Profiling... [5120/50176]	Loss: 5.0419
Profiling... [6144/50176]	Loss: 4.8310
Profiling... [7168/50176]	Loss: 4.8447
Profiling... [8192/50176]	Loss: 4.7052
Profiling... [9216/50176]	Loss: 4.6907
Profiling... [10240/50176]	Loss: 4.7872
Profiling... [11264/50176]	Loss: 4.7514
Profiling... [12288/50176]	Loss: 4.7113
Profiling... [13312/50176]	Loss: 4.6389
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.82818472702485,
                        "time": 9.052642615999503,
                        "accuracy": 0.009765625,
                        "total_cost": 106444.64830235939
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6235
Profiling... [2048/50176]	Loss: 7.4664
Profiling... [3072/50176]	Loss: 5.7249
Profiling... [4096/50176]	Loss: 4.9737
Profiling... [5120/50176]	Loss: 4.7063
Profiling... [6144/50176]	Loss: 4.9088
Profiling... [7168/50176]	Loss: 4.7031
Profiling... [8192/50176]	Loss: 4.7705
Profiling... [9216/50176]	Loss: 4.9860
Profiling... [10240/50176]	Loss: 4.8290
Profiling... [11264/50176]	Loss: 4.9267
Profiling... [12288/50176]	Loss: 4.6564
Profiling... [13312/50176]	Loss: 4.7441
Profile done
epoch 1 train time consumed: 13.48s
Validation Epoch: 0, Average loss: 0.0056, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.8718668209111,
                        "time": 9.580321023997385,
                        "accuracy": 0.009765625,
                        "total_cost": 112692.15854286871
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6448
Profiling... [2048/50176]	Loss: 7.4720
Profiling... [3072/50176]	Loss: 6.0459
Profiling... [4096/50176]	Loss: 5.2277
Profiling... [5120/50176]	Loss: 4.7953
Profiling... [6144/50176]	Loss: 4.7662
Profiling... [7168/50176]	Loss: 4.7912
Profiling... [8192/50176]	Loss: 5.3701
Profiling... [9216/50176]	Loss: 4.7139
Profiling... [10240/50176]	Loss: 4.6729
Profiling... [11264/50176]	Loss: 4.5962
Profiling... [12288/50176]	Loss: 4.6249
Profiling... [13312/50176]	Loss: 5.0316
Profile done
epoch 1 train time consumed: 21.09s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 114.57627991468556,
                        "time": 14.638563904998591,
                        "accuracy": 0.0099609375,
                        "total_cost": 168380.9576687065
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6345
Profiling... [2048/50176]	Loss: 7.1968
Profiling... [3072/50176]	Loss: 5.4608
Profiling... [4096/50176]	Loss: 5.0142
Profiling... [5120/50176]	Loss: 5.0129
Profiling... [6144/50176]	Loss: 4.8451
Profiling... [7168/50176]	Loss: 5.1838
Profiling... [8192/50176]	Loss: 4.6819
Profiling... [9216/50176]	Loss: 4.7821
Profiling... [10240/50176]	Loss: 4.7680
Profiling... [11264/50176]	Loss: 4.7611
Profiling... [12288/50176]	Loss: 4.7514
Profiling... [13312/50176]	Loss: 4.6798
Profile done
epoch 1 train time consumed: 12.59s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.67683227232061,
                        "time": 8.925167617002444,
                        "accuracy": 0.009765625,
                        "total_cost": 104807.41886129527
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6353
Profiling... [2048/50176]	Loss: 7.4736
Profiling... [3072/50176]	Loss: 6.3259
Profiling... [4096/50176]	Loss: 5.1966
Profiling... [5120/50176]	Loss: 4.9188
Profiling... [6144/50176]	Loss: 4.7172
Profiling... [7168/50176]	Loss: 4.7515
Profiling... [8192/50176]	Loss: 4.6749
Profiling... [9216/50176]	Loss: 4.6716
Profiling... [10240/50176]	Loss: 4.6761
Profiling... [11264/50176]	Loss: 4.6995
Profiling... [12288/50176]	Loss: 4.8242
Profiling... [13312/50176]	Loss: 4.6726
Profile done
epoch 1 train time consumed: 12.85s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.84962593939272,
                        "time": 8.897647253004834,
                        "accuracy": 0.009765625,
                        "total_cost": 104641.6853758229
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6379
Profiling... [2048/50176]	Loss: 7.2995
Profiling... [3072/50176]	Loss: 5.7653
Profiling... [4096/50176]	Loss: 5.2326
Profiling... [5120/50176]	Loss: 5.0261
Profiling... [6144/50176]	Loss: 4.8189
Profiling... [7168/50176]	Loss: 4.6686
Profiling... [8192/50176]	Loss: 4.6484
Profiling... [9216/50176]	Loss: 4.7763
Profiling... [10240/50176]	Loss: 4.7247
Profiling... [11264/50176]	Loss: 4.7797
Profiling... [12288/50176]	Loss: 4.6509
Profiling... [13312/50176]	Loss: 4.6557
Profile done
epoch 1 train time consumed: 13.66s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.88862773246069,
                        "time": 9.562561885002651,
                        "accuracy": 0.009765625,
                        "total_cost": 112499.67232764796
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6425
Profiling... [2048/50176]	Loss: 7.0661
Profiling... [3072/50176]	Loss: 5.9065
Profiling... [4096/50176]	Loss: 5.1244
Profiling... [5120/50176]	Loss: 5.0452
Profiling... [6144/50176]	Loss: 4.9034
Profiling... [7168/50176]	Loss: 4.7609
Profiling... [8192/50176]	Loss: 4.7842
Profiling... [9216/50176]	Loss: 4.6564
Profiling... [10240/50176]	Loss: 4.6524
Profiling... [11264/50176]	Loss: 4.7167
Profiling... [12288/50176]	Loss: 4.7496
Profiling... [13312/50176]	Loss: 4.5868
Profile done
epoch 1 train time consumed: 21.78s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0104
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 114.60478777370649,
                        "time": 15.919455893999839,
                        "accuracy": 0.01044921875,
                        "total_cost": 174601.17429398562
                    },
                    
[Training Loop] The optimal parameters are lr: 0.01 dr: 0.25 bs: 128 pl: 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[GPU_0] Set GPU power limit to 125W.
[GPU_0] Set GPU power limit to 125W.
Training Epoch: 0 [128/50048]	Loss: 4.6314
Training Epoch: 0 [256/50048]	Loss: 7.7290
Training Epoch: 0 [384/50048]	Loss: 8.0687
Training Epoch: 0 [512/50048]	Loss: 6.0582
Training Epoch: 0 [640/50048]	Loss: 5.1533
Training Epoch: 0 [768/50048]	Loss: 5.6123
Training Epoch: 0 [896/50048]	Loss: 5.0067
Training Epoch: 0 [1024/50048]	Loss: 5.1725
Training Epoch: 0 [1152/50048]	Loss: 5.0068
Training Epoch: 0 [1280/50048]	Loss: 4.8344
Training Epoch: 0 [1408/50048]	Loss: 4.7646
Training Epoch: 0 [1536/50048]	Loss: 4.6916
Training Epoch: 0 [1664/50048]	Loss: 4.7270
Training Epoch: 0 [1792/50048]	Loss: 4.7496
Training Epoch: 0 [1920/50048]	Loss: 4.6806
Training Epoch: 0 [2048/50048]	Loss: 4.8325
Training Epoch: 0 [2176/50048]	Loss: 4.6373
Training Epoch: 0 [2304/50048]	Loss: 4.6654
Training Epoch: 0 [2432/50048]	Loss: 4.7689
Training Epoch: 0 [2560/50048]	Loss: 4.6644
Training Epoch: 0 [2688/50048]	Loss: 4.6435
Training Epoch: 0 [2816/50048]	Loss: 4.6314
Training Epoch: 0 [2944/50048]	Loss: 4.6227
Training Epoch: 0 [3072/50048]	Loss: 4.6292
Training Epoch: 0 [3200/50048]	Loss: 4.5996
Training Epoch: 0 [3328/50048]	Loss: 4.5901
Training Epoch: 0 [3456/50048]	Loss: 4.5906
Training Epoch: 0 [3584/50048]	Loss: 4.6034
Training Epoch: 0 [3712/50048]	Loss: 4.6272
Training Epoch: 0 [3840/50048]	Loss: 4.6097
Training Epoch: 0 [3968/50048]	Loss: 4.5926
Training Epoch: 0 [4096/50048]	Loss: 4.6093
Training Epoch: 0 [4224/50048]	Loss: 4.6123
Training Epoch: 0 [4352/50048]	Loss: 4.6151
Training Epoch: 0 [4480/50048]	Loss: 4.5954
Training Epoch: 0 [4608/50048]	Loss: 4.6164
Training Epoch: 0 [4736/50048]	Loss: 4.5929
Training Epoch: 0 [4864/50048]	Loss: 4.6081
Training Epoch: 0 [4992/50048]	Loss: 4.5903
Training Epoch: 0 [5120/50048]	Loss: 4.6177
Training Epoch: 0 [5248/50048]	Loss: 4.5457
Training Epoch: 0 [5376/50048]	Loss: 4.5811
Training Epoch: 0 [5504/50048]	Loss: 4.6068
Training Epoch: 0 [5632/50048]	Loss: 4.5950
Training Epoch: 0 [5760/50048]	Loss: 4.5627
Training Epoch: 0 [5888/50048]	Loss: 4.6418
Training Epoch: 0 [6016/50048]	Loss: 4.5828
Training Epoch: 0 [6144/50048]	Loss: 4.5844
Training Epoch: 0 [6272/50048]	Loss: 4.6025
Training Epoch: 0 [6400/50048]	Loss: 4.6210
Training Epoch: 0 [6528/50048]	Loss: 4.5735
Training Epoch: 0 [6656/50048]	Loss: 4.5945
Training Epoch: 0 [6784/50048]	Loss: 4.5845
Training Epoch: 0 [6912/50048]	Loss: 4.5935
Training Epoch: 0 [7040/50048]	Loss: 4.5617
Training Epoch: 0 [7168/50048]	Loss: 4.5573
Training Epoch: 0 [7296/50048]	Loss: 4.5594
Training Epoch: 0 [7424/50048]	Loss: 4.5534
Training Epoch: 0 [7552/50048]	Loss: 4.5604
Training Epoch: 0 [7680/50048]	Loss: 4.5828
Training Epoch: 0 [7808/50048]	Loss: 4.5926
Training Epoch: 0 [7936/50048]	Loss: 4.5541
Training Epoch: 0 [8064/50048]	Loss: 4.5708
Training Epoch: 0 [8192/50048]	Loss: 4.5564
Training Epoch: 0 [8320/50048]	Loss: 4.5245
Training Epoch: 0 [8448/50048]	Loss: 4.5422
Training Epoch: 0 [8576/50048]	Loss: 4.5040
Training Epoch: 0 [8704/50048]	Loss: 4.6084
Training Epoch: 0 [8832/50048]	Loss: 4.4937
Training Epoch: 0 [8960/50048]	Loss: 4.5713
Training Epoch: 0 [9088/50048]	Loss: 4.5084
Training Epoch: 0 [9216/50048]	Loss: 4.4862
Training Epoch: 0 [9344/50048]	Loss: 4.5223
Training Epoch: 0 [9472/50048]	Loss: 4.4916
Training Epoch: 0 [9600/50048]	Loss: 4.4329
Training Epoch: 0 [9728/50048]	Loss: 4.4750
Training Epoch: 0 [9856/50048]	Loss: 4.4704
Training Epoch: 0 [9984/50048]	Loss: 4.4894
Training Epoch: 0 [10112/50048]	Loss: 4.5378
Training Epoch: 0 [10240/50048]	Loss: 4.4455
Training Epoch: 0 [10368/50048]	Loss: 4.4983
Training Epoch: 0 [10496/50048]	Loss: 4.5168
Training Epoch: 0 [10624/50048]	Loss: 4.5532
Training Epoch: 0 [10752/50048]	Loss: 4.4548
Training Epoch: 0 [10880/50048]	Loss: 4.4257
Training Epoch: 0 [11008/50048]	Loss: 4.4719
Training Epoch: 0 [11136/50048]	Loss: 4.4730
Training Epoch: 0 [11264/50048]	Loss: 4.4463
Training Epoch: 0 [11392/50048]	Loss: 4.4939
Training Epoch: 0 [11520/50048]	Loss: 4.4534
Training Epoch: 0 [11648/50048]	Loss: 4.2964
Training Epoch: 0 [11776/50048]	Loss: 4.3564
Training Epoch: 0 [11904/50048]	Loss: 4.4066
Training Epoch: 0 [12032/50048]	Loss: 4.3824
Training Epoch: 0 [12160/50048]	Loss: 4.4238
Training Epoch: 0 [12288/50048]	Loss: 4.3512
Training Epoch: 0 [12416/50048]	Loss: 4.3657
Training Epoch: 0 [12544/50048]	Loss: 4.4025
Training Epoch: 0 [12672/50048]	Loss: 4.2505
Training Epoch: 0 [12800/50048]	Loss: 4.3357
Training Epoch: 0 [12928/50048]	Loss: 4.3832
Training Epoch: 0 [13056/50048]	Loss: 4.4110
Training Epoch: 0 [13184/50048]	Loss: 4.3996
Training Epoch: 0 [13312/50048]	Loss: 4.3537
Training Epoch: 0 [13440/50048]	Loss: 4.4633
Training Epoch: 0 [13568/50048]	Loss: 4.2958
Training Epoch: 0 [13696/50048]	Loss: 4.3583
Training Epoch: 0 [13824/50048]	Loss: 4.3162
Training Epoch: 0 [13952/50048]	Loss: 4.3772
Training Epoch: 0 [14080/50048]	Loss: 4.3940
Training Epoch: 0 [14208/50048]	Loss: 4.4106
Training Epoch: 0 [14336/50048]	Loss: 4.3432
Training Epoch: 0 [14464/50048]	Loss: 4.3388
Training Epoch: 0 [14592/50048]	Loss: 4.3570
Training Epoch: 0 [14720/50048]	Loss: 4.3692
Training Epoch: 0 [14848/50048]	Loss: 4.3476
Training Epoch: 0 [14976/50048]	Loss: 4.3038
Training Epoch: 0 [15104/50048]	Loss: 4.2810
Training Epoch: 0 [15232/50048]	Loss: 4.3714
Training Epoch: 0 [15360/50048]	Loss: 4.2243
Training Epoch: 0 [15488/50048]	Loss: 4.3838
Training Epoch: 0 [15616/50048]	Loss: 4.2765
Training Epoch: 0 [15744/50048]	Loss: 4.3578
Training Epoch: 0 [15872/50048]	Loss: 4.3472
Training Epoch: 0 [16000/50048]	Loss: 4.1167
Training Epoch: 0 [16128/50048]	Loss: 4.2581
Training Epoch: 0 [16256/50048]	Loss: 4.2756
Training Epoch: 0 [16384/50048]	Loss: 4.3204
Training Epoch: 0 [16512/50048]	Loss: 4.2835
Training Epoch: 0 [16640/50048]	Loss: 4.1129
Training Epoch: 0 [16768/50048]	Loss: 4.2683
Training Epoch: 0 [16896/50048]	Loss: 4.2698
Training Epoch: 0 [17024/50048]	Loss: 4.4145
Training Epoch: 0 [17152/50048]	Loss: 4.3786
Training Epoch: 0 [17280/50048]	Loss: 4.2378
Training Epoch: 0 [17408/50048]	Loss: 4.2012
Training Epoch: 0 [17536/50048]	Loss: 4.2904
Training Epoch: 0 [17664/50048]	Loss: 4.2069
Training Epoch: 0 [17792/50048]	Loss: 4.3015
Training Epoch: 0 [17920/50048]	Loss: 4.1982
Training Epoch: 0 [18048/50048]	Loss: 4.1955
Training Epoch: 0 [18176/50048]	Loss: 4.2335
Training Epoch: 0 [18304/50048]	Loss: 4.2320
Training Epoch: 0 [18432/50048]	Loss: 4.2062
Training Epoch: 0 [18560/50048]	Loss: 4.1325
Training Epoch: 0 [18688/50048]	Loss: 4.1597
Training Epoch: 0 [18816/50048]	Loss: 4.1621
Training Epoch: 0 [18944/50048]	Loss: 4.0593
Training Epoch: 0 [19072/50048]	Loss: 4.1425
Training Epoch: 0 [19200/50048]	Loss: 4.1613
Training Epoch: 0 [19328/50048]	Loss: 4.1599
Training Epoch: 0 [19456/50048]	Loss: 4.2888
Training Epoch: 0 [19584/50048]	Loss: 4.0420
Training Epoch: 0 [19712/50048]	Loss: 4.2218
Training Epoch: 0 [19840/50048]	Loss: 4.2331
Training Epoch: 0 [19968/50048]	Loss: 4.2583
Training Epoch: 0 [20096/50048]	Loss: 4.1489
Training Epoch: 0 [20224/50048]	Loss: 4.1007
Training Epoch: 0 [20352/50048]	Loss: 4.1297
Training Epoch: 0 [20480/50048]	Loss: 4.0413
Training Epoch: 0 [20608/50048]	Loss: 4.0136
Training Epoch: 0 [20736/50048]	Loss: 4.1684
Training Epoch: 0 [20864/50048]	Loss: 4.2240
Training Epoch: 0 [20992/50048]	Loss: 4.1782
Training Epoch: 0 [21120/50048]	Loss: 4.0446
Training Epoch: 0 [21248/50048]	Loss: 4.1254
Training Epoch: 0 [21376/50048]	Loss: 4.0903
Training Epoch: 0 [21504/50048]	Loss: 4.1266
Training Epoch: 0 [21632/50048]	Loss: 4.1366
Training Epoch: 0 [21760/50048]	Loss: 4.0889
Training Epoch: 0 [21888/50048]	Loss: 4.0522
Training Epoch: 0 [22016/50048]	Loss: 4.0859
Training Epoch: 0 [22144/50048]	Loss: 4.0926
Training Epoch: 0 [22272/50048]	Loss: 4.3953
Training Epoch: 0 [22400/50048]	Loss: 4.2053
Training Epoch: 0 [22528/50048]	Loss: 4.1634
Training Epoch: 0 [22656/50048]	Loss: 4.1342
Training Epoch: 0 [22784/50048]	Loss: 4.2001
Training Epoch: 0 [22912/50048]	Loss: 4.0921
Training Epoch: 0 [23040/50048]	Loss: 4.1003
Training Epoch: 0 [23168/50048]	Loss: 4.0767
Training Epoch: 0 [23296/50048]	Loss: 4.0869
Training Epoch: 0 [23424/50048]	Loss: 4.1475
Training Epoch: 0 [23552/50048]	Loss: 4.0548
Training Epoch: 0 [23680/50048]	Loss: 4.0501
Training Epoch: 0 [23808/50048]	Loss: 4.0907
Training Epoch: 0 [23936/50048]	Loss: 4.1015
Training Epoch: 0 [24064/50048]	Loss: 4.0112
Training Epoch: 0 [24192/50048]	Loss: 4.0545
Training Epoch: 0 [24320/50048]	Loss: 4.1478
Training Epoch: 0 [24448/50048]	Loss: 4.0005
Training Epoch: 0 [24576/50048]	Loss: 4.0862
Training Epoch: 0 [24704/50048]	Loss: 4.0742
Training Epoch: 0 [24832/50048]	Loss: 4.1373
Training Epoch: 0 [24960/50048]	Loss: 4.1418
Training Epoch: 0 [25088/50048]	Loss: 4.0587
Training Epoch: 0 [25216/50048]	Loss: 4.0661
Training Epoch: 0 [25344/50048]	Loss: 4.2737
Training Epoch: 0 [25472/50048]	Loss: 4.0069
Training Epoch: 0 [25600/50048]	Loss: 4.2662
Training Epoch: 0 [25728/50048]	Loss: 4.0465
Training Epoch: 0 [25856/50048]	Loss: 4.0786
Training Epoch: 0 [25984/50048]	Loss: 4.1388
Training Epoch: 0 [26112/50048]	Loss: 4.1687
Training Epoch: 0 [26240/50048]	Loss: 4.0785
Training Epoch: 0 [26368/50048]	Loss: 4.1050
Training Epoch: 0 [26496/50048]	Loss: 4.1854
Training Epoch: 0 [26624/50048]	Loss: 4.0573
Training Epoch: 0 [26752/50048]	Loss: 3.8703
Training Epoch: 0 [26880/50048]	Loss: 4.0070
Training Epoch: 0 [27008/50048]	Loss: 4.0490
Training Epoch: 0 [27136/50048]	Loss: 3.9589
Training Epoch: 0 [27264/50048]	Loss: 3.9359
Training Epoch: 0 [27392/50048]	Loss: 3.8624
Training Epoch: 0 [27520/50048]	Loss: 4.0200
Training Epoch: 0 [27648/50048]	Loss: 4.1383
Training Epoch: 0 [27776/50048]	Loss: 4.0829
Training Epoch: 0 [27904/50048]	Loss: 3.9749
Training Epoch: 0 [28032/50048]	Loss: 3.9492
Training Epoch: 0 [28160/50048]	Loss: 4.0382
Training Epoch: 0 [28288/50048]	Loss: 4.0288
Training Epoch: 0 [28416/50048]	Loss: 4.2566
Training Epoch: 0 [28544/50048]	Loss: 4.1607
Training Epoch: 0 [28672/50048]	Loss: 3.9839
Training Epoch: 0 [28800/50048]	Loss: 4.0042
Training Epoch: 0 [28928/50048]	Loss: 3.9674
Training Epoch: 0 [29056/50048]	Loss: 4.1554
Training Epoch: 0 [29184/50048]	Loss: 3.9821
Training Epoch: 0 [29312/50048]	Loss: 4.1506
Training Epoch: 0 [29440/50048]	Loss: 4.1708
Training Epoch: 0 [29568/50048]	Loss: 4.2350
Training Epoch: 0 [29696/50048]	Loss: 3.8462
Training Epoch: 0 [29824/50048]	Loss: 4.0599
Training Epoch: 0 [29952/50048]	Loss: 3.9442
Training Epoch: 0 [30080/50048]	Loss: 4.1418
Training Epoch: 0 [30208/50048]	Loss: 4.0216
Training Epoch: 0 [30336/50048]	Loss: 3.8482
Training Epoch: 0 [30464/50048]	Loss: 3.9401
Training Epoch: 0 [30592/50048]	Loss: 4.2624
Training Epoch: 0 [30720/50048]	Loss: 3.9141
Training Epoch: 0 [30848/50048]	Loss: 3.8581
Training Epoch: 0 [30976/50048]	Loss: 4.0548
Training Epoch: 0 [31104/50048]	Loss: 4.0085
Training Epoch: 0 [31232/50048]	Loss: 3.9421
Training Epoch: 0 [31360/50048]	Loss: 3.8691
Training Epoch: 0 [31488/50048]	Loss: 3.9434
Training Epoch: 0 [31616/50048]	Loss: 4.0065
Training Epoch: 0 [31744/50048]	Loss: 3.9020
Training Epoch: 0 [31872/50048]	Loss: 3.8092
Training Epoch: 0 [32000/50048]	Loss: 3.9592
Training Epoch: 0 [32128/50048]	Loss: 3.8443
Training Epoch: 0 [32256/50048]	Loss: 4.0356
Training Epoch: 0 [32384/50048]	Loss: 3.9634
Training Epoch: 0 [32512/50048]	Loss: 4.0591
Training Epoch: 0 [32640/50048]	Loss: 3.9046
Training Epoch: 0 [32768/50048]	Loss: 3.8576
Training Epoch: 0 [32896/50048]	Loss: 3.9415
Training Epoch: 0 [33024/50048]	Loss: 3.8525
Training Epoch: 0 [33152/50048]	Loss: 3.7353
Training Epoch: 0 [33280/50048]	Loss: 3.9795
Training Epoch: 0 [33408/50048]	Loss: 3.8253
Training Epoch: 0 [33536/50048]	Loss: 3.9126
Training Epoch: 0 [33664/50048]	Loss: 4.0646
Training Epoch: 0 [33792/50048]	Loss: 3.8957
Training Epoch: 0 [33920/50048]	Loss: 3.9929
Training Epoch: 0 [34048/50048]	Loss: 3.7907
Training Epoch: 0 [34176/50048]	Loss: 3.9595
Training Epoch: 0 [34304/50048]	Loss: 3.7941
Training Epoch: 0 [34432/50048]	Loss: 3.8560
Training Epoch: 0 [34560/50048]	Loss: 3.8181
Training Epoch: 0 [34688/50048]	Loss: 4.0743
Training Epoch: 0 [34816/50048]	Loss: 4.0153
Training Epoch: 0 [34944/50048]	Loss: 4.0070
Training Epoch: 0 [35072/50048]	Loss: 3.8937
Training Epoch: 0 [35200/50048]	Loss: 3.9270
Training Epoch: 0 [35328/50048]	Loss: 3.8516
Training Epoch: 0 [35456/50048]	Loss: 3.9042
Training Epoch: 0 [35584/50048]	Loss: 3.9841
Training Epoch: 0 [35712/50048]	Loss: 3.9840
Training Epoch: 0 [35840/50048]	Loss: 3.8869
Training Epoch: 0 [35968/50048]	Loss: 3.8825
Training Epoch: 0 [36096/50048]	Loss: 3.9060
Training Epoch: 0 [36224/50048]	Loss: 3.7799
Training Epoch: 0 [36352/50048]	Loss: 3.8587
Training Epoch: 0 [36480/50048]	Loss: 3.9483
Training Epoch: 0 [36608/50048]	Loss: 3.8709
Training Epoch: 0 [36736/50048]	Loss: 3.9551
Training Epoch: 0 [36864/50048]	Loss: 3.8848
Training Epoch: 0 [36992/50048]	Loss: 3.9958
Training Epoch: 0 [37120/50048]	Loss: 3.9732
Training Epoch: 0 [37248/50048]	Loss: 3.8638
Training Epoch: 0 [37376/50048]	Loss: 3.8345
Training Epoch: 0 [37504/50048]	Loss: 3.9601
Training Epoch: 0 [37632/50048]	Loss: 3.8823
Training Epoch: 0 [37760/50048]	Loss: 3.8033
Training Epoch: 0 [37888/50048]	Loss: 4.0266
Training Epoch: 0 [38016/50048]	Loss: 3.9446
Training Epoch: 0 [38144/50048]	Loss: 3.9382
Training Epoch: 0 [38272/50048]	Loss: 3.7718
Training Epoch: 0 [38400/50048]	Loss: 3.7978
Training Epoch: 0 [38528/50048]	Loss: 4.0140
Training Epoch: 0 [38656/50048]	Loss: 3.9016
Training Epoch: 0 [38784/50048]	Loss: 3.8397
Training Epoch: 0 [38912/50048]	Loss: 4.0736
Training Epoch: 0 [39040/50048]	Loss: 3.9271
Training Epoch: 0 [39168/50048]	Loss: 3.7040
Training Epoch: 0 [39296/50048]	Loss: 3.7452
Training Epoch: 0 [39424/50048]	Loss: 3.8372
Training Epoch: 0 [39552/50048]	Loss: 3.8982
Training Epoch: 0 [39680/50048]	Loss: 3.8247
Training Epoch: 0 [39808/50048]	Loss: 3.9280
Training Epoch: 0 [39936/50048]	Loss: 3.9226
Training Epoch: 0 [40064/50048]	Loss: 3.6148
Training Epoch: 0 [40192/50048]	Loss: 3.9377
Training Epoch: 0 [40320/50048]	Loss: 3.8559
Training Epoch: 0 [40448/50048]	Loss: 3.8345
Training Epoch: 0 [40576/50048]	Loss: 3.8370
Training Epoch: 0 [40704/50048]	Loss: 4.1031
Training Epoch: 0 [40832/50048]	Loss: 4.0331
Training Epoch: 0 [40960/50048]	Loss: 3.8358
Training Epoch: 0 [41088/50048]	Loss: 3.9340
Training Epoch: 0 [41216/50048]	Loss: 3.9758
Training Epoch: 0 [41344/50048]	Loss: 3.8988
Training Epoch: 0 [41472/50048]	Loss: 3.8862
Training Epoch: 0 [41600/50048]	Loss: 3.7405
Training Epoch: 0 [41728/50048]	Loss: 3.9370
Training Epoch: 0 [41856/50048]	Loss: 3.9036
Training Epoch: 0 [41984/50048]	Loss: 3.8744
Training Epoch: 0 [42112/50048]	Loss: 3.8049
Training Epoch: 0 [42240/50048]	Loss: 3.9771
Training Epoch: 0 [42368/50048]	Loss: 3.8636
Training Epoch: 0 [42496/50048]	Loss: 3.8664
Training Epoch: 0 [42624/50048]	Loss: 3.9241
Training Epoch: 0 [42752/50048]	Loss: 3.9953
Training Epoch: 0 [42880/50048]	Loss: 3.8554
Training Epoch: 0 [43008/50048]	Loss: 3.7918
Training Epoch: 0 [43136/50048]	Loss: 3.9901
Training Epoch: 0 [43264/50048]	Loss: 3.7967
Training Epoch: 0 [43392/50048]	Loss: 3.8357
Training Epoch: 0 [43520/50048]	Loss: 3.7086
Training Epoch: 0 [43648/50048]	Loss: 3.7525
Training Epoch: 0 [43776/50048]	Loss: 3.8868
Training Epoch: 0 [43904/50048]	Loss: 3.6925
Training Epoch: 0 [44032/50048]	Loss: 3.8773
Training Epoch: 0 [44160/50048]	Loss: 3.9745
Training Epoch: 0 [44288/50048]	Loss: 3.8469
Training Epoch: 0 [44416/50048]	Loss: 3.9378
Training Epoch: 0 [44544/50048]	Loss: 3.7832
Training Epoch: 0 [44672/50048]	Loss: 3.9638
Training Epoch: 0 [44800/50048]	Loss: 3.8819
Training Epoch: 0 [44928/50048]	Loss: 3.8389
Training Epoch: 0 [45056/50048]	Loss: 3.5983
Training Epoch: 0 [45184/50048]	Loss: 3.7784
Training Epoch: 0 [45312/50048]	Loss: 3.8367
Training Epoch: 0 [45440/50048]	Loss: 3.9294
Training Epoch: 0 [45568/50048]	Loss: 3.5704
Training Epoch: 0 [45696/50048]	Loss: 3.9448
Training Epoch: 0 [45824/50048]	Loss: 3.8846
Training Epoch: 0 [45952/50048]	Loss: 3.8225
Training Epoch: 0 [46080/50048]	Loss: 3.6390
Training Epoch: 0 [46208/50048]	Loss: 4.0015
Training Epoch: 0 [46336/50048]	Loss: 3.6524
Training Epoch: 0 [46464/50048]	Loss: 3.7754
Training Epoch: 0 [46592/50048]	Loss: 3.6586
Training Epoch: 0 [46720/50048]	Loss: 3.8362
Training Epoch: 0 [46848/50048]	Loss: 3.7878
Training Epoch: 0 [46976/50048]	Loss: 3.8520
Training Epoch: 0 [47104/50048]	Loss: 3.8456
Training Epoch: 0 [47232/50048]	Loss: 3.7492
Training Epoch: 0 [47360/50048]	Loss: 3.6413
Training Epoch: 0 [47488/50048]	Loss: 3.9075
Training Epoch: 0 [47616/50048]	Loss: 3.6582
Training Epoch: 0 [47744/50048]	Loss: 3.6866
Training Epoch: 0 [47872/50048]	Loss: 3.7571
Training Epoch: 0 [48000/50048]	Loss: 3.6819
Training Epoch: 0 [48128/50048]	Loss: 3.7919
Training Epoch: 0 [48256/50048]	Loss: 3.7561
Training Epoch: 0 [48384/50048]	Loss: 3.7102
Training Epoch: 0 [48512/50048]	Loss: 3.7558
Training Epoch: 0 [48640/50048]	Loss: 3.7070
Training Epoch: 0 [48768/50048]	Loss: 3.7609
Training Epoch: 0 [48896/50048]	Loss: 3.8601
Training Epoch: 0 [49024/50048]	Loss: 3.5793
Training Epoch: 0 [49152/50048]	Loss: 3.8727
Training Epoch: 0 [49280/50048]	Loss: 3.9923
Training Epoch: 0 [49408/50048]	Loss: 3.9559
Training Epoch: 0 [49536/50048]	Loss: 3.9015
Training Epoch: 0 [49664/50048]	Loss: 3.8773
Training Epoch: 0 [49792/50048]	Loss: 3.9594
Training Epoch: 0 [49920/50048]	Loss: 3.7271
Training Epoch: 0 [50048/50048]	Loss: 4.0179
Validation Epoch: 0, Average loss: 0.0305, Accuracy: 0.0850
Training Epoch: 1 [128/50048]	Loss: 3.8176
Training Epoch: 1 [256/50048]	Loss: 3.8393
Training Epoch: 1 [384/50048]	Loss: 3.8819
Training Epoch: 1 [512/50048]	Loss: 3.7535
Training Epoch: 1 [640/50048]	Loss: 3.8239
Training Epoch: 1 [768/50048]	Loss: 3.6626
Training Epoch: 1 [896/50048]	Loss: 3.6732
Training Epoch: 1 [1024/50048]	Loss: 3.8580
Training Epoch: 1 [1152/50048]	Loss: 3.7355
Training Epoch: 1 [1280/50048]	Loss: 3.7395
Training Epoch: 1 [1408/50048]	Loss: 3.7389
Training Epoch: 1 [1536/50048]	Loss: 3.7878
Training Epoch: 1 [1664/50048]	Loss: 3.8841
Training Epoch: 1 [1792/50048]	Loss: 3.7933
Training Epoch: 1 [1920/50048]	Loss: 3.6815
Training Epoch: 1 [2048/50048]	Loss: 3.6132
Training Epoch: 1 [2176/50048]	Loss: 3.7915
Training Epoch: 1 [2304/50048]	Loss: 3.6179
Training Epoch: 1 [2432/50048]	Loss: 3.6716
Training Epoch: 1 [2560/50048]	Loss: 3.7749
Training Epoch: 1 [2688/50048]	Loss: 3.6175
Training Epoch: 1 [2816/50048]	Loss: 3.6969
Training Epoch: 1 [2944/50048]	Loss: 3.7089
Training Epoch: 1 [3072/50048]	Loss: 3.6855
Training Epoch: 1 [3200/50048]	Loss: 3.6717
Training Epoch: 1 [3328/50048]	Loss: 3.6885
Training Epoch: 1 [3456/50048]	Loss: 3.7234
Training Epoch: 1 [3584/50048]	Loss: 3.6766
Training Epoch: 1 [3712/50048]	Loss: 3.6533
Training Epoch: 1 [3840/50048]	Loss: 3.5474
Training Epoch: 1 [3968/50048]	Loss: 3.7000
Training Epoch: 1 [4096/50048]	Loss: 3.8148
Training Epoch: 1 [4224/50048]	Loss: 3.8466
Training Epoch: 1 [4352/50048]	Loss: 3.6955
Training Epoch: 1 [4480/50048]	Loss: 3.7800
Training Epoch: 1 [4608/50048]	Loss: 3.7497
Training Epoch: 1 [4736/50048]	Loss: 3.8233
Training Epoch: 1 [4864/50048]	Loss: 3.5973
Training Epoch: 1 [4992/50048]	Loss: 3.8267
Training Epoch: 1 [5120/50048]	Loss: 3.5595
Training Epoch: 1 [5248/50048]	Loss: 3.4741
Training Epoch: 1 [5376/50048]	Loss: 3.7310
Training Epoch: 1 [5504/50048]	Loss: 3.6945
Training Epoch: 1 [5632/50048]	Loss: 3.7394
Training Epoch: 1 [5760/50048]	Loss: 3.7642
Training Epoch: 1 [5888/50048]	Loss: 3.8745
Training Epoch: 1 [6016/50048]	Loss: 3.7856
Training Epoch: 1 [6144/50048]	Loss: 3.6311
Training Epoch: 1 [6272/50048]	Loss: 3.6702
Training Epoch: 1 [6400/50048]	Loss: 3.8621
Training Epoch: 1 [6528/50048]	Loss: 3.7432
Training Epoch: 1 [6656/50048]	Loss: 3.7000
Training Epoch: 1 [6784/50048]	Loss: 3.7684
Training Epoch: 1 [6912/50048]	Loss: 3.6703
Training Epoch: 1 [7040/50048]	Loss: 3.5109
Training Epoch: 1 [7168/50048]	Loss: 3.5890
Training Epoch: 1 [7296/50048]	Loss: 3.6235
Training Epoch: 1 [7424/50048]	Loss: 3.5789
Training Epoch: 1 [7552/50048]	Loss: 3.6863
Training Epoch: 1 [7680/50048]	Loss: 3.8503
Training Epoch: 1 [7808/50048]	Loss: 3.7561
Training Epoch: 1 [7936/50048]	Loss: 3.7665
Training Epoch: 1 [8064/50048]	Loss: 3.5479
Training Epoch: 1 [8192/50048]	Loss: 3.5887
Training Epoch: 1 [8320/50048]	Loss: 3.7610
Training Epoch: 1 [8448/50048]	Loss: 3.6764
Training Epoch: 1 [8576/50048]	Loss: 3.8934
Training Epoch: 1 [8704/50048]	Loss: 3.8300
Training Epoch: 1 [8832/50048]	Loss: 3.5940
Training Epoch: 1 [8960/50048]	Loss: 3.5954
Training Epoch: 1 [9088/50048]	Loss: 3.7817
Training Epoch: 1 [9216/50048]	Loss: 3.6299
Training Epoch: 1 [9344/50048]	Loss: 3.7527
Training Epoch: 1 [9472/50048]	Loss: 3.8432
Training Epoch: 1 [9600/50048]	Loss: 3.7729
Training Epoch: 1 [9728/50048]	Loss: 3.6042
Training Epoch: 1 [9856/50048]	Loss: 3.7523
Training Epoch: 1 [9984/50048]	Loss: 3.6680
Training Epoch: 1 [10112/50048]	Loss: 3.6705
Training Epoch: 1 [10240/50048]	Loss: 3.7064
Training Epoch: 1 [10368/50048]	Loss: 3.6247
Training Epoch: 1 [10496/50048]	Loss: 3.5643
Training Epoch: 1 [10624/50048]	Loss: 3.4491
Training Epoch: 1 [10752/50048]	Loss: 3.6889
Training Epoch: 1 [10880/50048]	Loss: 3.5654
Training Epoch: 1 [11008/50048]	Loss: 3.6996
Training Epoch: 1 [11136/50048]	Loss: 3.4676
Training Epoch: 1 [11264/50048]	Loss: 3.9717
Training Epoch: 1 [11392/50048]	Loss: 3.6124
Training Epoch: 1 [11520/50048]	Loss: 3.6409
Training Epoch: 1 [11648/50048]	Loss: 3.5196
Training Epoch: 1 [11776/50048]	Loss: 3.8960
Training Epoch: 1 [11904/50048]	Loss: 3.4441
Training Epoch: 1 [12032/50048]	Loss: 3.5570
Training Epoch: 1 [12160/50048]	Loss: 3.6002
Training Epoch: 1 [12288/50048]	Loss: 3.5664
Training Epoch: 1 [12416/50048]	Loss: 3.5447
Training Epoch: 1 [12544/50048]	Loss: 3.7325
Training Epoch: 1 [12672/50048]	Loss: 3.7934
Training Epoch: 1 [12800/50048]	Loss: 3.5852
Training Epoch: 1 [12928/50048]	Loss: 3.4053
Training Epoch: 1 [13056/50048]	Loss: 3.5567
Training Epoch: 1 [13184/50048]	Loss: 3.5406
Training Epoch: 1 [13312/50048]	Loss: 3.6562
Training Epoch: 1 [13440/50048]	Loss: 3.4069
Training Epoch: 1 [13568/50048]	Loss: 3.5665
Training Epoch: 1 [13696/50048]	Loss: 3.6487
Training Epoch: 1 [13824/50048]	Loss: 3.3293
Training Epoch: 1 [13952/50048]	Loss: 3.6129
Training Epoch: 1 [14080/50048]	Loss: 3.6838
Training Epoch: 1 [14208/50048]	Loss: 3.5895
Training Epoch: 1 [14336/50048]	Loss: 3.7529
Training Epoch: 1 [14464/50048]	Loss: 3.4296
Training Epoch: 1 [14592/50048]	Loss: 3.4845
Training Epoch: 1 [14720/50048]	Loss: 3.5567
Training Epoch: 1 [14848/50048]	Loss: 3.6833
Training Epoch: 1 [14976/50048]	Loss: 3.5574
Training Epoch: 1 [15104/50048]	Loss: 3.7358
Training Epoch: 1 [15232/50048]	Loss: 3.6391
Training Epoch: 1 [15360/50048]	Loss: 3.6505
Training Epoch: 1 [15488/50048]	Loss: 3.5392
Training Epoch: 1 [15616/50048]	Loss: 3.7262
Training Epoch: 1 [15744/50048]	Loss: 3.6430
Training Epoch: 1 [15872/50048]	Loss: 3.7120
Training Epoch: 1 [16000/50048]	Loss: 3.4780
Training Epoch: 1 [16128/50048]	Loss: 3.3754
Training Epoch: 1 [16256/50048]	Loss: 3.5795
Training Epoch: 1 [16384/50048]	Loss: 3.6437
Training Epoch: 1 [16512/50048]	Loss: 3.5117
Training Epoch: 1 [16640/50048]	Loss: 3.7607
Training Epoch: 1 [16768/50048]	Loss: 3.7130
Training Epoch: 1 [16896/50048]	Loss: 3.6243
Training Epoch: 1 [17024/50048]	Loss: 3.4527
Training Epoch: 1 [17152/50048]	Loss: 3.5177
Training Epoch: 1 [17280/50048]	Loss: 3.5865
Training Epoch: 1 [17408/50048]	Loss: 3.6355
Training Epoch: 1 [17536/50048]	Loss: 3.7499
Training Epoch: 1 [17664/50048]	Loss: 3.4909
Training Epoch: 1 [17792/50048]	Loss: 3.5324
Training Epoch: 1 [17920/50048]	Loss: 3.8314
Training Epoch: 1 [18048/50048]	Loss: 3.4465
Training Epoch: 1 [18176/50048]	Loss: 3.5171
Training Epoch: 1 [18304/50048]	Loss: 3.4767
Training Epoch: 1 [18432/50048]	Loss: 3.5233
Training Epoch: 1 [18560/50048]	Loss: 3.5547
Training Epoch: 1 [18688/50048]	Loss: 3.6967
Training Epoch: 1 [18816/50048]	Loss: 3.6504
Training Epoch: 1 [18944/50048]	Loss: 3.4508
Training Epoch: 1 [19072/50048]	Loss: 3.6159
Training Epoch: 1 [19200/50048]	Loss: 3.4841
Training Epoch: 1 [19328/50048]	Loss: 3.5908
Training Epoch: 1 [19456/50048]	Loss: 3.6318
Training Epoch: 1 [19584/50048]	Loss: 3.5793
Training Epoch: 1 [19712/50048]	Loss: 3.5259
Training Epoch: 1 [19840/50048]	Loss: 3.6742
Training Epoch: 1 [19968/50048]	Loss: 3.5699
Training Epoch: 1 [20096/50048]	Loss: 3.5764
Training Epoch: 1 [20224/50048]	Loss: 3.5235
Training Epoch: 1 [20352/50048]	Loss: 3.5115
Training Epoch: 1 [20480/50048]	Loss: 3.5335
Training Epoch: 1 [20608/50048]	Loss: 3.6995
Training Epoch: 1 [20736/50048]	Loss: 3.3086
Training Epoch: 1 [20864/50048]	Loss: 3.5907
Training Epoch: 1 [20992/50048]	Loss: 3.4114
Training Epoch: 1 [21120/50048]	Loss: 3.5512
Training Epoch: 1 [21248/50048]	Loss: 3.6359
Training Epoch: 1 [21376/50048]	Loss: 3.5593
Training Epoch: 1 [21504/50048]	Loss: 3.5996
Training Epoch: 1 [21632/50048]	Loss: 3.6017
Training Epoch: 1 [21760/50048]	Loss: 3.5520
Training Epoch: 1 [21888/50048]	Loss: 3.6184
Training Epoch: 1 [22016/50048]	Loss: 3.3694
Training Epoch: 1 [22144/50048]	Loss: 3.4546
Training Epoch: 1 [22272/50048]	Loss: 3.5168
Training Epoch: 1 [22400/50048]	Loss: 3.6288
Training Epoch: 1 [22528/50048]	Loss: 3.4817
Training Epoch: 1 [22656/50048]	Loss: 3.7310
Training Epoch: 1 [22784/50048]	Loss: 3.6864
Training Epoch: 1 [22912/50048]	Loss: 3.5151
Training Epoch: 1 [23040/50048]	Loss: 3.6854
Training Epoch: 1 [23168/50048]	Loss: 3.5003
Training Epoch: 1 [23296/50048]	Loss: 3.4469
Training Epoch: 1 [23424/50048]	Loss: 3.4141
Training Epoch: 1 [23552/50048]	Loss: 3.4125
Training Epoch: 1 [23680/50048]	Loss: 3.4938
Training Epoch: 1 [23808/50048]	Loss: 3.4583
Training Epoch: 1 [23936/50048]	Loss: 3.7256
Training Epoch: 1 [24064/50048]	Loss: 3.3185
Training Epoch: 1 [24192/50048]	Loss: 3.4321
Training Epoch: 1 [24320/50048]	Loss: 3.4961
Training Epoch: 1 [24448/50048]	Loss: 3.7133
Training Epoch: 1 [24576/50048]	Loss: 3.3356
Training Epoch: 1 [24704/50048]	Loss: 3.6668
Training Epoch: 1 [24832/50048]	Loss: 3.4737
Training Epoch: 1 [24960/50048]	Loss: 3.5014
Training Epoch: 1 [25088/50048]	Loss: 3.4569
Training Epoch: 1 [25216/50048]	Loss: 3.6819
Training Epoch: 1 [25344/50048]	Loss: 3.4861
Training Epoch: 1 [25472/50048]	Loss: 3.3512
Training Epoch: 1 [25600/50048]	Loss: 3.4943
Training Epoch: 1 [25728/50048]	Loss: 3.3799
Training Epoch: 1 [25856/50048]	Loss: 3.3364
Training Epoch: 1 [25984/50048]	Loss: 3.4533
Training Epoch: 1 [26112/50048]	Loss: 3.3840
Training Epoch: 1 [26240/50048]	Loss: 3.4842
Training Epoch: 1 [26368/50048]	Loss: 3.3178
Training Epoch: 1 [26496/50048]	Loss: 3.4458
Training Epoch: 1 [26624/50048]	Loss: 3.5466
Training Epoch: 1 [26752/50048]	Loss: 3.5147
Training Epoch: 1 [26880/50048]	Loss: 3.4334
Training Epoch: 1 [27008/50048]	Loss: 3.3320
Training Epoch: 1 [27136/50048]	Loss: 3.6232
Training Epoch: 1 [27264/50048]	Loss: 3.5292
Training Epoch: 1 [27392/50048]	Loss: 3.4862
Training Epoch: 1 [27520/50048]	Loss: 3.4615
Training Epoch: 1 [27648/50048]	Loss: 3.4860
Training Epoch: 1 [27776/50048]	Loss: 3.5228
Training Epoch: 1 [27904/50048]	Loss: 3.3922
Training Epoch: 1 [28032/50048]	Loss: 3.3352
Training Epoch: 1 [28160/50048]	Loss: 3.3558
Training Epoch: 1 [28288/50048]	Loss: 3.4833
Training Epoch: 1 [28416/50048]	Loss: 3.4999
Training Epoch: 1 [28544/50048]	Loss: 3.4605
Training Epoch: 1 [28672/50048]	Loss: 3.4275
Training Epoch: 1 [28800/50048]	Loss: 3.5648
Training Epoch: 1 [28928/50048]	Loss: 3.5856
Training Epoch: 1 [29056/50048]	Loss: 3.6068
Training Epoch: 1 [29184/50048]	Loss: 3.5182
Training Epoch: 1 [29312/50048]	Loss: 3.5305
Training Epoch: 1 [29440/50048]	Loss: 3.3062
Training Epoch: 1 [29568/50048]	Loss: 3.4877
Training Epoch: 1 [29696/50048]	Loss: 3.5709
Training Epoch: 1 [29824/50048]	Loss: 3.5696
Training Epoch: 1 [29952/50048]	Loss: 3.6082
Training Epoch: 1 [30080/50048]	Loss: 3.2722
Training Epoch: 1 [30208/50048]	Loss: 3.5092
Training Epoch: 1 [30336/50048]	Loss: 3.4441
Training Epoch: 1 [30464/50048]	Loss: 3.3991
Training Epoch: 1 [30592/50048]	Loss: 3.2446
Training Epoch: 1 [30720/50048]	Loss: 3.5435
Training Epoch: 1 [30848/50048]	Loss: 3.6892
Training Epoch: 1 [30976/50048]	Loss: 3.2363
Training Epoch: 1 [31104/50048]	Loss: 3.3005
Training Epoch: 1 [31232/50048]	Loss: 3.4680
Training Epoch: 1 [31360/50048]	Loss: 3.4258
Training Epoch: 1 [31488/50048]	Loss: 3.4611
Training Epoch: 1 [31616/50048]	Loss: 3.2484
Training Epoch: 1 [31744/50048]	Loss: 3.3168
Training Epoch: 1 [31872/50048]	Loss: 3.3480
Training Epoch: 1 [32000/50048]	Loss: 3.4008
Training Epoch: 1 [32128/50048]	Loss: 3.2618
Training Epoch: 1 [32256/50048]	Loss: 3.4568
Training Epoch: 1 [32384/50048]	Loss: 3.4978
Training Epoch: 1 [32512/50048]	Loss: 3.3968
Training Epoch: 1 [32640/50048]	Loss: 3.3152
Training Epoch: 1 [32768/50048]	Loss: 3.3634
Training Epoch: 1 [32896/50048]	Loss: 3.5791
Training Epoch: 1 [33024/50048]	Loss: 3.4280
Training Epoch: 1 [33152/50048]	Loss: 3.7840
Training Epoch: 1 [33280/50048]	Loss: 3.4358
Training Epoch: 1 [33408/50048]	Loss: 3.3223
Training Epoch: 1 [33536/50048]	Loss: 3.2905
Training Epoch: 1 [33664/50048]	Loss: 3.2917
Training Epoch: 1 [33792/50048]	Loss: 3.5373
Training Epoch: 1 [33920/50048]	Loss: 3.6404
Training Epoch: 1 [34048/50048]	Loss: 3.2045
Training Epoch: 1 [34176/50048]	Loss: 3.4746
Training Epoch: 1 [34304/50048]	Loss: 3.4393
Training Epoch: 1 [34432/50048]	Loss: 3.3358
Training Epoch: 1 [34560/50048]	Loss: 3.5653
Training Epoch: 1 [34688/50048]	Loss: 3.4809
Training Epoch: 1 [34816/50048]	Loss: 3.4513
Training Epoch: 1 [34944/50048]	Loss: 3.3266
Training Epoch: 1 [35072/50048]	Loss: 3.3438
Training Epoch: 1 [35200/50048]	Loss: 3.3514
Training Epoch: 1 [35328/50048]	Loss: 3.1574
Training Epoch: 1 [35456/50048]	Loss: 3.3678
Training Epoch: 1 [35584/50048]	Loss: 3.3160
Training Epoch: 1 [35712/50048]	Loss: 3.4305
Training Epoch: 1 [35840/50048]	Loss: 3.3504
Training Epoch: 1 [35968/50048]	Loss: 3.7374
Training Epoch: 1 [36096/50048]	Loss: 3.5448
Training Epoch: 1 [36224/50048]	Loss: 3.1816
Training Epoch: 1 [36352/50048]	Loss: 3.3720
Training Epoch: 1 [36480/50048]	Loss: 3.5001
Training Epoch: 1 [36608/50048]	Loss: 3.3619
Training Epoch: 1 [36736/50048]	Loss: 3.4215
Training Epoch: 1 [36864/50048]	Loss: 3.4603
Training Epoch: 1 [36992/50048]	Loss: 3.3174
Training Epoch: 1 [37120/50048]	Loss: 3.4347
Training Epoch: 1 [37248/50048]	Loss: 3.4524
Training Epoch: 1 [37376/50048]	Loss: 3.3593
Training Epoch: 1 [37504/50048]	Loss: 3.3777
Training Epoch: 1 [37632/50048]	Loss: 3.6341
Training Epoch: 1 [37760/50048]	Loss: 3.3887
Training Epoch: 1 [37888/50048]	Loss: 3.4828
Training Epoch: 1 [38016/50048]	Loss: 3.4107
Training Epoch: 1 [38144/50048]	Loss: 3.5006
Training Epoch: 1 [38272/50048]	Loss: 3.3242
Training Epoch: 1 [38400/50048]	Loss: 3.6452
Training Epoch: 1 [38528/50048]	Loss: 3.3647
Training Epoch: 1 [38656/50048]	Loss: 3.3828
Training Epoch: 1 [38784/50048]	Loss: 3.4238
Training Epoch: 1 [38912/50048]	Loss: 3.5023
Training Epoch: 1 [39040/50048]	Loss: 3.1959
Training Epoch: 1 [39168/50048]	Loss: 3.1750
Training Epoch: 1 [39296/50048]	Loss: 3.4459
Training Epoch: 1 [39424/50048]	Loss: 3.1059
Training Epoch: 1 [39552/50048]	Loss: 3.1947
Training Epoch: 1 [39680/50048]	Loss: 3.1009
Training Epoch: 1 [39808/50048]	Loss: 3.3207
Training Epoch: 1 [39936/50048]	Loss: 3.3188
Training Epoch: 1 [40064/50048]	Loss: 3.4513
Training Epoch: 1 [40192/50048]	Loss: 3.2327
Training Epoch: 1 [40320/50048]	Loss: 3.2283
Training Epoch: 1 [40448/50048]	Loss: 3.1940
Training Epoch: 1 [40576/50048]	Loss: 3.2167
Training Epoch: 1 [40704/50048]	Loss: 3.4209
Training Epoch: 1 [40832/50048]	Loss: 3.3709
Training Epoch: 1 [40960/50048]	Loss: 3.6353
Training Epoch: 1 [41088/50048]	Loss: 3.4312
Training Epoch: 1 [41216/50048]	Loss: 3.3730
Training Epoch: 1 [41344/50048]	Loss: 3.1269
Training Epoch: 1 [41472/50048]	Loss: 3.3892
Training Epoch: 1 [41600/50048]	Loss: 3.2263
Training Epoch: 1 [41728/50048]	Loss: 3.1931
Training Epoch: 1 [41856/50048]	Loss: 3.3351
Training Epoch: 1 [41984/50048]	Loss: 3.3055
Training Epoch: 1 [42112/50048]	Loss: 3.2042
Training Epoch: 1 [42240/50048]	Loss: 3.3261
Training Epoch: 1 [42368/50048]	Loss: 3.1475
Training Epoch: 1 [42496/50048]	Loss: 3.4334
Training Epoch: 1 [42624/50048]	Loss: 3.3426
Training Epoch: 1 [42752/50048]	Loss: 3.2542
Training Epoch: 1 [42880/50048]	Loss: 3.3626
Training Epoch: 1 [43008/50048]	Loss: 3.1733
Training Epoch: 1 [43136/50048]	Loss: 3.2895
Training Epoch: 1 [43264/50048]	Loss: 3.3792
Training Epoch: 1 [43392/50048]	Loss: 3.2750
Training Epoch: 1 [43520/50048]	Loss: 3.4519
Training Epoch: 1 [43648/50048]	Loss: 3.3549
Training Epoch: 1 [43776/50048]	Loss: 3.4145
Training Epoch: 1 [43904/50048]	Loss: 3.3643
Training Epoch: 1 [44032/50048]	Loss: 3.2923
Training Epoch: 1 [44160/50048]	Loss: 3.4741
Training Epoch: 1 [44288/50048]	Loss: 3.5202
Training Epoch: 1 [44416/50048]	Loss: 3.3186
Training Epoch: 1 [44544/50048]	Loss: 3.3817
Training Epoch: 1 [44672/50048]	Loss: 3.2605
Training Epoch: 1 [44800/50048]	Loss: 3.2653
Training Epoch: 1 [44928/50048]	Loss: 3.4357
Training Epoch: 1 [45056/50048]	Loss: 3.4805
Training Epoch: 1 [45184/50048]	Loss: 3.2953
Training Epoch: 1 [45312/50048]	Loss: 3.2529
Training Epoch: 1 [45440/50048]	Loss: 3.2457
Training Epoch: 1 [45568/50048]	Loss: 3.1971
Training Epoch: 1 [45696/50048]	Loss: 3.1821
Training Epoch: 1 [45824/50048]	Loss: 3.0897
Training Epoch: 1 [45952/50048]	Loss: 3.4721
Training Epoch: 1 [46080/50048]	Loss: 3.0690
Training Epoch: 1 [46208/50048]	Loss: 3.2913
Training Epoch: 1 [46336/50048]	Loss: 3.5255
Training Epoch: 1 [46464/50048]	Loss: 3.3092
Training Epoch: 1 [46592/50048]	Loss: 3.3474
Training Epoch: 1 [46720/50048]	Loss: 3.2591
Training Epoch: 1 [46848/50048]	Loss: 3.2483
Training Epoch: 1 [46976/50048]	Loss: 3.3626
Training Epoch: 1 [47104/50048]	Loss: 3.1660
Training Epoch: 1 [47232/50048]	Loss: 3.3903
Training Epoch: 1 [47360/50048]	Loss: 3.1942
Training Epoch: 1 [47488/50048]	Loss: 3.4548
Training Epoch: 1 [47616/50048]	Loss: 3.2501
Training Epoch: 1 [47744/50048]	Loss: 3.3909
Training Epoch: 1 [47872/50048]	Loss: 3.1357
Training Epoch: 1 [48000/50048]	Loss: 3.2997
Training Epoch: 1 [48128/50048]	Loss: 3.4128
Training Epoch: 1 [48256/50048]	Loss: 3.1657
Training Epoch: 1 [48384/50048]	Loss: 3.4599
Training Epoch: 1 [48512/50048]	Loss: 3.1961
Training Epoch: 1 [48640/50048]	Loss: 3.2607
Training Epoch: 1 [48768/50048]	Loss: 3.1824
Training Epoch: 1 [48896/50048]	Loss: 3.1234
Training Epoch: 1 [49024/50048]	Loss: 3.3233
Training Epoch: 1 [49152/50048]	Loss: 3.2848
Training Epoch: 1 [49280/50048]	Loss: 3.4416
Training Epoch: 1 [49408/50048]	Loss: 3.3093
Training Epoch: 1 [49536/50048]	Loss: 3.0739
Training Epoch: 1 [49664/50048]	Loss: 3.1486
Training Epoch: 1 [49792/50048]	Loss: 3.3696
Training Epoch: 1 [49920/50048]	Loss: 3.4256
Training Epoch: 1 [50048/50048]	Loss: 3.4882
Validation Epoch: 1, Average loss: 0.0314, Accuracy: 0.1224
Training Epoch: 2 [128/50048]	Loss: 3.1762
Training Epoch: 2 [256/50048]	Loss: 3.2703
Training Epoch: 2 [384/50048]	Loss: 3.0869
Training Epoch: 2 [512/50048]	Loss: 3.3130
Training Epoch: 2 [640/50048]	Loss: 2.9424
Training Epoch: 2 [768/50048]	Loss: 3.3943
Training Epoch: 2 [896/50048]	Loss: 3.4340
Training Epoch: 2 [1024/50048]	Loss: 3.2484
Training Epoch: 2 [1152/50048]	Loss: 3.0590
Training Epoch: 2 [1280/50048]	Loss: 3.0175
Training Epoch: 2 [1408/50048]	Loss: 3.3965
Training Epoch: 2 [1536/50048]	Loss: 3.4831
Training Epoch: 2 [1664/50048]	Loss: 3.0868
Training Epoch: 2 [1792/50048]	Loss: 3.3823
Training Epoch: 2 [1920/50048]	Loss: 3.1234
Training Epoch: 2 [2048/50048]	Loss: 3.2362
Training Epoch: 2 [2176/50048]	Loss: 3.3355
Training Epoch: 2 [2304/50048]	Loss: 3.1264
Training Epoch: 2 [2432/50048]	Loss: 3.2357
Training Epoch: 2 [2560/50048]	Loss: 3.0353
Training Epoch: 2 [2688/50048]	Loss: 3.2140
Training Epoch: 2 [2816/50048]	Loss: 3.1827
Training Epoch: 2 [2944/50048]	Loss: 3.2337
Training Epoch: 2 [3072/50048]	Loss: 3.2073
Training Epoch: 2 [3200/50048]	Loss: 3.2464
Training Epoch: 2 [3328/50048]	Loss: 3.1818
Training Epoch: 2 [3456/50048]	Loss: 3.1161
Training Epoch: 2 [3584/50048]	Loss: 3.5361
Training Epoch: 2 [3712/50048]	Loss: 3.5371
Training Epoch: 2 [3840/50048]	Loss: 3.2220
Training Epoch: 2 [3968/50048]	Loss: 3.1929
Training Epoch: 2 [4096/50048]	Loss: 3.2920
Training Epoch: 2 [4224/50048]	Loss: 3.2386
Training Epoch: 2 [4352/50048]	Loss: 3.1437
Training Epoch: 2 [4480/50048]	Loss: 3.1486
Training Epoch: 2 [4608/50048]	Loss: 3.0928
Training Epoch: 2 [4736/50048]	Loss: 3.3620
Training Epoch: 2 [4864/50048]	Loss: 3.1990
Training Epoch: 2 [4992/50048]	Loss: 3.2455
Training Epoch: 2 [5120/50048]	Loss: 3.1523
Training Epoch: 2 [5248/50048]	Loss: 3.2300
Training Epoch: 2 [5376/50048]	Loss: 2.9368
Training Epoch: 2 [5504/50048]	Loss: 3.4225
Training Epoch: 2 [5632/50048]	Loss: 3.2147
Training Epoch: 2 [5760/50048]	Loss: 3.0926
Training Epoch: 2 [5888/50048]	Loss: 3.0632
Training Epoch: 2 [6016/50048]	Loss: 3.1764
Training Epoch: 2 [6144/50048]	Loss: 3.3222
Training Epoch: 2 [6272/50048]	Loss: 3.3133
Training Epoch: 2 [6400/50048]	Loss: 3.0067
Training Epoch: 2 [6528/50048]	Loss: 3.1940
Training Epoch: 2 [6656/50048]	Loss: 2.9438
Training Epoch: 2 [6784/50048]	Loss: 3.0345
Training Epoch: 2 [6912/50048]	Loss: 3.2375
Training Epoch: 2 [7040/50048]	Loss: 3.0099
Training Epoch: 2 [7168/50048]	Loss: 3.2911
Training Epoch: 2 [7296/50048]	Loss: 3.0086
Training Epoch: 2 [7424/50048]	Loss: 3.1218
Training Epoch: 2 [7552/50048]	Loss: 3.0413
Training Epoch: 2 [7680/50048]	Loss: 3.3599
Training Epoch: 2 [7808/50048]	Loss: 3.1271
Training Epoch: 2 [7936/50048]	Loss: 3.2562
Training Epoch: 2 [8064/50048]	Loss: 3.0593
Training Epoch: 2 [8192/50048]	Loss: 3.1267
Training Epoch: 2 [8320/50048]	Loss: 3.4674
Training Epoch: 2 [8448/50048]	Loss: 3.2801
Training Epoch: 2 [8576/50048]	Loss: 3.1708
Training Epoch: 2 [8704/50048]	Loss: 3.4039
Training Epoch: 2 [8832/50048]	Loss: 3.1559
Training Epoch: 2 [8960/50048]	Loss: 3.4637
Training Epoch: 2 [9088/50048]	Loss: 3.0803
Training Epoch: 2 [9216/50048]	Loss: 2.9912
Training Epoch: 2 [9344/50048]	Loss: 3.2249
Training Epoch: 2 [9472/50048]	Loss: 2.8955
Training Epoch: 2 [9600/50048]	Loss: 3.1412
Training Epoch: 2 [9728/50048]	Loss: 3.0480
Training Epoch: 2 [9856/50048]	Loss: 3.2947
Training Epoch: 2 [9984/50048]	Loss: 3.2290
Training Epoch: 2 [10112/50048]	Loss: 3.3325
Training Epoch: 2 [10240/50048]	Loss: 2.9534
Training Epoch: 2 [10368/50048]	Loss: 3.2646
Training Epoch: 2 [10496/50048]	Loss: 3.3391
Training Epoch: 2 [10624/50048]	Loss: 3.0613
Training Epoch: 2 [10752/50048]	Loss: 3.2360
Training Epoch: 2 [10880/50048]	Loss: 3.0946
Training Epoch: 2 [11008/50048]	Loss: 3.3538
Training Epoch: 2 [11136/50048]	Loss: 3.0967
Training Epoch: 2 [11264/50048]	Loss: 3.2455
Training Epoch: 2 [11392/50048]	Loss: 3.0915
Training Epoch: 2 [11520/50048]	Loss: 2.8768
Training Epoch: 2 [11648/50048]	Loss: 3.1310
Training Epoch: 2 [11776/50048]	Loss: 3.2333
Training Epoch: 2 [11904/50048]	Loss: 3.2728
Training Epoch: 2 [12032/50048]	Loss: 2.9626
Training Epoch: 2 [12160/50048]	Loss: 3.0292
Training Epoch: 2 [12288/50048]	Loss: 3.2386
Training Epoch: 2 [12416/50048]	Loss: 2.9227
Training Epoch: 2 [12544/50048]	Loss: 2.9348
Training Epoch: 2 [12672/50048]	Loss: 3.0630
Training Epoch: 2 [12800/50048]	Loss: 3.0917
Training Epoch: 2 [12928/50048]	Loss: 3.2434
Training Epoch: 2 [13056/50048]	Loss: 3.0416
Training Epoch: 2 [13184/50048]	Loss: 3.2379
Training Epoch: 2 [13312/50048]	Loss: 3.1418
Training Epoch: 2 [13440/50048]	Loss: 3.2716
Training Epoch: 2 [13568/50048]	Loss: 3.0478
Training Epoch: 2 [13696/50048]	Loss: 2.8353
Training Epoch: 2 [13824/50048]	Loss: 3.2771
Training Epoch: 2 [13952/50048]	Loss: 3.3055
Training Epoch: 2 [14080/50048]	Loss: 3.1766
Training Epoch: 2 [14208/50048]	Loss: 3.1937
Training Epoch: 2 [14336/50048]	Loss: 3.3641
Training Epoch: 2 [14464/50048]	Loss: 3.0796
Training Epoch: 2 [14592/50048]	Loss: 3.1364
Training Epoch: 2 [14720/50048]	Loss: 3.0337
Training Epoch: 2 [14848/50048]	Loss: 2.9563
Training Epoch: 2 [14976/50048]	Loss: 3.2908
Training Epoch: 2 [15104/50048]	Loss: 2.8962
Training Epoch: 2 [15232/50048]	Loss: 3.3017
Training Epoch: 2 [15360/50048]	Loss: 3.1763
Training Epoch: 2 [15488/50048]	Loss: 3.0265
Training Epoch: 2 [15616/50048]	Loss: 3.0198
Training Epoch: 2 [15744/50048]	Loss: 3.2504
Training Epoch: 2 [15872/50048]	Loss: 2.8222
Training Epoch: 2 [16000/50048]	Loss: 3.0985
Training Epoch: 2 [16128/50048]	Loss: 3.1885
Training Epoch: 2 [16256/50048]	Loss: 2.9866
Training Epoch: 2 [16384/50048]	Loss: 3.1322
Training Epoch: 2 [16512/50048]	Loss: 3.2967
Training Epoch: 2 [16640/50048]	Loss: 2.8528
Training Epoch: 2 [16768/50048]	Loss: 3.1769
Training Epoch: 2 [16896/50048]	Loss: 2.9569
Training Epoch: 2 [17024/50048]	Loss: 3.1402
Training Epoch: 2 [17152/50048]	Loss: 2.9645
Training Epoch: 2 [17280/50048]	Loss: 3.1188
Training Epoch: 2 [17408/50048]	Loss: 2.9869
Training Epoch: 2 [17536/50048]	Loss: 3.0862
Training Epoch: 2 [17664/50048]	Loss: 3.1747
Training Epoch: 2 [17792/50048]	Loss: 2.9483
Training Epoch: 2 [17920/50048]	Loss: 2.9676
Training Epoch: 2 [18048/50048]	Loss: 3.1756
Training Epoch: 2 [18176/50048]	Loss: 3.0472
Training Epoch: 2 [18304/50048]	Loss: 3.0958
Training Epoch: 2 [18432/50048]	Loss: 3.1625
Training Epoch: 2 [18560/50048]	Loss: 3.0791
Training Epoch: 2 [18688/50048]	Loss: 3.1319
Training Epoch: 2 [18816/50048]	Loss: 3.0423
Training Epoch: 2 [18944/50048]	Loss: 2.9163
Training Epoch: 2 [19072/50048]	Loss: 3.1332
Training Epoch: 2 [19200/50048]	Loss: 3.1809
Training Epoch: 2 [19328/50048]	Loss: 2.8414
Training Epoch: 2 [19456/50048]	Loss: 3.1106
Training Epoch: 2 [19584/50048]	Loss: 3.0200
Training Epoch: 2 [19712/50048]	Loss: 3.1426
Training Epoch: 2 [19840/50048]	Loss: 3.0630
Training Epoch: 2 [19968/50048]	Loss: 3.1962
Training Epoch: 2 [20096/50048]	Loss: 3.3141
Training Epoch: 2 [20224/50048]	Loss: 2.8369
Training Epoch: 2 [20352/50048]	Loss: 3.1325
Training Epoch: 2 [20480/50048]	Loss: 2.9855
Training Epoch: 2 [20608/50048]	Loss: 2.9092
Training Epoch: 2 [20736/50048]	Loss: 3.1470
Training Epoch: 2 [20864/50048]	Loss: 2.9214
Training Epoch: 2 [20992/50048]	Loss: 3.1547
Training Epoch: 2 [21120/50048]	Loss: 2.9614
Training Epoch: 2 [21248/50048]	Loss: 3.0739
Training Epoch: 2 [21376/50048]	Loss: 3.0024
Training Epoch: 2 [21504/50048]	Loss: 3.0849
Training Epoch: 2 [21632/50048]	Loss: 3.0946
Training Epoch: 2 [21760/50048]	Loss: 3.0394
Training Epoch: 2 [21888/50048]	Loss: 3.0874
Training Epoch: 2 [22016/50048]	Loss: 2.9051
Training Epoch: 2 [22144/50048]	Loss: 3.1012
Training Epoch: 2 [22272/50048]	Loss: 2.8286
Training Epoch: 2 [22400/50048]	Loss: 3.0409
Training Epoch: 2 [22528/50048]	Loss: 2.9945
Training Epoch: 2 [22656/50048]	Loss: 2.9752
Training Epoch: 2 [22784/50048]	Loss: 3.0268
Training Epoch: 2 [22912/50048]	Loss: 3.1403
Training Epoch: 2 [23040/50048]	Loss: 3.2085
Training Epoch: 2 [23168/50048]	Loss: 3.0602
Training Epoch: 2 [23296/50048]	Loss: 3.0153
Training Epoch: 2 [23424/50048]	Loss: 3.1026
Training Epoch: 2 [23552/50048]	Loss: 3.2956
Training Epoch: 2 [23680/50048]	Loss: 3.0082
Training Epoch: 2 [23808/50048]	Loss: 2.9458
Training Epoch: 2 [23936/50048]	Loss: 3.1554
Training Epoch: 2 [24064/50048]	Loss: 3.0651
Training Epoch: 2 [24192/50048]	Loss: 3.1396
Training Epoch: 2 [24320/50048]	Loss: 2.9099
Training Epoch: 2 [24448/50048]	Loss: 3.0403
Training Epoch: 2 [24576/50048]	Loss: 3.0028
Training Epoch: 2 [24704/50048]	Loss: 3.0196
Training Epoch: 2 [24832/50048]	Loss: 2.9976
Training Epoch: 2 [24960/50048]	Loss: 2.8989
Training Epoch: 2 [25088/50048]	Loss: 3.1198
Training Epoch: 2 [25216/50048]	Loss: 3.1740
Training Epoch: 2 [25344/50048]	Loss: 2.8782
Training Epoch: 2 [25472/50048]	Loss: 2.9868
Training Epoch: 2 [25600/50048]	Loss: 2.8109
Training Epoch: 2 [25728/50048]	Loss: 2.9280
Training Epoch: 2 [25856/50048]	Loss: 3.0863
Training Epoch: 2 [25984/50048]	Loss: 3.0348
Training Epoch: 2 [26112/50048]	Loss: 2.8914
Training Epoch: 2 [26240/50048]	Loss: 3.1858
Training Epoch: 2 [26368/50048]	Loss: 2.9226
Training Epoch: 2 [26496/50048]	Loss: 3.0299
Training Epoch: 2 [26624/50048]	Loss: 3.1463
Training Epoch: 2 [26752/50048]	Loss: 2.7765
Training Epoch: 2 [26880/50048]	Loss: 2.9687
Training Epoch: 2 [27008/50048]	Loss: 3.1514
Training Epoch: 2 [27136/50048]	Loss: 2.9896
Training Epoch: 2 [27264/50048]	Loss: 3.0841
Training Epoch: 2 [27392/50048]	Loss: 2.9967
Training Epoch: 2 [27520/50048]	Loss: 3.1862
Training Epoch: 2 [27648/50048]	Loss: 3.1070
Training Epoch: 2 [27776/50048]	Loss: 2.9869
Training Epoch: 2 [27904/50048]	Loss: 2.9334
Training Epoch: 2 [28032/50048]	Loss: 2.8728
Training Epoch: 2 [28160/50048]	Loss: 3.1015
Training Epoch: 2 [28288/50048]	Loss: 3.0710
Training Epoch: 2 [28416/50048]	Loss: 3.3562
Training Epoch: 2 [28544/50048]	Loss: 3.2135
Training Epoch: 2 [28672/50048]	Loss: 3.2402
Training Epoch: 2 [28800/50048]	Loss: 2.8442
Training Epoch: 2 [28928/50048]	Loss: 3.1033
Training Epoch: 2 [29056/50048]	Loss: 3.0525
Training Epoch: 2 [29184/50048]	Loss: 2.9688
Training Epoch: 2 [29312/50048]	Loss: 3.0018
Training Epoch: 2 [29440/50048]	Loss: 2.9364
Training Epoch: 2 [29568/50048]	Loss: 3.1775
Training Epoch: 2 [29696/50048]	Loss: 3.1173
Training Epoch: 2 [29824/50048]	Loss: 2.9535
Training Epoch: 2 [29952/50048]	Loss: 2.8778
Training Epoch: 2 [30080/50048]	Loss: 3.0356
Training Epoch: 2 [30208/50048]	Loss: 3.3021
Training Epoch: 2 [30336/50048]	Loss: 3.1406
Training Epoch: 2 [30464/50048]	Loss: 2.9827
Training Epoch: 2 [30592/50048]	Loss: 3.1900
Training Epoch: 2 [30720/50048]	Loss: 3.1411
Training Epoch: 2 [30848/50048]	Loss: 2.9953
Training Epoch: 2 [30976/50048]	Loss: 3.0569
Training Epoch: 2 [31104/50048]	Loss: 3.1164
Training Epoch: 2 [31232/50048]	Loss: 3.1258
Training Epoch: 2 [31360/50048]	Loss: 3.1573
Training Epoch: 2 [31488/50048]	Loss: 3.4408
Training Epoch: 2 [31616/50048]	Loss: 3.0545
Training Epoch: 2 [31744/50048]	Loss: 2.8766
Training Epoch: 2 [31872/50048]	Loss: 2.8625
Training Epoch: 2 [32000/50048]	Loss: 3.1690
Training Epoch: 2 [32128/50048]	Loss: 3.0243
Training Epoch: 2 [32256/50048]	Loss: 3.1621
Training Epoch: 2 [32384/50048]	Loss: 3.1546
Training Epoch: 2 [32512/50048]	Loss: 3.0348
Training Epoch: 2 [32640/50048]	Loss: 3.1432
Training Epoch: 2 [32768/50048]	Loss: 3.0168
Training Epoch: 2 [32896/50048]	Loss: 3.0801
Training Epoch: 2 [33024/50048]	Loss: 2.8659
Training Epoch: 2 [33152/50048]	Loss: 2.9232
Training Epoch: 2 [33280/50048]	Loss: 2.8202
Training Epoch: 2 [33408/50048]	Loss: 2.9294
Training Epoch: 2 [33536/50048]	Loss: 3.0855
Training Epoch: 2 [33664/50048]	Loss: 2.9003
Training Epoch: 2 [33792/50048]	Loss: 2.9764
Training Epoch: 2 [33920/50048]	Loss: 3.1331
Training Epoch: 2 [34048/50048]	Loss: 2.7458
Training Epoch: 2 [34176/50048]	Loss: 2.8603
Training Epoch: 2 [34304/50048]	Loss: 2.9349
Training Epoch: 2 [34432/50048]	Loss: 3.1802
Training Epoch: 2 [34560/50048]	Loss: 2.9806
Training Epoch: 2 [34688/50048]	Loss: 3.0843
Training Epoch: 2 [34816/50048]	Loss: 2.9056
Training Epoch: 2 [34944/50048]	Loss: 3.0368
Training Epoch: 2 [35072/50048]	Loss: 2.9700
Training Epoch: 2 [35200/50048]	Loss: 2.9264
Training Epoch: 2 [35328/50048]	Loss: 2.9198
Training Epoch: 2 [35456/50048]	Loss: 3.0138
Training Epoch: 2 [35584/50048]	Loss: 3.0945
Training Epoch: 2 [35712/50048]	Loss: 3.3754
Training Epoch: 2 [35840/50048]	Loss: 2.9695
Training Epoch: 2 [35968/50048]	Loss: 3.0373
Training Epoch: 2 [36096/50048]	Loss: 3.0997
Training Epoch: 2 [36224/50048]	Loss: 3.0666
Training Epoch: 2 [36352/50048]	Loss: 3.3429
Training Epoch: 2 [36480/50048]	Loss: 3.0261
Training Epoch: 2 [36608/50048]	Loss: 2.8758
Training Epoch: 2 [36736/50048]	Loss: 3.2063
Training Epoch: 2 [36864/50048]	Loss: 3.2347
Training Epoch: 2 [36992/50048]	Loss: 2.8507
Training Epoch: 2 [37120/50048]	Loss: 2.8264
Training Epoch: 2 [37248/50048]	Loss: 3.1003
Training Epoch: 2 [37376/50048]	Loss: 2.9209
Training Epoch: 2 [37504/50048]	Loss: 3.0001
Training Epoch: 2 [37632/50048]	Loss: 2.9787
Training Epoch: 2 [37760/50048]	Loss: 2.7921
Training Epoch: 2 [37888/50048]	Loss: 2.8355
Training Epoch: 2 [38016/50048]	Loss: 3.2009
Training Epoch: 2 [38144/50048]	Loss: 3.0439
Training Epoch: 2 [38272/50048]	Loss: 2.8731
Training Epoch: 2 [38400/50048]	Loss: 3.0011
Training Epoch: 2 [38528/50048]	Loss: 2.9877
Training Epoch: 2 [38656/50048]	Loss: 2.9308
Training Epoch: 2 [38784/50048]	Loss: 3.1337
Training Epoch: 2 [38912/50048]	Loss: 3.0027
Training Epoch: 2 [39040/50048]	Loss: 3.0698
Training Epoch: 2 [39168/50048]	Loss: 2.8066
Training Epoch: 2 [39296/50048]	Loss: 2.8796
Training Epoch: 2 [39424/50048]	Loss: 2.9170
Training Epoch: 2 [39552/50048]	Loss: 3.0053
Training Epoch: 2 [39680/50048]	Loss: 2.8981
Training Epoch: 2 [39808/50048]	Loss: 3.0466
Training Epoch: 2 [39936/50048]	Loss: 2.5853
Training Epoch: 2 [40064/50048]	Loss: 2.8604
Training Epoch: 2 [40192/50048]	Loss: 3.0388
Training Epoch: 2 [40320/50048]	Loss: 2.8371
Training Epoch: 2 [40448/50048]	Loss: 2.7991
Training Epoch: 2 [40576/50048]	Loss: 2.9627
Training Epoch: 2 [40704/50048]	Loss: 2.6838
Training Epoch: 2 [40832/50048]	Loss: 2.7950
Training Epoch: 2 [40960/50048]	Loss: 2.7744
Training Epoch: 2 [41088/50048]	Loss: 2.9281
Training Epoch: 2 [41216/50048]	Loss: 2.7845
Training Epoch: 2 [41344/50048]	Loss: 2.9078
Training Epoch: 2 [41472/50048]	Loss: 3.0700
Training Epoch: 2 [41600/50048]	Loss: 2.8966
Training Epoch: 2 [41728/50048]	Loss: 2.9314
Training Epoch: 2 [41856/50048]	Loss: 2.9150
Training Epoch: 2 [41984/50048]	Loss: 2.9073
Training Epoch: 2 [42112/50048]	Loss: 3.0347
Training Epoch: 2 [42240/50048]	Loss: 2.9119
Training Epoch: 2 [42368/50048]	Loss: 3.0125
Training Epoch: 2 [42496/50048]	Loss: 2.9759
Training Epoch: 2 [42624/50048]	Loss: 3.1528
Training Epoch: 2 [42752/50048]	Loss: 3.1531
Training Epoch: 2 [42880/50048]	Loss: 2.8236
Training Epoch: 2 [43008/50048]	Loss: 3.0047
Training Epoch: 2 [43136/50048]	Loss: 2.9389
Training Epoch: 2 [43264/50048]	Loss: 2.8274
Training Epoch: 2 [43392/50048]	Loss: 2.8786
Training Epoch: 2 [43520/50048]	Loss: 2.8899
Training Epoch: 2 [43648/50048]	Loss: 2.8880
Training Epoch: 2 [43776/50048]	Loss: 2.8723
Training Epoch: 2 [43904/50048]	Loss: 2.7407
Training Epoch: 2 [44032/50048]	Loss: 2.9934
Training Epoch: 2 [44160/50048]	Loss: 2.8702
Training Epoch: 2 [44288/50048]	Loss: 3.0662
Training Epoch: 2 [44416/50048]	Loss: 2.7750
Training Epoch: 2 [44544/50048]	Loss: 2.7627
Training Epoch: 2 [44672/50048]	Loss: 2.7889
Training Epoch: 2 [44800/50048]	Loss: 3.0545
Training Epoch: 2 [44928/50048]	Loss: 2.9592
Training Epoch: 2 [45056/50048]	Loss: 3.0435
Training Epoch: 2 [45184/50048]	Loss: 2.9875
Training Epoch: 2 [45312/50048]	Loss: 2.8706
Training Epoch: 2 [45440/50048]	Loss: 2.9042
Training Epoch: 2 [45568/50048]	Loss: 2.9739
Training Epoch: 2 [45696/50048]	Loss: 2.9740
Training Epoch: 2 [45824/50048]	Loss: 2.7960
Training Epoch: 2 [45952/50048]	Loss: 3.2394
Training Epoch: 2 [46080/50048]	Loss: 2.9096
Training Epoch: 2 [46208/50048]	Loss: 3.1478
Training Epoch: 2 [46336/50048]	Loss: 2.8249
Training Epoch: 2 [46464/50048]	Loss: 3.0100
Training Epoch: 2 [46592/50048]	Loss: 2.6691
Training Epoch: 2 [46720/50048]	Loss: 2.8120
Training Epoch: 2 [46848/50048]	Loss: 2.7320
Training Epoch: 2 [46976/50048]	Loss: 2.6705
Training Epoch: 2 [47104/50048]	Loss: 2.8584
Training Epoch: 2 [47232/50048]	Loss: 3.1622
Training Epoch: 2 [47360/50048]	Loss: 2.8317
Training Epoch: 2 [47488/50048]	Loss: 2.7237
Training Epoch: 2 [47616/50048]	Loss: 2.9198
Training Epoch: 2 [47744/50048]	Loss: 2.9533
Training Epoch: 2 [47872/50048]	Loss: 3.2543
Training Epoch: 2 [48000/50048]	Loss: 2.9813
Training Epoch: 2 [48128/50048]	Loss: 2.8622
Training Epoch: 2 [48256/50048]	Loss: 2.7396
Training Epoch: 2 [48384/50048]	Loss: 2.6427
Training Epoch: 2 [48512/50048]	Loss: 2.7383
Training Epoch: 2 [48640/50048]	Loss: 2.9356
Training Epoch: 2 [48768/50048]	Loss: 3.2299
Training Epoch: 2 [48896/50048]	Loss: 2.8642
Training Epoch: 2 [49024/50048]	Loss: 3.0371
Training Epoch: 2 [49152/50048]	Loss: 2.9469
Training Epoch: 2 [49280/50048]	Loss: 2.9839
Training Epoch: 2 [49408/50048]	Loss: 2.8478
Training Epoch: 2 [49536/50048]	Loss: 3.0441
Training Epoch: 2 [49664/50048]	Loss: 2.7296
Training Epoch: 2 [49792/50048]	Loss: 2.8548
Training Epoch: 2 [49920/50048]	Loss: 3.0018
Training Epoch: 2 [50048/50048]	Loss: 3.1721
Validation Epoch: 2, Average loss: 0.0273, Accuracy: 0.1942
Training Epoch: 3 [128/50048]	Loss: 3.0250
Training Epoch: 3 [256/50048]	Loss: 2.6874
Training Epoch: 3 [384/50048]	Loss: 2.9340
Training Epoch: 3 [512/50048]	Loss: 2.8659
Training Epoch: 3 [640/50048]	Loss: 2.9129
Training Epoch: 3 [768/50048]	Loss: 2.7796
Training Epoch: 3 [896/50048]	Loss: 2.7546
Training Epoch: 3 [1024/50048]	Loss: 2.6579
Training Epoch: 3 [1152/50048]	Loss: 2.8184
Training Epoch: 3 [1280/50048]	Loss: 2.5947
Training Epoch: 3 [1408/50048]	Loss: 2.6447
Training Epoch: 3 [1536/50048]	Loss: 3.0034
Training Epoch: 3 [1664/50048]	Loss: 2.7855
Training Epoch: 3 [1792/50048]	Loss: 2.9114
Training Epoch: 3 [1920/50048]	Loss: 2.8124
Training Epoch: 3 [2048/50048]	Loss: 2.8170
Training Epoch: 3 [2176/50048]	Loss: 2.9195
Training Epoch: 3 [2304/50048]	Loss: 2.7699
Training Epoch: 3 [2432/50048]	Loss: 2.9067
Training Epoch: 3 [2560/50048]	Loss: 3.2214
Training Epoch: 3 [2688/50048]	Loss: 2.7117
Training Epoch: 3 [2816/50048]	Loss: 2.6214
Training Epoch: 3 [2944/50048]	Loss: 2.8293
Training Epoch: 3 [3072/50048]	Loss: 3.0557
Training Epoch: 3 [3200/50048]	Loss: 3.2107
Training Epoch: 3 [3328/50048]	Loss: 2.9467
Training Epoch: 3 [3456/50048]	Loss: 3.0509
Training Epoch: 3 [3584/50048]	Loss: 2.7238
Training Epoch: 3 [3712/50048]	Loss: 2.7931
Training Epoch: 3 [3840/50048]	Loss: 2.8033
Training Epoch: 3 [3968/50048]	Loss: 2.7365
Training Epoch: 3 [4096/50048]	Loss: 2.6960
Training Epoch: 3 [4224/50048]	Loss: 2.5303
Training Epoch: 3 [4352/50048]	Loss: 2.7209
Training Epoch: 3 [4480/50048]	Loss: 2.8163
Training Epoch: 3 [4608/50048]	Loss: 2.9667
Training Epoch: 3 [4736/50048]	Loss: 2.5616
Training Epoch: 3 [4864/50048]	Loss: 2.6065
Training Epoch: 3 [4992/50048]	Loss: 2.8751
Training Epoch: 3 [5120/50048]	Loss: 2.8024
Training Epoch: 3 [5248/50048]	Loss: 2.7613
Training Epoch: 3 [5376/50048]	Loss: 2.7624
Training Epoch: 3 [5504/50048]	Loss: 2.6964
Training Epoch: 3 [5632/50048]	Loss: 2.9460
Training Epoch: 3 [5760/50048]	Loss: 2.7511
Training Epoch: 3 [5888/50048]	Loss: 2.6542
Training Epoch: 3 [6016/50048]	Loss: 2.6137
Training Epoch: 3 [6144/50048]	Loss: 3.0403
Training Epoch: 3 [6272/50048]	Loss: 2.7052
Training Epoch: 3 [6400/50048]	Loss: 2.3343
Training Epoch: 3 [6528/50048]	Loss: 2.8127
Training Epoch: 3 [6656/50048]	Loss: 2.6860
Training Epoch: 3 [6784/50048]	Loss: 2.8421
Training Epoch: 3 [6912/50048]	Loss: 3.0011
Training Epoch: 3 [7040/50048]	Loss: 2.8430
Training Epoch: 3 [7168/50048]	Loss: 2.8046
Training Epoch: 3 [7296/50048]	Loss: 2.7442
Training Epoch: 3 [7424/50048]	Loss: 2.7296
Training Epoch: 3 [7552/50048]	Loss: 2.9256
Training Epoch: 3 [7680/50048]	Loss: 2.8439
Training Epoch: 3 [7808/50048]	Loss: 2.8375
Training Epoch: 3 [7936/50048]	Loss: 2.8145
Training Epoch: 3 [8064/50048]	Loss: 2.7924
Training Epoch: 3 [8192/50048]	Loss: 2.9378
Training Epoch: 3 [8320/50048]	Loss: 2.8462
Training Epoch: 3 [8448/50048]	Loss: 2.7106
Training Epoch: 3 [8576/50048]	Loss: 2.6851
Training Epoch: 3 [8704/50048]	Loss: 2.7945
Training Epoch: 3 [8832/50048]	Loss: 2.5948
Training Epoch: 3 [8960/50048]	Loss: 2.8821
Training Epoch: 3 [9088/50048]	Loss: 2.5724
Training Epoch: 3 [9216/50048]	Loss: 2.9090
Training Epoch: 3 [9344/50048]	Loss: 2.8790
Training Epoch: 3 [9472/50048]	Loss: 3.0365
Training Epoch: 3 [9600/50048]	Loss: 2.8847
Training Epoch: 3 [9728/50048]	Loss: 2.7383
Training Epoch: 3 [9856/50048]	Loss: 2.7713
Training Epoch: 3 [9984/50048]	Loss: 2.8028
Training Epoch: 3 [10112/50048]	Loss: 2.6564
Training Epoch: 3 [10240/50048]	Loss: 2.6517
Training Epoch: 3 [10368/50048]	Loss: 2.7684
Training Epoch: 3 [10496/50048]	Loss: 3.1334
Training Epoch: 3 [10624/50048]	Loss: 2.9099
Training Epoch: 3 [10752/50048]	Loss: 2.7430
Training Epoch: 3 [10880/50048]	Loss: 2.9028
Training Epoch: 3 [11008/50048]	Loss: 2.8266
Training Epoch: 3 [11136/50048]	Loss: 2.6659
Training Epoch: 3 [11264/50048]	Loss: 2.6585
Training Epoch: 3 [11392/50048]	Loss: 2.8779
Training Epoch: 3 [11520/50048]	Loss: 2.6494
Training Epoch: 3 [11648/50048]	Loss: 3.0938
Training Epoch: 3 [11776/50048]	Loss: 2.5438
Training Epoch: 3 [11904/50048]	Loss: 3.0722
Training Epoch: 3 [12032/50048]	Loss: 2.8949
Training Epoch: 3 [12160/50048]	Loss: 2.7424
Training Epoch: 3 [12288/50048]	Loss: 2.8660
Training Epoch: 3 [12416/50048]	Loss: 2.7610
Training Epoch: 3 [12544/50048]	Loss: 2.4879
Training Epoch: 3 [12672/50048]	Loss: 3.1142
Training Epoch: 3 [12800/50048]	Loss: 2.5924
Training Epoch: 3 [12928/50048]	Loss: 2.8684
Training Epoch: 3 [13056/50048]	Loss: 2.7314
Training Epoch: 3 [13184/50048]	Loss: 2.6338
Training Epoch: 3 [13312/50048]	Loss: 2.7706
Training Epoch: 3 [13440/50048]	Loss: 2.5988
Training Epoch: 3 [13568/50048]	Loss: 2.7947
Training Epoch: 3 [13696/50048]	Loss: 2.8498
Training Epoch: 3 [13824/50048]	Loss: 2.9570
Training Epoch: 3 [13952/50048]	Loss: 2.7365
Training Epoch: 3 [14080/50048]	Loss: 2.6602
Training Epoch: 3 [14208/50048]	Loss: 2.7375
Training Epoch: 3 [14336/50048]	Loss: 2.6537
Training Epoch: 3 [14464/50048]	Loss: 2.6711
Training Epoch: 3 [14592/50048]	Loss: 2.6552
Training Epoch: 3 [14720/50048]	Loss: 2.7098
Training Epoch: 3 [14848/50048]	Loss: 2.6510
Training Epoch: 3 [14976/50048]	Loss: 2.7775
Training Epoch: 3 [15104/50048]	Loss: 2.6414
Training Epoch: 3 [15232/50048]	Loss: 2.9331
Training Epoch: 3 [15360/50048]	Loss: 2.8574
Training Epoch: 3 [15488/50048]	Loss: 2.7323
Training Epoch: 3 [15616/50048]	Loss: 2.9329
Training Epoch: 3 [15744/50048]	Loss: 2.5227
Training Epoch: 3 [15872/50048]	Loss: 2.7981
Training Epoch: 3 [16000/50048]	Loss: 2.6701
Training Epoch: 3 [16128/50048]	Loss: 2.7666
Training Epoch: 3 [16256/50048]	Loss: 2.9315
Training Epoch: 3 [16384/50048]	Loss: 2.7101
Training Epoch: 3 [16512/50048]	Loss: 2.8400
Training Epoch: 3 [16640/50048]	Loss: 2.7984
Training Epoch: 3 [16768/50048]	Loss: 2.4310
Training Epoch: 3 [16896/50048]	Loss: 2.7396
Training Epoch: 3 [17024/50048]	Loss: 3.0458
Training Epoch: 3 [17152/50048]	Loss: 2.4689
Training Epoch: 3 [17280/50048]	Loss: 2.8249
Training Epoch: 3 [17408/50048]	Loss: 2.8487
Training Epoch: 3 [17536/50048]	Loss: 2.5680
Training Epoch: 3 [17664/50048]	Loss: 2.5955
Training Epoch: 3 [17792/50048]	Loss: 2.7805
Training Epoch: 3 [17920/50048]	Loss: 2.8992
Training Epoch: 3 [18048/50048]	Loss: 2.8895
Training Epoch: 3 [18176/50048]	Loss: 2.9967
Training Epoch: 3 [18304/50048]	Loss: 2.9808
Training Epoch: 3 [18432/50048]	Loss: 2.8026
Training Epoch: 3 [18560/50048]	Loss: 2.9691
Training Epoch: 3 [18688/50048]	Loss: 2.5580
Training Epoch: 3 [18816/50048]	Loss: 2.7179
Training Epoch: 3 [18944/50048]	Loss: 2.7822
Training Epoch: 3 [19072/50048]	Loss: 2.6326
Training Epoch: 3 [19200/50048]	Loss: 2.7633
Training Epoch: 3 [19328/50048]	Loss: 2.9117
Training Epoch: 3 [19456/50048]	Loss: 2.8337
Training Epoch: 3 [19584/50048]	Loss: 2.9008
Training Epoch: 3 [19712/50048]	Loss: 2.6100
Training Epoch: 3 [19840/50048]	Loss: 2.9281
Training Epoch: 3 [19968/50048]	Loss: 2.8746
Training Epoch: 3 [20096/50048]	Loss: 2.6383
Training Epoch: 3 [20224/50048]	Loss: 2.4672
Training Epoch: 3 [20352/50048]	Loss: 2.8425
Training Epoch: 3 [20480/50048]	Loss: 2.5524
Training Epoch: 3 [20608/50048]	Loss: 2.6191
Training Epoch: 3 [20736/50048]	Loss: 2.6146
Training Epoch: 3 [20864/50048]	Loss: 2.6678
Training Epoch: 3 [20992/50048]	Loss: 2.7110
Training Epoch: 3 [21120/50048]	Loss: 2.7630
Training Epoch: 3 [21248/50048]	Loss: 2.8590
Training Epoch: 3 [21376/50048]	Loss: 2.5360
Training Epoch: 3 [21504/50048]	Loss: 2.6786
Training Epoch: 3 [21632/50048]	Loss: 2.4736
Training Epoch: 3 [21760/50048]	Loss: 2.5349
Training Epoch: 3 [21888/50048]	Loss: 2.7697
Training Epoch: 3 [22016/50048]	Loss: 2.9576
Training Epoch: 3 [22144/50048]	Loss: 2.8773
Training Epoch: 3 [22272/50048]	Loss: 2.6121
Training Epoch: 3 [22400/50048]	Loss: 2.8057
Training Epoch: 3 [22528/50048]	Loss: 2.6909
Training Epoch: 3 [22656/50048]	Loss: 2.6537
Training Epoch: 3 [22784/50048]	Loss: 2.8888
Training Epoch: 3 [22912/50048]	Loss: 2.8808
Training Epoch: 3 [23040/50048]	Loss: 2.6975
Training Epoch: 3 [23168/50048]	Loss: 2.7623
Training Epoch: 3 [23296/50048]	Loss: 2.7022
Training Epoch: 3 [23424/50048]	Loss: 2.7801
Training Epoch: 3 [23552/50048]	Loss: 2.7672
Training Epoch: 3 [23680/50048]	Loss: 2.6179
Training Epoch: 3 [23808/50048]	Loss: 2.7312
Training Epoch: 3 [23936/50048]	Loss: 2.7843
Training Epoch: 3 [24064/50048]	Loss: 2.7763
Training Epoch: 3 [24192/50048]	Loss: 2.6882
Training Epoch: 3 [24320/50048]	Loss: 2.6340
Training Epoch: 3 [24448/50048]	Loss: 2.9416
Training Epoch: 3 [24576/50048]	Loss: 2.6483
Training Epoch: 3 [24704/50048]	Loss: 2.6317
Training Epoch: 3 [24832/50048]	Loss: 2.8090
Training Epoch: 3 [24960/50048]	Loss: 2.5922
Training Epoch: 3 [25088/50048]	Loss: 2.6200
Training Epoch: 3 [25216/50048]	Loss: 2.6385
Training Epoch: 3 [25344/50048]	Loss: 2.3638
Training Epoch: 3 [25472/50048]	Loss: 2.5015
Training Epoch: 3 [25600/50048]	Loss: 2.7094
Training Epoch: 3 [25728/50048]	Loss: 2.5083
Training Epoch: 3 [25856/50048]	Loss: 2.7042
Training Epoch: 3 [25984/50048]	Loss: 2.7829
Training Epoch: 3 [26112/50048]	Loss: 2.5737
Training Epoch: 3 [26240/50048]	Loss: 2.5391
Training Epoch: 3 [26368/50048]	Loss: 2.7573
Training Epoch: 3 [26496/50048]	Loss: 2.7184
Training Epoch: 3 [26624/50048]	Loss: 2.6052
Training Epoch: 3 [26752/50048]	Loss: 2.6397
Training Epoch: 3 [26880/50048]	Loss: 2.8336
Training Epoch: 3 [27008/50048]	Loss: 2.6953
Training Epoch: 3 [27136/50048]	Loss: 3.0124
Training Epoch: 3 [27264/50048]	Loss: 2.6102
Training Epoch: 3 [27392/50048]	Loss: 2.8047
Training Epoch: 3 [27520/50048]	Loss: 2.5877
Training Epoch: 3 [27648/50048]	Loss: 2.7370
Training Epoch: 3 [27776/50048]	Loss: 2.8927
Training Epoch: 3 [27904/50048]	Loss: 2.9197
Training Epoch: 3 [28032/50048]	Loss: 2.6313
Training Epoch: 3 [28160/50048]	Loss: 2.8210
Training Epoch: 3 [28288/50048]	Loss: 2.7969
Training Epoch: 3 [28416/50048]	Loss: 2.6670
Training Epoch: 3 [28544/50048]	Loss: 2.6889
Training Epoch: 3 [28672/50048]	Loss: 2.5224
Training Epoch: 3 [28800/50048]	Loss: 2.5393
Training Epoch: 3 [28928/50048]	Loss: 2.8683
Training Epoch: 3 [29056/50048]	Loss: 2.8838
Training Epoch: 3 [29184/50048]	Loss: 2.8467
Training Epoch: 3 [29312/50048]	Loss: 2.7597
Training Epoch: 3 [29440/50048]	Loss: 2.6713
Training Epoch: 3 [29568/50048]	Loss: 2.5638
Training Epoch: 3 [29696/50048]	Loss: 2.7518
Training Epoch: 3 [29824/50048]	Loss: 2.4928
Training Epoch: 3 [29952/50048]	Loss: 2.5492
Training Epoch: 3 [30080/50048]	Loss: 2.5374
Training Epoch: 3 [30208/50048]	Loss: 2.9329
Training Epoch: 3 [30336/50048]	Loss: 2.5671
Training Epoch: 3 [30464/50048]	Loss: 2.5655
Training Epoch: 3 [30592/50048]	Loss: 2.6746
Training Epoch: 3 [30720/50048]	Loss: 2.8129
Training Epoch: 3 [30848/50048]	Loss: 2.5302
Training Epoch: 3 [30976/50048]	Loss: 2.4659
Training Epoch: 3 [31104/50048]	Loss: 2.6611
Training Epoch: 3 [31232/50048]	Loss: 2.6942
Training Epoch: 3 [31360/50048]	Loss: 2.5227
Training Epoch: 3 [31488/50048]	Loss: 2.7039
Training Epoch: 3 [31616/50048]	Loss: 2.8410
Training Epoch: 3 [31744/50048]	Loss: 2.3217
Training Epoch: 3 [31872/50048]	Loss: 2.6885
Training Epoch: 3 [32000/50048]	Loss: 2.5899
Training Epoch: 3 [32128/50048]	Loss: 2.4649
Training Epoch: 3 [32256/50048]	Loss: 2.4607
Training Epoch: 3 [32384/50048]	Loss: 2.7520
Training Epoch: 3 [32512/50048]	Loss: 2.7364
Training Epoch: 3 [32640/50048]	Loss: 2.8221
Training Epoch: 3 [32768/50048]	Loss: 2.4971
Training Epoch: 3 [32896/50048]	Loss: 2.9224
Training Epoch: 3 [33024/50048]	Loss: 2.7098
Training Epoch: 3 [33152/50048]	Loss: 2.6498
Training Epoch: 3 [33280/50048]	Loss: 2.8195
Training Epoch: 3 [33408/50048]	Loss: 3.0215
Training Epoch: 3 [33536/50048]	Loss: 2.8373
Training Epoch: 3 [33664/50048]	Loss: 2.6374
Training Epoch: 3 [33792/50048]	Loss: 2.5817
Training Epoch: 3 [33920/50048]	Loss: 2.5112
Training Epoch: 3 [34048/50048]	Loss: 2.9741
Training Epoch: 3 [34176/50048]	Loss: 2.8996
Training Epoch: 3 [34304/50048]	Loss: 2.5674
Training Epoch: 3 [34432/50048]	Loss: 2.8005
Training Epoch: 3 [34560/50048]	Loss: 2.5323
Training Epoch: 3 [34688/50048]	Loss: 2.6148
Training Epoch: 3 [34816/50048]	Loss: 2.2076
Training Epoch: 3 [34944/50048]	Loss: 2.6333
Training Epoch: 3 [35072/50048]	Loss: 2.7188
Training Epoch: 3 [35200/50048]	Loss: 2.6544
Training Epoch: 3 [35328/50048]	Loss: 2.6196
Training Epoch: 3 [35456/50048]	Loss: 3.1013
Training Epoch: 3 [35584/50048]	Loss: 2.4784
Training Epoch: 3 [35712/50048]	Loss: 2.5780
Training Epoch: 3 [35840/50048]	Loss: 2.7785
Training Epoch: 3 [35968/50048]	Loss: 2.4662
Training Epoch: 3 [36096/50048]	Loss: 2.7997
Training Epoch: 3 [36224/50048]	Loss: 2.5678
Training Epoch: 3 [36352/50048]	Loss: 2.8357
Training Epoch: 3 [36480/50048]	Loss: 2.5629
Training Epoch: 3 [36608/50048]	Loss: 2.8810
Training Epoch: 3 [36736/50048]	Loss: 2.7196
Training Epoch: 3 [36864/50048]	Loss: 2.7113
Training Epoch: 3 [36992/50048]	Loss: 2.6523
Training Epoch: 3 [37120/50048]	Loss: 2.2786
Training Epoch: 3 [37248/50048]	Loss: 2.7283
Training Epoch: 3 [37376/50048]	Loss: 2.6055
Training Epoch: 3 [37504/50048]	Loss: 2.8105
Training Epoch: 3 [37632/50048]	Loss: 2.4757
Training Epoch: 3 [37760/50048]	Loss: 2.6582
Training Epoch: 3 [37888/50048]	Loss: 2.6116
Training Epoch: 3 [38016/50048]	Loss: 2.6458
Training Epoch: 3 [38144/50048]	Loss: 2.5966
Training Epoch: 3 [38272/50048]	Loss: 2.6822
Training Epoch: 3 [38400/50048]	Loss: 2.6737
Training Epoch: 3 [38528/50048]	Loss: 2.6873
Training Epoch: 3 [38656/50048]	Loss: 2.5181
Training Epoch: 3 [38784/50048]	Loss: 2.7732
Training Epoch: 3 [38912/50048]	Loss: 2.7910
Training Epoch: 3 [39040/50048]	Loss: 2.5857
Training Epoch: 3 [39168/50048]	Loss: 2.7029
Training Epoch: 3 [39296/50048]	Loss: 2.4259
Training Epoch: 3 [39424/50048]	Loss: 2.5291
Training Epoch: 3 [39552/50048]	Loss: 2.7514
Training Epoch: 3 [39680/50048]	Loss: 2.5799
Training Epoch: 3 [39808/50048]	Loss: 2.5741
Training Epoch: 3 [39936/50048]	Loss: 2.4052
Training Epoch: 3 [40064/50048]	Loss: 2.3918
Training Epoch: 3 [40192/50048]	Loss: 2.2891
Training Epoch: 3 [40320/50048]	Loss: 2.7745
Training Epoch: 3 [40448/50048]	Loss: 2.5043
Training Epoch: 3 [40576/50048]	Loss: 2.8782
Training Epoch: 3 [40704/50048]	Loss: 2.5979
Training Epoch: 3 [40832/50048]	Loss: 2.7307
Training Epoch: 3 [40960/50048]	Loss: 2.4876
Training Epoch: 3 [41088/50048]	Loss: 2.8136
Training Epoch: 3 [41216/50048]	Loss: 2.4993
Training Epoch: 3 [41344/50048]	Loss: 2.7693
Training Epoch: 3 [41472/50048]	Loss: 2.7090
Training Epoch: 3 [41600/50048]	Loss: 2.7612
Training Epoch: 3 [41728/50048]	Loss: 2.7937
Training Epoch: 3 [41856/50048]	Loss: 2.6308
Training Epoch: 3 [41984/50048]	Loss: 2.9120
Training Epoch: 3 [42112/50048]	Loss: 2.7788
Training Epoch: 3 [42240/50048]	Loss: 2.7583
Training Epoch: 3 [42368/50048]	Loss: 2.6305
Training Epoch: 3 [42496/50048]	Loss: 2.5424
Training Epoch: 3 [42624/50048]	Loss: 2.7672
Training Epoch: 3 [42752/50048]	Loss: 2.9071
Training Epoch: 3 [42880/50048]	Loss: 2.4568
Training Epoch: 3 [43008/50048]	Loss: 2.3384
Training Epoch: 3 [43136/50048]	Loss: 2.8751
Training Epoch: 3 [43264/50048]	Loss: 2.5138
Training Epoch: 3 [43392/50048]	Loss: 2.7054
Training Epoch: 3 [43520/50048]	Loss: 2.6812
Training Epoch: 3 [43648/50048]	Loss: 2.5566
Training Epoch: 3 [43776/50048]	Loss: 2.7091
Training Epoch: 3 [43904/50048]	Loss: 2.7007
Training Epoch: 3 [44032/50048]	Loss: 2.7952
Training Epoch: 3 [44160/50048]	Loss: 2.5498
Training Epoch: 3 [44288/50048]	Loss: 2.7573
Training Epoch: 3 [44416/50048]	Loss: 2.4835
Training Epoch: 3 [44544/50048]	Loss: 2.7108
Training Epoch: 3 [44672/50048]	Loss: 2.6484
Training Epoch: 3 [44800/50048]	Loss: 2.6206
Training Epoch: 3 [44928/50048]	Loss: 2.6325
Training Epoch: 3 [45056/50048]	Loss: 2.7793
Training Epoch: 3 [45184/50048]	Loss: 2.7164
Training Epoch: 3 [45312/50048]	Loss: 2.6781
Training Epoch: 3 [45440/50048]	Loss: 2.4258
Training Epoch: 3 [45568/50048]	Loss: 2.6685
Training Epoch: 3 [45696/50048]	Loss: 2.5792
Training Epoch: 3 [45824/50048]	Loss: 2.5395
Training Epoch: 3 [45952/50048]	Loss: 2.5912
Training Epoch: 3 [46080/50048]	Loss: 2.5577
Training Epoch: 3 [46208/50048]	Loss: 2.6881
Training Epoch: 3 [46336/50048]	Loss: 2.5072
Training Epoch: 3 [46464/50048]	Loss: 2.6387
Training Epoch: 3 [46592/50048]	Loss: 2.6246
Training Epoch: 3 [46720/50048]	Loss: 2.7061
Training Epoch: 3 [46848/50048]	Loss: 2.6181
Training Epoch: 3 [46976/50048]	Loss: 2.5837
Training Epoch: 3 [47104/50048]	Loss: 2.5376
Training Epoch: 3 [47232/50048]	Loss: 2.6588
Training Epoch: 3 [47360/50048]	Loss: 2.5258
Training Epoch: 3 [47488/50048]	Loss: 2.6207
Training Epoch: 3 [47616/50048]	Loss: 2.6561
Training Epoch: 3 [47744/50048]	Loss: 2.4972
Training Epoch: 3 [47872/50048]	Loss: 2.6432
Training Epoch: 3 [48000/50048]	Loss: 2.3381
Training Epoch: 3 [48128/50048]	Loss: 2.4670
Training Epoch: 3 [48256/50048]	Loss: 2.6258
Training Epoch: 3 [48384/50048]	Loss: 2.3185
Training Epoch: 3 [48512/50048]	Loss: 2.6303
Training Epoch: 3 [48640/50048]	Loss: 2.5069
Training Epoch: 3 [48768/50048]	Loss: 2.4008
Training Epoch: 3 [48896/50048]	Loss: 2.6368
Training Epoch: 3 [49024/50048]	Loss: 2.6194
Training Epoch: 3 [49152/50048]	Loss: 2.5307
Training Epoch: 3 [49280/50048]	Loss: 2.4917
Training Epoch: 3 [49408/50048]	Loss: 2.7223
Training Epoch: 3 [49536/50048]	Loss: 2.4858
Training Epoch: 3 [49664/50048]	Loss: 2.3808
Training Epoch: 3 [49792/50048]	Loss: 2.6385
Training Epoch: 3 [49920/50048]	Loss: 2.4401
Training Epoch: 3 [50048/50048]	Loss: 2.6120
Validation Epoch: 3, Average loss: 0.0227, Accuracy: 0.2803
Training Epoch: 4 [128/50048]	Loss: 2.6308
Training Epoch: 4 [256/50048]	Loss: 2.2615
Training Epoch: 4 [384/50048]	Loss: 2.6765
Training Epoch: 4 [512/50048]	Loss: 2.3715
Training Epoch: 4 [640/50048]	Loss: 2.5398
Training Epoch: 4 [768/50048]	Loss: 2.5289
Training Epoch: 4 [896/50048]	Loss: 2.5578
Training Epoch: 4 [1024/50048]	Loss: 2.5325
Training Epoch: 4 [1152/50048]	Loss: 2.6641
Training Epoch: 4 [1280/50048]	Loss: 2.5541
Training Epoch: 4 [1408/50048]	Loss: 2.4675
Training Epoch: 4 [1536/50048]	Loss: 2.5382
Training Epoch: 4 [1664/50048]	Loss: 2.3109
Training Epoch: 4 [1792/50048]	Loss: 2.5618
Training Epoch: 4 [1920/50048]	Loss: 2.3555
Training Epoch: 4 [2048/50048]	Loss: 2.5892
Training Epoch: 4 [2176/50048]	Loss: 2.5120
Training Epoch: 4 [2304/50048]	Loss: 2.5455
Training Epoch: 4 [2432/50048]	Loss: 2.5297
Training Epoch: 4 [2560/50048]	Loss: 2.5375
Training Epoch: 4 [2688/50048]	Loss: 2.4533
Training Epoch: 4 [2816/50048]	Loss: 2.4664
Training Epoch: 4 [2944/50048]	Loss: 2.2724
Training Epoch: 4 [3072/50048]	Loss: 2.6097
Training Epoch: 4 [3200/50048]	Loss: 2.6228
Training Epoch: 4 [3328/50048]	Loss: 2.3571
Training Epoch: 4 [3456/50048]	Loss: 2.4558
Training Epoch: 4 [3584/50048]	Loss: 2.6990
Training Epoch: 4 [3712/50048]	Loss: 2.2925
Training Epoch: 4 [3840/50048]	Loss: 2.6660
Training Epoch: 4 [3968/50048]	Loss: 2.5041
Training Epoch: 4 [4096/50048]	Loss: 2.4673
Training Epoch: 4 [4224/50048]	Loss: 2.5181
Training Epoch: 4 [4352/50048]	Loss: 2.6733
Training Epoch: 4 [4480/50048]	Loss: 2.5003
Training Epoch: 4 [4608/50048]	Loss: 2.6557
Training Epoch: 4 [4736/50048]	Loss: 2.3151
Training Epoch: 4 [4864/50048]	Loss: 2.4869
Training Epoch: 4 [4992/50048]	Loss: 2.5020
Training Epoch: 4 [5120/50048]	Loss: 2.5370
Training Epoch: 4 [5248/50048]	Loss: 2.4129
Training Epoch: 4 [5376/50048]	Loss: 2.3850
Training Epoch: 4 [5504/50048]	Loss: 2.5972
Training Epoch: 4 [5632/50048]	Loss: 2.3458
Training Epoch: 4 [5760/50048]	Loss: 2.6735
Training Epoch: 4 [5888/50048]	Loss: 2.5035
Training Epoch: 4 [6016/50048]	Loss: 2.8519
Training Epoch: 4 [6144/50048]	Loss: 2.3472
Training Epoch: 4 [6272/50048]	Loss: 2.5151
Training Epoch: 4 [6400/50048]	Loss: 2.5120
Training Epoch: 4 [6528/50048]	Loss: 2.3036
Training Epoch: 4 [6656/50048]	Loss: 2.7730
Training Epoch: 4 [6784/50048]	Loss: 2.4572
Training Epoch: 4 [6912/50048]	Loss: 2.6507
Training Epoch: 4 [7040/50048]	Loss: 2.4489
Training Epoch: 4 [7168/50048]	Loss: 2.6626
Training Epoch: 4 [7296/50048]	Loss: 2.6094
Training Epoch: 4 [7424/50048]	Loss: 2.8472
Training Epoch: 4 [7552/50048]	Loss: 2.6076
Training Epoch: 4 [7680/50048]	Loss: 2.6958
Training Epoch: 4 [7808/50048]	Loss: 2.4373
Training Epoch: 4 [7936/50048]	Loss: 2.5099
Training Epoch: 4 [8064/50048]	Loss: 2.3398
Training Epoch: 4 [8192/50048]	Loss: 2.4330
Training Epoch: 4 [8320/50048]	Loss: 2.7898
Training Epoch: 4 [8448/50048]	Loss: 2.2482
Training Epoch: 4 [8576/50048]	Loss: 2.2802
Training Epoch: 4 [8704/50048]	Loss: 2.6071
Training Epoch: 4 [8832/50048]	Loss: 2.4867
Training Epoch: 4 [8960/50048]	Loss: 2.4545
Training Epoch: 4 [9088/50048]	Loss: 2.4919
Training Epoch: 4 [9216/50048]	Loss: 2.5566
Training Epoch: 4 [9344/50048]	Loss: 2.6106
Training Epoch: 4 [9472/50048]	Loss: 2.3393
Training Epoch: 4 [9600/50048]	Loss: 2.4995
Training Epoch: 4 [9728/50048]	Loss: 2.5158
Training Epoch: 4 [9856/50048]	Loss: 2.5614
Training Epoch: 4 [9984/50048]	Loss: 2.4678
Training Epoch: 4 [10112/50048]	Loss: 2.3992
Training Epoch: 4 [10240/50048]	Loss: 2.5419
Training Epoch: 4 [10368/50048]	Loss: 2.3298
Training Epoch: 4 [10496/50048]	Loss: 2.0970
Training Epoch: 4 [10624/50048]	Loss: 2.6948
Training Epoch: 4 [10752/50048]	Loss: 2.6481
Training Epoch: 4 [10880/50048]	Loss: 2.5323
Training Epoch: 4 [11008/50048]	Loss: 2.7025
Training Epoch: 4 [11136/50048]	Loss: 2.5735
Training Epoch: 4 [11264/50048]	Loss: 2.2929
Training Epoch: 4 [11392/50048]	Loss: 2.4489
Training Epoch: 4 [11520/50048]	Loss: 2.8124
Training Epoch: 4 [11648/50048]	Loss: 2.7511
Training Epoch: 4 [11776/50048]	Loss: 2.6491
Training Epoch: 4 [11904/50048]	Loss: 2.2851
Training Epoch: 4 [12032/50048]	Loss: 2.6380
Training Epoch: 4 [12160/50048]	Loss: 2.6783
Training Epoch: 4 [12288/50048]	Loss: 2.3260
Training Epoch: 4 [12416/50048]	Loss: 2.3060
Training Epoch: 4 [12544/50048]	Loss: 2.4318
Training Epoch: 4 [12672/50048]	Loss: 2.4766
Training Epoch: 4 [12800/50048]	Loss: 2.3682
Training Epoch: 4 [12928/50048]	Loss: 2.2936
Training Epoch: 4 [13056/50048]	Loss: 2.3578
Training Epoch: 4 [13184/50048]	Loss: 2.4652
Training Epoch: 4 [13312/50048]	Loss: 2.5314
Training Epoch: 4 [13440/50048]	Loss: 2.5510
Training Epoch: 4 [13568/50048]	Loss: 2.6750
Training Epoch: 4 [13696/50048]	Loss: 2.3111
Training Epoch: 4 [13824/50048]	Loss: 2.4316
Training Epoch: 4 [13952/50048]	Loss: 2.5739
Training Epoch: 4 [14080/50048]	Loss: 2.3073
Training Epoch: 4 [14208/50048]	Loss: 2.5223
Training Epoch: 4 [14336/50048]	Loss: 2.5486
Training Epoch: 4 [14464/50048]	Loss: 2.2488
Training Epoch: 4 [14592/50048]	Loss: 2.4507
Training Epoch: 4 [14720/50048]	Loss: 2.2552
Training Epoch: 4 [14848/50048]	Loss: 2.3855
Training Epoch: 4 [14976/50048]	Loss: 2.5499
Training Epoch: 4 [15104/50048]	Loss: 2.6274
Training Epoch: 4 [15232/50048]	Loss: 2.5297
Training Epoch: 4 [15360/50048]	Loss: 2.4422
Training Epoch: 4 [15488/50048]	Loss: 2.3338
Training Epoch: 4 [15616/50048]	Loss: 2.3872
Training Epoch: 4 [15744/50048]	Loss: 2.4176
Training Epoch: 4 [15872/50048]	Loss: 2.4431
Training Epoch: 4 [16000/50048]	Loss: 2.2530
Training Epoch: 4 [16128/50048]	Loss: 2.7442
Training Epoch: 4 [16256/50048]	Loss: 2.4451
Training Epoch: 4 [16384/50048]	Loss: 2.2342
Training Epoch: 4 [16512/50048]	Loss: 2.4291
Training Epoch: 4 [16640/50048]	Loss: 2.5794
Training Epoch: 4 [16768/50048]	Loss: 2.6787
Training Epoch: 4 [16896/50048]	Loss: 2.4180
Training Epoch: 4 [17024/50048]	Loss: 2.3590
Training Epoch: 4 [17152/50048]	Loss: 2.4040
Training Epoch: 4 [17280/50048]	Loss: 2.4490
Training Epoch: 4 [17408/50048]	Loss: 2.4719
Training Epoch: 4 [17536/50048]	Loss: 2.4586
Training Epoch: 4 [17664/50048]	Loss: 2.6326
Training Epoch: 4 [17792/50048]	Loss: 2.3603
Training Epoch: 4 [17920/50048]	Loss: 2.2408
Training Epoch: 4 [18048/50048]	Loss: 2.6376
Training Epoch: 4 [18176/50048]	Loss: 2.2454
Training Epoch: 4 [18304/50048]	Loss: 2.2899
Training Epoch: 4 [18432/50048]	Loss: 2.4318
Training Epoch: 4 [18560/50048]	Loss: 2.8171
Training Epoch: 4 [18688/50048]	Loss: 2.3387
Training Epoch: 4 [18816/50048]	Loss: 2.4969
Training Epoch: 4 [18944/50048]	Loss: 2.6497
Training Epoch: 4 [19072/50048]	Loss: 2.3914
Training Epoch: 4 [19200/50048]	Loss: 2.4709
Training Epoch: 4 [19328/50048]	Loss: 2.3400
Training Epoch: 4 [19456/50048]	Loss: 2.1975
Training Epoch: 4 [19584/50048]	Loss: 2.4207
Training Epoch: 4 [19712/50048]	Loss: 2.5849
Training Epoch: 4 [19840/50048]	Loss: 2.5260
Training Epoch: 4 [19968/50048]	Loss: 2.4502
Training Epoch: 4 [20096/50048]	Loss: 2.4449
Training Epoch: 4 [20224/50048]	Loss: 2.7431
Training Epoch: 4 [20352/50048]	Loss: 2.6216
Training Epoch: 4 [20480/50048]	Loss: 2.8826
Training Epoch: 4 [20608/50048]	Loss: 2.3599
Training Epoch: 4 [20736/50048]	Loss: 2.5431
Training Epoch: 4 [20864/50048]	Loss: 2.4702
Training Epoch: 4 [20992/50048]	Loss: 2.5368
Training Epoch: 4 [21120/50048]	Loss: 2.3533
Training Epoch: 4 [21248/50048]	Loss: 2.5740
Training Epoch: 4 [21376/50048]	Loss: 2.5760
Training Epoch: 4 [21504/50048]	Loss: 2.2280
Training Epoch: 4 [21632/50048]	Loss: 2.4207
Training Epoch: 4 [21760/50048]	Loss: 2.4222
Training Epoch: 4 [21888/50048]	Loss: 2.8308
Training Epoch: 4 [22016/50048]	Loss: 2.8789
Training Epoch: 4 [22144/50048]	Loss: 2.5317
Training Epoch: 4 [22272/50048]	Loss: 2.6774
Training Epoch: 4 [22400/50048]	Loss: 2.4623
Training Epoch: 4 [22528/50048]	Loss: 2.3548
Training Epoch: 4 [22656/50048]	Loss: 2.4427
Training Epoch: 4 [22784/50048]	Loss: 2.3562
Training Epoch: 4 [22912/50048]	Loss: 2.7080
Training Epoch: 4 [23040/50048]	Loss: 2.5870
Training Epoch: 4 [23168/50048]	Loss: 2.4852
Training Epoch: 4 [23296/50048]	Loss: 2.4993
Training Epoch: 4 [23424/50048]	Loss: 2.2175
Training Epoch: 4 [23552/50048]	Loss: 2.5397
Training Epoch: 4 [23680/50048]	Loss: 2.4964
Training Epoch: 4 [23808/50048]	Loss: 2.4061
Training Epoch: 4 [23936/50048]	Loss: 2.1376
Training Epoch: 4 [24064/50048]	Loss: 2.3499
Training Epoch: 4 [24192/50048]	Loss: 2.1664
Training Epoch: 4 [24320/50048]	Loss: 2.4588
Training Epoch: 4 [24448/50048]	Loss: 2.4959
Training Epoch: 4 [24576/50048]	Loss: 2.5539
Training Epoch: 4 [24704/50048]	Loss: 2.3962
Training Epoch: 4 [24832/50048]	Loss: 2.8255
Training Epoch: 4 [24960/50048]	Loss: 2.4656
Training Epoch: 4 [25088/50048]	Loss: 2.1527
Training Epoch: 4 [25216/50048]	Loss: 2.7109
Training Epoch: 4 [25344/50048]	Loss: 2.4611
Training Epoch: 4 [25472/50048]	Loss: 2.1869
Training Epoch: 4 [25600/50048]	Loss: 2.5113
Training Epoch: 4 [25728/50048]	Loss: 2.3818
Training Epoch: 4 [25856/50048]	Loss: 2.3117
Training Epoch: 4 [25984/50048]	Loss: 2.2950
Training Epoch: 4 [26112/50048]	Loss: 2.5368
Training Epoch: 4 [26240/50048]	Loss: 2.3467
Training Epoch: 4 [26368/50048]	Loss: 2.4781
Training Epoch: 4 [26496/50048]	Loss: 2.5828
Training Epoch: 4 [26624/50048]	Loss: 2.2921
Training Epoch: 4 [26752/50048]	Loss: 2.2709
Training Epoch: 4 [26880/50048]	Loss: 1.9838
Training Epoch: 4 [27008/50048]	Loss: 2.5791
Training Epoch: 4 [27136/50048]	Loss: 2.4169
Training Epoch: 4 [27264/50048]	Loss: 2.3692
Training Epoch: 4 [27392/50048]	Loss: 2.2256
Training Epoch: 4 [27520/50048]	Loss: 2.4722
Training Epoch: 4 [27648/50048]	Loss: 2.3465
Training Epoch: 4 [27776/50048]	Loss: 2.5151
Training Epoch: 4 [27904/50048]	Loss: 2.5179
Training Epoch: 4 [28032/50048]	Loss: 2.3590
Training Epoch: 4 [28160/50048]	Loss: 2.4269
Training Epoch: 4 [28288/50048]	Loss: 2.3880
Training Epoch: 4 [28416/50048]	Loss: 2.5579
Training Epoch: 4 [28544/50048]	Loss: 2.3933
Training Epoch: 4 [28672/50048]	Loss: 2.2249
Training Epoch: 4 [28800/50048]	Loss: 2.3336
Training Epoch: 4 [28928/50048]	Loss: 2.5838
Training Epoch: 4 [29056/50048]	Loss: 2.3954
Training Epoch: 4 [29184/50048]	Loss: 2.2846
Training Epoch: 4 [29312/50048]	Loss: 2.4976
Training Epoch: 4 [29440/50048]	Loss: 2.4255
Training Epoch: 4 [29568/50048]	Loss: 2.4094
Training Epoch: 4 [29696/50048]	Loss: 2.6976
Training Epoch: 4 [29824/50048]	Loss: 2.4746
Training Epoch: 4 [29952/50048]	Loss: 2.2246
Training Epoch: 4 [30080/50048]	Loss: 2.6131
Training Epoch: 4 [30208/50048]	Loss: 2.3574
Training Epoch: 4 [30336/50048]	Loss: 2.3966
Training Epoch: 4 [30464/50048]	Loss: 2.3629
Training Epoch: 4 [30592/50048]	Loss: 2.5046
Training Epoch: 4 [30720/50048]	Loss: 2.3956
Training Epoch: 4 [30848/50048]	Loss: 2.4166
Training Epoch: 4 [30976/50048]	Loss: 2.3990
Training Epoch: 4 [31104/50048]	Loss: 2.4804
Training Epoch: 4 [31232/50048]	Loss: 2.4010
Training Epoch: 4 [31360/50048]	Loss: 2.4199
Training Epoch: 4 [31488/50048]	Loss: 2.3365
Training Epoch: 4 [31616/50048]	Loss: 2.4329
Training Epoch: 4 [31744/50048]	Loss: 2.3983
Training Epoch: 4 [31872/50048]	Loss: 2.3583
Training Epoch: 4 [32000/50048]	Loss: 2.4522
Training Epoch: 4 [32128/50048]	Loss: 2.4119
Training Epoch: 4 [32256/50048]	Loss: 2.5894
Training Epoch: 4 [32384/50048]	Loss: 2.1991
Training Epoch: 4 [32512/50048]	Loss: 2.6318
Training Epoch: 4 [32640/50048]	Loss: 2.2182
Training Epoch: 4 [32768/50048]	Loss: 2.4259
Training Epoch: 4 [32896/50048]	Loss: 2.3731
Training Epoch: 4 [33024/50048]	Loss: 2.5494
Training Epoch: 4 [33152/50048]	Loss: 2.6027
Training Epoch: 4 [33280/50048]	Loss: 2.6128
Training Epoch: 4 [33408/50048]	Loss: 2.4071
Training Epoch: 4 [33536/50048]	Loss: 2.8085
Training Epoch: 4 [33664/50048]	Loss: 2.0161
Training Epoch: 4 [33792/50048]	Loss: 2.6170
Training Epoch: 4 [33920/50048]	Loss: 2.4859
Training Epoch: 4 [34048/50048]	Loss: 2.5624
Training Epoch: 4 [34176/50048]	Loss: 2.5522
Training Epoch: 4 [34304/50048]	Loss: 2.5003
Training Epoch: 4 [34432/50048]	Loss: 2.3199
Training Epoch: 4 [34560/50048]	Loss: 2.3387
Training Epoch: 4 [34688/50048]	Loss: 2.5145
Training Epoch: 4 [34816/50048]	Loss: 2.4724
Training Epoch: 4 [34944/50048]	Loss: 2.2699
Training Epoch: 4 [35072/50048]	Loss: 2.7069
Training Epoch: 4 [35200/50048]	Loss: 2.1106
Training Epoch: 4 [35328/50048]	Loss: 2.4064
Training Epoch: 4 [35456/50048]	Loss: 2.1225
Training Epoch: 4 [35584/50048]	Loss: 2.3456
Training Epoch: 4 [35712/50048]	Loss: 2.5632
Training Epoch: 4 [35840/50048]	Loss: 2.5942
Training Epoch: 4 [35968/50048]	Loss: 2.3430
Training Epoch: 4 [36096/50048]	Loss: 2.4217
Training Epoch: 4 [36224/50048]	Loss: 2.4121
Training Epoch: 4 [36352/50048]	Loss: 2.6097
Training Epoch: 4 [36480/50048]	Loss: 2.5500
Training Epoch: 4 [36608/50048]	Loss: 2.4843
Training Epoch: 4 [36736/50048]	Loss: 2.5710
Training Epoch: 4 [36864/50048]	Loss: 2.4783
Training Epoch: 4 [36992/50048]	Loss: 2.3223
Training Epoch: 4 [37120/50048]	Loss: 2.5222
Training Epoch: 4 [37248/50048]	Loss: 2.1569
Training Epoch: 4 [37376/50048]	Loss: 2.3401
Training Epoch: 4 [37504/50048]	Loss: 2.6127
Training Epoch: 4 [37632/50048]	Loss: 2.6733
Training Epoch: 4 [37760/50048]	Loss: 2.5264
Training Epoch: 4 [37888/50048]	Loss: 2.2810
Training Epoch: 4 [38016/50048]	Loss: 2.2781
Training Epoch: 4 [38144/50048]	Loss: 2.3607
Training Epoch: 4 [38272/50048]	Loss: 2.6049
Training Epoch: 4 [38400/50048]	Loss: 2.1677
Training Epoch: 4 [38528/50048]	Loss: 2.2676
Training Epoch: 4 [38656/50048]	Loss: 2.1272
Training Epoch: 4 [38784/50048]	Loss: 2.5840
Training Epoch: 4 [38912/50048]	Loss: 2.3338
Training Epoch: 4 [39040/50048]	Loss: 2.4934
Training Epoch: 4 [39168/50048]	Loss: 2.3320
Training Epoch: 4 [39296/50048]	Loss: 2.3199
Training Epoch: 4 [39424/50048]	Loss: 2.7003
Training Epoch: 4 [39552/50048]	Loss: 2.7055
Training Epoch: 4 [39680/50048]	Loss: 2.2947
Training Epoch: 4 [39808/50048]	Loss: 2.4499
Training Epoch: 4 [39936/50048]	Loss: 2.2936
Training Epoch: 4 [40064/50048]	Loss: 2.2797
Training Epoch: 4 [40192/50048]	Loss: 2.6020
Training Epoch: 4 [40320/50048]	Loss: 2.3852
Training Epoch: 4 [40448/50048]	Loss: 2.7136
Training Epoch: 4 [40576/50048]	Loss: 2.6710
Training Epoch: 4 [40704/50048]	Loss: 2.4673
Training Epoch: 4 [40832/50048]	Loss: 2.4376
Training Epoch: 4 [40960/50048]	Loss: 2.5708
Training Epoch: 4 [41088/50048]	Loss: 2.3628
Training Epoch: 4 [41216/50048]	Loss: 2.2711
Training Epoch: 4 [41344/50048]	Loss: 2.3858
Training Epoch: 4 [41472/50048]	Loss: 2.4939
Training Epoch: 4 [41600/50048]	Loss: 2.5827
Training Epoch: 4 [41728/50048]	Loss: 2.2927
Training Epoch: 4 [41856/50048]	Loss: 2.2737
Training Epoch: 4 [41984/50048]	Loss: 2.2788
Training Epoch: 4 [42112/50048]	Loss: 2.3673
Training Epoch: 4 [42240/50048]	Loss: 2.5232
Training Epoch: 4 [42368/50048]	Loss: 2.3940
Training Epoch: 4 [42496/50048]	Loss: 2.3347
Training Epoch: 4 [42624/50048]	Loss: 2.5685
Training Epoch: 4 [42752/50048]	Loss: 2.3891
Training Epoch: 4 [42880/50048]	Loss: 2.4184
Training Epoch: 4 [43008/50048]	Loss: 2.2461
Training Epoch: 4 [43136/50048]	Loss: 2.4763
Training Epoch: 4 [43264/50048]	Loss: 2.4379
Training Epoch: 4 [43392/50048]	Loss: 2.3930
Training Epoch: 4 [43520/50048]	Loss: 2.3853
Training Epoch: 4 [43648/50048]	Loss: 2.5498
Training Epoch: 4 [43776/50048]	Loss: 2.1872
Training Epoch: 4 [43904/50048]	Loss: 2.2271
Training Epoch: 4 [44032/50048]	Loss: 2.2848
Training Epoch: 4 [44160/50048]	Loss: 2.2152
Training Epoch: 4 [44288/50048]	Loss: 2.2594
Training Epoch: 4 [44416/50048]	Loss: 2.4487
Training Epoch: 4 [44544/50048]	Loss: 2.4590
Training Epoch: 4 [44672/50048]	Loss: 2.1621
Training Epoch: 4 [44800/50048]	Loss: 2.4398
Training Epoch: 4 [44928/50048]	Loss: 2.4197
Training Epoch: 4 [45056/50048]	Loss: 2.1784
Training Epoch: 4 [45184/50048]	Loss: 2.6172
Training Epoch: 4 [45312/50048]	Loss: 2.4340
Training Epoch: 4 [45440/50048]	Loss: 2.1900
Training Epoch: 4 [45568/50048]	Loss: 2.2984
Training Epoch: 4 [45696/50048]	Loss: 2.4011
Training Epoch: 4 [45824/50048]	Loss: 2.3738
Training Epoch: 4 [45952/50048]	Loss: 2.2542
Training Epoch: 4 [46080/50048]	Loss: 2.5928
Training Epoch: 4 [46208/50048]	Loss: 2.4078
Training Epoch: 4 [46336/50048]	Loss: 2.1540
Training Epoch: 4 [46464/50048]	Loss: 2.4142
Training Epoch: 4 [46592/50048]	Loss: 2.4005
Training Epoch: 4 [46720/50048]	Loss: 2.2466
Training Epoch: 4 [46848/50048]	Loss: 2.4202
Training Epoch: 4 [46976/50048]	Loss: 2.7068
Training Epoch: 4 [47104/50048]	Loss: 2.4253
Training Epoch: 4 [47232/50048]	Loss: 2.0935
Training Epoch: 4 [47360/50048]	Loss: 2.6174
Training Epoch: 4 [47488/50048]	Loss: 2.2800
Training Epoch: 4 [47616/50048]	Loss: 2.5177
Training Epoch: 4 [47744/50048]	Loss: 2.4620
Training Epoch: 4 [47872/50048]	Loss: 2.5565
Training Epoch: 4 [48000/50048]	Loss: 2.6024
Training Epoch: 4 [48128/50048]	Loss: 2.4600
Training Epoch: 4 [48256/50048]	Loss: 2.1440
Training Epoch: 4 [48384/50048]	Loss: 2.2744
Training Epoch: 4 [48512/50048]	Loss: 2.4075
Training Epoch: 4 [48640/50048]	Loss: 2.2740
Training Epoch: 4 [48768/50048]	Loss: 2.2881
Training Epoch: 4 [48896/50048]	Loss: 2.4205
Training Epoch: 4 [49024/50048]	Loss: 2.4092
Training Epoch: 4 [49152/50048]	Loss: 1.7999
Training Epoch: 4 [49280/50048]	Loss: 2.5449
Training Epoch: 4 [49408/50048]	Loss: 2.2178
Training Epoch: 4 [49536/50048]	Loss: 2.2896
Training Epoch: 4 [49664/50048]	Loss: 2.1863
Training Epoch: 4 [49792/50048]	Loss: 2.4002
Training Epoch: 4 [49920/50048]	Loss: 2.1391
Training Epoch: 4 [50048/50048]	Loss: 2.3703
Validation Epoch: 4, Average loss: 0.0320, Accuracy: 0.1857
Training Epoch: 5 [128/50048]	Loss: 2.3008
Training Epoch: 5 [256/50048]	Loss: 2.0784
Training Epoch: 5 [384/50048]	Loss: 2.1317
Training Epoch: 5 [512/50048]	Loss: 2.3531
Training Epoch: 5 [640/50048]	Loss: 2.1316
Training Epoch: 5 [768/50048]	Loss: 2.1404
Training Epoch: 5 [896/50048]	Loss: 2.4301
Training Epoch: 5 [1024/50048]	Loss: 2.2469
Training Epoch: 5 [1152/50048]	Loss: 2.1702
Training Epoch: 5 [1280/50048]	Loss: 2.5423
Training Epoch: 5 [1408/50048]	Loss: 2.3712
Training Epoch: 5 [1536/50048]	Loss: 2.2593
Training Epoch: 5 [1664/50048]	Loss: 2.3847
Training Epoch: 5 [1792/50048]	Loss: 2.2289
Training Epoch: 5 [1920/50048]	Loss: 2.2308
Training Epoch: 5 [2048/50048]	Loss: 2.6340
Training Epoch: 5 [2176/50048]	Loss: 2.3483
Training Epoch: 5 [2304/50048]	Loss: 2.4548
Training Epoch: 5 [2432/50048]	Loss: 2.3426
Training Epoch: 5 [2560/50048]	Loss: 2.4135
Training Epoch: 5 [2688/50048]	Loss: 2.2155
Training Epoch: 5 [2816/50048]	Loss: 2.0920
Training Epoch: 5 [2944/50048]	Loss: 2.3589
Training Epoch: 5 [3072/50048]	Loss: 2.3771
Training Epoch: 5 [3200/50048]	Loss: 2.2167
Training Epoch: 5 [3328/50048]	Loss: 2.1350
Training Epoch: 5 [3456/50048]	Loss: 2.2231
Training Epoch: 5 [3584/50048]	Loss: 2.4748
Training Epoch: 5 [3712/50048]	Loss: 2.2327
Training Epoch: 5 [3840/50048]	Loss: 2.3345
Training Epoch: 5 [3968/50048]	Loss: 2.0739
Training Epoch: 5 [4096/50048]	Loss: 2.2901
Training Epoch: 5 [4224/50048]	Loss: 2.3663
Training Epoch: 5 [4352/50048]	Loss: 2.3196
Training Epoch: 5 [4480/50048]	Loss: 2.2870
Training Epoch: 5 [4608/50048]	Loss: 2.3110
Training Epoch: 5 [4736/50048]	Loss: 2.4529
Training Epoch: 5 [4864/50048]	Loss: 2.4753
Training Epoch: 5 [4992/50048]	Loss: 2.4240
Training Epoch: 5 [5120/50048]	Loss: 2.2895
Training Epoch: 5 [5248/50048]	Loss: 2.2396
Training Epoch: 5 [5376/50048]	Loss: 2.2425
Training Epoch: 5 [5504/50048]	Loss: 2.1038
Training Epoch: 5 [5632/50048]	Loss: 2.2879
Training Epoch: 5 [5760/50048]	Loss: 2.4308
Training Epoch: 5 [5888/50048]	Loss: 2.1116
Training Epoch: 5 [6016/50048]	Loss: 2.1943
Training Epoch: 5 [6144/50048]	Loss: 2.5459
Training Epoch: 5 [6272/50048]	Loss: 2.3847
Training Epoch: 5 [6400/50048]	Loss: 2.2949
Training Epoch: 5 [6528/50048]	Loss: 2.5305
Training Epoch: 5 [6656/50048]	Loss: 2.1990
Training Epoch: 5 [6784/50048]	Loss: 2.1257
Training Epoch: 5 [6912/50048]	Loss: 2.2459
Training Epoch: 5 [7040/50048]	Loss: 2.5252
Training Epoch: 5 [7168/50048]	Loss: 2.0967
Training Epoch: 5 [7296/50048]	Loss: 2.5035
Training Epoch: 5 [7424/50048]	Loss: 2.1520
Training Epoch: 5 [7552/50048]	Loss: 2.2815
Training Epoch: 5 [7680/50048]	Loss: 2.2433
Training Epoch: 5 [7808/50048]	Loss: 2.3138
Training Epoch: 5 [7936/50048]	Loss: 2.4705
Training Epoch: 5 [8064/50048]	Loss: 2.3078
Training Epoch: 5 [8192/50048]	Loss: 2.1423
Training Epoch: 5 [8320/50048]	Loss: 2.4019
Training Epoch: 5 [8448/50048]	Loss: 2.3549
Training Epoch: 5 [8576/50048]	Loss: 2.4490
Training Epoch: 5 [8704/50048]	Loss: 2.5608
Training Epoch: 5 [8832/50048]	Loss: 2.4007
Training Epoch: 5 [8960/50048]	Loss: 2.3888
Training Epoch: 5 [9088/50048]	Loss: 2.0977
Training Epoch: 5 [9216/50048]	Loss: 2.5111
Training Epoch: 5 [9344/50048]	Loss: 2.3605
Training Epoch: 5 [9472/50048]	Loss: 2.2423
Training Epoch: 5 [9600/50048]	Loss: 2.3444
Training Epoch: 5 [9728/50048]	Loss: 2.5811
Training Epoch: 5 [9856/50048]	Loss: 2.3472
Training Epoch: 5 [9984/50048]	Loss: 2.4740
Training Epoch: 5 [10112/50048]	Loss: 2.2659
Training Epoch: 5 [10240/50048]	Loss: 2.1566
Training Epoch: 5 [10368/50048]	Loss: 2.4420
Training Epoch: 5 [10496/50048]	Loss: 2.3317
Training Epoch: 5 [10624/50048]	Loss: 2.4220
Training Epoch: 5 [10752/50048]	Loss: 2.3590
Training Epoch: 5 [10880/50048]	Loss: 2.2185
Training Epoch: 5 [11008/50048]	Loss: 2.4045
Training Epoch: 5 [11136/50048]	Loss: 2.2322
Training Epoch: 5 [11264/50048]	Loss: 2.1778
Training Epoch: 5 [11392/50048]	Loss: 2.3817
Training Epoch: 5 [11520/50048]	Loss: 2.2253
Training Epoch: 5 [11648/50048]	Loss: 2.5700
Training Epoch: 5 [11776/50048]	Loss: 2.2830
Training Epoch: 5 [11904/50048]	Loss: 2.2802
Training Epoch: 5 [12032/50048]	Loss: 2.2528
Training Epoch: 5 [12160/50048]	Loss: 2.3626
Training Epoch: 5 [12288/50048]	Loss: 2.3000
Training Epoch: 5 [12416/50048]	Loss: 2.3351
Training Epoch: 5 [12544/50048]	Loss: 2.2270
Training Epoch: 5 [12672/50048]	Loss: 1.9545
Training Epoch: 5 [12800/50048]	Loss: 2.3600
Training Epoch: 5 [12928/50048]	Loss: 2.5234
Training Epoch: 5 [13056/50048]	Loss: 2.5169
Training Epoch: 5 [13184/50048]	Loss: 2.3659
Training Epoch: 5 [13312/50048]	Loss: 2.4836
Training Epoch: 5 [13440/50048]	Loss: 2.1845
Training Epoch: 5 [13568/50048]	Loss: 2.3114
Training Epoch: 5 [13696/50048]	Loss: 2.3590
Training Epoch: 5 [13824/50048]	Loss: 2.2072
Training Epoch: 5 [13952/50048]	Loss: 2.3617
Training Epoch: 5 [14080/50048]	Loss: 2.2511
Training Epoch: 5 [14208/50048]	Loss: 2.1783
Training Epoch: 5 [14336/50048]	Loss: 2.3256
Training Epoch: 5 [14464/50048]	Loss: 2.2943
Training Epoch: 5 [14592/50048]	Loss: 2.4988
Training Epoch: 5 [14720/50048]	Loss: 2.1277
Training Epoch: 5 [14848/50048]	Loss: 2.3448
Training Epoch: 5 [14976/50048]	Loss: 2.1989
Training Epoch: 5 [15104/50048]	Loss: 2.1801
Training Epoch: 5 [15232/50048]	Loss: 2.1471
Training Epoch: 5 [15360/50048]	Loss: 2.1184
Training Epoch: 5 [15488/50048]	Loss: 2.2992
Training Epoch: 5 [15616/50048]	Loss: 2.1892
Training Epoch: 5 [15744/50048]	Loss: 2.1561
Training Epoch: 5 [15872/50048]	Loss: 2.1132
Training Epoch: 5 [16000/50048]	Loss: 2.1709
Training Epoch: 5 [16128/50048]	Loss: 2.3456
Training Epoch: 5 [16256/50048]	Loss: 2.5631
Training Epoch: 5 [16384/50048]	Loss: 2.5713
Training Epoch: 5 [16512/50048]	Loss: 2.0581
Training Epoch: 5 [16640/50048]	Loss: 2.1260
Training Epoch: 5 [16768/50048]	Loss: 2.2742
Training Epoch: 5 [16896/50048]	Loss: 2.4121
Training Epoch: 5 [17024/50048]	Loss: 2.2559
Training Epoch: 5 [17152/50048]	Loss: 2.2444
Training Epoch: 5 [17280/50048]	Loss: 1.9244
Training Epoch: 5 [17408/50048]	Loss: 2.2744
Training Epoch: 5 [17536/50048]	Loss: 2.5495
Training Epoch: 5 [17664/50048]	Loss: 2.1759
Training Epoch: 5 [17792/50048]	Loss: 2.2564
Training Epoch: 5 [17920/50048]	Loss: 2.4123
Training Epoch: 5 [18048/50048]	Loss: 2.0911
Training Epoch: 5 [18176/50048]	Loss: 2.1737
Training Epoch: 5 [18304/50048]	Loss: 2.2932
Training Epoch: 5 [18432/50048]	Loss: 2.2332
Training Epoch: 5 [18560/50048]	Loss: 2.2249
Training Epoch: 5 [18688/50048]	Loss: 2.4217
Training Epoch: 5 [18816/50048]	Loss: 2.4057
Training Epoch: 5 [18944/50048]	Loss: 2.4959
Training Epoch: 5 [19072/50048]	Loss: 2.1953
Training Epoch: 5 [19200/50048]	Loss: 2.3732
Training Epoch: 5 [19328/50048]	Loss: 2.3858
Training Epoch: 5 [19456/50048]	Loss: 2.1711
Training Epoch: 5 [19584/50048]	Loss: 2.1666
Training Epoch: 5 [19712/50048]	Loss: 2.2220
Training Epoch: 5 [19840/50048]	Loss: 2.1063
Training Epoch: 5 [19968/50048]	Loss: 2.0985
Training Epoch: 5 [20096/50048]	Loss: 1.9696
Training Epoch: 5 [20224/50048]	Loss: 2.5092
Training Epoch: 5 [20352/50048]	Loss: 1.9588
Training Epoch: 5 [20480/50048]	Loss: 2.3290
Training Epoch: 5 [20608/50048]	Loss: 2.4135
Training Epoch: 5 [20736/50048]	Loss: 2.3274
Training Epoch: 5 [20864/50048]	Loss: 2.2409
Training Epoch: 5 [20992/50048]	Loss: 2.1502
Training Epoch: 5 [21120/50048]	Loss: 2.2289
Training Epoch: 5 [21248/50048]	Loss: 2.1592
Training Epoch: 5 [21376/50048]	Loss: 1.8982
Training Epoch: 5 [21504/50048]	Loss: 2.3308
Training Epoch: 5 [21632/50048]	Loss: 2.3568
Training Epoch: 5 [21760/50048]	Loss: 2.3260
Training Epoch: 5 [21888/50048]	Loss: 2.1487
Training Epoch: 5 [22016/50048]	Loss: 2.2479
Training Epoch: 5 [22144/50048]	Loss: 1.8456
Training Epoch: 5 [22272/50048]	Loss: 2.3462
Training Epoch: 5 [22400/50048]	Loss: 2.2943
Training Epoch: 5 [22528/50048]	Loss: 2.2884
Training Epoch: 5 [22656/50048]	Loss: 2.2515
Training Epoch: 5 [22784/50048]	Loss: 1.9171
Training Epoch: 5 [22912/50048]	Loss: 2.4241
Training Epoch: 5 [23040/50048]	Loss: 2.5795
Training Epoch: 5 [23168/50048]	Loss: 2.2361
Training Epoch: 5 [23296/50048]	Loss: 2.0345
Training Epoch: 5 [23424/50048]	Loss: 2.2513
Training Epoch: 5 [23552/50048]	Loss: 2.0742
Training Epoch: 5 [23680/50048]	Loss: 2.3454
Training Epoch: 5 [23808/50048]	Loss: 2.1875
Training Epoch: 5 [23936/50048]	Loss: 1.9994
Training Epoch: 5 [24064/50048]	Loss: 2.0301
Training Epoch: 5 [24192/50048]	Loss: 2.3765
Training Epoch: 5 [24320/50048]	Loss: 2.3806
Training Epoch: 5 [24448/50048]	Loss: 2.4202
Training Epoch: 5 [24576/50048]	Loss: 2.1764
Training Epoch: 5 [24704/50048]	Loss: 2.2004
Training Epoch: 5 [24832/50048]	Loss: 2.3548
Training Epoch: 5 [24960/50048]	Loss: 2.2242
Training Epoch: 5 [25088/50048]	Loss: 2.3316
Training Epoch: 5 [25216/50048]	Loss: 2.2721
Training Epoch: 5 [25344/50048]	Loss: 2.3913
Training Epoch: 5 [25472/50048]	Loss: 2.3990
Training Epoch: 5 [25600/50048]	Loss: 2.2236
Training Epoch: 5 [25728/50048]	Loss: 2.1951
Training Epoch: 5 [25856/50048]	Loss: 2.2512
Training Epoch: 5 [25984/50048]	Loss: 2.3209
Training Epoch: 5 [26112/50048]	Loss: 2.2828
Training Epoch: 5 [26240/50048]	Loss: 2.1545
Training Epoch: 5 [26368/50048]	Loss: 2.2634
Training Epoch: 5 [26496/50048]	Loss: 2.0627
Training Epoch: 5 [26624/50048]	Loss: 2.3687
Training Epoch: 5 [26752/50048]	Loss: 2.2222
Training Epoch: 5 [26880/50048]	Loss: 2.1448
Training Epoch: 5 [27008/50048]	Loss: 2.3629
Training Epoch: 5 [27136/50048]	Loss: 1.9806
Training Epoch: 5 [27264/50048]	Loss: 2.1874
Training Epoch: 5 [27392/50048]	Loss: 2.2710
Training Epoch: 5 [27520/50048]	Loss: 2.2109
Training Epoch: 5 [27648/50048]	Loss: 2.0615
Training Epoch: 5 [27776/50048]	Loss: 2.4748
Training Epoch: 5 [27904/50048]	Loss: 2.3277
Training Epoch: 5 [28032/50048]	Loss: 2.3082
Training Epoch: 5 [28160/50048]	Loss: 2.0959
Training Epoch: 5 [28288/50048]	Loss: 2.3037
Training Epoch: 5 [28416/50048]	Loss: 1.9943
Training Epoch: 5 [28544/50048]	Loss: 2.3460
Training Epoch: 5 [28672/50048]	Loss: 2.3556
Training Epoch: 5 [28800/50048]	Loss: 2.2729
Training Epoch: 5 [28928/50048]	Loss: 2.3370
Training Epoch: 5 [29056/50048]	Loss: 2.2345
Training Epoch: 5 [29184/50048]	Loss: 2.4470
Training Epoch: 5 [29312/50048]	Loss: 2.2923
Training Epoch: 5 [29440/50048]	Loss: 2.0575
Training Epoch: 5 [29568/50048]	Loss: 1.9316
Training Epoch: 5 [29696/50048]	Loss: 2.0867
Training Epoch: 5 [29824/50048]	Loss: 1.9638
Training Epoch: 5 [29952/50048]	Loss: 2.3810
Training Epoch: 5 [30080/50048]	Loss: 2.3246
Training Epoch: 5 [30208/50048]	Loss: 2.2437
Training Epoch: 5 [30336/50048]	Loss: 2.5070
Training Epoch: 5 [30464/50048]	Loss: 2.4252
Training Epoch: 5 [30592/50048]	Loss: 2.3135
Training Epoch: 5 [30720/50048]	Loss: 2.3306
Training Epoch: 5 [30848/50048]	Loss: 2.0476
Training Epoch: 5 [30976/50048]	Loss: 1.9251
Training Epoch: 5 [31104/50048]	Loss: 2.2345
Training Epoch: 5 [31232/50048]	Loss: 2.2567
Training Epoch: 5 [31360/50048]	Loss: 2.3346
Training Epoch: 5 [31488/50048]	Loss: 2.2782
Training Epoch: 5 [31616/50048]	Loss: 2.3891
Training Epoch: 5 [31744/50048]	Loss: 1.9021
Training Epoch: 5 [31872/50048]	Loss: 2.3546
Training Epoch: 5 [32000/50048]	Loss: 2.1232
Training Epoch: 5 [32128/50048]	Loss: 2.2788
Training Epoch: 5 [32256/50048]	Loss: 2.1598
Training Epoch: 5 [32384/50048]	Loss: 2.3440
Training Epoch: 5 [32512/50048]	Loss: 1.9834
Training Epoch: 5 [32640/50048]	Loss: 2.2451
Training Epoch: 5 [32768/50048]	Loss: 2.3530
Training Epoch: 5 [32896/50048]	Loss: 2.1848
Training Epoch: 5 [33024/50048]	Loss: 2.1579
Training Epoch: 5 [33152/50048]	Loss: 2.2400
Training Epoch: 5 [33280/50048]	Loss: 2.2087
Training Epoch: 5 [33408/50048]	Loss: 1.9355
Training Epoch: 5 [33536/50048]	Loss: 2.1483
Training Epoch: 5 [33664/50048]	Loss: 2.2551
Training Epoch: 5 [33792/50048]	Loss: 2.3444
Training Epoch: 5 [33920/50048]	Loss: 2.3194
Training Epoch: 5 [34048/50048]	Loss: 2.2484
Training Epoch: 5 [34176/50048]	Loss: 2.0812
Training Epoch: 5 [34304/50048]	Loss: 1.7758
Training Epoch: 5 [34432/50048]	Loss: 2.4459
Training Epoch: 5 [34560/50048]	Loss: 2.0364
Training Epoch: 5 [34688/50048]	Loss: 2.2662
Training Epoch: 5 [34816/50048]	Loss: 2.4275
Training Epoch: 5 [34944/50048]	Loss: 2.2982
Training Epoch: 5 [35072/50048]	Loss: 2.0269
Training Epoch: 5 [35200/50048]	Loss: 2.1679
Training Epoch: 5 [35328/50048]	Loss: 2.2044
Training Epoch: 5 [35456/50048]	Loss: 2.4532
Training Epoch: 5 [35584/50048]	Loss: 2.4029
Training Epoch: 5 [35712/50048]	Loss: 2.2594
Training Epoch: 5 [35840/50048]	Loss: 2.1101
Training Epoch: 5 [35968/50048]	Loss: 2.3222
Training Epoch: 5 [36096/50048]	Loss: 2.3476
Training Epoch: 5 [36224/50048]	Loss: 2.0686
Training Epoch: 5 [36352/50048]	Loss: 2.1118
Training Epoch: 5 [36480/50048]	Loss: 2.2549
Training Epoch: 5 [36608/50048]	Loss: 2.2598
Training Epoch: 5 [36736/50048]	Loss: 2.3714
Training Epoch: 5 [36864/50048]	Loss: 2.1320
Training Epoch: 5 [36992/50048]	Loss: 1.9572
Training Epoch: 5 [37120/50048]	Loss: 2.1801
Training Epoch: 5 [37248/50048]	Loss: 2.2465
Training Epoch: 5 [37376/50048]	Loss: 2.3197
Training Epoch: 5 [37504/50048]	Loss: 2.3668
Training Epoch: 5 [37632/50048]	Loss: 2.1982
Training Epoch: 5 [37760/50048]	Loss: 2.0859
Training Epoch: 5 [37888/50048]	Loss: 1.9585
Training Epoch: 5 [38016/50048]	Loss: 2.2444
Training Epoch: 5 [38144/50048]	Loss: 2.2539
Training Epoch: 5 [38272/50048]	Loss: 2.0818
Training Epoch: 5 [38400/50048]	Loss: 2.2404
Training Epoch: 5 [38528/50048]	Loss: 2.4714
Training Epoch: 5 [38656/50048]	Loss: 2.1638
Training Epoch: 5 [38784/50048]	Loss: 2.1711
Training Epoch: 5 [38912/50048]	Loss: 2.3742
Training Epoch: 5 [39040/50048]	Loss: 2.0908
Training Epoch: 5 [39168/50048]	Loss: 2.3447
Training Epoch: 5 [39296/50048]	Loss: 2.2941
Training Epoch: 5 [39424/50048]	Loss: 2.0135
Training Epoch: 5 [39552/50048]	Loss: 2.3235
Training Epoch: 5 [39680/50048]	Loss: 2.2647
Training Epoch: 5 [39808/50048]	Loss: 1.9983
Training Epoch: 5 [39936/50048]	Loss: 2.2352
Training Epoch: 5 [40064/50048]	Loss: 1.9516
Training Epoch: 5 [40192/50048]	Loss: 2.1554
Training Epoch: 5 [40320/50048]	Loss: 2.5132
Training Epoch: 5 [40448/50048]	Loss: 2.4966
Training Epoch: 5 [40576/50048]	Loss: 2.3428
Training Epoch: 5 [40704/50048]	Loss: 2.2009
Training Epoch: 5 [40832/50048]	Loss: 2.2796
Training Epoch: 5 [40960/50048]	Loss: 2.0157
Training Epoch: 5 [41088/50048]	Loss: 1.9760
Training Epoch: 5 [41216/50048]	Loss: 2.1124
Training Epoch: 5 [41344/50048]	Loss: 2.1374
Training Epoch: 5 [41472/50048]	Loss: 1.9455
Training Epoch: 5 [41600/50048]	Loss: 2.2576
Training Epoch: 5 [41728/50048]	Loss: 2.0325
Training Epoch: 5 [41856/50048]	Loss: 1.8490
Training Epoch: 5 [41984/50048]	Loss: 2.3459
Training Epoch: 5 [42112/50048]	Loss: 2.1065
Training Epoch: 5 [42240/50048]	Loss: 2.0239
Training Epoch: 5 [42368/50048]	Loss: 2.4967
Training Epoch: 5 [42496/50048]	Loss: 2.3305
Training Epoch: 5 [42624/50048]	Loss: 2.1987
Training Epoch: 5 [42752/50048]	Loss: 2.1506
Training Epoch: 5 [42880/50048]	Loss: 2.4968
Training Epoch: 5 [43008/50048]	Loss: 2.0819
Training Epoch: 5 [43136/50048]	Loss: 2.2326
Training Epoch: 5 [43264/50048]	Loss: 2.0196
Training Epoch: 5 [43392/50048]	Loss: 2.1777
Training Epoch: 5 [43520/50048]	Loss: 2.1067
Training Epoch: 5 [43648/50048]	Loss: 2.1503
Training Epoch: 5 [43776/50048]	Loss: 2.0840
Training Epoch: 5 [43904/50048]	Loss: 2.3507
Training Epoch: 5 [44032/50048]	Loss: 2.3197
Training Epoch: 5 [44160/50048]	Loss: 2.1476
Training Epoch: 5 [44288/50048]	Loss: 2.3517
Training Epoch: 5 [44416/50048]	Loss: 2.4554
Training Epoch: 5 [44544/50048]	Loss: 2.2275
Training Epoch: 5 [44672/50048]	Loss: 2.2619
Training Epoch: 5 [44800/50048]	Loss: 2.3898
Training Epoch: 5 [44928/50048]	Loss: 2.1512
Training Epoch: 5 [45056/50048]	Loss: 2.1147
Training Epoch: 5 [45184/50048]	Loss: 2.5394
Training Epoch: 5 [45312/50048]	Loss: 2.0601
Training Epoch: 5 [45440/50048]	Loss: 2.2470
Training Epoch: 5 [45568/50048]	Loss: 2.1330
Training Epoch: 5 [45696/50048]	Loss: 2.3751
Training Epoch: 5 [45824/50048]	Loss: 2.1761
Training Epoch: 5 [45952/50048]	Loss: 2.2766
Training Epoch: 5 [46080/50048]	Loss: 2.0470
Training Epoch: 5 [46208/50048]	Loss: 2.2891
Training Epoch: 5 [46336/50048]	Loss: 2.3345
Training Epoch: 5 [46464/50048]	Loss: 1.9686
Training Epoch: 5 [46592/50048]	Loss: 2.2693
Training Epoch: 5 [46720/50048]	Loss: 2.2646
Training Epoch: 5 [46848/50048]	Loss: 2.1271
Training Epoch: 5 [46976/50048]	Loss: 2.2282
Training Epoch: 5 [47104/50048]	Loss: 2.1510
Training Epoch: 5 [47232/50048]	Loss: 2.2010
Training Epoch: 5 [47360/50048]	Loss: 2.2326
Training Epoch: 5 [47488/50048]	Loss: 2.0571
Training Epoch: 5 [47616/50048]	Loss: 1.8731
Training Epoch: 5 [47744/50048]	Loss: 2.1581
Training Epoch: 5 [47872/50048]	Loss: 1.9075
Training Epoch: 5 [48000/50048]	Loss: 2.3296
Training Epoch: 5 [48128/50048]	Loss: 2.2991
Training Epoch: 5 [48256/50048]	Loss: 2.2550
Training Epoch: 5 [48384/50048]	Loss: 2.1459
Training Epoch: 5 [48512/50048]	Loss: 2.0573
Training Epoch: 5 [48640/50048]	Loss: 2.0947
Training Epoch: 5 [48768/50048]	Loss: 1.8849
Training Epoch: 5 [48896/50048]	Loss: 2.0317
Training Epoch: 5 [49024/50048]	Loss: 2.1693
Training Epoch: 5 [49152/50048]	Loss: 2.2084
Training Epoch: 5 [49280/50048]	Loss: 2.1723
Training Epoch: 5 [49408/50048]	Loss: 2.2909
Training Epoch: 5 [49536/50048]	Loss: 2.0251
Training Epoch: 5 [49664/50048]	Loss: 2.1690
Training Epoch: 5 [49792/50048]	Loss: 2.1424
Training Epoch: 5 [49920/50048]	Loss: 1.8613
Training Epoch: 5 [50048/50048]	Loss: 2.2356
Validation Epoch: 5, Average loss: 0.0209, Accuracy: 0.3503
[Training Loop] Model's accuracy 0.3502768987341772 surpasses threshold 0.3! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1275
Profiling... [256/50048]	Loss: 2.1280
Profiling... [384/50048]	Loss: 2.1238
Profiling... [512/50048]	Loss: 1.8701
Profiling... [640/50048]	Loss: 2.0977
Profiling... [768/50048]	Loss: 1.8675
Profiling... [896/50048]	Loss: 1.9277
Profiling... [1024/50048]	Loss: 1.8172
Profiling... [1152/50048]	Loss: 1.9897
Profiling... [1280/50048]	Loss: 2.0851
Profiling... [1408/50048]	Loss: 2.0358
Profiling... [1536/50048]	Loss: 2.1869
Profiling... [1664/50048]	Loss: 2.1934
Profile done
epoch 1 train time consumed: 3.34s
Validation Epoch: 6, Average loss: 0.0163, Accuracy: 0.4422
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.42874906945482,
                        "time": 2.175993370001379,
                        "accuracy": 0.442246835443038,
                        "total_cost": 572.8660235609973
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1883
Profiling... [256/50048]	Loss: 2.1033
Profiling... [384/50048]	Loss: 2.0717
Profiling... [512/50048]	Loss: 2.0098
Profiling... [640/50048]	Loss: 1.8412
Profiling... [768/50048]	Loss: 2.0659
Profiling... [896/50048]	Loss: 2.0775
Profiling... [1024/50048]	Loss: 2.1213
Profiling... [1152/50048]	Loss: 2.1717
Profiling... [1280/50048]	Loss: 2.0866
Profiling... [1408/50048]	Loss: 1.9997
Profiling... [1536/50048]	Loss: 2.0705
Profiling... [1664/50048]	Loss: 2.1192
Profile done
epoch 1 train time consumed: 3.22s
Validation Epoch: 6, Average loss: 0.0161, Accuracy: 0.4456
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.44002334786236,
                        "time": 2.1753635729983216,
                        "accuracy": 0.4456091772151899,
                        "total_cost": 568.4339510532401
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.2450
Profiling... [256/50048]	Loss: 2.2580
Profiling... [384/50048]	Loss: 1.8557
Profiling... [512/50048]	Loss: 2.2244
Profiling... [640/50048]	Loss: 1.9440
Profiling... [768/50048]	Loss: 1.9069
Profiling... [896/50048]	Loss: 1.8457
Profiling... [1024/50048]	Loss: 2.0404
Profiling... [1152/50048]	Loss: 2.0315
Profiling... [1280/50048]	Loss: 2.0642
Profiling... [1408/50048]	Loss: 2.0783
Profiling... [1536/50048]	Loss: 1.9976
Profiling... [1664/50048]	Loss: 2.0581
Profile done
epoch 1 train time consumed: 3.31s
Validation Epoch: 6, Average loss: 0.0164, Accuracy: 0.4404
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.45010671502773,
                        "time": 2.222815945002367,
                        "accuracy": 0.44036787974683544,
                        "total_cost": 587.7975345345358
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.2630
Profiling... [256/50048]	Loss: 2.0514
Profiling... [384/50048]	Loss: 2.2672
Profiling... [512/50048]	Loss: 2.3614
Profiling... [640/50048]	Loss: 1.9201
Profiling... [768/50048]	Loss: 2.1477
Profiling... [896/50048]	Loss: 2.0276
Profiling... [1024/50048]	Loss: 1.8122
Profiling... [1152/50048]	Loss: 2.1230
Profiling... [1280/50048]	Loss: 2.0085
Profiling... [1408/50048]	Loss: 1.8083
Profiling... [1536/50048]	Loss: 2.1587
Profiling... [1664/50048]	Loss: 1.9930
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 6, Average loss: 0.0164, Accuracy: 0.4404
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 116.41473783666605,
                        "time": 2.595767007005634,
                        "accuracy": 0.44036787974683544,
                        "total_cost": 686.2115733312625
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9775
Profiling... [256/50048]	Loss: 2.2001
Profiling... [384/50048]	Loss: 1.8541
Profiling... [512/50048]	Loss: 2.0368
Profiling... [640/50048]	Loss: 2.1035
Profiling... [768/50048]	Loss: 2.1664
Profiling... [896/50048]	Loss: 2.0527
Profiling... [1024/50048]	Loss: 2.4407
Profiling... [1152/50048]	Loss: 2.1019
Profiling... [1280/50048]	Loss: 2.3106
Profiling... [1408/50048]	Loss: 1.9812
Profiling... [1536/50048]	Loss: 1.9480
Profiling... [1664/50048]	Loss: 2.2814
Profile done
epoch 1 train time consumed: 3.26s
Validation Epoch: 6, Average loss: 0.0161, Accuracy: 0.4456
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.3628986068278,
                        "time": 2.1953928240036475,
                        "accuracy": 0.4456091772151899,
                        "total_cost": 573.2877275512844
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9127
Profiling... [256/50048]	Loss: 2.0760
Profiling... [384/50048]	Loss: 2.0730
Profiling... [512/50048]	Loss: 2.4081
Profiling... [640/50048]	Loss: 1.8119
Profiling... [768/50048]	Loss: 1.9999
Profiling... [896/50048]	Loss: 2.0623
Profiling... [1024/50048]	Loss: 1.8289
Profiling... [1152/50048]	Loss: 2.0587
Profiling... [1280/50048]	Loss: 2.1734
Profiling... [1408/50048]	Loss: 2.1879
Profiling... [1536/50048]	Loss: 2.0977
Profiling... [1664/50048]	Loss: 1.8633
Profile done
epoch 1 train time consumed: 3.36s
Validation Epoch: 6, Average loss: 0.0162, Accuracy: 0.4470
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.3727263630778,
                        "time": 2.183994077000534,
                        "accuracy": 0.44699367088607594,
                        "total_cost": 568.5927154126132
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0417
Profiling... [256/50048]	Loss: 2.1536
Profiling... [384/50048]	Loss: 2.2670
Profiling... [512/50048]	Loss: 1.8055
Profiling... [640/50048]	Loss: 1.9339
Profiling... [768/50048]	Loss: 2.1143
Profiling... [896/50048]	Loss: 2.1375
Profiling... [1024/50048]	Loss: 2.2403
Profiling... [1152/50048]	Loss: 2.2889
Profiling... [1280/50048]	Loss: 2.3085
Profiling... [1408/50048]	Loss: 2.0379
Profiling... [1536/50048]	Loss: 1.9266
Profiling... [1664/50048]	Loss: 1.8611
Profile done
epoch 1 train time consumed: 3.31s
Validation Epoch: 6, Average loss: 0.0162, Accuracy: 0.4477
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.37708042973165,
                        "time": 2.2157679250012734,
                        "accuracy": 0.447685917721519,
                        "total_cost": 575.9944456905974
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0677
Profiling... [256/50048]	Loss: 2.2772
Profiling... [384/50048]	Loss: 2.1050
Profiling... [512/50048]	Loss: 2.3828
Profiling... [640/50048]	Loss: 1.9273
Profiling... [768/50048]	Loss: 1.9352
Profiling... [896/50048]	Loss: 1.9461
Profiling... [1024/50048]	Loss: 2.0341
Profiling... [1152/50048]	Loss: 2.1192
Profiling... [1280/50048]	Loss: 1.9730
Profiling... [1408/50048]	Loss: 1.9987
Profiling... [1536/50048]	Loss: 1.9503
Profiling... [1664/50048]	Loss: 1.9548
Profile done
epoch 1 train time consumed: 3.84s
Validation Epoch: 6, Average loss: 0.0160, Accuracy: 0.4534
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 116.34455948595638,
                        "time": 2.5776180130051216,
                        "accuracy": 0.4534216772151899,
                        "total_cost": 661.3972099614043
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1718
Profiling... [256/50048]	Loss: 2.1361
Profiling... [384/50048]	Loss: 2.3305
Profiling... [512/50048]	Loss: 2.3309
Profiling... [640/50048]	Loss: 2.1176
Profiling... [768/50048]	Loss: 2.0590
Profiling... [896/50048]	Loss: 2.1330
Profiling... [1024/50048]	Loss: 2.3131
Profiling... [1152/50048]	Loss: 2.1902
Profiling... [1280/50048]	Loss: 2.0282
Profiling... [1408/50048]	Loss: 2.2080
Profiling... [1536/50048]	Loss: 2.2043
Profiling... [1664/50048]	Loss: 2.0600
Profile done
epoch 1 train time consumed: 3.31s
Validation Epoch: 6, Average loss: 0.0161, Accuracy: 0.4480
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.29139852593661,
                        "time": 2.1992231370022637,
                        "accuracy": 0.44798259493670883,
                        "total_cost": 570.8943542967853
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1542
Profiling... [256/50048]	Loss: 2.1920
Profiling... [384/50048]	Loss: 1.9771
Profiling... [512/50048]	Loss: 1.9400
Profiling... [640/50048]	Loss: 1.9637
Profiling... [768/50048]	Loss: 2.1602
Profiling... [896/50048]	Loss: 2.0963
Profiling... [1024/50048]	Loss: 1.8460
Profiling... [1152/50048]	Loss: 2.1439
Profiling... [1280/50048]	Loss: 2.0760
Profiling... [1408/50048]	Loss: 1.9584
Profiling... [1536/50048]	Loss: 2.0583
Profiling... [1664/50048]	Loss: 1.9584
Profile done
epoch 1 train time consumed: 3.35s
Validation Epoch: 6, Average loss: 0.0160, Accuracy: 0.4468
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.30243941456533,
                        "time": 2.1894268600008218,
                        "accuracy": 0.4467958860759494,
                        "total_cost": 569.9150163942711
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0753
Profiling... [256/50048]	Loss: 2.0032
Profiling... [384/50048]	Loss: 2.1777
Profiling... [512/50048]	Loss: 2.0372
Profiling... [640/50048]	Loss: 1.9348
Profiling... [768/50048]	Loss: 1.7151
Profiling... [896/50048]	Loss: 1.8859
Profiling... [1024/50048]	Loss: 1.9626
Profiling... [1152/50048]	Loss: 1.7922
Profiling... [1280/50048]	Loss: 2.1088
Profiling... [1408/50048]	Loss: 2.0097
Profiling... [1536/50048]	Loss: 2.0520
Profiling... [1664/50048]	Loss: 2.0375
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 6, Average loss: 0.0162, Accuracy: 0.4435
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.30778673377651,
                        "time": 2.233325966997654,
                        "accuracy": 0.4435324367088608,
                        "total_cost": 585.6464573459664
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0104
Profiling... [256/50048]	Loss: 2.2105
Profiling... [384/50048]	Loss: 1.9234
Profiling... [512/50048]	Loss: 1.9125
Profiling... [640/50048]	Loss: 2.0820
Profiling... [768/50048]	Loss: 2.2121
Profiling... [896/50048]	Loss: 1.7874
Profiling... [1024/50048]	Loss: 2.2379
Profiling... [1152/50048]	Loss: 1.8419
Profiling... [1280/50048]	Loss: 1.9437
Profiling... [1408/50048]	Loss: 1.8115
Profiling... [1536/50048]	Loss: 2.0286
Profiling... [1664/50048]	Loss: 1.7403
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 6, Average loss: 0.0161, Accuracy: 0.4479
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 116.27718567350573,
                        "time": 2.566297779994784,
                        "accuracy": 0.44788370253164556,
                        "total_cost": 666.2485858968607
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.5004
Profiling... [256/50048]	Loss: 2.1268
Profiling... [384/50048]	Loss: 2.0059
Profiling... [512/50048]	Loss: 2.1094
Profiling... [640/50048]	Loss: 1.8718
Profiling... [768/50048]	Loss: 2.1931
Profiling... [896/50048]	Loss: 2.3012
Profiling... [1024/50048]	Loss: 1.8580
Profiling... [1152/50048]	Loss: 1.9667
Profiling... [1280/50048]	Loss: 2.2300
Profiling... [1408/50048]	Loss: 1.8402
Profiling... [1536/50048]	Loss: 2.1001
Profiling... [1664/50048]	Loss: 1.9858
Profile done
epoch 1 train time consumed: 3.22s
Validation Epoch: 6, Average loss: 0.0193, Accuracy: 0.3751
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.22452558922335,
                        "time": 2.1967132360005053,
                        "accuracy": 0.3750988924050633,
                        "total_cost": 680.6523796237154
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.2962
Profiling... [256/50048]	Loss: 1.9018
Profiling... [384/50048]	Loss: 2.0698
Profiling... [512/50048]	Loss: 2.0920
Profiling... [640/50048]	Loss: 2.0679
Profiling... [768/50048]	Loss: 2.0337
Profiling... [896/50048]	Loss: 2.1447
Profiling... [1024/50048]	Loss: 2.0787
Profiling... [1152/50048]	Loss: 2.2716
Profiling... [1280/50048]	Loss: 2.0679
Profiling... [1408/50048]	Loss: 1.8966
Profiling... [1536/50048]	Loss: 1.9692
Profiling... [1664/50048]	Loss: 2.2540
Profile done
epoch 1 train time consumed: 3.29s
Validation Epoch: 6, Average loss: 0.0196, Accuracy: 0.3748
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.23864919043913,
                        "time": 2.176700295000046,
                        "accuracy": 0.37480221518987344,
                        "total_cost": 675.0672534180685
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1023
Profiling... [256/50048]	Loss: 2.3472
Profiling... [384/50048]	Loss: 1.8670
Profiling... [512/50048]	Loss: 2.2953
Profiling... [640/50048]	Loss: 2.0633
Profiling... [768/50048]	Loss: 2.0371
Profiling... [896/50048]	Loss: 1.9761
Profiling... [1024/50048]	Loss: 2.0050
Profiling... [1152/50048]	Loss: 2.1283
Profiling... [1280/50048]	Loss: 2.0856
Profiling... [1408/50048]	Loss: 2.1723
Profiling... [1536/50048]	Loss: 1.8715
Profiling... [1664/50048]	Loss: 1.9669
Profile done
epoch 1 train time consumed: 3.31s
Validation Epoch: 6, Average loss: 0.0191, Accuracy: 0.3792
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.24522323942045,
                        "time": 2.224927855997521,
                        "accuracy": 0.3791534810126582,
                        "total_cost": 682.1439028365462
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9349
Profiling... [256/50048]	Loss: 2.1663
Profiling... [384/50048]	Loss: 1.9430
Profiling... [512/50048]	Loss: 1.9615
Profiling... [640/50048]	Loss: 2.0711
Profiling... [768/50048]	Loss: 2.1317
Profiling... [896/50048]	Loss: 1.9623
Profiling... [1024/50048]	Loss: 2.1069
Profiling... [1152/50048]	Loss: 1.8417
Profiling... [1280/50048]	Loss: 1.7899
Profiling... [1408/50048]	Loss: 2.1601
Profiling... [1536/50048]	Loss: 2.4359
Profiling... [1664/50048]	Loss: 1.9605
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 6, Average loss: 0.0211, Accuracy: 0.3445
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 116.21333252352625,
                        "time": 2.5721969080041163,
                        "accuracy": 0.34454113924050633,
                        "total_cost": 867.5990775580659
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0136
Profiling... [256/50048]	Loss: 2.3578
Profiling... [384/50048]	Loss: 2.0341
Profiling... [512/50048]	Loss: 2.1343
Profiling... [640/50048]	Loss: 1.9764
Profiling... [768/50048]	Loss: 1.9694
Profiling... [896/50048]	Loss: 2.0714
Profiling... [1024/50048]	Loss: 2.3308
Profiling... [1152/50048]	Loss: 1.8094
Profiling... [1280/50048]	Loss: 2.0549
Profiling... [1408/50048]	Loss: 2.0420
Profiling... [1536/50048]	Loss: 1.8836
Profiling... [1664/50048]	Loss: 1.9960
Profile done
epoch 1 train time consumed: 3.23s
Validation Epoch: 6, Average loss: 0.0203, Accuracy: 0.3620
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.16378545300176,
                        "time": 2.1716906020010356,
                        "accuracy": 0.36204509493670883,
                        "total_cost": 696.796627517491
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.3754
Profiling... [256/50048]	Loss: 2.0428
Profiling... [384/50048]	Loss: 2.1979
Profiling... [512/50048]	Loss: 2.1505
Profiling... [640/50048]	Loss: 2.1555
Profiling... [768/50048]	Loss: 2.1464
Profiling... [896/50048]	Loss: 2.2294
Profiling... [1024/50048]	Loss: 1.9147
Profiling... [1152/50048]	Loss: 2.1664
Profiling... [1280/50048]	Loss: 2.0544
Profiling... [1408/50048]	Loss: 2.1054
Profiling... [1536/50048]	Loss: 2.2193
Profiling... [1664/50048]	Loss: 2.0987
Profile done
epoch 1 train time consumed: 3.36s
Validation Epoch: 6, Average loss: 0.0222, Accuracy: 0.3374
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.17572693923042,
                        "time": 2.172656499002187,
                        "accuracy": 0.3374208860759494,
                        "total_cost": 748.0566810674783
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1619
Profiling... [256/50048]	Loss: 2.1730
Profiling... [384/50048]	Loss: 1.9753
Profiling... [512/50048]	Loss: 1.9275
Profiling... [640/50048]	Loss: 2.0749
Profiling... [768/50048]	Loss: 2.0673
Profiling... [896/50048]	Loss: 2.2433
Profiling... [1024/50048]	Loss: 1.9316
Profiling... [1152/50048]	Loss: 1.9682
Profiling... [1280/50048]	Loss: 2.1238
Profiling... [1408/50048]	Loss: 2.0538
Profiling... [1536/50048]	Loss: 1.8923
Profiling... [1664/50048]	Loss: 1.9599
Profile done
epoch 1 train time consumed: 3.31s
Validation Epoch: 6, Average loss: 0.0180, Accuracy: 0.4100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.18239031442509,
                        "time": 2.2206616059993394,
                        "accuracy": 0.41000791139240506,
                        "total_cost": 629.260475946154
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.1244
Profiling... [256/50048]	Loss: 2.1890
Profiling... [384/50048]	Loss: 1.8032
Profiling... [512/50048]	Loss: 2.0608
Profiling... [640/50048]	Loss: 2.0647
Profiling... [768/50048]	Loss: 2.2259
Profiling... [896/50048]	Loss: 2.1176
Profiling... [1024/50048]	Loss: 2.1464
Profiling... [1152/50048]	Loss: 2.2684
Profiling... [1280/50048]	Loss: 2.1578
Profiling... [1408/50048]	Loss: 1.9093
Profiling... [1536/50048]	Loss: 2.2533
Profiling... [1664/50048]	Loss: 2.0693
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 6, Average loss: 0.0180, Accuracy: 0.4085
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 116.15223708661719,
                        "time": 2.576535945001524,
                        "accuracy": 0.4085245253164557,
                        "total_cost": 732.5641311600183
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0655
Profiling... [256/50048]	Loss: 2.0628
Profiling... [384/50048]	Loss: 2.0437
Profiling... [512/50048]	Loss: 2.2098
Profiling... [640/50048]	Loss: 2.1114
Profiling... [768/50048]	Loss: 2.0956
Profiling... [896/50048]	Loss: 1.9015
Profiling... [1024/50048]	Loss: 1.7966
Profiling... [1152/50048]	Loss: 1.9598
Profiling... [1280/50048]	Loss: 2.2146
Profiling... [1408/50048]	Loss: 1.9803
Profiling... [1536/50048]	Loss: 2.2873
Profiling... [1664/50048]	Loss: 2.2355
Profile done
epoch 1 train time consumed: 3.24s
Validation Epoch: 6, Average loss: 0.0205, Accuracy: 0.3650
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.10399453974314,
                        "time": 2.1836431590054417,
                        "accuracy": 0.3650118670886076,
                        "total_cost": 694.5793171934606
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9831
Profiling... [256/50048]	Loss: 2.1101
Profiling... [384/50048]	Loss: 2.0726
Profiling... [512/50048]	Loss: 2.1068
Profiling... [640/50048]	Loss: 2.1107
Profiling... [768/50048]	Loss: 1.8571
Profiling... [896/50048]	Loss: 1.9991
Profiling... [1024/50048]	Loss: 1.8813
Profiling... [1152/50048]	Loss: 2.1242
Profiling... [1280/50048]	Loss: 1.9495
Profiling... [1408/50048]	Loss: 2.2398
Profiling... [1536/50048]	Loss: 2.0457
Profiling... [1664/50048]	Loss: 2.1047
Profile done
epoch 1 train time consumed: 3.26s
Validation Epoch: 6, Average loss: 0.0229, Accuracy: 0.3229
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.11524235459662,
                        "time": 2.1978291060004267,
                        "accuracy": 0.32288370253164556,
                        "total_cost": 790.3819774620365
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0186
Profiling... [256/50048]	Loss: 2.0568
Profiling... [384/50048]	Loss: 2.0667
Profiling... [512/50048]	Loss: 2.3628
Profiling... [640/50048]	Loss: 2.0435
Profiling... [768/50048]	Loss: 2.1124
Profiling... [896/50048]	Loss: 2.0672
Profiling... [1024/50048]	Loss: 2.2586
Profiling... [1152/50048]	Loss: 2.0167
Profiling... [1280/50048]	Loss: 2.0566
Profiling... [1408/50048]	Loss: 1.9238
Profiling... [1536/50048]	Loss: 1.9661
Profiling... [1664/50048]	Loss: 1.9599
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 6, Average loss: 0.0170, Accuracy: 0.4280
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.12256705994972,
                        "time": 2.2445385079990956,
                        "accuracy": 0.42800632911392406,
                        "total_cost": 608.9666336321594
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.3414
Profiling... [256/50048]	Loss: 2.2150
Profiling... [384/50048]	Loss: 2.1122
Profiling... [512/50048]	Loss: 2.0018
Profiling... [640/50048]	Loss: 2.1357
Profiling... [768/50048]	Loss: 2.3826
Profiling... [896/50048]	Loss: 1.9442
Profiling... [1024/50048]	Loss: 2.1368
Profiling... [1152/50048]	Loss: 1.8325
Profiling... [1280/50048]	Loss: 1.7094
Profiling... [1408/50048]	Loss: 2.0933
Profiling... [1536/50048]	Loss: 2.2316
Profiling... [1664/50048]	Loss: 1.9985
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 6, Average loss: 0.0176, Accuracy: 0.4100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 116.0890690002816,
                        "time": 2.585936079005478,
                        "accuracy": 0.41000791139240506,
                        "total_cost": 732.1783398922614
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1349
Profiling... [256/50048]	Loss: 2.3157
Profiling... [384/50048]	Loss: 2.0777
Profiling... [512/50048]	Loss: 2.1449
Profiling... [640/50048]	Loss: 2.4304
Profiling... [768/50048]	Loss: 2.2303
Profiling... [896/50048]	Loss: 1.9238
Profiling... [1024/50048]	Loss: 2.1945
Profiling... [1152/50048]	Loss: 2.1740
Profiling... [1280/50048]	Loss: 2.0804
Profiling... [1408/50048]	Loss: 2.1740
Profiling... [1536/50048]	Loss: 2.1295
Profiling... [1664/50048]	Loss: 2.5126
Profile done
epoch 1 train time consumed: 3.35s
Validation Epoch: 6, Average loss: 0.0323, Accuracy: 0.1919
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.04315155911836,
                        "time": 2.180904989996634,
                        "accuracy": 0.1918512658227848,
                        "total_cost": 1319.142134428182
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.4167
Profiling... [256/50048]	Loss: 1.7893
Profiling... [384/50048]	Loss: 2.2064
Profiling... [512/50048]	Loss: 2.1371
Profiling... [640/50048]	Loss: 1.9913
Profiling... [768/50048]	Loss: 2.2220
Profiling... [896/50048]	Loss: 2.1707
Profiling... [1024/50048]	Loss: 2.0755
Profiling... [1152/50048]	Loss: 1.9988
Profiling... [1280/50048]	Loss: 2.3961
Profiling... [1408/50048]	Loss: 2.3210
Profiling... [1536/50048]	Loss: 2.2346
Profiling... [1664/50048]	Loss: 2.0498
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 6, Average loss: 0.0362, Accuracy: 0.2125
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.05095158967768,
                        "time": 2.2956135400017956,
                        "accuracy": 0.21251977848101267,
                        "total_cost": 1253.5686687776165
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1349
Profiling... [256/50048]	Loss: 2.2240
Profiling... [384/50048]	Loss: 1.9416
Profiling... [512/50048]	Loss: 2.1168
Profiling... [640/50048]	Loss: 2.0942
Profiling... [768/50048]	Loss: 2.1212
Profiling... [896/50048]	Loss: 2.1537
Profiling... [1024/50048]	Loss: 2.2440
Profiling... [1152/50048]	Loss: 2.2507
Profiling... [1280/50048]	Loss: 2.1842
Profiling... [1408/50048]	Loss: 2.1686
Profiling... [1536/50048]	Loss: 2.2847
Profiling... [1664/50048]	Loss: 2.1938
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 6, Average loss: 0.0273, Accuracy: 0.2457
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.05833638889132,
                        "time": 2.216357880002761,
                        "accuracy": 0.24574762658227847,
                        "total_cost": 1046.7112621712693
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.3864
Profiling... [256/50048]	Loss: 2.0223
Profiling... [384/50048]	Loss: 2.3462
Profiling... [512/50048]	Loss: 2.3086
Profiling... [640/50048]	Loss: 2.0989
Profiling... [768/50048]	Loss: 2.1762
Profiling... [896/50048]	Loss: 2.3581
Profiling... [1024/50048]	Loss: 2.2650
Profiling... [1152/50048]	Loss: 2.3447
Profiling... [1280/50048]	Loss: 2.1847
Profiling... [1408/50048]	Loss: 1.9364
Profiling... [1536/50048]	Loss: 2.1481
Profiling... [1664/50048]	Loss: 2.0982
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 6, Average loss: 0.0325, Accuracy: 0.2564
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 116.02363170772374,
                        "time": 2.596332351997262,
                        "accuracy": 0.25642800632911394,
                        "total_cost": 1174.7387226196954
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.3193
Profiling... [256/50048]	Loss: 2.0845
Profiling... [384/50048]	Loss: 2.3993
Profiling... [512/50048]	Loss: 2.2040
Profiling... [640/50048]	Loss: 2.2130
Profiling... [768/50048]	Loss: 2.1195
Profiling... [896/50048]	Loss: 2.3033
Profiling... [1024/50048]	Loss: 2.2247
Profiling... [1152/50048]	Loss: 2.1319
Profiling... [1280/50048]	Loss: 2.1469
Profiling... [1408/50048]	Loss: 2.6138
Profiling... [1536/50048]	Loss: 2.5693
Profiling... [1664/50048]	Loss: 1.9717
Profile done
epoch 1 train time consumed: 3.34s
Validation Epoch: 6, Average loss: 0.0212, Accuracy: 0.3286
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.97737479986365,
                        "time": 2.203280841000378,
                        "accuracy": 0.32861946202531644,
                        "total_cost": 777.5885405909826
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.2021
Profiling... [256/50048]	Loss: 2.1876
Profiling... [384/50048]	Loss: 2.3707
Profiling... [512/50048]	Loss: 2.1216
Profiling... [640/50048]	Loss: 2.0285
Profiling... [768/50048]	Loss: 2.1332
Profiling... [896/50048]	Loss: 2.0903
Profiling... [1024/50048]	Loss: 1.9738
Profiling... [1152/50048]	Loss: 2.2002
Profiling... [1280/50048]	Loss: 1.9282
Profiling... [1408/50048]	Loss: 2.3163
Profiling... [1536/50048]	Loss: 2.1359
Profiling... [1664/50048]	Loss: 2.2713
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 6, Average loss: 0.0342, Accuracy: 0.1765
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.98618908921917,
                        "time": 2.180704449994664,
                        "accuracy": 0.1765229430379747,
                        "total_cost": 1432.8539640899291
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1662
Profiling... [256/50048]	Loss: 2.0371
Profiling... [384/50048]	Loss: 2.1152
Profiling... [512/50048]	Loss: 2.0587
Profiling... [640/50048]	Loss: 2.1996
Profiling... [768/50048]	Loss: 2.0394
Profiling... [896/50048]	Loss: 2.4194
Profiling... [1024/50048]	Loss: 2.1801
Profiling... [1152/50048]	Loss: 2.1913
Profiling... [1280/50048]	Loss: 2.5265
Profiling... [1408/50048]	Loss: 2.4376
Profiling... [1536/50048]	Loss: 2.0074
Profiling... [1664/50048]	Loss: 2.2376
Profile done
epoch 1 train time consumed: 3.30s
Validation Epoch: 6, Average loss: 0.0411, Accuracy: 0.1618
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.98936145354416,
                        "time": 2.227711241997895,
                        "accuracy": 0.1617879746835443,
                        "total_cost": 1597.0952412725821
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.1300
Profiling... [256/50048]	Loss: 2.0898
Profiling... [384/50048]	Loss: 2.3074
Profiling... [512/50048]	Loss: 2.3353
Profiling... [640/50048]	Loss: 2.2773
Profiling... [768/50048]	Loss: 2.2705
Profiling... [896/50048]	Loss: 2.1649
Profiling... [1024/50048]	Loss: 2.3798
Profiling... [1152/50048]	Loss: 2.3008
Profiling... [1280/50048]	Loss: 2.0897
Profiling... [1408/50048]	Loss: 2.2850
Profiling... [1536/50048]	Loss: 2.2992
Profiling... [1664/50048]	Loss: 2.0404
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 6, Average loss: 0.0265, Accuracy: 0.2775
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.96201059578578,
                        "time": 2.5592476789970533,
                        "accuracy": 0.27749208860759494,
                        "total_cost": 1069.4917752728095
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0925
Profiling... [256/50048]	Loss: 2.1806
Profiling... [384/50048]	Loss: 2.3096
Profiling... [512/50048]	Loss: 2.3085
Profiling... [640/50048]	Loss: 2.1660
Profiling... [768/50048]	Loss: 2.1663
Profiling... [896/50048]	Loss: 2.2169
Profiling... [1024/50048]	Loss: 2.3017
Profiling... [1152/50048]	Loss: 2.5466
Profiling... [1280/50048]	Loss: 2.3612
Profiling... [1408/50048]	Loss: 2.2938
Profiling... [1536/50048]	Loss: 2.4524
Profiling... [1664/50048]	Loss: 2.3181
Profile done
epoch 1 train time consumed: 3.26s
Validation Epoch: 6, Average loss: 0.0415, Accuracy: 0.1710
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.91542923199289,
                        "time": 2.1814310060071875,
                        "accuracy": 0.1709849683544304,
                        "total_cost": 1478.8522864603572
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1566
Profiling... [256/50048]	Loss: 1.9618
Profiling... [384/50048]	Loss: 2.1974
Profiling... [512/50048]	Loss: 2.5014
Profiling... [640/50048]	Loss: 2.2170
Profiling... [768/50048]	Loss: 2.3672
Profiling... [896/50048]	Loss: 2.2834
Profiling... [1024/50048]	Loss: 2.5410
Profiling... [1152/50048]	Loss: 2.2039
Profiling... [1280/50048]	Loss: 1.9617
Profiling... [1408/50048]	Loss: 2.1186
Profiling... [1536/50048]	Loss: 2.2112
Profiling... [1664/50048]	Loss: 2.1895
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 6, Average loss: 0.0266, Accuracy: 0.2664
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.92781898370083,
                        "time": 2.190720215003239,
                        "accuracy": 0.26641613924050633,
                        "total_cost": 953.2658841646345
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.2191
Profiling... [256/50048]	Loss: 2.0699
Profiling... [384/50048]	Loss: 2.3830
Profiling... [512/50048]	Loss: 2.5277
Profiling... [640/50048]	Loss: 2.2164
Profiling... [768/50048]	Loss: 2.1140
Profiling... [896/50048]	Loss: 2.3274
Profiling... [1024/50048]	Loss: 2.2983
Profiling... [1152/50048]	Loss: 2.4633
Profiling... [1280/50048]	Loss: 2.0464
Profiling... [1408/50048]	Loss: 1.9752
Profiling... [1536/50048]	Loss: 2.2465
Profiling... [1664/50048]	Loss: 2.2000
Profile done
epoch 1 train time consumed: 3.36s
Validation Epoch: 6, Average loss: 0.0359, Accuracy: 0.1651
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.93214799045421,
                        "time": 2.250150334999489,
                        "accuracy": 0.16505142405063292,
                        "total_cost": 1580.5059734468286
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.3985
Profiling... [256/50048]	Loss: 2.0359
Profiling... [384/50048]	Loss: 2.1105
Profiling... [512/50048]	Loss: 2.3257
Profiling... [640/50048]	Loss: 2.2375
Profiling... [768/50048]	Loss: 2.3644
Profiling... [896/50048]	Loss: 2.3322
Profiling... [1024/50048]	Loss: 2.4185
Profiling... [1152/50048]	Loss: 2.0117
Profiling... [1280/50048]	Loss: 2.0509
Profiling... [1408/50048]	Loss: 2.4890
Profiling... [1536/50048]	Loss: 2.2094
Profiling... [1664/50048]	Loss: 2.3850
Profile done
epoch 1 train time consumed: 3.84s
Validation Epoch: 6, Average loss: 0.0244, Accuracy: 0.2904
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.9037842698341,
                        "time": 2.590743870001461,
                        "accuracy": 0.29044699367088606,
                        "total_cost": 1033.8444712817275
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.1497
Profiling... [512/50176]	Loss: 2.0731
Profiling... [768/50176]	Loss: 2.1864
Profiling... [1024/50176]	Loss: 1.9678
Profiling... [1280/50176]	Loss: 2.0751
Profiling... [1536/50176]	Loss: 1.9715
Profiling... [1792/50176]	Loss: 2.1235
Profiling... [2048/50176]	Loss: 2.2224
Profiling... [2304/50176]	Loss: 1.8589
Profiling... [2560/50176]	Loss: 1.9323
Profiling... [2816/50176]	Loss: 2.0430
Profiling... [3072/50176]	Loss: 1.9869
Profiling... [3328/50176]	Loss: 1.9910
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 6, Average loss: 0.0079, Accuracy: 0.4462
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.87122307523983,
                        "time": 2.404703796004469,
                        "accuracy": 0.44619140625,
                        "total_cost": 624.4763257959091
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1956
Profiling... [512/50176]	Loss: 2.1233
Profiling... [768/50176]	Loss: 2.0187
Profiling... [1024/50176]	Loss: 2.1930
Profiling... [1280/50176]	Loss: 1.8984
Profiling... [1536/50176]	Loss: 2.0965
Profiling... [1792/50176]	Loss: 2.0368
Profiling... [2048/50176]	Loss: 2.0966
Profiling... [2304/50176]	Loss: 1.9646
Profiling... [2560/50176]	Loss: 2.0863
Profiling... [2816/50176]	Loss: 1.9752
Profiling... [3072/50176]	Loss: 2.0102
Profiling... [3328/50176]	Loss: 2.0113
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 6, Average loss: 0.0081, Accuracy: 0.4348
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.90054086944188,
                        "time": 2.4182342009953572,
                        "accuracy": 0.434765625,
                        "total_cost": 644.6568811514124
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.2481
Profiling... [512/50176]	Loss: 2.1259
Profiling... [768/50176]	Loss: 2.0086
Profiling... [1024/50176]	Loss: 1.9804
Profiling... [1280/50176]	Loss: 2.2041
Profiling... [1536/50176]	Loss: 1.8494
Profiling... [1792/50176]	Loss: 1.9033
Profiling... [2048/50176]	Loss: 2.0370
Profiling... [2304/50176]	Loss: 1.7266
Profiling... [2560/50176]	Loss: 2.0384
Profiling... [2816/50176]	Loss: 2.0231
Profiling... [3072/50176]	Loss: 1.9918
Profiling... [3328/50176]	Loss: 1.8892
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 6, Average loss: 0.0079, Accuracy: 0.4493
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.91209220439049,
                        "time": 2.5520769909999217,
                        "accuracy": 0.44931640625,
                        "total_cost": 658.3703140563573
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1070
Profiling... [512/50176]	Loss: 2.1134
Profiling... [768/50176]	Loss: 2.0764
Profiling... [1024/50176]	Loss: 2.0543
Profiling... [1280/50176]	Loss: 2.2130
Profiling... [1536/50176]	Loss: 1.9852
Profiling... [1792/50176]	Loss: 2.1646
Profiling... [2048/50176]	Loss: 1.8531
Profiling... [2304/50176]	Loss: 2.0572
Profiling... [2560/50176]	Loss: 1.8656
Profiling... [2816/50176]	Loss: 1.8775
Profiling... [3072/50176]	Loss: 2.0876
Profiling... [3328/50176]	Loss: 2.1041
Profile done
epoch 1 train time consumed: 4.34s
Validation Epoch: 6, Average loss: 0.0079, Accuracy: 0.4480
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.8731170037378,
                        "time": 3.0266747799978475,
                        "accuracy": 0.448046875,
                        "total_cost": 782.753458362929
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9947
Profiling... [512/50176]	Loss: 2.2683
Profiling... [768/50176]	Loss: 2.1100
Profiling... [1024/50176]	Loss: 2.1253
Profiling... [1280/50176]	Loss: 2.0030
Profiling... [1536/50176]	Loss: 1.9809
Profiling... [1792/50176]	Loss: 1.8805
Profiling... [2048/50176]	Loss: 2.1104
Profiling... [2304/50176]	Loss: 2.0781
Profiling... [2560/50176]	Loss: 2.0503
Profiling... [2816/50176]	Loss: 2.0176
Profiling... [3072/50176]	Loss: 1.9779
Profiling... [3328/50176]	Loss: 1.9720
Profile done
epoch 1 train time consumed: 3.68s
Validation Epoch: 6, Average loss: 0.0078, Accuracy: 0.4511
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.83759826355416,
                        "time": 2.407457549998071,
                        "accuracy": 0.45107421875,
                        "total_cost": 618.2443795747015
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0759
Profiling... [512/50176]	Loss: 2.1238
Profiling... [768/50176]	Loss: 2.0032
Profiling... [1024/50176]	Loss: 2.0483
Profiling... [1280/50176]	Loss: 2.0221
Profiling... [1536/50176]	Loss: 1.9390
Profiling... [1792/50176]	Loss: 2.1688
Profiling... [2048/50176]	Loss: 2.1663
Profiling... [2304/50176]	Loss: 2.0414
Profiling... [2560/50176]	Loss: 2.0124
Profiling... [2816/50176]	Loss: 2.0452
Profiling... [3072/50176]	Loss: 1.9214
Profiling... [3328/50176]	Loss: 1.8786
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 6, Average loss: 0.0081, Accuracy: 0.4387
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.86358593410192,
                        "time": 2.4169108369969763,
                        "accuracy": 0.438671875,
                        "total_cost": 638.3631420579708
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1161
Profiling... [512/50176]	Loss: 2.1229
Profiling... [768/50176]	Loss: 1.9730
Profiling... [1024/50176]	Loss: 1.9536
Profiling... [1280/50176]	Loss: 2.0165
Profiling... [1536/50176]	Loss: 2.1000
Profiling... [1792/50176]	Loss: 2.0106
Profiling... [2048/50176]	Loss: 2.0962
Profiling... [2304/50176]	Loss: 1.9809
Profiling... [2560/50176]	Loss: 1.7414
Profiling... [2816/50176]	Loss: 2.0760
Profiling... [3072/50176]	Loss: 1.9320
Profiling... [3328/50176]	Loss: 1.9602
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 6, Average loss: 0.0079, Accuracy: 0.4504
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.87665368327102,
                        "time": 2.53361249199952,
                        "accuracy": 0.450390625,
                        "total_cost": 651.8486864664148
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0225
Profiling... [512/50176]	Loss: 1.9500
Profiling... [768/50176]	Loss: 2.1091
Profiling... [1024/50176]	Loss: 2.2512
Profiling... [1280/50176]	Loss: 1.9440
Profiling... [1536/50176]	Loss: 1.9183
Profiling... [1792/50176]	Loss: 2.2231
Profiling... [2048/50176]	Loss: 2.0911
Profiling... [2304/50176]	Loss: 1.9626
Profiling... [2560/50176]	Loss: 1.9107
Profiling... [2816/50176]	Loss: 1.9846
Profiling... [3072/50176]	Loss: 2.1282
Profiling... [3328/50176]	Loss: 1.9482
Profile done
epoch 1 train time consumed: 4.32s
Validation Epoch: 6, Average loss: 0.0079, Accuracy: 0.4479
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.84121011660805,
                        "time": 3.032205581002927,
                        "accuracy": 0.4478515625,
                        "total_cost": 784.3097875218681
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0644
Profiling... [512/50176]	Loss: 2.1962
Profiling... [768/50176]	Loss: 2.0885
Profiling... [1024/50176]	Loss: 2.1615
Profiling... [1280/50176]	Loss: 2.1484
Profiling... [1536/50176]	Loss: 1.9761
Profiling... [1792/50176]	Loss: 1.8283
Profiling... [2048/50176]	Loss: 2.0286
Profiling... [2304/50176]	Loss: 1.8442
Profiling... [2560/50176]	Loss: 1.8913
Profiling... [2816/50176]	Loss: 2.0296
Profiling... [3072/50176]	Loss: 2.1765
Profiling... [3328/50176]	Loss: 2.0728
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 6, Average loss: 0.0081, Accuracy: 0.4365
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.80342671966628,
                        "time": 2.4317539449984906,
                        "accuracy": 0.4365234375,
                        "total_cost": 645.1095532983658
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.2112
Profiling... [512/50176]	Loss: 1.9935
Profiling... [768/50176]	Loss: 2.1231
Profiling... [1024/50176]	Loss: 2.1237
Profiling... [1280/50176]	Loss: 1.8810
Profiling... [1536/50176]	Loss: 2.1781
Profiling... [1792/50176]	Loss: 1.8505
Profiling... [2048/50176]	Loss: 1.9785
Profiling... [2304/50176]	Loss: 1.8597
Profiling... [2560/50176]	Loss: 1.9427
Profiling... [2816/50176]	Loss: 1.9721
Profiling... [3072/50176]	Loss: 2.0175
Profiling... [3328/50176]	Loss: 1.7779
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 6, Average loss: 0.0080, Accuracy: 0.4411
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.82872549398114,
                        "time": 2.4310414630017476,
                        "accuracy": 0.44111328125,
                        "total_cost": 638.3494813046184
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0287
Profiling... [512/50176]	Loss: 2.0767
Profiling... [768/50176]	Loss: 2.1783
Profiling... [1024/50176]	Loss: 2.2219
Profiling... [1280/50176]	Loss: 1.9557
Profiling... [1536/50176]	Loss: 2.1954
Profiling... [1792/50176]	Loss: 1.8649
Profiling... [2048/50176]	Loss: 2.0947
Profiling... [2304/50176]	Loss: 2.0028
Profiling... [2560/50176]	Loss: 1.9707
Profiling... [2816/50176]	Loss: 1.8771
Profiling... [3072/50176]	Loss: 2.1396
Profiling... [3328/50176]	Loss: 2.0515
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 6, Average loss: 0.0079, Accuracy: 0.4485
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.8416511297694,
                        "time": 2.539672740997048,
                        "accuracy": 0.44853515625,
                        "total_cost": 655.9126515433878
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0149
Profiling... [512/50176]	Loss: 1.9537
Profiling... [768/50176]	Loss: 2.0967
Profiling... [1024/50176]	Loss: 1.9373
Profiling... [1280/50176]	Loss: 2.1371
Profiling... [1536/50176]	Loss: 1.8764
Profiling... [1792/50176]	Loss: 2.1165
Profiling... [2048/50176]	Loss: 2.1703
Profiling... [2304/50176]	Loss: 1.9072
Profiling... [2560/50176]	Loss: 2.0035
Profiling... [2816/50176]	Loss: 2.0318
Profiling... [3072/50176]	Loss: 1.8845
Profiling... [3328/50176]	Loss: 1.8580
Profile done
epoch 1 train time consumed: 4.48s
Validation Epoch: 6, Average loss: 0.0080, Accuracy: 0.4436
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.80677031359653,
                        "time": 3.0040798130066833,
                        "accuracy": 0.4435546875,
                        "total_cost": 784.3289468304333
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0402
Profiling... [512/50176]	Loss: 2.1249
Profiling... [768/50176]	Loss: 2.1173
Profiling... [1024/50176]	Loss: 2.1238
Profiling... [1280/50176]	Loss: 1.9223
Profiling... [1536/50176]	Loss: 2.0511
Profiling... [1792/50176]	Loss: 2.1486
Profiling... [2048/50176]	Loss: 1.9341
Profiling... [2304/50176]	Loss: 2.0796
Profiling... [2560/50176]	Loss: 2.0371
Profiling... [2816/50176]	Loss: 2.0743
Profiling... [3072/50176]	Loss: 1.9079
Profiling... [3328/50176]	Loss: 2.1044
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 6, Average loss: 0.0094, Accuracy: 0.3845
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.7703660174975,
                        "time": 2.399160187000234,
                        "accuracy": 0.38447265625,
                        "total_cost": 722.4223841890573
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0987
Profiling... [512/50176]	Loss: 2.0847
Profiling... [768/50176]	Loss: 1.9966
Profiling... [1024/50176]	Loss: 1.9801
Profiling... [1280/50176]	Loss: 1.8974
Profiling... [1536/50176]	Loss: 1.9553
Profiling... [1792/50176]	Loss: 1.9057
Profiling... [2048/50176]	Loss: 1.8864
Profiling... [2304/50176]	Loss: 2.1060
Profiling... [2560/50176]	Loss: 2.0054
Profiling... [2816/50176]	Loss: 2.0974
Profiling... [3072/50176]	Loss: 1.8756
Profiling... [3328/50176]	Loss: 2.1180
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 6, Average loss: 0.0095, Accuracy: 0.3761
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.8015319343931,
                        "time": 2.4025843490016996,
                        "accuracy": 0.37607421875,
                        "total_cost": 739.8086184709873
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1916
Profiling... [512/50176]	Loss: 2.0046
Profiling... [768/50176]	Loss: 1.9694
Profiling... [1024/50176]	Loss: 2.0613
Profiling... [1280/50176]	Loss: 2.0809
Profiling... [1536/50176]	Loss: 1.9181
Profiling... [1792/50176]	Loss: 2.0278
Profiling... [2048/50176]	Loss: 2.0598
Profiling... [2304/50176]	Loss: 2.0676
Profiling... [2560/50176]	Loss: 2.0559
Profiling... [2816/50176]	Loss: 2.0659
Profiling... [3072/50176]	Loss: 2.0650
Profiling... [3328/50176]	Loss: 2.2188
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 6, Average loss: 0.0096, Accuracy: 0.3771
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.81186961105566,
                        "time": 2.54813632099831,
                        "accuracy": 0.37705078125,
                        "total_cost": 782.6649513371121
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1749
Profiling... [512/50176]	Loss: 2.1144
Profiling... [768/50176]	Loss: 1.9919
Profiling... [1024/50176]	Loss: 1.7922
Profiling... [1280/50176]	Loss: 2.0592
Profiling... [1536/50176]	Loss: 2.0369
Profiling... [1792/50176]	Loss: 1.9777
Profiling... [2048/50176]	Loss: 1.9950
Profiling... [2304/50176]	Loss: 1.8740
Profiling... [2560/50176]	Loss: 1.9064
Profiling... [2816/50176]	Loss: 1.7804
Profiling... [3072/50176]	Loss: 2.0745
Profiling... [3328/50176]	Loss: 2.0041
Profile done
epoch 1 train time consumed: 4.34s
Validation Epoch: 6, Average loss: 0.0088, Accuracy: 0.4076
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.77570282519778,
                        "time": 3.0382095770037267,
                        "accuracy": 0.4076171875,
                        "total_cost": 862.9441051424096
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0818
Profiling... [512/50176]	Loss: 2.0985
Profiling... [768/50176]	Loss: 1.8631
Profiling... [1024/50176]	Loss: 2.0959
Profiling... [1280/50176]	Loss: 1.9327
Profiling... [1536/50176]	Loss: 2.1345
Profiling... [1792/50176]	Loss: 1.9612
Profiling... [2048/50176]	Loss: 2.0967
Profiling... [2304/50176]	Loss: 1.9135
Profiling... [2560/50176]	Loss: 1.9657
Profiling... [2816/50176]	Loss: 1.9454
Profiling... [3072/50176]	Loss: 2.1699
Profiling... [3328/50176]	Loss: 1.9136
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 6, Average loss: 0.0123, Accuracy: 0.3219
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.74514173701633,
                        "time": 2.4078996509997523,
                        "accuracy": 0.321875,
                        "total_cost": 865.8724237467292
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.2967
Profiling... [512/50176]	Loss: 2.0496
Profiling... [768/50176]	Loss: 2.0935
Profiling... [1024/50176]	Loss: 2.2279
Profiling... [1280/50176]	Loss: 1.9417
Profiling... [1536/50176]	Loss: 1.9681
Profiling... [1792/50176]	Loss: 2.0204
Profiling... [2048/50176]	Loss: 1.9719
Profiling... [2304/50176]	Loss: 1.8775
Profiling... [2560/50176]	Loss: 1.9963
Profiling... [2816/50176]	Loss: 2.0256
Profiling... [3072/50176]	Loss: 1.8987
Profiling... [3328/50176]	Loss: 1.9458
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 6, Average loss: 0.0101, Accuracy: 0.3530
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.76731617293962,
                        "time": 2.4083614600021974,
                        "accuracy": 0.35302734375,
                        "total_cost": 789.7675563517786
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0896
Profiling... [512/50176]	Loss: 1.9068
Profiling... [768/50176]	Loss: 2.1102
Profiling... [1024/50176]	Loss: 2.0261
Profiling... [1280/50176]	Loss: 2.2000
Profiling... [1536/50176]	Loss: 2.0317
Profiling... [1792/50176]	Loss: 2.1690
Profiling... [2048/50176]	Loss: 2.0623
Profiling... [2304/50176]	Loss: 2.0593
Profiling... [2560/50176]	Loss: 1.9287
Profiling... [2816/50176]	Loss: 1.9852
Profiling... [3072/50176]	Loss: 1.9645
Profiling... [3328/50176]	Loss: 1.9531
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 6, Average loss: 0.0102, Accuracy: 0.3583
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.77889896075776,
                        "time": 2.5541305279984954,
                        "accuracy": 0.35830078125,
                        "total_cost": 825.3245200919436
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0968
Profiling... [512/50176]	Loss: 2.0723
Profiling... [768/50176]	Loss: 2.1972
Profiling... [1024/50176]	Loss: 2.0976
Profiling... [1280/50176]	Loss: 2.0450
Profiling... [1536/50176]	Loss: 2.1050
Profiling... [1792/50176]	Loss: 1.8655
Profiling... [2048/50176]	Loss: 1.9473
Profiling... [2304/50176]	Loss: 2.0540
Profiling... [2560/50176]	Loss: 2.1625
Profiling... [2816/50176]	Loss: 2.0051
Profiling... [3072/50176]	Loss: 1.9808
Profiling... [3328/50176]	Loss: 1.8916
Profile done
epoch 1 train time consumed: 4.40s
Validation Epoch: 6, Average loss: 0.0082, Accuracy: 0.4288
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.74332940376986,
                        "time": 3.017360837002343,
                        "accuracy": 0.42880859375,
                        "total_cost": 814.4412084493047
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0540
Profiling... [512/50176]	Loss: 2.1032
Profiling... [768/50176]	Loss: 1.9357
Profiling... [1024/50176]	Loss: 2.0373
Profiling... [1280/50176]	Loss: 1.9704
Profiling... [1536/50176]	Loss: 1.8109
Profiling... [1792/50176]	Loss: 2.1143
Profiling... [2048/50176]	Loss: 2.1505
Profiling... [2304/50176]	Loss: 1.8806
Profiling... [2560/50176]	Loss: 2.0379
Profiling... [2816/50176]	Loss: 1.9128
Profiling... [3072/50176]	Loss: 1.9623
Profiling... [3328/50176]	Loss: 2.1157
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 6, Average loss: 0.0085, Accuracy: 0.4146
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.706793366714,
                        "time": 2.3994215580023592,
                        "accuracy": 0.4146484375,
                        "total_cost": 669.5536490751106
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1319
Profiling... [512/50176]	Loss: 1.8974
Profiling... [768/50176]	Loss: 2.1657
Profiling... [1024/50176]	Loss: 1.9490
Profiling... [1280/50176]	Loss: 1.9671
Profiling... [1536/50176]	Loss: 2.1653
Profiling... [1792/50176]	Loss: 1.9948
Profiling... [2048/50176]	Loss: 1.9169
Profiling... [2304/50176]	Loss: 1.9184
Profiling... [2560/50176]	Loss: 2.0017
Profiling... [2816/50176]	Loss: 1.9405
Profiling... [3072/50176]	Loss: 1.9857
Profiling... [3328/50176]	Loss: 2.1504
Profile done
epoch 1 train time consumed: 3.81s
Validation Epoch: 6, Average loss: 0.0098, Accuracy: 0.3639
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.73361703519322,
                        "time": 2.417567306001729,
                        "accuracy": 0.3638671875,
                        "total_cost": 768.9448742877038
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8981
Profiling... [512/50176]	Loss: 2.0122
Profiling... [768/50176]	Loss: 2.1526
Profiling... [1024/50176]	Loss: 2.0668
Profiling... [1280/50176]	Loss: 2.0853
Profiling... [1536/50176]	Loss: 2.1573
Profiling... [1792/50176]	Loss: 2.0744
Profiling... [2048/50176]	Loss: 1.7365
Profiling... [2304/50176]	Loss: 1.9665
Profiling... [2560/50176]	Loss: 2.0625
Profiling... [2816/50176]	Loss: 2.0138
Profiling... [3072/50176]	Loss: 2.2307
Profiling... [3328/50176]	Loss: 1.9886
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 6, Average loss: 0.0088, Accuracy: 0.4096
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.7401119558526,
                        "time": 2.545279154997843,
                        "accuracy": 0.4095703125,
                        "total_cost": 719.268182696587
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1799
Profiling... [512/50176]	Loss: 2.1856
Profiling... [768/50176]	Loss: 2.0944
Profiling... [1024/50176]	Loss: 1.9884
Profiling... [1280/50176]	Loss: 1.9913
Profiling... [1536/50176]	Loss: 1.8086
Profiling... [1792/50176]	Loss: 2.0125
Profiling... [2048/50176]	Loss: 2.2068
Profiling... [2304/50176]	Loss: 1.9305
Profiling... [2560/50176]	Loss: 1.9777
Profiling... [2816/50176]	Loss: 1.9191
Profiling... [3072/50176]	Loss: 1.8905
Profiling... [3328/50176]	Loss: 2.2489
Profile done
epoch 1 train time consumed: 4.39s
Validation Epoch: 6, Average loss: 0.0089, Accuracy: 0.4077
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.70672309218092,
                        "time": 3.02107028800674,
                        "accuracy": 0.40771484375,
                        "total_cost": 857.3593741187186
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.2760
Profiling... [512/50176]	Loss: 1.9965
Profiling... [768/50176]	Loss: 2.0808
Profiling... [1024/50176]	Loss: 2.1605
Profiling... [1280/50176]	Loss: 2.2352
Profiling... [1536/50176]	Loss: 2.1762
Profiling... [1792/50176]	Loss: 2.2730
Profiling... [2048/50176]	Loss: 1.9363
Profiling... [2304/50176]	Loss: 1.9773
Profiling... [2560/50176]	Loss: 2.1623
Profiling... [2816/50176]	Loss: 1.9869
Profiling... [3072/50176]	Loss: 2.1374
Profiling... [3328/50176]	Loss: 2.2378
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 6, Average loss: 0.0156, Accuracy: 0.2106
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.6705083565791,
                        "time": 2.392980575998081,
                        "accuracy": 0.21064453125,
                        "total_cost": 1314.0492092083095
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0903
Profiling... [512/50176]	Loss: 2.0720
Profiling... [768/50176]	Loss: 2.2154
Profiling... [1024/50176]	Loss: 2.0730
Profiling... [1280/50176]	Loss: 2.2640
Profiling... [1536/50176]	Loss: 2.1191
Profiling... [1792/50176]	Loss: 2.0872
Profiling... [2048/50176]	Loss: 2.0831
Profiling... [2304/50176]	Loss: 2.1341
Profiling... [2560/50176]	Loss: 2.2399
Profiling... [2816/50176]	Loss: 2.1262
Profiling... [3072/50176]	Loss: 1.8940
Profiling... [3328/50176]	Loss: 2.0897
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 6, Average loss: 0.0131, Accuracy: 0.2616
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.69442685248085,
                        "time": 2.3996177989974967,
                        "accuracy": 0.26162109375,
                        "total_cost": 1061.1621637256724
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1746
Profiling... [512/50176]	Loss: 1.9972
Profiling... [768/50176]	Loss: 2.0654
Profiling... [1024/50176]	Loss: 1.9505
Profiling... [1280/50176]	Loss: 2.1651
Profiling... [1536/50176]	Loss: 2.1439
Profiling... [1792/50176]	Loss: 2.2110
Profiling... [2048/50176]	Loss: 1.8749
Profiling... [2304/50176]	Loss: 2.1941
Profiling... [2560/50176]	Loss: 2.0595
Profiling... [2816/50176]	Loss: 2.2793
Profiling... [3072/50176]	Loss: 2.0612
Profiling... [3328/50176]	Loss: 2.0826
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 6, Average loss: 0.0093, Accuracy: 0.3790
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.70940870604072,
                        "time": 2.554410579999967,
                        "accuracy": 0.37900390625,
                        "total_cost": 779.858288872849
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0701
Profiling... [512/50176]	Loss: 1.9764
Profiling... [768/50176]	Loss: 2.0642
Profiling... [1024/50176]	Loss: 2.1607
Profiling... [1280/50176]	Loss: 2.1879
Profiling... [1536/50176]	Loss: 2.1620
Profiling... [1792/50176]	Loss: 2.1218
Profiling... [2048/50176]	Loss: 2.1441
Profiling... [2304/50176]	Loss: 2.2483
Profiling... [2560/50176]	Loss: 1.9340
Profiling... [2816/50176]	Loss: 2.1762
Profiling... [3072/50176]	Loss: 2.0506
Profiling... [3328/50176]	Loss: 2.0829
Profile done
epoch 1 train time consumed: 4.47s
Validation Epoch: 6, Average loss: 0.0245, Accuracy: 0.1002
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.67481750937918,
                        "time": 3.0415256729975226,
                        "accuracy": 0.1001953125,
                        "total_cost": 3511.4210275463747
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0068
Profiling... [512/50176]	Loss: 1.9906
Profiling... [768/50176]	Loss: 1.8118
Profiling... [1024/50176]	Loss: 2.1761
Profiling... [1280/50176]	Loss: 2.2227
Profiling... [1536/50176]	Loss: 2.0470
Profiling... [1792/50176]	Loss: 1.9659
Profiling... [2048/50176]	Loss: 2.1858
Profiling... [2304/50176]	Loss: 2.1658
Profiling... [2560/50176]	Loss: 2.1939
Profiling... [2816/50176]	Loss: 1.9254
Profiling... [3072/50176]	Loss: 2.0774
Profiling... [3328/50176]	Loss: 2.0096
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 6, Average loss: 0.0129, Accuracy: 0.2940
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.64334563273465,
                        "time": 2.400145129999146,
                        "accuracy": 0.29404296875,
                        "total_cost": 943.9464375466929
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.2217
Profiling... [512/50176]	Loss: 1.9112
Profiling... [768/50176]	Loss: 1.9613
Profiling... [1024/50176]	Loss: 1.9801
Profiling... [1280/50176]	Loss: 2.1941
Profiling... [1536/50176]	Loss: 2.2595
Profiling... [1792/50176]	Loss: 2.0412
Profiling... [2048/50176]	Loss: 2.2663
Profiling... [2304/50176]	Loss: 2.1918
Profiling... [2560/50176]	Loss: 2.1575
Profiling... [2816/50176]	Loss: 2.1828
Profiling... [3072/50176]	Loss: 2.0767
Profiling... [3328/50176]	Loss: 2.0545
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 6, Average loss: 0.0211, Accuracy: 0.1464
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.66677918353813,
                        "time": 2.397360322000168,
                        "accuracy": 0.14638671875,
                        "total_cost": 1894.263013448201
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.2163
Profiling... [512/50176]	Loss: 2.2980
Profiling... [768/50176]	Loss: 2.1845
Profiling... [1024/50176]	Loss: 2.2545
Profiling... [1280/50176]	Loss: 2.2364
Profiling... [1536/50176]	Loss: 1.9856
Profiling... [1792/50176]	Loss: 2.1884
Profiling... [2048/50176]	Loss: 2.1228
Profiling... [2304/50176]	Loss: 2.2178
Profiling... [2560/50176]	Loss: 2.0279
Profiling... [2816/50176]	Loss: 1.9899
Profiling... [3072/50176]	Loss: 2.1355
Profiling... [3328/50176]	Loss: 2.0251
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 6, Average loss: 0.0191, Accuracy: 0.1988
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.67812929668757,
                        "time": 2.5576506229990628,
                        "accuracy": 0.198828125,
                        "total_cost": 1488.040182761061
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0925
Profiling... [512/50176]	Loss: 2.3648
Profiling... [768/50176]	Loss: 2.2049
Profiling... [1024/50176]	Loss: 2.0591
Profiling... [1280/50176]	Loss: 2.1035
Profiling... [1536/50176]	Loss: 2.2006
Profiling... [1792/50176]	Loss: 2.1782
Profiling... [2048/50176]	Loss: 2.2581
Profiling... [2304/50176]	Loss: 1.9896
Profiling... [2560/50176]	Loss: 2.2192
Profiling... [2816/50176]	Loss: 2.0770
Profiling... [3072/50176]	Loss: 1.8918
Profiling... [3328/50176]	Loss: 1.9128
Profile done
epoch 1 train time consumed: 4.45s
Validation Epoch: 6, Average loss: 0.0148, Accuracy: 0.2170
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.6460857929787,
                        "time": 3.0504431030058186,
                        "accuracy": 0.2169921875,
                        "total_cost": 1625.7350500087064
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0420
Profiling... [512/50176]	Loss: 2.2719
Profiling... [768/50176]	Loss: 1.9728
Profiling... [1024/50176]	Loss: 2.3056
Profiling... [1280/50176]	Loss: 2.2055
Profiling... [1536/50176]	Loss: 2.0604
Profiling... [1792/50176]	Loss: 2.2904
Profiling... [2048/50176]	Loss: 2.1428
Profiling... [2304/50176]	Loss: 2.0519
Profiling... [2560/50176]	Loss: 2.1587
Profiling... [2816/50176]	Loss: 2.1712
Profiling... [3072/50176]	Loss: 1.9010
Profiling... [3328/50176]	Loss: 2.1153
Profile done
epoch 1 train time consumed: 3.71s
Validation Epoch: 6, Average loss: 0.0123, Accuracy: 0.2708
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.61361536903122,
                        "time": 2.428476197004784,
                        "accuracy": 0.27080078125,
                        "total_cost": 1036.7950626928214
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.2764
Profiling... [512/50176]	Loss: 1.9807
Profiling... [768/50176]	Loss: 2.1486
Profiling... [1024/50176]	Loss: 2.1283
Profiling... [1280/50176]	Loss: 2.1374
Profiling... [1536/50176]	Loss: 2.3324
Profiling... [1792/50176]	Loss: 2.3101
Profiling... [2048/50176]	Loss: 2.2877
Profiling... [2304/50176]	Loss: 2.0213
Profiling... [2560/50176]	Loss: 2.0842
Profiling... [2816/50176]	Loss: 2.1101
Profiling... [3072/50176]	Loss: 2.0875
Profiling... [3328/50176]	Loss: 2.0489
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 6, Average loss: 0.0170, Accuracy: 0.1805
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.63938033619958,
                        "time": 2.3953976510019857,
                        "accuracy": 0.18046875,
                        "total_cost": 1534.9045195949861
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9670
Profiling... [512/50176]	Loss: 1.9669
Profiling... [768/50176]	Loss: 2.1540
Profiling... [1024/50176]	Loss: 2.1599
Profiling... [1280/50176]	Loss: 2.0507
Profiling... [1536/50176]	Loss: 2.0049
Profiling... [1792/50176]	Loss: 2.0710
Profiling... [2048/50176]	Loss: 1.9664
Profiling... [2304/50176]	Loss: 2.1583
Profiling... [2560/50176]	Loss: 2.3134
Profiling... [2816/50176]	Loss: 2.0982
Profiling... [3072/50176]	Loss: 2.0732
Profiling... [3328/50176]	Loss: 1.9517
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 6, Average loss: 0.0164, Accuracy: 0.1934
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.64986250810085,
                        "time": 2.590559772004781,
                        "accuracy": 0.193359375,
                        "total_cost": 1549.4355081121357
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.2592
Profiling... [512/50176]	Loss: 2.0468
Profiling... [768/50176]	Loss: 2.2318
Profiling... [1024/50176]	Loss: 2.0462
Profiling... [1280/50176]	Loss: 2.0297
Profiling... [1536/50176]	Loss: 2.1208
Profiling... [1792/50176]	Loss: 2.2851
Profiling... [2048/50176]	Loss: 2.0001
Profiling... [2304/50176]	Loss: 2.0224
Profiling... [2560/50176]	Loss: 2.2381
Profiling... [2816/50176]	Loss: 2.1357
Profiling... [3072/50176]	Loss: 2.1279
Profiling... [3328/50176]	Loss: 2.0740
Profile done
epoch 1 train time consumed: 4.35s
Validation Epoch: 6, Average loss: 0.0235, Accuracy: 0.1291
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.61560843749616,
                        "time": 3.0336213339978713,
                        "accuracy": 0.1291015625,
                        "total_cost": 2716.7291356301957
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1025
Profiling... [1024/50176]	Loss: 2.0584
Profiling... [1536/50176]	Loss: 2.0817
Profiling... [2048/50176]	Loss: 2.0833
Profiling... [2560/50176]	Loss: 1.9324
Profiling... [3072/50176]	Loss: 1.9374
Profiling... [3584/50176]	Loss: 2.1775
Profiling... [4096/50176]	Loss: 2.0761
Profiling... [4608/50176]	Loss: 1.9781
Profiling... [5120/50176]	Loss: 2.0257
Profiling... [5632/50176]	Loss: 1.9860
Profiling... [6144/50176]	Loss: 2.0172
Profiling... [6656/50176]	Loss: 1.8763
Profile done
epoch 1 train time consumed: 6.65s
Validation Epoch: 6, Average loss: 0.0040, Accuracy: 0.4499
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.61650952197775,
                        "time": 4.528651503998844,
                        "accuracy": 0.44990234375,
                        "total_cost": 1163.7789556054101
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0496
Profiling... [1024/50176]	Loss: 2.0600
Profiling... [1536/50176]	Loss: 2.0714
Profiling... [2048/50176]	Loss: 2.0798
Profiling... [2560/50176]	Loss: 2.1088
Profiling... [3072/50176]	Loss: 2.0463
Profiling... [3584/50176]	Loss: 2.1210
Profiling... [4096/50176]	Loss: 1.9523
Profiling... [4608/50176]	Loss: 2.0277
Profiling... [5120/50176]	Loss: 1.8127
Profiling... [5632/50176]	Loss: 1.9855
Profiling... [6144/50176]	Loss: 2.0360
Profiling... [6656/50176]	Loss: 1.8995
Profile done
epoch 1 train time consumed: 6.69s
Validation Epoch: 6, Average loss: 0.0039, Accuracy: 0.4523
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.66713161038543,
                        "time": 4.531989663002605,
                        "accuracy": 0.45234375,
                        "total_cost": 1158.8581577780806
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.2581
Profiling... [1024/50176]	Loss: 2.1617
Profiling... [1536/50176]	Loss: 1.9481
Profiling... [2048/50176]	Loss: 2.0710
Profiling... [2560/50176]	Loss: 2.0992
Profiling... [3072/50176]	Loss: 2.0674
Profiling... [3584/50176]	Loss: 1.9668
Profiling... [4096/50176]	Loss: 1.8911
Profiling... [4608/50176]	Loss: 2.0137
Profiling... [5120/50176]	Loss: 1.9993
Profiling... [5632/50176]	Loss: 2.0874
Profiling... [6144/50176]	Loss: 1.9822
Profiling... [6656/50176]	Loss: 1.9004
Profile done
epoch 1 train time consumed: 7.03s
Validation Epoch: 6, Average loss: 0.0040, Accuracy: 0.4462
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.67888357722104,
                        "time": 4.864523094998731,
                        "accuracy": 0.44619140625,
                        "total_cost": 1261.1686215439324
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1300
Profiling... [1024/50176]	Loss: 2.1705
Profiling... [1536/50176]	Loss: 2.0832
Profiling... [2048/50176]	Loss: 2.0090
Profiling... [2560/50176]	Loss: 2.0348
Profiling... [3072/50176]	Loss: 1.9006
Profiling... [3584/50176]	Loss: 2.0536
Profiling... [4096/50176]	Loss: 2.0122
Profiling... [4608/50176]	Loss: 1.9893
Profiling... [5120/50176]	Loss: 2.0155
Profiling... [5632/50176]	Loss: 2.1161
Profiling... [6144/50176]	Loss: 1.9077
Profiling... [6656/50176]	Loss: 1.9319
Profile done
epoch 1 train time consumed: 8.54s
Validation Epoch: 6, Average loss: 0.0040, Accuracy: 0.4488
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.61806884497021,
                        "time": 6.056088625999109,
                        "accuracy": 0.448828125,
                        "total_cost": 1560.0476723512058
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1740
Profiling... [1024/50176]	Loss: 2.0658
Profiling... [1536/50176]	Loss: 2.0068
Profiling... [2048/50176]	Loss: 2.0535
Profiling... [2560/50176]	Loss: 2.1816
Profiling... [3072/50176]	Loss: 1.9484
Profiling... [3584/50176]	Loss: 1.9285
Profiling... [4096/50176]	Loss: 1.8958
Profiling... [4608/50176]	Loss: 2.0778
Profiling... [5120/50176]	Loss: 1.8712
Profiling... [5632/50176]	Loss: 1.9641
Profiling... [6144/50176]	Loss: 1.9950
Profiling... [6656/50176]	Loss: 2.0035
Profile done
epoch 1 train time consumed: 6.70s
Validation Epoch: 6, Average loss: 0.0039, Accuracy: 0.4549
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.6127555484089,
                        "time": 4.522635592998995,
                        "accuracy": 0.4548828125,
                        "total_cost": 1149.4704765261406
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9907
Profiling... [1024/50176]	Loss: 1.9933
Profiling... [1536/50176]	Loss: 2.2384
Profiling... [2048/50176]	Loss: 2.1506
Profiling... [2560/50176]	Loss: 2.0061
Profiling... [3072/50176]	Loss: 1.9570
Profiling... [3584/50176]	Loss: 2.0960
Profiling... [4096/50176]	Loss: 1.9855
Profiling... [4608/50176]	Loss: 1.8926
Profiling... [5120/50176]	Loss: 1.9633
Profiling... [5632/50176]	Loss: 1.8898
Profiling... [6144/50176]	Loss: 1.8798
Profiling... [6656/50176]	Loss: 2.0567
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 6, Average loss: 0.0040, Accuracy: 0.4525
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.6610288427969,
                        "time": 4.644702509001945,
                        "accuracy": 0.4525390625,
                        "total_cost": 1187.104308503501
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0899
Profiling... [1024/50176]	Loss: 2.0544
Profiling... [1536/50176]	Loss: 2.1090
Profiling... [2048/50176]	Loss: 2.0110
Profiling... [2560/50176]	Loss: 2.1979
Profiling... [3072/50176]	Loss: 2.0144
Profiling... [3584/50176]	Loss: 2.0933
Profiling... [4096/50176]	Loss: 1.8943
Profiling... [4608/50176]	Loss: 1.9052
Profiling... [5120/50176]	Loss: 2.0250
Profiling... [5632/50176]	Loss: 1.9711
Profiling... [6144/50176]	Loss: 1.8875
Profiling... [6656/50176]	Loss: 2.0050
Profile done
epoch 1 train time consumed: 7.08s
Validation Epoch: 6, Average loss: 0.0040, Accuracy: 0.4465
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.67695869132343,
                        "time": 4.892453824999393,
                        "accuracy": 0.446484375,
                        "total_cost": 1267.55651642605
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1400
Profiling... [1024/50176]	Loss: 2.1135
Profiling... [1536/50176]	Loss: 2.0675
Profiling... [2048/50176]	Loss: 2.0187
Profiling... [2560/50176]	Loss: 2.0976
Profiling... [3072/50176]	Loss: 2.0249
Profiling... [3584/50176]	Loss: 1.9854
Profiling... [4096/50176]	Loss: 1.9514
Profiling... [4608/50176]	Loss: 2.0284
Profiling... [5120/50176]	Loss: 1.9321
Profiling... [5632/50176]	Loss: 1.8826
Profiling... [6144/50176]	Loss: 2.0404
Profiling... [6656/50176]	Loss: 1.8961
Profile done
epoch 1 train time consumed: 8.44s
Validation Epoch: 6, Average loss: 0.0039, Accuracy: 0.4550
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.61170620240995,
                        "time": 6.005530803995498,
                        "accuracy": 0.45498046875,
                        "total_cost": 1526.0208087801577
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9555
Profiling... [1024/50176]	Loss: 2.0518
Profiling... [1536/50176]	Loss: 2.0437
Profiling... [2048/50176]	Loss: 1.8717
Profiling... [2560/50176]	Loss: 2.0646
Profiling... [3072/50176]	Loss: 2.1102
Profiling... [3584/50176]	Loss: 1.9695
Profiling... [4096/50176]	Loss: 1.9369
Profiling... [4608/50176]	Loss: 1.9320
Profiling... [5120/50176]	Loss: 2.0461
Profiling... [5632/50176]	Loss: 1.9659
Profiling... [6144/50176]	Loss: 1.9631
Profiling... [6656/50176]	Loss: 1.9584
Profile done
epoch 1 train time consumed: 6.71s
Validation Epoch: 6, Average loss: 0.0040, Accuracy: 0.4448
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.59950817126298,
                        "time": 4.6212577799960854,
                        "accuracy": 0.44482421875,
                        "total_cost": 1200.9578255459367
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0957
Profiling... [1024/50176]	Loss: 1.9851
Profiling... [1536/50176]	Loss: 1.9509
Profiling... [2048/50176]	Loss: 2.0801
Profiling... [2560/50176]	Loss: 1.9408
Profiling... [3072/50176]	Loss: 1.9371
Profiling... [3584/50176]	Loss: 2.0058
Profiling... [4096/50176]	Loss: 2.1248
Profiling... [4608/50176]	Loss: 1.9576
Profiling... [5120/50176]	Loss: 2.0780
Profiling... [5632/50176]	Loss: 1.7841
Profiling... [6144/50176]	Loss: 2.0547
Profiling... [6656/50176]	Loss: 1.9249
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 6, Average loss: 0.0041, Accuracy: 0.4379
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.64661168697957,
                        "time": 4.653442041002563,
                        "accuracy": 0.437890625,
                        "total_cost": 1228.970829699058
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1605
Profiling... [1024/50176]	Loss: 1.9745
Profiling... [1536/50176]	Loss: 2.0253
Profiling... [2048/50176]	Loss: 1.9503
Profiling... [2560/50176]	Loss: 1.9969
Profiling... [3072/50176]	Loss: 2.0976
Profiling... [3584/50176]	Loss: 1.9860
Profiling... [4096/50176]	Loss: 1.9547
Profiling... [4608/50176]	Loss: 1.8741
Profiling... [5120/50176]	Loss: 1.9444
Profiling... [5632/50176]	Loss: 1.9866
Profiling... [6144/50176]	Loss: 1.9245
Profiling... [6656/50176]	Loss: 1.9117
Profile done
epoch 1 train time consumed: 7.10s
Validation Epoch: 6, Average loss: 0.0040, Accuracy: 0.4433
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.66191302095274,
                        "time": 4.873046230000909,
                        "accuracy": 0.44326171875,
                        "total_cost": 1271.5419025826868
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0495
Profiling... [1024/50176]	Loss: 2.1847
Profiling... [1536/50176]	Loss: 2.0983
Profiling... [2048/50176]	Loss: 1.9775
Profiling... [2560/50176]	Loss: 2.0189
Profiling... [3072/50176]	Loss: 2.0425
Profiling... [3584/50176]	Loss: 2.0255
Profiling... [4096/50176]	Loss: 2.0177
Profiling... [4608/50176]	Loss: 2.0108
Profiling... [5120/50176]	Loss: 2.0063
Profiling... [5632/50176]	Loss: 2.0611
Profiling... [6144/50176]	Loss: 2.0262
Profiling... [6656/50176]	Loss: 2.0432
Profile done
epoch 1 train time consumed: 8.55s
Validation Epoch: 6, Average loss: 0.0040, Accuracy: 0.4471
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.5966185736648,
                        "time": 6.012877170003776,
                        "accuracy": 0.4470703125,
                        "total_cost": 1554.7180148563846
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1296
Profiling... [1024/50176]	Loss: 2.0039
Profiling... [1536/50176]	Loss: 1.9807
Profiling... [2048/50176]	Loss: 1.9076
Profiling... [2560/50176]	Loss: 1.9172
Profiling... [3072/50176]	Loss: 1.9750
Profiling... [3584/50176]	Loss: 2.0215
Profiling... [4096/50176]	Loss: 1.9319
Profiling... [4608/50176]	Loss: 1.9367
Profiling... [5120/50176]	Loss: 1.8205
Profiling... [5632/50176]	Loss: 1.9790
Profiling... [6144/50176]	Loss: 1.9603
Profiling... [6656/50176]	Loss: 1.8151
Profile done
epoch 1 train time consumed: 6.82s
Validation Epoch: 6, Average loss: 0.0043, Accuracy: 0.4187
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.59165259135762,
                        "time": 4.618259934999514,
                        "accuracy": 0.41865234375,
                        "total_cost": 1275.1207677505083
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.1891
Profiling... [1024/50176]	Loss: 1.9643
Profiling... [1536/50176]	Loss: 1.8355
Profiling... [2048/50176]	Loss: 2.0222
Profiling... [2560/50176]	Loss: 2.0979
Profiling... [3072/50176]	Loss: 1.9934
Profiling... [3584/50176]	Loss: 1.8923
Profiling... [4096/50176]	Loss: 2.0829
Profiling... [4608/50176]	Loss: 1.9856
Profiling... [5120/50176]	Loss: 1.9881
Profiling... [5632/50176]	Loss: 1.9697
Profiling... [6144/50176]	Loss: 1.8799
Profiling... [6656/50176]	Loss: 1.9334
Profile done
epoch 1 train time consumed: 6.74s
Validation Epoch: 6, Average loss: 0.0049, Accuracy: 0.3735
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.64234324156388,
                        "time": 4.645530306996079,
                        "accuracy": 0.37353515625,
                        "total_cost": 1438.2046811710998
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0850
Profiling... [1024/50176]	Loss: 2.2177
Profiling... [1536/50176]	Loss: 1.9240
Profiling... [2048/50176]	Loss: 2.0231
Profiling... [2560/50176]	Loss: 2.0806
Profiling... [3072/50176]	Loss: 2.0881
Profiling... [3584/50176]	Loss: 2.0479
Profiling... [4096/50176]	Loss: 1.9842
Profiling... [4608/50176]	Loss: 1.9731
Profiling... [5120/50176]	Loss: 1.9006
Profiling... [5632/50176]	Loss: 1.8846
Profiling... [6144/50176]	Loss: 1.8631
Profiling... [6656/50176]	Loss: 2.0013
Profile done
epoch 1 train time consumed: 7.16s
Validation Epoch: 6, Average loss: 0.0045, Accuracy: 0.3993
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.6551622583421,
                        "time": 4.884528227994451,
                        "accuracy": 0.39931640625,
                        "total_cost": 1414.719996279017
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9072
Profiling... [1024/50176]	Loss: 2.0730
Profiling... [1536/50176]	Loss: 1.9635
Profiling... [2048/50176]	Loss: 1.9209
Profiling... [2560/50176]	Loss: 2.0538
Profiling... [3072/50176]	Loss: 1.9445
Profiling... [3584/50176]	Loss: 1.9718
Profiling... [4096/50176]	Loss: 1.9408
Profiling... [4608/50176]	Loss: 1.9419
Profiling... [5120/50176]	Loss: 1.8479
Profiling... [5632/50176]	Loss: 1.9943
Profiling... [6144/50176]	Loss: 1.8494
Profiling... [6656/50176]	Loss: 1.9060
Profile done
epoch 1 train time consumed: 8.55s
Validation Epoch: 6, Average loss: 0.0051, Accuracy: 0.3654
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.59131870644943,
                        "time": 6.03446892699867,
                        "accuracy": 0.3654296875,
                        "total_cost": 1908.8000915767668
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.2320
Profiling... [1024/50176]	Loss: 1.9443
Profiling... [1536/50176]	Loss: 1.8823
Profiling... [2048/50176]	Loss: 1.9832
Profiling... [2560/50176]	Loss: 1.9043
Profiling... [3072/50176]	Loss: 1.8953
Profiling... [3584/50176]	Loss: 1.9261
Profiling... [4096/50176]	Loss: 1.8802
Profiling... [4608/50176]	Loss: 1.8640
Profiling... [5120/50176]	Loss: 2.0552
Profiling... [5632/50176]	Loss: 1.9204
Profiling... [6144/50176]	Loss: 1.9819
Profiling... [6656/50176]	Loss: 1.9338
Profile done
epoch 1 train time consumed: 6.68s
Validation Epoch: 6, Average loss: 0.0053, Accuracy: 0.3641
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.58389612870499,
                        "time": 4.624914459003776,
                        "accuracy": 0.3640625,
                        "total_cost": 1468.334784367075
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0699
Profiling... [1024/50176]	Loss: 2.1706
Profiling... [1536/50176]	Loss: 2.0256
Profiling... [2048/50176]	Loss: 1.9823
Profiling... [2560/50176]	Loss: 1.9057
Profiling... [3072/50176]	Loss: 2.0673
Profiling... [3584/50176]	Loss: 2.0254
Profiling... [4096/50176]	Loss: 1.9235
Profiling... [4608/50176]	Loss: 2.0532
Profiling... [5120/50176]	Loss: 1.9804
Profiling... [5632/50176]	Loss: 1.9379
Profiling... [6144/50176]	Loss: 1.9068
Profiling... [6656/50176]	Loss: 1.8189
Profile done
epoch 1 train time consumed: 6.72s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4236
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.63383277171569,
                        "time": 4.532310595001036,
                        "accuracy": 0.4236328125,
                        "total_cost": 1237.1290182150963
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9676
Profiling... [1024/50176]	Loss: 2.2031
Profiling... [1536/50176]	Loss: 2.0318
Profiling... [2048/50176]	Loss: 2.0575
Profiling... [2560/50176]	Loss: 1.9805
Profiling... [3072/50176]	Loss: 1.9188
Profiling... [3584/50176]	Loss: 1.9159
Profiling... [4096/50176]	Loss: 1.9588
Profiling... [4608/50176]	Loss: 1.9954
Profiling... [5120/50176]	Loss: 2.0109
Profiling... [5632/50176]	Loss: 1.9964
Profiling... [6144/50176]	Loss: 1.8711
Profiling... [6656/50176]	Loss: 1.8552
Profile done
epoch 1 train time consumed: 7.17s
Validation Epoch: 6, Average loss: 0.0046, Accuracy: 0.3894
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.64529113399922,
                        "time": 4.86890132100234,
                        "accuracy": 0.38935546875,
                        "total_cost": 1446.1476875558308
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0852
Profiling... [1024/50176]	Loss: 1.9131
Profiling... [1536/50176]	Loss: 2.0077
Profiling... [2048/50176]	Loss: 2.0238
Profiling... [2560/50176]	Loss: 2.0284
Profiling... [3072/50176]	Loss: 1.9924
Profiling... [3584/50176]	Loss: 1.9489
Profiling... [4096/50176]	Loss: 1.9881
Profiling... [4608/50176]	Loss: 1.9477
Profiling... [5120/50176]	Loss: 2.0631
Profiling... [5632/50176]	Loss: 1.9427
Profiling... [6144/50176]	Loss: 1.8248
Profiling... [6656/50176]	Loss: 2.0284
Profile done
epoch 1 train time consumed: 8.44s
Validation Epoch: 6, Average loss: 0.0042, Accuracy: 0.4279
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.58355886157828,
                        "time": 6.017827482995926,
                        "accuracy": 0.4279296875,
                        "total_cost": 1625.411691259871
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0247
Profiling... [1024/50176]	Loss: 1.9945
Profiling... [1536/50176]	Loss: 1.9821
Profiling... [2048/50176]	Loss: 1.8890
Profiling... [2560/50176]	Loss: 1.9215
Profiling... [3072/50176]	Loss: 2.0419
Profiling... [3584/50176]	Loss: 2.0278
Profiling... [4096/50176]	Loss: 1.9873
Profiling... [4608/50176]	Loss: 1.8608
Profiling... [5120/50176]	Loss: 1.9319
Profiling... [5632/50176]	Loss: 1.9612
Profiling... [6144/50176]	Loss: 2.0262
Profiling... [6656/50176]	Loss: 1.9970
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 6, Average loss: 0.0047, Accuracy: 0.3844
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.57609201173591,
                        "time": 4.630380113005231,
                        "accuracy": 0.384375,
                        "total_cost": 1392.2893996487926
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0161
Profiling... [1024/50176]	Loss: 2.1016
Profiling... [1536/50176]	Loss: 1.8836
Profiling... [2048/50176]	Loss: 1.7937
Profiling... [2560/50176]	Loss: 2.1016
Profiling... [3072/50176]	Loss: 1.9607
Profiling... [3584/50176]	Loss: 1.9674
Profiling... [4096/50176]	Loss: 1.8887
Profiling... [4608/50176]	Loss: 2.0338
Profiling... [5120/50176]	Loss: 1.9812
Profiling... [5632/50176]	Loss: 1.9473
Profiling... [6144/50176]	Loss: 1.9415
Profiling... [6656/50176]	Loss: 2.0267
Profile done
epoch 1 train time consumed: 6.77s
Validation Epoch: 6, Average loss: 0.0043, Accuracy: 0.4313
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.62617547111063,
                        "time": 4.5440536639944185,
                        "accuracy": 0.43134765625,
                        "total_cost": 1218.0697835961912
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9820
Profiling... [1024/50176]	Loss: 1.9866
Profiling... [1536/50176]	Loss: 1.9470
Profiling... [2048/50176]	Loss: 1.8729
Profiling... [2560/50176]	Loss: 1.9860
Profiling... [3072/50176]	Loss: 2.0246
Profiling... [3584/50176]	Loss: 2.0278
Profiling... [4096/50176]	Loss: 2.0703
Profiling... [4608/50176]	Loss: 1.9239
Profiling... [5120/50176]	Loss: 2.0003
Profiling... [5632/50176]	Loss: 2.0309
Profiling... [6144/50176]	Loss: 2.0279
Profiling... [6656/50176]	Loss: 1.9468
Profile done
epoch 1 train time consumed: 7.01s
Validation Epoch: 6, Average loss: 0.0041, Accuracy: 0.4352
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.64077122025157,
                        "time": 4.870693723998556,
                        "accuracy": 0.43515625,
                        "total_cost": 1294.3644463818046
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9897
Profiling... [1024/50176]	Loss: 2.0815
Profiling... [1536/50176]	Loss: 2.0345
Profiling... [2048/50176]	Loss: 1.9615
Profiling... [2560/50176]	Loss: 1.8720
Profiling... [3072/50176]	Loss: 2.0007
Profiling... [3584/50176]	Loss: 2.0779
Profiling... [4096/50176]	Loss: 1.9852
Profiling... [4608/50176]	Loss: 1.8142
Profiling... [5120/50176]	Loss: 1.9655
Profiling... [5632/50176]	Loss: 1.8662
Profiling... [6144/50176]	Loss: 1.8582
Profiling... [6656/50176]	Loss: 2.0278
Profile done
epoch 1 train time consumed: 8.30s
Validation Epoch: 6, Average loss: 0.0047, Accuracy: 0.3876
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.58123917787057,
                        "time": 5.929740125000535,
                        "accuracy": 0.38759765625,
                        "total_cost": 1768.2426624588315
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1551
Profiling... [1024/50176]	Loss: 1.9844
Profiling... [1536/50176]	Loss: 2.1412
Profiling... [2048/50176]	Loss: 2.0885
Profiling... [2560/50176]	Loss: 2.0861
Profiling... [3072/50176]	Loss: 2.0261
Profiling... [3584/50176]	Loss: 2.0850
Profiling... [4096/50176]	Loss: 1.9576
Profiling... [4608/50176]	Loss: 2.0495
Profiling... [5120/50176]	Loss: 2.1463
Profiling... [5632/50176]	Loss: 1.9938
Profiling... [6144/50176]	Loss: 2.0476
Profiling... [6656/50176]	Loss: 2.1184
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 6, Average loss: 0.0065, Accuracy: 0.2561
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.57559908709851,
                        "time": 4.525723268998263,
                        "accuracy": 0.2560546875,
                        "total_cost": 2042.7791548119817
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.2391
Profiling... [1024/50176]	Loss: 1.9164
Profiling... [1536/50176]	Loss: 1.9246
Profiling... [2048/50176]	Loss: 2.0861
Profiling... [2560/50176]	Loss: 2.1322
Profiling... [3072/50176]	Loss: 2.1037
Profiling... [3584/50176]	Loss: 2.1487
Profiling... [4096/50176]	Loss: 1.9784
Profiling... [4608/50176]	Loss: 2.0099
Profiling... [5120/50176]	Loss: 2.1604
Profiling... [5632/50176]	Loss: 2.0426
Profiling... [6144/50176]	Loss: 2.0689
Profiling... [6656/50176]	Loss: 2.0149
Profile done
epoch 1 train time consumed: 6.88s
Validation Epoch: 6, Average loss: 0.0088, Accuracy: 0.1481
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.62092960959515,
                        "time": 4.723824123000668,
                        "accuracy": 0.14814453125,
                        "total_cost": 3686.757329515448
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.2003
Profiling... [1024/50176]	Loss: 2.0044
Profiling... [1536/50176]	Loss: 1.9272
Profiling... [2048/50176]	Loss: 1.9476
Profiling... [2560/50176]	Loss: 1.9218
Profiling... [3072/50176]	Loss: 2.0123
Profiling... [3584/50176]	Loss: 1.9615
Profiling... [4096/50176]	Loss: 1.9769
Profiling... [4608/50176]	Loss: 1.9460
Profiling... [5120/50176]	Loss: 1.8239
Profiling... [5632/50176]	Loss: 1.8758
Profiling... [6144/50176]	Loss: 2.0225
Profiling... [6656/50176]	Loss: 2.0073
Profile done
epoch 1 train time consumed: 7.13s
Validation Epoch: 6, Average loss: 0.0057, Accuracy: 0.3343
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.63504557818192,
                        "time": 4.890067979002197,
                        "accuracy": 0.33427734375,
                        "total_cost": 1691.5990395544925
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1911
Profiling... [1024/50176]	Loss: 1.9775
Profiling... [1536/50176]	Loss: 1.8909
Profiling... [2048/50176]	Loss: 2.0913
Profiling... [2560/50176]	Loss: 2.1213
Profiling... [3072/50176]	Loss: 1.9489
Profiling... [3584/50176]	Loss: 2.1401
Profiling... [4096/50176]	Loss: 2.1036
Profiling... [4608/50176]	Loss: 2.0162
Profiling... [5120/50176]	Loss: 2.1719
Profiling... [5632/50176]	Loss: 2.0338
Profiling... [6144/50176]	Loss: 1.9789
Profiling... [6656/50176]	Loss: 2.0766
Profile done
epoch 1 train time consumed: 8.60s
Validation Epoch: 6, Average loss: 0.0059, Accuracy: 0.2986
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.57448601427984,
                        "time": 6.046117919999233,
                        "accuracy": 0.2986328125,
                        "total_cost": 2339.9202690951383
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1422
Profiling... [1024/50176]	Loss: 2.1143
Profiling... [1536/50176]	Loss: 2.0651
Profiling... [2048/50176]	Loss: 2.0362
Profiling... [2560/50176]	Loss: 2.0733
Profiling... [3072/50176]	Loss: 1.9406
Profiling... [3584/50176]	Loss: 1.8136
Profiling... [4096/50176]	Loss: 2.0607
Profiling... [4608/50176]	Loss: 2.1004
Profiling... [5120/50176]	Loss: 1.8604
Profiling... [5632/50176]	Loss: 2.0328
Profiling... [6144/50176]	Loss: 2.0116
Profiling... [6656/50176]	Loss: 1.9537
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 6, Average loss: 0.0057, Accuracy: 0.3130
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.56644791890179,
                        "time": 4.589007121001487,
                        "accuracy": 0.31298828125,
                        "total_cost": 1694.4252683539976
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.3093
Profiling... [1024/50176]	Loss: 2.0325
Profiling... [1536/50176]	Loss: 1.9372
Profiling... [2048/50176]	Loss: 1.9738
Profiling... [2560/50176]	Loss: 2.0446
Profiling... [3072/50176]	Loss: 2.0667
Profiling... [3584/50176]	Loss: 2.0553
Profiling... [4096/50176]	Loss: 1.9550
Profiling... [4608/50176]	Loss: 2.0732
Profiling... [5120/50176]	Loss: 2.0683
Profiling... [5632/50176]	Loss: 2.0366
Profiling... [6144/50176]	Loss: 1.9161
Profiling... [6656/50176]	Loss: 1.9998
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 6, Average loss: 0.0064, Accuracy: 0.2939
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.60959632811213,
                        "time": 4.622406656002568,
                        "accuracy": 0.2939453125,
                        "total_cost": 1818.0067680611014
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1036
Profiling... [1024/50176]	Loss: 2.1013
Profiling... [1536/50176]	Loss: 2.1652
Profiling... [2048/50176]	Loss: 2.1212
Profiling... [2560/50176]	Loss: 2.2342
Profiling... [3072/50176]	Loss: 2.0095
Profiling... [3584/50176]	Loss: 2.0696
Profiling... [4096/50176]	Loss: 2.1732
Profiling... [4608/50176]	Loss: 1.9452
Profiling... [5120/50176]	Loss: 2.1229
Profiling... [5632/50176]	Loss: 1.9997
Profiling... [6144/50176]	Loss: 2.1857
Profiling... [6656/50176]	Loss: 2.0250
Profile done
epoch 1 train time consumed: 7.19s
Validation Epoch: 6, Average loss: 0.0062, Accuracy: 0.2973
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.62319151641653,
                        "time": 4.920994268999493,
                        "accuracy": 0.297265625,
                        "total_cost": 1914.049304610032
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0897
Profiling... [1024/50176]	Loss: 1.9505
Profiling... [1536/50176]	Loss: 2.0368
Profiling... [2048/50176]	Loss: 2.0746
Profiling... [2560/50176]	Loss: 2.0423
Profiling... [3072/50176]	Loss: 2.0211
Profiling... [3584/50176]	Loss: 2.0101
Profiling... [4096/50176]	Loss: 2.0603
Profiling... [4608/50176]	Loss: 1.9667
Profiling... [5120/50176]	Loss: 2.1385
Profiling... [5632/50176]	Loss: 2.1175
Profiling... [6144/50176]	Loss: 2.0418
Profiling... [6656/50176]	Loss: 2.0020
Profile done
epoch 1 train time consumed: 8.54s
Validation Epoch: 6, Average loss: 0.0065, Accuracy: 0.2687
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.5645972996743,
                        "time": 6.017958832999284,
                        "accuracy": 0.26865234375,
                        "total_cost": 2588.709926717623
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0283
Profiling... [1024/50176]	Loss: 2.2100
Profiling... [1536/50176]	Loss: 1.9540
Profiling... [2048/50176]	Loss: 2.0114
Profiling... [2560/50176]	Loss: 2.2274
Profiling... [3072/50176]	Loss: 2.0342
Profiling... [3584/50176]	Loss: 2.1772
Profiling... [4096/50176]	Loss: 2.0310
Profiling... [4608/50176]	Loss: 1.8825
Profiling... [5120/50176]	Loss: 1.9791
Profiling... [5632/50176]	Loss: 2.0601
Profiling... [6144/50176]	Loss: 2.0502
Profiling... [6656/50176]	Loss: 1.9482
Profile done
epoch 1 train time consumed: 6.62s
Validation Epoch: 6, Average loss: 0.0099, Accuracy: 0.1627
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.55876889693336,
                        "time": 4.5323048520003795,
                        "accuracy": 0.1626953125,
                        "total_cost": 3219.192740803529
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.1576
Profiling... [1024/50176]	Loss: 2.0964
Profiling... [1536/50176]	Loss: 2.0324
Profiling... [2048/50176]	Loss: 2.1294
Profiling... [2560/50176]	Loss: 2.0422
Profiling... [3072/50176]	Loss: 2.0040
Profiling... [3584/50176]	Loss: 2.1432
Profiling... [4096/50176]	Loss: 2.0008
Profiling... [4608/50176]	Loss: 2.0298
Profiling... [5120/50176]	Loss: 1.9864
Profiling... [5632/50176]	Loss: 1.9090
Profiling... [6144/50176]	Loss: 2.0486
Profiling... [6656/50176]	Loss: 2.0696
Profile done
epoch 1 train time consumed: 6.79s
Validation Epoch: 6, Average loss: 0.0067, Accuracy: 0.2555
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.60649250962643,
                        "time": 4.551249634001579,
                        "accuracy": 0.25546875,
                        "total_cost": 2059.5630844189104
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0140
Profiling... [1024/50176]	Loss: 2.0116
Profiling... [1536/50176]	Loss: 1.9949
Profiling... [2048/50176]	Loss: 2.0542
Profiling... [2560/50176]	Loss: 2.1790
Profiling... [3072/50176]	Loss: 2.1492
Profiling... [3584/50176]	Loss: 1.9765
Profiling... [4096/50176]	Loss: 2.1188
Profiling... [4608/50176]	Loss: 2.1595
Profiling... [5120/50176]	Loss: 2.0386
Profiling... [5632/50176]	Loss: 1.9913
Profiling... [6144/50176]	Loss: 2.0688
Profiling... [6656/50176]	Loss: 1.9699
Profile done
epoch 1 train time consumed: 7.13s
Validation Epoch: 6, Average loss: 0.0051, Accuracy: 0.3471
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.6226380902469,
                        "time": 4.866115800999978,
                        "accuracy": 0.3470703125,
                        "total_cost": 1621.0926890044864
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0180
Profiling... [1024/50176]	Loss: 2.0567
Profiling... [1536/50176]	Loss: 2.0513
Profiling... [2048/50176]	Loss: 2.0381
Profiling... [2560/50176]	Loss: 2.0771
Profiling... [3072/50176]	Loss: 2.1022
Profiling... [3584/50176]	Loss: 2.0625
Profiling... [4096/50176]	Loss: 1.9144
Profiling... [4608/50176]	Loss: 1.9743
Profiling... [5120/50176]	Loss: 2.1607
Profiling... [5632/50176]	Loss: 1.9571
Profiling... [6144/50176]	Loss: 2.0383
Profiling... [6656/50176]	Loss: 1.9463
Profile done
epoch 1 train time consumed: 8.55s
Validation Epoch: 6, Average loss: 0.0075, Accuracy: 0.2244
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.56374533828561,
                        "time": 6.005797193000035,
                        "accuracy": 0.2244140625,
                        "total_cost": 3092.73139852921
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1562
Profiling... [2048/50176]	Loss: 2.1073
Profiling... [3072/50176]	Loss: 2.1035
Profiling... [4096/50176]	Loss: 2.0587
Profiling... [5120/50176]	Loss: 2.0631
Profiling... [6144/50176]	Loss: 2.0196
Profiling... [7168/50176]	Loss: 1.9173
Profiling... [8192/50176]	Loss: 1.9977
Profiling... [9216/50176]	Loss: 2.0042
Profiling... [10240/50176]	Loss: 1.9763
Profiling... [11264/50176]	Loss: 1.8620
Profiling... [12288/50176]	Loss: 1.9793
Profiling... [13312/50176]	Loss: 1.9597
Profile done
epoch 1 train time consumed: 12.66s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4487
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.60025134136042,
                        "time": 8.831754074002674,
                        "accuracy": 0.44873046875,
                        "total_cost": 2275.203182845589
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0559
Profiling... [2048/50176]	Loss: 2.0497
Profiling... [3072/50176]	Loss: 2.0191
Profiling... [4096/50176]	Loss: 2.0343
Profiling... [5120/50176]	Loss: 2.0852
Profiling... [6144/50176]	Loss: 1.9666
Profiling... [7168/50176]	Loss: 1.9283
Profiling... [8192/50176]	Loss: 2.0110
Profiling... [9216/50176]	Loss: 1.9122
Profiling... [10240/50176]	Loss: 1.9715
Profiling... [11264/50176]	Loss: 1.9017
Profiling... [12288/50176]	Loss: 1.9126
Profiling... [13312/50176]	Loss: 1.9291
Profile done
epoch 1 train time consumed: 12.77s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4461
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.68995205426108,
                        "time": 8.853171023001778,
                        "accuracy": 0.44609375,
                        "total_cost": 2295.9813518554097
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0107
Profiling... [2048/50176]	Loss: 2.0602
Profiling... [3072/50176]	Loss: 2.0598
Profiling... [4096/50176]	Loss: 1.9639
Profiling... [5120/50176]	Loss: 2.0422
Profiling... [6144/50176]	Loss: 2.0335
Profiling... [7168/50176]	Loss: 1.8926
Profiling... [8192/50176]	Loss: 1.8761
Profiling... [9216/50176]	Loss: 1.9663
Profiling... [10240/50176]	Loss: 1.9896
Profiling... [11264/50176]	Loss: 1.9623
Profiling... [12288/50176]	Loss: 1.9437
Profiling... [13312/50176]	Loss: 1.9678
Profile done
epoch 1 train time consumed: 13.65s
Validation Epoch: 6, Average loss: 0.0019, Accuracy: 0.4564
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.70224338992273,
                        "time": 9.578723858998273,
                        "accuracy": 0.4564453125,
                        "total_cost": 2428.0670848135355
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0917
Profiling... [2048/50176]	Loss: 2.0048
Profiling... [3072/50176]	Loss: 1.9884
Profiling... [4096/50176]	Loss: 2.0734
Profiling... [5120/50176]	Loss: 2.0266
Profiling... [6144/50176]	Loss: 2.0290
Profiling... [7168/50176]	Loss: 2.0532
Profiling... [8192/50176]	Loss: 2.0992
Profiling... [9216/50176]	Loss: 1.9225
Profiling... [10240/50176]	Loss: 1.8677
Profiling... [11264/50176]	Loss: 1.8974
Profiling... [12288/50176]	Loss: 1.8950
Profiling... [13312/50176]	Loss: 1.8912
Profile done
epoch 1 train time consumed: 17.05s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4491
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.59253686794929,
                        "time": 12.54382736900152,
                        "accuracy": 0.44912109375,
                        "total_cost": 3228.467439615776
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1401
Profiling... [2048/50176]	Loss: 2.0241
Profiling... [3072/50176]	Loss: 2.0634
Profiling... [4096/50176]	Loss: 2.0522
Profiling... [5120/50176]	Loss: 2.0116
Profiling... [6144/50176]	Loss: 1.9605
Profiling... [7168/50176]	Loss: 1.9589
Profiling... [8192/50176]	Loss: 1.8777
Profiling... [9216/50176]	Loss: 2.0984
Profiling... [10240/50176]	Loss: 1.9751
Profiling... [11264/50176]	Loss: 1.9820
Profiling... [12288/50176]	Loss: 1.9951
Profiling... [13312/50176]	Loss: 1.9506
Profile done
epoch 1 train time consumed: 12.80s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4561
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.62674798034593,
                        "time": 8.920593424001709,
                        "accuracy": 0.4560546875,
                        "total_cost": 2261.7007037608328
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0094
Profiling... [2048/50176]	Loss: 1.9631
Profiling... [3072/50176]	Loss: 2.0644
Profiling... [4096/50176]	Loss: 2.0397
Profiling... [5120/50176]	Loss: 2.0509
Profiling... [6144/50176]	Loss: 1.9994
Profiling... [7168/50176]	Loss: 2.0678
Profiling... [8192/50176]	Loss: 1.9919
Profiling... [9216/50176]	Loss: 2.0190
Profiling... [10240/50176]	Loss: 1.9804
Profiling... [11264/50176]	Loss: 1.9787
Profiling... [12288/50176]	Loss: 1.9471
Profiling... [13312/50176]	Loss: 1.8719
Profile done
epoch 1 train time consumed: 12.77s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4520
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.7092149449483,
                        "time": 8.838687527997536,
                        "accuracy": 0.451953125,
                        "total_cost": 2262.884220588807
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.1632
Profiling... [2048/50176]	Loss: 2.1230
Profiling... [3072/50176]	Loss: 2.0565
Profiling... [4096/50176]	Loss: 2.0098
Profiling... [5120/50176]	Loss: 1.9794
Profiling... [6144/50176]	Loss: 1.9647
Profiling... [7168/50176]	Loss: 2.0057
Profiling... [8192/50176]	Loss: 1.9602
Profiling... [9216/50176]	Loss: 2.0220
Profiling... [10240/50176]	Loss: 1.9730
Profiling... [11264/50176]	Loss: 1.9823
Profiling... [12288/50176]	Loss: 1.9246
Profiling... [13312/50176]	Loss: 1.9639
Profile done
epoch 1 train time consumed: 13.66s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4518
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.72474401582477,
                        "time": 9.600450104997435,
                        "accuracy": 0.4517578125,
                        "total_cost": 2459.303635922237
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0793
Profiling... [2048/50176]	Loss: 2.1549
Profiling... [3072/50176]	Loss: 2.0212
Profiling... [4096/50176]	Loss: 2.1077
Profiling... [5120/50176]	Loss: 2.0411
Profiling... [6144/50176]	Loss: 1.9907
Profiling... [7168/50176]	Loss: 1.9528
Profiling... [8192/50176]	Loss: 1.9725
Profiling... [9216/50176]	Loss: 2.0100
Profiling... [10240/50176]	Loss: 1.9462
Profiling... [11264/50176]	Loss: 1.9827
Profiling... [12288/50176]	Loss: 1.9253
Profiling... [13312/50176]	Loss: 1.8128
Profile done
epoch 1 train time consumed: 17.83s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4534
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.61690151000846,
                        "time": 13.319470465998165,
                        "accuracy": 0.45341796875,
                        "total_cost": 3396.3274752392053
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0864
Profiling... [2048/50176]	Loss: 1.9532
Profiling... [3072/50176]	Loss: 2.1938
Profiling... [4096/50176]	Loss: 1.9922
Profiling... [5120/50176]	Loss: 1.9619
Profiling... [6144/50176]	Loss: 2.0850
Profiling... [7168/50176]	Loss: 1.9835
Profiling... [8192/50176]	Loss: 1.9557
Profiling... [9216/50176]	Loss: 1.9775
Profiling... [10240/50176]	Loss: 2.0026
Profiling... [11264/50176]	Loss: 2.0400
Profiling... [12288/50176]	Loss: 1.8874
Profiling... [13312/50176]	Loss: 1.9543
Profile done
epoch 1 train time consumed: 12.85s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4552
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.64720478851832,
                        "time": 8.93348915000388,
                        "accuracy": 0.45517578125,
                        "total_cost": 2269.745210013862
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0817
Profiling... [2048/50176]	Loss: 2.0688
Profiling... [3072/50176]	Loss: 2.0663
Profiling... [4096/50176]	Loss: 2.0514
Profiling... [5120/50176]	Loss: 2.0805
Profiling... [6144/50176]	Loss: 2.0346
Profiling... [7168/50176]	Loss: 1.9842
Profiling... [8192/50176]	Loss: 1.9611
Profiling... [9216/50176]	Loss: 1.9792
Profiling... [10240/50176]	Loss: 1.8879
Profiling... [11264/50176]	Loss: 1.9049
Profiling... [12288/50176]	Loss: 1.8942
Profiling... [13312/50176]	Loss: 1.9884
Profile done
epoch 1 train time consumed: 12.89s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4554
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.72685100252833,
                        "time": 8.97439123300137,
                        "accuracy": 0.45537109375,
                        "total_cost": 2280.728951210347
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0888
Profiling... [2048/50176]	Loss: 2.1152
Profiling... [3072/50176]	Loss: 2.0868
Profiling... [4096/50176]	Loss: 2.0097
Profiling... [5120/50176]	Loss: 2.0725
Profiling... [6144/50176]	Loss: 1.9984
Profiling... [7168/50176]	Loss: 1.9576
Profiling... [8192/50176]	Loss: 1.9256
Profiling... [9216/50176]	Loss: 1.9420
Profiling... [10240/50176]	Loss: 1.9442
Profiling... [11264/50176]	Loss: 1.9122
Profiling... [12288/50176]	Loss: 1.9446
Profiling... [13312/50176]	Loss: 1.9166
Profile done
epoch 1 train time consumed: 13.66s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4487
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.74370856729514,
                        "time": 9.56882327600033,
                        "accuracy": 0.44873046875,
                        "total_cost": 2468.1432835940727
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0499
Profiling... [2048/50176]	Loss: 2.1173
Profiling... [3072/50176]	Loss: 1.9674
Profiling... [4096/50176]	Loss: 2.1023
Profiling... [5120/50176]	Loss: 1.9943
Profiling... [6144/50176]	Loss: 2.0024
Profiling... [7168/50176]	Loss: 2.0402
Profiling... [8192/50176]	Loss: 1.9309
Profiling... [9216/50176]	Loss: 1.9084
Profiling... [10240/50176]	Loss: 1.9572
Profiling... [11264/50176]	Loss: 1.9774
Profiling... [12288/50176]	Loss: 1.9315
Profiling... [13312/50176]	Loss: 1.9091
Profile done
epoch 1 train time consumed: 18.68s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4506
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.62434095352519,
                        "time": 12.664395883999532,
                        "accuracy": 0.4505859375,
                        "total_cost": 3249.7961116728884
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1176
Profiling... [2048/50176]	Loss: 2.0265
Profiling... [3072/50176]	Loss: 2.0598
Profiling... [4096/50176]	Loss: 1.9766
Profiling... [5120/50176]	Loss: 2.0062
Profiling... [6144/50176]	Loss: 1.9987
Profiling... [7168/50176]	Loss: 1.9384
Profiling... [8192/50176]	Loss: 1.8656
Profiling... [9216/50176]	Loss: 1.9311
Profiling... [10240/50176]	Loss: 1.8830
Profiling... [11264/50176]	Loss: 1.9976
Profiling... [12288/50176]	Loss: 1.8926
Profiling... [13312/50176]	Loss: 1.8805
Profile done
epoch 1 train time consumed: 12.83s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4345
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.65530970315261,
                        "time": 8.957094694000261,
                        "accuracy": 0.43447265625,
                        "total_cost": 2384.351571894958
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0224
Profiling... [2048/50176]	Loss: 2.0028
Profiling... [3072/50176]	Loss: 1.9948
Profiling... [4096/50176]	Loss: 1.9344
Profiling... [5120/50176]	Loss: 1.9218
Profiling... [6144/50176]	Loss: 1.9613
Profiling... [7168/50176]	Loss: 1.9384
Profiling... [8192/50176]	Loss: 1.9533
Profiling... [9216/50176]	Loss: 1.8550
Profiling... [10240/50176]	Loss: 2.0300
Profiling... [11264/50176]	Loss: 1.7940
Profiling... [12288/50176]	Loss: 1.8796
Profiling... [13312/50176]	Loss: 1.8456
Profile done
epoch 1 train time consumed: 12.86s
Validation Epoch: 6, Average loss: 0.0024, Accuracy: 0.3757
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.73418590555781,
                        "time": 8.884199248997902,
                        "accuracy": 0.37568359375,
                        "total_cost": 2736.8923866016985
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9654
Profiling... [2048/50176]	Loss: 2.0275
Profiling... [3072/50176]	Loss: 1.9167
Profiling... [4096/50176]	Loss: 1.9050
Profiling... [5120/50176]	Loss: 1.9183
Profiling... [6144/50176]	Loss: 2.0029
Profiling... [7168/50176]	Loss: 2.0026
Profiling... [8192/50176]	Loss: 2.0045
Profiling... [9216/50176]	Loss: 1.9339
Profiling... [10240/50176]	Loss: 1.8554
Profiling... [11264/50176]	Loss: 1.8552
Profiling... [12288/50176]	Loss: 1.9908
Profiling... [13312/50176]	Loss: 1.8933
Profile done
epoch 1 train time consumed: 13.62s
Validation Epoch: 6, Average loss: 0.0026, Accuracy: 0.3611
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.7486710110778,
                        "time": 9.570933801995125,
                        "accuracy": 0.3611328125,
                        "total_cost": 3067.6328197564094
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1189
Profiling... [2048/50176]	Loss: 2.0907
Profiling... [3072/50176]	Loss: 2.0588
Profiling... [4096/50176]	Loss: 1.9714
Profiling... [5120/50176]	Loss: 1.9817
Profiling... [6144/50176]	Loss: 2.0469
Profiling... [7168/50176]	Loss: 1.8896
Profiling... [8192/50176]	Loss: 1.9913
Profiling... [9216/50176]	Loss: 1.9322
Profiling... [10240/50176]	Loss: 1.8638
Profiling... [11264/50176]	Loss: 1.9473
Profiling... [12288/50176]	Loss: 1.9023
Profiling... [13312/50176]	Loss: 1.9179
Profile done
epoch 1 train time consumed: 17.23s
Validation Epoch: 6, Average loss: 0.0025, Accuracy: 0.3616
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.645271312199,
                        "time": 12.631092935997003,
                        "accuracy": 0.36162109375,
                        "total_cost": 4039.3831963873754
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1179
Profiling... [2048/50176]	Loss: 2.0185
Profiling... [3072/50176]	Loss: 2.0028
Profiling... [4096/50176]	Loss: 1.9848
Profiling... [5120/50176]	Loss: 1.9102
Profiling... [6144/50176]	Loss: 1.8974
Profiling... [7168/50176]	Loss: 1.9356
Profiling... [8192/50176]	Loss: 1.9770
Profiling... [9216/50176]	Loss: 1.9877
Profiling... [10240/50176]	Loss: 1.9899
Profiling... [11264/50176]	Loss: 1.9768
Profiling... [12288/50176]	Loss: 1.8281
Profiling... [13312/50176]	Loss: 1.9396
Profile done
epoch 1 train time consumed: 12.94s
Validation Epoch: 6, Average loss: 0.0024, Accuracy: 0.3736
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.6746342410136,
                        "time": 8.93983897400176,
                        "accuracy": 0.3736328125,
                        "total_cost": 2767.724270713541
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0350
Profiling... [2048/50176]	Loss: 2.1453
Profiling... [3072/50176]	Loss: 1.8929
Profiling... [4096/50176]	Loss: 1.8606
Profiling... [5120/50176]	Loss: 2.0081
Profiling... [6144/50176]	Loss: 1.8809
Profiling... [7168/50176]	Loss: 1.8896
Profiling... [8192/50176]	Loss: 1.9490
Profiling... [9216/50176]	Loss: 1.8739
Profiling... [10240/50176]	Loss: 1.9388
Profiling... [11264/50176]	Loss: 1.9318
Profiling... [12288/50176]	Loss: 2.0015
Profiling... [13312/50176]	Loss: 1.8609
Profile done
epoch 1 train time consumed: 12.89s
Validation Epoch: 6, Average loss: 0.0024, Accuracy: 0.3861
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.7471009258113,
                        "time": 8.958488646996557,
                        "accuracy": 0.3861328125,
                        "total_cost": 2685.3949107644025
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0614
Profiling... [2048/50176]	Loss: 1.9066
Profiling... [3072/50176]	Loss: 2.0465
Profiling... [4096/50176]	Loss: 1.8806
Profiling... [5120/50176]	Loss: 1.9958
Profiling... [6144/50176]	Loss: 1.9902
Profiling... [7168/50176]	Loss: 1.9199
Profiling... [8192/50176]	Loss: 1.8400
Profiling... [9216/50176]	Loss: 1.9677
Profiling... [10240/50176]	Loss: 1.9049
Profiling... [11264/50176]	Loss: 1.9129
Profiling... [12288/50176]	Loss: 1.7903
Profiling... [13312/50176]	Loss: 1.8672
Profile done
epoch 1 train time consumed: 13.61s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4052
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.76148963347354,
                        "time": 9.58914232400275,
                        "accuracy": 0.40517578125,
                        "total_cost": 2739.6834931973044
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0853
Profiling... [2048/50176]	Loss: 2.0033
Profiling... [3072/50176]	Loss: 1.9896
Profiling... [4096/50176]	Loss: 1.9772
Profiling... [5120/50176]	Loss: 1.9159
Profiling... [6144/50176]	Loss: 1.9008
Profiling... [7168/50176]	Loss: 1.9155
Profiling... [8192/50176]	Loss: 1.8721
Profiling... [9216/50176]	Loss: 1.9473
Profiling... [10240/50176]	Loss: 1.9153
Profiling... [11264/50176]	Loss: 1.9675
Profiling... [12288/50176]	Loss: 1.9146
Profiling... [13312/50176]	Loss: 1.9182
Profile done
epoch 1 train time consumed: 17.27s
Validation Epoch: 6, Average loss: 0.0022, Accuracy: 0.4072
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.65877380196552,
                        "time": 12.562317329000507,
                        "accuracy": 0.4072265625,
                        "total_cost": 3567.896478716025
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1182
Profiling... [2048/50176]	Loss: 2.0286
Profiling... [3072/50176]	Loss: 1.9066
Profiling... [4096/50176]	Loss: 2.0841
Profiling... [5120/50176]	Loss: 2.0439
Profiling... [6144/50176]	Loss: 1.9159
Profiling... [7168/50176]	Loss: 1.8893
Profiling... [8192/50176]	Loss: 2.0257
Profiling... [9216/50176]	Loss: 1.8635
Profiling... [10240/50176]	Loss: 1.8650
Profiling... [11264/50176]	Loss: 1.9170
Profiling... [12288/50176]	Loss: 1.9118
Profiling... [13312/50176]	Loss: 1.9424
Profile done
epoch 1 train time consumed: 13.04s
Validation Epoch: 6, Average loss: 0.0024, Accuracy: 0.3867
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.6824211495173,
                        "time": 9.20370328900026,
                        "accuracy": 0.38671875,
                        "total_cost": 2753.1809099334478
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0903
Profiling... [2048/50176]	Loss: 2.0526
Profiling... [3072/50176]	Loss: 2.0090
Profiling... [4096/50176]	Loss: 1.9878
Profiling... [5120/50176]	Loss: 2.0108
Profiling... [6144/50176]	Loss: 1.9224
Profiling... [7168/50176]	Loss: 1.9275
Profiling... [8192/50176]	Loss: 1.9880
Profiling... [9216/50176]	Loss: 1.9616
Profiling... [10240/50176]	Loss: 1.8403
Profiling... [11264/50176]	Loss: 1.9555
Profiling... [12288/50176]	Loss: 1.9011
Profiling... [13312/50176]	Loss: 1.8735
Profile done
epoch 1 train time consumed: 12.86s
Validation Epoch: 6, Average loss: 0.0024, Accuracy: 0.3763
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.75991930265593,
                        "time": 8.960208302996762,
                        "accuracy": 0.37626953125,
                        "total_cost": 2756.622325076694
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0277
Profiling... [2048/50176]	Loss: 2.0146
Profiling... [3072/50176]	Loss: 2.0428
Profiling... [4096/50176]	Loss: 1.9806
Profiling... [5120/50176]	Loss: 1.9343
Profiling... [6144/50176]	Loss: 1.9354
Profiling... [7168/50176]	Loss: 1.9423
Profiling... [8192/50176]	Loss: 1.9731
Profiling... [9216/50176]	Loss: 1.8965
Profiling... [10240/50176]	Loss: 1.8868
Profiling... [11264/50176]	Loss: 1.9184
Profiling... [12288/50176]	Loss: 1.9260
Profiling... [13312/50176]	Loss: 1.8699
Profile done
epoch 1 train time consumed: 13.65s
Validation Epoch: 6, Average loss: 0.0020, Accuracy: 0.4419
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.76926497715425,
                        "time": 9.61097510199761,
                        "accuracy": 0.44189453125,
                        "total_cost": 2517.921007360268
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0793
Profiling... [2048/50176]	Loss: 2.0787
Profiling... [3072/50176]	Loss: 1.8650
Profiling... [4096/50176]	Loss: 1.9640
Profiling... [5120/50176]	Loss: 1.9072
Profiling... [6144/50176]	Loss: 1.9368
Profiling... [7168/50176]	Loss: 1.9346
Profiling... [8192/50176]	Loss: 1.9557
Profiling... [9216/50176]	Loss: 1.9608
Profiling... [10240/50176]	Loss: 1.8348
Profiling... [11264/50176]	Loss: 1.9068
Profiling... [12288/50176]	Loss: 1.8680
Profiling... [13312/50176]	Loss: 1.9198
Profile done
epoch 1 train time consumed: 16.58s
Validation Epoch: 6, Average loss: 0.0021, Accuracy: 0.4325
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.67545144285695,
                        "time": 11.940194554998016,
                        "accuracy": 0.43251953125,
                        "total_cost": 3193.352659643478
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0019
Profiling... [2048/50176]	Loss: 2.0223
Profiling... [3072/50176]	Loss: 1.9605
Profiling... [4096/50176]	Loss: 2.0901
Profiling... [5120/50176]	Loss: 1.9980
Profiling... [6144/50176]	Loss: 1.9611
Profiling... [7168/50176]	Loss: 2.0191
Profiling... [8192/50176]	Loss: 1.9091
Profiling... [9216/50176]	Loss: 2.0131
Profiling... [10240/50176]	Loss: 1.9069
Profiling... [11264/50176]	Loss: 1.9806
Profiling... [12288/50176]	Loss: 1.9594
Profiling... [13312/50176]	Loss: 2.0564
Profile done
epoch 1 train time consumed: 12.76s
Validation Epoch: 6, Average loss: 0.0032, Accuracy: 0.2769
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.70105637458211,
                        "time": 8.902376256002754,
                        "accuracy": 0.27685546875,
                        "total_cost": 3720.404519058343
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0678
Profiling... [2048/50176]	Loss: 1.9925
Profiling... [3072/50176]	Loss: 2.0703
Profiling... [4096/50176]	Loss: 1.9787
Profiling... [5120/50176]	Loss: 1.9620
Profiling... [6144/50176]	Loss: 2.0003
Profiling... [7168/50176]	Loss: 1.9596
Profiling... [8192/50176]	Loss: 1.9531
Profiling... [9216/50176]	Loss: 1.9881
Profiling... [10240/50176]	Loss: 1.9341
Profiling... [11264/50176]	Loss: 1.9699
Profiling... [12288/50176]	Loss: 1.9122
Profiling... [13312/50176]	Loss: 1.8887
Profile done
epoch 1 train time consumed: 12.75s
Validation Epoch: 6, Average loss: 0.0043, Accuracy: 0.1565
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.77636851899949,
                        "time": 8.976525999001751,
                        "accuracy": 0.15654296875,
                        "total_cost": 6638.877430135658
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0713
Profiling... [2048/50176]	Loss: 1.9856
Profiling... [3072/50176]	Loss: 2.0631
Profiling... [4096/50176]	Loss: 1.8843
Profiling... [5120/50176]	Loss: 2.0050
Profiling... [6144/50176]	Loss: 1.9076
Profiling... [7168/50176]	Loss: 1.9256
Profiling... [8192/50176]	Loss: 1.8810
Profiling... [9216/50176]	Loss: 1.9496
Profiling... [10240/50176]	Loss: 1.9734
Profiling... [11264/50176]	Loss: 2.0036
Profiling... [12288/50176]	Loss: 1.9279
Profiling... [13312/50176]	Loss: 2.0984
Profile done
epoch 1 train time consumed: 13.70s
Validation Epoch: 6, Average loss: 0.0028, Accuracy: 0.3126
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.78969746290323,
                        "time": 9.634386924000864,
                        "accuracy": 0.31259765625,
                        "total_cost": 3568.685576703235
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1356
Profiling... [2048/50176]	Loss: 2.0926
Profiling... [3072/50176]	Loss: 2.0349
Profiling... [4096/50176]	Loss: 1.9893
Profiling... [5120/50176]	Loss: 1.9369
Profiling... [6144/50176]	Loss: 1.9582
Profiling... [7168/50176]	Loss: 1.9236
Profiling... [8192/50176]	Loss: 1.9888
Profiling... [9216/50176]	Loss: 1.9545
Profiling... [10240/50176]	Loss: 1.9262
Profiling... [11264/50176]	Loss: 1.8780
Profiling... [12288/50176]	Loss: 1.9483
Profiling... [13312/50176]	Loss: 1.9636
Profile done
epoch 1 train time consumed: 18.81s
Validation Epoch: 6, Average loss: 0.0045, Accuracy: 0.2076
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.67813168829893,
                        "time": 13.444599349000782,
                        "accuracy": 0.2076171875,
                        "total_cost": 7490.931520253498
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0439
Profiling... [2048/50176]	Loss: 1.9798
Profiling... [3072/50176]	Loss: 2.0885
Profiling... [4096/50176]	Loss: 2.0225
Profiling... [5120/50176]	Loss: 2.0070
Profiling... [6144/50176]	Loss: 2.0526
Profiling... [7168/50176]	Loss: 1.9722
Profiling... [8192/50176]	Loss: 1.9631
Profiling... [9216/50176]	Loss: 1.9748
Profiling... [10240/50176]	Loss: 1.9472
Profiling... [11264/50176]	Loss: 1.9664
Profiling... [12288/50176]	Loss: 1.9768
Profiling... [13312/50176]	Loss: 2.0007
Profile done
epoch 1 train time consumed: 12.71s
Validation Epoch: 6, Average loss: 0.0059, Accuracy: 0.1398
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.7101731642598,
                        "time": 8.81264925999858,
                        "accuracy": 0.13984375,
                        "total_cost": 7291.803687403418
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0360
Profiling... [2048/50176]	Loss: 2.0156
Profiling... [3072/50176]	Loss: 1.9636
Profiling... [4096/50176]	Loss: 2.1386
Profiling... [5120/50176]	Loss: 1.9874
Profiling... [6144/50176]	Loss: 1.9899
Profiling... [7168/50176]	Loss: 1.9515
Profiling... [8192/50176]	Loss: 1.9647
Profiling... [9216/50176]	Loss: 2.0541
Profiling... [10240/50176]	Loss: 1.8740
Profiling... [11264/50176]	Loss: 1.9404
Profiling... [12288/50176]	Loss: 1.8877
Profiling... [13312/50176]	Loss: 1.9547
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 6, Average loss: 0.0033, Accuracy: 0.2449
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.77890694408534,
                        "time": 8.85381411400158,
                        "accuracy": 0.244921875,
                        "total_cost": 4185.354698943156
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0736
Profiling... [2048/50176]	Loss: 2.0387
Profiling... [3072/50176]	Loss: 1.8479
Profiling... [4096/50176]	Loss: 1.9328
Profiling... [5120/50176]	Loss: 1.9166
Profiling... [6144/50176]	Loss: 1.9071
Profiling... [7168/50176]	Loss: 1.9812
Profiling... [8192/50176]	Loss: 2.0770
Profiling... [9216/50176]	Loss: 1.9863
Profiling... [10240/50176]	Loss: 1.9130
Profiling... [11264/50176]	Loss: 2.0062
Profiling... [12288/50176]	Loss: 1.9254
Profiling... [13312/50176]	Loss: 1.8829
Profile done
epoch 1 train time consumed: 13.67s
Validation Epoch: 6, Average loss: 0.0034, Accuracy: 0.2413
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.79268100227752,
                        "time": 9.572313507000217,
                        "accuracy": 0.24130859375,
                        "total_cost": 4593.304478489459
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.1326
Profiling... [2048/50176]	Loss: 2.0402
Profiling... [3072/50176]	Loss: 1.9823
Profiling... [4096/50176]	Loss: 2.0204
Profiling... [5120/50176]	Loss: 2.0540
Profiling... [6144/50176]	Loss: 1.9981
Profiling... [7168/50176]	Loss: 2.0252
Profiling... [8192/50176]	Loss: 1.9701
Profiling... [9216/50176]	Loss: 1.9709
Profiling... [10240/50176]	Loss: 1.9168
Profiling... [11264/50176]	Loss: 1.8736
Profiling... [12288/50176]	Loss: 1.8800
Profiling... [13312/50176]	Loss: 1.9124
Profile done
epoch 1 train time consumed: 17.89s
Validation Epoch: 6, Average loss: 0.0039, Accuracy: 0.2272
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.69529452329533,
                        "time": 13.286288069997681,
                        "accuracy": 0.22724609375,
                        "total_cost": 6764.301141610831
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0588
Profiling... [2048/50176]	Loss: 2.0178
Profiling... [3072/50176]	Loss: 1.9802
Profiling... [4096/50176]	Loss: 1.8826
Profiling... [5120/50176]	Loss: 1.9385
Profiling... [6144/50176]	Loss: 1.9809
Profiling... [7168/50176]	Loss: 1.9518
Profiling... [8192/50176]	Loss: 1.9820
Profiling... [9216/50176]	Loss: 1.9914
Profiling... [10240/50176]	Loss: 1.9562
Profiling... [11264/50176]	Loss: 1.9719
Profiling... [12288/50176]	Loss: 1.9109
Profiling... [13312/50176]	Loss: 1.9595
Profile done
epoch 1 train time consumed: 12.86s
Validation Epoch: 6, Average loss: 0.0030, Accuracy: 0.2899
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.72407120929648,
                        "time": 8.930472330001066,
                        "accuracy": 0.28994140625,
                        "total_cost": 3564.4119590100645
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0668
Profiling... [2048/50176]	Loss: 1.9198
Profiling... [3072/50176]	Loss: 2.0997
Profiling... [4096/50176]	Loss: 1.9842
Profiling... [5120/50176]	Loss: 2.0409
Profiling... [6144/50176]	Loss: 1.9453
Profiling... [7168/50176]	Loss: 1.8679
Profiling... [8192/50176]	Loss: 1.9187
Profiling... [9216/50176]	Loss: 2.0066
Profiling... [10240/50176]	Loss: 2.0023
Profiling... [11264/50176]	Loss: 1.9067
Profiling... [12288/50176]	Loss: 1.9286
Profiling... [13312/50176]	Loss: 1.9235
Profile done
epoch 1 train time consumed: 12.83s
Validation Epoch: 6, Average loss: 0.0030, Accuracy: 0.2989
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.7976428263265,
                        "time": 8.955727929002023,
                        "accuracy": 0.29892578125,
                        "total_cost": 3469.263104827406
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9614
Profiling... [2048/50176]	Loss: 2.0219
Profiling... [3072/50176]	Loss: 1.9715
Profiling... [4096/50176]	Loss: 1.9875
Profiling... [5120/50176]	Loss: 1.9911
Profiling... [6144/50176]	Loss: 1.9770
Profiling... [7168/50176]	Loss: 1.9993
Profiling... [8192/50176]	Loss: 2.0527
Profiling... [9216/50176]	Loss: 1.9343
Profiling... [10240/50176]	Loss: 1.9215
Profiling... [11264/50176]	Loss: 1.9119
Profiling... [12288/50176]	Loss: 1.9437
Profiling... [13312/50176]	Loss: 1.9050
Profile done
epoch 1 train time consumed: 13.71s
Validation Epoch: 6, Average loss: 0.0038, Accuracy: 0.2069
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.80948118809506,
                        "time": 9.603085645001556,
                        "accuracy": 0.20693359375,
                        "total_cost": 5374.324903940221
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0579
Profiling... [2048/50176]	Loss: 1.9946
Profiling... [3072/50176]	Loss: 1.9300
Profiling... [4096/50176]	Loss: 1.9823
Profiling... [5120/50176]	Loss: 1.9643
Profiling... [6144/50176]	Loss: 2.0385
Profiling... [7168/50176]	Loss: 1.9960
Profiling... [8192/50176]	Loss: 1.9093
Profiling... [9216/50176]	Loss: 1.9302
Profiling... [10240/50176]	Loss: 2.0097
Profiling... [11264/50176]	Loss: 1.9983
Profiling... [12288/50176]	Loss: 1.9833
Profiling... [13312/50176]	Loss: 1.8381
Profile done
epoch 1 train time consumed: 20.90s
Validation Epoch: 6, Average loss: 0.0075, Accuracy: 0.1076
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.68466926670186,
                        "time": 14.791791959003604,
                        "accuracy": 0.1076171875,
                        "total_cost": 15900.653049859642
                    },
                    
[Training Loop] The optimal parameters are lr: 0.005 dr: 0.25 bs: 128 pl: 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[GPU_0] Set GPU power limit to 125W.
[GPU_0] Set GPU power limit to 125W.
Training Epoch: 6 [128/50048]	Loss: 2.0065
Training Epoch: 6 [256/50048]	Loss: 2.1725
Training Epoch: 6 [384/50048]	Loss: 2.4297
Training Epoch: 6 [512/50048]	Loss: 1.9777
Training Epoch: 6 [640/50048]	Loss: 1.9576
Training Epoch: 6 [768/50048]	Loss: 2.1366
Training Epoch: 6 [896/50048]	Loss: 2.0391
Training Epoch: 6 [1024/50048]	Loss: 2.4078
Training Epoch: 6 [1152/50048]	Loss: 1.7974
Training Epoch: 6 [1280/50048]	Loss: 2.0602
Training Epoch: 6 [1408/50048]	Loss: 1.9487
Training Epoch: 6 [1536/50048]	Loss: 2.2647
Training Epoch: 6 [1664/50048]	Loss: 1.8820
Training Epoch: 6 [1792/50048]	Loss: 2.2172
Training Epoch: 6 [1920/50048]	Loss: 1.9592
Training Epoch: 6 [2048/50048]	Loss: 2.0503
Training Epoch: 6 [2176/50048]	Loss: 2.1647
Training Epoch: 6 [2304/50048]	Loss: 2.0390
Training Epoch: 6 [2432/50048]	Loss: 2.0085
Training Epoch: 6 [2560/50048]	Loss: 1.9709
Training Epoch: 6 [2688/50048]	Loss: 1.9334
Training Epoch: 6 [2816/50048]	Loss: 2.0429
Training Epoch: 6 [2944/50048]	Loss: 1.5819
Training Epoch: 6 [3072/50048]	Loss: 1.9971
Training Epoch: 6 [3200/50048]	Loss: 2.0036
Training Epoch: 6 [3328/50048]	Loss: 1.8252
Training Epoch: 6 [3456/50048]	Loss: 1.8253
Training Epoch: 6 [3584/50048]	Loss: 1.8095
Training Epoch: 6 [3712/50048]	Loss: 2.0008
Training Epoch: 6 [3840/50048]	Loss: 2.0088
Training Epoch: 6 [3968/50048]	Loss: 1.9943
Training Epoch: 6 [4096/50048]	Loss: 1.9275
Training Epoch: 6 [4224/50048]	Loss: 1.9458
Training Epoch: 6 [4352/50048]	Loss: 2.1021
Training Epoch: 6 [4480/50048]	Loss: 1.9845
Training Epoch: 6 [4608/50048]	Loss: 1.8946
Training Epoch: 6 [4736/50048]	Loss: 1.8474
Training Epoch: 6 [4864/50048]	Loss: 1.8947
Training Epoch: 6 [4992/50048]	Loss: 2.0872
Training Epoch: 6 [5120/50048]	Loss: 2.2101
Training Epoch: 6 [5248/50048]	Loss: 2.0110
Training Epoch: 6 [5376/50048]	Loss: 2.0785
Training Epoch: 6 [5504/50048]	Loss: 2.1928
Training Epoch: 6 [5632/50048]	Loss: 1.9293
Training Epoch: 6 [5760/50048]	Loss: 2.3111
Training Epoch: 6 [5888/50048]	Loss: 2.0486
Training Epoch: 6 [6016/50048]	Loss: 1.9041
Training Epoch: 6 [6144/50048]	Loss: 1.9592
Training Epoch: 6 [6272/50048]	Loss: 2.1194
Training Epoch: 6 [6400/50048]	Loss: 2.1998
Training Epoch: 6 [6528/50048]	Loss: 2.1183
Training Epoch: 6 [6656/50048]	Loss: 2.0968
Training Epoch: 6 [6784/50048]	Loss: 2.0522
Training Epoch: 6 [6912/50048]	Loss: 1.9806
Training Epoch: 6 [7040/50048]	Loss: 2.3665
Training Epoch: 6 [7168/50048]	Loss: 1.9323
Training Epoch: 6 [7296/50048]	Loss: 2.1776
Training Epoch: 6 [7424/50048]	Loss: 1.9679
Training Epoch: 6 [7552/50048]	Loss: 2.0065
Training Epoch: 6 [7680/50048]	Loss: 2.0105
Training Epoch: 6 [7808/50048]	Loss: 2.0011
Training Epoch: 6 [7936/50048]	Loss: 2.0617
Training Epoch: 6 [8064/50048]	Loss: 2.0995
Training Epoch: 6 [8192/50048]	Loss: 2.0547
Training Epoch: 6 [8320/50048]	Loss: 2.0095
Training Epoch: 6 [8448/50048]	Loss: 1.8323
Training Epoch: 6 [8576/50048]	Loss: 2.0218
Training Epoch: 6 [8704/50048]	Loss: 1.8714
Training Epoch: 6 [8832/50048]	Loss: 1.9439
Training Epoch: 6 [8960/50048]	Loss: 1.8759
Training Epoch: 6 [9088/50048]	Loss: 1.9520
Training Epoch: 6 [9216/50048]	Loss: 1.9512
Training Epoch: 6 [9344/50048]	Loss: 1.8965
Training Epoch: 6 [9472/50048]	Loss: 2.0939
Training Epoch: 6 [9600/50048]	Loss: 1.6824
Training Epoch: 6 [9728/50048]	Loss: 1.8575
Training Epoch: 6 [9856/50048]	Loss: 2.0921
Training Epoch: 6 [9984/50048]	Loss: 2.1062
Training Epoch: 6 [10112/50048]	Loss: 2.3461
Training Epoch: 6 [10240/50048]	Loss: 1.8803
Training Epoch: 6 [10368/50048]	Loss: 1.7941
Training Epoch: 6 [10496/50048]	Loss: 1.9683
Training Epoch: 6 [10624/50048]	Loss: 2.0520
Training Epoch: 6 [10752/50048]	Loss: 2.0365
Training Epoch: 6 [10880/50048]	Loss: 2.1472
Training Epoch: 6 [11008/50048]	Loss: 1.9301
Training Epoch: 6 [11136/50048]	Loss: 2.0357
Training Epoch: 6 [11264/50048]	Loss: 1.8312
Training Epoch: 6 [11392/50048]	Loss: 1.7566
Training Epoch: 6 [11520/50048]	Loss: 2.0454
Training Epoch: 6 [11648/50048]	Loss: 1.7694
Training Epoch: 6 [11776/50048]	Loss: 2.0834
Training Epoch: 6 [11904/50048]	Loss: 1.9939
Training Epoch: 6 [12032/50048]	Loss: 1.7831
Training Epoch: 6 [12160/50048]	Loss: 1.9774
Training Epoch: 6 [12288/50048]	Loss: 2.1899
Training Epoch: 6 [12416/50048]	Loss: 1.8613
Training Epoch: 6 [12544/50048]	Loss: 2.0088
Training Epoch: 6 [12672/50048]	Loss: 2.0861
Training Epoch: 6 [12800/50048]	Loss: 2.2597
Training Epoch: 6 [12928/50048]	Loss: 2.0945
Training Epoch: 6 [13056/50048]	Loss: 1.9956
Training Epoch: 6 [13184/50048]	Loss: 2.1594
Training Epoch: 6 [13312/50048]	Loss: 2.2346
Training Epoch: 6 [13440/50048]	Loss: 1.7888
Training Epoch: 6 [13568/50048]	Loss: 2.1366
Training Epoch: 6 [13696/50048]	Loss: 1.9917
Training Epoch: 6 [13824/50048]	Loss: 1.8974
Training Epoch: 6 [13952/50048]	Loss: 2.0068
Training Epoch: 6 [14080/50048]	Loss: 2.1636
Training Epoch: 6 [14208/50048]	Loss: 1.9683
Training Epoch: 6 [14336/50048]	Loss: 1.9488
Training Epoch: 6 [14464/50048]	Loss: 1.9418
Training Epoch: 6 [14592/50048]	Loss: 1.9672
Training Epoch: 6 [14720/50048]	Loss: 1.9763
Training Epoch: 6 [14848/50048]	Loss: 1.9308
Training Epoch: 6 [14976/50048]	Loss: 2.1339
Training Epoch: 6 [15104/50048]	Loss: 1.9240
Training Epoch: 6 [15232/50048]	Loss: 1.8495
Training Epoch: 6 [15360/50048]	Loss: 1.7447
Training Epoch: 6 [15488/50048]	Loss: 1.6710
Training Epoch: 6 [15616/50048]	Loss: 1.8525
Training Epoch: 6 [15744/50048]	Loss: 2.3338
Training Epoch: 6 [15872/50048]	Loss: 2.1222
Training Epoch: 6 [16000/50048]	Loss: 1.9109
Training Epoch: 6 [16128/50048]	Loss: 2.1158
Training Epoch: 6 [16256/50048]	Loss: 2.1638
Training Epoch: 6 [16384/50048]	Loss: 2.2802
Training Epoch: 6 [16512/50048]	Loss: 2.1749
Training Epoch: 6 [16640/50048]	Loss: 1.7704
Training Epoch: 6 [16768/50048]	Loss: 1.7669
Training Epoch: 6 [16896/50048]	Loss: 1.7994
Training Epoch: 6 [17024/50048]	Loss: 2.0030
Training Epoch: 6 [17152/50048]	Loss: 2.1254
Training Epoch: 6 [17280/50048]	Loss: 1.8795
Training Epoch: 6 [17408/50048]	Loss: 1.7886
Training Epoch: 6 [17536/50048]	Loss: 1.9197
Training Epoch: 6 [17664/50048]	Loss: 1.8598
Training Epoch: 6 [17792/50048]	Loss: 1.7896
Training Epoch: 6 [17920/50048]	Loss: 1.7381
Training Epoch: 6 [18048/50048]	Loss: 1.8808
Training Epoch: 6 [18176/50048]	Loss: 1.9483
Training Epoch: 6 [18304/50048]	Loss: 1.9662
Training Epoch: 6 [18432/50048]	Loss: 2.1345
Training Epoch: 6 [18560/50048]	Loss: 2.0302
Training Epoch: 6 [18688/50048]	Loss: 1.9945
Training Epoch: 6 [18816/50048]	Loss: 1.9825
Training Epoch: 6 [18944/50048]	Loss: 2.3555
Training Epoch: 6 [19072/50048]	Loss: 1.9872
Training Epoch: 6 [19200/50048]	Loss: 1.7485
Training Epoch: 6 [19328/50048]	Loss: 1.8575
Training Epoch: 6 [19456/50048]	Loss: 1.8913
Training Epoch: 6 [19584/50048]	Loss: 2.2523
Training Epoch: 6 [19712/50048]	Loss: 1.8315
Training Epoch: 6 [19840/50048]	Loss: 1.8884
Training Epoch: 6 [19968/50048]	Loss: 1.9506
Training Epoch: 6 [20096/50048]	Loss: 2.1931
Training Epoch: 6 [20224/50048]	Loss: 1.8961
Training Epoch: 6 [20352/50048]	Loss: 1.6524
Training Epoch: 6 [20480/50048]	Loss: 1.8428
Training Epoch: 6 [20608/50048]	Loss: 1.8027
Training Epoch: 6 [20736/50048]	Loss: 2.0718
Training Epoch: 6 [20864/50048]	Loss: 1.9608
Training Epoch: 6 [20992/50048]	Loss: 1.8620
Training Epoch: 6 [21120/50048]	Loss: 1.9379
Training Epoch: 6 [21248/50048]	Loss: 1.8247
Training Epoch: 6 [21376/50048]	Loss: 1.8927
Training Epoch: 6 [21504/50048]	Loss: 1.9766
Training Epoch: 6 [21632/50048]	Loss: 2.0453
Training Epoch: 6 [21760/50048]	Loss: 1.8932
Training Epoch: 6 [21888/50048]	Loss: 1.8573
Training Epoch: 6 [22016/50048]	Loss: 2.1014
Training Epoch: 6 [22144/50048]	Loss: 2.0224
Training Epoch: 6 [22272/50048]	Loss: 1.8256
Training Epoch: 6 [22400/50048]	Loss: 1.8902
Training Epoch: 6 [22528/50048]	Loss: 1.9489
Training Epoch: 6 [22656/50048]	Loss: 2.0262
Training Epoch: 6 [22784/50048]	Loss: 1.9736
Training Epoch: 6 [22912/50048]	Loss: 1.8380
Training Epoch: 6 [23040/50048]	Loss: 1.8271
Training Epoch: 6 [23168/50048]	Loss: 1.7623
Training Epoch: 6 [23296/50048]	Loss: 1.7573
Training Epoch: 6 [23424/50048]	Loss: 1.8400
Training Epoch: 6 [23552/50048]	Loss: 2.1368
Training Epoch: 6 [23680/50048]	Loss: 1.9608
Training Epoch: 6 [23808/50048]	Loss: 1.9591
Training Epoch: 6 [23936/50048]	Loss: 1.9446
Training Epoch: 6 [24064/50048]	Loss: 2.2126
Training Epoch: 6 [24192/50048]	Loss: 2.0178
Training Epoch: 6 [24320/50048]	Loss: 2.0718
Training Epoch: 6 [24448/50048]	Loss: 1.7725
Training Epoch: 6 [24576/50048]	Loss: 2.1358
Training Epoch: 6 [24704/50048]	Loss: 1.8808
Training Epoch: 6 [24832/50048]	Loss: 2.0194
Training Epoch: 6 [24960/50048]	Loss: 1.9476
Training Epoch: 6 [25088/50048]	Loss: 1.7091
Training Epoch: 6 [25216/50048]	Loss: 1.8264
Training Epoch: 6 [25344/50048]	Loss: 1.8122
Training Epoch: 6 [25472/50048]	Loss: 1.9884
Training Epoch: 6 [25600/50048]	Loss: 1.7551
Training Epoch: 6 [25728/50048]	Loss: 1.8114
Training Epoch: 6 [25856/50048]	Loss: 1.9113
Training Epoch: 6 [25984/50048]	Loss: 1.9813
Training Epoch: 6 [26112/50048]	Loss: 1.8036
Training Epoch: 6 [26240/50048]	Loss: 2.1253
Training Epoch: 6 [26368/50048]	Loss: 1.9562
Training Epoch: 6 [26496/50048]	Loss: 2.2221
Training Epoch: 6 [26624/50048]	Loss: 1.7344
Training Epoch: 6 [26752/50048]	Loss: 2.3208
Training Epoch: 6 [26880/50048]	Loss: 2.0170
Training Epoch: 6 [27008/50048]	Loss: 1.8771
Training Epoch: 6 [27136/50048]	Loss: 1.7358
Training Epoch: 6 [27264/50048]	Loss: 1.7621
Training Epoch: 6 [27392/50048]	Loss: 1.8645
Training Epoch: 6 [27520/50048]	Loss: 1.7264
Training Epoch: 6 [27648/50048]	Loss: 1.8061
Training Epoch: 6 [27776/50048]	Loss: 1.7812
Training Epoch: 6 [27904/50048]	Loss: 2.2127
Training Epoch: 6 [28032/50048]	Loss: 2.0696
Training Epoch: 6 [28160/50048]	Loss: 2.1519
Training Epoch: 6 [28288/50048]	Loss: 2.1039
Training Epoch: 6 [28416/50048]	Loss: 2.0925
Training Epoch: 6 [28544/50048]	Loss: 2.0858
Training Epoch: 6 [28672/50048]	Loss: 1.9254
Training Epoch: 6 [28800/50048]	Loss: 1.7179
Training Epoch: 6 [28928/50048]	Loss: 1.8346
Training Epoch: 6 [29056/50048]	Loss: 2.1335
Training Epoch: 6 [29184/50048]	Loss: 2.0568
Training Epoch: 6 [29312/50048]	Loss: 2.0290
Training Epoch: 6 [29440/50048]	Loss: 2.1500
Training Epoch: 6 [29568/50048]	Loss: 1.9596
Training Epoch: 6 [29696/50048]	Loss: 1.9419
Training Epoch: 6 [29824/50048]	Loss: 2.0051
Training Epoch: 6 [29952/50048]	Loss: 2.0106
Training Epoch: 6 [30080/50048]	Loss: 1.8202
Training Epoch: 6 [30208/50048]	Loss: 1.8371
Training Epoch: 6 [30336/50048]	Loss: 2.1305
Training Epoch: 6 [30464/50048]	Loss: 2.0368
Training Epoch: 6 [30592/50048]	Loss: 2.0094
Training Epoch: 6 [30720/50048]	Loss: 1.9482
Training Epoch: 6 [30848/50048]	Loss: 2.1034
Training Epoch: 6 [30976/50048]	Loss: 1.8905
Training Epoch: 6 [31104/50048]	Loss: 2.0200
Training Epoch: 6 [31232/50048]	Loss: 1.9738
Training Epoch: 6 [31360/50048]	Loss: 1.9554
Training Epoch: 6 [31488/50048]	Loss: 1.9124
Training Epoch: 6 [31616/50048]	Loss: 1.9647
Training Epoch: 6 [31744/50048]	Loss: 1.9363
Training Epoch: 6 [31872/50048]	Loss: 2.0414
Training Epoch: 6 [32000/50048]	Loss: 1.6874
Training Epoch: 6 [32128/50048]	Loss: 1.9253
Training Epoch: 6 [32256/50048]	Loss: 1.8049
Training Epoch: 6 [32384/50048]	Loss: 1.7892
Training Epoch: 6 [32512/50048]	Loss: 2.0909
Training Epoch: 6 [32640/50048]	Loss: 1.9926
Training Epoch: 6 [32768/50048]	Loss: 1.8051
Training Epoch: 6 [32896/50048]	Loss: 1.9244
Training Epoch: 6 [33024/50048]	Loss: 1.8730
Training Epoch: 6 [33152/50048]	Loss: 1.8980
Training Epoch: 6 [33280/50048]	Loss: 2.0235
Training Epoch: 6 [33408/50048]	Loss: 1.8231
Training Epoch: 6 [33536/50048]	Loss: 1.7779
Training Epoch: 6 [33664/50048]	Loss: 2.0745
Training Epoch: 6 [33792/50048]	Loss: 1.6413
Training Epoch: 6 [33920/50048]	Loss: 2.0909
Training Epoch: 6 [34048/50048]	Loss: 1.8553
Training Epoch: 6 [34176/50048]	Loss: 2.0312
Training Epoch: 6 [34304/50048]	Loss: 1.7784
Training Epoch: 6 [34432/50048]	Loss: 1.8926
Training Epoch: 6 [34560/50048]	Loss: 1.8510
Training Epoch: 6 [34688/50048]	Loss: 1.7586
Training Epoch: 6 [34816/50048]	Loss: 1.9054
Training Epoch: 6 [34944/50048]	Loss: 1.7199
Training Epoch: 6 [35072/50048]	Loss: 1.8260
Training Epoch: 6 [35200/50048]	Loss: 2.0015
Training Epoch: 6 [35328/50048]	Loss: 2.0061
Training Epoch: 6 [35456/50048]	Loss: 1.8712
Training Epoch: 6 [35584/50048]	Loss: 1.9550
Training Epoch: 6 [35712/50048]	Loss: 1.8928
Training Epoch: 6 [35840/50048]	Loss: 1.6365
Training Epoch: 6 [35968/50048]	Loss: 1.9074
Training Epoch: 6 [36096/50048]	Loss: 1.9062
Training Epoch: 6 [36224/50048]	Loss: 1.8188
Training Epoch: 6 [36352/50048]	Loss: 1.9392
Training Epoch: 6 [36480/50048]	Loss: 1.9778
Training Epoch: 6 [36608/50048]	Loss: 2.0092
Training Epoch: 6 [36736/50048]	Loss: 1.9197
Training Epoch: 6 [36864/50048]	Loss: 1.8155
Training Epoch: 6 [36992/50048]	Loss: 1.8808
Training Epoch: 6 [37120/50048]	Loss: 2.1604
Training Epoch: 6 [37248/50048]	Loss: 1.9422
Training Epoch: 6 [37376/50048]	Loss: 1.7927
Training Epoch: 6 [37504/50048]	Loss: 1.7135
Training Epoch: 6 [37632/50048]	Loss: 1.9686
Training Epoch: 6 [37760/50048]	Loss: 1.9946
Training Epoch: 6 [37888/50048]	Loss: 1.8917
Training Epoch: 6 [38016/50048]	Loss: 1.7193
Training Epoch: 6 [38144/50048]	Loss: 1.7604
Training Epoch: 6 [38272/50048]	Loss: 1.8964
Training Epoch: 6 [38400/50048]	Loss: 1.9742
Training Epoch: 6 [38528/50048]	Loss: 1.9486
Training Epoch: 6 [38656/50048]	Loss: 1.6897
Training Epoch: 6 [38784/50048]	Loss: 2.1753
Training Epoch: 6 [38912/50048]	Loss: 1.6021
Training Epoch: 6 [39040/50048]	Loss: 1.6933
Training Epoch: 6 [39168/50048]	Loss: 1.6857
Training Epoch: 6 [39296/50048]	Loss: 1.9633
Training Epoch: 6 [39424/50048]	Loss: 1.8881
Training Epoch: 6 [39552/50048]	Loss: 1.7238
Training Epoch: 6 [39680/50048]	Loss: 1.5458
Training Epoch: 6 [39808/50048]	Loss: 1.8375
Training Epoch: 6 [39936/50048]	Loss: 1.8772
Training Epoch: 6 [40064/50048]	Loss: 1.8638
Training Epoch: 6 [40192/50048]	Loss: 1.9718
Training Epoch: 6 [40320/50048]	Loss: 1.8930
Training Epoch: 6 [40448/50048]	Loss: 2.0715
Training Epoch: 6 [40576/50048]	Loss: 2.0658
Training Epoch: 6 [40704/50048]	Loss: 1.9472
Training Epoch: 6 [40832/50048]	Loss: 1.8026
Training Epoch: 6 [40960/50048]	Loss: 2.1072
Training Epoch: 6 [41088/50048]	Loss: 1.8396
Training Epoch: 6 [41216/50048]	Loss: 1.8116
Training Epoch: 6 [41344/50048]	Loss: 1.8034
Training Epoch: 6 [41472/50048]	Loss: 2.1144
Training Epoch: 6 [41600/50048]	Loss: 1.9113
Training Epoch: 6 [41728/50048]	Loss: 1.9512
Training Epoch: 6 [41856/50048]	Loss: 1.9828
Training Epoch: 6 [41984/50048]	Loss: 2.0402
Training Epoch: 6 [42112/50048]	Loss: 1.9844
Training Epoch: 6 [42240/50048]	Loss: 1.8970
Training Epoch: 6 [42368/50048]	Loss: 2.0781
Training Epoch: 6 [42496/50048]	Loss: 2.1057
Training Epoch: 6 [42624/50048]	Loss: 1.8300
Training Epoch: 6 [42752/50048]	Loss: 1.7066
Training Epoch: 6 [42880/50048]	Loss: 1.6877
Training Epoch: 6 [43008/50048]	Loss: 2.2285
Training Epoch: 6 [43136/50048]	Loss: 1.9041
Training Epoch: 6 [43264/50048]	Loss: 1.9082
Training Epoch: 6 [43392/50048]	Loss: 2.2107
Training Epoch: 6 [43520/50048]	Loss: 1.9461
Training Epoch: 6 [43648/50048]	Loss: 1.9373
Training Epoch: 6 [43776/50048]	Loss: 1.9499
Training Epoch: 6 [43904/50048]	Loss: 1.8284
Training Epoch: 6 [44032/50048]	Loss: 1.9669
Training Epoch: 6 [44160/50048]	Loss: 1.9489
Training Epoch: 6 [44288/50048]	Loss: 1.7373
Training Epoch: 6 [44416/50048]	Loss: 2.0492
Training Epoch: 6 [44544/50048]	Loss: 2.0657
Training Epoch: 6 [44672/50048]	Loss: 1.8171
Training Epoch: 6 [44800/50048]	Loss: 1.9332
Training Epoch: 6 [44928/50048]	Loss: 1.8914
Training Epoch: 6 [45056/50048]	Loss: 1.8354
Training Epoch: 6 [45184/50048]	Loss: 2.2623
Training Epoch: 6 [45312/50048]	Loss: 1.6664
Training Epoch: 6 [45440/50048]	Loss: 1.7147
Training Epoch: 6 [45568/50048]	Loss: 2.0798
Training Epoch: 6 [45696/50048]	Loss: 1.9740
Training Epoch: 6 [45824/50048]	Loss: 1.7299
Training Epoch: 6 [45952/50048]	Loss: 1.8445
Training Epoch: 6 [46080/50048]	Loss: 1.8704
Training Epoch: 6 [46208/50048]	Loss: 1.9706
Training Epoch: 6 [46336/50048]	Loss: 2.1436
Training Epoch: 6 [46464/50048]	Loss: 2.0347
Training Epoch: 6 [46592/50048]	Loss: 2.0323
Training Epoch: 6 [46720/50048]	Loss: 1.7825
Training Epoch: 6 [46848/50048]	Loss: 1.9939
Training Epoch: 6 [46976/50048]	Loss: 1.9344
Training Epoch: 6 [47104/50048]	Loss: 2.2246
Training Epoch: 6 [47232/50048]	Loss: 1.7697
Training Epoch: 6 [47360/50048]	Loss: 1.9012
Training Epoch: 6 [47488/50048]	Loss: 1.9547
Training Epoch: 6 [47616/50048]	Loss: 1.8244
Training Epoch: 6 [47744/50048]	Loss: 2.2145
Training Epoch: 6 [47872/50048]	Loss: 2.1456
Training Epoch: 6 [48000/50048]	Loss: 1.8954
Training Epoch: 6 [48128/50048]	Loss: 1.5745
Training Epoch: 6 [48256/50048]	Loss: 2.3377
Training Epoch: 6 [48384/50048]	Loss: 1.8044
Training Epoch: 6 [48512/50048]	Loss: 1.9695
Training Epoch: 6 [48640/50048]	Loss: 2.0750
Training Epoch: 6 [48768/50048]	Loss: 1.8751
Training Epoch: 6 [48896/50048]	Loss: 1.8071
Training Epoch: 6 [49024/50048]	Loss: 1.8120
Training Epoch: 6 [49152/50048]	Loss: 1.9405
Training Epoch: 6 [49280/50048]	Loss: 1.7530
Training Epoch: 6 [49408/50048]	Loss: 1.8135
Training Epoch: 6 [49536/50048]	Loss: 2.0967
Training Epoch: 6 [49664/50048]	Loss: 2.0128
Training Epoch: 6 [49792/50048]	Loss: 1.9263
Training Epoch: 6 [49920/50048]	Loss: 2.0925
Training Epoch: 6 [50048/50048]	Loss: 2.3906
Validation Epoch: 6, Average loss: 0.0165, Accuracy: 0.4434
[Training Loop] Model's accuracy 0.44343354430379744 surpasses threshold 0.4! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5276
Profiling... [256/50048]	Loss: 1.6877
Profiling... [384/50048]	Loss: 1.8761
Profiling... [512/50048]	Loss: 1.6944
Profiling... [640/50048]	Loss: 2.0309
Profiling... [768/50048]	Loss: 1.9033
Profiling... [896/50048]	Loss: 1.7599
Profiling... [1024/50048]	Loss: 2.1626
Profiling... [1152/50048]	Loss: 1.6569
Profiling... [1280/50048]	Loss: 1.9144
Profiling... [1408/50048]	Loss: 1.4984
Profiling... [1536/50048]	Loss: 1.8255
Profiling... [1664/50048]	Loss: 1.8123
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 7, Average loss: 0.0149, Accuracy: 0.4845
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.80018255485192,
                        "time": 2.1719507039961172,
                        "accuracy": 0.4844738924050633,
                        "total_cost": 519.1451840145864
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6867
Profiling... [256/50048]	Loss: 1.9291
Profiling... [384/50048]	Loss: 1.6951
Profiling... [512/50048]	Loss: 1.4131
Profiling... [640/50048]	Loss: 1.7871
Profiling... [768/50048]	Loss: 1.8765
Profiling... [896/50048]	Loss: 1.6539
Profiling... [1024/50048]	Loss: 1.9280
Profiling... [1152/50048]	Loss: 1.8837
Profiling... [1280/50048]	Loss: 1.5367
Profiling... [1408/50048]	Loss: 1.8438
Profiling... [1536/50048]	Loss: 1.6013
Profiling... [1664/50048]	Loss: 1.5706
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 7, Average loss: 0.0143, Accuracy: 0.5004
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.8071227020186,
                        "time": 2.197070148999046,
                        "accuracy": 0.5003955696202531,
                        "total_cost": 508.47047371575434
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6233
Profiling... [256/50048]	Loss: 1.6851
Profiling... [384/50048]	Loss: 1.8500
Profiling... [512/50048]	Loss: 1.6407
Profiling... [640/50048]	Loss: 1.5722
Profiling... [768/50048]	Loss: 2.0936
Profiling... [896/50048]	Loss: 1.8694
Profiling... [1024/50048]	Loss: 1.5280
Profiling... [1152/50048]	Loss: 2.0517
Profiling... [1280/50048]	Loss: 1.9204
Profiling... [1408/50048]	Loss: 1.7049
Profiling... [1536/50048]	Loss: 1.6395
Profiling... [1664/50048]	Loss: 1.8966
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 7, Average loss: 0.0143, Accuracy: 0.4977
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.80968926318329,
                        "time": 2.225817370999721,
                        "accuracy": 0.4977254746835443,
                        "total_cost": 517.8983821472372
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9390
Profiling... [256/50048]	Loss: 1.6036
Profiling... [384/50048]	Loss: 1.8225
Profiling... [512/50048]	Loss: 1.6942
Profiling... [640/50048]	Loss: 1.7416
Profiling... [768/50048]	Loss: 1.7540
Profiling... [896/50048]	Loss: 1.8899
Profiling... [1024/50048]	Loss: 1.9405
Profiling... [1152/50048]	Loss: 1.6720
Profiling... [1280/50048]	Loss: 1.4084
Profiling... [1408/50048]	Loss: 1.9400
Profiling... [1536/50048]	Loss: 2.0588
Profiling... [1664/50048]	Loss: 1.8860
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 7, Average loss: 0.0146, Accuracy: 0.4892
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.79138308415823,
                        "time": 2.5898422579994076,
                        "accuracy": 0.4892207278481013,
                        "total_cost": 612.9777418520608
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7591
Profiling... [256/50048]	Loss: 1.9361
Profiling... [384/50048]	Loss: 1.7456
Profiling... [512/50048]	Loss: 1.8651
Profiling... [640/50048]	Loss: 1.8559
Profiling... [768/50048]	Loss: 1.7729
Profiling... [896/50048]	Loss: 1.8755
Profiling... [1024/50048]	Loss: 1.6137
Profiling... [1152/50048]	Loss: 1.8735
Profiling... [1280/50048]	Loss: 1.7587
Profiling... [1408/50048]	Loss: 1.9036
Profiling... [1536/50048]	Loss: 1.9198
Profiling... [1664/50048]	Loss: 1.7534
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 7, Average loss: 0.0146, Accuracy: 0.4910
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.76375920470231,
                        "time": 2.1809482800017577,
                        "accuracy": 0.4910007911392405,
                        "total_cost": 514.2044087917469
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8870
Profiling... [256/50048]	Loss: 1.8072
Profiling... [384/50048]	Loss: 1.6843
Profiling... [512/50048]	Loss: 1.7267
Profiling... [640/50048]	Loss: 1.7056
Profiling... [768/50048]	Loss: 1.8740
Profiling... [896/50048]	Loss: 1.4924
Profiling... [1024/50048]	Loss: 1.9697
Profiling... [1152/50048]	Loss: 1.7401
Profiling... [1280/50048]	Loss: 1.5874
Profiling... [1408/50048]	Loss: 1.9272
Profiling... [1536/50048]	Loss: 1.8095
Profiling... [1664/50048]	Loss: 1.8962
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 7, Average loss: 0.0144, Accuracy: 0.4984
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.76786283559458,
                        "time": 2.1819214450006257,
                        "accuracy": 0.49841772151898733,
                        "total_cost": 506.7965516817043
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8945
Profiling... [256/50048]	Loss: 1.7996
Profiling... [384/50048]	Loss: 1.9110
Profiling... [512/50048]	Loss: 1.6933
Profiling... [640/50048]	Loss: 1.8795
Profiling... [768/50048]	Loss: 1.6237
Profiling... [896/50048]	Loss: 1.5474
Profiling... [1024/50048]	Loss: 1.8942
Profiling... [1152/50048]	Loss: 1.7669
Profiling... [1280/50048]	Loss: 1.8288
Profiling... [1408/50048]	Loss: 1.9794
Profiling... [1536/50048]	Loss: 1.9945
Profiling... [1664/50048]	Loss: 1.8499
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 7, Average loss: 0.0145, Accuracy: 0.4967
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.77156565279951,
                        "time": 2.21826262200193,
                        "accuracy": 0.4967365506329114,
                        "total_cost": 516.9978662754609
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7809
Profiling... [256/50048]	Loss: 1.7920
Profiling... [384/50048]	Loss: 2.0264
Profiling... [512/50048]	Loss: 2.0253
Profiling... [640/50048]	Loss: 1.7981
Profiling... [768/50048]	Loss: 2.0319
Profiling... [896/50048]	Loss: 1.7718
Profiling... [1024/50048]	Loss: 1.7945
Profiling... [1152/50048]	Loss: 1.7063
Profiling... [1280/50048]	Loss: 1.9498
Profiling... [1408/50048]	Loss: 1.7351
Profiling... [1536/50048]	Loss: 1.8870
Profiling... [1664/50048]	Loss: 1.8819
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 7, Average loss: 0.0144, Accuracy: 0.4966
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.75076743160442,
                        "time": 2.5897916960020666,
                        "accuracy": 0.4966376582278481,
                        "total_cost": 603.5997700414142
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7024
Profiling... [256/50048]	Loss: 1.8176
Profiling... [384/50048]	Loss: 1.9551
Profiling... [512/50048]	Loss: 2.0718
Profiling... [640/50048]	Loss: 1.5941
Profiling... [768/50048]	Loss: 1.8022
Profiling... [896/50048]	Loss: 1.5337
Profiling... [1024/50048]	Loss: 1.7271
Profiling... [1152/50048]	Loss: 1.9131
Profiling... [1280/50048]	Loss: 1.8093
Profiling... [1408/50048]	Loss: 1.8489
Profiling... [1536/50048]	Loss: 1.6034
Profiling... [1664/50048]	Loss: 1.7416
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 7, Average loss: 0.0146, Accuracy: 0.4904
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.72106911059113,
                        "time": 2.1773549980061944,
                        "accuracy": 0.4904074367088608,
                        "total_cost": 513.7887995612715
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5058
Profiling... [256/50048]	Loss: 1.7189
Profiling... [384/50048]	Loss: 1.5494
Profiling... [512/50048]	Loss: 1.3988
Profiling... [640/50048]	Loss: 1.8388
Profiling... [768/50048]	Loss: 2.1106
Profiling... [896/50048]	Loss: 1.9878
Profiling... [1024/50048]	Loss: 1.8621
Profiling... [1152/50048]	Loss: 1.9200
Profiling... [1280/50048]	Loss: 1.6006
Profiling... [1408/50048]	Loss: 2.1357
Profiling... [1536/50048]	Loss: 1.7202
Profiling... [1664/50048]	Loss: 1.6815
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 7, Average loss: 0.0144, Accuracy: 0.4961
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.72803956798188,
                        "time": 2.186297212007048,
                        "accuracy": 0.49614319620253167,
                        "total_cost": 509.9654539155183
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7683
Profiling... [256/50048]	Loss: 1.8703
Profiling... [384/50048]	Loss: 1.7465
Profiling... [512/50048]	Loss: 1.9429
Profiling... [640/50048]	Loss: 1.8153
Profiling... [768/50048]	Loss: 1.6940
Profiling... [896/50048]	Loss: 1.7820
Profiling... [1024/50048]	Loss: 1.7670
Profiling... [1152/50048]	Loss: 1.7537
Profiling... [1280/50048]	Loss: 1.7095
Profiling... [1408/50048]	Loss: 1.8503
Profiling... [1536/50048]	Loss: 1.6993
Profiling... [1664/50048]	Loss: 1.5960
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 7, Average loss: 0.0145, Accuracy: 0.4955
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.73172999034894,
                        "time": 2.226309552999737,
                        "accuracy": 0.4955498417721519,
                        "total_cost": 519.9369152077476
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8163
Profiling... [256/50048]	Loss: 1.9647
Profiling... [384/50048]	Loss: 1.4742
Profiling... [512/50048]	Loss: 1.7975
Profiling... [640/50048]	Loss: 1.8882
Profiling... [768/50048]	Loss: 1.8441
Profiling... [896/50048]	Loss: 1.6832
Profiling... [1024/50048]	Loss: 1.7506
Profiling... [1152/50048]	Loss: 1.6883
Profiling... [1280/50048]	Loss: 1.8137
Profiling... [1408/50048]	Loss: 1.6035
Profiling... [1536/50048]	Loss: 1.8296
Profiling... [1664/50048]	Loss: 1.7224
Profile done
epoch 1 train time consumed: 3.80s
Validation Epoch: 7, Average loss: 0.0148, Accuracy: 0.4887
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.71224376379101,
                        "time": 2.5920125120028388,
                        "accuracy": 0.48872626582278483,
                        "total_cost": 613.6923766982975
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1228
Profiling... [256/50048]	Loss: 2.0471
Profiling... [384/50048]	Loss: 2.0191
Profiling... [512/50048]	Loss: 1.7000
Profiling... [640/50048]	Loss: 1.7524
Profiling... [768/50048]	Loss: 1.7600
Profiling... [896/50048]	Loss: 1.8132
Profiling... [1024/50048]	Loss: 1.8841
Profiling... [1152/50048]	Loss: 1.8433
Profiling... [1280/50048]	Loss: 1.9026
Profiling... [1408/50048]	Loss: 2.0249
Profiling... [1536/50048]	Loss: 1.9207
Profiling... [1664/50048]	Loss: 1.6515
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 7, Average loss: 0.0180, Accuracy: 0.4197
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.6840899486185,
                        "time": 2.1843481329997303,
                        "accuracy": 0.4196993670886076,
                        "total_cost": 602.08412428625
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8287
Profiling... [256/50048]	Loss: 1.8168
Profiling... [384/50048]	Loss: 1.9075
Profiling... [512/50048]	Loss: 1.8124
Profiling... [640/50048]	Loss: 1.7094
Profiling... [768/50048]	Loss: 1.7302
Profiling... [896/50048]	Loss: 2.0746
Profiling... [1024/50048]	Loss: 1.8870
Profiling... [1152/50048]	Loss: 1.6413
Profiling... [1280/50048]	Loss: 2.0424
Profiling... [1408/50048]	Loss: 1.7939
Profiling... [1536/50048]	Loss: 1.7301
Profiling... [1664/50048]	Loss: 1.8132
Profile done
epoch 1 train time consumed: 3.43s
Validation Epoch: 7, Average loss: 0.0191, Accuracy: 0.3946
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.68955020427367,
                        "time": 2.1914407850053976,
                        "accuracy": 0.39458069620253167,
                        "total_cost": 642.5220522862168
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7769
Profiling... [256/50048]	Loss: 1.9254
Profiling... [384/50048]	Loss: 1.7189
Profiling... [512/50048]	Loss: 1.5599
Profiling... [640/50048]	Loss: 2.0759
Profiling... [768/50048]	Loss: 1.7501
Profiling... [896/50048]	Loss: 2.0603
Profiling... [1024/50048]	Loss: 1.9381
Profiling... [1152/50048]	Loss: 1.8060
Profiling... [1280/50048]	Loss: 1.7863
Profiling... [1408/50048]	Loss: 1.8171
Profiling... [1536/50048]	Loss: 1.9147
Profiling... [1664/50048]	Loss: 1.9903
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 7, Average loss: 0.0167, Accuracy: 0.4435
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.69473277714587,
                        "time": 2.2196986159979133,
                        "accuracy": 0.4435324367088608,
                        "total_cost": 579.0048640619494
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8990
Profiling... [256/50048]	Loss: 1.7732
Profiling... [384/50048]	Loss: 1.9202
Profiling... [512/50048]	Loss: 1.8598
Profiling... [640/50048]	Loss: 1.8218
Profiling... [768/50048]	Loss: 1.7369
Profiling... [896/50048]	Loss: 1.9015
Profiling... [1024/50048]	Loss: 2.0353
Profiling... [1152/50048]	Loss: 2.1059
Profiling... [1280/50048]	Loss: 1.9884
Profiling... [1408/50048]	Loss: 1.8974
Profiling... [1536/50048]	Loss: 2.2159
Profiling... [1664/50048]	Loss: 1.7908
Profile done
epoch 1 train time consumed: 3.84s
Validation Epoch: 7, Average loss: 0.0166, Accuracy: 0.4402
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.67415887895878,
                        "time": 2.6084384940040763,
                        "accuracy": 0.44017009493670883,
                        "total_cost": 685.4825719698301
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7963
Profiling... [256/50048]	Loss: 2.0231
Profiling... [384/50048]	Loss: 1.5702
Profiling... [512/50048]	Loss: 2.1695
Profiling... [640/50048]	Loss: 1.7258
Profiling... [768/50048]	Loss: 1.8882
Profiling... [896/50048]	Loss: 1.8340
Profiling... [1024/50048]	Loss: 1.9939
Profiling... [1152/50048]	Loss: 1.8336
Profiling... [1280/50048]	Loss: 1.8784
Profiling... [1408/50048]	Loss: 1.8704
Profiling... [1536/50048]	Loss: 1.7509
Profiling... [1664/50048]	Loss: 2.0059
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 7, Average loss: 0.0186, Accuracy: 0.4011
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.64464369147122,
                        "time": 2.19548656000552,
                        "accuracy": 0.40110759493670883,
                        "total_cost": 632.9879168738123
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8957
Profiling... [256/50048]	Loss: 1.8432
Profiling... [384/50048]	Loss: 1.8199
Profiling... [512/50048]	Loss: 1.8407
Profiling... [640/50048]	Loss: 1.6670
Profiling... [768/50048]	Loss: 2.1338
Profiling... [896/50048]	Loss: 1.8232
Profiling... [1024/50048]	Loss: 1.8988
Profiling... [1152/50048]	Loss: 1.9585
Profiling... [1280/50048]	Loss: 2.0773
Profiling... [1408/50048]	Loss: 1.7840
Profiling... [1536/50048]	Loss: 1.8347
Profiling... [1664/50048]	Loss: 1.6233
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 7, Average loss: 0.0177, Accuracy: 0.4220
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.6514959538354,
                        "time": 2.1839126679988112,
                        "accuracy": 0.4219738924050633,
                        "total_cost": 598.5506962220863
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7754
Profiling... [256/50048]	Loss: 1.6435
Profiling... [384/50048]	Loss: 2.0229
Profiling... [512/50048]	Loss: 1.7512
Profiling... [640/50048]	Loss: 1.7182
Profiling... [768/50048]	Loss: 1.5907
Profiling... [896/50048]	Loss: 1.6858
Profiling... [1024/50048]	Loss: 1.9625
Profiling... [1152/50048]	Loss: 1.8224
Profiling... [1280/50048]	Loss: 2.0617
Profiling... [1408/50048]	Loss: 1.8959
Profiling... [1536/50048]	Loss: 1.9053
Profiling... [1664/50048]	Loss: 1.9967
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 7, Average loss: 0.0203, Accuracy: 0.3733
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.65505671693954,
                        "time": 2.2305741689997376,
                        "accuracy": 0.37331882911392406,
                        "total_cost": 691.0371562005494
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8454
Profiling... [256/50048]	Loss: 1.9278
Profiling... [384/50048]	Loss: 2.0624
Profiling... [512/50048]	Loss: 2.0281
Profiling... [640/50048]	Loss: 2.2410
Profiling... [768/50048]	Loss: 1.8664
Profiling... [896/50048]	Loss: 1.7920
Profiling... [1024/50048]	Loss: 1.6308
Profiling... [1152/50048]	Loss: 1.7589
Profiling... [1280/50048]	Loss: 1.7560
Profiling... [1408/50048]	Loss: 1.7160
Profiling... [1536/50048]	Loss: 1.8555
Profiling... [1664/50048]	Loss: 1.8094
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 7, Average loss: 0.0175, Accuracy: 0.4238
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.63556220733928,
                        "time": 2.5683797219971893,
                        "accuracy": 0.42375395569620256,
                        "total_cost": 700.8690517758777
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8528
Profiling... [256/50048]	Loss: 1.8430
Profiling... [384/50048]	Loss: 1.6516
Profiling... [512/50048]	Loss: 1.9200
Profiling... [640/50048]	Loss: 2.0163
Profiling... [768/50048]	Loss: 1.7970
Profiling... [896/50048]	Loss: 1.6826
Profiling... [1024/50048]	Loss: 1.8612
Profiling... [1152/50048]	Loss: 1.8751
Profiling... [1280/50048]	Loss: 1.9814
Profiling... [1408/50048]	Loss: 1.8386
Profiling... [1536/50048]	Loss: 1.9561
Profiling... [1664/50048]	Loss: 1.7614
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 7, Average loss: 0.0164, Accuracy: 0.4468
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.60643885359501,
                        "time": 2.174251065000135,
                        "accuracy": 0.4467958860759494,
                        "total_cost": 562.5777466437426
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9133
Profiling... [256/50048]	Loss: 1.8958
Profiling... [384/50048]	Loss: 1.7415
Profiling... [512/50048]	Loss: 1.8149
Profiling... [640/50048]	Loss: 2.0312
Profiling... [768/50048]	Loss: 1.7799
Profiling... [896/50048]	Loss: 2.0767
Profiling... [1024/50048]	Loss: 1.8120
Profiling... [1152/50048]	Loss: 1.7491
Profiling... [1280/50048]	Loss: 2.2536
Profiling... [1408/50048]	Loss: 2.0664
Profiling... [1536/50048]	Loss: 1.8932
Profiling... [1664/50048]	Loss: 2.1909
Profile done
epoch 1 train time consumed: 3.36s
Validation Epoch: 7, Average loss: 0.0166, Accuracy: 0.4445
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.61417021712201,
                        "time": 2.1911785990014323,
                        "accuracy": 0.44452136075949367,
                        "total_cost": 569.8967876104615
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1151
Profiling... [256/50048]	Loss: 1.7432
Profiling... [384/50048]	Loss: 1.6407
Profiling... [512/50048]	Loss: 2.0016
Profiling... [640/50048]	Loss: 1.7237
Profiling... [768/50048]	Loss: 1.4739
Profiling... [896/50048]	Loss: 1.8108
Profiling... [1024/50048]	Loss: 1.5619
Profiling... [1152/50048]	Loss: 1.6876
Profiling... [1280/50048]	Loss: 2.2073
Profiling... [1408/50048]	Loss: 1.8102
Profiling... [1536/50048]	Loss: 2.0620
Profiling... [1664/50048]	Loss: 1.8635
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 7, Average loss: 0.0187, Accuracy: 0.4070
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.61800802707707,
                        "time": 2.254855518003751,
                        "accuracy": 0.40704113924050633,
                        "total_cost": 640.4804778870696
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0601
Profiling... [256/50048]	Loss: 1.9653
Profiling... [384/50048]	Loss: 1.5920
Profiling... [512/50048]	Loss: 2.2218
Profiling... [640/50048]	Loss: 1.8669
Profiling... [768/50048]	Loss: 1.8425
Profiling... [896/50048]	Loss: 1.8406
Profiling... [1024/50048]	Loss: 1.8471
Profiling... [1152/50048]	Loss: 1.8889
Profiling... [1280/50048]	Loss: 1.5875
Profiling... [1408/50048]	Loss: 2.1877
Profiling... [1536/50048]	Loss: 2.0396
Profiling... [1664/50048]	Loss: 2.1822
Profile done
epoch 1 train time consumed: 3.81s
Validation Epoch: 7, Average loss: 0.0167, Accuracy: 0.4405
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.59864230755453,
                        "time": 2.5733798279979965,
                        "accuracy": 0.4404667721518987,
                        "total_cost": 675.3726570675991
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8465
Profiling... [256/50048]	Loss: 1.7933
Profiling... [384/50048]	Loss: 2.1227
Profiling... [512/50048]	Loss: 2.2166
Profiling... [640/50048]	Loss: 2.0346
Profiling... [768/50048]	Loss: 2.0988
Profiling... [896/50048]	Loss: 2.2490
Profiling... [1024/50048]	Loss: 1.8589
Profiling... [1152/50048]	Loss: 2.2330
Profiling... [1280/50048]	Loss: 2.3480
Profiling... [1408/50048]	Loss: 1.9674
Profiling... [1536/50048]	Loss: 1.9718
Profiling... [1664/50048]	Loss: 2.2832
Profile done
epoch 1 train time consumed: 3.41s
Validation Epoch: 7, Average loss: 0.0303, Accuracy: 0.2330
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.57072360428093,
                        "time": 2.199531975995342,
                        "accuracy": 0.23299050632911392,
                        "total_cost": 1091.0380258046212
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7499
Profiling... [256/50048]	Loss: 1.8594
Profiling... [384/50048]	Loss: 2.0357
Profiling... [512/50048]	Loss: 2.1984
Profiling... [640/50048]	Loss: 1.8427
Profiling... [768/50048]	Loss: 1.8803
Profiling... [896/50048]	Loss: 2.0809
Profiling... [1024/50048]	Loss: 2.4297
Profiling... [1152/50048]	Loss: 1.9926
Profiling... [1280/50048]	Loss: 1.8611
Profiling... [1408/50048]	Loss: 1.9388
Profiling... [1536/50048]	Loss: 1.9370
Profiling... [1664/50048]	Loss: 2.1032
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 7, Average loss: 0.0369, Accuracy: 0.1805
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.57710897911643,
                        "time": 2.180501597998955,
                        "accuracy": 0.18047863924050633,
                        "total_cost": 1396.3761688452528
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6761
Profiling... [256/50048]	Loss: 1.8896
Profiling... [384/50048]	Loss: 1.9780
Profiling... [512/50048]	Loss: 2.0892
Profiling... [640/50048]	Loss: 1.9839
Profiling... [768/50048]	Loss: 1.8950
Profiling... [896/50048]	Loss: 2.1509
Profiling... [1024/50048]	Loss: 1.9726
Profiling... [1152/50048]	Loss: 2.0651
Profiling... [1280/50048]	Loss: 1.9673
Profiling... [1408/50048]	Loss: 1.9387
Profiling... [1536/50048]	Loss: 2.0210
Profiling... [1664/50048]	Loss: 2.0961
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 7, Average loss: 0.0297, Accuracy: 0.2316
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.57907604716341,
                        "time": 2.2387513689973275,
                        "accuracy": 0.23160601265822786,
                        "total_cost": 1117.2111283218926
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9428
Profiling... [256/50048]	Loss: 1.9494
Profiling... [384/50048]	Loss: 1.8769
Profiling... [512/50048]	Loss: 2.0492
Profiling... [640/50048]	Loss: 2.1637
Profiling... [768/50048]	Loss: 2.1227
Profiling... [896/50048]	Loss: 2.0895
Profiling... [1024/50048]	Loss: 2.0582
Profiling... [1152/50048]	Loss: 2.0546
Profiling... [1280/50048]	Loss: 2.1845
Profiling... [1408/50048]	Loss: 2.3351
Profiling... [1536/50048]	Loss: 2.4185
Profiling... [1664/50048]	Loss: 2.3948
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 7, Average loss: 0.0316, Accuracy: 0.2063
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.56314601660601,
                        "time": 2.578298560998519,
                        "accuracy": 0.2062895569620253,
                        "total_cost": 1444.3595568627165
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6408
Profiling... [256/50048]	Loss: 1.8415
Profiling... [384/50048]	Loss: 1.9413
Profiling... [512/50048]	Loss: 1.9389
Profiling... [640/50048]	Loss: 2.0625
Profiling... [768/50048]	Loss: 2.0689
Profiling... [896/50048]	Loss: 1.9030
Profiling... [1024/50048]	Loss: 1.8622
Profiling... [1152/50048]	Loss: 2.0925
Profiling... [1280/50048]	Loss: 1.7930
Profiling... [1408/50048]	Loss: 1.9482
Profiling... [1536/50048]	Loss: 1.7893
Profiling... [1664/50048]	Loss: 2.3136
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 7, Average loss: 0.0526, Accuracy: 0.1250
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.53390447271357,
                        "time": 2.1764684039953863,
                        "accuracy": 0.125,
                        "total_cost": 2011.6471414006587
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7783
Profiling... [256/50048]	Loss: 1.7790
Profiling... [384/50048]	Loss: 1.6677
Profiling... [512/50048]	Loss: 2.1121
Profiling... [640/50048]	Loss: 2.0582
Profiling... [768/50048]	Loss: 1.8510
Profiling... [896/50048]	Loss: 2.1723
Profiling... [1024/50048]	Loss: 2.2423
Profiling... [1152/50048]	Loss: 2.1980
Profiling... [1280/50048]	Loss: 2.0828
Profiling... [1408/50048]	Loss: 2.0473
Profiling... [1536/50048]	Loss: 2.1221
Profiling... [1664/50048]	Loss: 2.0293
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 7, Average loss: 0.0323, Accuracy: 0.2181
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.54084773151271,
                        "time": 2.1790492849977454,
                        "accuracy": 0.21805775316455697,
                        "total_cost": 1154.5987151733555
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6718
Profiling... [256/50048]	Loss: 2.2501
Profiling... [384/50048]	Loss: 2.0188
Profiling... [512/50048]	Loss: 2.0042
Profiling... [640/50048]	Loss: 1.8994
Profiling... [768/50048]	Loss: 2.2156
Profiling... [896/50048]	Loss: 2.0571
Profiling... [1024/50048]	Loss: 1.9279
Profiling... [1152/50048]	Loss: 2.0386
Profiling... [1280/50048]	Loss: 2.0861
Profiling... [1408/50048]	Loss: 2.1286
Profiling... [1536/50048]	Loss: 2.0948
Profiling... [1664/50048]	Loss: 2.1619
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 7, Average loss: 0.0282, Accuracy: 0.2394
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.54394250871907,
                        "time": 2.237718808995851,
                        "accuracy": 0.23941851265822786,
                        "total_cost": 1079.9284088210227
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8347
Profiling... [256/50048]	Loss: 1.9879
Profiling... [384/50048]	Loss: 2.1350
Profiling... [512/50048]	Loss: 1.8548
Profiling... [640/50048]	Loss: 2.0601
Profiling... [768/50048]	Loss: 2.2645
Profiling... [896/50048]	Loss: 2.1934
Profiling... [1024/50048]	Loss: 2.0944
Profiling... [1152/50048]	Loss: 2.1663
Profiling... [1280/50048]	Loss: 2.0082
Profiling... [1408/50048]	Loss: 2.2437
Profiling... [1536/50048]	Loss: 2.2759
Profiling... [1664/50048]	Loss: 2.1375
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 7, Average loss: 0.0321, Accuracy: 0.2188
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.52415612526806,
                        "time": 2.610703452002781,
                        "accuracy": 0.21884889240506328,
                        "total_cost": 1378.1166990222691
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7434
Profiling... [256/50048]	Loss: 1.7923
Profiling... [384/50048]	Loss: 2.1564
Profiling... [512/50048]	Loss: 1.9176
Profiling... [640/50048]	Loss: 2.0104
Profiling... [768/50048]	Loss: 2.0242
Profiling... [896/50048]	Loss: 2.1512
Profiling... [1024/50048]	Loss: 1.8593
Profiling... [1152/50048]	Loss: 1.9798
Profiling... [1280/50048]	Loss: 2.0658
Profiling... [1408/50048]	Loss: 2.2330
Profiling... [1536/50048]	Loss: 2.0582
Profiling... [1664/50048]	Loss: 2.0594
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 7, Average loss: 0.0289, Accuracy: 0.2263
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.49630789842747,
                        "time": 2.1945255219980027,
                        "accuracy": 0.22626582278481014,
                        "total_cost": 1120.1850648946265
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.0847
Profiling... [256/50048]	Loss: 1.6447
Profiling... [384/50048]	Loss: 1.8712
Profiling... [512/50048]	Loss: 1.7786
Profiling... [640/50048]	Loss: 1.9233
Profiling... [768/50048]	Loss: 2.0855
Profiling... [896/50048]	Loss: 2.0969
Profiling... [1024/50048]	Loss: 2.0382
Profiling... [1152/50048]	Loss: 2.2691
Profiling... [1280/50048]	Loss: 2.0266
Profiling... [1408/50048]	Loss: 2.4441
Profiling... [1536/50048]	Loss: 2.2799
Profiling... [1664/50048]	Loss: 2.0314
Profile done
epoch 1 train time consumed: 3.40s
Validation Epoch: 7, Average loss: 0.0313, Accuracy: 0.1973
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.50389280603073,
                        "time": 2.174616229000094,
                        "accuracy": 0.19729034810126583,
                        "total_cost": 1273.1319206743801
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5654
Profiling... [256/50048]	Loss: 1.8615
Profiling... [384/50048]	Loss: 1.8938
Profiling... [512/50048]	Loss: 2.0894
Profiling... [640/50048]	Loss: 2.2339
Profiling... [768/50048]	Loss: 1.8849
Profiling... [896/50048]	Loss: 2.1532
Profiling... [1024/50048]	Loss: 1.8540
Profiling... [1152/50048]	Loss: 2.0316
Profiling... [1280/50048]	Loss: 1.7055
Profiling... [1408/50048]	Loss: 1.9853
Profiling... [1536/50048]	Loss: 1.9183
Profiling... [1664/50048]	Loss: 2.0211
Profile done
epoch 1 train time consumed: 3.40s
Validation Epoch: 7, Average loss: 0.0302, Accuracy: 0.2050
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.50699115489671,
                        "time": 2.253287391999038,
                        "accuracy": 0.20500395569620253,
                        "total_cost": 1269.587437828617
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6445
Profiling... [256/50048]	Loss: 1.8767
Profiling... [384/50048]	Loss: 1.5956
Profiling... [512/50048]	Loss: 2.0572
Profiling... [640/50048]	Loss: 2.0659
Profiling... [768/50048]	Loss: 2.1328
Profiling... [896/50048]	Loss: 2.2523
Profiling... [1024/50048]	Loss: 2.3489
Profiling... [1152/50048]	Loss: 1.9941
Profiling... [1280/50048]	Loss: 2.1406
Profiling... [1408/50048]	Loss: 2.1754
Profiling... [1536/50048]	Loss: 2.1494
Profiling... [1664/50048]	Loss: 2.0826
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 7, Average loss: 0.0356, Accuracy: 0.1825
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.49005427469635,
                        "time": 2.570179353999265,
                        "accuracy": 0.18245648734177214,
                        "total_cost": 1626.8544759006872
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8203
Profiling... [512/50176]	Loss: 1.9053
Profiling... [768/50176]	Loss: 1.6958
Profiling... [1024/50176]	Loss: 1.7746
Profiling... [1280/50176]	Loss: 1.7405
Profiling... [1536/50176]	Loss: 1.8206
Profiling... [1792/50176]	Loss: 1.6489
Profiling... [2048/50176]	Loss: 1.8938
Profiling... [2304/50176]	Loss: 1.7041
Profiling... [2560/50176]	Loss: 1.8586
Profiling... [2816/50176]	Loss: 1.7596
Profiling... [3072/50176]	Loss: 1.8368
Profiling... [3328/50176]	Loss: 1.8941
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 7, Average loss: 0.0071, Accuracy: 0.4972
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.4699620988516,
                        "time": 2.5166775480029173,
                        "accuracy": 0.49716796875,
                        "total_cost": 584.5120348633233
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7725
Profiling... [512/50176]	Loss: 1.5949
Profiling... [768/50176]	Loss: 1.8572
Profiling... [1024/50176]	Loss: 1.6726
Profiling... [1280/50176]	Loss: 1.8205
Profiling... [1536/50176]	Loss: 1.9295
Profiling... [1792/50176]	Loss: 1.8176
Profiling... [2048/50176]	Loss: 1.6342
Profiling... [2304/50176]	Loss: 1.9066
Profiling... [2560/50176]	Loss: 1.6535
Profiling... [2816/50176]	Loss: 1.8089
Profiling... [3072/50176]	Loss: 1.6464
Profiling... [3328/50176]	Loss: 1.8810
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 7, Average loss: 0.0071, Accuracy: 0.4946
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.48326497931271,
                        "time": 2.393826015999366,
                        "accuracy": 0.49462890625,
                        "total_cost": 558.8974696523355
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7555
Profiling... [512/50176]	Loss: 1.7461
Profiling... [768/50176]	Loss: 1.7650
Profiling... [1024/50176]	Loss: 1.7392
Profiling... [1280/50176]	Loss: 1.6882
Profiling... [1536/50176]	Loss: 1.7975
Profiling... [1792/50176]	Loss: 1.8779
Profiling... [2048/50176]	Loss: 1.8241
Profiling... [2304/50176]	Loss: 1.6943
Profiling... [2560/50176]	Loss: 1.7580
Profiling... [2816/50176]	Loss: 1.8949
Profiling... [3072/50176]	Loss: 1.7849
Profiling... [3328/50176]	Loss: 1.6021
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 7, Average loss: 0.0071, Accuracy: 0.4948
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.48994694372949,
                        "time": 2.561000027002592,
                        "accuracy": 0.49482421875,
                        "total_cost": 597.7269220746264
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7133
Profiling... [512/50176]	Loss: 1.7653
Profiling... [768/50176]	Loss: 1.7438
Profiling... [1024/50176]	Loss: 1.8708
Profiling... [1280/50176]	Loss: 1.7779
Profiling... [1536/50176]	Loss: 1.8829
Profiling... [1792/50176]	Loss: 1.8575
Profiling... [2048/50176]	Loss: 1.7023
Profiling... [2304/50176]	Loss: 1.8064
Profiling... [2560/50176]	Loss: 1.9281
Profiling... [2816/50176]	Loss: 1.8602
Profiling... [3072/50176]	Loss: 1.8505
Profiling... [3328/50176]	Loss: 1.8485
Profile done
epoch 1 train time consumed: 4.50s
Validation Epoch: 7, Average loss: 0.0072, Accuracy: 0.4950
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.46611572499434,
                        "time": 3.025466981998761,
                        "accuracy": 0.49501953125,
                        "total_cost": 705.7073481191422
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8493
Profiling... [512/50176]	Loss: 1.9827
Profiling... [768/50176]	Loss: 1.6809
Profiling... [1024/50176]	Loss: 1.7700
Profiling... [1280/50176]	Loss: 1.7993
Profiling... [1536/50176]	Loss: 1.7475
Profiling... [1792/50176]	Loss: 1.9088
Profiling... [2048/50176]	Loss: 1.6363
Profiling... [2304/50176]	Loss: 1.7054
Profiling... [2560/50176]	Loss: 1.7828
Profiling... [2816/50176]	Loss: 1.6736
Profiling... [3072/50176]	Loss: 1.7154
Profiling... [3328/50176]	Loss: 1.8570
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 7, Average loss: 0.0071, Accuracy: 0.4936
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.44357597053327,
                        "time": 2.400663042004453,
                        "accuracy": 0.4935546875,
                        "total_cost": 561.520604075496
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7812
Profiling... [512/50176]	Loss: 1.7928
Profiling... [768/50176]	Loss: 1.8160
Profiling... [1024/50176]	Loss: 1.7950
Profiling... [1280/50176]	Loss: 1.8823
Profiling... [1536/50176]	Loss: 1.9772
Profiling... [1792/50176]	Loss: 1.6915
Profiling... [2048/50176]	Loss: 1.7291
Profiling... [2304/50176]	Loss: 1.6850
Profiling... [2560/50176]	Loss: 1.6704
Profiling... [2816/50176]	Loss: 1.7326
Profiling... [3072/50176]	Loss: 1.7803
Profiling... [3328/50176]	Loss: 1.6957
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 7, Average loss: 0.0072, Accuracy: 0.4936
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.45941063377191,
                        "time": 2.39227627600485,
                        "accuracy": 0.4935546875,
                        "total_cost": 559.6356713777025
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8274
Profiling... [512/50176]	Loss: 1.8530
Profiling... [768/50176]	Loss: 1.8005
Profiling... [1024/50176]	Loss: 1.8769
Profiling... [1280/50176]	Loss: 1.7724
Profiling... [1536/50176]	Loss: 1.7937
Profiling... [1792/50176]	Loss: 1.9198
Profiling... [2048/50176]	Loss: 1.9078
Profiling... [2304/50176]	Loss: 1.8035
Profiling... [2560/50176]	Loss: 1.7951
Profiling... [2816/50176]	Loss: 1.6434
Profiling... [3072/50176]	Loss: 1.6854
Profiling... [3328/50176]	Loss: 1.8927
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 7, Average loss: 0.0076, Accuracy: 0.4729
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.46584494961337,
                        "time": 2.569547598999634,
                        "accuracy": 0.4728515625,
                        "total_cost": 627.4590340530873
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6026
Profiling... [512/50176]	Loss: 1.8526
Profiling... [768/50176]	Loss: 1.8838
Profiling... [1024/50176]	Loss: 1.7053
Profiling... [1280/50176]	Loss: 1.8462
Profiling... [1536/50176]	Loss: 1.7608
Profiling... [1792/50176]	Loss: 1.8374
Profiling... [2048/50176]	Loss: 1.7829
Profiling... [2304/50176]	Loss: 1.8202
Profiling... [2560/50176]	Loss: 1.8910
Profiling... [2816/50176]	Loss: 1.7655
Profiling... [3072/50176]	Loss: 1.6875
Profiling... [3328/50176]	Loss: 1.7781
Profile done
epoch 1 train time consumed: 4.50s
Validation Epoch: 7, Average loss: 0.0071, Accuracy: 0.4962
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.44250524327168,
                        "time": 3.0278211499971803,
                        "accuracy": 0.49619140625,
                        "total_cost": 704.4444030699864
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8415
Profiling... [512/50176]	Loss: 1.9346
Profiling... [768/50176]	Loss: 1.6537
Profiling... [1024/50176]	Loss: 1.7983
Profiling... [1280/50176]	Loss: 1.7863
Profiling... [1536/50176]	Loss: 1.7769
Profiling... [1792/50176]	Loss: 1.5760
Profiling... [2048/50176]	Loss: 1.7829
Profiling... [2304/50176]	Loss: 1.6723
Profiling... [2560/50176]	Loss: 1.7705
Profiling... [2816/50176]	Loss: 1.7793
Profiling... [3072/50176]	Loss: 1.8434
Profiling... [3328/50176]	Loss: 1.7812
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 7, Average loss: 0.0071, Accuracy: 0.4948
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.42024274581814,
                        "time": 2.408230936001928,
                        "accuracy": 0.49482421875,
                        "total_cost": 561.732002373482
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9331
Profiling... [512/50176]	Loss: 1.7689
Profiling... [768/50176]	Loss: 1.7089
Profiling... [1024/50176]	Loss: 1.8690
Profiling... [1280/50176]	Loss: 1.9604
Profiling... [1536/50176]	Loss: 1.6657
Profiling... [1792/50176]	Loss: 1.6954
Profiling... [2048/50176]	Loss: 1.7924
Profiling... [2304/50176]	Loss: 1.9263
Profiling... [2560/50176]	Loss: 1.6769
Profiling... [2816/50176]	Loss: 1.7861
Profiling... [3072/50176]	Loss: 1.7154
Profiling... [3328/50176]	Loss: 1.7156
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 7, Average loss: 0.0073, Accuracy: 0.4854
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.43474940039661,
                        "time": 2.41675505299645,
                        "accuracy": 0.4853515625,
                        "total_cost": 574.7947167776707
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0747
Profiling... [512/50176]	Loss: 1.9621
Profiling... [768/50176]	Loss: 1.8149
Profiling... [1024/50176]	Loss: 1.7727
Profiling... [1280/50176]	Loss: 1.7439
Profiling... [1536/50176]	Loss: 1.8320
Profiling... [1792/50176]	Loss: 1.8768
Profiling... [2048/50176]	Loss: 1.7120
Profiling... [2304/50176]	Loss: 1.6336
Profiling... [2560/50176]	Loss: 1.7111
Profiling... [2816/50176]	Loss: 1.7070
Profiling... [3072/50176]	Loss: 1.8095
Profiling... [3328/50176]	Loss: 1.7506
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 7, Average loss: 0.0072, Accuracy: 0.4913
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.44001694406799,
                        "time": 2.556977143001859,
                        "accuracy": 0.49130859375,
                        "total_cost": 600.7985377596078
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1209
Profiling... [512/50176]	Loss: 1.6419
Profiling... [768/50176]	Loss: 1.7363
Profiling... [1024/50176]	Loss: 1.7944
Profiling... [1280/50176]	Loss: 1.5979
Profiling... [1536/50176]	Loss: 1.7480
Profiling... [1792/50176]	Loss: 1.7372
Profiling... [2048/50176]	Loss: 1.7872
Profiling... [2304/50176]	Loss: 1.7882
Profiling... [2560/50176]	Loss: 1.6740
Profiling... [2816/50176]	Loss: 1.7541
Profiling... [3072/50176]	Loss: 1.7440
Profiling... [3328/50176]	Loss: 1.6230
Profile done
epoch 1 train time consumed: 4.46s
Validation Epoch: 7, Average loss: 0.0071, Accuracy: 0.4934
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.41828186391933,
                        "time": 3.0130608300023596,
                        "accuracy": 0.493359375,
                        "total_cost": 704.8863805422711
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8363
Profiling... [512/50176]	Loss: 1.7222
Profiling... [768/50176]	Loss: 1.6622
Profiling... [1024/50176]	Loss: 1.8570
Profiling... [1280/50176]	Loss: 1.8027
Profiling... [1536/50176]	Loss: 1.9899
Profiling... [1792/50176]	Loss: 1.8493
Profiling... [2048/50176]	Loss: 1.7252
Profiling... [2304/50176]	Loss: 1.7124
Profiling... [2560/50176]	Loss: 1.8771
Profiling... [2816/50176]	Loss: 1.8855
Profiling... [3072/50176]	Loss: 1.7095
Profiling... [3328/50176]	Loss: 1.8778
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 7, Average loss: 0.0093, Accuracy: 0.4182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.39671635885097,
                        "time": 2.420201263994386,
                        "accuracy": 0.4181640625,
                        "total_cost": 667.8796765145086
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7418
Profiling... [512/50176]	Loss: 1.8446
Profiling... [768/50176]	Loss: 1.9278
Profiling... [1024/50176]	Loss: 1.7724
Profiling... [1280/50176]	Loss: 1.8753
Profiling... [1536/50176]	Loss: 1.6715
Profiling... [1792/50176]	Loss: 1.5785
Profiling... [2048/50176]	Loss: 2.1424
Profiling... [2304/50176]	Loss: 1.8264
Profiling... [2560/50176]	Loss: 1.7931
Profiling... [2816/50176]	Loss: 1.7397
Profiling... [3072/50176]	Loss: 1.7610
Profiling... [3328/50176]	Loss: 1.8003
Profile done
epoch 1 train time consumed: 3.81s
Validation Epoch: 7, Average loss: 0.0086, Accuracy: 0.4230
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.41111639220084,
                        "time": 2.407703718999983,
                        "accuracy": 0.423046875,
                        "total_cost": 656.8439352056242
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8173
Profiling... [512/50176]	Loss: 1.7118
Profiling... [768/50176]	Loss: 1.8087
Profiling... [1024/50176]	Loss: 1.7595
Profiling... [1280/50176]	Loss: 1.7646
Profiling... [1536/50176]	Loss: 1.7541
Profiling... [1792/50176]	Loss: 1.9915
Profiling... [2048/50176]	Loss: 1.9864
Profiling... [2304/50176]	Loss: 2.0374
Profiling... [2560/50176]	Loss: 1.7671
Profiling... [2816/50176]	Loss: 1.9354
Profiling... [3072/50176]	Loss: 1.6977
Profiling... [3328/50176]	Loss: 1.7819
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 7, Average loss: 0.0101, Accuracy: 0.3749
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.4175988583683,
                        "time": 2.5366584779985715,
                        "accuracy": 0.37490234375,
                        "total_cost": 780.936784032357
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8359
Profiling... [512/50176]	Loss: 1.9759
Profiling... [768/50176]	Loss: 1.8984
Profiling... [1024/50176]	Loss: 1.8113
Profiling... [1280/50176]	Loss: 1.9385
Profiling... [1536/50176]	Loss: 1.9672
Profiling... [1792/50176]	Loss: 1.9352
Profiling... [2048/50176]	Loss: 1.7460
Profiling... [2304/50176]	Loss: 1.8939
Profiling... [2560/50176]	Loss: 1.5912
Profiling... [2816/50176]	Loss: 1.7435
Profiling... [3072/50176]	Loss: 1.8658
Profiling... [3328/50176]	Loss: 1.7202
Profile done
epoch 1 train time consumed: 4.56s
Validation Epoch: 7, Average loss: 0.0096, Accuracy: 0.4044
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.39546696460222,
                        "time": 2.9972021479989053,
                        "accuracy": 0.40439453125,
                        "total_cost": 855.2626574512869
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8295
Profiling... [512/50176]	Loss: 1.7973
Profiling... [768/50176]	Loss: 1.7980
Profiling... [1024/50176]	Loss: 1.8412
Profiling... [1280/50176]	Loss: 1.9924
Profiling... [1536/50176]	Loss: 1.8881
Profiling... [1792/50176]	Loss: 1.7165
Profiling... [2048/50176]	Loss: 1.8130
Profiling... [2304/50176]	Loss: 1.8191
Profiling... [2560/50176]	Loss: 1.9362
Profiling... [2816/50176]	Loss: 1.8278
Profiling... [3072/50176]	Loss: 1.7647
Profiling... [3328/50176]	Loss: 1.8076
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 7, Average loss: 0.0084, Accuracy: 0.4256
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.37358826436522,
                        "time": 2.404508824001823,
                        "accuracy": 0.4255859375,
                        "total_cost": 651.8467519581034
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9096
Profiling... [512/50176]	Loss: 1.7546
Profiling... [768/50176]	Loss: 1.9456
Profiling... [1024/50176]	Loss: 1.7512
Profiling... [1280/50176]	Loss: 1.8530
Profiling... [1536/50176]	Loss: 1.8883
Profiling... [1792/50176]	Loss: 1.8104
Profiling... [2048/50176]	Loss: 1.7396
Profiling... [2304/50176]	Loss: 1.7751
Profiling... [2560/50176]	Loss: 1.9360
Profiling... [2816/50176]	Loss: 1.8062
Profiling... [3072/50176]	Loss: 1.9163
Profiling... [3328/50176]	Loss: 1.8317
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 7, Average loss: 0.0100, Accuracy: 0.3688
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.38681973246882,
                        "time": 2.4043840149970492,
                        "accuracy": 0.36875,
                        "total_cost": 752.3639997453399
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9557
Profiling... [512/50176]	Loss: 1.8843
Profiling... [768/50176]	Loss: 1.9829
Profiling... [1024/50176]	Loss: 1.6538
Profiling... [1280/50176]	Loss: 1.9271
Profiling... [1536/50176]	Loss: 1.8602
Profiling... [1792/50176]	Loss: 1.9245
Profiling... [2048/50176]	Loss: 1.8494
Profiling... [2304/50176]	Loss: 1.7893
Profiling... [2560/50176]	Loss: 1.9762
Profiling... [2816/50176]	Loss: 1.6752
Profiling... [3072/50176]	Loss: 1.8432
Profiling... [3328/50176]	Loss: 1.9662
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 7, Average loss: 0.0081, Accuracy: 0.4439
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.39170896033175,
                        "time": 2.550792715002899,
                        "accuracy": 0.4439453125,
                        "total_cost": 663.01033550782
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7391
Profiling... [512/50176]	Loss: 1.8705
Profiling... [768/50176]	Loss: 1.6601
Profiling... [1024/50176]	Loss: 1.8810
Profiling... [1280/50176]	Loss: 1.8356
Profiling... [1536/50176]	Loss: 1.7936
Profiling... [1792/50176]	Loss: 1.8105
Profiling... [2048/50176]	Loss: 1.9102
Profiling... [2304/50176]	Loss: 1.6606
Profiling... [2560/50176]	Loss: 1.9357
Profiling... [2816/50176]	Loss: 1.8184
Profiling... [3072/50176]	Loss: 1.9395
Profiling... [3328/50176]	Loss: 1.6646
Profile done
epoch 1 train time consumed: 4.50s
Validation Epoch: 7, Average loss: 0.0089, Accuracy: 0.4130
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.3721815883441,
                        "time": 3.0336233510024613,
                        "accuracy": 0.41298828125,
                        "total_cost": 847.4713690740998
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7375
Profiling... [512/50176]	Loss: 1.7067
Profiling... [768/50176]	Loss: 1.7505
Profiling... [1024/50176]	Loss: 1.9047
Profiling... [1280/50176]	Loss: 1.7514
Profiling... [1536/50176]	Loss: 1.7151
Profiling... [1792/50176]	Loss: 1.9004
Profiling... [2048/50176]	Loss: 1.8491
Profiling... [2304/50176]	Loss: 1.7658
Profiling... [2560/50176]	Loss: 1.7483
Profiling... [2816/50176]	Loss: 1.8271
Profiling... [3072/50176]	Loss: 1.7971
Profiling... [3328/50176]	Loss: 1.7291
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 7, Average loss: 0.0086, Accuracy: 0.4251
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.35066323152539,
                        "time": 2.5255139660002897,
                        "accuracy": 0.42509765625,
                        "total_cost": 685.3006754929937
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7238
Profiling... [512/50176]	Loss: 1.6962
Profiling... [768/50176]	Loss: 1.7965
Profiling... [1024/50176]	Loss: 1.7575
Profiling... [1280/50176]	Loss: 1.8727
Profiling... [1536/50176]	Loss: 1.6056
Profiling... [1792/50176]	Loss: 2.0136
Profiling... [2048/50176]	Loss: 1.8442
Profiling... [2304/50176]	Loss: 1.8194
Profiling... [2560/50176]	Loss: 1.9233
Profiling... [2816/50176]	Loss: 1.8004
Profiling... [3072/50176]	Loss: 1.7650
Profiling... [3328/50176]	Loss: 1.7138
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 7, Average loss: 0.0080, Accuracy: 0.4530
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.36637183217985,
                        "time": 2.4040304359950824,
                        "accuracy": 0.45302734375,
                        "total_cost": 612.2020513797875
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6632
Profiling... [512/50176]	Loss: 1.6378
Profiling... [768/50176]	Loss: 1.8039
Profiling... [1024/50176]	Loss: 2.0649
Profiling... [1280/50176]	Loss: 1.8797
Profiling... [1536/50176]	Loss: 1.7772
Profiling... [1792/50176]	Loss: 1.8224
Profiling... [2048/50176]	Loss: 1.8526
Profiling... [2304/50176]	Loss: 1.7516
Profiling... [2560/50176]	Loss: 2.0423
Profiling... [2816/50176]	Loss: 1.6513
Profiling... [3072/50176]	Loss: 1.7443
Profiling... [3328/50176]	Loss: 1.8477
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 7, Average loss: 0.0095, Accuracy: 0.3965
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.37063886714621,
                        "time": 2.570775350999611,
                        "accuracy": 0.396484375,
                        "total_cost": 748.0546859601646
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7281
Profiling... [512/50176]	Loss: 1.6742
Profiling... [768/50176]	Loss: 1.9214
Profiling... [1024/50176]	Loss: 1.7968
Profiling... [1280/50176]	Loss: 1.9543
Profiling... [1536/50176]	Loss: 1.9787
Profiling... [1792/50176]	Loss: 1.8202
Profiling... [2048/50176]	Loss: 1.7552
Profiling... [2304/50176]	Loss: 1.8409
Profiling... [2560/50176]	Loss: 1.7836
Profiling... [2816/50176]	Loss: 1.7865
Profiling... [3072/50176]	Loss: 1.7889
Profiling... [3328/50176]	Loss: 1.8488
Profile done
epoch 1 train time consumed: 4.51s
Validation Epoch: 7, Average loss: 0.0089, Accuracy: 0.4121
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.34931520341148,
                        "time": 3.0031254010027624,
                        "accuracy": 0.412109375,
                        "total_cost": 840.5740793342522
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8576
Profiling... [512/50176]	Loss: 1.7260
Profiling... [768/50176]	Loss: 1.6913
Profiling... [1024/50176]	Loss: 1.9444
Profiling... [1280/50176]	Loss: 1.9256
Profiling... [1536/50176]	Loss: 2.1480
Profiling... [1792/50176]	Loss: 2.1280
Profiling... [2048/50176]	Loss: 1.9585
Profiling... [2304/50176]	Loss: 1.8492
Profiling... [2560/50176]	Loss: 2.0097
Profiling... [2816/50176]	Loss: 2.1584
Profiling... [3072/50176]	Loss: 2.0060
Profiling... [3328/50176]	Loss: 1.8688
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 7, Average loss: 0.0158, Accuracy: 0.2184
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.32911608807622,
                        "time": 2.4072926950029796,
                        "accuracy": 0.218359375,
                        "total_cost": 1271.4404347419318
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7324
Profiling... [512/50176]	Loss: 1.8984
Profiling... [768/50176]	Loss: 1.9398
Profiling... [1024/50176]	Loss: 1.9787
Profiling... [1280/50176]	Loss: 1.9697
Profiling... [1536/50176]	Loss: 2.0469
Profiling... [1792/50176]	Loss: 1.7384
Profiling... [2048/50176]	Loss: 2.1990
Profiling... [2304/50176]	Loss: 1.9792
Profiling... [2560/50176]	Loss: 1.8548
Profiling... [2816/50176]	Loss: 1.9984
Profiling... [3072/50176]	Loss: 2.0525
Profiling... [3328/50176]	Loss: 2.1369
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 7, Average loss: 0.0141, Accuracy: 0.2421
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.3438285704666,
                        "time": 2.397808975998487,
                        "accuracy": 0.24208984375,
                        "total_cost": 1142.4373000872554
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9614
Profiling... [512/50176]	Loss: 1.9494
Profiling... [768/50176]	Loss: 1.9183
Profiling... [1024/50176]	Loss: 1.8858
Profiling... [1280/50176]	Loss: 1.8349
Profiling... [1536/50176]	Loss: 1.9996
Profiling... [1792/50176]	Loss: 1.9122
Profiling... [2048/50176]	Loss: 2.1219
Profiling... [2304/50176]	Loss: 1.9320
Profiling... [2560/50176]	Loss: 1.8425
Profiling... [2816/50176]	Loss: 1.8972
Profiling... [3072/50176]	Loss: 2.0770
Profiling... [3328/50176]	Loss: 2.0894
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 7, Average loss: 0.0168, Accuracy: 0.2021
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.35156397766819,
                        "time": 2.5463891990002594,
                        "accuracy": 0.2021484375,
                        "total_cost": 1453.041043666349
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8101
Profiling... [512/50176]	Loss: 1.8760
Profiling... [768/50176]	Loss: 1.7657
Profiling... [1024/50176]	Loss: 1.8406
Profiling... [1280/50176]	Loss: 1.8913
Profiling... [1536/50176]	Loss: 1.8606
Profiling... [1792/50176]	Loss: 1.9993
Profiling... [2048/50176]	Loss: 1.8505
Profiling... [2304/50176]	Loss: 1.9932
Profiling... [2560/50176]	Loss: 2.0747
Profiling... [2816/50176]	Loss: 2.0911
Profiling... [3072/50176]	Loss: 2.1251
Profiling... [3328/50176]	Loss: 1.9570
Profile done
epoch 1 train time consumed: 4.52s
Validation Epoch: 7, Average loss: 0.0142, Accuracy: 0.2643
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.32953143063746,
                        "time": 3.032730241997342,
                        "accuracy": 0.2642578125,
                        "total_cost": 1323.5686561398338
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7630
Profiling... [512/50176]	Loss: 1.7680
Profiling... [768/50176]	Loss: 2.0184
Profiling... [1024/50176]	Loss: 1.9673
Profiling... [1280/50176]	Loss: 1.9700
Profiling... [1536/50176]	Loss: 1.9112
Profiling... [1792/50176]	Loss: 1.9267
Profiling... [2048/50176]	Loss: 1.9622
Profiling... [2304/50176]	Loss: 1.9512
Profiling... [2560/50176]	Loss: 2.0567
Profiling... [2816/50176]	Loss: 1.9334
Profiling... [3072/50176]	Loss: 2.0823
Profiling... [3328/50176]	Loss: 1.9338
Profile done
epoch 1 train time consumed: 3.83s
Validation Epoch: 7, Average loss: 0.0188, Accuracy: 0.1602
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.30813795039076,
                        "time": 2.412074504994962,
                        "accuracy": 0.16015625,
                        "total_cost": 1736.6279478233262
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7948
Profiling... [512/50176]	Loss: 1.6716
Profiling... [768/50176]	Loss: 1.8724
Profiling... [1024/50176]	Loss: 1.9559
Profiling... [1280/50176]	Loss: 2.1026
Profiling... [1536/50176]	Loss: 2.0255
Profiling... [1792/50176]	Loss: 2.0006
Profiling... [2048/50176]	Loss: 1.9410
Profiling... [2304/50176]	Loss: 1.9144
Profiling... [2560/50176]	Loss: 2.1198
Profiling... [2816/50176]	Loss: 1.7897
Profiling... [3072/50176]	Loss: 1.6519
Profiling... [3328/50176]	Loss: 2.0477
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 7, Average loss: 0.0142, Accuracy: 0.2408
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.3218588729315,
                        "time": 2.4067646710027475,
                        "accuracy": 0.2408203125,
                        "total_cost": 1152.5297548550282
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8647
Profiling... [512/50176]	Loss: 1.7352
Profiling... [768/50176]	Loss: 1.8419
Profiling... [1024/50176]	Loss: 2.0323
Profiling... [1280/50176]	Loss: 2.0286
Profiling... [1536/50176]	Loss: 1.6905
Profiling... [1792/50176]	Loss: 1.9939
Profiling... [2048/50176]	Loss: 1.9679
Profiling... [2304/50176]	Loss: 2.0032
Profiling... [2560/50176]	Loss: 1.9309
Profiling... [2816/50176]	Loss: 1.7900
Profiling... [3072/50176]	Loss: 2.0001
Profiling... [3328/50176]	Loss: 1.9663
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 7, Average loss: 0.0170, Accuracy: 0.2009
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.32642477557967,
                        "time": 2.5424316249991534,
                        "accuracy": 0.20087890625,
                        "total_cost": 1459.6333433965005
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6722
Profiling... [512/50176]	Loss: 1.9047
Profiling... [768/50176]	Loss: 1.9275
Profiling... [1024/50176]	Loss: 1.7889
Profiling... [1280/50176]	Loss: 2.2112
Profiling... [1536/50176]	Loss: 2.1073
Profiling... [1792/50176]	Loss: 1.9679
Profiling... [2048/50176]	Loss: 2.1459
Profiling... [2304/50176]	Loss: 2.0746
Profiling... [2560/50176]	Loss: 2.0182
Profiling... [2816/50176]	Loss: 1.8103
Profiling... [3072/50176]	Loss: 2.0147
Profiling... [3328/50176]	Loss: 1.9338
Profile done
epoch 1 train time consumed: 4.53s
Validation Epoch: 7, Average loss: 0.0137, Accuracy: 0.2717
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.30427443245213,
                        "time": 3.0699074379954254,
                        "accuracy": 0.2716796875,
                        "total_cost": 1302.9073059164593
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6536
Profiling... [512/50176]	Loss: 1.7261
Profiling... [768/50176]	Loss: 1.8870
Profiling... [1024/50176]	Loss: 2.0157
Profiling... [1280/50176]	Loss: 1.7037
Profiling... [1536/50176]	Loss: 2.0179
Profiling... [1792/50176]	Loss: 1.9318
Profiling... [2048/50176]	Loss: 1.9688
Profiling... [2304/50176]	Loss: 1.9530
Profiling... [2560/50176]	Loss: 2.1116
Profiling... [2816/50176]	Loss: 1.9681
Profiling... [3072/50176]	Loss: 2.0929
Profiling... [3328/50176]	Loss: 1.7984
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 7, Average loss: 0.0102, Accuracy: 0.3508
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.28301226681468,
                        "time": 2.409226565003337,
                        "accuracy": 0.35078125,
                        "total_cost": 791.7837559641956
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7276
Profiling... [512/50176]	Loss: 1.7975
Profiling... [768/50176]	Loss: 1.9049
Profiling... [1024/50176]	Loss: 1.8888
Profiling... [1280/50176]	Loss: 2.0218
Profiling... [1536/50176]	Loss: 2.0668
Profiling... [1792/50176]	Loss: 1.8951
Profiling... [2048/50176]	Loss: 1.9396
Profiling... [2304/50176]	Loss: 2.0333
Profiling... [2560/50176]	Loss: 2.0872
Profiling... [2816/50176]	Loss: 2.0089
Profiling... [3072/50176]	Loss: 1.8855
Profiling... [3328/50176]	Loss: 1.7498
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 7, Average loss: 0.0129, Accuracy: 0.3018
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.29800761127584,
                        "time": 2.459668135001266,
                        "accuracy": 0.3017578125,
                        "total_cost": 939.809422003245
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8188
Profiling... [512/50176]	Loss: 1.9883
Profiling... [768/50176]	Loss: 1.8437
Profiling... [1024/50176]	Loss: 1.7973
Profiling... [1280/50176]	Loss: 1.8432
Profiling... [1536/50176]	Loss: 1.9127
Profiling... [1792/50176]	Loss: 1.9463
Profiling... [2048/50176]	Loss: 2.0827
Profiling... [2304/50176]	Loss: 1.9632
Profiling... [2560/50176]	Loss: 1.8959
Profiling... [2816/50176]	Loss: 1.9116
Profiling... [3072/50176]	Loss: 1.8313
Profiling... [3328/50176]	Loss: 1.7824
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 7, Average loss: 0.0133, Accuracy: 0.2564
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.30376490056547,
                        "time": 2.5499360440007877,
                        "accuracy": 0.2564453125,
                        "total_cost": 1146.5104324297008
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6476
Profiling... [512/50176]	Loss: 1.8402
Profiling... [768/50176]	Loss: 1.9384
Profiling... [1024/50176]	Loss: 1.9131
Profiling... [1280/50176]	Loss: 1.7953
Profiling... [1536/50176]	Loss: 1.9076
Profiling... [1792/50176]	Loss: 1.9458
Profiling... [2048/50176]	Loss: 1.8912
Profiling... [2304/50176]	Loss: 1.8340
Profiling... [2560/50176]	Loss: 1.8053
Profiling... [2816/50176]	Loss: 1.9596
Profiling... [3072/50176]	Loss: 1.9347
Profiling... [3328/50176]	Loss: 2.1084
Profile done
epoch 1 train time consumed: 4.53s
Validation Epoch: 7, Average loss: 0.0121, Accuracy: 0.2959
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.28174218505819,
                        "time": 3.0027734550021705,
                        "accuracy": 0.2958984375,
                        "total_cost": 1169.877604641615
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8009
Profiling... [1024/50176]	Loss: 1.6784
Profiling... [1536/50176]	Loss: 1.7605
Profiling... [2048/50176]	Loss: 1.8850
Profiling... [2560/50176]	Loss: 1.7184
Profiling... [3072/50176]	Loss: 1.7789
Profiling... [3584/50176]	Loss: 1.7961
Profiling... [4096/50176]	Loss: 1.6956
Profiling... [4608/50176]	Loss: 1.7147
Profiling... [5120/50176]	Loss: 1.7703
Profiling... [5632/50176]	Loss: 1.7601
Profiling... [6144/50176]	Loss: 1.8744
Profiling... [6656/50176]	Loss: 1.6680
Profile done
epoch 1 train time consumed: 6.81s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4942
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.28071845432302,
                        "time": 4.533238370997424,
                        "accuracy": 0.49423828125,
                        "total_cost": 1057.3745421167494
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7020
Profiling... [1024/50176]	Loss: 1.8578
Profiling... [1536/50176]	Loss: 1.7654
Profiling... [2048/50176]	Loss: 1.8860
Profiling... [2560/50176]	Loss: 1.8789
Profiling... [3072/50176]	Loss: 1.8067
Profiling... [3584/50176]	Loss: 1.7727
Profiling... [4096/50176]	Loss: 1.8919
Profiling... [4608/50176]	Loss: 1.8170
Profiling... [5120/50176]	Loss: 1.8648
Profiling... [5632/50176]	Loss: 1.8361
Profiling... [6144/50176]	Loss: 1.6787
Profiling... [6656/50176]	Loss: 1.8166
Profile done
epoch 1 train time consumed: 6.98s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4951
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.31082631429699,
                        "time": 4.706291208996845,
                        "accuracy": 0.4951171875,
                        "total_cost": 1096.0765287210677
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7998
Profiling... [1024/50176]	Loss: 1.7627
Profiling... [1536/50176]	Loss: 1.8095
Profiling... [2048/50176]	Loss: 1.7157
Profiling... [2560/50176]	Loss: 1.7203
Profiling... [3072/50176]	Loss: 1.8172
Profiling... [3584/50176]	Loss: 1.7576
Profiling... [4096/50176]	Loss: 1.8328
Profiling... [4608/50176]	Loss: 1.7446
Profiling... [5120/50176]	Loss: 1.7449
Profiling... [5632/50176]	Loss: 1.6909
Profiling... [6144/50176]	Loss: 1.6484
Profiling... [6656/50176]	Loss: 1.7560
Profile done
epoch 1 train time consumed: 7.26s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4935
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.32120679376177,
                        "time": 4.871742051996989,
                        "accuracy": 0.49345703125,
                        "total_cost": 1138.5290654407108
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7882
Profiling... [1024/50176]	Loss: 1.8389
Profiling... [1536/50176]	Loss: 1.7741
Profiling... [2048/50176]	Loss: 1.7597
Profiling... [2560/50176]	Loss: 1.7960
Profiling... [3072/50176]	Loss: 1.7003
Profiling... [3584/50176]	Loss: 1.7904
Profiling... [4096/50176]	Loss: 1.8377
Profiling... [4608/50176]	Loss: 1.7878
Profiling... [5120/50176]	Loss: 1.8527
Profiling... [5632/50176]	Loss: 1.7036
Profiling... [6144/50176]	Loss: 1.7173
Profiling... [6656/50176]	Loss: 1.6349
Profile done
epoch 1 train time consumed: 8.65s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4908
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.27828451773067,
                        "time": 6.016577148002398,
                        "accuracy": 0.4908203125,
                        "total_cost": 1413.105111232121
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8045
Profiling... [1024/50176]	Loss: 1.6022
Profiling... [1536/50176]	Loss: 1.9392
Profiling... [2048/50176]	Loss: 1.7291
Profiling... [2560/50176]	Loss: 1.8076
Profiling... [3072/50176]	Loss: 1.8227
Profiling... [3584/50176]	Loss: 1.7561
Profiling... [4096/50176]	Loss: 1.6804
Profiling... [4608/50176]	Loss: 1.7685
Profiling... [5120/50176]	Loss: 1.6746
Profiling... [5632/50176]	Loss: 1.7829
Profiling... [6144/50176]	Loss: 1.7404
Profiling... [6656/50176]	Loss: 1.6925
Profile done
epoch 1 train time consumed: 6.78s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4901
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.27511496870451,
                        "time": 4.522503332998895,
                        "accuracy": 0.49013671875,
                        "total_cost": 1063.6462678971589
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9193
Profiling... [1024/50176]	Loss: 1.7584
Profiling... [1536/50176]	Loss: 1.7661
Profiling... [2048/50176]	Loss: 1.7409
Profiling... [2560/50176]	Loss: 1.7376
Profiling... [3072/50176]	Loss: 1.7577
Profiling... [3584/50176]	Loss: 1.7045
Profiling... [4096/50176]	Loss: 1.7336
Profiling... [4608/50176]	Loss: 1.7597
Profiling... [5120/50176]	Loss: 1.7591
Profiling... [5632/50176]	Loss: 1.7835
Profiling... [6144/50176]	Loss: 1.7960
Profiling... [6656/50176]	Loss: 1.6677
Profile done
epoch 1 train time consumed: 6.79s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4958
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.30686612465333,
                        "time": 4.557727619998332,
                        "accuracy": 0.49580078125,
                        "total_cost": 1059.9767253024725
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8126
Profiling... [1024/50176]	Loss: 1.7351
Profiling... [1536/50176]	Loss: 1.7673
Profiling... [2048/50176]	Loss: 1.8446
Profiling... [2560/50176]	Loss: 1.7982
Profiling... [3072/50176]	Loss: 1.7207
Profiling... [3584/50176]	Loss: 1.8250
Profiling... [4096/50176]	Loss: 1.6419
Profiling... [4608/50176]	Loss: 1.7682
Profiling... [5120/50176]	Loss: 1.8304
Profiling... [5632/50176]	Loss: 1.7338
Profiling... [6144/50176]	Loss: 1.5882
Profiling... [6656/50176]	Loss: 1.6412
Profile done
epoch 1 train time consumed: 7.23s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4883
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.31454494324208,
                        "time": 4.880239010002697,
                        "accuracy": 0.48828125,
                        "total_cost": 1152.537683256769
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9209
Profiling... [1024/50176]	Loss: 1.6861
Profiling... [1536/50176]	Loss: 1.7518
Profiling... [2048/50176]	Loss: 1.8224
Profiling... [2560/50176]	Loss: 1.8737
Profiling... [3072/50176]	Loss: 1.7772
Profiling... [3584/50176]	Loss: 1.9403
Profiling... [4096/50176]	Loss: 1.7929
Profiling... [4608/50176]	Loss: 1.7389
Profiling... [5120/50176]	Loss: 1.7768
Profiling... [5632/50176]	Loss: 1.6752
Profiling... [6144/50176]	Loss: 1.8896
Profiling... [6656/50176]	Loss: 1.8355
Profile done
epoch 1 train time consumed: 8.62s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4948
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.27119505170892,
                        "time": 5.997760097998253,
                        "accuracy": 0.49482421875,
                        "total_cost": 1397.2011634277214
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8163
Profiling... [1024/50176]	Loss: 1.8746
Profiling... [1536/50176]	Loss: 1.7384
Profiling... [2048/50176]	Loss: 1.7970
Profiling... [2560/50176]	Loss: 1.8727
Profiling... [3072/50176]	Loss: 1.8036
Profiling... [3584/50176]	Loss: 1.7561
Profiling... [4096/50176]	Loss: 1.7511
Profiling... [4608/50176]	Loss: 1.6649
Profiling... [5120/50176]	Loss: 1.7707
Profiling... [5632/50176]	Loss: 1.5248
Profiling... [6144/50176]	Loss: 1.7185
Profiling... [6656/50176]	Loss: 1.7268
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4881
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.2700522532643,
                        "time": 4.565175552997971,
                        "accuracy": 0.4880859375,
                        "total_cost": 1078.146252757796
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7860
Profiling... [1024/50176]	Loss: 1.9502
Profiling... [1536/50176]	Loss: 1.7324
Profiling... [2048/50176]	Loss: 1.5979
Profiling... [2560/50176]	Loss: 1.7118
Profiling... [3072/50176]	Loss: 1.6828
Profiling... [3584/50176]	Loss: 1.7559
Profiling... [4096/50176]	Loss: 1.7497
Profiling... [4608/50176]	Loss: 1.7488
Profiling... [5120/50176]	Loss: 1.7137
Profiling... [5632/50176]	Loss: 1.6818
Profiling... [6144/50176]	Loss: 1.7831
Profiling... [6656/50176]	Loss: 1.7107
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4936
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.2985967624542,
                        "time": 4.5692255169997225,
                        "accuracy": 0.4935546875,
                        "total_cost": 1067.410164960225
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9106
Profiling... [1024/50176]	Loss: 1.8351
Profiling... [1536/50176]	Loss: 1.6990
Profiling... [2048/50176]	Loss: 1.7935
Profiling... [2560/50176]	Loss: 1.7840
Profiling... [3072/50176]	Loss: 1.6977
Profiling... [3584/50176]	Loss: 1.6096
Profiling... [4096/50176]	Loss: 1.7414
Profiling... [4608/50176]	Loss: 1.6913
Profiling... [5120/50176]	Loss: 1.7977
Profiling... [5632/50176]	Loss: 1.7419
Profiling... [6144/50176]	Loss: 1.7446
Profiling... [6656/50176]	Loss: 1.7489
Profile done
epoch 1 train time consumed: 7.24s
Validation Epoch: 7, Average loss: 0.0036, Accuracy: 0.4941
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.30467625406753,
                        "time": 4.870591507999052,
                        "accuracy": 0.494140625,
                        "total_cost": 1136.5225779516536
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6897
Profiling... [1024/50176]	Loss: 1.6931
Profiling... [1536/50176]	Loss: 1.7510
Profiling... [2048/50176]	Loss: 1.9005
Profiling... [2560/50176]	Loss: 1.8247
Profiling... [3072/50176]	Loss: 1.6715
Profiling... [3584/50176]	Loss: 1.7732
Profiling... [4096/50176]	Loss: 1.7224
Profiling... [4608/50176]	Loss: 1.6768
Profiling... [5120/50176]	Loss: 1.7585
Profiling... [5632/50176]	Loss: 1.7166
Profiling... [6144/50176]	Loss: 1.8244
Profiling... [6656/50176]	Loss: 1.6789
Profile done
epoch 1 train time consumed: 8.57s
Validation Epoch: 7, Average loss: 0.0035, Accuracy: 0.4971
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.26544739794438,
                        "time": 5.97555107199878,
                        "accuracy": 0.4970703125,
                        "total_cost": 1385.6682856375685
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8949
Profiling... [1024/50176]	Loss: 1.7806
Profiling... [1536/50176]	Loss: 1.7544
Profiling... [2048/50176]	Loss: 1.8601
Profiling... [2560/50176]	Loss: 1.7224
Profiling... [3072/50176]	Loss: 1.7853
Profiling... [3584/50176]	Loss: 1.8413
Profiling... [4096/50176]	Loss: 1.7445
Profiling... [4608/50176]	Loss: 1.7949
Profiling... [5120/50176]	Loss: 1.7196
Profiling... [5632/50176]	Loss: 1.7855
Profiling... [6144/50176]	Loss: 1.8112
Profiling... [6656/50176]	Loss: 1.7457
Profile done
epoch 1 train time consumed: 6.76s
Validation Epoch: 7, Average loss: 0.0044, Accuracy: 0.4251
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.26312320624402,
                        "time": 4.5403860789956525,
                        "accuracy": 0.42509765625,
                        "total_cost": 1231.1031884857425
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7312
Profiling... [1024/50176]	Loss: 1.7622
Profiling... [1536/50176]	Loss: 1.8203
Profiling... [2048/50176]	Loss: 1.7954
Profiling... [2560/50176]	Loss: 1.6944
Profiling... [3072/50176]	Loss: 1.7761
Profiling... [3584/50176]	Loss: 1.7309
Profiling... [4096/50176]	Loss: 1.7595
Profiling... [4608/50176]	Loss: 1.7805
Profiling... [5120/50176]	Loss: 1.7993
Profiling... [5632/50176]	Loss: 1.7665
Profiling... [6144/50176]	Loss: 1.8868
Profiling... [6656/50176]	Loss: 1.8294
Profile done
epoch 1 train time consumed: 6.99s
Validation Epoch: 7, Average loss: 0.0049, Accuracy: 0.3822
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.2943345245698,
                        "time": 4.755859529002919,
                        "accuracy": 0.3822265625,
                        "total_cost": 1434.551423905097
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8341
Profiling... [1024/50176]	Loss: 1.9158
Profiling... [1536/50176]	Loss: 1.6986
Profiling... [2048/50176]	Loss: 1.8000
Profiling... [2560/50176]	Loss: 1.8651
Profiling... [3072/50176]	Loss: 1.8732
Profiling... [3584/50176]	Loss: 1.9229
Profiling... [4096/50176]	Loss: 1.7889
Profiling... [4608/50176]	Loss: 1.7325
Profiling... [5120/50176]	Loss: 1.7431
Profiling... [5632/50176]	Loss: 1.8434
Profiling... [6144/50176]	Loss: 1.7570
Profiling... [6656/50176]	Loss: 1.7294
Profile done
epoch 1 train time consumed: 7.29s
Validation Epoch: 7, Average loss: 0.0047, Accuracy: 0.3945
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.30081437985118,
                        "time": 4.889181284997903,
                        "accuracy": 0.39453125,
                        "total_cost": 1428.8515391644783
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6986
Profiling... [1024/50176]	Loss: 1.7836
Profiling... [1536/50176]	Loss: 1.8531
Profiling... [2048/50176]	Loss: 1.8742
Profiling... [2560/50176]	Loss: 1.8820
Profiling... [3072/50176]	Loss: 1.5857
Profiling... [3584/50176]	Loss: 1.9832
Profiling... [4096/50176]	Loss: 1.6849
Profiling... [4608/50176]	Loss: 1.7664
Profiling... [5120/50176]	Loss: 1.6173
Profiling... [5632/50176]	Loss: 1.5389
Profiling... [6144/50176]	Loss: 1.7284
Profiling... [6656/50176]	Loss: 1.7444
Profile done
epoch 1 train time consumed: 8.61s
Validation Epoch: 7, Average loss: 0.0057, Accuracy: 0.3561
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.26055918299964,
                        "time": 6.004878062994976,
                        "accuracy": 0.3560546875,
                        "total_cost": 1943.87443183578
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.9164
Profiling... [1024/50176]	Loss: 1.6765
Profiling... [1536/50176]	Loss: 1.8680
Profiling... [2048/50176]	Loss: 1.7852
Profiling... [2560/50176]	Loss: 1.7209
Profiling... [3072/50176]	Loss: 1.7277
Profiling... [3584/50176]	Loss: 1.8208
Profiling... [4096/50176]	Loss: 1.8282
Profiling... [4608/50176]	Loss: 1.8628
Profiling... [5120/50176]	Loss: 1.8054
Profiling... [5632/50176]	Loss: 1.8481
Profiling... [6144/50176]	Loss: 1.8664
Profiling... [6656/50176]	Loss: 1.7979
Profile done
epoch 1 train time consumed: 6.88s
Validation Epoch: 7, Average loss: 0.0043, Accuracy: 0.4389
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.2562330288155,
                        "time": 4.632801578001818,
                        "accuracy": 0.4388671875,
                        "total_cost": 1216.6761914740816
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.9282
Profiling... [1024/50176]	Loss: 1.6440
Profiling... [1536/50176]	Loss: 1.8817
Profiling... [2048/50176]	Loss: 1.7207
Profiling... [2560/50176]	Loss: 1.7486
Profiling... [3072/50176]	Loss: 1.7395
Profiling... [3584/50176]	Loss: 1.8723
Profiling... [4096/50176]	Loss: 1.8009
Profiling... [4608/50176]	Loss: 1.8321
Profiling... [5120/50176]	Loss: 1.7996
Profiling... [5632/50176]	Loss: 1.8499
Profiling... [6144/50176]	Loss: 1.7777
Profiling... [6656/50176]	Loss: 1.7668
Profile done
epoch 1 train time consumed: 6.77s
Validation Epoch: 7, Average loss: 0.0042, Accuracy: 0.4391
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.28704981842058,
                        "time": 4.533201054997335,
                        "accuracy": 0.4390625,
                        "total_cost": 1190.3074752783361
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7932
Profiling... [1024/50176]	Loss: 1.6592
Profiling... [1536/50176]	Loss: 1.7830
Profiling... [2048/50176]	Loss: 1.7086
Profiling... [2560/50176]	Loss: 1.6903
Profiling... [3072/50176]	Loss: 1.7391
Profiling... [3584/50176]	Loss: 1.7638
Profiling... [4096/50176]	Loss: 1.8456
Profiling... [4608/50176]	Loss: 1.8225
Profiling... [5120/50176]	Loss: 1.7201
Profiling... [5632/50176]	Loss: 1.7043
Profiling... [6144/50176]	Loss: 1.8262
Profiling... [6656/50176]	Loss: 1.8290
Profile done
epoch 1 train time consumed: 7.25s
Validation Epoch: 7, Average loss: 0.0043, Accuracy: 0.4259
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.29622995889665,
                        "time": 4.861964402000012,
                        "accuracy": 0.42587890625,
                        "total_cost": 1316.2571743243336
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7203
Profiling... [1024/50176]	Loss: 1.7480
Profiling... [1536/50176]	Loss: 1.8278
Profiling... [2048/50176]	Loss: 1.7115
Profiling... [2560/50176]	Loss: 1.7739
Profiling... [3072/50176]	Loss: 1.8791
Profiling... [3584/50176]	Loss: 1.7571
Profiling... [4096/50176]	Loss: 1.7561
Profiling... [4608/50176]	Loss: 1.7810
Profiling... [5120/50176]	Loss: 1.9104
Profiling... [5632/50176]	Loss: 1.9201
Profiling... [6144/50176]	Loss: 1.7239
Profiling... [6656/50176]	Loss: 1.7941
Profile done
epoch 1 train time consumed: 8.60s
Validation Epoch: 7, Average loss: 0.0043, Accuracy: 0.4308
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.2553506354524,
                        "time": 6.00981826899806,
                        "accuracy": 0.43076171875,
                        "total_cost": 1607.9973723262033
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6677
Profiling... [1024/50176]	Loss: 1.8066
Profiling... [1536/50176]	Loss: 1.7723
Profiling... [2048/50176]	Loss: 1.6950
Profiling... [2560/50176]	Loss: 1.6566
Profiling... [3072/50176]	Loss: 1.8060
Profiling... [3584/50176]	Loss: 1.7415
Profiling... [4096/50176]	Loss: 1.6832
Profiling... [4608/50176]	Loss: 1.8249
Profiling... [5120/50176]	Loss: 1.7736
Profiling... [5632/50176]	Loss: 1.8027
Profiling... [6144/50176]	Loss: 1.6706
Profiling... [6656/50176]	Loss: 1.7196
Profile done
epoch 1 train time consumed: 6.89s
Validation Epoch: 7, Average loss: 0.0045, Accuracy: 0.4129
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.25015543530989,
                        "time": 4.521904610002821,
                        "accuracy": 0.412890625,
                        "total_cost": 1262.199182087192
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0145
Profiling... [1024/50176]	Loss: 1.7162
Profiling... [1536/50176]	Loss: 1.7469
Profiling... [2048/50176]	Loss: 1.8128
Profiling... [2560/50176]	Loss: 1.8490
Profiling... [3072/50176]	Loss: 1.7819
Profiling... [3584/50176]	Loss: 1.7676
Profiling... [4096/50176]	Loss: 1.7704
Profiling... [4608/50176]	Loss: 1.7937
Profiling... [5120/50176]	Loss: 1.8599
Profiling... [5632/50176]	Loss: 1.8084
Profiling... [6144/50176]	Loss: 1.6930
Profiling... [6656/50176]	Loss: 1.8770
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4587
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.279888559315,
                        "time": 4.545676994996029,
                        "accuracy": 0.45869140625,
                        "total_cost": 1142.4350451514135
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6959
Profiling... [1024/50176]	Loss: 1.8402
Profiling... [1536/50176]	Loss: 1.7198
Profiling... [2048/50176]	Loss: 1.7741
Profiling... [2560/50176]	Loss: 1.8522
Profiling... [3072/50176]	Loss: 1.7080
Profiling... [3584/50176]	Loss: 1.7016
Profiling... [4096/50176]	Loss: 1.8243
Profiling... [4608/50176]	Loss: 1.6354
Profiling... [5120/50176]	Loss: 1.7704
Profiling... [5632/50176]	Loss: 1.8037
Profiling... [6144/50176]	Loss: 1.7044
Profiling... [6656/50176]	Loss: 1.8329
Profile done
epoch 1 train time consumed: 7.19s
Validation Epoch: 7, Average loss: 0.0038, Accuracy: 0.4753
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.28829145534237,
                        "time": 4.878211540999473,
                        "accuracy": 0.47529296875,
                        "total_cost": 1183.2716048769912
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7518
Profiling... [1024/50176]	Loss: 1.7660
Profiling... [1536/50176]	Loss: 1.6899
Profiling... [2048/50176]	Loss: 1.8239
Profiling... [2560/50176]	Loss: 1.7137
Profiling... [3072/50176]	Loss: 1.8173
Profiling... [3584/50176]	Loss: 1.8133
Profiling... [4096/50176]	Loss: 1.7624
Profiling... [4608/50176]	Loss: 1.6952
Profiling... [5120/50176]	Loss: 1.7383
Profiling... [5632/50176]	Loss: 1.7903
Profiling... [6144/50176]	Loss: 1.8320
Profiling... [6656/50176]	Loss: 1.7160
Profile done
epoch 1 train time consumed: 8.64s
Validation Epoch: 7, Average loss: 0.0048, Accuracy: 0.3908
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.24944652809285,
                        "time": 6.01474697899539,
                        "accuracy": 0.3908203125,
                        "total_cost": 1773.6955786701521
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7302
Profiling... [1024/50176]	Loss: 1.8151
Profiling... [1536/50176]	Loss: 1.7206
Profiling... [2048/50176]	Loss: 1.7560
Profiling... [2560/50176]	Loss: 1.8620
Profiling... [3072/50176]	Loss: 1.9200
Profiling... [3584/50176]	Loss: 1.8699
Profiling... [4096/50176]	Loss: 1.9913
Profiling... [4608/50176]	Loss: 1.7754
Profiling... [5120/50176]	Loss: 1.8840
Profiling... [5632/50176]	Loss: 1.9423
Profiling... [6144/50176]	Loss: 1.9171
Profiling... [6656/50176]	Loss: 1.7955
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 7, Average loss: 0.0087, Accuracy: 0.1791
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.24596943477441,
                        "time": 4.552308111997263,
                        "accuracy": 0.1791015625,
                        "total_cost": 2929.2606619940134
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7472
Profiling... [1024/50176]	Loss: 1.7524
Profiling... [1536/50176]	Loss: 1.9361
Profiling... [2048/50176]	Loss: 1.8662
Profiling... [2560/50176]	Loss: 1.7974
Profiling... [3072/50176]	Loss: 1.8830
Profiling... [3584/50176]	Loss: 1.8218
Profiling... [4096/50176]	Loss: 1.7582
Profiling... [4608/50176]	Loss: 1.9286
Profiling... [5120/50176]	Loss: 1.8989
Profiling... [5632/50176]	Loss: 1.8478
Profiling... [6144/50176]	Loss: 2.1359
Profiling... [6656/50176]	Loss: 1.9040
Profile done
epoch 1 train time consumed: 6.81s
Validation Epoch: 7, Average loss: 0.0072, Accuracy: 0.2520
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.27760136257447,
                        "time": 4.537797935998242,
                        "accuracy": 0.251953125,
                        "total_cost": 2076.205490723398
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8289
Profiling... [1024/50176]	Loss: 1.8478
Profiling... [1536/50176]	Loss: 1.8067
Profiling... [2048/50176]	Loss: 1.8764
Profiling... [2560/50176]	Loss: 1.8689
Profiling... [3072/50176]	Loss: 1.8452
Profiling... [3584/50176]	Loss: 1.8913
Profiling... [4096/50176]	Loss: 1.8157
Profiling... [4608/50176]	Loss: 1.7994
Profiling... [5120/50176]	Loss: 2.0111
Profiling... [5632/50176]	Loss: 1.9417
Profiling... [6144/50176]	Loss: 1.7953
Profiling... [6656/50176]	Loss: 1.7633
Profile done
epoch 1 train time consumed: 7.13s
Validation Epoch: 7, Average loss: 0.0054, Accuracy: 0.3365
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.28854305722962,
                        "time": 4.861627938000311,
                        "accuracy": 0.3365234375,
                        "total_cost": 1665.5303595559499
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7381
Profiling... [1024/50176]	Loss: 1.7700
Profiling... [1536/50176]	Loss: 1.7732
Profiling... [2048/50176]	Loss: 1.7143
Profiling... [2560/50176]	Loss: 1.9007
Profiling... [3072/50176]	Loss: 1.8387
Profiling... [3584/50176]	Loss: 1.9090
Profiling... [4096/50176]	Loss: 1.8298
Profiling... [4608/50176]	Loss: 1.8651
Profiling... [5120/50176]	Loss: 1.7811
Profiling... [5632/50176]	Loss: 1.8060
Profiling... [6144/50176]	Loss: 1.8408
Profiling... [6656/50176]	Loss: 1.8996
Profile done
epoch 1 train time consumed: 8.60s
Validation Epoch: 7, Average loss: 0.0062, Accuracy: 0.3023
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.24938452734526,
                        "time": 6.0208979599992745,
                        "accuracy": 0.30234375,
                        "total_cost": 2295.085591125548
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7761
Profiling... [1024/50176]	Loss: 1.7088
Profiling... [1536/50176]	Loss: 1.8676
Profiling... [2048/50176]	Loss: 1.9269
Profiling... [2560/50176]	Loss: 1.8644
Profiling... [3072/50176]	Loss: 1.8549
Profiling... [3584/50176]	Loss: 2.0210
Profiling... [4096/50176]	Loss: 1.7855
Profiling... [4608/50176]	Loss: 1.9640
Profiling... [5120/50176]	Loss: 1.8592
Profiling... [5632/50176]	Loss: 1.7130
Profiling... [6144/50176]	Loss: 1.8646
Profiling... [6656/50176]	Loss: 1.8794
Profile done
epoch 1 train time consumed: 6.77s
Validation Epoch: 7, Average loss: 0.0069, Accuracy: 0.2473
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.24752713896092,
                        "time": 4.525989439003752,
                        "accuracy": 0.247265625,
                        "total_cost": 2109.50911879577
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7480
Profiling... [1024/50176]	Loss: 1.9422
Profiling... [1536/50176]	Loss: 1.8131
Profiling... [2048/50176]	Loss: 1.8124
Profiling... [2560/50176]	Loss: 1.7161
Profiling... [3072/50176]	Loss: 1.8430
Profiling... [3584/50176]	Loss: 1.8123
Profiling... [4096/50176]	Loss: 1.7005
Profiling... [4608/50176]	Loss: 1.7866
Profiling... [5120/50176]	Loss: 1.8786
Profiling... [5632/50176]	Loss: 1.8969
Profiling... [6144/50176]	Loss: 1.8353
Profiling... [6656/50176]	Loss: 1.8323
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 7, Average loss: 0.0050, Accuracy: 0.3612
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.27497317816045,
                        "time": 4.652938288003497,
                        "accuracy": 0.36123046875,
                        "total_cost": 1484.8341509100317
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6508
Profiling... [1024/50176]	Loss: 1.7319
Profiling... [1536/50176]	Loss: 1.6507
Profiling... [2048/50176]	Loss: 1.8831
Profiling... [2560/50176]	Loss: 1.8553
Profiling... [3072/50176]	Loss: 1.7794
Profiling... [3584/50176]	Loss: 1.8795
Profiling... [4096/50176]	Loss: 1.7268
Profiling... [4608/50176]	Loss: 1.7561
Profiling... [5120/50176]	Loss: 1.9886
Profiling... [5632/50176]	Loss: 1.8502
Profiling... [6144/50176]	Loss: 1.8380
Profiling... [6656/50176]	Loss: 1.7570
Profile done
epoch 1 train time consumed: 7.17s
Validation Epoch: 7, Average loss: 0.0052, Accuracy: 0.3484
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.28355995542042,
                        "time": 4.868893316001049,
                        "accuracy": 0.3484375,
                        "total_cost": 1610.915456894716
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.8058
Profiling... [1024/50176]	Loss: 1.7701
Profiling... [1536/50176]	Loss: 1.8387
Profiling... [2048/50176]	Loss: 1.8723
Profiling... [2560/50176]	Loss: 1.8545
Profiling... [3072/50176]	Loss: 1.8060
Profiling... [3584/50176]	Loss: 1.8649
Profiling... [4096/50176]	Loss: 1.8916
Profiling... [4608/50176]	Loss: 1.7047
Profiling... [5120/50176]	Loss: 1.9006
Profiling... [5632/50176]	Loss: 1.8163
Profiling... [6144/50176]	Loss: 1.9175
Profiling... [6656/50176]	Loss: 1.8805
Profile done
epoch 1 train time consumed: 8.62s
Validation Epoch: 7, Average loss: 0.0161, Accuracy: 0.1015
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.2448562437142,
                        "time": 6.019611784999142,
                        "accuracy": 0.10146484375,
                        "total_cost": 6837.139536867358
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8188
Profiling... [1024/50176]	Loss: 1.8168
Profiling... [1536/50176]	Loss: 1.9473
Profiling... [2048/50176]	Loss: 1.7520
Profiling... [2560/50176]	Loss: 1.9546
Profiling... [3072/50176]	Loss: 1.8246
Profiling... [3584/50176]	Loss: 1.8221
Profiling... [4096/50176]	Loss: 1.8087
Profiling... [4608/50176]	Loss: 1.9214
Profiling... [5120/50176]	Loss: 1.6897
Profiling... [5632/50176]	Loss: 1.9202
Profiling... [6144/50176]	Loss: 1.9060
Profiling... [6656/50176]	Loss: 1.8648
Profile done
epoch 1 train time consumed: 6.93s
Validation Epoch: 7, Average loss: 0.0074, Accuracy: 0.2919
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.2405877107183,
                        "time": 4.537028131002444,
                        "accuracy": 0.29189453125,
                        "total_cost": 1791.22844829517
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7978
Profiling... [1024/50176]	Loss: 1.6946
Profiling... [1536/50176]	Loss: 1.7652
Profiling... [2048/50176]	Loss: 1.7567
Profiling... [2560/50176]	Loss: 1.7837
Profiling... [3072/50176]	Loss: 1.8493
Profiling... [3584/50176]	Loss: 1.9214
Profiling... [4096/50176]	Loss: 1.9333
Profiling... [4608/50176]	Loss: 1.7944
Profiling... [5120/50176]	Loss: 1.8216
Profiling... [5632/50176]	Loss: 1.8295
Profiling... [6144/50176]	Loss: 1.8927
Profiling... [6656/50176]	Loss: 1.9014
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 7, Average loss: 0.0052, Accuracy: 0.3544
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.26855155201412,
                        "time": 4.5433469290001085,
                        "accuracy": 0.35439453125,
                        "total_cost": 1477.745770672453
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8365
Profiling... [1024/50176]	Loss: 1.8185
Profiling... [1536/50176]	Loss: 1.8331
Profiling... [2048/50176]	Loss: 1.9280
Profiling... [2560/50176]	Loss: 1.8298
Profiling... [3072/50176]	Loss: 1.8530
Profiling... [3584/50176]	Loss: 1.8188
Profiling... [4096/50176]	Loss: 1.8308
Profiling... [4608/50176]	Loss: 1.7962
Profiling... [5120/50176]	Loss: 1.9244
Profiling... [5632/50176]	Loss: 1.7919
Profiling... [6144/50176]	Loss: 1.8131
Profiling... [6656/50176]	Loss: 1.8414
Profile done
epoch 1 train time consumed: 7.19s
Validation Epoch: 7, Average loss: 0.0078, Accuracy: 0.2403
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.27620816936196,
                        "time": 4.867656607995741,
                        "accuracy": 0.24033203125,
                        "total_cost": 2334.7907206617406
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6556
Profiling... [1024/50176]	Loss: 1.8072
Profiling... [1536/50176]	Loss: 1.9098
Profiling... [2048/50176]	Loss: 1.7955
Profiling... [2560/50176]	Loss: 1.9691
Profiling... [3072/50176]	Loss: 1.8029
Profiling... [3584/50176]	Loss: 1.7899
Profiling... [4096/50176]	Loss: 1.8001
Profiling... [4608/50176]	Loss: 1.9151
Profiling... [5120/50176]	Loss: 1.6748
Profiling... [5632/50176]	Loss: 1.8657
Profiling... [6144/50176]	Loss: 1.9235
Profiling... [6656/50176]	Loss: 1.8570
Profile done
epoch 1 train time consumed: 8.57s
Validation Epoch: 7, Average loss: 0.0093, Accuracy: 0.1892
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.2382664985229,
                        "time": 6.005769923998741,
                        "accuracy": 0.18916015625,
                        "total_cost": 3658.7753401719906
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8366
Profiling... [2048/50176]	Loss: 1.7786
Profiling... [3072/50176]	Loss: 1.7808
Profiling... [4096/50176]	Loss: 1.8033
Profiling... [5120/50176]	Loss: 1.7621
Profiling... [6144/50176]	Loss: 1.7465
Profiling... [7168/50176]	Loss: 1.8517
Profiling... [8192/50176]	Loss: 1.7239
Profiling... [9216/50176]	Loss: 1.6767
Profiling... [10240/50176]	Loss: 1.7431
Profiling... [11264/50176]	Loss: 1.7319
Profiling... [12288/50176]	Loss: 1.6317
Profiling... [13312/50176]	Loss: 1.7189
Profile done
epoch 1 train time consumed: 13.04s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4958
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.25892425005728,
                        "time": 8.831860821999726,
                        "accuracy": 0.49580078125,
                        "total_cost": 2053.1447629095783
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8018
Profiling... [2048/50176]	Loss: 1.8384
Profiling... [3072/50176]	Loss: 1.6985
Profiling... [4096/50176]	Loss: 1.7502
Profiling... [5120/50176]	Loss: 1.8355
Profiling... [6144/50176]	Loss: 1.6797
Profiling... [7168/50176]	Loss: 1.7691
Profiling... [8192/50176]	Loss: 1.7743
Profiling... [9216/50176]	Loss: 1.7246
Profiling... [10240/50176]	Loss: 1.8112
Profiling... [11264/50176]	Loss: 1.6970
Profiling... [12288/50176]	Loss: 1.7720
Profiling... [13312/50176]	Loss: 1.7207
Profile done
epoch 1 train time consumed: 12.91s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4983
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.31212381051849,
                        "time": 8.968035091995262,
                        "accuracy": 0.49833984375,
                        "total_cost": 2075.136447215359
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7351
Profiling... [2048/50176]	Loss: 1.6733
Profiling... [3072/50176]	Loss: 1.7158
Profiling... [4096/50176]	Loss: 1.7752
Profiling... [5120/50176]	Loss: 1.7877
Profiling... [6144/50176]	Loss: 1.7895
Profiling... [7168/50176]	Loss: 1.7541
Profiling... [8192/50176]	Loss: 1.8532
Profiling... [9216/50176]	Loss: 1.6895
Profiling... [10240/50176]	Loss: 1.7537
Profiling... [11264/50176]	Loss: 1.6951
Profiling... [12288/50176]	Loss: 1.8394
Profiling... [13312/50176]	Loss: 1.7149
Profile done
epoch 1 train time consumed: 13.70s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4986
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.32386748229125,
                        "time": 9.580201493998175,
                        "accuracy": 0.4986328125,
                        "total_cost": 2215.710358105433
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8441
Profiling... [2048/50176]	Loss: 1.7728
Profiling... [3072/50176]	Loss: 1.7366
Profiling... [4096/50176]	Loss: 1.7455
Profiling... [5120/50176]	Loss: 1.8074
Profiling... [6144/50176]	Loss: 1.7949
Profiling... [7168/50176]	Loss: 1.7492
Profiling... [8192/50176]	Loss: 1.7863
Profiling... [9216/50176]	Loss: 1.7505
Profiling... [10240/50176]	Loss: 1.7597
Profiling... [11264/50176]	Loss: 1.6559
Profiling... [12288/50176]	Loss: 1.6912
Profiling... [13312/50176]	Loss: 1.7329
Profile done
epoch 1 train time consumed: 17.54s
Validation Epoch: 7, Average loss: 0.0017, Accuracy: 0.5033
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.25087165547572,
                        "time": 11.999552998997387,
                        "accuracy": 0.5033203125,
                        "total_cost": 2747.671628314279
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7728
Profiling... [2048/50176]	Loss: 1.8334
Profiling... [3072/50176]	Loss: 1.7435
Profiling... [4096/50176]	Loss: 1.7336
Profiling... [5120/50176]	Loss: 1.8011
Profiling... [6144/50176]	Loss: 1.7714
Profiling... [7168/50176]	Loss: 1.7731
Profiling... [8192/50176]	Loss: 1.8000
Profiling... [9216/50176]	Loss: 1.7875
Profiling... [10240/50176]	Loss: 1.7486
Profiling... [11264/50176]	Loss: 1.7099
Profiling... [12288/50176]	Loss: 1.6649
Profiling... [13312/50176]	Loss: 1.7375
Profile done
epoch 1 train time consumed: 12.86s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4889
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.27375311573414,
                        "time": 8.896385206993727,
                        "accuracy": 0.4888671875,
                        "total_cost": 2097.747073633295
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7856
Profiling... [2048/50176]	Loss: 1.8180
Profiling... [3072/50176]	Loss: 1.8127
Profiling... [4096/50176]	Loss: 1.7217
Profiling... [5120/50176]	Loss: 1.7814
Profiling... [6144/50176]	Loss: 1.6821
Profiling... [7168/50176]	Loss: 1.7367
Profiling... [8192/50176]	Loss: 1.7835
Profiling... [9216/50176]	Loss: 1.7612
Profiling... [10240/50176]	Loss: 1.6597
Profiling... [11264/50176]	Loss: 1.7548
Profiling... [12288/50176]	Loss: 1.7280
Profiling... [13312/50176]	Loss: 1.6934
Profile done
epoch 1 train time consumed: 13.02s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4979
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.32700159677943,
                        "time": 9.045466514005966,
                        "accuracy": 0.49794921875,
                        "total_cost": 2094.9656949419223
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8195
Profiling... [2048/50176]	Loss: 1.7851
Profiling... [3072/50176]	Loss: 1.8259
Profiling... [4096/50176]	Loss: 1.7787
Profiling... [5120/50176]	Loss: 1.7445
Profiling... [6144/50176]	Loss: 1.7341
Profiling... [7168/50176]	Loss: 1.6636
Profiling... [8192/50176]	Loss: 1.6222
Profiling... [9216/50176]	Loss: 1.6898
Profiling... [10240/50176]	Loss: 1.7789
Profiling... [11264/50176]	Loss: 1.6726
Profiling... [12288/50176]	Loss: 1.7020
Profiling... [13312/50176]	Loss: 1.8044
Profile done
epoch 1 train time consumed: 13.75s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.5020
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.33947889776769,
                        "time": 9.565545397003007,
                        "accuracy": 0.501953125,
                        "total_cost": 2197.9841672731236
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7477
Profiling... [2048/50176]	Loss: 1.7865
Profiling... [3072/50176]	Loss: 1.7938
Profiling... [4096/50176]	Loss: 1.8178
Profiling... [5120/50176]	Loss: 1.6870
Profiling... [6144/50176]	Loss: 1.7939
Profiling... [7168/50176]	Loss: 1.6849
Profiling... [8192/50176]	Loss: 1.6985
Profiling... [9216/50176]	Loss: 1.6334
Profiling... [10240/50176]	Loss: 1.7078
Profiling... [11264/50176]	Loss: 1.7154
Profiling... [12288/50176]	Loss: 1.7137
Profiling... [13312/50176]	Loss: 1.7998
Profile done
epoch 1 train time consumed: 17.34s
Validation Epoch: 7, Average loss: 0.0017, Accuracy: 0.5055
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.2694531010574,
                        "time": 12.637353464000626,
                        "accuracy": 0.50546875,
                        "total_cost": 2881.88107066976
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8653
Profiling... [2048/50176]	Loss: 1.7717
Profiling... [3072/50176]	Loss: 1.8266
Profiling... [4096/50176]	Loss: 1.7323
Profiling... [5120/50176]	Loss: 1.8031
Profiling... [6144/50176]	Loss: 1.7389
Profiling... [7168/50176]	Loss: 1.7167
Profiling... [8192/50176]	Loss: 1.6900
Profiling... [9216/50176]	Loss: 1.7076
Profiling... [10240/50176]	Loss: 1.7661
Profiling... [11264/50176]	Loss: 1.6534
Profiling... [12288/50176]	Loss: 1.7212
Profiling... [13312/50176]	Loss: 1.7205
Profile done
epoch 1 train time consumed: 12.93s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.5014
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.28754097348427,
                        "time": 8.912246000996674,
                        "accuracy": 0.5013671875,
                        "total_cost": 2049.3381928901667
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8201
Profiling... [2048/50176]	Loss: 1.8013
Profiling... [3072/50176]	Loss: 1.7984
Profiling... [4096/50176]	Loss: 1.8264
Profiling... [5120/50176]	Loss: 1.7956
Profiling... [6144/50176]	Loss: 1.7778
Profiling... [7168/50176]	Loss: 1.6357
Profiling... [8192/50176]	Loss: 1.7557
Profiling... [9216/50176]	Loss: 1.8120
Profiling... [10240/50176]	Loss: 1.7087
Profiling... [11264/50176]	Loss: 1.7467
Profiling... [12288/50176]	Loss: 1.7748
Profiling... [13312/50176]	Loss: 1.7326
Profile done
epoch 1 train time consumed: 13.02s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.5000
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.34415438543772,
                        "time": 9.057883680005034,
                        "accuracy": 0.5,
                        "total_cost": 2089.547867183675
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7630
Profiling... [2048/50176]	Loss: 1.7853
Profiling... [3072/50176]	Loss: 1.7806
Profiling... [4096/50176]	Loss: 1.7393
Profiling... [5120/50176]	Loss: 1.7888
Profiling... [6144/50176]	Loss: 1.7406
Profiling... [7168/50176]	Loss: 1.7633
Profiling... [8192/50176]	Loss: 1.8819
Profiling... [9216/50176]	Loss: 1.7729
Profiling... [10240/50176]	Loss: 1.7021
Profiling... [11264/50176]	Loss: 1.7597
Profiling... [12288/50176]	Loss: 1.6947
Profiling... [13312/50176]	Loss: 1.6954
Profile done
epoch 1 train time consumed: 13.67s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.5031
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.35362922697591,
                        "time": 9.57853368200449,
                        "accuracy": 0.503125,
                        "total_cost": 2196.1115486053095
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8020
Profiling... [2048/50176]	Loss: 1.7937
Profiling... [3072/50176]	Loss: 1.7877
Profiling... [4096/50176]	Loss: 1.8081
Profiling... [5120/50176]	Loss: 1.7689
Profiling... [6144/50176]	Loss: 1.8604
Profiling... [7168/50176]	Loss: 1.8174
Profiling... [8192/50176]	Loss: 1.7106
Profiling... [9216/50176]	Loss: 1.8359
Profiling... [10240/50176]	Loss: 1.7493
Profiling... [11264/50176]	Loss: 1.7176
Profiling... [12288/50176]	Loss: 1.7468
Profiling... [13312/50176]	Loss: 1.7003
Profile done
epoch 1 train time consumed: 18.86s
Validation Epoch: 7, Average loss: 0.0017, Accuracy: 0.5036
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.27667365900524,
                        "time": 13.418379741000535,
                        "accuracy": 0.50361328125,
                        "total_cost": 3071.456293997262
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8107
Profiling... [2048/50176]	Loss: 1.8179
Profiling... [3072/50176]	Loss: 1.6985
Profiling... [4096/50176]	Loss: 1.7274
Profiling... [5120/50176]	Loss: 1.7100
Profiling... [6144/50176]	Loss: 1.7451
Profiling... [7168/50176]	Loss: 1.7388
Profiling... [8192/50176]	Loss: 1.6405
Profiling... [9216/50176]	Loss: 1.7558
Profiling... [10240/50176]	Loss: 1.7270
Profiling... [11264/50176]	Loss: 1.8167
Profiling... [12288/50176]	Loss: 1.7401
Profiling... [13312/50176]	Loss: 1.7087
Profile done
epoch 1 train time consumed: 13.18s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3665
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.29539158031582,
                        "time": 9.137037854998198,
                        "accuracy": 0.36650390625,
                        "total_cost": 2874.3441458918587
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8639
Profiling... [2048/50176]	Loss: 1.8227
Profiling... [3072/50176]	Loss: 1.7328
Profiling... [4096/50176]	Loss: 1.7851
Profiling... [5120/50176]	Loss: 1.6719
Profiling... [6144/50176]	Loss: 1.7824
Profiling... [7168/50176]	Loss: 1.7967
Profiling... [8192/50176]	Loss: 1.8074
Profiling... [9216/50176]	Loss: 1.7589
Profiling... [10240/50176]	Loss: 1.7582
Profiling... [11264/50176]	Loss: 1.6924
Profiling... [12288/50176]	Loss: 1.7044
Profiling... [13312/50176]	Loss: 1.7116
Profile done
epoch 1 train time consumed: 12.93s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4387
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.34831474925272,
                        "time": 8.935789700000896,
                        "accuracy": 0.438671875,
                        "total_cost": 2349.6566376607434
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8802
Profiling... [2048/50176]	Loss: 1.8110
Profiling... [3072/50176]	Loss: 1.6901
Profiling... [4096/50176]	Loss: 1.8327
Profiling... [5120/50176]	Loss: 1.6844
Profiling... [6144/50176]	Loss: 1.7002
Profiling... [7168/50176]	Loss: 1.6991
Profiling... [8192/50176]	Loss: 1.7319
Profiling... [9216/50176]	Loss: 1.7056
Profiling... [10240/50176]	Loss: 1.6613
Profiling... [11264/50176]	Loss: 1.7363
Profiling... [12288/50176]	Loss: 1.8595
Profiling... [13312/50176]	Loss: 1.7955
Profile done
epoch 1 train time consumed: 13.70s
Validation Epoch: 7, Average loss: 0.0018, Accuracy: 0.4836
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.36109790394181,
                        "time": 9.589240023997263,
                        "accuracy": 0.48359375,
                        "total_cost": 2287.5094172179556
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8114
Profiling... [2048/50176]	Loss: 1.7202
Profiling... [3072/50176]	Loss: 1.7559
Profiling... [4096/50176]	Loss: 1.7713
Profiling... [5120/50176]	Loss: 1.7170
Profiling... [6144/50176]	Loss: 1.7762
Profiling... [7168/50176]	Loss: 1.7095
Profiling... [8192/50176]	Loss: 1.6817
Profiling... [9216/50176]	Loss: 1.7143
Profiling... [10240/50176]	Loss: 1.7362
Profiling... [11264/50176]	Loss: 1.8307
Profiling... [12288/50176]	Loss: 1.7235
Profiling... [13312/50176]	Loss: 1.7484
Profile done
epoch 1 train time consumed: 17.48s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4679
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.29359228950673,
                        "time": 12.613060146999487,
                        "accuracy": 0.46787109375,
                        "total_cost": 3108.1317771860845
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8336
Profiling... [2048/50176]	Loss: 1.7549
Profiling... [3072/50176]	Loss: 1.7427
Profiling... [4096/50176]	Loss: 1.8387
Profiling... [5120/50176]	Loss: 1.7083
Profiling... [6144/50176]	Loss: 1.7967
Profiling... [7168/50176]	Loss: 1.7343
Profiling... [8192/50176]	Loss: 1.7840
Profiling... [9216/50176]	Loss: 1.7792
Profiling... [10240/50176]	Loss: 1.6557
Profiling... [11264/50176]	Loss: 1.8235
Profiling... [12288/50176]	Loss: 1.6819
Profiling... [13312/50176]	Loss: 1.7057
Profile done
epoch 1 train time consumed: 13.09s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4218
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.31255382917404,
                        "time": 9.074319465005829,
                        "accuracy": 0.42177734375,
                        "total_cost": 2480.889424899571
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8189
Profiling... [2048/50176]	Loss: 1.8135
Profiling... [3072/50176]	Loss: 1.7106
Profiling... [4096/50176]	Loss: 1.7807
Profiling... [5120/50176]	Loss: 1.6973
Profiling... [6144/50176]	Loss: 1.7740
Profiling... [7168/50176]	Loss: 1.7931
Profiling... [8192/50176]	Loss: 1.7824
Profiling... [9216/50176]	Loss: 1.7748
Profiling... [10240/50176]	Loss: 1.7044
Profiling... [11264/50176]	Loss: 1.7756
Profiling... [12288/50176]	Loss: 1.7570
Profiling... [13312/50176]	Loss: 1.6775
Profile done
epoch 1 train time consumed: 12.83s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4695
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.36564025235684,
                        "time": 8.844268246997672,
                        "accuracy": 0.46953125,
                        "total_cost": 2173.0708422037424
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8190
Profiling... [2048/50176]	Loss: 1.7337
Profiling... [3072/50176]	Loss: 1.8287
Profiling... [4096/50176]	Loss: 1.7248
Profiling... [5120/50176]	Loss: 1.7035
Profiling... [6144/50176]	Loss: 1.7431
Profiling... [7168/50176]	Loss: 1.7586
Profiling... [8192/50176]	Loss: 1.7194
Profiling... [9216/50176]	Loss: 1.6612
Profiling... [10240/50176]	Loss: 1.6856
Profiling... [11264/50176]	Loss: 1.6848
Profiling... [12288/50176]	Loss: 1.8314
Profiling... [13312/50176]	Loss: 1.7797
Profile done
epoch 1 train time consumed: 13.73s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4220
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.37582041826015,
                        "time": 9.61050541199802,
                        "accuracy": 0.42197265625,
                        "total_cost": 2627.7056821579313
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7291
Profiling... [2048/50176]	Loss: 1.8273
Profiling... [3072/50176]	Loss: 1.7041
Profiling... [4096/50176]	Loss: 1.7964
Profiling... [5120/50176]	Loss: 1.7327
Profiling... [6144/50176]	Loss: 1.7206
Profiling... [7168/50176]	Loss: 1.6293
Profiling... [8192/50176]	Loss: 1.7156
Profiling... [9216/50176]	Loss: 1.6519
Profiling... [10240/50176]	Loss: 1.6702
Profiling... [11264/50176]	Loss: 1.7947
Profiling... [12288/50176]	Loss: 1.7085
Profiling... [13312/50176]	Loss: 1.7375
Profile done
epoch 1 train time consumed: 16.66s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4430
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.31162955856891,
                        "time": 11.873594424003386,
                        "accuracy": 0.44296875,
                        "total_cost": 3090.880613473
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7519
Profiling... [2048/50176]	Loss: 1.6827
Profiling... [3072/50176]	Loss: 1.7954
Profiling... [4096/50176]	Loss: 1.8602
Profiling... [5120/50176]	Loss: 1.7533
Profiling... [6144/50176]	Loss: 1.7066
Profiling... [7168/50176]	Loss: 1.7284
Profiling... [8192/50176]	Loss: 1.7189
Profiling... [9216/50176]	Loss: 1.8187
Profiling... [10240/50176]	Loss: 1.7762
Profiling... [11264/50176]	Loss: 1.7219
Profiling... [12288/50176]	Loss: 1.7347
Profiling... [13312/50176]	Loss: 1.7052
Profile done
epoch 1 train time consumed: 12.78s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4559
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.33197333803062,
                        "time": 8.827737063002132,
                        "accuracy": 0.455859375,
                        "total_cost": 2233.4087910011867
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.8403
Profiling... [2048/50176]	Loss: 1.8112
Profiling... [3072/50176]	Loss: 1.7152
Profiling... [4096/50176]	Loss: 1.7458
Profiling... [5120/50176]	Loss: 1.7299
Profiling... [6144/50176]	Loss: 1.8641
Profiling... [7168/50176]	Loss: 1.7697
Profiling... [8192/50176]	Loss: 1.7366
Profiling... [9216/50176]	Loss: 1.7841
Profiling... [10240/50176]	Loss: 1.7373
Profiling... [11264/50176]	Loss: 1.6263
Profiling... [12288/50176]	Loss: 1.7545
Profiling... [13312/50176]	Loss: 1.7608
Profile done
epoch 1 train time consumed: 12.91s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4316
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.38505472596125,
                        "time": 8.939371179003501,
                        "accuracy": 0.431640625,
                        "total_cost": 2389.6495671717644
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8404
Profiling... [2048/50176]	Loss: 1.6752
Profiling... [3072/50176]	Loss: 1.6853
Profiling... [4096/50176]	Loss: 1.8075
Profiling... [5120/50176]	Loss: 1.7837
Profiling... [6144/50176]	Loss: 1.7372
Profiling... [7168/50176]	Loss: 1.6843
Profiling... [8192/50176]	Loss: 1.8107
Profiling... [9216/50176]	Loss: 1.7615
Profiling... [10240/50176]	Loss: 1.7515
Profiling... [11264/50176]	Loss: 1.6878
Profiling... [12288/50176]	Loss: 1.7573
Profiling... [13312/50176]	Loss: 1.7144
Profile done
epoch 1 train time consumed: 13.82s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3767
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.39345317298445,
                        "time": 9.603019562004192,
                        "accuracy": 0.37666015625,
                        "total_cost": 2941.977190207212
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7515
Profiling... [2048/50176]	Loss: 1.7372
Profiling... [3072/50176]	Loss: 1.6908
Profiling... [4096/50176]	Loss: 1.7199
Profiling... [5120/50176]	Loss: 1.7610
Profiling... [6144/50176]	Loss: 1.7400
Profiling... [7168/50176]	Loss: 1.7524
Profiling... [8192/50176]	Loss: 1.8263
Profiling... [9216/50176]	Loss: 1.6847
Profiling... [10240/50176]	Loss: 1.7421
Profiling... [11264/50176]	Loss: 1.6265
Profiling... [12288/50176]	Loss: 1.6895
Profiling... [13312/50176]	Loss: 1.6839
Profile done
epoch 1 train time consumed: 18.78s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4553
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.32092047321251,
                        "time": 13.316871016999357,
                        "accuracy": 0.4552734375,
                        "total_cost": 3373.1680722168458
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8352
Profiling... [2048/50176]	Loss: 1.7965
Profiling... [3072/50176]	Loss: 1.8358
Profiling... [4096/50176]	Loss: 1.7833
Profiling... [5120/50176]	Loss: 1.7289
Profiling... [6144/50176]	Loss: 1.6815
Profiling... [7168/50176]	Loss: 1.7910
Profiling... [8192/50176]	Loss: 1.8015
Profiling... [9216/50176]	Loss: 1.7895
Profiling... [10240/50176]	Loss: 1.8448
Profiling... [11264/50176]	Loss: 1.8554
Profiling... [12288/50176]	Loss: 1.8730
Profiling... [13312/50176]	Loss: 1.8496
Profile done
epoch 1 train time consumed: 12.95s
Validation Epoch: 7, Average loss: 0.0027, Accuracy: 0.3150
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.34284359480529,
                        "time": 9.026157854001212,
                        "accuracy": 0.3150390625,
                        "total_cost": 3304.6781734124957
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7528
Profiling... [2048/50176]	Loss: 1.7516
Profiling... [3072/50176]	Loss: 1.7978
Profiling... [4096/50176]	Loss: 1.8151
Profiling... [5120/50176]	Loss: 1.7863
Profiling... [6144/50176]	Loss: 1.8829
Profiling... [7168/50176]	Loss: 1.8564
Profiling... [8192/50176]	Loss: 1.8488
Profiling... [9216/50176]	Loss: 1.8821
Profiling... [10240/50176]	Loss: 1.7568
Profiling... [11264/50176]	Loss: 1.8209
Profiling... [12288/50176]	Loss: 1.8476
Profiling... [13312/50176]	Loss: 1.8400
Profile done
epoch 1 train time consumed: 12.95s
Validation Epoch: 7, Average loss: 0.0042, Accuracy: 0.2010
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.39294171269832,
                        "time": 8.952548779998324,
                        "accuracy": 0.2009765625,
                        "total_cost": 5140.206035469609
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8793
Profiling... [2048/50176]	Loss: 1.7365
Profiling... [3072/50176]	Loss: 1.7273
Profiling... [4096/50176]	Loss: 1.7606
Profiling... [5120/50176]	Loss: 1.8302
Profiling... [6144/50176]	Loss: 1.8963
Profiling... [7168/50176]	Loss: 1.8100
Profiling... [8192/50176]	Loss: 1.7599
Profiling... [9216/50176]	Loss: 1.7862
Profiling... [10240/50176]	Loss: 1.6751
Profiling... [11264/50176]	Loss: 1.7568
Profiling... [12288/50176]	Loss: 1.7589
Profiling... [13312/50176]	Loss: 1.8306
Profile done
epoch 1 train time consumed: 13.68s
Validation Epoch: 7, Average loss: 0.0031, Accuracy: 0.2986
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.4028559565588,
                        "time": 9.554035840999859,
                        "accuracy": 0.2986328125,
                        "total_cost": 3692.0357569974226
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8573
Profiling... [2048/50176]	Loss: 1.7664
Profiling... [3072/50176]	Loss: 1.8624
Profiling... [4096/50176]	Loss: 1.7702
Profiling... [5120/50176]	Loss: 1.8303
Profiling... [6144/50176]	Loss: 1.8590
Profiling... [7168/50176]	Loss: 1.7758
Profiling... [8192/50176]	Loss: 1.8219
Profiling... [9216/50176]	Loss: 1.7814
Profiling... [10240/50176]	Loss: 1.8518
Profiling... [11264/50176]	Loss: 1.7925
Profiling... [12288/50176]	Loss: 1.7599
Profiling... [13312/50176]	Loss: 1.7293
Profile done
epoch 1 train time consumed: 21.62s
Validation Epoch: 7, Average loss: 0.0028, Accuracy: 0.3239
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.31734729135171,
                        "time": 16.909941583995533,
                        "accuracy": 0.32392578125,
                        "total_cost": 6019.927153662095
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.8456
Profiling... [2048/50176]	Loss: 1.7629
Profiling... [3072/50176]	Loss: 1.7466
Profiling... [4096/50176]	Loss: 1.7378
Profiling... [5120/50176]	Loss: 1.8352
Profiling... [6144/50176]	Loss: 1.8807
Profiling... [7168/50176]	Loss: 1.8012
Profiling... [8192/50176]	Loss: 1.7430
Profiling... [9216/50176]	Loss: 1.8759
Profiling... [10240/50176]	Loss: 1.8748
Profiling... [11264/50176]	Loss: 1.7884
Profiling... [12288/50176]	Loss: 1.8265
Profiling... [13312/50176]	Loss: 1.8476
Profile done
epoch 1 train time consumed: 12.83s
Validation Epoch: 7, Average loss: 0.0052, Accuracy: 0.1616
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.33881278323746,
                        "time": 8.85371400800068,
                        "accuracy": 0.16162109375,
                        "total_cost": 6318.339015727131
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7846
Profiling... [2048/50176]	Loss: 1.7995
Profiling... [3072/50176]	Loss: 1.8583
Profiling... [4096/50176]	Loss: 1.9236
Profiling... [5120/50176]	Loss: 1.8242
Profiling... [6144/50176]	Loss: 1.7979
Profiling... [7168/50176]	Loss: 1.7976
Profiling... [8192/50176]	Loss: 1.7148
Profiling... [9216/50176]	Loss: 1.7764
Profiling... [10240/50176]	Loss: 1.7480
Profiling... [11264/50176]	Loss: 1.7802
Profiling... [12288/50176]	Loss: 1.7426
Profiling... [13312/50176]	Loss: 1.7515
Profile done
epoch 1 train time consumed: 13.26s
Validation Epoch: 7, Average loss: 0.0041, Accuracy: 0.2210
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.38554622823699,
                        "time": 9.046440455000266,
                        "accuracy": 0.22099609375,
                        "total_cost": 4723.2892473757865
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8438
Profiling... [2048/50176]	Loss: 1.7732
Profiling... [3072/50176]	Loss: 1.8144
Profiling... [4096/50176]	Loss: 1.8361
Profiling... [5120/50176]	Loss: 1.6736
Profiling... [6144/50176]	Loss: 1.7828
Profiling... [7168/50176]	Loss: 1.8558
Profiling... [8192/50176]	Loss: 1.8609
Profiling... [9216/50176]	Loss: 1.9061
Profiling... [10240/50176]	Loss: 1.8437
Profiling... [11264/50176]	Loss: 1.7770
Profiling... [12288/50176]	Loss: 1.7763
Profiling... [13312/50176]	Loss: 1.8229
Profile done
epoch 1 train time consumed: 13.71s
Validation Epoch: 7, Average loss: 0.0030, Accuracy: 0.3193
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.39476207180356,
                        "time": 9.559320334003132,
                        "accuracy": 0.3193359375,
                        "total_cost": 3454.3418575005985
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8159
Profiling... [2048/50176]	Loss: 1.7713
Profiling... [3072/50176]	Loss: 1.8469
Profiling... [4096/50176]	Loss: 1.7996
Profiling... [5120/50176]	Loss: 1.8013
Profiling... [6144/50176]	Loss: 1.8725
Profiling... [7168/50176]	Loss: 1.8644
Profiling... [8192/50176]	Loss: 1.8354
Profiling... [9216/50176]	Loss: 1.7371
Profiling... [10240/50176]	Loss: 1.7915
Profiling... [11264/50176]	Loss: 1.8371
Profiling... [12288/50176]	Loss: 1.8785
Profiling... [13312/50176]	Loss: 1.7812
Profile done
epoch 1 train time consumed: 19.43s
Validation Epoch: 7, Average loss: 0.0025, Accuracy: 0.3479
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.32292663137852,
                        "time": 13.951981400998193,
                        "accuracy": 0.3478515625,
                        "total_cost": 4625.488285595015
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7659
Profiling... [2048/50176]	Loss: 1.7539
Profiling... [3072/50176]	Loss: 1.8007
Profiling... [4096/50176]	Loss: 1.8173
Profiling... [5120/50176]	Loss: 1.8362
Profiling... [6144/50176]	Loss: 1.8137
Profiling... [7168/50176]	Loss: 1.8658
Profiling... [8192/50176]	Loss: 1.8166
Profiling... [9216/50176]	Loss: 1.8569
Profiling... [10240/50176]	Loss: 1.7467
Profiling... [11264/50176]	Loss: 1.7393
Profiling... [12288/50176]	Loss: 1.7643
Profiling... [13312/50176]	Loss: 1.8533
Profile done
epoch 1 train time consumed: 13.03s
Validation Epoch: 7, Average loss: 0.0042, Accuracy: 0.2259
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.34167312217646,
                        "time": 8.953518435999285,
                        "accuracy": 0.22587890625,
                        "total_cost": 4571.979800519377
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7380
Profiling... [2048/50176]	Loss: 1.8255
Profiling... [3072/50176]	Loss: 1.8327
Profiling... [4096/50176]	Loss: 1.8331
Profiling... [5120/50176]	Loss: 1.7896
Profiling... [6144/50176]	Loss: 1.8438
Profiling... [7168/50176]	Loss: 1.7498
Profiling... [8192/50176]	Loss: 1.7528
Profiling... [9216/50176]	Loss: 1.9538
Profiling... [10240/50176]	Loss: 1.7705
Profiling... [11264/50176]	Loss: 1.8519
Profiling... [12288/50176]	Loss: 1.7783
Profiling... [13312/50176]	Loss: 1.8450
Profile done
epoch 1 train time consumed: 13.32s
Validation Epoch: 7, Average loss: 0.0032, Accuracy: 0.2846
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.39033637338582,
                        "time": 9.092193690004933,
                        "accuracy": 0.2845703125,
                        "total_cost": 3686.791074742364
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7380
Profiling... [2048/50176]	Loss: 1.7868
Profiling... [3072/50176]	Loss: 1.7509
Profiling... [4096/50176]	Loss: 1.8136
Profiling... [5120/50176]	Loss: 1.8279
Profiling... [6144/50176]	Loss: 1.8405
Profiling... [7168/50176]	Loss: 1.7933
Profiling... [8192/50176]	Loss: 1.8368
Profiling... [9216/50176]	Loss: 1.8131
Profiling... [10240/50176]	Loss: 1.8105
Profiling... [11264/50176]	Loss: 1.7421
Profiling... [12288/50176]	Loss: 1.8335
Profiling... [13312/50176]	Loss: 1.8400
Profile done
epoch 1 train time consumed: 13.79s
Validation Epoch: 7, Average loss: 0.0031, Accuracy: 0.2803
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.39792368700907,
                        "time": 9.573421221000899,
                        "accuracy": 0.2802734375,
                        "total_cost": 3941.696870523647
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6252
Profiling... [2048/50176]	Loss: 1.7303
Profiling... [3072/50176]	Loss: 1.8058
Profiling... [4096/50176]	Loss: 1.7329
Profiling... [5120/50176]	Loss: 1.7810
Profiling... [6144/50176]	Loss: 1.7653
Profiling... [7168/50176]	Loss: 1.8348
Profiling... [8192/50176]	Loss: 1.7905
Profiling... [9216/50176]	Loss: 1.7426
Profiling... [10240/50176]	Loss: 1.7736
Profiling... [11264/50176]	Loss: 1.8011
Profiling... [12288/50176]	Loss: 1.8409
Profiling... [13312/50176]	Loss: 1.8340
Profile done
epoch 1 train time consumed: 18.19s
Validation Epoch: 7, Average loss: 0.0030, Accuracy: 0.3019
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.32800977574165,
                        "time": 12.70180502400035,
                        "accuracy": 0.30185546875,
                        "total_cost": 4852.898309391576
                    },
                    
[Training Loop] The optimal parameters are lr: 0.005 dr: 0.0 bs: 128 pl: 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[GPU_0] Set GPU power limit to 125W.
[GPU_0] Set GPU power limit to 125W.
Training Epoch: 7 [128/50048]	Loss: 1.8347
Training Epoch: 7 [256/50048]	Loss: 1.6878
Training Epoch: 7 [384/50048]	Loss: 1.8291
Training Epoch: 7 [512/50048]	Loss: 1.8304
Training Epoch: 7 [640/50048]	Loss: 1.8995
Training Epoch: 7 [768/50048]	Loss: 1.8529
Training Epoch: 7 [896/50048]	Loss: 1.8024
Training Epoch: 7 [1024/50048]	Loss: 2.1482
Training Epoch: 7 [1152/50048]	Loss: 1.9215
Training Epoch: 7 [1280/50048]	Loss: 1.6311
Training Epoch: 7 [1408/50048]	Loss: 2.0709
Training Epoch: 7 [1536/50048]	Loss: 1.8800
Training Epoch: 7 [1664/50048]	Loss: 1.8977
Training Epoch: 7 [1792/50048]	Loss: 1.9681
Training Epoch: 7 [1920/50048]	Loss: 2.0164
Training Epoch: 7 [2048/50048]	Loss: 2.0628
Training Epoch: 7 [2176/50048]	Loss: 1.8842
Training Epoch: 7 [2304/50048]	Loss: 1.6583
Training Epoch: 7 [2432/50048]	Loss: 1.7365
Training Epoch: 7 [2560/50048]	Loss: 2.0526
Training Epoch: 7 [2688/50048]	Loss: 1.9160
Training Epoch: 7 [2816/50048]	Loss: 2.0676
Training Epoch: 7 [2944/50048]	Loss: 1.6248
Training Epoch: 7 [3072/50048]	Loss: 1.6700
Training Epoch: 7 [3200/50048]	Loss: 2.0213
Training Epoch: 7 [3328/50048]	Loss: 1.8625
Training Epoch: 7 [3456/50048]	Loss: 1.8556
Training Epoch: 7 [3584/50048]	Loss: 1.8458
Training Epoch: 7 [3712/50048]	Loss: 1.8550
Training Epoch: 7 [3840/50048]	Loss: 1.9442
Training Epoch: 7 [3968/50048]	Loss: 1.6487
Training Epoch: 7 [4096/50048]	Loss: 1.9290
Training Epoch: 7 [4224/50048]	Loss: 2.0788
Training Epoch: 7 [4352/50048]	Loss: 1.7449
Training Epoch: 7 [4480/50048]	Loss: 1.7181
Training Epoch: 7 [4608/50048]	Loss: 1.8109
Training Epoch: 7 [4736/50048]	Loss: 1.9099
Training Epoch: 7 [4864/50048]	Loss: 1.8718
Training Epoch: 7 [4992/50048]	Loss: 1.8623
Training Epoch: 7 [5120/50048]	Loss: 1.7586
Training Epoch: 7 [5248/50048]	Loss: 1.7980
Training Epoch: 7 [5376/50048]	Loss: 1.7281
Training Epoch: 7 [5504/50048]	Loss: 1.7058
Training Epoch: 7 [5632/50048]	Loss: 1.8566
Training Epoch: 7 [5760/50048]	Loss: 1.6683
Training Epoch: 7 [5888/50048]	Loss: 1.6301
Training Epoch: 7 [6016/50048]	Loss: 1.9674
Training Epoch: 7 [6144/50048]	Loss: 1.9970
Training Epoch: 7 [6272/50048]	Loss: 1.7544
Training Epoch: 7 [6400/50048]	Loss: 1.5601
Training Epoch: 7 [6528/50048]	Loss: 1.8786
Training Epoch: 7 [6656/50048]	Loss: 1.7857
Training Epoch: 7 [6784/50048]	Loss: 1.8002
Training Epoch: 7 [6912/50048]	Loss: 1.7695
Training Epoch: 7 [7040/50048]	Loss: 1.8558
Training Epoch: 7 [7168/50048]	Loss: 1.7949
Training Epoch: 7 [7296/50048]	Loss: 1.7927
Training Epoch: 7 [7424/50048]	Loss: 1.7955
Training Epoch: 7 [7552/50048]	Loss: 1.7328
Training Epoch: 7 [7680/50048]	Loss: 2.1289
Training Epoch: 7 [7808/50048]	Loss: 1.7174
Training Epoch: 7 [7936/50048]	Loss: 1.6160
Training Epoch: 7 [8064/50048]	Loss: 1.5243
Training Epoch: 7 [8192/50048]	Loss: 1.7013
Training Epoch: 7 [8320/50048]	Loss: 2.0861
Training Epoch: 7 [8448/50048]	Loss: 1.8903
Training Epoch: 7 [8576/50048]	Loss: 2.0024
Training Epoch: 7 [8704/50048]	Loss: 1.7803
Training Epoch: 7 [8832/50048]	Loss: 1.8470
Training Epoch: 7 [8960/50048]	Loss: 1.8596
Training Epoch: 7 [9088/50048]	Loss: 1.6783
Training Epoch: 7 [9216/50048]	Loss: 1.9388
Training Epoch: 7 [9344/50048]	Loss: 1.7969
Training Epoch: 7 [9472/50048]	Loss: 2.1593
Training Epoch: 7 [9600/50048]	Loss: 1.7972
Training Epoch: 7 [9728/50048]	Loss: 1.8528
Training Epoch: 7 [9856/50048]	Loss: 1.7706
Training Epoch: 7 [9984/50048]	Loss: 1.8068
Training Epoch: 7 [10112/50048]	Loss: 1.7075
Training Epoch: 7 [10240/50048]	Loss: 1.7226
Training Epoch: 7 [10368/50048]	Loss: 1.9900
Training Epoch: 7 [10496/50048]	Loss: 1.8210
Training Epoch: 7 [10624/50048]	Loss: 1.8112
Training Epoch: 7 [10752/50048]	Loss: 1.8171
Training Epoch: 7 [10880/50048]	Loss: 1.7361
Training Epoch: 7 [11008/50048]	Loss: 1.8071
Training Epoch: 7 [11136/50048]	Loss: 2.0677
Training Epoch: 7 [11264/50048]	Loss: 1.8001
Training Epoch: 7 [11392/50048]	Loss: 1.8073
Training Epoch: 7 [11520/50048]	Loss: 1.8529
Training Epoch: 7 [11648/50048]	Loss: 1.7558
Training Epoch: 7 [11776/50048]	Loss: 1.5648
Training Epoch: 7 [11904/50048]	Loss: 1.7828
Training Epoch: 7 [12032/50048]	Loss: 1.8622
Training Epoch: 7 [12160/50048]	Loss: 1.5726
Training Epoch: 7 [12288/50048]	Loss: 1.8934
Training Epoch: 7 [12416/50048]	Loss: 1.9878
Training Epoch: 7 [12544/50048]	Loss: 1.4383
Training Epoch: 7 [12672/50048]	Loss: 1.9812
Training Epoch: 7 [12800/50048]	Loss: 1.8052
Training Epoch: 7 [12928/50048]	Loss: 1.8835
Training Epoch: 7 [13056/50048]	Loss: 1.8194
Training Epoch: 7 [13184/50048]	Loss: 1.5517
Training Epoch: 7 [13312/50048]	Loss: 1.8425
Training Epoch: 7 [13440/50048]	Loss: 1.3950
Training Epoch: 7 [13568/50048]	Loss: 1.9154
Training Epoch: 7 [13696/50048]	Loss: 1.7671
Training Epoch: 7 [13824/50048]	Loss: 1.6950
Training Epoch: 7 [13952/50048]	Loss: 1.6886
Training Epoch: 7 [14080/50048]	Loss: 1.9358
Training Epoch: 7 [14208/50048]	Loss: 1.8552
Training Epoch: 7 [14336/50048]	Loss: 1.8623
Training Epoch: 7 [14464/50048]	Loss: 1.6202
Training Epoch: 7 [14592/50048]	Loss: 1.9878
Training Epoch: 7 [14720/50048]	Loss: 1.8171
Training Epoch: 7 [14848/50048]	Loss: 1.8677
Training Epoch: 7 [14976/50048]	Loss: 1.7820
Training Epoch: 7 [15104/50048]	Loss: 2.0899
Training Epoch: 7 [15232/50048]	Loss: 1.8892
Training Epoch: 7 [15360/50048]	Loss: 1.5659
Training Epoch: 7 [15488/50048]	Loss: 1.9526
Training Epoch: 7 [15616/50048]	Loss: 1.8394
Training Epoch: 7 [15744/50048]	Loss: 1.7020
Training Epoch: 7 [15872/50048]	Loss: 1.6901
Training Epoch: 7 [16000/50048]	Loss: 1.5099
Training Epoch: 7 [16128/50048]	Loss: 1.5646
Training Epoch: 7 [16256/50048]	Loss: 1.7062
Training Epoch: 7 [16384/50048]	Loss: 1.9221
Training Epoch: 7 [16512/50048]	Loss: 1.8435
Training Epoch: 7 [16640/50048]	Loss: 1.9241
Training Epoch: 7 [16768/50048]	Loss: 1.9227
Training Epoch: 7 [16896/50048]	Loss: 1.6260
Training Epoch: 7 [17024/50048]	Loss: 1.6297
Training Epoch: 7 [17152/50048]	Loss: 1.6631
Training Epoch: 7 [17280/50048]	Loss: 1.7916
Training Epoch: 7 [17408/50048]	Loss: 1.6319
Training Epoch: 7 [17536/50048]	Loss: 1.7792
Training Epoch: 7 [17664/50048]	Loss: 1.7297
Training Epoch: 7 [17792/50048]	Loss: 1.7145
Training Epoch: 7 [17920/50048]	Loss: 1.8656
Training Epoch: 7 [18048/50048]	Loss: 2.0515
Training Epoch: 7 [18176/50048]	Loss: 1.7199
Training Epoch: 7 [18304/50048]	Loss: 1.7756
Training Epoch: 7 [18432/50048]	Loss: 2.0448
Training Epoch: 7 [18560/50048]	Loss: 1.9678
Training Epoch: 7 [18688/50048]	Loss: 1.9376
Training Epoch: 7 [18816/50048]	Loss: 1.5870
Training Epoch: 7 [18944/50048]	Loss: 2.0306
Training Epoch: 7 [19072/50048]	Loss: 1.8223
Training Epoch: 7 [19200/50048]	Loss: 1.7795
Training Epoch: 7 [19328/50048]	Loss: 1.9507
Training Epoch: 7 [19456/50048]	Loss: 1.8127
Training Epoch: 7 [19584/50048]	Loss: 1.6475
Training Epoch: 7 [19712/50048]	Loss: 1.9183
Training Epoch: 7 [19840/50048]	Loss: 1.8040
Training Epoch: 7 [19968/50048]	Loss: 1.6352
Training Epoch: 7 [20096/50048]	Loss: 1.8192
Training Epoch: 7 [20224/50048]	Loss: 1.7570
Training Epoch: 7 [20352/50048]	Loss: 1.6664
Training Epoch: 7 [20480/50048]	Loss: 1.9972
Training Epoch: 7 [20608/50048]	Loss: 1.8888
Training Epoch: 7 [20736/50048]	Loss: 1.8873
Training Epoch: 7 [20864/50048]	Loss: 1.8932
Training Epoch: 7 [20992/50048]	Loss: 1.8581
Training Epoch: 7 [21120/50048]	Loss: 1.8202
Training Epoch: 7 [21248/50048]	Loss: 1.9739
Training Epoch: 7 [21376/50048]	Loss: 1.7075
Training Epoch: 7 [21504/50048]	Loss: 1.6622
Training Epoch: 7 [21632/50048]	Loss: 2.0050
Training Epoch: 7 [21760/50048]	Loss: 1.8843
Training Epoch: 7 [21888/50048]	Loss: 1.6436
Training Epoch: 7 [22016/50048]	Loss: 1.9942
Training Epoch: 7 [22144/50048]	Loss: 1.8794
Training Epoch: 7 [22272/50048]	Loss: 1.8400
Training Epoch: 7 [22400/50048]	Loss: 1.6289
Training Epoch: 7 [22528/50048]	Loss: 1.7936
Training Epoch: 7 [22656/50048]	Loss: 1.6663
Training Epoch: 7 [22784/50048]	Loss: 1.7371
Training Epoch: 7 [22912/50048]	Loss: 1.8582
Training Epoch: 7 [23040/50048]	Loss: 1.8573
Training Epoch: 7 [23168/50048]	Loss: 1.7633
Training Epoch: 7 [23296/50048]	Loss: 2.1227
Training Epoch: 7 [23424/50048]	Loss: 1.5449
Training Epoch: 7 [23552/50048]	Loss: 1.8812
Training Epoch: 7 [23680/50048]	Loss: 1.8195
Training Epoch: 7 [23808/50048]	Loss: 1.7193
Training Epoch: 7 [23936/50048]	Loss: 1.8267
Training Epoch: 7 [24064/50048]	Loss: 1.6108
Training Epoch: 7 [24192/50048]	Loss: 1.7022
Training Epoch: 7 [24320/50048]	Loss: 2.0300
Training Epoch: 7 [24448/50048]	Loss: 1.8055
Training Epoch: 7 [24576/50048]	Loss: 2.0525
Training Epoch: 7 [24704/50048]	Loss: 1.7864
Training Epoch: 7 [24832/50048]	Loss: 1.9210
Training Epoch: 7 [24960/50048]	Loss: 1.8187
Training Epoch: 7 [25088/50048]	Loss: 1.6602
Training Epoch: 7 [25216/50048]	Loss: 1.9848
Training Epoch: 7 [25344/50048]	Loss: 1.5331
Training Epoch: 7 [25472/50048]	Loss: 1.8667
Training Epoch: 7 [25600/50048]	Loss: 1.7308
Training Epoch: 7 [25728/50048]	Loss: 1.6594
Training Epoch: 7 [25856/50048]	Loss: 1.8183
Training Epoch: 7 [25984/50048]	Loss: 1.9156
Training Epoch: 7 [26112/50048]	Loss: 1.7124
Training Epoch: 7 [26240/50048]	Loss: 1.6385
Training Epoch: 7 [26368/50048]	Loss: 1.7225
Training Epoch: 7 [26496/50048]	Loss: 1.5210
Training Epoch: 7 [26624/50048]	Loss: 1.9798
Training Epoch: 7 [26752/50048]	Loss: 1.9367
Training Epoch: 7 [26880/50048]	Loss: 1.8677
Training Epoch: 7 [27008/50048]	Loss: 1.8419
Training Epoch: 7 [27136/50048]	Loss: 1.9674
Training Epoch: 7 [27264/50048]	Loss: 1.8216
Training Epoch: 7 [27392/50048]	Loss: 2.1380
Training Epoch: 7 [27520/50048]	Loss: 1.7390
Training Epoch: 7 [27648/50048]	Loss: 1.8097
Training Epoch: 7 [27776/50048]	Loss: 1.9202
Training Epoch: 7 [27904/50048]	Loss: 1.6359
Training Epoch: 7 [28032/50048]	Loss: 1.7520
Training Epoch: 7 [28160/50048]	Loss: 1.9416
Training Epoch: 7 [28288/50048]	Loss: 1.7538
Training Epoch: 7 [28416/50048]	Loss: 1.8111
Training Epoch: 7 [28544/50048]	Loss: 1.5604
Training Epoch: 7 [28672/50048]	Loss: 1.5782
Training Epoch: 7 [28800/50048]	Loss: 1.7293
Training Epoch: 7 [28928/50048]	Loss: 1.8617
Training Epoch: 7 [29056/50048]	Loss: 1.8432
Training Epoch: 7 [29184/50048]	Loss: 1.5659
Training Epoch: 7 [29312/50048]	Loss: 1.8582
Training Epoch: 7 [29440/50048]	Loss: 1.8934
Training Epoch: 7 [29568/50048]	Loss: 1.8404
Training Epoch: 7 [29696/50048]	Loss: 1.6642
Training Epoch: 7 [29824/50048]	Loss: 1.6939
Training Epoch: 7 [29952/50048]	Loss: 1.8925
Training Epoch: 7 [30080/50048]	Loss: 1.6562
Training Epoch: 7 [30208/50048]	Loss: 1.6015
Training Epoch: 7 [30336/50048]	Loss: 1.8478
Training Epoch: 7 [30464/50048]	Loss: 1.4856
Training Epoch: 7 [30592/50048]	Loss: 1.6805
Training Epoch: 7 [30720/50048]	Loss: 1.7249
Training Epoch: 7 [30848/50048]	Loss: 1.6647
Training Epoch: 7 [30976/50048]	Loss: 1.6791
Training Epoch: 7 [31104/50048]	Loss: 1.6566
Training Epoch: 7 [31232/50048]	Loss: 1.7182
Training Epoch: 7 [31360/50048]	Loss: 1.7490
Training Epoch: 7 [31488/50048]	Loss: 1.6839
Training Epoch: 7 [31616/50048]	Loss: 1.6487
Training Epoch: 7 [31744/50048]	Loss: 1.7839
Training Epoch: 7 [31872/50048]	Loss: 1.5897
Training Epoch: 7 [32000/50048]	Loss: 1.6619
Training Epoch: 7 [32128/50048]	Loss: 1.7631
Training Epoch: 7 [32256/50048]	Loss: 1.7593
Training Epoch: 7 [32384/50048]	Loss: 1.4455
Training Epoch: 7 [32512/50048]	Loss: 1.8547
Training Epoch: 7 [32640/50048]	Loss: 1.6820
Training Epoch: 7 [32768/50048]	Loss: 1.8179
Training Epoch: 7 [32896/50048]	Loss: 1.6163
Training Epoch: 7 [33024/50048]	Loss: 1.9256
Training Epoch: 7 [33152/50048]	Loss: 1.7169
Training Epoch: 7 [33280/50048]	Loss: 1.6258
Training Epoch: 7 [33408/50048]	Loss: 1.9245
Training Epoch: 7 [33536/50048]	Loss: 1.8725
Training Epoch: 7 [33664/50048]	Loss: 1.5148
Training Epoch: 7 [33792/50048]	Loss: 1.9299
Training Epoch: 7 [33920/50048]	Loss: 1.7092
Training Epoch: 7 [34048/50048]	Loss: 1.7878
Training Epoch: 7 [34176/50048]	Loss: 1.9275
Training Epoch: 7 [34304/50048]	Loss: 2.0461
Training Epoch: 7 [34432/50048]	Loss: 1.7738
Training Epoch: 7 [34560/50048]	Loss: 1.7081
Training Epoch: 7 [34688/50048]	Loss: 1.8088
Training Epoch: 7 [34816/50048]	Loss: 1.8765
Training Epoch: 7 [34944/50048]	Loss: 1.8312
Training Epoch: 7 [35072/50048]	Loss: 1.6066
Training Epoch: 7 [35200/50048]	Loss: 1.4695
Training Epoch: 7 [35328/50048]	Loss: 1.9394
Training Epoch: 7 [35456/50048]	Loss: 2.0644
Training Epoch: 7 [35584/50048]	Loss: 1.8636
Training Epoch: 7 [35712/50048]	Loss: 1.7206
Training Epoch: 7 [35840/50048]	Loss: 1.5604
Training Epoch: 7 [35968/50048]	Loss: 1.8262
Training Epoch: 7 [36096/50048]	Loss: 1.4550
Training Epoch: 7 [36224/50048]	Loss: 1.7565
Training Epoch: 7 [36352/50048]	Loss: 1.6940
Training Epoch: 7 [36480/50048]	Loss: 1.5182
Training Epoch: 7 [36608/50048]	Loss: 1.8453
Training Epoch: 7 [36736/50048]	Loss: 1.6885
Training Epoch: 7 [36864/50048]	Loss: 2.0255
Training Epoch: 7 [36992/50048]	Loss: 1.6459
Training Epoch: 7 [37120/50048]	Loss: 1.7666
Training Epoch: 7 [37248/50048]	Loss: 1.7469
Training Epoch: 7 [37376/50048]	Loss: 2.0484
Training Epoch: 7 [37504/50048]	Loss: 1.9665
Training Epoch: 7 [37632/50048]	Loss: 1.7542
Training Epoch: 7 [37760/50048]	Loss: 1.6950
Training Epoch: 7 [37888/50048]	Loss: 1.7930
Training Epoch: 7 [38016/50048]	Loss: 1.8775
Training Epoch: 7 [38144/50048]	Loss: 1.9982
Training Epoch: 7 [38272/50048]	Loss: 1.6416
Training Epoch: 7 [38400/50048]	Loss: 1.6488
Training Epoch: 7 [38528/50048]	Loss: 1.8102
Training Epoch: 7 [38656/50048]	Loss: 1.7428
Training Epoch: 7 [38784/50048]	Loss: 1.5454
Training Epoch: 7 [38912/50048]	Loss: 1.9133
Training Epoch: 7 [39040/50048]	Loss: 1.8405
Training Epoch: 7 [39168/50048]	Loss: 1.8783
Training Epoch: 7 [39296/50048]	Loss: 2.1609
Training Epoch: 7 [39424/50048]	Loss: 1.8455
Training Epoch: 7 [39552/50048]	Loss: 1.8405
Training Epoch: 7 [39680/50048]	Loss: 1.7305
Training Epoch: 7 [39808/50048]	Loss: 1.9700
Training Epoch: 7 [39936/50048]	Loss: 1.5706
Training Epoch: 7 [40064/50048]	Loss: 1.9457
Training Epoch: 7 [40192/50048]	Loss: 1.7357
Training Epoch: 7 [40320/50048]	Loss: 1.7157
Training Epoch: 7 [40448/50048]	Loss: 1.7065
Training Epoch: 7 [40576/50048]	Loss: 1.9336
Training Epoch: 7 [40704/50048]	Loss: 1.7260
Training Epoch: 7 [40832/50048]	Loss: 1.8476
Training Epoch: 7 [40960/50048]	Loss: 1.7785
Training Epoch: 7 [41088/50048]	Loss: 1.8940
Training Epoch: 7 [41216/50048]	Loss: 1.8337
Training Epoch: 7 [41344/50048]	Loss: 1.4055
Training Epoch: 7 [41472/50048]	Loss: 1.5865
Training Epoch: 7 [41600/50048]	Loss: 1.4967
Training Epoch: 7 [41728/50048]	Loss: 1.8319
Training Epoch: 7 [41856/50048]	Loss: 1.8130
Training Epoch: 7 [41984/50048]	Loss: 1.7316
Training Epoch: 7 [42112/50048]	Loss: 2.0993
Training Epoch: 7 [42240/50048]	Loss: 2.0729
Training Epoch: 7 [42368/50048]	Loss: 1.6716
Training Epoch: 7 [42496/50048]	Loss: 1.6313
Training Epoch: 7 [42624/50048]	Loss: 1.8658
Training Epoch: 7 [42752/50048]	Loss: 1.6670
Training Epoch: 7 [42880/50048]	Loss: 1.8305
Training Epoch: 7 [43008/50048]	Loss: 1.7125
Training Epoch: 7 [43136/50048]	Loss: 1.8918
Training Epoch: 7 [43264/50048]	Loss: 1.6322
Training Epoch: 7 [43392/50048]	Loss: 1.3985
Training Epoch: 7 [43520/50048]	Loss: 1.5718
Training Epoch: 7 [43648/50048]	Loss: 2.0232
Training Epoch: 7 [43776/50048]	Loss: 1.7303
Training Epoch: 7 [43904/50048]	Loss: 1.6918
Training Epoch: 7 [44032/50048]	Loss: 1.9202
Training Epoch: 7 [44160/50048]	Loss: 1.8396
Training Epoch: 7 [44288/50048]	Loss: 1.9560
Training Epoch: 7 [44416/50048]	Loss: 1.7808
Training Epoch: 7 [44544/50048]	Loss: 1.8835
Training Epoch: 7 [44672/50048]	Loss: 1.7443
Training Epoch: 7 [44800/50048]	Loss: 1.7674
Training Epoch: 7 [44928/50048]	Loss: 1.6428
Training Epoch: 7 [45056/50048]	Loss: 1.9455
Training Epoch: 7 [45184/50048]	Loss: 1.8107
Training Epoch: 7 [45312/50048]	Loss: 1.6279
Training Epoch: 7 [45440/50048]	Loss: 1.7178
Training Epoch: 7 [45568/50048]	Loss: 2.0825
Training Epoch: 7 [45696/50048]	Loss: 1.8168
Training Epoch: 7 [45824/50048]	Loss: 1.8885
Training Epoch: 7 [45952/50048]	Loss: 1.9215
Training Epoch: 7 [46080/50048]	Loss: 1.7055
Training Epoch: 7 [46208/50048]	Loss: 1.6778
Training Epoch: 7 [46336/50048]	Loss: 1.6785
Training Epoch: 7 [46464/50048]	Loss: 1.6567
Training Epoch: 7 [46592/50048]	Loss: 1.7268
Training Epoch: 7 [46720/50048]	Loss: 1.5388
Training Epoch: 7 [46848/50048]	Loss: 1.8763
Training Epoch: 7 [46976/50048]	Loss: 1.5584
Training Epoch: 7 [47104/50048]	Loss: 1.6004
Training Epoch: 7 [47232/50048]	Loss: 1.8322
Training Epoch: 7 [47360/50048]	Loss: 1.7906
Training Epoch: 7 [47488/50048]	Loss: 2.0257
Training Epoch: 7 [47616/50048]	Loss: 1.9559
Training Epoch: 7 [47744/50048]	Loss: 2.0887
Training Epoch: 7 [47872/50048]	Loss: 1.9452
Training Epoch: 7 [48000/50048]	Loss: 1.9211
Training Epoch: 7 [48128/50048]	Loss: 1.7359
Training Epoch: 7 [48256/50048]	Loss: 1.6651
Training Epoch: 7 [48384/50048]	Loss: 1.6891
Training Epoch: 7 [48512/50048]	Loss: 1.9123
Training Epoch: 7 [48640/50048]	Loss: 1.8829
Training Epoch: 7 [48768/50048]	Loss: 1.7534
Training Epoch: 7 [48896/50048]	Loss: 1.8835
Training Epoch: 7 [49024/50048]	Loss: 1.7897
Training Epoch: 7 [49152/50048]	Loss: 2.0106
Training Epoch: 7 [49280/50048]	Loss: 1.5061
Training Epoch: 7 [49408/50048]	Loss: 1.8541
Training Epoch: 7 [49536/50048]	Loss: 1.8775
Training Epoch: 7 [49664/50048]	Loss: 1.5933
Training Epoch: 7 [49792/50048]	Loss: 1.7368
Training Epoch: 7 [49920/50048]	Loss: 1.7903
Training Epoch: 7 [50048/50048]	Loss: 1.2316
Validation Epoch: 7, Average loss: 0.0151, Accuracy: 0.4817
Training Epoch: 8 [128/50048]	Loss: 1.7016
Training Epoch: 8 [256/50048]	Loss: 1.4872
Training Epoch: 8 [384/50048]	Loss: 1.4733
Training Epoch: 8 [512/50048]	Loss: 1.4940
Training Epoch: 8 [640/50048]	Loss: 1.7700
Training Epoch: 8 [768/50048]	Loss: 1.7139
Training Epoch: 8 [896/50048]	Loss: 1.7821
Training Epoch: 8 [1024/50048]	Loss: 1.8953
Training Epoch: 8 [1152/50048]	Loss: 1.7349
Training Epoch: 8 [1280/50048]	Loss: 1.9612
Training Epoch: 8 [1408/50048]	Loss: 1.7272
Training Epoch: 8 [1536/50048]	Loss: 1.4884
Training Epoch: 8 [1664/50048]	Loss: 1.5804
Training Epoch: 8 [1792/50048]	Loss: 1.6558
Training Epoch: 8 [1920/50048]	Loss: 1.7088
Training Epoch: 8 [2048/50048]	Loss: 1.7600
Training Epoch: 8 [2176/50048]	Loss: 1.5072
Training Epoch: 8 [2304/50048]	Loss: 1.5545
Training Epoch: 8 [2432/50048]	Loss: 1.7781
Training Epoch: 8 [2560/50048]	Loss: 1.8719
Training Epoch: 8 [2688/50048]	Loss: 1.5423
Training Epoch: 8 [2816/50048]	Loss: 1.5243
Training Epoch: 8 [2944/50048]	Loss: 1.4738
Training Epoch: 8 [3072/50048]	Loss: 1.6122
Training Epoch: 8 [3200/50048]	Loss: 1.6739
Training Epoch: 8 [3328/50048]	Loss: 1.8303
Training Epoch: 8 [3456/50048]	Loss: 1.6554
Training Epoch: 8 [3584/50048]	Loss: 1.7060
Training Epoch: 8 [3712/50048]	Loss: 1.9401
Training Epoch: 8 [3840/50048]	Loss: 1.7714
Training Epoch: 8 [3968/50048]	Loss: 1.8141
Training Epoch: 8 [4096/50048]	Loss: 2.0183
Training Epoch: 8 [4224/50048]	Loss: 1.8503
Training Epoch: 8 [4352/50048]	Loss: 1.5932
Training Epoch: 8 [4480/50048]	Loss: 1.5681
Training Epoch: 8 [4608/50048]	Loss: 1.7835
Training Epoch: 8 [4736/50048]	Loss: 1.6249
Training Epoch: 8 [4864/50048]	Loss: 1.8839
Training Epoch: 8 [4992/50048]	Loss: 1.8273
Training Epoch: 8 [5120/50048]	Loss: 1.6746
Training Epoch: 8 [5248/50048]	Loss: 1.7942
Training Epoch: 8 [5376/50048]	Loss: 1.8390
Training Epoch: 8 [5504/50048]	Loss: 1.4905
Training Epoch: 8 [5632/50048]	Loss: 1.6723
Training Epoch: 8 [5760/50048]	Loss: 1.7249
Training Epoch: 8 [5888/50048]	Loss: 1.5803
Training Epoch: 8 [6016/50048]	Loss: 1.4169
Training Epoch: 8 [6144/50048]	Loss: 1.6496
Training Epoch: 8 [6272/50048]	Loss: 1.7158
Training Epoch: 8 [6400/50048]	Loss: 1.8909
Training Epoch: 8 [6528/50048]	Loss: 1.6145
Training Epoch: 8 [6656/50048]	Loss: 1.4958
Training Epoch: 8 [6784/50048]	Loss: 1.8513
Training Epoch: 8 [6912/50048]	Loss: 1.5310
Training Epoch: 8 [7040/50048]	Loss: 1.5296
Training Epoch: 8 [7168/50048]	Loss: 1.8039
Training Epoch: 8 [7296/50048]	Loss: 1.6283
Training Epoch: 8 [7424/50048]	Loss: 1.5587
Training Epoch: 8 [7552/50048]	Loss: 1.6653
Training Epoch: 8 [7680/50048]	Loss: 1.7762
Training Epoch: 8 [7808/50048]	Loss: 2.0628
Training Epoch: 8 [7936/50048]	Loss: 1.7740
Training Epoch: 8 [8064/50048]	Loss: 1.8285
Training Epoch: 8 [8192/50048]	Loss: 1.8609
Training Epoch: 8 [8320/50048]	Loss: 1.6133
Training Epoch: 8 [8448/50048]	Loss: 1.5602
Training Epoch: 8 [8576/50048]	Loss: 1.5120
Training Epoch: 8 [8704/50048]	Loss: 1.5100
Training Epoch: 8 [8832/50048]	Loss: 1.7920
Training Epoch: 8 [8960/50048]	Loss: 1.6301
Training Epoch: 8 [9088/50048]	Loss: 1.6904
Training Epoch: 8 [9216/50048]	Loss: 1.4801
Training Epoch: 8 [9344/50048]	Loss: 1.6187
Training Epoch: 8 [9472/50048]	Loss: 1.5641
Training Epoch: 8 [9600/50048]	Loss: 1.5352
Training Epoch: 8 [9728/50048]	Loss: 1.7731
Training Epoch: 8 [9856/50048]	Loss: 1.7781
Training Epoch: 8 [9984/50048]	Loss: 1.8825
Training Epoch: 8 [10112/50048]	Loss: 1.8912
Training Epoch: 8 [10240/50048]	Loss: 1.7610
Training Epoch: 8 [10368/50048]	Loss: 1.6697
Training Epoch: 8 [10496/50048]	Loss: 1.4226
Training Epoch: 8 [10624/50048]	Loss: 1.5584
Training Epoch: 8 [10752/50048]	Loss: 1.5505
Training Epoch: 8 [10880/50048]	Loss: 1.7197
Training Epoch: 8 [11008/50048]	Loss: 1.6617
Training Epoch: 8 [11136/50048]	Loss: 1.6671
Training Epoch: 8 [11264/50048]	Loss: 1.7332
Training Epoch: 8 [11392/50048]	Loss: 1.8655
Training Epoch: 8 [11520/50048]	Loss: 1.8976
Training Epoch: 8 [11648/50048]	Loss: 1.4589
Training Epoch: 8 [11776/50048]	Loss: 1.5330
Training Epoch: 8 [11904/50048]	Loss: 1.6898
Training Epoch: 8 [12032/50048]	Loss: 1.6606
Training Epoch: 8 [12160/50048]	Loss: 1.6470
Training Epoch: 8 [12288/50048]	Loss: 1.6589
Training Epoch: 8 [12416/50048]	Loss: 1.4856
Training Epoch: 8 [12544/50048]	Loss: 1.6287
Training Epoch: 8 [12672/50048]	Loss: 1.8032
Training Epoch: 8 [12800/50048]	Loss: 1.6105
Training Epoch: 8 [12928/50048]	Loss: 1.7240
Training Epoch: 8 [13056/50048]	Loss: 1.8300
Training Epoch: 8 [13184/50048]	Loss: 1.7030
Training Epoch: 8 [13312/50048]	Loss: 1.7621
Training Epoch: 8 [13440/50048]	Loss: 1.7202
Training Epoch: 8 [13568/50048]	Loss: 2.0032
Training Epoch: 8 [13696/50048]	Loss: 1.6948
Training Epoch: 8 [13824/50048]	Loss: 1.4257
Training Epoch: 8 [13952/50048]	Loss: 1.5846
Training Epoch: 8 [14080/50048]	Loss: 1.7310
Training Epoch: 8 [14208/50048]	Loss: 1.5177
Training Epoch: 8 [14336/50048]	Loss: 1.6594
Training Epoch: 8 [14464/50048]	Loss: 1.7211
Training Epoch: 8 [14592/50048]	Loss: 1.5022
Training Epoch: 8 [14720/50048]	Loss: 1.7245
Training Epoch: 8 [14848/50048]	Loss: 1.5754
Training Epoch: 8 [14976/50048]	Loss: 1.9221
Training Epoch: 8 [15104/50048]	Loss: 1.5370
Training Epoch: 8 [15232/50048]	Loss: 1.9301
Training Epoch: 8 [15360/50048]	Loss: 1.5582
Training Epoch: 8 [15488/50048]	Loss: 1.5008
Training Epoch: 8 [15616/50048]	Loss: 1.8084
Training Epoch: 8 [15744/50048]	Loss: 1.5488
Training Epoch: 8 [15872/50048]	Loss: 1.6992
Training Epoch: 8 [16000/50048]	Loss: 2.0091
Training Epoch: 8 [16128/50048]	Loss: 1.5799
Training Epoch: 8 [16256/50048]	Loss: 1.6663
Training Epoch: 8 [16384/50048]	Loss: 1.6438
Training Epoch: 8 [16512/50048]	Loss: 1.7080
Training Epoch: 8 [16640/50048]	Loss: 1.7077
Training Epoch: 8 [16768/50048]	Loss: 1.5699
Training Epoch: 8 [16896/50048]	Loss: 1.7895
Training Epoch: 8 [17024/50048]	Loss: 1.4037
Training Epoch: 8 [17152/50048]	Loss: 1.6070
Training Epoch: 8 [17280/50048]	Loss: 1.4727
Training Epoch: 8 [17408/50048]	Loss: 1.8621
Training Epoch: 8 [17536/50048]	Loss: 1.5196
Training Epoch: 8 [17664/50048]	Loss: 1.6264
Training Epoch: 8 [17792/50048]	Loss: 1.4399
Training Epoch: 8 [17920/50048]	Loss: 1.4271
Training Epoch: 8 [18048/50048]	Loss: 1.3589
Training Epoch: 8 [18176/50048]	Loss: 1.9089
Training Epoch: 8 [18304/50048]	Loss: 1.8078
Training Epoch: 8 [18432/50048]	Loss: 1.8159
Training Epoch: 8 [18560/50048]	Loss: 1.5647
Training Epoch: 8 [18688/50048]	Loss: 1.6135
Training Epoch: 8 [18816/50048]	Loss: 1.5507
Training Epoch: 8 [18944/50048]	Loss: 1.6557
Training Epoch: 8 [19072/50048]	Loss: 1.5445
Training Epoch: 8 [19200/50048]	Loss: 1.6576
Training Epoch: 8 [19328/50048]	Loss: 1.8145
Training Epoch: 8 [19456/50048]	Loss: 1.9006
Training Epoch: 8 [19584/50048]	Loss: 1.6553
Training Epoch: 8 [19712/50048]	Loss: 1.8339
Training Epoch: 8 [19840/50048]	Loss: 1.6197
Training Epoch: 8 [19968/50048]	Loss: 1.7780
Training Epoch: 8 [20096/50048]	Loss: 1.5594
Training Epoch: 8 [20224/50048]	Loss: 1.6660
Training Epoch: 8 [20352/50048]	Loss: 1.7792
Training Epoch: 8 [20480/50048]	Loss: 1.8673
Training Epoch: 8 [20608/50048]	Loss: 1.9918
Training Epoch: 8 [20736/50048]	Loss: 1.6129
Training Epoch: 8 [20864/50048]	Loss: 1.9023
Training Epoch: 8 [20992/50048]	Loss: 1.8935
Training Epoch: 8 [21120/50048]	Loss: 1.5504
Training Epoch: 8 [21248/50048]	Loss: 1.8017
Training Epoch: 8 [21376/50048]	Loss: 1.5828
Training Epoch: 8 [21504/50048]	Loss: 1.6089
Training Epoch: 8 [21632/50048]	Loss: 1.5461
Training Epoch: 8 [21760/50048]	Loss: 1.8215
Training Epoch: 8 [21888/50048]	Loss: 1.6804
Training Epoch: 8 [22016/50048]	Loss: 1.7028
Training Epoch: 8 [22144/50048]	Loss: 1.3860
Training Epoch: 8 [22272/50048]	Loss: 1.6246
Training Epoch: 8 [22400/50048]	Loss: 1.6103
Training Epoch: 8 [22528/50048]	Loss: 1.7128
Training Epoch: 8 [22656/50048]	Loss: 1.3710
Training Epoch: 8 [22784/50048]	Loss: 1.6450
Training Epoch: 8 [22912/50048]	Loss: 1.4383
Training Epoch: 8 [23040/50048]	Loss: 1.5415
Training Epoch: 8 [23168/50048]	Loss: 1.9160
Training Epoch: 8 [23296/50048]	Loss: 1.6635
Training Epoch: 8 [23424/50048]	Loss: 1.8220
Training Epoch: 8 [23552/50048]	Loss: 1.8171
Training Epoch: 8 [23680/50048]	Loss: 1.4835
Training Epoch: 8 [23808/50048]	Loss: 1.5168
Training Epoch: 8 [23936/50048]	Loss: 1.7650
Training Epoch: 8 [24064/50048]	Loss: 1.6378
Training Epoch: 8 [24192/50048]	Loss: 1.6884
Training Epoch: 8 [24320/50048]	Loss: 1.6932
Training Epoch: 8 [24448/50048]	Loss: 1.7640
Training Epoch: 8 [24576/50048]	Loss: 1.7616
Training Epoch: 8 [24704/50048]	Loss: 1.6633
Training Epoch: 8 [24832/50048]	Loss: 1.6846
Training Epoch: 8 [24960/50048]	Loss: 1.8269
Training Epoch: 8 [25088/50048]	Loss: 1.5983
Training Epoch: 8 [25216/50048]	Loss: 1.7593
Training Epoch: 8 [25344/50048]	Loss: 1.5313
Training Epoch: 8 [25472/50048]	Loss: 1.8945
Training Epoch: 8 [25600/50048]	Loss: 1.5492
Training Epoch: 8 [25728/50048]	Loss: 1.6078
Training Epoch: 8 [25856/50048]	Loss: 1.5784
Training Epoch: 8 [25984/50048]	Loss: 1.5420
Training Epoch: 8 [26112/50048]	Loss: 1.5914
Training Epoch: 8 [26240/50048]	Loss: 1.7524
Training Epoch: 8 [26368/50048]	Loss: 1.8376
Training Epoch: 8 [26496/50048]	Loss: 1.8197
Training Epoch: 8 [26624/50048]	Loss: 1.8616
Training Epoch: 8 [26752/50048]	Loss: 1.6579
Training Epoch: 8 [26880/50048]	Loss: 1.8007
Training Epoch: 8 [27008/50048]	Loss: 1.6120
Training Epoch: 8 [27136/50048]	Loss: 1.6828
Training Epoch: 8 [27264/50048]	Loss: 1.6106
Training Epoch: 8 [27392/50048]	Loss: 1.6140
Training Epoch: 8 [27520/50048]	Loss: 1.5557
Training Epoch: 8 [27648/50048]	Loss: 1.6640
Training Epoch: 8 [27776/50048]	Loss: 1.9951
Training Epoch: 8 [27904/50048]	Loss: 1.9508
Training Epoch: 8 [28032/50048]	Loss: 1.6833
Training Epoch: 8 [28160/50048]	Loss: 1.7674
Training Epoch: 8 [28288/50048]	Loss: 1.8667
Training Epoch: 8 [28416/50048]	Loss: 1.7128
Training Epoch: 8 [28544/50048]	Loss: 1.9746
Training Epoch: 8 [28672/50048]	Loss: 1.6137
Training Epoch: 8 [28800/50048]	Loss: 1.9563
Training Epoch: 8 [28928/50048]	Loss: 1.8443
Training Epoch: 8 [29056/50048]	Loss: 1.9375
Training Epoch: 8 [29184/50048]	Loss: 1.8546
Training Epoch: 8 [29312/50048]	Loss: 1.6385
Training Epoch: 8 [29440/50048]	Loss: 1.8703
Training Epoch: 8 [29568/50048]	Loss: 1.6918
Training Epoch: 8 [29696/50048]	Loss: 1.9588
Training Epoch: 8 [29824/50048]	Loss: 1.5136
Training Epoch: 8 [29952/50048]	Loss: 1.7565
Training Epoch: 8 [30080/50048]	Loss: 1.6476
Training Epoch: 8 [30208/50048]	Loss: 1.9572
Training Epoch: 8 [30336/50048]	Loss: 1.6425
Training Epoch: 8 [30464/50048]	Loss: 1.6389
Training Epoch: 8 [30592/50048]	Loss: 1.9310
Training Epoch: 8 [30720/50048]	Loss: 1.4929
Training Epoch: 8 [30848/50048]	Loss: 1.8867
Training Epoch: 8 [30976/50048]	Loss: 1.6335
Training Epoch: 8 [31104/50048]	Loss: 1.5336
Training Epoch: 8 [31232/50048]	Loss: 1.6739
Training Epoch: 8 [31360/50048]	Loss: 2.0039
Training Epoch: 8 [31488/50048]	Loss: 1.7339
Training Epoch: 8 [31616/50048]	Loss: 1.8297
Training Epoch: 8 [31744/50048]	Loss: 1.9774
Training Epoch: 8 [31872/50048]	Loss: 1.7068
Training Epoch: 8 [32000/50048]	Loss: 1.9163
Training Epoch: 8 [32128/50048]	Loss: 1.6951
Training Epoch: 8 [32256/50048]	Loss: 1.8612
Training Epoch: 8 [32384/50048]	Loss: 1.5584
Training Epoch: 8 [32512/50048]	Loss: 1.7437
Training Epoch: 8 [32640/50048]	Loss: 1.6110
Training Epoch: 8 [32768/50048]	Loss: 1.8316
Training Epoch: 8 [32896/50048]	Loss: 1.8394
Training Epoch: 8 [33024/50048]	Loss: 1.7180
Training Epoch: 8 [33152/50048]	Loss: 1.5100
Training Epoch: 8 [33280/50048]	Loss: 1.8554
Training Epoch: 8 [33408/50048]	Loss: 1.5168
Training Epoch: 8 [33536/50048]	Loss: 1.7265
Training Epoch: 8 [33664/50048]	Loss: 1.6980
Training Epoch: 8 [33792/50048]	Loss: 1.5470
Training Epoch: 8 [33920/50048]	Loss: 1.5944
Training Epoch: 8 [34048/50048]	Loss: 1.7542
Training Epoch: 8 [34176/50048]	Loss: 1.5659
Training Epoch: 8 [34304/50048]	Loss: 1.5248
Training Epoch: 8 [34432/50048]	Loss: 1.6308
Training Epoch: 8 [34560/50048]	Loss: 1.8098
Training Epoch: 8 [34688/50048]	Loss: 1.7924
Training Epoch: 8 [34816/50048]	Loss: 1.5203
Training Epoch: 8 [34944/50048]	Loss: 1.8655
Training Epoch: 8 [35072/50048]	Loss: 1.5196
Training Epoch: 8 [35200/50048]	Loss: 1.7506
Training Epoch: 8 [35328/50048]	Loss: 1.8050
Training Epoch: 8 [35456/50048]	Loss: 1.4227
Training Epoch: 8 [35584/50048]	Loss: 1.6104
Training Epoch: 8 [35712/50048]	Loss: 1.8861
Training Epoch: 8 [35840/50048]	Loss: 1.4771
Training Epoch: 8 [35968/50048]	Loss: 1.5153
Training Epoch: 8 [36096/50048]	Loss: 1.6905
Training Epoch: 8 [36224/50048]	Loss: 1.5729
Training Epoch: 8 [36352/50048]	Loss: 1.5343
Training Epoch: 8 [36480/50048]	Loss: 1.6600
Training Epoch: 8 [36608/50048]	Loss: 1.6357
Training Epoch: 8 [36736/50048]	Loss: 1.6236
Training Epoch: 8 [36864/50048]	Loss: 1.8597
Training Epoch: 8 [36992/50048]	Loss: 1.6209
Training Epoch: 8 [37120/50048]	Loss: 1.6499
Training Epoch: 8 [37248/50048]	Loss: 1.9145
Training Epoch: 8 [37376/50048]	Loss: 1.6263
Training Epoch: 8 [37504/50048]	Loss: 1.7863
Training Epoch: 8 [37632/50048]	Loss: 1.8220
Training Epoch: 8 [37760/50048]	Loss: 1.8222
Training Epoch: 8 [37888/50048]	Loss: 1.7185
Training Epoch: 8 [38016/50048]	Loss: 1.8450
Training Epoch: 8 [38144/50048]	Loss: 1.6694
Training Epoch: 8 [38272/50048]	Loss: 1.8093
Training Epoch: 8 [38400/50048]	Loss: 1.7077
Training Epoch: 8 [38528/50048]	Loss: 1.5961
Training Epoch: 8 [38656/50048]	Loss: 1.7453
Training Epoch: 8 [38784/50048]	Loss: 1.8023
Training Epoch: 8 [38912/50048]	Loss: 1.5828
Training Epoch: 8 [39040/50048]	Loss: 1.8020
Training Epoch: 8 [39168/50048]	Loss: 1.5246
Training Epoch: 8 [39296/50048]	Loss: 1.6596
Training Epoch: 8 [39424/50048]	Loss: 1.4767
Training Epoch: 8 [39552/50048]	Loss: 1.8971
Training Epoch: 8 [39680/50048]	Loss: 1.4407
Training Epoch: 8 [39808/50048]	Loss: 1.7496
Training Epoch: 8 [39936/50048]	Loss: 1.7185
Training Epoch: 8 [40064/50048]	Loss: 1.7638
Training Epoch: 8 [40192/50048]	Loss: 1.9666
Training Epoch: 8 [40320/50048]	Loss: 1.7566
Training Epoch: 8 [40448/50048]	Loss: 1.4383
Training Epoch: 8 [40576/50048]	Loss: 1.7121
Training Epoch: 8 [40704/50048]	Loss: 1.7622
Training Epoch: 8 [40832/50048]	Loss: 1.8952
Training Epoch: 8 [40960/50048]	Loss: 1.7393
Training Epoch: 8 [41088/50048]	Loss: 1.6631
Training Epoch: 8 [41216/50048]	Loss: 1.8355
Training Epoch: 8 [41344/50048]	Loss: 1.4859
Training Epoch: 8 [41472/50048]	Loss: 1.9722
Training Epoch: 8 [41600/50048]	Loss: 1.7306
Training Epoch: 8 [41728/50048]	Loss: 1.7302
Training Epoch: 8 [41856/50048]	Loss: 1.7095
Training Epoch: 8 [41984/50048]	Loss: 1.5348
Training Epoch: 8 [42112/50048]	Loss: 1.7027
Training Epoch: 8 [42240/50048]	Loss: 1.6774
Training Epoch: 8 [42368/50048]	Loss: 1.7897
Training Epoch: 8 [42496/50048]	Loss: 1.6117
Training Epoch: 8 [42624/50048]	Loss: 1.7256
Training Epoch: 8 [42752/50048]	Loss: 1.5940
Training Epoch: 8 [42880/50048]	Loss: 1.6302
Training Epoch: 8 [43008/50048]	Loss: 1.6550
Training Epoch: 8 [43136/50048]	Loss: 1.9612
Training Epoch: 8 [43264/50048]	Loss: 1.6935
Training Epoch: 8 [43392/50048]	Loss: 1.6195
Training Epoch: 8 [43520/50048]	Loss: 1.8219
Training Epoch: 8 [43648/50048]	Loss: 1.6741
Training Epoch: 8 [43776/50048]	Loss: 1.7578
Training Epoch: 8 [43904/50048]	Loss: 1.7255
Training Epoch: 8 [44032/50048]	Loss: 1.5060
Training Epoch: 8 [44160/50048]	Loss: 1.4862
Training Epoch: 8 [44288/50048]	Loss: 1.4772
Training Epoch: 8 [44416/50048]	Loss: 1.5471
Training Epoch: 8 [44544/50048]	Loss: 1.6054
Training Epoch: 8 [44672/50048]	Loss: 1.7237
Training Epoch: 8 [44800/50048]	Loss: 1.7230
Training Epoch: 8 [44928/50048]	Loss: 1.6633
Training Epoch: 8 [45056/50048]	Loss: 1.6770
Training Epoch: 8 [45184/50048]	Loss: 1.6061
Training Epoch: 8 [45312/50048]	Loss: 1.5927
Training Epoch: 8 [45440/50048]	Loss: 1.6497
Training Epoch: 8 [45568/50048]	Loss: 1.5163
Training Epoch: 8 [45696/50048]	Loss: 1.5689
Training Epoch: 8 [45824/50048]	Loss: 1.6482
Training Epoch: 8 [45952/50048]	Loss: 1.7259
Training Epoch: 8 [46080/50048]	Loss: 1.8233
Training Epoch: 8 [46208/50048]	Loss: 1.6084
Training Epoch: 8 [46336/50048]	Loss: 1.6904
Training Epoch: 8 [46464/50048]	Loss: 1.6408
Training Epoch: 8 [46592/50048]	Loss: 1.7520
Training Epoch: 8 [46720/50048]	Loss: 1.6739
Training Epoch: 8 [46848/50048]	Loss: 1.8045
Training Epoch: 8 [46976/50048]	Loss: 1.7309
Training Epoch: 8 [47104/50048]	Loss: 1.7250
Training Epoch: 8 [47232/50048]	Loss: 1.5454
Training Epoch: 8 [47360/50048]	Loss: 1.7287
Training Epoch: 8 [47488/50048]	Loss: 1.6540
Training Epoch: 8 [47616/50048]	Loss: 1.6570
Training Epoch: 8 [47744/50048]	Loss: 1.4913
Training Epoch: 8 [47872/50048]	Loss: 1.6005
Training Epoch: 8 [48000/50048]	Loss: 1.7762
Training Epoch: 8 [48128/50048]	Loss: 1.7516
Training Epoch: 8 [48256/50048]	Loss: 1.7948
Training Epoch: 8 [48384/50048]	Loss: 1.6902
Training Epoch: 8 [48512/50048]	Loss: 1.3345
Training Epoch: 8 [48640/50048]	Loss: 1.4833
Training Epoch: 8 [48768/50048]	Loss: 1.5817
Training Epoch: 8 [48896/50048]	Loss: 1.7180
Training Epoch: 8 [49024/50048]	Loss: 1.5762
Training Epoch: 8 [49152/50048]	Loss: 1.9061
Training Epoch: 8 [49280/50048]	Loss: 1.7139
Training Epoch: 8 [49408/50048]	Loss: 1.8633
Training Epoch: 8 [49536/50048]	Loss: 1.5927
Training Epoch: 8 [49664/50048]	Loss: 1.8786
Training Epoch: 8 [49792/50048]	Loss: 1.7766
Training Epoch: 8 [49920/50048]	Loss: 1.5508
Training Epoch: 8 [50048/50048]	Loss: 1.6469
Validation Epoch: 8, Average loss: 0.0158, Accuracy: 0.4685
Training Epoch: 9 [128/50048]	Loss: 1.3222
Training Epoch: 9 [256/50048]	Loss: 1.8379
Training Epoch: 9 [384/50048]	Loss: 1.4055
Training Epoch: 9 [512/50048]	Loss: 1.5276
Training Epoch: 9 [640/50048]	Loss: 1.5703
Training Epoch: 9 [768/50048]	Loss: 1.5526
Training Epoch: 9 [896/50048]	Loss: 1.3204
Training Epoch: 9 [1024/50048]	Loss: 1.7962
Training Epoch: 9 [1152/50048]	Loss: 1.8227
Training Epoch: 9 [1280/50048]	Loss: 1.4759
Training Epoch: 9 [1408/50048]	Loss: 1.4981
Training Epoch: 9 [1536/50048]	Loss: 1.6055
Training Epoch: 9 [1664/50048]	Loss: 1.5348
Training Epoch: 9 [1792/50048]	Loss: 1.6805
Training Epoch: 9 [1920/50048]	Loss: 1.3652
Training Epoch: 9 [2048/50048]	Loss: 1.5647
Training Epoch: 9 [2176/50048]	Loss: 1.6566
Training Epoch: 9 [2304/50048]	Loss: 1.4513
Training Epoch: 9 [2432/50048]	Loss: 1.7230
Training Epoch: 9 [2560/50048]	Loss: 1.7514
Training Epoch: 9 [2688/50048]	Loss: 1.3836
Training Epoch: 9 [2816/50048]	Loss: 1.6356
Training Epoch: 9 [2944/50048]	Loss: 1.9910
Training Epoch: 9 [3072/50048]	Loss: 1.5883
Training Epoch: 9 [3200/50048]	Loss: 1.7397
Training Epoch: 9 [3328/50048]	Loss: 1.4976
Training Epoch: 9 [3456/50048]	Loss: 1.5509
Training Epoch: 9 [3584/50048]	Loss: 1.5913
Training Epoch: 9 [3712/50048]	Loss: 1.5573
Training Epoch: 9 [3840/50048]	Loss: 1.4041
Training Epoch: 9 [3968/50048]	Loss: 1.6214
Training Epoch: 9 [4096/50048]	Loss: 1.5265
Training Epoch: 9 [4224/50048]	Loss: 1.6599
Training Epoch: 9 [4352/50048]	Loss: 1.5964
Training Epoch: 9 [4480/50048]	Loss: 1.4866
Training Epoch: 9 [4608/50048]	Loss: 1.6199
Training Epoch: 9 [4736/50048]	Loss: 1.7719
Training Epoch: 9 [4864/50048]	Loss: 1.6112
Training Epoch: 9 [4992/50048]	Loss: 1.8224
Training Epoch: 9 [5120/50048]	Loss: 1.4749
Training Epoch: 9 [5248/50048]	Loss: 1.9059
Training Epoch: 9 [5376/50048]	Loss: 1.5127
Training Epoch: 9 [5504/50048]	Loss: 1.7422
Training Epoch: 9 [5632/50048]	Loss: 1.6802
Training Epoch: 9 [5760/50048]	Loss: 1.4011
Training Epoch: 9 [5888/50048]	Loss: 1.4219
Training Epoch: 9 [6016/50048]	Loss: 1.7084
Training Epoch: 9 [6144/50048]	Loss: 1.5074
Training Epoch: 9 [6272/50048]	Loss: 1.4667
Training Epoch: 9 [6400/50048]	Loss: 1.7129
Training Epoch: 9 [6528/50048]	Loss: 1.8051
Training Epoch: 9 [6656/50048]	Loss: 1.6676
Training Epoch: 9 [6784/50048]	Loss: 1.5617
Training Epoch: 9 [6912/50048]	Loss: 1.4523
Training Epoch: 9 [7040/50048]	Loss: 1.5631
Training Epoch: 9 [7168/50048]	Loss: 1.7612
Training Epoch: 9 [7296/50048]	Loss: 1.3195
Training Epoch: 9 [7424/50048]	Loss: 1.6031
Training Epoch: 9 [7552/50048]	Loss: 1.9682
Training Epoch: 9 [7680/50048]	Loss: 1.5772
Training Epoch: 9 [7808/50048]	Loss: 1.6313
Training Epoch: 9 [7936/50048]	Loss: 1.6226
Training Epoch: 9 [8064/50048]	Loss: 1.5498
Training Epoch: 9 [8192/50048]	Loss: 1.4374
Training Epoch: 9 [8320/50048]	Loss: 1.6796
Training Epoch: 9 [8448/50048]	Loss: 1.5618
Training Epoch: 9 [8576/50048]	Loss: 1.9043
Training Epoch: 9 [8704/50048]	Loss: 1.4056
Training Epoch: 9 [8832/50048]	Loss: 1.5645
Training Epoch: 9 [8960/50048]	Loss: 1.6937
Training Epoch: 9 [9088/50048]	Loss: 1.1909
Training Epoch: 9 [9216/50048]	Loss: 1.5941
Training Epoch: 9 [9344/50048]	Loss: 1.4843
Training Epoch: 9 [9472/50048]	Loss: 1.9634
Training Epoch: 9 [9600/50048]	Loss: 1.7198
Training Epoch: 9 [9728/50048]	Loss: 1.5579
Training Epoch: 9 [9856/50048]	Loss: 1.5891
Training Epoch: 9 [9984/50048]	Loss: 1.2380
Training Epoch: 9 [10112/50048]	Loss: 1.4486
Training Epoch: 9 [10240/50048]	Loss: 1.4455
Training Epoch: 9 [10368/50048]	Loss: 1.3159
Training Epoch: 9 [10496/50048]	Loss: 1.7744
Training Epoch: 9 [10624/50048]	Loss: 1.7158
Training Epoch: 9 [10752/50048]	Loss: 1.9395
Training Epoch: 9 [10880/50048]	Loss: 1.6253
Training Epoch: 9 [11008/50048]	Loss: 1.3579
Training Epoch: 9 [11136/50048]	Loss: 1.7668
Training Epoch: 9 [11264/50048]	Loss: 1.6207
Training Epoch: 9 [11392/50048]	Loss: 1.9141
Training Epoch: 9 [11520/50048]	Loss: 1.6115
Training Epoch: 9 [11648/50048]	Loss: 1.7423
Training Epoch: 9 [11776/50048]	Loss: 1.6349
Training Epoch: 9 [11904/50048]	Loss: 1.7728
Training Epoch: 9 [12032/50048]	Loss: 1.5234
Training Epoch: 9 [12160/50048]	Loss: 1.6627
Training Epoch: 9 [12288/50048]	Loss: 1.5282
Training Epoch: 9 [12416/50048]	Loss: 1.2432
Training Epoch: 9 [12544/50048]	Loss: 1.5939
Training Epoch: 9 [12672/50048]	Loss: 1.6389
Training Epoch: 9 [12800/50048]	Loss: 1.4120
Training Epoch: 9 [12928/50048]	Loss: 1.7861
Training Epoch: 9 [13056/50048]	Loss: 1.5563
Training Epoch: 9 [13184/50048]	Loss: 1.7315
Training Epoch: 9 [13312/50048]	Loss: 1.7061
Training Epoch: 9 [13440/50048]	Loss: 2.0309
Training Epoch: 9 [13568/50048]	Loss: 1.6815
Training Epoch: 9 [13696/50048]	Loss: 1.7403
Training Epoch: 9 [13824/50048]	Loss: 1.8472
Training Epoch: 9 [13952/50048]	Loss: 1.6473
Training Epoch: 9 [14080/50048]	Loss: 1.4983
Training Epoch: 9 [14208/50048]	Loss: 1.3985
Training Epoch: 9 [14336/50048]	Loss: 1.5122
Training Epoch: 9 [14464/50048]	Loss: 1.4353
Training Epoch: 9 [14592/50048]	Loss: 1.4719
Training Epoch: 9 [14720/50048]	Loss: 1.8229
Training Epoch: 9 [14848/50048]	Loss: 1.6870
Training Epoch: 9 [14976/50048]	Loss: 1.4247
Training Epoch: 9 [15104/50048]	Loss: 1.6347
Training Epoch: 9 [15232/50048]	Loss: 1.8138
Training Epoch: 9 [15360/50048]	Loss: 1.8687
Training Epoch: 9 [15488/50048]	Loss: 1.7040
Training Epoch: 9 [15616/50048]	Loss: 1.6968
Training Epoch: 9 [15744/50048]	Loss: 1.7100
Training Epoch: 9 [15872/50048]	Loss: 1.7678
Training Epoch: 9 [16000/50048]	Loss: 1.6488
Training Epoch: 9 [16128/50048]	Loss: 1.8098
Training Epoch: 9 [16256/50048]	Loss: 1.6361
Training Epoch: 9 [16384/50048]	Loss: 1.7161
Training Epoch: 9 [16512/50048]	Loss: 1.6832
Training Epoch: 9 [16640/50048]	Loss: 1.6251
Training Epoch: 9 [16768/50048]	Loss: 1.5349
Training Epoch: 9 [16896/50048]	Loss: 1.6371
Training Epoch: 9 [17024/50048]	Loss: 1.5310
Training Epoch: 9 [17152/50048]	Loss: 1.6340
Training Epoch: 9 [17280/50048]	Loss: 1.3279
Training Epoch: 9 [17408/50048]	Loss: 1.4596
Training Epoch: 9 [17536/50048]	Loss: 1.6829
Training Epoch: 9 [17664/50048]	Loss: 1.5763
Training Epoch: 9 [17792/50048]	Loss: 1.6987
Training Epoch: 9 [17920/50048]	Loss: 1.7414
Training Epoch: 9 [18048/50048]	Loss: 1.2880
Training Epoch: 9 [18176/50048]	Loss: 1.9061
Training Epoch: 9 [18304/50048]	Loss: 1.5003
Training Epoch: 9 [18432/50048]	Loss: 1.6589
Training Epoch: 9 [18560/50048]	Loss: 1.5332
Training Epoch: 9 [18688/50048]	Loss: 1.9288
Training Epoch: 9 [18816/50048]	Loss: 1.8171
Training Epoch: 9 [18944/50048]	Loss: 1.2837
Training Epoch: 9 [19072/50048]	Loss: 1.4637
Training Epoch: 9 [19200/50048]	Loss: 1.5646
Training Epoch: 9 [19328/50048]	Loss: 1.6973
Training Epoch: 9 [19456/50048]	Loss: 1.5819
Training Epoch: 9 [19584/50048]	Loss: 1.7968
Training Epoch: 9 [19712/50048]	Loss: 1.4761
Training Epoch: 9 [19840/50048]	Loss: 1.4983
Training Epoch: 9 [19968/50048]	Loss: 1.6399
Training Epoch: 9 [20096/50048]	Loss: 1.5698
Training Epoch: 9 [20224/50048]	Loss: 1.6039
Training Epoch: 9 [20352/50048]	Loss: 1.7025
Training Epoch: 9 [20480/50048]	Loss: 1.8729
Training Epoch: 9 [20608/50048]	Loss: 1.5664
Training Epoch: 9 [20736/50048]	Loss: 1.5824
Training Epoch: 9 [20864/50048]	Loss: 1.6693
Training Epoch: 9 [20992/50048]	Loss: 1.7437
Training Epoch: 9 [21120/50048]	Loss: 1.7186
Training Epoch: 9 [21248/50048]	Loss: 1.4088
Training Epoch: 9 [21376/50048]	Loss: 1.8186
Training Epoch: 9 [21504/50048]	Loss: 1.3749
Training Epoch: 9 [21632/50048]	Loss: 1.7147
Training Epoch: 9 [21760/50048]	Loss: 1.4023
Training Epoch: 9 [21888/50048]	Loss: 2.0486
Training Epoch: 9 [22016/50048]	Loss: 1.6386
Training Epoch: 9 [22144/50048]	Loss: 1.4417
Training Epoch: 9 [22272/50048]	Loss: 1.6830
Training Epoch: 9 [22400/50048]	Loss: 1.6150
Training Epoch: 9 [22528/50048]	Loss: 1.6824
Training Epoch: 9 [22656/50048]	Loss: 1.5584
Training Epoch: 9 [22784/50048]	Loss: 1.6312
Training Epoch: 9 [22912/50048]	Loss: 1.5484
Training Epoch: 9 [23040/50048]	Loss: 1.5787
Training Epoch: 9 [23168/50048]	Loss: 1.5299
Training Epoch: 9 [23296/50048]	Loss: 1.4702
Training Epoch: 9 [23424/50048]	Loss: 1.4540
Training Epoch: 9 [23552/50048]	Loss: 1.7795
Training Epoch: 9 [23680/50048]	Loss: 1.4204
Training Epoch: 9 [23808/50048]	Loss: 1.4441
Training Epoch: 9 [23936/50048]	Loss: 1.4538
Training Epoch: 9 [24064/50048]	Loss: 1.5273
Training Epoch: 9 [24192/50048]	Loss: 1.5623
Training Epoch: 9 [24320/50048]	Loss: 1.4188
Training Epoch: 9 [24448/50048]	Loss: 1.5746
Training Epoch: 9 [24576/50048]	Loss: 1.6893
Training Epoch: 9 [24704/50048]	Loss: 1.7174
Training Epoch: 9 [24832/50048]	Loss: 1.5043
Training Epoch: 9 [24960/50048]	Loss: 1.6785
Training Epoch: 9 [25088/50048]	Loss: 1.2881
Training Epoch: 9 [25216/50048]	Loss: 1.8159
Training Epoch: 9 [25344/50048]	Loss: 1.4423
Training Epoch: 9 [25472/50048]	Loss: 1.6722
Training Epoch: 9 [25600/50048]	Loss: 1.4843
Training Epoch: 9 [25728/50048]	Loss: 1.3860
Training Epoch: 9 [25856/50048]	Loss: 1.7453
Training Epoch: 9 [25984/50048]	Loss: 1.5893
Training Epoch: 9 [26112/50048]	Loss: 1.6462
Training Epoch: 9 [26240/50048]	Loss: 1.4235
Training Epoch: 9 [26368/50048]	Loss: 1.5196
Training Epoch: 9 [26496/50048]	Loss: 1.7858
Training Epoch: 9 [26624/50048]	Loss: 1.5954
Training Epoch: 9 [26752/50048]	Loss: 1.6589
Training Epoch: 9 [26880/50048]	Loss: 1.6322
Training Epoch: 9 [27008/50048]	Loss: 1.4724
Training Epoch: 9 [27136/50048]	Loss: 1.5748
Training Epoch: 9 [27264/50048]	Loss: 1.5459
Training Epoch: 9 [27392/50048]	Loss: 1.3770
Training Epoch: 9 [27520/50048]	Loss: 1.6548
Training Epoch: 9 [27648/50048]	Loss: 1.6258
Training Epoch: 9 [27776/50048]	Loss: 1.6263
Training Epoch: 9 [27904/50048]	Loss: 1.4754
Training Epoch: 9 [28032/50048]	Loss: 1.6388
Training Epoch: 9 [28160/50048]	Loss: 1.6876
Training Epoch: 9 [28288/50048]	Loss: 1.7690
Training Epoch: 9 [28416/50048]	Loss: 1.4437
Training Epoch: 9 [28544/50048]	Loss: 1.4861
Training Epoch: 9 [28672/50048]	Loss: 1.5081
Training Epoch: 9 [28800/50048]	Loss: 1.6993
Training Epoch: 9 [28928/50048]	Loss: 1.3990
Training Epoch: 9 [29056/50048]	Loss: 1.4943
Training Epoch: 9 [29184/50048]	Loss: 1.8129
Training Epoch: 9 [29312/50048]	Loss: 1.7662
Training Epoch: 9 [29440/50048]	Loss: 1.6843
Training Epoch: 9 [29568/50048]	Loss: 1.6662
Training Epoch: 9 [29696/50048]	Loss: 1.6053
Training Epoch: 9 [29824/50048]	Loss: 1.3667
Training Epoch: 9 [29952/50048]	Loss: 1.5811
Training Epoch: 9 [30080/50048]	Loss: 1.5053
Training Epoch: 9 [30208/50048]	Loss: 1.4046
Training Epoch: 9 [30336/50048]	Loss: 1.4083
Training Epoch: 9 [30464/50048]	Loss: 1.4445
Training Epoch: 9 [30592/50048]	Loss: 1.6012
Training Epoch: 9 [30720/50048]	Loss: 1.4378
Training Epoch: 9 [30848/50048]	Loss: 1.8660
Training Epoch: 9 [30976/50048]	Loss: 1.5497
Training Epoch: 9 [31104/50048]	Loss: 1.5730
Training Epoch: 9 [31232/50048]	Loss: 1.4551
Training Epoch: 9 [31360/50048]	Loss: 1.6838
Training Epoch: 9 [31488/50048]	Loss: 1.6264
Training Epoch: 9 [31616/50048]	Loss: 1.5949
Training Epoch: 9 [31744/50048]	Loss: 1.6595
Training Epoch: 9 [31872/50048]	Loss: 1.5550
Training Epoch: 9 [32000/50048]	Loss: 1.5579
Training Epoch: 9 [32128/50048]	Loss: 1.5167
Training Epoch: 9 [32256/50048]	Loss: 1.7057
Training Epoch: 9 [32384/50048]	Loss: 1.6529
Training Epoch: 9 [32512/50048]	Loss: 1.8118
Training Epoch: 9 [32640/50048]	Loss: 1.8995
Training Epoch: 9 [32768/50048]	Loss: 1.3966
Training Epoch: 9 [32896/50048]	Loss: 1.6336
Training Epoch: 9 [33024/50048]	Loss: 1.5032
Training Epoch: 9 [33152/50048]	Loss: 1.5098
Training Epoch: 9 [33280/50048]	Loss: 1.7015
Training Epoch: 9 [33408/50048]	Loss: 1.5860
Training Epoch: 9 [33536/50048]	Loss: 1.5562
Training Epoch: 9 [33664/50048]	Loss: 1.7788
Training Epoch: 9 [33792/50048]	Loss: 1.5437
Training Epoch: 9 [33920/50048]	Loss: 1.7729
Training Epoch: 9 [34048/50048]	Loss: 1.6415
Training Epoch: 9 [34176/50048]	Loss: 1.6790
Training Epoch: 9 [34304/50048]	Loss: 1.7683
Training Epoch: 9 [34432/50048]	Loss: 1.4246
Training Epoch: 9 [34560/50048]	Loss: 1.6981
Training Epoch: 9 [34688/50048]	Loss: 1.7097
Training Epoch: 9 [34816/50048]	Loss: 1.4262
Training Epoch: 9 [34944/50048]	Loss: 1.2913
Training Epoch: 9 [35072/50048]	Loss: 1.5556
Training Epoch: 9 [35200/50048]	Loss: 1.8095
Training Epoch: 9 [35328/50048]	Loss: 1.3987
Training Epoch: 9 [35456/50048]	Loss: 1.6437
Training Epoch: 9 [35584/50048]	Loss: 1.5447
Training Epoch: 9 [35712/50048]	Loss: 1.4810
Training Epoch: 9 [35840/50048]	Loss: 1.5424
Training Epoch: 9 [35968/50048]	Loss: 1.5965
Training Epoch: 9 [36096/50048]	Loss: 1.4418
Training Epoch: 9 [36224/50048]	Loss: 1.5796
Training Epoch: 9 [36352/50048]	Loss: 1.6068
Training Epoch: 9 [36480/50048]	Loss: 1.6287
Training Epoch: 9 [36608/50048]	Loss: 1.8159
Training Epoch: 9 [36736/50048]	Loss: 1.6817
Training Epoch: 9 [36864/50048]	Loss: 1.8261
Training Epoch: 9 [36992/50048]	Loss: 1.6503
Training Epoch: 9 [37120/50048]	Loss: 1.5726
Training Epoch: 9 [37248/50048]	Loss: 1.4044
Training Epoch: 9 [37376/50048]	Loss: 1.6944
Training Epoch: 9 [37504/50048]	Loss: 1.5146
Training Epoch: 9 [37632/50048]	Loss: 1.5796
Training Epoch: 9 [37760/50048]	Loss: 1.6205
Training Epoch: 9 [37888/50048]	Loss: 1.5332
Training Epoch: 9 [38016/50048]	Loss: 1.4086
Training Epoch: 9 [38144/50048]	Loss: 1.6313
Training Epoch: 9 [38272/50048]	Loss: 1.7564
Training Epoch: 9 [38400/50048]	Loss: 1.5651
Training Epoch: 9 [38528/50048]	Loss: 1.5796
Training Epoch: 9 [38656/50048]	Loss: 1.5917
Training Epoch: 9 [38784/50048]	Loss: 1.5649
Training Epoch: 9 [38912/50048]	Loss: 1.7694
Training Epoch: 9 [39040/50048]	Loss: 1.7119
Training Epoch: 9 [39168/50048]	Loss: 1.5781
Training Epoch: 9 [39296/50048]	Loss: 1.6571
Training Epoch: 9 [39424/50048]	Loss: 1.7264
Training Epoch: 9 [39552/50048]	Loss: 1.5469
Training Epoch: 9 [39680/50048]	Loss: 1.9036
Training Epoch: 9 [39808/50048]	Loss: 1.7859
Training Epoch: 9 [39936/50048]	Loss: 1.6766
Training Epoch: 9 [40064/50048]	Loss: 1.6483
Training Epoch: 9 [40192/50048]	Loss: 1.4124
Training Epoch: 9 [40320/50048]	Loss: 1.8250
Training Epoch: 9 [40448/50048]	Loss: 1.5044
Training Epoch: 9 [40576/50048]	Loss: 1.8590
Training Epoch: 9 [40704/50048]	Loss: 1.4895
Training Epoch: 9 [40832/50048]	Loss: 1.4701
Training Epoch: 9 [40960/50048]	Loss: 1.5487
Training Epoch: 9 [41088/50048]	Loss: 1.6414
Training Epoch: 9 [41216/50048]	Loss: 1.7841
Training Epoch: 9 [41344/50048]	Loss: 1.7077
Training Epoch: 9 [41472/50048]	Loss: 1.6313
Training Epoch: 9 [41600/50048]	Loss: 1.4615
Training Epoch: 9 [41728/50048]	Loss: 1.6539
Training Epoch: 9 [41856/50048]	Loss: 1.8025
Training Epoch: 9 [41984/50048]	Loss: 1.6508
Training Epoch: 9 [42112/50048]	Loss: 1.5729
Training Epoch: 9 [42240/50048]	Loss: 1.4814
Training Epoch: 9 [42368/50048]	Loss: 1.8419
Training Epoch: 9 [42496/50048]	Loss: 1.4730
Training Epoch: 9 [42624/50048]	Loss: 1.6814
Training Epoch: 9 [42752/50048]	Loss: 1.6728
Training Epoch: 9 [42880/50048]	Loss: 1.7546
Training Epoch: 9 [43008/50048]	Loss: 1.5475
Training Epoch: 9 [43136/50048]	Loss: 1.3976
Training Epoch: 9 [43264/50048]	Loss: 1.8940
Training Epoch: 9 [43392/50048]	Loss: 1.5953
Training Epoch: 9 [43520/50048]	Loss: 1.4508
Training Epoch: 9 [43648/50048]	Loss: 1.5573
Training Epoch: 9 [43776/50048]	Loss: 1.7899
Training Epoch: 9 [43904/50048]	Loss: 1.4983
Training Epoch: 9 [44032/50048]	Loss: 1.5402
Training Epoch: 9 [44160/50048]	Loss: 1.2428
Training Epoch: 9 [44288/50048]	Loss: 1.5695
Training Epoch: 9 [44416/50048]	Loss: 1.5639
Training Epoch: 9 [44544/50048]	Loss: 1.8837
Training Epoch: 9 [44672/50048]	Loss: 1.6350
Training Epoch: 9 [44800/50048]	Loss: 1.8395
Training Epoch: 9 [44928/50048]	Loss: 1.5564
Training Epoch: 9 [45056/50048]	Loss: 1.5474
Training Epoch: 9 [45184/50048]	Loss: 1.4155
Training Epoch: 9 [45312/50048]	Loss: 1.6667
Training Epoch: 9 [45440/50048]	Loss: 1.9094
Training Epoch: 9 [45568/50048]	Loss: 1.4134
Training Epoch: 9 [45696/50048]	Loss: 1.7211
Training Epoch: 9 [45824/50048]	Loss: 1.4863
Training Epoch: 9 [45952/50048]	Loss: 1.5243
Training Epoch: 9 [46080/50048]	Loss: 1.7415
Training Epoch: 9 [46208/50048]	Loss: 1.6371
Training Epoch: 9 [46336/50048]	Loss: 1.4408
Training Epoch: 9 [46464/50048]	Loss: 1.4654
Training Epoch: 9 [46592/50048]	Loss: 1.4682
Training Epoch: 9 [46720/50048]	Loss: 1.6739
Training Epoch: 9 [46848/50048]	Loss: 1.7756
Training Epoch: 9 [46976/50048]	Loss: 1.4100
Training Epoch: 9 [47104/50048]	Loss: 1.6642
Training Epoch: 9 [47232/50048]	Loss: 1.6908
Training Epoch: 9 [47360/50048]	Loss: 1.5323
Training Epoch: 9 [47488/50048]	Loss: 1.5816
Training Epoch: 9 [47616/50048]	Loss: 1.6446
Training Epoch: 9 [47744/50048]	Loss: 1.4062
Training Epoch: 9 [47872/50048]	Loss: 1.2254
Training Epoch: 9 [48000/50048]	Loss: 1.5425
Training Epoch: 9 [48128/50048]	Loss: 1.5630
Training Epoch: 9 [48256/50048]	Loss: 1.8040
Training Epoch: 9 [48384/50048]	Loss: 1.7124
Training Epoch: 9 [48512/50048]	Loss: 1.7215
Training Epoch: 9 [48640/50048]	Loss: 1.5814
Training Epoch: 9 [48768/50048]	Loss: 1.6939
Training Epoch: 9 [48896/50048]	Loss: 1.5503
Training Epoch: 9 [49024/50048]	Loss: 1.7605
Training Epoch: 9 [49152/50048]	Loss: 1.7539
Training Epoch: 9 [49280/50048]	Loss: 1.8138
Training Epoch: 9 [49408/50048]	Loss: 1.6814
Training Epoch: 9 [49536/50048]	Loss: 1.7920
Training Epoch: 9 [49664/50048]	Loss: 1.5875
Training Epoch: 9 [49792/50048]	Loss: 1.6031
Training Epoch: 9 [49920/50048]	Loss: 1.7068
Training Epoch: 9 [50048/50048]	Loss: 1.6018
Validation Epoch: 9, Average loss: 0.0148, Accuracy: 0.4849
Training Epoch: 10 [128/50048]	Loss: 1.7532
Training Epoch: 10 [256/50048]	Loss: 1.4521
Training Epoch: 10 [384/50048]	Loss: 1.6742
Training Epoch: 10 [512/50048]	Loss: 1.3911
Training Epoch: 10 [640/50048]	Loss: 1.4760
Training Epoch: 10 [768/50048]	Loss: 1.4870
Training Epoch: 10 [896/50048]	Loss: 1.4712
Training Epoch: 10 [1024/50048]	Loss: 1.4535
Training Epoch: 10 [1152/50048]	Loss: 1.5361
Training Epoch: 10 [1280/50048]	Loss: 1.4648
Training Epoch: 10 [1408/50048]	Loss: 1.4822
Training Epoch: 10 [1536/50048]	Loss: 1.5353
Training Epoch: 10 [1664/50048]	Loss: 1.3726
Training Epoch: 10 [1792/50048]	Loss: 1.5885
Training Epoch: 10 [1920/50048]	Loss: 1.4949
Training Epoch: 10 [2048/50048]	Loss: 1.4634
Training Epoch: 10 [2176/50048]	Loss: 1.3910
Training Epoch: 10 [2304/50048]	Loss: 1.4787
Training Epoch: 10 [2432/50048]	Loss: 1.4776
Training Epoch: 10 [2560/50048]	Loss: 1.3312
Training Epoch: 10 [2688/50048]	Loss: 1.5706
Training Epoch: 10 [2816/50048]	Loss: 1.4301
Training Epoch: 10 [2944/50048]	Loss: 1.4206
Training Epoch: 10 [3072/50048]	Loss: 1.4624
Training Epoch: 10 [3200/50048]	Loss: 1.2721
Training Epoch: 10 [3328/50048]	Loss: 1.5766
Training Epoch: 10 [3456/50048]	Loss: 1.5710
Training Epoch: 10 [3584/50048]	Loss: 1.3736
Training Epoch: 10 [3712/50048]	Loss: 1.7869
Training Epoch: 10 [3840/50048]	Loss: 1.7423
Training Epoch: 10 [3968/50048]	Loss: 1.3590
Training Epoch: 10 [4096/50048]	Loss: 1.4808
Training Epoch: 10 [4224/50048]	Loss: 1.6385
Training Epoch: 10 [4352/50048]	Loss: 1.7324
Training Epoch: 10 [4480/50048]	Loss: 1.3517
Training Epoch: 10 [4608/50048]	Loss: 1.3207
Training Epoch: 10 [4736/50048]	Loss: 1.4038
Training Epoch: 10 [4864/50048]	Loss: 1.6190
Training Epoch: 10 [4992/50048]	Loss: 1.2841
Training Epoch: 10 [5120/50048]	Loss: 1.5165
Training Epoch: 10 [5248/50048]	Loss: 1.3578
Training Epoch: 10 [5376/50048]	Loss: 1.7209
Training Epoch: 10 [5504/50048]	Loss: 1.5441
Training Epoch: 10 [5632/50048]	Loss: 1.6230
Training Epoch: 10 [5760/50048]	Loss: 1.4665
Training Epoch: 10 [5888/50048]	Loss: 1.5489
Training Epoch: 10 [6016/50048]	Loss: 1.4577
Training Epoch: 10 [6144/50048]	Loss: 1.2428
Training Epoch: 10 [6272/50048]	Loss: 1.5601
Training Epoch: 10 [6400/50048]	Loss: 1.6386
Training Epoch: 10 [6528/50048]	Loss: 1.5028
Training Epoch: 10 [6656/50048]	Loss: 1.4728
Training Epoch: 10 [6784/50048]	Loss: 1.3933
Training Epoch: 10 [6912/50048]	Loss: 1.4076
Training Epoch: 10 [7040/50048]	Loss: 1.4826
Training Epoch: 10 [7168/50048]	Loss: 1.4462
Training Epoch: 10 [7296/50048]	Loss: 1.7243
Training Epoch: 10 [7424/50048]	Loss: 1.5115
Training Epoch: 10 [7552/50048]	Loss: 1.7041
Training Epoch: 10 [7680/50048]	Loss: 1.3289
Training Epoch: 10 [7808/50048]	Loss: 1.4661
Training Epoch: 10 [7936/50048]	Loss: 1.4198
Training Epoch: 10 [8064/50048]	Loss: 1.4732
Training Epoch: 10 [8192/50048]	Loss: 1.6215
Training Epoch: 10 [8320/50048]	Loss: 1.5655
Training Epoch: 10 [8448/50048]	Loss: 1.6647
Training Epoch: 10 [8576/50048]	Loss: 1.5271
Training Epoch: 10 [8704/50048]	Loss: 1.4587
Training Epoch: 10 [8832/50048]	Loss: 1.3468
Training Epoch: 10 [8960/50048]	Loss: 1.5396
Training Epoch: 10 [9088/50048]	Loss: 1.6656
Training Epoch: 10 [9216/50048]	Loss: 1.6661
Training Epoch: 10 [9344/50048]	Loss: 1.6436
Training Epoch: 10 [9472/50048]	Loss: 1.5577
Training Epoch: 10 [9600/50048]	Loss: 1.5850
Training Epoch: 10 [9728/50048]	Loss: 1.7739
Training Epoch: 10 [9856/50048]	Loss: 1.5865
Training Epoch: 10 [9984/50048]	Loss: 1.5244
Training Epoch: 10 [10112/50048]	Loss: 1.6956
Training Epoch: 10 [10240/50048]	Loss: 1.7109
Training Epoch: 10 [10368/50048]	Loss: 1.5724
Training Epoch: 10 [10496/50048]	Loss: 1.6517
Training Epoch: 10 [10624/50048]	Loss: 1.4873
Training Epoch: 10 [10752/50048]	Loss: 1.7396
Training Epoch: 10 [10880/50048]	Loss: 1.6885
Training Epoch: 10 [11008/50048]	Loss: 1.5025
Training Epoch: 10 [11136/50048]	Loss: 1.5365
Training Epoch: 10 [11264/50048]	Loss: 1.7123
Training Epoch: 10 [11392/50048]	Loss: 1.3655
Training Epoch: 10 [11520/50048]	Loss: 1.9178
Training Epoch: 10 [11648/50048]	Loss: 1.6197
Training Epoch: 10 [11776/50048]	Loss: 1.4968
Training Epoch: 10 [11904/50048]	Loss: 1.6456
Training Epoch: 10 [12032/50048]	Loss: 1.3484
Training Epoch: 10 [12160/50048]	Loss: 1.5936
Training Epoch: 10 [12288/50048]	Loss: 1.5655
Training Epoch: 10 [12416/50048]	Loss: 1.7729
Training Epoch: 10 [12544/50048]	Loss: 1.5747
Training Epoch: 10 [12672/50048]	Loss: 1.6027
Training Epoch: 10 [12800/50048]	Loss: 1.6550
Training Epoch: 10 [12928/50048]	Loss: 1.6384
Training Epoch: 10 [13056/50048]	Loss: 1.5643
Training Epoch: 10 [13184/50048]	Loss: 1.6820
Training Epoch: 10 [13312/50048]	Loss: 1.4524
Training Epoch: 10 [13440/50048]	Loss: 1.4409
Training Epoch: 10 [13568/50048]	Loss: 1.6357
Training Epoch: 10 [13696/50048]	Loss: 1.6121
Training Epoch: 10 [13824/50048]	Loss: 1.4543
Training Epoch: 10 [13952/50048]	Loss: 1.4864
Training Epoch: 10 [14080/50048]	Loss: 1.5240
Training Epoch: 10 [14208/50048]	Loss: 1.5710
Training Epoch: 10 [14336/50048]	Loss: 1.4889
Training Epoch: 10 [14464/50048]	Loss: 1.5746
Training Epoch: 10 [14592/50048]	Loss: 1.4385
Training Epoch: 10 [14720/50048]	Loss: 1.4119
Training Epoch: 10 [14848/50048]	Loss: 1.5647
Training Epoch: 10 [14976/50048]	Loss: 1.2800
Training Epoch: 10 [15104/50048]	Loss: 1.6410
Training Epoch: 10 [15232/50048]	Loss: 1.5234
Training Epoch: 10 [15360/50048]	Loss: 1.3662
Training Epoch: 10 [15488/50048]	Loss: 1.5791
Training Epoch: 10 [15616/50048]	Loss: 1.4414
Training Epoch: 10 [15744/50048]	Loss: 1.6450
Training Epoch: 10 [15872/50048]	Loss: 1.6232
Training Epoch: 10 [16000/50048]	Loss: 1.5180
Training Epoch: 10 [16128/50048]	Loss: 1.3607
Training Epoch: 10 [16256/50048]	Loss: 1.4927
Training Epoch: 10 [16384/50048]	Loss: 1.5611
Training Epoch: 10 [16512/50048]	Loss: 1.4390
Training Epoch: 10 [16640/50048]	Loss: 1.8361
Training Epoch: 10 [16768/50048]	Loss: 1.4365
Training Epoch: 10 [16896/50048]	Loss: 1.4404
Training Epoch: 10 [17024/50048]	Loss: 1.7258
Training Epoch: 10 [17152/50048]	Loss: 1.3606
Training Epoch: 10 [17280/50048]	Loss: 1.5163
Training Epoch: 10 [17408/50048]	Loss: 1.6631
Training Epoch: 10 [17536/50048]	Loss: 1.6834
Training Epoch: 10 [17664/50048]	Loss: 1.4661
Training Epoch: 10 [17792/50048]	Loss: 1.5978
Training Epoch: 10 [17920/50048]	Loss: 1.6919
Training Epoch: 10 [18048/50048]	Loss: 1.6464
Training Epoch: 10 [18176/50048]	Loss: 1.4391
Training Epoch: 10 [18304/50048]	Loss: 1.5408
Training Epoch: 10 [18432/50048]	Loss: 1.6152
Training Epoch: 10 [18560/50048]	Loss: 1.6054
Training Epoch: 10 [18688/50048]	Loss: 1.4003
Training Epoch: 10 [18816/50048]	Loss: 1.6662
Training Epoch: 10 [18944/50048]	Loss: 1.4809
Training Epoch: 10 [19072/50048]	Loss: 1.4975
Training Epoch: 10 [19200/50048]	Loss: 1.4501
Training Epoch: 10 [19328/50048]	Loss: 1.4682
Training Epoch: 10 [19456/50048]	Loss: 1.7416
Training Epoch: 10 [19584/50048]	Loss: 1.5794
Training Epoch: 10 [19712/50048]	Loss: 1.5118
Training Epoch: 10 [19840/50048]	Loss: 1.5875
Training Epoch: 10 [19968/50048]	Loss: 1.7034
Training Epoch: 10 [20096/50048]	Loss: 1.5562
Training Epoch: 10 [20224/50048]	Loss: 1.6084
Training Epoch: 10 [20352/50048]	Loss: 1.7171
Training Epoch: 10 [20480/50048]	Loss: 1.5785
Training Epoch: 10 [20608/50048]	Loss: 1.5819
Training Epoch: 10 [20736/50048]	Loss: 1.6079
Training Epoch: 10 [20864/50048]	Loss: 1.5176
Training Epoch: 10 [20992/50048]	Loss: 1.7438
Training Epoch: 10 [21120/50048]	Loss: 1.7326
Training Epoch: 10 [21248/50048]	Loss: 1.6494
Training Epoch: 10 [21376/50048]	Loss: 1.4510
Training Epoch: 10 [21504/50048]	Loss: 1.4846
Training Epoch: 10 [21632/50048]	Loss: 1.6576
Training Epoch: 10 [21760/50048]	Loss: 1.4974
Training Epoch: 10 [21888/50048]	Loss: 1.6700
Training Epoch: 10 [22016/50048]	Loss: 1.6280
Training Epoch: 10 [22144/50048]	Loss: 1.4713
Training Epoch: 10 [22272/50048]	Loss: 1.9311
Training Epoch: 10 [22400/50048]	Loss: 1.6534
Training Epoch: 10 [22528/50048]	Loss: 1.5946
Training Epoch: 10 [22656/50048]	Loss: 1.7372
Training Epoch: 10 [22784/50048]	Loss: 1.3289
Training Epoch: 10 [22912/50048]	Loss: 1.6436
Training Epoch: 10 [23040/50048]	Loss: 1.3941
Training Epoch: 10 [23168/50048]	Loss: 1.3138
Training Epoch: 10 [23296/50048]	Loss: 1.3831
Training Epoch: 10 [23424/50048]	Loss: 1.5492
Training Epoch: 10 [23552/50048]	Loss: 1.5909
Training Epoch: 10 [23680/50048]	Loss: 1.7403
Training Epoch: 10 [23808/50048]	Loss: 1.4387
Training Epoch: 10 [23936/50048]	Loss: 1.5902
Training Epoch: 10 [24064/50048]	Loss: 1.6611
Training Epoch: 10 [24192/50048]	Loss: 1.5172
Training Epoch: 10 [24320/50048]	Loss: 1.4765
Training Epoch: 10 [24448/50048]	Loss: 1.7058
Training Epoch: 10 [24576/50048]	Loss: 1.3863
Training Epoch: 10 [24704/50048]	Loss: 1.4998
Training Epoch: 10 [24832/50048]	Loss: 1.5074
Training Epoch: 10 [24960/50048]	Loss: 1.5662
Training Epoch: 10 [25088/50048]	Loss: 1.4667
Training Epoch: 10 [25216/50048]	Loss: 1.6357
Training Epoch: 10 [25344/50048]	Loss: 1.3124
Training Epoch: 10 [25472/50048]	Loss: 1.5727
Training Epoch: 10 [25600/50048]	Loss: 1.4088
Training Epoch: 10 [25728/50048]	Loss: 1.7316
Training Epoch: 10 [25856/50048]	Loss: 1.6447
Training Epoch: 10 [25984/50048]	Loss: 1.5944
Training Epoch: 10 [26112/50048]	Loss: 1.4970
Training Epoch: 10 [26240/50048]	Loss: 1.6417
Training Epoch: 10 [26368/50048]	Loss: 1.8552
Training Epoch: 10 [26496/50048]	Loss: 1.4449
Training Epoch: 10 [26624/50048]	Loss: 1.6504
Training Epoch: 10 [26752/50048]	Loss: 1.3874
Training Epoch: 10 [26880/50048]	Loss: 1.5030
Training Epoch: 10 [27008/50048]	Loss: 1.7546
Training Epoch: 10 [27136/50048]	Loss: 1.6003
Training Epoch: 10 [27264/50048]	Loss: 1.5928
Training Epoch: 10 [27392/50048]	Loss: 1.4208
Training Epoch: 10 [27520/50048]	Loss: 1.4494
Training Epoch: 10 [27648/50048]	Loss: 1.6625
Training Epoch: 10 [27776/50048]	Loss: 1.5354
Training Epoch: 10 [27904/50048]	Loss: 1.5543
Training Epoch: 10 [28032/50048]	Loss: 1.6485
Training Epoch: 10 [28160/50048]	Loss: 1.7049
Training Epoch: 10 [28288/50048]	Loss: 1.3845
Training Epoch: 10 [28416/50048]	Loss: 1.4569
Training Epoch: 10 [28544/50048]	Loss: 1.3126
Training Epoch: 10 [28672/50048]	Loss: 1.4407
Training Epoch: 10 [28800/50048]	Loss: 1.8481
Training Epoch: 10 [28928/50048]	Loss: 1.5598
Training Epoch: 10 [29056/50048]	Loss: 1.4675
Training Epoch: 10 [29184/50048]	Loss: 1.4121
Training Epoch: 10 [29312/50048]	Loss: 1.6096
Training Epoch: 10 [29440/50048]	Loss: 1.4514
Training Epoch: 10 [29568/50048]	Loss: 1.4361
Training Epoch: 10 [29696/50048]	Loss: 1.4155
Training Epoch: 10 [29824/50048]	Loss: 1.1914
Training Epoch: 10 [29952/50048]	Loss: 1.3850
Training Epoch: 10 [30080/50048]	Loss: 1.5888
Training Epoch: 10 [30208/50048]	Loss: 1.5817
Training Epoch: 10 [30336/50048]	Loss: 1.8311
Training Epoch: 10 [30464/50048]	Loss: 1.3815
Training Epoch: 10 [30592/50048]	Loss: 1.5632
Training Epoch: 10 [30720/50048]	Loss: 1.3863
Training Epoch: 10 [30848/50048]	Loss: 1.5425
Training Epoch: 10 [30976/50048]	Loss: 1.6380
Training Epoch: 10 [31104/50048]	Loss: 1.2763
Training Epoch: 10 [31232/50048]	Loss: 1.2908
Training Epoch: 10 [31360/50048]	Loss: 1.5770
Training Epoch: 10 [31488/50048]	Loss: 1.5048
Training Epoch: 10 [31616/50048]	Loss: 1.6768
Training Epoch: 10 [31744/50048]	Loss: 1.7190
Training Epoch: 10 [31872/50048]	Loss: 1.5088
Training Epoch: 10 [32000/50048]	Loss: 1.5551
Training Epoch: 10 [32128/50048]	Loss: 1.4573
Training Epoch: 10 [32256/50048]	Loss: 1.7259
Training Epoch: 10 [32384/50048]	Loss: 1.4831
Training Epoch: 10 [32512/50048]	Loss: 1.6511
Training Epoch: 10 [32640/50048]	Loss: 1.5704
Training Epoch: 10 [32768/50048]	Loss: 1.5420
Training Epoch: 10 [32896/50048]	Loss: 1.3862
Training Epoch: 10 [33024/50048]	Loss: 1.5262
Training Epoch: 10 [33152/50048]	Loss: 1.4433
Training Epoch: 10 [33280/50048]	Loss: 1.4296
Training Epoch: 10 [33408/50048]	Loss: 1.6781
Training Epoch: 10 [33536/50048]	Loss: 1.6109
Training Epoch: 10 [33664/50048]	Loss: 1.4452
Training Epoch: 10 [33792/50048]	Loss: 1.4091
Training Epoch: 10 [33920/50048]	Loss: 1.6561
Training Epoch: 10 [34048/50048]	Loss: 1.4133
Training Epoch: 10 [34176/50048]	Loss: 1.4672
Training Epoch: 10 [34304/50048]	Loss: 1.4943
Training Epoch: 10 [34432/50048]	Loss: 1.5720
Training Epoch: 10 [34560/50048]	Loss: 1.6131
Training Epoch: 10 [34688/50048]	Loss: 1.4530
Training Epoch: 10 [34816/50048]	Loss: 1.3708
Training Epoch: 10 [34944/50048]	Loss: 1.3917
Training Epoch: 10 [35072/50048]	Loss: 1.6412
Training Epoch: 10 [35200/50048]	Loss: 1.6659
Training Epoch: 10 [35328/50048]	Loss: 1.4733
Training Epoch: 10 [35456/50048]	Loss: 1.5849
Training Epoch: 10 [35584/50048]	Loss: 1.6225
Training Epoch: 10 [35712/50048]	Loss: 1.5038
Training Epoch: 10 [35840/50048]	Loss: 1.4559
Training Epoch: 10 [35968/50048]	Loss: 1.2980
Training Epoch: 10 [36096/50048]	Loss: 1.4973
Training Epoch: 10 [36224/50048]	Loss: 1.5160
Training Epoch: 10 [36352/50048]	Loss: 1.6552
Training Epoch: 10 [36480/50048]	Loss: 1.4000
Training Epoch: 10 [36608/50048]	Loss: 1.5173
Training Epoch: 10 [36736/50048]	Loss: 1.5249
Training Epoch: 10 [36864/50048]	Loss: 1.5142
Training Epoch: 10 [36992/50048]	Loss: 1.7248
Training Epoch: 10 [37120/50048]	Loss: 1.5094
Training Epoch: 10 [37248/50048]	Loss: 1.7432
Training Epoch: 10 [37376/50048]	Loss: 1.5961
Training Epoch: 10 [37504/50048]	Loss: 1.5007
Training Epoch: 10 [37632/50048]	Loss: 1.6694
Training Epoch: 10 [37760/50048]	Loss: 1.6926
Training Epoch: 10 [37888/50048]	Loss: 1.1966
Training Epoch: 10 [38016/50048]	Loss: 1.5052
Training Epoch: 10 [38144/50048]	Loss: 1.5884
Training Epoch: 10 [38272/50048]	Loss: 1.4565
Training Epoch: 10 [38400/50048]	Loss: 1.4593
Training Epoch: 10 [38528/50048]	Loss: 1.4011
Training Epoch: 10 [38656/50048]	Loss: 1.6345
Training Epoch: 10 [38784/50048]	Loss: 1.6342
Training Epoch: 10 [38912/50048]	Loss: 1.4400
Training Epoch: 10 [39040/50048]	Loss: 1.6041
Training Epoch: 10 [39168/50048]	Loss: 1.3857
Training Epoch: 10 [39296/50048]	Loss: 1.7539
Training Epoch: 10 [39424/50048]	Loss: 1.6786
Training Epoch: 10 [39552/50048]	Loss: 1.5176
Training Epoch: 10 [39680/50048]	Loss: 1.7485
Training Epoch: 10 [39808/50048]	Loss: 1.4803
Training Epoch: 10 [39936/50048]	Loss: 1.5151
Training Epoch: 10 [40064/50048]	Loss: 1.5053
Training Epoch: 10 [40192/50048]	Loss: 1.5657
Training Epoch: 10 [40320/50048]	Loss: 1.5466
Training Epoch: 10 [40448/50048]	Loss: 1.6185
Training Epoch: 10 [40576/50048]	Loss: 1.5290
Training Epoch: 10 [40704/50048]	Loss: 1.6203
Training Epoch: 10 [40832/50048]	Loss: 1.4852
Training Epoch: 10 [40960/50048]	Loss: 1.7323
Training Epoch: 10 [41088/50048]	Loss: 1.5204
Training Epoch: 10 [41216/50048]	Loss: 1.6328
Training Epoch: 10 [41344/50048]	Loss: 1.1681
Training Epoch: 10 [41472/50048]	Loss: 1.7518
Training Epoch: 10 [41600/50048]	Loss: 1.7359
Training Epoch: 10 [41728/50048]	Loss: 1.5981
Training Epoch: 10 [41856/50048]	Loss: 1.5428
Training Epoch: 10 [41984/50048]	Loss: 1.6914
Training Epoch: 10 [42112/50048]	Loss: 1.6736
Training Epoch: 10 [42240/50048]	Loss: 1.4385
Training Epoch: 10 [42368/50048]	Loss: 1.5800
Training Epoch: 10 [42496/50048]	Loss: 1.7695
Training Epoch: 10 [42624/50048]	Loss: 1.5655
Training Epoch: 10 [42752/50048]	Loss: 1.5369
Training Epoch: 10 [42880/50048]	Loss: 1.7289
Training Epoch: 10 [43008/50048]	Loss: 1.4043
Training Epoch: 10 [43136/50048]	Loss: 1.5027
Training Epoch: 10 [43264/50048]	Loss: 1.2856
Training Epoch: 10 [43392/50048]	Loss: 1.3067
Training Epoch: 10 [43520/50048]	Loss: 1.6604
Training Epoch: 10 [43648/50048]	Loss: 1.6410
Training Epoch: 10 [43776/50048]	Loss: 1.4199
Training Epoch: 10 [43904/50048]	Loss: 1.5746
Training Epoch: 10 [44032/50048]	Loss: 1.5824
Training Epoch: 10 [44160/50048]	Loss: 1.5093
Training Epoch: 10 [44288/50048]	Loss: 1.6474
Training Epoch: 10 [44416/50048]	Loss: 1.2372
Training Epoch: 10 [44544/50048]	Loss: 1.4296
Training Epoch: 10 [44672/50048]	Loss: 1.5442
Training Epoch: 10 [44800/50048]	Loss: 1.4109
Training Epoch: 10 [44928/50048]	Loss: 1.4120
Training Epoch: 10 [45056/50048]	Loss: 1.2768
Training Epoch: 10 [45184/50048]	Loss: 1.2859
Training Epoch: 10 [45312/50048]	Loss: 1.5598
Training Epoch: 10 [45440/50048]	Loss: 1.4401
Training Epoch: 10 [45568/50048]	Loss: 1.4481
Training Epoch: 10 [45696/50048]	Loss: 1.6408
Training Epoch: 10 [45824/50048]	Loss: 1.4890
Training Epoch: 10 [45952/50048]	Loss: 1.6735
Training Epoch: 10 [46080/50048]	Loss: 1.6216
Training Epoch: 10 [46208/50048]	Loss: 1.6431
Training Epoch: 10 [46336/50048]	Loss: 1.7487
Training Epoch: 10 [46464/50048]	Loss: 1.5586
Training Epoch: 10 [46592/50048]	Loss: 1.4780
Training Epoch: 10 [46720/50048]	Loss: 1.5565
Training Epoch: 10 [46848/50048]	Loss: 1.6131
Training Epoch: 10 [46976/50048]	Loss: 1.6920
Training Epoch: 10 [47104/50048]	Loss: 1.7015
Training Epoch: 10 [47232/50048]	Loss: 1.2603
Training Epoch: 10 [47360/50048]	Loss: 1.3377
Training Epoch: 10 [47488/50048]	Loss: 1.4785
Training Epoch: 10 [47616/50048]	Loss: 1.6602
Training Epoch: 10 [47744/50048]	Loss: 1.7288
Training Epoch: 10 [47872/50048]	Loss: 1.5997
Training Epoch: 10 [48000/50048]	Loss: 1.5309
Training Epoch: 10 [48128/50048]	Loss: 1.4693
Training Epoch: 10 [48256/50048]	Loss: 1.5292
Training Epoch: 10 [48384/50048]	Loss: 1.3931
Training Epoch: 10 [48512/50048]	Loss: 1.6900
Training Epoch: 10 [48640/50048]	Loss: 1.4939
Training Epoch: 10 [48768/50048]	Loss: 1.7698
Training Epoch: 10 [48896/50048]	Loss: 1.3344
Training Epoch: 10 [49024/50048]	Loss: 1.3109
Training Epoch: 10 [49152/50048]	Loss: 1.6821
Training Epoch: 10 [49280/50048]	Loss: 1.4439
Training Epoch: 10 [49408/50048]	Loss: 1.6251
Training Epoch: 10 [49536/50048]	Loss: 1.7650
Training Epoch: 10 [49664/50048]	Loss: 1.5817
Training Epoch: 10 [49792/50048]	Loss: 1.5468
Training Epoch: 10 [49920/50048]	Loss: 1.4759
Training Epoch: 10 [50048/50048]	Loss: 1.5894
Validation Epoch: 10, Average loss: 0.0139, Accuracy: 0.5199
[Training Loop] Model's accuracy 0.5198773734177216 surpasses threshold 0.5! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6438
Profiling... [256/50048]	Loss: 1.3251
Profiling... [384/50048]	Loss: 1.5464
Profiling... [512/50048]	Loss: 1.3829
Profiling... [640/50048]	Loss: 1.1794
Profiling... [768/50048]	Loss: 1.4238
Profiling... [896/50048]	Loss: 1.3821
Profiling... [1024/50048]	Loss: 1.2915
Profiling... [1152/50048]	Loss: 1.2673
Profiling... [1280/50048]	Loss: 1.3994
Profiling... [1408/50048]	Loss: 1.5678
Profiling... [1536/50048]	Loss: 1.2269
Profiling... [1664/50048]	Loss: 1.6479
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 11, Average loss: 0.0123, Accuracy: 0.5614
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.72817995848864,
                        "time": 2.1785577689952333,
                        "accuracy": 0.5614121835443038,
                        "total_cost": 449.0827469196664
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.3399
Profiling... [256/50048]	Loss: 1.2795
Profiling... [384/50048]	Loss: 1.4284
Profiling... [512/50048]	Loss: 1.3646
Profiling... [640/50048]	Loss: 1.3220
Profiling... [768/50048]	Loss: 1.3622
Profiling... [896/50048]	Loss: 1.4106
Profiling... [1024/50048]	Loss: 1.1575
Profiling... [1152/50048]	Loss: 1.2937
Profiling... [1280/50048]	Loss: 1.3993
Profiling... [1408/50048]	Loss: 1.3589
Profiling... [1536/50048]	Loss: 1.2810
Profiling... [1664/50048]	Loss: 1.2187
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 11, Average loss: 0.0125, Accuracy: 0.5567
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.7309015563051,
                        "time": 2.187775774997135,
                        "accuracy": 0.5566653481012658,
                        "total_cost": 454.83927409363895
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5289
Profiling... [256/50048]	Loss: 1.5419
Profiling... [384/50048]	Loss: 1.3466
Profiling... [512/50048]	Loss: 1.2870
Profiling... [640/50048]	Loss: 1.3580
Profiling... [768/50048]	Loss: 1.3717
Profiling... [896/50048]	Loss: 1.3864
Profiling... [1024/50048]	Loss: 1.5761
Profiling... [1152/50048]	Loss: 1.3309
Profiling... [1280/50048]	Loss: 1.3014
Profiling... [1408/50048]	Loss: 1.4688
Profiling... [1536/50048]	Loss: 1.6198
Profiling... [1664/50048]	Loss: 1.2585
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 11, Average loss: 0.0125, Accuracy: 0.5566
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.73115673433458,
                        "time": 2.223593670001719,
                        "accuracy": 0.5565664556962026,
                        "total_cost": 462.36898559497394
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.3577
Profiling... [256/50048]	Loss: 1.3605
Profiling... [384/50048]	Loss: 1.5547
Profiling... [512/50048]	Loss: 1.4944
Profiling... [640/50048]	Loss: 1.7058
Profiling... [768/50048]	Loss: 1.3064
Profiling... [896/50048]	Loss: 1.3627
Profiling... [1024/50048]	Loss: 1.5217
Profiling... [1152/50048]	Loss: 1.3833
Profiling... [1280/50048]	Loss: 1.3635
Profiling... [1408/50048]	Loss: 1.4284
Profiling... [1536/50048]	Loss: 1.5096
Profiling... [1664/50048]	Loss: 1.4680
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 11, Average loss: 0.0125, Accuracy: 0.5572
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.71709867036631,
                        "time": 2.595476267997583,
                        "accuracy": 0.5571598101265823,
                        "total_cost": 539.0571572853313
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7256
Profiling... [256/50048]	Loss: 1.5967
Profiling... [384/50048]	Loss: 1.4900
Profiling... [512/50048]	Loss: 1.4053
Profiling... [640/50048]	Loss: 1.3324
Profiling... [768/50048]	Loss: 1.3381
Profiling... [896/50048]	Loss: 1.6382
Profiling... [1024/50048]	Loss: 1.5341
Profiling... [1152/50048]	Loss: 1.4741
Profiling... [1280/50048]	Loss: 1.4506
Profiling... [1408/50048]	Loss: 1.4691
Profiling... [1536/50048]	Loss: 1.3888
Profiling... [1664/50048]	Loss: 1.5532
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 11, Average loss: 0.0126, Accuracy: 0.5531
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.6977964627739,
                        "time": 2.16752454899688,
                        "accuracy": 0.5531052215189873,
                        "total_cost": 453.3998312458491
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4558
Profiling... [256/50048]	Loss: 1.4420
Profiling... [384/50048]	Loss: 1.2844
Profiling... [512/50048]	Loss: 1.4952
Profiling... [640/50048]	Loss: 1.5466
Profiling... [768/50048]	Loss: 1.3155
Profiling... [896/50048]	Loss: 1.3515
Profiling... [1024/50048]	Loss: 1.4749
Profiling... [1152/50048]	Loss: 1.1161
Profiling... [1280/50048]	Loss: 1.4018
Profiling... [1408/50048]	Loss: 1.5779
Profiling... [1536/50048]	Loss: 1.3445
Profiling... [1664/50048]	Loss: 1.3346
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 11, Average loss: 0.0126, Accuracy: 0.5562
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.70087477736098,
                        "time": 2.177598826994654,
                        "accuracy": 0.5561708860759493,
                        "total_cost": 453.0084107333711
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4815
Profiling... [256/50048]	Loss: 1.2882
Profiling... [384/50048]	Loss: 1.4713
Profiling... [512/50048]	Loss: 1.3046
Profiling... [640/50048]	Loss: 1.3581
Profiling... [768/50048]	Loss: 1.5543
Profiling... [896/50048]	Loss: 1.3050
Profiling... [1024/50048]	Loss: 1.2547
Profiling... [1152/50048]	Loss: 1.3516
Profiling... [1280/50048]	Loss: 1.2401
Profiling... [1408/50048]	Loss: 1.3869
Profiling... [1536/50048]	Loss: 1.4162
Profiling... [1664/50048]	Loss: 1.3517
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 11, Average loss: 0.0125, Accuracy: 0.5582
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.70234788502827,
                        "time": 2.222585807001451,
                        "accuracy": 0.5582476265822784,
                        "total_cost": 460.6529145862949
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.4594
Profiling... [256/50048]	Loss: 1.2735
Profiling... [384/50048]	Loss: 1.4424
Profiling... [512/50048]	Loss: 1.4610
Profiling... [640/50048]	Loss: 1.2987
Profiling... [768/50048]	Loss: 1.4635
Profiling... [896/50048]	Loss: 1.5123
Profiling... [1024/50048]	Loss: 1.2423
Profiling... [1152/50048]	Loss: 1.5108
Profiling... [1280/50048]	Loss: 1.3269
Profiling... [1408/50048]	Loss: 1.4040
Profiling... [1536/50048]	Loss: 1.3339
Profiling... [1664/50048]	Loss: 1.3762
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 11, Average loss: 0.0125, Accuracy: 0.5587
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.68749455560408,
                        "time": 2.5618087240000023,
                        "accuracy": 0.5587420886075949,
                        "total_cost": 530.42224463314
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4028
Profiling... [256/50048]	Loss: 1.4577
Profiling... [384/50048]	Loss: 1.5405
Profiling... [512/50048]	Loss: 1.4813
Profiling... [640/50048]	Loss: 1.6008
Profiling... [768/50048]	Loss: 1.5424
Profiling... [896/50048]	Loss: 1.3991
Profiling... [1024/50048]	Loss: 1.2311
Profiling... [1152/50048]	Loss: 1.2193
Profiling... [1280/50048]	Loss: 1.3230
Profiling... [1408/50048]	Loss: 1.4272
Profiling... [1536/50048]	Loss: 1.3337
Profiling... [1664/50048]	Loss: 1.1956
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 11, Average loss: 0.0125, Accuracy: 0.5578
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.66765006763129,
                        "time": 2.172783835994778,
                        "accuracy": 0.557753164556962,
                        "total_cost": 450.59502372179327
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.2594
Profiling... [256/50048]	Loss: 1.5514
Profiling... [384/50048]	Loss: 1.4678
Profiling... [512/50048]	Loss: 1.4817
Profiling... [640/50048]	Loss: 1.3952
Profiling... [768/50048]	Loss: 1.4014
Profiling... [896/50048]	Loss: 1.1242
Profiling... [1024/50048]	Loss: 1.4462
Profiling... [1152/50048]	Loss: 1.5371
Profiling... [1280/50048]	Loss: 1.2894
Profiling... [1408/50048]	Loss: 1.3256
Profiling... [1536/50048]	Loss: 1.5035
Profiling... [1664/50048]	Loss: 1.5418
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 11, Average loss: 0.0124, Accuracy: 0.5594
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.66985969197513,
                        "time": 2.191680374999123,
                        "accuracy": 0.559434335443038,
                        "total_cost": 453.1566001665565
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.2941
Profiling... [256/50048]	Loss: 1.4339
Profiling... [384/50048]	Loss: 1.3482
Profiling... [512/50048]	Loss: 1.5356
Profiling... [640/50048]	Loss: 1.3651
Profiling... [768/50048]	Loss: 1.2771
Profiling... [896/50048]	Loss: 1.2807
Profiling... [1024/50048]	Loss: 1.5247
Profiling... [1152/50048]	Loss: 1.3481
Profiling... [1280/50048]	Loss: 1.3604
Profiling... [1408/50048]	Loss: 1.2239
Profiling... [1536/50048]	Loss: 1.4592
Profiling... [1664/50048]	Loss: 1.6036
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 11, Average loss: 0.0124, Accuracy: 0.5564
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.67217603351055,
                        "time": 2.2413713130008546,
                        "accuracy": 0.556368670886076,
                        "total_cost": 465.99370281038597
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.3035
Profiling... [256/50048]	Loss: 1.3925
Profiling... [384/50048]	Loss: 1.3263
Profiling... [512/50048]	Loss: 1.4966
Profiling... [640/50048]	Loss: 1.6915
Profiling... [768/50048]	Loss: 1.4663
Profiling... [896/50048]	Loss: 1.2285
Profiling... [1024/50048]	Loss: 1.2605
Profiling... [1152/50048]	Loss: 1.3110
Profiling... [1280/50048]	Loss: 1.3967
Profiling... [1408/50048]	Loss: 1.2822
Profiling... [1536/50048]	Loss: 1.3901
Profiling... [1664/50048]	Loss: 1.4011
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 11, Average loss: 0.0125, Accuracy: 0.5593
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.65742023555839,
                        "time": 2.5793093210013467,
                        "accuracy": 0.5593354430379747,
                        "total_cost": 533.3405307489019
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5058
Profiling... [256/50048]	Loss: 1.4191
Profiling... [384/50048]	Loss: 1.4239
Profiling... [512/50048]	Loss: 1.5357
Profiling... [640/50048]	Loss: 1.2720
Profiling... [768/50048]	Loss: 1.3782
Profiling... [896/50048]	Loss: 1.3917
Profiling... [1024/50048]	Loss: 1.6019
Profiling... [1152/50048]	Loss: 1.4726
Profiling... [1280/50048]	Loss: 1.3111
Profiling... [1408/50048]	Loss: 1.4105
Profiling... [1536/50048]	Loss: 1.5507
Profiling... [1664/50048]	Loss: 1.4567
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 11, Average loss: 0.0153, Accuracy: 0.4938
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.6379206896548,
                        "time": 2.1715514840034302,
                        "accuracy": 0.49376977848101267,
                        "total_cost": 508.5643334697268
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4809
Profiling... [256/50048]	Loss: 1.3447
Profiling... [384/50048]	Loss: 1.3467
Profiling... [512/50048]	Loss: 1.3898
Profiling... [640/50048]	Loss: 1.3423
Profiling... [768/50048]	Loss: 1.3850
Profiling... [896/50048]	Loss: 1.5186
Profiling... [1024/50048]	Loss: 1.4363
Profiling... [1152/50048]	Loss: 1.4907
Profiling... [1280/50048]	Loss: 1.3790
Profiling... [1408/50048]	Loss: 1.4381
Profiling... [1536/50048]	Loss: 1.3177
Profiling... [1664/50048]	Loss: 1.3668
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 11, Average loss: 0.0187, Accuracy: 0.4293
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.64050571979918,
                        "time": 2.1907967969964375,
                        "accuracy": 0.42929193037974683,
                        "total_cost": 590.1458462307414
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5206
Profiling... [256/50048]	Loss: 1.2949
Profiling... [384/50048]	Loss: 1.4772
Profiling... [512/50048]	Loss: 1.3891
Profiling... [640/50048]	Loss: 1.4058
Profiling... [768/50048]	Loss: 1.3757
Profiling... [896/50048]	Loss: 1.1900
Profiling... [1024/50048]	Loss: 1.4800
Profiling... [1152/50048]	Loss: 1.3545
Profiling... [1280/50048]	Loss: 1.5457
Profiling... [1408/50048]	Loss: 1.3761
Profiling... [1536/50048]	Loss: 1.7203
Profiling... [1664/50048]	Loss: 1.5497
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 11, Average loss: 0.0161, Accuracy: 0.4558
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.64274480064734,
                        "time": 2.2191103040022426,
                        "accuracy": 0.45579509493670883,
                        "total_cost": 563.0249412970378
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.3661
Profiling... [256/50048]	Loss: 1.4924
Profiling... [384/50048]	Loss: 1.3547
Profiling... [512/50048]	Loss: 1.4149
Profiling... [640/50048]	Loss: 1.2584
Profiling... [768/50048]	Loss: 1.5675
Profiling... [896/50048]	Loss: 1.4063
Profiling... [1024/50048]	Loss: 1.5369
Profiling... [1152/50048]	Loss: 1.7396
Profiling... [1280/50048]	Loss: 1.2112
Profiling... [1408/50048]	Loss: 1.5608
Profiling... [1536/50048]	Loss: 1.5007
Profiling... [1664/50048]	Loss: 1.7695
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 11, Average loss: 0.0227, Accuracy: 0.3561
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.62746322707638,
                        "time": 2.5894827869997243,
                        "accuracy": 0.3561115506329114,
                        "total_cost": 840.7908286007904
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.3974
Profiling... [256/50048]	Loss: 1.6239
Profiling... [384/50048]	Loss: 1.3976
Profiling... [512/50048]	Loss: 1.3344
Profiling... [640/50048]	Loss: 1.4363
Profiling... [768/50048]	Loss: 1.5524
Profiling... [896/50048]	Loss: 1.4392
Profiling... [1024/50048]	Loss: 1.4614
Profiling... [1152/50048]	Loss: 1.4303
Profiling... [1280/50048]	Loss: 1.4746
Profiling... [1408/50048]	Loss: 1.3651
Profiling... [1536/50048]	Loss: 1.7521
Profiling... [1664/50048]	Loss: 1.4011
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 11, Average loss: 0.0159, Accuracy: 0.4751
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.60678999540104,
                        "time": 2.1688358890023665,
                        "accuracy": 0.4750791139240506,
                        "total_cost": 527.7692658037357
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5428
Profiling... [256/50048]	Loss: 1.5175
Profiling... [384/50048]	Loss: 1.5206
Profiling... [512/50048]	Loss: 1.3714
Profiling... [640/50048]	Loss: 1.4230
Profiling... [768/50048]	Loss: 1.4515
Profiling... [896/50048]	Loss: 1.3578
Profiling... [1024/50048]	Loss: 1.5671
Profiling... [1152/50048]	Loss: 1.5430
Profiling... [1280/50048]	Loss: 1.5931
Profiling... [1408/50048]	Loss: 1.3447
Profiling... [1536/50048]	Loss: 1.5186
Profiling... [1664/50048]	Loss: 1.7471
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 11, Average loss: 0.0167, Accuracy: 0.4598
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.61093296766937,
                        "time": 2.177259324998886,
                        "accuracy": 0.4597507911392405,
                        "total_cost": 547.5030967362592
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.3357
Profiling... [256/50048]	Loss: 1.2261
Profiling... [384/50048]	Loss: 1.2745
Profiling... [512/50048]	Loss: 1.2596
Profiling... [640/50048]	Loss: 1.3526
Profiling... [768/50048]	Loss: 1.5739
Profiling... [896/50048]	Loss: 1.8797
Profiling... [1024/50048]	Loss: 1.6487
Profiling... [1152/50048]	Loss: 1.4593
Profiling... [1280/50048]	Loss: 1.3231
Profiling... [1408/50048]	Loss: 1.3392
Profiling... [1536/50048]	Loss: 1.5522
Profiling... [1664/50048]	Loss: 1.4557
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 11, Average loss: 0.0185, Accuracy: 0.4419
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.61123907477466,
                        "time": 2.2245065399984014,
                        "accuracy": 0.44185126582278483,
                        "total_cost": 582.0464425744169
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7674
Profiling... [256/50048]	Loss: 1.2659
Profiling... [384/50048]	Loss: 1.3622
Profiling... [512/50048]	Loss: 1.4564
Profiling... [640/50048]	Loss: 1.3908
Profiling... [768/50048]	Loss: 1.4916
Profiling... [896/50048]	Loss: 1.3641
Profiling... [1024/50048]	Loss: 1.3462
Profiling... [1152/50048]	Loss: 1.6691
Profiling... [1280/50048]	Loss: 1.6980
Profiling... [1408/50048]	Loss: 1.5336
Profiling... [1536/50048]	Loss: 1.4032
Profiling... [1664/50048]	Loss: 1.4923
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 11, Average loss: 0.0162, Accuracy: 0.4683
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.59665691868686,
                        "time": 2.5848805630012066,
                        "accuracy": 0.46825553797468356,
                        "total_cost": 638.1206998841465
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.2908
Profiling... [256/50048]	Loss: 1.5481
Profiling... [384/50048]	Loss: 1.2751
Profiling... [512/50048]	Loss: 1.4291
Profiling... [640/50048]	Loss: 1.3145
Profiling... [768/50048]	Loss: 1.6453
Profiling... [896/50048]	Loss: 1.4535
Profiling... [1024/50048]	Loss: 1.5009
Profiling... [1152/50048]	Loss: 1.4705
Profiling... [1280/50048]	Loss: 1.6527
Profiling... [1408/50048]	Loss: 1.5701
Profiling... [1536/50048]	Loss: 1.3849
Profiling... [1664/50048]	Loss: 1.2980
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 11, Average loss: 0.0151, Accuracy: 0.4851
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.57499316700282,
                        "time": 2.2792854299987084,
                        "accuracy": 0.48506724683544306,
                        "total_cost": 543.0760368100399
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.5784
Profiling... [256/50048]	Loss: 1.4757
Profiling... [384/50048]	Loss: 1.5171
Profiling... [512/50048]	Loss: 1.4334
Profiling... [640/50048]	Loss: 1.6042
Profiling... [768/50048]	Loss: 1.4850
Profiling... [896/50048]	Loss: 1.2809
Profiling... [1024/50048]	Loss: 1.4924
Profiling... [1152/50048]	Loss: 1.4358
Profiling... [1280/50048]	Loss: 1.3092
Profiling... [1408/50048]	Loss: 1.4314
Profiling... [1536/50048]	Loss: 1.3443
Profiling... [1664/50048]	Loss: 1.3788
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 11, Average loss: 0.0168, Accuracy: 0.4568
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.57674754165278,
                        "time": 2.172954698005924,
                        "accuracy": 0.4567840189873418,
                        "total_cost": 549.8069680888697
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4093
Profiling... [256/50048]	Loss: 1.4942
Profiling... [384/50048]	Loss: 1.3752
Profiling... [512/50048]	Loss: 1.3526
Profiling... [640/50048]	Loss: 1.1820
Profiling... [768/50048]	Loss: 1.4254
Profiling... [896/50048]	Loss: 1.4913
Profiling... [1024/50048]	Loss: 1.3587
Profiling... [1152/50048]	Loss: 1.3455
Profiling... [1280/50048]	Loss: 1.5734
Profiling... [1408/50048]	Loss: 1.3685
Profiling... [1536/50048]	Loss: 1.2805
Profiling... [1664/50048]	Loss: 1.5881
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 11, Average loss: 0.0178, Accuracy: 0.4411
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.57691271668726,
                        "time": 2.338812540998333,
                        "accuracy": 0.4410601265822785,
                        "total_cost": 612.8704832293015
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.4185
Profiling... [256/50048]	Loss: 1.3465
Profiling... [384/50048]	Loss: 1.5042
Profiling... [512/50048]	Loss: 1.3630
Profiling... [640/50048]	Loss: 1.3826
Profiling... [768/50048]	Loss: 1.3889
Profiling... [896/50048]	Loss: 1.4905
Profiling... [1024/50048]	Loss: 1.4724
Profiling... [1152/50048]	Loss: 1.3818
Profiling... [1280/50048]	Loss: 1.5196
Profiling... [1408/50048]	Loss: 1.2323
Profiling... [1536/50048]	Loss: 1.3121
Profiling... [1664/50048]	Loss: 1.5770
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 11, Average loss: 0.0148, Accuracy: 0.4986
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.56345009674601,
                        "time": 2.562965482000436,
                        "accuracy": 0.49861550632911394,
                        "total_cost": 594.0150874155554
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.3724
Profiling... [256/50048]	Loss: 1.4650
Profiling... [384/50048]	Loss: 1.6626
Profiling... [512/50048]	Loss: 1.3999
Profiling... [640/50048]	Loss: 1.6233
Profiling... [768/50048]	Loss: 1.5964
Profiling... [896/50048]	Loss: 1.6346
Profiling... [1024/50048]	Loss: 1.6054
Profiling... [1152/50048]	Loss: 1.8688
Profiling... [1280/50048]	Loss: 1.6234
Profiling... [1408/50048]	Loss: 1.6168
Profiling... [1536/50048]	Loss: 1.8805
Profiling... [1664/50048]	Loss: 1.6812
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 11, Average loss: 0.0299, Accuracy: 0.2594
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.54327165092077,
                        "time": 2.166474311998172,
                        "accuracy": 0.25939477848101267,
                        "total_cost": 965.0214681336373
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4233
Profiling... [256/50048]	Loss: 1.2136
Profiling... [384/50048]	Loss: 1.8809
Profiling... [512/50048]	Loss: 1.5024
Profiling... [640/50048]	Loss: 1.6222
Profiling... [768/50048]	Loss: 1.4914
Profiling... [896/50048]	Loss: 1.7872
Profiling... [1024/50048]	Loss: 1.8515
Profiling... [1152/50048]	Loss: 1.8179
Profiling... [1280/50048]	Loss: 1.7088
Profiling... [1408/50048]	Loss: 1.5483
Profiling... [1536/50048]	Loss: 1.7455
Profiling... [1664/50048]	Loss: 1.9457
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 11, Average loss: 0.0349, Accuracy: 0.2172
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.54804445802964,
                        "time": 2.179984107002383,
                        "accuracy": 0.21716772151898733,
                        "total_cost": 1159.9002777753321
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4800
Profiling... [256/50048]	Loss: 1.7266
Profiling... [384/50048]	Loss: 1.4400
Profiling... [512/50048]	Loss: 1.8448
Profiling... [640/50048]	Loss: 1.6449
Profiling... [768/50048]	Loss: 1.5427
Profiling... [896/50048]	Loss: 1.6288
Profiling... [1024/50048]	Loss: 1.5283
Profiling... [1152/50048]	Loss: 1.5823
Profiling... [1280/50048]	Loss: 1.8411
Profiling... [1408/50048]	Loss: 1.6403
Profiling... [1536/50048]	Loss: 1.6895
Profiling... [1664/50048]	Loss: 1.5982
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 11, Average loss: 0.0535, Accuracy: 0.1619
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.54895173437806,
                        "time": 2.226137292003841,
                        "accuracy": 0.16188686708860758,
                        "total_cost": 1588.9357496001153
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6320
Profiling... [256/50048]	Loss: 1.5312
Profiling... [384/50048]	Loss: 1.4516
Profiling... [512/50048]	Loss: 1.6163
Profiling... [640/50048]	Loss: 1.7570
Profiling... [768/50048]	Loss: 1.6214
Profiling... [896/50048]	Loss: 1.9329
Profiling... [1024/50048]	Loss: 1.6948
Profiling... [1152/50048]	Loss: 1.7733
Profiling... [1280/50048]	Loss: 1.7749
Profiling... [1408/50048]	Loss: 1.8551
Profiling... [1536/50048]	Loss: 1.4342
Profiling... [1664/50048]	Loss: 1.7422
Profile done
epoch 1 train time consumed: 4.01s
Validation Epoch: 11, Average loss: 0.0431, Accuracy: 0.1379
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.5355091637771,
                        "time": 2.5812744249997195,
                        "accuracy": 0.13785601265822786,
                        "total_cost": 2163.3358548034207
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.3224
Profiling... [256/50048]	Loss: 1.6283
Profiling... [384/50048]	Loss: 1.5582
Profiling... [512/50048]	Loss: 1.5814
Profiling... [640/50048]	Loss: 1.6420
Profiling... [768/50048]	Loss: 1.5484
Profiling... [896/50048]	Loss: 1.6720
Profiling... [1024/50048]	Loss: 1.7056
Profiling... [1152/50048]	Loss: 1.5324
Profiling... [1280/50048]	Loss: 1.8944
Profiling... [1408/50048]	Loss: 1.6721
Profiling... [1536/50048]	Loss: 1.7817
Profiling... [1664/50048]	Loss: 1.9162
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 11, Average loss: 0.0340, Accuracy: 0.2565
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.51487871073827,
                        "time": 2.175387111004966,
                        "accuracy": 0.2565268987341772,
                        "total_cost": 979.5837376767169
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4506
Profiling... [256/50048]	Loss: 1.3788
Profiling... [384/50048]	Loss: 1.4204
Profiling... [512/50048]	Loss: 1.6688
Profiling... [640/50048]	Loss: 1.4070
Profiling... [768/50048]	Loss: 1.8545
Profiling... [896/50048]	Loss: 1.9085
Profiling... [1024/50048]	Loss: 1.7720
Profiling... [1152/50048]	Loss: 1.7797
Profiling... [1280/50048]	Loss: 1.6424
Profiling... [1408/50048]	Loss: 1.7970
Profiling... [1536/50048]	Loss: 1.6379
Profiling... [1664/50048]	Loss: 1.5359
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 11, Average loss: 0.0329, Accuracy: 0.2491
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.51760318848306,
                        "time": 2.1705626330003724,
                        "accuracy": 0.2491099683544304,
                        "total_cost": 1006.5361679061314
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4037
Profiling... [256/50048]	Loss: 1.4820
Profiling... [384/50048]	Loss: 1.3676
Profiling... [512/50048]	Loss: 1.6183
Profiling... [640/50048]	Loss: 1.7324
Profiling... [768/50048]	Loss: 1.3241
Profiling... [896/50048]	Loss: 1.6643
Profiling... [1024/50048]	Loss: 1.6931
Profiling... [1152/50048]	Loss: 1.6277
Profiling... [1280/50048]	Loss: 1.7826
Profiling... [1408/50048]	Loss: 1.6138
Profiling... [1536/50048]	Loss: 2.0410
Profiling... [1664/50048]	Loss: 1.5978
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 11, Average loss: 0.0346, Accuracy: 0.2516
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.51912752500473,
                        "time": 2.2203297290034243,
                        "accuracy": 0.25158227848101267,
                        "total_cost": 1019.5096199180957
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5439
Profiling... [256/50048]	Loss: 1.5124
Profiling... [384/50048]	Loss: 1.5641
Profiling... [512/50048]	Loss: 1.4400
Profiling... [640/50048]	Loss: 1.3850
Profiling... [768/50048]	Loss: 1.8277
Profiling... [896/50048]	Loss: 1.6514
Profiling... [1024/50048]	Loss: 2.0000
Profiling... [1152/50048]	Loss: 1.6815
Profiling... [1280/50048]	Loss: 1.7002
Profiling... [1408/50048]	Loss: 1.5587
Profiling... [1536/50048]	Loss: 1.7095
Profiling... [1664/50048]	Loss: 1.7322
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 11, Average loss: 0.0941, Accuracy: 0.0486
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.50534957410194,
                        "time": 2.5580690749993664,
                        "accuracy": 0.04855617088607595,
                        "total_cost": 6085.1310420614545
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.4579
Profiling... [256/50048]	Loss: 1.2548
Profiling... [384/50048]	Loss: 1.4596
Profiling... [512/50048]	Loss: 1.5815
Profiling... [640/50048]	Loss: 1.8359
Profiling... [768/50048]	Loss: 1.8119
Profiling... [896/50048]	Loss: 1.7917
Profiling... [1024/50048]	Loss: 1.6243
Profiling... [1152/50048]	Loss: 1.5479
Profiling... [1280/50048]	Loss: 1.7069
Profiling... [1408/50048]	Loss: 1.7441
Profiling... [1536/50048]	Loss: 1.7772
Profiling... [1664/50048]	Loss: 1.7163
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 11, Average loss: 0.0330, Accuracy: 0.2476
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.48645720524475,
                        "time": 2.1689610379980877,
                        "accuracy": 0.247626582278481,
                        "total_cost": 1011.5457871680073
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.4684
Profiling... [256/50048]	Loss: 1.4122
Profiling... [384/50048]	Loss: 1.5532
Profiling... [512/50048]	Loss: 1.7471
Profiling... [640/50048]	Loss: 1.9224
Profiling... [768/50048]	Loss: 1.6982
Profiling... [896/50048]	Loss: 1.6394
Profiling... [1024/50048]	Loss: 1.8302
Profiling... [1152/50048]	Loss: 1.6543
Profiling... [1280/50048]	Loss: 1.4688
Profiling... [1408/50048]	Loss: 1.6331
Profiling... [1536/50048]	Loss: 1.5714
Profiling... [1664/50048]	Loss: 1.8380
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 11, Average loss: 0.0387, Accuracy: 0.1754
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.48925543347893,
                        "time": 2.1771881280001253,
                        "accuracy": 0.17543512658227847,
                        "total_cost": 1433.246811740515
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.4811
Profiling... [256/50048]	Loss: 1.4985
Profiling... [384/50048]	Loss: 1.3525
Profiling... [512/50048]	Loss: 1.4899
Profiling... [640/50048]	Loss: 1.8152
Profiling... [768/50048]	Loss: 1.7091
Profiling... [896/50048]	Loss: 1.5700
Profiling... [1024/50048]	Loss: 1.7139
Profiling... [1152/50048]	Loss: 1.5755
Profiling... [1280/50048]	Loss: 1.4372
Profiling... [1408/50048]	Loss: 1.7734
Profiling... [1536/50048]	Loss: 1.4158
Profiling... [1664/50048]	Loss: 1.4330
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 11, Average loss: 0.0264, Accuracy: 0.2962
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.49036320798385,
                        "time": 2.233107738997205,
                        "accuracy": 0.29618275316455694,
                        "total_cost": 870.7543606229433
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.4371
Profiling... [256/50048]	Loss: 1.6518
Profiling... [384/50048]	Loss: 1.4335
Profiling... [512/50048]	Loss: 1.8454
Profiling... [640/50048]	Loss: 1.8339
Profiling... [768/50048]	Loss: 1.6303
Profiling... [896/50048]	Loss: 1.8294
Profiling... [1024/50048]	Loss: 1.6359
Profiling... [1152/50048]	Loss: 1.4614
Profiling... [1280/50048]	Loss: 1.3448
Profiling... [1408/50048]	Loss: 1.9550
Profiling... [1536/50048]	Loss: 1.6252
Profiling... [1664/50048]	Loss: 1.4945
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 11, Average loss: 0.0323, Accuracy: 0.2305
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.47648225746497,
                        "time": 2.5613010479937657,
                        "accuracy": 0.23051819620253164,
                        "total_cost": 1283.0658919646287
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.3179
Profiling... [512/50176]	Loss: 1.4356
Profiling... [768/50176]	Loss: 1.5387
Profiling... [1024/50176]	Loss: 1.4621
Profiling... [1280/50176]	Loss: 1.3734
Profiling... [1536/50176]	Loss: 1.3375
Profiling... [1792/50176]	Loss: 1.4564
Profiling... [2048/50176]	Loss: 1.2844
Profiling... [2304/50176]	Loss: 1.5077
Profiling... [2560/50176]	Loss: 1.2702
Profiling... [2816/50176]	Loss: 1.4169
Profiling... [3072/50176]	Loss: 1.3615
Profiling... [3328/50176]	Loss: 1.3323
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 11, Average loss: 0.0063, Accuracy: 0.5528
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.46281580784064,
                        "time": 2.402376763995562,
                        "accuracy": 0.55283203125,
                        "total_cost": 501.7531006209326
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.3016
Profiling... [512/50176]	Loss: 1.4504
Profiling... [768/50176]	Loss: 1.3173
Profiling... [1024/50176]	Loss: 1.3236
Profiling... [1280/50176]	Loss: 1.2937
Profiling... [1536/50176]	Loss: 1.4303
Profiling... [1792/50176]	Loss: 1.3120
Profiling... [2048/50176]	Loss: 1.1942
Profiling... [2304/50176]	Loss: 1.4862
Profiling... [2560/50176]	Loss: 1.2745
Profiling... [2816/50176]	Loss: 1.5526
Profiling... [3072/50176]	Loss: 1.5233
Profiling... [3328/50176]	Loss: 1.2722
Profile done
epoch 1 train time consumed: 4.04s
Validation Epoch: 11, Average loss: 0.0061, Accuracy: 0.5563
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.47277999816836,
                        "time": 2.397809269998106,
                        "accuracy": 0.55634765625,
                        "total_cost": 497.6774849351402
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.2976
Profiling... [512/50176]	Loss: 1.2467
Profiling... [768/50176]	Loss: 1.3216
Profiling... [1024/50176]	Loss: 1.2542
Profiling... [1280/50176]	Loss: 1.5231
Profiling... [1536/50176]	Loss: 1.5102
Profiling... [1792/50176]	Loss: 1.3842
Profiling... [2048/50176]	Loss: 1.3851
Profiling... [2304/50176]	Loss: 1.2587
Profiling... [2560/50176]	Loss: 1.2384
Profiling... [2816/50176]	Loss: 1.3313
Profiling... [3072/50176]	Loss: 1.3713
Profiling... [3328/50176]	Loss: 1.3051
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 11, Average loss: 0.0061, Accuracy: 0.5575
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.47453892504366,
                        "time": 2.5357394169986947,
                        "accuracy": 0.55751953125,
                        "total_cost": 525.2073220743928
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.3593
Profiling... [512/50176]	Loss: 1.4571
Profiling... [768/50176]	Loss: 1.3899
Profiling... [1024/50176]	Loss: 1.4007
Profiling... [1280/50176]	Loss: 1.5140
Profiling... [1536/50176]	Loss: 1.4059
Profiling... [1792/50176]	Loss: 1.3249
Profiling... [2048/50176]	Loss: 1.3972
Profiling... [2304/50176]	Loss: 1.4565
Profiling... [2560/50176]	Loss: 1.3649
Profiling... [2816/50176]	Loss: 1.2188
Profiling... [3072/50176]	Loss: 1.4365
Profiling... [3328/50176]	Loss: 1.2484
Profile done
epoch 1 train time consumed: 4.58s
Validation Epoch: 11, Average loss: 0.0062, Accuracy: 0.5541
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.45812981288198,
                        "time": 3.0125433579960372,
                        "accuracy": 0.5541015625,
                        "total_cost": 627.7235901034693
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.2819
Profiling... [512/50176]	Loss: 1.5466
Profiling... [768/50176]	Loss: 1.4045
Profiling... [1024/50176]	Loss: 1.4280
Profiling... [1280/50176]	Loss: 1.3841
Profiling... [1536/50176]	Loss: 1.3407
Profiling... [1792/50176]	Loss: 1.4522
Profiling... [2048/50176]	Loss: 1.3849
Profiling... [2304/50176]	Loss: 1.5482
Profiling... [2560/50176]	Loss: 1.4555
Profiling... [2816/50176]	Loss: 1.2179
Profiling... [3072/50176]	Loss: 1.3945
Profiling... [3328/50176]	Loss: 1.2538
Profile done
epoch 1 train time consumed: 4.06s
Validation Epoch: 11, Average loss: 0.0063, Accuracy: 0.5472
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.44153868769469,
                        "time": 2.4262382129963953,
                        "accuracy": 0.54716796875,
                        "total_cost": 511.8879183864628
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5129
Profiling... [512/50176]	Loss: 1.2261
Profiling... [768/50176]	Loss: 1.4775
Profiling... [1024/50176]	Loss: 1.3636
Profiling... [1280/50176]	Loss: 1.3808
Profiling... [1536/50176]	Loss: 1.3701
Profiling... [1792/50176]	Loss: 1.3330
Profiling... [2048/50176]	Loss: 1.4457
Profiling... [2304/50176]	Loss: 1.3026
Profiling... [2560/50176]	Loss: 1.3113
Profiling... [2816/50176]	Loss: 1.3103
Profiling... [3072/50176]	Loss: 1.3542
Profiling... [3328/50176]	Loss: 1.4342
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 11, Average loss: 0.0062, Accuracy: 0.5513
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.45091206637143,
                        "time": 2.5057735800000955,
                        "accuracy": 0.55126953125,
                        "total_cost": 524.7774978364139
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.4147
Profiling... [512/50176]	Loss: 1.4721
Profiling... [768/50176]	Loss: 1.3248
Profiling... [1024/50176]	Loss: 1.3294
Profiling... [1280/50176]	Loss: 1.3870
Profiling... [1536/50176]	Loss: 1.3431
Profiling... [1792/50176]	Loss: 1.4057
Profiling... [2048/50176]	Loss: 1.4243
Profiling... [2304/50176]	Loss: 1.4223
Profiling... [2560/50176]	Loss: 1.3782
Profiling... [2816/50176]	Loss: 1.4993
Profiling... [3072/50176]	Loss: 1.2667
Profiling... [3328/50176]	Loss: 1.3660
Profile done
epoch 1 train time consumed: 4.06s
Validation Epoch: 11, Average loss: 0.0062, Accuracy: 0.5509
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.4536595396493,
                        "time": 2.538439418996859,
                        "accuracy": 0.55087890625,
                        "total_cost": 532.0082455832614
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.4143
Profiling... [512/50176]	Loss: 1.4531
Profiling... [768/50176]	Loss: 1.3453
Profiling... [1024/50176]	Loss: 1.4146
Profiling... [1280/50176]	Loss: 1.3079
Profiling... [1536/50176]	Loss: 1.3980
Profiling... [1792/50176]	Loss: 1.4964
Profiling... [2048/50176]	Loss: 1.2830
Profiling... [2304/50176]	Loss: 1.1165
Profiling... [2560/50176]	Loss: 1.3303
Profiling... [2816/50176]	Loss: 1.1930
Profiling... [3072/50176]	Loss: 1.1875
Profiling... [3328/50176]	Loss: 1.4405
Profile done
epoch 1 train time consumed: 4.56s
Validation Epoch: 11, Average loss: 0.0061, Accuracy: 0.5532
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.43605963494939,
                        "time": 2.993683858003351,
                        "accuracy": 0.55322265625,
                        "total_cost": 624.6654298346267
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5498
Profiling... [512/50176]	Loss: 1.4188
Profiling... [768/50176]	Loss: 1.4308
Profiling... [1024/50176]	Loss: 1.2881
Profiling... [1280/50176]	Loss: 1.4850
Profiling... [1536/50176]	Loss: 1.5408
Profiling... [1792/50176]	Loss: 1.4113
Profiling... [2048/50176]	Loss: 1.4021
Profiling... [2304/50176]	Loss: 1.4972
Profiling... [2560/50176]	Loss: 1.6453
Profiling... [2816/50176]	Loss: 1.4114
Profiling... [3072/50176]	Loss: 1.3471
Profiling... [3328/50176]	Loss: 1.2879
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 11, Average loss: 0.0062, Accuracy: 0.5512
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.42129454430096,
                        "time": 2.394043104002776,
                        "accuracy": 0.551171875,
                        "total_cost": 501.3382699522848
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4426
Profiling... [512/50176]	Loss: 1.4101
Profiling... [768/50176]	Loss: 1.3731
Profiling... [1024/50176]	Loss: 1.3212
Profiling... [1280/50176]	Loss: 1.3294
Profiling... [1536/50176]	Loss: 1.2904
Profiling... [1792/50176]	Loss: 1.4025
Profiling... [2048/50176]	Loss: 1.4712
Profiling... [2304/50176]	Loss: 1.3610
Profiling... [2560/50176]	Loss: 1.3474
Profiling... [2816/50176]	Loss: 1.4680
Profiling... [3072/50176]	Loss: 1.3468
Profiling... [3328/50176]	Loss: 1.4299
Profile done
epoch 1 train time consumed: 4.04s
Validation Epoch: 11, Average loss: 0.0062, Accuracy: 0.5546
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.42959430223371,
                        "time": 2.4022607919978327,
                        "accuracy": 0.55458984375,
                        "total_cost": 499.99471096241564
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.3884
Profiling... [512/50176]	Loss: 1.4372
Profiling... [768/50176]	Loss: 1.4682
Profiling... [1024/50176]	Loss: 1.3883
Profiling... [1280/50176]	Loss: 1.3862
Profiling... [1536/50176]	Loss: 1.3594
Profiling... [1792/50176]	Loss: 1.5975
Profiling... [2048/50176]	Loss: 1.3403
Profiling... [2304/50176]	Loss: 1.3155
Profiling... [2560/50176]	Loss: 1.4104
Profiling... [2816/50176]	Loss: 1.3120
Profiling... [3072/50176]	Loss: 1.4164
Profiling... [3328/50176]	Loss: 1.2526
Profile done
epoch 1 train time consumed: 4.04s
Validation Epoch: 11, Average loss: 0.0061, Accuracy: 0.5570
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.43183758689469,
                        "time": 2.531834143002925,
                        "accuracy": 0.55703125,
                        "total_cost": 524.6640428020302
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.4558
Profiling... [512/50176]	Loss: 1.2447
Profiling... [768/50176]	Loss: 1.4176
Profiling... [1024/50176]	Loss: 1.2453
Profiling... [1280/50176]	Loss: 1.5882
Profiling... [1536/50176]	Loss: 1.2218
Profiling... [1792/50176]	Loss: 1.3152
Profiling... [2048/50176]	Loss: 1.3875
Profiling... [2304/50176]	Loss: 1.3085
Profiling... [2560/50176]	Loss: 1.5226
Profiling... [2816/50176]	Loss: 1.1699
Profiling... [3072/50176]	Loss: 1.3190
Profiling... [3328/50176]	Loss: 1.2356
Profile done
epoch 1 train time consumed: 4.54s
Validation Epoch: 11, Average loss: 0.0062, Accuracy: 0.5548
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.41575807751539,
                        "time": 3.008003273003851,
                        "accuracy": 0.55478515625,
                        "total_cost": 625.7755351639996
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.1937
Profiling... [512/50176]	Loss: 1.2637
Profiling... [768/50176]	Loss: 1.4549
Profiling... [1024/50176]	Loss: 1.4084
Profiling... [1280/50176]	Loss: 1.4198
Profiling... [1536/50176]	Loss: 1.3656
Profiling... [1792/50176]	Loss: 1.4006
Profiling... [2048/50176]	Loss: 1.3449
Profiling... [2304/50176]	Loss: 1.5566
Profiling... [2560/50176]	Loss: 1.4286
Profiling... [2816/50176]	Loss: 1.4857
Profiling... [3072/50176]	Loss: 1.4174
Profiling... [3328/50176]	Loss: 1.4589
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 11, Average loss: 0.0077, Accuracy: 0.4833
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.40048482233533,
                        "time": 2.407371165994846,
                        "accuracy": 0.48330078125,
                        "total_cost": 574.8217476176818
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.2013
Profiling... [512/50176]	Loss: 1.4334
Profiling... [768/50176]	Loss: 1.6184
Profiling... [1024/50176]	Loss: 1.2437
Profiling... [1280/50176]	Loss: 1.4536
Profiling... [1536/50176]	Loss: 1.4022
Profiling... [1792/50176]	Loss: 1.6780
Profiling... [2048/50176]	Loss: 1.4010
Profiling... [2304/50176]	Loss: 1.3658
Profiling... [2560/50176]	Loss: 1.4635
Profiling... [2816/50176]	Loss: 1.5973
Profiling... [3072/50176]	Loss: 1.3040
Profiling... [3328/50176]	Loss: 1.4212
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 11, Average loss: 0.0091, Accuracy: 0.4295
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.40970829007684,
                        "time": 2.3970918839986552,
                        "accuracy": 0.4294921875,
                        "total_cost": 644.127374439833
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.3733
Profiling... [512/50176]	Loss: 1.3785
Profiling... [768/50176]	Loss: 1.3256
Profiling... [1024/50176]	Loss: 1.3391
Profiling... [1280/50176]	Loss: 1.2008
Profiling... [1536/50176]	Loss: 1.2691
Profiling... [1792/50176]	Loss: 1.6729
Profiling... [2048/50176]	Loss: 1.3687
Profiling... [2304/50176]	Loss: 1.2413
Profiling... [2560/50176]	Loss: 1.4244
Profiling... [2816/50176]	Loss: 1.2825
Profiling... [3072/50176]	Loss: 1.3586
Profiling... [3328/50176]	Loss: 1.3792
Profile done
epoch 1 train time consumed: 4.23s
Validation Epoch: 11, Average loss: 0.0087, Accuracy: 0.4385
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.4131232059659,
                        "time": 2.5391640559973894,
                        "accuracy": 0.4384765625,
                        "total_cost": 668.3432573091904
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.4751
Profiling... [512/50176]	Loss: 1.2779
Profiling... [768/50176]	Loss: 1.4165
Profiling... [1024/50176]	Loss: 1.3808
Profiling... [1280/50176]	Loss: 1.4631
Profiling... [1536/50176]	Loss: 1.5013
Profiling... [1792/50176]	Loss: 1.5612
Profiling... [2048/50176]	Loss: 1.5149
Profiling... [2304/50176]	Loss: 1.5170
Profiling... [2560/50176]	Loss: 1.4676
Profiling... [2816/50176]	Loss: 1.2155
Profiling... [3072/50176]	Loss: 1.5102
Profiling... [3328/50176]	Loss: 1.4020
Profile done
epoch 1 train time consumed: 4.68s
Validation Epoch: 11, Average loss: 0.0086, Accuracy: 0.4424
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.39570566097066,
                        "time": 3.020795584001462,
                        "accuracy": 0.4423828125,
                        "total_cost": 787.9755456670069
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.4084
Profiling... [512/50176]	Loss: 1.3687
Profiling... [768/50176]	Loss: 1.3288
Profiling... [1024/50176]	Loss: 1.5437
Profiling... [1280/50176]	Loss: 1.2432
Profiling... [1536/50176]	Loss: 1.3371
Profiling... [1792/50176]	Loss: 1.5172
Profiling... [2048/50176]	Loss: 1.6198
Profiling... [2304/50176]	Loss: 1.3863
Profiling... [2560/50176]	Loss: 1.4178
Profiling... [2816/50176]	Loss: 1.3716
Profiling... [3072/50176]	Loss: 1.2674
Profiling... [3328/50176]	Loss: 1.4391
Profile done
epoch 1 train time consumed: 4.01s
Validation Epoch: 11, Average loss: 0.0079, Accuracy: 0.4767
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.38018006095324,
                        "time": 2.4562257479992695,
                        "accuracy": 0.47666015625,
                        "total_cost": 594.5530906213755
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4409
Profiling... [512/50176]	Loss: 1.4526
Profiling... [768/50176]	Loss: 1.5858
Profiling... [1024/50176]	Loss: 1.3488
Profiling... [1280/50176]	Loss: 1.5748
Profiling... [1536/50176]	Loss: 1.4096
Profiling... [1792/50176]	Loss: 1.2629
Profiling... [2048/50176]	Loss: 1.3846
Profiling... [2304/50176]	Loss: 1.4763
Profiling... [2560/50176]	Loss: 1.2692
Profiling... [2816/50176]	Loss: 1.4662
Profiling... [3072/50176]	Loss: 1.3723
Profiling... [3328/50176]	Loss: 1.3247
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 11, Average loss: 0.0101, Accuracy: 0.3901
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.38949616670074,
                        "time": 2.3934891909957514,
                        "accuracy": 0.39013671875,
                        "total_cost": 707.9146836379238
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.3791
Profiling... [512/50176]	Loss: 1.4249
Profiling... [768/50176]	Loss: 1.5381
Profiling... [1024/50176]	Loss: 1.4795
Profiling... [1280/50176]	Loss: 1.3990
Profiling... [1536/50176]	Loss: 1.3664
Profiling... [1792/50176]	Loss: 1.4133
Profiling... [2048/50176]	Loss: 1.4588
Profiling... [2304/50176]	Loss: 1.4307
Profiling... [2560/50176]	Loss: 1.3929
Profiling... [2816/50176]	Loss: 1.3758
Profiling... [3072/50176]	Loss: 1.4701
Profiling... [3328/50176]	Loss: 1.3209
Profile done
epoch 1 train time consumed: 4.05s
Validation Epoch: 11, Average loss: 0.0078, Accuracy: 0.4682
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.39205573075158,
                        "time": 2.5404169620014727,
                        "accuracy": 0.4681640625,
                        "total_cost": 626.1564249362275
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.3859
Profiling... [512/50176]	Loss: 1.4601
Profiling... [768/50176]	Loss: 1.5323
Profiling... [1024/50176]	Loss: 1.4394
Profiling... [1280/50176]	Loss: 1.3281
Profiling... [1536/50176]	Loss: 1.2744
Profiling... [1792/50176]	Loss: 1.5188
Profiling... [2048/50176]	Loss: 1.4780
Profiling... [2304/50176]	Loss: 1.4966
Profiling... [2560/50176]	Loss: 1.3155
Profiling... [2816/50176]	Loss: 1.2818
Profiling... [3072/50176]	Loss: 1.6985
Profiling... [3328/50176]	Loss: 1.4566
Profile done
epoch 1 train time consumed: 4.71s
Validation Epoch: 11, Average loss: 0.0080, Accuracy: 0.4685
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.37537203109794,
                        "time": 3.0091176880014245,
                        "accuracy": 0.46845703125,
                        "total_cost": 741.1097487684932
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.3793
Profiling... [512/50176]	Loss: 1.4469
Profiling... [768/50176]	Loss: 1.2909
Profiling... [1024/50176]	Loss: 1.3522
Profiling... [1280/50176]	Loss: 1.5263
Profiling... [1536/50176]	Loss: 1.4660
Profiling... [1792/50176]	Loss: 1.3955
Profiling... [2048/50176]	Loss: 1.4453
Profiling... [2304/50176]	Loss: 1.4075
Profiling... [2560/50176]	Loss: 1.3620
Profiling... [2816/50176]	Loss: 1.6138
Profiling... [3072/50176]	Loss: 1.3296
Profiling... [3328/50176]	Loss: 1.4774
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 11, Average loss: 0.0072, Accuracy: 0.5037
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.35934164910752,
                        "time": 2.406445615000848,
                        "accuracy": 0.5037109375,
                        "total_cost": 551.1216080371087
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4578
Profiling... [512/50176]	Loss: 1.3582
Profiling... [768/50176]	Loss: 1.3579
Profiling... [1024/50176]	Loss: 1.3845
Profiling... [1280/50176]	Loss: 1.4762
Profiling... [1536/50176]	Loss: 1.2632
Profiling... [1792/50176]	Loss: 1.3167
Profiling... [2048/50176]	Loss: 1.3152
Profiling... [2304/50176]	Loss: 1.5459
Profiling... [2560/50176]	Loss: 1.4874
Profiling... [2816/50176]	Loss: 1.4513
Profiling... [3072/50176]	Loss: 1.2972
Profiling... [3328/50176]	Loss: 1.4566
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 11, Average loss: 0.0073, Accuracy: 0.5020
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.36854759472259,
                        "time": 2.3880351900006644,
                        "accuracy": 0.501953125,
                        "total_cost": 548.8643017721307
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6523
Profiling... [512/50176]	Loss: 1.2655
Profiling... [768/50176]	Loss: 1.3059
Profiling... [1024/50176]	Loss: 1.4947
Profiling... [1280/50176]	Loss: 1.5028
Profiling... [1536/50176]	Loss: 1.4210
Profiling... [1792/50176]	Loss: 1.4389
Profiling... [2048/50176]	Loss: 1.4607
Profiling... [2304/50176]	Loss: 1.5060
Profiling... [2560/50176]	Loss: 1.4164
Profiling... [2816/50176]	Loss: 1.3412
Profiling... [3072/50176]	Loss: 1.6369
Profiling... [3328/50176]	Loss: 1.4023
Profile done
epoch 1 train time consumed: 4.12s
Validation Epoch: 11, Average loss: 0.0082, Accuracy: 0.4535
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.3710642235415,
                        "time": 2.5206820980019984,
                        "accuracy": 0.453515625,
                        "total_cost": 641.2431241276855
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.4240
Profiling... [512/50176]	Loss: 1.3108
Profiling... [768/50176]	Loss: 1.3404
Profiling... [1024/50176]	Loss: 1.5398
Profiling... [1280/50176]	Loss: 1.5565
Profiling... [1536/50176]	Loss: 1.4943
Profiling... [1792/50176]	Loss: 1.4991
Profiling... [2048/50176]	Loss: 1.5390
Profiling... [2304/50176]	Loss: 1.3070
Profiling... [2560/50176]	Loss: 1.3588
Profiling... [2816/50176]	Loss: 1.3775
Profiling... [3072/50176]	Loss: 1.3815
Profiling... [3328/50176]	Loss: 1.4774
Profile done
epoch 1 train time consumed: 4.61s
Validation Epoch: 11, Average loss: 0.0100, Accuracy: 0.4058
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.35483636385861,
                        "time": 3.027255924003839,
                        "accuracy": 0.40576171875,
                        "total_cost": 860.624833758999
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.3574
Profiling... [512/50176]	Loss: 1.5017
Profiling... [768/50176]	Loss: 1.4539
Profiling... [1024/50176]	Loss: 1.6799
Profiling... [1280/50176]	Loss: 1.5687
Profiling... [1536/50176]	Loss: 1.4214
Profiling... [1792/50176]	Loss: 1.6287
Profiling... [2048/50176]	Loss: 1.5424
Profiling... [2304/50176]	Loss: 1.7967
Profiling... [2560/50176]	Loss: 1.4967
Profiling... [2816/50176]	Loss: 1.5578
Profiling... [3072/50176]	Loss: 1.3209
Profiling... [3328/50176]	Loss: 1.6210
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 11, Average loss: 0.0159, Accuracy: 0.2232
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.34014128087578,
                        "time": 2.410299348004628,
                        "accuracy": 0.2232421875,
                        "total_cost": 1245.3034546978563
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.2160
Profiling... [512/50176]	Loss: 1.5091
Profiling... [768/50176]	Loss: 1.5458
Profiling... [1024/50176]	Loss: 1.3900
Profiling... [1280/50176]	Loss: 1.5766
Profiling... [1536/50176]	Loss: 1.4682
Profiling... [1792/50176]	Loss: 1.6471
Profiling... [2048/50176]	Loss: 1.5567
Profiling... [2304/50176]	Loss: 1.7151
Profiling... [2560/50176]	Loss: 1.5289
Profiling... [2816/50176]	Loss: 1.5388
Profiling... [3072/50176]	Loss: 1.6754
Profiling... [3328/50176]	Loss: 1.5385
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 11, Average loss: 0.0237, Accuracy: 0.1309
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.34818375250782,
                        "time": 2.3897177600010764,
                        "accuracy": 0.130859375,
                        "total_cost": 2106.4566699729044
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.2995
Profiling... [512/50176]	Loss: 1.3708
Profiling... [768/50176]	Loss: 1.5019
Profiling... [1024/50176]	Loss: 1.8386
Profiling... [1280/50176]	Loss: 1.4039
Profiling... [1536/50176]	Loss: 1.4028
Profiling... [1792/50176]	Loss: 1.5314
Profiling... [2048/50176]	Loss: 1.4685
Profiling... [2304/50176]	Loss: 1.5297
Profiling... [2560/50176]	Loss: 1.6418
Profiling... [2816/50176]	Loss: 1.5694
Profiling... [3072/50176]	Loss: 1.7144
Profiling... [3328/50176]	Loss: 1.6731
Profile done
epoch 1 train time consumed: 4.13s
Validation Epoch: 11, Average loss: 0.0173, Accuracy: 0.1833
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.35193157102063,
                        "time": 2.542513346001215,
                        "accuracy": 0.18330078125,
                        "total_cost": 1600.0140507111932
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.2296
Profiling... [512/50176]	Loss: 1.4019
Profiling... [768/50176]	Loss: 1.4849
Profiling... [1024/50176]	Loss: 1.5719
Profiling... [1280/50176]	Loss: 1.7321
Profiling... [1536/50176]	Loss: 1.4879
Profiling... [1792/50176]	Loss: 1.5194
Profiling... [2048/50176]	Loss: 1.6885
Profiling... [2304/50176]	Loss: 1.5464
Profiling... [2560/50176]	Loss: 1.5748
Profiling... [2816/50176]	Loss: 1.5555
Profiling... [3072/50176]	Loss: 1.5216
Profiling... [3328/50176]	Loss: 1.3520
Profile done
epoch 1 train time consumed: 4.72s
Validation Epoch: 11, Average loss: 0.0165, Accuracy: 0.2213
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.3351869076653,
                        "time": 3.0370583859985345,
                        "accuracy": 0.2212890625,
                        "total_cost": 1582.9056015754654
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5562
Profiling... [512/50176]	Loss: 1.5000
Profiling... [768/50176]	Loss: 1.4372
Profiling... [1024/50176]	Loss: 1.6098
Profiling... [1280/50176]	Loss: 1.5735
Profiling... [1536/50176]	Loss: 1.5200
Profiling... [1792/50176]	Loss: 1.5809
Profiling... [2048/50176]	Loss: 1.4182
Profiling... [2304/50176]	Loss: 1.4987
Profiling... [2560/50176]	Loss: 1.5786
Profiling... [2816/50176]	Loss: 1.4547
Profiling... [3072/50176]	Loss: 1.5239
Profiling... [3328/50176]	Loss: 1.5154
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 11, Average loss: 0.0142, Accuracy: 0.2386
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.31926426733901,
                        "time": 2.3836035350032034,
                        "accuracy": 0.23857421875,
                        "total_cost": 1152.1588854059605
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.4534
Profiling... [512/50176]	Loss: 1.5575
Profiling... [768/50176]	Loss: 1.3689
Profiling... [1024/50176]	Loss: 1.6802
Profiling... [1280/50176]	Loss: 1.5052
Profiling... [1536/50176]	Loss: 1.6448
Profiling... [1792/50176]	Loss: 1.4697
Profiling... [2048/50176]	Loss: 1.6439
Profiling... [2304/50176]	Loss: 1.6389
Profiling... [2560/50176]	Loss: 1.5498
Profiling... [2816/50176]	Loss: 1.6779
Profiling... [3072/50176]	Loss: 1.4442
Profiling... [3328/50176]	Loss: 1.3906
Profile done
epoch 1 train time consumed: 4.13s
Validation Epoch: 11, Average loss: 0.0166, Accuracy: 0.2585
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.32715836976104,
                        "time": 2.513975482004753,
                        "accuracy": 0.25849609375,
                        "total_cost": 1121.601662697692
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.4880
Profiling... [512/50176]	Loss: 1.3917
Profiling... [768/50176]	Loss: 1.5090
Profiling... [1024/50176]	Loss: 1.4908
Profiling... [1280/50176]	Loss: 1.4516
Profiling... [1536/50176]	Loss: 1.6876
Profiling... [1792/50176]	Loss: 1.5716
Profiling... [2048/50176]	Loss: 1.5160
Profiling... [2304/50176]	Loss: 1.3268
Profiling... [2560/50176]	Loss: 1.6409
Profiling... [2816/50176]	Loss: 1.6000
Profiling... [3072/50176]	Loss: 1.6256
Profiling... [3328/50176]	Loss: 1.5105
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 11, Average loss: 0.0214, Accuracy: 0.1848
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.33128993907388,
                        "time": 2.523341075000644,
                        "accuracy": 0.184765625,
                        "total_cost": 1575.077513125473
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.2904
Profiling... [512/50176]	Loss: 1.5561
Profiling... [768/50176]	Loss: 1.4949
Profiling... [1024/50176]	Loss: 1.5696
Profiling... [1280/50176]	Loss: 1.6473
Profiling... [1536/50176]	Loss: 1.6410
Profiling... [1792/50176]	Loss: 1.5781
Profiling... [2048/50176]	Loss: 1.4342
Profiling... [2304/50176]	Loss: 1.6320
Profiling... [2560/50176]	Loss: 1.6183
Profiling... [2816/50176]	Loss: 1.5534
Profiling... [3072/50176]	Loss: 1.5536
Profiling... [3328/50176]	Loss: 1.5729
Profile done
epoch 1 train time consumed: 4.72s
Validation Epoch: 11, Average loss: 0.0124, Accuracy: 0.3126
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.31499890245837,
                        "time": 3.044974207004998,
                        "accuracy": 0.31259765625,
                        "total_cost": 1123.2688099810262
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5490
Profiling... [512/50176]	Loss: 1.4508
Profiling... [768/50176]	Loss: 1.5804
Profiling... [1024/50176]	Loss: 1.6053
Profiling... [1280/50176]	Loss: 1.5951
Profiling... [1536/50176]	Loss: 1.6902
Profiling... [1792/50176]	Loss: 1.6641
Profiling... [2048/50176]	Loss: 1.4993
Profiling... [2304/50176]	Loss: 1.6860
Profiling... [2560/50176]	Loss: 1.4750
Profiling... [2816/50176]	Loss: 1.5546
Profiling... [3072/50176]	Loss: 1.6026
Profiling... [3328/50176]	Loss: 1.5256
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 11, Average loss: 0.0163, Accuracy: 0.2375
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.30009241117699,
                        "time": 2.3937639180003316,
                        "accuracy": 0.2375,
                        "total_cost": 1162.11031981465
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.3309
Profiling... [512/50176]	Loss: 1.5054
Profiling... [768/50176]	Loss: 1.4891
Profiling... [1024/50176]	Loss: 1.5477
Profiling... [1280/50176]	Loss: 1.3008
Profiling... [1536/50176]	Loss: 1.4347
Profiling... [1792/50176]	Loss: 1.6469
Profiling... [2048/50176]	Loss: 1.7461
Profiling... [2304/50176]	Loss: 1.3902
Profiling... [2560/50176]	Loss: 1.5625
Profiling... [2816/50176]	Loss: 1.4757
Profiling... [3072/50176]	Loss: 1.5576
Profiling... [3328/50176]	Loss: 1.3679
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 11, Average loss: 0.0183, Accuracy: 0.1996
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.3083921568924,
                        "time": 2.419607804993575,
                        "accuracy": 0.199609375,
                        "total_cost": 1397.7353801347092
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.4021
Profiling... [512/50176]	Loss: 1.5077
Profiling... [768/50176]	Loss: 1.5195
Profiling... [1024/50176]	Loss: 1.4661
Profiling... [1280/50176]	Loss: 1.5338
Profiling... [1536/50176]	Loss: 1.5223
Profiling... [1792/50176]	Loss: 1.5274
Profiling... [2048/50176]	Loss: 1.7034
Profiling... [2304/50176]	Loss: 1.5412
Profiling... [2560/50176]	Loss: 1.6081
Profiling... [2816/50176]	Loss: 1.4313
Profiling... [3072/50176]	Loss: 1.3654
Profiling... [3328/50176]	Loss: 1.7368
Profile done
epoch 1 train time consumed: 4.15s
Validation Epoch: 11, Average loss: 0.0203, Accuracy: 0.2014
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.3113300206793,
                        "time": 2.5408690069962176,
                        "accuracy": 0.2013671875,
                        "total_cost": 1455.008575342279
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5171
Profiling... [512/50176]	Loss: 1.4235
Profiling... [768/50176]	Loss: 1.4512
Profiling... [1024/50176]	Loss: 1.4754
Profiling... [1280/50176]	Loss: 1.5404
Profiling... [1536/50176]	Loss: 1.7358
Profiling... [1792/50176]	Loss: 1.7536
Profiling... [2048/50176]	Loss: 1.5991
Profiling... [2304/50176]	Loss: 1.4222
Profiling... [2560/50176]	Loss: 1.5671
Profiling... [2816/50176]	Loss: 1.6301
Profiling... [3072/50176]	Loss: 1.5337
Profiling... [3328/50176]	Loss: 1.6095
Profile done
epoch 1 train time consumed: 4.56s
Validation Epoch: 11, Average loss: 0.0124, Accuracy: 0.3141
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.29508991675813,
                        "time": 3.0257366959995124,
                        "accuracy": 0.3140625,
                        "total_cost": 1110.7743981841143
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4424
Profiling... [1024/50176]	Loss: 1.3366
Profiling... [1536/50176]	Loss: 1.3643
Profiling... [2048/50176]	Loss: 1.3137
Profiling... [2560/50176]	Loss: 1.4511
Profiling... [3072/50176]	Loss: 1.2848
Profiling... [3584/50176]	Loss: 1.3732
Profiling... [4096/50176]	Loss: 1.3207
Profiling... [4608/50176]	Loss: 1.2850
Profiling... [5120/50176]	Loss: 1.2841
Profiling... [5632/50176]	Loss: 1.2308
Profiling... [6144/50176]	Loss: 1.4342
Profiling... [6656/50176]	Loss: 1.3451
Profile done
epoch 1 train time consumed: 6.92s
Validation Epoch: 11, Average loss: 0.0031, Accuracy: 0.5535
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.29417456062947,
                        "time": 4.500129705003928,
                        "accuracy": 0.553515625,
                        "total_cost": 937.3515693512662
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.3777
Profiling... [1024/50176]	Loss: 1.4185
Profiling... [1536/50176]	Loss: 1.2573
Profiling... [2048/50176]	Loss: 1.2870
Profiling... [2560/50176]	Loss: 1.3528
Profiling... [3072/50176]	Loss: 1.3881
Profiling... [3584/50176]	Loss: 1.4578
Profiling... [4096/50176]	Loss: 1.3092
Profiling... [4608/50176]	Loss: 1.3769
Profiling... [5120/50176]	Loss: 1.3372
Profiling... [5632/50176]	Loss: 1.2936
Profiling... [6144/50176]	Loss: 1.3330
Profiling... [6656/50176]	Loss: 1.4760
Profile done
epoch 1 train time consumed: 6.99s
Validation Epoch: 11, Average loss: 0.0030, Accuracy: 0.5572
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.31313444553534,
                        "time": 4.575890899002843,
                        "accuracy": 0.5572265625,
                        "total_cost": 946.9403613450613
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.3888
Profiling... [1024/50176]	Loss: 1.4610
Profiling... [1536/50176]	Loss: 1.3270
Profiling... [2048/50176]	Loss: 1.4302
Profiling... [2560/50176]	Loss: 1.2996
Profiling... [3072/50176]	Loss: 1.3354
Profiling... [3584/50176]	Loss: 1.3346
Profiling... [4096/50176]	Loss: 1.2877
Profiling... [4608/50176]	Loss: 1.3472
Profiling... [5120/50176]	Loss: 1.3140
Profiling... [5632/50176]	Loss: 1.4555
Profiling... [6144/50176]	Loss: 1.2711
Profiling... [6656/50176]	Loss: 1.3626
Profile done
epoch 1 train time consumed: 7.37s
Validation Epoch: 11, Average loss: 0.0031, Accuracy: 0.5563
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.31722656745427,
                        "time": 4.853394372003095,
                        "accuracy": 0.55625,
                        "total_cost": 1006.1662533348104
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.3001
Profiling... [1024/50176]	Loss: 1.4302
Profiling... [1536/50176]	Loss: 1.3394
Profiling... [2048/50176]	Loss: 1.2880
Profiling... [2560/50176]	Loss: 1.4420
Profiling... [3072/50176]	Loss: 1.2375
Profiling... [3584/50176]	Loss: 1.3527
Profiling... [4096/50176]	Loss: 1.4227
Profiling... [4608/50176]	Loss: 1.3771
Profiling... [5120/50176]	Loss: 1.2465
Profiling... [5632/50176]	Loss: 1.4118
Profiling... [6144/50176]	Loss: 1.3153
Profiling... [6656/50176]	Loss: 1.3383
Profile done
epoch 1 train time consumed: 8.78s
Validation Epoch: 11, Average loss: 0.0031, Accuracy: 0.5559
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.28729132063687,
                        "time": 5.962847866001539,
                        "accuracy": 0.555859375,
                        "total_cost": 1236.7167127987314
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4463
Profiling... [1024/50176]	Loss: 1.3010
Profiling... [1536/50176]	Loss: 1.4259
Profiling... [2048/50176]	Loss: 1.3445
Profiling... [2560/50176]	Loss: 1.3318
Profiling... [3072/50176]	Loss: 1.2319
Profiling... [3584/50176]	Loss: 1.4477
Profiling... [4096/50176]	Loss: 1.4440
Profiling... [4608/50176]	Loss: 1.3746
Profiling... [5120/50176]	Loss: 1.2977
Profiling... [5632/50176]	Loss: 1.3224
Profiling... [6144/50176]	Loss: 1.3746
Profiling... [6656/50176]	Loss: 1.4450
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 11, Average loss: 0.0031, Accuracy: 0.5567
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.28398138543804,
                        "time": 4.512107032998756,
                        "accuracy": 0.55673828125,
                        "total_cost": 934.3235066096554
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4068
Profiling... [1024/50176]	Loss: 1.4141
Profiling... [1536/50176]	Loss: 1.3437
Profiling... [2048/50176]	Loss: 1.2371
Profiling... [2560/50176]	Loss: 1.4961
Profiling... [3072/50176]	Loss: 1.2888
Profiling... [3584/50176]	Loss: 1.2917
Profiling... [4096/50176]	Loss: 1.3720
Profiling... [4608/50176]	Loss: 1.3199
Profiling... [5120/50176]	Loss: 1.4577
Profiling... [5632/50176]	Loss: 1.4193
Profiling... [6144/50176]	Loss: 1.2994
Profiling... [6656/50176]	Loss: 1.3073
Profile done
epoch 1 train time consumed: 7.06s
Validation Epoch: 11, Average loss: 0.0031, Accuracy: 0.5535
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.3035163797331,
                        "time": 4.625899890001165,
                        "accuracy": 0.553515625,
                        "total_cost": 963.6268601049064
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4302
Profiling... [1024/50176]	Loss: 1.3826
Profiling... [1536/50176]	Loss: 1.3166
Profiling... [2048/50176]	Loss: 1.4246
Profiling... [2560/50176]	Loss: 1.3426
Profiling... [3072/50176]	Loss: 1.2174
Profiling... [3584/50176]	Loss: 1.3546
Profiling... [4096/50176]	Loss: 1.3982
Profiling... [4608/50176]	Loss: 1.3309
Profiling... [5120/50176]	Loss: 1.2859
Profiling... [5632/50176]	Loss: 1.3513
Profiling... [6144/50176]	Loss: 1.3752
Profiling... [6656/50176]	Loss: 1.4556
Profile done
epoch 1 train time consumed: 7.24s
Validation Epoch: 11, Average loss: 0.0030, Accuracy: 0.5594
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.30772772745198,
                        "time": 4.880188899995119,
                        "accuracy": 0.559375,
                        "total_cost": 1005.9861326286849
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.3594
Profiling... [1024/50176]	Loss: 1.2247
Profiling... [1536/50176]	Loss: 1.3017
Profiling... [2048/50176]	Loss: 1.3841
Profiling... [2560/50176]	Loss: 1.4151
Profiling... [3072/50176]	Loss: 1.4502
Profiling... [3584/50176]	Loss: 1.3699
Profiling... [4096/50176]	Loss: 1.3500
Profiling... [4608/50176]	Loss: 1.3275
Profiling... [5120/50176]	Loss: 1.2994
Profiling... [5632/50176]	Loss: 1.4568
Profiling... [6144/50176]	Loss: 1.3076
Profiling... [6656/50176]	Loss: 1.3784
Profile done
epoch 1 train time consumed: 8.84s
Validation Epoch: 11, Average loss: 0.0031, Accuracy: 0.5563
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.27693662437176,
                        "time": 6.006352059004712,
                        "accuracy": 0.55634765625,
                        "total_cost": 1244.5345241796388
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4469
Profiling... [1024/50176]	Loss: 1.3756
Profiling... [1536/50176]	Loss: 1.4029
Profiling... [2048/50176]	Loss: 1.3631
Profiling... [2560/50176]	Loss: 1.2441
Profiling... [3072/50176]	Loss: 1.3184
Profiling... [3584/50176]	Loss: 1.4156
Profiling... [4096/50176]	Loss: 1.2975
Profiling... [4608/50176]	Loss: 1.2516
Profiling... [5120/50176]	Loss: 1.4078
Profiling... [5632/50176]	Loss: 1.3176
Profiling... [6144/50176]	Loss: 1.2877
Profiling... [6656/50176]	Loss: 1.2458
Profile done
epoch 1 train time consumed: 6.89s
Validation Epoch: 11, Average loss: 0.0031, Accuracy: 0.5510
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.2737861632523,
                        "time": 4.558416309999302,
                        "accuracy": 0.5509765625,
                        "total_cost": 953.6992001577946
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5848
Profiling... [1024/50176]	Loss: 1.3656
Profiling... [1536/50176]	Loss: 1.3045
Profiling... [2048/50176]	Loss: 1.4058
Profiling... [2560/50176]	Loss: 1.3586
Profiling... [3072/50176]	Loss: 1.2543
Profiling... [3584/50176]	Loss: 1.4365
Profiling... [4096/50176]	Loss: 1.4354
Profiling... [4608/50176]	Loss: 1.4701
Profiling... [5120/50176]	Loss: 1.2421
Profiling... [5632/50176]	Loss: 1.4432
Profiling... [6144/50176]	Loss: 1.4690
Profiling... [6656/50176]	Loss: 1.4346
Profile done
epoch 1 train time consumed: 6.88s
Validation Epoch: 11, Average loss: 0.0031, Accuracy: 0.5493
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.29657801890556,
                        "time": 4.521772845997475,
                        "accuracy": 0.54931640625,
                        "total_cost": 949.0794918749372
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.3384
Profiling... [1024/50176]	Loss: 1.2794
Profiling... [1536/50176]	Loss: 1.3781
Profiling... [2048/50176]	Loss: 1.5146
Profiling... [2560/50176]	Loss: 1.4071
Profiling... [3072/50176]	Loss: 1.4705
Profiling... [3584/50176]	Loss: 1.3473
Profiling... [4096/50176]	Loss: 1.2682
Profiling... [4608/50176]	Loss: 1.3327
Profiling... [5120/50176]	Loss: 1.4225
Profiling... [5632/50176]	Loss: 1.3225
Profiling... [6144/50176]	Loss: 1.4289
Profiling... [6656/50176]	Loss: 1.4124
Profile done
epoch 1 train time consumed: 7.38s
Validation Epoch: 11, Average loss: 0.0031, Accuracy: 0.5558
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.30134566896024,
                        "time": 4.8805698930009385,
                        "accuracy": 0.55576171875,
                        "total_cost": 1012.5495465216791
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4051
Profiling... [1024/50176]	Loss: 1.4781
Profiling... [1536/50176]	Loss: 1.2958
Profiling... [2048/50176]	Loss: 1.4128
Profiling... [2560/50176]	Loss: 1.4040
Profiling... [3072/50176]	Loss: 1.3896
Profiling... [3584/50176]	Loss: 1.3601
Profiling... [4096/50176]	Loss: 1.3853
Profiling... [4608/50176]	Loss: 1.3514
Profiling... [5120/50176]	Loss: 1.2604
Profiling... [5632/50176]	Loss: 1.4451
Profiling... [6144/50176]	Loss: 1.2737
Profiling... [6656/50176]	Loss: 1.4052
Profile done
epoch 1 train time consumed: 8.71s
Validation Epoch: 11, Average loss: 0.0031, Accuracy: 0.5541
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.27003684154882,
                        "time": 5.999105996997969,
                        "accuracy": 0.5541015625,
                        "total_cost": 1247.9971472563973
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4153
Profiling... [1024/50176]	Loss: 1.4746
Profiling... [1536/50176]	Loss: 1.3888
Profiling... [2048/50176]	Loss: 1.3198
Profiling... [2560/50176]	Loss: 1.2828
Profiling... [3072/50176]	Loss: 1.2944
Profiling... [3584/50176]	Loss: 1.4220
Profiling... [4096/50176]	Loss: 1.4809
Profiling... [4608/50176]	Loss: 1.3997
Profiling... [5120/50176]	Loss: 1.4643
Profiling... [5632/50176]	Loss: 1.3495
Profiling... [6144/50176]	Loss: 1.3470
Profiling... [6656/50176]	Loss: 1.3195
Profile done
epoch 1 train time consumed: 7.02s
Validation Epoch: 11, Average loss: 0.0053, Accuracy: 0.3817
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.26644564216927,
                        "time": 4.535250595996331,
                        "accuracy": 0.38173828125,
                        "total_cost": 1369.4257085908303
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.3650
Profiling... [1024/50176]	Loss: 1.3030
Profiling... [1536/50176]	Loss: 1.4296
Profiling... [2048/50176]	Loss: 1.3820
Profiling... [2560/50176]	Loss: 1.3098
Profiling... [3072/50176]	Loss: 1.3861
Profiling... [3584/50176]	Loss: 1.4608
Profiling... [4096/50176]	Loss: 1.4179
Profiling... [4608/50176]	Loss: 1.3450
Profiling... [5120/50176]	Loss: 1.4152
Profiling... [5632/50176]	Loss: 1.4682
Profiling... [6144/50176]	Loss: 1.4025
Profiling... [6656/50176]	Loss: 1.4799
Profile done
epoch 1 train time consumed: 7.07s
Validation Epoch: 11, Average loss: 0.0040, Accuracy: 0.4627
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.2863103720899,
                        "time": 4.737797016001423,
                        "accuracy": 0.4626953125,
                        "total_cost": 1180.4812422142302
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.3584
Profiling... [1024/50176]	Loss: 1.3499
Profiling... [1536/50176]	Loss: 1.4235
Profiling... [2048/50176]	Loss: 1.4465
Profiling... [2560/50176]	Loss: 1.3472
Profiling... [3072/50176]	Loss: 1.2930
Profiling... [3584/50176]	Loss: 1.4148
Profiling... [4096/50176]	Loss: 1.2850
Profiling... [4608/50176]	Loss: 1.2803
Profiling... [5120/50176]	Loss: 1.4602
Profiling... [5632/50176]	Loss: 1.3291
Profiling... [6144/50176]	Loss: 1.4734
Profiling... [6656/50176]	Loss: 1.3708
Profile done
epoch 1 train time consumed: 7.36s
Validation Epoch: 11, Average loss: 0.0054, Accuracy: 0.3614
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.29110895461723,
                        "time": 4.8525867110001855,
                        "accuracy": 0.36142578125,
                        "total_cost": 1547.9252788075698
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4018
Profiling... [1024/50176]	Loss: 1.5073
Profiling... [1536/50176]	Loss: 1.4729
Profiling... [2048/50176]	Loss: 1.3632
Profiling... [2560/50176]	Loss: 1.3806
Profiling... [3072/50176]	Loss: 1.4228
Profiling... [3584/50176]	Loss: 1.3798
Profiling... [4096/50176]	Loss: 1.4039
Profiling... [4608/50176]	Loss: 1.4389
Profiling... [5120/50176]	Loss: 1.3690
Profiling... [5632/50176]	Loss: 1.4140
Profiling... [6144/50176]	Loss: 1.3138
Profiling... [6656/50176]	Loss: 1.3453
Profile done
epoch 1 train time consumed: 8.68s
Validation Epoch: 11, Average loss: 0.0041, Accuracy: 0.4560
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.26184381234145,
                        "time": 5.999887215999479,
                        "accuracy": 0.45595703125,
                        "total_cost": 1516.7176198298755
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4776
Profiling... [1024/50176]	Loss: 1.3433
Profiling... [1536/50176]	Loss: 1.3734
Profiling... [2048/50176]	Loss: 1.3071
Profiling... [2560/50176]	Loss: 1.5537
Profiling... [3072/50176]	Loss: 1.3612
Profiling... [3584/50176]	Loss: 1.4224
Profiling... [4096/50176]	Loss: 1.4515
Profiling... [4608/50176]	Loss: 1.4880
Profiling... [5120/50176]	Loss: 1.4118
Profiling... [5632/50176]	Loss: 1.3442
Profiling... [6144/50176]	Loss: 1.4380
Profiling... [6656/50176]	Loss: 1.4472
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 11, Average loss: 0.0039, Accuracy: 0.4721
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.2581039456468,
                        "time": 4.621082571997249,
                        "accuracy": 0.4720703125,
                        "total_cost": 1128.2582304403559
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4367
Profiling... [1024/50176]	Loss: 1.2910
Profiling... [1536/50176]	Loss: 1.3516
Profiling... [2048/50176]	Loss: 1.4587
Profiling... [2560/50176]	Loss: 1.3651
Profiling... [3072/50176]	Loss: 1.3843
Profiling... [3584/50176]	Loss: 1.3915
Profiling... [4096/50176]	Loss: 1.2744
Profiling... [4608/50176]	Loss: 1.3093
Profiling... [5120/50176]	Loss: 1.5158
Profiling... [5632/50176]	Loss: 1.4782
Profiling... [6144/50176]	Loss: 1.4075
Profiling... [6656/50176]	Loss: 1.4264
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 11, Average loss: 0.0043, Accuracy: 0.4406
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.27771440522413,
                        "time": 4.603306450000673,
                        "accuracy": 0.440625,
                        "total_cost": 1204.3316794619093
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4474
Profiling... [1024/50176]	Loss: 1.3863
Profiling... [1536/50176]	Loss: 1.3814
Profiling... [2048/50176]	Loss: 1.3612
Profiling... [2560/50176]	Loss: 1.4407
Profiling... [3072/50176]	Loss: 1.4157
Profiling... [3584/50176]	Loss: 1.2571
Profiling... [4096/50176]	Loss: 1.3920
Profiling... [4608/50176]	Loss: 1.4035
Profiling... [5120/50176]	Loss: 1.4355
Profiling... [5632/50176]	Loss: 1.5320
Profiling... [6144/50176]	Loss: 1.3796
Profiling... [6656/50176]	Loss: 1.3578
Profile done
epoch 1 train time consumed: 7.31s
Validation Epoch: 11, Average loss: 0.0048, Accuracy: 0.4053
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.2828071280366,
                        "time": 4.850572335999459,
                        "accuracy": 0.4052734375,
                        "total_cost": 1379.7785478393598
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4402
Profiling... [1024/50176]	Loss: 1.3594
Profiling... [1536/50176]	Loss: 1.4329
Profiling... [2048/50176]	Loss: 1.2251
Profiling... [2560/50176]	Loss: 1.4449
Profiling... [3072/50176]	Loss: 1.3575
Profiling... [3584/50176]	Loss: 1.3163
Profiling... [4096/50176]	Loss: 1.3498
Profiling... [4608/50176]	Loss: 1.4085
Profiling... [5120/50176]	Loss: 1.3493
Profiling... [5632/50176]	Loss: 1.3352
Profiling... [6144/50176]	Loss: 1.3567
Profiling... [6656/50176]	Loss: 1.2878
Profile done
epoch 1 train time consumed: 8.70s
Validation Epoch: 11, Average loss: 0.0042, Accuracy: 0.4568
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.25460710947526,
                        "time": 6.018334382002649,
                        "accuracy": 0.4568359375,
                        "total_cost": 1518.3585784582938
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.3756
Profiling... [1024/50176]	Loss: 1.4742
Profiling... [1536/50176]	Loss: 1.3327
Profiling... [2048/50176]	Loss: 1.3296
Profiling... [2560/50176]	Loss: 1.3789
Profiling... [3072/50176]	Loss: 1.3358
Profiling... [3584/50176]	Loss: 1.3629
Profiling... [4096/50176]	Loss: 1.3827
Profiling... [4608/50176]	Loss: 1.4614
Profiling... [5120/50176]	Loss: 1.4231
Profiling... [5632/50176]	Loss: 1.2837
Profiling... [6144/50176]	Loss: 1.4233
Profiling... [6656/50176]	Loss: 1.3928
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 11, Average loss: 0.0035, Accuracy: 0.5061
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.25003578721625,
                        "time": 4.599294854000618,
                        "accuracy": 0.5060546875,
                        "total_cost": 1047.4537823928974
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4541
Profiling... [1024/50176]	Loss: 1.3909
Profiling... [1536/50176]	Loss: 1.3948
Profiling... [2048/50176]	Loss: 1.2992
Profiling... [2560/50176]	Loss: 1.2813
Profiling... [3072/50176]	Loss: 1.3715
Profiling... [3584/50176]	Loss: 1.4742
Profiling... [4096/50176]	Loss: 1.3786
Profiling... [4608/50176]	Loss: 1.2708
Profiling... [5120/50176]	Loss: 1.2922
Profiling... [5632/50176]	Loss: 1.3943
Profiling... [6144/50176]	Loss: 1.4260
Profiling... [6656/50176]	Loss: 1.4089
Profile done
epoch 1 train time consumed: 7.05s
Validation Epoch: 11, Average loss: 0.0048, Accuracy: 0.4174
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.26983450625835,
                        "time": 4.549875580996741,
                        "accuracy": 0.4173828125,
                        "total_cost": 1256.552473026331
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.3584
Profiling... [1024/50176]	Loss: 1.3981
Profiling... [1536/50176]	Loss: 1.4359
Profiling... [2048/50176]	Loss: 1.3368
Profiling... [2560/50176]	Loss: 1.2820
Profiling... [3072/50176]	Loss: 1.3657
Profiling... [3584/50176]	Loss: 1.4196
Profiling... [4096/50176]	Loss: 1.4992
Profiling... [4608/50176]	Loss: 1.3102
Profiling... [5120/50176]	Loss: 1.4658
Profiling... [5632/50176]	Loss: 1.2542
Profiling... [6144/50176]	Loss: 1.2856
Profiling... [6656/50176]	Loss: 1.3573
Profile done
epoch 1 train time consumed: 7.34s
Validation Epoch: 11, Average loss: 0.0036, Accuracy: 0.5057
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.273609162697,
                        "time": 4.854576684003405,
                        "accuracy": 0.5056640625,
                        "total_cost": 1106.6726248163031
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.3911
Profiling... [1024/50176]	Loss: 1.3877
Profiling... [1536/50176]	Loss: 1.3218
Profiling... [2048/50176]	Loss: 1.3404
Profiling... [2560/50176]	Loss: 1.3382
Profiling... [3072/50176]	Loss: 1.4029
Profiling... [3584/50176]	Loss: 1.3500
Profiling... [4096/50176]	Loss: 1.4205
Profiling... [4608/50176]	Loss: 1.4229
Profiling... [5120/50176]	Loss: 1.3007
Profiling... [5632/50176]	Loss: 1.3325
Profiling... [6144/50176]	Loss: 1.5210
Profiling... [6656/50176]	Loss: 1.4898
Profile done
epoch 1 train time consumed: 8.70s
Validation Epoch: 11, Average loss: 0.0041, Accuracy: 0.4652
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.24521001998697,
                        "time": 6.000010871997802,
                        "accuracy": 0.465234375,
                        "total_cost": 1486.2885251451848
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.3634
Profiling... [1024/50176]	Loss: 1.3124
Profiling... [1536/50176]	Loss: 1.5209
Profiling... [2048/50176]	Loss: 1.3703
Profiling... [2560/50176]	Loss: 1.4987
Profiling... [3072/50176]	Loss: 1.4006
Profiling... [3584/50176]	Loss: 1.4817
Profiling... [4096/50176]	Loss: 1.5447
Profiling... [4608/50176]	Loss: 1.3966
Profiling... [5120/50176]	Loss: 1.3950
Profiling... [5632/50176]	Loss: 1.4144
Profiling... [6144/50176]	Loss: 1.4279
Profiling... [6656/50176]	Loss: 1.4946
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 11, Average loss: 0.0155, Accuracy: 0.1394
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.24218728746376,
                        "time": 4.514344116003485,
                        "accuracy": 0.13935546875,
                        "total_cost": 3733.2075645329405
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4526
Profiling... [1024/50176]	Loss: 1.3793
Profiling... [1536/50176]	Loss: 1.5076
Profiling... [2048/50176]	Loss: 1.4209
Profiling... [2560/50176]	Loss: 1.3762
Profiling... [3072/50176]	Loss: 1.5579
Profiling... [3584/50176]	Loss: 1.4690
Profiling... [4096/50176]	Loss: 1.4966
Profiling... [4608/50176]	Loss: 1.5581
Profiling... [5120/50176]	Loss: 1.4418
Profiling... [5632/50176]	Loss: 1.4639
Profiling... [6144/50176]	Loss: 1.4310
Profiling... [6656/50176]	Loss: 1.5303
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 11, Average loss: 0.0084, Accuracy: 0.2054
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.2624526731153,
                        "time": 4.511596576994634,
                        "accuracy": 0.20537109375,
                        "total_cost": 2532.088023882538
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.3604
Profiling... [1024/50176]	Loss: 1.4521
Profiling... [1536/50176]	Loss: 1.4424
Profiling... [2048/50176]	Loss: 1.4723
Profiling... [2560/50176]	Loss: 1.4711
Profiling... [3072/50176]	Loss: 1.4556
Profiling... [3584/50176]	Loss: 1.4126
Profiling... [4096/50176]	Loss: 1.6024
Profiling... [4608/50176]	Loss: 1.5391
Profiling... [5120/50176]	Loss: 1.4573
Profiling... [5632/50176]	Loss: 1.3454
Profiling... [6144/50176]	Loss: 1.5221
Profiling... [6656/50176]	Loss: 1.5079
Profile done
epoch 1 train time consumed: 7.24s
Validation Epoch: 11, Average loss: 0.0098, Accuracy: 0.1924
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.2670407649678,
                        "time": 4.836383920999651,
                        "accuracy": 0.1923828125,
                        "total_cost": 2897.7415151205446
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.2556
Profiling... [1024/50176]	Loss: 1.4841
Profiling... [1536/50176]	Loss: 1.4772
Profiling... [2048/50176]	Loss: 1.4500
Profiling... [2560/50176]	Loss: 1.4532
Profiling... [3072/50176]	Loss: 1.4646
Profiling... [3584/50176]	Loss: 1.3579
Profiling... [4096/50176]	Loss: 1.5631
Profiling... [4608/50176]	Loss: 1.3830
Profiling... [5120/50176]	Loss: 1.4284
Profiling... [5632/50176]	Loss: 1.5056
Profiling... [6144/50176]	Loss: 1.4521
Profiling... [6656/50176]	Loss: 1.5215
Profile done
epoch 1 train time consumed: 8.76s
Validation Epoch: 11, Average loss: 0.0107, Accuracy: 0.1479
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.238613908507,
                        "time": 6.004971802998625,
                        "accuracy": 0.14794921875,
                        "total_cost": 4677.311803224577
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4402
Profiling... [1024/50176]	Loss: 1.4385
Profiling... [1536/50176]	Loss: 1.4472
Profiling... [2048/50176]	Loss: 1.4010
Profiling... [2560/50176]	Loss: 1.4453
Profiling... [3072/50176]	Loss: 1.4967
Profiling... [3584/50176]	Loss: 1.5508
Profiling... [4096/50176]	Loss: 1.3710
Profiling... [4608/50176]	Loss: 1.4663
Profiling... [5120/50176]	Loss: 1.5516
Profiling... [5632/50176]	Loss: 1.5369
Profiling... [6144/50176]	Loss: 1.5663
Profiling... [6656/50176]	Loss: 1.4610
Profile done
epoch 1 train time consumed: 6.83s
Validation Epoch: 11, Average loss: 0.0082, Accuracy: 0.2324
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.23534426965335,
                        "time": 4.500095028000942,
                        "accuracy": 0.232421875,
                        "total_cost": 2231.1583184579504
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4849
Profiling... [1024/50176]	Loss: 1.3188
Profiling... [1536/50176]	Loss: 1.4701
Profiling... [2048/50176]	Loss: 1.4786
Profiling... [2560/50176]	Loss: 1.5305
Profiling... [3072/50176]	Loss: 1.4743
Profiling... [3584/50176]	Loss: 1.5503
Profiling... [4096/50176]	Loss: 1.4721
Profiling... [4608/50176]	Loss: 1.5094
Profiling... [5120/50176]	Loss: 1.4024
Profiling... [5632/50176]	Loss: 1.4999
Profiling... [6144/50176]	Loss: 1.5804
Profiling... [6656/50176]	Loss: 1.4839
Profile done
epoch 1 train time consumed: 6.91s
Validation Epoch: 11, Average loss: 0.0124, Accuracy: 0.1611
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.25616105580775,
                        "time": 4.48797092300083,
                        "accuracy": 0.1611328125,
                        "total_cost": 3210.1860042638173
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.4419
Profiling... [1024/50176]	Loss: 1.4554
Profiling... [1536/50176]	Loss: 1.4990
Profiling... [2048/50176]	Loss: 1.4715
Profiling... [2560/50176]	Loss: 1.4784
Profiling... [3072/50176]	Loss: 1.4218
Profiling... [3584/50176]	Loss: 1.5881
Profiling... [4096/50176]	Loss: 1.4346
Profiling... [4608/50176]	Loss: 1.5434
Profiling... [5120/50176]	Loss: 1.5361
Profiling... [5632/50176]	Loss: 1.4129
Profiling... [6144/50176]	Loss: 1.4373
Profiling... [6656/50176]	Loss: 1.4743
Profile done
epoch 1 train time consumed: 7.25s
Validation Epoch: 11, Average loss: 0.0068, Accuracy: 0.3148
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.26092167857809,
                        "time": 4.829965816999902,
                        "accuracy": 0.31484375,
                        "total_cost": 1768.1987072744341
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4339
Profiling... [1024/50176]	Loss: 1.4440
Profiling... [1536/50176]	Loss: 1.3939
Profiling... [2048/50176]	Loss: 1.3322
Profiling... [2560/50176]	Loss: 1.5416
Profiling... [3072/50176]	Loss: 1.5212
Profiling... [3584/50176]	Loss: 1.3624
Profiling... [4096/50176]	Loss: 1.4568
Profiling... [4608/50176]	Loss: 1.4832
Profiling... [5120/50176]	Loss: 1.4475
Profiling... [5632/50176]	Loss: 1.4611
Profiling... [6144/50176]	Loss: 1.5100
Profiling... [6656/50176]	Loss: 1.4307
Profile done
epoch 1 train time consumed: 8.70s
Validation Epoch: 11, Average loss: 0.0165, Accuracy: 0.1110
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.23229334739172,
                        "time": 5.968631966999965,
                        "accuracy": 0.11103515625,
                        "total_cost": 6194.246695662749
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.4795
Profiling... [1024/50176]	Loss: 1.4926
Profiling... [1536/50176]	Loss: 1.4930
Profiling... [2048/50176]	Loss: 1.5249
Profiling... [2560/50176]	Loss: 1.5521
Profiling... [3072/50176]	Loss: 1.4441
Profiling... [3584/50176]	Loss: 1.5533
Profiling... [4096/50176]	Loss: 1.3922
Profiling... [4608/50176]	Loss: 1.4438
Profiling... [5120/50176]	Loss: 1.4294
Profiling... [5632/50176]	Loss: 1.3931
Profiling... [6144/50176]	Loss: 1.4209
Profiling... [6656/50176]	Loss: 1.3738
Profile done
epoch 1 train time consumed: 6.90s
Validation Epoch: 11, Average loss: 0.0122, Accuracy: 0.1574
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.22887053942607,
                        "time": 4.501627988000109,
                        "accuracy": 0.157421875,
                        "total_cost": 3295.0789631105695
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.4047
Profiling... [1024/50176]	Loss: 1.3751
Profiling... [1536/50176]	Loss: 1.4288
Profiling... [2048/50176]	Loss: 1.4450
Profiling... [2560/50176]	Loss: 1.4934
Profiling... [3072/50176]	Loss: 1.3842
Profiling... [3584/50176]	Loss: 1.6372
Profiling... [4096/50176]	Loss: 1.4509
Profiling... [4608/50176]	Loss: 1.4524
Profiling... [5120/50176]	Loss: 1.4335
Profiling... [5632/50176]	Loss: 1.6119
Profiling... [6144/50176]	Loss: 1.4968
Profiling... [6656/50176]	Loss: 1.3530
Profile done
epoch 1 train time consumed: 6.88s
Validation Epoch: 11, Average loss: 0.0114, Accuracy: 0.2034
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.24918672532814,
                        "time": 4.497019513000851,
                        "accuracy": 0.20341796875,
                        "total_cost": 2547.8469023463754
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.3226
Profiling... [1024/50176]	Loss: 1.4880
Profiling... [1536/50176]	Loss: 1.4907
Profiling... [2048/50176]	Loss: 1.4188
Profiling... [2560/50176]	Loss: 1.5345
Profiling... [3072/50176]	Loss: 1.4457
Profiling... [3584/50176]	Loss: 1.3098
Profiling... [4096/50176]	Loss: 1.5264
Profiling... [4608/50176]	Loss: 1.4609
Profiling... [5120/50176]	Loss: 1.3210
Profiling... [5632/50176]	Loss: 1.4313
Profiling... [6144/50176]	Loss: 1.5016
Profiling... [6656/50176]	Loss: 1.5190
Profile done
epoch 1 train time consumed: 7.25s
Validation Epoch: 11, Average loss: 0.0108, Accuracy: 0.1575
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.25513414985102,
                        "time": 4.84032995099551,
                        "accuracy": 0.15751953125,
                        "total_cost": 3541.6108301270056
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.4536
Profiling... [1024/50176]	Loss: 1.4991
Profiling... [1536/50176]	Loss: 1.4583
Profiling... [2048/50176]	Loss: 1.4787
Profiling... [2560/50176]	Loss: 1.4372
Profiling... [3072/50176]	Loss: 1.5077
Profiling... [3584/50176]	Loss: 1.4353
Profiling... [4096/50176]	Loss: 1.4847
Profiling... [4608/50176]	Loss: 1.4292
Profiling... [5120/50176]	Loss: 1.4713
Profiling... [5632/50176]	Loss: 1.4307
Profiling... [6144/50176]	Loss: 1.4828
Profiling... [6656/50176]	Loss: 1.4104
Profile done
epoch 1 train time consumed: 8.72s
Validation Epoch: 11, Average loss: 0.0060, Accuracy: 0.3192
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.22706454781016,
                        "time": 5.9679072049984825,
                        "accuracy": 0.31923828125,
                        "total_cost": 2154.0788467883704
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.3497
Profiling... [2048/50176]	Loss: 1.3956
Profiling... [3072/50176]	Loss: 1.4012
Profiling... [4096/50176]	Loss: 1.3552
Profiling... [5120/50176]	Loss: 1.3603
Profiling... [6144/50176]	Loss: 1.3441
Profiling... [7168/50176]	Loss: 1.3194
Profiling... [8192/50176]	Loss: 1.3592
Profiling... [9216/50176]	Loss: 1.2316
Profiling... [10240/50176]	Loss: 1.2909
Profiling... [11264/50176]	Loss: 1.2676
Profiling... [12288/50176]	Loss: 1.3315
Profiling... [13312/50176]	Loss: 1.2882
Profile done
epoch 1 train time consumed: 12.84s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5561
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.24277726691999,
                        "time": 8.857850652006164,
                        "accuracy": 0.5560546875,
                        "total_cost": 1835.7966090390855
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.3009
Profiling... [2048/50176]	Loss: 1.3759
Profiling... [3072/50176]	Loss: 1.4020
Profiling... [4096/50176]	Loss: 1.3271
Profiling... [5120/50176]	Loss: 1.3072
Profiling... [6144/50176]	Loss: 1.2849
Profiling... [7168/50176]	Loss: 1.3240
Profiling... [8192/50176]	Loss: 1.2683
Profiling... [9216/50176]	Loss: 1.3223
Profiling... [10240/50176]	Loss: 1.2821
Profiling... [11264/50176]	Loss: 1.3258
Profiling... [12288/50176]	Loss: 1.2658
Profiling... [13312/50176]	Loss: 1.3430
Profile done
epoch 1 train time consumed: 13.10s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5613
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.28013557454237,
                        "time": 8.991551434999565,
                        "accuracy": 0.561328125,
                        "total_cost": 1846.5977781751465
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4954
Profiling... [2048/50176]	Loss: 1.3736
Profiling... [3072/50176]	Loss: 1.4379
Profiling... [4096/50176]	Loss: 1.3656
Profiling... [5120/50176]	Loss: 1.3540
Profiling... [6144/50176]	Loss: 1.3229
Profiling... [7168/50176]	Loss: 1.3353
Profiling... [8192/50176]	Loss: 1.3982
Profiling... [9216/50176]	Loss: 1.3224
Profiling... [10240/50176]	Loss: 1.2951
Profiling... [11264/50176]	Loss: 1.3370
Profiling... [12288/50176]	Loss: 1.4005
Profiling... [13312/50176]	Loss: 1.3855
Profile done
epoch 1 train time consumed: 13.75s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5526
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.288172465486,
                        "time": 9.523694289004197,
                        "accuracy": 0.55263671875,
                        "total_cost": 1986.783129761553
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.4244
Profiling... [2048/50176]	Loss: 1.2520
Profiling... [3072/50176]	Loss: 1.3301
Profiling... [4096/50176]	Loss: 1.3339
Profiling... [5120/50176]	Loss: 1.3824
Profiling... [6144/50176]	Loss: 1.3848
Profiling... [7168/50176]	Loss: 1.2672
Profiling... [8192/50176]	Loss: 1.3427
Profiling... [9216/50176]	Loss: 1.3601
Profiling... [10240/50176]	Loss: 1.3161
Profiling... [11264/50176]	Loss: 1.2816
Profiling... [12288/50176]	Loss: 1.2913
Profiling... [13312/50176]	Loss: 1.3297
Profile done
epoch 1 train time consumed: 18.66s
Validation Epoch: 11, Average loss: 0.0016, Accuracy: 0.5576
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 115.23328362005523,
                        "time": 13.829070091000176,
                        "accuracy": 0.5576171875,
                        "total_cost": 2857.8192920171537
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4164
Profiling... [2048/50176]	Loss: 1.3811
Profiling... [3072/50176]	Loss: 1.3388
Profiling... [4096/50176]	Loss: 1.2134
Profiling... [5120/50176]	Loss: 1.3768
Profiling... [6144/50176]	Loss: 1.3395
Profiling... [7168/50176]	Loss: 1.3962
Profiling... [8192/50176]	Loss: 1.3999
Profiling... [9216/50176]	Loss: 1.3809
Profiling... [10240/50176]	Loss: 1.3139
Profiling... [11264/50176]	Loss: 1.3514
Profiling... [12288/50176]	Loss: 1.3621
Profiling... [13312/50176]	Loss: 1.3258
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5595
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.24539207219048,
                        "time": 9.07609506299923,
                        "accuracy": 0.55947265625,
                        "total_cost": 1869.5786511368376
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.4348
Profiling... [2048/50176]	Loss: 1.3782
Profiling... [3072/50176]	Loss: 1.3198
Profiling... [4096/50176]	Loss: 1.3691
Profiling... [5120/50176]	Loss: 1.4590
Profiling... [6144/50176]	Loss: 1.3486
Profiling... [7168/50176]	Loss: 1.3070
Profiling... [8192/50176]	Loss: 1.2893
Profiling... [9216/50176]	Loss: 1.2862
Profiling... [10240/50176]	Loss: 1.3802
Profiling... [11264/50176]	Loss: 1.3395
Profiling... [12288/50176]	Loss: 1.3434
Profiling... [13312/50176]	Loss: 1.2996
Profile done
epoch 1 train time consumed: 12.89s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5575
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.28588400451407,
                        "time": 8.7880329039981,
                        "accuracy": 0.55751953125,
                        "total_cost": 1817.2208957893401
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.2499
Profiling... [2048/50176]	Loss: 1.4098
Profiling... [3072/50176]	Loss: 1.4014
Profiling... [4096/50176]	Loss: 1.3120
Profiling... [5120/50176]	Loss: 1.3555
Profiling... [6144/50176]	Loss: 1.3334
Profiling... [7168/50176]	Loss: 1.3082
Profiling... [8192/50176]	Loss: 1.3078
Profiling... [9216/50176]	Loss: 1.3063
Profiling... [10240/50176]	Loss: 1.2819
Profiling... [11264/50176]	Loss: 1.3509
Profiling... [12288/50176]	Loss: 1.3330
Profiling... [13312/50176]	Loss: 1.3394
Profile done
epoch 1 train time consumed: 13.80s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5594
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.2919455425547,
                        "time": 9.527791594002338,
                        "accuracy": 0.559375,
                        "total_cost": 1963.7588908988212
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.4169
Profiling... [2048/50176]	Loss: 1.4118
Profiling... [3072/50176]	Loss: 1.3761
Profiling... [4096/50176]	Loss: 1.3311
Profiling... [5120/50176]	Loss: 1.2814
Profiling... [6144/50176]	Loss: 1.3856
Profiling... [7168/50176]	Loss: 1.4383
Profiling... [8192/50176]	Loss: 1.3953
Profiling... [9216/50176]	Loss: 1.4213
Profiling... [10240/50176]	Loss: 1.3118
Profiling... [11264/50176]	Loss: 1.3029
Profiling... [12288/50176]	Loss: 1.3329
Profiling... [13312/50176]	Loss: 1.3373
Profile done
epoch 1 train time consumed: 17.27s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5585
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 115.240977692196,
                        "time": 12.534759745001793,
                        "accuracy": 0.55849609375,
                        "total_cost": 2586.4423839594456
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4457
Profiling... [2048/50176]	Loss: 1.3685
Profiling... [3072/50176]	Loss: 1.4951
Profiling... [4096/50176]	Loss: 1.3998
Profiling... [5120/50176]	Loss: 1.3256
Profiling... [6144/50176]	Loss: 1.2721
Profiling... [7168/50176]	Loss: 1.3369
Profiling... [8192/50176]	Loss: 1.2759
Profiling... [9216/50176]	Loss: 1.3117
Profiling... [10240/50176]	Loss: 1.3387
Profiling... [11264/50176]	Loss: 1.3911
Profiling... [12288/50176]	Loss: 1.4391
Profiling... [13312/50176]	Loss: 1.2240
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5610
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.25748217755148,
                        "time": 8.755200070998399,
                        "accuracy": 0.56103515625,
                        "total_cost": 1798.6436409598803
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.4161
Profiling... [2048/50176]	Loss: 1.4134
Profiling... [3072/50176]	Loss: 1.3819
Profiling... [4096/50176]	Loss: 1.2460
Profiling... [5120/50176]	Loss: 1.3795
Profiling... [6144/50176]	Loss: 1.3455
Profiling... [7168/50176]	Loss: 1.3667
Profiling... [8192/50176]	Loss: 1.3550
Profiling... [9216/50176]	Loss: 1.3304
Profiling... [10240/50176]	Loss: 1.2887
Profiling... [11264/50176]	Loss: 1.3196
Profiling... [12288/50176]	Loss: 1.2835
Profiling... [13312/50176]	Loss: 1.3183
Profile done
epoch 1 train time consumed: 12.92s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5614
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.29491979073352,
                        "time": 8.869837209997058,
                        "accuracy": 0.56142578125,
                        "total_cost": 1821.518006185211
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.3269
Profiling... [2048/50176]	Loss: 1.3297
Profiling... [3072/50176]	Loss: 1.3968
Profiling... [4096/50176]	Loss: 1.3019
Profiling... [5120/50176]	Loss: 1.4066
Profiling... [6144/50176]	Loss: 1.3102
Profiling... [7168/50176]	Loss: 1.2399
Profiling... [8192/50176]	Loss: 1.2758
Profiling... [9216/50176]	Loss: 1.3048
Profiling... [10240/50176]	Loss: 1.3895
Profiling... [11264/50176]	Loss: 1.3222
Profiling... [12288/50176]	Loss: 1.3428
Profiling... [13312/50176]	Loss: 1.3753
Profile done
epoch 1 train time consumed: 13.81s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5567
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.30145105911257,
                        "time": 9.532241467997665,
                        "accuracy": 0.55673828125,
                        "total_cost": 1974.1435251017708
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.3450
Profiling... [2048/50176]	Loss: 1.4418
Profiling... [3072/50176]	Loss: 1.2816
Profiling... [4096/50176]	Loss: 1.3923
Profiling... [5120/50176]	Loss: 1.3566
Profiling... [6144/50176]	Loss: 1.3556
Profiling... [7168/50176]	Loss: 1.3244
Profiling... [8192/50176]	Loss: 1.3569
Profiling... [9216/50176]	Loss: 1.2654
Profiling... [10240/50176]	Loss: 1.3168
Profiling... [11264/50176]	Loss: 1.3126
Profiling... [12288/50176]	Loss: 1.3877
Profiling... [13312/50176]	Loss: 1.4096
Profile done
epoch 1 train time consumed: 19.42s
Validation Epoch: 11, Average loss: 0.0015, Accuracy: 0.5611
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 115.24467229323999,
                        "time": 14.55458500799432,
                        "accuracy": 0.5611328125,
                        "total_cost": 2989.2003145162876
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.3877
Profiling... [2048/50176]	Loss: 1.3751
Profiling... [3072/50176]	Loss: 1.4199
Profiling... [4096/50176]	Loss: 1.3593
Profiling... [5120/50176]	Loss: 1.3334
Profiling... [6144/50176]	Loss: 1.3380
Profiling... [7168/50176]	Loss: 1.3232
Profiling... [8192/50176]	Loss: 1.2692
Profiling... [9216/50176]	Loss: 1.3130
Profiling... [10240/50176]	Loss: 1.3787
Profiling... [11264/50176]	Loss: 1.3672
Profiling... [12288/50176]	Loss: 1.2977
Profiling... [13312/50176]	Loss: 1.3316
Profile done
epoch 1 train time consumed: 12.82s
Validation Epoch: 11, Average loss: 0.0017, Accuracy: 0.5135
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.26057289272835,
                        "time": 8.761242970998865,
                        "accuracy": 0.5134765625,
                        "total_cost": 1966.6445517456673
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.3243
Profiling... [2048/50176]	Loss: 1.4390
Profiling... [3072/50176]	Loss: 1.3406
Profiling... [4096/50176]	Loss: 1.4063
Profiling... [5120/50176]	Loss: 1.3773
Profiling... [6144/50176]	Loss: 1.2911
Profiling... [7168/50176]	Loss: 1.2924
Profiling... [8192/50176]	Loss: 1.3878
Profiling... [9216/50176]	Loss: 1.3586
Profiling... [10240/50176]	Loss: 1.3479
Profiling... [11264/50176]	Loss: 1.4197
Profiling... [12288/50176]	Loss: 1.3070
Profiling... [13312/50176]	Loss: 1.3590
Profile done
epoch 1 train time consumed: 12.81s
Validation Epoch: 11, Average loss: 0.0035, Accuracy: 0.3007
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.2981852294089,
                        "time": 8.779590493999422,
                        "accuracy": 0.30068359375,
                        "total_cost": 3366.5649608310327
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.3728
Profiling... [2048/50176]	Loss: 1.3656
Profiling... [3072/50176]	Loss: 1.3125
Profiling... [4096/50176]	Loss: 1.3332
Profiling... [5120/50176]	Loss: 1.3209
Profiling... [6144/50176]	Loss: 1.3540
Profiling... [7168/50176]	Loss: 1.3519
Profiling... [8192/50176]	Loss: 1.4413
Profiling... [9216/50176]	Loss: 1.3206
Profiling... [10240/50176]	Loss: 1.3314
Profiling... [11264/50176]	Loss: 1.3528
Profiling... [12288/50176]	Loss: 1.2917
Profiling... [13312/50176]	Loss: 1.3873
Profile done
epoch 1 train time consumed: 13.89s
Validation Epoch: 11, Average loss: 0.0022, Accuracy: 0.4522
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.30510569958282,
                        "time": 9.549505162001878,
                        "accuracy": 0.45224609375,
                        "total_cost": 2434.75116159218
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.3235
Profiling... [2048/50176]	Loss: 1.3315
Profiling... [3072/50176]	Loss: 1.2770
Profiling... [4096/50176]	Loss: 1.3704
Profiling... [5120/50176]	Loss: 1.3546
Profiling... [6144/50176]	Loss: 1.3363
Profiling... [7168/50176]	Loss: 1.3543
Profiling... [8192/50176]	Loss: 1.3708
Profiling... [9216/50176]	Loss: 1.3110
Profiling... [10240/50176]	Loss: 1.3140
Profiling... [11264/50176]	Loss: 1.2843
Profiling... [12288/50176]	Loss: 1.2934
Profiling... [13312/50176]	Loss: 1.2947
Profile done
epoch 1 train time consumed: 17.52s
Validation Epoch: 11, Average loss: 0.0019, Accuracy: 0.4855
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 115.2541003270355,
                        "time": 12.593575591003173,
                        "accuracy": 0.485546875,
                        "total_cost": 2989.3328520373757
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.2789
Profiling... [2048/50176]	Loss: 1.3329
Profiling... [3072/50176]	Loss: 1.4358
Profiling... [4096/50176]	Loss: 1.3117
Profiling... [5120/50176]	Loss: 1.3155
Profiling... [6144/50176]	Loss: 1.4294
Profiling... [7168/50176]	Loss: 1.2940
Profiling... [8192/50176]	Loss: 1.2647
Profiling... [9216/50176]	Loss: 1.3458
Profiling... [10240/50176]	Loss: 1.3665
Profiling... [11264/50176]	Loss: 1.3707
Profiling... [12288/50176]	Loss: 1.4003
Profiling... [13312/50176]	Loss: 1.3258
Profile done
epoch 1 train time consumed: 13.09s
Validation Epoch: 11, Average loss: 0.0018, Accuracy: 0.5078
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.26574060223321,
                        "time": 8.955259391994332,
                        "accuracy": 0.5078125,
                        "total_cost": 2032.7081474034833
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.3417
Profiling... [2048/50176]	Loss: 1.3262
Profiling... [3072/50176]	Loss: 1.3233
Profiling... [4096/50176]	Loss: 1.3400
Profiling... [5120/50176]	Loss: 1.2736
Profiling... [6144/50176]	Loss: 1.3359
Profiling... [7168/50176]	Loss: 1.3377
Profiling... [8192/50176]	Loss: 1.3713
Profiling... [9216/50176]	Loss: 1.3876
Profiling... [10240/50176]	Loss: 1.3392
Profiling... [11264/50176]	Loss: 1.3501
Profiling... [12288/50176]	Loss: 1.2441
Profiling... [13312/50176]	Loss: 1.3439
Profile done
epoch 1 train time consumed: 13.06s
Validation Epoch: 11, Average loss: 0.0021, Accuracy: 0.4672
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.30240491332844,
                        "time": 8.793039830001362,
                        "accuracy": 0.4671875,
                        "total_cost": 2170.132203660932
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.4306
Profiling... [2048/50176]	Loss: 1.3978
Profiling... [3072/50176]	Loss: 1.3795
Profiling... [4096/50176]	Loss: 1.2899
Profiling... [5120/50176]	Loss: 1.3289
Profiling... [6144/50176]	Loss: 1.3136
Profiling... [7168/50176]	Loss: 1.3151
Profiling... [8192/50176]	Loss: 1.3069
Profiling... [9216/50176]	Loss: 1.3560
Profiling... [10240/50176]	Loss: 1.3007
Profiling... [11264/50176]	Loss: 1.3348
Profiling... [12288/50176]	Loss: 1.2776
Profiling... [13312/50176]	Loss: 1.3539
Profile done
epoch 1 train time consumed: 13.69s
Validation Epoch: 11, Average loss: 0.0021, Accuracy: 0.4537
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.31132467004831,
                        "time": 9.514035580999916,
                        "accuracy": 0.4537109375,
                        "total_cost": 2418.006609776898
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.3793
Profiling... [2048/50176]	Loss: 1.3048
Profiling... [3072/50176]	Loss: 1.4117
Profiling... [4096/50176]	Loss: 1.3493
Profiling... [5120/50176]	Loss: 1.3218
Profiling... [6144/50176]	Loss: 1.3036
Profiling... [7168/50176]	Loss: 1.4097
Profiling... [8192/50176]	Loss: 1.3308
Profiling... [9216/50176]	Loss: 1.4011
Profiling... [10240/50176]	Loss: 1.3244
Profiling... [11264/50176]	Loss: 1.3408
Profiling... [12288/50176]	Loss: 1.3058
Profiling... [13312/50176]	Loss: 1.3120
Profile done
epoch 1 train time consumed: 18.77s
Validation Epoch: 11, Average loss: 0.0022, Accuracy: 0.4530
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 115.25589614929824,
                        "time": 12.560387020996131,
                        "accuracy": 0.45302734375,
                        "total_cost": 3195.521599433089
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4168
Profiling... [2048/50176]	Loss: 1.3763
Profiling... [3072/50176]	Loss: 1.3560
Profiling... [4096/50176]	Loss: 1.3406
Profiling... [5120/50176]	Loss: 1.3564
Profiling... [6144/50176]	Loss: 1.3083
Profiling... [7168/50176]	Loss: 1.2549
Profiling... [8192/50176]	Loss: 1.3508
Profiling... [9216/50176]	Loss: 1.2539
Profiling... [10240/50176]	Loss: 1.2938
Profiling... [11264/50176]	Loss: 1.3754
Profiling... [12288/50176]	Loss: 1.2675
Profiling... [13312/50176]	Loss: 1.4314
Profile done
epoch 1 train time consumed: 13.12s
Validation Epoch: 11, Average loss: 0.0021, Accuracy: 0.4591
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.2728340910511,
                        "time": 8.96307128299668,
                        "accuracy": 0.45908203125,
                        "total_cost": 2250.5751883556013
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.4352
Profiling... [2048/50176]	Loss: 1.3677
Profiling... [3072/50176]	Loss: 1.2959
Profiling... [4096/50176]	Loss: 1.4274
Profiling... [5120/50176]	Loss: 1.3843
Profiling... [6144/50176]	Loss: 1.3337
Profiling... [7168/50176]	Loss: 1.3664
Profiling... [8192/50176]	Loss: 1.3358
Profiling... [9216/50176]	Loss: 1.3233
Profiling... [10240/50176]	Loss: 1.3545
Profiling... [11264/50176]	Loss: 1.2771
Profiling... [12288/50176]	Loss: 1.3168
Profiling... [13312/50176]	Loss: 1.3381
Profile done
epoch 1 train time consumed: 12.90s
Validation Epoch: 11, Average loss: 0.0018, Accuracy: 0.5040
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.3100027746833,
                        "time": 8.791388249999727,
                        "accuracy": 0.50400390625,
                        "total_cost": 2011.3633861360308
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.3608
Profiling... [2048/50176]	Loss: 1.4150
Profiling... [3072/50176]	Loss: 1.3439
Profiling... [4096/50176]	Loss: 1.3035
Profiling... [5120/50176]	Loss: 1.3304
Profiling... [6144/50176]	Loss: 1.3029
Profiling... [7168/50176]	Loss: 1.3636
Profiling... [8192/50176]	Loss: 1.3062
Profiling... [9216/50176]	Loss: 1.3254
Profiling... [10240/50176]	Loss: 1.3818
Profiling... [11264/50176]	Loss: 1.3420
Profiling... [12288/50176]	Loss: 1.3468
Profiling... [13312/50176]	Loss: 1.3198
Profile done
epoch 1 train time consumed: 13.78s
Validation Epoch: 11, Average loss: 0.0019, Accuracy: 0.5009
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.31731822098395,
                        "time": 9.533064869996451,
                        "accuracy": 0.50087890625,
                        "total_cost": 2194.7969090276774
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.3311
Profiling... [2048/50176]	Loss: 1.3292
Profiling... [3072/50176]	Loss: 1.4018
Profiling... [4096/50176]	Loss: 1.3514
Profiling... [5120/50176]	Loss: 1.3440
Profiling... [6144/50176]	Loss: 1.4482
Profiling... [7168/50176]	Loss: 1.2703
Profiling... [8192/50176]	Loss: 1.3637
Profiling... [9216/50176]	Loss: 1.2872
Profiling... [10240/50176]	Loss: 1.3033
Profiling... [11264/50176]	Loss: 1.3260
Profiling... [12288/50176]	Loss: 1.3518
Profiling... [13312/50176]	Loss: 1.3509
Profile done
epoch 1 train time consumed: 18.83s
Validation Epoch: 11, Average loss: 0.0019, Accuracy: 0.4863
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 115.2639956292904,
                        "time": 13.996723283002211,
                        "accuracy": 0.486328125,
                        "total_cost": 3317.3451593332265
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4201
Profiling... [2048/50176]	Loss: 1.4064
Profiling... [3072/50176]	Loss: 1.4731
Profiling... [4096/50176]	Loss: 1.4954
Profiling... [5120/50176]	Loss: 1.4367
Profiling... [6144/50176]	Loss: 1.4026
Profiling... [7168/50176]	Loss: 1.4345
Profiling... [8192/50176]	Loss: 1.3923
Profiling... [9216/50176]	Loss: 1.4192
Profiling... [10240/50176]	Loss: 1.4110
Profiling... [11264/50176]	Loss: 1.4154
Profiling... [12288/50176]	Loss: 1.3987
Profiling... [13312/50176]	Loss: 1.3941
Profile done
epoch 1 train time consumed: 12.94s
Validation Epoch: 11, Average loss: 0.0071, Accuracy: 0.1472
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.2782513810444,
                        "time": 8.757040106000204,
                        "accuracy": 0.14716796875,
                        "total_cost": 6859.483617717455
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.3051
Profiling... [2048/50176]	Loss: 1.4691
Profiling... [3072/50176]	Loss: 1.4406
Profiling... [4096/50176]	Loss: 1.4325
Profiling... [5120/50176]	Loss: 1.4934
Profiling... [6144/50176]	Loss: 1.4146
Profiling... [7168/50176]	Loss: 1.3973
Profiling... [8192/50176]	Loss: 1.4242
Profiling... [9216/50176]	Loss: 1.3184
Profiling... [10240/50176]	Loss: 1.3682
Profiling... [11264/50176]	Loss: 1.3778
Profiling... [12288/50176]	Loss: 1.3687
Profiling... [13312/50176]	Loss: 1.4144
Profile done
epoch 1 train time consumed: 13.12s
Validation Epoch: 11, Average loss: 0.0039, Accuracy: 0.2447
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.31321213016622,
                        "time": 9.081358178998926,
                        "accuracy": 0.2447265625,
                        "total_cost": 4279.063831188832
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.3882
Profiling... [2048/50176]	Loss: 1.4011
Profiling... [3072/50176]	Loss: 1.3606
Profiling... [4096/50176]	Loss: 1.4053
Profiling... [5120/50176]	Loss: 1.4225
Profiling... [6144/50176]	Loss: 1.4102
Profiling... [7168/50176]	Loss: 1.4675
Profiling... [8192/50176]	Loss: 1.4562
Profiling... [9216/50176]	Loss: 1.4190
Profiling... [10240/50176]	Loss: 1.4404
Profiling... [11264/50176]	Loss: 1.4241
Profiling... [12288/50176]	Loss: 1.4320
Profiling... [13312/50176]	Loss: 1.4241
Profile done
epoch 1 train time consumed: 13.77s
Validation Epoch: 11, Average loss: 0.0027, Accuracy: 0.3647
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.32062895001515,
                        "time": 9.52910080699803,
                        "accuracy": 0.36474609375,
                        "total_cost": 3012.7859275836586
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.3977
Profiling... [2048/50176]	Loss: 1.3550
Profiling... [3072/50176]	Loss: 1.3882
Profiling... [4096/50176]	Loss: 1.4442
Profiling... [5120/50176]	Loss: 1.3409
Profiling... [6144/50176]	Loss: 1.4999
Profiling... [7168/50176]	Loss: 1.5190
Profiling... [8192/50176]	Loss: 1.3074
Profiling... [9216/50176]	Loss: 1.3707
Profiling... [10240/50176]	Loss: 1.3483
Profiling... [11264/50176]	Loss: 1.3773
Profiling... [12288/50176]	Loss: 1.4200
Profiling... [13312/50176]	Loss: 1.4084
Profile done
epoch 1 train time consumed: 18.85s
Validation Epoch: 11, Average loss: 0.0046, Accuracy: 0.2155
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 115.26475644962676,
                        "time": 13.170410974998958,
                        "accuracy": 0.21552734375,
                        "total_cost": 7043.580582219031
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4135
Profiling... [2048/50176]	Loss: 1.3880
Profiling... [3072/50176]	Loss: 1.4575
Profiling... [4096/50176]	Loss: 1.4491
Profiling... [5120/50176]	Loss: 1.4403
Profiling... [6144/50176]	Loss: 1.3638
Profiling... [7168/50176]	Loss: 1.3839
Profiling... [8192/50176]	Loss: 1.4892
Profiling... [9216/50176]	Loss: 1.4684
Profiling... [10240/50176]	Loss: 1.3653
Profiling... [11264/50176]	Loss: 1.5269
Profiling... [12288/50176]	Loss: 1.3865
Profiling... [13312/50176]	Loss: 1.3868
Profile done
epoch 1 train time consumed: 12.75s
Validation Epoch: 11, Average loss: 0.0026, Accuracy: 0.3726
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.28125578445292,
                        "time": 8.755054019995441,
                        "accuracy": 0.37255859375,
                        "total_cost": 2709.0869431482474
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.3739
Profiling... [2048/50176]	Loss: 1.3244
Profiling... [3072/50176]	Loss: 1.3626
Profiling... [4096/50176]	Loss: 1.3378
Profiling... [5120/50176]	Loss: 1.4466
Profiling... [6144/50176]	Loss: 1.4653
Profiling... [7168/50176]	Loss: 1.3935
Profiling... [8192/50176]	Loss: 1.3552
Profiling... [9216/50176]	Loss: 1.4567
Profiling... [10240/50176]	Loss: 1.4360
Profiling... [11264/50176]	Loss: 1.4686
Profiling... [12288/50176]	Loss: 1.4463
Profiling... [13312/50176]	Loss: 1.4107
Profile done
epoch 1 train time consumed: 13.20s
Validation Epoch: 11, Average loss: 0.0074, Accuracy: 0.1000
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.31817413661027,
                        "time": 9.014681128996017,
                        "accuracy": 0.1,
                        "total_cost": 10395.565682195771
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.3955
Profiling... [2048/50176]	Loss: 1.3932
Profiling... [3072/50176]	Loss: 1.3956
Profiling... [4096/50176]	Loss: 1.5036
Profiling... [5120/50176]	Loss: 1.3685
Profiling... [6144/50176]	Loss: 1.3939
Profiling... [7168/50176]	Loss: 1.3695
Profiling... [8192/50176]	Loss: 1.4675
Profiling... [9216/50176]	Loss: 1.3804
Profiling... [10240/50176]	Loss: 1.3709
Profiling... [11264/50176]	Loss: 1.4709
Profiling... [12288/50176]	Loss: 1.3800
Profiling... [13312/50176]	Loss: 1.4372
Profile done
epoch 1 train time consumed: 13.77s
Validation Epoch: 11, Average loss: 0.0024, Accuracy: 0.3875
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.3229684372954,
                        "time": 9.531726198998513,
                        "accuracy": 0.3875,
                        "total_cost": 2836.7147344517343
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.4657
Profiling... [2048/50176]	Loss: 1.4245
Profiling... [3072/50176]	Loss: 1.4257
Profiling... [4096/50176]	Loss: 1.4120
Profiling... [5120/50176]	Loss: 1.4274
Profiling... [6144/50176]	Loss: 1.3913
Profiling... [7168/50176]	Loss: 1.3837
Profiling... [8192/50176]	Loss: 1.4531
Profiling... [9216/50176]	Loss: 1.3594
Profiling... [10240/50176]	Loss: 1.4587
Profiling... [11264/50176]	Loss: 1.3291
Profiling... [12288/50176]	Loss: 1.4661
Profiling... [13312/50176]	Loss: 1.3040
Profile done
epoch 1 train time consumed: 21.08s
Validation Epoch: 11, Average loss: 0.0045, Accuracy: 0.2275
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 115.26280663401417,
                        "time": 15.434479679002834,
                        "accuracy": 0.2275390625,
                        "total_cost": 7818.532023430152
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.4804
Profiling... [2048/50176]	Loss: 1.3900
Profiling... [3072/50176]	Loss: 1.3331
Profiling... [4096/50176]	Loss: 1.4494
Profiling... [5120/50176]	Loss: 1.4032
Profiling... [6144/50176]	Loss: 1.4616
Profiling... [7168/50176]	Loss: 1.3505
Profiling... [8192/50176]	Loss: 1.4079
Profiling... [9216/50176]	Loss: 1.3546
Profiling... [10240/50176]	Loss: 1.3466
Profiling... [11264/50176]	Loss: 1.4958
Profiling... [12288/50176]	Loss: 1.4301
Profiling... [13312/50176]	Loss: 1.4223
Profile done
epoch 1 train time consumed: 12.93s
Validation Epoch: 11, Average loss: 0.0041, Accuracy: 0.2512
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.27827423035451,
                        "time": 8.774133739003446,
                        "accuracy": 0.251171875,
                        "total_cost": 4026.991458731775
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.4787
Profiling... [2048/50176]	Loss: 1.3557
Profiling... [3072/50176]	Loss: 1.4148
Profiling... [4096/50176]	Loss: 1.4423
Profiling... [5120/50176]	Loss: 1.4244
Profiling... [6144/50176]	Loss: 1.4531
Profiling... [7168/50176]	Loss: 1.3604
Profiling... [8192/50176]	Loss: 1.4292
Profiling... [9216/50176]	Loss: 1.4100
Profiling... [10240/50176]	Loss: 1.4677
Profiling... [11264/50176]	Loss: 1.3285
Profiling... [12288/50176]	Loss: 1.4451
Profiling... [13312/50176]	Loss: 1.4052
Profile done
epoch 1 train time consumed: 13.07s
Validation Epoch: 11, Average loss: 0.0030, Accuracy: 0.3642
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.3135821148047,
                        "time": 9.016499137003848,
                        "accuracy": 0.36416015625,
                        "total_cost": 2855.130622552722
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.3462
Profiling... [2048/50176]	Loss: 1.3888
Profiling... [3072/50176]	Loss: 1.3588
Profiling... [4096/50176]	Loss: 1.3940
Profiling... [5120/50176]	Loss: 1.4745
Profiling... [6144/50176]	Loss: 1.4232
Profiling... [7168/50176]	Loss: 1.3925
Profiling... [8192/50176]	Loss: 1.4132
Profiling... [9216/50176]	Loss: 1.4276
Profiling... [10240/50176]	Loss: 1.3603
Profiling... [11264/50176]	Loss: 1.4007
Profiling... [12288/50176]	Loss: 1.4142
Profiling... [13312/50176]	Loss: 1.3433
Profile done
epoch 1 train time consumed: 13.76s
Validation Epoch: 11, Average loss: 0.0041, Accuracy: 0.2401
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.3197207515105,
                        "time": 9.522700198001985,
                        "accuracy": 0.24013671875,
                        "total_cost": 4573.041279776971
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.4054
Profiling... [2048/50176]	Loss: 1.4289
Profiling... [3072/50176]	Loss: 1.3874
Profiling... [4096/50176]	Loss: 1.4424
Profiling... [5120/50176]	Loss: 1.3759
Profiling... [6144/50176]	Loss: 1.3948
Profiling... [7168/50176]	Loss: 1.3840
Profiling... [8192/50176]	Loss: 1.4256
Profiling... [9216/50176]	Loss: 1.4169
Profiling... [10240/50176]	Loss: 1.4322
Profiling... [11264/50176]	Loss: 1.3658
Profiling... [12288/50176]	Loss: 1.3419
Profiling... [13312/50176]	Loss: 1.4501
Profile done
epoch 1 train time consumed: 20.35s
Validation Epoch: 11, Average loss: 0.0037, Accuracy: 0.2603
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 115.26237636569059,
                        "time": 14.679451735995826,
                        "accuracy": 0.26025390625,
                        "total_cost": 6501.299116759525
                    },
                    
[Training Loop] The optimal parameters are lr: 0.01 dr: 0.5 bs: 256 pl: 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[GPU_0] Set GPU power limit to 100W.
[GPU_0] Set GPU power limit to 100W.
Training Epoch: 11 [256/50176]	Loss: 1.4972
Training Epoch: 11 [512/50176]	Loss: 1.5042
Training Epoch: 11 [768/50176]	Loss: 1.6246
Training Epoch: 11 [1024/50176]	Loss: 1.6851
Training Epoch: 11 [1280/50176]	Loss: 1.4913
Training Epoch: 11 [1536/50176]	Loss: 1.6247
Training Epoch: 11 [1792/50176]	Loss: 1.6607
Training Epoch: 11 [2048/50176]	Loss: 1.6888
Training Epoch: 11 [2304/50176]	Loss: 1.6817
Training Epoch: 11 [2560/50176]	Loss: 1.7933
Training Epoch: 11 [2816/50176]	Loss: 1.7454
Training Epoch: 11 [3072/50176]	Loss: 1.6139
Training Epoch: 11 [3328/50176]	Loss: 1.6552
Training Epoch: 11 [3584/50176]	Loss: 1.6112
Training Epoch: 11 [3840/50176]	Loss: 1.7704
Training Epoch: 11 [4096/50176]	Loss: 1.6151
Training Epoch: 11 [4352/50176]	Loss: 1.7174
Training Epoch: 11 [4608/50176]	Loss: 1.6434
Training Epoch: 11 [4864/50176]	Loss: 1.7195
Training Epoch: 11 [5120/50176]	Loss: 1.7525
Training Epoch: 11 [5376/50176]	Loss: 1.5869
Training Epoch: 11 [5632/50176]	Loss: 1.6286
Training Epoch: 11 [5888/50176]	Loss: 1.6703
Training Epoch: 11 [6144/50176]	Loss: 1.6207
Training Epoch: 11 [6400/50176]	Loss: 1.5840
Training Epoch: 11 [6656/50176]	Loss: 1.7749
Training Epoch: 11 [6912/50176]	Loss: 1.6724
Training Epoch: 11 [7168/50176]	Loss: 1.6156
Training Epoch: 11 [7424/50176]	Loss: 1.6974
Training Epoch: 11 [7680/50176]	Loss: 1.9240
Training Epoch: 11 [7936/50176]	Loss: 1.5866
Training Epoch: 11 [8192/50176]	Loss: 1.9634
Training Epoch: 11 [8448/50176]	Loss: 1.6916
Training Epoch: 11 [8704/50176]	Loss: 1.5951
Training Epoch: 11 [8960/50176]	Loss: 1.7976
Training Epoch: 11 [9216/50176]	Loss: 1.6072
Training Epoch: 11 [9472/50176]	Loss: 1.5741
Training Epoch: 11 [9728/50176]	Loss: 1.4270
Training Epoch: 11 [9984/50176]	Loss: 1.7197
Training Epoch: 11 [10240/50176]	Loss: 1.5921
Training Epoch: 11 [10496/50176]	Loss: 1.6113
Training Epoch: 11 [10752/50176]	Loss: 1.7591
Training Epoch: 11 [11008/50176]	Loss: 1.6483
Training Epoch: 11 [11264/50176]	Loss: 1.6397
Training Epoch: 11 [11520/50176]	Loss: 1.8361
Training Epoch: 11 [11776/50176]	Loss: 1.6135
Training Epoch: 11 [12032/50176]	Loss: 1.6809
Training Epoch: 11 [12288/50176]	Loss: 1.7531
Training Epoch: 11 [12544/50176]	Loss: 1.7694
Training Epoch: 11 [12800/50176]	Loss: 1.6633
Training Epoch: 11 [13056/50176]	Loss: 1.8463
Training Epoch: 11 [13312/50176]	Loss: 1.6257
Training Epoch: 11 [13568/50176]	Loss: 1.5894
Training Epoch: 11 [13824/50176]	Loss: 1.6406
Training Epoch: 11 [14080/50176]	Loss: 1.7764
Training Epoch: 11 [14336/50176]	Loss: 1.5647
Training Epoch: 11 [14592/50176]	Loss: 1.6470
Training Epoch: 11 [14848/50176]	Loss: 1.7062
Training Epoch: 11 [15104/50176]	Loss: 1.5782
Training Epoch: 11 [15360/50176]	Loss: 1.7554
Training Epoch: 11 [15616/50176]	Loss: 1.5875
Training Epoch: 11 [15872/50176]	Loss: 1.8492
Training Epoch: 11 [16128/50176]	Loss: 1.7368
Training Epoch: 11 [16384/50176]	Loss: 1.6470
Training Epoch: 11 [16640/50176]	Loss: 1.6076
Training Epoch: 11 [16896/50176]	Loss: 1.6656
Training Epoch: 11 [17152/50176]	Loss: 1.5926
Training Epoch: 11 [17408/50176]	Loss: 1.6250
Training Epoch: 11 [17664/50176]	Loss: 1.5073
Training Epoch: 11 [17920/50176]	Loss: 1.5455
Training Epoch: 11 [18176/50176]	Loss: 1.6589
Training Epoch: 11 [18432/50176]	Loss: 1.7576
Training Epoch: 11 [18688/50176]	Loss: 1.5847
Training Epoch: 11 [18944/50176]	Loss: 1.8145
Training Epoch: 11 [19200/50176]	Loss: 1.6407
Training Epoch: 11 [19456/50176]	Loss: 1.7451
Training Epoch: 11 [19712/50176]	Loss: 1.7093
Training Epoch: 11 [19968/50176]	Loss: 1.5853
Training Epoch: 11 [20224/50176]	Loss: 1.4577
Training Epoch: 11 [20480/50176]	Loss: 1.7664
Training Epoch: 11 [20736/50176]	Loss: 1.6051
Training Epoch: 11 [20992/50176]	Loss: 1.7129
Training Epoch: 11 [21248/50176]	Loss: 1.6316
Training Epoch: 11 [21504/50176]	Loss: 1.6559
Training Epoch: 11 [21760/50176]	Loss: 1.9600
Training Epoch: 11 [22016/50176]	Loss: 1.5989
Training Epoch: 11 [22272/50176]	Loss: 1.6411
Training Epoch: 11 [22528/50176]	Loss: 1.6726
Training Epoch: 11 [22784/50176]	Loss: 1.6722
Training Epoch: 11 [23040/50176]	Loss: 1.7279
Training Epoch: 11 [23296/50176]	Loss: 1.6310
Training Epoch: 11 [23552/50176]	Loss: 1.6978
Training Epoch: 11 [23808/50176]	Loss: 1.6551
Training Epoch: 11 [24064/50176]	Loss: 1.5859
Training Epoch: 11 [24320/50176]	Loss: 1.5666
Training Epoch: 11 [24576/50176]	Loss: 1.6862
Training Epoch: 11 [24832/50176]	Loss: 1.5591
Training Epoch: 11 [25088/50176]	Loss: 1.4853
Training Epoch: 11 [25344/50176]	Loss: 1.8256
Training Epoch: 11 [25600/50176]	Loss: 1.9084
Training Epoch: 11 [25856/50176]	Loss: 1.7461
Training Epoch: 11 [26112/50176]	Loss: 1.5818
Training Epoch: 11 [26368/50176]	Loss: 1.6518
Training Epoch: 11 [26624/50176]	Loss: 1.5333
Training Epoch: 11 [26880/50176]	Loss: 1.7692
Training Epoch: 11 [27136/50176]	Loss: 1.6421
Training Epoch: 11 [27392/50176]	Loss: 1.5431
Training Epoch: 11 [27648/50176]	Loss: 1.5440
Training Epoch: 11 [27904/50176]	Loss: 1.6696
Training Epoch: 11 [28160/50176]	Loss: 1.7602
Training Epoch: 11 [28416/50176]	Loss: 1.7201
Training Epoch: 11 [28672/50176]	Loss: 1.5484
Training Epoch: 11 [28928/50176]	Loss: 1.6427
Training Epoch: 11 [29184/50176]	Loss: 1.4647
Training Epoch: 11 [29440/50176]	Loss: 1.6420
Training Epoch: 11 [29696/50176]	Loss: 1.5941
Training Epoch: 11 [29952/50176]	Loss: 1.4854
Training Epoch: 11 [30208/50176]	Loss: 1.6575
Training Epoch: 11 [30464/50176]	Loss: 1.5993
Training Epoch: 11 [30720/50176]	Loss: 1.6115
Training Epoch: 11 [30976/50176]	Loss: 1.5233
Training Epoch: 11 [31232/50176]	Loss: 1.6391
Training Epoch: 11 [31488/50176]	Loss: 1.7211
Training Epoch: 11 [31744/50176]	Loss: 1.6602
Training Epoch: 11 [32000/50176]	Loss: 1.6304
Training Epoch: 11 [32256/50176]	Loss: 1.7839
Training Epoch: 11 [32512/50176]	Loss: 1.5473
Training Epoch: 11 [32768/50176]	Loss: 1.7872
Training Epoch: 11 [33024/50176]	Loss: 1.6973
Training Epoch: 11 [33280/50176]	Loss: 1.6401
Training Epoch: 11 [33536/50176]	Loss: 1.8707
Training Epoch: 11 [33792/50176]	Loss: 1.6406
Training Epoch: 11 [34048/50176]	Loss: 1.7782
Training Epoch: 11 [34304/50176]	Loss: 1.6457
Training Epoch: 11 [34560/50176]	Loss: 1.6735
Training Epoch: 11 [34816/50176]	Loss: 1.7066
Training Epoch: 11 [35072/50176]	Loss: 1.5944
Training Epoch: 11 [35328/50176]	Loss: 1.6061
Training Epoch: 11 [35584/50176]	Loss: 1.5151
Training Epoch: 11 [35840/50176]	Loss: 1.6206
Training Epoch: 11 [36096/50176]	Loss: 1.5011
Training Epoch: 11 [36352/50176]	Loss: 1.7150
Training Epoch: 11 [36608/50176]	Loss: 1.7338
Training Epoch: 11 [36864/50176]	Loss: 1.5428
Training Epoch: 11 [37120/50176]	Loss: 1.6243
Training Epoch: 11 [37376/50176]	Loss: 1.6547
Training Epoch: 11 [37632/50176]	Loss: 1.6632
Training Epoch: 11 [37888/50176]	Loss: 1.6807
Training Epoch: 11 [38144/50176]	Loss: 1.7102
Training Epoch: 11 [38400/50176]	Loss: 1.7065
Training Epoch: 11 [38656/50176]	Loss: 1.7613
Training Epoch: 11 [38912/50176]	Loss: 1.5313
Training Epoch: 11 [39168/50176]	Loss: 1.6195
Training Epoch: 11 [39424/50176]	Loss: 1.7070
Training Epoch: 11 [39680/50176]	Loss: 1.5975
Training Epoch: 11 [39936/50176]	Loss: 1.7357
Training Epoch: 11 [40192/50176]	Loss: 1.5984
Training Epoch: 11 [40448/50176]	Loss: 1.6190
Training Epoch: 11 [40704/50176]	Loss: 1.5022
Training Epoch: 11 [40960/50176]	Loss: 1.6557
Training Epoch: 11 [41216/50176]	Loss: 1.6810
Training Epoch: 11 [41472/50176]	Loss: 1.5743
Training Epoch: 11 [41728/50176]	Loss: 1.5907
Training Epoch: 11 [41984/50176]	Loss: 1.5896
Training Epoch: 11 [42240/50176]	Loss: 1.7560
Training Epoch: 11 [42496/50176]	Loss: 1.7248
Training Epoch: 11 [42752/50176]	Loss: 1.9673
Training Epoch: 11 [43008/50176]	Loss: 1.6056
Training Epoch: 11 [43264/50176]	Loss: 1.6531
Training Epoch: 11 [43520/50176]	Loss: 1.5748
Training Epoch: 11 [43776/50176]	Loss: 1.6685
Training Epoch: 11 [44032/50176]	Loss: 1.7886
Training Epoch: 11 [44288/50176]	Loss: 1.8011
Training Epoch: 11 [44544/50176]	Loss: 1.5302
Training Epoch: 11 [44800/50176]	Loss: 1.7063
Training Epoch: 11 [45056/50176]	Loss: 1.8256
Training Epoch: 11 [45312/50176]	Loss: 1.6858
Training Epoch: 11 [45568/50176]	Loss: 1.6677
Training Epoch: 11 [45824/50176]	Loss: 1.6591
Training Epoch: 11 [46080/50176]	Loss: 1.7626
Training Epoch: 11 [46336/50176]	Loss: 1.6631
Training Epoch: 11 [46592/50176]	Loss: 1.5862
Training Epoch: 11 [46848/50176]	Loss: 1.6476
Training Epoch: 11 [47104/50176]	Loss: 1.9509
Training Epoch: 11 [47360/50176]	Loss: 1.6218
Training Epoch: 11 [47616/50176]	Loss: 1.6107
Training Epoch: 11 [47872/50176]	Loss: 1.8195
Training Epoch: 11 [48128/50176]	Loss: 1.6873
Training Epoch: 11 [48384/50176]	Loss: 1.6627
Training Epoch: 11 [48640/50176]	Loss: 1.6791
Training Epoch: 11 [48896/50176]	Loss: 1.7408
Training Epoch: 11 [49152/50176]	Loss: 1.7742
Training Epoch: 11 [49408/50176]	Loss: 1.7490
Training Epoch: 11 [49664/50176]	Loss: 1.7475
Training Epoch: 11 [49920/50176]	Loss: 1.8466
Training Epoch: 11 [50176/50176]	Loss: 1.5167
Validation Epoch: 11, Average loss: 0.0100, Accuracy: 0.3935
Training Epoch: 12 [256/50176]	Loss: 1.5600
Training Epoch: 12 [512/50176]	Loss: 1.5859
Training Epoch: 12 [768/50176]	Loss: 1.5767
Training Epoch: 12 [1024/50176]	Loss: 1.3213
Training Epoch: 12 [1280/50176]	Loss: 1.6899
Training Epoch: 12 [1536/50176]	Loss: 1.5118
Training Epoch: 12 [1792/50176]	Loss: 1.4848
Training Epoch: 12 [2048/50176]	Loss: 1.4727
Training Epoch: 12 [2304/50176]	Loss: 1.7050
Training Epoch: 12 [2560/50176]	Loss: 1.6039
Training Epoch: 12 [2816/50176]	Loss: 1.6181
Training Epoch: 12 [3072/50176]	Loss: 1.6223
Training Epoch: 12 [3328/50176]	Loss: 1.5762
Training Epoch: 12 [3584/50176]	Loss: 1.5321
Training Epoch: 12 [3840/50176]	Loss: 1.6021
Training Epoch: 12 [4096/50176]	Loss: 1.6674
Training Epoch: 12 [4352/50176]	Loss: 1.8370
Training Epoch: 12 [4608/50176]	Loss: 1.6191
Training Epoch: 12 [4864/50176]	Loss: 1.6077
Training Epoch: 12 [5120/50176]	Loss: 1.7488
Training Epoch: 12 [5376/50176]	Loss: 1.4649
Training Epoch: 12 [5632/50176]	Loss: 1.7213
Training Epoch: 12 [5888/50176]	Loss: 1.5078
Training Epoch: 12 [6144/50176]	Loss: 1.5079
Training Epoch: 12 [6400/50176]	Loss: 1.6162
Training Epoch: 12 [6656/50176]	Loss: 1.5743
Training Epoch: 12 [6912/50176]	Loss: 1.5710
Training Epoch: 12 [7168/50176]	Loss: 1.3878
Training Epoch: 12 [7424/50176]	Loss: 1.4138
Training Epoch: 12 [7680/50176]	Loss: 1.6318
Training Epoch: 12 [7936/50176]	Loss: 1.7673
Training Epoch: 12 [8192/50176]	Loss: 1.4646
Training Epoch: 12 [8448/50176]	Loss: 1.5966
Training Epoch: 12 [8704/50176]	Loss: 1.6779
Training Epoch: 12 [8960/50176]	Loss: 1.4731
Training Epoch: 12 [9216/50176]	Loss: 1.6030
Training Epoch: 12 [9472/50176]	Loss: 1.4198
Training Epoch: 12 [9728/50176]	Loss: 1.8055
Training Epoch: 12 [9984/50176]	Loss: 1.6768
Training Epoch: 12 [10240/50176]	Loss: 1.5270
Training Epoch: 12 [10496/50176]	Loss: 1.5645
Training Epoch: 12 [10752/50176]	Loss: 1.5885
Training Epoch: 12 [11008/50176]	Loss: 1.5854
Training Epoch: 12 [11264/50176]	Loss: 1.7112
Training Epoch: 12 [11520/50176]	Loss: 1.7862
Training Epoch: 12 [11776/50176]	Loss: 1.5289
Training Epoch: 12 [12032/50176]	Loss: 1.5481
Training Epoch: 12 [12288/50176]	Loss: 1.5553
Training Epoch: 12 [12544/50176]	Loss: 1.5421
Training Epoch: 12 [12800/50176]	Loss: 1.4935
Training Epoch: 12 [13056/50176]	Loss: 1.5134
Training Epoch: 12 [13312/50176]	Loss: 1.6424
Training Epoch: 12 [13568/50176]	Loss: 1.5443
Training Epoch: 12 [13824/50176]	Loss: 1.4437
Training Epoch: 12 [14080/50176]	Loss: 1.5422
Training Epoch: 12 [14336/50176]	Loss: 1.5112
Training Epoch: 12 [14592/50176]	Loss: 1.4332
Training Epoch: 12 [14848/50176]	Loss: 1.5941
Training Epoch: 12 [15104/50176]	Loss: 1.6453
Training Epoch: 12 [15360/50176]	Loss: 1.5966
Training Epoch: 12 [15616/50176]	Loss: 1.5551
Training Epoch: 12 [15872/50176]	Loss: 1.7962
Training Epoch: 12 [16128/50176]	Loss: 1.4227
Training Epoch: 12 [16384/50176]	Loss: 1.6112
Training Epoch: 12 [16640/50176]	Loss: 1.5414
Training Epoch: 12 [16896/50176]	Loss: 1.4869
Training Epoch: 12 [17152/50176]	Loss: 1.4565
Training Epoch: 12 [17408/50176]	Loss: 1.4858
Training Epoch: 12 [17664/50176]	Loss: 1.6132
Training Epoch: 12 [17920/50176]	Loss: 1.5834
Training Epoch: 12 [18176/50176]	Loss: 1.5272
Training Epoch: 12 [18432/50176]	Loss: 1.5622
Training Epoch: 12 [18688/50176]	Loss: 1.5471
Training Epoch: 12 [18944/50176]	Loss: 1.6440
Training Epoch: 12 [19200/50176]	Loss: 1.6072
Training Epoch: 12 [19456/50176]	Loss: 1.6909
Training Epoch: 12 [19712/50176]	Loss: 1.4737
Training Epoch: 12 [19968/50176]	Loss: 1.4792
Training Epoch: 12 [20224/50176]	Loss: 1.6528
Training Epoch: 12 [20480/50176]	Loss: 1.2994
Training Epoch: 12 [20736/50176]	Loss: 1.4690
Training Epoch: 12 [20992/50176]	Loss: 1.6400
Training Epoch: 12 [21248/50176]	Loss: 1.5915
Training Epoch: 12 [21504/50176]	Loss: 1.6048
Training Epoch: 12 [21760/50176]	Loss: 1.4422
Training Epoch: 12 [22016/50176]	Loss: 1.5197
Training Epoch: 12 [22272/50176]	Loss: 1.3629
Training Epoch: 12 [22528/50176]	Loss: 1.8818
Training Epoch: 12 [22784/50176]	Loss: 1.7010
Training Epoch: 12 [23040/50176]	Loss: 1.7094
Training Epoch: 12 [23296/50176]	Loss: 1.6570
Training Epoch: 12 [23552/50176]	Loss: 1.5215
Training Epoch: 12 [23808/50176]	Loss: 1.5017
Training Epoch: 12 [24064/50176]	Loss: 1.5683
Training Epoch: 12 [24320/50176]	Loss: 1.6756
Training Epoch: 12 [24576/50176]	Loss: 1.3866
Training Epoch: 12 [24832/50176]	Loss: 1.4605
Training Epoch: 12 [25088/50176]	Loss: 1.6382
Training Epoch: 12 [25344/50176]	Loss: 1.5999
Training Epoch: 12 [25600/50176]	Loss: 1.6395
Training Epoch: 12 [25856/50176]	Loss: 1.5597
Training Epoch: 12 [26112/50176]	Loss: 1.6461
Training Epoch: 12 [26368/50176]	Loss: 1.5644
Training Epoch: 12 [26624/50176]	Loss: 1.4506
Training Epoch: 12 [26880/50176]	Loss: 1.5077
Training Epoch: 12 [27136/50176]	Loss: 1.6640
Training Epoch: 12 [27392/50176]	Loss: 1.5099
Training Epoch: 12 [27648/50176]	Loss: 1.6090
Training Epoch: 12 [27904/50176]	Loss: 1.6491
Training Epoch: 12 [28160/50176]	Loss: 1.4727
Training Epoch: 12 [28416/50176]	Loss: 1.5407
Training Epoch: 12 [28672/50176]	Loss: 1.6689
Training Epoch: 12 [28928/50176]	Loss: 1.5528
Training Epoch: 12 [29184/50176]	Loss: 1.8889
Training Epoch: 12 [29440/50176]	Loss: 1.6227
Training Epoch: 12 [29696/50176]	Loss: 1.6975
Training Epoch: 12 [29952/50176]	Loss: 1.5443
Training Epoch: 12 [30208/50176]	Loss: 1.4557
Training Epoch: 12 [30464/50176]	Loss: 1.6552
Training Epoch: 12 [30720/50176]	Loss: 1.7006
Training Epoch: 12 [30976/50176]	Loss: 1.4840
Training Epoch: 12 [31232/50176]	Loss: 1.5971
Training Epoch: 12 [31488/50176]	Loss: 1.6277
Training Epoch: 12 [31744/50176]	Loss: 1.6572
Training Epoch: 12 [32000/50176]	Loss: 1.5030
Training Epoch: 12 [32256/50176]	Loss: 1.6625
Training Epoch: 12 [32512/50176]	Loss: 1.7769
Training Epoch: 12 [32768/50176]	Loss: 1.8067
Training Epoch: 12 [33024/50176]	Loss: 1.6966
Training Epoch: 12 [33280/50176]	Loss: 1.5579
Training Epoch: 12 [33536/50176]	Loss: 1.5520
Training Epoch: 12 [33792/50176]	Loss: 1.4822
Training Epoch: 12 [34048/50176]	Loss: 1.4477
Training Epoch: 12 [34304/50176]	Loss: 1.6214
Training Epoch: 12 [34560/50176]	Loss: 1.4420
Training Epoch: 12 [34816/50176]	Loss: 1.5123
Training Epoch: 12 [35072/50176]	Loss: 1.5279
Training Epoch: 12 [35328/50176]	Loss: 1.4558
Training Epoch: 12 [35584/50176]	Loss: 1.6302
Training Epoch: 12 [35840/50176]	Loss: 1.6608
Training Epoch: 12 [36096/50176]	Loss: 1.5963
Training Epoch: 12 [36352/50176]	Loss: 1.6095
Training Epoch: 12 [36608/50176]	Loss: 1.4152
Training Epoch: 12 [36864/50176]	Loss: 1.6960
Training Epoch: 12 [37120/50176]	Loss: 1.5574
Training Epoch: 12 [37376/50176]	Loss: 1.5995
Training Epoch: 12 [37632/50176]	Loss: 1.6748
Training Epoch: 12 [37888/50176]	Loss: 1.6003
Training Epoch: 12 [38144/50176]	Loss: 1.6722
Training Epoch: 12 [38400/50176]	Loss: 1.5813
Training Epoch: 12 [38656/50176]	Loss: 1.4372
Training Epoch: 12 [38912/50176]	Loss: 1.5662
Training Epoch: 12 [39168/50176]	Loss: 1.4886
Training Epoch: 12 [39424/50176]	Loss: 1.6063
Training Epoch: 12 [39680/50176]	Loss: 1.3152
Training Epoch: 12 [39936/50176]	Loss: 1.6588
Training Epoch: 12 [40192/50176]	Loss: 1.7086
Training Epoch: 12 [40448/50176]	Loss: 1.8197
Training Epoch: 12 [40704/50176]	Loss: 1.4692
Training Epoch: 12 [40960/50176]	Loss: 1.6537
Training Epoch: 12 [41216/50176]	Loss: 1.6781
Training Epoch: 12 [41472/50176]	Loss: 1.5641
Training Epoch: 12 [41728/50176]	Loss: 1.5764
Training Epoch: 12 [41984/50176]	Loss: 1.7815
Training Epoch: 12 [42240/50176]	Loss: 1.5712
Training Epoch: 12 [42496/50176]	Loss: 1.6128
Training Epoch: 12 [42752/50176]	Loss: 1.7495
Training Epoch: 12 [43008/50176]	Loss: 1.6610
Training Epoch: 12 [43264/50176]	Loss: 1.4649
Training Epoch: 12 [43520/50176]	Loss: 1.6013
Training Epoch: 12 [43776/50176]	Loss: 1.5905
Training Epoch: 12 [44032/50176]	Loss: 1.7833
Training Epoch: 12 [44288/50176]	Loss: 1.5905
Training Epoch: 12 [44544/50176]	Loss: 1.6832
Training Epoch: 12 [44800/50176]	Loss: 1.5755
Training Epoch: 12 [45056/50176]	Loss: 1.6041
Training Epoch: 12 [45312/50176]	Loss: 1.4524
Training Epoch: 12 [45568/50176]	Loss: 1.7045
Training Epoch: 12 [45824/50176]	Loss: 1.5781
Training Epoch: 12 [46080/50176]	Loss: 1.6715
Training Epoch: 12 [46336/50176]	Loss: 1.6374
Training Epoch: 12 [46592/50176]	Loss: 1.4589
Training Epoch: 12 [46848/50176]	Loss: 1.4815
Training Epoch: 12 [47104/50176]	Loss: 1.7412
Training Epoch: 12 [47360/50176]	Loss: 1.6157
Training Epoch: 12 [47616/50176]	Loss: 1.5616
Training Epoch: 12 [47872/50176]	Loss: 1.4449
Training Epoch: 12 [48128/50176]	Loss: 1.5873
Training Epoch: 12 [48384/50176]	Loss: 1.4185
Training Epoch: 12 [48640/50176]	Loss: 1.4885
Training Epoch: 12 [48896/50176]	Loss: 1.5646
Training Epoch: 12 [49152/50176]	Loss: 1.6981
Training Epoch: 12 [49408/50176]	Loss: 1.7235
Training Epoch: 12 [49664/50176]	Loss: 1.5071
Training Epoch: 12 [49920/50176]	Loss: 1.6645
Training Epoch: 12 [50176/50176]	Loss: 1.7183
Validation Epoch: 12, Average loss: 0.0108, Accuracy: 0.3881
Training Epoch: 13 [256/50176]	Loss: 1.3261
Training Epoch: 13 [512/50176]	Loss: 1.4793
Training Epoch: 13 [768/50176]	Loss: 1.6803
Training Epoch: 13 [1024/50176]	Loss: 1.4289
Training Epoch: 13 [1280/50176]	Loss: 1.5430
Training Epoch: 13 [1536/50176]	Loss: 1.6732
Training Epoch: 13 [1792/50176]	Loss: 1.4946
Training Epoch: 13 [2048/50176]	Loss: 1.5186
Training Epoch: 13 [2304/50176]	Loss: 1.5961
Training Epoch: 13 [2560/50176]	Loss: 1.4206
Training Epoch: 13 [2816/50176]	Loss: 1.5734
Training Epoch: 13 [3072/50176]	Loss: 1.5489
Training Epoch: 13 [3328/50176]	Loss: 1.6324
Training Epoch: 13 [3584/50176]	Loss: 1.6075
Training Epoch: 13 [3840/50176]	Loss: 1.4654
Training Epoch: 13 [4096/50176]	Loss: 1.3851
Training Epoch: 13 [4352/50176]	Loss: 1.4220
Training Epoch: 13 [4608/50176]	Loss: 1.5383
Training Epoch: 13 [4864/50176]	Loss: 1.4050
Training Epoch: 13 [5120/50176]	Loss: 1.4196
Training Epoch: 13 [5376/50176]	Loss: 1.6424
Training Epoch: 13 [5632/50176]	Loss: 1.5985
Training Epoch: 13 [5888/50176]	Loss: 1.6218
Training Epoch: 13 [6144/50176]	Loss: 1.5365
Training Epoch: 13 [6400/50176]	Loss: 1.4865
Training Epoch: 13 [6656/50176]	Loss: 1.4627
Training Epoch: 13 [6912/50176]	Loss: 1.5931
Training Epoch: 13 [7168/50176]	Loss: 1.5055
Training Epoch: 13 [7424/50176]	Loss: 1.6794
Training Epoch: 13 [7680/50176]	Loss: 1.4677
Training Epoch: 13 [7936/50176]	Loss: 1.5708
Training Epoch: 13 [8192/50176]	Loss: 1.3222
Training Epoch: 13 [8448/50176]	Loss: 1.5086
Training Epoch: 13 [8704/50176]	Loss: 1.5115
Training Epoch: 13 [8960/50176]	Loss: 1.5317
Training Epoch: 13 [9216/50176]	Loss: 1.5909
Training Epoch: 13 [9472/50176]	Loss: 1.5543
Training Epoch: 13 [9728/50176]	Loss: 1.5345
Training Epoch: 13 [9984/50176]	Loss: 1.5679
Training Epoch: 13 [10240/50176]	Loss: 1.5535
Training Epoch: 13 [10496/50176]	Loss: 1.3843
Training Epoch: 13 [10752/50176]	Loss: 1.4083
Training Epoch: 13 [11008/50176]	Loss: 1.3576
Training Epoch: 13 [11264/50176]	Loss: 1.4703
Training Epoch: 13 [11520/50176]	Loss: 1.5417
Training Epoch: 13 [11776/50176]	Loss: 1.5123
Training Epoch: 13 [12032/50176]	Loss: 1.4084
Training Epoch: 13 [12288/50176]	Loss: 1.3641
Training Epoch: 13 [12544/50176]	Loss: 1.3990
Training Epoch: 13 [12800/50176]	Loss: 1.4747
Training Epoch: 13 [13056/50176]	Loss: 1.4726
Training Epoch: 13 [13312/50176]	Loss: 1.5352
Training Epoch: 13 [13568/50176]	Loss: 1.5417
Training Epoch: 13 [13824/50176]	Loss: 1.5056
Training Epoch: 13 [14080/50176]	Loss: 1.4070
Training Epoch: 13 [14336/50176]	Loss: 1.4394
Training Epoch: 13 [14592/50176]	Loss: 1.5922
Training Epoch: 13 [14848/50176]	Loss: 1.6426
Training Epoch: 13 [15104/50176]	Loss: 1.6288
Training Epoch: 13 [15360/50176]	Loss: 1.5096
Training Epoch: 13 [15616/50176]	Loss: 1.5328
Training Epoch: 13 [15872/50176]	Loss: 1.5227
Training Epoch: 13 [16128/50176]	Loss: 1.6090
Training Epoch: 13 [16384/50176]	Loss: 1.4705
Training Epoch: 13 [16640/50176]	Loss: 1.6161
Training Epoch: 13 [16896/50176]	Loss: 1.3765
Training Epoch: 13 [17152/50176]	Loss: 1.7058
Training Epoch: 13 [17408/50176]	Loss: 1.4125
Training Epoch: 13 [17664/50176]	Loss: 1.6027
Training Epoch: 13 [17920/50176]	Loss: 1.5141
Training Epoch: 13 [18176/50176]	Loss: 1.4775
Training Epoch: 13 [18432/50176]	Loss: 1.5709
Training Epoch: 13 [18688/50176]	Loss: 1.6227
Training Epoch: 13 [18944/50176]	Loss: 1.4608
Training Epoch: 13 [19200/50176]	Loss: 1.4987
Training Epoch: 13 [19456/50176]	Loss: 1.4788
Training Epoch: 13 [19712/50176]	Loss: 1.4275
Training Epoch: 13 [19968/50176]	Loss: 1.4775
Training Epoch: 13 [20224/50176]	Loss: 1.4746
Training Epoch: 13 [20480/50176]	Loss: 1.5729
Training Epoch: 13 [20736/50176]	Loss: 1.5077
Training Epoch: 13 [20992/50176]	Loss: 1.5848
Training Epoch: 13 [21248/50176]	Loss: 1.4786
Training Epoch: 13 [21504/50176]	Loss: 1.5968
Training Epoch: 13 [21760/50176]	Loss: 1.6319
Training Epoch: 13 [22016/50176]	Loss: 1.7574
Training Epoch: 13 [22272/50176]	Loss: 1.3523
Training Epoch: 13 [22528/50176]	Loss: 1.6294
Training Epoch: 13 [22784/50176]	Loss: 1.4753
Training Epoch: 13 [23040/50176]	Loss: 1.6957
Training Epoch: 13 [23296/50176]	Loss: 1.6678
Training Epoch: 13 [23552/50176]	Loss: 1.6174
Training Epoch: 13 [23808/50176]	Loss: 1.5038
Training Epoch: 13 [24064/50176]	Loss: 1.5018
Training Epoch: 13 [24320/50176]	Loss: 1.6301
Training Epoch: 13 [24576/50176]	Loss: 1.5406
Training Epoch: 13 [24832/50176]	Loss: 1.4370
Training Epoch: 13 [25088/50176]	Loss: 1.4663
Training Epoch: 13 [25344/50176]	Loss: 1.6062
Training Epoch: 13 [25600/50176]	Loss: 1.5846
Training Epoch: 13 [25856/50176]	Loss: 1.5212
Training Epoch: 13 [26112/50176]	Loss: 1.4198
Training Epoch: 13 [26368/50176]	Loss: 1.5348
Training Epoch: 13 [26624/50176]	Loss: 1.4767
Training Epoch: 13 [26880/50176]	Loss: 1.6225
Training Epoch: 13 [27136/50176]	Loss: 1.7202
Training Epoch: 13 [27392/50176]	Loss: 1.6179
Training Epoch: 13 [27648/50176]	Loss: 1.3930
Training Epoch: 13 [27904/50176]	Loss: 1.5794
Training Epoch: 13 [28160/50176]	Loss: 1.5174
Training Epoch: 13 [28416/50176]	Loss: 1.4655
Training Epoch: 13 [28672/50176]	Loss: 1.4976
Training Epoch: 13 [28928/50176]	Loss: 1.3830
Training Epoch: 13 [29184/50176]	Loss: 1.4564
Training Epoch: 13 [29440/50176]	Loss: 1.7243
Training Epoch: 13 [29696/50176]	Loss: 1.5272
Training Epoch: 13 [29952/50176]	Loss: 1.5531
Training Epoch: 13 [30208/50176]	Loss: 1.6151
Training Epoch: 13 [30464/50176]	Loss: 1.5941
Training Epoch: 13 [30720/50176]	Loss: 1.6675
Training Epoch: 13 [30976/50176]	Loss: 1.4260
Training Epoch: 13 [31232/50176]	Loss: 1.5051
Training Epoch: 13 [31488/50176]	Loss: 1.5947
Training Epoch: 13 [31744/50176]	Loss: 1.4247
Training Epoch: 13 [32000/50176]	Loss: 1.5481
Training Epoch: 13 [32256/50176]	Loss: 1.4771
Training Epoch: 13 [32512/50176]	Loss: 1.4686
Training Epoch: 13 [32768/50176]	Loss: 1.4670
Training Epoch: 13 [33024/50176]	Loss: 1.3484
Training Epoch: 13 [33280/50176]	Loss: 1.5847
Training Epoch: 13 [33536/50176]	Loss: 1.5602
Training Epoch: 13 [33792/50176]	Loss: 1.4982
Training Epoch: 13 [34048/50176]	Loss: 1.4304
Training Epoch: 13 [34304/50176]	Loss: 1.4899
Training Epoch: 13 [34560/50176]	Loss: 1.4426
Training Epoch: 13 [34816/50176]	Loss: 1.6170
Training Epoch: 13 [35072/50176]	Loss: 1.3470
Training Epoch: 13 [35328/50176]	Loss: 1.5147
Training Epoch: 13 [35584/50176]	Loss: 1.6366
Training Epoch: 13 [35840/50176]	Loss: 1.5583
Training Epoch: 13 [36096/50176]	Loss: 1.3701
Training Epoch: 13 [36352/50176]	Loss: 1.5712
Training Epoch: 13 [36608/50176]	Loss: 1.7619
Training Epoch: 13 [36864/50176]	Loss: 1.4464
Training Epoch: 13 [37120/50176]	Loss: 1.5682
Training Epoch: 13 [37376/50176]	Loss: 1.5738
Training Epoch: 13 [37632/50176]	Loss: 1.6257
Training Epoch: 13 [37888/50176]	Loss: 1.5798
Training Epoch: 13 [38144/50176]	Loss: 1.4298
Training Epoch: 13 [38400/50176]	Loss: 1.7208
Training Epoch: 13 [38656/50176]	Loss: 1.4665
Training Epoch: 13 [38912/50176]	Loss: 1.4587
Training Epoch: 13 [39168/50176]	Loss: 1.6000
Training Epoch: 13 [39424/50176]	Loss: 1.3761
Training Epoch: 13 [39680/50176]	Loss: 1.4060
Training Epoch: 13 [39936/50176]	Loss: 1.4877
Training Epoch: 13 [40192/50176]	Loss: 1.4727
Training Epoch: 13 [40448/50176]	Loss: 1.6130
Training Epoch: 13 [40704/50176]	Loss: 1.6114
Training Epoch: 13 [40960/50176]	Loss: 1.4458
Training Epoch: 13 [41216/50176]	Loss: 1.5614
Training Epoch: 13 [41472/50176]	Loss: 1.4163
Training Epoch: 13 [41728/50176]	Loss: 1.5730
Training Epoch: 13 [41984/50176]	Loss: 1.6888
Training Epoch: 13 [42240/50176]	Loss: 1.6671
Training Epoch: 13 [42496/50176]	Loss: 1.6120
Training Epoch: 13 [42752/50176]	Loss: 1.7389
Training Epoch: 13 [43008/50176]	Loss: 1.5487
Training Epoch: 13 [43264/50176]	Loss: 1.5111
Training Epoch: 13 [43520/50176]	Loss: 1.6380
Training Epoch: 13 [43776/50176]	Loss: 1.6246
Training Epoch: 13 [44032/50176]	Loss: 1.6240
Training Epoch: 13 [44288/50176]	Loss: 1.7561
Training Epoch: 13 [44544/50176]	Loss: 1.5844
Training Epoch: 13 [44800/50176]	Loss: 1.4885
Training Epoch: 13 [45056/50176]	Loss: 1.5021
Training Epoch: 13 [45312/50176]	Loss: 1.4702
Training Epoch: 13 [45568/50176]	Loss: 1.3419
Training Epoch: 13 [45824/50176]	Loss: 1.5751
Training Epoch: 13 [46080/50176]	Loss: 1.6200
Training Epoch: 13 [46336/50176]	Loss: 1.5353
Training Epoch: 13 [46592/50176]	Loss: 1.7400
Training Epoch: 13 [46848/50176]	Loss: 1.5364
Training Epoch: 13 [47104/50176]	Loss: 1.5000
Training Epoch: 13 [47360/50176]	Loss: 1.5273
Training Epoch: 13 [47616/50176]	Loss: 1.3447
Training Epoch: 13 [47872/50176]	Loss: 1.5501
Training Epoch: 13 [48128/50176]	Loss: 1.6792
Training Epoch: 13 [48384/50176]	Loss: 1.5899
Training Epoch: 13 [48640/50176]	Loss: 1.5824
Training Epoch: 13 [48896/50176]	Loss: 1.4589
Training Epoch: 13 [49152/50176]	Loss: 1.3589
Training Epoch: 13 [49408/50176]	Loss: 1.5813
Training Epoch: 13 [49664/50176]	Loss: 1.4848
Training Epoch: 13 [49920/50176]	Loss: 1.6780
Training Epoch: 13 [50176/50176]	Loss: 1.5954
Validation Epoch: 13, Average loss: 0.0075, Accuracy: 0.4971
Training Epoch: 14 [256/50176]	Loss: 1.3542
Training Epoch: 14 [512/50176]	Loss: 1.2719
Training Epoch: 14 [768/50176]	Loss: 1.3554
Training Epoch: 14 [1024/50176]	Loss: 1.2926
Training Epoch: 14 [1280/50176]	Loss: 1.5861
Training Epoch: 14 [1536/50176]	Loss: 1.2845
Training Epoch: 14 [1792/50176]	Loss: 1.3975
Training Epoch: 14 [2048/50176]	Loss: 1.6388
Training Epoch: 14 [2304/50176]	Loss: 1.5389
Training Epoch: 14 [2560/50176]	Loss: 1.4483
Training Epoch: 14 [2816/50176]	Loss: 1.5145
Training Epoch: 14 [3072/50176]	Loss: 1.2988
Training Epoch: 14 [3328/50176]	Loss: 1.3742
Training Epoch: 14 [3584/50176]	Loss: 1.4172
Training Epoch: 14 [3840/50176]	Loss: 1.3037
Training Epoch: 14 [4096/50176]	Loss: 1.4377
Training Epoch: 14 [4352/50176]	Loss: 1.6155
Training Epoch: 14 [4608/50176]	Loss: 1.6241
Training Epoch: 14 [4864/50176]	Loss: 1.3361
Training Epoch: 14 [5120/50176]	Loss: 1.5616
Training Epoch: 14 [5376/50176]	Loss: 1.3964
Training Epoch: 14 [5632/50176]	Loss: 1.5707
Training Epoch: 14 [5888/50176]	Loss: 1.5591
Training Epoch: 14 [6144/50176]	Loss: 1.3225
Training Epoch: 14 [6400/50176]	Loss: 1.4105
Training Epoch: 14 [6656/50176]	Loss: 1.4958
Training Epoch: 14 [6912/50176]	Loss: 1.3404
Training Epoch: 14 [7168/50176]	Loss: 1.5026
Training Epoch: 14 [7424/50176]	Loss: 1.5696
Training Epoch: 14 [7680/50176]	Loss: 1.4407
Training Epoch: 14 [7936/50176]	Loss: 1.3158
Training Epoch: 14 [8192/50176]	Loss: 1.4839
Training Epoch: 14 [8448/50176]	Loss: 1.4220
Training Epoch: 14 [8704/50176]	Loss: 1.5753
Training Epoch: 14 [8960/50176]	Loss: 1.4132
Training Epoch: 14 [9216/50176]	Loss: 1.4509
Training Epoch: 14 [9472/50176]	Loss: 1.6278
Training Epoch: 14 [9728/50176]	Loss: 1.3297
Training Epoch: 14 [9984/50176]	Loss: 1.3781
Training Epoch: 14 [10240/50176]	Loss: 1.3786
Training Epoch: 14 [10496/50176]	Loss: 1.2779
Training Epoch: 14 [10752/50176]	Loss: 1.2923
Training Epoch: 14 [11008/50176]	Loss: 1.3694
Training Epoch: 14 [11264/50176]	Loss: 1.4600
Training Epoch: 14 [11520/50176]	Loss: 1.4732
Training Epoch: 14 [11776/50176]	Loss: 1.4786
Training Epoch: 14 [12032/50176]	Loss: 1.3686
Training Epoch: 14 [12288/50176]	Loss: 1.4932
Training Epoch: 14 [12544/50176]	Loss: 1.4959
Training Epoch: 14 [12800/50176]	Loss: 1.5278
Training Epoch: 14 [13056/50176]	Loss: 1.7107
Training Epoch: 14 [13312/50176]	Loss: 1.4917
Training Epoch: 14 [13568/50176]	Loss: 1.4635
Training Epoch: 14 [13824/50176]	Loss: 1.5794
Training Epoch: 14 [14080/50176]	Loss: 1.3761
Training Epoch: 14 [14336/50176]	Loss: 1.3184
Training Epoch: 14 [14592/50176]	Loss: 1.5599
Training Epoch: 14 [14848/50176]	Loss: 1.4168
Training Epoch: 14 [15104/50176]	Loss: 1.0943
Training Epoch: 14 [15360/50176]	Loss: 1.4344
Training Epoch: 14 [15616/50176]	Loss: 1.5978
Training Epoch: 14 [15872/50176]	Loss: 1.3736
Training Epoch: 14 [16128/50176]	Loss: 1.5533
Training Epoch: 14 [16384/50176]	Loss: 1.6234
Training Epoch: 14 [16640/50176]	Loss: 1.5418
Training Epoch: 14 [16896/50176]	Loss: 1.4530
Training Epoch: 14 [17152/50176]	Loss: 1.4217
Training Epoch: 14 [17408/50176]	Loss: 1.5488
Training Epoch: 14 [17664/50176]	Loss: 1.4612
Training Epoch: 14 [17920/50176]	Loss: 1.2801
Training Epoch: 14 [18176/50176]	Loss: 1.4090
Training Epoch: 14 [18432/50176]	Loss: 1.3865
Training Epoch: 14 [18688/50176]	Loss: 1.6044
Training Epoch: 14 [18944/50176]	Loss: 1.3751
Training Epoch: 14 [19200/50176]	Loss: 1.4837
Training Epoch: 14 [19456/50176]	Loss: 1.3529
Training Epoch: 14 [19712/50176]	Loss: 1.4150
Training Epoch: 14 [19968/50176]	Loss: 1.4712
Training Epoch: 14 [20224/50176]	Loss: 1.4761
Training Epoch: 14 [20480/50176]	Loss: 1.4972
Training Epoch: 14 [20736/50176]	Loss: 1.4334
Training Epoch: 14 [20992/50176]	Loss: 1.3779
Training Epoch: 14 [21248/50176]	Loss: 1.3709
Training Epoch: 14 [21504/50176]	Loss: 1.5310
Training Epoch: 14 [21760/50176]	Loss: 1.4281
Training Epoch: 14 [22016/50176]	Loss: 1.7922
Training Epoch: 14 [22272/50176]	Loss: 1.3597
Training Epoch: 14 [22528/50176]	Loss: 1.6548
Training Epoch: 14 [22784/50176]	Loss: 1.2206
Training Epoch: 14 [23040/50176]	Loss: 1.6029
Training Epoch: 14 [23296/50176]	Loss: 1.3568
Training Epoch: 14 [23552/50176]	Loss: 1.4521
Training Epoch: 14 [23808/50176]	Loss: 1.3956
Training Epoch: 14 [24064/50176]	Loss: 1.4118
Training Epoch: 14 [24320/50176]	Loss: 1.3113
Training Epoch: 14 [24576/50176]	Loss: 1.3656
Training Epoch: 14 [24832/50176]	Loss: 1.4809
Training Epoch: 14 [25088/50176]	Loss: 1.5391
Training Epoch: 14 [25344/50176]	Loss: 1.4591
Training Epoch: 14 [25600/50176]	Loss: 1.5768
Training Epoch: 14 [25856/50176]	Loss: 1.5369
Training Epoch: 14 [26112/50176]	Loss: 1.4898
Training Epoch: 14 [26368/50176]	Loss: 1.3879
Training Epoch: 14 [26624/50176]	Loss: 1.4780
Training Epoch: 14 [26880/50176]	Loss: 1.4809
Training Epoch: 14 [27136/50176]	Loss: 1.5373
Training Epoch: 14 [27392/50176]	Loss: 1.5090
Training Epoch: 14 [27648/50176]	Loss: 1.5041
Training Epoch: 14 [27904/50176]	Loss: 1.6167
Training Epoch: 14 [28160/50176]	Loss: 1.5479
Training Epoch: 14 [28416/50176]	Loss: 1.6456
Training Epoch: 14 [28672/50176]	Loss: 1.4414
Training Epoch: 14 [28928/50176]	Loss: 1.4436
Training Epoch: 14 [29184/50176]	Loss: 1.3667
Training Epoch: 14 [29440/50176]	Loss: 1.3900
Training Epoch: 14 [29696/50176]	Loss: 1.4459
Training Epoch: 14 [29952/50176]	Loss: 1.5350
Training Epoch: 14 [30208/50176]	Loss: 1.6431
Training Epoch: 14 [30464/50176]	Loss: 1.5601
Training Epoch: 14 [30720/50176]	Loss: 1.5887
Training Epoch: 14 [30976/50176]	Loss: 1.5209
Training Epoch: 14 [31232/50176]	Loss: 1.5555
Training Epoch: 14 [31488/50176]	Loss: 1.4129
Training Epoch: 14 [31744/50176]	Loss: 1.5246
Training Epoch: 14 [32000/50176]	Loss: 1.5631
Training Epoch: 14 [32256/50176]	Loss: 1.4093
Training Epoch: 14 [32512/50176]	Loss: 1.4959
Training Epoch: 14 [32768/50176]	Loss: 1.6485
Training Epoch: 14 [33024/50176]	Loss: 1.3360
Training Epoch: 14 [33280/50176]	Loss: 1.4057
Training Epoch: 14 [33536/50176]	Loss: 1.5726
Training Epoch: 14 [33792/50176]	Loss: 1.4789
Training Epoch: 14 [34048/50176]	Loss: 1.7169
Training Epoch: 14 [34304/50176]	Loss: 1.5422
Training Epoch: 14 [34560/50176]	Loss: 1.4443
Training Epoch: 14 [34816/50176]	Loss: 1.5907
Training Epoch: 14 [35072/50176]	Loss: 1.6102
Training Epoch: 14 [35328/50176]	Loss: 1.6016
Training Epoch: 14 [35584/50176]	Loss: 1.4744
Training Epoch: 14 [35840/50176]	Loss: 1.4243
Training Epoch: 14 [36096/50176]	Loss: 1.4362
Training Epoch: 14 [36352/50176]	Loss: 1.4498
Training Epoch: 14 [36608/50176]	Loss: 1.2825
Training Epoch: 14 [36864/50176]	Loss: 1.5367
Training Epoch: 14 [37120/50176]	Loss: 1.4256
Training Epoch: 14 [37376/50176]	Loss: 1.4102
Training Epoch: 14 [37632/50176]	Loss: 1.4633
Training Epoch: 14 [37888/50176]	Loss: 1.5370
Training Epoch: 14 [38144/50176]	Loss: 1.4156
Training Epoch: 14 [38400/50176]	Loss: 1.4546
Training Epoch: 14 [38656/50176]	Loss: 1.5938
Training Epoch: 14 [38912/50176]	Loss: 1.3757
Training Epoch: 14 [39168/50176]	Loss: 1.4800
Training Epoch: 14 [39424/50176]	Loss: 1.5385
Training Epoch: 14 [39680/50176]	Loss: 1.5668
Training Epoch: 14 [39936/50176]	Loss: 1.4537
Training Epoch: 14 [40192/50176]	Loss: 1.5263
Training Epoch: 14 [40448/50176]	Loss: 1.6308
Training Epoch: 14 [40704/50176]	Loss: 1.4390
Training Epoch: 14 [40960/50176]	Loss: 1.6533
Training Epoch: 14 [41216/50176]	Loss: 1.6121
Training Epoch: 14 [41472/50176]	Loss: 1.6012
Training Epoch: 14 [41728/50176]	Loss: 1.5608
Training Epoch: 14 [41984/50176]	Loss: 1.5827
Training Epoch: 14 [42240/50176]	Loss: 1.7027
Training Epoch: 14 [42496/50176]	Loss: 1.2861
Training Epoch: 14 [42752/50176]	Loss: 1.4183
Training Epoch: 14 [43008/50176]	Loss: 1.3265
Training Epoch: 14 [43264/50176]	Loss: 1.5346
Training Epoch: 14 [43520/50176]	Loss: 1.3839
Training Epoch: 14 [43776/50176]	Loss: 1.5653
Training Epoch: 14 [44032/50176]	Loss: 1.3368
Training Epoch: 14 [44288/50176]	Loss: 1.7285
Training Epoch: 14 [44544/50176]	Loss: 1.4905
Training Epoch: 14 [44800/50176]	Loss: 1.2762
Training Epoch: 14 [45056/50176]	Loss: 1.3613
Training Epoch: 14 [45312/50176]	Loss: 1.4491
Training Epoch: 14 [45568/50176]	Loss: 1.6257
Training Epoch: 14 [45824/50176]	Loss: 1.5339
Training Epoch: 14 [46080/50176]	Loss: 1.3437
Training Epoch: 14 [46336/50176]	Loss: 1.7080
Training Epoch: 14 [46592/50176]	Loss: 1.4453
Training Epoch: 14 [46848/50176]	Loss: 1.4002
Training Epoch: 14 [47104/50176]	Loss: 1.5333
Training Epoch: 14 [47360/50176]	Loss: 1.2510
Training Epoch: 14 [47616/50176]	Loss: 1.3806
Training Epoch: 14 [47872/50176]	Loss: 1.6404
Training Epoch: 14 [48128/50176]	Loss: 1.4293
Training Epoch: 14 [48384/50176]	Loss: 1.6692
Training Epoch: 14 [48640/50176]	Loss: 1.4974
Training Epoch: 14 [48896/50176]	Loss: 1.4782
Training Epoch: 14 [49152/50176]	Loss: 1.4821
Training Epoch: 14 [49408/50176]	Loss: 1.3515
Training Epoch: 14 [49664/50176]	Loss: 1.3502
Training Epoch: 14 [49920/50176]	Loss: 1.4866
Training Epoch: 14 [50176/50176]	Loss: 1.6892
Validation Epoch: 14, Average loss: 0.0082, Accuracy: 0.4551
Training Epoch: 15 [256/50176]	Loss: 1.4176
Training Epoch: 15 [512/50176]	Loss: 1.2469
Training Epoch: 15 [768/50176]	Loss: 1.4081
Training Epoch: 15 [1024/50176]	Loss: 1.3137
Training Epoch: 15 [1280/50176]	Loss: 1.5479
Training Epoch: 15 [1536/50176]	Loss: 1.3886
Training Epoch: 15 [1792/50176]	Loss: 1.4924
Training Epoch: 15 [2048/50176]	Loss: 1.5343
Training Epoch: 15 [2304/50176]	Loss: 1.3394
Training Epoch: 15 [2560/50176]	Loss: 1.4046
Training Epoch: 15 [2816/50176]	Loss: 1.2838
Training Epoch: 15 [3072/50176]	Loss: 1.4929
Training Epoch: 15 [3328/50176]	Loss: 1.3969
Training Epoch: 15 [3584/50176]	Loss: 1.3762
Training Epoch: 15 [3840/50176]	Loss: 1.4667
Training Epoch: 15 [4096/50176]	Loss: 1.3028
Training Epoch: 15 [4352/50176]	Loss: 1.5347
Training Epoch: 15 [4608/50176]	Loss: 1.5183
Training Epoch: 15 [4864/50176]	Loss: 1.4009
Training Epoch: 15 [5120/50176]	Loss: 1.4754
Training Epoch: 15 [5376/50176]	Loss: 1.4351
Training Epoch: 15 [5632/50176]	Loss: 1.3394
Training Epoch: 15 [5888/50176]	Loss: 1.3121
Training Epoch: 15 [6144/50176]	Loss: 1.4024
Training Epoch: 15 [6400/50176]	Loss: 1.2880
Training Epoch: 15 [6656/50176]	Loss: 1.2824
Training Epoch: 15 [6912/50176]	Loss: 1.3636
Training Epoch: 15 [7168/50176]	Loss: 1.4035
Training Epoch: 15 [7424/50176]	Loss: 1.3022
Training Epoch: 15 [7680/50176]	Loss: 1.4613
Training Epoch: 15 [7936/50176]	Loss: 1.2258
Training Epoch: 15 [8192/50176]	Loss: 1.3337
Training Epoch: 15 [8448/50176]	Loss: 1.2971
Training Epoch: 15 [8704/50176]	Loss: 1.3896
Training Epoch: 15 [8960/50176]	Loss: 1.2656
Training Epoch: 15 [9216/50176]	Loss: 1.3884
Training Epoch: 15 [9472/50176]	Loss: 1.2283
Training Epoch: 15 [9728/50176]	Loss: 1.3454
Training Epoch: 15 [9984/50176]	Loss: 1.4532
Training Epoch: 15 [10240/50176]	Loss: 1.3902
Training Epoch: 15 [10496/50176]	Loss: 1.2976
Training Epoch: 15 [10752/50176]	Loss: 1.3766
Training Epoch: 15 [11008/50176]	Loss: 1.2251
Training Epoch: 15 [11264/50176]	Loss: 1.1987
Training Epoch: 15 [11520/50176]	Loss: 1.4189
Training Epoch: 15 [11776/50176]	Loss: 1.2159
Training Epoch: 15 [12032/50176]	Loss: 1.4543
Training Epoch: 15 [12288/50176]	Loss: 1.5727
Training Epoch: 15 [12544/50176]	Loss: 1.3885
Training Epoch: 15 [12800/50176]	Loss: 1.3434
Training Epoch: 15 [13056/50176]	Loss: 1.5677
Training Epoch: 15 [13312/50176]	Loss: 1.3163
Training Epoch: 15 [13568/50176]	Loss: 1.4600
Training Epoch: 15 [13824/50176]	Loss: 1.3738
Training Epoch: 15 [14080/50176]	Loss: 1.5215
Training Epoch: 15 [14336/50176]	Loss: 1.4751
Training Epoch: 15 [14592/50176]	Loss: 1.4611
Training Epoch: 15 [14848/50176]	Loss: 1.3264
Training Epoch: 15 [15104/50176]	Loss: 1.4480
Training Epoch: 15 [15360/50176]	Loss: 1.4144
Training Epoch: 15 [15616/50176]	Loss: 1.3689
Training Epoch: 15 [15872/50176]	Loss: 1.3331
Training Epoch: 15 [16128/50176]	Loss: 1.4845
Training Epoch: 15 [16384/50176]	Loss: 1.5019
Training Epoch: 15 [16640/50176]	Loss: 1.3609
Training Epoch: 15 [16896/50176]	Loss: 1.3460
Training Epoch: 15 [17152/50176]	Loss: 1.2821
Training Epoch: 15 [17408/50176]	Loss: 1.3198
Training Epoch: 15 [17664/50176]	Loss: 1.2357
Training Epoch: 15 [17920/50176]	Loss: 1.4352
Training Epoch: 15 [18176/50176]	Loss: 1.2750
Training Epoch: 15 [18432/50176]	Loss: 1.4378
Training Epoch: 15 [18688/50176]	Loss: 1.4970
Training Epoch: 15 [18944/50176]	Loss: 1.4528
Training Epoch: 15 [19200/50176]	Loss: 1.4209
Training Epoch: 15 [19456/50176]	Loss: 1.3558
Training Epoch: 15 [19712/50176]	Loss: 1.4102
Training Epoch: 15 [19968/50176]	Loss: 1.4253
Training Epoch: 15 [20224/50176]	Loss: 1.3515
Training Epoch: 15 [20480/50176]	Loss: 1.6311
Training Epoch: 15 [20736/50176]	Loss: 1.4792
Training Epoch: 15 [20992/50176]	Loss: 1.4214
Training Epoch: 15 [21248/50176]	Loss: 1.3398
Training Epoch: 15 [21504/50176]	Loss: 1.5003
Training Epoch: 15 [21760/50176]	Loss: 1.4782
Training Epoch: 15 [22016/50176]	Loss: 1.4776
Training Epoch: 15 [22272/50176]	Loss: 1.3907
Training Epoch: 15 [22528/50176]	Loss: 1.3455
Training Epoch: 15 [22784/50176]	Loss: 1.3268
Training Epoch: 15 [23040/50176]	Loss: 1.2979
Training Epoch: 15 [23296/50176]	Loss: 1.4256
Training Epoch: 15 [23552/50176]	Loss: 1.4832
Training Epoch: 15 [23808/50176]	Loss: 1.3467
Training Epoch: 15 [24064/50176]	Loss: 1.5329
Training Epoch: 15 [24320/50176]	Loss: 1.2678
Training Epoch: 15 [24576/50176]	Loss: 1.4534
Training Epoch: 15 [24832/50176]	Loss: 1.4674
Training Epoch: 15 [25088/50176]	Loss: 1.4209
Training Epoch: 15 [25344/50176]	Loss: 1.3338
Training Epoch: 15 [25600/50176]	Loss: 1.3702
Training Epoch: 15 [25856/50176]	Loss: 1.5299
Training Epoch: 15 [26112/50176]	Loss: 1.4503
Training Epoch: 15 [26368/50176]	Loss: 1.3849
Training Epoch: 15 [26624/50176]	Loss: 1.5154
Training Epoch: 15 [26880/50176]	Loss: 1.4854
Training Epoch: 15 [27136/50176]	Loss: 1.4078
Training Epoch: 15 [27392/50176]	Loss: 1.5722
Training Epoch: 15 [27648/50176]	Loss: 1.3699
Training Epoch: 15 [27904/50176]	Loss: 1.2596
Training Epoch: 15 [28160/50176]	Loss: 1.5585
Training Epoch: 15 [28416/50176]	Loss: 1.3309
Training Epoch: 15 [28672/50176]	Loss: 1.4919
Training Epoch: 15 [28928/50176]	Loss: 1.5478
Training Epoch: 15 [29184/50176]	Loss: 1.5254
Training Epoch: 15 [29440/50176]	Loss: 1.4911
Training Epoch: 15 [29696/50176]	Loss: 1.2635
Training Epoch: 15 [29952/50176]	Loss: 1.2990
Training Epoch: 15 [30208/50176]	Loss: 1.3893
Training Epoch: 15 [30464/50176]	Loss: 1.5154
Training Epoch: 15 [30720/50176]	Loss: 1.6024
Training Epoch: 15 [30976/50176]	Loss: 1.3963
Training Epoch: 15 [31232/50176]	Loss: 1.4493
Training Epoch: 15 [31488/50176]	Loss: 1.4270
Training Epoch: 15 [31744/50176]	Loss: 1.4948
Training Epoch: 15 [32000/50176]	Loss: 1.5827
Training Epoch: 15 [32256/50176]	Loss: 1.5258
Training Epoch: 15 [32512/50176]	Loss: 1.4228
Training Epoch: 15 [32768/50176]	Loss: 1.5414
Training Epoch: 15 [33024/50176]	Loss: 1.3481
Training Epoch: 15 [33280/50176]	Loss: 1.3446
Training Epoch: 15 [33536/50176]	Loss: 1.5554
Training Epoch: 15 [33792/50176]	Loss: 1.4774
Training Epoch: 15 [34048/50176]	Loss: 1.5892
Training Epoch: 15 [34304/50176]	Loss: 1.3762
Training Epoch: 15 [34560/50176]	Loss: 1.4495
Training Epoch: 15 [34816/50176]	Loss: 1.6360
Training Epoch: 15 [35072/50176]	Loss: 1.4493
Training Epoch: 15 [35328/50176]	Loss: 1.4381
Training Epoch: 15 [35584/50176]	Loss: 1.4569
Training Epoch: 15 [35840/50176]	Loss: 1.3906
Training Epoch: 15 [36096/50176]	Loss: 1.4608
Training Epoch: 15 [36352/50176]	Loss: 1.4948
Training Epoch: 15 [36608/50176]	Loss: 1.6153
Training Epoch: 15 [36864/50176]	Loss: 1.7206
Training Epoch: 15 [37120/50176]	Loss: 1.3771
Training Epoch: 15 [37376/50176]	Loss: 1.6079
Training Epoch: 15 [37632/50176]	Loss: 1.3141
Training Epoch: 15 [37888/50176]	Loss: 1.3006
Training Epoch: 15 [38144/50176]	Loss: 1.4741
Training Epoch: 15 [38400/50176]	Loss: 1.4161
Training Epoch: 15 [38656/50176]	Loss: 1.6347
Training Epoch: 15 [38912/50176]	Loss: 1.5767
Training Epoch: 15 [39168/50176]	Loss: 1.5039
Training Epoch: 15 [39424/50176]	Loss: 1.3902
Training Epoch: 15 [39680/50176]	Loss: 1.6249
Training Epoch: 15 [39936/50176]	Loss: 1.4816
Training Epoch: 15 [40192/50176]	Loss: 1.4852
Training Epoch: 15 [40448/50176]	Loss: 1.6162
Training Epoch: 15 [40704/50176]	Loss: 1.4095
Training Epoch: 15 [40960/50176]	Loss: 1.5322
Training Epoch: 15 [41216/50176]	Loss: 1.7050
Training Epoch: 15 [41472/50176]	Loss: 1.6190
Training Epoch: 15 [41728/50176]	Loss: 1.2884
Training Epoch: 15 [41984/50176]	Loss: 1.4594
Training Epoch: 15 [42240/50176]	Loss: 1.4845
Training Epoch: 15 [42496/50176]	Loss: 1.2980
Training Epoch: 15 [42752/50176]	Loss: 1.3921
Training Epoch: 15 [43008/50176]	Loss: 1.3959
Training Epoch: 15 [43264/50176]	Loss: 1.5713
Training Epoch: 15 [43520/50176]	Loss: 1.4355
Training Epoch: 15 [43776/50176]	Loss: 1.3637
Training Epoch: 15 [44032/50176]	Loss: 1.3420
Training Epoch: 15 [44288/50176]	Loss: 1.4121
Training Epoch: 15 [44544/50176]	Loss: 1.3341
Training Epoch: 15 [44800/50176]	Loss: 1.4288
Training Epoch: 15 [45056/50176]	Loss: 1.5077
Training Epoch: 15 [45312/50176]	Loss: 1.4249
Training Epoch: 15 [45568/50176]	Loss: 1.4429
Training Epoch: 15 [45824/50176]	Loss: 1.3940
Training Epoch: 15 [46080/50176]	Loss: 1.2984
Training Epoch: 15 [46336/50176]	Loss: 1.4492
Training Epoch: 15 [46592/50176]	Loss: 1.3515
Training Epoch: 15 [46848/50176]	Loss: 1.3624
Training Epoch: 15 [47104/50176]	Loss: 1.5248
Training Epoch: 15 [47360/50176]	Loss: 1.3449
Training Epoch: 15 [47616/50176]	Loss: 1.4441
Training Epoch: 15 [47872/50176]	Loss: 1.3776
Training Epoch: 15 [48128/50176]	Loss: 1.3660
Training Epoch: 15 [48384/50176]	Loss: 1.5033
Training Epoch: 15 [48640/50176]	Loss: 1.3830
Training Epoch: 15 [48896/50176]	Loss: 1.3471
Training Epoch: 15 [49152/50176]	Loss: 1.3879
Training Epoch: 15 [49408/50176]	Loss: 1.4308
Training Epoch: 15 [49664/50176]	Loss: 1.3881
Training Epoch: 15 [49920/50176]	Loss: 1.4322
Training Epoch: 15 [50176/50176]	Loss: 1.3724
Validation Epoch: 15, Average loss: 0.0079, Accuracy: 0.4712
Training Epoch: 16 [256/50176]	Loss: 1.2029
Training Epoch: 16 [512/50176]	Loss: 1.2617
Training Epoch: 16 [768/50176]	Loss: 1.5081
Training Epoch: 16 [1024/50176]	Loss: 1.2609
Training Epoch: 16 [1280/50176]	Loss: 1.2947
Training Epoch: 16 [1536/50176]	Loss: 1.3055
Training Epoch: 16 [1792/50176]	Loss: 1.4513
Training Epoch: 16 [2048/50176]	Loss: 1.2961
Training Epoch: 16 [2304/50176]	Loss: 1.2320
Training Epoch: 16 [2560/50176]	Loss: 1.4357
Training Epoch: 16 [2816/50176]	Loss: 1.1789
Training Epoch: 16 [3072/50176]	Loss: 1.3044
Training Epoch: 16 [3328/50176]	Loss: 1.3833
Training Epoch: 16 [3584/50176]	Loss: 1.3850
Training Epoch: 16 [3840/50176]	Loss: 1.3074
Training Epoch: 16 [4096/50176]	Loss: 1.4363
Training Epoch: 16 [4352/50176]	Loss: 1.2666
Training Epoch: 16 [4608/50176]	Loss: 1.2844
Training Epoch: 16 [4864/50176]	Loss: 1.3771
Training Epoch: 16 [5120/50176]	Loss: 1.2301
Training Epoch: 16 [5376/50176]	Loss: 1.2662
Training Epoch: 16 [5632/50176]	Loss: 1.4760
Training Epoch: 16 [5888/50176]	Loss: 1.3075
Training Epoch: 16 [6144/50176]	Loss: 1.5922
Training Epoch: 16 [6400/50176]	Loss: 1.3672
Training Epoch: 16 [6656/50176]	Loss: 1.5794
Training Epoch: 16 [6912/50176]	Loss: 1.4456
Training Epoch: 16 [7168/50176]	Loss: 1.3428
Training Epoch: 16 [7424/50176]	Loss: 1.3712
Training Epoch: 16 [7680/50176]	Loss: 1.2450
Training Epoch: 16 [7936/50176]	Loss: 1.2310
Training Epoch: 16 [8192/50176]	Loss: 1.2971
Training Epoch: 16 [8448/50176]	Loss: 1.2818
Training Epoch: 16 [8704/50176]	Loss: 1.3383
Training Epoch: 16 [8960/50176]	Loss: 1.3647
Training Epoch: 16 [9216/50176]	Loss: 1.4235
Training Epoch: 16 [9472/50176]	Loss: 1.2256
Training Epoch: 16 [9728/50176]	Loss: 1.5361
Training Epoch: 16 [9984/50176]	Loss: 1.4730
Training Epoch: 16 [10240/50176]	Loss: 1.2763
Training Epoch: 16 [10496/50176]	Loss: 1.3716
Training Epoch: 16 [10752/50176]	Loss: 1.5103
Training Epoch: 16 [11008/50176]	Loss: 1.3741
Training Epoch: 16 [11264/50176]	Loss: 1.3750
Training Epoch: 16 [11520/50176]	Loss: 1.4036
Training Epoch: 16 [11776/50176]	Loss: 1.5100
Training Epoch: 16 [12032/50176]	Loss: 1.2965
Training Epoch: 16 [12288/50176]	Loss: 1.3895
Training Epoch: 16 [12544/50176]	Loss: 1.4819
Training Epoch: 16 [12800/50176]	Loss: 1.2199
Training Epoch: 16 [13056/50176]	Loss: 1.3462
Training Epoch: 16 [13312/50176]	Loss: 1.4559
Training Epoch: 16 [13568/50176]	Loss: 1.2263
Training Epoch: 16 [13824/50176]	Loss: 1.1921
Training Epoch: 16 [14080/50176]	Loss: 1.4608
Training Epoch: 16 [14336/50176]	Loss: 1.3465
Training Epoch: 16 [14592/50176]	Loss: 1.2790
Training Epoch: 16 [14848/50176]	Loss: 1.2036
Training Epoch: 16 [15104/50176]	Loss: 1.2494
Training Epoch: 16 [15360/50176]	Loss: 1.4232
Training Epoch: 16 [15616/50176]	Loss: 1.4751
Training Epoch: 16 [15872/50176]	Loss: 1.3149
Training Epoch: 16 [16128/50176]	Loss: 1.3264
Training Epoch: 16 [16384/50176]	Loss: 1.3887
Training Epoch: 16 [16640/50176]	Loss: 1.2727
Training Epoch: 16 [16896/50176]	Loss: 1.3333
Training Epoch: 16 [17152/50176]	Loss: 1.3541
Training Epoch: 16 [17408/50176]	Loss: 1.4622
Training Epoch: 16 [17664/50176]	Loss: 1.3098
Training Epoch: 16 [17920/50176]	Loss: 1.5643
Training Epoch: 16 [18176/50176]	Loss: 1.5288
Training Epoch: 16 [18432/50176]	Loss: 1.3705
Training Epoch: 16 [18688/50176]	Loss: 1.3538
Training Epoch: 16 [18944/50176]	Loss: 1.3902
Training Epoch: 16 [19200/50176]	Loss: 1.3529
Training Epoch: 16 [19456/50176]	Loss: 1.4190
Training Epoch: 16 [19712/50176]	Loss: 1.2876
Training Epoch: 16 [19968/50176]	Loss: 1.5064
Training Epoch: 16 [20224/50176]	Loss: 1.3145
Training Epoch: 16 [20480/50176]	Loss: 1.2428
Training Epoch: 16 [20736/50176]	Loss: 1.2702
Training Epoch: 16 [20992/50176]	Loss: 1.3767
Training Epoch: 16 [21248/50176]	Loss: 1.4194
Training Epoch: 16 [21504/50176]	Loss: 1.4061
Training Epoch: 16 [21760/50176]	Loss: 1.2922
Training Epoch: 16 [22016/50176]	Loss: 1.2457
Training Epoch: 16 [22272/50176]	Loss: 1.4106
Training Epoch: 16 [22528/50176]	Loss: 1.3953
Training Epoch: 16 [22784/50176]	Loss: 1.4142
Training Epoch: 16 [23040/50176]	Loss: 1.4476
Training Epoch: 16 [23296/50176]	Loss: 1.2574
Training Epoch: 16 [23552/50176]	Loss: 1.3187
Training Epoch: 16 [23808/50176]	Loss: 1.4445
Training Epoch: 16 [24064/50176]	Loss: 1.3459
Training Epoch: 16 [24320/50176]	Loss: 1.4645
Training Epoch: 16 [24576/50176]	Loss: 1.4122
Training Epoch: 16 [24832/50176]	Loss: 1.4342
Training Epoch: 16 [25088/50176]	Loss: 1.4832
Training Epoch: 16 [25344/50176]	Loss: 1.4418
Training Epoch: 16 [25600/50176]	Loss: 1.3248
Training Epoch: 16 [25856/50176]	Loss: 1.4085
Training Epoch: 16 [26112/50176]	Loss: 1.3809
Training Epoch: 16 [26368/50176]	Loss: 1.2179
Training Epoch: 16 [26624/50176]	Loss: 1.4317
Training Epoch: 16 [26880/50176]	Loss: 1.2636
Training Epoch: 16 [27136/50176]	Loss: 1.5371
Training Epoch: 16 [27392/50176]	Loss: 1.2960
Training Epoch: 16 [27648/50176]	Loss: 1.3718
Training Epoch: 16 [27904/50176]	Loss: 1.2501
Training Epoch: 16 [28160/50176]	Loss: 1.3766
Training Epoch: 16 [28416/50176]	Loss: 1.4809
Training Epoch: 16 [28672/50176]	Loss: 1.3552
Training Epoch: 16 [28928/50176]	Loss: 1.2897
Training Epoch: 16 [29184/50176]	Loss: 1.4399
Training Epoch: 16 [29440/50176]	Loss: 1.4223
Training Epoch: 16 [29696/50176]	Loss: 1.2500
Training Epoch: 16 [29952/50176]	Loss: 1.4549
Training Epoch: 16 [30208/50176]	Loss: 1.3137
Training Epoch: 16 [30464/50176]	Loss: 1.4766
Training Epoch: 16 [30720/50176]	Loss: 1.4639
Training Epoch: 16 [30976/50176]	Loss: 1.2289
Training Epoch: 16 [31232/50176]	Loss: 1.2197
Training Epoch: 16 [31488/50176]	Loss: 1.4964
Training Epoch: 16 [31744/50176]	Loss: 1.3720
Training Epoch: 16 [32000/50176]	Loss: 1.4787
Training Epoch: 16 [32256/50176]	Loss: 1.5032
Training Epoch: 16 [32512/50176]	Loss: 1.6539
Training Epoch: 16 [32768/50176]	Loss: 1.4748
Training Epoch: 16 [33024/50176]	Loss: 1.5248
Training Epoch: 16 [33280/50176]	Loss: 1.4323
Training Epoch: 16 [33536/50176]	Loss: 1.3706
Training Epoch: 16 [33792/50176]	Loss: 1.4326
Training Epoch: 16 [34048/50176]	Loss: 1.4006
Training Epoch: 16 [34304/50176]	Loss: 1.3185
Training Epoch: 16 [34560/50176]	Loss: 1.4242
Training Epoch: 16 [34816/50176]	Loss: 1.2917
Training Epoch: 16 [35072/50176]	Loss: 1.2723
Training Epoch: 16 [35328/50176]	Loss: 1.2743
Training Epoch: 16 [35584/50176]	Loss: 1.3422
Training Epoch: 16 [35840/50176]	Loss: 1.5738
Training Epoch: 16 [36096/50176]	Loss: 1.4975
Training Epoch: 16 [36352/50176]	Loss: 1.3602
Training Epoch: 16 [36608/50176]	Loss: 1.4569
Training Epoch: 16 [36864/50176]	Loss: 1.6276
Training Epoch: 16 [37120/50176]	Loss: 1.5691
Training Epoch: 16 [37376/50176]	Loss: 1.3164
Training Epoch: 16 [37632/50176]	Loss: 1.5009
Training Epoch: 16 [37888/50176]	Loss: 1.2636
Training Epoch: 16 [38144/50176]	Loss: 1.2509
Training Epoch: 16 [38400/50176]	Loss: 1.3165
Training Epoch: 16 [38656/50176]	Loss: 1.4139
Training Epoch: 16 [38912/50176]	Loss: 1.3909
Training Epoch: 16 [39168/50176]	Loss: 1.3752
Training Epoch: 16 [39424/50176]	Loss: 1.6291
Training Epoch: 16 [39680/50176]	Loss: 1.2837
Training Epoch: 16 [39936/50176]	Loss: 1.3793
Training Epoch: 16 [40192/50176]	Loss: 1.3374
Training Epoch: 16 [40448/50176]	Loss: 1.4617
Training Epoch: 16 [40704/50176]	Loss: 1.3819
Training Epoch: 16 [40960/50176]	Loss: 1.3441
Training Epoch: 16 [41216/50176]	Loss: 1.4237
Training Epoch: 16 [41472/50176]	Loss: 1.3951
Training Epoch: 16 [41728/50176]	Loss: 1.4568
Training Epoch: 16 [41984/50176]	Loss: 1.3099
Training Epoch: 16 [42240/50176]	Loss: 1.3066
Training Epoch: 16 [42496/50176]	Loss: 1.4358
Training Epoch: 16 [42752/50176]	Loss: 1.5707
Training Epoch: 16 [43008/50176]	Loss: 1.6812
Training Epoch: 16 [43264/50176]	Loss: 1.4910
Training Epoch: 16 [43520/50176]	Loss: 1.4157
Training Epoch: 16 [43776/50176]	Loss: 1.3825
Training Epoch: 16 [44032/50176]	Loss: 1.4423
Training Epoch: 16 [44288/50176]	Loss: 1.4785
Training Epoch: 16 [44544/50176]	Loss: 1.3549
Training Epoch: 16 [44800/50176]	Loss: 1.4376
Training Epoch: 16 [45056/50176]	Loss: 1.2853
Training Epoch: 16 [45312/50176]	Loss: 1.3413
Training Epoch: 16 [45568/50176]	Loss: 1.3989
Training Epoch: 16 [45824/50176]	Loss: 1.4812
Training Epoch: 16 [46080/50176]	Loss: 1.3892
Training Epoch: 16 [46336/50176]	Loss: 1.4612
Training Epoch: 16 [46592/50176]	Loss: 1.5436
Training Epoch: 16 [46848/50176]	Loss: 1.3797
Training Epoch: 16 [47104/50176]	Loss: 1.4038
Training Epoch: 16 [47360/50176]	Loss: 1.3314
Training Epoch: 16 [47616/50176]	Loss: 1.3048
Training Epoch: 16 [47872/50176]	Loss: 1.4357
Training Epoch: 16 [48128/50176]	Loss: 1.2878
Training Epoch: 16 [48384/50176]	Loss: 1.2885
Training Epoch: 16 [48640/50176]	Loss: 1.7402
Training Epoch: 16 [48896/50176]	Loss: 1.3608
Training Epoch: 16 [49152/50176]	Loss: 1.2970
Training Epoch: 16 [49408/50176]	Loss: 1.3275
Training Epoch: 16 [49664/50176]	Loss: 1.2292
Training Epoch: 16 [49920/50176]	Loss: 1.3838
Training Epoch: 16 [50176/50176]	Loss: 1.3118
Validation Epoch: 16, Average loss: 0.0096, Accuracy: 0.4174
Training Epoch: 17 [256/50176]	Loss: 1.2734
Training Epoch: 17 [512/50176]	Loss: 1.1830
Training Epoch: 17 [768/50176]	Loss: 1.2576
Training Epoch: 17 [1024/50176]	Loss: 1.4433
Training Epoch: 17 [1280/50176]	Loss: 1.4012
Training Epoch: 17 [1536/50176]	Loss: 1.2576
Training Epoch: 17 [1792/50176]	Loss: 1.2049
Training Epoch: 17 [2048/50176]	Loss: 1.3173
Training Epoch: 17 [2304/50176]	Loss: 1.2672
Training Epoch: 17 [2560/50176]	Loss: 1.3804
Training Epoch: 17 [2816/50176]	Loss: 1.3340
Training Epoch: 17 [3072/50176]	Loss: 1.4270
Training Epoch: 17 [3328/50176]	Loss: 1.2786
Training Epoch: 17 [3584/50176]	Loss: 1.3393
Training Epoch: 17 [3840/50176]	Loss: 1.1972
Training Epoch: 17 [4096/50176]	Loss: 1.2643
Training Epoch: 17 [4352/50176]	Loss: 1.1665
Training Epoch: 17 [4608/50176]	Loss: 1.4861
Training Epoch: 17 [4864/50176]	Loss: 1.1409
Training Epoch: 17 [5120/50176]	Loss: 1.3038
Training Epoch: 17 [5376/50176]	Loss: 1.2970
Training Epoch: 17 [5632/50176]	Loss: 1.1548
Training Epoch: 17 [5888/50176]	Loss: 1.4045
Training Epoch: 17 [6144/50176]	Loss: 1.2350
Training Epoch: 17 [6400/50176]	Loss: 1.4834
Training Epoch: 17 [6656/50176]	Loss: 1.2834
Training Epoch: 17 [6912/50176]	Loss: 1.3316
Training Epoch: 17 [7168/50176]	Loss: 1.2895
Training Epoch: 17 [7424/50176]	Loss: 1.2691
Training Epoch: 17 [7680/50176]	Loss: 1.2053
Training Epoch: 17 [7936/50176]	Loss: 1.2962
Training Epoch: 17 [8192/50176]	Loss: 1.2210
Training Epoch: 17 [8448/50176]	Loss: 1.2399
Training Epoch: 17 [8704/50176]	Loss: 1.2856
Training Epoch: 17 [8960/50176]	Loss: 1.2074
Training Epoch: 17 [9216/50176]	Loss: 1.3681
Training Epoch: 17 [9472/50176]	Loss: 1.3848
Training Epoch: 17 [9728/50176]	Loss: 1.4034
Training Epoch: 17 [9984/50176]	Loss: 1.2622
Training Epoch: 17 [10240/50176]	Loss: 1.1613
Training Epoch: 17 [10496/50176]	Loss: 1.3127
Training Epoch: 17 [10752/50176]	Loss: 1.3721
Training Epoch: 17 [11008/50176]	Loss: 1.4654
Training Epoch: 17 [11264/50176]	Loss: 1.2148
Training Epoch: 17 [11520/50176]	Loss: 1.2019
Training Epoch: 17 [11776/50176]	Loss: 1.3366
Training Epoch: 17 [12032/50176]	Loss: 1.2840
Training Epoch: 17 [12288/50176]	Loss: 1.3865
Training Epoch: 17 [12544/50176]	Loss: 1.2895
Training Epoch: 17 [12800/50176]	Loss: 1.2377
Training Epoch: 17 [13056/50176]	Loss: 1.1521
Training Epoch: 17 [13312/50176]	Loss: 1.2684
Training Epoch: 17 [13568/50176]	Loss: 1.2846
Training Epoch: 17 [13824/50176]	Loss: 1.5264
Training Epoch: 17 [14080/50176]	Loss: 1.2760
Training Epoch: 17 [14336/50176]	Loss: 1.3230
Training Epoch: 17 [14592/50176]	Loss: 1.3522
Training Epoch: 17 [14848/50176]	Loss: 1.3661
Training Epoch: 17 [15104/50176]	Loss: 1.3353
Training Epoch: 17 [15360/50176]	Loss: 1.2592
Training Epoch: 17 [15616/50176]	Loss: 1.4195
Training Epoch: 17 [15872/50176]	Loss: 1.3001
Training Epoch: 17 [16128/50176]	Loss: 1.3092
Training Epoch: 17 [16384/50176]	Loss: 1.3830
Training Epoch: 17 [16640/50176]	Loss: 1.3863
Training Epoch: 17 [16896/50176]	Loss: 1.2167
Training Epoch: 17 [17152/50176]	Loss: 1.3440
Training Epoch: 17 [17408/50176]	Loss: 1.4027
Training Epoch: 17 [17664/50176]	Loss: 1.4105
Training Epoch: 17 [17920/50176]	Loss: 1.3100
Training Epoch: 17 [18176/50176]	Loss: 1.3586
Training Epoch: 17 [18432/50176]	Loss: 1.3833
Training Epoch: 17 [18688/50176]	Loss: 1.3592
Training Epoch: 17 [18944/50176]	Loss: 1.4909
Training Epoch: 17 [19200/50176]	Loss: 1.2746
Training Epoch: 17 [19456/50176]	Loss: 1.3168
Training Epoch: 17 [19712/50176]	Loss: 1.3561
Training Epoch: 17 [19968/50176]	Loss: 1.1925
Training Epoch: 17 [20224/50176]	Loss: 1.4319
Training Epoch: 17 [20480/50176]	Loss: 1.4178
Training Epoch: 17 [20736/50176]	Loss: 1.2610
Training Epoch: 17 [20992/50176]	Loss: 1.2724
Training Epoch: 17 [21248/50176]	Loss: 1.4147
Training Epoch: 17 [21504/50176]	Loss: 1.2001
Training Epoch: 17 [21760/50176]	Loss: 1.3892
Training Epoch: 17 [22016/50176]	Loss: 1.2391
Training Epoch: 17 [22272/50176]	Loss: 1.3816
Training Epoch: 17 [22528/50176]	Loss: 1.2266
Training Epoch: 17 [22784/50176]	Loss: 1.3089
Training Epoch: 17 [23040/50176]	Loss: 1.3640
Training Epoch: 17 [23296/50176]	Loss: 1.3358
Training Epoch: 17 [23552/50176]	Loss: 1.3664
Training Epoch: 17 [23808/50176]	Loss: 1.4274
Training Epoch: 17 [24064/50176]	Loss: 1.3037
Training Epoch: 17 [24320/50176]	Loss: 1.3849
Training Epoch: 17 [24576/50176]	Loss: 1.1694
Training Epoch: 17 [24832/50176]	Loss: 1.3320
Training Epoch: 17 [25088/50176]	Loss: 1.2903
Training Epoch: 17 [25344/50176]	Loss: 1.5500
Training Epoch: 17 [25600/50176]	Loss: 1.3316
Training Epoch: 17 [25856/50176]	Loss: 1.2560
Training Epoch: 17 [26112/50176]	Loss: 1.2433
Training Epoch: 17 [26368/50176]	Loss: 1.3274
Training Epoch: 17 [26624/50176]	Loss: 1.3000
Training Epoch: 17 [26880/50176]	Loss: 1.3735
Training Epoch: 17 [27136/50176]	Loss: 1.3039
Training Epoch: 17 [27392/50176]	Loss: 1.3361
Training Epoch: 17 [27648/50176]	Loss: 1.3477
Training Epoch: 17 [27904/50176]	Loss: 1.2420
Training Epoch: 17 [28160/50176]	Loss: 1.2691
Training Epoch: 17 [28416/50176]	Loss: 1.2134
Training Epoch: 17 [28672/50176]	Loss: 1.4024
Training Epoch: 17 [28928/50176]	Loss: 1.3644
Training Epoch: 17 [29184/50176]	Loss: 1.1049
Training Epoch: 17 [29440/50176]	Loss: 1.2900
Training Epoch: 17 [29696/50176]	Loss: 1.2013
Training Epoch: 17 [29952/50176]	Loss: 1.2689
Training Epoch: 17 [30208/50176]	Loss: 1.2231
Training Epoch: 17 [30464/50176]	Loss: 1.4125
Training Epoch: 17 [30720/50176]	Loss: 1.2106
Training Epoch: 17 [30976/50176]	Loss: 1.3170
Training Epoch: 17 [31232/50176]	Loss: 1.3542
Training Epoch: 17 [31488/50176]	Loss: 1.4030
Training Epoch: 17 [31744/50176]	Loss: 1.4688
Training Epoch: 17 [32000/50176]	Loss: 1.2591
Training Epoch: 17 [32256/50176]	Loss: 1.2310
Training Epoch: 17 [32512/50176]	Loss: 1.2336
Training Epoch: 17 [32768/50176]	Loss: 1.4872
Training Epoch: 17 [33024/50176]	Loss: 1.3910
Training Epoch: 17 [33280/50176]	Loss: 1.4981
Training Epoch: 17 [33536/50176]	Loss: 1.4227
Training Epoch: 17 [33792/50176]	Loss: 1.5030
Training Epoch: 17 [34048/50176]	Loss: 1.1592
Training Epoch: 17 [34304/50176]	Loss: 1.4197
Training Epoch: 17 [34560/50176]	Loss: 1.3681
Training Epoch: 17 [34816/50176]	Loss: 1.3564
Training Epoch: 17 [35072/50176]	Loss: 1.3720
Training Epoch: 17 [35328/50176]	Loss: 1.3577
Training Epoch: 17 [35584/50176]	Loss: 1.4436
Training Epoch: 17 [35840/50176]	Loss: 1.3352
Training Epoch: 17 [36096/50176]	Loss: 1.3296
Training Epoch: 17 [36352/50176]	Loss: 1.2517
Training Epoch: 17 [36608/50176]	Loss: 1.4312
Training Epoch: 17 [36864/50176]	Loss: 1.3144
Training Epoch: 17 [37120/50176]	Loss: 1.5182
Training Epoch: 17 [37376/50176]	Loss: 1.4504
Training Epoch: 17 [37632/50176]	Loss: 1.3283
Training Epoch: 17 [37888/50176]	Loss: 1.4609
Training Epoch: 17 [38144/50176]	Loss: 1.4613
Training Epoch: 17 [38400/50176]	Loss: 1.2828
Training Epoch: 17 [38656/50176]	Loss: 1.5260
Training Epoch: 17 [38912/50176]	Loss: 1.2569
Training Epoch: 17 [39168/50176]	Loss: 1.4070
Training Epoch: 17 [39424/50176]	Loss: 1.3028
Training Epoch: 17 [39680/50176]	Loss: 1.4282
Training Epoch: 17 [39936/50176]	Loss: 1.3687
Training Epoch: 17 [40192/50176]	Loss: 1.3636
Training Epoch: 17 [40448/50176]	Loss: 1.1406
Training Epoch: 17 [40704/50176]	Loss: 1.3694
Training Epoch: 17 [40960/50176]	Loss: 1.3828
Training Epoch: 17 [41216/50176]	Loss: 1.2895
Training Epoch: 17 [41472/50176]	Loss: 1.1935
Training Epoch: 17 [41728/50176]	Loss: 1.3096
Training Epoch: 17 [41984/50176]	Loss: 1.2646
Training Epoch: 17 [42240/50176]	Loss: 1.5349
Training Epoch: 17 [42496/50176]	Loss: 1.2884
Training Epoch: 17 [42752/50176]	Loss: 1.2330
Training Epoch: 17 [43008/50176]	Loss: 1.1853
Training Epoch: 17 [43264/50176]	Loss: 1.4586
Training Epoch: 17 [43520/50176]	Loss: 1.3828
Training Epoch: 17 [43776/50176]	Loss: 1.3938
Training Epoch: 17 [44032/50176]	Loss: 1.5232
Training Epoch: 17 [44288/50176]	Loss: 1.2191
Training Epoch: 17 [44544/50176]	Loss: 1.4512
Training Epoch: 17 [44800/50176]	Loss: 1.4421
Training Epoch: 17 [45056/50176]	Loss: 1.2323
Training Epoch: 17 [45312/50176]	Loss: 1.4027
Training Epoch: 17 [45568/50176]	Loss: 1.4829
Training Epoch: 17 [45824/50176]	Loss: 1.3642
Training Epoch: 17 [46080/50176]	Loss: 1.3606
Training Epoch: 17 [46336/50176]	Loss: 1.4241
Training Epoch: 17 [46592/50176]	Loss: 1.5009
Training Epoch: 17 [46848/50176]	Loss: 1.3934
Training Epoch: 17 [47104/50176]	Loss: 1.3911
Training Epoch: 17 [47360/50176]	Loss: 1.2103
Training Epoch: 17 [47616/50176]	Loss: 1.1973
Training Epoch: 17 [47872/50176]	Loss: 1.4159
Training Epoch: 17 [48128/50176]	Loss: 1.3322
Training Epoch: 17 [48384/50176]	Loss: 1.3365
Training Epoch: 17 [48640/50176]	Loss: 1.2061
Training Epoch: 17 [48896/50176]	Loss: 1.4392
Training Epoch: 17 [49152/50176]	Loss: 1.4305
Training Epoch: 17 [49408/50176]	Loss: 1.2403
Training Epoch: 17 [49664/50176]	Loss: 1.4044
Training Epoch: 17 [49920/50176]	Loss: 1.3314
Training Epoch: 17 [50176/50176]	Loss: 1.4101
Validation Epoch: 17, Average loss: 0.0076, Accuracy: 0.5079
Training Epoch: 18 [256/50176]	Loss: 1.3237
Training Epoch: 18 [512/50176]	Loss: 1.2192
Training Epoch: 18 [768/50176]	Loss: 1.2351
Training Epoch: 18 [1024/50176]	Loss: 1.1755
Training Epoch: 18 [1280/50176]	Loss: 1.2946
Training Epoch: 18 [1536/50176]	Loss: 1.2029
Training Epoch: 18 [1792/50176]	Loss: 1.2148
Training Epoch: 18 [2048/50176]	Loss: 1.3394
Training Epoch: 18 [2304/50176]	Loss: 1.1788
Training Epoch: 18 [2560/50176]	Loss: 1.2205
Training Epoch: 18 [2816/50176]	Loss: 1.3616
Training Epoch: 18 [3072/50176]	Loss: 1.4082
Training Epoch: 18 [3328/50176]	Loss: 1.2739
Training Epoch: 18 [3584/50176]	Loss: 1.3233
Training Epoch: 18 [3840/50176]	Loss: 1.2411
Training Epoch: 18 [4096/50176]	Loss: 1.2189
Training Epoch: 18 [4352/50176]	Loss: 1.2365
Training Epoch: 18 [4608/50176]	Loss: 1.1450
Training Epoch: 18 [4864/50176]	Loss: 1.3285
Training Epoch: 18 [5120/50176]	Loss: 1.4203
Training Epoch: 18 [5376/50176]	Loss: 1.2652
Training Epoch: 18 [5632/50176]	Loss: 1.0531
Training Epoch: 18 [5888/50176]	Loss: 1.2519
Training Epoch: 18 [6144/50176]	Loss: 1.0896
Training Epoch: 18 [6400/50176]	Loss: 1.3458
Training Epoch: 18 [6656/50176]	Loss: 1.2866
Training Epoch: 18 [6912/50176]	Loss: 1.1051
Training Epoch: 18 [7168/50176]	Loss: 1.3449
Training Epoch: 18 [7424/50176]	Loss: 1.2963
Training Epoch: 18 [7680/50176]	Loss: 1.2229
Training Epoch: 18 [7936/50176]	Loss: 1.3132
Training Epoch: 18 [8192/50176]	Loss: 1.2176
Training Epoch: 18 [8448/50176]	Loss: 1.2618
Training Epoch: 18 [8704/50176]	Loss: 1.2740
Training Epoch: 18 [8960/50176]	Loss: 1.1804
Training Epoch: 18 [9216/50176]	Loss: 1.2016
Training Epoch: 18 [9472/50176]	Loss: 1.3485
Training Epoch: 18 [9728/50176]	Loss: 1.2845
Training Epoch: 18 [9984/50176]	Loss: 1.3791
Training Epoch: 18 [10240/50176]	Loss: 1.2479
Training Epoch: 18 [10496/50176]	Loss: 1.2253
Training Epoch: 18 [10752/50176]	Loss: 1.4423
Training Epoch: 18 [11008/50176]	Loss: 1.2820
Training Epoch: 18 [11264/50176]	Loss: 1.3185
Training Epoch: 18 [11520/50176]	Loss: 1.3168
Training Epoch: 18 [11776/50176]	Loss: 1.1748
Training Epoch: 18 [12032/50176]	Loss: 1.2758
Training Epoch: 18 [12288/50176]	Loss: 1.2321
Training Epoch: 18 [12544/50176]	Loss: 1.2839
Training Epoch: 18 [12800/50176]	Loss: 1.3107
Training Epoch: 18 [13056/50176]	Loss: 1.3112
Training Epoch: 18 [13312/50176]	Loss: 1.2524
Training Epoch: 18 [13568/50176]	Loss: 1.1871
Training Epoch: 18 [13824/50176]	Loss: 1.1250
Training Epoch: 18 [14080/50176]	Loss: 1.2398
Training Epoch: 18 [14336/50176]	Loss: 1.2495
Training Epoch: 18 [14592/50176]	Loss: 1.3245
Training Epoch: 18 [14848/50176]	Loss: 1.2998
Training Epoch: 18 [15104/50176]	Loss: 1.2747
Training Epoch: 18 [15360/50176]	Loss: 1.2440
Training Epoch: 18 [15616/50176]	Loss: 1.4019
Training Epoch: 18 [15872/50176]	Loss: 1.2598
Training Epoch: 18 [16128/50176]	Loss: 1.1989
Training Epoch: 18 [16384/50176]	Loss: 1.2505
Training Epoch: 18 [16640/50176]	Loss: 1.3227
Training Epoch: 18 [16896/50176]	Loss: 1.2518
Training Epoch: 18 [17152/50176]	Loss: 1.2992
Training Epoch: 18 [17408/50176]	Loss: 1.2075
Training Epoch: 18 [17664/50176]	Loss: 1.3174
Training Epoch: 18 [17920/50176]	Loss: 1.2742
Training Epoch: 18 [18176/50176]	Loss: 1.3466
Training Epoch: 18 [18432/50176]	Loss: 1.2428
Training Epoch: 18 [18688/50176]	Loss: 1.1466
Training Epoch: 18 [18944/50176]	Loss: 1.2609
Training Epoch: 18 [19200/50176]	Loss: 1.3543
Training Epoch: 18 [19456/50176]	Loss: 1.4018
Training Epoch: 18 [19712/50176]	Loss: 1.1572
Training Epoch: 18 [19968/50176]	Loss: 1.2624
Training Epoch: 18 [20224/50176]	Loss: 1.2076
Training Epoch: 18 [20480/50176]	Loss: 1.2515
Training Epoch: 18 [20736/50176]	Loss: 1.2623
Training Epoch: 18 [20992/50176]	Loss: 1.1725
Training Epoch: 18 [21248/50176]	Loss: 1.2161
Training Epoch: 18 [21504/50176]	Loss: 1.2136
Training Epoch: 18 [21760/50176]	Loss: 1.2851
Training Epoch: 18 [22016/50176]	Loss: 1.2364
Training Epoch: 18 [22272/50176]	Loss: 1.4504
Training Epoch: 18 [22528/50176]	Loss: 1.1032
Training Epoch: 18 [22784/50176]	Loss: 1.4140
Training Epoch: 18 [23040/50176]	Loss: 1.4002
Training Epoch: 18 [23296/50176]	Loss: 1.2268
Training Epoch: 18 [23552/50176]	Loss: 1.3803
Training Epoch: 18 [23808/50176]	Loss: 1.4816
Training Epoch: 18 [24064/50176]	Loss: 1.2944
Training Epoch: 18 [24320/50176]	Loss: 1.2500
Training Epoch: 18 [24576/50176]	Loss: 1.3229
Training Epoch: 18 [24832/50176]	Loss: 1.2394
Training Epoch: 18 [25088/50176]	Loss: 1.4246
Training Epoch: 18 [25344/50176]	Loss: 1.1919
Training Epoch: 18 [25600/50176]	Loss: 1.2499
Training Epoch: 18 [25856/50176]	Loss: 1.2990
Training Epoch: 18 [26112/50176]	Loss: 1.4744
Training Epoch: 18 [26368/50176]	Loss: 1.1342
Training Epoch: 18 [26624/50176]	Loss: 1.1102
Training Epoch: 18 [26880/50176]	Loss: 1.4101
Training Epoch: 18 [27136/50176]	Loss: 1.3014
Training Epoch: 18 [27392/50176]	Loss: 1.3298
Training Epoch: 18 [27648/50176]	Loss: 1.4250
Training Epoch: 18 [27904/50176]	Loss: 1.2542
Training Epoch: 18 [28160/50176]	Loss: 1.3939
Training Epoch: 18 [28416/50176]	Loss: 1.3089
Training Epoch: 18 [28672/50176]	Loss: 1.4282
Training Epoch: 18 [28928/50176]	Loss: 1.2650
Training Epoch: 18 [29184/50176]	Loss: 1.3260
Training Epoch: 18 [29440/50176]	Loss: 1.3403
Training Epoch: 18 [29696/50176]	Loss: 1.1198
Training Epoch: 18 [29952/50176]	Loss: 1.2311
Training Epoch: 18 [30208/50176]	Loss: 1.3181
Training Epoch: 18 [30464/50176]	Loss: 1.2558
Training Epoch: 18 [30720/50176]	Loss: 1.1729
Training Epoch: 18 [30976/50176]	Loss: 1.2748
Training Epoch: 18 [31232/50176]	Loss: 1.2614
Training Epoch: 18 [31488/50176]	Loss: 1.4206
Training Epoch: 18 [31744/50176]	Loss: 1.3886
Training Epoch: 18 [32000/50176]	Loss: 1.1549
Training Epoch: 18 [32256/50176]	Loss: 1.2898
Training Epoch: 18 [32512/50176]	Loss: 1.3155
Training Epoch: 18 [32768/50176]	Loss: 1.2681
Training Epoch: 18 [33024/50176]	Loss: 1.2647
Training Epoch: 18 [33280/50176]	Loss: 1.0816
Training Epoch: 18 [33536/50176]	Loss: 1.4676
Training Epoch: 18 [33792/50176]	Loss: 1.4278
Training Epoch: 18 [34048/50176]	Loss: 1.3645
Training Epoch: 18 [34304/50176]	Loss: 1.4337
Training Epoch: 18 [34560/50176]	Loss: 1.3948
Training Epoch: 18 [34816/50176]	Loss: 1.3389
Training Epoch: 18 [35072/50176]	Loss: 1.3477
Training Epoch: 18 [35328/50176]	Loss: 1.0604
Training Epoch: 18 [35584/50176]	Loss: 1.3012
Training Epoch: 18 [35840/50176]	Loss: 1.2812
Training Epoch: 18 [36096/50176]	Loss: 1.3896
Training Epoch: 18 [36352/50176]	Loss: 1.2157
Training Epoch: 18 [36608/50176]	Loss: 1.4004
Training Epoch: 18 [36864/50176]	Loss: 1.4198
Training Epoch: 18 [37120/50176]	Loss: 1.4125
Training Epoch: 18 [37376/50176]	Loss: 1.3833
Training Epoch: 18 [37632/50176]	Loss: 1.3484
Training Epoch: 18 [37888/50176]	Loss: 1.3428
Training Epoch: 18 [38144/50176]	Loss: 1.4639
Training Epoch: 18 [38400/50176]	Loss: 1.4855
Training Epoch: 18 [38656/50176]	Loss: 1.3495
Training Epoch: 18 [38912/50176]	Loss: 1.2545
Training Epoch: 18 [39168/50176]	Loss: 1.2825
Training Epoch: 18 [39424/50176]	Loss: 1.3631
Training Epoch: 18 [39680/50176]	Loss: 1.3986
Training Epoch: 18 [39936/50176]	Loss: 1.3221
Training Epoch: 18 [40192/50176]	Loss: 1.2696
Training Epoch: 18 [40448/50176]	Loss: 1.2967
Training Epoch: 18 [40704/50176]	Loss: 1.3916
Training Epoch: 18 [40960/50176]	Loss: 1.3152
Training Epoch: 18 [41216/50176]	Loss: 1.3659
Training Epoch: 18 [41472/50176]	Loss: 1.3006
Training Epoch: 18 [41728/50176]	Loss: 1.2777
Training Epoch: 18 [41984/50176]	Loss: 1.3788
Training Epoch: 18 [42240/50176]	Loss: 1.5215
Training Epoch: 18 [42496/50176]	Loss: 1.3068
Training Epoch: 18 [42752/50176]	Loss: 1.2977
Training Epoch: 18 [43008/50176]	Loss: 1.1785
Training Epoch: 18 [43264/50176]	Loss: 1.3296
Training Epoch: 18 [43520/50176]	Loss: 1.1864
Training Epoch: 18 [43776/50176]	Loss: 1.4515
Training Epoch: 18 [44032/50176]	Loss: 1.4141
Training Epoch: 18 [44288/50176]	Loss: 1.4759
Training Epoch: 18 [44544/50176]	Loss: 1.2008
Training Epoch: 18 [44800/50176]	Loss: 1.3106
Training Epoch: 18 [45056/50176]	Loss: 1.3049
Training Epoch: 18 [45312/50176]	Loss: 1.2545
Training Epoch: 18 [45568/50176]	Loss: 1.2447
Training Epoch: 18 [45824/50176]	Loss: 1.1646
Training Epoch: 18 [46080/50176]	Loss: 1.3365
Training Epoch: 18 [46336/50176]	Loss: 1.3372
Training Epoch: 18 [46592/50176]	Loss: 1.2062
Training Epoch: 18 [46848/50176]	Loss: 1.2826
Training Epoch: 18 [47104/50176]	Loss: 1.2828
Training Epoch: 18 [47360/50176]	Loss: 1.2127
Training Epoch: 18 [47616/50176]	Loss: 1.3405
Training Epoch: 18 [47872/50176]	Loss: 1.2518
Training Epoch: 18 [48128/50176]	Loss: 1.4969
Training Epoch: 18 [48384/50176]	Loss: 1.3657
Training Epoch: 18 [48640/50176]	Loss: 1.1193
Training Epoch: 18 [48896/50176]	Loss: 1.3134
Training Epoch: 18 [49152/50176]	Loss: 1.2254
Training Epoch: 18 [49408/50176]	Loss: 1.2018
Training Epoch: 18 [49664/50176]	Loss: 1.4046
Training Epoch: 18 [49920/50176]	Loss: 1.2430
Training Epoch: 18 [50176/50176]	Loss: 1.1695
Validation Epoch: 18, Average loss: 0.0076, Accuracy: 0.5093
Training Epoch: 19 [256/50176]	Loss: 1.2940
Training Epoch: 19 [512/50176]	Loss: 1.2310
Training Epoch: 19 [768/50176]	Loss: 1.2333
Training Epoch: 19 [1024/50176]	Loss: 1.2375
Training Epoch: 19 [1280/50176]	Loss: 1.1414
Training Epoch: 19 [1536/50176]	Loss: 1.3352
Training Epoch: 19 [1792/50176]	Loss: 1.2097
Training Epoch: 19 [2048/50176]	Loss: 1.2778
Training Epoch: 19 [2304/50176]	Loss: 1.2839
Training Epoch: 19 [2560/50176]	Loss: 1.1962
Training Epoch: 19 [2816/50176]	Loss: 1.2614
Training Epoch: 19 [3072/50176]	Loss: 1.1681
Training Epoch: 19 [3328/50176]	Loss: 1.2008
Training Epoch: 19 [3584/50176]	Loss: 1.2823
Training Epoch: 19 [3840/50176]	Loss: 1.2568
Training Epoch: 19 [4096/50176]	Loss: 1.0592
Training Epoch: 19 [4352/50176]	Loss: 1.2224
Training Epoch: 19 [4608/50176]	Loss: 1.1881
Training Epoch: 19 [4864/50176]	Loss: 1.1038
Training Epoch: 19 [5120/50176]	Loss: 1.2730
Training Epoch: 19 [5376/50176]	Loss: 1.0887
Training Epoch: 19 [5632/50176]	Loss: 1.2104
Training Epoch: 19 [5888/50176]	Loss: 1.0752
Training Epoch: 19 [6144/50176]	Loss: 1.1216
Training Epoch: 19 [6400/50176]	Loss: 1.2204
Training Epoch: 19 [6656/50176]	Loss: 1.2715
Training Epoch: 19 [6912/50176]	Loss: 1.1123
Training Epoch: 19 [7168/50176]	Loss: 1.3536
Training Epoch: 19 [7424/50176]	Loss: 1.1942
Training Epoch: 19 [7680/50176]	Loss: 1.1806
Training Epoch: 19 [7936/50176]	Loss: 1.2854
Training Epoch: 19 [8192/50176]	Loss: 1.1439
Training Epoch: 19 [8448/50176]	Loss: 1.1464
Training Epoch: 19 [8704/50176]	Loss: 1.1847
Training Epoch: 19 [8960/50176]	Loss: 1.2237
Training Epoch: 19 [9216/50176]	Loss: 1.1193
Training Epoch: 19 [9472/50176]	Loss: 1.0313
Training Epoch: 19 [9728/50176]	Loss: 1.2327
Training Epoch: 19 [9984/50176]	Loss: 1.2491
Training Epoch: 19 [10240/50176]	Loss: 1.2909
Training Epoch: 19 [10496/50176]	Loss: 1.2592
Training Epoch: 19 [10752/50176]	Loss: 1.1412
Training Epoch: 19 [11008/50176]	Loss: 1.2467
Training Epoch: 19 [11264/50176]	Loss: 1.2255
Training Epoch: 19 [11520/50176]	Loss: 1.1728
Training Epoch: 19 [11776/50176]	Loss: 1.2452
Training Epoch: 19 [12032/50176]	Loss: 1.0437
Training Epoch: 19 [12288/50176]	Loss: 1.2060
Training Epoch: 19 [12544/50176]	Loss: 1.2360
Training Epoch: 19 [12800/50176]	Loss: 1.3376
Training Epoch: 19 [13056/50176]	Loss: 1.3881
Training Epoch: 19 [13312/50176]	Loss: 1.1698
Training Epoch: 19 [13568/50176]	Loss: 1.2647
Training Epoch: 19 [13824/50176]	Loss: 1.3691
Training Epoch: 19 [14080/50176]	Loss: 1.2847
Training Epoch: 19 [14336/50176]	Loss: 1.3851
Training Epoch: 19 [14592/50176]	Loss: 1.3111
Training Epoch: 19 [14848/50176]	Loss: 1.2123
Training Epoch: 19 [15104/50176]	Loss: 1.3222
Training Epoch: 19 [15360/50176]	Loss: 1.1096
Training Epoch: 19 [15616/50176]	Loss: 1.3267
Training Epoch: 19 [15872/50176]	Loss: 1.2933
Training Epoch: 19 [16128/50176]	Loss: 1.1728
Training Epoch: 19 [16384/50176]	Loss: 1.0892
Training Epoch: 19 [16640/50176]	Loss: 1.2982
Training Epoch: 19 [16896/50176]	Loss: 1.1304
Training Epoch: 19 [17152/50176]	Loss: 1.1152
Training Epoch: 19 [17408/50176]	Loss: 1.1484
Training Epoch: 19 [17664/50176]	Loss: 1.1836
Training Epoch: 19 [17920/50176]	Loss: 1.3501
Training Epoch: 19 [18176/50176]	Loss: 1.2982
Training Epoch: 19 [18432/50176]	Loss: 1.2764
Training Epoch: 19 [18688/50176]	Loss: 1.2532
Training Epoch: 19 [18944/50176]	Loss: 1.1991
Training Epoch: 19 [19200/50176]	Loss: 1.2902
Training Epoch: 19 [19456/50176]	Loss: 1.3137
Training Epoch: 19 [19712/50176]	Loss: 1.1967
Training Epoch: 19 [19968/50176]	Loss: 1.2674
Training Epoch: 19 [20224/50176]	Loss: 1.2258
Training Epoch: 19 [20480/50176]	Loss: 1.4274
Training Epoch: 19 [20736/50176]	Loss: 1.1871
Training Epoch: 19 [20992/50176]	Loss: 1.2281
Training Epoch: 19 [21248/50176]	Loss: 1.3752
Training Epoch: 19 [21504/50176]	Loss: 1.2554
Training Epoch: 19 [21760/50176]	Loss: 1.2237
Training Epoch: 19 [22016/50176]	Loss: 1.2970
Training Epoch: 19 [22272/50176]	Loss: 1.1200
Training Epoch: 19 [22528/50176]	Loss: 1.1728
Training Epoch: 19 [22784/50176]	Loss: 1.2019
Training Epoch: 19 [23040/50176]	Loss: 1.1747
Training Epoch: 19 [23296/50176]	Loss: 1.2028
Training Epoch: 19 [23552/50176]	Loss: 1.3892
Training Epoch: 19 [23808/50176]	Loss: 1.2808
Training Epoch: 19 [24064/50176]	Loss: 1.2706
Training Epoch: 19 [24320/50176]	Loss: 1.2297
Training Epoch: 19 [24576/50176]	Loss: 1.3894
Training Epoch: 19 [24832/50176]	Loss: 1.2780
Training Epoch: 19 [25088/50176]	Loss: 1.4959
Training Epoch: 19 [25344/50176]	Loss: 1.3389
Training Epoch: 19 [25600/50176]	Loss: 1.2998
Training Epoch: 19 [25856/50176]	Loss: 1.1149
Training Epoch: 19 [26112/50176]	Loss: 1.2706
Training Epoch: 19 [26368/50176]	Loss: 1.3121
Training Epoch: 19 [26624/50176]	Loss: 1.3197
Training Epoch: 19 [26880/50176]	Loss: 1.1586
Training Epoch: 19 [27136/50176]	Loss: 1.1651
Training Epoch: 19 [27392/50176]	Loss: 1.1089
Training Epoch: 19 [27648/50176]	Loss: 1.4161
Training Epoch: 19 [27904/50176]	Loss: 1.3250
Training Epoch: 19 [28160/50176]	Loss: 1.3303
Training Epoch: 19 [28416/50176]	Loss: 1.2355
Training Epoch: 19 [28672/50176]	Loss: 1.3185
Training Epoch: 19 [28928/50176]	Loss: 1.2646
Training Epoch: 19 [29184/50176]	Loss: 1.2941
Training Epoch: 19 [29440/50176]	Loss: 1.2995
Training Epoch: 19 [29696/50176]	Loss: 1.1438
Training Epoch: 19 [29952/50176]	Loss: 1.3374
Training Epoch: 19 [30208/50176]	Loss: 1.1509
Training Epoch: 19 [30464/50176]	Loss: 1.1068
Training Epoch: 19 [30720/50176]	Loss: 1.2442
Training Epoch: 19 [30976/50176]	Loss: 1.3993
Training Epoch: 19 [31232/50176]	Loss: 1.1985
Training Epoch: 19 [31488/50176]	Loss: 1.4088
Training Epoch: 19 [31744/50176]	Loss: 1.3536
Training Epoch: 19 [32000/50176]	Loss: 1.3585
Training Epoch: 19 [32256/50176]	Loss: 1.2715
Training Epoch: 19 [32512/50176]	Loss: 1.4425
Training Epoch: 19 [32768/50176]	Loss: 1.2880
Training Epoch: 19 [33024/50176]	Loss: 1.3277
Training Epoch: 19 [33280/50176]	Loss: 1.2335
Training Epoch: 19 [33536/50176]	Loss: 1.2861
Training Epoch: 19 [33792/50176]	Loss: 1.3459
Training Epoch: 19 [34048/50176]	Loss: 1.3135
Training Epoch: 19 [34304/50176]	Loss: 1.2332
Training Epoch: 19 [34560/50176]	Loss: 1.3504
Training Epoch: 19 [34816/50176]	Loss: 1.2312
Training Epoch: 19 [35072/50176]	Loss: 1.1090
Training Epoch: 19 [35328/50176]	Loss: 1.2084
Training Epoch: 19 [35584/50176]	Loss: 1.1590
Training Epoch: 19 [35840/50176]	Loss: 1.1894
Training Epoch: 19 [36096/50176]	Loss: 1.3202
Training Epoch: 19 [36352/50176]	Loss: 1.1957
Training Epoch: 19 [36608/50176]	Loss: 1.1232
Training Epoch: 19 [36864/50176]	Loss: 1.2338
Training Epoch: 19 [37120/50176]	Loss: 1.1611
Training Epoch: 19 [37376/50176]	Loss: 1.1825
Training Epoch: 19 [37632/50176]	Loss: 1.2339
Training Epoch: 19 [37888/50176]	Loss: 1.3284
Training Epoch: 19 [38144/50176]	Loss: 1.2836
Training Epoch: 19 [38400/50176]	Loss: 1.2387
Training Epoch: 19 [38656/50176]	Loss: 1.1866
Training Epoch: 19 [38912/50176]	Loss: 1.2224
Training Epoch: 19 [39168/50176]	Loss: 1.2657
Training Epoch: 19 [39424/50176]	Loss: 1.2511
Training Epoch: 19 [39680/50176]	Loss: 1.1674
Training Epoch: 19 [39936/50176]	Loss: 1.2075
Training Epoch: 19 [40192/50176]	Loss: 1.3387
Training Epoch: 19 [40448/50176]	Loss: 1.1277
Training Epoch: 19 [40704/50176]	Loss: 1.0268
Training Epoch: 19 [40960/50176]	Loss: 1.2037
Training Epoch: 19 [41216/50176]	Loss: 1.1916
Training Epoch: 19 [41472/50176]	Loss: 1.1425
Training Epoch: 19 [41728/50176]	Loss: 1.1808
Training Epoch: 19 [41984/50176]	Loss: 1.4342
Training Epoch: 19 [42240/50176]	Loss: 1.1129
Training Epoch: 19 [42496/50176]	Loss: 1.2929
Training Epoch: 19 [42752/50176]	Loss: 1.2240
Training Epoch: 19 [43008/50176]	Loss: 1.3271
Training Epoch: 19 [43264/50176]	Loss: 1.3339
Training Epoch: 19 [43520/50176]	Loss: 1.2936
Training Epoch: 19 [43776/50176]	Loss: 1.3061
Training Epoch: 19 [44032/50176]	Loss: 1.2855
Training Epoch: 19 [44288/50176]	Loss: 1.3845
Training Epoch: 19 [44544/50176]	Loss: 1.2084
Training Epoch: 19 [44800/50176]	Loss: 1.2338
Training Epoch: 19 [45056/50176]	Loss: 1.2209
Training Epoch: 19 [45312/50176]	Loss: 1.3475
Training Epoch: 19 [45568/50176]	Loss: 1.3180
Training Epoch: 19 [45824/50176]	Loss: 1.3343
Training Epoch: 19 [46080/50176]	Loss: 1.1903
Training Epoch: 19 [46336/50176]	Loss: 1.2637
Training Epoch: 19 [46592/50176]	Loss: 1.1681
Training Epoch: 19 [46848/50176]	Loss: 1.1935
Training Epoch: 19 [47104/50176]	Loss: 1.1930
Training Epoch: 19 [47360/50176]	Loss: 1.1690
Training Epoch: 19 [47616/50176]	Loss: 1.3025
Training Epoch: 19 [47872/50176]	Loss: 1.3935
Training Epoch: 19 [48128/50176]	Loss: 1.2207
Training Epoch: 19 [48384/50176]	Loss: 1.4024
Training Epoch: 19 [48640/50176]	Loss: 1.2818
Training Epoch: 19 [48896/50176]	Loss: 1.3496
Training Epoch: 19 [49152/50176]	Loss: 1.0488
Training Epoch: 19 [49408/50176]	Loss: 1.2625
Training Epoch: 19 [49664/50176]	Loss: 1.1823
Training Epoch: 19 [49920/50176]	Loss: 1.3045
Training Epoch: 19 [50176/50176]	Loss: 1.1296
Validation Epoch: 19, Average loss: 0.0070, Accuracy: 0.5322
Training Epoch: 20 [256/50176]	Loss: 1.3109
Training Epoch: 20 [512/50176]	Loss: 1.1726
Training Epoch: 20 [768/50176]	Loss: 1.1606
Training Epoch: 20 [1024/50176]	Loss: 1.1336
Training Epoch: 20 [1280/50176]	Loss: 1.0740
Training Epoch: 20 [1536/50176]	Loss: 1.1985
Training Epoch: 20 [1792/50176]	Loss: 1.1347
Training Epoch: 20 [2048/50176]	Loss: 1.2190
Training Epoch: 20 [2304/50176]	Loss: 1.2384
Training Epoch: 20 [2560/50176]	Loss: 1.3587
Training Epoch: 20 [2816/50176]	Loss: 1.3089
Training Epoch: 20 [3072/50176]	Loss: 1.1567
Training Epoch: 20 [3328/50176]	Loss: 1.1291
Training Epoch: 20 [3584/50176]	Loss: 1.0289
Training Epoch: 20 [3840/50176]	Loss: 1.2669
Training Epoch: 20 [4096/50176]	Loss: 1.3294
Training Epoch: 20 [4352/50176]	Loss: 1.0907
Training Epoch: 20 [4608/50176]	Loss: 1.3017
Training Epoch: 20 [4864/50176]	Loss: 1.3262
Training Epoch: 20 [5120/50176]	Loss: 1.2041
Training Epoch: 20 [5376/50176]	Loss: 1.2377
Training Epoch: 20 [5632/50176]	Loss: 1.2656
Training Epoch: 20 [5888/50176]	Loss: 0.9784
Training Epoch: 20 [6144/50176]	Loss: 1.0517
Training Epoch: 20 [6400/50176]	Loss: 1.1842
Training Epoch: 20 [6656/50176]	Loss: 1.2898
Training Epoch: 20 [6912/50176]	Loss: 1.1014
Training Epoch: 20 [7168/50176]	Loss: 1.2013
Training Epoch: 20 [7424/50176]	Loss: 1.0512
Training Epoch: 20 [7680/50176]	Loss: 1.0896
Training Epoch: 20 [7936/50176]	Loss: 1.2084
Training Epoch: 20 [8192/50176]	Loss: 1.2551
Training Epoch: 20 [8448/50176]	Loss: 1.2094
Training Epoch: 20 [8704/50176]	Loss: 1.3200
Training Epoch: 20 [8960/50176]	Loss: 1.1224
Training Epoch: 20 [9216/50176]	Loss: 1.1926
Training Epoch: 20 [9472/50176]	Loss: 0.9670
Training Epoch: 20 [9728/50176]	Loss: 1.2805
Training Epoch: 20 [9984/50176]	Loss: 1.2146
Training Epoch: 20 [10240/50176]	Loss: 1.1535
Training Epoch: 20 [10496/50176]	Loss: 1.1415
Training Epoch: 20 [10752/50176]	Loss: 1.0688
Training Epoch: 20 [11008/50176]	Loss: 1.1833
Training Epoch: 20 [11264/50176]	Loss: 1.1779
Training Epoch: 20 [11520/50176]	Loss: 1.2013
Training Epoch: 20 [11776/50176]	Loss: 1.0302
Training Epoch: 20 [12032/50176]	Loss: 1.0322
Training Epoch: 20 [12288/50176]	Loss: 1.0512
Training Epoch: 20 [12544/50176]	Loss: 0.9608
Training Epoch: 20 [12800/50176]	Loss: 1.2336
Training Epoch: 20 [13056/50176]	Loss: 1.0706
Training Epoch: 20 [13312/50176]	Loss: 1.1730
Training Epoch: 20 [13568/50176]	Loss: 1.2877
Training Epoch: 20 [13824/50176]	Loss: 1.3720
Training Epoch: 20 [14080/50176]	Loss: 1.2103
Training Epoch: 20 [14336/50176]	Loss: 1.1439
Training Epoch: 20 [14592/50176]	Loss: 1.1404
Training Epoch: 20 [14848/50176]	Loss: 1.1886
Training Epoch: 20 [15104/50176]	Loss: 1.2945
Training Epoch: 20 [15360/50176]	Loss: 1.0269
Training Epoch: 20 [15616/50176]	Loss: 1.0611
Training Epoch: 20 [15872/50176]	Loss: 1.1545
Training Epoch: 20 [16128/50176]	Loss: 1.1739
Training Epoch: 20 [16384/50176]	Loss: 1.1676
Training Epoch: 20 [16640/50176]	Loss: 1.2391
Training Epoch: 20 [16896/50176]	Loss: 1.2364
Training Epoch: 20 [17152/50176]	Loss: 1.3231
Training Epoch: 20 [17408/50176]	Loss: 1.4243
Training Epoch: 20 [17664/50176]	Loss: 1.0984
Training Epoch: 20 [17920/50176]	Loss: 1.2435
Training Epoch: 20 [18176/50176]	Loss: 1.1762
Training Epoch: 20 [18432/50176]	Loss: 1.2693
Training Epoch: 20 [18688/50176]	Loss: 1.2090
Training Epoch: 20 [18944/50176]	Loss: 1.1442
Training Epoch: 20 [19200/50176]	Loss: 1.1717
Training Epoch: 20 [19456/50176]	Loss: 1.2770
Training Epoch: 20 [19712/50176]	Loss: 1.1756
Training Epoch: 20 [19968/50176]	Loss: 1.2773
Training Epoch: 20 [20224/50176]	Loss: 1.0031
Training Epoch: 20 [20480/50176]	Loss: 1.2456
Training Epoch: 20 [20736/50176]	Loss: 1.3749
Training Epoch: 20 [20992/50176]	Loss: 1.0606
Training Epoch: 20 [21248/50176]	Loss: 1.0378
Training Epoch: 20 [21504/50176]	Loss: 1.1922
Training Epoch: 20 [21760/50176]	Loss: 1.1653
Training Epoch: 20 [22016/50176]	Loss: 1.1894
Training Epoch: 20 [22272/50176]	Loss: 1.3070
Training Epoch: 20 [22528/50176]	Loss: 1.3636
Training Epoch: 20 [22784/50176]	Loss: 1.0183
Training Epoch: 20 [23040/50176]	Loss: 1.3014
Training Epoch: 20 [23296/50176]	Loss: 1.3364
Training Epoch: 20 [23552/50176]	Loss: 1.4378
Training Epoch: 20 [23808/50176]	Loss: 1.2155
Training Epoch: 20 [24064/50176]	Loss: 1.1905
Training Epoch: 20 [24320/50176]	Loss: 1.0784
Training Epoch: 20 [24576/50176]	Loss: 1.2143
Training Epoch: 20 [24832/50176]	Loss: 1.3031
Training Epoch: 20 [25088/50176]	Loss: 0.9849
Training Epoch: 20 [25344/50176]	Loss: 1.1473
Training Epoch: 20 [25600/50176]	Loss: 1.1340
Training Epoch: 20 [25856/50176]	Loss: 1.1872
Training Epoch: 20 [26112/50176]	Loss: 1.0681
Training Epoch: 20 [26368/50176]	Loss: 1.2107
Training Epoch: 20 [26624/50176]	Loss: 1.1566
Training Epoch: 20 [26880/50176]	Loss: 1.1168
Training Epoch: 20 [27136/50176]	Loss: 1.3615
Training Epoch: 20 [27392/50176]	Loss: 1.0282
Training Epoch: 20 [27648/50176]	Loss: 1.3166
Training Epoch: 20 [27904/50176]	Loss: 1.2583
Training Epoch: 20 [28160/50176]	Loss: 1.1426
Training Epoch: 20 [28416/50176]	Loss: 1.3022
Training Epoch: 20 [28672/50176]	Loss: 1.2106
Training Epoch: 20 [28928/50176]	Loss: 1.1356
Training Epoch: 20 [29184/50176]	Loss: 1.2538
Training Epoch: 20 [29440/50176]	Loss: 1.3453
Training Epoch: 20 [29696/50176]	Loss: 1.1414
Training Epoch: 20 [29952/50176]	Loss: 1.3386
Training Epoch: 20 [30208/50176]	Loss: 1.3995
Training Epoch: 20 [30464/50176]	Loss: 1.3187
Training Epoch: 20 [30720/50176]	Loss: 1.2061
Training Epoch: 20 [30976/50176]	Loss: 1.0353
Training Epoch: 20 [31232/50176]	Loss: 1.2400
Training Epoch: 20 [31488/50176]	Loss: 1.1826
Training Epoch: 20 [31744/50176]	Loss: 1.3119
Training Epoch: 20 [32000/50176]	Loss: 1.3187
Training Epoch: 20 [32256/50176]	Loss: 1.1519
Training Epoch: 20 [32512/50176]	Loss: 1.1798
Training Epoch: 20 [32768/50176]	Loss: 1.3329
Training Epoch: 20 [33024/50176]	Loss: 1.4220
Training Epoch: 20 [33280/50176]	Loss: 1.2593
Training Epoch: 20 [33536/50176]	Loss: 1.2696
Training Epoch: 20 [33792/50176]	Loss: 1.2106
Training Epoch: 20 [34048/50176]	Loss: 1.3502
Training Epoch: 20 [34304/50176]	Loss: 1.4098
Training Epoch: 20 [34560/50176]	Loss: 1.2368
Training Epoch: 20 [34816/50176]	Loss: 1.3259
Training Epoch: 20 [35072/50176]	Loss: 1.2030
Training Epoch: 20 [35328/50176]	Loss: 1.2392
Training Epoch: 20 [35584/50176]	Loss: 1.1998
Training Epoch: 20 [35840/50176]	Loss: 1.1765
Training Epoch: 20 [36096/50176]	Loss: 1.1079
Training Epoch: 20 [36352/50176]	Loss: 1.2575
Training Epoch: 20 [36608/50176]	Loss: 1.2444
Training Epoch: 20 [36864/50176]	Loss: 1.2834
Training Epoch: 20 [37120/50176]	Loss: 1.2052
Training Epoch: 20 [37376/50176]	Loss: 1.2271
Training Epoch: 20 [37632/50176]	Loss: 1.4325
Training Epoch: 20 [37888/50176]	Loss: 1.3005
Training Epoch: 20 [38144/50176]	Loss: 1.1005
Training Epoch: 20 [38400/50176]	Loss: 1.2261
Training Epoch: 20 [38656/50176]	Loss: 1.3170
Training Epoch: 20 [38912/50176]	Loss: 1.2760
Training Epoch: 20 [39168/50176]	Loss: 1.3631
Training Epoch: 20 [39424/50176]	Loss: 1.0754
Training Epoch: 20 [39680/50176]	Loss: 1.1153
Training Epoch: 20 [39936/50176]	Loss: 1.1809
Training Epoch: 20 [40192/50176]	Loss: 1.0468
Training Epoch: 20 [40448/50176]	Loss: 1.2517
Training Epoch: 20 [40704/50176]	Loss: 1.3388
Training Epoch: 20 [40960/50176]	Loss: 1.2648
Training Epoch: 20 [41216/50176]	Loss: 1.2179
Training Epoch: 20 [41472/50176]	Loss: 1.0932
Training Epoch: 20 [41728/50176]	Loss: 1.1778
Training Epoch: 20 [41984/50176]	Loss: 1.1139
Training Epoch: 20 [42240/50176]	Loss: 1.0810
Training Epoch: 20 [42496/50176]	Loss: 1.1646
Training Epoch: 20 [42752/50176]	Loss: 1.3147
Training Epoch: 20 [43008/50176]	Loss: 1.3488
Training Epoch: 20 [43264/50176]	Loss: 1.2487
Training Epoch: 20 [43520/50176]	Loss: 1.2986
Training Epoch: 20 [43776/50176]	Loss: 1.3608
Training Epoch: 20 [44032/50176]	Loss: 1.1018
Training Epoch: 20 [44288/50176]	Loss: 1.0574
Training Epoch: 20 [44544/50176]	Loss: 1.2161
Training Epoch: 20 [44800/50176]	Loss: 1.3280
Training Epoch: 20 [45056/50176]	Loss: 1.1578
Training Epoch: 20 [45312/50176]	Loss: 1.3415
Training Epoch: 20 [45568/50176]	Loss: 1.2611
Training Epoch: 20 [45824/50176]	Loss: 1.1355
Training Epoch: 20 [46080/50176]	Loss: 1.2088
Training Epoch: 20 [46336/50176]	Loss: 1.3424
Training Epoch: 20 [46592/50176]	Loss: 1.2873
Training Epoch: 20 [46848/50176]	Loss: 1.1042
Training Epoch: 20 [47104/50176]	Loss: 1.1079
Training Epoch: 20 [47360/50176]	Loss: 1.0802
Training Epoch: 20 [47616/50176]	Loss: 1.2709
Training Epoch: 20 [47872/50176]	Loss: 1.2080
Training Epoch: 20 [48128/50176]	Loss: 1.1563
Training Epoch: 20 [48384/50176]	Loss: 1.2487
Training Epoch: 20 [48640/50176]	Loss: 1.3103
Training Epoch: 20 [48896/50176]	Loss: 1.3618
Training Epoch: 20 [49152/50176]	Loss: 1.4174
Training Epoch: 20 [49408/50176]	Loss: 1.0805
Training Epoch: 20 [49664/50176]	Loss: 1.3501
Training Epoch: 20 [49920/50176]	Loss: 1.1102
Training Epoch: 20 [50176/50176]	Loss: 1.4976
Validation Epoch: 20, Average loss: 0.0076, Accuracy: 0.5003
Training Epoch: 21 [256/50176]	Loss: 1.1648
Training Epoch: 21 [512/50176]	Loss: 1.0636
Training Epoch: 21 [768/50176]	Loss: 1.1615
Training Epoch: 21 [1024/50176]	Loss: 1.0792
Training Epoch: 21 [1280/50176]	Loss: 1.1983
Training Epoch: 21 [1536/50176]	Loss: 1.0938
Training Epoch: 21 [1792/50176]	Loss: 1.2504
Training Epoch: 21 [2048/50176]	Loss: 1.1720
Training Epoch: 21 [2304/50176]	Loss: 1.2293
Training Epoch: 21 [2560/50176]	Loss: 1.1705
Training Epoch: 21 [2816/50176]	Loss: 1.1982
Training Epoch: 21 [3072/50176]	Loss: 1.1049
Training Epoch: 21 [3328/50176]	Loss: 0.9633
Training Epoch: 21 [3584/50176]	Loss: 1.1401
Training Epoch: 21 [3840/50176]	Loss: 0.8926
Training Epoch: 21 [4096/50176]	Loss: 1.1225
Training Epoch: 21 [4352/50176]	Loss: 1.1569
Training Epoch: 21 [4608/50176]	Loss: 1.0853
Training Epoch: 21 [4864/50176]	Loss: 1.2353
Training Epoch: 21 [5120/50176]	Loss: 1.2409
Training Epoch: 21 [5376/50176]	Loss: 1.1552
Training Epoch: 21 [5632/50176]	Loss: 1.2615
Training Epoch: 21 [5888/50176]	Loss: 1.2960
Training Epoch: 21 [6144/50176]	Loss: 1.2041
Training Epoch: 21 [6400/50176]	Loss: 1.1417
Training Epoch: 21 [6656/50176]	Loss: 1.0243
Training Epoch: 21 [6912/50176]	Loss: 1.0618
Training Epoch: 21 [7168/50176]	Loss: 1.0822
Training Epoch: 21 [7424/50176]	Loss: 1.0771
Training Epoch: 21 [7680/50176]	Loss: 1.1489
Training Epoch: 21 [7936/50176]	Loss: 0.9961
Training Epoch: 21 [8192/50176]	Loss: 1.2115
Training Epoch: 21 [8448/50176]	Loss: 1.1768
Training Epoch: 21 [8704/50176]	Loss: 1.1503
Training Epoch: 21 [8960/50176]	Loss: 1.3561
Training Epoch: 21 [9216/50176]	Loss: 1.0231
Training Epoch: 21 [9472/50176]	Loss: 1.0940
Training Epoch: 21 [9728/50176]	Loss: 1.3010
Training Epoch: 21 [9984/50176]	Loss: 0.9806
Training Epoch: 21 [10240/50176]	Loss: 1.0863
Training Epoch: 21 [10496/50176]	Loss: 1.2664
Training Epoch: 21 [10752/50176]	Loss: 1.1709
Training Epoch: 21 [11008/50176]	Loss: 1.1407
Training Epoch: 21 [11264/50176]	Loss: 1.0269
Training Epoch: 21 [11520/50176]	Loss: 1.1252
Training Epoch: 21 [11776/50176]	Loss: 1.1717
Training Epoch: 21 [12032/50176]	Loss: 1.1534
Training Epoch: 21 [12288/50176]	Loss: 1.0585
Training Epoch: 21 [12544/50176]	Loss: 1.1966
Training Epoch: 21 [12800/50176]	Loss: 1.0777
Training Epoch: 21 [13056/50176]	Loss: 1.1133
Training Epoch: 21 [13312/50176]	Loss: 1.0740
Training Epoch: 21 [13568/50176]	Loss: 1.2155
Training Epoch: 21 [13824/50176]	Loss: 1.2317
Training Epoch: 21 [14080/50176]	Loss: 1.1788
Training Epoch: 21 [14336/50176]	Loss: 1.2107
Training Epoch: 21 [14592/50176]	Loss: 1.2703
Training Epoch: 21 [14848/50176]	Loss: 1.2901
Training Epoch: 21 [15104/50176]	Loss: 1.2406
Training Epoch: 21 [15360/50176]	Loss: 1.1020
Training Epoch: 21 [15616/50176]	Loss: 1.2699
Training Epoch: 21 [15872/50176]	Loss: 1.0934
Training Epoch: 21 [16128/50176]	Loss: 1.2050
Training Epoch: 21 [16384/50176]	Loss: 1.0691
Training Epoch: 21 [16640/50176]	Loss: 1.1179
Training Epoch: 21 [16896/50176]	Loss: 1.1696
Training Epoch: 21 [17152/50176]	Loss: 1.1315
Training Epoch: 21 [17408/50176]	Loss: 1.2842
Training Epoch: 21 [17664/50176]	Loss: 1.1811
Training Epoch: 21 [17920/50176]	Loss: 1.0874
Training Epoch: 21 [18176/50176]	Loss: 1.1682
Training Epoch: 21 [18432/50176]	Loss: 1.1876
Training Epoch: 21 [18688/50176]	Loss: 1.2183
Training Epoch: 21 [18944/50176]	Loss: 1.0380
Training Epoch: 21 [19200/50176]	Loss: 1.0919
Training Epoch: 21 [19456/50176]	Loss: 1.1776
Training Epoch: 21 [19712/50176]	Loss: 1.0371
Training Epoch: 21 [19968/50176]	Loss: 1.3871
Training Epoch: 21 [20224/50176]	Loss: 1.1898
Training Epoch: 21 [20480/50176]	Loss: 1.2139
Training Epoch: 21 [20736/50176]	Loss: 1.2047
Training Epoch: 21 [20992/50176]	Loss: 1.1921
Training Epoch: 21 [21248/50176]	Loss: 1.0704
Training Epoch: 21 [21504/50176]	Loss: 1.0748
Training Epoch: 21 [21760/50176]	Loss: 1.3099
Training Epoch: 21 [22016/50176]	Loss: 1.1556
Training Epoch: 21 [22272/50176]	Loss: 1.1554
Training Epoch: 21 [22528/50176]	Loss: 1.1467
Training Epoch: 21 [22784/50176]	Loss: 1.1626
Training Epoch: 21 [23040/50176]	Loss: 1.0298
Training Epoch: 21 [23296/50176]	Loss: 1.1492
Training Epoch: 21 [23552/50176]	Loss: 1.3264
Training Epoch: 21 [23808/50176]	Loss: 1.2254
Training Epoch: 21 [24064/50176]	Loss: 1.1518
Training Epoch: 21 [24320/50176]	Loss: 1.2030
Training Epoch: 21 [24576/50176]	Loss: 1.1332
Training Epoch: 21 [24832/50176]	Loss: 1.0517
Training Epoch: 21 [25088/50176]	Loss: 1.2534
Training Epoch: 21 [25344/50176]	Loss: 1.2986
Training Epoch: 21 [25600/50176]	Loss: 1.3219
Training Epoch: 21 [25856/50176]	Loss: 1.1238
Training Epoch: 21 [26112/50176]	Loss: 1.1999
Training Epoch: 21 [26368/50176]	Loss: 1.0061
Training Epoch: 21 [26624/50176]	Loss: 1.2894
Training Epoch: 21 [26880/50176]	Loss: 1.1432
Training Epoch: 21 [27136/50176]	Loss: 1.3542
Training Epoch: 21 [27392/50176]	Loss: 1.2153
Training Epoch: 21 [27648/50176]	Loss: 1.4456
Training Epoch: 21 [27904/50176]	Loss: 1.1963
Training Epoch: 21 [28160/50176]	Loss: 1.1509
Training Epoch: 21 [28416/50176]	Loss: 1.1904
Training Epoch: 21 [28672/50176]	Loss: 1.2597
Training Epoch: 21 [28928/50176]	Loss: 1.2160
Training Epoch: 21 [29184/50176]	Loss: 1.3166
Training Epoch: 21 [29440/50176]	Loss: 1.2057
Training Epoch: 21 [29696/50176]	Loss: 1.2331
Training Epoch: 21 [29952/50176]	Loss: 1.2490
Training Epoch: 21 [30208/50176]	Loss: 1.2938
Training Epoch: 21 [30464/50176]	Loss: 1.0856
Training Epoch: 21 [30720/50176]	Loss: 1.2947
Training Epoch: 21 [30976/50176]	Loss: 1.1466
Training Epoch: 21 [31232/50176]	Loss: 1.1140
Training Epoch: 21 [31488/50176]	Loss: 1.0188
Training Epoch: 21 [31744/50176]	Loss: 1.2455
Training Epoch: 21 [32000/50176]	Loss: 1.1987
Training Epoch: 21 [32256/50176]	Loss: 1.1665
Training Epoch: 21 [32512/50176]	Loss: 1.3159
Training Epoch: 21 [32768/50176]	Loss: 1.0757
Training Epoch: 21 [33024/50176]	Loss: 1.1079
Training Epoch: 21 [33280/50176]	Loss: 1.2453
Training Epoch: 21 [33536/50176]	Loss: 1.2913
Training Epoch: 21 [33792/50176]	Loss: 1.1806
Training Epoch: 21 [34048/50176]	Loss: 1.2544
Training Epoch: 21 [34304/50176]	Loss: 1.3924
Training Epoch: 21 [34560/50176]	Loss: 1.0745
Training Epoch: 21 [34816/50176]	Loss: 1.3307
Training Epoch: 21 [35072/50176]	Loss: 1.3874
Training Epoch: 21 [35328/50176]	Loss: 1.1773
Training Epoch: 21 [35584/50176]	Loss: 1.2189
Training Epoch: 21 [35840/50176]	Loss: 1.2830
Training Epoch: 21 [36096/50176]	Loss: 1.0972
Training Epoch: 21 [36352/50176]	Loss: 1.2650
Training Epoch: 21 [36608/50176]	Loss: 1.2342
Training Epoch: 21 [36864/50176]	Loss: 1.1844
Training Epoch: 21 [37120/50176]	Loss: 1.2261
Training Epoch: 21 [37376/50176]	Loss: 1.0859
Training Epoch: 21 [37632/50176]	Loss: 1.2280
Training Epoch: 21 [37888/50176]	Loss: 1.1955
Training Epoch: 21 [38144/50176]	Loss: 1.1105
Training Epoch: 21 [38400/50176]	Loss: 1.1751
Training Epoch: 21 [38656/50176]	Loss: 1.2947
Training Epoch: 21 [38912/50176]	Loss: 1.1689
Training Epoch: 21 [39168/50176]	Loss: 1.2997
Training Epoch: 21 [39424/50176]	Loss: 1.1597
Training Epoch: 21 [39680/50176]	Loss: 1.1535
Training Epoch: 21 [39936/50176]	Loss: 1.2313
Training Epoch: 21 [40192/50176]	Loss: 1.3530
Training Epoch: 21 [40448/50176]	Loss: 1.3183
Training Epoch: 21 [40704/50176]	Loss: 1.0791
Training Epoch: 21 [40960/50176]	Loss: 1.2009
Training Epoch: 21 [41216/50176]	Loss: 1.2772
Training Epoch: 21 [41472/50176]	Loss: 1.0933
Training Epoch: 21 [41728/50176]	Loss: 1.2377
Training Epoch: 21 [41984/50176]	Loss: 1.2876
Training Epoch: 21 [42240/50176]	Loss: 1.0816
Training Epoch: 21 [42496/50176]	Loss: 1.1565
Training Epoch: 21 [42752/50176]	Loss: 1.2729
Training Epoch: 21 [43008/50176]	Loss: 1.1911
Training Epoch: 21 [43264/50176]	Loss: 1.2843
Training Epoch: 21 [43520/50176]	Loss: 1.1684
Training Epoch: 21 [43776/50176]	Loss: 1.0961
Training Epoch: 21 [44032/50176]	Loss: 1.3444
Training Epoch: 21 [44288/50176]	Loss: 1.3334
Training Epoch: 21 [44544/50176]	Loss: 1.1436
Training Epoch: 21 [44800/50176]	Loss: 0.9740
Training Epoch: 21 [45056/50176]	Loss: 1.1555
Training Epoch: 21 [45312/50176]	Loss: 1.1532
Training Epoch: 21 [45568/50176]	Loss: 1.1065
Training Epoch: 21 [45824/50176]	Loss: 1.3830
Training Epoch: 21 [46080/50176]	Loss: 1.1987
Training Epoch: 21 [46336/50176]	Loss: 1.0746
Training Epoch: 21 [46592/50176]	Loss: 1.1828
Training Epoch: 21 [46848/50176]	Loss: 1.1925
Training Epoch: 21 [47104/50176]	Loss: 1.2387
Training Epoch: 21 [47360/50176]	Loss: 1.1393
Training Epoch: 21 [47616/50176]	Loss: 1.1881
Training Epoch: 21 [47872/50176]	Loss: 1.2770
Training Epoch: 21 [48128/50176]	Loss: 1.2633
Training Epoch: 21 [48384/50176]	Loss: 1.0934
Training Epoch: 21 [48640/50176]	Loss: 1.1212
Training Epoch: 21 [48896/50176]	Loss: 0.9053
Training Epoch: 21 [49152/50176]	Loss: 1.2030
Training Epoch: 21 [49408/50176]	Loss: 1.0908
Training Epoch: 21 [49664/50176]	Loss: 1.2270
Training Epoch: 21 [49920/50176]	Loss: 1.3048
Training Epoch: 21 [50176/50176]	Loss: 1.3237
Validation Epoch: 21, Average loss: 0.0090, Accuracy: 0.4644
Training Epoch: 22 [256/50176]	Loss: 1.1155
Training Epoch: 22 [512/50176]	Loss: 0.9683
Training Epoch: 22 [768/50176]	Loss: 1.1025
Training Epoch: 22 [1024/50176]	Loss: 1.0749
Training Epoch: 22 [1280/50176]	Loss: 1.1968
Training Epoch: 22 [1536/50176]	Loss: 1.1245
Training Epoch: 22 [1792/50176]	Loss: 1.1305
Training Epoch: 22 [2048/50176]	Loss: 1.0079
Training Epoch: 22 [2304/50176]	Loss: 1.2450
Training Epoch: 22 [2560/50176]	Loss: 1.2337
Training Epoch: 22 [2816/50176]	Loss: 1.0932
Training Epoch: 22 [3072/50176]	Loss: 1.0381
Training Epoch: 22 [3328/50176]	Loss: 1.1050
Training Epoch: 22 [3584/50176]	Loss: 1.0727
Training Epoch: 22 [3840/50176]	Loss: 1.2307
Training Epoch: 22 [4096/50176]	Loss: 1.0840
Training Epoch: 22 [4352/50176]	Loss: 1.0989
Training Epoch: 22 [4608/50176]	Loss: 1.0800
Training Epoch: 22 [4864/50176]	Loss: 1.1941
Training Epoch: 22 [5120/50176]	Loss: 1.1050
Training Epoch: 22 [5376/50176]	Loss: 1.0602
Training Epoch: 22 [5632/50176]	Loss: 1.0522
Training Epoch: 22 [5888/50176]	Loss: 1.0642
Training Epoch: 22 [6144/50176]	Loss: 1.1676
Training Epoch: 22 [6400/50176]	Loss: 1.1531
Training Epoch: 22 [6656/50176]	Loss: 1.1809
Training Epoch: 22 [6912/50176]	Loss: 1.0197
Training Epoch: 22 [7168/50176]	Loss: 1.0267
Training Epoch: 22 [7424/50176]	Loss: 1.1523
Training Epoch: 22 [7680/50176]	Loss: 1.1169
Training Epoch: 22 [7936/50176]	Loss: 1.1468
Training Epoch: 22 [8192/50176]	Loss: 1.1792
Training Epoch: 22 [8448/50176]	Loss: 1.0107
Training Epoch: 22 [8704/50176]	Loss: 1.1550
Training Epoch: 22 [8960/50176]	Loss: 1.0531
Training Epoch: 22 [9216/50176]	Loss: 0.9366
Training Epoch: 22 [9472/50176]	Loss: 1.3631
Training Epoch: 22 [9728/50176]	Loss: 1.1705
Training Epoch: 22 [9984/50176]	Loss: 1.0089
Training Epoch: 22 [10240/50176]	Loss: 1.0232
Training Epoch: 22 [10496/50176]	Loss: 1.1082
Training Epoch: 22 [10752/50176]	Loss: 1.0048
Training Epoch: 22 [11008/50176]	Loss: 1.1104
Training Epoch: 22 [11264/50176]	Loss: 1.0827
Training Epoch: 22 [11520/50176]	Loss: 1.2009
Training Epoch: 22 [11776/50176]	Loss: 1.2357
Training Epoch: 22 [12032/50176]	Loss: 1.2431
Training Epoch: 22 [12288/50176]	Loss: 1.1561
Training Epoch: 22 [12544/50176]	Loss: 1.1378
Training Epoch: 22 [12800/50176]	Loss: 1.0863
Training Epoch: 22 [13056/50176]	Loss: 1.1733
Training Epoch: 22 [13312/50176]	Loss: 1.0158
Training Epoch: 22 [13568/50176]	Loss: 1.1955
Training Epoch: 22 [13824/50176]	Loss: 1.0016
Training Epoch: 22 [14080/50176]	Loss: 1.1302
Training Epoch: 22 [14336/50176]	Loss: 1.1930
Training Epoch: 22 [14592/50176]	Loss: 1.2730
Training Epoch: 22 [14848/50176]	Loss: 1.2324
Training Epoch: 22 [15104/50176]	Loss: 0.9742
Training Epoch: 22 [15360/50176]	Loss: 1.1388
Training Epoch: 22 [15616/50176]	Loss: 1.0256
Training Epoch: 22 [15872/50176]	Loss: 0.9779
Training Epoch: 22 [16128/50176]	Loss: 0.9109
Training Epoch: 22 [16384/50176]	Loss: 1.1903
Training Epoch: 22 [16640/50176]	Loss: 1.1191
Training Epoch: 22 [16896/50176]	Loss: 1.0305
Training Epoch: 22 [17152/50176]	Loss: 1.3011
Training Epoch: 22 [17408/50176]	Loss: 1.1136
Training Epoch: 22 [17664/50176]	Loss: 1.2403
Training Epoch: 22 [17920/50176]	Loss: 1.1207
Training Epoch: 22 [18176/50176]	Loss: 1.1066
Training Epoch: 22 [18432/50176]	Loss: 1.1753
Training Epoch: 22 [18688/50176]	Loss: 1.0541
Training Epoch: 22 [18944/50176]	Loss: 1.1088
Training Epoch: 22 [19200/50176]	Loss: 1.0037
Training Epoch: 22 [19456/50176]	Loss: 1.0526
Training Epoch: 22 [19712/50176]	Loss: 1.0370
Training Epoch: 22 [19968/50176]	Loss: 1.1552
Training Epoch: 22 [20224/50176]	Loss: 1.0493
Training Epoch: 22 [20480/50176]	Loss: 1.1817
Training Epoch: 22 [20736/50176]	Loss: 1.1525
Training Epoch: 22 [20992/50176]	Loss: 1.1211
Training Epoch: 22 [21248/50176]	Loss: 1.1541
Training Epoch: 22 [21504/50176]	Loss: 1.2048
Training Epoch: 22 [21760/50176]	Loss: 1.0773
Training Epoch: 22 [22016/50176]	Loss: 1.1192
Training Epoch: 22 [22272/50176]	Loss: 0.9367
Training Epoch: 22 [22528/50176]	Loss: 0.9758
Training Epoch: 22 [22784/50176]	Loss: 1.3328
Training Epoch: 22 [23040/50176]	Loss: 1.0909
Training Epoch: 22 [23296/50176]	Loss: 1.2154
Training Epoch: 22 [23552/50176]	Loss: 1.2355
Training Epoch: 22 [23808/50176]	Loss: 1.1859
Training Epoch: 22 [24064/50176]	Loss: 1.2138
Training Epoch: 22 [24320/50176]	Loss: 1.0396
Training Epoch: 22 [24576/50176]	Loss: 1.1334
Training Epoch: 22 [24832/50176]	Loss: 1.2682
Training Epoch: 22 [25088/50176]	Loss: 1.0835
Training Epoch: 22 [25344/50176]	Loss: 1.2187
Training Epoch: 22 [25600/50176]	Loss: 1.2099
Training Epoch: 22 [25856/50176]	Loss: 1.2114
Training Epoch: 22 [26112/50176]	Loss: 0.9279
Training Epoch: 22 [26368/50176]	Loss: 1.1086
Training Epoch: 22 [26624/50176]	Loss: 1.1185
Training Epoch: 22 [26880/50176]	Loss: 1.0653
Training Epoch: 22 [27136/50176]	Loss: 1.1924
Training Epoch: 22 [27392/50176]	Loss: 1.1245
Training Epoch: 22 [27648/50176]	Loss: 1.2461
Training Epoch: 22 [27904/50176]	Loss: 1.0777
Training Epoch: 22 [28160/50176]	Loss: 1.2681
Training Epoch: 22 [28416/50176]	Loss: 1.2218
Training Epoch: 22 [28672/50176]	Loss: 1.0935
Training Epoch: 22 [28928/50176]	Loss: 1.1099
Training Epoch: 22 [29184/50176]	Loss: 1.2892
Training Epoch: 22 [29440/50176]	Loss: 1.1922
Training Epoch: 22 [29696/50176]	Loss: 1.1572
Training Epoch: 22 [29952/50176]	Loss: 1.2879
Training Epoch: 22 [30208/50176]	Loss: 1.1953
Training Epoch: 22 [30464/50176]	Loss: 1.1991
Training Epoch: 22 [30720/50176]	Loss: 1.2862
Training Epoch: 22 [30976/50176]	Loss: 1.0976
Training Epoch: 22 [31232/50176]	Loss: 1.2127
Training Epoch: 22 [31488/50176]	Loss: 1.1181
Training Epoch: 22 [31744/50176]	Loss: 1.3298
Training Epoch: 22 [32000/50176]	Loss: 1.1267
Training Epoch: 22 [32256/50176]	Loss: 1.3902
Training Epoch: 22 [32512/50176]	Loss: 1.1678
Training Epoch: 22 [32768/50176]	Loss: 1.3419
Training Epoch: 22 [33024/50176]	Loss: 1.1797
Training Epoch: 22 [33280/50176]	Loss: 1.1823
Training Epoch: 22 [33536/50176]	Loss: 1.0661
Training Epoch: 22 [33792/50176]	Loss: 1.1872
Training Epoch: 22 [34048/50176]	Loss: 1.1979
Training Epoch: 22 [34304/50176]	Loss: 1.0796
Training Epoch: 22 [34560/50176]	Loss: 1.1581
Training Epoch: 22 [34816/50176]	Loss: 1.1206
Training Epoch: 22 [35072/50176]	Loss: 1.1400
Training Epoch: 22 [35328/50176]	Loss: 1.1488
Training Epoch: 22 [35584/50176]	Loss: 1.0269
Training Epoch: 22 [35840/50176]	Loss: 1.1795
Training Epoch: 22 [36096/50176]	Loss: 1.1632
Training Epoch: 22 [36352/50176]	Loss: 1.3898
Training Epoch: 22 [36608/50176]	Loss: 1.0818
Training Epoch: 22 [36864/50176]	Loss: 1.0779
Training Epoch: 22 [37120/50176]	Loss: 1.1951
Training Epoch: 22 [37376/50176]	Loss: 1.1252
Training Epoch: 22 [37632/50176]	Loss: 1.1952
Training Epoch: 22 [37888/50176]	Loss: 1.3577
Training Epoch: 22 [38144/50176]	Loss: 1.1539
Training Epoch: 22 [38400/50176]	Loss: 1.1516
Training Epoch: 22 [38656/50176]	Loss: 1.1804
Training Epoch: 22 [38912/50176]	Loss: 1.1704
Training Epoch: 22 [39168/50176]	Loss: 1.1659
Training Epoch: 22 [39424/50176]	Loss: 1.2915
Training Epoch: 22 [39680/50176]	Loss: 1.3496
Training Epoch: 22 [39936/50176]	Loss: 1.0122
Training Epoch: 22 [40192/50176]	Loss: 1.0566
Training Epoch: 22 [40448/50176]	Loss: 1.0060
Training Epoch: 22 [40704/50176]	Loss: 1.2206
Training Epoch: 22 [40960/50176]	Loss: 1.0121
Training Epoch: 22 [41216/50176]	Loss: 1.1210
Training Epoch: 22 [41472/50176]	Loss: 1.1542
Training Epoch: 22 [41728/50176]	Loss: 1.1658
Training Epoch: 22 [41984/50176]	Loss: 1.2648
Training Epoch: 22 [42240/50176]	Loss: 1.1358
Training Epoch: 22 [42496/50176]	Loss: 1.0171
Training Epoch: 22 [42752/50176]	Loss: 1.1662
Training Epoch: 22 [43008/50176]	Loss: 1.0513
Training Epoch: 22 [43264/50176]	Loss: 1.0430
Training Epoch: 22 [43520/50176]	Loss: 1.1972
Training Epoch: 22 [43776/50176]	Loss: 1.1767
Training Epoch: 22 [44032/50176]	Loss: 1.0665
Training Epoch: 22 [44288/50176]	Loss: 1.2124
Training Epoch: 22 [44544/50176]	Loss: 1.1245
Training Epoch: 22 [44800/50176]	Loss: 1.1321
Training Epoch: 22 [45056/50176]	Loss: 1.0244
Training Epoch: 22 [45312/50176]	Loss: 1.3042
Training Epoch: 22 [45568/50176]	Loss: 1.0376
Training Epoch: 22 [45824/50176]	Loss: 1.2224
Training Epoch: 22 [46080/50176]	Loss: 1.0834
Training Epoch: 22 [46336/50176]	Loss: 1.0839
Training Epoch: 22 [46592/50176]	Loss: 1.1118
Training Epoch: 22 [46848/50176]	Loss: 1.1857
Training Epoch: 22 [47104/50176]	Loss: 1.2594
Training Epoch: 22 [47360/50176]	Loss: 1.2266
Training Epoch: 22 [47616/50176]	Loss: 1.1052
Training Epoch: 22 [47872/50176]	Loss: 0.9881
Training Epoch: 22 [48128/50176]	Loss: 1.1672
Training Epoch: 22 [48384/50176]	Loss: 1.1014
Training Epoch: 22 [48640/50176]	Loss: 1.1763
Training Epoch: 22 [48896/50176]	Loss: 1.1177
Training Epoch: 22 [49152/50176]	Loss: 1.2306
Training Epoch: 22 [49408/50176]	Loss: 1.1238
Training Epoch: 22 [49664/50176]	Loss: 1.2270
Training Epoch: 22 [49920/50176]	Loss: 1.1474
Training Epoch: 22 [50176/50176]	Loss: 1.1400
Validation Epoch: 22, Average loss: 0.0090, Accuracy: 0.4764
Training Epoch: 23 [256/50176]	Loss: 1.0048
Training Epoch: 23 [512/50176]	Loss: 1.1433
Training Epoch: 23 [768/50176]	Loss: 1.1044
Training Epoch: 23 [1024/50176]	Loss: 1.1462
Training Epoch: 23 [1280/50176]	Loss: 1.1396
Training Epoch: 23 [1536/50176]	Loss: 1.0333
Training Epoch: 23 [1792/50176]	Loss: 1.1606
Training Epoch: 23 [2048/50176]	Loss: 1.1591
Training Epoch: 23 [2304/50176]	Loss: 1.0147
Training Epoch: 23 [2560/50176]	Loss: 1.1054
Training Epoch: 23 [2816/50176]	Loss: 0.9962
Training Epoch: 23 [3072/50176]	Loss: 1.0955
Training Epoch: 23 [3328/50176]	Loss: 1.0260
Training Epoch: 23 [3584/50176]	Loss: 1.0302
Training Epoch: 23 [3840/50176]	Loss: 1.1368
Training Epoch: 23 [4096/50176]	Loss: 1.0636
Training Epoch: 23 [4352/50176]	Loss: 0.9681
Training Epoch: 23 [4608/50176]	Loss: 1.0218
Training Epoch: 23 [4864/50176]	Loss: 1.0606
Training Epoch: 23 [5120/50176]	Loss: 1.1369
Training Epoch: 23 [5376/50176]	Loss: 1.0406
Training Epoch: 23 [5632/50176]	Loss: 1.1030
Training Epoch: 23 [5888/50176]	Loss: 1.1136
Training Epoch: 23 [6144/50176]	Loss: 1.0132
Training Epoch: 23 [6400/50176]	Loss: 1.2270
Training Epoch: 23 [6656/50176]	Loss: 1.1323
Training Epoch: 23 [6912/50176]	Loss: 1.1866
Training Epoch: 23 [7168/50176]	Loss: 1.0785
Training Epoch: 23 [7424/50176]	Loss: 1.0760
Training Epoch: 23 [7680/50176]	Loss: 1.0052
Training Epoch: 23 [7936/50176]	Loss: 1.0783
Training Epoch: 23 [8192/50176]	Loss: 1.0284
Training Epoch: 23 [8448/50176]	Loss: 0.9662
Training Epoch: 23 [8704/50176]	Loss: 0.9984
Training Epoch: 23 [8960/50176]	Loss: 1.1064
Training Epoch: 23 [9216/50176]	Loss: 0.9463
Training Epoch: 23 [9472/50176]	Loss: 0.9109
Training Epoch: 23 [9728/50176]	Loss: 1.0759
Training Epoch: 23 [9984/50176]	Loss: 1.0512
Training Epoch: 23 [10240/50176]	Loss: 0.8833
Training Epoch: 23 [10496/50176]	Loss: 1.0120
Training Epoch: 23 [10752/50176]	Loss: 1.1567
Training Epoch: 23 [11008/50176]	Loss: 1.0783
Training Epoch: 23 [11264/50176]	Loss: 1.1336
Training Epoch: 23 [11520/50176]	Loss: 1.1129
Training Epoch: 23 [11776/50176]	Loss: 1.1848
Training Epoch: 23 [12032/50176]	Loss: 1.0212
Training Epoch: 23 [12288/50176]	Loss: 0.9094
Training Epoch: 23 [12544/50176]	Loss: 0.9828
Training Epoch: 23 [12800/50176]	Loss: 1.1094
Training Epoch: 23 [13056/50176]	Loss: 1.1749
Training Epoch: 23 [13312/50176]	Loss: 1.1554
Training Epoch: 23 [13568/50176]	Loss: 1.1122
Training Epoch: 23 [13824/50176]	Loss: 1.0761
Training Epoch: 23 [14080/50176]	Loss: 1.0303
Training Epoch: 23 [14336/50176]	Loss: 1.0346
Training Epoch: 23 [14592/50176]	Loss: 1.1252
Training Epoch: 23 [14848/50176]	Loss: 1.2764
Training Epoch: 23 [15104/50176]	Loss: 0.9989
Training Epoch: 23 [15360/50176]	Loss: 1.0532
Training Epoch: 23 [15616/50176]	Loss: 0.9293
Training Epoch: 23 [15872/50176]	Loss: 1.0232
Training Epoch: 23 [16128/50176]	Loss: 0.9680
Training Epoch: 23 [16384/50176]	Loss: 0.9183
Training Epoch: 23 [16640/50176]	Loss: 1.0335
Training Epoch: 23 [16896/50176]	Loss: 1.0891
Training Epoch: 23 [17152/50176]	Loss: 1.0465
Training Epoch: 23 [17408/50176]	Loss: 1.0356
Training Epoch: 23 [17664/50176]	Loss: 1.1081
Training Epoch: 23 [17920/50176]	Loss: 1.2331
Training Epoch: 23 [18176/50176]	Loss: 1.0766
Training Epoch: 23 [18432/50176]	Loss: 1.2703
Training Epoch: 23 [18688/50176]	Loss: 1.1341
Training Epoch: 23 [18944/50176]	Loss: 1.0905
Training Epoch: 23 [19200/50176]	Loss: 1.0982
Training Epoch: 23 [19456/50176]	Loss: 0.9190
Training Epoch: 23 [19712/50176]	Loss: 1.0239
Training Epoch: 23 [19968/50176]	Loss: 1.1831
Training Epoch: 23 [20224/50176]	Loss: 1.1720
Training Epoch: 23 [20480/50176]	Loss: 1.3983
Training Epoch: 23 [20736/50176]	Loss: 1.0858
Training Epoch: 23 [20992/50176]	Loss: 0.9422
Training Epoch: 23 [21248/50176]	Loss: 1.2451
Training Epoch: 23 [21504/50176]	Loss: 1.1899
Training Epoch: 23 [21760/50176]	Loss: 1.1262
Training Epoch: 23 [22016/50176]	Loss: 1.2384
Training Epoch: 23 [22272/50176]	Loss: 1.2119
Training Epoch: 23 [22528/50176]	Loss: 1.3550
Training Epoch: 23 [22784/50176]	Loss: 1.1505
Training Epoch: 23 [23040/50176]	Loss: 1.1749
Training Epoch: 23 [23296/50176]	Loss: 1.0298
Training Epoch: 23 [23552/50176]	Loss: 1.1696
Training Epoch: 23 [23808/50176]	Loss: 1.2141
Training Epoch: 23 [24064/50176]	Loss: 1.2127
Training Epoch: 23 [24320/50176]	Loss: 1.1183
Training Epoch: 23 [24576/50176]	Loss: 1.2340
Training Epoch: 23 [24832/50176]	Loss: 1.0941
Training Epoch: 23 [25088/50176]	Loss: 1.1297
Training Epoch: 23 [25344/50176]	Loss: 1.1764
Training Epoch: 23 [25600/50176]	Loss: 1.0628
Training Epoch: 23 [25856/50176]	Loss: 1.1198
Training Epoch: 23 [26112/50176]	Loss: 1.1238
Training Epoch: 23 [26368/50176]	Loss: 1.0708
Training Epoch: 23 [26624/50176]	Loss: 1.0705
Training Epoch: 23 [26880/50176]	Loss: 1.1908
Training Epoch: 23 [27136/50176]	Loss: 1.2720
Training Epoch: 23 [27392/50176]	Loss: 1.1256
Training Epoch: 23 [27648/50176]	Loss: 1.1500
Training Epoch: 23 [27904/50176]	Loss: 1.2145
Training Epoch: 23 [28160/50176]	Loss: 1.1086
Training Epoch: 23 [28416/50176]	Loss: 1.1815
Training Epoch: 23 [28672/50176]	Loss: 1.1057
Training Epoch: 23 [28928/50176]	Loss: 1.0583
Training Epoch: 23 [29184/50176]	Loss: 1.2739
Training Epoch: 23 [29440/50176]	Loss: 1.3717
Training Epoch: 23 [29696/50176]	Loss: 1.0159
Training Epoch: 23 [29952/50176]	Loss: 1.0493
Training Epoch: 23 [30208/50176]	Loss: 1.2554
Training Epoch: 23 [30464/50176]	Loss: 1.1522
Training Epoch: 23 [30720/50176]	Loss: 1.0987
Training Epoch: 23 [30976/50176]	Loss: 1.1442
Training Epoch: 23 [31232/50176]	Loss: 1.0849
Training Epoch: 23 [31488/50176]	Loss: 1.0035
Training Epoch: 23 [31744/50176]	Loss: 1.1312
Training Epoch: 23 [32000/50176]	Loss: 1.1878
Training Epoch: 23 [32256/50176]	Loss: 1.2651
Training Epoch: 23 [32512/50176]	Loss: 1.2544
Training Epoch: 23 [32768/50176]	Loss: 1.1905
Training Epoch: 23 [33024/50176]	Loss: 1.1213
Training Epoch: 23 [33280/50176]	Loss: 1.0733
Training Epoch: 23 [33536/50176]	Loss: 1.1545
Training Epoch: 23 [33792/50176]	Loss: 1.2281
Training Epoch: 23 [34048/50176]	Loss: 1.0782
Training Epoch: 23 [34304/50176]	Loss: 1.0041
Training Epoch: 23 [34560/50176]	Loss: 1.0991
Training Epoch: 23 [34816/50176]	Loss: 1.2090
Training Epoch: 23 [35072/50176]	Loss: 1.1296
Training Epoch: 23 [35328/50176]	Loss: 1.0494
Training Epoch: 23 [35584/50176]	Loss: 0.9338
Training Epoch: 23 [35840/50176]	Loss: 1.2907
Training Epoch: 23 [36096/50176]	Loss: 1.0568
Training Epoch: 23 [36352/50176]	Loss: 1.2525
Training Epoch: 23 [36608/50176]	Loss: 1.1890
Training Epoch: 23 [36864/50176]	Loss: 1.1297
Training Epoch: 23 [37120/50176]	Loss: 1.0115
Training Epoch: 23 [37376/50176]	Loss: 1.1886
Training Epoch: 23 [37632/50176]	Loss: 1.0756
Training Epoch: 23 [37888/50176]	Loss: 1.1485
Training Epoch: 23 [38144/50176]	Loss: 1.0770
Training Epoch: 23 [38400/50176]	Loss: 1.0881
Training Epoch: 23 [38656/50176]	Loss: 1.1432
Training Epoch: 23 [38912/50176]	Loss: 1.0691
Training Epoch: 23 [39168/50176]	Loss: 1.1507
Training Epoch: 23 [39424/50176]	Loss: 1.2601
Training Epoch: 23 [39680/50176]	Loss: 1.1151
Training Epoch: 23 [39936/50176]	Loss: 1.0349
Training Epoch: 23 [40192/50176]	Loss: 1.1721
Training Epoch: 23 [40448/50176]	Loss: 1.2082
Training Epoch: 23 [40704/50176]	Loss: 0.9905
Training Epoch: 23 [40960/50176]	Loss: 1.1876
Training Epoch: 23 [41216/50176]	Loss: 1.2208
Training Epoch: 23 [41472/50176]	Loss: 1.0746
Training Epoch: 23 [41728/50176]	Loss: 0.9134
Training Epoch: 23 [41984/50176]	Loss: 1.1217
Training Epoch: 23 [42240/50176]	Loss: 1.1732
Training Epoch: 23 [42496/50176]	Loss: 1.1306
Training Epoch: 23 [42752/50176]	Loss: 1.2083
Training Epoch: 23 [43008/50176]	Loss: 1.0974
Training Epoch: 23 [43264/50176]	Loss: 1.0750
Training Epoch: 23 [43520/50176]	Loss: 1.1945
Training Epoch: 23 [43776/50176]	Loss: 1.1605
Training Epoch: 23 [44032/50176]	Loss: 1.2308
Training Epoch: 23 [44288/50176]	Loss: 1.1169
Training Epoch: 23 [44544/50176]	Loss: 1.0694
Training Epoch: 23 [44800/50176]	Loss: 0.8955
Training Epoch: 23 [45056/50176]	Loss: 1.0877
Training Epoch: 23 [45312/50176]	Loss: 1.2644
Training Epoch: 23 [45568/50176]	Loss: 1.2169
Training Epoch: 23 [45824/50176]	Loss: 0.9180
Training Epoch: 23 [46080/50176]	Loss: 1.2068
Training Epoch: 23 [46336/50176]	Loss: 1.0147
Training Epoch: 23 [46592/50176]	Loss: 1.0750
Training Epoch: 23 [46848/50176]	Loss: 1.1830
Training Epoch: 23 [47104/50176]	Loss: 1.1610
Training Epoch: 23 [47360/50176]	Loss: 1.1060
Training Epoch: 23 [47616/50176]	Loss: 1.1871
Training Epoch: 23 [47872/50176]	Loss: 1.1840
Training Epoch: 23 [48128/50176]	Loss: 1.1713
Training Epoch: 23 [48384/50176]	Loss: 1.1969
Training Epoch: 23 [48640/50176]	Loss: 1.0736
Training Epoch: 23 [48896/50176]	Loss: 1.0256
Training Epoch: 23 [49152/50176]	Loss: 1.2077
Training Epoch: 23 [49408/50176]	Loss: 0.9578
Training Epoch: 23 [49664/50176]	Loss: 1.0944
Training Epoch: 23 [49920/50176]	Loss: 1.2407
Training Epoch: 23 [50176/50176]	Loss: 1.2089
Validation Epoch: 23, Average loss: 0.0101, Accuracy: 0.4385
Training Epoch: 24 [256/50176]	Loss: 0.9682
Training Epoch: 24 [512/50176]	Loss: 0.9313
Training Epoch: 24 [768/50176]	Loss: 1.1840
Training Epoch: 24 [1024/50176]	Loss: 1.1246
Training Epoch: 24 [1280/50176]	Loss: 1.1549
Training Epoch: 24 [1536/50176]	Loss: 1.0221
Training Epoch: 24 [1792/50176]	Loss: 0.9278
Training Epoch: 24 [2048/50176]	Loss: 1.0153
Training Epoch: 24 [2304/50176]	Loss: 1.0690
Training Epoch: 24 [2560/50176]	Loss: 1.0481
Training Epoch: 24 [2816/50176]	Loss: 1.1653
Training Epoch: 24 [3072/50176]	Loss: 1.1290
Training Epoch: 24 [3328/50176]	Loss: 1.1889
Training Epoch: 24 [3584/50176]	Loss: 0.9730
Training Epoch: 24 [3840/50176]	Loss: 0.8947
Training Epoch: 24 [4096/50176]	Loss: 1.1319
Training Epoch: 24 [4352/50176]	Loss: 0.9930
Training Epoch: 24 [4608/50176]	Loss: 0.9356
Training Epoch: 24 [4864/50176]	Loss: 1.1102
Training Epoch: 24 [5120/50176]	Loss: 1.0264
Training Epoch: 24 [5376/50176]	Loss: 1.0873
Training Epoch: 24 [5632/50176]	Loss: 1.1805
Training Epoch: 24 [5888/50176]	Loss: 0.9937
Training Epoch: 24 [6144/50176]	Loss: 1.0309
Training Epoch: 24 [6400/50176]	Loss: 1.0551
Training Epoch: 24 [6656/50176]	Loss: 1.0342
Training Epoch: 24 [6912/50176]	Loss: 0.9670
Training Epoch: 24 [7168/50176]	Loss: 1.0960
Training Epoch: 24 [7424/50176]	Loss: 0.9695
Training Epoch: 24 [7680/50176]	Loss: 1.0607
Training Epoch: 24 [7936/50176]	Loss: 1.0485
Training Epoch: 24 [8192/50176]	Loss: 1.1493
Training Epoch: 24 [8448/50176]	Loss: 1.1630
Training Epoch: 24 [8704/50176]	Loss: 1.0088
Training Epoch: 24 [8960/50176]	Loss: 0.9746
Training Epoch: 24 [9216/50176]	Loss: 1.0804
Training Epoch: 24 [9472/50176]	Loss: 1.0856
Training Epoch: 24 [9728/50176]	Loss: 1.0732
Training Epoch: 24 [9984/50176]	Loss: 1.0686
Training Epoch: 24 [10240/50176]	Loss: 1.0557
Training Epoch: 24 [10496/50176]	Loss: 1.0965
Training Epoch: 24 [10752/50176]	Loss: 1.1598
Training Epoch: 24 [11008/50176]	Loss: 1.1289
Training Epoch: 24 [11264/50176]	Loss: 1.1423
Training Epoch: 24 [11520/50176]	Loss: 1.1021
Training Epoch: 24 [11776/50176]	Loss: 1.1151
Training Epoch: 24 [12032/50176]	Loss: 0.9184
Training Epoch: 24 [12288/50176]	Loss: 1.0605
Training Epoch: 24 [12544/50176]	Loss: 1.0297
Training Epoch: 24 [12800/50176]	Loss: 0.9610
Training Epoch: 24 [13056/50176]	Loss: 1.2607
Training Epoch: 24 [13312/50176]	Loss: 1.0394
Training Epoch: 24 [13568/50176]	Loss: 0.9776
Training Epoch: 24 [13824/50176]	Loss: 1.1720
Training Epoch: 24 [14080/50176]	Loss: 0.9980
Training Epoch: 24 [14336/50176]	Loss: 1.2147
Training Epoch: 24 [14592/50176]	Loss: 1.0276
Training Epoch: 24 [14848/50176]	Loss: 1.0990
Training Epoch: 24 [15104/50176]	Loss: 0.9326
Training Epoch: 24 [15360/50176]	Loss: 1.0844
Training Epoch: 24 [15616/50176]	Loss: 1.2379
Training Epoch: 24 [15872/50176]	Loss: 1.0091
Training Epoch: 24 [16128/50176]	Loss: 1.0475
Training Epoch: 24 [16384/50176]	Loss: 1.0238
Training Epoch: 24 [16640/50176]	Loss: 1.1916
Training Epoch: 24 [16896/50176]	Loss: 1.1098
Training Epoch: 24 [17152/50176]	Loss: 1.0225
Training Epoch: 24 [17408/50176]	Loss: 1.0110
Training Epoch: 24 [17664/50176]	Loss: 1.0258
Training Epoch: 24 [17920/50176]	Loss: 1.0420
Training Epoch: 24 [18176/50176]	Loss: 1.0045
Training Epoch: 24 [18432/50176]	Loss: 1.1680
Training Epoch: 24 [18688/50176]	Loss: 1.0576
Training Epoch: 24 [18944/50176]	Loss: 1.1224
Training Epoch: 24 [19200/50176]	Loss: 1.0629
Training Epoch: 24 [19456/50176]	Loss: 0.9755
Training Epoch: 24 [19712/50176]	Loss: 1.1836
Training Epoch: 24 [19968/50176]	Loss: 0.9847
Training Epoch: 24 [20224/50176]	Loss: 0.9428
Training Epoch: 24 [20480/50176]	Loss: 0.9246
Training Epoch: 24 [20736/50176]	Loss: 0.8930
Training Epoch: 24 [20992/50176]	Loss: 1.0663
Training Epoch: 24 [21248/50176]	Loss: 0.9087
Training Epoch: 24 [21504/50176]	Loss: 0.8735
Training Epoch: 24 [21760/50176]	Loss: 0.9624
Training Epoch: 24 [22016/50176]	Loss: 1.0457
Training Epoch: 24 [22272/50176]	Loss: 1.0563
Training Epoch: 24 [22528/50176]	Loss: 0.9052
Training Epoch: 24 [22784/50176]	Loss: 1.0548
Training Epoch: 24 [23040/50176]	Loss: 1.1706
Training Epoch: 24 [23296/50176]	Loss: 0.9762
Training Epoch: 24 [23552/50176]	Loss: 1.0948
Training Epoch: 24 [23808/50176]	Loss: 1.1173
Training Epoch: 24 [24064/50176]	Loss: 1.0543
Training Epoch: 24 [24320/50176]	Loss: 1.1299
Training Epoch: 24 [24576/50176]	Loss: 1.2440
Training Epoch: 24 [24832/50176]	Loss: 0.9715
Training Epoch: 24 [25088/50176]	Loss: 1.0994
Training Epoch: 24 [25344/50176]	Loss: 1.0559
Training Epoch: 24 [25600/50176]	Loss: 1.1370
Training Epoch: 24 [25856/50176]	Loss: 1.0779
Training Epoch: 24 [26112/50176]	Loss: 0.9835
Training Epoch: 24 [26368/50176]	Loss: 1.1388
Training Epoch: 24 [26624/50176]	Loss: 1.2411
Training Epoch: 24 [26880/50176]	Loss: 0.9664
Training Epoch: 24 [27136/50176]	Loss: 1.1452
Training Epoch: 24 [27392/50176]	Loss: 1.0391
Training Epoch: 24 [27648/50176]	Loss: 1.2264
Training Epoch: 24 [27904/50176]	Loss: 1.1863
Training Epoch: 24 [28160/50176]	Loss: 1.0783
Training Epoch: 24 [28416/50176]	Loss: 1.1575
Training Epoch: 24 [28672/50176]	Loss: 1.1660
Training Epoch: 24 [28928/50176]	Loss: 1.0681
Training Epoch: 24 [29184/50176]	Loss: 1.0093
Training Epoch: 24 [29440/50176]	Loss: 1.3173
Training Epoch: 24 [29696/50176]	Loss: 1.1142
Training Epoch: 24 [29952/50176]	Loss: 1.0587
Training Epoch: 24 [30208/50176]	Loss: 1.1091
Training Epoch: 24 [30464/50176]	Loss: 0.8417
Training Epoch: 24 [30720/50176]	Loss: 0.9446
Training Epoch: 24 [30976/50176]	Loss: 1.1351
Training Epoch: 24 [31232/50176]	Loss: 1.3059
Training Epoch: 24 [31488/50176]	Loss: 0.9268
Training Epoch: 24 [31744/50176]	Loss: 1.0408
Training Epoch: 24 [32000/50176]	Loss: 0.9800
Training Epoch: 24 [32256/50176]	Loss: 1.0710
Training Epoch: 24 [32512/50176]	Loss: 1.0775
Training Epoch: 24 [32768/50176]	Loss: 1.2765
Training Epoch: 24 [33024/50176]	Loss: 1.1367
Training Epoch: 24 [33280/50176]	Loss: 1.1986
Training Epoch: 24 [33536/50176]	Loss: 1.0064
Training Epoch: 24 [33792/50176]	Loss: 1.1458
Training Epoch: 24 [34048/50176]	Loss: 1.0329
Training Epoch: 24 [34304/50176]	Loss: 1.1273
Training Epoch: 24 [34560/50176]	Loss: 1.0934
Training Epoch: 24 [34816/50176]	Loss: 1.0883
Training Epoch: 24 [35072/50176]	Loss: 1.0158
Training Epoch: 24 [35328/50176]	Loss: 1.0326
Training Epoch: 24 [35584/50176]	Loss: 0.9854
Training Epoch: 24 [35840/50176]	Loss: 1.0306
Training Epoch: 24 [36096/50176]	Loss: 1.0538
Training Epoch: 24 [36352/50176]	Loss: 1.2723
Training Epoch: 24 [36608/50176]	Loss: 0.9709
Training Epoch: 24 [36864/50176]	Loss: 1.2660
Training Epoch: 24 [37120/50176]	Loss: 1.0862
Training Epoch: 24 [37376/50176]	Loss: 0.9937
Training Epoch: 24 [37632/50176]	Loss: 1.0212
Training Epoch: 24 [37888/50176]	Loss: 0.9809
Training Epoch: 24 [38144/50176]	Loss: 1.0349
Training Epoch: 24 [38400/50176]	Loss: 1.1670
Training Epoch: 24 [38656/50176]	Loss: 1.1575
Training Epoch: 24 [38912/50176]	Loss: 1.2146
Training Epoch: 24 [39168/50176]	Loss: 1.0980
Training Epoch: 24 [39424/50176]	Loss: 1.0463
Training Epoch: 24 [39680/50176]	Loss: 1.1436
Training Epoch: 24 [39936/50176]	Loss: 1.2906
Training Epoch: 24 [40192/50176]	Loss: 1.1984
Training Epoch: 24 [40448/50176]	Loss: 0.9766
Training Epoch: 24 [40704/50176]	Loss: 0.9132
Training Epoch: 24 [40960/50176]	Loss: 1.1387
Training Epoch: 24 [41216/50176]	Loss: 1.0755
Training Epoch: 24 [41472/50176]	Loss: 1.0769
Training Epoch: 24 [41728/50176]	Loss: 1.1276
Training Epoch: 24 [41984/50176]	Loss: 1.2073
Training Epoch: 24 [42240/50176]	Loss: 1.0807
Training Epoch: 24 [42496/50176]	Loss: 1.2107
Training Epoch: 24 [42752/50176]	Loss: 1.2167
Training Epoch: 24 [43008/50176]	Loss: 1.0497
Training Epoch: 24 [43264/50176]	Loss: 1.1656
Training Epoch: 24 [43520/50176]	Loss: 1.1439
Training Epoch: 24 [43776/50176]	Loss: 1.1971
Training Epoch: 24 [44032/50176]	Loss: 0.9843
Training Epoch: 24 [44288/50176]	Loss: 1.0697
Training Epoch: 24 [44544/50176]	Loss: 1.0699
Training Epoch: 24 [44800/50176]	Loss: 1.0870
Training Epoch: 24 [45056/50176]	Loss: 0.9822
Training Epoch: 24 [45312/50176]	Loss: 1.2247
Training Epoch: 24 [45568/50176]	Loss: 1.1509
Training Epoch: 24 [45824/50176]	Loss: 1.2052
Training Epoch: 24 [46080/50176]	Loss: 1.2876
Training Epoch: 24 [46336/50176]	Loss: 1.2197
Training Epoch: 24 [46592/50176]	Loss: 1.1025
Training Epoch: 24 [46848/50176]	Loss: 1.0374
Training Epoch: 24 [47104/50176]	Loss: 1.0105
Training Epoch: 24 [47360/50176]	Loss: 1.0560
Training Epoch: 24 [47616/50176]	Loss: 1.0946
Training Epoch: 24 [47872/50176]	Loss: 1.1405
Training Epoch: 24 [48128/50176]	Loss: 1.0434
Training Epoch: 24 [48384/50176]	Loss: 1.0101
Training Epoch: 24 [48640/50176]	Loss: 1.2279
Training Epoch: 24 [48896/50176]	Loss: 1.1368
Training Epoch: 24 [49152/50176]	Loss: 1.2530
Training Epoch: 24 [49408/50176]	Loss: 1.1002
Training Epoch: 24 [49664/50176]	Loss: 1.0926
Training Epoch: 24 [49920/50176]	Loss: 1.0626
Training Epoch: 24 [50176/50176]	Loss: 1.1579
Validation Epoch: 24, Average loss: 0.0136, Accuracy: 0.3475
Training Epoch: 25 [256/50176]	Loss: 0.9286
Training Epoch: 25 [512/50176]	Loss: 1.1221
Training Epoch: 25 [768/50176]	Loss: 1.0153
Training Epoch: 25 [1024/50176]	Loss: 0.9983
Training Epoch: 25 [1280/50176]	Loss: 0.9626
Training Epoch: 25 [1536/50176]	Loss: 1.0731
Training Epoch: 25 [1792/50176]	Loss: 0.9821
Training Epoch: 25 [2048/50176]	Loss: 0.9944
Training Epoch: 25 [2304/50176]	Loss: 0.9870
Training Epoch: 25 [2560/50176]	Loss: 1.1134
Training Epoch: 25 [2816/50176]	Loss: 1.1062
Training Epoch: 25 [3072/50176]	Loss: 0.9536
Training Epoch: 25 [3328/50176]	Loss: 1.0411
Training Epoch: 25 [3584/50176]	Loss: 0.9252
Training Epoch: 25 [3840/50176]	Loss: 1.0296
Training Epoch: 25 [4096/50176]	Loss: 0.8515
Training Epoch: 25 [4352/50176]	Loss: 0.8779
Training Epoch: 25 [4608/50176]	Loss: 0.8917
Training Epoch: 25 [4864/50176]	Loss: 1.1357
Training Epoch: 25 [5120/50176]	Loss: 0.9750
Training Epoch: 25 [5376/50176]	Loss: 1.0360
Training Epoch: 25 [5632/50176]	Loss: 0.9092
Training Epoch: 25 [5888/50176]	Loss: 0.9712
Training Epoch: 25 [6144/50176]	Loss: 1.1409
Training Epoch: 25 [6400/50176]	Loss: 0.9012
Training Epoch: 25 [6656/50176]	Loss: 1.2023
Training Epoch: 25 [6912/50176]	Loss: 0.9878
Training Epoch: 25 [7168/50176]	Loss: 1.0057
Training Epoch: 25 [7424/50176]	Loss: 1.0149
Training Epoch: 25 [7680/50176]	Loss: 1.0168
Training Epoch: 25 [7936/50176]	Loss: 1.0184
Training Epoch: 25 [8192/50176]	Loss: 1.0897
Training Epoch: 25 [8448/50176]	Loss: 1.0311
Training Epoch: 25 [8704/50176]	Loss: 0.9505
Training Epoch: 25 [8960/50176]	Loss: 1.0893
Training Epoch: 25 [9216/50176]	Loss: 1.0570
Training Epoch: 25 [9472/50176]	Loss: 1.0081
Training Epoch: 25 [9728/50176]	Loss: 0.8949
Training Epoch: 25 [9984/50176]	Loss: 0.9588
Training Epoch: 25 [10240/50176]	Loss: 0.9190
Training Epoch: 25 [10496/50176]	Loss: 0.9669
Training Epoch: 25 [10752/50176]	Loss: 1.2166
Training Epoch: 25 [11008/50176]	Loss: 0.9893
Training Epoch: 25 [11264/50176]	Loss: 1.1565
Training Epoch: 25 [11520/50176]	Loss: 1.0076
Training Epoch: 25 [11776/50176]	Loss: 0.9032
Training Epoch: 25 [12032/50176]	Loss: 1.1941
Training Epoch: 25 [12288/50176]	Loss: 0.9397
Training Epoch: 25 [12544/50176]	Loss: 1.0682
Training Epoch: 25 [12800/50176]	Loss: 1.0819
Training Epoch: 25 [13056/50176]	Loss: 1.1643
Training Epoch: 25 [13312/50176]	Loss: 0.9111
Training Epoch: 25 [13568/50176]	Loss: 0.9452
Training Epoch: 25 [13824/50176]	Loss: 1.0750
Training Epoch: 25 [14080/50176]	Loss: 1.1549
Training Epoch: 25 [14336/50176]	Loss: 0.9624
Training Epoch: 25 [14592/50176]	Loss: 1.0402
Training Epoch: 25 [14848/50176]	Loss: 1.0077
Training Epoch: 25 [15104/50176]	Loss: 1.1381
Training Epoch: 25 [15360/50176]	Loss: 0.9607
Training Epoch: 25 [15616/50176]	Loss: 1.0847
Training Epoch: 25 [15872/50176]	Loss: 0.9728
Training Epoch: 25 [16128/50176]	Loss: 1.0107
Training Epoch: 25 [16384/50176]	Loss: 1.0198
Training Epoch: 25 [16640/50176]	Loss: 1.2484
Training Epoch: 25 [16896/50176]	Loss: 1.1369
Training Epoch: 25 [17152/50176]	Loss: 0.8832
Training Epoch: 25 [17408/50176]	Loss: 0.9662
Training Epoch: 25 [17664/50176]	Loss: 1.0970
Training Epoch: 25 [17920/50176]	Loss: 0.9500
Training Epoch: 25 [18176/50176]	Loss: 1.0450
Training Epoch: 25 [18432/50176]	Loss: 1.0827
Training Epoch: 25 [18688/50176]	Loss: 1.1812
Training Epoch: 25 [18944/50176]	Loss: 1.0459
Training Epoch: 25 [19200/50176]	Loss: 1.0374
Training Epoch: 25 [19456/50176]	Loss: 0.9313
Training Epoch: 25 [19712/50176]	Loss: 1.1169
Training Epoch: 25 [19968/50176]	Loss: 1.0300
Training Epoch: 25 [20224/50176]	Loss: 1.0465
Training Epoch: 25 [20480/50176]	Loss: 1.1612
Training Epoch: 25 [20736/50176]	Loss: 1.0158
Training Epoch: 25 [20992/50176]	Loss: 0.8608
Training Epoch: 25 [21248/50176]	Loss: 0.9848
Training Epoch: 25 [21504/50176]	Loss: 1.0346
Training Epoch: 25 [21760/50176]	Loss: 0.9889
Training Epoch: 25 [22016/50176]	Loss: 0.9797
Training Epoch: 25 [22272/50176]	Loss: 0.9893
Training Epoch: 25 [22528/50176]	Loss: 1.2140
Training Epoch: 25 [22784/50176]	Loss: 1.0593
Training Epoch: 25 [23040/50176]	Loss: 1.0381
Training Epoch: 25 [23296/50176]	Loss: 0.9487
Training Epoch: 25 [23552/50176]	Loss: 0.9355
Training Epoch: 25 [23808/50176]	Loss: 1.0486
Training Epoch: 25 [24064/50176]	Loss: 0.9925
Training Epoch: 25 [24320/50176]	Loss: 0.8942
Training Epoch: 25 [24576/50176]	Loss: 1.0235
Training Epoch: 25 [24832/50176]	Loss: 0.9938
Training Epoch: 25 [25088/50176]	Loss: 1.1083
Training Epoch: 25 [25344/50176]	Loss: 1.0365
Training Epoch: 25 [25600/50176]	Loss: 0.8747
Training Epoch: 25 [25856/50176]	Loss: 1.0530
Training Epoch: 25 [26112/50176]	Loss: 1.1247
Training Epoch: 25 [26368/50176]	Loss: 1.0018
Training Epoch: 25 [26624/50176]	Loss: 1.0452
Training Epoch: 25 [26880/50176]	Loss: 0.9717
Training Epoch: 25 [27136/50176]	Loss: 1.1332
Training Epoch: 25 [27392/50176]	Loss: 1.0672
Training Epoch: 25 [27648/50176]	Loss: 1.0590
Training Epoch: 25 [27904/50176]	Loss: 1.0087
Training Epoch: 25 [28160/50176]	Loss: 1.0279
Training Epoch: 25 [28416/50176]	Loss: 1.2162
Training Epoch: 25 [28672/50176]	Loss: 1.0864
Training Epoch: 25 [28928/50176]	Loss: 1.0086
Training Epoch: 25 [29184/50176]	Loss: 0.9943
Training Epoch: 25 [29440/50176]	Loss: 1.0245
Training Epoch: 25 [29696/50176]	Loss: 1.1121
Training Epoch: 25 [29952/50176]	Loss: 1.1637
Training Epoch: 25 [30208/50176]	Loss: 0.9752
Training Epoch: 25 [30464/50176]	Loss: 0.9296
Training Epoch: 25 [30720/50176]	Loss: 1.1998
Training Epoch: 25 [30976/50176]	Loss: 1.2818
Training Epoch: 25 [31232/50176]	Loss: 1.1767
Training Epoch: 25 [31488/50176]	Loss: 1.1674
Training Epoch: 25 [31744/50176]	Loss: 0.9884
Training Epoch: 25 [32000/50176]	Loss: 1.1022
Training Epoch: 25 [32256/50176]	Loss: 1.2055
Training Epoch: 25 [32512/50176]	Loss: 1.0015
Training Epoch: 25 [32768/50176]	Loss: 1.0257
Training Epoch: 25 [33024/50176]	Loss: 0.9928
Training Epoch: 25 [33280/50176]	Loss: 1.2405
Training Epoch: 25 [33536/50176]	Loss: 1.0415
Training Epoch: 25 [33792/50176]	Loss: 1.1967
Training Epoch: 25 [34048/50176]	Loss: 1.0145
Training Epoch: 25 [34304/50176]	Loss: 0.9728
Training Epoch: 25 [34560/50176]	Loss: 1.1987
Training Epoch: 25 [34816/50176]	Loss: 0.9173
Training Epoch: 25 [35072/50176]	Loss: 1.2791
Training Epoch: 25 [35328/50176]	Loss: 1.0716
Training Epoch: 25 [35584/50176]	Loss: 0.9334
Training Epoch: 25 [35840/50176]	Loss: 1.0503
Training Epoch: 25 [36096/50176]	Loss: 1.1998
Training Epoch: 25 [36352/50176]	Loss: 1.0013
Training Epoch: 25 [36608/50176]	Loss: 1.2421
Training Epoch: 25 [36864/50176]	Loss: 1.1359
Training Epoch: 25 [37120/50176]	Loss: 0.9145
Training Epoch: 25 [37376/50176]	Loss: 0.9538
Training Epoch: 25 [37632/50176]	Loss: 1.0838
Training Epoch: 25 [37888/50176]	Loss: 1.0030
Training Epoch: 25 [38144/50176]	Loss: 1.0220
Training Epoch: 25 [38400/50176]	Loss: 1.0462
Training Epoch: 25 [38656/50176]	Loss: 0.9933
Training Epoch: 25 [38912/50176]	Loss: 1.0522
Training Epoch: 25 [39168/50176]	Loss: 1.0968
Training Epoch: 25 [39424/50176]	Loss: 1.1155
Training Epoch: 25 [39680/50176]	Loss: 0.9899
Training Epoch: 25 [39936/50176]	Loss: 1.0395
Training Epoch: 25 [40192/50176]	Loss: 0.8965
Training Epoch: 25 [40448/50176]	Loss: 0.9851
Training Epoch: 25 [40704/50176]	Loss: 1.1678
Training Epoch: 25 [40960/50176]	Loss: 1.0158
Training Epoch: 25 [41216/50176]	Loss: 0.9588
Training Epoch: 25 [41472/50176]	Loss: 1.1225
Training Epoch: 25 [41728/50176]	Loss: 1.0513
Training Epoch: 25 [41984/50176]	Loss: 1.0272
Training Epoch: 25 [42240/50176]	Loss: 0.9580
Training Epoch: 25 [42496/50176]	Loss: 0.9929
Training Epoch: 25 [42752/50176]	Loss: 0.9840
Training Epoch: 25 [43008/50176]	Loss: 1.0986
Training Epoch: 25 [43264/50176]	Loss: 0.9867
Training Epoch: 25 [43520/50176]	Loss: 1.1106
Training Epoch: 25 [43776/50176]	Loss: 1.1016
Training Epoch: 25 [44032/50176]	Loss: 1.2346
Training Epoch: 25 [44288/50176]	Loss: 1.0577
Training Epoch: 25 [44544/50176]	Loss: 1.1680
Training Epoch: 25 [44800/50176]	Loss: 1.0768
Training Epoch: 25 [45056/50176]	Loss: 1.1493
Training Epoch: 25 [45312/50176]	Loss: 1.1145
Training Epoch: 25 [45568/50176]	Loss: 1.0339
Training Epoch: 25 [45824/50176]	Loss: 1.0451
Training Epoch: 25 [46080/50176]	Loss: 1.0552
Training Epoch: 25 [46336/50176]	Loss: 1.2396
Training Epoch: 25 [46592/50176]	Loss: 0.8923
Training Epoch: 25 [46848/50176]	Loss: 1.0290
Training Epoch: 25 [47104/50176]	Loss: 1.0025
Training Epoch: 25 [47360/50176]	Loss: 1.2014
Training Epoch: 25 [47616/50176]	Loss: 1.1548
Training Epoch: 25 [47872/50176]	Loss: 1.0215
Training Epoch: 25 [48128/50176]	Loss: 1.1412
Training Epoch: 25 [48384/50176]	Loss: 1.0501
Training Epoch: 25 [48640/50176]	Loss: 1.2203
Training Epoch: 25 [48896/50176]	Loss: 1.1081
Training Epoch: 25 [49152/50176]	Loss: 1.0167
Training Epoch: 25 [49408/50176]	Loss: 1.2376
Training Epoch: 25 [49664/50176]	Loss: 1.0128
Training Epoch: 25 [49920/50176]	Loss: 1.1879
Training Epoch: 25 [50176/50176]	Loss: 1.4447
Validation Epoch: 25, Average loss: 0.0080, Accuracy: 0.5204
Training Epoch: 26 [256/50176]	Loss: 0.9334
Training Epoch: 26 [512/50176]	Loss: 1.0170
Training Epoch: 26 [768/50176]	Loss: 1.0228
Training Epoch: 26 [1024/50176]	Loss: 1.0402
Training Epoch: 26 [1280/50176]	Loss: 0.9742
Training Epoch: 26 [1536/50176]	Loss: 1.0286
Training Epoch: 26 [1792/50176]	Loss: 0.9454
Training Epoch: 26 [2048/50176]	Loss: 1.1153
Training Epoch: 26 [2304/50176]	Loss: 0.9132
Training Epoch: 26 [2560/50176]	Loss: 0.8205
Training Epoch: 26 [2816/50176]	Loss: 0.9753
Training Epoch: 26 [3072/50176]	Loss: 0.9208
Training Epoch: 26 [3328/50176]	Loss: 0.9744
Training Epoch: 26 [3584/50176]	Loss: 1.1458
Training Epoch: 26 [3840/50176]	Loss: 1.1272
Training Epoch: 26 [4096/50176]	Loss: 0.8926
Training Epoch: 26 [4352/50176]	Loss: 0.9975
Training Epoch: 26 [4608/50176]	Loss: 0.9490
Training Epoch: 26 [4864/50176]	Loss: 0.9881
Training Epoch: 26 [5120/50176]	Loss: 1.1122
Training Epoch: 26 [5376/50176]	Loss: 0.9558
Training Epoch: 26 [5632/50176]	Loss: 1.1474
Training Epoch: 26 [5888/50176]	Loss: 1.1400
Training Epoch: 26 [6144/50176]	Loss: 0.9467
Training Epoch: 26 [6400/50176]	Loss: 0.9847
Training Epoch: 26 [6656/50176]	Loss: 0.9595
Training Epoch: 26 [6912/50176]	Loss: 1.1113
Training Epoch: 26 [7168/50176]	Loss: 1.0425
Training Epoch: 26 [7424/50176]	Loss: 0.9475
Training Epoch: 26 [7680/50176]	Loss: 0.9400
Training Epoch: 26 [7936/50176]	Loss: 1.0195
Training Epoch: 26 [8192/50176]	Loss: 0.8521
Training Epoch: 26 [8448/50176]	Loss: 1.0451
Training Epoch: 26 [8704/50176]	Loss: 1.0104
Training Epoch: 26 [8960/50176]	Loss: 1.1636
Training Epoch: 26 [9216/50176]	Loss: 1.0363
Training Epoch: 26 [9472/50176]	Loss: 1.0083
Training Epoch: 26 [9728/50176]	Loss: 0.9292
Training Epoch: 26 [9984/50176]	Loss: 0.8509
Training Epoch: 26 [10240/50176]	Loss: 1.1116
Training Epoch: 26 [10496/50176]	Loss: 1.0337
Training Epoch: 26 [10752/50176]	Loss: 1.1645
Training Epoch: 26 [11008/50176]	Loss: 0.9945
Training Epoch: 26 [11264/50176]	Loss: 1.0197
Training Epoch: 26 [11520/50176]	Loss: 1.1092
Training Epoch: 26 [11776/50176]	Loss: 0.9720
Training Epoch: 26 [12032/50176]	Loss: 1.0312
Training Epoch: 26 [12288/50176]	Loss: 0.9940
Training Epoch: 26 [12544/50176]	Loss: 1.0327
Training Epoch: 26 [12800/50176]	Loss: 1.0630
Training Epoch: 26 [13056/50176]	Loss: 0.9863
Training Epoch: 26 [13312/50176]	Loss: 0.9411
Training Epoch: 26 [13568/50176]	Loss: 0.7998
Training Epoch: 26 [13824/50176]	Loss: 0.9534
Training Epoch: 26 [14080/50176]	Loss: 1.0212
Training Epoch: 26 [14336/50176]	Loss: 0.8670
Training Epoch: 26 [14592/50176]	Loss: 1.1672
Training Epoch: 26 [14848/50176]	Loss: 0.9920
Training Epoch: 26 [15104/50176]	Loss: 1.1327
Training Epoch: 26 [15360/50176]	Loss: 1.1273
Training Epoch: 26 [15616/50176]	Loss: 1.0182
Training Epoch: 26 [15872/50176]	Loss: 1.0588
Training Epoch: 26 [16128/50176]	Loss: 0.8599
Training Epoch: 26 [16384/50176]	Loss: 1.0063
Training Epoch: 26 [16640/50176]	Loss: 0.9197
Training Epoch: 26 [16896/50176]	Loss: 1.0840
Training Epoch: 26 [17152/50176]	Loss: 1.0585
Training Epoch: 26 [17408/50176]	Loss: 1.0369
Training Epoch: 26 [17664/50176]	Loss: 0.8804
Training Epoch: 26 [17920/50176]	Loss: 0.9696
Training Epoch: 26 [18176/50176]	Loss: 1.0407
Training Epoch: 26 [18432/50176]	Loss: 0.9759
Training Epoch: 26 [18688/50176]	Loss: 1.1068
Training Epoch: 26 [18944/50176]	Loss: 1.0483
Training Epoch: 26 [19200/50176]	Loss: 1.0194
Training Epoch: 26 [19456/50176]	Loss: 0.8261
Training Epoch: 26 [19712/50176]	Loss: 1.0958
Training Epoch: 26 [19968/50176]	Loss: 1.0005
Training Epoch: 26 [20224/50176]	Loss: 0.9565
Training Epoch: 26 [20480/50176]	Loss: 1.0931
Training Epoch: 26 [20736/50176]	Loss: 1.0256
Training Epoch: 26 [20992/50176]	Loss: 1.0615
Training Epoch: 26 [21248/50176]	Loss: 0.8574
Training Epoch: 26 [21504/50176]	Loss: 0.7554
Training Epoch: 26 [21760/50176]	Loss: 1.1107
Training Epoch: 26 [22016/50176]	Loss: 0.9807
Training Epoch: 26 [22272/50176]	Loss: 1.3099
Training Epoch: 26 [22528/50176]	Loss: 1.0685
Training Epoch: 26 [22784/50176]	Loss: 1.0716
Training Epoch: 26 [23040/50176]	Loss: 1.0051
Training Epoch: 26 [23296/50176]	Loss: 0.9924
Training Epoch: 26 [23552/50176]	Loss: 0.9406
Training Epoch: 26 [23808/50176]	Loss: 1.0841
Training Epoch: 26 [24064/50176]	Loss: 1.2499
Training Epoch: 26 [24320/50176]	Loss: 1.0067
Training Epoch: 26 [24576/50176]	Loss: 0.8892
Training Epoch: 26 [24832/50176]	Loss: 1.0597
Training Epoch: 26 [25088/50176]	Loss: 0.9647
Training Epoch: 26 [25344/50176]	Loss: 0.9961
Training Epoch: 26 [25600/50176]	Loss: 0.9496
Training Epoch: 26 [25856/50176]	Loss: 0.8480
Training Epoch: 26 [26112/50176]	Loss: 0.8853
Training Epoch: 26 [26368/50176]	Loss: 1.0604
Training Epoch: 26 [26624/50176]	Loss: 0.9628
Training Epoch: 26 [26880/50176]	Loss: 1.0294
Training Epoch: 26 [27136/50176]	Loss: 1.0242
Training Epoch: 26 [27392/50176]	Loss: 1.0942
Training Epoch: 26 [27648/50176]	Loss: 1.0020
Training Epoch: 26 [27904/50176]	Loss: 0.9570
Training Epoch: 26 [28160/50176]	Loss: 1.1203
Training Epoch: 26 [28416/50176]	Loss: 0.9929
Training Epoch: 26 [28672/50176]	Loss: 1.0372
Training Epoch: 26 [28928/50176]	Loss: 1.0741
Training Epoch: 26 [29184/50176]	Loss: 0.8875
Training Epoch: 26 [29440/50176]	Loss: 1.2282
Training Epoch: 26 [29696/50176]	Loss: 1.0561
Training Epoch: 26 [29952/50176]	Loss: 1.0395
Training Epoch: 26 [30208/50176]	Loss: 1.1554
Training Epoch: 26 [30464/50176]	Loss: 1.0961
Training Epoch: 26 [30720/50176]	Loss: 0.9029
Training Epoch: 26 [30976/50176]	Loss: 1.1034
Training Epoch: 26 [31232/50176]	Loss: 1.0479
Training Epoch: 26 [31488/50176]	Loss: 0.9648
Training Epoch: 26 [31744/50176]	Loss: 1.2112
Training Epoch: 26 [32000/50176]	Loss: 1.1690
Training Epoch: 26 [32256/50176]	Loss: 1.0551
Training Epoch: 26 [32512/50176]	Loss: 1.2559
Training Epoch: 26 [32768/50176]	Loss: 0.9322
Training Epoch: 26 [33024/50176]	Loss: 0.9200
Training Epoch: 26 [33280/50176]	Loss: 0.8862
Training Epoch: 26 [33536/50176]	Loss: 1.0850
Training Epoch: 26 [33792/50176]	Loss: 1.0491
Training Epoch: 26 [34048/50176]	Loss: 1.2189
Training Epoch: 26 [34304/50176]	Loss: 1.0251
Training Epoch: 26 [34560/50176]	Loss: 0.9007
Training Epoch: 26 [34816/50176]	Loss: 1.1374
Training Epoch: 26 [35072/50176]	Loss: 1.2061
Training Epoch: 26 [35328/50176]	Loss: 1.1179
Training Epoch: 26 [35584/50176]	Loss: 1.0909
Training Epoch: 26 [35840/50176]	Loss: 0.8978
Training Epoch: 26 [36096/50176]	Loss: 0.9647
Training Epoch: 26 [36352/50176]	Loss: 1.0907
Training Epoch: 26 [36608/50176]	Loss: 1.1642
Training Epoch: 26 [36864/50176]	Loss: 0.9137
Training Epoch: 26 [37120/50176]	Loss: 1.0698
Training Epoch: 26 [37376/50176]	Loss: 1.0896
Training Epoch: 26 [37632/50176]	Loss: 1.0687
Training Epoch: 26 [37888/50176]	Loss: 0.9492
Training Epoch: 26 [38144/50176]	Loss: 1.1249
Training Epoch: 26 [38400/50176]	Loss: 0.9902
Training Epoch: 26 [38656/50176]	Loss: 1.0662
Training Epoch: 26 [38912/50176]	Loss: 1.0522
Training Epoch: 26 [39168/50176]	Loss: 1.1456
Training Epoch: 26 [39424/50176]	Loss: 1.0145
Training Epoch: 26 [39680/50176]	Loss: 1.0883
Training Epoch: 26 [39936/50176]	Loss: 1.0214
Training Epoch: 26 [40192/50176]	Loss: 0.9912
Training Epoch: 26 [40448/50176]	Loss: 0.9868
Training Epoch: 26 [40704/50176]	Loss: 1.1377
Training Epoch: 26 [40960/50176]	Loss: 0.9295
Training Epoch: 26 [41216/50176]	Loss: 0.9255
Training Epoch: 26 [41472/50176]	Loss: 0.9435
Training Epoch: 26 [41728/50176]	Loss: 0.9765
Training Epoch: 26 [41984/50176]	Loss: 1.0679
Training Epoch: 26 [42240/50176]	Loss: 1.0477
Training Epoch: 26 [42496/50176]	Loss: 1.1520
Training Epoch: 26 [42752/50176]	Loss: 0.8195
Training Epoch: 26 [43008/50176]	Loss: 1.1186
Training Epoch: 26 [43264/50176]	Loss: 0.9737
Training Epoch: 26 [43520/50176]	Loss: 0.9944
Training Epoch: 26 [43776/50176]	Loss: 1.0477
Training Epoch: 26 [44032/50176]	Loss: 0.9827
Training Epoch: 26 [44288/50176]	Loss: 1.0755
Training Epoch: 26 [44544/50176]	Loss: 1.0449
Training Epoch: 26 [44800/50176]	Loss: 0.9988
Training Epoch: 26 [45056/50176]	Loss: 1.2284
Training Epoch: 26 [45312/50176]	Loss: 1.0636
Training Epoch: 26 [45568/50176]	Loss: 1.0475
Training Epoch: 26 [45824/50176]	Loss: 1.0392
Training Epoch: 26 [46080/50176]	Loss: 0.9098
Training Epoch: 26 [46336/50176]	Loss: 1.0370
Training Epoch: 26 [46592/50176]	Loss: 1.1223
Training Epoch: 26 [46848/50176]	Loss: 1.1029
Training Epoch: 26 [47104/50176]	Loss: 1.1648
Training Epoch: 26 [47360/50176]	Loss: 0.9096
Training Epoch: 26 [47616/50176]	Loss: 1.0174
Training Epoch: 26 [47872/50176]	Loss: 1.1174
Training Epoch: 26 [48128/50176]	Loss: 1.0705
Training Epoch: 26 [48384/50176]	Loss: 1.1150
Training Epoch: 26 [48640/50176]	Loss: 1.1696
Training Epoch: 26 [48896/50176]	Loss: 1.0768
Training Epoch: 26 [49152/50176]	Loss: 1.0260
Training Epoch: 26 [49408/50176]	Loss: 1.0845
Training Epoch: 26 [49664/50176]	Loss: 1.2274
Training Epoch: 26 [49920/50176]	Loss: 0.9876
Training Epoch: 26 [50176/50176]	Loss: 0.9846
Validation Epoch: 26, Average loss: 0.0084, Accuracy: 0.4938
Training Epoch: 27 [256/50176]	Loss: 0.8940
Training Epoch: 27 [512/50176]	Loss: 0.8598
Training Epoch: 27 [768/50176]	Loss: 0.7762
Training Epoch: 27 [1024/50176]	Loss: 0.9405
Training Epoch: 27 [1280/50176]	Loss: 1.0473
Training Epoch: 27 [1536/50176]	Loss: 1.0118
Training Epoch: 27 [1792/50176]	Loss: 0.9528
Training Epoch: 27 [2048/50176]	Loss: 0.9585
Training Epoch: 27 [2304/50176]	Loss: 0.7779
Training Epoch: 27 [2560/50176]	Loss: 1.0143
Training Epoch: 27 [2816/50176]	Loss: 0.9764
Training Epoch: 27 [3072/50176]	Loss: 0.9238
Training Epoch: 27 [3328/50176]	Loss: 0.9777
Training Epoch: 27 [3584/50176]	Loss: 0.9573
Training Epoch: 27 [3840/50176]	Loss: 1.0361
Training Epoch: 27 [4096/50176]	Loss: 0.8523
Training Epoch: 27 [4352/50176]	Loss: 1.0551
Training Epoch: 27 [4608/50176]	Loss: 1.0263
Training Epoch: 27 [4864/50176]	Loss: 0.8506
Training Epoch: 27 [5120/50176]	Loss: 0.9847
Training Epoch: 27 [5376/50176]	Loss: 1.0272
Training Epoch: 27 [5632/50176]	Loss: 0.9779
Training Epoch: 27 [5888/50176]	Loss: 0.9673
Training Epoch: 27 [6144/50176]	Loss: 0.8917
Training Epoch: 27 [6400/50176]	Loss: 0.8633
Training Epoch: 27 [6656/50176]	Loss: 1.0462
Training Epoch: 27 [6912/50176]	Loss: 0.9036
Training Epoch: 27 [7168/50176]	Loss: 0.9810
Training Epoch: 27 [7424/50176]	Loss: 1.0389
Training Epoch: 27 [7680/50176]	Loss: 0.9708
Training Epoch: 27 [7936/50176]	Loss: 0.9782
Training Epoch: 27 [8192/50176]	Loss: 1.0122
Training Epoch: 27 [8448/50176]	Loss: 1.0136
Training Epoch: 27 [8704/50176]	Loss: 0.9358
Training Epoch: 27 [8960/50176]	Loss: 0.8984
Training Epoch: 27 [9216/50176]	Loss: 1.0255
Training Epoch: 27 [9472/50176]	Loss: 0.8735
Training Epoch: 27 [9728/50176]	Loss: 1.0446
Training Epoch: 27 [9984/50176]	Loss: 1.0542
Training Epoch: 27 [10240/50176]	Loss: 0.9672
Training Epoch: 27 [10496/50176]	Loss: 1.0151
Training Epoch: 27 [10752/50176]	Loss: 0.9946
Training Epoch: 27 [11008/50176]	Loss: 1.0198
Training Epoch: 27 [11264/50176]	Loss: 0.9404
Training Epoch: 27 [11520/50176]	Loss: 1.0666
Training Epoch: 27 [11776/50176]	Loss: 0.9468
Training Epoch: 27 [12032/50176]	Loss: 0.8970
Training Epoch: 27 [12288/50176]	Loss: 1.0445
Training Epoch: 27 [12544/50176]	Loss: 0.9441
Training Epoch: 27 [12800/50176]	Loss: 0.8693
Training Epoch: 27 [13056/50176]	Loss: 1.0570
Training Epoch: 27 [13312/50176]	Loss: 1.0501
Training Epoch: 27 [13568/50176]	Loss: 0.9109
Training Epoch: 27 [13824/50176]	Loss: 0.8864
Training Epoch: 27 [14080/50176]	Loss: 1.0073
Training Epoch: 27 [14336/50176]	Loss: 0.9427
Training Epoch: 27 [14592/50176]	Loss: 1.0603
Training Epoch: 27 [14848/50176]	Loss: 0.9172
Training Epoch: 27 [15104/50176]	Loss: 1.0193
Training Epoch: 27 [15360/50176]	Loss: 0.9127
Training Epoch: 27 [15616/50176]	Loss: 1.1183
Training Epoch: 27 [15872/50176]	Loss: 0.9454
Training Epoch: 27 [16128/50176]	Loss: 0.9126
Training Epoch: 27 [16384/50176]	Loss: 1.0013
Training Epoch: 27 [16640/50176]	Loss: 1.0391
Training Epoch: 27 [16896/50176]	Loss: 1.0390
Training Epoch: 27 [17152/50176]	Loss: 1.0007
Training Epoch: 27 [17408/50176]	Loss: 1.1276
Training Epoch: 27 [17664/50176]	Loss: 1.0213
Training Epoch: 27 [17920/50176]	Loss: 0.9927
Training Epoch: 27 [18176/50176]	Loss: 0.8167
Training Epoch: 27 [18432/50176]	Loss: 0.8943
Training Epoch: 27 [18688/50176]	Loss: 0.9795
Training Epoch: 27 [18944/50176]	Loss: 0.8724
Training Epoch: 27 [19200/50176]	Loss: 0.9537
Training Epoch: 27 [19456/50176]	Loss: 1.0091
Training Epoch: 27 [19712/50176]	Loss: 1.0330
Training Epoch: 27 [19968/50176]	Loss: 0.9134
Training Epoch: 27 [20224/50176]	Loss: 0.8823
Training Epoch: 27 [20480/50176]	Loss: 0.8789
Training Epoch: 27 [20736/50176]	Loss: 0.9208
Training Epoch: 27 [20992/50176]	Loss: 1.1338
Training Epoch: 27 [21248/50176]	Loss: 1.1575
Training Epoch: 27 [21504/50176]	Loss: 0.8757
Training Epoch: 27 [21760/50176]	Loss: 1.0067
Training Epoch: 27 [22016/50176]	Loss: 0.9977
Training Epoch: 27 [22272/50176]	Loss: 0.9720
Training Epoch: 27 [22528/50176]	Loss: 0.8840
Training Epoch: 27 [22784/50176]	Loss: 1.0045
Training Epoch: 27 [23040/50176]	Loss: 1.0125
Training Epoch: 27 [23296/50176]	Loss: 0.9303
Training Epoch: 27 [23552/50176]	Loss: 1.0696
Training Epoch: 27 [23808/50176]	Loss: 0.8949
Training Epoch: 27 [24064/50176]	Loss: 1.0897
Training Epoch: 27 [24320/50176]	Loss: 0.9503
Training Epoch: 27 [24576/50176]	Loss: 1.0241
Training Epoch: 27 [24832/50176]	Loss: 1.0596
Training Epoch: 27 [25088/50176]	Loss: 1.1096
Training Epoch: 27 [25344/50176]	Loss: 1.1685
Training Epoch: 27 [25600/50176]	Loss: 1.1148
Training Epoch: 27 [25856/50176]	Loss: 1.0420
Training Epoch: 27 [26112/50176]	Loss: 0.9554
Training Epoch: 27 [26368/50176]	Loss: 1.0215
Training Epoch: 27 [26624/50176]	Loss: 0.9105
Training Epoch: 27 [26880/50176]	Loss: 0.9433
Training Epoch: 27 [27136/50176]	Loss: 0.8488
Training Epoch: 27 [27392/50176]	Loss: 0.9472
Training Epoch: 27 [27648/50176]	Loss: 1.0170
Training Epoch: 27 [27904/50176]	Loss: 0.9407
Training Epoch: 27 [28160/50176]	Loss: 0.9589
Training Epoch: 27 [28416/50176]	Loss: 0.9157
Training Epoch: 27 [28672/50176]	Loss: 1.1365
Training Epoch: 27 [28928/50176]	Loss: 1.0055
Training Epoch: 27 [29184/50176]	Loss: 1.0214
Training Epoch: 27 [29440/50176]	Loss: 1.0657
Training Epoch: 27 [29696/50176]	Loss: 0.8642
Training Epoch: 27 [29952/50176]	Loss: 0.9535
Training Epoch: 27 [30208/50176]	Loss: 0.9189
Training Epoch: 27 [30464/50176]	Loss: 0.9970
Training Epoch: 27 [30720/50176]	Loss: 1.0912
Training Epoch: 27 [30976/50176]	Loss: 0.9917
Training Epoch: 27 [31232/50176]	Loss: 1.0424
Training Epoch: 27 [31488/50176]	Loss: 1.0106
Training Epoch: 27 [31744/50176]	Loss: 0.9492
Training Epoch: 27 [32000/50176]	Loss: 1.0325
Training Epoch: 27 [32256/50176]	Loss: 1.0674
Training Epoch: 27 [32512/50176]	Loss: 0.9119
Training Epoch: 27 [32768/50176]	Loss: 0.9054
Training Epoch: 27 [33024/50176]	Loss: 1.1195
Training Epoch: 27 [33280/50176]	Loss: 0.9731
Training Epoch: 27 [33536/50176]	Loss: 1.1381
Training Epoch: 27 [33792/50176]	Loss: 0.8889
Training Epoch: 27 [34048/50176]	Loss: 0.8851
Training Epoch: 27 [34304/50176]	Loss: 1.0153
Training Epoch: 27 [34560/50176]	Loss: 1.0615
Training Epoch: 27 [34816/50176]	Loss: 1.0340
Training Epoch: 27 [35072/50176]	Loss: 1.0595
Training Epoch: 27 [35328/50176]	Loss: 0.9357
Training Epoch: 27 [35584/50176]	Loss: 1.0867
Training Epoch: 27 [35840/50176]	Loss: 1.1154
Training Epoch: 27 [36096/50176]	Loss: 1.0445
Training Epoch: 27 [36352/50176]	Loss: 0.9021
Training Epoch: 27 [36608/50176]	Loss: 1.0463
Training Epoch: 27 [36864/50176]	Loss: 0.9876
Training Epoch: 27 [37120/50176]	Loss: 1.0048
Training Epoch: 27 [37376/50176]	Loss: 1.1061
Training Epoch: 27 [37632/50176]	Loss: 1.0339
Training Epoch: 27 [37888/50176]	Loss: 0.8499
Training Epoch: 27 [38144/50176]	Loss: 0.9103
Training Epoch: 27 [38400/50176]	Loss: 1.0604
Training Epoch: 27 [38656/50176]	Loss: 0.7989
Training Epoch: 27 [38912/50176]	Loss: 0.9811
Training Epoch: 27 [39168/50176]	Loss: 0.9162
Training Epoch: 27 [39424/50176]	Loss: 1.0324
Training Epoch: 27 [39680/50176]	Loss: 1.0797
Training Epoch: 27 [39936/50176]	Loss: 1.0129
Training Epoch: 27 [40192/50176]	Loss: 1.0023
Training Epoch: 27 [40448/50176]	Loss: 1.1172
Training Epoch: 27 [40704/50176]	Loss: 1.0542
Training Epoch: 27 [40960/50176]	Loss: 0.9615
Training Epoch: 27 [41216/50176]	Loss: 1.0114
Training Epoch: 27 [41472/50176]	Loss: 0.8707
Training Epoch: 27 [41728/50176]	Loss: 0.9538
Training Epoch: 27 [41984/50176]	Loss: 1.0133
Training Epoch: 27 [42240/50176]	Loss: 0.9893
Training Epoch: 27 [42496/50176]	Loss: 1.1867
Training Epoch: 27 [42752/50176]	Loss: 1.2311
Training Epoch: 27 [43008/50176]	Loss: 0.9953
Training Epoch: 27 [43264/50176]	Loss: 1.0802
Training Epoch: 27 [43520/50176]	Loss: 1.1006
Training Epoch: 27 [43776/50176]	Loss: 1.0927
Training Epoch: 27 [44032/50176]	Loss: 1.0750
Training Epoch: 27 [44288/50176]	Loss: 0.8993
Training Epoch: 27 [44544/50176]	Loss: 1.1175
Training Epoch: 27 [44800/50176]	Loss: 1.0975
Training Epoch: 27 [45056/50176]	Loss: 1.0405
Training Epoch: 27 [45312/50176]	Loss: 1.0640
Training Epoch: 27 [45568/50176]	Loss: 1.1350
Training Epoch: 27 [45824/50176]	Loss: 0.8711
Training Epoch: 27 [46080/50176]	Loss: 1.0106
Training Epoch: 27 [46336/50176]	Loss: 1.0094
Training Epoch: 27 [46592/50176]	Loss: 1.1212
Training Epoch: 27 [46848/50176]	Loss: 1.1128
Training Epoch: 27 [47104/50176]	Loss: 1.1732
Training Epoch: 27 [47360/50176]	Loss: 1.2036
Training Epoch: 27 [47616/50176]	Loss: 0.9937
Training Epoch: 27 [47872/50176]	Loss: 0.8863
Training Epoch: 27 [48128/50176]	Loss: 1.0332
Training Epoch: 27 [48384/50176]	Loss: 1.0452
Training Epoch: 27 [48640/50176]	Loss: 1.1213
Training Epoch: 27 [48896/50176]	Loss: 0.9585
Training Epoch: 27 [49152/50176]	Loss: 0.9304
Training Epoch: 27 [49408/50176]	Loss: 0.9432
Training Epoch: 27 [49664/50176]	Loss: 0.9572
Training Epoch: 27 [49920/50176]	Loss: 0.9964
Training Epoch: 27 [50176/50176]	Loss: 1.3074
Validation Epoch: 27, Average loss: 0.0081, Accuracy: 0.5018
Training Epoch: 28 [256/50176]	Loss: 0.8931
Training Epoch: 28 [512/50176]	Loss: 1.0585
Training Epoch: 28 [768/50176]	Loss: 0.9188
Training Epoch: 28 [1024/50176]	Loss: 0.8948
Training Epoch: 28 [1280/50176]	Loss: 0.7649
Training Epoch: 28 [1536/50176]	Loss: 0.9008
Training Epoch: 28 [1792/50176]	Loss: 0.9633
Training Epoch: 28 [2048/50176]	Loss: 0.8139
Training Epoch: 28 [2304/50176]	Loss: 0.8186
Training Epoch: 28 [2560/50176]	Loss: 0.9509
Training Epoch: 28 [2816/50176]	Loss: 0.7715
Training Epoch: 28 [3072/50176]	Loss: 0.7768
Training Epoch: 28 [3328/50176]	Loss: 0.8877
Training Epoch: 28 [3584/50176]	Loss: 0.9415
Training Epoch: 28 [3840/50176]	Loss: 0.8628
Training Epoch: 28 [4096/50176]	Loss: 1.0312
Training Epoch: 28 [4352/50176]	Loss: 0.7826
Training Epoch: 28 [4608/50176]	Loss: 0.8064
Training Epoch: 28 [4864/50176]	Loss: 0.8969
Training Epoch: 28 [5120/50176]	Loss: 0.7927
Training Epoch: 28 [5376/50176]	Loss: 0.8846
Training Epoch: 28 [5632/50176]	Loss: 0.9131
Training Epoch: 28 [5888/50176]	Loss: 0.9355
Training Epoch: 28 [6144/50176]	Loss: 0.9513
Training Epoch: 28 [6400/50176]	Loss: 0.7841
Training Epoch: 28 [6656/50176]	Loss: 0.8343
Training Epoch: 28 [6912/50176]	Loss: 0.9833
Training Epoch: 28 [7168/50176]	Loss: 0.9960
Training Epoch: 28 [7424/50176]	Loss: 0.7891
Training Epoch: 28 [7680/50176]	Loss: 0.8261
Training Epoch: 28 [7936/50176]	Loss: 0.9535
Training Epoch: 28 [8192/50176]	Loss: 0.8390
Training Epoch: 28 [8448/50176]	Loss: 0.9182
Training Epoch: 28 [8704/50176]	Loss: 0.8936
Training Epoch: 28 [8960/50176]	Loss: 0.8891
Training Epoch: 28 [9216/50176]	Loss: 0.7961
Training Epoch: 28 [9472/50176]	Loss: 0.9935
Training Epoch: 28 [9728/50176]	Loss: 0.8710
Training Epoch: 28 [9984/50176]	Loss: 0.9681
Training Epoch: 28 [10240/50176]	Loss: 0.7924
Training Epoch: 28 [10496/50176]	Loss: 1.1316
Training Epoch: 28 [10752/50176]	Loss: 0.9068
Training Epoch: 28 [11008/50176]	Loss: 1.0162
Training Epoch: 28 [11264/50176]	Loss: 0.9184
Training Epoch: 28 [11520/50176]	Loss: 0.9903
Training Epoch: 28 [11776/50176]	Loss: 1.0097
Training Epoch: 28 [12032/50176]	Loss: 0.8720
Training Epoch: 28 [12288/50176]	Loss: 0.9897
Training Epoch: 28 [12544/50176]	Loss: 0.9341
Training Epoch: 28 [12800/50176]	Loss: 1.1867
Training Epoch: 28 [13056/50176]	Loss: 0.8885
Training Epoch: 28 [13312/50176]	Loss: 0.9914
Training Epoch: 28 [13568/50176]	Loss: 0.8274
Training Epoch: 28 [13824/50176]	Loss: 0.9466
Training Epoch: 28 [14080/50176]	Loss: 0.9908
Training Epoch: 28 [14336/50176]	Loss: 1.0831
Training Epoch: 28 [14592/50176]	Loss: 0.9607
Training Epoch: 28 [14848/50176]	Loss: 1.0099
Training Epoch: 28 [15104/50176]	Loss: 0.9304
Training Epoch: 28 [15360/50176]	Loss: 0.8148
Training Epoch: 28 [15616/50176]	Loss: 0.9226
Training Epoch: 28 [15872/50176]	Loss: 0.8480
Training Epoch: 28 [16128/50176]	Loss: 0.9285
Training Epoch: 28 [16384/50176]	Loss: 0.9261
Training Epoch: 28 [16640/50176]	Loss: 0.9620
Training Epoch: 28 [16896/50176]	Loss: 0.8659
Training Epoch: 28 [17152/50176]	Loss: 0.9565
Training Epoch: 28 [17408/50176]	Loss: 0.9565
Training Epoch: 28 [17664/50176]	Loss: 0.8258
Training Epoch: 28 [17920/50176]	Loss: 0.8618
Training Epoch: 28 [18176/50176]	Loss: 0.8365
Training Epoch: 28 [18432/50176]	Loss: 0.9323
Training Epoch: 28 [18688/50176]	Loss: 0.9538
Training Epoch: 28 [18944/50176]	Loss: 0.9951
Training Epoch: 28 [19200/50176]	Loss: 1.0839
Training Epoch: 28 [19456/50176]	Loss: 0.9037
Training Epoch: 28 [19712/50176]	Loss: 1.0049
Training Epoch: 28 [19968/50176]	Loss: 0.8120
Training Epoch: 28 [20224/50176]	Loss: 0.9428
Training Epoch: 28 [20480/50176]	Loss: 0.9208
Training Epoch: 28 [20736/50176]	Loss: 0.9196
Training Epoch: 28 [20992/50176]	Loss: 1.0152
Training Epoch: 28 [21248/50176]	Loss: 0.9495
Training Epoch: 28 [21504/50176]	Loss: 0.9154
Training Epoch: 28 [21760/50176]	Loss: 1.0503
Training Epoch: 28 [22016/50176]	Loss: 0.9374
Training Epoch: 28 [22272/50176]	Loss: 0.7492
Training Epoch: 28 [22528/50176]	Loss: 1.0229
Training Epoch: 28 [22784/50176]	Loss: 0.8769
Training Epoch: 28 [23040/50176]	Loss: 0.8662
Training Epoch: 28 [23296/50176]	Loss: 0.9240
Training Epoch: 28 [23552/50176]	Loss: 1.0503
Training Epoch: 28 [23808/50176]	Loss: 0.9109
Training Epoch: 28 [24064/50176]	Loss: 0.9125
Training Epoch: 28 [24320/50176]	Loss: 0.8081
Training Epoch: 28 [24576/50176]	Loss: 0.9664
Training Epoch: 28 [24832/50176]	Loss: 1.1582
Training Epoch: 28 [25088/50176]	Loss: 0.9445
Training Epoch: 28 [25344/50176]	Loss: 0.9145
Training Epoch: 28 [25600/50176]	Loss: 0.9384
Training Epoch: 28 [25856/50176]	Loss: 0.9217
Training Epoch: 28 [26112/50176]	Loss: 1.0226
Training Epoch: 28 [26368/50176]	Loss: 1.0405
Training Epoch: 28 [26624/50176]	Loss: 0.9973
Training Epoch: 28 [26880/50176]	Loss: 0.9133
Training Epoch: 28 [27136/50176]	Loss: 1.0881
Training Epoch: 28 [27392/50176]	Loss: 0.9076
Training Epoch: 28 [27648/50176]	Loss: 1.0216
Training Epoch: 28 [27904/50176]	Loss: 0.9686
Training Epoch: 28 [28160/50176]	Loss: 0.9281
Training Epoch: 28 [28416/50176]	Loss: 1.1170
Training Epoch: 28 [28672/50176]	Loss: 1.1429
Training Epoch: 28 [28928/50176]	Loss: 1.0106
Training Epoch: 28 [29184/50176]	Loss: 1.0197
Training Epoch: 28 [29440/50176]	Loss: 1.0805
Training Epoch: 28 [29696/50176]	Loss: 1.1792
Training Epoch: 28 [29952/50176]	Loss: 0.9597
Training Epoch: 28 [30208/50176]	Loss: 0.9140
Training Epoch: 28 [30464/50176]	Loss: 0.9913
Training Epoch: 28 [30720/50176]	Loss: 0.9378
Training Epoch: 28 [30976/50176]	Loss: 1.1247
Training Epoch: 28 [31232/50176]	Loss: 0.8849
Training Epoch: 28 [31488/50176]	Loss: 1.0357
Training Epoch: 28 [31744/50176]	Loss: 0.9958
Training Epoch: 28 [32000/50176]	Loss: 0.9161
Training Epoch: 28 [32256/50176]	Loss: 1.0291
Training Epoch: 28 [32512/50176]	Loss: 0.9744
Training Epoch: 28 [32768/50176]	Loss: 1.0279
Training Epoch: 28 [33024/50176]	Loss: 0.9444
Training Epoch: 28 [33280/50176]	Loss: 0.9134
Training Epoch: 28 [33536/50176]	Loss: 0.8731
Training Epoch: 28 [33792/50176]	Loss: 1.0178
Training Epoch: 28 [34048/50176]	Loss: 0.9669
Training Epoch: 28 [34304/50176]	Loss: 1.0747
Training Epoch: 28 [34560/50176]	Loss: 0.9887
Training Epoch: 28 [34816/50176]	Loss: 0.9088
Training Epoch: 28 [35072/50176]	Loss: 0.8950
Training Epoch: 28 [35328/50176]	Loss: 0.9416
Training Epoch: 28 [35584/50176]	Loss: 0.9327
Training Epoch: 28 [35840/50176]	Loss: 1.1626
Training Epoch: 28 [36096/50176]	Loss: 0.9465
Training Epoch: 28 [36352/50176]	Loss: 1.2426
Training Epoch: 28 [36608/50176]	Loss: 1.0154
Training Epoch: 28 [36864/50176]	Loss: 1.0522
Training Epoch: 28 [37120/50176]	Loss: 1.0122
Training Epoch: 28 [37376/50176]	Loss: 1.0416
Training Epoch: 28 [37632/50176]	Loss: 0.8737
Training Epoch: 28 [37888/50176]	Loss: 1.0414
Training Epoch: 28 [38144/50176]	Loss: 0.9196
Training Epoch: 28 [38400/50176]	Loss: 0.9562
Training Epoch: 28 [38656/50176]	Loss: 0.9809
Training Epoch: 28 [38912/50176]	Loss: 0.9931
Training Epoch: 28 [39168/50176]	Loss: 1.0246
Training Epoch: 28 [39424/50176]	Loss: 0.9896
Training Epoch: 28 [39680/50176]	Loss: 0.9502
Training Epoch: 28 [39936/50176]	Loss: 1.0006
Training Epoch: 28 [40192/50176]	Loss: 0.9911
Training Epoch: 28 [40448/50176]	Loss: 1.0272
Training Epoch: 28 [40704/50176]	Loss: 1.0683
Training Epoch: 28 [40960/50176]	Loss: 0.9916
Training Epoch: 28 [41216/50176]	Loss: 1.1064
Training Epoch: 28 [41472/50176]	Loss: 1.0594
Training Epoch: 28 [41728/50176]	Loss: 0.9472
Training Epoch: 28 [41984/50176]	Loss: 1.0225
Training Epoch: 28 [42240/50176]	Loss: 0.9885
Training Epoch: 28 [42496/50176]	Loss: 0.8246
Training Epoch: 28 [42752/50176]	Loss: 1.0976
Training Epoch: 28 [43008/50176]	Loss: 0.9723
Training Epoch: 28 [43264/50176]	Loss: 1.1589
Training Epoch: 28 [43520/50176]	Loss: 1.0633
Training Epoch: 28 [43776/50176]	Loss: 1.1094
Training Epoch: 28 [44032/50176]	Loss: 0.8465
Training Epoch: 28 [44288/50176]	Loss: 0.8838
Training Epoch: 28 [44544/50176]	Loss: 1.0781
Training Epoch: 28 [44800/50176]	Loss: 1.1134
Training Epoch: 28 [45056/50176]	Loss: 1.0579
Training Epoch: 28 [45312/50176]	Loss: 0.9778
Training Epoch: 28 [45568/50176]	Loss: 0.9534
Training Epoch: 28 [45824/50176]	Loss: 0.9581
Training Epoch: 28 [46080/50176]	Loss: 0.8901
Training Epoch: 28 [46336/50176]	Loss: 1.1830
Training Epoch: 28 [46592/50176]	Loss: 1.0021
Training Epoch: 28 [46848/50176]	Loss: 1.0739
Training Epoch: 28 [47104/50176]	Loss: 1.0687
Training Epoch: 28 [47360/50176]	Loss: 1.0327
Training Epoch: 28 [47616/50176]	Loss: 1.0274
Training Epoch: 28 [47872/50176]	Loss: 0.8280
Training Epoch: 28 [48128/50176]	Loss: 0.9547
Training Epoch: 28 [48384/50176]	Loss: 0.9043
Training Epoch: 28 [48640/50176]	Loss: 0.9402
Training Epoch: 28 [48896/50176]	Loss: 1.0044
Training Epoch: 28 [49152/50176]	Loss: 1.0813
Training Epoch: 28 [49408/50176]	Loss: 1.0021
Training Epoch: 28 [49664/50176]	Loss: 1.1445
Training Epoch: 28 [49920/50176]	Loss: 1.1487
Training Epoch: 28 [50176/50176]	Loss: 0.9920
Validation Epoch: 28, Average loss: 0.0105, Accuracy: 0.4450
Training Epoch: 29 [256/50176]	Loss: 0.8743
Training Epoch: 29 [512/50176]	Loss: 0.9697
Training Epoch: 29 [768/50176]	Loss: 0.8141
Training Epoch: 29 [1024/50176]	Loss: 0.8687
Training Epoch: 29 [1280/50176]	Loss: 0.9106
Training Epoch: 29 [1536/50176]	Loss: 0.8928
Training Epoch: 29 [1792/50176]	Loss: 0.9494
Training Epoch: 29 [2048/50176]	Loss: 0.9513
Training Epoch: 29 [2304/50176]	Loss: 0.8686
Training Epoch: 29 [2560/50176]	Loss: 0.9640
Training Epoch: 29 [2816/50176]	Loss: 0.9395
Training Epoch: 29 [3072/50176]	Loss: 0.8623
Training Epoch: 29 [3328/50176]	Loss: 0.8350
Training Epoch: 29 [3584/50176]	Loss: 0.9474
Training Epoch: 29 [3840/50176]	Loss: 0.9142
Training Epoch: 29 [4096/50176]	Loss: 0.8085
Training Epoch: 29 [4352/50176]	Loss: 1.0313
Training Epoch: 29 [4608/50176]	Loss: 0.9746
Training Epoch: 29 [4864/50176]	Loss: 0.9944
Training Epoch: 29 [5120/50176]	Loss: 0.8553
Training Epoch: 29 [5376/50176]	Loss: 0.9108
Training Epoch: 29 [5632/50176]	Loss: 1.0098
Training Epoch: 29 [5888/50176]	Loss: 0.8803
Training Epoch: 29 [6144/50176]	Loss: 0.8502
Training Epoch: 29 [6400/50176]	Loss: 0.8250
Training Epoch: 29 [6656/50176]	Loss: 0.9064
Training Epoch: 29 [6912/50176]	Loss: 0.9526
Training Epoch: 29 [7168/50176]	Loss: 0.7979
Training Epoch: 29 [7424/50176]	Loss: 0.7465
Training Epoch: 29 [7680/50176]	Loss: 0.9810
Training Epoch: 29 [7936/50176]	Loss: 0.9396
Training Epoch: 29 [8192/50176]	Loss: 0.8758
Training Epoch: 29 [8448/50176]	Loss: 0.8984
Training Epoch: 29 [8704/50176]	Loss: 0.8449
Training Epoch: 29 [8960/50176]	Loss: 0.7646
Training Epoch: 29 [9216/50176]	Loss: 0.8932
Training Epoch: 29 [9472/50176]	Loss: 0.8055
Training Epoch: 29 [9728/50176]	Loss: 1.0620
Training Epoch: 29 [9984/50176]	Loss: 0.8564
Training Epoch: 29 [10240/50176]	Loss: 0.8519
Training Epoch: 29 [10496/50176]	Loss: 0.7670
Training Epoch: 29 [10752/50176]	Loss: 0.8836
Training Epoch: 29 [11008/50176]	Loss: 0.8730
Training Epoch: 29 [11264/50176]	Loss: 0.8785
Training Epoch: 29 [11520/50176]	Loss: 0.9793
Training Epoch: 29 [11776/50176]	Loss: 1.0557
Training Epoch: 29 [12032/50176]	Loss: 1.0115
Training Epoch: 29 [12288/50176]	Loss: 0.8759
Training Epoch: 29 [12544/50176]	Loss: 1.0448
Training Epoch: 29 [12800/50176]	Loss: 0.9148
Training Epoch: 29 [13056/50176]	Loss: 1.0521
Training Epoch: 29 [13312/50176]	Loss: 1.0411
Training Epoch: 29 [13568/50176]	Loss: 0.8723
Training Epoch: 29 [13824/50176]	Loss: 0.9080
Training Epoch: 29 [14080/50176]	Loss: 0.9270
Training Epoch: 29 [14336/50176]	Loss: 0.9160
Training Epoch: 29 [14592/50176]	Loss: 0.9806
Training Epoch: 29 [14848/50176]	Loss: 0.9226
Training Epoch: 29 [15104/50176]	Loss: 0.8323
Training Epoch: 29 [15360/50176]	Loss: 0.9194
Training Epoch: 29 [15616/50176]	Loss: 0.9731
Training Epoch: 29 [15872/50176]	Loss: 0.8614
Training Epoch: 29 [16128/50176]	Loss: 0.8746
Training Epoch: 29 [16384/50176]	Loss: 0.8850
Training Epoch: 29 [16640/50176]	Loss: 0.9158
Training Epoch: 29 [16896/50176]	Loss: 0.8948
Training Epoch: 29 [17152/50176]	Loss: 0.9807
Training Epoch: 29 [17408/50176]	Loss: 0.8881
Training Epoch: 29 [17664/50176]	Loss: 0.9805
Training Epoch: 29 [17920/50176]	Loss: 0.7957
Training Epoch: 29 [18176/50176]	Loss: 0.9002
Training Epoch: 29 [18432/50176]	Loss: 0.9303
Training Epoch: 29 [18688/50176]	Loss: 0.9061
Training Epoch: 29 [18944/50176]	Loss: 0.9623
Training Epoch: 29 [19200/50176]	Loss: 0.9478
Training Epoch: 29 [19456/50176]	Loss: 0.9634
Training Epoch: 29 [19712/50176]	Loss: 0.9048
Training Epoch: 29 [19968/50176]	Loss: 0.9452
Training Epoch: 29 [20224/50176]	Loss: 1.0355
Training Epoch: 29 [20480/50176]	Loss: 0.9030
Training Epoch: 29 [20736/50176]	Loss: 0.9962
Training Epoch: 29 [20992/50176]	Loss: 0.9984
Training Epoch: 29 [21248/50176]	Loss: 0.8249
Training Epoch: 29 [21504/50176]	Loss: 0.8860
Training Epoch: 29 [21760/50176]	Loss: 0.8436
Training Epoch: 29 [22016/50176]	Loss: 0.9537
Training Epoch: 29 [22272/50176]	Loss: 0.9572
Training Epoch: 29 [22528/50176]	Loss: 0.8676
Training Epoch: 29 [22784/50176]	Loss: 1.0412
Training Epoch: 29 [23040/50176]	Loss: 0.8737
Training Epoch: 29 [23296/50176]	Loss: 0.9499
Training Epoch: 29 [23552/50176]	Loss: 0.8499
Training Epoch: 29 [23808/50176]	Loss: 0.8597
Training Epoch: 29 [24064/50176]	Loss: 0.9879
Training Epoch: 29 [24320/50176]	Loss: 1.0614
Training Epoch: 29 [24576/50176]	Loss: 1.0070
Training Epoch: 29 [24832/50176]	Loss: 0.8564
Training Epoch: 29 [25088/50176]	Loss: 1.0499
Training Epoch: 29 [25344/50176]	Loss: 0.8878
Training Epoch: 29 [25600/50176]	Loss: 0.7681
Training Epoch: 29 [25856/50176]	Loss: 0.9279
Training Epoch: 29 [26112/50176]	Loss: 0.8729
Training Epoch: 29 [26368/50176]	Loss: 0.9287
Training Epoch: 29 [26624/50176]	Loss: 0.9487
Training Epoch: 29 [26880/50176]	Loss: 0.9536
Training Epoch: 29 [27136/50176]	Loss: 0.9933
Training Epoch: 29 [27392/50176]	Loss: 0.9134
Training Epoch: 29 [27648/50176]	Loss: 0.8770
Training Epoch: 29 [27904/50176]	Loss: 0.9141
Training Epoch: 29 [28160/50176]	Loss: 0.9551
Training Epoch: 29 [28416/50176]	Loss: 0.9233
Training Epoch: 29 [28672/50176]	Loss: 0.9664
Training Epoch: 29 [28928/50176]	Loss: 1.0552
Training Epoch: 29 [29184/50176]	Loss: 0.8526
Training Epoch: 29 [29440/50176]	Loss: 0.7150
Training Epoch: 29 [29696/50176]	Loss: 0.8620
Training Epoch: 29 [29952/50176]	Loss: 0.9633
Training Epoch: 29 [30208/50176]	Loss: 0.8936
Training Epoch: 29 [30464/50176]	Loss: 0.9598
Training Epoch: 29 [30720/50176]	Loss: 0.9236
Training Epoch: 29 [30976/50176]	Loss: 1.0821
Training Epoch: 29 [31232/50176]	Loss: 0.8768
Training Epoch: 29 [31488/50176]	Loss: 0.9914
Training Epoch: 29 [31744/50176]	Loss: 1.0346
Training Epoch: 29 [32000/50176]	Loss: 0.8606
Training Epoch: 29 [32256/50176]	Loss: 0.9780
Training Epoch: 29 [32512/50176]	Loss: 1.0102
Training Epoch: 29 [32768/50176]	Loss: 1.0599
Training Epoch: 29 [33024/50176]	Loss: 0.9368
Training Epoch: 29 [33280/50176]	Loss: 0.8367
Training Epoch: 29 [33536/50176]	Loss: 0.8894
Training Epoch: 29 [33792/50176]	Loss: 0.9205
Training Epoch: 29 [34048/50176]	Loss: 0.8317
Training Epoch: 29 [34304/50176]	Loss: 0.8630
Training Epoch: 29 [34560/50176]	Loss: 0.9355
Training Epoch: 29 [34816/50176]	Loss: 0.8863
Training Epoch: 29 [35072/50176]	Loss: 0.9331
Training Epoch: 29 [35328/50176]	Loss: 0.8686
Training Epoch: 29 [35584/50176]	Loss: 0.8549
Training Epoch: 29 [35840/50176]	Loss: 0.9844
Training Epoch: 29 [36096/50176]	Loss: 0.9298
Training Epoch: 29 [36352/50176]	Loss: 1.0044
Training Epoch: 29 [36608/50176]	Loss: 0.9033
Training Epoch: 29 [36864/50176]	Loss: 1.0557
Training Epoch: 29 [37120/50176]	Loss: 0.8139
Training Epoch: 29 [37376/50176]	Loss: 1.1221
Training Epoch: 29 [37632/50176]	Loss: 0.9901
Training Epoch: 29 [37888/50176]	Loss: 1.0064
Training Epoch: 29 [38144/50176]	Loss: 1.0365
Training Epoch: 29 [38400/50176]	Loss: 0.8430
Training Epoch: 29 [38656/50176]	Loss: 1.0724
Training Epoch: 29 [38912/50176]	Loss: 0.9364
Training Epoch: 29 [39168/50176]	Loss: 1.0188
Training Epoch: 29 [39424/50176]	Loss: 0.9385
Training Epoch: 29 [39680/50176]	Loss: 1.0394
Training Epoch: 29 [39936/50176]	Loss: 1.0726
Training Epoch: 29 [40192/50176]	Loss: 0.8885
Training Epoch: 29 [40448/50176]	Loss: 0.9144
Training Epoch: 29 [40704/50176]	Loss: 1.1684
Training Epoch: 29 [40960/50176]	Loss: 0.8793
Training Epoch: 29 [41216/50176]	Loss: 0.9880
Training Epoch: 29 [41472/50176]	Loss: 1.0916
Training Epoch: 29 [41728/50176]	Loss: 1.0093
Training Epoch: 29 [41984/50176]	Loss: 0.8099
Training Epoch: 29 [42240/50176]	Loss: 1.1080
Training Epoch: 29 [42496/50176]	Loss: 0.9333
Training Epoch: 29 [42752/50176]	Loss: 0.7267
Training Epoch: 29 [43008/50176]	Loss: 0.8940
Training Epoch: 29 [43264/50176]	Loss: 0.9692
Training Epoch: 29 [43520/50176]	Loss: 1.0390
Training Epoch: 29 [43776/50176]	Loss: 1.0495
Training Epoch: 29 [44032/50176]	Loss: 1.0619
Training Epoch: 29 [44288/50176]	Loss: 1.0601
Training Epoch: 29 [44544/50176]	Loss: 1.0053
Training Epoch: 29 [44800/50176]	Loss: 0.8349
Training Epoch: 29 [45056/50176]	Loss: 0.9904
Training Epoch: 29 [45312/50176]	Loss: 0.9571
Training Epoch: 29 [45568/50176]	Loss: 1.1384
Training Epoch: 29 [45824/50176]	Loss: 1.0057
Training Epoch: 29 [46080/50176]	Loss: 0.8964
Training Epoch: 29 [46336/50176]	Loss: 1.0046
Training Epoch: 29 [46592/50176]	Loss: 0.8798
Training Epoch: 29 [46848/50176]	Loss: 0.8866
Training Epoch: 29 [47104/50176]	Loss: 0.9274
Training Epoch: 29 [47360/50176]	Loss: 1.2221
Training Epoch: 29 [47616/50176]	Loss: 0.9252
Training Epoch: 29 [47872/50176]	Loss: 1.0419
Training Epoch: 29 [48128/50176]	Loss: 1.0002
Training Epoch: 29 [48384/50176]	Loss: 1.0239
Training Epoch: 29 [48640/50176]	Loss: 1.0650
Training Epoch: 29 [48896/50176]	Loss: 0.9174
Training Epoch: 29 [49152/50176]	Loss: 0.9995
Training Epoch: 29 [49408/50176]	Loss: 1.0683
Training Epoch: 29 [49664/50176]	Loss: 0.8616
Training Epoch: 29 [49920/50176]	Loss: 1.0538
Training Epoch: 29 [50176/50176]	Loss: 1.1937
Validation Epoch: 29, Average loss: 0.0089, Accuracy: 0.4859
Training Epoch: 30 [256/50176]	Loss: 0.7822
Training Epoch: 30 [512/50176]	Loss: 0.9102
Training Epoch: 30 [768/50176]	Loss: 0.7697
Training Epoch: 30 [1024/50176]	Loss: 0.7898
Training Epoch: 30 [1280/50176]	Loss: 0.8940
Training Epoch: 30 [1536/50176]	Loss: 1.1007
Training Epoch: 30 [1792/50176]	Loss: 0.7777
Training Epoch: 30 [2048/50176]	Loss: 0.9889
Training Epoch: 30 [2304/50176]	Loss: 0.8662
Training Epoch: 30 [2560/50176]	Loss: 0.7987
Training Epoch: 30 [2816/50176]	Loss: 0.8515
Training Epoch: 30 [3072/50176]	Loss: 0.7540
Training Epoch: 30 [3328/50176]	Loss: 0.8634
Training Epoch: 30 [3584/50176]	Loss: 0.9779
Training Epoch: 30 [3840/50176]	Loss: 0.8203
Training Epoch: 30 [4096/50176]	Loss: 0.9522
Training Epoch: 30 [4352/50176]	Loss: 0.9054
Training Epoch: 30 [4608/50176]	Loss: 0.9014
Training Epoch: 30 [4864/50176]	Loss: 0.9520
Training Epoch: 30 [5120/50176]	Loss: 0.8938
Training Epoch: 30 [5376/50176]	Loss: 0.8221
Training Epoch: 30 [5632/50176]	Loss: 0.8988
Training Epoch: 30 [5888/50176]	Loss: 0.7912
Training Epoch: 30 [6144/50176]	Loss: 0.9191
Training Epoch: 30 [6400/50176]	Loss: 0.8281
Training Epoch: 30 [6656/50176]	Loss: 0.9303
Training Epoch: 30 [6912/50176]	Loss: 0.7731
Training Epoch: 30 [7168/50176]	Loss: 0.8784
Training Epoch: 30 [7424/50176]	Loss: 0.8507
Training Epoch: 30 [7680/50176]	Loss: 0.7925
Training Epoch: 30 [7936/50176]	Loss: 0.9029
Training Epoch: 30 [8192/50176]	Loss: 1.0211
Training Epoch: 30 [8448/50176]	Loss: 0.9782
Training Epoch: 30 [8704/50176]	Loss: 0.8587
Training Epoch: 30 [8960/50176]	Loss: 0.7542
Training Epoch: 30 [9216/50176]	Loss: 0.8057
Training Epoch: 30 [9472/50176]	Loss: 0.8446
Training Epoch: 30 [9728/50176]	Loss: 0.8658
Training Epoch: 30 [9984/50176]	Loss: 0.8674
Training Epoch: 30 [10240/50176]	Loss: 0.8707
Training Epoch: 30 [10496/50176]	Loss: 0.9323
Training Epoch: 30 [10752/50176]	Loss: 0.9004
Training Epoch: 30 [11008/50176]	Loss: 0.8855
Training Epoch: 30 [11264/50176]	Loss: 0.7906
Training Epoch: 30 [11520/50176]	Loss: 0.8574
Training Epoch: 30 [11776/50176]	Loss: 0.7632
Training Epoch: 30 [12032/50176]	Loss: 0.9876
Training Epoch: 30 [12288/50176]	Loss: 0.8879
Training Epoch: 30 [12544/50176]	Loss: 0.8383
Training Epoch: 30 [12800/50176]	Loss: 0.7810
Training Epoch: 30 [13056/50176]	Loss: 0.8862
Training Epoch: 30 [13312/50176]	Loss: 0.8573
Training Epoch: 30 [13568/50176]	Loss: 1.0665
Training Epoch: 30 [13824/50176]	Loss: 0.7600
Training Epoch: 30 [14080/50176]	Loss: 0.8296
Training Epoch: 30 [14336/50176]	Loss: 0.9963
Training Epoch: 30 [14592/50176]	Loss: 0.8876
Training Epoch: 30 [14848/50176]	Loss: 0.8196
Training Epoch: 30 [15104/50176]	Loss: 0.9514
Training Epoch: 30 [15360/50176]	Loss: 0.8734
Training Epoch: 30 [15616/50176]	Loss: 0.8235
Training Epoch: 30 [15872/50176]	Loss: 0.8232
Training Epoch: 30 [16128/50176]	Loss: 0.7981
Training Epoch: 30 [16384/50176]	Loss: 1.1583
Training Epoch: 30 [16640/50176]	Loss: 0.9305
Training Epoch: 30 [16896/50176]	Loss: 0.8694
Training Epoch: 30 [17152/50176]	Loss: 0.8961
Training Epoch: 30 [17408/50176]	Loss: 0.9145
Training Epoch: 30 [17664/50176]	Loss: 0.9005
Training Epoch: 30 [17920/50176]	Loss: 1.0036
Training Epoch: 30 [18176/50176]	Loss: 0.8347
Training Epoch: 30 [18432/50176]	Loss: 0.9144
Training Epoch: 30 [18688/50176]	Loss: 0.9351
Training Epoch: 30 [18944/50176]	Loss: 0.9786
Training Epoch: 30 [19200/50176]	Loss: 0.8948
Training Epoch: 30 [19456/50176]	Loss: 0.8680
Training Epoch: 30 [19712/50176]	Loss: 0.7459
Training Epoch: 30 [19968/50176]	Loss: 0.8580
Training Epoch: 30 [20224/50176]	Loss: 0.9623
Training Epoch: 30 [20480/50176]	Loss: 0.7480
Training Epoch: 30 [20736/50176]	Loss: 1.0571
Training Epoch: 30 [20992/50176]	Loss: 1.0159
Training Epoch: 30 [21248/50176]	Loss: 0.7407
Training Epoch: 30 [21504/50176]	Loss: 0.9528
Training Epoch: 30 [21760/50176]	Loss: 0.8955
Training Epoch: 30 [22016/50176]	Loss: 0.9390
Training Epoch: 30 [22272/50176]	Loss: 0.8278
Training Epoch: 30 [22528/50176]	Loss: 0.8787
Training Epoch: 30 [22784/50176]	Loss: 0.8772
Training Epoch: 30 [23040/50176]	Loss: 0.9136
Training Epoch: 30 [23296/50176]	Loss: 0.8446
Training Epoch: 30 [23552/50176]	Loss: 0.8836
Training Epoch: 30 [23808/50176]	Loss: 0.8271
Training Epoch: 30 [24064/50176]	Loss: 0.8111
Training Epoch: 30 [24320/50176]	Loss: 0.8043
Training Epoch: 30 [24576/50176]	Loss: 0.7916
Training Epoch: 30 [24832/50176]	Loss: 0.8245
Training Epoch: 30 [25088/50176]	Loss: 0.7259
Training Epoch: 30 [25344/50176]	Loss: 0.8388
Training Epoch: 30 [25600/50176]	Loss: 0.7245
Training Epoch: 30 [25856/50176]	Loss: 1.0166
Training Epoch: 30 [26112/50176]	Loss: 1.0537
Training Epoch: 30 [26368/50176]	Loss: 0.7177
Training Epoch: 30 [26624/50176]	Loss: 0.7276
Training Epoch: 30 [26880/50176]	Loss: 0.9504
Training Epoch: 30 [27136/50176]	Loss: 0.9377
Training Epoch: 30 [27392/50176]	Loss: 0.7699
Training Epoch: 30 [27648/50176]	Loss: 0.8471
Training Epoch: 30 [27904/50176]	Loss: 0.9374
Training Epoch: 30 [28160/50176]	Loss: 1.0669
Training Epoch: 30 [28416/50176]	Loss: 0.9690
Training Epoch: 30 [28672/50176]	Loss: 1.1522
Training Epoch: 30 [28928/50176]	Loss: 0.9406
Training Epoch: 30 [29184/50176]	Loss: 0.9459
Training Epoch: 30 [29440/50176]	Loss: 1.0591
Training Epoch: 30 [29696/50176]	Loss: 0.9836
Training Epoch: 30 [29952/50176]	Loss: 0.9941
Training Epoch: 30 [30208/50176]	Loss: 0.9924
Training Epoch: 30 [30464/50176]	Loss: 0.7821
Training Epoch: 30 [30720/50176]	Loss: 0.8708
Training Epoch: 30 [30976/50176]	Loss: 1.0955
Training Epoch: 30 [31232/50176]	Loss: 0.9435
Training Epoch: 30 [31488/50176]	Loss: 0.8547
Training Epoch: 30 [31744/50176]	Loss: 0.7056
Training Epoch: 30 [32000/50176]	Loss: 0.9733
Training Epoch: 30 [32256/50176]	Loss: 1.0350
Training Epoch: 30 [32512/50176]	Loss: 0.9473
Training Epoch: 30 [32768/50176]	Loss: 1.1461
Training Epoch: 30 [33024/50176]	Loss: 0.9575
Training Epoch: 30 [33280/50176]	Loss: 0.9183
Training Epoch: 30 [33536/50176]	Loss: 0.9953
Training Epoch: 30 [33792/50176]	Loss: 0.9529
Training Epoch: 30 [34048/50176]	Loss: 0.9739
Training Epoch: 30 [34304/50176]	Loss: 0.9850
Training Epoch: 30 [34560/50176]	Loss: 0.7404
Training Epoch: 30 [34816/50176]	Loss: 0.9539
Training Epoch: 30 [35072/50176]	Loss: 0.8456
Training Epoch: 30 [35328/50176]	Loss: 0.9595
Training Epoch: 30 [35584/50176]	Loss: 1.0587
Training Epoch: 30 [35840/50176]	Loss: 0.7955
Training Epoch: 30 [36096/50176]	Loss: 0.9817
Training Epoch: 30 [36352/50176]	Loss: 0.9729
Training Epoch: 30 [36608/50176]	Loss: 0.8313
Training Epoch: 30 [36864/50176]	Loss: 1.0422
Training Epoch: 30 [37120/50176]	Loss: 0.8600
Training Epoch: 30 [37376/50176]	Loss: 0.9145
Training Epoch: 30 [37632/50176]	Loss: 0.9581
Training Epoch: 30 [37888/50176]	Loss: 0.8609
Training Epoch: 30 [38144/50176]	Loss: 1.1686
Training Epoch: 30 [38400/50176]	Loss: 1.0503
Training Epoch: 30 [38656/50176]	Loss: 0.9606
Training Epoch: 30 [38912/50176]	Loss: 0.9498
Training Epoch: 30 [39168/50176]	Loss: 0.8726
Training Epoch: 30 [39424/50176]	Loss: 0.8422
Training Epoch: 30 [39680/50176]	Loss: 1.0692
Training Epoch: 30 [39936/50176]	Loss: 1.0342
Training Epoch: 30 [40192/50176]	Loss: 1.0989
Training Epoch: 30 [40448/50176]	Loss: 0.8625
Training Epoch: 30 [40704/50176]	Loss: 0.9386
Training Epoch: 30 [40960/50176]	Loss: 0.8284
Training Epoch: 30 [41216/50176]	Loss: 1.0346
Training Epoch: 30 [41472/50176]	Loss: 1.0609
Training Epoch: 30 [41728/50176]	Loss: 1.0764
Training Epoch: 30 [41984/50176]	Loss: 1.1572
Training Epoch: 30 [42240/50176]	Loss: 0.9961
Training Epoch: 30 [42496/50176]	Loss: 1.0301
Training Epoch: 30 [42752/50176]	Loss: 1.1174
Training Epoch: 30 [43008/50176]	Loss: 0.8256
Training Epoch: 30 [43264/50176]	Loss: 1.0409
Training Epoch: 30 [43520/50176]	Loss: 0.8724
Training Epoch: 30 [43776/50176]	Loss: 1.0807
Training Epoch: 30 [44032/50176]	Loss: 0.9024
Training Epoch: 30 [44288/50176]	Loss: 1.0587
Training Epoch: 30 [44544/50176]	Loss: 0.9021
Training Epoch: 30 [44800/50176]	Loss: 0.9111
Training Epoch: 30 [45056/50176]	Loss: 0.8537
Training Epoch: 30 [45312/50176]	Loss: 0.9988
Training Epoch: 30 [45568/50176]	Loss: 0.9412
Training Epoch: 30 [45824/50176]	Loss: 0.8208
Training Epoch: 30 [46080/50176]	Loss: 0.8690
Training Epoch: 30 [46336/50176]	Loss: 0.9261
Training Epoch: 30 [46592/50176]	Loss: 0.9928
Training Epoch: 30 [46848/50176]	Loss: 0.8866
Training Epoch: 30 [47104/50176]	Loss: 1.1510
Training Epoch: 30 [47360/50176]	Loss: 0.7930
Training Epoch: 30 [47616/50176]	Loss: 0.9416
Training Epoch: 30 [47872/50176]	Loss: 0.9744
Training Epoch: 30 [48128/50176]	Loss: 0.9280
Training Epoch: 30 [48384/50176]	Loss: 0.9737
Training Epoch: 30 [48640/50176]	Loss: 1.0439
Training Epoch: 30 [48896/50176]	Loss: 0.9265
Training Epoch: 30 [49152/50176]	Loss: 1.0366
Training Epoch: 30 [49408/50176]	Loss: 1.0161
Training Epoch: 30 [49664/50176]	Loss: 1.0799
Training Epoch: 30 [49920/50176]	Loss: 1.0084
Training Epoch: 30 [50176/50176]	Loss: 1.1554
Validation Epoch: 30, Average loss: 0.0086, Accuracy: 0.5097
Training Epoch: 31 [256/50176]	Loss: 0.8833
Training Epoch: 31 [512/50176]	Loss: 0.7747
Training Epoch: 31 [768/50176]	Loss: 0.7038
Training Epoch: 31 [1024/50176]	Loss: 0.8518
Training Epoch: 31 [1280/50176]	Loss: 0.9178
Training Epoch: 31 [1536/50176]	Loss: 0.7850
Training Epoch: 31 [1792/50176]	Loss: 0.9138
Training Epoch: 31 [2048/50176]	Loss: 0.8870
Training Epoch: 31 [2304/50176]	Loss: 0.8412
Training Epoch: 31 [2560/50176]	Loss: 0.9267
Training Epoch: 31 [2816/50176]	Loss: 1.0533
Training Epoch: 31 [3072/50176]	Loss: 0.7934
Training Epoch: 31 [3328/50176]	Loss: 0.7146
Training Epoch: 31 [3584/50176]	Loss: 0.8043
Training Epoch: 31 [3840/50176]	Loss: 0.9011
Training Epoch: 31 [4096/50176]	Loss: 0.7511
Training Epoch: 31 [4352/50176]	Loss: 0.8384
Training Epoch: 31 [4608/50176]	Loss: 0.8586
Training Epoch: 31 [4864/50176]	Loss: 0.8299
Training Epoch: 31 [5120/50176]	Loss: 0.9171
Training Epoch: 31 [5376/50176]	Loss: 0.9041
Training Epoch: 31 [5632/50176]	Loss: 0.8597
Training Epoch: 31 [5888/50176]	Loss: 0.7817
Training Epoch: 31 [6144/50176]	Loss: 0.9126
Training Epoch: 31 [6400/50176]	Loss: 0.7446
Training Epoch: 31 [6656/50176]	Loss: 0.7598
Training Epoch: 31 [6912/50176]	Loss: 0.8322
Training Epoch: 31 [7168/50176]	Loss: 0.9552
Training Epoch: 31 [7424/50176]	Loss: 0.7952
Training Epoch: 31 [7680/50176]	Loss: 0.8435
Training Epoch: 31 [7936/50176]	Loss: 0.9458
Training Epoch: 31 [8192/50176]	Loss: 0.7853
Training Epoch: 31 [8448/50176]	Loss: 0.9330
Training Epoch: 31 [8704/50176]	Loss: 0.8126
Training Epoch: 31 [8960/50176]	Loss: 0.7181
Training Epoch: 31 [9216/50176]	Loss: 0.7378
Training Epoch: 31 [9472/50176]	Loss: 0.8471
Training Epoch: 31 [9728/50176]	Loss: 0.7986
Training Epoch: 31 [9984/50176]	Loss: 0.9082
Training Epoch: 31 [10240/50176]	Loss: 0.8757
Training Epoch: 31 [10496/50176]	Loss: 0.8874
Training Epoch: 31 [10752/50176]	Loss: 0.8267
Training Epoch: 31 [11008/50176]	Loss: 0.8522
Training Epoch: 31 [11264/50176]	Loss: 0.8570
Training Epoch: 31 [11520/50176]	Loss: 0.8630
Training Epoch: 31 [11776/50176]	Loss: 0.7990
Training Epoch: 31 [12032/50176]	Loss: 0.8287
Training Epoch: 31 [12288/50176]	Loss: 0.9800
Training Epoch: 31 [12544/50176]	Loss: 0.7851
Training Epoch: 31 [12800/50176]	Loss: 0.8908
Training Epoch: 31 [13056/50176]	Loss: 0.9694
Training Epoch: 31 [13312/50176]	Loss: 0.8446
Training Epoch: 31 [13568/50176]	Loss: 0.8878
Training Epoch: 31 [13824/50176]	Loss: 0.8740
Training Epoch: 31 [14080/50176]	Loss: 0.8214
Training Epoch: 31 [14336/50176]	Loss: 0.8365
Training Epoch: 31 [14592/50176]	Loss: 0.8230
Training Epoch: 31 [14848/50176]	Loss: 0.8334
Training Epoch: 31 [15104/50176]	Loss: 0.9171
Training Epoch: 31 [15360/50176]	Loss: 0.8786
Training Epoch: 31 [15616/50176]	Loss: 0.9613
Training Epoch: 31 [15872/50176]	Loss: 0.9342
Training Epoch: 31 [16128/50176]	Loss: 1.0605
Training Epoch: 31 [16384/50176]	Loss: 0.6560
Training Epoch: 31 [16640/50176]	Loss: 0.7896
Training Epoch: 31 [16896/50176]	Loss: 0.8903
Training Epoch: 31 [17152/50176]	Loss: 0.7383
Training Epoch: 31 [17408/50176]	Loss: 1.0741
Training Epoch: 31 [17664/50176]	Loss: 0.8397
Training Epoch: 31 [17920/50176]	Loss: 0.7680
Training Epoch: 31 [18176/50176]	Loss: 0.8448
Training Epoch: 31 [18432/50176]	Loss: 0.9804
Training Epoch: 31 [18688/50176]	Loss: 0.8276
Training Epoch: 31 [18944/50176]	Loss: 0.8545
Training Epoch: 31 [19200/50176]	Loss: 0.9242
Training Epoch: 31 [19456/50176]	Loss: 0.9871
Training Epoch: 31 [19712/50176]	Loss: 1.0406
Training Epoch: 31 [19968/50176]	Loss: 0.8710
Training Epoch: 31 [20224/50176]	Loss: 0.8911
Training Epoch: 31 [20480/50176]	Loss: 0.8785
Training Epoch: 31 [20736/50176]	Loss: 0.8557
Training Epoch: 31 [20992/50176]	Loss: 0.8349
Training Epoch: 31 [21248/50176]	Loss: 0.8219
Training Epoch: 31 [21504/50176]	Loss: 0.9580
Training Epoch: 31 [21760/50176]	Loss: 0.9286
Training Epoch: 31 [22016/50176]	Loss: 0.9347
Training Epoch: 31 [22272/50176]	Loss: 0.8235
Training Epoch: 31 [22528/50176]	Loss: 0.9885
Training Epoch: 31 [22784/50176]	Loss: 0.8550
Training Epoch: 31 [23040/50176]	Loss: 0.9405
Training Epoch: 31 [23296/50176]	Loss: 0.9752
Training Epoch: 31 [23552/50176]	Loss: 0.8597
Training Epoch: 31 [23808/50176]	Loss: 0.8529
Training Epoch: 31 [24064/50176]	Loss: 0.8212
Training Epoch: 31 [24320/50176]	Loss: 0.9633
Training Epoch: 31 [24576/50176]	Loss: 0.9498
Training Epoch: 31 [24832/50176]	Loss: 0.8441
Training Epoch: 31 [25088/50176]	Loss: 0.9030
Training Epoch: 31 [25344/50176]	Loss: 0.7737
Training Epoch: 31 [25600/50176]	Loss: 0.9357
Training Epoch: 31 [25856/50176]	Loss: 0.8435
Training Epoch: 31 [26112/50176]	Loss: 0.7468
Training Epoch: 31 [26368/50176]	Loss: 0.9751
Training Epoch: 31 [26624/50176]	Loss: 0.9644
Training Epoch: 31 [26880/50176]	Loss: 0.8442
Training Epoch: 31 [27136/50176]	Loss: 0.8362
Training Epoch: 31 [27392/50176]	Loss: 1.0413
Training Epoch: 31 [27648/50176]	Loss: 0.8035
Training Epoch: 31 [27904/50176]	Loss: 0.8978
Training Epoch: 31 [28160/50176]	Loss: 0.7732
Training Epoch: 31 [28416/50176]	Loss: 0.9007
Training Epoch: 31 [28672/50176]	Loss: 0.8439
Training Epoch: 31 [28928/50176]	Loss: 0.7823
Training Epoch: 31 [29184/50176]	Loss: 0.9587
Training Epoch: 31 [29440/50176]	Loss: 0.8668
Training Epoch: 31 [29696/50176]	Loss: 1.0306
Training Epoch: 31 [29952/50176]	Loss: 0.9453
Training Epoch: 31 [30208/50176]	Loss: 0.9695
Training Epoch: 31 [30464/50176]	Loss: 0.8377
Training Epoch: 31 [30720/50176]	Loss: 0.9828
Training Epoch: 31 [30976/50176]	Loss: 0.8738
Training Epoch: 31 [31232/50176]	Loss: 0.9283
Training Epoch: 31 [31488/50176]	Loss: 0.8212
Training Epoch: 31 [31744/50176]	Loss: 1.1200
Training Epoch: 31 [32000/50176]	Loss: 0.9093
Training Epoch: 31 [32256/50176]	Loss: 0.8068
Training Epoch: 31 [32512/50176]	Loss: 0.8413
Training Epoch: 31 [32768/50176]	Loss: 0.7991
Training Epoch: 31 [33024/50176]	Loss: 0.9562
Training Epoch: 31 [33280/50176]	Loss: 0.9403
Training Epoch: 31 [33536/50176]	Loss: 1.0696
Training Epoch: 31 [33792/50176]	Loss: 0.8495
Training Epoch: 31 [34048/50176]	Loss: 0.8193
Training Epoch: 31 [34304/50176]	Loss: 0.7878
Training Epoch: 31 [34560/50176]	Loss: 1.0141
Training Epoch: 31 [34816/50176]	Loss: 1.0074
Training Epoch: 31 [35072/50176]	Loss: 0.8684
Training Epoch: 31 [35328/50176]	Loss: 0.8327
Training Epoch: 31 [35584/50176]	Loss: 0.7504
Training Epoch: 31 [35840/50176]	Loss: 0.8597
Training Epoch: 31 [36096/50176]	Loss: 1.0616
Training Epoch: 31 [36352/50176]	Loss: 0.8050
Training Epoch: 31 [36608/50176]	Loss: 1.0041
Training Epoch: 31 [36864/50176]	Loss: 0.8946
Training Epoch: 31 [37120/50176]	Loss: 0.8466
Training Epoch: 31 [37376/50176]	Loss: 0.9908
Training Epoch: 31 [37632/50176]	Loss: 1.0355
Training Epoch: 31 [37888/50176]	Loss: 0.8515
Training Epoch: 31 [38144/50176]	Loss: 0.9603
Training Epoch: 31 [38400/50176]	Loss: 0.9141
Training Epoch: 31 [38656/50176]	Loss: 0.9648
Training Epoch: 31 [38912/50176]	Loss: 0.9107
Training Epoch: 31 [39168/50176]	Loss: 0.9098
Training Epoch: 31 [39424/50176]	Loss: 0.8544
Training Epoch: 31 [39680/50176]	Loss: 0.7816
Training Epoch: 31 [39936/50176]	Loss: 0.9780
Training Epoch: 31 [40192/50176]	Loss: 0.9061
Training Epoch: 31 [40448/50176]	Loss: 0.8932
Training Epoch: 31 [40704/50176]	Loss: 0.9172
Training Epoch: 31 [40960/50176]	Loss: 0.9254
Training Epoch: 31 [41216/50176]	Loss: 0.9998
Training Epoch: 31 [41472/50176]	Loss: 1.0867
Training Epoch: 31 [41728/50176]	Loss: 0.8804
Training Epoch: 31 [41984/50176]	Loss: 0.9750
Training Epoch: 31 [42240/50176]	Loss: 0.9494
Training Epoch: 31 [42496/50176]	Loss: 0.8360
Training Epoch: 31 [42752/50176]	Loss: 0.7906
Training Epoch: 31 [43008/50176]	Loss: 0.9793
Training Epoch: 31 [43264/50176]	Loss: 0.9276
Training Epoch: 31 [43520/50176]	Loss: 0.8770
Training Epoch: 31 [43776/50176]	Loss: 0.9550
Training Epoch: 31 [44032/50176]	Loss: 1.0166
Training Epoch: 31 [44288/50176]	Loss: 0.9137
Training Epoch: 31 [44544/50176]	Loss: 0.9404
Training Epoch: 31 [44800/50176]	Loss: 1.0150
Training Epoch: 31 [45056/50176]	Loss: 0.9499
Training Epoch: 31 [45312/50176]	Loss: 0.9005
Training Epoch: 31 [45568/50176]	Loss: 0.9266
Training Epoch: 31 [45824/50176]	Loss: 0.7915
Training Epoch: 31 [46080/50176]	Loss: 0.9520
Training Epoch: 31 [46336/50176]	Loss: 0.8653
Training Epoch: 31 [46592/50176]	Loss: 0.9718
Training Epoch: 31 [46848/50176]	Loss: 1.0264
Training Epoch: 31 [47104/50176]	Loss: 0.9666
Training Epoch: 31 [47360/50176]	Loss: 0.9445
Training Epoch: 31 [47616/50176]	Loss: 1.0824
Training Epoch: 31 [47872/50176]	Loss: 0.8995
Training Epoch: 31 [48128/50176]	Loss: 0.8675
Training Epoch: 31 [48384/50176]	Loss: 0.8944
Training Epoch: 31 [48640/50176]	Loss: 0.8989
Training Epoch: 31 [48896/50176]	Loss: 0.9422
Training Epoch: 31 [49152/50176]	Loss: 1.1027
Training Epoch: 31 [49408/50176]	Loss: 1.0189
Training Epoch: 31 [49664/50176]	Loss: 0.9611
Training Epoch: 31 [49920/50176]	Loss: 0.8444
Training Epoch: 31 [50176/50176]	Loss: 0.7723
Validation Epoch: 31, Average loss: 0.0083, Accuracy: 0.5110
Training Epoch: 32 [256/50176]	Loss: 0.7362
Training Epoch: 32 [512/50176]	Loss: 0.8355
Training Epoch: 32 [768/50176]	Loss: 0.8031
Training Epoch: 32 [1024/50176]	Loss: 0.8322
Training Epoch: 32 [1280/50176]	Loss: 0.9368
Training Epoch: 32 [1536/50176]	Loss: 0.8747
Training Epoch: 32 [1792/50176]	Loss: 0.7630
Training Epoch: 32 [2048/50176]	Loss: 0.8547
Training Epoch: 32 [2304/50176]	Loss: 0.8153
Training Epoch: 32 [2560/50176]	Loss: 0.8354
Training Epoch: 32 [2816/50176]	Loss: 0.8520
Training Epoch: 32 [3072/50176]	Loss: 0.8285
Training Epoch: 32 [3328/50176]	Loss: 0.8770
Training Epoch: 32 [3584/50176]	Loss: 0.7394
Training Epoch: 32 [3840/50176]	Loss: 0.8684
Training Epoch: 32 [4096/50176]	Loss: 0.8620
Training Epoch: 32 [4352/50176]	Loss: 0.6353
Training Epoch: 32 [4608/50176]	Loss: 0.8022
Training Epoch: 32 [4864/50176]	Loss: 0.7633
Training Epoch: 32 [5120/50176]	Loss: 0.7096
Training Epoch: 32 [5376/50176]	Loss: 0.8551
Training Epoch: 32 [5632/50176]	Loss: 0.8177
Training Epoch: 32 [5888/50176]	Loss: 0.8052
Training Epoch: 32 [6144/50176]	Loss: 0.9095
Training Epoch: 32 [6400/50176]	Loss: 0.8388
Training Epoch: 32 [6656/50176]	Loss: 0.9532
Training Epoch: 32 [6912/50176]	Loss: 0.8201
Training Epoch: 32 [7168/50176]	Loss: 0.9571
Training Epoch: 32 [7424/50176]	Loss: 0.7383
Training Epoch: 32 [7680/50176]	Loss: 1.0056
Training Epoch: 32 [7936/50176]	Loss: 0.7990
Training Epoch: 32 [8192/50176]	Loss: 0.8020
Training Epoch: 32 [8448/50176]	Loss: 0.8058
Training Epoch: 32 [8704/50176]	Loss: 0.7392
Training Epoch: 32 [8960/50176]	Loss: 0.7454
Training Epoch: 32 [9216/50176]	Loss: 0.7619
Training Epoch: 32 [9472/50176]	Loss: 0.7876
Training Epoch: 32 [9728/50176]	Loss: 0.7886
Training Epoch: 32 [9984/50176]	Loss: 0.8603
Training Epoch: 32 [10240/50176]	Loss: 0.9744
Training Epoch: 32 [10496/50176]	Loss: 0.8845
Training Epoch: 32 [10752/50176]	Loss: 0.7608
Training Epoch: 32 [11008/50176]	Loss: 0.7057
Training Epoch: 32 [11264/50176]	Loss: 0.6534
Training Epoch: 32 [11520/50176]	Loss: 0.8077
Training Epoch: 32 [11776/50176]	Loss: 0.8432
Training Epoch: 32 [12032/50176]	Loss: 0.9619
Training Epoch: 32 [12288/50176]	Loss: 0.7027
Training Epoch: 32 [12544/50176]	Loss: 0.8427
Training Epoch: 32 [12800/50176]	Loss: 0.8133
Training Epoch: 32 [13056/50176]	Loss: 0.6810
Training Epoch: 32 [13312/50176]	Loss: 0.9569
Training Epoch: 32 [13568/50176]	Loss: 0.8479
Training Epoch: 32 [13824/50176]	Loss: 0.9194
Training Epoch: 32 [14080/50176]	Loss: 0.9179
Training Epoch: 32 [14336/50176]	Loss: 0.8237
Training Epoch: 32 [14592/50176]	Loss: 0.9002
Training Epoch: 32 [14848/50176]	Loss: 0.8981
Training Epoch: 32 [15104/50176]	Loss: 0.8835
Training Epoch: 32 [15360/50176]	Loss: 0.8869
Training Epoch: 32 [15616/50176]	Loss: 0.9593
Training Epoch: 32 [15872/50176]	Loss: 0.8691
Training Epoch: 32 [16128/50176]	Loss: 0.9044
Training Epoch: 32 [16384/50176]	Loss: 0.8412
Training Epoch: 32 [16640/50176]	Loss: 0.8188
Training Epoch: 32 [16896/50176]	Loss: 0.8617
Training Epoch: 32 [17152/50176]	Loss: 1.0883
Training Epoch: 32 [17408/50176]	Loss: 0.8211
Training Epoch: 32 [17664/50176]	Loss: 0.8461
Training Epoch: 32 [17920/50176]	Loss: 0.7371
Training Epoch: 32 [18176/50176]	Loss: 0.8856
Training Epoch: 32 [18432/50176]	Loss: 0.8939
Training Epoch: 32 [18688/50176]	Loss: 0.9293
Training Epoch: 32 [18944/50176]	Loss: 0.8536
Training Epoch: 32 [19200/50176]	Loss: 0.8668
Training Epoch: 32 [19456/50176]	Loss: 0.6850
Training Epoch: 32 [19712/50176]	Loss: 0.8974
Training Epoch: 32 [19968/50176]	Loss: 0.9081
Training Epoch: 32 [20224/50176]	Loss: 0.8726
Training Epoch: 32 [20480/50176]	Loss: 0.8665
Training Epoch: 32 [20736/50176]	Loss: 0.7622
Training Epoch: 32 [20992/50176]	Loss: 0.9045
Training Epoch: 32 [21248/50176]	Loss: 0.9345
Training Epoch: 32 [21504/50176]	Loss: 0.8449
Training Epoch: 32 [21760/50176]	Loss: 0.7856
Training Epoch: 32 [22016/50176]	Loss: 1.0005
Training Epoch: 32 [22272/50176]	Loss: 0.8905
Training Epoch: 32 [22528/50176]	Loss: 0.8896
Training Epoch: 32 [22784/50176]	Loss: 0.7836
Training Epoch: 32 [23040/50176]	Loss: 0.7653
Training Epoch: 32 [23296/50176]	Loss: 0.8061
Training Epoch: 32 [23552/50176]	Loss: 0.8435
Training Epoch: 32 [23808/50176]	Loss: 0.8883
Training Epoch: 32 [24064/50176]	Loss: 0.8527
Training Epoch: 32 [24320/50176]	Loss: 0.9438
Training Epoch: 32 [24576/50176]	Loss: 0.8964
Training Epoch: 32 [24832/50176]	Loss: 0.8580
Training Epoch: 32 [25088/50176]	Loss: 1.0524
Training Epoch: 32 [25344/50176]	Loss: 0.9133
Training Epoch: 32 [25600/50176]	Loss: 0.8539
Training Epoch: 32 [25856/50176]	Loss: 0.9776
Training Epoch: 32 [26112/50176]	Loss: 0.8798
Training Epoch: 32 [26368/50176]	Loss: 0.9355
Training Epoch: 32 [26624/50176]	Loss: 0.8802
Training Epoch: 32 [26880/50176]	Loss: 0.8436
Training Epoch: 32 [27136/50176]	Loss: 0.8392
Training Epoch: 32 [27392/50176]	Loss: 0.7760
Training Epoch: 32 [27648/50176]	Loss: 0.7933
Training Epoch: 32 [27904/50176]	Loss: 0.9641
Training Epoch: 32 [28160/50176]	Loss: 0.8079
Training Epoch: 32 [28416/50176]	Loss: 1.0625
Training Epoch: 32 [28672/50176]	Loss: 0.8732
Training Epoch: 32 [28928/50176]	Loss: 0.8292
Training Epoch: 32 [29184/50176]	Loss: 0.8569
Training Epoch: 32 [29440/50176]	Loss: 0.7743
Training Epoch: 32 [29696/50176]	Loss: 0.9466
Training Epoch: 32 [29952/50176]	Loss: 0.8165
Training Epoch: 32 [30208/50176]	Loss: 0.7220
Training Epoch: 32 [30464/50176]	Loss: 0.8893
Training Epoch: 32 [30720/50176]	Loss: 0.9706
Training Epoch: 32 [30976/50176]	Loss: 0.9617
Training Epoch: 32 [31232/50176]	Loss: 0.8262
Training Epoch: 32 [31488/50176]	Loss: 0.7709
Training Epoch: 32 [31744/50176]	Loss: 0.9566
Training Epoch: 32 [32000/50176]	Loss: 0.9163
Training Epoch: 32 [32256/50176]	Loss: 0.8967
Training Epoch: 32 [32512/50176]	Loss: 0.8911
Training Epoch: 32 [32768/50176]	Loss: 0.8635
Training Epoch: 32 [33024/50176]	Loss: 0.9257
Training Epoch: 32 [33280/50176]	Loss: 0.8539
Training Epoch: 32 [33536/50176]	Loss: 0.9322
Training Epoch: 32 [33792/50176]	Loss: 0.7440
Training Epoch: 32 [34048/50176]	Loss: 0.7969
Training Epoch: 32 [34304/50176]	Loss: 0.6968
Training Epoch: 32 [34560/50176]	Loss: 0.7559
Training Epoch: 32 [34816/50176]	Loss: 0.8196
Training Epoch: 32 [35072/50176]	Loss: 0.8875
Training Epoch: 32 [35328/50176]	Loss: 0.8728
Training Epoch: 32 [35584/50176]	Loss: 0.8939
Training Epoch: 32 [35840/50176]	Loss: 0.9303
Training Epoch: 32 [36096/50176]	Loss: 0.7864
Training Epoch: 32 [36352/50176]	Loss: 0.7538
Training Epoch: 32 [36608/50176]	Loss: 0.9113
Training Epoch: 32 [36864/50176]	Loss: 0.8082
Training Epoch: 32 [37120/50176]	Loss: 0.9614
Training Epoch: 32 [37376/50176]	Loss: 0.9789
Training Epoch: 32 [37632/50176]	Loss: 0.8838
Training Epoch: 32 [37888/50176]	Loss: 0.8445
Training Epoch: 32 [38144/50176]	Loss: 0.8287
Training Epoch: 32 [38400/50176]	Loss: 0.8685
Training Epoch: 32 [38656/50176]	Loss: 0.8996
Training Epoch: 32 [38912/50176]	Loss: 0.8401
Training Epoch: 32 [39168/50176]	Loss: 0.9720
Training Epoch: 32 [39424/50176]	Loss: 0.9879
Training Epoch: 32 [39680/50176]	Loss: 0.7352
Training Epoch: 32 [39936/50176]	Loss: 0.9423
Training Epoch: 32 [40192/50176]	Loss: 0.8360
Training Epoch: 32 [40448/50176]	Loss: 0.9338
Training Epoch: 32 [40704/50176]	Loss: 0.8380
Training Epoch: 32 [40960/50176]	Loss: 1.0033
Training Epoch: 32 [41216/50176]	Loss: 0.9662
Training Epoch: 32 [41472/50176]	Loss: 0.8472
Training Epoch: 32 [41728/50176]	Loss: 0.9018
Training Epoch: 32 [41984/50176]	Loss: 0.8169
Training Epoch: 32 [42240/50176]	Loss: 0.9761
Training Epoch: 32 [42496/50176]	Loss: 1.0659
Training Epoch: 32 [42752/50176]	Loss: 0.8261
Training Epoch: 32 [43008/50176]	Loss: 0.9258
Training Epoch: 32 [43264/50176]	Loss: 0.8519
Training Epoch: 32 [43520/50176]	Loss: 0.8848
Training Epoch: 32 [43776/50176]	Loss: 0.8660
Training Epoch: 32 [44032/50176]	Loss: 0.9345
Training Epoch: 32 [44288/50176]	Loss: 0.9528
Training Epoch: 32 [44544/50176]	Loss: 0.9544
Training Epoch: 32 [44800/50176]	Loss: 1.0062
Training Epoch: 32 [45056/50176]	Loss: 0.9444
Training Epoch: 32 [45312/50176]	Loss: 0.7553
Training Epoch: 32 [45568/50176]	Loss: 0.8333
Training Epoch: 32 [45824/50176]	Loss: 0.9225
Training Epoch: 32 [46080/50176]	Loss: 0.8580
Training Epoch: 32 [46336/50176]	Loss: 0.9680
Training Epoch: 32 [46592/50176]	Loss: 0.8756
Training Epoch: 32 [46848/50176]	Loss: 1.2060
Training Epoch: 32 [47104/50176]	Loss: 0.9487
Training Epoch: 32 [47360/50176]	Loss: 0.8761
Training Epoch: 32 [47616/50176]	Loss: 0.9407
Training Epoch: 32 [47872/50176]	Loss: 0.9168
Training Epoch: 32 [48128/50176]	Loss: 0.7414
Training Epoch: 32 [48384/50176]	Loss: 0.8989
Training Epoch: 32 [48640/50176]	Loss: 0.8964
Training Epoch: 32 [48896/50176]	Loss: 0.8527
Training Epoch: 32 [49152/50176]	Loss: 0.8653
Training Epoch: 32 [49408/50176]	Loss: 0.7720
Training Epoch: 32 [49664/50176]	Loss: 0.9562
Training Epoch: 32 [49920/50176]	Loss: 0.9318
Training Epoch: 32 [50176/50176]	Loss: 1.0490
Validation Epoch: 32, Average loss: 0.0093, Accuracy: 0.4896
Training Epoch: 33 [256/50176]	Loss: 0.6801
Training Epoch: 33 [512/50176]	Loss: 0.7088
Training Epoch: 33 [768/50176]	Loss: 0.7963
Training Epoch: 33 [1024/50176]	Loss: 0.9186
Training Epoch: 33 [1280/50176]	Loss: 0.7258
Training Epoch: 33 [1536/50176]	Loss: 0.8243
Training Epoch: 33 [1792/50176]	Loss: 0.8728
Training Epoch: 33 [2048/50176]	Loss: 0.8028
Training Epoch: 33 [2304/50176]	Loss: 0.8215
Training Epoch: 33 [2560/50176]	Loss: 0.7917
Training Epoch: 33 [2816/50176]	Loss: 0.7124
Training Epoch: 33 [3072/50176]	Loss: 0.6721
Training Epoch: 33 [3328/50176]	Loss: 0.8294
Training Epoch: 33 [3584/50176]	Loss: 0.8717
Training Epoch: 33 [3840/50176]	Loss: 0.8162
Training Epoch: 33 [4096/50176]	Loss: 0.7488
Training Epoch: 33 [4352/50176]	Loss: 1.0457
Training Epoch: 33 [4608/50176]	Loss: 0.6934
Training Epoch: 33 [4864/50176]	Loss: 0.8335
Training Epoch: 33 [5120/50176]	Loss: 0.7489
Training Epoch: 33 [5376/50176]	Loss: 0.8673
Training Epoch: 33 [5632/50176]	Loss: 0.7496
Training Epoch: 33 [5888/50176]	Loss: 0.8186
Training Epoch: 33 [6144/50176]	Loss: 0.6662
Training Epoch: 33 [6400/50176]	Loss: 0.9026
Training Epoch: 33 [6656/50176]	Loss: 0.8357
Training Epoch: 33 [6912/50176]	Loss: 0.9291
Training Epoch: 33 [7168/50176]	Loss: 0.7951
Training Epoch: 33 [7424/50176]	Loss: 0.6756
Training Epoch: 33 [7680/50176]	Loss: 0.8254
Training Epoch: 33 [7936/50176]	Loss: 0.8406
Training Epoch: 33 [8192/50176]	Loss: 0.8508
Training Epoch: 33 [8448/50176]	Loss: 0.7480
Training Epoch: 33 [8704/50176]	Loss: 0.8241
Training Epoch: 33 [8960/50176]	Loss: 0.6504
Training Epoch: 33 [9216/50176]	Loss: 0.8494
Training Epoch: 33 [9472/50176]	Loss: 0.7404
Training Epoch: 33 [9728/50176]	Loss: 0.7658
Training Epoch: 33 [9984/50176]	Loss: 0.7921
Training Epoch: 33 [10240/50176]	Loss: 0.7897
Training Epoch: 33 [10496/50176]	Loss: 0.8076
Training Epoch: 33 [10752/50176]	Loss: 0.7261
Training Epoch: 33 [11008/50176]	Loss: 0.9211
Training Epoch: 33 [11264/50176]	Loss: 0.7933
Training Epoch: 33 [11520/50176]	Loss: 0.8418
Training Epoch: 33 [11776/50176]	Loss: 0.7569
Training Epoch: 33 [12032/50176]	Loss: 0.8956
Training Epoch: 33 [12288/50176]	Loss: 0.9910
Training Epoch: 33 [12544/50176]	Loss: 0.7646
Training Epoch: 33 [12800/50176]	Loss: 0.7173
Training Epoch: 33 [13056/50176]	Loss: 0.8198
Training Epoch: 33 [13312/50176]	Loss: 0.7923
Training Epoch: 33 [13568/50176]	Loss: 0.6714
Training Epoch: 33 [13824/50176]	Loss: 0.8659
Training Epoch: 33 [14080/50176]	Loss: 0.9215
Training Epoch: 33 [14336/50176]	Loss: 0.7507
Training Epoch: 33 [14592/50176]	Loss: 0.7795
Training Epoch: 33 [14848/50176]	Loss: 0.8809
Training Epoch: 33 [15104/50176]	Loss: 0.6992
Training Epoch: 33 [15360/50176]	Loss: 0.8000
Training Epoch: 33 [15616/50176]	Loss: 0.9402
Training Epoch: 33 [15872/50176]	Loss: 0.7229
Training Epoch: 33 [16128/50176]	Loss: 0.8435
Training Epoch: 33 [16384/50176]	Loss: 0.7894
Training Epoch: 33 [16640/50176]	Loss: 0.8311
Training Epoch: 33 [16896/50176]	Loss: 0.7904
Training Epoch: 33 [17152/50176]	Loss: 0.8319
Training Epoch: 33 [17408/50176]	Loss: 0.7628
Training Epoch: 33 [17664/50176]	Loss: 0.8536
Training Epoch: 33 [17920/50176]	Loss: 0.9965
Training Epoch: 33 [18176/50176]	Loss: 0.7201
Training Epoch: 33 [18432/50176]	Loss: 0.8109
Training Epoch: 33 [18688/50176]	Loss: 0.7701
Training Epoch: 33 [18944/50176]	Loss: 0.6795
Training Epoch: 33 [19200/50176]	Loss: 0.8076
Training Epoch: 33 [19456/50176]	Loss: 1.0312
Training Epoch: 33 [19712/50176]	Loss: 0.8404
Training Epoch: 33 [19968/50176]	Loss: 0.9298
Training Epoch: 33 [20224/50176]	Loss: 0.7702
Training Epoch: 33 [20480/50176]	Loss: 0.8074
Training Epoch: 33 [20736/50176]	Loss: 0.9038
Training Epoch: 33 [20992/50176]	Loss: 0.7633
Training Epoch: 33 [21248/50176]	Loss: 0.8771
Training Epoch: 33 [21504/50176]	Loss: 0.8016
Training Epoch: 33 [21760/50176]	Loss: 0.7727
Training Epoch: 33 [22016/50176]	Loss: 0.9180
Training Epoch: 33 [22272/50176]	Loss: 0.7936
Training Epoch: 33 [22528/50176]	Loss: 0.9925
Training Epoch: 33 [22784/50176]	Loss: 0.7819
Training Epoch: 33 [23040/50176]	Loss: 0.8149
Training Epoch: 33 [23296/50176]	Loss: 0.8230
Training Epoch: 33 [23552/50176]	Loss: 0.9470
Training Epoch: 33 [23808/50176]	Loss: 0.6977
Training Epoch: 33 [24064/50176]	Loss: 0.7969
Training Epoch: 33 [24320/50176]	Loss: 0.7884
Training Epoch: 33 [24576/50176]	Loss: 0.7673
Training Epoch: 33 [24832/50176]	Loss: 0.7945
Training Epoch: 33 [25088/50176]	Loss: 0.7035
Training Epoch: 33 [25344/50176]	Loss: 0.7100
Training Epoch: 33 [25600/50176]	Loss: 1.0028
Training Epoch: 33 [25856/50176]	Loss: 0.7318
Training Epoch: 33 [26112/50176]	Loss: 0.7839
Training Epoch: 33 [26368/50176]	Loss: 0.9218
Training Epoch: 33 [26624/50176]	Loss: 0.8853
Training Epoch: 33 [26880/50176]	Loss: 0.8539
Training Epoch: 33 [27136/50176]	Loss: 0.8344
Training Epoch: 33 [27392/50176]	Loss: 0.8106
Training Epoch: 33 [27648/50176]	Loss: 0.8716
Training Epoch: 33 [27904/50176]	Loss: 0.8910
Training Epoch: 33 [28160/50176]	Loss: 0.7985
Training Epoch: 33 [28416/50176]	Loss: 0.9264
Training Epoch: 33 [28672/50176]	Loss: 0.9448
Training Epoch: 33 [28928/50176]	Loss: 0.6914
Training Epoch: 33 [29184/50176]	Loss: 0.7751
Training Epoch: 33 [29440/50176]	Loss: 0.7895
Training Epoch: 33 [29696/50176]	Loss: 0.8283
Training Epoch: 33 [29952/50176]	Loss: 0.8988
Training Epoch: 33 [30208/50176]	Loss: 0.7637
Training Epoch: 33 [30464/50176]	Loss: 1.1133
Training Epoch: 33 [30720/50176]	Loss: 0.8611
Training Epoch: 33 [30976/50176]	Loss: 0.8994
Training Epoch: 33 [31232/50176]	Loss: 0.8323
Training Epoch: 33 [31488/50176]	Loss: 0.8489
Training Epoch: 33 [31744/50176]	Loss: 0.9158
Training Epoch: 33 [32000/50176]	Loss: 0.8188
Training Epoch: 33 [32256/50176]	Loss: 0.8128
Training Epoch: 33 [32512/50176]	Loss: 0.8002
Training Epoch: 33 [32768/50176]	Loss: 0.7424
Training Epoch: 33 [33024/50176]	Loss: 0.9330
Training Epoch: 33 [33280/50176]	Loss: 0.6438
Training Epoch: 33 [33536/50176]	Loss: 0.9348
Training Epoch: 33 [33792/50176]	Loss: 0.8032
Training Epoch: 33 [34048/50176]	Loss: 0.7856
Training Epoch: 33 [34304/50176]	Loss: 0.9340
Training Epoch: 33 [34560/50176]	Loss: 0.7072
Training Epoch: 33 [34816/50176]	Loss: 0.8267
Training Epoch: 33 [35072/50176]	Loss: 0.9385
Training Epoch: 33 [35328/50176]	Loss: 0.8005
Training Epoch: 33 [35584/50176]	Loss: 0.7596
Training Epoch: 33 [35840/50176]	Loss: 0.9861
Training Epoch: 33 [36096/50176]	Loss: 0.8276
Training Epoch: 33 [36352/50176]	Loss: 0.8091
Training Epoch: 33 [36608/50176]	Loss: 0.8421
Training Epoch: 33 [36864/50176]	Loss: 0.9949
Training Epoch: 33 [37120/50176]	Loss: 0.9031
Training Epoch: 33 [37376/50176]	Loss: 0.8191
Training Epoch: 33 [37632/50176]	Loss: 0.8609
Training Epoch: 33 [37888/50176]	Loss: 0.8260
Training Epoch: 33 [38144/50176]	Loss: 0.9057
Training Epoch: 33 [38400/50176]	Loss: 1.0074
Training Epoch: 33 [38656/50176]	Loss: 0.8807
Training Epoch: 33 [38912/50176]	Loss: 0.8262
Training Epoch: 33 [39168/50176]	Loss: 0.8514
Training Epoch: 33 [39424/50176]	Loss: 0.9292
Training Epoch: 33 [39680/50176]	Loss: 0.9266
Training Epoch: 33 [39936/50176]	Loss: 0.8783
Training Epoch: 33 [40192/50176]	Loss: 0.8593
Training Epoch: 33 [40448/50176]	Loss: 0.8381
Training Epoch: 33 [40704/50176]	Loss: 0.9474
Training Epoch: 33 [40960/50176]	Loss: 1.0390
Training Epoch: 33 [41216/50176]	Loss: 0.8462
Training Epoch: 33 [41472/50176]	Loss: 0.9186
Training Epoch: 33 [41728/50176]	Loss: 0.7387
Training Epoch: 33 [41984/50176]	Loss: 1.0433
Training Epoch: 33 [42240/50176]	Loss: 0.8992
Training Epoch: 33 [42496/50176]	Loss: 0.9688
Training Epoch: 33 [42752/50176]	Loss: 0.8281
Training Epoch: 33 [43008/50176]	Loss: 0.9264
Training Epoch: 33 [43264/50176]	Loss: 1.0523
Training Epoch: 33 [43520/50176]	Loss: 0.9088
Training Epoch: 33 [43776/50176]	Loss: 0.9110
Training Epoch: 33 [44032/50176]	Loss: 0.8489
Training Epoch: 33 [44288/50176]	Loss: 0.8425
Training Epoch: 33 [44544/50176]	Loss: 0.7237
Training Epoch: 33 [44800/50176]	Loss: 1.0254
Training Epoch: 33 [45056/50176]	Loss: 0.9100
Training Epoch: 33 [45312/50176]	Loss: 0.9439
Training Epoch: 33 [45568/50176]	Loss: 0.9024
Training Epoch: 33 [45824/50176]	Loss: 0.8517
Training Epoch: 33 [46080/50176]	Loss: 0.9111
Training Epoch: 33 [46336/50176]	Loss: 0.9689
Training Epoch: 33 [46592/50176]	Loss: 0.9850
Training Epoch: 33 [46848/50176]	Loss: 0.8040
Training Epoch: 33 [47104/50176]	Loss: 0.9632
Training Epoch: 33 [47360/50176]	Loss: 0.8555
Training Epoch: 33 [47616/50176]	Loss: 0.7878
Training Epoch: 33 [47872/50176]	Loss: 1.0326
Training Epoch: 33 [48128/50176]	Loss: 0.9526
Training Epoch: 33 [48384/50176]	Loss: 1.1131
Training Epoch: 33 [48640/50176]	Loss: 0.8591
Training Epoch: 33 [48896/50176]	Loss: 0.8582
Training Epoch: 33 [49152/50176]	Loss: 0.9131
Training Epoch: 33 [49408/50176]	Loss: 0.8933
Training Epoch: 33 [49664/50176]	Loss: 0.7779
Training Epoch: 33 [49920/50176]	Loss: 0.8774
Training Epoch: 33 [50176/50176]	Loss: 0.5909
Validation Epoch: 33, Average loss: 0.0121, Accuracy: 0.4313
Training Epoch: 34 [256/50176]	Loss: 0.7684
Training Epoch: 34 [512/50176]	Loss: 0.6830
Training Epoch: 34 [768/50176]	Loss: 0.7775
Training Epoch: 34 [1024/50176]	Loss: 0.6564
Training Epoch: 34 [1280/50176]	Loss: 0.6583
Training Epoch: 34 [1536/50176]	Loss: 0.7376
Training Epoch: 34 [1792/50176]	Loss: 0.8606
Training Epoch: 34 [2048/50176]	Loss: 0.7217
Training Epoch: 34 [2304/50176]	Loss: 0.6282
Training Epoch: 34 [2560/50176]	Loss: 0.8652
Training Epoch: 34 [2816/50176]	Loss: 0.6849
Training Epoch: 34 [3072/50176]	Loss: 0.7580
Training Epoch: 34 [3328/50176]	Loss: 0.7531
Training Epoch: 34 [3584/50176]	Loss: 0.7111
Training Epoch: 34 [3840/50176]	Loss: 0.7223
Training Epoch: 34 [4096/50176]	Loss: 0.7430
Training Epoch: 34 [4352/50176]	Loss: 0.7791
Training Epoch: 34 [4608/50176]	Loss: 0.7180
Training Epoch: 34 [4864/50176]	Loss: 0.9266
Training Epoch: 34 [5120/50176]	Loss: 0.7688
Training Epoch: 34 [5376/50176]	Loss: 0.7206
Training Epoch: 34 [5632/50176]	Loss: 0.9619
Training Epoch: 34 [5888/50176]	Loss: 0.6999
Training Epoch: 34 [6144/50176]	Loss: 0.7617
Training Epoch: 34 [6400/50176]	Loss: 0.7487
Training Epoch: 34 [6656/50176]	Loss: 0.5724
Training Epoch: 34 [6912/50176]	Loss: 0.6953
Training Epoch: 34 [7168/50176]	Loss: 0.7608
Training Epoch: 34 [7424/50176]	Loss: 0.7637
Training Epoch: 34 [7680/50176]	Loss: 0.6912
Training Epoch: 34 [7936/50176]	Loss: 0.7222
Training Epoch: 34 [8192/50176]	Loss: 0.7604
Training Epoch: 34 [8448/50176]	Loss: 0.8259
Training Epoch: 34 [8704/50176]	Loss: 0.8054
Training Epoch: 34 [8960/50176]	Loss: 0.7335
Training Epoch: 34 [9216/50176]	Loss: 0.7556
Training Epoch: 34 [9472/50176]	Loss: 0.7711
Training Epoch: 34 [9728/50176]	Loss: 0.7948
Training Epoch: 34 [9984/50176]	Loss: 0.7843
Training Epoch: 34 [10240/50176]	Loss: 0.7938
Training Epoch: 34 [10496/50176]	Loss: 0.7588
Training Epoch: 34 [10752/50176]	Loss: 0.7807
Training Epoch: 34 [11008/50176]	Loss: 0.5747
Training Epoch: 34 [11264/50176]	Loss: 0.7537
Training Epoch: 34 [11520/50176]	Loss: 0.7238
Training Epoch: 34 [11776/50176]	Loss: 0.8124
Training Epoch: 34 [12032/50176]	Loss: 0.8074
Training Epoch: 34 [12288/50176]	Loss: 0.7036
Training Epoch: 34 [12544/50176]	Loss: 0.7679
Training Epoch: 34 [12800/50176]	Loss: 0.9469
Training Epoch: 34 [13056/50176]	Loss: 0.7516
Training Epoch: 34 [13312/50176]	Loss: 0.6817
Training Epoch: 34 [13568/50176]	Loss: 0.7912
Training Epoch: 34 [13824/50176]	Loss: 0.6518
Training Epoch: 34 [14080/50176]	Loss: 0.8022
Training Epoch: 34 [14336/50176]	Loss: 0.7892
Training Epoch: 34 [14592/50176]	Loss: 0.7182
Training Epoch: 34 [14848/50176]	Loss: 0.8050
Training Epoch: 34 [15104/50176]	Loss: 0.7260
Training Epoch: 34 [15360/50176]	Loss: 0.8477
Training Epoch: 34 [15616/50176]	Loss: 0.7755
Training Epoch: 34 [15872/50176]	Loss: 0.7769
Training Epoch: 34 [16128/50176]	Loss: 0.7801
Training Epoch: 34 [16384/50176]	Loss: 0.6585
Training Epoch: 34 [16640/50176]	Loss: 0.7995
Training Epoch: 34 [16896/50176]	Loss: 0.8716
Training Epoch: 34 [17152/50176]	Loss: 0.7695
Training Epoch: 34 [17408/50176]	Loss: 0.8708
Training Epoch: 34 [17664/50176]	Loss: 0.7760
Training Epoch: 34 [17920/50176]	Loss: 0.8696
Training Epoch: 34 [18176/50176]	Loss: 0.7423
Training Epoch: 34 [18432/50176]	Loss: 0.9298
Training Epoch: 34 [18688/50176]	Loss: 0.8695
Training Epoch: 34 [18944/50176]	Loss: 0.9181
Training Epoch: 34 [19200/50176]	Loss: 0.8717
Training Epoch: 34 [19456/50176]	Loss: 0.8620
Training Epoch: 34 [19712/50176]	Loss: 0.8181
Training Epoch: 34 [19968/50176]	Loss: 0.8196
Training Epoch: 34 [20224/50176]	Loss: 0.7293
Training Epoch: 34 [20480/50176]	Loss: 0.7932
Training Epoch: 34 [20736/50176]	Loss: 0.9384
Training Epoch: 34 [20992/50176]	Loss: 0.9201
Training Epoch: 34 [21248/50176]	Loss: 0.7155
Training Epoch: 34 [21504/50176]	Loss: 0.8771
Training Epoch: 34 [21760/50176]	Loss: 0.7599
Training Epoch: 34 [22016/50176]	Loss: 0.9566
Training Epoch: 34 [22272/50176]	Loss: 0.7941
Training Epoch: 34 [22528/50176]	Loss: 0.8073
Training Epoch: 34 [22784/50176]	Loss: 0.8461
Training Epoch: 34 [23040/50176]	Loss: 0.9101
Training Epoch: 34 [23296/50176]	Loss: 1.0066
Training Epoch: 34 [23552/50176]	Loss: 0.8287
Training Epoch: 34 [23808/50176]	Loss: 0.8897
Training Epoch: 34 [24064/50176]	Loss: 0.9561
Training Epoch: 34 [24320/50176]	Loss: 0.9641
Training Epoch: 34 [24576/50176]	Loss: 1.1426
Training Epoch: 34 [24832/50176]	Loss: 0.8909
Training Epoch: 34 [25088/50176]	Loss: 0.6948
Training Epoch: 34 [25344/50176]	Loss: 0.8595
Training Epoch: 34 [25600/50176]	Loss: 0.7472
Training Epoch: 34 [25856/50176]	Loss: 0.8288
Training Epoch: 34 [26112/50176]	Loss: 0.6674
Training Epoch: 34 [26368/50176]	Loss: 0.8935
Training Epoch: 34 [26624/50176]	Loss: 0.7874
Training Epoch: 34 [26880/50176]	Loss: 0.8109
Training Epoch: 34 [27136/50176]	Loss: 0.8927
Training Epoch: 34 [27392/50176]	Loss: 0.8853
Training Epoch: 34 [27648/50176]	Loss: 0.8121
Training Epoch: 34 [27904/50176]	Loss: 0.8327
Training Epoch: 34 [28160/50176]	Loss: 0.7925
Training Epoch: 34 [28416/50176]	Loss: 0.7760
Training Epoch: 34 [28672/50176]	Loss: 0.8969
Training Epoch: 34 [28928/50176]	Loss: 0.9427
Training Epoch: 34 [29184/50176]	Loss: 0.6925
Training Epoch: 34 [29440/50176]	Loss: 0.8546
Training Epoch: 34 [29696/50176]	Loss: 0.6322
Training Epoch: 34 [29952/50176]	Loss: 0.8731
Training Epoch: 34 [30208/50176]	Loss: 0.7682
Training Epoch: 34 [30464/50176]	Loss: 0.7553
Training Epoch: 34 [30720/50176]	Loss: 0.7274
Training Epoch: 34 [30976/50176]	Loss: 0.8123
Training Epoch: 34 [31232/50176]	Loss: 0.9357
Training Epoch: 34 [31488/50176]	Loss: 0.7314
Training Epoch: 34 [31744/50176]	Loss: 0.8328
Training Epoch: 34 [32000/50176]	Loss: 1.0179
Training Epoch: 34 [32256/50176]	Loss: 0.8303
Training Epoch: 34 [32512/50176]	Loss: 0.7403
Training Epoch: 34 [32768/50176]	Loss: 0.7504
Training Epoch: 34 [33024/50176]	Loss: 0.8239
Training Epoch: 34 [33280/50176]	Loss: 0.7575
Training Epoch: 34 [33536/50176]	Loss: 0.8517
Training Epoch: 34 [33792/50176]	Loss: 0.9833
Training Epoch: 34 [34048/50176]	Loss: 0.6701
Training Epoch: 34 [34304/50176]	Loss: 0.8256
Training Epoch: 34 [34560/50176]	Loss: 0.7816
Training Epoch: 34 [34816/50176]	Loss: 0.8432
Training Epoch: 34 [35072/50176]	Loss: 0.7395
Training Epoch: 34 [35328/50176]	Loss: 0.8622
Training Epoch: 34 [35584/50176]	Loss: 0.6993
Training Epoch: 34 [35840/50176]	Loss: 0.6909
Training Epoch: 34 [36096/50176]	Loss: 0.9380
Training Epoch: 34 [36352/50176]	Loss: 0.8149
Training Epoch: 34 [36608/50176]	Loss: 0.9767
Training Epoch: 34 [36864/50176]	Loss: 0.7619
Training Epoch: 34 [37120/50176]	Loss: 0.8076
Training Epoch: 34 [37376/50176]	Loss: 0.9249
Training Epoch: 34 [37632/50176]	Loss: 0.8161
Training Epoch: 34 [37888/50176]	Loss: 0.7482
Training Epoch: 34 [38144/50176]	Loss: 0.6779
Training Epoch: 34 [38400/50176]	Loss: 0.9166
Training Epoch: 34 [38656/50176]	Loss: 0.9224
Training Epoch: 34 [38912/50176]	Loss: 0.8159
Training Epoch: 34 [39168/50176]	Loss: 0.8428
Training Epoch: 34 [39424/50176]	Loss: 1.0156
Training Epoch: 34 [39680/50176]	Loss: 0.9189
Training Epoch: 34 [39936/50176]	Loss: 0.8381
Training Epoch: 34 [40192/50176]	Loss: 0.7428
Training Epoch: 34 [40448/50176]	Loss: 0.8179
Training Epoch: 34 [40704/50176]	Loss: 0.7452
Training Epoch: 34 [40960/50176]	Loss: 0.8145
Training Epoch: 34 [41216/50176]	Loss: 0.9163
Training Epoch: 34 [41472/50176]	Loss: 0.8066
Training Epoch: 34 [41728/50176]	Loss: 0.9998
Training Epoch: 34 [41984/50176]	Loss: 0.9724
Training Epoch: 34 [42240/50176]	Loss: 0.7603
Training Epoch: 34 [42496/50176]	Loss: 0.8568
Training Epoch: 34 [42752/50176]	Loss: 0.8141
Training Epoch: 34 [43008/50176]	Loss: 0.9082
Training Epoch: 34 [43264/50176]	Loss: 0.9352
Training Epoch: 34 [43520/50176]	Loss: 0.7714
Training Epoch: 34 [43776/50176]	Loss: 0.7736
Training Epoch: 34 [44032/50176]	Loss: 0.8709
Training Epoch: 34 [44288/50176]	Loss: 0.8059
Training Epoch: 34 [44544/50176]	Loss: 0.9464
Training Epoch: 34 [44800/50176]	Loss: 0.8597
Training Epoch: 34 [45056/50176]	Loss: 1.0912
Training Epoch: 34 [45312/50176]	Loss: 0.8474
Training Epoch: 34 [45568/50176]	Loss: 1.0887
Training Epoch: 34 [45824/50176]	Loss: 0.8169
Training Epoch: 34 [46080/50176]	Loss: 0.9080
Training Epoch: 34 [46336/50176]	Loss: 0.7462
Training Epoch: 34 [46592/50176]	Loss: 0.8987
Training Epoch: 34 [46848/50176]	Loss: 0.8492
Training Epoch: 34 [47104/50176]	Loss: 0.8852
Training Epoch: 34 [47360/50176]	Loss: 0.7952
Training Epoch: 34 [47616/50176]	Loss: 0.8832
Training Epoch: 34 [47872/50176]	Loss: 0.8800
Training Epoch: 34 [48128/50176]	Loss: 0.8911
Training Epoch: 34 [48384/50176]	Loss: 0.9268
Training Epoch: 34 [48640/50176]	Loss: 0.7121
Training Epoch: 34 [48896/50176]	Loss: 0.8312
Training Epoch: 34 [49152/50176]	Loss: 0.9926
Training Epoch: 34 [49408/50176]	Loss: 0.8171
Training Epoch: 34 [49664/50176]	Loss: 0.9660
Training Epoch: 34 [49920/50176]	Loss: 0.9512
Training Epoch: 34 [50176/50176]	Loss: 1.0405
Validation Epoch: 34, Average loss: 0.0106, Accuracy: 0.4720
Training Epoch: 35 [256/50176]	Loss: 0.7251
Training Epoch: 35 [512/50176]	Loss: 0.7748
Training Epoch: 35 [768/50176]	Loss: 0.6683
Training Epoch: 35 [1024/50176]	Loss: 0.7875
Training Epoch: 35 [1280/50176]	Loss: 0.8179
Training Epoch: 35 [1536/50176]	Loss: 0.6572
Training Epoch: 35 [1792/50176]	Loss: 0.7955
Training Epoch: 35 [2048/50176]	Loss: 0.6868
Training Epoch: 35 [2304/50176]	Loss: 0.7089
Training Epoch: 35 [2560/50176]	Loss: 0.7002
Training Epoch: 35 [2816/50176]	Loss: 0.8423
Training Epoch: 35 [3072/50176]	Loss: 0.7662
Training Epoch: 35 [3328/50176]	Loss: 0.6817
Training Epoch: 35 [3584/50176]	Loss: 0.9108
Training Epoch: 35 [3840/50176]	Loss: 0.6995
Training Epoch: 35 [4096/50176]	Loss: 0.7994
Training Epoch: 35 [4352/50176]	Loss: 0.7240
Training Epoch: 35 [4608/50176]	Loss: 0.8000
Training Epoch: 35 [4864/50176]	Loss: 0.7732
Training Epoch: 35 [5120/50176]	Loss: 0.7128
Training Epoch: 35 [5376/50176]	Loss: 0.8405
Training Epoch: 35 [5632/50176]	Loss: 0.6968
Training Epoch: 35 [5888/50176]	Loss: 0.7777
Training Epoch: 35 [6144/50176]	Loss: 0.6304
Training Epoch: 35 [6400/50176]	Loss: 0.6976
Training Epoch: 35 [6656/50176]	Loss: 0.7307
Training Epoch: 35 [6912/50176]	Loss: 0.7634
Training Epoch: 35 [7168/50176]	Loss: 0.8658
Training Epoch: 35 [7424/50176]	Loss: 0.7271
Training Epoch: 35 [7680/50176]	Loss: 0.7556
Training Epoch: 35 [7936/50176]	Loss: 0.7373
Training Epoch: 35 [8192/50176]	Loss: 0.7796
Training Epoch: 35 [8448/50176]	Loss: 0.7346
Training Epoch: 35 [8704/50176]	Loss: 0.7699
Training Epoch: 35 [8960/50176]	Loss: 0.7660
Training Epoch: 35 [9216/50176]	Loss: 0.8443
Training Epoch: 35 [9472/50176]	Loss: 0.7384
Training Epoch: 35 [9728/50176]	Loss: 0.7354
Training Epoch: 35 [9984/50176]	Loss: 0.8204
Training Epoch: 35 [10240/50176]	Loss: 0.8556
Training Epoch: 35 [10496/50176]	Loss: 0.7975
Training Epoch: 35 [10752/50176]	Loss: 0.9985
Training Epoch: 35 [11008/50176]	Loss: 0.6669
Training Epoch: 35 [11264/50176]	Loss: 0.8103
Training Epoch: 35 [11520/50176]	Loss: 0.7004
Training Epoch: 35 [11776/50176]	Loss: 0.7780
Training Epoch: 35 [12032/50176]	Loss: 0.7266
Training Epoch: 35 [12288/50176]	Loss: 0.7959
Training Epoch: 35 [12544/50176]	Loss: 0.8736
Training Epoch: 35 [12800/50176]	Loss: 0.8706
Training Epoch: 35 [13056/50176]	Loss: 0.6653
Training Epoch: 35 [13312/50176]	Loss: 0.8875
Training Epoch: 35 [13568/50176]	Loss: 0.7975
Training Epoch: 35 [13824/50176]	Loss: 0.8933
Training Epoch: 35 [14080/50176]	Loss: 0.8148
Training Epoch: 35 [14336/50176]	Loss: 0.6786
Training Epoch: 35 [14592/50176]	Loss: 0.7676
Training Epoch: 35 [14848/50176]	Loss: 0.8446
Training Epoch: 35 [15104/50176]	Loss: 0.6993
Training Epoch: 35 [15360/50176]	Loss: 0.6797
Training Epoch: 35 [15616/50176]	Loss: 0.8685
Training Epoch: 35 [15872/50176]	Loss: 0.7416
Training Epoch: 35 [16128/50176]	Loss: 0.9069
Training Epoch: 35 [16384/50176]	Loss: 0.7634
Training Epoch: 35 [16640/50176]	Loss: 0.9277
Training Epoch: 35 [16896/50176]	Loss: 0.6734
Training Epoch: 35 [17152/50176]	Loss: 0.8473
Training Epoch: 35 [17408/50176]	Loss: 0.6433
Training Epoch: 35 [17664/50176]	Loss: 0.6931
Training Epoch: 35 [17920/50176]	Loss: 0.8643
Training Epoch: 35 [18176/50176]	Loss: 0.7866
Training Epoch: 35 [18432/50176]	Loss: 0.9479
Training Epoch: 35 [18688/50176]	Loss: 0.9304
Training Epoch: 35 [18944/50176]	Loss: 0.6841
Training Epoch: 35 [19200/50176]	Loss: 0.8673
Training Epoch: 35 [19456/50176]	Loss: 0.8509
Training Epoch: 35 [19712/50176]	Loss: 0.8567
Training Epoch: 35 [19968/50176]	Loss: 0.6775
Training Epoch: 35 [20224/50176]	Loss: 1.0593
Training Epoch: 35 [20480/50176]	Loss: 0.9026
Training Epoch: 35 [20736/50176]	Loss: 0.8338
Training Epoch: 35 [20992/50176]	Loss: 0.8429
Training Epoch: 35 [21248/50176]	Loss: 0.8946
Training Epoch: 35 [21504/50176]	Loss: 0.8096
Training Epoch: 35 [21760/50176]	Loss: 0.7769
Training Epoch: 35 [22016/50176]	Loss: 0.7317
Training Epoch: 35 [22272/50176]	Loss: 0.8130
Training Epoch: 35 [22528/50176]	Loss: 0.8033
Training Epoch: 35 [22784/50176]	Loss: 0.7973
Training Epoch: 35 [23040/50176]	Loss: 0.7354
Training Epoch: 35 [23296/50176]	Loss: 0.9298
Training Epoch: 35 [23552/50176]	Loss: 0.6841
Training Epoch: 35 [23808/50176]	Loss: 0.8248
Training Epoch: 35 [24064/50176]	Loss: 0.8400
Training Epoch: 35 [24320/50176]	Loss: 0.9416
Training Epoch: 35 [24576/50176]	Loss: 0.7944
Training Epoch: 35 [24832/50176]	Loss: 0.8927
Training Epoch: 35 [25088/50176]	Loss: 0.8301
Training Epoch: 35 [25344/50176]	Loss: 0.7776
Training Epoch: 35 [25600/50176]	Loss: 0.8027
Training Epoch: 35 [25856/50176]	Loss: 0.8918
Training Epoch: 35 [26112/50176]	Loss: 0.8389
Training Epoch: 35 [26368/50176]	Loss: 0.6573
Training Epoch: 35 [26624/50176]	Loss: 0.7909
Training Epoch: 35 [26880/50176]	Loss: 0.8413
Training Epoch: 35 [27136/50176]	Loss: 0.7826
Training Epoch: 35 [27392/50176]	Loss: 0.7055
Training Epoch: 35 [27648/50176]	Loss: 0.7383
Training Epoch: 35 [27904/50176]	Loss: 0.6906
Training Epoch: 35 [28160/50176]	Loss: 0.8441
Training Epoch: 35 [28416/50176]	Loss: 0.7565
Training Epoch: 35 [28672/50176]	Loss: 0.8362
Training Epoch: 35 [28928/50176]	Loss: 0.7146
Training Epoch: 35 [29184/50176]	Loss: 0.9218
Training Epoch: 35 [29440/50176]	Loss: 0.8477
Training Epoch: 35 [29696/50176]	Loss: 1.0735
Training Epoch: 35 [29952/50176]	Loss: 0.7813
Training Epoch: 35 [30208/50176]	Loss: 0.8198
Training Epoch: 35 [30464/50176]	Loss: 0.7822
Training Epoch: 35 [30720/50176]	Loss: 0.7177
Training Epoch: 35 [30976/50176]	Loss: 0.8127
Training Epoch: 35 [31232/50176]	Loss: 0.8754
Training Epoch: 35 [31488/50176]	Loss: 0.7612
Training Epoch: 35 [31744/50176]	Loss: 0.7635
Training Epoch: 35 [32000/50176]	Loss: 0.7449
Training Epoch: 35 [32256/50176]	Loss: 0.8528
Training Epoch: 35 [32512/50176]	Loss: 0.8158
Training Epoch: 35 [32768/50176]	Loss: 0.8738
Training Epoch: 35 [33024/50176]	Loss: 0.7421
Training Epoch: 35 [33280/50176]	Loss: 0.8715
Training Epoch: 35 [33536/50176]	Loss: 0.7208
Training Epoch: 35 [33792/50176]	Loss: 0.8953
Training Epoch: 35 [34048/50176]	Loss: 0.6916
Training Epoch: 35 [34304/50176]	Loss: 0.7849
Training Epoch: 35 [34560/50176]	Loss: 0.6689
Training Epoch: 35 [34816/50176]	Loss: 0.8193
Training Epoch: 35 [35072/50176]	Loss: 0.8753
Training Epoch: 35 [35328/50176]	Loss: 0.7213
Training Epoch: 35 [35584/50176]	Loss: 0.6672
Training Epoch: 35 [35840/50176]	Loss: 0.7658
Training Epoch: 35 [36096/50176]	Loss: 0.8038
Training Epoch: 35 [36352/50176]	Loss: 0.6860
Training Epoch: 35 [36608/50176]	Loss: 0.8981
Training Epoch: 35 [36864/50176]	Loss: 0.9797
Training Epoch: 35 [37120/50176]	Loss: 0.7186
Training Epoch: 35 [37376/50176]	Loss: 0.7520
Training Epoch: 35 [37632/50176]	Loss: 0.8315
Training Epoch: 35 [37888/50176]	Loss: 0.7705
Training Epoch: 35 [38144/50176]	Loss: 0.8287
Training Epoch: 35 [38400/50176]	Loss: 0.6595
Training Epoch: 35 [38656/50176]	Loss: 0.7466
Training Epoch: 35 [38912/50176]	Loss: 0.8050
Training Epoch: 35 [39168/50176]	Loss: 0.7652
Training Epoch: 35 [39424/50176]	Loss: 0.7640
Training Epoch: 35 [39680/50176]	Loss: 0.9126
Training Epoch: 35 [39936/50176]	Loss: 0.8166
Training Epoch: 35 [40192/50176]	Loss: 0.6674
Training Epoch: 35 [40448/50176]	Loss: 0.9412
Training Epoch: 35 [40704/50176]	Loss: 0.8578
Training Epoch: 35 [40960/50176]	Loss: 0.7305
Training Epoch: 35 [41216/50176]	Loss: 0.7601
Training Epoch: 35 [41472/50176]	Loss: 0.7875
Training Epoch: 35 [41728/50176]	Loss: 0.8085
Training Epoch: 35 [41984/50176]	Loss: 0.8338
Training Epoch: 35 [42240/50176]	Loss: 0.9493
Training Epoch: 35 [42496/50176]	Loss: 0.8394
Training Epoch: 35 [42752/50176]	Loss: 0.8426
Training Epoch: 35 [43008/50176]	Loss: 0.7168
Training Epoch: 35 [43264/50176]	Loss: 0.8565
Training Epoch: 35 [43520/50176]	Loss: 0.8421
Training Epoch: 35 [43776/50176]	Loss: 0.8108
Training Epoch: 35 [44032/50176]	Loss: 1.0622
Training Epoch: 35 [44288/50176]	Loss: 0.9018
Training Epoch: 35 [44544/50176]	Loss: 0.8026
Training Epoch: 35 [44800/50176]	Loss: 0.7537
Training Epoch: 35 [45056/50176]	Loss: 0.7945
Training Epoch: 35 [45312/50176]	Loss: 0.8035
Training Epoch: 35 [45568/50176]	Loss: 0.8248
Training Epoch: 35 [45824/50176]	Loss: 0.7930
Training Epoch: 35 [46080/50176]	Loss: 0.8152
Training Epoch: 35 [46336/50176]	Loss: 0.8204
Training Epoch: 35 [46592/50176]	Loss: 0.9544
Training Epoch: 35 [46848/50176]	Loss: 0.8022
Training Epoch: 35 [47104/50176]	Loss: 0.8261
Training Epoch: 35 [47360/50176]	Loss: 0.8735
Training Epoch: 35 [47616/50176]	Loss: 0.9259
Training Epoch: 35 [47872/50176]	Loss: 0.8798
Training Epoch: 35 [48128/50176]	Loss: 0.9057
Training Epoch: 35 [48384/50176]	Loss: 0.7942
Training Epoch: 35 [48640/50176]	Loss: 0.9481
Training Epoch: 35 [48896/50176]	Loss: 0.8981
Training Epoch: 35 [49152/50176]	Loss: 0.9740
Training Epoch: 35 [49408/50176]	Loss: 0.7863
Training Epoch: 35 [49664/50176]	Loss: 0.8825
Training Epoch: 35 [49920/50176]	Loss: 0.7519
Training Epoch: 35 [50176/50176]	Loss: 0.7845
Validation Epoch: 35, Average loss: 0.0077, Accuracy: 0.5400
Training Epoch: 36 [256/50176]	Loss: 0.6895
Training Epoch: 36 [512/50176]	Loss: 0.6857
Training Epoch: 36 [768/50176]	Loss: 0.7440
Training Epoch: 36 [1024/50176]	Loss: 0.6845
Training Epoch: 36 [1280/50176]	Loss: 0.7487
Training Epoch: 36 [1536/50176]	Loss: 0.7966
Training Epoch: 36 [1792/50176]	Loss: 0.7203
Training Epoch: 36 [2048/50176]	Loss: 0.6224
Training Epoch: 36 [2304/50176]	Loss: 0.6968
Training Epoch: 36 [2560/50176]	Loss: 0.7852
Training Epoch: 36 [2816/50176]	Loss: 0.7943
Training Epoch: 36 [3072/50176]	Loss: 0.8825
Training Epoch: 36 [3328/50176]	Loss: 0.6743
Training Epoch: 36 [3584/50176]	Loss: 0.6786
Training Epoch: 36 [3840/50176]	Loss: 0.8423
Training Epoch: 36 [4096/50176]	Loss: 0.6812
Training Epoch: 36 [4352/50176]	Loss: 0.7092
Training Epoch: 36 [4608/50176]	Loss: 0.7748
Training Epoch: 36 [4864/50176]	Loss: 0.6622
Training Epoch: 36 [5120/50176]	Loss: 0.8111
Training Epoch: 36 [5376/50176]	Loss: 0.6701
Training Epoch: 36 [5632/50176]	Loss: 0.7463
Training Epoch: 36 [5888/50176]	Loss: 0.7054
Training Epoch: 36 [6144/50176]	Loss: 0.6341
Training Epoch: 36 [6400/50176]	Loss: 0.7351
Training Epoch: 36 [6656/50176]	Loss: 0.6568
Training Epoch: 36 [6912/50176]	Loss: 0.7084
Training Epoch: 36 [7168/50176]	Loss: 0.7328
Training Epoch: 36 [7424/50176]	Loss: 0.8000
Training Epoch: 36 [7680/50176]	Loss: 0.7432
Training Epoch: 36 [7936/50176]	Loss: 0.7604
Training Epoch: 36 [8192/50176]	Loss: 0.7323
Training Epoch: 36 [8448/50176]	Loss: 0.7157
Training Epoch: 36 [8704/50176]	Loss: 0.7968
Training Epoch: 36 [8960/50176]	Loss: 0.8142
Training Epoch: 36 [9216/50176]	Loss: 0.8776
Training Epoch: 36 [9472/50176]	Loss: 0.6650
Training Epoch: 36 [9728/50176]	Loss: 0.7515
Training Epoch: 36 [9984/50176]	Loss: 0.7234
Training Epoch: 36 [10240/50176]	Loss: 0.7263
Training Epoch: 36 [10496/50176]	Loss: 0.7846
Training Epoch: 36 [10752/50176]	Loss: 0.7344
Training Epoch: 36 [11008/50176]	Loss: 0.6743
Training Epoch: 36 [11264/50176]	Loss: 0.8254
Training Epoch: 36 [11520/50176]	Loss: 0.8804
Training Epoch: 36 [11776/50176]	Loss: 0.8575
Training Epoch: 36 [12032/50176]	Loss: 0.7266
Training Epoch: 36 [12288/50176]	Loss: 0.7014
Training Epoch: 36 [12544/50176]	Loss: 0.7988
Training Epoch: 36 [12800/50176]	Loss: 0.7242
Training Epoch: 36 [13056/50176]	Loss: 0.7260
Training Epoch: 36 [13312/50176]	Loss: 0.6576
Training Epoch: 36 [13568/50176]	Loss: 0.6257
Training Epoch: 36 [13824/50176]	Loss: 0.8215
Training Epoch: 36 [14080/50176]	Loss: 0.8622
Training Epoch: 36 [14336/50176]	Loss: 0.7948
Training Epoch: 36 [14592/50176]	Loss: 0.8203
Training Epoch: 36 [14848/50176]	Loss: 0.7333
Training Epoch: 36 [15104/50176]	Loss: 0.6361
Training Epoch: 36 [15360/50176]	Loss: 0.6080
Training Epoch: 36 [15616/50176]	Loss: 0.7889
Training Epoch: 36 [15872/50176]	Loss: 0.8364
Training Epoch: 36 [16128/50176]	Loss: 0.6482
Training Epoch: 36 [16384/50176]	Loss: 0.8783
Training Epoch: 36 [16640/50176]	Loss: 0.7787
Training Epoch: 36 [16896/50176]	Loss: 0.6823
Training Epoch: 36 [17152/50176]	Loss: 0.7403
Training Epoch: 36 [17408/50176]	Loss: 0.6185
Training Epoch: 36 [17664/50176]	Loss: 0.6824
Training Epoch: 36 [17920/50176]	Loss: 0.7757
Training Epoch: 36 [18176/50176]	Loss: 0.7173
Training Epoch: 36 [18432/50176]	Loss: 0.8606
Training Epoch: 36 [18688/50176]	Loss: 0.7222
Training Epoch: 36 [18944/50176]	Loss: 0.6093
Training Epoch: 36 [19200/50176]	Loss: 0.6227
Training Epoch: 36 [19456/50176]	Loss: 0.7649
Training Epoch: 36 [19712/50176]	Loss: 0.7233
Training Epoch: 36 [19968/50176]	Loss: 0.7588
Training Epoch: 36 [20224/50176]	Loss: 0.7673
Training Epoch: 36 [20480/50176]	Loss: 0.7317
Training Epoch: 36 [20736/50176]	Loss: 0.6545
Training Epoch: 36 [20992/50176]	Loss: 0.6910
Training Epoch: 36 [21248/50176]	Loss: 0.6690
Training Epoch: 36 [21504/50176]	Loss: 0.7635
Training Epoch: 36 [21760/50176]	Loss: 0.8823
Training Epoch: 36 [22016/50176]	Loss: 0.5842
Training Epoch: 36 [22272/50176]	Loss: 0.6917
Training Epoch: 36 [22528/50176]	Loss: 0.7200
Training Epoch: 36 [22784/50176]	Loss: 0.6837
Training Epoch: 36 [23040/50176]	Loss: 0.7294
Training Epoch: 36 [23296/50176]	Loss: 0.7116
Training Epoch: 36 [23552/50176]	Loss: 0.6466
Training Epoch: 36 [23808/50176]	Loss: 0.9451
Training Epoch: 36 [24064/50176]	Loss: 0.7062
Training Epoch: 36 [24320/50176]	Loss: 0.9131
Training Epoch: 36 [24576/50176]	Loss: 0.7992
Training Epoch: 36 [24832/50176]	Loss: 0.8198
Training Epoch: 36 [25088/50176]	Loss: 0.8277
Training Epoch: 36 [25344/50176]	Loss: 0.7912
Training Epoch: 36 [25600/50176]	Loss: 0.7543
Training Epoch: 36 [25856/50176]	Loss: 0.6571
Training Epoch: 36 [26112/50176]	Loss: 0.8450
Training Epoch: 36 [26368/50176]	Loss: 0.7030
Training Epoch: 36 [26624/50176]	Loss: 0.7600
Training Epoch: 36 [26880/50176]	Loss: 0.9398
Training Epoch: 36 [27136/50176]	Loss: 0.7330
Training Epoch: 36 [27392/50176]	Loss: 0.9406
Training Epoch: 36 [27648/50176]	Loss: 0.7054
Training Epoch: 36 [27904/50176]	Loss: 0.9040
Training Epoch: 36 [28160/50176]	Loss: 0.7757
Training Epoch: 36 [28416/50176]	Loss: 0.8867
Training Epoch: 36 [28672/50176]	Loss: 0.6982
Training Epoch: 36 [28928/50176]	Loss: 0.6421
Training Epoch: 36 [29184/50176]	Loss: 0.8482
Training Epoch: 36 [29440/50176]	Loss: 0.7119
Training Epoch: 36 [29696/50176]	Loss: 0.7211
Training Epoch: 36 [29952/50176]	Loss: 0.9962
Training Epoch: 36 [30208/50176]	Loss: 0.8759
Training Epoch: 36 [30464/50176]	Loss: 0.8942
Training Epoch: 36 [30720/50176]	Loss: 0.9425
Training Epoch: 36 [30976/50176]	Loss: 0.7645
Training Epoch: 36 [31232/50176]	Loss: 0.7785
Training Epoch: 36 [31488/50176]	Loss: 0.8607
Training Epoch: 36 [31744/50176]	Loss: 0.8411
Training Epoch: 36 [32000/50176]	Loss: 0.9028
Training Epoch: 36 [32256/50176]	Loss: 0.8155
Training Epoch: 36 [32512/50176]	Loss: 0.8558
Training Epoch: 36 [32768/50176]	Loss: 0.6932
Training Epoch: 36 [33024/50176]	Loss: 0.8324
Training Epoch: 36 [33280/50176]	Loss: 0.8450
Training Epoch: 36 [33536/50176]	Loss: 0.8040
Training Epoch: 36 [33792/50176]	Loss: 0.7763
Training Epoch: 36 [34048/50176]	Loss: 0.7976
Training Epoch: 36 [34304/50176]	Loss: 0.8136
Training Epoch: 36 [34560/50176]	Loss: 0.7240
Training Epoch: 36 [34816/50176]	Loss: 0.8481
Training Epoch: 36 [35072/50176]	Loss: 0.8855
Training Epoch: 36 [35328/50176]	Loss: 0.8217
Training Epoch: 36 [35584/50176]	Loss: 0.8754
Training Epoch: 36 [35840/50176]	Loss: 0.7633
Training Epoch: 36 [36096/50176]	Loss: 0.9576
Training Epoch: 36 [36352/50176]	Loss: 0.7558
Training Epoch: 36 [36608/50176]	Loss: 0.8772
Training Epoch: 36 [36864/50176]	Loss: 0.9380
Training Epoch: 36 [37120/50176]	Loss: 0.7537
Training Epoch: 36 [37376/50176]	Loss: 0.7551
Training Epoch: 36 [37632/50176]	Loss: 0.8550
Training Epoch: 36 [37888/50176]	Loss: 0.7952
Training Epoch: 36 [38144/50176]	Loss: 0.7554
Training Epoch: 36 [38400/50176]	Loss: 0.7540
Training Epoch: 36 [38656/50176]	Loss: 0.8308
Training Epoch: 36 [38912/50176]	Loss: 0.8337
Training Epoch: 36 [39168/50176]	Loss: 0.8549
Training Epoch: 36 [39424/50176]	Loss: 0.8087
Training Epoch: 36 [39680/50176]	Loss: 0.7341
Training Epoch: 36 [39936/50176]	Loss: 0.8572
Training Epoch: 36 [40192/50176]	Loss: 0.9989
Training Epoch: 36 [40448/50176]	Loss: 0.6851
Training Epoch: 36 [40704/50176]	Loss: 0.7570
Training Epoch: 36 [40960/50176]	Loss: 0.7782
Training Epoch: 36 [41216/50176]	Loss: 0.7971
Training Epoch: 36 [41472/50176]	Loss: 0.8421
Training Epoch: 36 [41728/50176]	Loss: 0.7003
Training Epoch: 36 [41984/50176]	Loss: 0.7112
Training Epoch: 36 [42240/50176]	Loss: 0.8957
Training Epoch: 36 [42496/50176]	Loss: 1.0211
Training Epoch: 36 [42752/50176]	Loss: 0.7585
Training Epoch: 36 [43008/50176]	Loss: 0.9652
Training Epoch: 36 [43264/50176]	Loss: 0.8522
Training Epoch: 36 [43520/50176]	Loss: 0.6443
Training Epoch: 36 [43776/50176]	Loss: 0.8366
Training Epoch: 36 [44032/50176]	Loss: 0.9475
Training Epoch: 36 [44288/50176]	Loss: 0.8832
Training Epoch: 36 [44544/50176]	Loss: 0.7953
Training Epoch: 36 [44800/50176]	Loss: 0.8808
Training Epoch: 36 [45056/50176]	Loss: 0.6924
Training Epoch: 36 [45312/50176]	Loss: 0.6851
Training Epoch: 36 [45568/50176]	Loss: 0.7695
Training Epoch: 36 [45824/50176]	Loss: 0.8320
Training Epoch: 36 [46080/50176]	Loss: 0.9170
Training Epoch: 36 [46336/50176]	Loss: 0.8855
Training Epoch: 36 [46592/50176]	Loss: 0.7262
Training Epoch: 36 [46848/50176]	Loss: 0.7040
Training Epoch: 36 [47104/50176]	Loss: 0.7374
Training Epoch: 36 [47360/50176]	Loss: 0.8611
Training Epoch: 36 [47616/50176]	Loss: 0.7456
Training Epoch: 36 [47872/50176]	Loss: 0.7794
Training Epoch: 36 [48128/50176]	Loss: 1.0195
Training Epoch: 36 [48384/50176]	Loss: 0.7890
Training Epoch: 36 [48640/50176]	Loss: 0.7344
Training Epoch: 36 [48896/50176]	Loss: 0.7973
Training Epoch: 36 [49152/50176]	Loss: 0.7007
Training Epoch: 36 [49408/50176]	Loss: 0.8653
Training Epoch: 36 [49664/50176]	Loss: 0.8859
Training Epoch: 36 [49920/50176]	Loss: 0.8337
Training Epoch: 36 [50176/50176]	Loss: 0.6725
Validation Epoch: 36, Average loss: 0.0082, Accuracy: 0.5239
Training Epoch: 37 [256/50176]	Loss: 0.7798
Training Epoch: 37 [512/50176]	Loss: 0.6547
Training Epoch: 37 [768/50176]	Loss: 0.6862
Training Epoch: 37 [1024/50176]	Loss: 0.8237
Training Epoch: 37 [1280/50176]	Loss: 0.5005
Training Epoch: 37 [1536/50176]	Loss: 0.6389
Training Epoch: 37 [1792/50176]	Loss: 0.7190
Training Epoch: 37 [2048/50176]	Loss: 0.5596
Training Epoch: 37 [2304/50176]	Loss: 0.6679
Training Epoch: 37 [2560/50176]	Loss: 0.7432
Training Epoch: 37 [2816/50176]	Loss: 0.7032
Training Epoch: 37 [3072/50176]	Loss: 0.7172
Training Epoch: 37 [3328/50176]	Loss: 0.7565
Training Epoch: 37 [3584/50176]	Loss: 0.6332
Training Epoch: 37 [3840/50176]	Loss: 0.8284
Training Epoch: 37 [4096/50176]	Loss: 0.8164
Training Epoch: 37 [4352/50176]	Loss: 0.7344
Training Epoch: 37 [4608/50176]	Loss: 0.7283
Training Epoch: 37 [4864/50176]	Loss: 0.7389
Training Epoch: 37 [5120/50176]	Loss: 0.6794
Training Epoch: 37 [5376/50176]	Loss: 0.6313
Training Epoch: 37 [5632/50176]	Loss: 0.7334
Training Epoch: 37 [5888/50176]	Loss: 0.6871
Training Epoch: 37 [6144/50176]	Loss: 0.7579
Training Epoch: 37 [6400/50176]	Loss: 0.7269
Training Epoch: 37 [6656/50176]	Loss: 0.7159
Training Epoch: 37 [6912/50176]	Loss: 0.7941
Training Epoch: 37 [7168/50176]	Loss: 0.6930
Training Epoch: 37 [7424/50176]	Loss: 0.6384
Training Epoch: 37 [7680/50176]	Loss: 0.7666
Training Epoch: 37 [7936/50176]	Loss: 0.6400
Training Epoch: 37 [8192/50176]	Loss: 0.5215
Training Epoch: 37 [8448/50176]	Loss: 0.8443
Training Epoch: 37 [8704/50176]	Loss: 0.8105
Training Epoch: 37 [8960/50176]	Loss: 0.7678
Training Epoch: 37 [9216/50176]	Loss: 0.8146
Training Epoch: 37 [9472/50176]	Loss: 0.7317
Training Epoch: 37 [9728/50176]	Loss: 0.7133
Training Epoch: 37 [9984/50176]	Loss: 0.7953
Training Epoch: 37 [10240/50176]	Loss: 0.7361
Training Epoch: 37 [10496/50176]	Loss: 0.7148
Training Epoch: 37 [10752/50176]	Loss: 0.8377
Training Epoch: 37 [11008/50176]	Loss: 0.6874
Training Epoch: 37 [11264/50176]	Loss: 0.6662
Training Epoch: 37 [11520/50176]	Loss: 0.8728
Training Epoch: 37 [11776/50176]	Loss: 0.6263
Training Epoch: 37 [12032/50176]	Loss: 0.7322
Training Epoch: 37 [12288/50176]	Loss: 0.8760
Training Epoch: 37 [12544/50176]	Loss: 0.6493
Training Epoch: 37 [12800/50176]	Loss: 0.6344
Training Epoch: 37 [13056/50176]	Loss: 0.7299
Training Epoch: 37 [13312/50176]	Loss: 0.6794
Training Epoch: 37 [13568/50176]	Loss: 0.8351
Training Epoch: 37 [13824/50176]	Loss: 0.7341
Training Epoch: 37 [14080/50176]	Loss: 0.8879
Training Epoch: 37 [14336/50176]	Loss: 0.7770
Training Epoch: 37 [14592/50176]	Loss: 0.5490
Training Epoch: 37 [14848/50176]	Loss: 0.7958
Training Epoch: 37 [15104/50176]	Loss: 0.6936
Training Epoch: 37 [15360/50176]	Loss: 0.7920
Training Epoch: 37 [15616/50176]	Loss: 0.7560
Training Epoch: 37 [15872/50176]	Loss: 0.7984
Training Epoch: 37 [16128/50176]	Loss: 0.7515
Training Epoch: 37 [16384/50176]	Loss: 0.6553
Training Epoch: 37 [16640/50176]	Loss: 0.8045
Training Epoch: 37 [16896/50176]	Loss: 0.7303
Training Epoch: 37 [17152/50176]	Loss: 0.6431
Training Epoch: 37 [17408/50176]	Loss: 0.6591
Training Epoch: 37 [17664/50176]	Loss: 0.7412
Training Epoch: 37 [17920/50176]	Loss: 0.6530
Training Epoch: 37 [18176/50176]	Loss: 0.7418
Training Epoch: 37 [18432/50176]	Loss: 0.8179
Training Epoch: 37 [18688/50176]	Loss: 0.6971
Training Epoch: 37 [18944/50176]	Loss: 0.7812
Training Epoch: 37 [19200/50176]	Loss: 0.6774
Training Epoch: 37 [19456/50176]	Loss: 0.7770
Training Epoch: 37 [19712/50176]	Loss: 0.6855
Training Epoch: 37 [19968/50176]	Loss: 0.7274
Training Epoch: 37 [20224/50176]	Loss: 0.7923
Training Epoch: 37 [20480/50176]	Loss: 0.6700
Training Epoch: 37 [20736/50176]	Loss: 0.7951
Training Epoch: 37 [20992/50176]	Loss: 0.5970
Training Epoch: 37 [21248/50176]	Loss: 0.6708
Training Epoch: 37 [21504/50176]	Loss: 0.6651
Training Epoch: 37 [21760/50176]	Loss: 0.8430
Training Epoch: 37 [22016/50176]	Loss: 0.7531
Training Epoch: 37 [22272/50176]	Loss: 0.6012
Training Epoch: 37 [22528/50176]	Loss: 0.8376
Training Epoch: 37 [22784/50176]	Loss: 0.7294
Training Epoch: 37 [23040/50176]	Loss: 0.8562
Training Epoch: 37 [23296/50176]	Loss: 0.6800
Training Epoch: 37 [23552/50176]	Loss: 0.6983
Training Epoch: 37 [23808/50176]	Loss: 0.9420
Training Epoch: 37 [24064/50176]	Loss: 0.7150
Training Epoch: 37 [24320/50176]	Loss: 0.6314
Training Epoch: 37 [24576/50176]	Loss: 0.6742
Training Epoch: 37 [24832/50176]	Loss: 0.8730
Training Epoch: 37 [25088/50176]	Loss: 0.7810
Training Epoch: 37 [25344/50176]	Loss: 0.7682
Training Epoch: 37 [25600/50176]	Loss: 0.8253
Training Epoch: 37 [25856/50176]	Loss: 0.7578
Training Epoch: 37 [26112/50176]	Loss: 0.8161
Training Epoch: 37 [26368/50176]	Loss: 0.8129
Training Epoch: 37 [26624/50176]	Loss: 0.6856
Training Epoch: 37 [26880/50176]	Loss: 0.7514
Training Epoch: 37 [27136/50176]	Loss: 0.8266
Training Epoch: 37 [27392/50176]	Loss: 0.6340
Training Epoch: 37 [27648/50176]	Loss: 0.9004
Training Epoch: 37 [27904/50176]	Loss: 0.7484
Training Epoch: 37 [28160/50176]	Loss: 0.7817
Training Epoch: 37 [28416/50176]	Loss: 0.7866
Training Epoch: 37 [28672/50176]	Loss: 0.7866
Training Epoch: 37 [28928/50176]	Loss: 0.8164
Training Epoch: 37 [29184/50176]	Loss: 0.7456
Training Epoch: 37 [29440/50176]	Loss: 0.8030
Training Epoch: 37 [29696/50176]	Loss: 0.8073
Training Epoch: 37 [29952/50176]	Loss: 0.8693
Training Epoch: 37 [30208/50176]	Loss: 0.7431
Training Epoch: 37 [30464/50176]	Loss: 0.7389
Training Epoch: 37 [30720/50176]	Loss: 0.8000
Training Epoch: 37 [30976/50176]	Loss: 0.7596
Training Epoch: 37 [31232/50176]	Loss: 0.7744
Training Epoch: 37 [31488/50176]	Loss: 0.7315
Training Epoch: 37 [31744/50176]	Loss: 0.7447
Training Epoch: 37 [32000/50176]	Loss: 0.8688
Training Epoch: 37 [32256/50176]	Loss: 0.6909
Training Epoch: 37 [32512/50176]	Loss: 0.7002
Training Epoch: 37 [32768/50176]	Loss: 0.6917
Training Epoch: 37 [33024/50176]	Loss: 0.8860
Training Epoch: 37 [33280/50176]	Loss: 0.7845
Training Epoch: 37 [33536/50176]	Loss: 0.7574
Training Epoch: 37 [33792/50176]	Loss: 0.7367
Training Epoch: 37 [34048/50176]	Loss: 0.9048
Training Epoch: 37 [34304/50176]	Loss: 0.8103
Training Epoch: 37 [34560/50176]	Loss: 0.6950
Training Epoch: 37 [34816/50176]	Loss: 0.8158
Training Epoch: 37 [35072/50176]	Loss: 0.9150
Training Epoch: 37 [35328/50176]	Loss: 0.7193
Training Epoch: 37 [35584/50176]	Loss: 0.8162
Training Epoch: 37 [35840/50176]	Loss: 0.7598
Training Epoch: 37 [36096/50176]	Loss: 0.9330
Training Epoch: 37 [36352/50176]	Loss: 0.8290
Training Epoch: 37 [36608/50176]	Loss: 0.7981
Training Epoch: 37 [36864/50176]	Loss: 0.7534
Training Epoch: 37 [37120/50176]	Loss: 0.8225
Training Epoch: 37 [37376/50176]	Loss: 0.8472
Training Epoch: 37 [37632/50176]	Loss: 0.9548
Training Epoch: 37 [37888/50176]	Loss: 0.7149
Training Epoch: 37 [38144/50176]	Loss: 0.8784
Training Epoch: 37 [38400/50176]	Loss: 0.7212
Training Epoch: 37 [38656/50176]	Loss: 0.6662
Training Epoch: 37 [38912/50176]	Loss: 0.6228
Training Epoch: 37 [39168/50176]	Loss: 0.7627
Training Epoch: 37 [39424/50176]	Loss: 0.8688
Training Epoch: 37 [39680/50176]	Loss: 0.8493
Training Epoch: 37 [39936/50176]	Loss: 0.7842
Training Epoch: 37 [40192/50176]	Loss: 0.7568
Training Epoch: 37 [40448/50176]	Loss: 0.7362
Training Epoch: 37 [40704/50176]	Loss: 0.8646
Training Epoch: 37 [40960/50176]	Loss: 0.8018
Training Epoch: 37 [41216/50176]	Loss: 0.6856
Training Epoch: 37 [41472/50176]	Loss: 0.8964
Training Epoch: 37 [41728/50176]	Loss: 0.7388
Training Epoch: 37 [41984/50176]	Loss: 0.8560
Training Epoch: 37 [42240/50176]	Loss: 0.9051
Training Epoch: 37 [42496/50176]	Loss: 0.7742
Training Epoch: 37 [42752/50176]	Loss: 0.8972
Training Epoch: 37 [43008/50176]	Loss: 0.7732
Training Epoch: 37 [43264/50176]	Loss: 0.8730
Training Epoch: 37 [43520/50176]	Loss: 0.8382
Training Epoch: 37 [43776/50176]	Loss: 0.8696
Training Epoch: 37 [44032/50176]	Loss: 0.7454
Training Epoch: 37 [44288/50176]	Loss: 0.7989
Training Epoch: 37 [44544/50176]	Loss: 0.7258
Training Epoch: 37 [44800/50176]	Loss: 0.8605
Training Epoch: 37 [45056/50176]	Loss: 0.9045
Training Epoch: 37 [45312/50176]	Loss: 0.8311
Training Epoch: 37 [45568/50176]	Loss: 0.8653
Training Epoch: 37 [45824/50176]	Loss: 0.6720
Training Epoch: 37 [46080/50176]	Loss: 0.7410
Training Epoch: 37 [46336/50176]	Loss: 0.8098
Training Epoch: 37 [46592/50176]	Loss: 0.7115
Training Epoch: 37 [46848/50176]	Loss: 0.8338
Training Epoch: 37 [47104/50176]	Loss: 0.7878
Training Epoch: 37 [47360/50176]	Loss: 0.6618
Training Epoch: 37 [47616/50176]	Loss: 0.7611
Training Epoch: 37 [47872/50176]	Loss: 0.7315
Training Epoch: 37 [48128/50176]	Loss: 0.7920
Training Epoch: 37 [48384/50176]	Loss: 0.8601
Training Epoch: 37 [48640/50176]	Loss: 0.9041
Training Epoch: 37 [48896/50176]	Loss: 0.7811
Training Epoch: 37 [49152/50176]	Loss: 0.7749
Training Epoch: 37 [49408/50176]	Loss: 0.8928
Training Epoch: 37 [49664/50176]	Loss: 0.8613
Training Epoch: 37 [49920/50176]	Loss: 0.6720
Training Epoch: 37 [50176/50176]	Loss: 0.5972
Validation Epoch: 37, Average loss: 0.0102, Accuracy: 0.4797
Training Epoch: 38 [256/50176]	Loss: 0.5749
Training Epoch: 38 [512/50176]	Loss: 0.6643
Training Epoch: 38 [768/50176]	Loss: 0.6251
Training Epoch: 38 [1024/50176]	Loss: 0.6491
Training Epoch: 38 [1280/50176]	Loss: 0.8045
Training Epoch: 38 [1536/50176]	Loss: 0.5889
Training Epoch: 38 [1792/50176]	Loss: 0.6148
Training Epoch: 38 [2048/50176]	Loss: 0.6864
Training Epoch: 38 [2304/50176]	Loss: 0.6874
Training Epoch: 38 [2560/50176]	Loss: 0.6301
Training Epoch: 38 [2816/50176]	Loss: 0.6231
Training Epoch: 38 [3072/50176]	Loss: 0.7986
Training Epoch: 38 [3328/50176]	Loss: 0.6915
Training Epoch: 38 [3584/50176]	Loss: 0.6171
Training Epoch: 38 [3840/50176]	Loss: 0.6289
Training Epoch: 38 [4096/50176]	Loss: 0.8011
Training Epoch: 38 [4352/50176]	Loss: 0.6931
Training Epoch: 38 [4608/50176]	Loss: 0.6328
Training Epoch: 38 [4864/50176]	Loss: 0.5628
Training Epoch: 38 [5120/50176]	Loss: 0.6585
Training Epoch: 38 [5376/50176]	Loss: 0.7061
Training Epoch: 38 [5632/50176]	Loss: 0.6026
Training Epoch: 38 [5888/50176]	Loss: 0.6910
Training Epoch: 38 [6144/50176]	Loss: 0.8086
Training Epoch: 38 [6400/50176]	Loss: 0.7916
Training Epoch: 38 [6656/50176]	Loss: 0.6534
Training Epoch: 38 [6912/50176]	Loss: 0.5837
Training Epoch: 38 [7168/50176]	Loss: 0.5807
Training Epoch: 38 [7424/50176]	Loss: 0.8024
Training Epoch: 38 [7680/50176]	Loss: 0.6923
Training Epoch: 38 [7936/50176]	Loss: 0.6719
Training Epoch: 38 [8192/50176]	Loss: 0.7502
Training Epoch: 38 [8448/50176]	Loss: 0.7279
Training Epoch: 38 [8704/50176]	Loss: 0.7526
Training Epoch: 38 [8960/50176]	Loss: 0.6931
Training Epoch: 38 [9216/50176]	Loss: 0.6815
Training Epoch: 38 [9472/50176]	Loss: 0.7499
Training Epoch: 38 [9728/50176]	Loss: 0.7336
Training Epoch: 38 [9984/50176]	Loss: 0.7665
Training Epoch: 38 [10240/50176]	Loss: 0.7613
Training Epoch: 38 [10496/50176]	Loss: 0.6424
Training Epoch: 38 [10752/50176]	Loss: 0.8941
Training Epoch: 38 [11008/50176]	Loss: 0.6383
Training Epoch: 38 [11264/50176]	Loss: 0.7453
Training Epoch: 38 [11520/50176]	Loss: 0.7795
Training Epoch: 38 [11776/50176]	Loss: 0.6201
Training Epoch: 38 [12032/50176]	Loss: 0.7514
Training Epoch: 38 [12288/50176]	Loss: 0.6863
Training Epoch: 38 [12544/50176]	Loss: 0.7946
Training Epoch: 38 [12800/50176]	Loss: 0.5665
Training Epoch: 38 [13056/50176]	Loss: 0.8073
Training Epoch: 38 [13312/50176]	Loss: 0.6373
Training Epoch: 38 [13568/50176]	Loss: 0.6289
Training Epoch: 38 [13824/50176]	Loss: 0.6842
Training Epoch: 38 [14080/50176]	Loss: 0.6838
Training Epoch: 38 [14336/50176]	Loss: 0.7851
Training Epoch: 38 [14592/50176]	Loss: 0.8043
Training Epoch: 38 [14848/50176]	Loss: 0.6458
Training Epoch: 38 [15104/50176]	Loss: 0.7373
Training Epoch: 38 [15360/50176]	Loss: 0.6807
Training Epoch: 38 [15616/50176]	Loss: 0.7969
Training Epoch: 38 [15872/50176]	Loss: 0.6901
Training Epoch: 38 [16128/50176]	Loss: 0.5687
Training Epoch: 38 [16384/50176]	Loss: 0.8486
Training Epoch: 38 [16640/50176]	Loss: 0.9085
Training Epoch: 38 [16896/50176]	Loss: 0.8600
Training Epoch: 38 [17152/50176]	Loss: 0.7430
Training Epoch: 38 [17408/50176]	Loss: 0.7027
Training Epoch: 38 [17664/50176]	Loss: 0.6764
Training Epoch: 38 [17920/50176]	Loss: 0.7161
Training Epoch: 38 [18176/50176]	Loss: 0.7238
Training Epoch: 38 [18432/50176]	Loss: 0.7933
Training Epoch: 38 [18688/50176]	Loss: 0.6666
Training Epoch: 38 [18944/50176]	Loss: 0.7042
Training Epoch: 38 [19200/50176]	Loss: 0.7885
Training Epoch: 38 [19456/50176]	Loss: 0.6474
Training Epoch: 38 [19712/50176]	Loss: 0.7003
Training Epoch: 38 [19968/50176]	Loss: 0.8093
Training Epoch: 38 [20224/50176]	Loss: 0.8156
Training Epoch: 38 [20480/50176]	Loss: 0.6666
Training Epoch: 38 [20736/50176]	Loss: 0.7841
Training Epoch: 38 [20992/50176]	Loss: 0.8108
Training Epoch: 38 [21248/50176]	Loss: 0.8520
Training Epoch: 38 [21504/50176]	Loss: 0.7847
Training Epoch: 38 [21760/50176]	Loss: 0.7364
Training Epoch: 38 [22016/50176]	Loss: 0.6446
Training Epoch: 38 [22272/50176]	Loss: 0.6249
Training Epoch: 38 [22528/50176]	Loss: 0.7005
Training Epoch: 38 [22784/50176]	Loss: 0.7194
Training Epoch: 38 [23040/50176]	Loss: 0.7607
Training Epoch: 38 [23296/50176]	Loss: 0.6890
Training Epoch: 38 [23552/50176]	Loss: 0.7029
Training Epoch: 38 [23808/50176]	Loss: 0.6918
Training Epoch: 38 [24064/50176]	Loss: 0.8232
Training Epoch: 38 [24320/50176]	Loss: 0.6917
Training Epoch: 38 [24576/50176]	Loss: 0.8343
Training Epoch: 38 [24832/50176]	Loss: 0.7371
Training Epoch: 38 [25088/50176]	Loss: 0.5683
Training Epoch: 38 [25344/50176]	Loss: 0.6852
Training Epoch: 38 [25600/50176]	Loss: 0.8233
Training Epoch: 38 [25856/50176]	Loss: 0.6427
Training Epoch: 38 [26112/50176]	Loss: 0.9029
Training Epoch: 38 [26368/50176]	Loss: 0.6579
Training Epoch: 38 [26624/50176]	Loss: 0.6411
Training Epoch: 38 [26880/50176]	Loss: 0.8199
Training Epoch: 38 [27136/50176]	Loss: 0.7584
Training Epoch: 38 [27392/50176]	Loss: 0.8771
Training Epoch: 38 [27648/50176]	Loss: 0.8154
Training Epoch: 38 [27904/50176]	Loss: 0.7785
Training Epoch: 38 [28160/50176]	Loss: 0.7986
Training Epoch: 38 [28416/50176]	Loss: 0.7604
Training Epoch: 38 [28672/50176]	Loss: 0.7010
Training Epoch: 38 [28928/50176]	Loss: 0.7906
Training Epoch: 38 [29184/50176]	Loss: 0.8662
Training Epoch: 38 [29440/50176]	Loss: 0.7252
Training Epoch: 38 [29696/50176]	Loss: 0.6322
Training Epoch: 38 [29952/50176]	Loss: 0.7551
Training Epoch: 38 [30208/50176]	Loss: 0.8206
Training Epoch: 38 [30464/50176]	Loss: 0.7307
Training Epoch: 38 [30720/50176]	Loss: 0.6301
Training Epoch: 38 [30976/50176]	Loss: 0.8364
Training Epoch: 38 [31232/50176]	Loss: 0.9106
Training Epoch: 38 [31488/50176]	Loss: 0.7031
Training Epoch: 38 [31744/50176]	Loss: 0.5981
Training Epoch: 38 [32000/50176]	Loss: 0.7280
Training Epoch: 38 [32256/50176]	Loss: 0.7179
Training Epoch: 38 [32512/50176]	Loss: 0.6818
Training Epoch: 38 [32768/50176]	Loss: 0.8228
Training Epoch: 38 [33024/50176]	Loss: 0.8508
Training Epoch: 38 [33280/50176]	Loss: 0.7103
Training Epoch: 38 [33536/50176]	Loss: 0.7140
Training Epoch: 38 [33792/50176]	Loss: 0.8087
Training Epoch: 38 [34048/50176]	Loss: 0.7199
Training Epoch: 38 [34304/50176]	Loss: 0.6847
Training Epoch: 38 [34560/50176]	Loss: 0.9523
Training Epoch: 38 [34816/50176]	Loss: 0.6998
Training Epoch: 38 [35072/50176]	Loss: 0.6982
Training Epoch: 38 [35328/50176]	Loss: 0.7884
Training Epoch: 38 [35584/50176]	Loss: 0.9243
Training Epoch: 38 [35840/50176]	Loss: 0.7321
Training Epoch: 38 [36096/50176]	Loss: 0.6658
Training Epoch: 38 [36352/50176]	Loss: 0.8492
Training Epoch: 38 [36608/50176]	Loss: 0.6622
Training Epoch: 38 [36864/50176]	Loss: 0.8557
Training Epoch: 38 [37120/50176]	Loss: 0.7428
Training Epoch: 38 [37376/50176]	Loss: 0.6839
Training Epoch: 38 [37632/50176]	Loss: 0.8060
Training Epoch: 38 [37888/50176]	Loss: 0.7513
Training Epoch: 38 [38144/50176]	Loss: 0.7164
Training Epoch: 38 [38400/50176]	Loss: 0.8252
Training Epoch: 38 [38656/50176]	Loss: 0.6333
Training Epoch: 38 [38912/50176]	Loss: 0.7655
Training Epoch: 38 [39168/50176]	Loss: 0.7682
Training Epoch: 38 [39424/50176]	Loss: 0.6388
Training Epoch: 38 [39680/50176]	Loss: 0.7821
Training Epoch: 38 [39936/50176]	Loss: 0.7462
Training Epoch: 38 [40192/50176]	Loss: 0.8824
Training Epoch: 38 [40448/50176]	Loss: 0.5988
Training Epoch: 38 [40704/50176]	Loss: 0.8251
Training Epoch: 38 [40960/50176]	Loss: 0.7534
Training Epoch: 38 [41216/50176]	Loss: 0.7136
Training Epoch: 38 [41472/50176]	Loss: 0.7418
Training Epoch: 38 [41728/50176]	Loss: 0.7456
Training Epoch: 38 [41984/50176]	Loss: 0.7130
Training Epoch: 38 [42240/50176]	Loss: 0.7523
Training Epoch: 38 [42496/50176]	Loss: 0.8861
Training Epoch: 38 [42752/50176]	Loss: 0.7211
Training Epoch: 38 [43008/50176]	Loss: 0.7437
Training Epoch: 38 [43264/50176]	Loss: 0.8309
Training Epoch: 38 [43520/50176]	Loss: 0.8410
Training Epoch: 38 [43776/50176]	Loss: 0.6946
Training Epoch: 38 [44032/50176]	Loss: 0.7800
Training Epoch: 38 [44288/50176]	Loss: 0.6886
Training Epoch: 38 [44544/50176]	Loss: 0.8403
Training Epoch: 38 [44800/50176]	Loss: 0.8315
Training Epoch: 38 [45056/50176]	Loss: 0.8396
Training Epoch: 38 [45312/50176]	Loss: 0.9655
Training Epoch: 38 [45568/50176]	Loss: 0.8673
Training Epoch: 38 [45824/50176]	Loss: 0.9757
Training Epoch: 38 [46080/50176]	Loss: 0.7496
Training Epoch: 38 [46336/50176]	Loss: 0.6755
Training Epoch: 38 [46592/50176]	Loss: 0.8290
Training Epoch: 38 [46848/50176]	Loss: 0.7931
Training Epoch: 38 [47104/50176]	Loss: 0.8664
Training Epoch: 38 [47360/50176]	Loss: 0.8995
Training Epoch: 38 [47616/50176]	Loss: 0.7633
Training Epoch: 38 [47872/50176]	Loss: 0.6254
Training Epoch: 38 [48128/50176]	Loss: 0.7558
Training Epoch: 38 [48384/50176]	Loss: 0.6707
Training Epoch: 38 [48640/50176]	Loss: 0.9009
Training Epoch: 38 [48896/50176]	Loss: 0.6306
Training Epoch: 38 [49152/50176]	Loss: 0.8295
Training Epoch: 38 [49408/50176]	Loss: 0.7862
Training Epoch: 38 [49664/50176]	Loss: 0.7872
Training Epoch: 38 [49920/50176]	Loss: 0.7544
Training Epoch: 38 [50176/50176]	Loss: 0.6211
Validation Epoch: 38, Average loss: 0.0076, Accuracy: 0.5538
Training Epoch: 39 [256/50176]	Loss: 0.7999
Training Epoch: 39 [512/50176]	Loss: 0.7239
Training Epoch: 39 [768/50176]	Loss: 0.6740
Training Epoch: 39 [1024/50176]	Loss: 0.8395
Training Epoch: 39 [1280/50176]	Loss: 0.6832
Training Epoch: 39 [1536/50176]	Loss: 0.6438
Training Epoch: 39 [1792/50176]	Loss: 0.7132
Training Epoch: 39 [2048/50176]	Loss: 0.5497
Training Epoch: 39 [2304/50176]	Loss: 0.6465
Training Epoch: 39 [2560/50176]	Loss: 0.6261
Training Epoch: 39 [2816/50176]	Loss: 0.6642
Training Epoch: 39 [3072/50176]	Loss: 0.6935
Training Epoch: 39 [3328/50176]	Loss: 0.6637
Training Epoch: 39 [3584/50176]	Loss: 0.6341
Training Epoch: 39 [3840/50176]	Loss: 0.6336
Training Epoch: 39 [4096/50176]	Loss: 0.5998
Training Epoch: 39 [4352/50176]	Loss: 0.6329
Training Epoch: 39 [4608/50176]	Loss: 0.6715
Training Epoch: 39 [4864/50176]	Loss: 0.6167
Training Epoch: 39 [5120/50176]	Loss: 0.6958
Training Epoch: 39 [5376/50176]	Loss: 0.6972
Training Epoch: 39 [5632/50176]	Loss: 0.6952
Training Epoch: 39 [5888/50176]	Loss: 0.7119
Training Epoch: 39 [6144/50176]	Loss: 0.6998
Training Epoch: 39 [6400/50176]	Loss: 0.6011
Training Epoch: 39 [6656/50176]	Loss: 0.6395
Training Epoch: 39 [6912/50176]	Loss: 0.6090
Training Epoch: 39 [7168/50176]	Loss: 0.5801
Training Epoch: 39 [7424/50176]	Loss: 0.8103
Training Epoch: 39 [7680/50176]	Loss: 0.6209
Training Epoch: 39 [7936/50176]	Loss: 0.7874
Training Epoch: 39 [8192/50176]	Loss: 0.7955
Training Epoch: 39 [8448/50176]	Loss: 0.6284
Training Epoch: 39 [8704/50176]	Loss: 0.7573
Training Epoch: 39 [8960/50176]	Loss: 0.7127
Training Epoch: 39 [9216/50176]	Loss: 0.7414
Training Epoch: 39 [9472/50176]	Loss: 0.6358
Training Epoch: 39 [9728/50176]	Loss: 0.7829
Training Epoch: 39 [9984/50176]	Loss: 0.8186
Training Epoch: 39 [10240/50176]	Loss: 0.6823
Training Epoch: 39 [10496/50176]	Loss: 0.6409
Training Epoch: 39 [10752/50176]	Loss: 0.5922
Training Epoch: 39 [11008/50176]	Loss: 0.6237
Training Epoch: 39 [11264/50176]	Loss: 0.7292
Training Epoch: 39 [11520/50176]	Loss: 0.6980
Training Epoch: 39 [11776/50176]	Loss: 0.7419
Training Epoch: 39 [12032/50176]	Loss: 0.6615
Training Epoch: 39 [12288/50176]	Loss: 0.6471
Training Epoch: 39 [12544/50176]	Loss: 0.6910
Training Epoch: 39 [12800/50176]	Loss: 0.7638
Training Epoch: 39 [13056/50176]	Loss: 0.7490
Training Epoch: 39 [13312/50176]	Loss: 0.7241
Training Epoch: 39 [13568/50176]	Loss: 0.6297
Training Epoch: 39 [13824/50176]	Loss: 0.6114
Training Epoch: 39 [14080/50176]	Loss: 0.6409
Training Epoch: 39 [14336/50176]	Loss: 0.7181
Training Epoch: 39 [14592/50176]	Loss: 0.7687
Training Epoch: 39 [14848/50176]	Loss: 0.6304
Training Epoch: 39 [15104/50176]	Loss: 0.7779
Training Epoch: 39 [15360/50176]	Loss: 0.7291
Training Epoch: 39 [15616/50176]	Loss: 0.7251
Training Epoch: 39 [15872/50176]	Loss: 0.6271
Training Epoch: 39 [16128/50176]	Loss: 0.7090
Training Epoch: 39 [16384/50176]	Loss: 0.6753
Training Epoch: 39 [16640/50176]	Loss: 0.7275
Training Epoch: 39 [16896/50176]	Loss: 0.6697
Training Epoch: 39 [17152/50176]	Loss: 0.5444
Training Epoch: 39 [17408/50176]	Loss: 0.7354
Training Epoch: 39 [17664/50176]	Loss: 0.7316
Training Epoch: 39 [17920/50176]	Loss: 0.7521
Training Epoch: 39 [18176/50176]	Loss: 0.8114
Training Epoch: 39 [18432/50176]	Loss: 0.6475
Training Epoch: 39 [18688/50176]	Loss: 0.7207
Training Epoch: 39 [18944/50176]	Loss: 0.7476
Training Epoch: 39 [19200/50176]	Loss: 0.8483
Training Epoch: 39 [19456/50176]	Loss: 0.6400
Training Epoch: 39 [19712/50176]	Loss: 0.6503
Training Epoch: 39 [19968/50176]	Loss: 0.6732
Training Epoch: 39 [20224/50176]	Loss: 0.8038
Training Epoch: 39 [20480/50176]	Loss: 0.9070
Training Epoch: 39 [20736/50176]	Loss: 0.6673
Training Epoch: 39 [20992/50176]	Loss: 0.8680
Training Epoch: 39 [21248/50176]	Loss: 0.7743
Training Epoch: 39 [21504/50176]	Loss: 0.7913
Training Epoch: 39 [21760/50176]	Loss: 0.7217
Training Epoch: 39 [22016/50176]	Loss: 0.6885
Training Epoch: 39 [22272/50176]	Loss: 0.6374
Training Epoch: 39 [22528/50176]	Loss: 0.6220
Training Epoch: 39 [22784/50176]	Loss: 0.7695
Training Epoch: 39 [23040/50176]	Loss: 0.6232
Training Epoch: 39 [23296/50176]	Loss: 0.7176
Training Epoch: 39 [23552/50176]	Loss: 0.6517
Training Epoch: 39 [23808/50176]	Loss: 0.8005
Training Epoch: 39 [24064/50176]	Loss: 0.6545
Training Epoch: 39 [24320/50176]	Loss: 0.7126
Training Epoch: 39 [24576/50176]	Loss: 0.7175
Training Epoch: 39 [24832/50176]	Loss: 0.6938
Training Epoch: 39 [25088/50176]	Loss: 0.8228
Training Epoch: 39 [25344/50176]	Loss: 0.6843
Training Epoch: 39 [25600/50176]	Loss: 0.6654
Training Epoch: 39 [25856/50176]	Loss: 0.6873
Training Epoch: 39 [26112/50176]	Loss: 0.7450
Training Epoch: 39 [26368/50176]	Loss: 0.8288
Training Epoch: 39 [26624/50176]	Loss: 0.7744
Training Epoch: 39 [26880/50176]	Loss: 0.7711
Training Epoch: 39 [27136/50176]	Loss: 0.7143
Training Epoch: 39 [27392/50176]	Loss: 0.6465
Training Epoch: 39 [27648/50176]	Loss: 0.7772
Training Epoch: 39 [27904/50176]	Loss: 1.0088
Training Epoch: 39 [28160/50176]	Loss: 0.7239
Training Epoch: 39 [28416/50176]	Loss: 0.7921
Training Epoch: 39 [28672/50176]	Loss: 0.8209
Training Epoch: 39 [28928/50176]	Loss: 0.7791
Training Epoch: 39 [29184/50176]	Loss: 0.6446
Training Epoch: 39 [29440/50176]	Loss: 0.7746
Training Epoch: 39 [29696/50176]	Loss: 0.7613
Training Epoch: 39 [29952/50176]	Loss: 0.7602
Training Epoch: 39 [30208/50176]	Loss: 0.8014
Training Epoch: 39 [30464/50176]	Loss: 0.7624
Training Epoch: 39 [30720/50176]	Loss: 0.7693
Training Epoch: 39 [30976/50176]	Loss: 0.6600
Training Epoch: 39 [31232/50176]	Loss: 0.6803
Training Epoch: 39 [31488/50176]	Loss: 0.8154
Training Epoch: 39 [31744/50176]	Loss: 0.7052
Training Epoch: 39 [32000/50176]	Loss: 0.8056
Training Epoch: 39 [32256/50176]	Loss: 0.8172
Training Epoch: 39 [32512/50176]	Loss: 0.7642
Training Epoch: 39 [32768/50176]	Loss: 0.7737
Training Epoch: 39 [33024/50176]	Loss: 0.6099
Training Epoch: 39 [33280/50176]	Loss: 0.6615
Training Epoch: 39 [33536/50176]	Loss: 0.9049
Training Epoch: 39 [33792/50176]	Loss: 0.7762
Training Epoch: 39 [34048/50176]	Loss: 0.7729
Training Epoch: 39 [34304/50176]	Loss: 0.8658
Training Epoch: 39 [34560/50176]	Loss: 0.7597
Training Epoch: 39 [34816/50176]	Loss: 0.7021
Training Epoch: 39 [35072/50176]	Loss: 0.7057
Training Epoch: 39 [35328/50176]	Loss: 0.7423
Training Epoch: 39 [35584/50176]	Loss: 0.7893
Training Epoch: 39 [35840/50176]	Loss: 0.6641
Training Epoch: 39 [36096/50176]	Loss: 0.7993
Training Epoch: 39 [36352/50176]	Loss: 0.7860
Training Epoch: 39 [36608/50176]	Loss: 0.8303
Training Epoch: 39 [36864/50176]	Loss: 0.6903
Training Epoch: 39 [37120/50176]	Loss: 0.8197
Training Epoch: 39 [37376/50176]	Loss: 0.7220
Training Epoch: 39 [37632/50176]	Loss: 0.6805
Training Epoch: 39 [37888/50176]	Loss: 0.6808
Training Epoch: 39 [38144/50176]	Loss: 0.8030
Training Epoch: 39 [38400/50176]	Loss: 0.6526
Training Epoch: 39 [38656/50176]	Loss: 0.6741
Training Epoch: 39 [38912/50176]	Loss: 0.7309
Training Epoch: 39 [39168/50176]	Loss: 0.7905
Training Epoch: 39 [39424/50176]	Loss: 0.7876
Training Epoch: 39 [39680/50176]	Loss: 0.6428
Training Epoch: 39 [39936/50176]	Loss: 0.7835
Training Epoch: 39 [40192/50176]	Loss: 0.8888
Training Epoch: 39 [40448/50176]	Loss: 0.7597
Training Epoch: 39 [40704/50176]	Loss: 0.6898
Training Epoch: 39 [40960/50176]	Loss: 0.7998
Training Epoch: 39 [41216/50176]	Loss: 0.7522
Training Epoch: 39 [41472/50176]	Loss: 0.6694
Training Epoch: 39 [41728/50176]	Loss: 0.7416
Training Epoch: 39 [41984/50176]	Loss: 0.8093
Training Epoch: 39 [42240/50176]	Loss: 0.7895
Training Epoch: 39 [42496/50176]	Loss: 0.7805
Training Epoch: 39 [42752/50176]	Loss: 0.7613
Training Epoch: 39 [43008/50176]	Loss: 0.6826
Training Epoch: 39 [43264/50176]	Loss: 0.7405
Training Epoch: 39 [43520/50176]	Loss: 0.7317
Training Epoch: 39 [43776/50176]	Loss: 0.7159
Training Epoch: 39 [44032/50176]	Loss: 0.7201
Training Epoch: 39 [44288/50176]	Loss: 0.8253
Training Epoch: 39 [44544/50176]	Loss: 0.8727
Training Epoch: 39 [44800/50176]	Loss: 0.8864
Training Epoch: 39 [45056/50176]	Loss: 0.7333
Training Epoch: 39 [45312/50176]	Loss: 0.6658
Training Epoch: 39 [45568/50176]	Loss: 0.7651
Training Epoch: 39 [45824/50176]	Loss: 0.7095
Training Epoch: 39 [46080/50176]	Loss: 0.7139
Training Epoch: 39 [46336/50176]	Loss: 0.6940
Training Epoch: 39 [46592/50176]	Loss: 0.8955
Training Epoch: 39 [46848/50176]	Loss: 0.8599
Training Epoch: 39 [47104/50176]	Loss: 0.6310
Training Epoch: 39 [47360/50176]	Loss: 0.7540
Training Epoch: 39 [47616/50176]	Loss: 0.8508
Training Epoch: 39 [47872/50176]	Loss: 0.8126
Training Epoch: 39 [48128/50176]	Loss: 0.6597
Training Epoch: 39 [48384/50176]	Loss: 0.8678
Training Epoch: 39 [48640/50176]	Loss: 0.7034
Training Epoch: 39 [48896/50176]	Loss: 0.6964
Training Epoch: 39 [49152/50176]	Loss: 0.7340
Training Epoch: 39 [49408/50176]	Loss: 0.7025
Training Epoch: 39 [49664/50176]	Loss: 0.7048
Training Epoch: 39 [49920/50176]	Loss: 0.7864
Training Epoch: 39 [50176/50176]	Loss: 0.5456
Validation Epoch: 39, Average loss: 0.0086, Accuracy: 0.5257
Training Epoch: 40 [256/50176]	Loss: 0.7197
Training Epoch: 40 [512/50176]	Loss: 0.7563
Training Epoch: 40 [768/50176]	Loss: 0.6055
Training Epoch: 40 [1024/50176]	Loss: 0.6236
Training Epoch: 40 [1280/50176]	Loss: 0.8019
Training Epoch: 40 [1536/50176]	Loss: 0.6555
Training Epoch: 40 [1792/50176]	Loss: 0.6921
Training Epoch: 40 [2048/50176]	Loss: 0.6375
Training Epoch: 40 [2304/50176]	Loss: 0.5971
Training Epoch: 40 [2560/50176]	Loss: 0.6966
Training Epoch: 40 [2816/50176]	Loss: 0.6565
Training Epoch: 40 [3072/50176]	Loss: 0.7945
Training Epoch: 40 [3328/50176]	Loss: 0.6110
Training Epoch: 40 [3584/50176]	Loss: 0.7391
Training Epoch: 40 [3840/50176]	Loss: 0.7215
Training Epoch: 40 [4096/50176]	Loss: 0.7100
Training Epoch: 40 [4352/50176]	Loss: 0.7210
Training Epoch: 40 [4608/50176]	Loss: 0.5114
Training Epoch: 40 [4864/50176]	Loss: 0.6055
Training Epoch: 40 [5120/50176]	Loss: 0.7666
Training Epoch: 40 [5376/50176]	Loss: 0.5725
Training Epoch: 40 [5632/50176]	Loss: 0.5494
Training Epoch: 40 [5888/50176]	Loss: 0.7387
Training Epoch: 40 [6144/50176]	Loss: 0.6415
Training Epoch: 40 [6400/50176]	Loss: 0.7192
Training Epoch: 40 [6656/50176]	Loss: 0.8131
Training Epoch: 40 [6912/50176]	Loss: 0.7525
Training Epoch: 40 [7168/50176]	Loss: 0.6218
Training Epoch: 40 [7424/50176]	Loss: 0.5752
Training Epoch: 40 [7680/50176]	Loss: 0.7018
Training Epoch: 40 [7936/50176]	Loss: 0.5439
Training Epoch: 40 [8192/50176]	Loss: 0.6849
Training Epoch: 40 [8448/50176]	Loss: 0.7057
Training Epoch: 40 [8704/50176]	Loss: 0.6566
Training Epoch: 40 [8960/50176]	Loss: 0.6008
Training Epoch: 40 [9216/50176]	Loss: 0.6786
Training Epoch: 40 [9472/50176]	Loss: 0.6023
Training Epoch: 40 [9728/50176]	Loss: 0.6809
Training Epoch: 40 [9984/50176]	Loss: 0.7096
Training Epoch: 40 [10240/50176]	Loss: 0.7347
Training Epoch: 40 [10496/50176]	Loss: 0.7479
Training Epoch: 40 [10752/50176]	Loss: 0.6886
Training Epoch: 40 [11008/50176]	Loss: 0.6927
Training Epoch: 40 [11264/50176]	Loss: 0.6976
Training Epoch: 40 [11520/50176]	Loss: 0.6875
Training Epoch: 40 [11776/50176]	Loss: 0.6100
Training Epoch: 40 [12032/50176]	Loss: 0.6770
Training Epoch: 40 [12288/50176]	Loss: 0.6725
Training Epoch: 40 [12544/50176]	Loss: 0.6448
Training Epoch: 40 [12800/50176]	Loss: 0.7031
Training Epoch: 40 [13056/50176]	Loss: 0.7395
Training Epoch: 40 [13312/50176]	Loss: 0.5382
Training Epoch: 40 [13568/50176]	Loss: 0.7365
Training Epoch: 40 [13824/50176]	Loss: 0.7084
Training Epoch: 40 [14080/50176]	Loss: 0.6423
Training Epoch: 40 [14336/50176]	Loss: 0.7073
Training Epoch: 40 [14592/50176]	Loss: 0.8795
Training Epoch: 40 [14848/50176]	Loss: 0.7196
Training Epoch: 40 [15104/50176]	Loss: 0.6564
Training Epoch: 40 [15360/50176]	Loss: 0.7550
Training Epoch: 40 [15616/50176]	Loss: 0.7002
Training Epoch: 40 [15872/50176]	Loss: 0.6161
Training Epoch: 40 [16128/50176]	Loss: 0.7032
Training Epoch: 40 [16384/50176]	Loss: 0.6799
Training Epoch: 40 [16640/50176]	Loss: 0.6280
Training Epoch: 40 [16896/50176]	Loss: 0.7034
Training Epoch: 40 [17152/50176]	Loss: 0.5955
Training Epoch: 40 [17408/50176]	Loss: 0.5581
Training Epoch: 40 [17664/50176]	Loss: 0.7492
Training Epoch: 40 [17920/50176]	Loss: 0.7192
Training Epoch: 40 [18176/50176]	Loss: 0.6526
Training Epoch: 40 [18432/50176]	Loss: 0.7190
Training Epoch: 40 [18688/50176]	Loss: 0.6432
Training Epoch: 40 [18944/50176]	Loss: 0.6204
Training Epoch: 40 [19200/50176]	Loss: 0.7291
Training Epoch: 40 [19456/50176]	Loss: 0.6565
Training Epoch: 40 [19712/50176]	Loss: 0.7910
Training Epoch: 40 [19968/50176]	Loss: 0.6695
Training Epoch: 40 [20224/50176]	Loss: 0.6971
Training Epoch: 40 [20480/50176]	Loss: 0.6218
Training Epoch: 40 [20736/50176]	Loss: 0.7034
Training Epoch: 40 [20992/50176]	Loss: 0.6335
Training Epoch: 40 [21248/50176]	Loss: 0.6280
Training Epoch: 40 [21504/50176]	Loss: 0.7062
Training Epoch: 40 [21760/50176]	Loss: 0.6169
Training Epoch: 40 [22016/50176]	Loss: 0.7092
Training Epoch: 40 [22272/50176]	Loss: 0.7053
Training Epoch: 40 [22528/50176]	Loss: 0.7549
Training Epoch: 40 [22784/50176]	Loss: 0.6566
Training Epoch: 40 [23040/50176]	Loss: 0.8112
Training Epoch: 40 [23296/50176]	Loss: 0.5879
Training Epoch: 40 [23552/50176]	Loss: 0.8781
Training Epoch: 40 [23808/50176]	Loss: 0.6999
Training Epoch: 40 [24064/50176]	Loss: 0.7660
Training Epoch: 40 [24320/50176]	Loss: 0.7803
Training Epoch: 40 [24576/50176]	Loss: 0.8854
Training Epoch: 40 [24832/50176]	Loss: 0.8227
Training Epoch: 40 [25088/50176]	Loss: 0.6279
Training Epoch: 40 [25344/50176]	Loss: 0.7405
Training Epoch: 40 [25600/50176]	Loss: 0.6142
Training Epoch: 40 [25856/50176]	Loss: 0.6017
Training Epoch: 40 [26112/50176]	Loss: 0.7492
Training Epoch: 40 [26368/50176]	Loss: 0.6706
Training Epoch: 40 [26624/50176]	Loss: 0.6938
Training Epoch: 40 [26880/50176]	Loss: 0.7407
Training Epoch: 40 [27136/50176]	Loss: 0.8806
Training Epoch: 40 [27392/50176]	Loss: 0.6785
Training Epoch: 40 [27648/50176]	Loss: 0.6970
Training Epoch: 40 [27904/50176]	Loss: 0.6708
Training Epoch: 40 [28160/50176]	Loss: 0.7257
Training Epoch: 40 [28416/50176]	Loss: 0.7642
Training Epoch: 40 [28672/50176]	Loss: 0.7488
Training Epoch: 40 [28928/50176]	Loss: 0.6945
Training Epoch: 40 [29184/50176]	Loss: 0.7142
Training Epoch: 40 [29440/50176]	Loss: 0.7461
Training Epoch: 40 [29696/50176]	Loss: 0.7132
Training Epoch: 40 [29952/50176]	Loss: 0.7391
Training Epoch: 40 [30208/50176]	Loss: 0.7863
Training Epoch: 40 [30464/50176]	Loss: 0.6673
Training Epoch: 40 [30720/50176]	Loss: 0.7004
Training Epoch: 40 [30976/50176]	Loss: 0.9243
Training Epoch: 40 [31232/50176]	Loss: 0.6563
Training Epoch: 40 [31488/50176]	Loss: 0.5808
Training Epoch: 40 [31744/50176]	Loss: 0.6893
Training Epoch: 40 [32000/50176]	Loss: 0.9196
Training Epoch: 40 [32256/50176]	Loss: 0.6558
Training Epoch: 40 [32512/50176]	Loss: 0.7033
Training Epoch: 40 [32768/50176]	Loss: 0.7058
Training Epoch: 40 [33024/50176]	Loss: 0.7399
Training Epoch: 40 [33280/50176]	Loss: 0.6718
Training Epoch: 40 [33536/50176]	Loss: 0.6802
Training Epoch: 40 [33792/50176]	Loss: 0.7902
Training Epoch: 40 [34048/50176]	Loss: 0.6544
Training Epoch: 40 [34304/50176]	Loss: 0.8583
Training Epoch: 40 [34560/50176]	Loss: 0.9260
Training Epoch: 40 [34816/50176]	Loss: 0.6939
Training Epoch: 40 [35072/50176]	Loss: 0.7669
Training Epoch: 40 [35328/50176]	Loss: 0.7472
Training Epoch: 40 [35584/50176]	Loss: 0.7430
Training Epoch: 40 [35840/50176]	Loss: 0.5910
Training Epoch: 40 [36096/50176]	Loss: 0.8520
Training Epoch: 40 [36352/50176]	Loss: 0.6930
Training Epoch: 40 [36608/50176]	Loss: 0.7660
Training Epoch: 40 [36864/50176]	Loss: 0.8520
Training Epoch: 40 [37120/50176]	Loss: 0.7847
Training Epoch: 40 [37376/50176]	Loss: 0.6465
Training Epoch: 40 [37632/50176]	Loss: 0.7099
Training Epoch: 40 [37888/50176]	Loss: 0.6762
Training Epoch: 40 [38144/50176]	Loss: 0.7140
Training Epoch: 40 [38400/50176]	Loss: 0.7304
Training Epoch: 40 [38656/50176]	Loss: 0.7764
Training Epoch: 40 [38912/50176]	Loss: 0.7127
Training Epoch: 40 [39168/50176]	Loss: 0.6925
Training Epoch: 40 [39424/50176]	Loss: 0.6939
Training Epoch: 40 [39680/50176]	Loss: 0.7731
Training Epoch: 40 [39936/50176]	Loss: 0.7336
Training Epoch: 40 [40192/50176]	Loss: 0.8263
Training Epoch: 40 [40448/50176]	Loss: 0.7880
Training Epoch: 40 [40704/50176]	Loss: 0.9497
Training Epoch: 40 [40960/50176]	Loss: 0.8409
Training Epoch: 40 [41216/50176]	Loss: 0.8149
Training Epoch: 40 [41472/50176]	Loss: 0.6280
Training Epoch: 40 [41728/50176]	Loss: 0.7392
Training Epoch: 40 [41984/50176]	Loss: 0.6559
Training Epoch: 40 [42240/50176]	Loss: 0.6646
Training Epoch: 40 [42496/50176]	Loss: 0.7220
Training Epoch: 40 [42752/50176]	Loss: 0.7845
Training Epoch: 40 [43008/50176]	Loss: 0.7465
Training Epoch: 40 [43264/50176]	Loss: 0.6777
Training Epoch: 40 [43520/50176]	Loss: 0.7680
Training Epoch: 40 [43776/50176]	Loss: 0.8446
Training Epoch: 40 [44032/50176]	Loss: 0.6292
Training Epoch: 40 [44288/50176]	Loss: 0.5328
Training Epoch: 40 [44544/50176]	Loss: 0.7259
Training Epoch: 40 [44800/50176]	Loss: 0.7474
Training Epoch: 40 [45056/50176]	Loss: 0.6742
Training Epoch: 40 [45312/50176]	Loss: 0.8455
Training Epoch: 40 [45568/50176]	Loss: 0.6414
Training Epoch: 40 [45824/50176]	Loss: 0.7404
Training Epoch: 40 [46080/50176]	Loss: 0.7210
Training Epoch: 40 [46336/50176]	Loss: 0.6634
Training Epoch: 40 [46592/50176]	Loss: 0.7763
Training Epoch: 40 [46848/50176]	Loss: 0.7345
Training Epoch: 40 [47104/50176]	Loss: 0.9795
Training Epoch: 40 [47360/50176]	Loss: 0.7016
Training Epoch: 40 [47616/50176]	Loss: 0.7673
Training Epoch: 40 [47872/50176]	Loss: 0.7275
Training Epoch: 40 [48128/50176]	Loss: 0.7321
Training Epoch: 40 [48384/50176]	Loss: 0.7586
Training Epoch: 40 [48640/50176]	Loss: 0.8565
Training Epoch: 40 [48896/50176]	Loss: 0.9221
Training Epoch: 40 [49152/50176]	Loss: 0.6629
Training Epoch: 40 [49408/50176]	Loss: 0.6955
Training Epoch: 40 [49664/50176]	Loss: 0.8323
Training Epoch: 40 [49920/50176]	Loss: 0.7836
Training Epoch: 40 [50176/50176]	Loss: 0.6760
Validation Epoch: 40, Average loss: 0.0087, Accuracy: 0.5219
Training Epoch: 41 [256/50176]	Loss: 0.5762
Training Epoch: 41 [512/50176]	Loss: 0.6830
Training Epoch: 41 [768/50176]	Loss: 0.6142
Training Epoch: 41 [1024/50176]	Loss: 0.6765
Training Epoch: 41 [1280/50176]	Loss: 0.6843
Training Epoch: 41 [1536/50176]	Loss: 0.5729
Training Epoch: 41 [1792/50176]	Loss: 0.6691
Training Epoch: 41 [2048/50176]	Loss: 0.6636
Training Epoch: 41 [2304/50176]	Loss: 0.5905
Training Epoch: 41 [2560/50176]	Loss: 0.5487
Training Epoch: 41 [2816/50176]	Loss: 0.7094
Training Epoch: 41 [3072/50176]	Loss: 0.7029
Training Epoch: 41 [3328/50176]	Loss: 0.6879
Training Epoch: 41 [3584/50176]	Loss: 0.6083
Training Epoch: 41 [3840/50176]	Loss: 0.4930
Training Epoch: 41 [4096/50176]	Loss: 0.6604
Training Epoch: 41 [4352/50176]	Loss: 0.7147
Training Epoch: 41 [4608/50176]	Loss: 0.6807
Training Epoch: 41 [4864/50176]	Loss: 0.6793
Training Epoch: 41 [5120/50176]	Loss: 0.7081
Training Epoch: 41 [5376/50176]	Loss: 0.7072
Training Epoch: 41 [5632/50176]	Loss: 0.5767
Training Epoch: 41 [5888/50176]	Loss: 0.6517
Training Epoch: 41 [6144/50176]	Loss: 0.7830
Training Epoch: 41 [6400/50176]	Loss: 0.7058
Training Epoch: 41 [6656/50176]	Loss: 0.5676
Training Epoch: 41 [6912/50176]	Loss: 0.6737
Training Epoch: 41 [7168/50176]	Loss: 0.6348
Training Epoch: 41 [7424/50176]	Loss: 0.6265
Training Epoch: 41 [7680/50176]	Loss: 0.6878
Training Epoch: 41 [7936/50176]	Loss: 0.7272
Training Epoch: 41 [8192/50176]	Loss: 0.5443
Training Epoch: 41 [8448/50176]	Loss: 0.6390
Training Epoch: 41 [8704/50176]	Loss: 0.6441
Training Epoch: 41 [8960/50176]	Loss: 0.5876
Training Epoch: 41 [9216/50176]	Loss: 0.6735
Training Epoch: 41 [9472/50176]	Loss: 0.6751
Training Epoch: 41 [9728/50176]	Loss: 0.7084
Training Epoch: 41 [9984/50176]	Loss: 0.6877
Training Epoch: 41 [10240/50176]	Loss: 0.6935
Training Epoch: 41 [10496/50176]	Loss: 0.6080
Training Epoch: 41 [10752/50176]	Loss: 0.7686
Training Epoch: 41 [11008/50176]	Loss: 0.5851
Training Epoch: 41 [11264/50176]	Loss: 0.6658
Training Epoch: 41 [11520/50176]	Loss: 0.6803
Training Epoch: 41 [11776/50176]	Loss: 0.6040
Training Epoch: 41 [12032/50176]	Loss: 0.7860
Training Epoch: 41 [12288/50176]	Loss: 0.7351
Training Epoch: 41 [12544/50176]	Loss: 0.7474
Training Epoch: 41 [12800/50176]	Loss: 0.7170
Training Epoch: 41 [13056/50176]	Loss: 0.5161
Training Epoch: 41 [13312/50176]	Loss: 0.6877
Training Epoch: 41 [13568/50176]	Loss: 0.5885
Training Epoch: 41 [13824/50176]	Loss: 0.6425
Training Epoch: 41 [14080/50176]	Loss: 0.7260
Training Epoch: 41 [14336/50176]	Loss: 0.5840
Training Epoch: 41 [14592/50176]	Loss: 0.7954
Training Epoch: 41 [14848/50176]	Loss: 0.6031
Training Epoch: 41 [15104/50176]	Loss: 0.5770
Training Epoch: 41 [15360/50176]	Loss: 0.6927
Training Epoch: 41 [15616/50176]	Loss: 0.7050
Training Epoch: 41 [15872/50176]	Loss: 0.7252
Training Epoch: 41 [16128/50176]	Loss: 0.6783
Training Epoch: 41 [16384/50176]	Loss: 0.6575
Training Epoch: 41 [16640/50176]	Loss: 0.6153
Training Epoch: 41 [16896/50176]	Loss: 0.7145
Training Epoch: 41 [17152/50176]	Loss: 0.6226
Training Epoch: 41 [17408/50176]	Loss: 0.5741
Training Epoch: 41 [17664/50176]	Loss: 0.6708
Training Epoch: 41 [17920/50176]	Loss: 0.6785
Training Epoch: 41 [18176/50176]	Loss: 0.6000
Training Epoch: 41 [18432/50176]	Loss: 0.7182
Training Epoch: 41 [18688/50176]	Loss: 0.6169
Training Epoch: 41 [18944/50176]	Loss: 0.7591
Training Epoch: 41 [19200/50176]	Loss: 0.8022
Training Epoch: 41 [19456/50176]	Loss: 0.6818
Training Epoch: 41 [19712/50176]	Loss: 0.7636
Training Epoch: 41 [19968/50176]	Loss: 0.7199
Training Epoch: 41 [20224/50176]	Loss: 0.6833
Training Epoch: 41 [20480/50176]	Loss: 0.6390
Training Epoch: 41 [20736/50176]	Loss: 0.7630
Training Epoch: 41 [20992/50176]	Loss: 0.6815
Training Epoch: 41 [21248/50176]	Loss: 0.7426
Training Epoch: 41 [21504/50176]	Loss: 0.7223
Training Epoch: 41 [21760/50176]	Loss: 0.7253
Training Epoch: 41 [22016/50176]	Loss: 0.6107
Training Epoch: 41 [22272/50176]	Loss: 0.7211
Training Epoch: 41 [22528/50176]	Loss: 0.7120
Training Epoch: 41 [22784/50176]	Loss: 0.7627
Training Epoch: 41 [23040/50176]	Loss: 0.6688
Training Epoch: 41 [23296/50176]	Loss: 0.7134
Training Epoch: 41 [23552/50176]	Loss: 0.6214
Training Epoch: 41 [23808/50176]	Loss: 0.6375
Training Epoch: 41 [24064/50176]	Loss: 0.6550
Training Epoch: 41 [24320/50176]	Loss: 0.6638
Training Epoch: 41 [24576/50176]	Loss: 0.7491
Training Epoch: 41 [24832/50176]	Loss: 0.6495
Training Epoch: 41 [25088/50176]	Loss: 0.7027
Training Epoch: 41 [25344/50176]	Loss: 0.7070
Training Epoch: 41 [25600/50176]	Loss: 0.5601
Training Epoch: 41 [25856/50176]	Loss: 0.6909
Training Epoch: 41 [26112/50176]	Loss: 0.6301
Training Epoch: 41 [26368/50176]	Loss: 0.6785
Training Epoch: 41 [26624/50176]	Loss: 0.7501
Training Epoch: 41 [26880/50176]	Loss: 0.7365
Training Epoch: 41 [27136/50176]	Loss: 0.7575
Training Epoch: 41 [27392/50176]	Loss: 0.7988
Training Epoch: 41 [27648/50176]	Loss: 0.6742
Training Epoch: 41 [27904/50176]	Loss: 0.5936
Training Epoch: 41 [28160/50176]	Loss: 0.7435
Training Epoch: 41 [28416/50176]	Loss: 0.5973
Training Epoch: 41 [28672/50176]	Loss: 0.6586
Training Epoch: 41 [28928/50176]	Loss: 0.5896
Training Epoch: 41 [29184/50176]	Loss: 0.5775
Training Epoch: 41 [29440/50176]	Loss: 0.6787
Training Epoch: 41 [29696/50176]	Loss: 0.6858
Training Epoch: 41 [29952/50176]	Loss: 0.8370
Training Epoch: 41 [30208/50176]	Loss: 0.6169
Training Epoch: 41 [30464/50176]	Loss: 0.5100
Training Epoch: 41 [30720/50176]	Loss: 0.5872
Training Epoch: 41 [30976/50176]	Loss: 0.6378
Training Epoch: 41 [31232/50176]	Loss: 0.7116
Training Epoch: 41 [31488/50176]	Loss: 0.7668
Training Epoch: 41 [31744/50176]	Loss: 0.6595
Training Epoch: 41 [32000/50176]	Loss: 0.7141
Training Epoch: 41 [32256/50176]	Loss: 0.5956
Training Epoch: 41 [32512/50176]	Loss: 0.7326
Training Epoch: 41 [32768/50176]	Loss: 0.7081
Training Epoch: 41 [33024/50176]	Loss: 0.7231
Training Epoch: 41 [33280/50176]	Loss: 0.8500
Training Epoch: 41 [33536/50176]	Loss: 0.7450
Training Epoch: 41 [33792/50176]	Loss: 0.6729
Training Epoch: 41 [34048/50176]	Loss: 0.7896
Training Epoch: 41 [34304/50176]	Loss: 0.7803
Training Epoch: 41 [34560/50176]	Loss: 0.7444
Training Epoch: 41 [34816/50176]	Loss: 0.6805
Training Epoch: 41 [35072/50176]	Loss: 0.5404
Training Epoch: 41 [35328/50176]	Loss: 0.7007
Training Epoch: 41 [35584/50176]	Loss: 0.8417
Training Epoch: 41 [35840/50176]	Loss: 0.7031
Training Epoch: 41 [36096/50176]	Loss: 0.6422
Training Epoch: 41 [36352/50176]	Loss: 0.7934
Training Epoch: 41 [36608/50176]	Loss: 0.6862
Training Epoch: 41 [36864/50176]	Loss: 0.8628
Training Epoch: 41 [37120/50176]	Loss: 0.7370
Training Epoch: 41 [37376/50176]	Loss: 0.7803
Training Epoch: 41 [37632/50176]	Loss: 0.7278
Training Epoch: 41 [37888/50176]	Loss: 0.7119
Training Epoch: 41 [38144/50176]	Loss: 0.6890
Training Epoch: 41 [38400/50176]	Loss: 0.7154
Training Epoch: 41 [38656/50176]	Loss: 0.7527
Training Epoch: 41 [38912/50176]	Loss: 0.6478
Training Epoch: 41 [39168/50176]	Loss: 0.6899
Training Epoch: 41 [39424/50176]	Loss: 0.6059
Training Epoch: 41 [39680/50176]	Loss: 0.5462
Training Epoch: 41 [39936/50176]	Loss: 0.6702
Training Epoch: 41 [40192/50176]	Loss: 0.6477
Training Epoch: 41 [40448/50176]	Loss: 0.6637
Training Epoch: 41 [40704/50176]	Loss: 0.6141
Training Epoch: 41 [40960/50176]	Loss: 0.7459
Training Epoch: 41 [41216/50176]	Loss: 0.7632
Training Epoch: 41 [41472/50176]	Loss: 0.6877
Training Epoch: 41 [41728/50176]	Loss: 0.7729
Training Epoch: 41 [41984/50176]	Loss: 0.7455
Training Epoch: 41 [42240/50176]	Loss: 0.6902
Training Epoch: 41 [42496/50176]	Loss: 0.7626
Training Epoch: 41 [42752/50176]	Loss: 0.7423
Training Epoch: 41 [43008/50176]	Loss: 0.5958
Training Epoch: 41 [43264/50176]	Loss: 0.7235
Training Epoch: 41 [43520/50176]	Loss: 0.6792
Training Epoch: 41 [43776/50176]	Loss: 0.6069
Training Epoch: 41 [44032/50176]	Loss: 0.6433
Training Epoch: 41 [44288/50176]	Loss: 0.7054
Training Epoch: 41 [44544/50176]	Loss: 0.6947
Training Epoch: 41 [44800/50176]	Loss: 0.8426
Training Epoch: 41 [45056/50176]	Loss: 0.7553
Training Epoch: 41 [45312/50176]	Loss: 0.7983
Training Epoch: 41 [45568/50176]	Loss: 0.7275
Training Epoch: 41 [45824/50176]	Loss: 0.7136
Training Epoch: 41 [46080/50176]	Loss: 0.7911
Training Epoch: 41 [46336/50176]	Loss: 0.7529
Training Epoch: 41 [46592/50176]	Loss: 0.8440
Training Epoch: 41 [46848/50176]	Loss: 0.7107
Training Epoch: 41 [47104/50176]	Loss: 0.7986
Training Epoch: 41 [47360/50176]	Loss: 0.7344
Training Epoch: 41 [47616/50176]	Loss: 0.6764
Training Epoch: 41 [47872/50176]	Loss: 0.6477
Training Epoch: 41 [48128/50176]	Loss: 0.7584
Training Epoch: 41 [48384/50176]	Loss: 0.6369
Training Epoch: 41 [48640/50176]	Loss: 0.7185
Training Epoch: 41 [48896/50176]	Loss: 0.7919
Training Epoch: 41 [49152/50176]	Loss: 0.7116
Training Epoch: 41 [49408/50176]	Loss: 0.7166
Training Epoch: 41 [49664/50176]	Loss: 0.7680
Training Epoch: 41 [49920/50176]	Loss: 0.6980
Training Epoch: 41 [50176/50176]	Loss: 0.8363
Validation Epoch: 41, Average loss: 0.0094, Accuracy: 0.5146
Training Epoch: 42 [256/50176]	Loss: 0.7040
Training Epoch: 42 [512/50176]	Loss: 0.5865
Training Epoch: 42 [768/50176]	Loss: 0.5496
Training Epoch: 42 [1024/50176]	Loss: 0.6305
Training Epoch: 42 [1280/50176]	Loss: 0.7129
Training Epoch: 42 [1536/50176]	Loss: 0.5653
Training Epoch: 42 [1792/50176]	Loss: 0.7055
Training Epoch: 42 [2048/50176]	Loss: 0.6658
Training Epoch: 42 [2304/50176]	Loss: 0.5735
Training Epoch: 42 [2560/50176]	Loss: 0.6202
Training Epoch: 42 [2816/50176]	Loss: 0.6894
Training Epoch: 42 [3072/50176]	Loss: 0.5882
Training Epoch: 42 [3328/50176]	Loss: 0.6030
Training Epoch: 42 [3584/50176]	Loss: 0.7362
Training Epoch: 42 [3840/50176]	Loss: 0.6133
Training Epoch: 42 [4096/50176]	Loss: 0.6270
Training Epoch: 42 [4352/50176]	Loss: 0.4857
Training Epoch: 42 [4608/50176]	Loss: 0.6401
Training Epoch: 42 [4864/50176]	Loss: 0.6870
Training Epoch: 42 [5120/50176]	Loss: 0.6682
Training Epoch: 42 [5376/50176]	Loss: 0.6907
Training Epoch: 42 [5632/50176]	Loss: 0.6415
Training Epoch: 42 [5888/50176]	Loss: 0.7005
Training Epoch: 42 [6144/50176]	Loss: 0.6828
Training Epoch: 42 [6400/50176]	Loss: 0.6343
Training Epoch: 42 [6656/50176]	Loss: 0.8044
Training Epoch: 42 [6912/50176]	Loss: 0.7445
Training Epoch: 42 [7168/50176]	Loss: 0.5871
Training Epoch: 42 [7424/50176]	Loss: 0.5809
Training Epoch: 42 [7680/50176]	Loss: 0.5955
Training Epoch: 42 [7936/50176]	Loss: 0.6840
Training Epoch: 42 [8192/50176]	Loss: 0.7654
Training Epoch: 42 [8448/50176]	Loss: 0.5448
Training Epoch: 42 [8704/50176]	Loss: 0.7096
Training Epoch: 42 [8960/50176]	Loss: 0.7792
Training Epoch: 42 [9216/50176]	Loss: 0.5405
Training Epoch: 42 [9472/50176]	Loss: 0.5877
Training Epoch: 42 [9728/50176]	Loss: 0.6743
Training Epoch: 42 [9984/50176]	Loss: 0.7645
Training Epoch: 42 [10240/50176]	Loss: 0.5549
Training Epoch: 42 [10496/50176]	Loss: 0.5840
Training Epoch: 42 [10752/50176]	Loss: 0.7681
Training Epoch: 42 [11008/50176]	Loss: 0.7390
Training Epoch: 42 [11264/50176]	Loss: 0.6652
Training Epoch: 42 [11520/50176]	Loss: 0.8112
Training Epoch: 42 [11776/50176]	Loss: 0.6251
Training Epoch: 42 [12032/50176]	Loss: 0.6266
Training Epoch: 42 [12288/50176]	Loss: 0.5790
Training Epoch: 42 [12544/50176]	Loss: 0.6705
Training Epoch: 42 [12800/50176]	Loss: 0.7364
Training Epoch: 42 [13056/50176]	Loss: 0.6638
Training Epoch: 42 [13312/50176]	Loss: 0.5796
Training Epoch: 42 [13568/50176]	Loss: 0.7218
Training Epoch: 42 [13824/50176]	Loss: 0.5820
Training Epoch: 42 [14080/50176]	Loss: 0.7705
Training Epoch: 42 [14336/50176]	Loss: 0.6676
Training Epoch: 42 [14592/50176]	Loss: 0.6600
Training Epoch: 42 [14848/50176]	Loss: 0.6066
Training Epoch: 42 [15104/50176]	Loss: 0.7609
Training Epoch: 42 [15360/50176]	Loss: 0.6193
Training Epoch: 42 [15616/50176]	Loss: 0.7011
Training Epoch: 42 [15872/50176]	Loss: 0.7192
Training Epoch: 42 [16128/50176]	Loss: 0.7278
Training Epoch: 42 [16384/50176]	Loss: 0.7944
Training Epoch: 42 [16640/50176]	Loss: 0.5938
Training Epoch: 42 [16896/50176]	Loss: 0.6558
Training Epoch: 42 [17152/50176]	Loss: 0.6441
Training Epoch: 42 [17408/50176]	Loss: 0.6308
Training Epoch: 42 [17664/50176]	Loss: 0.5618
Training Epoch: 42 [17920/50176]	Loss: 0.7019
Training Epoch: 42 [18176/50176]	Loss: 0.5718
Training Epoch: 42 [18432/50176]	Loss: 0.7344
Training Epoch: 42 [18688/50176]	Loss: 0.6924
Training Epoch: 42 [18944/50176]	Loss: 0.6558
Training Epoch: 42 [19200/50176]	Loss: 0.7410
Training Epoch: 42 [19456/50176]	Loss: 0.6597
Training Epoch: 42 [19712/50176]	Loss: 0.7686
Training Epoch: 42 [19968/50176]	Loss: 0.6296
Training Epoch: 42 [20224/50176]	Loss: 0.6958
Training Epoch: 42 [20480/50176]	Loss: 0.7215
Training Epoch: 42 [20736/50176]	Loss: 0.6180
Training Epoch: 42 [20992/50176]	Loss: 0.6566
Training Epoch: 42 [21248/50176]	Loss: 0.6557
Training Epoch: 42 [21504/50176]	Loss: 0.6234
Training Epoch: 42 [21760/50176]	Loss: 0.7264
Training Epoch: 42 [22016/50176]	Loss: 0.6919
Training Epoch: 42 [22272/50176]	Loss: 0.6223
Training Epoch: 42 [22528/50176]	Loss: 0.6123
Training Epoch: 42 [22784/50176]	Loss: 0.7003
Training Epoch: 42 [23040/50176]	Loss: 0.6323
Training Epoch: 42 [23296/50176]	Loss: 0.7040
Training Epoch: 42 [23552/50176]	Loss: 0.6649
Training Epoch: 42 [23808/50176]	Loss: 0.7102
Training Epoch: 42 [24064/50176]	Loss: 0.7138
Training Epoch: 42 [24320/50176]	Loss: 0.6048
Training Epoch: 42 [24576/50176]	Loss: 0.6080
Training Epoch: 42 [24832/50176]	Loss: 0.7348
Training Epoch: 42 [25088/50176]	Loss: 0.5862
Training Epoch: 42 [25344/50176]	Loss: 0.7337
Training Epoch: 42 [25600/50176]	Loss: 0.6745
Training Epoch: 42 [25856/50176]	Loss: 0.6722
Training Epoch: 42 [26112/50176]	Loss: 0.7286
Training Epoch: 42 [26368/50176]	Loss: 0.6386
Training Epoch: 42 [26624/50176]	Loss: 0.6192
Training Epoch: 42 [26880/50176]	Loss: 0.6434
Training Epoch: 42 [27136/50176]	Loss: 0.6359
Training Epoch: 42 [27392/50176]	Loss: 0.6388
Training Epoch: 42 [27648/50176]	Loss: 0.5327
Training Epoch: 42 [27904/50176]	Loss: 0.6898
Training Epoch: 42 [28160/50176]	Loss: 0.7777
Training Epoch: 42 [28416/50176]	Loss: 0.6273
Training Epoch: 42 [28672/50176]	Loss: 0.5738
Training Epoch: 42 [28928/50176]	Loss: 0.6985
Training Epoch: 42 [29184/50176]	Loss: 0.7585
Training Epoch: 42 [29440/50176]	Loss: 0.7252
Training Epoch: 42 [29696/50176]	Loss: 0.7386
Training Epoch: 42 [29952/50176]	Loss: 0.7149
Training Epoch: 42 [30208/50176]	Loss: 0.5539
Training Epoch: 42 [30464/50176]	Loss: 0.7514
Training Epoch: 42 [30720/50176]	Loss: 0.5686
Training Epoch: 42 [30976/50176]	Loss: 0.8170
Training Epoch: 42 [31232/50176]	Loss: 0.7359
Training Epoch: 42 [31488/50176]	Loss: 0.7028
Training Epoch: 42 [31744/50176]	Loss: 0.7563
Training Epoch: 42 [32000/50176]	Loss: 0.6105
Training Epoch: 42 [32256/50176]	Loss: 0.6602
Training Epoch: 42 [32512/50176]	Loss: 0.7228
Training Epoch: 42 [32768/50176]	Loss: 0.6507
Training Epoch: 42 [33024/50176]	Loss: 0.7737
Training Epoch: 42 [33280/50176]	Loss: 0.6829
Training Epoch: 42 [33536/50176]	Loss: 0.6757
Training Epoch: 42 [33792/50176]	Loss: 0.6884
Training Epoch: 42 [34048/50176]	Loss: 0.7883
Training Epoch: 42 [34304/50176]	Loss: 0.6608
Training Epoch: 42 [34560/50176]	Loss: 0.6206
Training Epoch: 42 [34816/50176]	Loss: 0.6615
Training Epoch: 42 [35072/50176]	Loss: 0.5862
Training Epoch: 42 [35328/50176]	Loss: 0.5392
Training Epoch: 42 [35584/50176]	Loss: 0.7504
Training Epoch: 42 [35840/50176]	Loss: 0.7668
Training Epoch: 42 [36096/50176]	Loss: 0.6375
Training Epoch: 42 [36352/50176]	Loss: 0.7888
Training Epoch: 42 [36608/50176]	Loss: 0.6834
Training Epoch: 42 [36864/50176]	Loss: 0.6077
Training Epoch: 42 [37120/50176]	Loss: 0.5930
Training Epoch: 42 [37376/50176]	Loss: 0.6743
Training Epoch: 42 [37632/50176]	Loss: 0.7643
Training Epoch: 42 [37888/50176]	Loss: 0.5938
Training Epoch: 42 [38144/50176]	Loss: 0.8303
Training Epoch: 42 [38400/50176]	Loss: 0.7663
Training Epoch: 42 [38656/50176]	Loss: 0.7760
Training Epoch: 42 [38912/50176]	Loss: 0.5855
Training Epoch: 42 [39168/50176]	Loss: 0.5599
Training Epoch: 42 [39424/50176]	Loss: 0.7312
Training Epoch: 42 [39680/50176]	Loss: 0.5687
Training Epoch: 42 [39936/50176]	Loss: 0.7178
Training Epoch: 42 [40192/50176]	Loss: 0.7942
Training Epoch: 42 [40448/50176]	Loss: 0.7430
Training Epoch: 42 [40704/50176]	Loss: 0.6786
Training Epoch: 42 [40960/50176]	Loss: 0.7080
Training Epoch: 42 [41216/50176]	Loss: 0.7398
Training Epoch: 42 [41472/50176]	Loss: 0.7798
Training Epoch: 42 [41728/50176]	Loss: 0.6505
Training Epoch: 42 [41984/50176]	Loss: 0.6457
Training Epoch: 42 [42240/50176]	Loss: 0.6959
Training Epoch: 42 [42496/50176]	Loss: 0.8481
Training Epoch: 42 [42752/50176]	Loss: 0.7336
Training Epoch: 42 [43008/50176]	Loss: 0.7559
Training Epoch: 42 [43264/50176]	Loss: 0.5756
Training Epoch: 42 [43520/50176]	Loss: 0.7290
Training Epoch: 42 [43776/50176]	Loss: 0.5797
Training Epoch: 42 [44032/50176]	Loss: 0.7464
Training Epoch: 42 [44288/50176]	Loss: 0.7565
Training Epoch: 42 [44544/50176]	Loss: 0.7010
Training Epoch: 42 [44800/50176]	Loss: 0.4387
Training Epoch: 42 [45056/50176]	Loss: 0.6981
Training Epoch: 42 [45312/50176]	Loss: 0.6441
Training Epoch: 42 [45568/50176]	Loss: 0.7148
Training Epoch: 42 [45824/50176]	Loss: 0.8241
Training Epoch: 42 [46080/50176]	Loss: 0.5580
Training Epoch: 42 [46336/50176]	Loss: 0.6336
Training Epoch: 42 [46592/50176]	Loss: 0.6941
Training Epoch: 42 [46848/50176]	Loss: 0.7987
Training Epoch: 42 [47104/50176]	Loss: 0.6663
Training Epoch: 42 [47360/50176]	Loss: 0.7770
Training Epoch: 42 [47616/50176]	Loss: 0.7446
Training Epoch: 42 [47872/50176]	Loss: 0.7267
Training Epoch: 42 [48128/50176]	Loss: 0.7698
Training Epoch: 42 [48384/50176]	Loss: 0.5986
Training Epoch: 42 [48640/50176]	Loss: 0.7286
Training Epoch: 42 [48896/50176]	Loss: 0.6653
Training Epoch: 42 [49152/50176]	Loss: 0.7994
Training Epoch: 42 [49408/50176]	Loss: 0.8482
Training Epoch: 42 [49664/50176]	Loss: 0.7413
Training Epoch: 42 [49920/50176]	Loss: 0.7407
Training Epoch: 42 [50176/50176]	Loss: 0.7474
Validation Epoch: 42, Average loss: 0.0092, Accuracy: 0.5326
Training Epoch: 43 [256/50176]	Loss: 0.6368
Training Epoch: 43 [512/50176]	Loss: 0.5410
Training Epoch: 43 [768/50176]	Loss: 0.6310
Training Epoch: 43 [1024/50176]	Loss: 0.6601
Training Epoch: 43 [1280/50176]	Loss: 0.5788
Training Epoch: 43 [1536/50176]	Loss: 0.8022
Training Epoch: 43 [1792/50176]	Loss: 0.6462
Training Epoch: 43 [2048/50176]	Loss: 0.6066
Training Epoch: 43 [2304/50176]	Loss: 0.5670
Training Epoch: 43 [2560/50176]	Loss: 0.7278
Training Epoch: 43 [2816/50176]	Loss: 0.5609
Training Epoch: 43 [3072/50176]	Loss: 0.6692
Training Epoch: 43 [3328/50176]	Loss: 0.5561
Training Epoch: 43 [3584/50176]	Loss: 0.6319
Training Epoch: 43 [3840/50176]	Loss: 0.7000
Training Epoch: 43 [4096/50176]	Loss: 0.6159
Training Epoch: 43 [4352/50176]	Loss: 0.6580
Training Epoch: 43 [4608/50176]	Loss: 0.6227
Training Epoch: 43 [4864/50176]	Loss: 0.5300
Training Epoch: 43 [5120/50176]	Loss: 0.6729
Training Epoch: 43 [5376/50176]	Loss: 0.6246
Training Epoch: 43 [5632/50176]	Loss: 0.5554
Training Epoch: 43 [5888/50176]	Loss: 0.6586
Training Epoch: 43 [6144/50176]	Loss: 0.5333
Training Epoch: 43 [6400/50176]	Loss: 0.7277
Training Epoch: 43 [6656/50176]	Loss: 0.6400
Training Epoch: 43 [6912/50176]	Loss: 0.5560
Training Epoch: 43 [7168/50176]	Loss: 0.5506
Training Epoch: 43 [7424/50176]	Loss: 0.6949
Training Epoch: 43 [7680/50176]	Loss: 0.5597
Training Epoch: 43 [7936/50176]	Loss: 0.7249
Training Epoch: 43 [8192/50176]	Loss: 0.5263
Training Epoch: 43 [8448/50176]	Loss: 0.5705
Training Epoch: 43 [8704/50176]	Loss: 0.7185
Training Epoch: 43 [8960/50176]	Loss: 0.5487
Training Epoch: 43 [9216/50176]	Loss: 0.6279
Training Epoch: 43 [9472/50176]	Loss: 0.4731
Training Epoch: 43 [9728/50176]	Loss: 0.5687
Training Epoch: 43 [9984/50176]	Loss: 0.5662
Training Epoch: 43 [10240/50176]	Loss: 0.6206
Training Epoch: 43 [10496/50176]	Loss: 0.7284
Training Epoch: 43 [10752/50176]	Loss: 0.7484
Training Epoch: 43 [11008/50176]	Loss: 0.5648
Training Epoch: 43 [11264/50176]	Loss: 0.5959
Training Epoch: 43 [11520/50176]	Loss: 0.5254
Training Epoch: 43 [11776/50176]	Loss: 0.6168
Training Epoch: 43 [12032/50176]	Loss: 0.5666
Training Epoch: 43 [12288/50176]	Loss: 0.5789
Training Epoch: 43 [12544/50176]	Loss: 0.7405
Training Epoch: 43 [12800/50176]	Loss: 0.5965
Training Epoch: 43 [13056/50176]	Loss: 0.6343
Training Epoch: 43 [13312/50176]	Loss: 0.6566
Training Epoch: 43 [13568/50176]	Loss: 0.6838
Training Epoch: 43 [13824/50176]	Loss: 0.7339
Training Epoch: 43 [14080/50176]	Loss: 0.6533
Training Epoch: 43 [14336/50176]	Loss: 0.5499
Training Epoch: 43 [14592/50176]	Loss: 0.6671
Training Epoch: 43 [14848/50176]	Loss: 0.6299
Training Epoch: 43 [15104/50176]	Loss: 0.7060
Training Epoch: 43 [15360/50176]	Loss: 0.7172
Training Epoch: 43 [15616/50176]	Loss: 0.5523
Training Epoch: 43 [15872/50176]	Loss: 0.6724
Training Epoch: 43 [16128/50176]	Loss: 0.6715
Training Epoch: 43 [16384/50176]	Loss: 0.7584
Training Epoch: 43 [16640/50176]	Loss: 0.5736
Training Epoch: 43 [16896/50176]	Loss: 0.6175
Training Epoch: 43 [17152/50176]	Loss: 0.6353
Training Epoch: 43 [17408/50176]	Loss: 0.6769
Training Epoch: 43 [17664/50176]	Loss: 0.6806
Training Epoch: 43 [17920/50176]	Loss: 0.6717
Training Epoch: 43 [18176/50176]	Loss: 0.6253
Training Epoch: 43 [18432/50176]	Loss: 0.6901
Training Epoch: 43 [18688/50176]	Loss: 0.5327
Training Epoch: 43 [18944/50176]	Loss: 0.5710
Training Epoch: 43 [19200/50176]	Loss: 0.7075
Training Epoch: 43 [19456/50176]	Loss: 0.7491
Training Epoch: 43 [19712/50176]	Loss: 0.6903
Training Epoch: 43 [19968/50176]	Loss: 0.6545
Training Epoch: 43 [20224/50176]	Loss: 0.6691
Training Epoch: 43 [20480/50176]	Loss: 0.5774
Training Epoch: 43 [20736/50176]	Loss: 0.7878
Training Epoch: 43 [20992/50176]	Loss: 0.6646
Training Epoch: 43 [21248/50176]	Loss: 0.5599
Training Epoch: 43 [21504/50176]	Loss: 0.6054
Training Epoch: 43 [21760/50176]	Loss: 0.5293
Training Epoch: 43 [22016/50176]	Loss: 0.7288
Training Epoch: 43 [22272/50176]	Loss: 0.6280
Training Epoch: 43 [22528/50176]	Loss: 0.7202
Training Epoch: 43 [22784/50176]	Loss: 0.6160
Training Epoch: 43 [23040/50176]	Loss: 0.6911
Training Epoch: 43 [23296/50176]	Loss: 0.6930
Training Epoch: 43 [23552/50176]	Loss: 0.7124
Training Epoch: 43 [23808/50176]	Loss: 0.7064
Training Epoch: 43 [24064/50176]	Loss: 0.6427
Training Epoch: 43 [24320/50176]	Loss: 0.6920
Training Epoch: 43 [24576/50176]	Loss: 0.5994
Training Epoch: 43 [24832/50176]	Loss: 0.6524
Training Epoch: 43 [25088/50176]	Loss: 0.7527
Training Epoch: 43 [25344/50176]	Loss: 0.5220
Training Epoch: 43 [25600/50176]	Loss: 0.6246
Training Epoch: 43 [25856/50176]	Loss: 0.6650
Training Epoch: 43 [26112/50176]	Loss: 0.6297
Training Epoch: 43 [26368/50176]	Loss: 0.6766
Training Epoch: 43 [26624/50176]	Loss: 0.6635
Training Epoch: 43 [26880/50176]	Loss: 0.7393
Training Epoch: 43 [27136/50176]	Loss: 0.6300
Training Epoch: 43 [27392/50176]	Loss: 0.7006
Training Epoch: 43 [27648/50176]	Loss: 0.6627
Training Epoch: 43 [27904/50176]	Loss: 0.8927
Training Epoch: 43 [28160/50176]	Loss: 0.7197
Training Epoch: 43 [28416/50176]	Loss: 0.7491
Training Epoch: 43 [28672/50176]	Loss: 0.6562
Training Epoch: 43 [28928/50176]	Loss: 0.6927
Training Epoch: 43 [29184/50176]	Loss: 0.6284
Training Epoch: 43 [29440/50176]	Loss: 0.6400
Training Epoch: 43 [29696/50176]	Loss: 0.8026
Training Epoch: 43 [29952/50176]	Loss: 0.6731
Training Epoch: 43 [30208/50176]	Loss: 0.6571
Training Epoch: 43 [30464/50176]	Loss: 0.6597
Training Epoch: 43 [30720/50176]	Loss: 0.5956
Training Epoch: 43 [30976/50176]	Loss: 0.7363
Training Epoch: 43 [31232/50176]	Loss: 0.6914
Training Epoch: 43 [31488/50176]	Loss: 0.7290
Training Epoch: 43 [31744/50176]	Loss: 0.5936
Training Epoch: 43 [32000/50176]	Loss: 0.6574
Training Epoch: 43 [32256/50176]	Loss: 0.5781
Training Epoch: 43 [32512/50176]	Loss: 0.6769
Training Epoch: 43 [32768/50176]	Loss: 0.6967
Training Epoch: 43 [33024/50176]	Loss: 0.5463
Training Epoch: 43 [33280/50176]	Loss: 0.5847
Training Epoch: 43 [33536/50176]	Loss: 0.6377
Training Epoch: 43 [33792/50176]	Loss: 0.7380
Training Epoch: 43 [34048/50176]	Loss: 0.6796
Training Epoch: 43 [34304/50176]	Loss: 0.7380
Training Epoch: 43 [34560/50176]	Loss: 0.5990
Training Epoch: 43 [34816/50176]	Loss: 0.6933
Training Epoch: 43 [35072/50176]	Loss: 0.5797
Training Epoch: 43 [35328/50176]	Loss: 0.7337
Training Epoch: 43 [35584/50176]	Loss: 0.8625
Training Epoch: 43 [35840/50176]	Loss: 0.9139
Training Epoch: 43 [36096/50176]	Loss: 0.6989
Training Epoch: 43 [36352/50176]	Loss: 0.6301
Training Epoch: 43 [36608/50176]	Loss: 0.6281
Training Epoch: 43 [36864/50176]	Loss: 0.6456
Training Epoch: 43 [37120/50176]	Loss: 0.7270
Training Epoch: 43 [37376/50176]	Loss: 0.6640
Training Epoch: 43 [37632/50176]	Loss: 0.6641
Training Epoch: 43 [37888/50176]	Loss: 0.7166
Training Epoch: 43 [38144/50176]	Loss: 0.7048
Training Epoch: 43 [38400/50176]	Loss: 0.6392
Training Epoch: 43 [38656/50176]	Loss: 0.5826
Training Epoch: 43 [38912/50176]	Loss: 0.6854
Training Epoch: 43 [39168/50176]	Loss: 0.8105
Training Epoch: 43 [39424/50176]	Loss: 0.7373
Training Epoch: 43 [39680/50176]	Loss: 0.7039
Training Epoch: 43 [39936/50176]	Loss: 0.6877
Training Epoch: 43 [40192/50176]	Loss: 0.6887
Training Epoch: 43 [40448/50176]	Loss: 0.7008
Training Epoch: 43 [40704/50176]	Loss: 0.7102
Training Epoch: 43 [40960/50176]	Loss: 0.7005
Training Epoch: 43 [41216/50176]	Loss: 0.6045
Training Epoch: 43 [41472/50176]	Loss: 0.6796
Training Epoch: 43 [41728/50176]	Loss: 0.5633
Training Epoch: 43 [41984/50176]	Loss: 0.6529
Training Epoch: 43 [42240/50176]	Loss: 0.6664
Training Epoch: 43 [42496/50176]	Loss: 0.6772
Training Epoch: 43 [42752/50176]	Loss: 0.6134
Training Epoch: 43 [43008/50176]	Loss: 0.6469
Training Epoch: 43 [43264/50176]	Loss: 0.6704
Training Epoch: 43 [43520/50176]	Loss: 0.6064
Training Epoch: 43 [43776/50176]	Loss: 0.6171
Training Epoch: 43 [44032/50176]	Loss: 0.7826
Training Epoch: 43 [44288/50176]	Loss: 0.7598
Training Epoch: 43 [44544/50176]	Loss: 0.5277
Training Epoch: 43 [44800/50176]	Loss: 0.6954
Training Epoch: 43 [45056/50176]	Loss: 0.7036
Training Epoch: 43 [45312/50176]	Loss: 0.6035
Training Epoch: 43 [45568/50176]	Loss: 0.6632
Training Epoch: 43 [45824/50176]	Loss: 0.6806
Training Epoch: 43 [46080/50176]	Loss: 0.7016
Training Epoch: 43 [46336/50176]	Loss: 0.7394
Training Epoch: 43 [46592/50176]	Loss: 0.5814
Training Epoch: 43 [46848/50176]	Loss: 0.5336
Training Epoch: 43 [47104/50176]	Loss: 0.6479
Training Epoch: 43 [47360/50176]	Loss: 0.7544
Training Epoch: 43 [47616/50176]	Loss: 0.8035
Training Epoch: 43 [47872/50176]	Loss: 0.7406
Training Epoch: 43 [48128/50176]	Loss: 0.6258
Training Epoch: 43 [48384/50176]	Loss: 0.6217
Training Epoch: 43 [48640/50176]	Loss: 0.5935
Training Epoch: 43 [48896/50176]	Loss: 0.6697
Training Epoch: 43 [49152/50176]	Loss: 0.6313
Training Epoch: 43 [49408/50176]	Loss: 0.6700
Training Epoch: 43 [49664/50176]	Loss: 0.7257
Training Epoch: 43 [49920/50176]	Loss: 0.6935
Training Epoch: 43 [50176/50176]	Loss: 0.7081
Validation Epoch: 43, Average loss: 0.0090, Accuracy: 0.5315
Training Epoch: 44 [256/50176]	Loss: 0.5600
Training Epoch: 44 [512/50176]	Loss: 0.6447
Training Epoch: 44 [768/50176]	Loss: 0.5976
Training Epoch: 44 [1024/50176]	Loss: 0.6062
Training Epoch: 44 [1280/50176]	Loss: 0.5028
Training Epoch: 44 [1536/50176]	Loss: 0.7214
Training Epoch: 44 [1792/50176]	Loss: 0.5427
Training Epoch: 44 [2048/50176]	Loss: 0.6270
Training Epoch: 44 [2304/50176]	Loss: 0.5758
Training Epoch: 44 [2560/50176]	Loss: 0.5999
Training Epoch: 44 [2816/50176]	Loss: 0.8190
Training Epoch: 44 [3072/50176]	Loss: 0.5179
Training Epoch: 44 [3328/50176]	Loss: 0.5724
Training Epoch: 44 [3584/50176]	Loss: 0.6137
Training Epoch: 44 [3840/50176]	Loss: 0.5792
Training Epoch: 44 [4096/50176]	Loss: 0.6161
Training Epoch: 44 [4352/50176]	Loss: 0.5876
Training Epoch: 44 [4608/50176]	Loss: 0.5493
Training Epoch: 44 [4864/50176]	Loss: 0.5645
Training Epoch: 44 [5120/50176]	Loss: 0.5421
Training Epoch: 44 [5376/50176]	Loss: 0.7260
Training Epoch: 44 [5632/50176]	Loss: 0.5438
Training Epoch: 44 [5888/50176]	Loss: 0.4151
Training Epoch: 44 [6144/50176]	Loss: 0.5347
Training Epoch: 44 [6400/50176]	Loss: 0.6362
Training Epoch: 44 [6656/50176]	Loss: 0.5544
Training Epoch: 44 [6912/50176]	Loss: 0.5093
Training Epoch: 44 [7168/50176]	Loss: 0.6343
Training Epoch: 44 [7424/50176]	Loss: 0.5922
Training Epoch: 44 [7680/50176]	Loss: 0.8055
Training Epoch: 44 [7936/50176]	Loss: 0.6558
Training Epoch: 44 [8192/50176]	Loss: 0.6755
Training Epoch: 44 [8448/50176]	Loss: 0.6017
Training Epoch: 44 [8704/50176]	Loss: 0.5689
Training Epoch: 44 [8960/50176]	Loss: 0.6186
Training Epoch: 44 [9216/50176]	Loss: 0.4744
Training Epoch: 44 [9472/50176]	Loss: 0.5792
Training Epoch: 44 [9728/50176]	Loss: 0.5640
Training Epoch: 44 [9984/50176]	Loss: 0.5752
Training Epoch: 44 [10240/50176]	Loss: 0.6723
Training Epoch: 44 [10496/50176]	Loss: 0.6547
Training Epoch: 44 [10752/50176]	Loss: 0.5431
Training Epoch: 44 [11008/50176]	Loss: 0.6160
Training Epoch: 44 [11264/50176]	Loss: 0.6737
Training Epoch: 44 [11520/50176]	Loss: 0.5002
Training Epoch: 44 [11776/50176]	Loss: 0.6450
Training Epoch: 44 [12032/50176]	Loss: 0.5757
Training Epoch: 44 [12288/50176]	Loss: 0.6032
Training Epoch: 44 [12544/50176]	Loss: 0.5689
Training Epoch: 44 [12800/50176]	Loss: 0.5657
Training Epoch: 44 [13056/50176]	Loss: 0.5065
Training Epoch: 44 [13312/50176]	Loss: 0.7122
Training Epoch: 44 [13568/50176]	Loss: 0.5118
Training Epoch: 44 [13824/50176]	Loss: 0.6369
Training Epoch: 44 [14080/50176]	Loss: 0.6359
Training Epoch: 44 [14336/50176]	Loss: 0.6440
Training Epoch: 44 [14592/50176]	Loss: 0.6292
Training Epoch: 44 [14848/50176]	Loss: 0.5570
Training Epoch: 44 [15104/50176]	Loss: 0.6617
Training Epoch: 44 [15360/50176]	Loss: 0.5722
Training Epoch: 44 [15616/50176]	Loss: 0.6267
Training Epoch: 44 [15872/50176]	Loss: 0.4902
Training Epoch: 44 [16128/50176]	Loss: 0.7138
Training Epoch: 44 [16384/50176]	Loss: 0.5913
Training Epoch: 44 [16640/50176]	Loss: 0.7020
Training Epoch: 44 [16896/50176]	Loss: 0.5382
Training Epoch: 44 [17152/50176]	Loss: 0.4988
Training Epoch: 44 [17408/50176]	Loss: 0.6807
Training Epoch: 44 [17664/50176]	Loss: 0.5675
Training Epoch: 44 [17920/50176]	Loss: 0.6793
Training Epoch: 44 [18176/50176]	Loss: 0.6680
Training Epoch: 44 [18432/50176]	Loss: 0.7493
Training Epoch: 44 [18688/50176]	Loss: 0.5194
Training Epoch: 44 [18944/50176]	Loss: 0.5730
Training Epoch: 44 [19200/50176]	Loss: 0.6146
Training Epoch: 44 [19456/50176]	Loss: 0.5735
Training Epoch: 44 [19712/50176]	Loss: 0.6628
Training Epoch: 44 [19968/50176]	Loss: 0.5318
Training Epoch: 44 [20224/50176]	Loss: 0.5687
Training Epoch: 44 [20480/50176]	Loss: 0.6667
Training Epoch: 44 [20736/50176]	Loss: 0.6127
Training Epoch: 44 [20992/50176]	Loss: 0.7484
Training Epoch: 44 [21248/50176]	Loss: 0.5805
Training Epoch: 44 [21504/50176]	Loss: 0.6239
Training Epoch: 44 [21760/50176]	Loss: 0.5931
Training Epoch: 44 [22016/50176]	Loss: 0.5788
Training Epoch: 44 [22272/50176]	Loss: 0.6727
Training Epoch: 44 [22528/50176]	Loss: 0.5413
Training Epoch: 44 [22784/50176]	Loss: 0.6505
Training Epoch: 44 [23040/50176]	Loss: 0.7151
Training Epoch: 44 [23296/50176]	Loss: 0.7212
Training Epoch: 44 [23552/50176]	Loss: 0.5816
Training Epoch: 44 [23808/50176]	Loss: 0.6764
Training Epoch: 44 [24064/50176]	Loss: 0.6405
Training Epoch: 44 [24320/50176]	Loss: 0.5900
Training Epoch: 44 [24576/50176]	Loss: 0.6710
Training Epoch: 44 [24832/50176]	Loss: 0.5672
Training Epoch: 44 [25088/50176]	Loss: 0.7322
Training Epoch: 44 [25344/50176]	Loss: 0.7419
Training Epoch: 44 [25600/50176]	Loss: 0.6692
Training Epoch: 44 [25856/50176]	Loss: 0.7060
Training Epoch: 44 [26112/50176]	Loss: 0.6226
Training Epoch: 44 [26368/50176]	Loss: 0.5891
Training Epoch: 44 [26624/50176]	Loss: 0.5771
Training Epoch: 44 [26880/50176]	Loss: 0.7656
Training Epoch: 44 [27136/50176]	Loss: 0.7791
Training Epoch: 44 [27392/50176]	Loss: 0.7301
Training Epoch: 44 [27648/50176]	Loss: 0.6604
Training Epoch: 44 [27904/50176]	Loss: 0.4979
Training Epoch: 44 [28160/50176]	Loss: 0.6739
Training Epoch: 44 [28416/50176]	Loss: 0.8094
Training Epoch: 44 [28672/50176]	Loss: 0.5227
Training Epoch: 44 [28928/50176]	Loss: 0.7642
Training Epoch: 44 [29184/50176]	Loss: 0.5693
Training Epoch: 44 [29440/50176]	Loss: 0.6954
Training Epoch: 44 [29696/50176]	Loss: 0.7198
Training Epoch: 44 [29952/50176]	Loss: 0.6433
Training Epoch: 44 [30208/50176]	Loss: 0.6484
Training Epoch: 44 [30464/50176]	Loss: 0.8035
Training Epoch: 44 [30720/50176]	Loss: 0.5636
Training Epoch: 44 [30976/50176]	Loss: 0.6934
Training Epoch: 44 [31232/50176]	Loss: 0.7391
Training Epoch: 44 [31488/50176]	Loss: 0.6906
Training Epoch: 44 [31744/50176]	Loss: 0.6549
Training Epoch: 44 [32000/50176]	Loss: 0.6468
Training Epoch: 44 [32256/50176]	Loss: 0.7566
Training Epoch: 44 [32512/50176]	Loss: 0.6393
Training Epoch: 44 [32768/50176]	Loss: 0.6683
Training Epoch: 44 [33024/50176]	Loss: 0.8758
Training Epoch: 44 [33280/50176]	Loss: 0.6732
Training Epoch: 44 [33536/50176]	Loss: 0.6558
Training Epoch: 44 [33792/50176]	Loss: 0.6287
Training Epoch: 44 [34048/50176]	Loss: 0.6697
Training Epoch: 44 [34304/50176]	Loss: 0.6711
Training Epoch: 44 [34560/50176]	Loss: 0.5814
Training Epoch: 44 [34816/50176]	Loss: 0.6104
Training Epoch: 44 [35072/50176]	Loss: 0.6544
Training Epoch: 44 [35328/50176]	Loss: 0.5509
Training Epoch: 44 [35584/50176]	Loss: 0.7412
Training Epoch: 44 [35840/50176]	Loss: 0.7773
Training Epoch: 44 [36096/50176]	Loss: 0.6336
Training Epoch: 44 [36352/50176]	Loss: 0.7045
Training Epoch: 44 [36608/50176]	Loss: 0.5980
Training Epoch: 44 [36864/50176]	Loss: 0.6294
Training Epoch: 44 [37120/50176]	Loss: 0.5858
Training Epoch: 44 [37376/50176]	Loss: 0.7057
Training Epoch: 44 [37632/50176]	Loss: 0.5601
Training Epoch: 44 [37888/50176]	Loss: 0.7557
Training Epoch: 44 [38144/50176]	Loss: 0.7109
Training Epoch: 44 [38400/50176]	Loss: 0.6885
Training Epoch: 44 [38656/50176]	Loss: 0.7186
Training Epoch: 44 [38912/50176]	Loss: 0.6503
Training Epoch: 44 [39168/50176]	Loss: 0.6612
Training Epoch: 44 [39424/50176]	Loss: 0.6916
Training Epoch: 44 [39680/50176]	Loss: 0.7172
Training Epoch: 44 [39936/50176]	Loss: 0.6868
Training Epoch: 44 [40192/50176]	Loss: 0.6888
Training Epoch: 44 [40448/50176]	Loss: 0.6953
Training Epoch: 44 [40704/50176]	Loss: 0.6414
Training Epoch: 44 [40960/50176]	Loss: 0.7012
Training Epoch: 44 [41216/50176]	Loss: 0.7735
Training Epoch: 44 [41472/50176]	Loss: 0.6783
Training Epoch: 44 [41728/50176]	Loss: 0.5918
Training Epoch: 44 [41984/50176]	Loss: 0.7491
Training Epoch: 44 [42240/50176]	Loss: 0.8178
Training Epoch: 44 [42496/50176]	Loss: 0.7349
Training Epoch: 44 [42752/50176]	Loss: 0.5604
Training Epoch: 44 [43008/50176]	Loss: 0.6240
Training Epoch: 44 [43264/50176]	Loss: 0.6841
Training Epoch: 44 [43520/50176]	Loss: 0.7691
Training Epoch: 44 [43776/50176]	Loss: 0.5832
Training Epoch: 44 [44032/50176]	Loss: 0.6785
Training Epoch: 44 [44288/50176]	Loss: 0.8156
Training Epoch: 44 [44544/50176]	Loss: 0.6137
Training Epoch: 44 [44800/50176]	Loss: 0.5929
Training Epoch: 44 [45056/50176]	Loss: 0.5969
Training Epoch: 44 [45312/50176]	Loss: 0.7808
Training Epoch: 44 [45568/50176]	Loss: 0.7573
Training Epoch: 44 [45824/50176]	Loss: 0.7072
Training Epoch: 44 [46080/50176]	Loss: 0.7336
Training Epoch: 44 [46336/50176]	Loss: 0.6784
Training Epoch: 44 [46592/50176]	Loss: 0.7989
Training Epoch: 44 [46848/50176]	Loss: 0.7360
Training Epoch: 44 [47104/50176]	Loss: 0.5854
Training Epoch: 44 [47360/50176]	Loss: 0.7664
Training Epoch: 44 [47616/50176]	Loss: 0.7969
Training Epoch: 44 [47872/50176]	Loss: 0.6696
Training Epoch: 44 [48128/50176]	Loss: 0.6068
Training Epoch: 44 [48384/50176]	Loss: 0.7051
Training Epoch: 44 [48640/50176]	Loss: 0.6249
Training Epoch: 44 [48896/50176]	Loss: 0.6571
Training Epoch: 44 [49152/50176]	Loss: 0.6607
Training Epoch: 44 [49408/50176]	Loss: 0.7815
Training Epoch: 44 [49664/50176]	Loss: 0.6600
Training Epoch: 44 [49920/50176]	Loss: 0.7232
Training Epoch: 44 [50176/50176]	Loss: 0.7310
Validation Epoch: 44, Average loss: 0.0108, Accuracy: 0.4972
Training Epoch: 45 [256/50176]	Loss: 0.5293
Training Epoch: 45 [512/50176]	Loss: 0.6467
Training Epoch: 45 [768/50176]	Loss: 0.6589
Training Epoch: 45 [1024/50176]	Loss: 0.5852
Training Epoch: 45 [1280/50176]	Loss: 0.6660
Training Epoch: 45 [1536/50176]	Loss: 0.5930
Training Epoch: 45 [1792/50176]	Loss: 0.6528
Training Epoch: 45 [2048/50176]	Loss: 0.7082
Training Epoch: 45 [2304/50176]	Loss: 0.6191
Training Epoch: 45 [2560/50176]	Loss: 0.6605
Training Epoch: 45 [2816/50176]	Loss: 0.6276
Training Epoch: 45 [3072/50176]	Loss: 0.6063
Training Epoch: 45 [3328/50176]	Loss: 0.5284
Training Epoch: 45 [3584/50176]	Loss: 0.6532
Training Epoch: 45 [3840/50176]	Loss: 0.6662
Training Epoch: 45 [4096/50176]	Loss: 0.6317
Training Epoch: 45 [4352/50176]	Loss: 0.4918
Training Epoch: 45 [4608/50176]	Loss: 0.5290
Training Epoch: 45 [4864/50176]	Loss: 0.7716
Training Epoch: 45 [5120/50176]	Loss: 0.5070
Training Epoch: 45 [5376/50176]	Loss: 0.4643
Training Epoch: 45 [5632/50176]	Loss: 0.5808
Training Epoch: 45 [5888/50176]	Loss: 0.5324
Training Epoch: 45 [6144/50176]	Loss: 0.6108
Training Epoch: 45 [6400/50176]	Loss: 0.5705
Training Epoch: 45 [6656/50176]	Loss: 0.6939
Training Epoch: 45 [6912/50176]	Loss: 0.7279
Training Epoch: 45 [7168/50176]	Loss: 0.6019
Training Epoch: 45 [7424/50176]	Loss: 0.5484
Training Epoch: 45 [7680/50176]	Loss: 0.6167
Training Epoch: 45 [7936/50176]	Loss: 0.4986
Training Epoch: 45 [8192/50176]	Loss: 0.5354
Training Epoch: 45 [8448/50176]	Loss: 0.5628
Training Epoch: 45 [8704/50176]	Loss: 0.6041
Training Epoch: 45 [8960/50176]	Loss: 0.5631
Training Epoch: 45 [9216/50176]	Loss: 0.7349
Training Epoch: 45 [9472/50176]	Loss: 0.6913
Training Epoch: 45 [9728/50176]	Loss: 0.6179
Training Epoch: 45 [9984/50176]	Loss: 0.5798
Training Epoch: 45 [10240/50176]	Loss: 0.5766
Training Epoch: 45 [10496/50176]	Loss: 0.6584
Training Epoch: 45 [10752/50176]	Loss: 0.5110
Training Epoch: 45 [11008/50176]	Loss: 0.6191
Training Epoch: 45 [11264/50176]	Loss: 0.6475
Training Epoch: 45 [11520/50176]	Loss: 0.5918
Training Epoch: 45 [11776/50176]	Loss: 0.6318
Training Epoch: 45 [12032/50176]	Loss: 0.5777
Training Epoch: 45 [12288/50176]	Loss: 0.5927
Training Epoch: 45 [12544/50176]	Loss: 0.5294
Training Epoch: 45 [12800/50176]	Loss: 0.6483
Training Epoch: 45 [13056/50176]	Loss: 0.5908
Training Epoch: 45 [13312/50176]	Loss: 0.5229
Training Epoch: 45 [13568/50176]	Loss: 0.6463
Training Epoch: 45 [13824/50176]	Loss: 0.5602
Training Epoch: 45 [14080/50176]	Loss: 0.6490
Training Epoch: 45 [14336/50176]	Loss: 0.5457
Training Epoch: 45 [14592/50176]	Loss: 0.5520
Training Epoch: 45 [14848/50176]	Loss: 0.6762
Training Epoch: 45 [15104/50176]	Loss: 0.7335
Training Epoch: 45 [15360/50176]	Loss: 0.5045
Training Epoch: 45 [15616/50176]	Loss: 0.6030
Training Epoch: 45 [15872/50176]	Loss: 0.5232
Training Epoch: 45 [16128/50176]	Loss: 0.7539
Training Epoch: 45 [16384/50176]	Loss: 0.5208
Training Epoch: 45 [16640/50176]	Loss: 0.4833
Training Epoch: 45 [16896/50176]	Loss: 0.6235
Training Epoch: 45 [17152/50176]	Loss: 0.5610
Training Epoch: 45 [17408/50176]	Loss: 0.6030
Training Epoch: 45 [17664/50176]	Loss: 0.6463
Training Epoch: 45 [17920/50176]	Loss: 0.7119
Training Epoch: 45 [18176/50176]	Loss: 0.6817
Training Epoch: 45 [18432/50176]	Loss: 0.5749
Training Epoch: 45 [18688/50176]	Loss: 0.6563
Training Epoch: 45 [18944/50176]	Loss: 0.5706
Training Epoch: 45 [19200/50176]	Loss: 0.6280
Training Epoch: 45 [19456/50176]	Loss: 0.7418
Training Epoch: 45 [19712/50176]	Loss: 0.6112
Training Epoch: 45 [19968/50176]	Loss: 0.6752
Training Epoch: 45 [20224/50176]	Loss: 0.6522
Training Epoch: 45 [20480/50176]	Loss: 0.5582
Training Epoch: 45 [20736/50176]	Loss: 0.5607
Training Epoch: 45 [20992/50176]	Loss: 0.5701
Training Epoch: 45 [21248/50176]	Loss: 0.6824
Training Epoch: 45 [21504/50176]	Loss: 0.6008
Training Epoch: 45 [21760/50176]	Loss: 0.6321
Training Epoch: 45 [22016/50176]	Loss: 0.4699
Training Epoch: 45 [22272/50176]	Loss: 0.7816
Training Epoch: 45 [22528/50176]	Loss: 0.7902
Training Epoch: 45 [22784/50176]	Loss: 0.5581
Training Epoch: 45 [23040/50176]	Loss: 0.6190
Training Epoch: 45 [23296/50176]	Loss: 0.5903
Training Epoch: 45 [23552/50176]	Loss: 0.6498
Training Epoch: 45 [23808/50176]	Loss: 0.6635
Training Epoch: 45 [24064/50176]	Loss: 0.6906
Training Epoch: 45 [24320/50176]	Loss: 0.6427
Training Epoch: 45 [24576/50176]	Loss: 0.5316
Training Epoch: 45 [24832/50176]	Loss: 0.6223
Training Epoch: 45 [25088/50176]	Loss: 0.6915
Training Epoch: 45 [25344/50176]	Loss: 0.6422
Training Epoch: 45 [25600/50176]	Loss: 0.6845
Training Epoch: 45 [25856/50176]	Loss: 0.6029
Training Epoch: 45 [26112/50176]	Loss: 0.5625
Training Epoch: 45 [26368/50176]	Loss: 0.5720
Training Epoch: 45 [26624/50176]	Loss: 0.7014
Training Epoch: 45 [26880/50176]	Loss: 0.5994
Training Epoch: 45 [27136/50176]	Loss: 0.6125
Training Epoch: 45 [27392/50176]	Loss: 0.8483
Training Epoch: 45 [27648/50176]	Loss: 0.5602
Training Epoch: 45 [27904/50176]	Loss: 0.7644
Training Epoch: 45 [28160/50176]	Loss: 0.5694
Training Epoch: 45 [28416/50176]	Loss: 0.6252
Training Epoch: 45 [28672/50176]	Loss: 0.7944
Training Epoch: 45 [28928/50176]	Loss: 0.7624
Training Epoch: 45 [29184/50176]	Loss: 0.6605
Training Epoch: 45 [29440/50176]	Loss: 0.5472
Training Epoch: 45 [29696/50176]	Loss: 0.5972
Training Epoch: 45 [29952/50176]	Loss: 0.6188
Training Epoch: 45 [30208/50176]	Loss: 0.6415
Training Epoch: 45 [30464/50176]	Loss: 0.6875
Training Epoch: 45 [30720/50176]	Loss: 0.5472
Training Epoch: 45 [30976/50176]	Loss: 0.7394
Training Epoch: 45 [31232/50176]	Loss: 0.5935
Training Epoch: 45 [31488/50176]	Loss: 0.6923
Training Epoch: 45 [31744/50176]	Loss: 0.6199
Training Epoch: 45 [32000/50176]	Loss: 0.6262
Training Epoch: 45 [32256/50176]	Loss: 0.6363
Training Epoch: 45 [32512/50176]	Loss: 0.4883
Training Epoch: 45 [32768/50176]	Loss: 0.6288
Training Epoch: 45 [33024/50176]	Loss: 0.6008
Training Epoch: 45 [33280/50176]	Loss: 0.5097
Training Epoch: 45 [33536/50176]	Loss: 0.5315
Training Epoch: 45 [33792/50176]	Loss: 0.6454
Training Epoch: 45 [34048/50176]	Loss: 0.6077
Training Epoch: 45 [34304/50176]	Loss: 0.6664
Training Epoch: 45 [34560/50176]	Loss: 0.5628
Training Epoch: 45 [34816/50176]	Loss: 0.5388
Training Epoch: 45 [35072/50176]	Loss: 0.6594
Training Epoch: 45 [35328/50176]	Loss: 0.7244
Training Epoch: 45 [35584/50176]	Loss: 0.6883
Training Epoch: 45 [35840/50176]	Loss: 0.6774
Training Epoch: 45 [36096/50176]	Loss: 0.5710
Training Epoch: 45 [36352/50176]	Loss: 0.7591
Training Epoch: 45 [36608/50176]	Loss: 0.8094
Training Epoch: 45 [36864/50176]	Loss: 0.7258
Training Epoch: 45 [37120/50176]	Loss: 0.6460
Training Epoch: 45 [37376/50176]	Loss: 0.6824
Training Epoch: 45 [37632/50176]	Loss: 0.7458
Training Epoch: 45 [37888/50176]	Loss: 0.5973
Training Epoch: 45 [38144/50176]	Loss: 0.9083
Training Epoch: 45 [38400/50176]	Loss: 0.5935
Training Epoch: 45 [38656/50176]	Loss: 0.5253
Training Epoch: 45 [38912/50176]	Loss: 0.6562
Training Epoch: 45 [39168/50176]	Loss: 0.6734
Training Epoch: 45 [39424/50176]	Loss: 0.5289
Training Epoch: 45 [39680/50176]	Loss: 0.6231
Training Epoch: 45 [39936/50176]	Loss: 0.7265
Training Epoch: 45 [40192/50176]	Loss: 0.6663
Training Epoch: 45 [40448/50176]	Loss: 0.6840
Training Epoch: 45 [40704/50176]	Loss: 0.6751
Training Epoch: 45 [40960/50176]	Loss: 0.6361
Training Epoch: 45 [41216/50176]	Loss: 0.6767
Training Epoch: 45 [41472/50176]	Loss: 0.7690
Training Epoch: 45 [41728/50176]	Loss: 0.5296
Training Epoch: 45 [41984/50176]	Loss: 0.6595
Training Epoch: 45 [42240/50176]	Loss: 0.7015
Training Epoch: 45 [42496/50176]	Loss: 0.6098
Training Epoch: 45 [42752/50176]	Loss: 0.6364
Training Epoch: 45 [43008/50176]	Loss: 0.5870
Training Epoch: 45 [43264/50176]	Loss: 0.6223
Training Epoch: 45 [43520/50176]	Loss: 0.6234
Training Epoch: 45 [43776/50176]	Loss: 0.5447
Training Epoch: 45 [44032/50176]	Loss: 0.6030
Training Epoch: 45 [44288/50176]	Loss: 0.6505
Training Epoch: 45 [44544/50176]	Loss: 0.5673
Training Epoch: 45 [44800/50176]	Loss: 0.8883
Training Epoch: 45 [45056/50176]	Loss: 0.6784
Training Epoch: 45 [45312/50176]	Loss: 0.6195
Training Epoch: 45 [45568/50176]	Loss: 0.5238
Training Epoch: 45 [45824/50176]	Loss: 0.6631
Training Epoch: 45 [46080/50176]	Loss: 0.5599
Training Epoch: 45 [46336/50176]	Loss: 0.6267
Training Epoch: 45 [46592/50176]	Loss: 0.6923
Training Epoch: 45 [46848/50176]	Loss: 0.6303
Training Epoch: 45 [47104/50176]	Loss: 0.6681
Training Epoch: 45 [47360/50176]	Loss: 0.6766
Training Epoch: 45 [47616/50176]	Loss: 0.6671
Training Epoch: 45 [47872/50176]	Loss: 0.6426
Training Epoch: 45 [48128/50176]	Loss: 0.6685
Training Epoch: 45 [48384/50176]	Loss: 0.5475
Training Epoch: 45 [48640/50176]	Loss: 0.7050
Training Epoch: 45 [48896/50176]	Loss: 0.6408
Training Epoch: 45 [49152/50176]	Loss: 0.6792
Training Epoch: 45 [49408/50176]	Loss: 0.6556
Training Epoch: 45 [49664/50176]	Loss: 0.5998
Training Epoch: 45 [49920/50176]	Loss: 0.7935
Training Epoch: 45 [50176/50176]	Loss: 0.9832
Validation Epoch: 45, Average loss: 0.0092, Accuracy: 0.5437
Training Epoch: 46 [256/50176]	Loss: 0.6226
Training Epoch: 46 [512/50176]	Loss: 0.5289
Training Epoch: 46 [768/50176]	Loss: 0.6285
Training Epoch: 46 [1024/50176]	Loss: 0.5829
Training Epoch: 46 [1280/50176]	Loss: 0.5640
Training Epoch: 46 [1536/50176]	Loss: 0.6805
Training Epoch: 46 [1792/50176]	Loss: 0.5956
Training Epoch: 46 [2048/50176]	Loss: 0.6077
Training Epoch: 46 [2304/50176]	Loss: 0.6895
Training Epoch: 46 [2560/50176]	Loss: 0.5305
Training Epoch: 46 [2816/50176]	Loss: 0.6155
Training Epoch: 46 [3072/50176]	Loss: 0.6176
Training Epoch: 46 [3328/50176]	Loss: 0.5331
Training Epoch: 46 [3584/50176]	Loss: 0.7295
Training Epoch: 46 [3840/50176]	Loss: 0.5417
Training Epoch: 46 [4096/50176]	Loss: 0.4960
Training Epoch: 46 [4352/50176]	Loss: 0.5731
Training Epoch: 46 [4608/50176]	Loss: 0.5647
Training Epoch: 46 [4864/50176]	Loss: 0.5711
Training Epoch: 46 [5120/50176]	Loss: 0.7709
Training Epoch: 46 [5376/50176]	Loss: 0.4566
Training Epoch: 46 [5632/50176]	Loss: 0.5549
Training Epoch: 46 [5888/50176]	Loss: 0.6739
Training Epoch: 46 [6144/50176]	Loss: 0.5504
Training Epoch: 46 [6400/50176]	Loss: 0.4502
Training Epoch: 46 [6656/50176]	Loss: 0.6098
Training Epoch: 46 [6912/50176]	Loss: 0.6572
Training Epoch: 46 [7168/50176]	Loss: 0.6028
Training Epoch: 46 [7424/50176]	Loss: 0.6304
Training Epoch: 46 [7680/50176]	Loss: 0.5534
Training Epoch: 46 [7936/50176]	Loss: 0.6074
Training Epoch: 46 [8192/50176]	Loss: 0.4982
Training Epoch: 46 [8448/50176]	Loss: 0.5075
Training Epoch: 46 [8704/50176]	Loss: 0.4999
Training Epoch: 46 [8960/50176]	Loss: 0.5987
Training Epoch: 46 [9216/50176]	Loss: 0.6466
Training Epoch: 46 [9472/50176]	Loss: 0.5548
Training Epoch: 46 [9728/50176]	Loss: 0.4818
Training Epoch: 46 [9984/50176]	Loss: 0.6021
Training Epoch: 46 [10240/50176]	Loss: 0.6370
Training Epoch: 46 [10496/50176]	Loss: 0.7323
Training Epoch: 46 [10752/50176]	Loss: 0.6116
Training Epoch: 46 [11008/50176]	Loss: 0.5254
Training Epoch: 46 [11264/50176]	Loss: 0.5256
Training Epoch: 46 [11520/50176]	Loss: 0.5066
Training Epoch: 46 [11776/50176]	Loss: 0.5627
Training Epoch: 46 [12032/50176]	Loss: 0.5524
Training Epoch: 46 [12288/50176]	Loss: 0.6245
Training Epoch: 46 [12544/50176]	Loss: 0.6293
Training Epoch: 46 [12800/50176]	Loss: 0.4857
Training Epoch: 46 [13056/50176]	Loss: 0.6277
Training Epoch: 46 [13312/50176]	Loss: 0.6358
Training Epoch: 46 [13568/50176]	Loss: 0.4923
Training Epoch: 46 [13824/50176]	Loss: 0.6205
Training Epoch: 46 [14080/50176]	Loss: 0.4577
Training Epoch: 46 [14336/50176]	Loss: 0.5821
Training Epoch: 46 [14592/50176]	Loss: 0.6468
Training Epoch: 46 [14848/50176]	Loss: 0.5445
Training Epoch: 46 [15104/50176]	Loss: 0.5734
Training Epoch: 46 [15360/50176]	Loss: 0.4816
Training Epoch: 46 [15616/50176]	Loss: 0.6366
Training Epoch: 46 [15872/50176]	Loss: 0.7006
Training Epoch: 46 [16128/50176]	Loss: 0.6081
Training Epoch: 46 [16384/50176]	Loss: 0.5466
Training Epoch: 46 [16640/50176]	Loss: 0.6255
Training Epoch: 46 [16896/50176]	Loss: 0.5992
Training Epoch: 46 [17152/50176]	Loss: 0.5488
Training Epoch: 46 [17408/50176]	Loss: 0.6231
Training Epoch: 46 [17664/50176]	Loss: 0.5770
Training Epoch: 46 [17920/50176]	Loss: 0.6084
Training Epoch: 46 [18176/50176]	Loss: 0.5506
Training Epoch: 46 [18432/50176]	Loss: 0.5425
Training Epoch: 46 [18688/50176]	Loss: 0.5452
Training Epoch: 46 [18944/50176]	Loss: 0.4788
Training Epoch: 46 [19200/50176]	Loss: 0.6086
Training Epoch: 46 [19456/50176]	Loss: 0.6255
Training Epoch: 46 [19712/50176]	Loss: 0.4937
Training Epoch: 46 [19968/50176]	Loss: 0.5470
Training Epoch: 46 [20224/50176]	Loss: 0.5192
Training Epoch: 46 [20480/50176]	Loss: 0.7506
Training Epoch: 46 [20736/50176]	Loss: 0.6274
Training Epoch: 46 [20992/50176]	Loss: 0.5735
Training Epoch: 46 [21248/50176]	Loss: 0.6508
Training Epoch: 46 [21504/50176]	Loss: 0.7038
Training Epoch: 46 [21760/50176]	Loss: 0.5497
Training Epoch: 46 [22016/50176]	Loss: 0.5296
Training Epoch: 46 [22272/50176]	Loss: 0.6335
Training Epoch: 46 [22528/50176]	Loss: 0.6171
Training Epoch: 46 [22784/50176]	Loss: 0.6485
Training Epoch: 46 [23040/50176]	Loss: 0.7173
Training Epoch: 46 [23296/50176]	Loss: 0.6001
Training Epoch: 46 [23552/50176]	Loss: 0.5756
Training Epoch: 46 [23808/50176]	Loss: 0.5784
Training Epoch: 46 [24064/50176]	Loss: 0.5881
Training Epoch: 46 [24320/50176]	Loss: 0.7077
Training Epoch: 46 [24576/50176]	Loss: 0.6378
Training Epoch: 46 [24832/50176]	Loss: 0.5834
Training Epoch: 46 [25088/50176]	Loss: 0.4376
Training Epoch: 46 [25344/50176]	Loss: 0.5245
Training Epoch: 46 [25600/50176]	Loss: 0.6441
Training Epoch: 46 [25856/50176]	Loss: 0.6556
Training Epoch: 46 [26112/50176]	Loss: 0.6906
Training Epoch: 46 [26368/50176]	Loss: 0.6729
Training Epoch: 46 [26624/50176]	Loss: 0.5715
Training Epoch: 46 [26880/50176]	Loss: 0.5470
Training Epoch: 46 [27136/50176]	Loss: 0.5808
Training Epoch: 46 [27392/50176]	Loss: 0.6107
Training Epoch: 46 [27648/50176]	Loss: 0.6457
Training Epoch: 46 [27904/50176]	Loss: 0.5923
Training Epoch: 46 [28160/50176]	Loss: 0.6052
Training Epoch: 46 [28416/50176]	Loss: 0.5745
Training Epoch: 46 [28672/50176]	Loss: 0.6007
Training Epoch: 46 [28928/50176]	Loss: 0.6089
Training Epoch: 46 [29184/50176]	Loss: 0.6599
Training Epoch: 46 [29440/50176]	Loss: 0.5771
Training Epoch: 46 [29696/50176]	Loss: 0.5289
Training Epoch: 46 [29952/50176]	Loss: 0.7160
Training Epoch: 46 [30208/50176]	Loss: 0.7496
Training Epoch: 46 [30464/50176]	Loss: 0.5451
Training Epoch: 46 [30720/50176]	Loss: 0.7446
Training Epoch: 46 [30976/50176]	Loss: 0.5934
Training Epoch: 46 [31232/50176]	Loss: 0.5141
Training Epoch: 46 [31488/50176]	Loss: 0.6815
Training Epoch: 46 [31744/50176]	Loss: 0.5793
Training Epoch: 46 [32000/50176]	Loss: 0.5388
Training Epoch: 46 [32256/50176]	Loss: 0.8219
Training Epoch: 46 [32512/50176]	Loss: 0.6519
Training Epoch: 46 [32768/50176]	Loss: 0.6251
Training Epoch: 46 [33024/50176]	Loss: 0.6703
Training Epoch: 46 [33280/50176]	Loss: 0.7219
Training Epoch: 46 [33536/50176]	Loss: 0.5829
Training Epoch: 46 [33792/50176]	Loss: 0.7368
Training Epoch: 46 [34048/50176]	Loss: 0.5207
Training Epoch: 46 [34304/50176]	Loss: 0.6709
Training Epoch: 46 [34560/50176]	Loss: 0.6600
Training Epoch: 46 [34816/50176]	Loss: 0.6831
Training Epoch: 46 [35072/50176]	Loss: 0.5922
Training Epoch: 46 [35328/50176]	Loss: 0.7319
Training Epoch: 46 [35584/50176]	Loss: 0.6098
Training Epoch: 46 [35840/50176]	Loss: 0.6829
Training Epoch: 46 [36096/50176]	Loss: 0.6034
Training Epoch: 46 [36352/50176]	Loss: 0.5799
Training Epoch: 46 [36608/50176]	Loss: 0.6859
Training Epoch: 46 [36864/50176]	Loss: 0.8072
Training Epoch: 46 [37120/50176]	Loss: 0.5611
Training Epoch: 46 [37376/50176]	Loss: 0.5022
Training Epoch: 46 [37632/50176]	Loss: 0.7446
Training Epoch: 46 [37888/50176]	Loss: 0.6040
Training Epoch: 46 [38144/50176]	Loss: 0.7336
Training Epoch: 46 [38400/50176]	Loss: 0.6562
Training Epoch: 46 [38656/50176]	Loss: 0.7127
Training Epoch: 46 [38912/50176]	Loss: 0.5454
Training Epoch: 46 [39168/50176]	Loss: 0.5602
Training Epoch: 46 [39424/50176]	Loss: 0.6879
Training Epoch: 46 [39680/50176]	Loss: 0.6694
Training Epoch: 46 [39936/50176]	Loss: 0.6977
Training Epoch: 46 [40192/50176]	Loss: 0.6553
Training Epoch: 46 [40448/50176]	Loss: 0.6440
Training Epoch: 46 [40704/50176]	Loss: 0.7192
Training Epoch: 46 [40960/50176]	Loss: 0.5321
Training Epoch: 46 [41216/50176]	Loss: 0.7151
Training Epoch: 46 [41472/50176]	Loss: 0.5905
Training Epoch: 46 [41728/50176]	Loss: 0.6603
Training Epoch: 46 [41984/50176]	Loss: 0.6593
Training Epoch: 46 [42240/50176]	Loss: 0.7148
Training Epoch: 46 [42496/50176]	Loss: 0.5682
Training Epoch: 46 [42752/50176]	Loss: 0.6941
Training Epoch: 46 [43008/50176]	Loss: 0.6277
Training Epoch: 46 [43264/50176]	Loss: 0.7788
Training Epoch: 46 [43520/50176]	Loss: 0.6743
Training Epoch: 46 [43776/50176]	Loss: 0.6902
Training Epoch: 46 [44032/50176]	Loss: 0.5297
Training Epoch: 46 [44288/50176]	Loss: 0.5290
Training Epoch: 46 [44544/50176]	Loss: 0.6214
Training Epoch: 46 [44800/50176]	Loss: 0.5823
Training Epoch: 46 [45056/50176]	Loss: 0.5784
Training Epoch: 46 [45312/50176]	Loss: 0.6380
Training Epoch: 46 [45568/50176]	Loss: 0.5570
Training Epoch: 46 [45824/50176]	Loss: 0.8420
Training Epoch: 46 [46080/50176]	Loss: 0.6403
Training Epoch: 46 [46336/50176]	Loss: 0.6792
Training Epoch: 46 [46592/50176]	Loss: 0.6838
Training Epoch: 46 [46848/50176]	Loss: 0.6741
Training Epoch: 46 [47104/50176]	Loss: 0.5736
Training Epoch: 46 [47360/50176]	Loss: 0.6238
Training Epoch: 46 [47616/50176]	Loss: 0.5697
Training Epoch: 46 [47872/50176]	Loss: 0.6869
Training Epoch: 46 [48128/50176]	Loss: 0.5764
Training Epoch: 46 [48384/50176]	Loss: 0.6916
Training Epoch: 46 [48640/50176]	Loss: 0.6441
Training Epoch: 46 [48896/50176]	Loss: 0.6186
Training Epoch: 46 [49152/50176]	Loss: 0.6467
Training Epoch: 46 [49408/50176]	Loss: 0.6095
Training Epoch: 46 [49664/50176]	Loss: 0.6243
Training Epoch: 46 [49920/50176]	Loss: 0.6132
Training Epoch: 46 [50176/50176]	Loss: 0.6647
Validation Epoch: 46, Average loss: 0.0082, Accuracy: 0.5517
Training Epoch: 47 [256/50176]	Loss: 0.5206
Training Epoch: 47 [512/50176]	Loss: 0.6152
Training Epoch: 47 [768/50176]	Loss: 0.4442
Training Epoch: 47 [1024/50176]	Loss: 0.4165
Training Epoch: 47 [1280/50176]	Loss: 0.4916
Training Epoch: 47 [1536/50176]	Loss: 0.4973
Training Epoch: 47 [1792/50176]	Loss: 0.5162
Training Epoch: 47 [2048/50176]	Loss: 0.6261
Training Epoch: 47 [2304/50176]	Loss: 0.6305
Training Epoch: 47 [2560/50176]	Loss: 0.5337
Training Epoch: 47 [2816/50176]	Loss: 0.5351
Training Epoch: 47 [3072/50176]	Loss: 0.4881
Training Epoch: 47 [3328/50176]	Loss: 0.5671
Training Epoch: 47 [3584/50176]	Loss: 0.4976
Training Epoch: 47 [3840/50176]	Loss: 0.4635
Training Epoch: 47 [4096/50176]	Loss: 0.5509
Training Epoch: 47 [4352/50176]	Loss: 0.5047
Training Epoch: 47 [4608/50176]	Loss: 0.6407
Training Epoch: 47 [4864/50176]	Loss: 0.4471
Training Epoch: 47 [5120/50176]	Loss: 0.5674
Training Epoch: 47 [5376/50176]	Loss: 0.6936
Training Epoch: 47 [5632/50176]	Loss: 0.5186
Training Epoch: 47 [5888/50176]	Loss: 0.5417
Training Epoch: 47 [6144/50176]	Loss: 0.5173
Training Epoch: 47 [6400/50176]	Loss: 0.5535
Training Epoch: 47 [6656/50176]	Loss: 0.4972
Training Epoch: 47 [6912/50176]	Loss: 0.4935
Training Epoch: 47 [7168/50176]	Loss: 0.6812
Training Epoch: 47 [7424/50176]	Loss: 0.5827
Training Epoch: 47 [7680/50176]	Loss: 0.5662
Training Epoch: 47 [7936/50176]	Loss: 0.5221
Training Epoch: 47 [8192/50176]	Loss: 0.5536
Training Epoch: 47 [8448/50176]	Loss: 0.4868
Training Epoch: 47 [8704/50176]	Loss: 0.4421
Training Epoch: 47 [8960/50176]	Loss: 0.5337
Training Epoch: 47 [9216/50176]	Loss: 0.4428
Training Epoch: 47 [9472/50176]	Loss: 0.3953
Training Epoch: 47 [9728/50176]	Loss: 0.5543
Training Epoch: 47 [9984/50176]	Loss: 0.6375
Training Epoch: 47 [10240/50176]	Loss: 0.5530
Training Epoch: 47 [10496/50176]	Loss: 0.6927
Training Epoch: 47 [10752/50176]	Loss: 0.6559
Training Epoch: 47 [11008/50176]	Loss: 0.4711
Training Epoch: 47 [11264/50176]	Loss: 0.5553
Training Epoch: 47 [11520/50176]	Loss: 0.6328
Training Epoch: 47 [11776/50176]	Loss: 0.5748
Training Epoch: 47 [12032/50176]	Loss: 0.5512
Training Epoch: 47 [12288/50176]	Loss: 0.5009
Training Epoch: 47 [12544/50176]	Loss: 0.6122
Training Epoch: 47 [12800/50176]	Loss: 0.7127
Training Epoch: 47 [13056/50176]	Loss: 0.4682
Training Epoch: 47 [13312/50176]	Loss: 0.5641
Training Epoch: 47 [13568/50176]	Loss: 0.5912
Training Epoch: 47 [13824/50176]	Loss: 0.5999
Training Epoch: 47 [14080/50176]	Loss: 0.5910
Training Epoch: 47 [14336/50176]	Loss: 0.5799
Training Epoch: 47 [14592/50176]	Loss: 0.5376
Training Epoch: 47 [14848/50176]	Loss: 0.5555
Training Epoch: 47 [15104/50176]	Loss: 0.5632
Training Epoch: 47 [15360/50176]	Loss: 0.5737
Training Epoch: 47 [15616/50176]	Loss: 0.5860
Training Epoch: 47 [15872/50176]	Loss: 0.6006
Training Epoch: 47 [16128/50176]	Loss: 0.5410
Training Epoch: 47 [16384/50176]	Loss: 0.5863
Training Epoch: 47 [16640/50176]	Loss: 0.4756
Training Epoch: 47 [16896/50176]	Loss: 0.5985
Training Epoch: 47 [17152/50176]	Loss: 0.5418
Training Epoch: 47 [17408/50176]	Loss: 0.5531
Training Epoch: 47 [17664/50176]	Loss: 0.5927
Training Epoch: 47 [17920/50176]	Loss: 0.7035
Training Epoch: 47 [18176/50176]	Loss: 0.6648
Training Epoch: 47 [18432/50176]	Loss: 0.5988
Training Epoch: 47 [18688/50176]	Loss: 0.5349
Training Epoch: 47 [18944/50176]	Loss: 0.6501
Training Epoch: 47 [19200/50176]	Loss: 0.6684
Training Epoch: 47 [19456/50176]	Loss: 0.5561
Training Epoch: 47 [19712/50176]	Loss: 0.5489
Training Epoch: 47 [19968/50176]	Loss: 0.4377
Training Epoch: 47 [20224/50176]	Loss: 0.6600
Training Epoch: 47 [20480/50176]	Loss: 0.5547
Training Epoch: 47 [20736/50176]	Loss: 0.5581
Training Epoch: 47 [20992/50176]	Loss: 0.5523
Training Epoch: 47 [21248/50176]	Loss: 0.5465
Training Epoch: 47 [21504/50176]	Loss: 0.4678
Training Epoch: 47 [21760/50176]	Loss: 0.5841
Training Epoch: 47 [22016/50176]	Loss: 0.6947
Training Epoch: 47 [22272/50176]	Loss: 0.6437
Training Epoch: 47 [22528/50176]	Loss: 0.5881
Training Epoch: 47 [22784/50176]	Loss: 0.5373
Training Epoch: 47 [23040/50176]	Loss: 0.7186
Training Epoch: 47 [23296/50176]	Loss: 0.5177
Training Epoch: 47 [23552/50176]	Loss: 0.6171
Training Epoch: 47 [23808/50176]	Loss: 0.4913
Training Epoch: 47 [24064/50176]	Loss: 0.5597
Training Epoch: 47 [24320/50176]	Loss: 0.6680
Training Epoch: 47 [24576/50176]	Loss: 0.7378
Training Epoch: 47 [24832/50176]	Loss: 0.6594
Training Epoch: 47 [25088/50176]	Loss: 0.5975
Training Epoch: 47 [25344/50176]	Loss: 0.5612
Training Epoch: 47 [25600/50176]	Loss: 0.6330
Training Epoch: 47 [25856/50176]	Loss: 0.6340
Training Epoch: 47 [26112/50176]	Loss: 0.5940
Training Epoch: 47 [26368/50176]	Loss: 0.4898
Training Epoch: 47 [26624/50176]	Loss: 0.5764
Training Epoch: 47 [26880/50176]	Loss: 0.6771
Training Epoch: 47 [27136/50176]	Loss: 0.7974
Training Epoch: 47 [27392/50176]	Loss: 0.5629
Training Epoch: 47 [27648/50176]	Loss: 0.5926
Training Epoch: 47 [27904/50176]	Loss: 0.6442
Training Epoch: 47 [28160/50176]	Loss: 0.6305
Training Epoch: 47 [28416/50176]	Loss: 0.6656
Training Epoch: 47 [28672/50176]	Loss: 0.6775
Training Epoch: 47 [28928/50176]	Loss: 0.4663
Training Epoch: 47 [29184/50176]	Loss: 0.5586
Training Epoch: 47 [29440/50176]	Loss: 0.6508
Training Epoch: 47 [29696/50176]	Loss: 0.5596
Training Epoch: 47 [29952/50176]	Loss: 0.6123
Training Epoch: 47 [30208/50176]	Loss: 0.6790
Training Epoch: 47 [30464/50176]	Loss: 0.6514
Training Epoch: 47 [30720/50176]	Loss: 0.7727
Training Epoch: 47 [30976/50176]	Loss: 0.6446
Training Epoch: 47 [31232/50176]	Loss: 0.6302
Training Epoch: 47 [31488/50176]	Loss: 0.5772
Training Epoch: 47 [31744/50176]	Loss: 0.6537
Training Epoch: 47 [32000/50176]	Loss: 0.6693
Training Epoch: 47 [32256/50176]	Loss: 0.6230
Training Epoch: 47 [32512/50176]	Loss: 0.7235
Training Epoch: 47 [32768/50176]	Loss: 0.7210
Training Epoch: 47 [33024/50176]	Loss: 0.5562
Training Epoch: 47 [33280/50176]	Loss: 0.5466
Training Epoch: 47 [33536/50176]	Loss: 0.6355
Training Epoch: 47 [33792/50176]	Loss: 0.6307
Training Epoch: 47 [34048/50176]	Loss: 0.6083
Training Epoch: 47 [34304/50176]	Loss: 0.6795
Training Epoch: 47 [34560/50176]	Loss: 0.6745
Training Epoch: 47 [34816/50176]	Loss: 0.6738
Training Epoch: 47 [35072/50176]	Loss: 0.6311
Training Epoch: 47 [35328/50176]	Loss: 0.6655
Training Epoch: 47 [35584/50176]	Loss: 0.6156
Training Epoch: 47 [35840/50176]	Loss: 0.5874
Training Epoch: 47 [36096/50176]	Loss: 0.5941
Training Epoch: 47 [36352/50176]	Loss: 0.4839
Training Epoch: 47 [36608/50176]	Loss: 0.4835
Training Epoch: 47 [36864/50176]	Loss: 0.6226
Training Epoch: 47 [37120/50176]	Loss: 0.5952
Training Epoch: 47 [37376/50176]	Loss: 0.6144
Training Epoch: 47 [37632/50176]	Loss: 0.4956
Training Epoch: 47 [37888/50176]	Loss: 0.6937
Training Epoch: 47 [38144/50176]	Loss: 0.5805
Training Epoch: 47 [38400/50176]	Loss: 0.7007
Training Epoch: 47 [38656/50176]	Loss: 0.5563
Training Epoch: 47 [38912/50176]	Loss: 0.7104
Training Epoch: 47 [39168/50176]	Loss: 0.5683
Training Epoch: 47 [39424/50176]	Loss: 0.6056
Training Epoch: 47 [39680/50176]	Loss: 0.5896
Training Epoch: 47 [39936/50176]	Loss: 0.5611
Training Epoch: 47 [40192/50176]	Loss: 0.6217
Training Epoch: 47 [40448/50176]	Loss: 0.7554
Training Epoch: 47 [40704/50176]	Loss: 0.7050
Training Epoch: 47 [40960/50176]	Loss: 0.6832
Training Epoch: 47 [41216/50176]	Loss: 0.7118
Training Epoch: 47 [41472/50176]	Loss: 0.6337
Training Epoch: 47 [41728/50176]	Loss: 0.7579
Training Epoch: 47 [41984/50176]	Loss: 0.6222
Training Epoch: 47 [42240/50176]	Loss: 0.5862
Training Epoch: 47 [42496/50176]	Loss: 0.5869
Training Epoch: 47 [42752/50176]	Loss: 0.6350
Training Epoch: 47 [43008/50176]	Loss: 0.5591
Training Epoch: 47 [43264/50176]	Loss: 0.5081
Training Epoch: 47 [43520/50176]	Loss: 0.6822
Training Epoch: 47 [43776/50176]	Loss: 0.5928
Training Epoch: 47 [44032/50176]	Loss: 0.6621
Training Epoch: 47 [44288/50176]	Loss: 0.6396
Training Epoch: 47 [44544/50176]	Loss: 0.6561
Training Epoch: 47 [44800/50176]	Loss: 0.5586
Training Epoch: 47 [45056/50176]	Loss: 0.6153
Training Epoch: 47 [45312/50176]	Loss: 0.6865
Training Epoch: 47 [45568/50176]	Loss: 0.6113
Training Epoch: 47 [45824/50176]	Loss: 0.5969
Training Epoch: 47 [46080/50176]	Loss: 0.6373
Training Epoch: 47 [46336/50176]	Loss: 0.6534
Training Epoch: 47 [46592/50176]	Loss: 0.6461
Training Epoch: 47 [46848/50176]	Loss: 0.7026
Training Epoch: 47 [47104/50176]	Loss: 0.5619
Training Epoch: 47 [47360/50176]	Loss: 0.6185
Training Epoch: 47 [47616/50176]	Loss: 0.5616
Training Epoch: 47 [47872/50176]	Loss: 0.7421
Training Epoch: 47 [48128/50176]	Loss: 0.7351
Training Epoch: 47 [48384/50176]	Loss: 0.6351
Training Epoch: 47 [48640/50176]	Loss: 0.6343
Training Epoch: 47 [48896/50176]	Loss: 0.5683
Training Epoch: 47 [49152/50176]	Loss: 0.4939
Training Epoch: 47 [49408/50176]	Loss: 0.6484
Training Epoch: 47 [49664/50176]	Loss: 0.6578
Training Epoch: 47 [49920/50176]	Loss: 0.6791
Training Epoch: 47 [50176/50176]	Loss: 0.6448
Validation Epoch: 47, Average loss: 0.0091, Accuracy: 0.5364
Training Epoch: 48 [256/50176]	Loss: 0.3841
Training Epoch: 48 [512/50176]	Loss: 0.5313
Training Epoch: 48 [768/50176]	Loss: 0.5027
Training Epoch: 48 [1024/50176]	Loss: 0.6115
Training Epoch: 48 [1280/50176]	Loss: 0.5559
Training Epoch: 48 [1536/50176]	Loss: 0.4842
Training Epoch: 48 [1792/50176]	Loss: 0.5264
Training Epoch: 48 [2048/50176]	Loss: 0.5385
Training Epoch: 48 [2304/50176]	Loss: 0.4862
Training Epoch: 48 [2560/50176]	Loss: 0.4339
Training Epoch: 48 [2816/50176]	Loss: 0.4986
Training Epoch: 48 [3072/50176]	Loss: 0.4699
Training Epoch: 48 [3328/50176]	Loss: 0.6452
Training Epoch: 48 [3584/50176]	Loss: 0.5288
Training Epoch: 48 [3840/50176]	Loss: 0.4953
Training Epoch: 48 [4096/50176]	Loss: 0.4560
Training Epoch: 48 [4352/50176]	Loss: 0.6176
Training Epoch: 48 [4608/50176]	Loss: 0.6294
Training Epoch: 48 [4864/50176]	Loss: 0.4721
Training Epoch: 48 [5120/50176]	Loss: 0.5705
Training Epoch: 48 [5376/50176]	Loss: 0.6104
Training Epoch: 48 [5632/50176]	Loss: 0.5264
Training Epoch: 48 [5888/50176]	Loss: 0.5601
Training Epoch: 48 [6144/50176]	Loss: 0.6744
Training Epoch: 48 [6400/50176]	Loss: 0.6382
Training Epoch: 48 [6656/50176]	Loss: 0.6242
Training Epoch: 48 [6912/50176]	Loss: 0.6077
Training Epoch: 48 [7168/50176]	Loss: 0.5925
Training Epoch: 48 [7424/50176]	Loss: 0.5838
Training Epoch: 48 [7680/50176]	Loss: 0.5414
Training Epoch: 48 [7936/50176]	Loss: 0.5625
Training Epoch: 48 [8192/50176]	Loss: 0.5710
Training Epoch: 48 [8448/50176]	Loss: 0.6585
Training Epoch: 48 [8704/50176]	Loss: 0.6424
Training Epoch: 48 [8960/50176]	Loss: 0.6290
Training Epoch: 48 [9216/50176]	Loss: 0.3979
Training Epoch: 48 [9472/50176]	Loss: 0.5125
Training Epoch: 48 [9728/50176]	Loss: 0.5808
Training Epoch: 48 [9984/50176]	Loss: 0.5914
Training Epoch: 48 [10240/50176]	Loss: 0.5366
Training Epoch: 48 [10496/50176]	Loss: 0.6375
Training Epoch: 48 [10752/50176]	Loss: 0.6958
Training Epoch: 48 [11008/50176]	Loss: 0.5119
Training Epoch: 48 [11264/50176]	Loss: 0.5945
Training Epoch: 48 [11520/50176]	Loss: 0.4539
Training Epoch: 48 [11776/50176]	Loss: 0.6279
Training Epoch: 48 [12032/50176]	Loss: 0.5477
Training Epoch: 48 [12288/50176]	Loss: 0.6116
Training Epoch: 48 [12544/50176]	Loss: 0.5780
Training Epoch: 48 [12800/50176]	Loss: 0.5177
Training Epoch: 48 [13056/50176]	Loss: 0.4431
Training Epoch: 48 [13312/50176]	Loss: 0.6525
Training Epoch: 48 [13568/50176]	Loss: 0.6276
Training Epoch: 48 [13824/50176]	Loss: 0.5121
Training Epoch: 48 [14080/50176]	Loss: 0.5561
Training Epoch: 48 [14336/50176]	Loss: 0.6484
Training Epoch: 48 [14592/50176]	Loss: 0.6163
Training Epoch: 48 [14848/50176]	Loss: 0.5393
Training Epoch: 48 [15104/50176]	Loss: 0.5463
Training Epoch: 48 [15360/50176]	Loss: 0.5787
Training Epoch: 48 [15616/50176]	Loss: 0.6615
Training Epoch: 48 [15872/50176]	Loss: 0.4913
Training Epoch: 48 [16128/50176]	Loss: 0.5198
Training Epoch: 48 [16384/50176]	Loss: 0.5475
Training Epoch: 48 [16640/50176]	Loss: 0.5973
Training Epoch: 48 [16896/50176]	Loss: 0.5839
Training Epoch: 48 [17152/50176]	Loss: 0.4725
Training Epoch: 48 [17408/50176]	Loss: 0.5886
Training Epoch: 48 [17664/50176]	Loss: 0.5454
Training Epoch: 48 [17920/50176]	Loss: 0.6244
Training Epoch: 48 [18176/50176]	Loss: 0.7052
Training Epoch: 48 [18432/50176]	Loss: 0.4259
Training Epoch: 48 [18688/50176]	Loss: 0.6173
Training Epoch: 48 [18944/50176]	Loss: 0.5974
Training Epoch: 48 [19200/50176]	Loss: 0.5269
Training Epoch: 48 [19456/50176]	Loss: 0.6210
Training Epoch: 48 [19712/50176]	Loss: 0.6808
Training Epoch: 48 [19968/50176]	Loss: 0.4227
Training Epoch: 48 [20224/50176]	Loss: 0.6023
Training Epoch: 48 [20480/50176]	Loss: 0.6179
Training Epoch: 48 [20736/50176]	Loss: 0.5894
Training Epoch: 48 [20992/50176]	Loss: 0.5172
Training Epoch: 48 [21248/50176]	Loss: 0.5892
Training Epoch: 48 [21504/50176]	Loss: 0.5363
Training Epoch: 48 [21760/50176]	Loss: 0.5822
Training Epoch: 48 [22016/50176]	Loss: 0.4695
Training Epoch: 48 [22272/50176]	Loss: 0.6272
Training Epoch: 48 [22528/50176]	Loss: 0.5978
Training Epoch: 48 [22784/50176]	Loss: 0.6064
Training Epoch: 48 [23040/50176]	Loss: 0.6130
Training Epoch: 48 [23296/50176]	Loss: 0.6308
Training Epoch: 48 [23552/50176]	Loss: 0.4504
Training Epoch: 48 [23808/50176]	Loss: 0.6132
Training Epoch: 48 [24064/50176]	Loss: 0.6065
Training Epoch: 48 [24320/50176]	Loss: 0.5592
Training Epoch: 48 [24576/50176]	Loss: 0.4721
Training Epoch: 48 [24832/50176]	Loss: 0.7295
Training Epoch: 48 [25088/50176]	Loss: 0.5371
Training Epoch: 48 [25344/50176]	Loss: 0.6183
Training Epoch: 48 [25600/50176]	Loss: 0.6132
Training Epoch: 48 [25856/50176]	Loss: 0.5942
Training Epoch: 48 [26112/50176]	Loss: 0.5913
Training Epoch: 48 [26368/50176]	Loss: 0.6464
Training Epoch: 48 [26624/50176]	Loss: 0.6151
Training Epoch: 48 [26880/50176]	Loss: 0.6825
Training Epoch: 48 [27136/50176]	Loss: 0.7291
Training Epoch: 48 [27392/50176]	Loss: 0.5746
Training Epoch: 48 [27648/50176]	Loss: 0.7316
Training Epoch: 48 [27904/50176]	Loss: 0.5447
Training Epoch: 48 [28160/50176]	Loss: 0.5581
Training Epoch: 48 [28416/50176]	Loss: 0.5491
Training Epoch: 48 [28672/50176]	Loss: 0.6330
Training Epoch: 48 [28928/50176]	Loss: 0.5633
Training Epoch: 48 [29184/50176]	Loss: 0.5523
Training Epoch: 48 [29440/50176]	Loss: 0.5575
Training Epoch: 48 [29696/50176]	Loss: 0.6974
Training Epoch: 48 [29952/50176]	Loss: 0.5235
Training Epoch: 48 [30208/50176]	Loss: 0.5812
Training Epoch: 48 [30464/50176]	Loss: 0.6329
Training Epoch: 48 [30720/50176]	Loss: 0.4661
Training Epoch: 48 [30976/50176]	Loss: 0.7081
Training Epoch: 48 [31232/50176]	Loss: 0.5089
Training Epoch: 48 [31488/50176]	Loss: 0.5966
Training Epoch: 48 [31744/50176]	Loss: 0.6777
Training Epoch: 48 [32000/50176]	Loss: 0.6653
Training Epoch: 48 [32256/50176]	Loss: 0.6038
Training Epoch: 48 [32512/50176]	Loss: 0.6661
Training Epoch: 48 [32768/50176]	Loss: 0.5482
Training Epoch: 48 [33024/50176]	Loss: 0.5708
Training Epoch: 48 [33280/50176]	Loss: 0.6180
Training Epoch: 48 [33536/50176]	Loss: 0.6200
Training Epoch: 48 [33792/50176]	Loss: 0.6297
Training Epoch: 48 [34048/50176]	Loss: 0.6089
Training Epoch: 48 [34304/50176]	Loss: 0.7862
Training Epoch: 48 [34560/50176]	Loss: 0.6358
Training Epoch: 48 [34816/50176]	Loss: 0.5196
Training Epoch: 48 [35072/50176]	Loss: 0.6029
Training Epoch: 48 [35328/50176]	Loss: 0.5957
Training Epoch: 48 [35584/50176]	Loss: 0.6310
Training Epoch: 48 [35840/50176]	Loss: 0.6126
Training Epoch: 48 [36096/50176]	Loss: 0.5533
Training Epoch: 48 [36352/50176]	Loss: 0.5985
Training Epoch: 48 [36608/50176]	Loss: 0.5287
Training Epoch: 48 [36864/50176]	Loss: 0.6257
Training Epoch: 48 [37120/50176]	Loss: 0.6618
Training Epoch: 48 [37376/50176]	Loss: 0.5277
Training Epoch: 48 [37632/50176]	Loss: 0.6403
Training Epoch: 48 [37888/50176]	Loss: 0.6794
Training Epoch: 48 [38144/50176]	Loss: 0.6179
Training Epoch: 48 [38400/50176]	Loss: 0.6264
Training Epoch: 48 [38656/50176]	Loss: 0.6956
Training Epoch: 48 [38912/50176]	Loss: 0.6708
Training Epoch: 48 [39168/50176]	Loss: 0.6090
Training Epoch: 48 [39424/50176]	Loss: 0.5316
Training Epoch: 48 [39680/50176]	Loss: 0.5634
Training Epoch: 48 [39936/50176]	Loss: 0.6041
Training Epoch: 48 [40192/50176]	Loss: 0.6135
Training Epoch: 48 [40448/50176]	Loss: 0.6571
Training Epoch: 48 [40704/50176]	Loss: 0.5806
Training Epoch: 48 [40960/50176]	Loss: 0.8328
Training Epoch: 48 [41216/50176]	Loss: 0.7134
Training Epoch: 48 [41472/50176]	Loss: 0.7265
Training Epoch: 48 [41728/50176]	Loss: 0.6692
Training Epoch: 48 [41984/50176]	Loss: 0.5318
Training Epoch: 48 [42240/50176]	Loss: 0.5793
Training Epoch: 48 [42496/50176]	Loss: 0.6189
Training Epoch: 48 [42752/50176]	Loss: 0.4998
Training Epoch: 48 [43008/50176]	Loss: 0.5899
Training Epoch: 48 [43264/50176]	Loss: 0.6378
Training Epoch: 48 [43520/50176]	Loss: 0.6929
Training Epoch: 48 [43776/50176]	Loss: 0.4337
Training Epoch: 48 [44032/50176]	Loss: 0.4773
Training Epoch: 48 [44288/50176]	Loss: 0.5873
Training Epoch: 48 [44544/50176]	Loss: 0.6790
Training Epoch: 48 [44800/50176]	Loss: 0.6630
Training Epoch: 48 [45056/50176]	Loss: 0.6347
Training Epoch: 48 [45312/50176]	Loss: 0.5569
Training Epoch: 48 [45568/50176]	Loss: 0.6543
Training Epoch: 48 [45824/50176]	Loss: 0.6265
Training Epoch: 48 [46080/50176]	Loss: 0.7179
Training Epoch: 48 [46336/50176]	Loss: 0.6653
Training Epoch: 48 [46592/50176]	Loss: 0.6958
Training Epoch: 48 [46848/50176]	Loss: 0.6148
Training Epoch: 48 [47104/50176]	Loss: 0.7325
Training Epoch: 48 [47360/50176]	Loss: 0.6680
Training Epoch: 48 [47616/50176]	Loss: 0.5368
Training Epoch: 48 [47872/50176]	Loss: 0.5023
Training Epoch: 48 [48128/50176]	Loss: 0.5859
Training Epoch: 48 [48384/50176]	Loss: 0.6363
Training Epoch: 48 [48640/50176]	Loss: 0.5769
Training Epoch: 48 [48896/50176]	Loss: 0.5698
Training Epoch: 48 [49152/50176]	Loss: 0.5718
Training Epoch: 48 [49408/50176]	Loss: 0.6393
Training Epoch: 48 [49664/50176]	Loss: 0.6628
Training Epoch: 48 [49920/50176]	Loss: 0.6583
Training Epoch: 48 [50176/50176]	Loss: 0.6384
Validation Epoch: 48, Average loss: 0.0084, Accuracy: 0.5468
Training Epoch: 49 [256/50176]	Loss: 0.4607
Training Epoch: 49 [512/50176]	Loss: 0.5830
Training Epoch: 49 [768/50176]	Loss: 0.4729
Training Epoch: 49 [1024/50176]	Loss: 0.5064
Training Epoch: 49 [1280/50176]	Loss: 0.4892
Training Epoch: 49 [1536/50176]	Loss: 0.5866
Training Epoch: 49 [1792/50176]	Loss: 0.5537
Training Epoch: 49 [2048/50176]	Loss: 0.5873
Training Epoch: 49 [2304/50176]	Loss: 0.5634
Training Epoch: 49 [2560/50176]	Loss: 0.5696
Training Epoch: 49 [2816/50176]	Loss: 0.5439
Training Epoch: 49 [3072/50176]	Loss: 0.4501
Training Epoch: 49 [3328/50176]	Loss: 0.5332
Training Epoch: 49 [3584/50176]	Loss: 0.4746
Training Epoch: 49 [3840/50176]	Loss: 0.4731
Training Epoch: 49 [4096/50176]	Loss: 0.4944
Training Epoch: 49 [4352/50176]	Loss: 0.5772
Training Epoch: 49 [4608/50176]	Loss: 0.4844
Training Epoch: 49 [4864/50176]	Loss: 0.4301
Training Epoch: 49 [5120/50176]	Loss: 0.4210
Training Epoch: 49 [5376/50176]	Loss: 0.5187
Training Epoch: 49 [5632/50176]	Loss: 0.4886
Training Epoch: 49 [5888/50176]	Loss: 0.4532
Training Epoch: 49 [6144/50176]	Loss: 0.4940
Training Epoch: 49 [6400/50176]	Loss: 0.5006
Training Epoch: 49 [6656/50176]	Loss: 0.5335
Training Epoch: 49 [6912/50176]	Loss: 0.5432
Training Epoch: 49 [7168/50176]	Loss: 0.5461
Training Epoch: 49 [7424/50176]	Loss: 0.5927
Training Epoch: 49 [7680/50176]	Loss: 0.5292
Training Epoch: 49 [7936/50176]	Loss: 0.4567
Training Epoch: 49 [8192/50176]	Loss: 0.6340
Training Epoch: 49 [8448/50176]	Loss: 0.4854
Training Epoch: 49 [8704/50176]	Loss: 0.5386
Training Epoch: 49 [8960/50176]	Loss: 0.5013
Training Epoch: 49 [9216/50176]	Loss: 0.5116
Training Epoch: 49 [9472/50176]	Loss: 0.5230
Training Epoch: 49 [9728/50176]	Loss: 0.5486
Training Epoch: 49 [9984/50176]	Loss: 0.5146
Training Epoch: 49 [10240/50176]	Loss: 0.3738
Training Epoch: 49 [10496/50176]	Loss: 0.6117
Training Epoch: 49 [10752/50176]	Loss: 0.5077
Training Epoch: 49 [11008/50176]	Loss: 0.4125
Training Epoch: 49 [11264/50176]	Loss: 0.4728
Training Epoch: 49 [11520/50176]	Loss: 0.5033
Training Epoch: 49 [11776/50176]	Loss: 0.5682
Training Epoch: 49 [12032/50176]	Loss: 0.4840
Training Epoch: 49 [12288/50176]	Loss: 0.6360
Training Epoch: 49 [12544/50176]	Loss: 0.5771
Training Epoch: 49 [12800/50176]	Loss: 0.7180
Training Epoch: 49 [13056/50176]	Loss: 0.5030
Training Epoch: 49 [13312/50176]	Loss: 0.6131
Training Epoch: 49 [13568/50176]	Loss: 0.6135
Training Epoch: 49 [13824/50176]	Loss: 0.5476
Training Epoch: 49 [14080/50176]	Loss: 0.7226
Training Epoch: 49 [14336/50176]	Loss: 0.4536
Training Epoch: 49 [14592/50176]	Loss: 0.6764
Training Epoch: 49 [14848/50176]	Loss: 0.5516
Training Epoch: 49 [15104/50176]	Loss: 0.4936
Training Epoch: 49 [15360/50176]	Loss: 0.5432
Training Epoch: 49 [15616/50176]	Loss: 0.5665
Training Epoch: 49 [15872/50176]	Loss: 0.5714
Training Epoch: 49 [16128/50176]	Loss: 0.4738
Training Epoch: 49 [16384/50176]	Loss: 0.5055
Training Epoch: 49 [16640/50176]	Loss: 0.5598
Training Epoch: 49 [16896/50176]	Loss: 0.4316
Training Epoch: 49 [17152/50176]	Loss: 0.5941
Training Epoch: 49 [17408/50176]	Loss: 0.5643
Training Epoch: 49 [17664/50176]	Loss: 0.5753
Training Epoch: 49 [17920/50176]	Loss: 0.4487
Training Epoch: 49 [18176/50176]	Loss: 0.4675
Training Epoch: 49 [18432/50176]	Loss: 0.6380
Training Epoch: 49 [18688/50176]	Loss: 0.7120
Training Epoch: 49 [18944/50176]	Loss: 0.5853
Training Epoch: 49 [19200/50176]	Loss: 0.5721
Training Epoch: 49 [19456/50176]	Loss: 0.7840
Training Epoch: 49 [19712/50176]	Loss: 0.6938
Training Epoch: 49 [19968/50176]	Loss: 0.4862
Training Epoch: 49 [20224/50176]	Loss: 0.5664
Training Epoch: 49 [20480/50176]	Loss: 0.5831
Training Epoch: 49 [20736/50176]	Loss: 0.5503
Training Epoch: 49 [20992/50176]	Loss: 0.5326
Training Epoch: 49 [21248/50176]	Loss: 0.6939
Training Epoch: 49 [21504/50176]	Loss: 0.6856
Training Epoch: 49 [21760/50176]	Loss: 0.4705
Training Epoch: 49 [22016/50176]	Loss: 0.6244
Training Epoch: 49 [22272/50176]	Loss: 0.5410
Training Epoch: 49 [22528/50176]	Loss: 0.6366
Training Epoch: 49 [22784/50176]	Loss: 0.4916
Training Epoch: 49 [23040/50176]	Loss: 0.5238
Training Epoch: 49 [23296/50176]	Loss: 0.6572
Training Epoch: 49 [23552/50176]	Loss: 0.5968
Training Epoch: 49 [23808/50176]	Loss: 0.6242
Training Epoch: 49 [24064/50176]	Loss: 0.6733
Training Epoch: 49 [24320/50176]	Loss: 0.4499
Training Epoch: 49 [24576/50176]	Loss: 0.6017
Training Epoch: 49 [24832/50176]	Loss: 0.5421
Training Epoch: 49 [25088/50176]	Loss: 0.5747
Training Epoch: 49 [25344/50176]	Loss: 0.4878
Training Epoch: 49 [25600/50176]	Loss: 0.6093
Training Epoch: 49 [25856/50176]	Loss: 0.5681
Training Epoch: 49 [26112/50176]	Loss: 0.6914
Training Epoch: 49 [26368/50176]	Loss: 0.6750
Training Epoch: 49 [26624/50176]	Loss: 0.5101
Training Epoch: 49 [26880/50176]	Loss: 0.6274
Training Epoch: 49 [27136/50176]	Loss: 0.5418
Training Epoch: 49 [27392/50176]	Loss: 0.7120
Training Epoch: 49 [27648/50176]	Loss: 0.4808
Training Epoch: 49 [27904/50176]	Loss: 0.5526
Training Epoch: 49 [28160/50176]	Loss: 0.5739
Training Epoch: 49 [28416/50176]	Loss: 0.6848
Training Epoch: 49 [28672/50176]	Loss: 0.6233
Training Epoch: 49 [28928/50176]	Loss: 0.5195
Training Epoch: 49 [29184/50176]	Loss: 0.4752
Training Epoch: 49 [29440/50176]	Loss: 0.5699
Training Epoch: 49 [29696/50176]	Loss: 0.7304
Training Epoch: 49 [29952/50176]	Loss: 0.6126
Training Epoch: 49 [30208/50176]	Loss: 0.5434
Training Epoch: 49 [30464/50176]	Loss: 0.6650
Training Epoch: 49 [30720/50176]	Loss: 0.6383
Training Epoch: 49 [30976/50176]	Loss: 0.5885
Training Epoch: 49 [31232/50176]	Loss: 0.5145
Training Epoch: 49 [31488/50176]	Loss: 0.6893
Training Epoch: 49 [31744/50176]	Loss: 0.6114
Training Epoch: 49 [32000/50176]	Loss: 0.5900
Training Epoch: 49 [32256/50176]	Loss: 0.5237
Training Epoch: 49 [32512/50176]	Loss: 0.6027
Training Epoch: 49 [32768/50176]	Loss: 0.5393
Training Epoch: 49 [33024/50176]	Loss: 0.5791
Training Epoch: 49 [33280/50176]	Loss: 0.6215
Training Epoch: 49 [33536/50176]	Loss: 0.5636
Training Epoch: 49 [33792/50176]	Loss: 0.6423
Training Epoch: 49 [34048/50176]	Loss: 0.5292
Training Epoch: 49 [34304/50176]	Loss: 0.5420
Training Epoch: 49 [34560/50176]	Loss: 0.7405
Training Epoch: 49 [34816/50176]	Loss: 0.5726
Training Epoch: 49 [35072/50176]	Loss: 0.6588
Training Epoch: 49 [35328/50176]	Loss: 0.5593
Training Epoch: 49 [35584/50176]	Loss: 0.6405
Training Epoch: 49 [35840/50176]	Loss: 0.4835
Training Epoch: 49 [36096/50176]	Loss: 0.5218
Training Epoch: 49 [36352/50176]	Loss: 0.7653
Training Epoch: 49 [36608/50176]	Loss: 0.4922
Training Epoch: 49 [36864/50176]	Loss: 0.6002
Training Epoch: 49 [37120/50176]	Loss: 0.5485
Training Epoch: 49 [37376/50176]	Loss: 0.6046
Training Epoch: 49 [37632/50176]	Loss: 0.6555
Training Epoch: 49 [37888/50176]	Loss: 0.5284
Training Epoch: 49 [38144/50176]	Loss: 0.6550
Training Epoch: 49 [38400/50176]	Loss: 0.5815
Training Epoch: 49 [38656/50176]	Loss: 0.6095
Training Epoch: 49 [38912/50176]	Loss: 0.5725
Training Epoch: 49 [39168/50176]	Loss: 0.5534
Training Epoch: 49 [39424/50176]	Loss: 0.6369
Training Epoch: 49 [39680/50176]	Loss: 0.7706
Training Epoch: 49 [39936/50176]	Loss: 0.6685
Training Epoch: 49 [40192/50176]	Loss: 0.6490
Training Epoch: 49 [40448/50176]	Loss: 0.5459
Training Epoch: 49 [40704/50176]	Loss: 0.6504
Training Epoch: 49 [40960/50176]	Loss: 0.7591
Training Epoch: 49 [41216/50176]	Loss: 0.4630
Training Epoch: 49 [41472/50176]	Loss: 0.6422
Training Epoch: 49 [41728/50176]	Loss: 0.5626
Training Epoch: 49 [41984/50176]	Loss: 0.7666
Training Epoch: 49 [42240/50176]	Loss: 0.4686
Training Epoch: 49 [42496/50176]	Loss: 0.6266
Training Epoch: 49 [42752/50176]	Loss: 0.5566
Training Epoch: 49 [43008/50176]	Loss: 0.5642
Training Epoch: 49 [43264/50176]	Loss: 0.6539
Training Epoch: 49 [43520/50176]	Loss: 0.6348
Training Epoch: 49 [43776/50176]	Loss: 0.4974
Training Epoch: 49 [44032/50176]	Loss: 0.6166
Training Epoch: 49 [44288/50176]	Loss: 0.6704
Training Epoch: 49 [44544/50176]	Loss: 0.4826
Training Epoch: 49 [44800/50176]	Loss: 0.6202
Training Epoch: 49 [45056/50176]	Loss: 0.6742
Training Epoch: 49 [45312/50176]	Loss: 0.5690
Training Epoch: 49 [45568/50176]	Loss: 0.5562
Training Epoch: 49 [45824/50176]	Loss: 0.5919
Training Epoch: 49 [46080/50176]	Loss: 0.6518
Training Epoch: 49 [46336/50176]	Loss: 0.5208
Training Epoch: 49 [46592/50176]	Loss: 0.5170
Training Epoch: 49 [46848/50176]	Loss: 0.5576
Training Epoch: 49 [47104/50176]	Loss: 0.6769
Training Epoch: 49 [47360/50176]	Loss: 0.5913
Training Epoch: 49 [47616/50176]	Loss: 0.5831
Training Epoch: 49 [47872/50176]	Loss: 0.7300
Training Epoch: 49 [48128/50176]	Loss: 0.6255
Training Epoch: 49 [48384/50176]	Loss: 0.5496
Training Epoch: 49 [48640/50176]	Loss: 0.5948
Training Epoch: 49 [48896/50176]	Loss: 0.5580
Training Epoch: 49 [49152/50176]	Loss: 0.5599
Training Epoch: 49 [49408/50176]	Loss: 0.5313
Training Epoch: 49 [49664/50176]	Loss: 0.6337
Training Epoch: 49 [49920/50176]	Loss: 0.5555
Training Epoch: 49 [50176/50176]	Loss: 0.6414
Validation Epoch: 49, Average loss: 0.0094, Accuracy: 0.5380
Training Epoch: 50 [256/50176]	Loss: 0.4654
Training Epoch: 50 [512/50176]	Loss: 0.5473
Training Epoch: 50 [768/50176]	Loss: 0.5814
Training Epoch: 50 [1024/50176]	Loss: 0.5152
Training Epoch: 50 [1280/50176]	Loss: 0.5313
Training Epoch: 50 [1536/50176]	Loss: 0.4535
Training Epoch: 50 [1792/50176]	Loss: 0.5767
Training Epoch: 50 [2048/50176]	Loss: 0.4427
Training Epoch: 50 [2304/50176]	Loss: 0.4944
Training Epoch: 50 [2560/50176]	Loss: 0.5783
Training Epoch: 50 [2816/50176]	Loss: 0.5299
Training Epoch: 50 [3072/50176]	Loss: 0.5084
Training Epoch: 50 [3328/50176]	Loss: 0.4967
Training Epoch: 50 [3584/50176]	Loss: 0.5895
Training Epoch: 50 [3840/50176]	Loss: 0.4649
Training Epoch: 50 [4096/50176]	Loss: 0.5607
Training Epoch: 50 [4352/50176]	Loss: 0.5354
Training Epoch: 50 [4608/50176]	Loss: 0.5265
Training Epoch: 50 [4864/50176]	Loss: 0.5701
Training Epoch: 50 [5120/50176]	Loss: 0.5137
Training Epoch: 50 [5376/50176]	Loss: 0.5701
Training Epoch: 50 [5632/50176]	Loss: 0.4710
Training Epoch: 50 [5888/50176]	Loss: 0.4831
Training Epoch: 50 [6144/50176]	Loss: 0.5811
Training Epoch: 50 [6400/50176]	Loss: 0.6438
Training Epoch: 50 [6656/50176]	Loss: 0.4810
Training Epoch: 50 [6912/50176]	Loss: 0.5332
Training Epoch: 50 [7168/50176]	Loss: 0.4398
Training Epoch: 50 [7424/50176]	Loss: 0.5089
Training Epoch: 50 [7680/50176]	Loss: 0.4824
Training Epoch: 50 [7936/50176]	Loss: 0.4519
Training Epoch: 50 [8192/50176]	Loss: 0.6243
Training Epoch: 50 [8448/50176]	Loss: 0.4463
Training Epoch: 50 [8704/50176]	Loss: 0.4453
Training Epoch: 50 [8960/50176]	Loss: 0.4950
Training Epoch: 50 [9216/50176]	Loss: 0.6065
Training Epoch: 50 [9472/50176]	Loss: 0.5297
Training Epoch: 50 [9728/50176]	Loss: 0.5388
Training Epoch: 50 [9984/50176]	Loss: 0.5024
Training Epoch: 50 [10240/50176]	Loss: 0.4876
Training Epoch: 50 [10496/50176]	Loss: 0.4246
Training Epoch: 50 [10752/50176]	Loss: 0.5527
Training Epoch: 50 [11008/50176]	Loss: 0.5146
Training Epoch: 50 [11264/50176]	Loss: 0.4745
Training Epoch: 50 [11520/50176]	Loss: 0.5279
Training Epoch: 50 [11776/50176]	Loss: 0.6253
Training Epoch: 50 [12032/50176]	Loss: 0.6728
Training Epoch: 50 [12288/50176]	Loss: 0.5960
Training Epoch: 50 [12544/50176]	Loss: 0.5650
Training Epoch: 50 [12800/50176]	Loss: 0.5039
Training Epoch: 50 [13056/50176]	Loss: 0.5755
Training Epoch: 50 [13312/50176]	Loss: 0.5849
Training Epoch: 50 [13568/50176]	Loss: 0.5476
Training Epoch: 50 [13824/50176]	Loss: 0.6189
Training Epoch: 50 [14080/50176]	Loss: 0.4831
Training Epoch: 50 [14336/50176]	Loss: 0.5572
Training Epoch: 50 [14592/50176]	Loss: 0.5801
Training Epoch: 50 [14848/50176]	Loss: 0.5142
Training Epoch: 50 [15104/50176]	Loss: 0.6158
Training Epoch: 50 [15360/50176]	Loss: 0.5426
Training Epoch: 50 [15616/50176]	Loss: 0.5271
Training Epoch: 50 [15872/50176]	Loss: 0.5775
Training Epoch: 50 [16128/50176]	Loss: 0.6798
Training Epoch: 50 [16384/50176]	Loss: 0.5589
Training Epoch: 50 [16640/50176]	Loss: 0.5684
Training Epoch: 50 [16896/50176]	Loss: 0.6230
Training Epoch: 50 [17152/50176]	Loss: 0.5220
Training Epoch: 50 [17408/50176]	Loss: 0.5817
Training Epoch: 50 [17664/50176]	Loss: 0.5906
Training Epoch: 50 [17920/50176]	Loss: 0.5430
Training Epoch: 50 [18176/50176]	Loss: 0.5501
Training Epoch: 50 [18432/50176]	Loss: 0.6890
Training Epoch: 50 [18688/50176]	Loss: 0.5481
Training Epoch: 50 [18944/50176]	Loss: 0.6269
Training Epoch: 50 [19200/50176]	Loss: 0.4810
Training Epoch: 50 [19456/50176]	Loss: 0.6729
Training Epoch: 50 [19712/50176]	Loss: 0.4982
Training Epoch: 50 [19968/50176]	Loss: 0.4964
Training Epoch: 50 [20224/50176]	Loss: 0.4448
Training Epoch: 50 [20480/50176]	Loss: 0.5112
Training Epoch: 50 [20736/50176]	Loss: 0.4867
Training Epoch: 50 [20992/50176]	Loss: 0.4803
Training Epoch: 50 [21248/50176]	Loss: 0.5240
Training Epoch: 50 [21504/50176]	Loss: 0.4630
Training Epoch: 50 [21760/50176]	Loss: 0.7279
Training Epoch: 50 [22016/50176]	Loss: 0.5845
Training Epoch: 50 [22272/50176]	Loss: 0.6617
Training Epoch: 50 [22528/50176]	Loss: 0.5161
Training Epoch: 50 [22784/50176]	Loss: 0.5561
Training Epoch: 50 [23040/50176]	Loss: 0.5825
Training Epoch: 50 [23296/50176]	Loss: 0.5779
Training Epoch: 50 [23552/50176]	Loss: 0.6575
Training Epoch: 50 [23808/50176]	Loss: 0.4810
Training Epoch: 50 [24064/50176]	Loss: 0.5536
Training Epoch: 50 [24320/50176]	Loss: 0.5597
Training Epoch: 50 [24576/50176]	Loss: 0.5077
Training Epoch: 50 [24832/50176]	Loss: 0.5342
Training Epoch: 50 [25088/50176]	Loss: 0.6278
Training Epoch: 50 [25344/50176]	Loss: 0.4853
Training Epoch: 50 [25600/50176]	Loss: 0.4555
Training Epoch: 50 [25856/50176]	Loss: 0.5143
Training Epoch: 50 [26112/50176]	Loss: 0.4824
Training Epoch: 50 [26368/50176]	Loss: 0.4861
Training Epoch: 50 [26624/50176]	Loss: 0.5858
Training Epoch: 50 [26880/50176]	Loss: 0.5465
Training Epoch: 50 [27136/50176]	Loss: 0.5362
Training Epoch: 50 [27392/50176]	Loss: 0.5760
Training Epoch: 50 [27648/50176]	Loss: 0.6638
Training Epoch: 50 [27904/50176]	Loss: 0.6495
Training Epoch: 50 [28160/50176]	Loss: 0.4646
Training Epoch: 50 [28416/50176]	Loss: 0.4868
Training Epoch: 50 [28672/50176]	Loss: 0.7124
Training Epoch: 50 [28928/50176]	Loss: 0.7243
Training Epoch: 50 [29184/50176]	Loss: 0.5883
Training Epoch: 50 [29440/50176]	Loss: 0.4994
Training Epoch: 50 [29696/50176]	Loss: 0.5446
Training Epoch: 50 [29952/50176]	Loss: 0.6625
Training Epoch: 50 [30208/50176]	Loss: 0.5405
Training Epoch: 50 [30464/50176]	Loss: 0.5788
Training Epoch: 50 [30720/50176]	Loss: 0.4889
Training Epoch: 50 [30976/50176]	Loss: 0.6276
Training Epoch: 50 [31232/50176]	Loss: 0.5168
Training Epoch: 50 [31488/50176]	Loss: 0.6695
Training Epoch: 50 [31744/50176]	Loss: 0.5953
Training Epoch: 50 [32000/50176]	Loss: 0.6323
Training Epoch: 50 [32256/50176]	Loss: 0.4410
Training Epoch: 50 [32512/50176]	Loss: 0.4832
Training Epoch: 50 [32768/50176]	Loss: 0.5577
Training Epoch: 50 [33024/50176]	Loss: 0.5401
Training Epoch: 50 [33280/50176]	Loss: 0.5763
Training Epoch: 50 [33536/50176]	Loss: 0.5156
Training Epoch: 50 [33792/50176]	Loss: 0.5510
Training Epoch: 50 [34048/50176]	Loss: 0.4576
Training Epoch: 50 [34304/50176]	Loss: 0.5331
Training Epoch: 50 [34560/50176]	Loss: 0.5963
Training Epoch: 50 [34816/50176]	Loss: 0.6451
Training Epoch: 50 [35072/50176]	Loss: 0.5568
Training Epoch: 50 [35328/50176]	Loss: 0.4997
Training Epoch: 50 [35584/50176]	Loss: 0.5422
Training Epoch: 50 [35840/50176]	Loss: 0.6100
Training Epoch: 50 [36096/50176]	Loss: 0.4415
Training Epoch: 50 [36352/50176]	Loss: 0.5680
Training Epoch: 50 [36608/50176]	Loss: 0.5044
Training Epoch: 50 [36864/50176]	Loss: 0.5440
Training Epoch: 50 [37120/50176]	Loss: 0.5275
Training Epoch: 50 [37376/50176]	Loss: 0.5998
Training Epoch: 50 [37632/50176]	Loss: 0.7168
Training Epoch: 50 [37888/50176]	Loss: 0.6139
Training Epoch: 50 [38144/50176]	Loss: 0.5812
Training Epoch: 50 [38400/50176]	Loss: 0.5922
Training Epoch: 50 [38656/50176]	Loss: 0.5298
Training Epoch: 50 [38912/50176]	Loss: 0.5379
Training Epoch: 50 [39168/50176]	Loss: 0.5397
Training Epoch: 50 [39424/50176]	Loss: 0.5400
Training Epoch: 50 [39680/50176]	Loss: 0.6410
Training Epoch: 50 [39936/50176]	Loss: 0.5542
Training Epoch: 50 [40192/50176]	Loss: 0.5791
Training Epoch: 50 [40448/50176]	Loss: 0.6159
Training Epoch: 50 [40704/50176]	Loss: 0.4966
Training Epoch: 50 [40960/50176]	Loss: 0.6060
Training Epoch: 50 [41216/50176]	Loss: 0.5289
Training Epoch: 50 [41472/50176]	Loss: 0.5583
Training Epoch: 50 [41728/50176]	Loss: 0.5195
Training Epoch: 50 [41984/50176]	Loss: 0.6068
Training Epoch: 50 [42240/50176]	Loss: 0.6980
Training Epoch: 50 [42496/50176]	Loss: 0.5668
Training Epoch: 50 [42752/50176]	Loss: 0.4535
Training Epoch: 50 [43008/50176]	Loss: 0.6788
Training Epoch: 50 [43264/50176]	Loss: 0.6738
Training Epoch: 50 [43520/50176]	Loss: 0.5851
Training Epoch: 50 [43776/50176]	Loss: 0.5647
Training Epoch: 50 [44032/50176]	Loss: 0.5690
Training Epoch: 50 [44288/50176]	Loss: 0.5137
Training Epoch: 50 [44544/50176]	Loss: 0.7101
Training Epoch: 50 [44800/50176]	Loss: 0.5866
Training Epoch: 50 [45056/50176]	Loss: 0.6711
Training Epoch: 50 [45312/50176]	Loss: 0.5909
Training Epoch: 50 [45568/50176]	Loss: 0.5432
Training Epoch: 50 [45824/50176]	Loss: 0.5499
Training Epoch: 50 [46080/50176]	Loss: 0.6027
Training Epoch: 50 [46336/50176]	Loss: 0.5948
Training Epoch: 50 [46592/50176]	Loss: 0.6749
Training Epoch: 50 [46848/50176]	Loss: 0.5219
Training Epoch: 50 [47104/50176]	Loss: 0.5577
Training Epoch: 50 [47360/50176]	Loss: 0.6503
Training Epoch: 50 [47616/50176]	Loss: 0.5645
Training Epoch: 50 [47872/50176]	Loss: 0.5883
Training Epoch: 50 [48128/50176]	Loss: 0.5547
Training Epoch: 50 [48384/50176]	Loss: 0.5942
Training Epoch: 50 [48640/50176]	Loss: 0.4697
Training Epoch: 50 [48896/50176]	Loss: 0.5854
Training Epoch: 50 [49152/50176]	Loss: 0.5062
Training Epoch: 50 [49408/50176]	Loss: 0.4948
Training Epoch: 50 [49664/50176]	Loss: 0.5474
Training Epoch: 50 [49920/50176]	Loss: 0.5263
Training Epoch: 50 [50176/50176]	Loss: 0.5966
Validation Epoch: 50, Average loss: 0.0085, Accuracy: 0.5526
Training Epoch: 51 [256/50176]	Loss: 0.4301
Training Epoch: 51 [512/50176]	Loss: 0.3977
Training Epoch: 51 [768/50176]	Loss: 0.5405
Training Epoch: 51 [1024/50176]	Loss: 0.5341
Training Epoch: 51 [1280/50176]	Loss: 0.4454
Training Epoch: 51 [1536/50176]	Loss: 0.4099
Training Epoch: 51 [1792/50176]	Loss: 0.4987
Training Epoch: 51 [2048/50176]	Loss: 0.4981
Training Epoch: 51 [2304/50176]	Loss: 0.5278
Training Epoch: 51 [2560/50176]	Loss: 0.3854
Training Epoch: 51 [2816/50176]	Loss: 0.4124
Training Epoch: 51 [3072/50176]	Loss: 0.4143
Training Epoch: 51 [3328/50176]	Loss: 0.4345
Training Epoch: 51 [3584/50176]	Loss: 0.4925
Training Epoch: 51 [3840/50176]	Loss: 0.5270
Training Epoch: 51 [4096/50176]	Loss: 0.4868
Training Epoch: 51 [4352/50176]	Loss: 0.4890
Training Epoch: 51 [4608/50176]	Loss: 0.5144
Training Epoch: 51 [4864/50176]	Loss: 0.4156
Training Epoch: 51 [5120/50176]	Loss: 0.5226
Training Epoch: 51 [5376/50176]	Loss: 0.5754
Training Epoch: 51 [5632/50176]	Loss: 0.6000
Training Epoch: 51 [5888/50176]	Loss: 0.5364
Training Epoch: 51 [6144/50176]	Loss: 0.4144
Training Epoch: 51 [6400/50176]	Loss: 0.6379
Training Epoch: 51 [6656/50176]	Loss: 0.5808
Training Epoch: 51 [6912/50176]	Loss: 0.6262
Training Epoch: 51 [7168/50176]	Loss: 0.5322
Training Epoch: 51 [7424/50176]	Loss: 0.3905
Training Epoch: 51 [7680/50176]	Loss: 0.4633
Training Epoch: 51 [7936/50176]	Loss: 0.4194
Training Epoch: 51 [8192/50176]	Loss: 0.5695
Training Epoch: 51 [8448/50176]	Loss: 0.5259
Training Epoch: 51 [8704/50176]	Loss: 0.6108
Training Epoch: 51 [8960/50176]	Loss: 0.6078
Training Epoch: 51 [9216/50176]	Loss: 0.4742
Training Epoch: 51 [9472/50176]	Loss: 0.4788
Training Epoch: 51 [9728/50176]	Loss: 0.4724
Training Epoch: 51 [9984/50176]	Loss: 0.4641
Training Epoch: 51 [10240/50176]	Loss: 0.5197
Training Epoch: 51 [10496/50176]	Loss: 0.5826
Training Epoch: 51 [10752/50176]	Loss: 0.4053
Training Epoch: 51 [11008/50176]	Loss: 0.5326
Training Epoch: 51 [11264/50176]	Loss: 0.5127
Training Epoch: 51 [11520/50176]	Loss: 0.5210
Training Epoch: 51 [11776/50176]	Loss: 0.5709
Training Epoch: 51 [12032/50176]	Loss: 0.4973
Training Epoch: 51 [12288/50176]	Loss: 0.4493
Training Epoch: 51 [12544/50176]	Loss: 0.5223
Training Epoch: 51 [12800/50176]	Loss: 0.5319
Training Epoch: 51 [13056/50176]	Loss: 0.5281
Training Epoch: 51 [13312/50176]	Loss: 0.6126
Training Epoch: 51 [13568/50176]	Loss: 0.5622
Training Epoch: 51 [13824/50176]	Loss: 0.4903
Training Epoch: 51 [14080/50176]	Loss: 0.5747
Training Epoch: 51 [14336/50176]	Loss: 0.6115
Training Epoch: 51 [14592/50176]	Loss: 0.5748
Training Epoch: 51 [14848/50176]	Loss: 0.5384
Training Epoch: 51 [15104/50176]	Loss: 0.5453
Training Epoch: 51 [15360/50176]	Loss: 0.6888
Training Epoch: 51 [15616/50176]	Loss: 0.5281
Training Epoch: 51 [15872/50176]	Loss: 0.5884
Training Epoch: 51 [16128/50176]	Loss: 0.3827
Training Epoch: 51 [16384/50176]	Loss: 0.6066
Training Epoch: 51 [16640/50176]	Loss: 0.5179
Training Epoch: 51 [16896/50176]	Loss: 0.4530
Training Epoch: 51 [17152/50176]	Loss: 0.4413
Training Epoch: 51 [17408/50176]	Loss: 0.4358
Training Epoch: 51 [17664/50176]	Loss: 0.5108
Training Epoch: 51 [17920/50176]	Loss: 0.6159
Training Epoch: 51 [18176/50176]	Loss: 0.5334
Training Epoch: 51 [18432/50176]	Loss: 0.5215
Training Epoch: 51 [18688/50176]	Loss: 0.6150
Training Epoch: 51 [18944/50176]	Loss: 0.4536
Training Epoch: 51 [19200/50176]	Loss: 0.4332
Training Epoch: 51 [19456/50176]	Loss: 0.6066
Training Epoch: 51 [19712/50176]	Loss: 0.5371
Training Epoch: 51 [19968/50176]	Loss: 0.3934
Training Epoch: 51 [20224/50176]	Loss: 0.4134
Training Epoch: 51 [20480/50176]	Loss: 0.4864
Training Epoch: 51 [20736/50176]	Loss: 0.5442
Training Epoch: 51 [20992/50176]	Loss: 0.4789
Training Epoch: 51 [21248/50176]	Loss: 0.6177
Training Epoch: 51 [21504/50176]	Loss: 0.7124
Training Epoch: 51 [21760/50176]	Loss: 0.6379
Training Epoch: 51 [22016/50176]	Loss: 0.5562
Training Epoch: 51 [22272/50176]	Loss: 0.4430
Training Epoch: 51 [22528/50176]	Loss: 0.5842
Training Epoch: 51 [22784/50176]	Loss: 0.6055
Training Epoch: 51 [23040/50176]	Loss: 0.5165
Training Epoch: 51 [23296/50176]	Loss: 0.4856
Training Epoch: 51 [23552/50176]	Loss: 0.6602
Training Epoch: 51 [23808/50176]	Loss: 0.4977
Training Epoch: 51 [24064/50176]	Loss: 0.6049
Training Epoch: 51 [24320/50176]	Loss: 0.6797
Training Epoch: 51 [24576/50176]	Loss: 0.5439
Training Epoch: 51 [24832/50176]	Loss: 0.5186
Training Epoch: 51 [25088/50176]	Loss: 0.5190
Training Epoch: 51 [25344/50176]	Loss: 0.6581
Training Epoch: 51 [25600/50176]	Loss: 0.6914
Training Epoch: 51 [25856/50176]	Loss: 0.3997
Training Epoch: 51 [26112/50176]	Loss: 0.6095
Training Epoch: 51 [26368/50176]	Loss: 0.5414
Training Epoch: 51 [26624/50176]	Loss: 0.6018
Training Epoch: 51 [26880/50176]	Loss: 0.4110
Training Epoch: 51 [27136/50176]	Loss: 0.4916
Training Epoch: 51 [27392/50176]	Loss: 0.4908
Training Epoch: 51 [27648/50176]	Loss: 0.5166
Training Epoch: 51 [27904/50176]	Loss: 0.5056
Training Epoch: 51 [28160/50176]	Loss: 0.4775
Training Epoch: 51 [28416/50176]	Loss: 0.5593
Training Epoch: 51 [28672/50176]	Loss: 0.6031
Training Epoch: 51 [28928/50176]	Loss: 0.5301
Training Epoch: 51 [29184/50176]	Loss: 0.4921
Training Epoch: 51 [29440/50176]	Loss: 0.4617
Training Epoch: 51 [29696/50176]	Loss: 0.5835
Training Epoch: 51 [29952/50176]	Loss: 0.6950
Training Epoch: 51 [30208/50176]	Loss: 0.6081
Training Epoch: 51 [30464/50176]	Loss: 0.7574
Training Epoch: 51 [30720/50176]	Loss: 0.4243
Training Epoch: 51 [30976/50176]	Loss: 0.5183
Training Epoch: 51 [31232/50176]	Loss: 0.6161
Training Epoch: 51 [31488/50176]	Loss: 0.5899
Training Epoch: 51 [31744/50176]	Loss: 0.5290
Training Epoch: 51 [32000/50176]	Loss: 0.5986
Training Epoch: 51 [32256/50176]	Loss: 0.5134
Training Epoch: 51 [32512/50176]	Loss: 0.5731
Training Epoch: 51 [32768/50176]	Loss: 0.5883
Training Epoch: 51 [33024/50176]	Loss: 0.5100
Training Epoch: 51 [33280/50176]	Loss: 0.5707
Training Epoch: 51 [33536/50176]	Loss: 0.6388
Training Epoch: 51 [33792/50176]	Loss: 0.5216
Training Epoch: 51 [34048/50176]	Loss: 0.5423
Training Epoch: 51 [34304/50176]	Loss: 0.5246
Training Epoch: 51 [34560/50176]	Loss: 0.3989
Training Epoch: 51 [34816/50176]	Loss: 0.5940
Training Epoch: 51 [35072/50176]	Loss: 0.5731
Training Epoch: 51 [35328/50176]	Loss: 0.6668
Training Epoch: 51 [35584/50176]	Loss: 0.5656
Training Epoch: 51 [35840/50176]	Loss: 0.5735
Training Epoch: 51 [36096/50176]	Loss: 0.4842
Training Epoch: 51 [36352/50176]	Loss: 0.5973
Training Epoch: 51 [36608/50176]	Loss: 0.5701
Training Epoch: 51 [36864/50176]	Loss: 0.5546
Training Epoch: 51 [37120/50176]	Loss: 0.5950
Training Epoch: 51 [37376/50176]	Loss: 0.5871
Training Epoch: 51 [37632/50176]	Loss: 0.6874
Training Epoch: 51 [37888/50176]	Loss: 0.5668
Training Epoch: 51 [38144/50176]	Loss: 0.5361
Training Epoch: 51 [38400/50176]	Loss: 0.4878
Training Epoch: 51 [38656/50176]	Loss: 0.6025
Training Epoch: 51 [38912/50176]	Loss: 0.6097
Training Epoch: 51 [39168/50176]	Loss: 0.5681
Training Epoch: 51 [39424/50176]	Loss: 0.5126
Training Epoch: 51 [39680/50176]	Loss: 0.5225
Training Epoch: 51 [39936/50176]	Loss: 0.5240
Training Epoch: 51 [40192/50176]	Loss: 0.5525
Training Epoch: 51 [40448/50176]	Loss: 0.5588
Training Epoch: 51 [40704/50176]	Loss: 0.5659
Training Epoch: 51 [40960/50176]	Loss: 0.5595
Training Epoch: 51 [41216/50176]	Loss: 0.5558
Training Epoch: 51 [41472/50176]	Loss: 0.5871
Training Epoch: 51 [41728/50176]	Loss: 0.6962
Training Epoch: 51 [41984/50176]	Loss: 0.7541
Training Epoch: 51 [42240/50176]	Loss: 0.4993
Training Epoch: 51 [42496/50176]	Loss: 0.5433
Training Epoch: 51 [42752/50176]	Loss: 0.7392
Training Epoch: 51 [43008/50176]	Loss: 0.5522
Training Epoch: 51 [43264/50176]	Loss: 0.5668
Training Epoch: 51 [43520/50176]	Loss: 0.6685
Training Epoch: 51 [43776/50176]	Loss: 0.5547
Training Epoch: 51 [44032/50176]	Loss: 0.6134
Training Epoch: 51 [44288/50176]	Loss: 0.7051
Training Epoch: 51 [44544/50176]	Loss: 0.4381
Training Epoch: 51 [44800/50176]	Loss: 0.5483
Training Epoch: 51 [45056/50176]	Loss: 0.5270
Training Epoch: 51 [45312/50176]	Loss: 0.6855
Training Epoch: 51 [45568/50176]	Loss: 0.7691
Training Epoch: 51 [45824/50176]	Loss: 0.5339
Training Epoch: 51 [46080/50176]	Loss: 0.5632
Training Epoch: 51 [46336/50176]	Loss: 0.6167
Training Epoch: 51 [46592/50176]	Loss: 0.5773
Training Epoch: 51 [46848/50176]	Loss: 0.5500
Training Epoch: 51 [47104/50176]	Loss: 0.7268
Training Epoch: 51 [47360/50176]	Loss: 0.6282
Training Epoch: 51 [47616/50176]	Loss: 0.5567
Training Epoch: 51 [47872/50176]	Loss: 0.6865
Training Epoch: 51 [48128/50176]	Loss: 0.6612
Training Epoch: 51 [48384/50176]	Loss: 0.6396
Training Epoch: 51 [48640/50176]	Loss: 0.6579
Training Epoch: 51 [48896/50176]	Loss: 0.6195
Training Epoch: 51 [49152/50176]	Loss: 0.6032
Training Epoch: 51 [49408/50176]	Loss: 0.5522
Training Epoch: 51 [49664/50176]	Loss: 0.5369
Training Epoch: 51 [49920/50176]	Loss: 0.5876
Training Epoch: 51 [50176/50176]	Loss: 0.4878
Validation Epoch: 51, Average loss: 0.0087, Accuracy: 0.5477
Training Epoch: 52 [256/50176]	Loss: 0.5483
Training Epoch: 52 [512/50176]	Loss: 0.5285
Training Epoch: 52 [768/50176]	Loss: 0.4250
Training Epoch: 52 [1024/50176]	Loss: 0.5019
Training Epoch: 52 [1280/50176]	Loss: 0.5254
Training Epoch: 52 [1536/50176]	Loss: 0.6424
Training Epoch: 52 [1792/50176]	Loss: 0.4961
Training Epoch: 52 [2048/50176]	Loss: 0.5774
Training Epoch: 52 [2304/50176]	Loss: 0.5243
Training Epoch: 52 [2560/50176]	Loss: 0.5272
Training Epoch: 52 [2816/50176]	Loss: 0.5715
Training Epoch: 52 [3072/50176]	Loss: 0.5358
Training Epoch: 52 [3328/50176]	Loss: 0.5050
Training Epoch: 52 [3584/50176]	Loss: 0.5044
Training Epoch: 52 [3840/50176]	Loss: 0.5402
Training Epoch: 52 [4096/50176]	Loss: 0.5076
Training Epoch: 52 [4352/50176]	Loss: 0.4768
Training Epoch: 52 [4608/50176]	Loss: 0.4390
Training Epoch: 52 [4864/50176]	Loss: 0.5097
Training Epoch: 52 [5120/50176]	Loss: 0.5332
Training Epoch: 52 [5376/50176]	Loss: 0.4276
Training Epoch: 52 [5632/50176]	Loss: 0.5220
Training Epoch: 52 [5888/50176]	Loss: 0.4193
Training Epoch: 52 [6144/50176]	Loss: 0.5313
Training Epoch: 52 [6400/50176]	Loss: 0.5907
Training Epoch: 52 [6656/50176]	Loss: 0.4822
Training Epoch: 52 [6912/50176]	Loss: 0.4297
Training Epoch: 52 [7168/50176]	Loss: 0.5627
Training Epoch: 52 [7424/50176]	Loss: 0.5333
Training Epoch: 52 [7680/50176]	Loss: 0.5039
Training Epoch: 52 [7936/50176]	Loss: 0.4224
Training Epoch: 52 [8192/50176]	Loss: 0.5413
Training Epoch: 52 [8448/50176]	Loss: 0.5281
Training Epoch: 52 [8704/50176]	Loss: 0.4546
Training Epoch: 52 [8960/50176]	Loss: 0.5543
Training Epoch: 52 [9216/50176]	Loss: 0.5479
Training Epoch: 52 [9472/50176]	Loss: 0.5953
Training Epoch: 52 [9728/50176]	Loss: 0.5063
Training Epoch: 52 [9984/50176]	Loss: 0.4769
Training Epoch: 52 [10240/50176]	Loss: 0.5448
Training Epoch: 52 [10496/50176]	Loss: 0.4849
Training Epoch: 52 [10752/50176]	Loss: 0.6395
Training Epoch: 52 [11008/50176]	Loss: 0.5422
Training Epoch: 52 [11264/50176]	Loss: 0.4891
Training Epoch: 52 [11520/50176]	Loss: 0.5666
Training Epoch: 52 [11776/50176]	Loss: 0.5532
Training Epoch: 52 [12032/50176]	Loss: 0.5068
Training Epoch: 52 [12288/50176]	Loss: 0.4979
Training Epoch: 52 [12544/50176]	Loss: 0.4019
Training Epoch: 52 [12800/50176]	Loss: 0.5451
Training Epoch: 52 [13056/50176]	Loss: 0.5350
Training Epoch: 52 [13312/50176]	Loss: 0.5040
Training Epoch: 52 [13568/50176]	Loss: 0.5344
Training Epoch: 52 [13824/50176]	Loss: 0.4156
Training Epoch: 52 [14080/50176]	Loss: 0.5731
Training Epoch: 52 [14336/50176]	Loss: 0.4676
Training Epoch: 52 [14592/50176]	Loss: 0.4694
Training Epoch: 52 [14848/50176]	Loss: 0.4998
Training Epoch: 52 [15104/50176]	Loss: 0.6355
Training Epoch: 52 [15360/50176]	Loss: 0.5685
Training Epoch: 52 [15616/50176]	Loss: 0.5697
Training Epoch: 52 [15872/50176]	Loss: 0.6240
Training Epoch: 52 [16128/50176]	Loss: 0.4053
Training Epoch: 52 [16384/50176]	Loss: 0.6134
Training Epoch: 52 [16640/50176]	Loss: 0.5459
Training Epoch: 52 [16896/50176]	Loss: 0.5051
Training Epoch: 52 [17152/50176]	Loss: 0.6129
Training Epoch: 52 [17408/50176]	Loss: 0.5310
Training Epoch: 52 [17664/50176]	Loss: 0.5108
Training Epoch: 52 [17920/50176]	Loss: 0.5493
Training Epoch: 52 [18176/50176]	Loss: 0.4322
Training Epoch: 52 [18432/50176]	Loss: 0.5471
Training Epoch: 52 [18688/50176]	Loss: 0.4905
Training Epoch: 52 [18944/50176]	Loss: 0.4816
Training Epoch: 52 [19200/50176]	Loss: 0.4206
Training Epoch: 52 [19456/50176]	Loss: 0.5498
Training Epoch: 52 [19712/50176]	Loss: 0.5741
Training Epoch: 52 [19968/50176]	Loss: 0.5650
Training Epoch: 52 [20224/50176]	Loss: 0.5244
Training Epoch: 52 [20480/50176]	Loss: 0.4138
Training Epoch: 52 [20736/50176]	Loss: 0.5910
Training Epoch: 52 [20992/50176]	Loss: 0.5008
Training Epoch: 52 [21248/50176]	Loss: 0.4828
Training Epoch: 52 [21504/50176]	Loss: 0.5234
Training Epoch: 52 [21760/50176]	Loss: 0.4856
Training Epoch: 52 [22016/50176]	Loss: 0.3942
Training Epoch: 52 [22272/50176]	Loss: 0.4038
Training Epoch: 52 [22528/50176]	Loss: 0.5986
Training Epoch: 52 [22784/50176]	Loss: 0.5832
Training Epoch: 52 [23040/50176]	Loss: 0.5967
Training Epoch: 52 [23296/50176]	Loss: 0.5674
Training Epoch: 52 [23552/50176]	Loss: 0.4997
Training Epoch: 52 [23808/50176]	Loss: 0.4570
Training Epoch: 52 [24064/50176]	Loss: 0.5542
Training Epoch: 52 [24320/50176]	Loss: 0.5867
Training Epoch: 52 [24576/50176]	Loss: 0.5415
Training Epoch: 52 [24832/50176]	Loss: 0.5920
Training Epoch: 52 [25088/50176]	Loss: 0.5221
Training Epoch: 52 [25344/50176]	Loss: 0.4943
Training Epoch: 52 [25600/50176]	Loss: 0.5770
Training Epoch: 52 [25856/50176]	Loss: 0.7023
Training Epoch: 52 [26112/50176]	Loss: 0.5617
Training Epoch: 52 [26368/50176]	Loss: 0.4624
Training Epoch: 52 [26624/50176]	Loss: 0.5772
Training Epoch: 52 [26880/50176]	Loss: 0.4307
Training Epoch: 52 [27136/50176]	Loss: 0.4827
Training Epoch: 52 [27392/50176]	Loss: 0.5390
Training Epoch: 52 [27648/50176]	Loss: 0.6521
Training Epoch: 52 [27904/50176]	Loss: 0.4746
Training Epoch: 52 [28160/50176]	Loss: 0.5818
Training Epoch: 52 [28416/50176]	Loss: 0.6508
Training Epoch: 52 [28672/50176]	Loss: 0.5269
Training Epoch: 52 [28928/50176]	Loss: 0.6801
Training Epoch: 52 [29184/50176]	Loss: 0.5410
Training Epoch: 52 [29440/50176]	Loss: 0.5695
Training Epoch: 52 [29696/50176]	Loss: 0.5878
Training Epoch: 52 [29952/50176]	Loss: 0.6042
Training Epoch: 52 [30208/50176]	Loss: 0.4915
Training Epoch: 52 [30464/50176]	Loss: 0.4732
Training Epoch: 52 [30720/50176]	Loss: 0.5635
Training Epoch: 52 [30976/50176]	Loss: 0.5235
Training Epoch: 52 [31232/50176]	Loss: 0.6721
Training Epoch: 52 [31488/50176]	Loss: 0.5027
Training Epoch: 52 [31744/50176]	Loss: 0.5055
Training Epoch: 52 [32000/50176]	Loss: 0.5356
Training Epoch: 52 [32256/50176]	Loss: 0.5250
Training Epoch: 52 [32512/50176]	Loss: 0.4791
Training Epoch: 52 [32768/50176]	Loss: 0.5542
Training Epoch: 52 [33024/50176]	Loss: 0.4758
Training Epoch: 52 [33280/50176]	Loss: 0.5284
Training Epoch: 52 [33536/50176]	Loss: 0.6419
Training Epoch: 52 [33792/50176]	Loss: 0.5862
Training Epoch: 52 [34048/50176]	Loss: 0.5758
Training Epoch: 52 [34304/50176]	Loss: 0.5023
Training Epoch: 52 [34560/50176]	Loss: 0.4961
Training Epoch: 52 [34816/50176]	Loss: 0.5900
Training Epoch: 52 [35072/50176]	Loss: 0.6330
Training Epoch: 52 [35328/50176]	Loss: 0.6086
Training Epoch: 52 [35584/50176]	Loss: 0.6909
Training Epoch: 52 [35840/50176]	Loss: 0.7316
Training Epoch: 52 [36096/50176]	Loss: 0.5601
Training Epoch: 52 [36352/50176]	Loss: 0.5312
Training Epoch: 52 [36608/50176]	Loss: 0.5175
Training Epoch: 52 [36864/50176]	Loss: 0.5929
Training Epoch: 52 [37120/50176]	Loss: 0.5646
Training Epoch: 52 [37376/50176]	Loss: 0.4561
Training Epoch: 52 [37632/50176]	Loss: 0.6118
Training Epoch: 52 [37888/50176]	Loss: 0.6089
Training Epoch: 52 [38144/50176]	Loss: 0.6361
Training Epoch: 52 [38400/50176]	Loss: 0.5245
Training Epoch: 52 [38656/50176]	Loss: 0.6384
Training Epoch: 52 [38912/50176]	Loss: 0.5758
Training Epoch: 52 [39168/50176]	Loss: 0.5387
Training Epoch: 52 [39424/50176]	Loss: 0.4686
Training Epoch: 52 [39680/50176]	Loss: 0.5108
Training Epoch: 52 [39936/50176]	Loss: 0.4782
Training Epoch: 52 [40192/50176]	Loss: 0.4752
Training Epoch: 52 [40448/50176]	Loss: 0.5671
Training Epoch: 52 [40704/50176]	Loss: 0.4431
Training Epoch: 52 [40960/50176]	Loss: 0.5969
Training Epoch: 52 [41216/50176]	Loss: 0.4874
Training Epoch: 52 [41472/50176]	Loss: 0.6212
Training Epoch: 52 [41728/50176]	Loss: 0.4767
Training Epoch: 52 [41984/50176]	Loss: 0.6252
Training Epoch: 52 [42240/50176]	Loss: 0.5397
Training Epoch: 52 [42496/50176]	Loss: 0.5239
Training Epoch: 52 [42752/50176]	Loss: 0.6527
Training Epoch: 52 [43008/50176]	Loss: 0.5455
Training Epoch: 52 [43264/50176]	Loss: 0.6075
Training Epoch: 52 [43520/50176]	Loss: 0.5911
Training Epoch: 52 [43776/50176]	Loss: 0.4621
Training Epoch: 52 [44032/50176]	Loss: 0.5913
Training Epoch: 52 [44288/50176]	Loss: 0.5205
Training Epoch: 52 [44544/50176]	Loss: 0.5300
Training Epoch: 52 [44800/50176]	Loss: 0.5599
Training Epoch: 52 [45056/50176]	Loss: 0.5174
Training Epoch: 52 [45312/50176]	Loss: 0.5130
Training Epoch: 52 [45568/50176]	Loss: 0.5231
Training Epoch: 52 [45824/50176]	Loss: 0.6308
Training Epoch: 52 [46080/50176]	Loss: 0.5859
Training Epoch: 52 [46336/50176]	Loss: 0.5672
Training Epoch: 52 [46592/50176]	Loss: 0.5619
Training Epoch: 52 [46848/50176]	Loss: 0.5296
Training Epoch: 52 [47104/50176]	Loss: 0.5640
Training Epoch: 52 [47360/50176]	Loss: 0.6072
Training Epoch: 52 [47616/50176]	Loss: 0.7271
Training Epoch: 52 [47872/50176]	Loss: 0.5819
Training Epoch: 52 [48128/50176]	Loss: 0.5109
Training Epoch: 52 [48384/50176]	Loss: 0.6384
Training Epoch: 52 [48640/50176]	Loss: 0.5717
Training Epoch: 52 [48896/50176]	Loss: 0.5152
Training Epoch: 52 [49152/50176]	Loss: 0.6764
Training Epoch: 52 [49408/50176]	Loss: 0.5478
Training Epoch: 52 [49664/50176]	Loss: 0.6601
Training Epoch: 52 [49920/50176]	Loss: 0.4393
Training Epoch: 52 [50176/50176]	Loss: 0.6495
Validation Epoch: 52, Average loss: 0.0094, Accuracy: 0.5436
Training Epoch: 53 [256/50176]	Loss: 0.4535
Training Epoch: 53 [512/50176]	Loss: 0.4596
Training Epoch: 53 [768/50176]	Loss: 0.4440
Training Epoch: 53 [1024/50176]	Loss: 0.3844
Training Epoch: 53 [1280/50176]	Loss: 0.4357
Training Epoch: 53 [1536/50176]	Loss: 0.5106
Training Epoch: 53 [1792/50176]	Loss: 0.4714
Training Epoch: 53 [2048/50176]	Loss: 0.4647
Training Epoch: 53 [2304/50176]	Loss: 0.3744
Training Epoch: 53 [2560/50176]	Loss: 0.5286
Training Epoch: 53 [2816/50176]	Loss: 0.5456
Training Epoch: 53 [3072/50176]	Loss: 0.4958
Training Epoch: 53 [3328/50176]	Loss: 0.5038
Training Epoch: 53 [3584/50176]	Loss: 0.4392
Training Epoch: 53 [3840/50176]	Loss: 0.5306
Training Epoch: 53 [4096/50176]	Loss: 0.5727
Training Epoch: 53 [4352/50176]	Loss: 0.5113
Training Epoch: 53 [4608/50176]	Loss: 0.5320
Training Epoch: 53 [4864/50176]	Loss: 0.5049
Training Epoch: 53 [5120/50176]	Loss: 0.4500
Training Epoch: 53 [5376/50176]	Loss: 0.5163
Training Epoch: 53 [5632/50176]	Loss: 0.4574
Training Epoch: 53 [5888/50176]	Loss: 0.5517
Training Epoch: 53 [6144/50176]	Loss: 0.4187
Training Epoch: 53 [6400/50176]	Loss: 0.5738
Training Epoch: 53 [6656/50176]	Loss: 0.5412
Training Epoch: 53 [6912/50176]	Loss: 0.4987
Training Epoch: 53 [7168/50176]	Loss: 0.4901
Training Epoch: 53 [7424/50176]	Loss: 0.4608
Training Epoch: 53 [7680/50176]	Loss: 0.6042
Training Epoch: 53 [7936/50176]	Loss: 0.5326
Training Epoch: 53 [8192/50176]	Loss: 0.4900
Training Epoch: 53 [8448/50176]	Loss: 0.5269
Training Epoch: 53 [8704/50176]	Loss: 0.5565
Training Epoch: 53 [8960/50176]	Loss: 0.5091
Training Epoch: 53 [9216/50176]	Loss: 0.4644
Training Epoch: 53 [9472/50176]	Loss: 0.4594
Training Epoch: 53 [9728/50176]	Loss: 0.5113
Training Epoch: 53 [9984/50176]	Loss: 0.5749
Training Epoch: 53 [10240/50176]	Loss: 0.6038
Training Epoch: 53 [10496/50176]	Loss: 0.3988
Training Epoch: 53 [10752/50176]	Loss: 0.6053
Training Epoch: 53 [11008/50176]	Loss: 0.5808
Training Epoch: 53 [11264/50176]	Loss: 0.4156
Training Epoch: 53 [11520/50176]	Loss: 0.5243
Training Epoch: 53 [11776/50176]	Loss: 0.4743
Training Epoch: 53 [12032/50176]	Loss: 0.5510
Training Epoch: 53 [12288/50176]	Loss: 0.4447
Training Epoch: 53 [12544/50176]	Loss: 0.4329
Training Epoch: 53 [12800/50176]	Loss: 0.5213
Training Epoch: 53 [13056/50176]	Loss: 0.4363
Training Epoch: 53 [13312/50176]	Loss: 0.6494
Training Epoch: 53 [13568/50176]	Loss: 0.4764
Training Epoch: 53 [13824/50176]	Loss: 0.4278
Training Epoch: 53 [14080/50176]	Loss: 0.4473
Training Epoch: 53 [14336/50176]	Loss: 0.4431
Training Epoch: 53 [14592/50176]	Loss: 0.5397
Training Epoch: 53 [14848/50176]	Loss: 0.4251
Training Epoch: 53 [15104/50176]	Loss: 0.5068
Training Epoch: 53 [15360/50176]	Loss: 0.6421
Training Epoch: 53 [15616/50176]	Loss: 0.5559
Training Epoch: 53 [15872/50176]	Loss: 0.4994
Training Epoch: 53 [16128/50176]	Loss: 0.5265
Training Epoch: 53 [16384/50176]	Loss: 0.5084
Training Epoch: 53 [16640/50176]	Loss: 0.4084
Training Epoch: 53 [16896/50176]	Loss: 0.5566
Training Epoch: 53 [17152/50176]	Loss: 0.4944
Training Epoch: 53 [17408/50176]	Loss: 0.5188
Training Epoch: 53 [17664/50176]	Loss: 0.5300
Training Epoch: 53 [17920/50176]	Loss: 0.6194
Training Epoch: 53 [18176/50176]	Loss: 0.5701
Training Epoch: 53 [18432/50176]	Loss: 0.5201
Training Epoch: 53 [18688/50176]	Loss: 0.6493
Training Epoch: 53 [18944/50176]	Loss: 0.5348
Training Epoch: 53 [19200/50176]	Loss: 0.5354
Training Epoch: 53 [19456/50176]	Loss: 0.4582
Training Epoch: 53 [19712/50176]	Loss: 0.4263
Training Epoch: 53 [19968/50176]	Loss: 0.5822
Training Epoch: 53 [20224/50176]	Loss: 0.5724
Training Epoch: 53 [20480/50176]	Loss: 0.4666
Training Epoch: 53 [20736/50176]	Loss: 0.5274
Training Epoch: 53 [20992/50176]	Loss: 0.4626
Training Epoch: 53 [21248/50176]	Loss: 0.5894
Training Epoch: 53 [21504/50176]	Loss: 0.5326
Training Epoch: 53 [21760/50176]	Loss: 0.5355
Training Epoch: 53 [22016/50176]	Loss: 0.4958
Training Epoch: 53 [22272/50176]	Loss: 0.4277
Training Epoch: 53 [22528/50176]	Loss: 0.4757
Training Epoch: 53 [22784/50176]	Loss: 0.6648
Training Epoch: 53 [23040/50176]	Loss: 0.5384
Training Epoch: 53 [23296/50176]	Loss: 0.5082
Training Epoch: 53 [23552/50176]	Loss: 0.5407
Training Epoch: 53 [23808/50176]	Loss: 0.5498
Training Epoch: 53 [24064/50176]	Loss: 0.5035
Training Epoch: 53 [24320/50176]	Loss: 0.5738
Training Epoch: 53 [24576/50176]	Loss: 0.5497
Training Epoch: 53 [24832/50176]	Loss: 0.5603
Training Epoch: 53 [25088/50176]	Loss: 0.4455
Training Epoch: 53 [25344/50176]	Loss: 0.5007
Training Epoch: 53 [25600/50176]	Loss: 0.4675
Training Epoch: 53 [25856/50176]	Loss: 0.5290
Training Epoch: 53 [26112/50176]	Loss: 0.5231
Training Epoch: 53 [26368/50176]	Loss: 0.5058
Training Epoch: 53 [26624/50176]	Loss: 0.5371
Training Epoch: 53 [26880/50176]	Loss: 0.5266
Training Epoch: 53 [27136/50176]	Loss: 0.7226
Training Epoch: 53 [27392/50176]	Loss: 0.5449
Training Epoch: 53 [27648/50176]	Loss: 0.4541
Training Epoch: 53 [27904/50176]	Loss: 0.5729
Training Epoch: 53 [28160/50176]	Loss: 0.5608
Training Epoch: 53 [28416/50176]	Loss: 0.4820
Training Epoch: 53 [28672/50176]	Loss: 0.7272
Training Epoch: 53 [28928/50176]	Loss: 0.5881
Training Epoch: 53 [29184/50176]	Loss: 0.6228
Training Epoch: 53 [29440/50176]	Loss: 0.4659
Training Epoch: 53 [29696/50176]	Loss: 0.5836
Training Epoch: 53 [29952/50176]	Loss: 0.5921
Training Epoch: 53 [30208/50176]	Loss: 0.5298
Training Epoch: 53 [30464/50176]	Loss: 0.4844
Training Epoch: 53 [30720/50176]	Loss: 0.5309
Training Epoch: 53 [30976/50176]	Loss: 0.5750
Training Epoch: 53 [31232/50176]	Loss: 0.5914
Training Epoch: 53 [31488/50176]	Loss: 0.5214
Training Epoch: 53 [31744/50176]	Loss: 0.5824
Training Epoch: 53 [32000/50176]	Loss: 0.4252
Training Epoch: 53 [32256/50176]	Loss: 0.5283
Training Epoch: 53 [32512/50176]	Loss: 0.5308
Training Epoch: 53 [32768/50176]	Loss: 0.6258
Training Epoch: 53 [33024/50176]	Loss: 0.4414
Training Epoch: 53 [33280/50176]	Loss: 0.5569
Training Epoch: 53 [33536/50176]	Loss: 0.5095
Training Epoch: 53 [33792/50176]	Loss: 0.5395
Training Epoch: 53 [34048/50176]	Loss: 0.5071
Training Epoch: 53 [34304/50176]	Loss: 0.6369
Training Epoch: 53 [34560/50176]	Loss: 0.4718
Training Epoch: 53 [34816/50176]	Loss: 0.6300
Training Epoch: 53 [35072/50176]	Loss: 0.6774
Training Epoch: 53 [35328/50176]	Loss: 0.6027
Training Epoch: 53 [35584/50176]	Loss: 0.4161
Training Epoch: 53 [35840/50176]	Loss: 0.4889
Training Epoch: 53 [36096/50176]	Loss: 0.4394
Training Epoch: 53 [36352/50176]	Loss: 0.6203
Training Epoch: 53 [36608/50176]	Loss: 0.6825
Training Epoch: 53 [36864/50176]	Loss: 0.5427
Training Epoch: 53 [37120/50176]	Loss: 0.5606
Training Epoch: 53 [37376/50176]	Loss: 0.5411
Training Epoch: 53 [37632/50176]	Loss: 0.4001
Training Epoch: 53 [37888/50176]	Loss: 0.4351
Training Epoch: 53 [38144/50176]	Loss: 0.4567
Training Epoch: 53 [38400/50176]	Loss: 0.7828
Training Epoch: 53 [38656/50176]	Loss: 0.4895
Training Epoch: 53 [38912/50176]	Loss: 0.5901
Training Epoch: 53 [39168/50176]	Loss: 0.5886
Training Epoch: 53 [39424/50176]	Loss: 0.5205
Training Epoch: 53 [39680/50176]	Loss: 0.5636
Training Epoch: 53 [39936/50176]	Loss: 0.5148
Training Epoch: 53 [40192/50176]	Loss: 0.5489
Training Epoch: 53 [40448/50176]	Loss: 0.5494
Training Epoch: 53 [40704/50176]	Loss: 0.5553
Training Epoch: 53 [40960/50176]	Loss: 0.4639
Training Epoch: 53 [41216/50176]	Loss: 0.5063
Training Epoch: 53 [41472/50176]	Loss: 0.6455
Training Epoch: 53 [41728/50176]	Loss: 0.5478
Training Epoch: 53 [41984/50176]	Loss: 0.5960
Training Epoch: 53 [42240/50176]	Loss: 0.5729
Training Epoch: 53 [42496/50176]	Loss: 0.5535
Training Epoch: 53 [42752/50176]	Loss: 0.5662
Training Epoch: 53 [43008/50176]	Loss: 0.4619
Training Epoch: 53 [43264/50176]	Loss: 0.5980
Training Epoch: 53 [43520/50176]	Loss: 0.5995
Training Epoch: 53 [43776/50176]	Loss: 0.4731
Training Epoch: 53 [44032/50176]	Loss: 0.5261
Training Epoch: 53 [44288/50176]	Loss: 0.4372
Training Epoch: 53 [44544/50176]	Loss: 0.5457
Training Epoch: 53 [44800/50176]	Loss: 0.5415
Training Epoch: 53 [45056/50176]	Loss: 0.5255
Training Epoch: 53 [45312/50176]	Loss: 0.5894
Training Epoch: 53 [45568/50176]	Loss: 0.4854
Training Epoch: 53 [45824/50176]	Loss: 0.5451
Training Epoch: 53 [46080/50176]	Loss: 0.5669
Training Epoch: 53 [46336/50176]	Loss: 0.5150
Training Epoch: 53 [46592/50176]	Loss: 0.6379
Training Epoch: 53 [46848/50176]	Loss: 0.5149
Training Epoch: 53 [47104/50176]	Loss: 0.4457
Training Epoch: 53 [47360/50176]	Loss: 0.6069
Training Epoch: 53 [47616/50176]	Loss: 0.4914
Training Epoch: 53 [47872/50176]	Loss: 0.4725
Training Epoch: 53 [48128/50176]	Loss: 0.5955
Training Epoch: 53 [48384/50176]	Loss: 0.6345
Training Epoch: 53 [48640/50176]	Loss: 0.5809
Training Epoch: 53 [48896/50176]	Loss: 0.6430
Training Epoch: 53 [49152/50176]	Loss: 0.6042
Training Epoch: 53 [49408/50176]	Loss: 0.4803
Training Epoch: 53 [49664/50176]	Loss: 0.6502
Training Epoch: 53 [49920/50176]	Loss: 0.5131
Training Epoch: 53 [50176/50176]	Loss: 0.4678
Validation Epoch: 53, Average loss: 0.0089, Accuracy: 0.5489
Training Epoch: 54 [256/50176]	Loss: 0.3591
Training Epoch: 54 [512/50176]	Loss: 0.4675
Training Epoch: 54 [768/50176]	Loss: 0.4578
Training Epoch: 54 [1024/50176]	Loss: 0.4842
Training Epoch: 54 [1280/50176]	Loss: 0.4145
Training Epoch: 54 [1536/50176]	Loss: 0.5198
Training Epoch: 54 [1792/50176]	Loss: 0.5080
Training Epoch: 54 [2048/50176]	Loss: 0.3435
Training Epoch: 54 [2304/50176]	Loss: 0.4551
Training Epoch: 54 [2560/50176]	Loss: 0.4626
Training Epoch: 54 [2816/50176]	Loss: 0.3746
Training Epoch: 54 [3072/50176]	Loss: 0.5531
Training Epoch: 54 [3328/50176]	Loss: 0.5669
Training Epoch: 54 [3584/50176]	Loss: 0.4422
Training Epoch: 54 [3840/50176]	Loss: 0.5208
Training Epoch: 54 [4096/50176]	Loss: 0.5210
Training Epoch: 54 [4352/50176]	Loss: 0.3977
Training Epoch: 54 [4608/50176]	Loss: 0.3786
Training Epoch: 54 [4864/50176]	Loss: 0.3893
Training Epoch: 54 [5120/50176]	Loss: 0.4839
Training Epoch: 54 [5376/50176]	Loss: 0.5045
Training Epoch: 54 [5632/50176]	Loss: 0.4981
Training Epoch: 54 [5888/50176]	Loss: 0.5735
Training Epoch: 54 [6144/50176]	Loss: 0.4215
Training Epoch: 54 [6400/50176]	Loss: 0.5357
Training Epoch: 54 [6656/50176]	Loss: 0.3951
Training Epoch: 54 [6912/50176]	Loss: 0.4219
Training Epoch: 54 [7168/50176]	Loss: 0.5513
Training Epoch: 54 [7424/50176]	Loss: 0.5098
Training Epoch: 54 [7680/50176]	Loss: 0.4515
Training Epoch: 54 [7936/50176]	Loss: 0.5157
Training Epoch: 54 [8192/50176]	Loss: 0.5465
Training Epoch: 54 [8448/50176]	Loss: 0.5806
Training Epoch: 54 [8704/50176]	Loss: 0.4599
Training Epoch: 54 [8960/50176]	Loss: 0.5097
Training Epoch: 54 [9216/50176]	Loss: 0.5522
Training Epoch: 54 [9472/50176]	Loss: 0.3474
Training Epoch: 54 [9728/50176]	Loss: 0.4351
Training Epoch: 54 [9984/50176]	Loss: 0.4862
Training Epoch: 54 [10240/50176]	Loss: 0.4911
Training Epoch: 54 [10496/50176]	Loss: 0.4630
Training Epoch: 54 [10752/50176]	Loss: 0.4782
Training Epoch: 54 [11008/50176]	Loss: 0.4361
Training Epoch: 54 [11264/50176]	Loss: 0.5279
Training Epoch: 54 [11520/50176]	Loss: 0.4763
Training Epoch: 54 [11776/50176]	Loss: 0.4311
Training Epoch: 54 [12032/50176]	Loss: 0.2975
Training Epoch: 54 [12288/50176]	Loss: 0.6307
Training Epoch: 54 [12544/50176]	Loss: 0.4654
Training Epoch: 54 [12800/50176]	Loss: 0.6993
Training Epoch: 54 [13056/50176]	Loss: 0.4914
Training Epoch: 54 [13312/50176]	Loss: 0.4891
Training Epoch: 54 [13568/50176]	Loss: 0.5223
Training Epoch: 54 [13824/50176]	Loss: 0.4468
Training Epoch: 54 [14080/50176]	Loss: 0.5125
Training Epoch: 54 [14336/50176]	Loss: 0.4341
Training Epoch: 54 [14592/50176]	Loss: 0.5793
Training Epoch: 54 [14848/50176]	Loss: 0.5552
Training Epoch: 54 [15104/50176]	Loss: 0.3980
Training Epoch: 54 [15360/50176]	Loss: 0.4563
Training Epoch: 54 [15616/50176]	Loss: 0.5632
Training Epoch: 54 [15872/50176]	Loss: 0.5689
Training Epoch: 54 [16128/50176]	Loss: 0.4629
Training Epoch: 54 [16384/50176]	Loss: 0.4967
Training Epoch: 54 [16640/50176]	Loss: 0.3980
Training Epoch: 54 [16896/50176]	Loss: 0.4920
Training Epoch: 54 [17152/50176]	Loss: 0.5800
Training Epoch: 54 [17408/50176]	Loss: 0.4211
Training Epoch: 54 [17664/50176]	Loss: 0.5464
Training Epoch: 54 [17920/50176]	Loss: 0.4688
Training Epoch: 54 [18176/50176]	Loss: 0.5812
Training Epoch: 54 [18432/50176]	Loss: 0.4903
Training Epoch: 54 [18688/50176]	Loss: 0.4688
Training Epoch: 54 [18944/50176]	Loss: 0.3636
Training Epoch: 54 [19200/50176]	Loss: 0.6604
Training Epoch: 54 [19456/50176]	Loss: 0.4907
Training Epoch: 54 [19712/50176]	Loss: 0.5251
Training Epoch: 54 [19968/50176]	Loss: 0.4528
Training Epoch: 54 [20224/50176]	Loss: 0.4316
Training Epoch: 54 [20480/50176]	Loss: 0.6010
Training Epoch: 54 [20736/50176]	Loss: 0.5435
Training Epoch: 54 [20992/50176]	Loss: 0.4415
Training Epoch: 54 [21248/50176]	Loss: 0.4267
Training Epoch: 54 [21504/50176]	Loss: 0.5141
Training Epoch: 54 [21760/50176]	Loss: 0.4846
Training Epoch: 54 [22016/50176]	Loss: 0.5525
Training Epoch: 54 [22272/50176]	Loss: 0.5826
Training Epoch: 54 [22528/50176]	Loss: 0.4371
Training Epoch: 54 [22784/50176]	Loss: 0.5911
Training Epoch: 54 [23040/50176]	Loss: 0.5457
Training Epoch: 54 [23296/50176]	Loss: 0.4914
Training Epoch: 54 [23552/50176]	Loss: 0.5143
Training Epoch: 54 [23808/50176]	Loss: 0.5200
Training Epoch: 54 [24064/50176]	Loss: 0.4991
Training Epoch: 54 [24320/50176]	Loss: 0.5034
Training Epoch: 54 [24576/50176]	Loss: 0.5069
Training Epoch: 54 [24832/50176]	Loss: 0.4853
Training Epoch: 54 [25088/50176]	Loss: 0.5407
Training Epoch: 54 [25344/50176]	Loss: 0.5413
Training Epoch: 54 [25600/50176]	Loss: 0.4657
Training Epoch: 54 [25856/50176]	Loss: 0.4853
Training Epoch: 54 [26112/50176]	Loss: 0.5714
Training Epoch: 54 [26368/50176]	Loss: 0.4809
Training Epoch: 54 [26624/50176]	Loss: 0.5436
Training Epoch: 54 [26880/50176]	Loss: 0.6240
Training Epoch: 54 [27136/50176]	Loss: 0.5301
Training Epoch: 54 [27392/50176]	Loss: 0.5660
Training Epoch: 54 [27648/50176]	Loss: 0.5382
Training Epoch: 54 [27904/50176]	Loss: 0.4796
Training Epoch: 54 [28160/50176]	Loss: 0.4853
Training Epoch: 54 [28416/50176]	Loss: 0.4855
Training Epoch: 54 [28672/50176]	Loss: 0.4950
Training Epoch: 54 [28928/50176]	Loss: 0.4590
Training Epoch: 54 [29184/50176]	Loss: 0.5687
Training Epoch: 54 [29440/50176]	Loss: 0.6002
Training Epoch: 54 [29696/50176]	Loss: 0.5581
Training Epoch: 54 [29952/50176]	Loss: 0.4741
Training Epoch: 54 [30208/50176]	Loss: 0.4881
Training Epoch: 54 [30464/50176]	Loss: 0.5945
Training Epoch: 54 [30720/50176]	Loss: 0.4746
Training Epoch: 54 [30976/50176]	Loss: 0.5303
Training Epoch: 54 [31232/50176]	Loss: 0.4184
Training Epoch: 54 [31488/50176]	Loss: 0.3998
Training Epoch: 54 [31744/50176]	Loss: 0.5870
Training Epoch: 54 [32000/50176]	Loss: 0.6875
Training Epoch: 54 [32256/50176]	Loss: 0.5379
Training Epoch: 54 [32512/50176]	Loss: 0.4888
Training Epoch: 54 [32768/50176]	Loss: 0.5185
Training Epoch: 54 [33024/50176]	Loss: 0.4714
Training Epoch: 54 [33280/50176]	Loss: 0.3872
Training Epoch: 54 [33536/50176]	Loss: 0.7075
Training Epoch: 54 [33792/50176]	Loss: 0.6198
Training Epoch: 54 [34048/50176]	Loss: 0.5223
Training Epoch: 54 [34304/50176]	Loss: 0.4726
Training Epoch: 54 [34560/50176]	Loss: 0.6890
Training Epoch: 54 [34816/50176]	Loss: 0.5589
Training Epoch: 54 [35072/50176]	Loss: 0.5087
Training Epoch: 54 [35328/50176]	Loss: 0.4872
Training Epoch: 54 [35584/50176]	Loss: 0.6727
Training Epoch: 54 [35840/50176]	Loss: 0.5118
Training Epoch: 54 [36096/50176]	Loss: 0.5909
Training Epoch: 54 [36352/50176]	Loss: 0.4948
Training Epoch: 54 [36608/50176]	Loss: 0.6306
Training Epoch: 54 [36864/50176]	Loss: 0.4358
Training Epoch: 54 [37120/50176]	Loss: 0.5050
Training Epoch: 54 [37376/50176]	Loss: 0.5001
Training Epoch: 54 [37632/50176]	Loss: 0.6024
Training Epoch: 54 [37888/50176]	Loss: 0.5118
Training Epoch: 54 [38144/50176]	Loss: 0.6023
Training Epoch: 54 [38400/50176]	Loss: 0.4071
Training Epoch: 54 [38656/50176]	Loss: 0.6814
Training Epoch: 54 [38912/50176]	Loss: 0.4759
Training Epoch: 54 [39168/50176]	Loss: 0.6509
Training Epoch: 54 [39424/50176]	Loss: 0.5273
Training Epoch: 54 [39680/50176]	Loss: 0.5397
Training Epoch: 54 [39936/50176]	Loss: 0.5309
Training Epoch: 54 [40192/50176]	Loss: 0.5872
Training Epoch: 54 [40448/50176]	Loss: 0.6059
Training Epoch: 54 [40704/50176]	Loss: 0.5926
Training Epoch: 54 [40960/50176]	Loss: 0.6361
Training Epoch: 54 [41216/50176]	Loss: 0.4659
Training Epoch: 54 [41472/50176]	Loss: 0.4351
Training Epoch: 54 [41728/50176]	Loss: 0.5605
Training Epoch: 54 [41984/50176]	Loss: 0.5060
Training Epoch: 54 [42240/50176]	Loss: 0.6067
Training Epoch: 54 [42496/50176]	Loss: 0.6509
Training Epoch: 54 [42752/50176]	Loss: 0.5114
Training Epoch: 54 [43008/50176]	Loss: 0.5506
Training Epoch: 54 [43264/50176]	Loss: 0.5452
Training Epoch: 54 [43520/50176]	Loss: 0.6260
Training Epoch: 54 [43776/50176]	Loss: 0.5307
Training Epoch: 54 [44032/50176]	Loss: 0.6047
Training Epoch: 54 [44288/50176]	Loss: 0.5537
Training Epoch: 54 [44544/50176]	Loss: 0.5998
Training Epoch: 54 [44800/50176]	Loss: 0.5608
Training Epoch: 54 [45056/50176]	Loss: 0.6402
Training Epoch: 54 [45312/50176]	Loss: 0.6959
Training Epoch: 54 [45568/50176]	Loss: 0.5357
Training Epoch: 54 [45824/50176]	Loss: 0.5277
Training Epoch: 54 [46080/50176]	Loss: 0.6022
Training Epoch: 54 [46336/50176]	Loss: 0.5774
Training Epoch: 54 [46592/50176]	Loss: 0.5059
Training Epoch: 54 [46848/50176]	Loss: 0.5242
Training Epoch: 54 [47104/50176]	Loss: 0.4844
Training Epoch: 54 [47360/50176]	Loss: 0.5832
Training Epoch: 54 [47616/50176]	Loss: 0.4582
Training Epoch: 54 [47872/50176]	Loss: 0.6579
Training Epoch: 54 [48128/50176]	Loss: 0.5417
Training Epoch: 54 [48384/50176]	Loss: 0.6385
Training Epoch: 54 [48640/50176]	Loss: 0.5631
Training Epoch: 54 [48896/50176]	Loss: 0.5203
Training Epoch: 54 [49152/50176]	Loss: 0.5607
Training Epoch: 54 [49408/50176]	Loss: 0.4367
Training Epoch: 54 [49664/50176]	Loss: 0.5520
Training Epoch: 54 [49920/50176]	Loss: 0.5020
Training Epoch: 54 [50176/50176]	Loss: 0.7427
Validation Epoch: 54, Average loss: 0.0080, Accuracy: 0.5757
Training Epoch: 55 [256/50176]	Loss: 0.3683
Training Epoch: 55 [512/50176]	Loss: 0.4937
Training Epoch: 55 [768/50176]	Loss: 0.3845
Training Epoch: 55 [1024/50176]	Loss: 0.4326
Training Epoch: 55 [1280/50176]	Loss: 0.4244
Training Epoch: 55 [1536/50176]	Loss: 0.4530
Training Epoch: 55 [1792/50176]	Loss: 0.6512
Training Epoch: 55 [2048/50176]	Loss: 0.4179
Training Epoch: 55 [2304/50176]	Loss: 0.4034
Training Epoch: 55 [2560/50176]	Loss: 0.3890
Training Epoch: 55 [2816/50176]	Loss: 0.5677
Training Epoch: 55 [3072/50176]	Loss: 0.4984
Training Epoch: 55 [3328/50176]	Loss: 0.4682
Training Epoch: 55 [3584/50176]	Loss: 0.5304
Training Epoch: 55 [3840/50176]	Loss: 0.4888
Training Epoch: 55 [4096/50176]	Loss: 0.4906
Training Epoch: 55 [4352/50176]	Loss: 0.4750
Training Epoch: 55 [4608/50176]	Loss: 0.4241
Training Epoch: 55 [4864/50176]	Loss: 0.4391
Training Epoch: 55 [5120/50176]	Loss: 0.5294
Training Epoch: 55 [5376/50176]	Loss: 0.5762
Training Epoch: 55 [5632/50176]	Loss: 0.5562
Training Epoch: 55 [5888/50176]	Loss: 0.5496
Training Epoch: 55 [6144/50176]	Loss: 0.4235
Training Epoch: 55 [6400/50176]	Loss: 0.5030
Training Epoch: 55 [6656/50176]	Loss: 0.5248
Training Epoch: 55 [6912/50176]	Loss: 0.3914
Training Epoch: 55 [7168/50176]	Loss: 0.3862
Training Epoch: 55 [7424/50176]	Loss: 0.5995
Training Epoch: 55 [7680/50176]	Loss: 0.4582
Training Epoch: 55 [7936/50176]	Loss: 0.5792
Training Epoch: 55 [8192/50176]	Loss: 0.4925
Training Epoch: 55 [8448/50176]	Loss: 0.3996
Training Epoch: 55 [8704/50176]	Loss: 0.5004
Training Epoch: 55 [8960/50176]	Loss: 0.5514
Training Epoch: 55 [9216/50176]	Loss: 0.4182
Training Epoch: 55 [9472/50176]	Loss: 0.5593
Training Epoch: 55 [9728/50176]	Loss: 0.5332
Training Epoch: 55 [9984/50176]	Loss: 0.4295
Training Epoch: 55 [10240/50176]	Loss: 0.4020
Training Epoch: 55 [10496/50176]	Loss: 0.4468
Training Epoch: 55 [10752/50176]	Loss: 0.4869
Training Epoch: 55 [11008/50176]	Loss: 0.4263
Training Epoch: 55 [11264/50176]	Loss: 0.5287
Training Epoch: 55 [11520/50176]	Loss: 0.5971
Training Epoch: 55 [11776/50176]	Loss: 0.5615
Training Epoch: 55 [12032/50176]	Loss: 0.4874
Training Epoch: 55 [12288/50176]	Loss: 0.5158
Training Epoch: 55 [12544/50176]	Loss: 0.4814
Training Epoch: 55 [12800/50176]	Loss: 0.4190
Training Epoch: 55 [13056/50176]	Loss: 0.4349
Training Epoch: 55 [13312/50176]	Loss: 0.4609
Training Epoch: 55 [13568/50176]	Loss: 0.4342
Training Epoch: 55 [13824/50176]	Loss: 0.4343
Training Epoch: 55 [14080/50176]	Loss: 0.4528
Training Epoch: 55 [14336/50176]	Loss: 0.4397
Training Epoch: 55 [14592/50176]	Loss: 0.4954
Training Epoch: 55 [14848/50176]	Loss: 0.4427
Training Epoch: 55 [15104/50176]	Loss: 0.5910
Training Epoch: 55 [15360/50176]	Loss: 0.4583
Training Epoch: 55 [15616/50176]	Loss: 0.4935
Training Epoch: 55 [15872/50176]	Loss: 0.4755
Training Epoch: 55 [16128/50176]	Loss: 0.5605
Training Epoch: 55 [16384/50176]	Loss: 0.5090
Training Epoch: 55 [16640/50176]	Loss: 0.4205
Training Epoch: 55 [16896/50176]	Loss: 0.5626
Training Epoch: 55 [17152/50176]	Loss: 0.4063
Training Epoch: 55 [17408/50176]	Loss: 0.3883
Training Epoch: 55 [17664/50176]	Loss: 0.5152
Training Epoch: 55 [17920/50176]	Loss: 0.4732
Training Epoch: 55 [18176/50176]	Loss: 0.4815
Training Epoch: 55 [18432/50176]	Loss: 0.5347
Training Epoch: 55 [18688/50176]	Loss: 0.5210
Training Epoch: 55 [18944/50176]	Loss: 0.5690
Training Epoch: 55 [19200/50176]	Loss: 0.4476
Training Epoch: 55 [19456/50176]	Loss: 0.4756
Training Epoch: 55 [19712/50176]	Loss: 0.4170
Training Epoch: 55 [19968/50176]	Loss: 0.4643
Training Epoch: 55 [20224/50176]	Loss: 0.3640
Training Epoch: 55 [20480/50176]	Loss: 0.4667
Training Epoch: 55 [20736/50176]	Loss: 0.5173
Training Epoch: 55 [20992/50176]	Loss: 0.5269
Training Epoch: 55 [21248/50176]	Loss: 0.4811
Training Epoch: 55 [21504/50176]	Loss: 0.5164
Training Epoch: 55 [21760/50176]	Loss: 0.4770
Training Epoch: 55 [22016/50176]	Loss: 0.4560
Training Epoch: 55 [22272/50176]	Loss: 0.4374
Training Epoch: 55 [22528/50176]	Loss: 0.4958
Training Epoch: 55 [22784/50176]	Loss: 0.4422
Training Epoch: 55 [23040/50176]	Loss: 0.4619
Training Epoch: 55 [23296/50176]	Loss: 0.5863
Training Epoch: 55 [23552/50176]	Loss: 0.5294
Training Epoch: 55 [23808/50176]	Loss: 0.4366
Training Epoch: 55 [24064/50176]	Loss: 0.5094
Training Epoch: 55 [24320/50176]	Loss: 0.4789
Training Epoch: 55 [24576/50176]	Loss: 0.3906
Training Epoch: 55 [24832/50176]	Loss: 0.6498
Training Epoch: 55 [25088/50176]	Loss: 0.5184
Training Epoch: 55 [25344/50176]	Loss: 0.5095
Training Epoch: 55 [25600/50176]	Loss: 0.5135
Training Epoch: 55 [25856/50176]	Loss: 0.5080
Training Epoch: 55 [26112/50176]	Loss: 0.5544
Training Epoch: 55 [26368/50176]	Loss: 0.5022
Training Epoch: 55 [26624/50176]	Loss: 0.5427
Training Epoch: 55 [26880/50176]	Loss: 0.4190
Training Epoch: 55 [27136/50176]	Loss: 0.3692
Training Epoch: 55 [27392/50176]	Loss: 0.4676
Training Epoch: 55 [27648/50176]	Loss: 0.4986
Training Epoch: 55 [27904/50176]	Loss: 0.6619
Training Epoch: 55 [28160/50176]	Loss: 0.5200
Training Epoch: 55 [28416/50176]	Loss: 0.5165
Training Epoch: 55 [28672/50176]	Loss: 0.6422
Training Epoch: 55 [28928/50176]	Loss: 0.5761
Training Epoch: 55 [29184/50176]	Loss: 0.5681
Training Epoch: 55 [29440/50176]	Loss: 0.6379
Training Epoch: 55 [29696/50176]	Loss: 0.4432
Training Epoch: 55 [29952/50176]	Loss: 0.4516
Training Epoch: 55 [30208/50176]	Loss: 0.5907
Training Epoch: 55 [30464/50176]	Loss: 0.4454
Training Epoch: 55 [30720/50176]	Loss: 0.4296
Training Epoch: 55 [30976/50176]	Loss: 0.5785
Training Epoch: 55 [31232/50176]	Loss: 0.5561
Training Epoch: 55 [31488/50176]	Loss: 0.4367
Training Epoch: 55 [31744/50176]	Loss: 0.4241
Training Epoch: 55 [32000/50176]	Loss: 0.4794
Training Epoch: 55 [32256/50176]	Loss: 0.5987
Training Epoch: 55 [32512/50176]	Loss: 0.5846
Training Epoch: 55 [32768/50176]	Loss: 0.5424
Training Epoch: 55 [33024/50176]	Loss: 0.5996
Training Epoch: 55 [33280/50176]	Loss: 0.5899
Training Epoch: 55 [33536/50176]	Loss: 0.4820
Training Epoch: 55 [33792/50176]	Loss: 0.4097
Training Epoch: 55 [34048/50176]	Loss: 0.4080
Training Epoch: 55 [34304/50176]	Loss: 0.5128
Training Epoch: 55 [34560/50176]	Loss: 0.6213
Training Epoch: 55 [34816/50176]	Loss: 0.5471
Training Epoch: 55 [35072/50176]	Loss: 0.5483
Training Epoch: 55 [35328/50176]	Loss: 0.4063
Training Epoch: 55 [35584/50176]	Loss: 0.6060
Training Epoch: 55 [35840/50176]	Loss: 0.5642
Training Epoch: 55 [36096/50176]	Loss: 0.5318
Training Epoch: 55 [36352/50176]	Loss: 0.4576
Training Epoch: 55 [36608/50176]	Loss: 0.6173
Training Epoch: 55 [36864/50176]	Loss: 0.5072
Training Epoch: 55 [37120/50176]	Loss: 0.4748
Training Epoch: 55 [37376/50176]	Loss: 0.4023
Training Epoch: 55 [37632/50176]	Loss: 0.5762
Training Epoch: 55 [37888/50176]	Loss: 0.5106
Training Epoch: 55 [38144/50176]	Loss: 0.4693
Training Epoch: 55 [38400/50176]	Loss: 0.5261
Training Epoch: 55 [38656/50176]	Loss: 0.5648
Training Epoch: 55 [38912/50176]	Loss: 0.3974
Training Epoch: 55 [39168/50176]	Loss: 0.4529
Training Epoch: 55 [39424/50176]	Loss: 0.4619
Training Epoch: 55 [39680/50176]	Loss: 0.7388
Training Epoch: 55 [39936/50176]	Loss: 0.5181
Training Epoch: 55 [40192/50176]	Loss: 0.5788
Training Epoch: 55 [40448/50176]	Loss: 0.5328
Training Epoch: 55 [40704/50176]	Loss: 0.4104
Training Epoch: 55 [40960/50176]	Loss: 0.5502
Training Epoch: 55 [41216/50176]	Loss: 0.6028
Training Epoch: 55 [41472/50176]	Loss: 0.5429
Training Epoch: 55 [41728/50176]	Loss: 0.4364
Training Epoch: 55 [41984/50176]	Loss: 0.4950
Training Epoch: 55 [42240/50176]	Loss: 0.4658
Training Epoch: 55 [42496/50176]	Loss: 0.6888
Training Epoch: 55 [42752/50176]	Loss: 0.5539
Training Epoch: 55 [43008/50176]	Loss: 0.6506
Training Epoch: 55 [43264/50176]	Loss: 0.7058
Training Epoch: 55 [43520/50176]	Loss: 0.4562
Training Epoch: 55 [43776/50176]	Loss: 0.6212
Training Epoch: 55 [44032/50176]	Loss: 0.4732
Training Epoch: 55 [44288/50176]	Loss: 0.5582
Training Epoch: 55 [44544/50176]	Loss: 0.4878
Training Epoch: 55 [44800/50176]	Loss: 0.5481
Training Epoch: 55 [45056/50176]	Loss: 0.4944
Training Epoch: 55 [45312/50176]	Loss: 0.4885
Training Epoch: 55 [45568/50176]	Loss: 0.5108
Training Epoch: 55 [45824/50176]	Loss: 0.4302
Training Epoch: 55 [46080/50176]	Loss: 0.6326
Training Epoch: 55 [46336/50176]	Loss: 0.5906
Training Epoch: 55 [46592/50176]	Loss: 0.3444
Training Epoch: 55 [46848/50176]	Loss: 0.5320
Training Epoch: 55 [47104/50176]	Loss: 0.5691
Training Epoch: 55 [47360/50176]	Loss: 0.5143
Training Epoch: 55 [47616/50176]	Loss: 0.5759
Training Epoch: 55 [47872/50176]	Loss: 0.5195
Training Epoch: 55 [48128/50176]	Loss: 0.6498
Training Epoch: 55 [48384/50176]	Loss: 0.5505
Training Epoch: 55 [48640/50176]	Loss: 0.5465
Training Epoch: 55 [48896/50176]	Loss: 0.4369
Training Epoch: 55 [49152/50176]	Loss: 0.4869
Training Epoch: 55 [49408/50176]	Loss: 0.5204
Training Epoch: 55 [49664/50176]	Loss: 0.4434
Training Epoch: 55 [49920/50176]	Loss: 0.6426
Training Epoch: 55 [50176/50176]	Loss: 0.7228
Validation Epoch: 55, Average loss: 0.0094, Accuracy: 0.5627
Training Epoch: 56 [256/50176]	Loss: 0.4392
Training Epoch: 56 [512/50176]	Loss: 0.4831
Training Epoch: 56 [768/50176]	Loss: 0.3925
Training Epoch: 56 [1024/50176]	Loss: 0.4748
Training Epoch: 56 [1280/50176]	Loss: 0.6368
Training Epoch: 56 [1536/50176]	Loss: 0.5528
Training Epoch: 56 [1792/50176]	Loss: 0.3622
Training Epoch: 56 [2048/50176]	Loss: 0.4650
Training Epoch: 56 [2304/50176]	Loss: 0.4636
Training Epoch: 56 [2560/50176]	Loss: 0.4229
Training Epoch: 56 [2816/50176]	Loss: 0.4013
Training Epoch: 56 [3072/50176]	Loss: 0.4229
Training Epoch: 56 [3328/50176]	Loss: 0.4957
Training Epoch: 56 [3584/50176]	Loss: 0.4093
Training Epoch: 56 [3840/50176]	Loss: 0.5661
Training Epoch: 56 [4096/50176]	Loss: 0.4779
Training Epoch: 56 [4352/50176]	Loss: 0.5042
Training Epoch: 56 [4608/50176]	Loss: 0.4430
Training Epoch: 56 [4864/50176]	Loss: 0.4145
Training Epoch: 56 [5120/50176]	Loss: 0.3999
Training Epoch: 56 [5376/50176]	Loss: 0.4161
Training Epoch: 56 [5632/50176]	Loss: 0.4762
Training Epoch: 56 [5888/50176]	Loss: 0.4830
Training Epoch: 56 [6144/50176]	Loss: 0.5325
Training Epoch: 56 [6400/50176]	Loss: 0.4972
Training Epoch: 56 [6656/50176]	Loss: 0.5547
Training Epoch: 56 [6912/50176]	Loss: 0.5216
Training Epoch: 56 [7168/50176]	Loss: 0.5595
Training Epoch: 56 [7424/50176]	Loss: 0.4879
Training Epoch: 56 [7680/50176]	Loss: 0.4832
Training Epoch: 56 [7936/50176]	Loss: 0.3520
Training Epoch: 56 [8192/50176]	Loss: 0.3664
Training Epoch: 56 [8448/50176]	Loss: 0.5351
Training Epoch: 56 [8704/50176]	Loss: 0.4753
Training Epoch: 56 [8960/50176]	Loss: 0.4946
Training Epoch: 56 [9216/50176]	Loss: 0.5898
Training Epoch: 56 [9472/50176]	Loss: 0.5486
Training Epoch: 56 [9728/50176]	Loss: 0.5444
Training Epoch: 56 [9984/50176]	Loss: 0.4775
Training Epoch: 56 [10240/50176]	Loss: 0.4601
Training Epoch: 56 [10496/50176]	Loss: 0.3756
Training Epoch: 56 [10752/50176]	Loss: 0.5218
Training Epoch: 56 [11008/50176]	Loss: 0.4411
Training Epoch: 56 [11264/50176]	Loss: 0.4967
Training Epoch: 56 [11520/50176]	Loss: 0.4199
Training Epoch: 56 [11776/50176]	Loss: 0.4776
Training Epoch: 56 [12032/50176]	Loss: 0.5466
Training Epoch: 56 [12288/50176]	Loss: 0.4462
Training Epoch: 56 [12544/50176]	Loss: 0.5673
Training Epoch: 56 [12800/50176]	Loss: 0.4354
Training Epoch: 56 [13056/50176]	Loss: 0.4796
Training Epoch: 56 [13312/50176]	Loss: 0.5035
Training Epoch: 56 [13568/50176]	Loss: 0.3686
Training Epoch: 56 [13824/50176]	Loss: 0.4675
Training Epoch: 56 [14080/50176]	Loss: 0.3844
Training Epoch: 56 [14336/50176]	Loss: 0.4656
Training Epoch: 56 [14592/50176]	Loss: 0.4217
Training Epoch: 56 [14848/50176]	Loss: 0.4644
Training Epoch: 56 [15104/50176]	Loss: 0.4871
Training Epoch: 56 [15360/50176]	Loss: 0.5181
Training Epoch: 56 [15616/50176]	Loss: 0.4174
Training Epoch: 56 [15872/50176]	Loss: 0.5501
Training Epoch: 56 [16128/50176]	Loss: 0.4783
Training Epoch: 56 [16384/50176]	Loss: 0.5051
Training Epoch: 56 [16640/50176]	Loss: 0.4994
Training Epoch: 56 [16896/50176]	Loss: 0.4157
Training Epoch: 56 [17152/50176]	Loss: 0.6302
Training Epoch: 56 [17408/50176]	Loss: 0.4807
Training Epoch: 56 [17664/50176]	Loss: 0.6208
Training Epoch: 56 [17920/50176]	Loss: 0.5227
Training Epoch: 56 [18176/50176]	Loss: 0.3786
Training Epoch: 56 [18432/50176]	Loss: 0.4202
Training Epoch: 56 [18688/50176]	Loss: 0.3958
Training Epoch: 56 [18944/50176]	Loss: 0.5510
Training Epoch: 56 [19200/50176]	Loss: 0.4123
Training Epoch: 56 [19456/50176]	Loss: 0.4582
Training Epoch: 56 [19712/50176]	Loss: 0.5084
Training Epoch: 56 [19968/50176]	Loss: 0.4932
Training Epoch: 56 [20224/50176]	Loss: 0.4783
Training Epoch: 56 [20480/50176]	Loss: 0.5359
Training Epoch: 56 [20736/50176]	Loss: 0.4133
Training Epoch: 56 [20992/50176]	Loss: 0.5095
Training Epoch: 56 [21248/50176]	Loss: 0.4630
Training Epoch: 56 [21504/50176]	Loss: 0.4558
Training Epoch: 56 [21760/50176]	Loss: 0.4409
Training Epoch: 56 [22016/50176]	Loss: 0.5147
Training Epoch: 56 [22272/50176]	Loss: 0.4278
Training Epoch: 56 [22528/50176]	Loss: 0.4798
Training Epoch: 56 [22784/50176]	Loss: 0.5324
Training Epoch: 56 [23040/50176]	Loss: 0.4232
Training Epoch: 56 [23296/50176]	Loss: 0.4564
Training Epoch: 56 [23552/50176]	Loss: 0.4251
Training Epoch: 56 [23808/50176]	Loss: 0.4457
Training Epoch: 56 [24064/50176]	Loss: 0.4627
Training Epoch: 56 [24320/50176]	Loss: 0.5601
Training Epoch: 56 [24576/50176]	Loss: 0.6059
Training Epoch: 56 [24832/50176]	Loss: 0.5523
Training Epoch: 56 [25088/50176]	Loss: 0.5230
Training Epoch: 56 [25344/50176]	Loss: 0.5405
Training Epoch: 56 [25600/50176]	Loss: 0.4676
Training Epoch: 56 [25856/50176]	Loss: 0.4762
Training Epoch: 56 [26112/50176]	Loss: 0.4545
Training Epoch: 56 [26368/50176]	Loss: 0.4598
Training Epoch: 56 [26624/50176]	Loss: 0.4432
Training Epoch: 56 [26880/50176]	Loss: 0.4287
Training Epoch: 56 [27136/50176]	Loss: 0.5505
Training Epoch: 56 [27392/50176]	Loss: 0.4514
Training Epoch: 56 [27648/50176]	Loss: 0.4364
Training Epoch: 56 [27904/50176]	Loss: 0.4877
Training Epoch: 56 [28160/50176]	Loss: 0.5004
Training Epoch: 56 [28416/50176]	Loss: 0.5835
Training Epoch: 56 [28672/50176]	Loss: 0.4693
Training Epoch: 56 [28928/50176]	Loss: 0.4360
Training Epoch: 56 [29184/50176]	Loss: 0.5340
Training Epoch: 56 [29440/50176]	Loss: 0.4137
Training Epoch: 56 [29696/50176]	Loss: 0.4942
Training Epoch: 56 [29952/50176]	Loss: 0.4311
Training Epoch: 56 [30208/50176]	Loss: 0.5278
Training Epoch: 56 [30464/50176]	Loss: 0.5501
Training Epoch: 56 [30720/50176]	Loss: 0.5195
Training Epoch: 56 [30976/50176]	Loss: 0.6112
Training Epoch: 56 [31232/50176]	Loss: 0.4954
Training Epoch: 56 [31488/50176]	Loss: 0.5377
Training Epoch: 56 [31744/50176]	Loss: 0.5776
Training Epoch: 56 [32000/50176]	Loss: 0.5495
Training Epoch: 56 [32256/50176]	Loss: 0.5162
Training Epoch: 56 [32512/50176]	Loss: 0.4877
Training Epoch: 56 [32768/50176]	Loss: 0.4814
Training Epoch: 56 [33024/50176]	Loss: 0.4856
Training Epoch: 56 [33280/50176]	Loss: 0.6123
Training Epoch: 56 [33536/50176]	Loss: 0.5133
Training Epoch: 56 [33792/50176]	Loss: 0.5791
Training Epoch: 56 [34048/50176]	Loss: 0.4260
Training Epoch: 56 [34304/50176]	Loss: 0.6080
Training Epoch: 56 [34560/50176]	Loss: 0.4245
Training Epoch: 56 [34816/50176]	Loss: 0.4801
Training Epoch: 56 [35072/50176]	Loss: 0.5182
Training Epoch: 56 [35328/50176]	Loss: 0.6707
Training Epoch: 56 [35584/50176]	Loss: 0.5395
Training Epoch: 56 [35840/50176]	Loss: 0.5013
Training Epoch: 56 [36096/50176]	Loss: 0.5605
Training Epoch: 56 [36352/50176]	Loss: 0.6153
Training Epoch: 56 [36608/50176]	Loss: 0.3120
Training Epoch: 56 [36864/50176]	Loss: 0.5830
Training Epoch: 56 [37120/50176]	Loss: 0.5222
Training Epoch: 56 [37376/50176]	Loss: 0.4660
Training Epoch: 56 [37632/50176]	Loss: 0.5659
Training Epoch: 56 [37888/50176]	Loss: 0.6451
Training Epoch: 56 [38144/50176]	Loss: 0.5664
Training Epoch: 56 [38400/50176]	Loss: 0.4973
Training Epoch: 56 [38656/50176]	Loss: 0.4476
Training Epoch: 56 [38912/50176]	Loss: 0.4436
Training Epoch: 56 [39168/50176]	Loss: 0.3859
Training Epoch: 56 [39424/50176]	Loss: 0.5347
Training Epoch: 56 [39680/50176]	Loss: 0.4842
Training Epoch: 56 [39936/50176]	Loss: 0.4055
Training Epoch: 56 [40192/50176]	Loss: 0.4741
Training Epoch: 56 [40448/50176]	Loss: 0.4831
Training Epoch: 56 [40704/50176]	Loss: 0.5159
Training Epoch: 56 [40960/50176]	Loss: 0.5407
Training Epoch: 56 [41216/50176]	Loss: 0.5032
Training Epoch: 56 [41472/50176]	Loss: 0.5780
Training Epoch: 56 [41728/50176]	Loss: 0.5027
Training Epoch: 56 [41984/50176]	Loss: 0.4980
Training Epoch: 56 [42240/50176]	Loss: 0.4900
Training Epoch: 56 [42496/50176]	Loss: 0.6183
Training Epoch: 56 [42752/50176]	Loss: 0.4354
Training Epoch: 56 [43008/50176]	Loss: 0.5222
Training Epoch: 56 [43264/50176]	Loss: 0.6053
Training Epoch: 56 [43520/50176]	Loss: 0.5084
Training Epoch: 56 [43776/50176]	Loss: 0.5253
Training Epoch: 56 [44032/50176]	Loss: 0.4472
Training Epoch: 56 [44288/50176]	Loss: 0.5780
Training Epoch: 56 [44544/50176]	Loss: 0.5073
Training Epoch: 56 [44800/50176]	Loss: 0.4959
Training Epoch: 56 [45056/50176]	Loss: 0.5781
Training Epoch: 56 [45312/50176]	Loss: 0.5169
Training Epoch: 56 [45568/50176]	Loss: 0.4701
Training Epoch: 56 [45824/50176]	Loss: 0.5620
Training Epoch: 56 [46080/50176]	Loss: 0.5157
Training Epoch: 56 [46336/50176]	Loss: 0.4730
Training Epoch: 56 [46592/50176]	Loss: 0.4814
Training Epoch: 56 [46848/50176]	Loss: 0.5662
Training Epoch: 56 [47104/50176]	Loss: 0.5970
Training Epoch: 56 [47360/50176]	Loss: 0.5123
Training Epoch: 56 [47616/50176]	Loss: 0.6305
Training Epoch: 56 [47872/50176]	Loss: 0.5457
Training Epoch: 56 [48128/50176]	Loss: 0.4271
Training Epoch: 56 [48384/50176]	Loss: 0.5127
Training Epoch: 56 [48640/50176]	Loss: 0.5921
Training Epoch: 56 [48896/50176]	Loss: 0.5509
Training Epoch: 56 [49152/50176]	Loss: 0.4992
Training Epoch: 56 [49408/50176]	Loss: 0.6212
Training Epoch: 56 [49664/50176]	Loss: 0.4521
Training Epoch: 56 [49920/50176]	Loss: 0.5811
Training Epoch: 56 [50176/50176]	Loss: 0.5198
Validation Epoch: 56, Average loss: 0.0106, Accuracy: 0.5246
Training Epoch: 57 [256/50176]	Loss: 0.4048
Training Epoch: 57 [512/50176]	Loss: 0.3958
Training Epoch: 57 [768/50176]	Loss: 0.2889
Training Epoch: 57 [1024/50176]	Loss: 0.5178
Training Epoch: 57 [1280/50176]	Loss: 0.3859
Training Epoch: 57 [1536/50176]	Loss: 0.4372
Training Epoch: 57 [1792/50176]	Loss: 0.3870
Training Epoch: 57 [2048/50176]	Loss: 0.4246
Training Epoch: 57 [2304/50176]	Loss: 0.4131
Training Epoch: 57 [2560/50176]	Loss: 0.4063
Training Epoch: 57 [2816/50176]	Loss: 0.3793
Training Epoch: 57 [3072/50176]	Loss: 0.4552
Training Epoch: 57 [3328/50176]	Loss: 0.4410
Training Epoch: 57 [3584/50176]	Loss: 0.4174
Training Epoch: 57 [3840/50176]	Loss: 0.4867
Training Epoch: 57 [4096/50176]	Loss: 0.4894
Training Epoch: 57 [4352/50176]	Loss: 0.4499
Training Epoch: 57 [4608/50176]	Loss: 0.4473
Training Epoch: 57 [4864/50176]	Loss: 0.5855
Training Epoch: 57 [5120/50176]	Loss: 0.3936
Training Epoch: 57 [5376/50176]	Loss: 0.4259
Training Epoch: 57 [5632/50176]	Loss: 0.4756
Training Epoch: 57 [5888/50176]	Loss: 0.4993
Training Epoch: 57 [6144/50176]	Loss: 0.4741
Training Epoch: 57 [6400/50176]	Loss: 0.5040
Training Epoch: 57 [6656/50176]	Loss: 0.4121
Training Epoch: 57 [6912/50176]	Loss: 0.4526
Training Epoch: 57 [7168/50176]	Loss: 0.3415
Training Epoch: 57 [7424/50176]	Loss: 0.4218
Training Epoch: 57 [7680/50176]	Loss: 0.4274
Training Epoch: 57 [7936/50176]	Loss: 0.4552
Training Epoch: 57 [8192/50176]	Loss: 0.4623
Training Epoch: 57 [8448/50176]	Loss: 0.4031
Training Epoch: 57 [8704/50176]	Loss: 0.4635
Training Epoch: 57 [8960/50176]	Loss: 0.5014
Training Epoch: 57 [9216/50176]	Loss: 0.3270
Training Epoch: 57 [9472/50176]	Loss: 0.5430
Training Epoch: 57 [9728/50176]	Loss: 0.3331
Training Epoch: 57 [9984/50176]	Loss: 0.4816
Training Epoch: 57 [10240/50176]	Loss: 0.4060
Training Epoch: 57 [10496/50176]	Loss: 0.4489
Training Epoch: 57 [10752/50176]	Loss: 0.5276
Training Epoch: 57 [11008/50176]	Loss: 0.5934
Training Epoch: 57 [11264/50176]	Loss: 0.4612
Training Epoch: 57 [11520/50176]	Loss: 0.4083
Training Epoch: 57 [11776/50176]	Loss: 0.4604
Training Epoch: 57 [12032/50176]	Loss: 0.3950
Training Epoch: 57 [12288/50176]	Loss: 0.4486
Training Epoch: 57 [12544/50176]	Loss: 0.5461
Training Epoch: 57 [12800/50176]	Loss: 0.5144
Training Epoch: 57 [13056/50176]	Loss: 0.4526
Training Epoch: 57 [13312/50176]	Loss: 0.4183
Training Epoch: 57 [13568/50176]	Loss: 0.4488
Training Epoch: 57 [13824/50176]	Loss: 0.4840
Training Epoch: 57 [14080/50176]	Loss: 0.5146
Training Epoch: 57 [14336/50176]	Loss: 0.4631
Training Epoch: 57 [14592/50176]	Loss: 0.4093
Training Epoch: 57 [14848/50176]	Loss: 0.4556
Training Epoch: 57 [15104/50176]	Loss: 0.3722
Training Epoch: 57 [15360/50176]	Loss: 0.4830
Training Epoch: 57 [15616/50176]	Loss: 0.4606
Training Epoch: 57 [15872/50176]	Loss: 0.5124
Training Epoch: 57 [16128/50176]	Loss: 0.3780
Training Epoch: 57 [16384/50176]	Loss: 0.3463
Training Epoch: 57 [16640/50176]	Loss: 0.3822
Training Epoch: 57 [16896/50176]	Loss: 0.5024
Training Epoch: 57 [17152/50176]	Loss: 0.5044
Training Epoch: 57 [17408/50176]	Loss: 0.4150
Training Epoch: 57 [17664/50176]	Loss: 0.5097
Training Epoch: 57 [17920/50176]	Loss: 0.5057
Training Epoch: 57 [18176/50176]	Loss: 0.4904
Training Epoch: 57 [18432/50176]	Loss: 0.5458
Training Epoch: 57 [18688/50176]	Loss: 0.4101
Training Epoch: 57 [18944/50176]	Loss: 0.6265
Training Epoch: 57 [19200/50176]	Loss: 0.4711
Training Epoch: 57 [19456/50176]	Loss: 0.4942
Training Epoch: 57 [19712/50176]	Loss: 0.4881
Training Epoch: 57 [19968/50176]	Loss: 0.5704
Training Epoch: 57 [20224/50176]	Loss: 0.5066
Training Epoch: 57 [20480/50176]	Loss: 0.5200
Training Epoch: 57 [20736/50176]	Loss: 0.3726
Training Epoch: 57 [20992/50176]	Loss: 0.5132
Training Epoch: 57 [21248/50176]	Loss: 0.5342
Training Epoch: 57 [21504/50176]	Loss: 0.4830
Training Epoch: 57 [21760/50176]	Loss: 0.5015
Training Epoch: 57 [22016/50176]	Loss: 0.3489
Training Epoch: 57 [22272/50176]	Loss: 0.4322
Training Epoch: 57 [22528/50176]	Loss: 0.5073
Training Epoch: 57 [22784/50176]	Loss: 0.4642
Training Epoch: 57 [23040/50176]	Loss: 0.4867
Training Epoch: 57 [23296/50176]	Loss: 0.4934
Training Epoch: 57 [23552/50176]	Loss: 0.4603
Training Epoch: 57 [23808/50176]	Loss: 0.5610
Training Epoch: 57 [24064/50176]	Loss: 0.5242
Training Epoch: 57 [24320/50176]	Loss: 0.5041
Training Epoch: 57 [24576/50176]	Loss: 0.5423
Training Epoch: 57 [24832/50176]	Loss: 0.5173
Training Epoch: 57 [25088/50176]	Loss: 0.4670
Training Epoch: 57 [25344/50176]	Loss: 0.4151
Training Epoch: 57 [25600/50176]	Loss: 0.4145
Training Epoch: 57 [25856/50176]	Loss: 0.3916
Training Epoch: 57 [26112/50176]	Loss: 0.4031
Training Epoch: 57 [26368/50176]	Loss: 0.4037
Training Epoch: 57 [26624/50176]	Loss: 0.5082
Training Epoch: 57 [26880/50176]	Loss: 0.5754
Training Epoch: 57 [27136/50176]	Loss: 0.5583
Training Epoch: 57 [27392/50176]	Loss: 0.4673
Training Epoch: 57 [27648/50176]	Loss: 0.5940
Training Epoch: 57 [27904/50176]	Loss: 0.4834
Training Epoch: 57 [28160/50176]	Loss: 0.4765
Training Epoch: 57 [28416/50176]	Loss: 0.3864
Training Epoch: 57 [28672/50176]	Loss: 0.5028
Training Epoch: 57 [28928/50176]	Loss: 0.5315
Training Epoch: 57 [29184/50176]	Loss: 0.4382
Training Epoch: 57 [29440/50176]	Loss: 0.5174
Training Epoch: 57 [29696/50176]	Loss: 0.5618
Training Epoch: 57 [29952/50176]	Loss: 0.4201
Training Epoch: 57 [30208/50176]	Loss: 0.5103
Training Epoch: 57 [30464/50176]	Loss: 0.4139
Training Epoch: 57 [30720/50176]	Loss: 0.5051
Training Epoch: 57 [30976/50176]	Loss: 0.4663
Training Epoch: 57 [31232/50176]	Loss: 0.4898
Training Epoch: 57 [31488/50176]	Loss: 0.3473
Training Epoch: 57 [31744/50176]	Loss: 0.5045
Training Epoch: 57 [32000/50176]	Loss: 0.5096
Training Epoch: 57 [32256/50176]	Loss: 0.5477
Training Epoch: 57 [32512/50176]	Loss: 0.4426
Training Epoch: 57 [32768/50176]	Loss: 0.4772
Training Epoch: 57 [33024/50176]	Loss: 0.4457
Training Epoch: 57 [33280/50176]	Loss: 0.5546
Training Epoch: 57 [33536/50176]	Loss: 0.4203
Training Epoch: 57 [33792/50176]	Loss: 0.5648
Training Epoch: 57 [34048/50176]	Loss: 0.6117
Training Epoch: 57 [34304/50176]	Loss: 0.4458
Training Epoch: 57 [34560/50176]	Loss: 0.6306
Training Epoch: 57 [34816/50176]	Loss: 0.5356
Training Epoch: 57 [35072/50176]	Loss: 0.6656
Training Epoch: 57 [35328/50176]	Loss: 0.4877
Training Epoch: 57 [35584/50176]	Loss: 0.4386
Training Epoch: 57 [35840/50176]	Loss: 0.5739
Training Epoch: 57 [36096/50176]	Loss: 0.4717
Training Epoch: 57 [36352/50176]	Loss: 0.4649
Training Epoch: 57 [36608/50176]	Loss: 0.4994
Training Epoch: 57 [36864/50176]	Loss: 0.4377
Training Epoch: 57 [37120/50176]	Loss: 0.5335
Training Epoch: 57 [37376/50176]	Loss: 0.5758
Training Epoch: 57 [37632/50176]	Loss: 0.5567
Training Epoch: 57 [37888/50176]	Loss: 0.4618
Training Epoch: 57 [38144/50176]	Loss: 0.5241
Training Epoch: 57 [38400/50176]	Loss: 0.3946
Training Epoch: 57 [38656/50176]	Loss: 0.4594
Training Epoch: 57 [38912/50176]	Loss: 0.4399
Training Epoch: 57 [39168/50176]	Loss: 0.5255
Training Epoch: 57 [39424/50176]	Loss: 0.4682
Training Epoch: 57 [39680/50176]	Loss: 0.4848
Training Epoch: 57 [39936/50176]	Loss: 0.5698
Training Epoch: 57 [40192/50176]	Loss: 0.5548
Training Epoch: 57 [40448/50176]	Loss: 0.4064
Training Epoch: 57 [40704/50176]	Loss: 0.6236
Training Epoch: 57 [40960/50176]	Loss: 0.5480
Training Epoch: 57 [41216/50176]	Loss: 0.4494
Training Epoch: 57 [41472/50176]	Loss: 0.4870
Training Epoch: 57 [41728/50176]	Loss: 0.5243
Training Epoch: 57 [41984/50176]	Loss: 0.5030
Training Epoch: 57 [42240/50176]	Loss: 0.5719
Training Epoch: 57 [42496/50176]	Loss: 0.5207
Training Epoch: 57 [42752/50176]	Loss: 0.5213
Training Epoch: 57 [43008/50176]	Loss: 0.6290
Training Epoch: 57 [43264/50176]	Loss: 0.4747
Training Epoch: 57 [43520/50176]	Loss: 0.5868
Training Epoch: 57 [43776/50176]	Loss: 0.4820
Training Epoch: 57 [44032/50176]	Loss: 0.5160
Training Epoch: 57 [44288/50176]	Loss: 0.6082
Training Epoch: 57 [44544/50176]	Loss: 0.5515
Training Epoch: 57 [44800/50176]	Loss: 0.6531
Training Epoch: 57 [45056/50176]	Loss: 0.4335
Training Epoch: 57 [45312/50176]	Loss: 0.5122
Training Epoch: 57 [45568/50176]	Loss: 0.4503
Training Epoch: 57 [45824/50176]	Loss: 0.5861
Training Epoch: 57 [46080/50176]	Loss: 0.4943
Training Epoch: 57 [46336/50176]	Loss: 0.4835
Training Epoch: 57 [46592/50176]	Loss: 0.4634
Training Epoch: 57 [46848/50176]	Loss: 0.5693
Training Epoch: 57 [47104/50176]	Loss: 0.4888
Training Epoch: 57 [47360/50176]	Loss: 0.4783
Training Epoch: 57 [47616/50176]	Loss: 0.4185
Training Epoch: 57 [47872/50176]	Loss: 0.4992
Training Epoch: 57 [48128/50176]	Loss: 0.5861
Training Epoch: 57 [48384/50176]	Loss: 0.4902
Training Epoch: 57 [48640/50176]	Loss: 0.4707
Training Epoch: 57 [48896/50176]	Loss: 0.5400
Training Epoch: 57 [49152/50176]	Loss: 0.4777
Training Epoch: 57 [49408/50176]	Loss: 0.5086
Training Epoch: 57 [49664/50176]	Loss: 0.5483
Training Epoch: 57 [49920/50176]	Loss: 0.6235
Training Epoch: 57 [50176/50176]	Loss: 0.6491
Validation Epoch: 57, Average loss: 0.0100, Accuracy: 0.5442
Training Epoch: 58 [256/50176]	Loss: 0.3707
Training Epoch: 58 [512/50176]	Loss: 0.3924
Training Epoch: 58 [768/50176]	Loss: 0.3699
Training Epoch: 58 [1024/50176]	Loss: 0.4876
Training Epoch: 58 [1280/50176]	Loss: 0.4841
Training Epoch: 58 [1536/50176]	Loss: 0.3768
Training Epoch: 58 [1792/50176]	Loss: 0.4316
Training Epoch: 58 [2048/50176]	Loss: 0.4395
Training Epoch: 58 [2304/50176]	Loss: 0.5326
Training Epoch: 58 [2560/50176]	Loss: 0.4898
Training Epoch: 58 [2816/50176]	Loss: 0.4210
Training Epoch: 58 [3072/50176]	Loss: 0.5384
Training Epoch: 58 [3328/50176]	Loss: 0.4677
Training Epoch: 58 [3584/50176]	Loss: 0.4025
Training Epoch: 58 [3840/50176]	Loss: 0.4449
Training Epoch: 58 [4096/50176]	Loss: 0.4516
Training Epoch: 58 [4352/50176]	Loss: 0.3293
Training Epoch: 58 [4608/50176]	Loss: 0.4435
Training Epoch: 58 [4864/50176]	Loss: 0.4632
Training Epoch: 58 [5120/50176]	Loss: 0.4066
Training Epoch: 58 [5376/50176]	Loss: 0.5296
Training Epoch: 58 [5632/50176]	Loss: 0.5497
Training Epoch: 58 [5888/50176]	Loss: 0.5074
Training Epoch: 58 [6144/50176]	Loss: 0.5708
Training Epoch: 58 [6400/50176]	Loss: 0.2956
Training Epoch: 58 [6656/50176]	Loss: 0.5088
Training Epoch: 58 [6912/50176]	Loss: 0.4740
Training Epoch: 58 [7168/50176]	Loss: 0.5336
Training Epoch: 58 [7424/50176]	Loss: 0.4124
Training Epoch: 58 [7680/50176]	Loss: 0.4144
Training Epoch: 58 [7936/50176]	Loss: 0.4515
Training Epoch: 58 [8192/50176]	Loss: 0.4645
Training Epoch: 58 [8448/50176]	Loss: 0.5074
Training Epoch: 58 [8704/50176]	Loss: 0.4066
Training Epoch: 58 [8960/50176]	Loss: 0.4684
Training Epoch: 58 [9216/50176]	Loss: 0.4227
Training Epoch: 58 [9472/50176]	Loss: 0.4276
Training Epoch: 58 [9728/50176]	Loss: 0.3432
Training Epoch: 58 [9984/50176]	Loss: 0.4721
Training Epoch: 58 [10240/50176]	Loss: 0.4890
Training Epoch: 58 [10496/50176]	Loss: 0.3939
Training Epoch: 58 [10752/50176]	Loss: 0.3060
Training Epoch: 58 [11008/50176]	Loss: 0.4192
Training Epoch: 58 [11264/50176]	Loss: 0.4492
Training Epoch: 58 [11520/50176]	Loss: 0.4247
Training Epoch: 58 [11776/50176]	Loss: 0.4816
Training Epoch: 58 [12032/50176]	Loss: 0.4166
Training Epoch: 58 [12288/50176]	Loss: 0.3409
Training Epoch: 58 [12544/50176]	Loss: 0.3369
Training Epoch: 58 [12800/50176]	Loss: 0.3866
Training Epoch: 58 [13056/50176]	Loss: 0.4171
Training Epoch: 58 [13312/50176]	Loss: 0.3886
Training Epoch: 58 [13568/50176]	Loss: 0.4957
Training Epoch: 58 [13824/50176]	Loss: 0.4321
Training Epoch: 58 [14080/50176]	Loss: 0.5333
Training Epoch: 58 [14336/50176]	Loss: 0.4338
Training Epoch: 58 [14592/50176]	Loss: 0.4523
Training Epoch: 58 [14848/50176]	Loss: 0.4709
Training Epoch: 58 [15104/50176]	Loss: 0.3524
Training Epoch: 58 [15360/50176]	Loss: 0.3967
Training Epoch: 58 [15616/50176]	Loss: 0.4564
Training Epoch: 58 [15872/50176]	Loss: 0.3706
Training Epoch: 58 [16128/50176]	Loss: 0.4190
Training Epoch: 58 [16384/50176]	Loss: 0.3882
Training Epoch: 58 [16640/50176]	Loss: 0.4289
Training Epoch: 58 [16896/50176]	Loss: 0.5105
Training Epoch: 58 [17152/50176]	Loss: 0.3833
Training Epoch: 58 [17408/50176]	Loss: 0.4351
Training Epoch: 58 [17664/50176]	Loss: 0.4122
Training Epoch: 58 [17920/50176]	Loss: 0.4468
Training Epoch: 58 [18176/50176]	Loss: 0.4424
Training Epoch: 58 [18432/50176]	Loss: 0.5207
Training Epoch: 58 [18688/50176]	Loss: 0.4398
Training Epoch: 58 [18944/50176]	Loss: 0.4171
Training Epoch: 58 [19200/50176]	Loss: 0.5193
Training Epoch: 58 [19456/50176]	Loss: 0.6097
Training Epoch: 58 [19712/50176]	Loss: 0.4377
Training Epoch: 58 [19968/50176]	Loss: 0.5393
Training Epoch: 58 [20224/50176]	Loss: 0.4727
Training Epoch: 58 [20480/50176]	Loss: 0.3659
Training Epoch: 58 [20736/50176]	Loss: 0.5037
Training Epoch: 58 [20992/50176]	Loss: 0.5232
Training Epoch: 58 [21248/50176]	Loss: 0.5157
Training Epoch: 58 [21504/50176]	Loss: 0.4787
Training Epoch: 58 [21760/50176]	Loss: 0.5438
Training Epoch: 58 [22016/50176]	Loss: 0.4142
Training Epoch: 58 [22272/50176]	Loss: 0.5460
Training Epoch: 58 [22528/50176]	Loss: 0.4429
Training Epoch: 58 [22784/50176]	Loss: 0.6189
Training Epoch: 58 [23040/50176]	Loss: 0.4535
Training Epoch: 58 [23296/50176]	Loss: 0.4518
Training Epoch: 58 [23552/50176]	Loss: 0.5278
Training Epoch: 58 [23808/50176]	Loss: 0.3968
Training Epoch: 58 [24064/50176]	Loss: 0.3969
Training Epoch: 58 [24320/50176]	Loss: 0.4129
Training Epoch: 58 [24576/50176]	Loss: 0.5938
Training Epoch: 58 [24832/50176]	Loss: 0.4951
Training Epoch: 58 [25088/50176]	Loss: 0.4423
Training Epoch: 58 [25344/50176]	Loss: 0.3282
Training Epoch: 58 [25600/50176]	Loss: 0.5229
Training Epoch: 58 [25856/50176]	Loss: 0.4944
Training Epoch: 58 [26112/50176]	Loss: 0.5207
Training Epoch: 58 [26368/50176]	Loss: 0.5360
Training Epoch: 58 [26624/50176]	Loss: 0.4651
Training Epoch: 58 [26880/50176]	Loss: 0.3788
Training Epoch: 58 [27136/50176]	Loss: 0.6153
Training Epoch: 58 [27392/50176]	Loss: 0.5490
Training Epoch: 58 [27648/50176]	Loss: 0.4473
Training Epoch: 58 [27904/50176]	Loss: 0.3992
Training Epoch: 58 [28160/50176]	Loss: 0.4075
Training Epoch: 58 [28416/50176]	Loss: 0.3218
Training Epoch: 58 [28672/50176]	Loss: 0.4464
Training Epoch: 58 [28928/50176]	Loss: 0.4214
Training Epoch: 58 [29184/50176]	Loss: 0.3768
Training Epoch: 58 [29440/50176]	Loss: 0.4360
Training Epoch: 58 [29696/50176]	Loss: 0.5496
Training Epoch: 58 [29952/50176]	Loss: 0.4975
Training Epoch: 58 [30208/50176]	Loss: 0.4306
Training Epoch: 58 [30464/50176]	Loss: 0.4368
Training Epoch: 58 [30720/50176]	Loss: 0.5766
Training Epoch: 58 [30976/50176]	Loss: 0.4787
Training Epoch: 58 [31232/50176]	Loss: 0.5364
Training Epoch: 58 [31488/50176]	Loss: 0.3969
Training Epoch: 58 [31744/50176]	Loss: 0.4765
Training Epoch: 58 [32000/50176]	Loss: 0.4465
Training Epoch: 58 [32256/50176]	Loss: 0.4574
Training Epoch: 58 [32512/50176]	Loss: 0.3801
Training Epoch: 58 [32768/50176]	Loss: 0.5723
Training Epoch: 58 [33024/50176]	Loss: 0.4679
Training Epoch: 58 [33280/50176]	Loss: 0.3941
Training Epoch: 58 [33536/50176]	Loss: 0.4549
Training Epoch: 58 [33792/50176]	Loss: 0.3363
Training Epoch: 58 [34048/50176]	Loss: 0.5608
Training Epoch: 58 [34304/50176]	Loss: 0.4927
Training Epoch: 58 [34560/50176]	Loss: 0.4848
Training Epoch: 58 [34816/50176]	Loss: 0.4959
Training Epoch: 58 [35072/50176]	Loss: 0.4269
Training Epoch: 58 [35328/50176]	Loss: 0.4362
Training Epoch: 58 [35584/50176]	Loss: 0.5700
Training Epoch: 58 [35840/50176]	Loss: 0.3892
Training Epoch: 58 [36096/50176]	Loss: 0.5734
Training Epoch: 58 [36352/50176]	Loss: 0.4694
Training Epoch: 58 [36608/50176]	Loss: 0.4592
Training Epoch: 58 [36864/50176]	Loss: 0.4413
Training Epoch: 58 [37120/50176]	Loss: 0.4569
Training Epoch: 58 [37376/50176]	Loss: 0.3834
Training Epoch: 58 [37632/50176]	Loss: 0.5790
Training Epoch: 58 [37888/50176]	Loss: 0.5360
Training Epoch: 58 [38144/50176]	Loss: 0.5550
Training Epoch: 58 [38400/50176]	Loss: 0.5611
Training Epoch: 58 [38656/50176]	Loss: 0.4785
Training Epoch: 58 [38912/50176]	Loss: 0.4599
Training Epoch: 58 [39168/50176]	Loss: 0.4557
Training Epoch: 58 [39424/50176]	Loss: 0.5012
Training Epoch: 58 [39680/50176]	Loss: 0.4663
Training Epoch: 58 [39936/50176]	Loss: 0.4574
Training Epoch: 58 [40192/50176]	Loss: 0.5164
Training Epoch: 58 [40448/50176]	Loss: 0.5168
Training Epoch: 58 [40704/50176]	Loss: 0.5233
Training Epoch: 58 [40960/50176]	Loss: 0.5681
Training Epoch: 58 [41216/50176]	Loss: 0.4622
Training Epoch: 58 [41472/50176]	Loss: 0.5078
Training Epoch: 58 [41728/50176]	Loss: 0.4162
Training Epoch: 58 [41984/50176]	Loss: 0.4775
Training Epoch: 58 [42240/50176]	Loss: 0.4923
Training Epoch: 58 [42496/50176]	Loss: 0.4750
Training Epoch: 58 [42752/50176]	Loss: 0.4251
Training Epoch: 58 [43008/50176]	Loss: 0.4727
Training Epoch: 58 [43264/50176]	Loss: 0.5552
Training Epoch: 58 [43520/50176]	Loss: 0.6016
Training Epoch: 58 [43776/50176]	Loss: 0.4251
Training Epoch: 58 [44032/50176]	Loss: 0.4711
Training Epoch: 58 [44288/50176]	Loss: 0.4421
Training Epoch: 58 [44544/50176]	Loss: 0.4558
Training Epoch: 58 [44800/50176]	Loss: 0.6665
Training Epoch: 58 [45056/50176]	Loss: 0.4399
Training Epoch: 58 [45312/50176]	Loss: 0.4280
Training Epoch: 58 [45568/50176]	Loss: 0.5453
Training Epoch: 58 [45824/50176]	Loss: 0.4748
Training Epoch: 58 [46080/50176]	Loss: 0.4252
Training Epoch: 58 [46336/50176]	Loss: 0.5865
Training Epoch: 58 [46592/50176]	Loss: 0.5459
Training Epoch: 58 [46848/50176]	Loss: 0.4353
Training Epoch: 58 [47104/50176]	Loss: 0.5793
Training Epoch: 58 [47360/50176]	Loss: 0.4800
Training Epoch: 58 [47616/50176]	Loss: 0.5380
Training Epoch: 58 [47872/50176]	Loss: 0.5069
Training Epoch: 58 [48128/50176]	Loss: 0.3940
Training Epoch: 58 [48384/50176]	Loss: 0.5137
Training Epoch: 58 [48640/50176]	Loss: 0.5219
Training Epoch: 58 [48896/50176]	Loss: 0.6052
Training Epoch: 58 [49152/50176]	Loss: 0.6198
Training Epoch: 58 [49408/50176]	Loss: 0.6010
Training Epoch: 58 [49664/50176]	Loss: 0.4030
Training Epoch: 58 [49920/50176]	Loss: 0.4338
Training Epoch: 58 [50176/50176]	Loss: 0.5966
Validation Epoch: 58, Average loss: 0.0095, Accuracy: 0.5557
Training Epoch: 59 [256/50176]	Loss: 0.4625
Training Epoch: 59 [512/50176]	Loss: 0.3498
Training Epoch: 59 [768/50176]	Loss: 0.4062
Training Epoch: 59 [1024/50176]	Loss: 0.4109
Training Epoch: 59 [1280/50176]	Loss: 0.4620
Training Epoch: 59 [1536/50176]	Loss: 0.4747
Training Epoch: 59 [1792/50176]	Loss: 0.4605
Training Epoch: 59 [2048/50176]	Loss: 0.4538
Training Epoch: 59 [2304/50176]	Loss: 0.4675
Training Epoch: 59 [2560/50176]	Loss: 0.4121
Training Epoch: 59 [2816/50176]	Loss: 0.5136
Training Epoch: 59 [3072/50176]	Loss: 0.4528
Training Epoch: 59 [3328/50176]	Loss: 0.3598
Training Epoch: 59 [3584/50176]	Loss: 0.5213
Training Epoch: 59 [3840/50176]	Loss: 0.4805
Training Epoch: 59 [4096/50176]	Loss: 0.4666
Training Epoch: 59 [4352/50176]	Loss: 0.4153
Training Epoch: 59 [4608/50176]	Loss: 0.4765
Training Epoch: 59 [4864/50176]	Loss: 0.5375
Training Epoch: 59 [5120/50176]	Loss: 0.4079
Training Epoch: 59 [5376/50176]	Loss: 0.5442
Training Epoch: 59 [5632/50176]	Loss: 0.4778
Training Epoch: 59 [5888/50176]	Loss: 0.4243
Training Epoch: 59 [6144/50176]	Loss: 0.4614
Training Epoch: 59 [6400/50176]	Loss: 0.4381
Training Epoch: 59 [6656/50176]	Loss: 0.3618
Training Epoch: 59 [6912/50176]	Loss: 0.5468
Training Epoch: 59 [7168/50176]	Loss: 0.4444
Training Epoch: 59 [7424/50176]	Loss: 0.3542
Training Epoch: 59 [7680/50176]	Loss: 0.4449
Training Epoch: 59 [7936/50176]	Loss: 0.4615
Training Epoch: 59 [8192/50176]	Loss: 0.4164
Training Epoch: 59 [8448/50176]	Loss: 0.4656
Training Epoch: 59 [8704/50176]	Loss: 0.4177
Training Epoch: 59 [8960/50176]	Loss: 0.4853
Training Epoch: 59 [9216/50176]	Loss: 0.3569
Training Epoch: 59 [9472/50176]	Loss: 0.5526
Training Epoch: 59 [9728/50176]	Loss: 0.4111
Training Epoch: 59 [9984/50176]	Loss: 0.4292
Training Epoch: 59 [10240/50176]	Loss: 0.3623
Training Epoch: 59 [10496/50176]	Loss: 0.3897
Training Epoch: 59 [10752/50176]	Loss: 0.5032
Training Epoch: 59 [11008/50176]	Loss: 0.4619
Training Epoch: 59 [11264/50176]	Loss: 0.4146
Training Epoch: 59 [11520/50176]	Loss: 0.4874
Training Epoch: 59 [11776/50176]	Loss: 0.5000
Training Epoch: 59 [12032/50176]	Loss: 0.4739
Training Epoch: 59 [12288/50176]	Loss: 0.3837
Training Epoch: 59 [12544/50176]	Loss: 0.4706
Training Epoch: 59 [12800/50176]	Loss: 0.4063
Training Epoch: 59 [13056/50176]	Loss: 0.5292
Training Epoch: 59 [13312/50176]	Loss: 0.3943
Training Epoch: 59 [13568/50176]	Loss: 0.4767
Training Epoch: 59 [13824/50176]	Loss: 0.5215
Training Epoch: 59 [14080/50176]	Loss: 0.5400
Training Epoch: 59 [14336/50176]	Loss: 0.5142
Training Epoch: 59 [14592/50176]	Loss: 0.4960
Training Epoch: 59 [14848/50176]	Loss: 0.3887
Training Epoch: 59 [15104/50176]	Loss: 0.3633
Training Epoch: 59 [15360/50176]	Loss: 0.4314
Training Epoch: 59 [15616/50176]	Loss: 0.6241
Training Epoch: 59 [15872/50176]	Loss: 0.3582
Training Epoch: 59 [16128/50176]	Loss: 0.4239
Training Epoch: 59 [16384/50176]	Loss: 0.4828
Training Epoch: 59 [16640/50176]	Loss: 0.4620
Training Epoch: 59 [16896/50176]	Loss: 0.5460
Training Epoch: 59 [17152/50176]	Loss: 0.3920
Training Epoch: 59 [17408/50176]	Loss: 0.5248
Training Epoch: 59 [17664/50176]	Loss: 0.4907
Training Epoch: 59 [17920/50176]	Loss: 0.5122
Training Epoch: 59 [18176/50176]	Loss: 0.3119
Training Epoch: 59 [18432/50176]	Loss: 0.4285
Training Epoch: 59 [18688/50176]	Loss: 0.4518
Training Epoch: 59 [18944/50176]	Loss: 0.6032
Training Epoch: 59 [19200/50176]	Loss: 0.4312
Training Epoch: 59 [19456/50176]	Loss: 0.3959
Training Epoch: 59 [19712/50176]	Loss: 0.4144
Training Epoch: 59 [19968/50176]	Loss: 0.4718
Training Epoch: 59 [20224/50176]	Loss: 0.5482
Training Epoch: 59 [20480/50176]	Loss: 0.4889
Training Epoch: 59 [20736/50176]	Loss: 0.3671
Training Epoch: 59 [20992/50176]	Loss: 0.4241
Training Epoch: 59 [21248/50176]	Loss: 0.4731
Training Epoch: 59 [21504/50176]	Loss: 0.5001
Training Epoch: 59 [21760/50176]	Loss: 0.4654
Training Epoch: 59 [22016/50176]	Loss: 0.3626
Training Epoch: 59 [22272/50176]	Loss: 0.4487
Training Epoch: 59 [22528/50176]	Loss: 0.6222
Training Epoch: 59 [22784/50176]	Loss: 0.5242
Training Epoch: 59 [23040/50176]	Loss: 0.3698
Training Epoch: 59 [23296/50176]	Loss: 0.4485
Training Epoch: 59 [23552/50176]	Loss: 0.4159
Training Epoch: 59 [23808/50176]	Loss: 0.5144
Training Epoch: 59 [24064/50176]	Loss: 0.5058
Training Epoch: 59 [24320/50176]	Loss: 0.5461
Training Epoch: 59 [24576/50176]	Loss: 0.4578
Training Epoch: 59 [24832/50176]	Loss: 0.4889
Training Epoch: 59 [25088/50176]	Loss: 0.6216
Training Epoch: 59 [25344/50176]	Loss: 0.4673
Training Epoch: 59 [25600/50176]	Loss: 0.5233
Training Epoch: 59 [25856/50176]	Loss: 0.3795
Training Epoch: 59 [26112/50176]	Loss: 0.4479
Training Epoch: 59 [26368/50176]	Loss: 0.4486
Training Epoch: 59 [26624/50176]	Loss: 0.3964
Training Epoch: 59 [26880/50176]	Loss: 0.4753
Training Epoch: 59 [27136/50176]	Loss: 0.4351
Training Epoch: 59 [27392/50176]	Loss: 0.5272
Training Epoch: 59 [27648/50176]	Loss: 0.4000
Training Epoch: 59 [27904/50176]	Loss: 0.4376
Training Epoch: 59 [28160/50176]	Loss: 0.4182
Training Epoch: 59 [28416/50176]	Loss: 0.4856
Training Epoch: 59 [28672/50176]	Loss: 0.5301
Training Epoch: 59 [28928/50176]	Loss: 0.4953
Training Epoch: 59 [29184/50176]	Loss: 0.4454
Training Epoch: 59 [29440/50176]	Loss: 0.6007
Training Epoch: 59 [29696/50176]	Loss: 0.5155
Training Epoch: 59 [29952/50176]	Loss: 0.5689
Training Epoch: 59 [30208/50176]	Loss: 0.5533
Training Epoch: 59 [30464/50176]	Loss: 0.2903
Training Epoch: 59 [30720/50176]	Loss: 0.4324
Training Epoch: 59 [30976/50176]	Loss: 0.5931
Training Epoch: 59 [31232/50176]	Loss: 0.4022
Training Epoch: 59 [31488/50176]	Loss: 0.5025
Training Epoch: 59 [31744/50176]	Loss: 0.3864
Training Epoch: 59 [32000/50176]	Loss: 0.4504
Training Epoch: 59 [32256/50176]	Loss: 0.4275
Training Epoch: 59 [32512/50176]	Loss: 0.4412
Training Epoch: 59 [32768/50176]	Loss: 0.4194
Training Epoch: 59 [33024/50176]	Loss: 0.4189
Training Epoch: 59 [33280/50176]	Loss: 0.5116
Training Epoch: 59 [33536/50176]	Loss: 0.4200
Training Epoch: 59 [33792/50176]	Loss: 0.3837
Training Epoch: 59 [34048/50176]	Loss: 0.4851
Training Epoch: 59 [34304/50176]	Loss: 0.4164
Training Epoch: 59 [34560/50176]	Loss: 0.5090
Training Epoch: 59 [34816/50176]	Loss: 0.4779
Training Epoch: 59 [35072/50176]	Loss: 0.5133
Training Epoch: 59 [35328/50176]	Loss: 0.4730
Training Epoch: 59 [35584/50176]	Loss: 0.3861
Training Epoch: 59 [35840/50176]	Loss: 0.4543
Training Epoch: 59 [36096/50176]	Loss: 0.4121
Training Epoch: 59 [36352/50176]	Loss: 0.4773
Training Epoch: 59 [36608/50176]	Loss: 0.3696
Training Epoch: 59 [36864/50176]	Loss: 0.3702
Training Epoch: 59 [37120/50176]	Loss: 0.5330
Training Epoch: 59 [37376/50176]	Loss: 0.4779
Training Epoch: 59 [37632/50176]	Loss: 0.5236
Training Epoch: 59 [37888/50176]	Loss: 0.4861
Training Epoch: 59 [38144/50176]	Loss: 0.5362
Training Epoch: 59 [38400/50176]	Loss: 0.5731
Training Epoch: 59 [38656/50176]	Loss: 0.4440
Training Epoch: 59 [38912/50176]	Loss: 0.4386
Training Epoch: 59 [39168/50176]	Loss: 0.4217
Training Epoch: 59 [39424/50176]	Loss: 0.3956
Training Epoch: 59 [39680/50176]	Loss: 0.5272
Training Epoch: 59 [39936/50176]	Loss: 0.5043
Training Epoch: 59 [40192/50176]	Loss: 0.6836
Training Epoch: 59 [40448/50176]	Loss: 0.5068
Training Epoch: 59 [40704/50176]	Loss: 0.5571
Training Epoch: 59 [40960/50176]	Loss: 0.4267
Training Epoch: 59 [41216/50176]	Loss: 0.3715
Training Epoch: 59 [41472/50176]	Loss: 0.4798
Training Epoch: 59 [41728/50176]	Loss: 0.4834
Training Epoch: 59 [41984/50176]	Loss: 0.5403
Training Epoch: 59 [42240/50176]	Loss: 0.5058
Training Epoch: 59 [42496/50176]	Loss: 0.5663
Training Epoch: 59 [42752/50176]	Loss: 0.4974
Training Epoch: 59 [43008/50176]	Loss: 0.6080
Training Epoch: 59 [43264/50176]	Loss: 0.3787
Training Epoch: 59 [43520/50176]	Loss: 0.5191
Training Epoch: 59 [43776/50176]	Loss: 0.4869
Training Epoch: 59 [44032/50176]	Loss: 0.4098
Training Epoch: 59 [44288/50176]	Loss: 0.4591
Training Epoch: 59 [44544/50176]	Loss: 0.4313
Training Epoch: 59 [44800/50176]	Loss: 0.4850
Training Epoch: 59 [45056/50176]	Loss: 0.4132
Training Epoch: 59 [45312/50176]	Loss: 0.6597
Training Epoch: 59 [45568/50176]	Loss: 0.6144
Training Epoch: 59 [45824/50176]	Loss: 0.5090
Training Epoch: 59 [46080/50176]	Loss: 0.4968
Training Epoch: 59 [46336/50176]	Loss: 0.4624
Training Epoch: 59 [46592/50176]	Loss: 0.6607
Training Epoch: 59 [46848/50176]	Loss: 0.5299
Training Epoch: 59 [47104/50176]	Loss: 0.5086
Training Epoch: 59 [47360/50176]	Loss: 0.4631
Training Epoch: 59 [47616/50176]	Loss: 0.5354
Training Epoch: 59 [47872/50176]	Loss: 0.4999
Training Epoch: 59 [48128/50176]	Loss: 0.5207
Training Epoch: 59 [48384/50176]	Loss: 0.5784
Training Epoch: 59 [48640/50176]	Loss: 0.5530
Training Epoch: 59 [48896/50176]	Loss: 0.4154
Training Epoch: 59 [49152/50176]	Loss: 0.4244
Training Epoch: 59 [49408/50176]	Loss: 0.5123
Training Epoch: 59 [49664/50176]	Loss: 0.5072
Training Epoch: 59 [49920/50176]	Loss: 0.4015
Training Epoch: 59 [50176/50176]	Loss: 0.5931
Validation Epoch: 59, Average loss: 0.0090, Accuracy: 0.5787
Training Epoch: 60 [256/50176]	Loss: 0.4628
Training Epoch: 60 [512/50176]	Loss: 0.3887
Training Epoch: 60 [768/50176]	Loss: 0.4699
Training Epoch: 60 [1024/50176]	Loss: 0.3828
Training Epoch: 60 [1280/50176]	Loss: 0.4310
Training Epoch: 60 [1536/50176]	Loss: 0.4403
Training Epoch: 60 [1792/50176]	Loss: 0.4500
Training Epoch: 60 [2048/50176]	Loss: 0.4361
Training Epoch: 60 [2304/50176]	Loss: 0.4993
Training Epoch: 60 [2560/50176]	Loss: 0.4839
Training Epoch: 60 [2816/50176]	Loss: 0.4644
Training Epoch: 60 [3072/50176]	Loss: 0.4610
Training Epoch: 60 [3328/50176]	Loss: 0.4321
Training Epoch: 60 [3584/50176]	Loss: 0.3871
Training Epoch: 60 [3840/50176]	Loss: 0.3862
Training Epoch: 60 [4096/50176]	Loss: 0.3587
Training Epoch: 60 [4352/50176]	Loss: 0.3727
Training Epoch: 60 [4608/50176]	Loss: 0.3447
Training Epoch: 60 [4864/50176]	Loss: 0.4658
Training Epoch: 60 [5120/50176]	Loss: 0.3796
Training Epoch: 60 [5376/50176]	Loss: 0.5027
Training Epoch: 60 [5632/50176]	Loss: 0.4651
Training Epoch: 60 [5888/50176]	Loss: 0.4115
Training Epoch: 60 [6144/50176]	Loss: 0.3860
Training Epoch: 60 [6400/50176]	Loss: 0.4628
Training Epoch: 60 [6656/50176]	Loss: 0.4531
Training Epoch: 60 [6912/50176]	Loss: 0.4284
Training Epoch: 60 [7168/50176]	Loss: 0.4461
Training Epoch: 60 [7424/50176]	Loss: 0.4583
Training Epoch: 60 [7680/50176]	Loss: 0.4127
Training Epoch: 60 [7936/50176]	Loss: 0.4430
Training Epoch: 60 [8192/50176]	Loss: 0.4012
Training Epoch: 60 [8448/50176]	Loss: 0.3862
Training Epoch: 60 [8704/50176]	Loss: 0.3855
Training Epoch: 60 [8960/50176]	Loss: 0.4117
Training Epoch: 60 [9216/50176]	Loss: 0.3830
Training Epoch: 60 [9472/50176]	Loss: 0.4580
Training Epoch: 60 [9728/50176]	Loss: 0.3391
Training Epoch: 60 [9984/50176]	Loss: 0.3663
Training Epoch: 60 [10240/50176]	Loss: 0.4378
Training Epoch: 60 [10496/50176]	Loss: 0.4076
Training Epoch: 60 [10752/50176]	Loss: 0.4387
Training Epoch: 60 [11008/50176]	Loss: 0.3444
Training Epoch: 60 [11264/50176]	Loss: 0.5039
Training Epoch: 60 [11520/50176]	Loss: 0.4000
Training Epoch: 60 [11776/50176]	Loss: 0.2958
Training Epoch: 60 [12032/50176]	Loss: 0.4646
Training Epoch: 60 [12288/50176]	Loss: 0.4162
Training Epoch: 60 [12544/50176]	Loss: 0.4564
Training Epoch: 60 [12800/50176]	Loss: 0.4737
Training Epoch: 60 [13056/50176]	Loss: 0.4534
Training Epoch: 60 [13312/50176]	Loss: 0.3410
Training Epoch: 60 [13568/50176]	Loss: 0.3682
Training Epoch: 60 [13824/50176]	Loss: 0.4994
Training Epoch: 60 [14080/50176]	Loss: 0.3612
Training Epoch: 60 [14336/50176]	Loss: 0.3919
Training Epoch: 60 [14592/50176]	Loss: 0.4015
Training Epoch: 60 [14848/50176]	Loss: 0.3514
Training Epoch: 60 [15104/50176]	Loss: 0.4030
Training Epoch: 60 [15360/50176]	Loss: 0.3607
Training Epoch: 60 [15616/50176]	Loss: 0.4153
Training Epoch: 60 [15872/50176]	Loss: 0.3448
Training Epoch: 60 [16128/50176]	Loss: 0.4883
Training Epoch: 60 [16384/50176]	Loss: 0.5373
Training Epoch: 60 [16640/50176]	Loss: 0.4945
Training Epoch: 60 [16896/50176]	Loss: 0.4647
Training Epoch: 60 [17152/50176]	Loss: 0.4711
Training Epoch: 60 [17408/50176]	Loss: 0.5152
Training Epoch: 60 [17664/50176]	Loss: 0.4469
Training Epoch: 60 [17920/50176]	Loss: 0.4372
Training Epoch: 60 [18176/50176]	Loss: 0.4907
Training Epoch: 60 [18432/50176]	Loss: 0.4434
Training Epoch: 60 [18688/50176]	Loss: 0.4109
Training Epoch: 60 [18944/50176]	Loss: 0.5880
Training Epoch: 60 [19200/50176]	Loss: 0.4052
Training Epoch: 60 [19456/50176]	Loss: 0.4563
Training Epoch: 60 [19712/50176]	Loss: 0.4485
Training Epoch: 60 [19968/50176]	Loss: 0.5005
Training Epoch: 60 [20224/50176]	Loss: 0.5047
Training Epoch: 60 [20480/50176]	Loss: 0.4767
Training Epoch: 60 [20736/50176]	Loss: 0.3615
Training Epoch: 60 [20992/50176]	Loss: 0.4500
Training Epoch: 60 [21248/50176]	Loss: 0.4437
Training Epoch: 60 [21504/50176]	Loss: 0.4975
Training Epoch: 60 [21760/50176]	Loss: 0.4423
Training Epoch: 60 [22016/50176]	Loss: 0.4097
Training Epoch: 60 [22272/50176]	Loss: 0.4293
Training Epoch: 60 [22528/50176]	Loss: 0.4567
Training Epoch: 60 [22784/50176]	Loss: 0.5219
Training Epoch: 60 [23040/50176]	Loss: 0.2739
Training Epoch: 60 [23296/50176]	Loss: 0.5252
Training Epoch: 60 [23552/50176]	Loss: 0.3751
Training Epoch: 60 [23808/50176]	Loss: 0.3990
Training Epoch: 60 [24064/50176]	Loss: 0.5644
Training Epoch: 60 [24320/50176]	Loss: 0.4760
Training Epoch: 60 [24576/50176]	Loss: 0.4623
Training Epoch: 60 [24832/50176]	Loss: 0.4405
Training Epoch: 60 [25088/50176]	Loss: 0.4609
Training Epoch: 60 [25344/50176]	Loss: 0.4926
Training Epoch: 60 [25600/50176]	Loss: 0.4502
Training Epoch: 60 [25856/50176]	Loss: 0.4344
Training Epoch: 60 [26112/50176]	Loss: 0.4660
Training Epoch: 60 [26368/50176]	Loss: 0.4047
Training Epoch: 60 [26624/50176]	Loss: 0.4912
Training Epoch: 60 [26880/50176]	Loss: 0.4774
Training Epoch: 60 [27136/50176]	Loss: 0.4122
Training Epoch: 60 [27392/50176]	Loss: 0.4084
Training Epoch: 60 [27648/50176]	Loss: 0.5255
Training Epoch: 60 [27904/50176]	Loss: 0.5071
Training Epoch: 60 [28160/50176]	Loss: 0.4001
Training Epoch: 60 [28416/50176]	Loss: 0.5417
Training Epoch: 60 [28672/50176]	Loss: 0.5345
Training Epoch: 60 [28928/50176]	Loss: 0.4864
Training Epoch: 60 [29184/50176]	Loss: 0.5088
Training Epoch: 60 [29440/50176]	Loss: 0.4651
Training Epoch: 60 [29696/50176]	Loss: 0.5754
Training Epoch: 60 [29952/50176]	Loss: 0.4928
Training Epoch: 60 [30208/50176]	Loss: 0.5591
Training Epoch: 60 [30464/50176]	Loss: 0.4355
Training Epoch: 60 [30720/50176]	Loss: 0.5497
Training Epoch: 60 [30976/50176]	Loss: 0.4380
Training Epoch: 60 [31232/50176]	Loss: 0.3678
Training Epoch: 60 [31488/50176]	Loss: 0.4625
Training Epoch: 60 [31744/50176]	Loss: 0.4337
Training Epoch: 60 [32000/50176]	Loss: 0.4675
Training Epoch: 60 [32256/50176]	Loss: 0.4736
Training Epoch: 60 [32512/50176]	Loss: 0.4274
Training Epoch: 60 [32768/50176]	Loss: 0.5268
Training Epoch: 60 [33024/50176]	Loss: 0.4312
Training Epoch: 60 [33280/50176]	Loss: 0.5123
Training Epoch: 60 [33536/50176]	Loss: 0.3311
Training Epoch: 60 [33792/50176]	Loss: 0.4791
Training Epoch: 60 [34048/50176]	Loss: 0.4879
Training Epoch: 60 [34304/50176]	Loss: 0.4201
Training Epoch: 60 [34560/50176]	Loss: 0.4477
Training Epoch: 60 [34816/50176]	Loss: 0.5536
Training Epoch: 60 [35072/50176]	Loss: 0.3603
Training Epoch: 60 [35328/50176]	Loss: 0.4692
Training Epoch: 60 [35584/50176]	Loss: 0.4009
Training Epoch: 60 [35840/50176]	Loss: 0.4063
Training Epoch: 60 [36096/50176]	Loss: 0.5244
Training Epoch: 60 [36352/50176]	Loss: 0.4790
Training Epoch: 60 [36608/50176]	Loss: 0.4259
Training Epoch: 60 [36864/50176]	Loss: 0.5149
Training Epoch: 60 [37120/50176]	Loss: 0.4215
Training Epoch: 60 [37376/50176]	Loss: 0.5264
Training Epoch: 60 [37632/50176]	Loss: 0.3934
Training Epoch: 60 [37888/50176]	Loss: 0.5453
Training Epoch: 60 [38144/50176]	Loss: 0.5139
Training Epoch: 60 [38400/50176]	Loss: 0.4817
Training Epoch: 60 [38656/50176]	Loss: 0.5319
Training Epoch: 60 [38912/50176]	Loss: 0.5098
Training Epoch: 60 [39168/50176]	Loss: 0.4287
Training Epoch: 60 [39424/50176]	Loss: 0.5252
Training Epoch: 60 [39680/50176]	Loss: 0.4346
Training Epoch: 60 [39936/50176]	Loss: 0.4262
Training Epoch: 60 [40192/50176]	Loss: 0.5465
Training Epoch: 60 [40448/50176]	Loss: 0.4812
Training Epoch: 60 [40704/50176]	Loss: 0.5195
Training Epoch: 60 [40960/50176]	Loss: 0.5041
Training Epoch: 60 [41216/50176]	Loss: 0.3781
Training Epoch: 60 [41472/50176]	Loss: 0.5943
Training Epoch: 60 [41728/50176]	Loss: 0.3025
Training Epoch: 60 [41984/50176]	Loss: 0.4308
Training Epoch: 60 [42240/50176]	Loss: 0.4271
Training Epoch: 60 [42496/50176]	Loss: 0.4597
Training Epoch: 60 [42752/50176]	Loss: 0.3860
Training Epoch: 60 [43008/50176]	Loss: 0.4839
Training Epoch: 60 [43264/50176]	Loss: 0.4715
Training Epoch: 60 [43520/50176]	Loss: 0.3910
Training Epoch: 60 [43776/50176]	Loss: 0.5010
Training Epoch: 60 [44032/50176]	Loss: 0.4199
Training Epoch: 60 [44288/50176]	Loss: 0.4573
Training Epoch: 60 [44544/50176]	Loss: 0.5264
Training Epoch: 60 [44800/50176]	Loss: 0.4035
Training Epoch: 60 [45056/50176]	Loss: 0.3917
Training Epoch: 60 [45312/50176]	Loss: 0.5896
Training Epoch: 60 [45568/50176]	Loss: 0.5238
Training Epoch: 60 [45824/50176]	Loss: 0.4291
Training Epoch: 60 [46080/50176]	Loss: 0.4705
Training Epoch: 60 [46336/50176]	Loss: 0.5608
Training Epoch: 60 [46592/50176]	Loss: 0.5684
Training Epoch: 60 [46848/50176]	Loss: 0.4914
Training Epoch: 60 [47104/50176]	Loss: 0.4515
Training Epoch: 60 [47360/50176]	Loss: 0.4337
Training Epoch: 60 [47616/50176]	Loss: 0.5089
Training Epoch: 60 [47872/50176]	Loss: 0.4845
Training Epoch: 60 [48128/50176]	Loss: 0.5343
Training Epoch: 60 [48384/50176]	Loss: 0.6213
Training Epoch: 60 [48640/50176]	Loss: 0.4534
Training Epoch: 60 [48896/50176]	Loss: 0.4617
Training Epoch: 60 [49152/50176]	Loss: 0.4471
Training Epoch: 60 [49408/50176]	Loss: 0.5289
Training Epoch: 60 [49664/50176]	Loss: 0.4730
Training Epoch: 60 [49920/50176]	Loss: 0.4509
Training Epoch: 60 [50176/50176]	Loss: 0.5074
Validation Epoch: 60, Average loss: 0.0102, Accuracy: 0.5464
Training Epoch: 61 [256/50176]	Loss: 0.3816
Training Epoch: 61 [512/50176]	Loss: 0.3063
Training Epoch: 61 [768/50176]	Loss: 0.4195
Training Epoch: 61 [1024/50176]	Loss: 0.3878
Training Epoch: 61 [1280/50176]	Loss: 0.4193
Training Epoch: 61 [1536/50176]	Loss: 0.3704
Training Epoch: 61 [1792/50176]	Loss: 0.3911
Training Epoch: 61 [2048/50176]	Loss: 0.4318
Training Epoch: 61 [2304/50176]	Loss: 0.5153
Training Epoch: 61 [2560/50176]	Loss: 0.4381
Training Epoch: 61 [2816/50176]	Loss: 0.3986
Training Epoch: 61 [3072/50176]	Loss: 0.4384
Training Epoch: 61 [3328/50176]	Loss: 0.3594
Training Epoch: 61 [3584/50176]	Loss: 0.4723
Training Epoch: 61 [3840/50176]	Loss: 0.4307
Training Epoch: 61 [4096/50176]	Loss: 0.4532
Training Epoch: 61 [4352/50176]	Loss: 0.4162
Training Epoch: 61 [4608/50176]	Loss: 0.3799
Training Epoch: 61 [4864/50176]	Loss: 0.4763
Training Epoch: 61 [5120/50176]	Loss: 0.4432
Training Epoch: 61 [5376/50176]	Loss: 0.4567
Training Epoch: 61 [5632/50176]	Loss: 0.3972
Training Epoch: 61 [5888/50176]	Loss: 0.3931
Training Epoch: 61 [6144/50176]	Loss: 0.2936
Training Epoch: 61 [6400/50176]	Loss: 0.4902
Training Epoch: 61 [6656/50176]	Loss: 0.4548
Training Epoch: 61 [6912/50176]	Loss: 0.4866
Training Epoch: 61 [7168/50176]	Loss: 0.3570
Training Epoch: 61 [7424/50176]	Loss: 0.4427
Training Epoch: 61 [7680/50176]	Loss: 0.4159
Training Epoch: 61 [7936/50176]	Loss: 0.4478
Training Epoch: 61 [8192/50176]	Loss: 0.3601
Training Epoch: 61 [8448/50176]	Loss: 0.4354
Training Epoch: 61 [8704/50176]	Loss: 0.3863
Training Epoch: 61 [8960/50176]	Loss: 0.3823
Training Epoch: 61 [9216/50176]	Loss: 0.4174
Training Epoch: 61 [9472/50176]	Loss: 0.4729
Training Epoch: 61 [9728/50176]	Loss: 0.5248
Training Epoch: 61 [9984/50176]	Loss: 0.4506
Training Epoch: 61 [10240/50176]	Loss: 0.4033
Training Epoch: 61 [10496/50176]	Loss: 0.3695
Training Epoch: 61 [10752/50176]	Loss: 0.3706
Training Epoch: 61 [11008/50176]	Loss: 0.4626
Training Epoch: 61 [11264/50176]	Loss: 0.3542
Training Epoch: 61 [11520/50176]	Loss: 0.4680
Training Epoch: 61 [11776/50176]	Loss: 0.3279
Training Epoch: 61 [12032/50176]	Loss: 0.4813
Training Epoch: 61 [12288/50176]	Loss: 0.3550
Training Epoch: 61 [12544/50176]	Loss: 0.3866
Training Epoch: 61 [12800/50176]	Loss: 0.4080
Training Epoch: 61 [13056/50176]	Loss: 0.4748
Training Epoch: 61 [13312/50176]	Loss: 0.3873
Training Epoch: 61 [13568/50176]	Loss: 0.3795
Training Epoch: 61 [13824/50176]	Loss: 0.3838
Training Epoch: 61 [14080/50176]	Loss: 0.4523
Training Epoch: 61 [14336/50176]	Loss: 0.3502
Training Epoch: 61 [14592/50176]	Loss: 0.4312
Training Epoch: 61 [14848/50176]	Loss: 0.4195
Training Epoch: 61 [15104/50176]	Loss: 0.3526
Training Epoch: 61 [15360/50176]	Loss: 0.4193
Training Epoch: 61 [15616/50176]	Loss: 0.3291
Training Epoch: 61 [15872/50176]	Loss: 0.4453
Training Epoch: 61 [16128/50176]	Loss: 0.5781
Training Epoch: 61 [16384/50176]	Loss: 0.5009
Training Epoch: 61 [16640/50176]	Loss: 0.4082
Training Epoch: 61 [16896/50176]	Loss: 0.3740
Training Epoch: 61 [17152/50176]	Loss: 0.3834
Training Epoch: 61 [17408/50176]	Loss: 0.5321
Training Epoch: 61 [17664/50176]	Loss: 0.4380
Training Epoch: 61 [17920/50176]	Loss: 0.3695
Training Epoch: 61 [18176/50176]	Loss: 0.3409
Training Epoch: 61 [18432/50176]	Loss: 0.4042
Training Epoch: 61 [18688/50176]	Loss: 0.4226
Training Epoch: 61 [18944/50176]	Loss: 0.3653
Training Epoch: 61 [19200/50176]	Loss: 0.4431
Training Epoch: 61 [19456/50176]	Loss: 0.3828
Training Epoch: 61 [19712/50176]	Loss: 0.4812
Training Epoch: 61 [19968/50176]	Loss: 0.4388
Training Epoch: 61 [20224/50176]	Loss: 0.3950
Training Epoch: 61 [20480/50176]	Loss: 0.4306
Training Epoch: 61 [20736/50176]	Loss: 0.5835
Training Epoch: 61 [20992/50176]	Loss: 0.4655
Training Epoch: 61 [21248/50176]	Loss: 0.5632
Training Epoch: 61 [21504/50176]	Loss: 0.4407
Training Epoch: 61 [21760/50176]	Loss: 0.5260
Training Epoch: 61 [22016/50176]	Loss: 0.4616
Training Epoch: 61 [22272/50176]	Loss: 0.3966
Training Epoch: 61 [22528/50176]	Loss: 0.4883
Training Epoch: 61 [22784/50176]	Loss: 0.4796
Training Epoch: 61 [23040/50176]	Loss: 0.3899
Training Epoch: 61 [23296/50176]	Loss: 0.3833
Training Epoch: 61 [23552/50176]	Loss: 0.4281
Training Epoch: 61 [23808/50176]	Loss: 0.4809
Training Epoch: 61 [24064/50176]	Loss: 0.4295
Training Epoch: 61 [24320/50176]	Loss: 0.4379
Training Epoch: 61 [24576/50176]	Loss: 0.3757
Training Epoch: 61 [24832/50176]	Loss: 0.3119
Training Epoch: 61 [25088/50176]	Loss: 0.5136
Training Epoch: 61 [25344/50176]	Loss: 0.3871
Training Epoch: 61 [25600/50176]	Loss: 0.3936
Training Epoch: 61 [25856/50176]	Loss: 0.3961
Training Epoch: 61 [26112/50176]	Loss: 0.4517
Training Epoch: 61 [26368/50176]	Loss: 0.4447
Training Epoch: 61 [26624/50176]	Loss: 0.4467
Training Epoch: 61 [26880/50176]	Loss: 0.3709
Training Epoch: 61 [27136/50176]	Loss: 0.3222
Training Epoch: 61 [27392/50176]	Loss: 0.4847
Training Epoch: 61 [27648/50176]	Loss: 0.4200
Training Epoch: 61 [27904/50176]	Loss: 0.4834
Training Epoch: 61 [28160/50176]	Loss: 0.5152
Training Epoch: 61 [28416/50176]	Loss: 0.4253
Training Epoch: 61 [28672/50176]	Loss: 0.3405
Training Epoch: 61 [28928/50176]	Loss: 0.3615
Training Epoch: 61 [29184/50176]	Loss: 0.5290
Training Epoch: 61 [29440/50176]	Loss: 0.5867
Training Epoch: 61 [29696/50176]	Loss: 0.5347
Training Epoch: 61 [29952/50176]	Loss: 0.4691
Training Epoch: 61 [30208/50176]	Loss: 0.4333
Training Epoch: 61 [30464/50176]	Loss: 0.3981
Training Epoch: 61 [30720/50176]	Loss: 0.4177
Training Epoch: 61 [30976/50176]	Loss: 0.5379
Training Epoch: 61 [31232/50176]	Loss: 0.4463
Training Epoch: 61 [31488/50176]	Loss: 0.4202
Training Epoch: 61 [31744/50176]	Loss: 0.3318
Training Epoch: 61 [32000/50176]	Loss: 0.4538
Training Epoch: 61 [32256/50176]	Loss: 0.4250
Training Epoch: 61 [32512/50176]	Loss: 0.4980
Training Epoch: 61 [32768/50176]	Loss: 0.4387
Training Epoch: 61 [33024/50176]	Loss: 0.5301
Training Epoch: 61 [33280/50176]	Loss: 0.5403
Training Epoch: 61 [33536/50176]	Loss: 0.5412
Training Epoch: 61 [33792/50176]	Loss: 0.3914
Training Epoch: 61 [34048/50176]	Loss: 0.3925
Training Epoch: 61 [34304/50176]	Loss: 0.4887
Training Epoch: 61 [34560/50176]	Loss: 0.5491
Training Epoch: 61 [34816/50176]	Loss: 0.4549
Training Epoch: 61 [35072/50176]	Loss: 0.5014
Training Epoch: 61 [35328/50176]	Loss: 0.4390
Training Epoch: 61 [35584/50176]	Loss: 0.4620
Training Epoch: 61 [35840/50176]	Loss: 0.5335
Training Epoch: 61 [36096/50176]	Loss: 0.4386
Training Epoch: 61 [36352/50176]	Loss: 0.4215
Training Epoch: 61 [36608/50176]	Loss: 0.4708
Training Epoch: 61 [36864/50176]	Loss: 0.3536
Training Epoch: 61 [37120/50176]	Loss: 0.5397
Training Epoch: 61 [37376/50176]	Loss: 0.4648
Training Epoch: 61 [37632/50176]	Loss: 0.5138
Training Epoch: 61 [37888/50176]	Loss: 0.4828
Training Epoch: 61 [38144/50176]	Loss: 0.3510
Training Epoch: 61 [38400/50176]	Loss: 0.4353
Training Epoch: 61 [38656/50176]	Loss: 0.4626
Training Epoch: 61 [38912/50176]	Loss: 0.4950
Training Epoch: 61 [39168/50176]	Loss: 0.5063
Training Epoch: 61 [39424/50176]	Loss: 0.4501
Training Epoch: 61 [39680/50176]	Loss: 0.5770
Training Epoch: 61 [39936/50176]	Loss: 0.4662
Training Epoch: 61 [40192/50176]	Loss: 0.5282
Training Epoch: 61 [40448/50176]	Loss: 0.5107
Training Epoch: 61 [40704/50176]	Loss: 0.4366
Training Epoch: 61 [40960/50176]	Loss: 0.4165
Training Epoch: 61 [41216/50176]	Loss: 0.4684
Training Epoch: 61 [41472/50176]	Loss: 0.4637
Training Epoch: 61 [41728/50176]	Loss: 0.4933
Training Epoch: 61 [41984/50176]	Loss: 0.4609
Training Epoch: 61 [42240/50176]	Loss: 0.3907
Training Epoch: 61 [42496/50176]	Loss: 0.5259
Training Epoch: 61 [42752/50176]	Loss: 0.5024
Training Epoch: 61 [43008/50176]	Loss: 0.3973
Training Epoch: 61 [43264/50176]	Loss: 0.3882
Training Epoch: 61 [43520/50176]	Loss: 0.4376
Training Epoch: 61 [43776/50176]	Loss: 0.5263
Training Epoch: 61 [44032/50176]	Loss: 0.4389
Training Epoch: 61 [44288/50176]	Loss: 0.5706
Training Epoch: 61 [44544/50176]	Loss: 0.4752
Training Epoch: 61 [44800/50176]	Loss: 0.4186
Training Epoch: 61 [45056/50176]	Loss: 0.5747
Training Epoch: 61 [45312/50176]	Loss: 0.4630
Training Epoch: 61 [45568/50176]	Loss: 0.6353
Training Epoch: 61 [45824/50176]	Loss: 0.4453
Training Epoch: 61 [46080/50176]	Loss: 0.4932
Training Epoch: 61 [46336/50176]	Loss: 0.4129
Training Epoch: 61 [46592/50176]	Loss: 0.4602
Training Epoch: 61 [46848/50176]	Loss: 0.5182
Training Epoch: 61 [47104/50176]	Loss: 0.3853
Training Epoch: 61 [47360/50176]	Loss: 0.4427
Training Epoch: 61 [47616/50176]	Loss: 0.4908
Training Epoch: 61 [47872/50176]	Loss: 0.4985
Training Epoch: 61 [48128/50176]	Loss: 0.4566
Training Epoch: 61 [48384/50176]	Loss: 0.6085
Training Epoch: 61 [48640/50176]	Loss: 0.4401
Training Epoch: 61 [48896/50176]	Loss: 0.4548
Training Epoch: 61 [49152/50176]	Loss: 0.3677
Training Epoch: 61 [49408/50176]	Loss: 0.4908
Training Epoch: 61 [49664/50176]	Loss: 0.5156
Training Epoch: 61 [49920/50176]	Loss: 0.4310
Training Epoch: 61 [50176/50176]	Loss: 0.6957
Validation Epoch: 61, Average loss: 0.0094, Accuracy: 0.5598
Training Epoch: 62 [256/50176]	Loss: 0.4117
Training Epoch: 62 [512/50176]	Loss: 0.3728
Training Epoch: 62 [768/50176]	Loss: 0.2810
Training Epoch: 62 [1024/50176]	Loss: 0.4187
Training Epoch: 62 [1280/50176]	Loss: 0.4887
Training Epoch: 62 [1536/50176]	Loss: 0.4409
Training Epoch: 62 [1792/50176]	Loss: 0.3846
Training Epoch: 62 [2048/50176]	Loss: 0.3915
Training Epoch: 62 [2304/50176]	Loss: 0.3020
Training Epoch: 62 [2560/50176]	Loss: 0.4428
Training Epoch: 62 [2816/50176]	Loss: 0.3879
Training Epoch: 62 [3072/50176]	Loss: 0.3656
Training Epoch: 62 [3328/50176]	Loss: 0.4071
Training Epoch: 62 [3584/50176]	Loss: 0.4173
Training Epoch: 62 [3840/50176]	Loss: 0.4544
Training Epoch: 62 [4096/50176]	Loss: 0.3514
Training Epoch: 62 [4352/50176]	Loss: 0.3350
Training Epoch: 62 [4608/50176]	Loss: 0.4547
Training Epoch: 62 [4864/50176]	Loss: 0.4076
Training Epoch: 62 [5120/50176]	Loss: 0.4834
Training Epoch: 62 [5376/50176]	Loss: 0.3295
Training Epoch: 62 [5632/50176]	Loss: 0.4458
Training Epoch: 62 [5888/50176]	Loss: 0.3272
Training Epoch: 62 [6144/50176]	Loss: 0.3904
Training Epoch: 62 [6400/50176]	Loss: 0.4109
Training Epoch: 62 [6656/50176]	Loss: 0.4235
Training Epoch: 62 [6912/50176]	Loss: 0.3810
Training Epoch: 62 [7168/50176]	Loss: 0.3785
Training Epoch: 62 [7424/50176]	Loss: 0.4843
Training Epoch: 62 [7680/50176]	Loss: 0.6011
Training Epoch: 62 [7936/50176]	Loss: 0.3805
Training Epoch: 62 [8192/50176]	Loss: 0.4365
Training Epoch: 62 [8448/50176]	Loss: 0.4197
Training Epoch: 62 [8704/50176]	Loss: 0.4579
Training Epoch: 62 [8960/50176]	Loss: 0.3451
Training Epoch: 62 [9216/50176]	Loss: 0.3964
Training Epoch: 62 [9472/50176]	Loss: 0.3700
Training Epoch: 62 [9728/50176]	Loss: 0.4815
Training Epoch: 62 [9984/50176]	Loss: 0.3622
Training Epoch: 62 [10240/50176]	Loss: 0.4667
Training Epoch: 62 [10496/50176]	Loss: 0.2729
Training Epoch: 62 [10752/50176]	Loss: 0.4392
Training Epoch: 62 [11008/50176]	Loss: 0.4344
Training Epoch: 62 [11264/50176]	Loss: 0.4020
Training Epoch: 62 [11520/50176]	Loss: 0.3981
Training Epoch: 62 [11776/50176]	Loss: 0.4306
Training Epoch: 62 [12032/50176]	Loss: 0.3777
Training Epoch: 62 [12288/50176]	Loss: 0.3588
Training Epoch: 62 [12544/50176]	Loss: 0.3469
Training Epoch: 62 [12800/50176]	Loss: 0.3875
Training Epoch: 62 [13056/50176]	Loss: 0.3981
Training Epoch: 62 [13312/50176]	Loss: 0.3651
Training Epoch: 62 [13568/50176]	Loss: 0.4398
Training Epoch: 62 [13824/50176]	Loss: 0.4232
Training Epoch: 62 [14080/50176]	Loss: 0.5319
Training Epoch: 62 [14336/50176]	Loss: 0.3656
Training Epoch: 62 [14592/50176]	Loss: 0.4099
Training Epoch: 62 [14848/50176]	Loss: 0.4129
Training Epoch: 62 [15104/50176]	Loss: 0.3863
Training Epoch: 62 [15360/50176]	Loss: 0.4981
Training Epoch: 62 [15616/50176]	Loss: 0.4708
Training Epoch: 62 [15872/50176]	Loss: 0.4827
Training Epoch: 62 [16128/50176]	Loss: 0.3379
Training Epoch: 62 [16384/50176]	Loss: 0.3693
Training Epoch: 62 [16640/50176]	Loss: 0.4414
Training Epoch: 62 [16896/50176]	Loss: 0.5457
Training Epoch: 62 [17152/50176]	Loss: 0.5723
Training Epoch: 62 [17408/50176]	Loss: 0.4677
Training Epoch: 62 [17664/50176]	Loss: 0.4502
Training Epoch: 62 [17920/50176]	Loss: 0.4425
Training Epoch: 62 [18176/50176]	Loss: 0.3418
Training Epoch: 62 [18432/50176]	Loss: 0.3358
Training Epoch: 62 [18688/50176]	Loss: 0.3425
Training Epoch: 62 [18944/50176]	Loss: 0.4954
Training Epoch: 62 [19200/50176]	Loss: 0.4805
Training Epoch: 62 [19456/50176]	Loss: 0.4189
Training Epoch: 62 [19712/50176]	Loss: 0.4925
Training Epoch: 62 [19968/50176]	Loss: 0.4695
Training Epoch: 62 [20224/50176]	Loss: 0.4183
Training Epoch: 62 [20480/50176]	Loss: 0.3983
Training Epoch: 62 [20736/50176]	Loss: 0.3946
Training Epoch: 62 [20992/50176]	Loss: 0.4516
Training Epoch: 62 [21248/50176]	Loss: 0.4676
Training Epoch: 62 [21504/50176]	Loss: 0.3228
Training Epoch: 62 [21760/50176]	Loss: 0.3518
Training Epoch: 62 [22016/50176]	Loss: 0.4622
Training Epoch: 62 [22272/50176]	Loss: 0.4601
Training Epoch: 62 [22528/50176]	Loss: 0.3217
Training Epoch: 62 [22784/50176]	Loss: 0.3929
Training Epoch: 62 [23040/50176]	Loss: 0.4402
Training Epoch: 62 [23296/50176]	Loss: 0.6435
Training Epoch: 62 [23552/50176]	Loss: 0.4058
Training Epoch: 62 [23808/50176]	Loss: 0.5660
Training Epoch: 62 [24064/50176]	Loss: 0.3530
Training Epoch: 62 [24320/50176]	Loss: 0.4532
Training Epoch: 62 [24576/50176]	Loss: 0.4490
Training Epoch: 62 [24832/50176]	Loss: 0.5541
Training Epoch: 62 [25088/50176]	Loss: 0.4796
Training Epoch: 62 [25344/50176]	Loss: 0.4742
Training Epoch: 62 [25600/50176]	Loss: 0.4322
Training Epoch: 62 [25856/50176]	Loss: 0.4335
Training Epoch: 62 [26112/50176]	Loss: 0.3922
Training Epoch: 62 [26368/50176]	Loss: 0.4191
Training Epoch: 62 [26624/50176]	Loss: 0.3989
Training Epoch: 62 [26880/50176]	Loss: 0.5353
Training Epoch: 62 [27136/50176]	Loss: 0.4932
Training Epoch: 62 [27392/50176]	Loss: 0.3498
Training Epoch: 62 [27648/50176]	Loss: 0.3794
Training Epoch: 62 [27904/50176]	Loss: 0.4433
Training Epoch: 62 [28160/50176]	Loss: 0.4884
Training Epoch: 62 [28416/50176]	Loss: 0.3625
Training Epoch: 62 [28672/50176]	Loss: 0.4765
Training Epoch: 62 [28928/50176]	Loss: 0.4005
Training Epoch: 62 [29184/50176]	Loss: 0.4313
Training Epoch: 62 [29440/50176]	Loss: 0.4878
Training Epoch: 62 [29696/50176]	Loss: 0.5321
Training Epoch: 62 [29952/50176]	Loss: 0.4171
Training Epoch: 62 [30208/50176]	Loss: 0.4796
Training Epoch: 62 [30464/50176]	Loss: 0.5763
Training Epoch: 62 [30720/50176]	Loss: 0.5207
Training Epoch: 62 [30976/50176]	Loss: 0.4226
Training Epoch: 62 [31232/50176]	Loss: 0.4421
Training Epoch: 62 [31488/50176]	Loss: 0.4947
Training Epoch: 62 [31744/50176]	Loss: 0.4614
Training Epoch: 62 [32000/50176]	Loss: 0.4323
Training Epoch: 62 [32256/50176]	Loss: 0.4377
Training Epoch: 62 [32512/50176]	Loss: 0.5503
Training Epoch: 62 [32768/50176]	Loss: 0.5151
Training Epoch: 62 [33024/50176]	Loss: 0.4091
Training Epoch: 62 [33280/50176]	Loss: 0.4001
Training Epoch: 62 [33536/50176]	Loss: 0.4474
Training Epoch: 62 [33792/50176]	Loss: 0.4139
Training Epoch: 62 [34048/50176]	Loss: 0.4927
Training Epoch: 62 [34304/50176]	Loss: 0.3903
Training Epoch: 62 [34560/50176]	Loss: 0.3990
Training Epoch: 62 [34816/50176]	Loss: 0.4258
Training Epoch: 62 [35072/50176]	Loss: 0.5482
Training Epoch: 62 [35328/50176]	Loss: 0.3697
Training Epoch: 62 [35584/50176]	Loss: 0.4548
Training Epoch: 62 [35840/50176]	Loss: 0.4565
Training Epoch: 62 [36096/50176]	Loss: 0.4142
Training Epoch: 62 [36352/50176]	Loss: 0.3730
Training Epoch: 62 [36608/50176]	Loss: 0.4662
Training Epoch: 62 [36864/50176]	Loss: 0.4637
Training Epoch: 62 [37120/50176]	Loss: 0.5325
Training Epoch: 62 [37376/50176]	Loss: 0.4987
Training Epoch: 62 [37632/50176]	Loss: 0.4926
Training Epoch: 62 [37888/50176]	Loss: 0.4965
Training Epoch: 62 [38144/50176]	Loss: 0.5591
Training Epoch: 62 [38400/50176]	Loss: 0.5105
Training Epoch: 62 [38656/50176]	Loss: 0.4351
Training Epoch: 62 [38912/50176]	Loss: 0.3416
Training Epoch: 62 [39168/50176]	Loss: 0.4643
Training Epoch: 62 [39424/50176]	Loss: 0.5337
Training Epoch: 62 [39680/50176]	Loss: 0.4792
Training Epoch: 62 [39936/50176]	Loss: 0.5001
Training Epoch: 62 [40192/50176]	Loss: 0.4971
Training Epoch: 62 [40448/50176]	Loss: 0.4358
Training Epoch: 62 [40704/50176]	Loss: 0.3916
Training Epoch: 62 [40960/50176]	Loss: 0.4351
Training Epoch: 62 [41216/50176]	Loss: 0.4691
Training Epoch: 62 [41472/50176]	Loss: 0.4495
Training Epoch: 62 [41728/50176]	Loss: 0.4727
Training Epoch: 62 [41984/50176]	Loss: 0.5128
Training Epoch: 62 [42240/50176]	Loss: 0.4824
Training Epoch: 62 [42496/50176]	Loss: 0.4998
Training Epoch: 62 [42752/50176]	Loss: 0.4490
Training Epoch: 62 [43008/50176]	Loss: 0.3887
Training Epoch: 62 [43264/50176]	Loss: 0.5083
Training Epoch: 62 [43520/50176]	Loss: 0.3835
Training Epoch: 62 [43776/50176]	Loss: 0.4519
Training Epoch: 62 [44032/50176]	Loss: 0.5427
Training Epoch: 62 [44288/50176]	Loss: 0.4233
Training Epoch: 62 [44544/50176]	Loss: 0.4844
Training Epoch: 62 [44800/50176]	Loss: 0.5819
Training Epoch: 62 [45056/50176]	Loss: 0.3809
Training Epoch: 62 [45312/50176]	Loss: 0.4063
Training Epoch: 62 [45568/50176]	Loss: 0.4109
Training Epoch: 62 [45824/50176]	Loss: 0.5257
Training Epoch: 62 [46080/50176]	Loss: 0.4576
Training Epoch: 62 [46336/50176]	Loss: 0.3514
Training Epoch: 62 [46592/50176]	Loss: 0.5800
Training Epoch: 62 [46848/50176]	Loss: 0.6988
Training Epoch: 62 [47104/50176]	Loss: 0.4486
Training Epoch: 62 [47360/50176]	Loss: 0.4362
Training Epoch: 62 [47616/50176]	Loss: 0.6413
Training Epoch: 62 [47872/50176]	Loss: 0.5263
Training Epoch: 62 [48128/50176]	Loss: 0.4789
Training Epoch: 62 [48384/50176]	Loss: 0.6514
Training Epoch: 62 [48640/50176]	Loss: 0.4051
Training Epoch: 62 [48896/50176]	Loss: 0.5181
Training Epoch: 62 [49152/50176]	Loss: 0.4262
Training Epoch: 62 [49408/50176]	Loss: 0.5089
Training Epoch: 62 [49664/50176]	Loss: 0.4619
Training Epoch: 62 [49920/50176]	Loss: 0.5179
Training Epoch: 62 [50176/50176]	Loss: 0.6817
Validation Epoch: 62, Average loss: 0.0108, Accuracy: 0.5327
Training Epoch: 63 [256/50176]	Loss: 0.4260
Training Epoch: 63 [512/50176]	Loss: 0.4233
Training Epoch: 63 [768/50176]	Loss: 0.4374
Training Epoch: 63 [1024/50176]	Loss: 0.4290
Training Epoch: 63 [1280/50176]	Loss: 0.4218
Training Epoch: 63 [1536/50176]	Loss: 0.4480
Training Epoch: 63 [1792/50176]	Loss: 0.4738
Training Epoch: 63 [2048/50176]	Loss: 0.3398
Training Epoch: 63 [2304/50176]	Loss: 0.3601
Training Epoch: 63 [2560/50176]	Loss: 0.2868
Training Epoch: 63 [2816/50176]	Loss: 0.3750
Training Epoch: 63 [3072/50176]	Loss: 0.3822
Training Epoch: 63 [3328/50176]	Loss: 0.4137
Training Epoch: 63 [3584/50176]	Loss: 0.4066
Training Epoch: 63 [3840/50176]	Loss: 0.5121
Training Epoch: 63 [4096/50176]	Loss: 0.4390
Training Epoch: 63 [4352/50176]	Loss: 0.3934
Training Epoch: 63 [4608/50176]	Loss: 0.4033
Training Epoch: 63 [4864/50176]	Loss: 0.4561
Training Epoch: 63 [5120/50176]	Loss: 0.3591
Training Epoch: 63 [5376/50176]	Loss: 0.5010
Training Epoch: 63 [5632/50176]	Loss: 0.4918
Training Epoch: 63 [5888/50176]	Loss: 0.4652
Training Epoch: 63 [6144/50176]	Loss: 0.4260
Training Epoch: 63 [6400/50176]	Loss: 0.3313
Training Epoch: 63 [6656/50176]	Loss: 0.4580
Training Epoch: 63 [6912/50176]	Loss: 0.4966
Training Epoch: 63 [7168/50176]	Loss: 0.3886
Training Epoch: 63 [7424/50176]	Loss: 0.3515
Training Epoch: 63 [7680/50176]	Loss: 0.4336
Training Epoch: 63 [7936/50176]	Loss: 0.4191
Training Epoch: 63 [8192/50176]	Loss: 0.4486
Training Epoch: 63 [8448/50176]	Loss: 0.3344
Training Epoch: 63 [8704/50176]	Loss: 0.3857
Training Epoch: 63 [8960/50176]	Loss: 0.3665
Training Epoch: 63 [9216/50176]	Loss: 0.4620
Training Epoch: 63 [9472/50176]	Loss: 0.4755
Training Epoch: 63 [9728/50176]	Loss: 0.3193
Training Epoch: 63 [9984/50176]	Loss: 0.4523
Training Epoch: 63 [10240/50176]	Loss: 0.3942
Training Epoch: 63 [10496/50176]	Loss: 0.3235
Training Epoch: 63 [10752/50176]	Loss: 0.4152
Training Epoch: 63 [11008/50176]	Loss: 0.4698
Training Epoch: 63 [11264/50176]	Loss: 0.4251
Training Epoch: 63 [11520/50176]	Loss: 0.5288
Training Epoch: 63 [11776/50176]	Loss: 0.3599
Training Epoch: 63 [12032/50176]	Loss: 0.3778
Training Epoch: 63 [12288/50176]	Loss: 0.4170
Training Epoch: 63 [12544/50176]	Loss: 0.3809
Training Epoch: 63 [12800/50176]	Loss: 0.3277
Training Epoch: 63 [13056/50176]	Loss: 0.3837
Training Epoch: 63 [13312/50176]	Loss: 0.3944
Training Epoch: 63 [13568/50176]	Loss: 0.3637
Training Epoch: 63 [13824/50176]	Loss: 0.4370
Training Epoch: 63 [14080/50176]	Loss: 0.4533
Training Epoch: 63 [14336/50176]	Loss: 0.4906
Training Epoch: 63 [14592/50176]	Loss: 0.4872
Training Epoch: 63 [14848/50176]	Loss: 0.3744
Training Epoch: 63 [15104/50176]	Loss: 0.4137
Training Epoch: 63 [15360/50176]	Loss: 0.4640
Training Epoch: 63 [15616/50176]	Loss: 0.3966
Training Epoch: 63 [15872/50176]	Loss: 0.4472
Training Epoch: 63 [16128/50176]	Loss: 0.4101
Training Epoch: 63 [16384/50176]	Loss: 0.4365
Training Epoch: 63 [16640/50176]	Loss: 0.4274
Training Epoch: 63 [16896/50176]	Loss: 0.5118
Training Epoch: 63 [17152/50176]	Loss: 0.3818
Training Epoch: 63 [17408/50176]	Loss: 0.3600
Training Epoch: 63 [17664/50176]	Loss: 0.4235
Training Epoch: 63 [17920/50176]	Loss: 0.4582
Training Epoch: 63 [18176/50176]	Loss: 0.4621
Training Epoch: 63 [18432/50176]	Loss: 0.5140
Training Epoch: 63 [18688/50176]	Loss: 0.5407
Training Epoch: 63 [18944/50176]	Loss: 0.3725
Training Epoch: 63 [19200/50176]	Loss: 0.3339
Training Epoch: 63 [19456/50176]	Loss: 0.4029
Training Epoch: 63 [19712/50176]	Loss: 0.5044
Training Epoch: 63 [19968/50176]	Loss: 0.4935
Training Epoch: 63 [20224/50176]	Loss: 0.4448
Training Epoch: 63 [20480/50176]	Loss: 0.3983
Training Epoch: 63 [20736/50176]	Loss: 0.3362
Training Epoch: 63 [20992/50176]	Loss: 0.4676
Training Epoch: 63 [21248/50176]	Loss: 0.4178
Training Epoch: 63 [21504/50176]	Loss: 0.3901
Training Epoch: 63 [21760/50176]	Loss: 0.4068
Training Epoch: 63 [22016/50176]	Loss: 0.5750
Training Epoch: 63 [22272/50176]	Loss: 0.4900
Training Epoch: 63 [22528/50176]	Loss: 0.3793
Training Epoch: 63 [22784/50176]	Loss: 0.4875
Training Epoch: 63 [23040/50176]	Loss: 0.4119
Training Epoch: 63 [23296/50176]	Loss: 0.4097
Training Epoch: 63 [23552/50176]	Loss: 0.3770
Training Epoch: 63 [23808/50176]	Loss: 0.4823
Training Epoch: 63 [24064/50176]	Loss: 0.3843
Training Epoch: 63 [24320/50176]	Loss: 0.5519
Training Epoch: 63 [24576/50176]	Loss: 0.4599
Training Epoch: 63 [24832/50176]	Loss: 0.5048
Training Epoch: 63 [25088/50176]	Loss: 0.3733
Training Epoch: 63 [25344/50176]	Loss: 0.4318
Training Epoch: 63 [25600/50176]	Loss: 0.4508
Training Epoch: 63 [25856/50176]	Loss: 0.3757
Training Epoch: 63 [26112/50176]	Loss: 0.3980
Training Epoch: 63 [26368/50176]	Loss: 0.3918
Training Epoch: 63 [26624/50176]	Loss: 0.4679
Training Epoch: 63 [26880/50176]	Loss: 0.3645
Training Epoch: 63 [27136/50176]	Loss: 0.5394
Training Epoch: 63 [27392/50176]	Loss: 0.5235
Training Epoch: 63 [27648/50176]	Loss: 0.5039
Training Epoch: 63 [27904/50176]	Loss: 0.4857
Training Epoch: 63 [28160/50176]	Loss: 0.4350
Training Epoch: 63 [28416/50176]	Loss: 0.4438
Training Epoch: 63 [28672/50176]	Loss: 0.4323
Training Epoch: 63 [28928/50176]	Loss: 0.3426
Training Epoch: 63 [29184/50176]	Loss: 0.3017
Training Epoch: 63 [29440/50176]	Loss: 0.4061
Training Epoch: 63 [29696/50176]	Loss: 0.4386
Training Epoch: 63 [29952/50176]	Loss: 0.4742
Training Epoch: 63 [30208/50176]	Loss: 0.4200
Training Epoch: 63 [30464/50176]	Loss: 0.4165
Training Epoch: 63 [30720/50176]	Loss: 0.4082
Training Epoch: 63 [30976/50176]	Loss: 0.3917
Training Epoch: 63 [31232/50176]	Loss: 0.4424
Training Epoch: 63 [31488/50176]	Loss: 0.3523
Training Epoch: 63 [31744/50176]	Loss: 0.3872
Training Epoch: 63 [32000/50176]	Loss: 0.4725
Training Epoch: 63 [32256/50176]	Loss: 0.4468
Training Epoch: 63 [32512/50176]	Loss: 0.4159
Training Epoch: 63 [32768/50176]	Loss: 0.3669
Training Epoch: 63 [33024/50176]	Loss: 0.4469
Training Epoch: 63 [33280/50176]	Loss: 0.3693
Training Epoch: 63 [33536/50176]	Loss: 0.3639
Training Epoch: 63 [33792/50176]	Loss: 0.4752
Training Epoch: 63 [34048/50176]	Loss: 0.2994
Training Epoch: 63 [34304/50176]	Loss: 0.4052
Training Epoch: 63 [34560/50176]	Loss: 0.5641
Training Epoch: 63 [34816/50176]	Loss: 0.4456
Training Epoch: 63 [35072/50176]	Loss: 0.3639
Training Epoch: 63 [35328/50176]	Loss: 0.5209
Training Epoch: 63 [35584/50176]	Loss: 0.4394
Training Epoch: 63 [35840/50176]	Loss: 0.4035
Training Epoch: 63 [36096/50176]	Loss: 0.5882
Training Epoch: 63 [36352/50176]	Loss: 0.4223
Training Epoch: 63 [36608/50176]	Loss: 0.4004
Training Epoch: 63 [36864/50176]	Loss: 0.4811
Training Epoch: 63 [37120/50176]	Loss: 0.4220
Training Epoch: 63 [37376/50176]	Loss: 0.4458
Training Epoch: 63 [37632/50176]	Loss: 0.5249
Training Epoch: 63 [37888/50176]	Loss: 0.4336
Training Epoch: 63 [38144/50176]	Loss: 0.4414
Training Epoch: 63 [38400/50176]	Loss: 0.4368
Training Epoch: 63 [38656/50176]	Loss: 0.3876
Training Epoch: 63 [38912/50176]	Loss: 0.5118
Training Epoch: 63 [39168/50176]	Loss: 0.4712
Training Epoch: 63 [39424/50176]	Loss: 0.4910
Training Epoch: 63 [39680/50176]	Loss: 0.4891
Training Epoch: 63 [39936/50176]	Loss: 0.3929
Training Epoch: 63 [40192/50176]	Loss: 0.5343
Training Epoch: 63 [40448/50176]	Loss: 0.4040
Training Epoch: 63 [40704/50176]	Loss: 0.4692
Training Epoch: 63 [40960/50176]	Loss: 0.5220
Training Epoch: 63 [41216/50176]	Loss: 0.3510
Training Epoch: 63 [41472/50176]	Loss: 0.5185
Training Epoch: 63 [41728/50176]	Loss: 0.4173
Training Epoch: 63 [41984/50176]	Loss: 0.4971
Training Epoch: 63 [42240/50176]	Loss: 0.4833
Training Epoch: 63 [42496/50176]	Loss: 0.4926
Training Epoch: 63 [42752/50176]	Loss: 0.5437
Training Epoch: 63 [43008/50176]	Loss: 0.4361
Training Epoch: 63 [43264/50176]	Loss: 0.4614
Training Epoch: 63 [43520/50176]	Loss: 0.4947
Training Epoch: 63 [43776/50176]	Loss: 0.4947
Training Epoch: 63 [44032/50176]	Loss: 0.5553
Training Epoch: 63 [44288/50176]	Loss: 0.3831
Training Epoch: 63 [44544/50176]	Loss: 0.5523
Training Epoch: 63 [44800/50176]	Loss: 0.4045
Training Epoch: 63 [45056/50176]	Loss: 0.4727
Training Epoch: 63 [45312/50176]	Loss: 0.6201
Training Epoch: 63 [45568/50176]	Loss: 0.5707
Training Epoch: 63 [45824/50176]	Loss: 0.4403
Training Epoch: 63 [46080/50176]	Loss: 0.5423
Training Epoch: 63 [46336/50176]	Loss: 0.4311
Training Epoch: 63 [46592/50176]	Loss: 0.4765
Training Epoch: 63 [46848/50176]	Loss: 0.4649
Training Epoch: 63 [47104/50176]	Loss: 0.4336
Training Epoch: 63 [47360/50176]	Loss: 0.4990
Training Epoch: 63 [47616/50176]	Loss: 0.5037
Training Epoch: 63 [47872/50176]	Loss: 0.3700
Training Epoch: 63 [48128/50176]	Loss: 0.5050
Training Epoch: 63 [48384/50176]	Loss: 0.4868
Training Epoch: 63 [48640/50176]	Loss: 0.4997
Training Epoch: 63 [48896/50176]	Loss: 0.4868
Training Epoch: 63 [49152/50176]	Loss: 0.6140
Training Epoch: 63 [49408/50176]	Loss: 0.4599
Training Epoch: 63 [49664/50176]	Loss: 0.5429
Training Epoch: 63 [49920/50176]	Loss: 0.4841
Training Epoch: 63 [50176/50176]	Loss: 0.2853
Validation Epoch: 63, Average loss: 0.0110, Accuracy: 0.5378
Training Epoch: 64 [256/50176]	Loss: 0.2972
Training Epoch: 64 [512/50176]	Loss: 0.3579
Training Epoch: 64 [768/50176]	Loss: 0.4065
Training Epoch: 64 [1024/50176]	Loss: 0.3544
Training Epoch: 64 [1280/50176]	Loss: 0.4129
Training Epoch: 64 [1536/50176]	Loss: 0.4089
Training Epoch: 64 [1792/50176]	Loss: 0.3901
Training Epoch: 64 [2048/50176]	Loss: 0.3694
Training Epoch: 64 [2304/50176]	Loss: 0.4468
Training Epoch: 64 [2560/50176]	Loss: 0.3659
Training Epoch: 64 [2816/50176]	Loss: 0.4547
Training Epoch: 64 [3072/50176]	Loss: 0.4057
Training Epoch: 64 [3328/50176]	Loss: 0.4115
Training Epoch: 64 [3584/50176]	Loss: 0.3455
Training Epoch: 64 [3840/50176]	Loss: 0.3411
Training Epoch: 64 [4096/50176]	Loss: 0.4002
Training Epoch: 64 [4352/50176]	Loss: 0.4587
Training Epoch: 64 [4608/50176]	Loss: 0.3872
Training Epoch: 64 [4864/50176]	Loss: 0.3323
Training Epoch: 64 [5120/50176]	Loss: 0.3559
Training Epoch: 64 [5376/50176]	Loss: 0.4202
Training Epoch: 64 [5632/50176]	Loss: 0.3620
Training Epoch: 64 [5888/50176]	Loss: 0.4102
Training Epoch: 64 [6144/50176]	Loss: 0.3966
Training Epoch: 64 [6400/50176]	Loss: 0.3604
Training Epoch: 64 [6656/50176]	Loss: 0.4838
Training Epoch: 64 [6912/50176]	Loss: 0.4613
Training Epoch: 64 [7168/50176]	Loss: 0.3408
Training Epoch: 64 [7424/50176]	Loss: 0.3933
Training Epoch: 64 [7680/50176]	Loss: 0.3281
Training Epoch: 64 [7936/50176]	Loss: 0.4407
Training Epoch: 64 [8192/50176]	Loss: 0.3777
Training Epoch: 64 [8448/50176]	Loss: 0.3805
Training Epoch: 64 [8704/50176]	Loss: 0.3616
Training Epoch: 64 [8960/50176]	Loss: 0.4475
Training Epoch: 64 [9216/50176]	Loss: 0.3912
Training Epoch: 64 [9472/50176]	Loss: 0.3964
Training Epoch: 64 [9728/50176]	Loss: 0.3744
Training Epoch: 64 [9984/50176]	Loss: 0.4386
Training Epoch: 64 [10240/50176]	Loss: 0.3262
Training Epoch: 64 [10496/50176]	Loss: 0.3632
Training Epoch: 64 [10752/50176]	Loss: 0.4125
Training Epoch: 64 [11008/50176]	Loss: 0.3588
Training Epoch: 64 [11264/50176]	Loss: 0.3378
Training Epoch: 64 [11520/50176]	Loss: 0.3971
Training Epoch: 64 [11776/50176]	Loss: 0.3626
Training Epoch: 64 [12032/50176]	Loss: 0.4571
Training Epoch: 64 [12288/50176]	Loss: 0.4456
Training Epoch: 64 [12544/50176]	Loss: 0.4585
Training Epoch: 64 [12800/50176]	Loss: 0.3983
Training Epoch: 64 [13056/50176]	Loss: 0.3754
Training Epoch: 64 [13312/50176]	Loss: 0.4920
Training Epoch: 64 [13568/50176]	Loss: 0.3658
Training Epoch: 64 [13824/50176]	Loss: 0.3950
Training Epoch: 64 [14080/50176]	Loss: 0.2582
Training Epoch: 64 [14336/50176]	Loss: 0.3487
Training Epoch: 64 [14592/50176]	Loss: 0.3576
Training Epoch: 64 [14848/50176]	Loss: 0.4046
Training Epoch: 64 [15104/50176]	Loss: 0.4246
Training Epoch: 64 [15360/50176]	Loss: 0.3334
Training Epoch: 64 [15616/50176]	Loss: 0.3893
Training Epoch: 64 [15872/50176]	Loss: 0.4802
Training Epoch: 64 [16128/50176]	Loss: 0.4657
Training Epoch: 64 [16384/50176]	Loss: 0.3469
Training Epoch: 64 [16640/50176]	Loss: 0.4832
Training Epoch: 64 [16896/50176]	Loss: 0.4431
Training Epoch: 64 [17152/50176]	Loss: 0.4003
Training Epoch: 64 [17408/50176]	Loss: 0.4957
Training Epoch: 64 [17664/50176]	Loss: 0.4339
Training Epoch: 64 [17920/50176]	Loss: 0.4709
Training Epoch: 64 [18176/50176]	Loss: 0.3193
Training Epoch: 64 [18432/50176]	Loss: 0.3135
Training Epoch: 64 [18688/50176]	Loss: 0.4297
Training Epoch: 64 [18944/50176]	Loss: 0.3564
Training Epoch: 64 [19200/50176]	Loss: 0.3796
Training Epoch: 64 [19456/50176]	Loss: 0.3764
Training Epoch: 64 [19712/50176]	Loss: 0.3842
Training Epoch: 64 [19968/50176]	Loss: 0.4833
Training Epoch: 64 [20224/50176]	Loss: 0.5278
Training Epoch: 64 [20480/50176]	Loss: 0.4883
Training Epoch: 64 [20736/50176]	Loss: 0.3725
Training Epoch: 64 [20992/50176]	Loss: 0.3061
Training Epoch: 64 [21248/50176]	Loss: 0.4417
Training Epoch: 64 [21504/50176]	Loss: 0.4367
Training Epoch: 64 [21760/50176]	Loss: 0.3762
Training Epoch: 64 [22016/50176]	Loss: 0.3586
Training Epoch: 64 [22272/50176]	Loss: 0.3527
Training Epoch: 64 [22528/50176]	Loss: 0.3571
Training Epoch: 64 [22784/50176]	Loss: 0.4708
Training Epoch: 64 [23040/50176]	Loss: 0.3774
Training Epoch: 64 [23296/50176]	Loss: 0.3550
Training Epoch: 64 [23552/50176]	Loss: 0.5320
Training Epoch: 64 [23808/50176]	Loss: 0.4471
Training Epoch: 64 [24064/50176]	Loss: 0.3682
Training Epoch: 64 [24320/50176]	Loss: 0.4211
Training Epoch: 64 [24576/50176]	Loss: 0.3742
Training Epoch: 64 [24832/50176]	Loss: 0.4981
Training Epoch: 64 [25088/50176]	Loss: 0.4895
Training Epoch: 64 [25344/50176]	Loss: 0.5094
Training Epoch: 64 [25600/50176]	Loss: 0.4088
Training Epoch: 64 [25856/50176]	Loss: 0.4145
Training Epoch: 64 [26112/50176]	Loss: 0.4962
Training Epoch: 64 [26368/50176]	Loss: 0.4423
Training Epoch: 64 [26624/50176]	Loss: 0.4551
Training Epoch: 64 [26880/50176]	Loss: 0.4195
Training Epoch: 64 [27136/50176]	Loss: 0.3736
Training Epoch: 64 [27392/50176]	Loss: 0.4094
Training Epoch: 64 [27648/50176]	Loss: 0.5658
Training Epoch: 64 [27904/50176]	Loss: 0.5159
Training Epoch: 64 [28160/50176]	Loss: 0.4604
Training Epoch: 64 [28416/50176]	Loss: 0.4200
Training Epoch: 64 [28672/50176]	Loss: 0.4755
Training Epoch: 64 [28928/50176]	Loss: 0.4960
Training Epoch: 64 [29184/50176]	Loss: 0.5519
Training Epoch: 64 [29440/50176]	Loss: 0.3897
Training Epoch: 64 [29696/50176]	Loss: 0.4569
Training Epoch: 64 [29952/50176]	Loss: 0.4435
Training Epoch: 64 [30208/50176]	Loss: 0.4156
Training Epoch: 64 [30464/50176]	Loss: 0.4267
Training Epoch: 64 [30720/50176]	Loss: 0.5396
Training Epoch: 64 [30976/50176]	Loss: 0.4132
Training Epoch: 64 [31232/50176]	Loss: 0.3638
Training Epoch: 64 [31488/50176]	Loss: 0.3857
Training Epoch: 64 [31744/50176]	Loss: 0.3759
Training Epoch: 64 [32000/50176]	Loss: 0.5461
Training Epoch: 64 [32256/50176]	Loss: 0.3927
Training Epoch: 64 [32512/50176]	Loss: 0.3671
Training Epoch: 64 [32768/50176]	Loss: 0.3958
Training Epoch: 64 [33024/50176]	Loss: 0.4126
Training Epoch: 64 [33280/50176]	Loss: 0.4487
Training Epoch: 64 [33536/50176]	Loss: 0.4460
Training Epoch: 64 [33792/50176]	Loss: 0.4187
Training Epoch: 64 [34048/50176]	Loss: 0.4221
Training Epoch: 64 [34304/50176]	Loss: 0.4489
Training Epoch: 64 [34560/50176]	Loss: 0.5139
Training Epoch: 64 [34816/50176]	Loss: 0.4887
Training Epoch: 64 [35072/50176]	Loss: 0.5158
Training Epoch: 64 [35328/50176]	Loss: 0.4816
Training Epoch: 64 [35584/50176]	Loss: 0.4406
Training Epoch: 64 [35840/50176]	Loss: 0.3678
Training Epoch: 64 [36096/50176]	Loss: 0.4704
Training Epoch: 64 [36352/50176]	Loss: 0.3638
Training Epoch: 64 [36608/50176]	Loss: 0.4608
Training Epoch: 64 [36864/50176]	Loss: 0.4852
Training Epoch: 64 [37120/50176]	Loss: 0.3640
Training Epoch: 64 [37376/50176]	Loss: 0.5787
Training Epoch: 64 [37632/50176]	Loss: 0.4802
Training Epoch: 64 [37888/50176]	Loss: 0.3801
Training Epoch: 64 [38144/50176]	Loss: 0.5046
Training Epoch: 64 [38400/50176]	Loss: 0.5465
Training Epoch: 64 [38656/50176]	Loss: 0.3848
Training Epoch: 64 [38912/50176]	Loss: 0.4047
Training Epoch: 64 [39168/50176]	Loss: 0.4439
Training Epoch: 64 [39424/50176]	Loss: 0.4509
Training Epoch: 64 [39680/50176]	Loss: 0.4888
Training Epoch: 64 [39936/50176]	Loss: 0.3773
Training Epoch: 64 [40192/50176]	Loss: 0.5617
Training Epoch: 64 [40448/50176]	Loss: 0.4676
Training Epoch: 64 [40704/50176]	Loss: 0.4467
Training Epoch: 64 [40960/50176]	Loss: 0.3564
Training Epoch: 64 [41216/50176]	Loss: 0.4819
Training Epoch: 64 [41472/50176]	Loss: 0.5378
Training Epoch: 64 [41728/50176]	Loss: 0.4930
Training Epoch: 64 [41984/50176]	Loss: 0.4177
Training Epoch: 64 [42240/50176]	Loss: 0.6709
Training Epoch: 64 [42496/50176]	Loss: 0.4427
Training Epoch: 64 [42752/50176]	Loss: 0.4050
Training Epoch: 64 [43008/50176]	Loss: 0.4867
Training Epoch: 64 [43264/50176]	Loss: 0.4547
Training Epoch: 64 [43520/50176]	Loss: 0.5200
Training Epoch: 64 [43776/50176]	Loss: 0.4868
Training Epoch: 64 [44032/50176]	Loss: 0.4618
Training Epoch: 64 [44288/50176]	Loss: 0.4307
Training Epoch: 64 [44544/50176]	Loss: 0.5498
Training Epoch: 64 [44800/50176]	Loss: 0.4906
Training Epoch: 64 [45056/50176]	Loss: 0.5625
Training Epoch: 64 [45312/50176]	Loss: 0.5144
Training Epoch: 64 [45568/50176]	Loss: 0.5139
Training Epoch: 64 [45824/50176]	Loss: 0.4328
Training Epoch: 64 [46080/50176]	Loss: 0.4098
Training Epoch: 64 [46336/50176]	Loss: 0.4110
Training Epoch: 64 [46592/50176]	Loss: 0.4783
Training Epoch: 64 [46848/50176]	Loss: 0.4366
Training Epoch: 64 [47104/50176]	Loss: 0.5108
Training Epoch: 64 [47360/50176]	Loss: 0.4362
Training Epoch: 64 [47616/50176]	Loss: 0.3807
Training Epoch: 64 [47872/50176]	Loss: 0.4967
Training Epoch: 64 [48128/50176]	Loss: 0.4554
Training Epoch: 64 [48384/50176]	Loss: 0.6038
Training Epoch: 64 [48640/50176]	Loss: 0.3897
Training Epoch: 64 [48896/50176]	Loss: 0.4385
Training Epoch: 64 [49152/50176]	Loss: 0.4287
Training Epoch: 64 [49408/50176]	Loss: 0.5528
Training Epoch: 64 [49664/50176]	Loss: 0.4714
Training Epoch: 64 [49920/50176]	Loss: 0.3911
Training Epoch: 64 [50176/50176]	Loss: 0.6735
Validation Epoch: 64, Average loss: 0.0105, Accuracy: 0.5423
Training Epoch: 65 [256/50176]	Loss: 0.3495
Training Epoch: 65 [512/50176]	Loss: 0.2877
Training Epoch: 65 [768/50176]	Loss: 0.4186
Training Epoch: 65 [1024/50176]	Loss: 0.3592
Training Epoch: 65 [1280/50176]	Loss: 0.3419
Training Epoch: 65 [1536/50176]	Loss: 0.2910
Training Epoch: 65 [1792/50176]	Loss: 0.3874
Training Epoch: 65 [2048/50176]	Loss: 0.3710
Training Epoch: 65 [2304/50176]	Loss: 0.2891
Training Epoch: 65 [2560/50176]	Loss: 0.3918
Training Epoch: 65 [2816/50176]	Loss: 0.5161
Training Epoch: 65 [3072/50176]	Loss: 0.4388
Training Epoch: 65 [3328/50176]	Loss: 0.4370
Training Epoch: 65 [3584/50176]	Loss: 0.3516
Training Epoch: 65 [3840/50176]	Loss: 0.3829
Training Epoch: 65 [4096/50176]	Loss: 0.3810
Training Epoch: 65 [4352/50176]	Loss: 0.3031
Training Epoch: 65 [4608/50176]	Loss: 0.3439
Training Epoch: 65 [4864/50176]	Loss: 0.4043
Training Epoch: 65 [5120/50176]	Loss: 0.4358
Training Epoch: 65 [5376/50176]	Loss: 0.3754
Training Epoch: 65 [5632/50176]	Loss: 0.4289
Training Epoch: 65 [5888/50176]	Loss: 0.3843
Training Epoch: 65 [6144/50176]	Loss: 0.3937
Training Epoch: 65 [6400/50176]	Loss: 0.3110
Training Epoch: 65 [6656/50176]	Loss: 0.4283
Training Epoch: 65 [6912/50176]	Loss: 0.4540
Training Epoch: 65 [7168/50176]	Loss: 0.5278
Training Epoch: 65 [7424/50176]	Loss: 0.4578
Training Epoch: 65 [7680/50176]	Loss: 0.5295
Training Epoch: 65 [7936/50176]	Loss: 0.3739
Training Epoch: 65 [8192/50176]	Loss: 0.4925
Training Epoch: 65 [8448/50176]	Loss: 0.4167
Training Epoch: 65 [8704/50176]	Loss: 0.4075
Training Epoch: 65 [8960/50176]	Loss: 0.3490
Training Epoch: 65 [9216/50176]	Loss: 0.3764
Training Epoch: 65 [9472/50176]	Loss: 0.4272
Training Epoch: 65 [9728/50176]	Loss: 0.4391
Training Epoch: 65 [9984/50176]	Loss: 0.4270
Training Epoch: 65 [10240/50176]	Loss: 0.5764
Training Epoch: 65 [10496/50176]	Loss: 0.4049
Training Epoch: 65 [10752/50176]	Loss: 0.4436
Training Epoch: 65 [11008/50176]	Loss: 0.4128
Training Epoch: 65 [11264/50176]	Loss: 0.3425
Training Epoch: 65 [11520/50176]	Loss: 0.3974
Training Epoch: 65 [11776/50176]	Loss: 0.3544
Training Epoch: 65 [12032/50176]	Loss: 0.3556
Training Epoch: 65 [12288/50176]	Loss: 0.3932
Training Epoch: 65 [12544/50176]	Loss: 0.5291
Training Epoch: 65 [12800/50176]	Loss: 0.3726
Training Epoch: 65 [13056/50176]	Loss: 0.3559
Training Epoch: 65 [13312/50176]	Loss: 0.3236
Training Epoch: 65 [13568/50176]	Loss: 0.4600
Training Epoch: 65 [13824/50176]	Loss: 0.5603
Training Epoch: 65 [14080/50176]	Loss: 0.4531
Training Epoch: 65 [14336/50176]	Loss: 0.3467
Training Epoch: 65 [14592/50176]	Loss: 0.5022
Training Epoch: 65 [14848/50176]	Loss: 0.3675
Training Epoch: 65 [15104/50176]	Loss: 0.4329
Training Epoch: 65 [15360/50176]	Loss: 0.3947
Training Epoch: 65 [15616/50176]	Loss: 0.3443
Training Epoch: 65 [15872/50176]	Loss: 0.4877
Training Epoch: 65 [16128/50176]	Loss: 0.4445
Training Epoch: 65 [16384/50176]	Loss: 0.3522
Training Epoch: 65 [16640/50176]	Loss: 0.4869
Training Epoch: 65 [16896/50176]	Loss: 0.3162
Training Epoch: 65 [17152/50176]	Loss: 0.3579
Training Epoch: 65 [17408/50176]	Loss: 0.4262
Training Epoch: 65 [17664/50176]	Loss: 0.3540
Training Epoch: 65 [17920/50176]	Loss: 0.3396
Training Epoch: 65 [18176/50176]	Loss: 0.3500
Training Epoch: 65 [18432/50176]	Loss: 0.5168
Training Epoch: 65 [18688/50176]	Loss: 0.4439
Training Epoch: 65 [18944/50176]	Loss: 0.3826
Training Epoch: 65 [19200/50176]	Loss: 0.4051
Training Epoch: 65 [19456/50176]	Loss: 0.3274
Training Epoch: 65 [19712/50176]	Loss: 0.3922
Training Epoch: 65 [19968/50176]	Loss: 0.4583
Training Epoch: 65 [20224/50176]	Loss: 0.4100
Training Epoch: 65 [20480/50176]	Loss: 0.4432
Training Epoch: 65 [20736/50176]	Loss: 0.3292
Training Epoch: 65 [20992/50176]	Loss: 0.4286
Training Epoch: 65 [21248/50176]	Loss: 0.3016
Training Epoch: 65 [21504/50176]	Loss: 0.4329
Training Epoch: 65 [21760/50176]	Loss: 0.3698
Training Epoch: 65 [22016/50176]	Loss: 0.3923
Training Epoch: 65 [22272/50176]	Loss: 0.3980
Training Epoch: 65 [22528/50176]	Loss: 0.3959
Training Epoch: 65 [22784/50176]	Loss: 0.3722
Training Epoch: 65 [23040/50176]	Loss: 0.4228
Training Epoch: 65 [23296/50176]	Loss: 0.4366
Training Epoch: 65 [23552/50176]	Loss: 0.4224
Training Epoch: 65 [23808/50176]	Loss: 0.3580
Training Epoch: 65 [24064/50176]	Loss: 0.3970
Training Epoch: 65 [24320/50176]	Loss: 0.4572
Training Epoch: 65 [24576/50176]	Loss: 0.3920
Training Epoch: 65 [24832/50176]	Loss: 0.3450
Training Epoch: 65 [25088/50176]	Loss: 0.3237
Training Epoch: 65 [25344/50176]	Loss: 0.4527
Training Epoch: 65 [25600/50176]	Loss: 0.2934
Training Epoch: 65 [25856/50176]	Loss: 0.4132
Training Epoch: 65 [26112/50176]	Loss: 0.3335
Training Epoch: 65 [26368/50176]	Loss: 0.3522
Training Epoch: 65 [26624/50176]	Loss: 0.3342
Training Epoch: 65 [26880/50176]	Loss: 0.4154
Training Epoch: 65 [27136/50176]	Loss: 0.5331
Training Epoch: 65 [27392/50176]	Loss: 0.3785
Training Epoch: 65 [27648/50176]	Loss: 0.3932
Training Epoch: 65 [27904/50176]	Loss: 0.3713
Training Epoch: 65 [28160/50176]	Loss: 0.3830
Training Epoch: 65 [28416/50176]	Loss: 0.3991
Training Epoch: 65 [28672/50176]	Loss: 0.5226
Training Epoch: 65 [28928/50176]	Loss: 0.5006
Training Epoch: 65 [29184/50176]	Loss: 0.4408
Training Epoch: 65 [29440/50176]	Loss: 0.4405
Training Epoch: 65 [29696/50176]	Loss: 0.4425
Training Epoch: 65 [29952/50176]	Loss: 0.3748
Training Epoch: 65 [30208/50176]	Loss: 0.4093
Training Epoch: 65 [30464/50176]	Loss: 0.3347
Training Epoch: 65 [30720/50176]	Loss: 0.4605
Training Epoch: 65 [30976/50176]	Loss: 0.3810
Training Epoch: 65 [31232/50176]	Loss: 0.4283
Training Epoch: 65 [31488/50176]	Loss: 0.3241
Training Epoch: 65 [31744/50176]	Loss: 0.2765
Training Epoch: 65 [32000/50176]	Loss: 0.4611
Training Epoch: 65 [32256/50176]	Loss: 0.3526
Training Epoch: 65 [32512/50176]	Loss: 0.4305
Training Epoch: 65 [32768/50176]	Loss: 0.3650
Training Epoch: 65 [33024/50176]	Loss: 0.3337
Training Epoch: 65 [33280/50176]	Loss: 0.5657
Training Epoch: 65 [33536/50176]	Loss: 0.4941
Training Epoch: 65 [33792/50176]	Loss: 0.4454
Training Epoch: 65 [34048/50176]	Loss: 0.3629
Training Epoch: 65 [34304/50176]	Loss: 0.3822
Training Epoch: 65 [34560/50176]	Loss: 0.4068
Training Epoch: 65 [34816/50176]	Loss: 0.5019
Training Epoch: 65 [35072/50176]	Loss: 0.4157
Training Epoch: 65 [35328/50176]	Loss: 0.3522
Training Epoch: 65 [35584/50176]	Loss: 0.4122
Training Epoch: 65 [35840/50176]	Loss: 0.3849
Training Epoch: 65 [36096/50176]	Loss: 0.5149
Training Epoch: 65 [36352/50176]	Loss: 0.3437
Training Epoch: 65 [36608/50176]	Loss: 0.4389
Training Epoch: 65 [36864/50176]	Loss: 0.3914
Training Epoch: 65 [37120/50176]	Loss: 0.5226
Training Epoch: 65 [37376/50176]	Loss: 0.4794
Training Epoch: 65 [37632/50176]	Loss: 0.3311
Training Epoch: 65 [37888/50176]	Loss: 0.4500
Training Epoch: 65 [38144/50176]	Loss: 0.4103
Training Epoch: 65 [38400/50176]	Loss: 0.4178
Training Epoch: 65 [38656/50176]	Loss: 0.3449
Training Epoch: 65 [38912/50176]	Loss: 0.5457
Training Epoch: 65 [39168/50176]	Loss: 0.4839
Training Epoch: 65 [39424/50176]	Loss: 0.4824
Training Epoch: 65 [39680/50176]	Loss: 0.4420
Training Epoch: 65 [39936/50176]	Loss: 0.3912
Training Epoch: 65 [40192/50176]	Loss: 0.3787
Training Epoch: 65 [40448/50176]	Loss: 0.4587
Training Epoch: 65 [40704/50176]	Loss: 0.4582
Training Epoch: 65 [40960/50176]	Loss: 0.5806
Training Epoch: 65 [41216/50176]	Loss: 0.4127
Training Epoch: 65 [41472/50176]	Loss: 0.4741
Training Epoch: 65 [41728/50176]	Loss: 0.4716
Training Epoch: 65 [41984/50176]	Loss: 0.3249
Training Epoch: 65 [42240/50176]	Loss: 0.4320
Training Epoch: 65 [42496/50176]	Loss: 0.3779
Training Epoch: 65 [42752/50176]	Loss: 0.4460
Training Epoch: 65 [43008/50176]	Loss: 0.4754
Training Epoch: 65 [43264/50176]	Loss: 0.4278
Training Epoch: 65 [43520/50176]	Loss: 0.4474
Training Epoch: 65 [43776/50176]	Loss: 0.4054
Training Epoch: 65 [44032/50176]	Loss: 0.4054
Training Epoch: 65 [44288/50176]	Loss: 0.3694
Training Epoch: 65 [44544/50176]	Loss: 0.4715
Training Epoch: 65 [44800/50176]	Loss: 0.5344
Training Epoch: 65 [45056/50176]	Loss: 0.4652
Training Epoch: 65 [45312/50176]	Loss: 0.4431
Training Epoch: 65 [45568/50176]	Loss: 0.5342
Training Epoch: 65 [45824/50176]	Loss: 0.4000
Training Epoch: 65 [46080/50176]	Loss: 0.5663
Training Epoch: 65 [46336/50176]	Loss: 0.4483
Training Epoch: 65 [46592/50176]	Loss: 0.4377
Training Epoch: 65 [46848/50176]	Loss: 0.3861
Training Epoch: 65 [47104/50176]	Loss: 0.4325
Training Epoch: 65 [47360/50176]	Loss: 0.4613
Training Epoch: 65 [47616/50176]	Loss: 0.4162
Training Epoch: 65 [47872/50176]	Loss: 0.4547
Training Epoch: 65 [48128/50176]	Loss: 0.4291
Training Epoch: 65 [48384/50176]	Loss: 0.4975
Training Epoch: 65 [48640/50176]	Loss: 0.5215
Training Epoch: 65 [48896/50176]	Loss: 0.4330
Training Epoch: 65 [49152/50176]	Loss: 0.5303
Training Epoch: 65 [49408/50176]	Loss: 0.4446
Training Epoch: 65 [49664/50176]	Loss: 0.3487
Training Epoch: 65 [49920/50176]	Loss: 0.4594
Training Epoch: 65 [50176/50176]	Loss: 0.6143
Validation Epoch: 65, Average loss: 0.0111, Accuracy: 0.5274
Training Epoch: 66 [256/50176]	Loss: 0.2787
Training Epoch: 66 [512/50176]	Loss: 0.3478
Training Epoch: 66 [768/50176]	Loss: 0.3730
Training Epoch: 66 [1024/50176]	Loss: 0.3744
Training Epoch: 66 [1280/50176]	Loss: 0.3981
Training Epoch: 66 [1536/50176]	Loss: 0.4291
Training Epoch: 66 [1792/50176]	Loss: 0.3991
Training Epoch: 66 [2048/50176]	Loss: 0.4082
Training Epoch: 66 [2304/50176]	Loss: 0.3463
Training Epoch: 66 [2560/50176]	Loss: 0.4170
Training Epoch: 66 [2816/50176]	Loss: 0.3232
Training Epoch: 66 [3072/50176]	Loss: 0.4043
Training Epoch: 66 [3328/50176]	Loss: 0.3489
Training Epoch: 66 [3584/50176]	Loss: 0.3294
Training Epoch: 66 [3840/50176]	Loss: 0.4644
Training Epoch: 66 [4096/50176]	Loss: 0.5163
Training Epoch: 66 [4352/50176]	Loss: 0.4372
Training Epoch: 66 [4608/50176]	Loss: 0.4381
Training Epoch: 66 [4864/50176]	Loss: 0.3864
Training Epoch: 66 [5120/50176]	Loss: 0.3346
Training Epoch: 66 [5376/50176]	Loss: 0.3290
Training Epoch: 66 [5632/50176]	Loss: 0.3998
Training Epoch: 66 [5888/50176]	Loss: 0.4276
Training Epoch: 66 [6144/50176]	Loss: 0.2567
Training Epoch: 66 [6400/50176]	Loss: 0.5311
Training Epoch: 66 [6656/50176]	Loss: 0.3141
Training Epoch: 66 [6912/50176]	Loss: 0.3101
Training Epoch: 66 [7168/50176]	Loss: 0.3799
Training Epoch: 66 [7424/50176]	Loss: 0.2617
Training Epoch: 66 [7680/50176]	Loss: 0.3805
Training Epoch: 66 [7936/50176]	Loss: 0.3487
Training Epoch: 66 [8192/50176]	Loss: 0.3598
Training Epoch: 66 [8448/50176]	Loss: 0.4320
Training Epoch: 66 [8704/50176]	Loss: 0.3050
Training Epoch: 66 [8960/50176]	Loss: 0.4381
Training Epoch: 66 [9216/50176]	Loss: 0.4059
Training Epoch: 66 [9472/50176]	Loss: 0.3462
Training Epoch: 66 [9728/50176]	Loss: 0.3846
Training Epoch: 66 [9984/50176]	Loss: 0.3534
Training Epoch: 66 [10240/50176]	Loss: 0.4040
Training Epoch: 66 [10496/50176]	Loss: 0.4745
Training Epoch: 66 [10752/50176]	Loss: 0.3435
Training Epoch: 66 [11008/50176]	Loss: 0.3992
Training Epoch: 66 [11264/50176]	Loss: 0.4586
Training Epoch: 66 [11520/50176]	Loss: 0.3667
Training Epoch: 66 [11776/50176]	Loss: 0.3935
Training Epoch: 66 [12032/50176]	Loss: 0.3962
Training Epoch: 66 [12288/50176]	Loss: 0.4239
Training Epoch: 66 [12544/50176]	Loss: 0.3284
Training Epoch: 66 [12800/50176]	Loss: 0.3696
Training Epoch: 66 [13056/50176]	Loss: 0.3537
Training Epoch: 66 [13312/50176]	Loss: 0.3665
Training Epoch: 66 [13568/50176]	Loss: 0.3108
Training Epoch: 66 [13824/50176]	Loss: 0.3618
Training Epoch: 66 [14080/50176]	Loss: 0.3763
Training Epoch: 66 [14336/50176]	Loss: 0.3430
Training Epoch: 66 [14592/50176]	Loss: 0.4331
Training Epoch: 66 [14848/50176]	Loss: 0.4492
Training Epoch: 66 [15104/50176]	Loss: 0.3057
Training Epoch: 66 [15360/50176]	Loss: 0.3365
Training Epoch: 66 [15616/50176]	Loss: 0.4204
Training Epoch: 66 [15872/50176]	Loss: 0.2759
Training Epoch: 66 [16128/50176]	Loss: 0.3522
Training Epoch: 66 [16384/50176]	Loss: 0.3585
Training Epoch: 66 [16640/50176]	Loss: 0.3703
Training Epoch: 66 [16896/50176]	Loss: 0.3400
Training Epoch: 66 [17152/50176]	Loss: 0.3783
Training Epoch: 66 [17408/50176]	Loss: 0.3703
Training Epoch: 66 [17664/50176]	Loss: 0.4346
Training Epoch: 66 [17920/50176]	Loss: 0.4112
Training Epoch: 66 [18176/50176]	Loss: 0.4356
Training Epoch: 66 [18432/50176]	Loss: 0.3669
Training Epoch: 66 [18688/50176]	Loss: 0.3239
Training Epoch: 66 [18944/50176]	Loss: 0.3294
Training Epoch: 66 [19200/50176]	Loss: 0.4813
Training Epoch: 66 [19456/50176]	Loss: 0.3340
Training Epoch: 66 [19712/50176]	Loss: 0.5435
Training Epoch: 66 [19968/50176]	Loss: 0.3872
Training Epoch: 66 [20224/50176]	Loss: 0.3984
Training Epoch: 66 [20480/50176]	Loss: 0.4182
Training Epoch: 66 [20736/50176]	Loss: 0.3956
Training Epoch: 66 [20992/50176]	Loss: 0.3577
Training Epoch: 66 [21248/50176]	Loss: 0.3519
Training Epoch: 66 [21504/50176]	Loss: 0.3018
Training Epoch: 66 [21760/50176]	Loss: 0.4388
Training Epoch: 66 [22016/50176]	Loss: 0.4164
Training Epoch: 66 [22272/50176]	Loss: 0.4086
Training Epoch: 66 [22528/50176]	Loss: 0.5250
Training Epoch: 66 [22784/50176]	Loss: 0.3536
Training Epoch: 66 [23040/50176]	Loss: 0.5443
Training Epoch: 66 [23296/50176]	Loss: 0.3428
Training Epoch: 66 [23552/50176]	Loss: 0.3583
Training Epoch: 66 [23808/50176]	Loss: 0.3806
Training Epoch: 66 [24064/50176]	Loss: 0.4400
Training Epoch: 66 [24320/50176]	Loss: 0.3935
Training Epoch: 66 [24576/50176]	Loss: 0.4714
Training Epoch: 66 [24832/50176]	Loss: 0.3607
Training Epoch: 66 [25088/50176]	Loss: 0.4523
Training Epoch: 66 [25344/50176]	Loss: 0.3879
Training Epoch: 66 [25600/50176]	Loss: 0.3979
Training Epoch: 66 [25856/50176]	Loss: 0.4109
Training Epoch: 66 [26112/50176]	Loss: 0.3530
Training Epoch: 66 [26368/50176]	Loss: 0.3652
Training Epoch: 66 [26624/50176]	Loss: 0.4172
Training Epoch: 66 [26880/50176]	Loss: 0.3519
Training Epoch: 66 [27136/50176]	Loss: 0.4925
Training Epoch: 66 [27392/50176]	Loss: 0.3958
Training Epoch: 66 [27648/50176]	Loss: 0.5035
Training Epoch: 66 [27904/50176]	Loss: 0.4978
Training Epoch: 66 [28160/50176]	Loss: 0.4014
Training Epoch: 66 [28416/50176]	Loss: 0.4253
Training Epoch: 66 [28672/50176]	Loss: 0.4447
Training Epoch: 66 [28928/50176]	Loss: 0.4525
Training Epoch: 66 [29184/50176]	Loss: 0.3590
Training Epoch: 66 [29440/50176]	Loss: 0.4834
Training Epoch: 66 [29696/50176]	Loss: 0.4689
Training Epoch: 66 [29952/50176]	Loss: 0.3168
Training Epoch: 66 [30208/50176]	Loss: 0.4429
Training Epoch: 66 [30464/50176]	Loss: 0.4236
Training Epoch: 66 [30720/50176]	Loss: 0.4015
Training Epoch: 66 [30976/50176]	Loss: 0.3669
Training Epoch: 66 [31232/50176]	Loss: 0.3877
Training Epoch: 66 [31488/50176]	Loss: 0.4574
Training Epoch: 66 [31744/50176]	Loss: 0.3685
Training Epoch: 66 [32000/50176]	Loss: 0.3890
Training Epoch: 66 [32256/50176]	Loss: 0.3754
Training Epoch: 66 [32512/50176]	Loss: 0.3564
Training Epoch: 66 [32768/50176]	Loss: 0.4044
Training Epoch: 66 [33024/50176]	Loss: 0.3960
Training Epoch: 66 [33280/50176]	Loss: 0.4292
Training Epoch: 66 [33536/50176]	Loss: 0.2953
Training Epoch: 66 [33792/50176]	Loss: 0.4298
Training Epoch: 66 [34048/50176]	Loss: 0.4120
Training Epoch: 66 [34304/50176]	Loss: 0.4106
Training Epoch: 66 [34560/50176]	Loss: 0.3969
Training Epoch: 66 [34816/50176]	Loss: 0.3525
Training Epoch: 66 [35072/50176]	Loss: 0.4014
Training Epoch: 66 [35328/50176]	Loss: 0.3732
Training Epoch: 66 [35584/50176]	Loss: 0.3690
Training Epoch: 66 [35840/50176]	Loss: 0.3760
Training Epoch: 66 [36096/50176]	Loss: 0.5029
Training Epoch: 66 [36352/50176]	Loss: 0.5264
Training Epoch: 66 [36608/50176]	Loss: 0.4366
Training Epoch: 66 [36864/50176]	Loss: 0.3744
Training Epoch: 66 [37120/50176]	Loss: 0.4495
Training Epoch: 66 [37376/50176]	Loss: 0.3781
Training Epoch: 66 [37632/50176]	Loss: 0.3473
Training Epoch: 66 [37888/50176]	Loss: 0.4154
Training Epoch: 66 [38144/50176]	Loss: 0.4014
Training Epoch: 66 [38400/50176]	Loss: 0.4179
Training Epoch: 66 [38656/50176]	Loss: 0.3785
Training Epoch: 66 [38912/50176]	Loss: 0.3995
Training Epoch: 66 [39168/50176]	Loss: 0.3593
Training Epoch: 66 [39424/50176]	Loss: 0.5417
Training Epoch: 66 [39680/50176]	Loss: 0.4622
Training Epoch: 66 [39936/50176]	Loss: 0.3628
Training Epoch: 66 [40192/50176]	Loss: 0.4533
Training Epoch: 66 [40448/50176]	Loss: 0.5652
Training Epoch: 66 [40704/50176]	Loss: 0.4234
Training Epoch: 66 [40960/50176]	Loss: 0.4453
Training Epoch: 66 [41216/50176]	Loss: 0.4233
Training Epoch: 66 [41472/50176]	Loss: 0.4630
Training Epoch: 66 [41728/50176]	Loss: 0.4869
Training Epoch: 66 [41984/50176]	Loss: 0.3689
Training Epoch: 66 [42240/50176]	Loss: 0.3643
Training Epoch: 66 [42496/50176]	Loss: 0.4290
Training Epoch: 66 [42752/50176]	Loss: 0.3840
Training Epoch: 66 [43008/50176]	Loss: 0.4455
Training Epoch: 66 [43264/50176]	Loss: 0.3607
Training Epoch: 66 [43520/50176]	Loss: 0.4546
Training Epoch: 66 [43776/50176]	Loss: 0.3927
Training Epoch: 66 [44032/50176]	Loss: 0.4229
Training Epoch: 66 [44288/50176]	Loss: 0.4112
Training Epoch: 66 [44544/50176]	Loss: 0.4469
Training Epoch: 66 [44800/50176]	Loss: 0.5521
Training Epoch: 66 [45056/50176]	Loss: 0.3738
Training Epoch: 66 [45312/50176]	Loss: 0.3627
Training Epoch: 66 [45568/50176]	Loss: 0.3882
Training Epoch: 66 [45824/50176]	Loss: 0.3643
Training Epoch: 66 [46080/50176]	Loss: 0.4555
Training Epoch: 66 [46336/50176]	Loss: 0.4221
Training Epoch: 66 [46592/50176]	Loss: 0.4605
Training Epoch: 66 [46848/50176]	Loss: 0.4538
Training Epoch: 66 [47104/50176]	Loss: 0.2462
Training Epoch: 66 [47360/50176]	Loss: 0.4730
Training Epoch: 66 [47616/50176]	Loss: 0.4580
Training Epoch: 66 [47872/50176]	Loss: 0.4509
Training Epoch: 66 [48128/50176]	Loss: 0.4827
Training Epoch: 66 [48384/50176]	Loss: 0.3339
Training Epoch: 66 [48640/50176]	Loss: 0.5706
Training Epoch: 66 [48896/50176]	Loss: 0.4162
Training Epoch: 66 [49152/50176]	Loss: 0.4415
Training Epoch: 66 [49408/50176]	Loss: 0.3838
Training Epoch: 66 [49664/50176]	Loss: 0.2993
Training Epoch: 66 [49920/50176]	Loss: 0.5258
Training Epoch: 66 [50176/50176]	Loss: 0.3805
Validation Epoch: 66, Average loss: 0.0106, Accuracy: 0.5592
Training Epoch: 67 [256/50176]	Loss: 0.3586
Training Epoch: 67 [512/50176]	Loss: 0.4263
Training Epoch: 67 [768/50176]	Loss: 0.3510
Training Epoch: 67 [1024/50176]	Loss: 0.2651
Training Epoch: 67 [1280/50176]	Loss: 0.3982
Training Epoch: 67 [1536/50176]	Loss: 0.4589
Training Epoch: 67 [1792/50176]	Loss: 0.3777
Training Epoch: 67 [2048/50176]	Loss: 0.4101
Training Epoch: 67 [2304/50176]	Loss: 0.4020
Training Epoch: 67 [2560/50176]	Loss: 0.3468
Training Epoch: 67 [2816/50176]	Loss: 0.3128
Training Epoch: 67 [3072/50176]	Loss: 0.4294
Training Epoch: 67 [3328/50176]	Loss: 0.3936
Training Epoch: 67 [3584/50176]	Loss: 0.3549
Training Epoch: 67 [3840/50176]	Loss: 0.3662
Training Epoch: 67 [4096/50176]	Loss: 0.3742
Training Epoch: 67 [4352/50176]	Loss: 0.3509
Training Epoch: 67 [4608/50176]	Loss: 0.4271
Training Epoch: 67 [4864/50176]	Loss: 0.3122
Training Epoch: 67 [5120/50176]	Loss: 0.4043
Training Epoch: 67 [5376/50176]	Loss: 0.3529
Training Epoch: 67 [5632/50176]	Loss: 0.3193
Training Epoch: 67 [5888/50176]	Loss: 0.4287
Training Epoch: 67 [6144/50176]	Loss: 0.3188
Training Epoch: 67 [6400/50176]	Loss: 0.4275
Training Epoch: 67 [6656/50176]	Loss: 0.3106
Training Epoch: 67 [6912/50176]	Loss: 0.3346
Training Epoch: 67 [7168/50176]	Loss: 0.4443
Training Epoch: 67 [7424/50176]	Loss: 0.3667
Training Epoch: 67 [7680/50176]	Loss: 0.3172
Training Epoch: 67 [7936/50176]	Loss: 0.4121
Training Epoch: 67 [8192/50176]	Loss: 0.5018
Training Epoch: 67 [8448/50176]	Loss: 0.4303
Training Epoch: 67 [8704/50176]	Loss: 0.3706
Training Epoch: 67 [8960/50176]	Loss: 0.4400
Training Epoch: 67 [9216/50176]	Loss: 0.4145
Training Epoch: 67 [9472/50176]	Loss: 0.3712
Training Epoch: 67 [9728/50176]	Loss: 0.4665
Training Epoch: 67 [9984/50176]	Loss: 0.3975
Training Epoch: 67 [10240/50176]	Loss: 0.4132
Training Epoch: 67 [10496/50176]	Loss: 0.3244
Training Epoch: 67 [10752/50176]	Loss: 0.3676
Training Epoch: 67 [11008/50176]	Loss: 0.3211
Training Epoch: 67 [11264/50176]	Loss: 0.4278
Training Epoch: 67 [11520/50176]	Loss: 0.4580
Training Epoch: 67 [11776/50176]	Loss: 0.3641
Training Epoch: 67 [12032/50176]	Loss: 0.4255
Training Epoch: 67 [12288/50176]	Loss: 0.3694
Training Epoch: 67 [12544/50176]	Loss: 0.3753
Training Epoch: 67 [12800/50176]	Loss: 0.3535
Training Epoch: 67 [13056/50176]	Loss: 0.3028
Training Epoch: 67 [13312/50176]	Loss: 0.3478
Training Epoch: 67 [13568/50176]	Loss: 0.4038
Training Epoch: 67 [13824/50176]	Loss: 0.3503
Training Epoch: 67 [14080/50176]	Loss: 0.3143
Training Epoch: 67 [14336/50176]	Loss: 0.3216
Training Epoch: 67 [14592/50176]	Loss: 0.3583
Training Epoch: 67 [14848/50176]	Loss: 0.3421
Training Epoch: 67 [15104/50176]	Loss: 0.4436
Training Epoch: 67 [15360/50176]	Loss: 0.5681
Training Epoch: 67 [15616/50176]	Loss: 0.2499
Training Epoch: 67 [15872/50176]	Loss: 0.4136
Training Epoch: 67 [16128/50176]	Loss: 0.4151
Training Epoch: 67 [16384/50176]	Loss: 0.3922
Training Epoch: 67 [16640/50176]	Loss: 0.3417
Training Epoch: 67 [16896/50176]	Loss: 0.4146
Training Epoch: 67 [17152/50176]	Loss: 0.3182
Training Epoch: 67 [17408/50176]	Loss: 0.3973
Training Epoch: 67 [17664/50176]	Loss: 0.3789
Training Epoch: 67 [17920/50176]	Loss: 0.4242
Training Epoch: 67 [18176/50176]	Loss: 0.4139
Training Epoch: 67 [18432/50176]	Loss: 0.3613
Training Epoch: 67 [18688/50176]	Loss: 0.5153
Training Epoch: 67 [18944/50176]	Loss: 0.3657
Training Epoch: 67 [19200/50176]	Loss: 0.4172
Training Epoch: 67 [19456/50176]	Loss: 0.4691
Training Epoch: 67 [19712/50176]	Loss: 0.3888
Training Epoch: 67 [19968/50176]	Loss: 0.4811
Training Epoch: 67 [20224/50176]	Loss: 0.5892
Training Epoch: 67 [20480/50176]	Loss: 0.4433
Training Epoch: 67 [20736/50176]	Loss: 0.3653
Training Epoch: 67 [20992/50176]	Loss: 0.4816
Training Epoch: 67 [21248/50176]	Loss: 0.3485
Training Epoch: 67 [21504/50176]	Loss: 0.5038
Training Epoch: 67 [21760/50176]	Loss: 0.3956
Training Epoch: 67 [22016/50176]	Loss: 0.3440
Training Epoch: 67 [22272/50176]	Loss: 0.3233
Training Epoch: 67 [22528/50176]	Loss: 0.4888
Training Epoch: 67 [22784/50176]	Loss: 0.3923
Training Epoch: 67 [23040/50176]	Loss: 0.3204
Training Epoch: 67 [23296/50176]	Loss: 0.4675
Training Epoch: 67 [23552/50176]	Loss: 0.4931
Training Epoch: 67 [23808/50176]	Loss: 0.3810
Training Epoch: 67 [24064/50176]	Loss: 0.3472
Training Epoch: 67 [24320/50176]	Loss: 0.3922
Training Epoch: 67 [24576/50176]	Loss: 0.3854
Training Epoch: 67 [24832/50176]	Loss: 0.4115
Training Epoch: 67 [25088/50176]	Loss: 0.4195
Training Epoch: 67 [25344/50176]	Loss: 0.3904
Training Epoch: 67 [25600/50176]	Loss: 0.4204
Training Epoch: 67 [25856/50176]	Loss: 0.5111
Training Epoch: 67 [26112/50176]	Loss: 0.4152
Training Epoch: 67 [26368/50176]	Loss: 0.4918
Training Epoch: 67 [26624/50176]	Loss: 0.2985
Training Epoch: 67 [26880/50176]	Loss: 0.3592
Training Epoch: 67 [27136/50176]	Loss: 0.5355
Training Epoch: 67 [27392/50176]	Loss: 0.5292
Training Epoch: 67 [27648/50176]	Loss: 0.3543
Training Epoch: 67 [27904/50176]	Loss: 0.3297
Training Epoch: 67 [28160/50176]	Loss: 0.4039
Training Epoch: 67 [28416/50176]	Loss: 0.5198
Training Epoch: 67 [28672/50176]	Loss: 0.4202
Training Epoch: 67 [28928/50176]	Loss: 0.3638
Training Epoch: 67 [29184/50176]	Loss: 0.3355
Training Epoch: 67 [29440/50176]	Loss: 0.3801
Training Epoch: 67 [29696/50176]	Loss: 0.4660
Training Epoch: 67 [29952/50176]	Loss: 0.3046
Training Epoch: 67 [30208/50176]	Loss: 0.3553
Training Epoch: 67 [30464/50176]	Loss: 0.4844
Training Epoch: 67 [30720/50176]	Loss: 0.4500
Training Epoch: 67 [30976/50176]	Loss: 0.3605
Training Epoch: 67 [31232/50176]	Loss: 0.3579
Training Epoch: 67 [31488/50176]	Loss: 0.4205
Training Epoch: 67 [31744/50176]	Loss: 0.3196
Training Epoch: 67 [32000/50176]	Loss: 0.4555
Training Epoch: 67 [32256/50176]	Loss: 0.5090
Training Epoch: 67 [32512/50176]	Loss: 0.3757
Training Epoch: 67 [32768/50176]	Loss: 0.3999
Training Epoch: 67 [33024/50176]	Loss: 0.3819
Training Epoch: 67 [33280/50176]	Loss: 0.4976
Training Epoch: 67 [33536/50176]	Loss: 0.3751
Training Epoch: 67 [33792/50176]	Loss: 0.3773
Training Epoch: 67 [34048/50176]	Loss: 0.3874
Training Epoch: 67 [34304/50176]	Loss: 0.4328
Training Epoch: 67 [34560/50176]	Loss: 0.4476
Training Epoch: 67 [34816/50176]	Loss: 0.3877
Training Epoch: 67 [35072/50176]	Loss: 0.4531
Training Epoch: 67 [35328/50176]	Loss: 0.4699
Training Epoch: 67 [35584/50176]	Loss: 0.3912
Training Epoch: 67 [35840/50176]	Loss: 0.3435
Training Epoch: 67 [36096/50176]	Loss: 0.4170
Training Epoch: 67 [36352/50176]	Loss: 0.3751
Training Epoch: 67 [36608/50176]	Loss: 0.3692
Training Epoch: 67 [36864/50176]	Loss: 0.4019
Training Epoch: 67 [37120/50176]	Loss: 0.4112
Training Epoch: 67 [37376/50176]	Loss: 0.4768
Training Epoch: 67 [37632/50176]	Loss: 0.4371
Training Epoch: 67 [37888/50176]	Loss: 0.3213
Training Epoch: 67 [38144/50176]	Loss: 0.4757
Training Epoch: 67 [38400/50176]	Loss: 0.4399
Training Epoch: 67 [38656/50176]	Loss: 0.4196
Training Epoch: 67 [38912/50176]	Loss: 0.3783
Training Epoch: 67 [39168/50176]	Loss: 0.4328
Training Epoch: 67 [39424/50176]	Loss: 0.4482
Training Epoch: 67 [39680/50176]	Loss: 0.3926
Training Epoch: 67 [39936/50176]	Loss: 0.3698
Training Epoch: 67 [40192/50176]	Loss: 0.4384
Training Epoch: 67 [40448/50176]	Loss: 0.4732
Training Epoch: 67 [40704/50176]	Loss: 0.4459
Training Epoch: 67 [40960/50176]	Loss: 0.4344
Training Epoch: 67 [41216/50176]	Loss: 0.4705
Training Epoch: 67 [41472/50176]	Loss: 0.4454
Training Epoch: 67 [41728/50176]	Loss: 0.4269
Training Epoch: 67 [41984/50176]	Loss: 0.4205
Training Epoch: 67 [42240/50176]	Loss: 0.4198
Training Epoch: 67 [42496/50176]	Loss: 0.3558
Training Epoch: 67 [42752/50176]	Loss: 0.3567
Training Epoch: 67 [43008/50176]	Loss: 0.3580
Training Epoch: 67 [43264/50176]	Loss: 0.3783
Training Epoch: 67 [43520/50176]	Loss: 0.4545
Training Epoch: 67 [43776/50176]	Loss: 0.4526
Training Epoch: 67 [44032/50176]	Loss: 0.4722
Training Epoch: 67 [44288/50176]	Loss: 0.5128
Training Epoch: 67 [44544/50176]	Loss: 0.4563
Training Epoch: 67 [44800/50176]	Loss: 0.5349
Training Epoch: 67 [45056/50176]	Loss: 0.4773
Training Epoch: 67 [45312/50176]	Loss: 0.4738
Training Epoch: 67 [45568/50176]	Loss: 0.4445
Training Epoch: 67 [45824/50176]	Loss: 0.4478
Training Epoch: 67 [46080/50176]	Loss: 0.3281
Training Epoch: 67 [46336/50176]	Loss: 0.4387
Training Epoch: 67 [46592/50176]	Loss: 0.3683
Training Epoch: 67 [46848/50176]	Loss: 0.4532
Training Epoch: 67 [47104/50176]	Loss: 0.4594
Training Epoch: 67 [47360/50176]	Loss: 0.4615
Training Epoch: 67 [47616/50176]	Loss: 0.4263
Training Epoch: 67 [47872/50176]	Loss: 0.4200
Training Epoch: 67 [48128/50176]	Loss: 0.4288
Training Epoch: 67 [48384/50176]	Loss: 0.4034
Training Epoch: 67 [48640/50176]	Loss: 0.4709
Training Epoch: 67 [48896/50176]	Loss: 0.4573
Training Epoch: 67 [49152/50176]	Loss: 0.2948
Training Epoch: 67 [49408/50176]	Loss: 0.3635
Training Epoch: 67 [49664/50176]	Loss: 0.3765
Training Epoch: 67 [49920/50176]	Loss: 0.3044
Training Epoch: 67 [50176/50176]	Loss: 0.7551
Validation Epoch: 67, Average loss: 0.0100, Accuracy: 0.5681
Training Epoch: 68 [256/50176]	Loss: 0.3056
Training Epoch: 68 [512/50176]	Loss: 0.4248
Training Epoch: 68 [768/50176]	Loss: 0.2691
Training Epoch: 68 [1024/50176]	Loss: 0.2934
Training Epoch: 68 [1280/50176]	Loss: 0.3779
Training Epoch: 68 [1536/50176]	Loss: 0.3415
Training Epoch: 68 [1792/50176]	Loss: 0.4532
Training Epoch: 68 [2048/50176]	Loss: 0.4408
Training Epoch: 68 [2304/50176]	Loss: 0.5255
Training Epoch: 68 [2560/50176]	Loss: 0.3796
Training Epoch: 68 [2816/50176]	Loss: 0.4422
Training Epoch: 68 [3072/50176]	Loss: 0.4116
Training Epoch: 68 [3328/50176]	Loss: 0.3175
Training Epoch: 68 [3584/50176]	Loss: 0.5313
Training Epoch: 68 [3840/50176]	Loss: 0.4367
Training Epoch: 68 [4096/50176]	Loss: 0.3852
Training Epoch: 68 [4352/50176]	Loss: 0.3311
Training Epoch: 68 [4608/50176]	Loss: 0.4382
Training Epoch: 68 [4864/50176]	Loss: 0.4595
Training Epoch: 68 [5120/50176]	Loss: 0.4090
Training Epoch: 68 [5376/50176]	Loss: 0.4304
Training Epoch: 68 [5632/50176]	Loss: 0.3506
Training Epoch: 68 [5888/50176]	Loss: 0.5306
Training Epoch: 68 [6144/50176]	Loss: 0.4341
Training Epoch: 68 [6400/50176]	Loss: 0.4080
Training Epoch: 68 [6656/50176]	Loss: 0.4297
Training Epoch: 68 [6912/50176]	Loss: 0.3842
Training Epoch: 68 [7168/50176]	Loss: 0.3899
Training Epoch: 68 [7424/50176]	Loss: 0.3111
Training Epoch: 68 [7680/50176]	Loss: 0.3203
Training Epoch: 68 [7936/50176]	Loss: 0.3759
Training Epoch: 68 [8192/50176]	Loss: 0.5094
Training Epoch: 68 [8448/50176]	Loss: 0.3775
Training Epoch: 68 [8704/50176]	Loss: 0.3830
Training Epoch: 68 [8960/50176]	Loss: 0.3186
Training Epoch: 68 [9216/50176]	Loss: 0.3677
Training Epoch: 68 [9472/50176]	Loss: 0.4527
Training Epoch: 68 [9728/50176]	Loss: 0.4079
Training Epoch: 68 [9984/50176]	Loss: 0.4472
Training Epoch: 68 [10240/50176]	Loss: 0.3952
Training Epoch: 68 [10496/50176]	Loss: 0.4036
Training Epoch: 68 [10752/50176]	Loss: 0.4017
Training Epoch: 68 [11008/50176]	Loss: 0.3842
Training Epoch: 68 [11264/50176]	Loss: 0.4118
Training Epoch: 68 [11520/50176]	Loss: 0.3428
Training Epoch: 68 [11776/50176]	Loss: 0.3927
Training Epoch: 68 [12032/50176]	Loss: 0.3074
Training Epoch: 68 [12288/50176]	Loss: 0.3627
Training Epoch: 68 [12544/50176]	Loss: 0.3518
Training Epoch: 68 [12800/50176]	Loss: 0.4270
Training Epoch: 68 [13056/50176]	Loss: 0.3762
Training Epoch: 68 [13312/50176]	Loss: 0.4181
Training Epoch: 68 [13568/50176]	Loss: 0.4309
Training Epoch: 68 [13824/50176]	Loss: 0.3434
Training Epoch: 68 [14080/50176]	Loss: 0.4413
Training Epoch: 68 [14336/50176]	Loss: 0.4090
Training Epoch: 68 [14592/50176]	Loss: 0.4196
Training Epoch: 68 [14848/50176]	Loss: 0.3357
Training Epoch: 68 [15104/50176]	Loss: 0.4230
Training Epoch: 68 [15360/50176]	Loss: 0.3414
Training Epoch: 68 [15616/50176]	Loss: 0.3425
Training Epoch: 68 [15872/50176]	Loss: 0.2922
Training Epoch: 68 [16128/50176]	Loss: 0.3747
Training Epoch: 68 [16384/50176]	Loss: 0.3969
Training Epoch: 68 [16640/50176]	Loss: 0.3367
Training Epoch: 68 [16896/50176]	Loss: 0.4783
Training Epoch: 68 [17152/50176]	Loss: 0.4447
Training Epoch: 68 [17408/50176]	Loss: 0.3656
Training Epoch: 68 [17664/50176]	Loss: 0.4087
Training Epoch: 68 [17920/50176]	Loss: 0.3870
Training Epoch: 68 [18176/50176]	Loss: 0.3935
Training Epoch: 68 [18432/50176]	Loss: 0.3487
Training Epoch: 68 [18688/50176]	Loss: 0.2612
Training Epoch: 68 [18944/50176]	Loss: 0.4177
Training Epoch: 68 [19200/50176]	Loss: 0.3436
Training Epoch: 68 [19456/50176]	Loss: 0.4059
Training Epoch: 68 [19712/50176]	Loss: 0.4284
Training Epoch: 68 [19968/50176]	Loss: 0.3864
Training Epoch: 68 [20224/50176]	Loss: 0.4287
Training Epoch: 68 [20480/50176]	Loss: 0.4598
Training Epoch: 68 [20736/50176]	Loss: 0.3656
Training Epoch: 68 [20992/50176]	Loss: 0.3546
Training Epoch: 68 [21248/50176]	Loss: 0.4257
Training Epoch: 68 [21504/50176]	Loss: 0.3667
Training Epoch: 68 [21760/50176]	Loss: 0.3907
Training Epoch: 68 [22016/50176]	Loss: 0.3246
Training Epoch: 68 [22272/50176]	Loss: 0.3751
Training Epoch: 68 [22528/50176]	Loss: 0.3159
Training Epoch: 68 [22784/50176]	Loss: 0.4384
Training Epoch: 68 [23040/50176]	Loss: 0.3828
Training Epoch: 68 [23296/50176]	Loss: 0.4791
Training Epoch: 68 [23552/50176]	Loss: 0.3832
Training Epoch: 68 [23808/50176]	Loss: 0.4078
Training Epoch: 68 [24064/50176]	Loss: 0.2866
Training Epoch: 68 [24320/50176]	Loss: 0.3583
Training Epoch: 68 [24576/50176]	Loss: 0.3421
Training Epoch: 68 [24832/50176]	Loss: 0.3342
Training Epoch: 68 [25088/50176]	Loss: 0.4155
Training Epoch: 68 [25344/50176]	Loss: 0.3298
Training Epoch: 68 [25600/50176]	Loss: 0.3950
Training Epoch: 68 [25856/50176]	Loss: 0.4563
Training Epoch: 68 [26112/50176]	Loss: 0.3280
Training Epoch: 68 [26368/50176]	Loss: 0.4125
Training Epoch: 68 [26624/50176]	Loss: 0.4501
Training Epoch: 68 [26880/50176]	Loss: 0.4446
Training Epoch: 68 [27136/50176]	Loss: 0.4351
Training Epoch: 68 [27392/50176]	Loss: 0.4014
Training Epoch: 68 [27648/50176]	Loss: 0.3510
Training Epoch: 68 [27904/50176]	Loss: 0.4172
Training Epoch: 68 [28160/50176]	Loss: 0.4081
Training Epoch: 68 [28416/50176]	Loss: 0.4769
Training Epoch: 68 [28672/50176]	Loss: 0.4096
Training Epoch: 68 [28928/50176]	Loss: 0.3842
Training Epoch: 68 [29184/50176]	Loss: 0.4132
Training Epoch: 68 [29440/50176]	Loss: 0.4116
Training Epoch: 68 [29696/50176]	Loss: 0.4946
Training Epoch: 68 [29952/50176]	Loss: 0.4635
Training Epoch: 68 [30208/50176]	Loss: 0.4223
Training Epoch: 68 [30464/50176]	Loss: 0.4692
Training Epoch: 68 [30720/50176]	Loss: 0.3727
Training Epoch: 68 [30976/50176]	Loss: 0.3076
Training Epoch: 68 [31232/50176]	Loss: 0.4009
Training Epoch: 68 [31488/50176]	Loss: 0.3923
Training Epoch: 68 [31744/50176]	Loss: 0.3313
Training Epoch: 68 [32000/50176]	Loss: 0.3508
Training Epoch: 68 [32256/50176]	Loss: 0.3836
Training Epoch: 68 [32512/50176]	Loss: 0.3063
Training Epoch: 68 [32768/50176]	Loss: 0.4352
Training Epoch: 68 [33024/50176]	Loss: 0.4168
Training Epoch: 68 [33280/50176]	Loss: 0.2798
Training Epoch: 68 [33536/50176]	Loss: 0.4048
Training Epoch: 68 [33792/50176]	Loss: 0.2983
Training Epoch: 68 [34048/50176]	Loss: 0.4185
Training Epoch: 68 [34304/50176]	Loss: 0.3256
Training Epoch: 68 [34560/50176]	Loss: 0.4884
Training Epoch: 68 [34816/50176]	Loss: 0.4274
Training Epoch: 68 [35072/50176]	Loss: 0.3185
Training Epoch: 68 [35328/50176]	Loss: 0.3985
Training Epoch: 68 [35584/50176]	Loss: 0.3413
Training Epoch: 68 [35840/50176]	Loss: 0.4383
Training Epoch: 68 [36096/50176]	Loss: 0.4921
Training Epoch: 68 [36352/50176]	Loss: 0.3446
Training Epoch: 68 [36608/50176]	Loss: 0.3836
Training Epoch: 68 [36864/50176]	Loss: 0.3944
Training Epoch: 68 [37120/50176]	Loss: 0.4128
Training Epoch: 68 [37376/50176]	Loss: 0.4667
Training Epoch: 68 [37632/50176]	Loss: 0.4743
Training Epoch: 68 [37888/50176]	Loss: 0.3200
Training Epoch: 68 [38144/50176]	Loss: 0.3124
Training Epoch: 68 [38400/50176]	Loss: 0.4239
Training Epoch: 68 [38656/50176]	Loss: 0.4677
Training Epoch: 68 [38912/50176]	Loss: 0.3887
Training Epoch: 68 [39168/50176]	Loss: 0.4301
Training Epoch: 68 [39424/50176]	Loss: 0.3603
Training Epoch: 68 [39680/50176]	Loss: 0.3172
Training Epoch: 68 [39936/50176]	Loss: 0.3576
Training Epoch: 68 [40192/50176]	Loss: 0.4049
Training Epoch: 68 [40448/50176]	Loss: 0.4014
Training Epoch: 68 [40704/50176]	Loss: 0.4239
Training Epoch: 68 [40960/50176]	Loss: 0.4677
Training Epoch: 68 [41216/50176]	Loss: 0.4603
Training Epoch: 68 [41472/50176]	Loss: 0.3440
Training Epoch: 68 [41728/50176]	Loss: 0.3639
Training Epoch: 68 [41984/50176]	Loss: 0.3935
Training Epoch: 68 [42240/50176]	Loss: 0.3948
Training Epoch: 68 [42496/50176]	Loss: 0.3596
Training Epoch: 68 [42752/50176]	Loss: 0.2915
Training Epoch: 68 [43008/50176]	Loss: 0.4762
Training Epoch: 68 [43264/50176]	Loss: 0.3893
Training Epoch: 68 [43520/50176]	Loss: 0.3709
Training Epoch: 68 [43776/50176]	Loss: 0.3125
Training Epoch: 68 [44032/50176]	Loss: 0.4050
Training Epoch: 68 [44288/50176]	Loss: 0.4854
Training Epoch: 68 [44544/50176]	Loss: 0.3011
Training Epoch: 68 [44800/50176]	Loss: 0.5102
Training Epoch: 68 [45056/50176]	Loss: 0.3154
Training Epoch: 68 [45312/50176]	Loss: 0.3710
Training Epoch: 68 [45568/50176]	Loss: 0.3062
Training Epoch: 68 [45824/50176]	Loss: 0.4713
Training Epoch: 68 [46080/50176]	Loss: 0.5154
Training Epoch: 68 [46336/50176]	Loss: 0.3536
Training Epoch: 68 [46592/50176]	Loss: 0.4029
Training Epoch: 68 [46848/50176]	Loss: 0.4531
Training Epoch: 68 [47104/50176]	Loss: 0.4565
Training Epoch: 68 [47360/50176]	Loss: 0.4003
Training Epoch: 68 [47616/50176]	Loss: 0.4247
Training Epoch: 68 [47872/50176]	Loss: 0.5435
Training Epoch: 68 [48128/50176]	Loss: 0.3273
Training Epoch: 68 [48384/50176]	Loss: 0.4057
Training Epoch: 68 [48640/50176]	Loss: 0.4843
Training Epoch: 68 [48896/50176]	Loss: 0.4658
Training Epoch: 68 [49152/50176]	Loss: 0.4437
Training Epoch: 68 [49408/50176]	Loss: 0.3991
Training Epoch: 68 [49664/50176]	Loss: 0.4456
Training Epoch: 68 [49920/50176]	Loss: 0.4187
Training Epoch: 68 [50176/50176]	Loss: 0.5993
Validation Epoch: 68, Average loss: 0.0111, Accuracy: 0.5411
Training Epoch: 69 [256/50176]	Loss: 0.4001
Training Epoch: 69 [512/50176]	Loss: 0.3089
Training Epoch: 69 [768/50176]	Loss: 0.4358
Training Epoch: 69 [1024/50176]	Loss: 0.3019
Training Epoch: 69 [1280/50176]	Loss: 0.3149
Training Epoch: 69 [1536/50176]	Loss: 0.3621
Training Epoch: 69 [1792/50176]	Loss: 0.3051
Training Epoch: 69 [2048/50176]	Loss: 0.3892
Training Epoch: 69 [2304/50176]	Loss: 0.4201
Training Epoch: 69 [2560/50176]	Loss: 0.3962
Training Epoch: 69 [2816/50176]	Loss: 0.3332
Training Epoch: 69 [3072/50176]	Loss: 0.3645
Training Epoch: 69 [3328/50176]	Loss: 0.3192
Training Epoch: 69 [3584/50176]	Loss: 0.3231
Training Epoch: 69 [3840/50176]	Loss: 0.3145
Training Epoch: 69 [4096/50176]	Loss: 0.3865
Training Epoch: 69 [4352/50176]	Loss: 0.2411
Training Epoch: 69 [4608/50176]	Loss: 0.3674
Training Epoch: 69 [4864/50176]	Loss: 0.3177
Training Epoch: 69 [5120/50176]	Loss: 0.4437
Training Epoch: 69 [5376/50176]	Loss: 0.3585
Training Epoch: 69 [5632/50176]	Loss: 0.3524
Training Epoch: 69 [5888/50176]	Loss: 0.3798
Training Epoch: 69 [6144/50176]	Loss: 0.4665
Training Epoch: 69 [6400/50176]	Loss: 0.3797
Training Epoch: 69 [6656/50176]	Loss: 0.3495
Training Epoch: 69 [6912/50176]	Loss: 0.3696
Training Epoch: 69 [7168/50176]	Loss: 0.3877
Training Epoch: 69 [7424/50176]	Loss: 0.2876
Training Epoch: 69 [7680/50176]	Loss: 0.3684
Training Epoch: 69 [7936/50176]	Loss: 0.3430
Training Epoch: 69 [8192/50176]	Loss: 0.4286
Training Epoch: 69 [8448/50176]	Loss: 0.2453
Training Epoch: 69 [8704/50176]	Loss: 0.3130
Training Epoch: 69 [8960/50176]	Loss: 0.3460
Training Epoch: 69 [9216/50176]	Loss: 0.2456
Training Epoch: 69 [9472/50176]	Loss: 0.3209
Training Epoch: 69 [9728/50176]	Loss: 0.4452
Training Epoch: 69 [9984/50176]	Loss: 0.3504
Training Epoch: 69 [10240/50176]	Loss: 0.3368
Training Epoch: 69 [10496/50176]	Loss: 0.4240
Training Epoch: 69 [10752/50176]	Loss: 0.3159
Training Epoch: 69 [11008/50176]	Loss: 0.4795
Training Epoch: 69 [11264/50176]	Loss: 0.3416
Training Epoch: 69 [11520/50176]	Loss: 0.3521
Training Epoch: 69 [11776/50176]	Loss: 0.3611
Training Epoch: 69 [12032/50176]	Loss: 0.4200
Training Epoch: 69 [12288/50176]	Loss: 0.4487
Training Epoch: 69 [12544/50176]	Loss: 0.4246
Training Epoch: 69 [12800/50176]	Loss: 0.3110
Training Epoch: 69 [13056/50176]	Loss: 0.3070
Training Epoch: 69 [13312/50176]	Loss: 0.3796
Training Epoch: 69 [13568/50176]	Loss: 0.3509
Training Epoch: 69 [13824/50176]	Loss: 0.3053
Training Epoch: 69 [14080/50176]	Loss: 0.3851
Training Epoch: 69 [14336/50176]	Loss: 0.3597
Training Epoch: 69 [14592/50176]	Loss: 0.2865
Training Epoch: 69 [14848/50176]	Loss: 0.3571
Training Epoch: 69 [15104/50176]	Loss: 0.4043
Training Epoch: 69 [15360/50176]	Loss: 0.3383
Training Epoch: 69 [15616/50176]	Loss: 0.3541
Training Epoch: 69 [15872/50176]	Loss: 0.4170
Training Epoch: 69 [16128/50176]	Loss: 0.3412
Training Epoch: 69 [16384/50176]	Loss: 0.4149
Training Epoch: 69 [16640/50176]	Loss: 0.3324
Training Epoch: 69 [16896/50176]	Loss: 0.3627
Training Epoch: 69 [17152/50176]	Loss: 0.2780
Training Epoch: 69 [17408/50176]	Loss: 0.2527
Training Epoch: 69 [17664/50176]	Loss: 0.3573
Training Epoch: 69 [17920/50176]	Loss: 0.2922
Training Epoch: 69 [18176/50176]	Loss: 0.2873
Training Epoch: 69 [18432/50176]	Loss: 0.3098
Training Epoch: 69 [18688/50176]	Loss: 0.4066
Training Epoch: 69 [18944/50176]	Loss: 0.3710
Training Epoch: 69 [19200/50176]	Loss: 0.3001
Training Epoch: 69 [19456/50176]	Loss: 0.3889
Training Epoch: 69 [19712/50176]	Loss: 0.3979
Training Epoch: 69 [19968/50176]	Loss: 0.4034
Training Epoch: 69 [20224/50176]	Loss: 0.3091
Training Epoch: 69 [20480/50176]	Loss: 0.3640
Training Epoch: 69 [20736/50176]	Loss: 0.4605
Training Epoch: 69 [20992/50176]	Loss: 0.2738
Training Epoch: 69 [21248/50176]	Loss: 0.3409
Training Epoch: 69 [21504/50176]	Loss: 0.3012
Training Epoch: 69 [21760/50176]	Loss: 0.4026
Training Epoch: 69 [22016/50176]	Loss: 0.4321
Training Epoch: 69 [22272/50176]	Loss: 0.4706
Training Epoch: 69 [22528/50176]	Loss: 0.3250
Training Epoch: 69 [22784/50176]	Loss: 0.3671
Training Epoch: 69 [23040/50176]	Loss: 0.4766
Training Epoch: 69 [23296/50176]	Loss: 0.4016
Training Epoch: 69 [23552/50176]	Loss: 0.3829
Training Epoch: 69 [23808/50176]	Loss: 0.3039
Training Epoch: 69 [24064/50176]	Loss: 0.4474
Training Epoch: 69 [24320/50176]	Loss: 0.4550
Training Epoch: 69 [24576/50176]	Loss: 0.3472
Training Epoch: 69 [24832/50176]	Loss: 0.3527
Training Epoch: 69 [25088/50176]	Loss: 0.3817
Training Epoch: 69 [25344/50176]	Loss: 0.4531
Training Epoch: 69 [25600/50176]	Loss: 0.4371
Training Epoch: 69 [25856/50176]	Loss: 0.3605
Training Epoch: 69 [26112/50176]	Loss: 0.3373
Training Epoch: 69 [26368/50176]	Loss: 0.3935
Training Epoch: 69 [26624/50176]	Loss: 0.3483
Training Epoch: 69 [26880/50176]	Loss: 0.4082
Training Epoch: 69 [27136/50176]	Loss: 0.3256
Training Epoch: 69 [27392/50176]	Loss: 0.3632
Training Epoch: 69 [27648/50176]	Loss: 0.4152
Training Epoch: 69 [27904/50176]	Loss: 0.3644
Training Epoch: 69 [28160/50176]	Loss: 0.4400
Training Epoch: 69 [28416/50176]	Loss: 0.3118
Training Epoch: 69 [28672/50176]	Loss: 0.4185
Training Epoch: 69 [28928/50176]	Loss: 0.3757
Training Epoch: 69 [29184/50176]	Loss: 0.4131
Training Epoch: 69 [29440/50176]	Loss: 0.3465
Training Epoch: 69 [29696/50176]	Loss: 0.5268
Training Epoch: 69 [29952/50176]	Loss: 0.3224
Training Epoch: 69 [30208/50176]	Loss: 0.4012
Training Epoch: 69 [30464/50176]	Loss: 0.3749
Training Epoch: 69 [30720/50176]	Loss: 0.4903
Training Epoch: 69 [30976/50176]	Loss: 0.4162
Training Epoch: 69 [31232/50176]	Loss: 0.4756
Training Epoch: 69 [31488/50176]	Loss: 0.3901
Training Epoch: 69 [31744/50176]	Loss: 0.3101
Training Epoch: 69 [32000/50176]	Loss: 0.3579
Training Epoch: 69 [32256/50176]	Loss: 0.4295
Training Epoch: 69 [32512/50176]	Loss: 0.3459
Training Epoch: 69 [32768/50176]	Loss: 0.4153
Training Epoch: 69 [33024/50176]	Loss: 0.3971
Training Epoch: 69 [33280/50176]	Loss: 0.4142
Training Epoch: 69 [33536/50176]	Loss: 0.4100
Training Epoch: 69 [33792/50176]	Loss: 0.3916
Training Epoch: 69 [34048/50176]	Loss: 0.4535
Training Epoch: 69 [34304/50176]	Loss: 0.2970
Training Epoch: 69 [34560/50176]	Loss: 0.5369
Training Epoch: 69 [34816/50176]	Loss: 0.3872
Training Epoch: 69 [35072/50176]	Loss: 0.3439
Training Epoch: 69 [35328/50176]	Loss: 0.3951
Training Epoch: 69 [35584/50176]	Loss: 0.3187
Training Epoch: 69 [35840/50176]	Loss: 0.4280
Training Epoch: 69 [36096/50176]	Loss: 0.4325
Training Epoch: 69 [36352/50176]	Loss: 0.4629
Training Epoch: 69 [36608/50176]	Loss: 0.3699
Training Epoch: 69 [36864/50176]	Loss: 0.4236
Training Epoch: 69 [37120/50176]	Loss: 0.4935
Training Epoch: 69 [37376/50176]	Loss: 0.2988
Training Epoch: 69 [37632/50176]	Loss: 0.3917
Training Epoch: 69 [37888/50176]	Loss: 0.3910
Training Epoch: 69 [38144/50176]	Loss: 0.5092
Training Epoch: 69 [38400/50176]	Loss: 0.3748
Training Epoch: 69 [38656/50176]	Loss: 0.3850
Training Epoch: 69 [38912/50176]	Loss: 0.4060
Training Epoch: 69 [39168/50176]	Loss: 0.5869
Training Epoch: 69 [39424/50176]	Loss: 0.3610
Training Epoch: 69 [39680/50176]	Loss: 0.4347
Training Epoch: 69 [39936/50176]	Loss: 0.3614
Training Epoch: 69 [40192/50176]	Loss: 0.2870
Training Epoch: 69 [40448/50176]	Loss: 0.3320
Training Epoch: 69 [40704/50176]	Loss: 0.3833
Training Epoch: 69 [40960/50176]	Loss: 0.4011
Training Epoch: 69 [41216/50176]	Loss: 0.3741
Training Epoch: 69 [41472/50176]	Loss: 0.3769
Training Epoch: 69 [41728/50176]	Loss: 0.4289
Training Epoch: 69 [41984/50176]	Loss: 0.3249
Training Epoch: 69 [42240/50176]	Loss: 0.5524
Training Epoch: 69 [42496/50176]	Loss: 0.3301
Training Epoch: 69 [42752/50176]	Loss: 0.3103
Training Epoch: 69 [43008/50176]	Loss: 0.3700
Training Epoch: 69 [43264/50176]	Loss: 0.3794
Training Epoch: 69 [43520/50176]	Loss: 0.4829
Training Epoch: 69 [43776/50176]	Loss: 0.6081
Training Epoch: 69 [44032/50176]	Loss: 0.4893
Training Epoch: 69 [44288/50176]	Loss: 0.4457
Training Epoch: 69 [44544/50176]	Loss: 0.4344
Training Epoch: 69 [44800/50176]	Loss: 0.3776
Training Epoch: 69 [45056/50176]	Loss: 0.4092
Training Epoch: 69 [45312/50176]	Loss: 0.4775
Training Epoch: 69 [45568/50176]	Loss: 0.4678
Training Epoch: 69 [45824/50176]	Loss: 0.4237
Training Epoch: 69 [46080/50176]	Loss: 0.4418
Training Epoch: 69 [46336/50176]	Loss: 0.3574
Training Epoch: 69 [46592/50176]	Loss: 0.4907
Training Epoch: 69 [46848/50176]	Loss: 0.3440
Training Epoch: 69 [47104/50176]	Loss: 0.3378
Training Epoch: 69 [47360/50176]	Loss: 0.3633
Training Epoch: 69 [47616/50176]	Loss: 0.4988
Training Epoch: 69 [47872/50176]	Loss: 0.3243
Training Epoch: 69 [48128/50176]	Loss: 0.4255
Training Epoch: 69 [48384/50176]	Loss: 0.3966
Training Epoch: 69 [48640/50176]	Loss: 0.4706
Training Epoch: 69 [48896/50176]	Loss: 0.3904
Training Epoch: 69 [49152/50176]	Loss: 0.4897
Training Epoch: 69 [49408/50176]	Loss: 0.4658
Training Epoch: 69 [49664/50176]	Loss: 0.4324
Training Epoch: 69 [49920/50176]	Loss: 0.4448
Training Epoch: 69 [50176/50176]	Loss: 0.4542
Validation Epoch: 69, Average loss: 0.0105, Accuracy: 0.5534
Training Epoch: 70 [256/50176]	Loss: 0.3475
Training Epoch: 70 [512/50176]	Loss: 0.3576
Training Epoch: 70 [768/50176]	Loss: 0.3877
Training Epoch: 70 [1024/50176]	Loss: 0.4394
Training Epoch: 70 [1280/50176]	Loss: 0.3584
Training Epoch: 70 [1536/50176]	Loss: 0.3570
Training Epoch: 70 [1792/50176]	Loss: 0.3966
Training Epoch: 70 [2048/50176]	Loss: 0.3692
Training Epoch: 70 [2304/50176]	Loss: 0.3369
Training Epoch: 70 [2560/50176]	Loss: 0.3502
Training Epoch: 70 [2816/50176]	Loss: 0.4117
Training Epoch: 70 [3072/50176]	Loss: 0.3356
Training Epoch: 70 [3328/50176]	Loss: 0.2937
Training Epoch: 70 [3584/50176]	Loss: 0.3932
Training Epoch: 70 [3840/50176]	Loss: 0.3982
Training Epoch: 70 [4096/50176]	Loss: 0.4179
Training Epoch: 70 [4352/50176]	Loss: 0.3892
Training Epoch: 70 [4608/50176]	Loss: 0.4105
Training Epoch: 70 [4864/50176]	Loss: 0.3939
Training Epoch: 70 [5120/50176]	Loss: 0.3379
Training Epoch: 70 [5376/50176]	Loss: 0.2893
Training Epoch: 70 [5632/50176]	Loss: 0.3521
Training Epoch: 70 [5888/50176]	Loss: 0.3501
Training Epoch: 70 [6144/50176]	Loss: 0.4480
Training Epoch: 70 [6400/50176]	Loss: 0.3851
Training Epoch: 70 [6656/50176]	Loss: 0.4560
Training Epoch: 70 [6912/50176]	Loss: 0.3413
Training Epoch: 70 [7168/50176]	Loss: 0.3785
Training Epoch: 70 [7424/50176]	Loss: 0.3753
Training Epoch: 70 [7680/50176]	Loss: 0.3327
Training Epoch: 70 [7936/50176]	Loss: 0.4193
Training Epoch: 70 [8192/50176]	Loss: 0.5395
Training Epoch: 70 [8448/50176]	Loss: 0.3509
Training Epoch: 70 [8704/50176]	Loss: 0.2964
Training Epoch: 70 [8960/50176]	Loss: 0.4539
Training Epoch: 70 [9216/50176]	Loss: 0.3107
Training Epoch: 70 [9472/50176]	Loss: 0.3298
Training Epoch: 70 [9728/50176]	Loss: 0.3769
Training Epoch: 70 [9984/50176]	Loss: 0.3796
Training Epoch: 70 [10240/50176]	Loss: 0.3470
Training Epoch: 70 [10496/50176]	Loss: 0.2601
Training Epoch: 70 [10752/50176]	Loss: 0.4447
Training Epoch: 70 [11008/50176]	Loss: 0.3317
Training Epoch: 70 [11264/50176]	Loss: 0.4056
Training Epoch: 70 [11520/50176]	Loss: 0.3917
Training Epoch: 70 [11776/50176]	Loss: 0.3350
Training Epoch: 70 [12032/50176]	Loss: 0.3620
Training Epoch: 70 [12288/50176]	Loss: 0.2773
Training Epoch: 70 [12544/50176]	Loss: 0.3627
Training Epoch: 70 [12800/50176]	Loss: 0.3582
Training Epoch: 70 [13056/50176]	Loss: 0.3328
Training Epoch: 70 [13312/50176]	Loss: 0.3658
Training Epoch: 70 [13568/50176]	Loss: 0.4014
Training Epoch: 70 [13824/50176]	Loss: 0.4180
Training Epoch: 70 [14080/50176]	Loss: 0.3470
Training Epoch: 70 [14336/50176]	Loss: 0.4009
Training Epoch: 70 [14592/50176]	Loss: 0.3273
Training Epoch: 70 [14848/50176]	Loss: 0.3491
Training Epoch: 70 [15104/50176]	Loss: 0.4173
Training Epoch: 70 [15360/50176]	Loss: 0.3140
Training Epoch: 70 [15616/50176]	Loss: 0.5037
Training Epoch: 70 [15872/50176]	Loss: 0.3328
Training Epoch: 70 [16128/50176]	Loss: 0.4163
Training Epoch: 70 [16384/50176]	Loss: 0.3619
Training Epoch: 70 [16640/50176]	Loss: 0.4747
Training Epoch: 70 [16896/50176]	Loss: 0.4757
Training Epoch: 70 [17152/50176]	Loss: 0.3582
Training Epoch: 70 [17408/50176]	Loss: 0.1919
Training Epoch: 70 [17664/50176]	Loss: 0.4841
Training Epoch: 70 [17920/50176]	Loss: 0.2987
Training Epoch: 70 [18176/50176]	Loss: 0.2932
Training Epoch: 70 [18432/50176]	Loss: 0.4153
Training Epoch: 70 [18688/50176]	Loss: 0.4588
Training Epoch: 70 [18944/50176]	Loss: 0.3648
Training Epoch: 70 [19200/50176]	Loss: 0.3285
Training Epoch: 70 [19456/50176]	Loss: 0.3210
Training Epoch: 70 [19712/50176]	Loss: 0.3601
Training Epoch: 70 [19968/50176]	Loss: 0.4253
Training Epoch: 70 [20224/50176]	Loss: 0.4398
Training Epoch: 70 [20480/50176]	Loss: 0.4363
Training Epoch: 70 [20736/50176]	Loss: 0.2982
Training Epoch: 70 [20992/50176]	Loss: 0.3484
Training Epoch: 70 [21248/50176]	Loss: 0.4134
Training Epoch: 70 [21504/50176]	Loss: 0.3739
Training Epoch: 70 [21760/50176]	Loss: 0.3774
Training Epoch: 70 [22016/50176]	Loss: 0.3074
Training Epoch: 70 [22272/50176]	Loss: 0.3768
Training Epoch: 70 [22528/50176]	Loss: 0.4515
Training Epoch: 70 [22784/50176]	Loss: 0.3254
Training Epoch: 70 [23040/50176]	Loss: 0.3925
Training Epoch: 70 [23296/50176]	Loss: 0.4168
Training Epoch: 70 [23552/50176]	Loss: 0.3867
Training Epoch: 70 [23808/50176]	Loss: 0.3351
Training Epoch: 70 [24064/50176]	Loss: 0.3751
Training Epoch: 70 [24320/50176]	Loss: 0.3704
Training Epoch: 70 [24576/50176]	Loss: 0.4274
Training Epoch: 70 [24832/50176]	Loss: 0.2853
Training Epoch: 70 [25088/50176]	Loss: 0.3729
Training Epoch: 70 [25344/50176]	Loss: 0.3219
Training Epoch: 70 [25600/50176]	Loss: 0.3736
Training Epoch: 70 [25856/50176]	Loss: 0.3486
Training Epoch: 70 [26112/50176]	Loss: 0.4570
Training Epoch: 70 [26368/50176]	Loss: 0.4118
Training Epoch: 70 [26624/50176]	Loss: 0.4503
Training Epoch: 70 [26880/50176]	Loss: 0.5719
Training Epoch: 70 [27136/50176]	Loss: 0.3183
Training Epoch: 70 [27392/50176]	Loss: 0.3564
Training Epoch: 70 [27648/50176]	Loss: 0.3680
Training Epoch: 70 [27904/50176]	Loss: 0.2808
Training Epoch: 70 [28160/50176]	Loss: 0.4157
Training Epoch: 70 [28416/50176]	Loss: 0.3670
Training Epoch: 70 [28672/50176]	Loss: 0.3697
Training Epoch: 70 [28928/50176]	Loss: 0.3610
Training Epoch: 70 [29184/50176]	Loss: 0.3635
Training Epoch: 70 [29440/50176]	Loss: 0.3908
Training Epoch: 70 [29696/50176]	Loss: 0.4639
Training Epoch: 70 [29952/50176]	Loss: 0.3913
Training Epoch: 70 [30208/50176]	Loss: 0.3374
Training Epoch: 70 [30464/50176]	Loss: 0.4085
Training Epoch: 70 [30720/50176]	Loss: 0.4028
Training Epoch: 70 [30976/50176]	Loss: 0.4188
Training Epoch: 70 [31232/50176]	Loss: 0.3670
Training Epoch: 70 [31488/50176]	Loss: 0.3988
Training Epoch: 70 [31744/50176]	Loss: 0.3972
Training Epoch: 70 [32000/50176]	Loss: 0.3789
Training Epoch: 70 [32256/50176]	Loss: 0.4566
Training Epoch: 70 [32512/50176]	Loss: 0.3459
Training Epoch: 70 [32768/50176]	Loss: 0.3612
Training Epoch: 70 [33024/50176]	Loss: 0.2512
Training Epoch: 70 [33280/50176]	Loss: 0.4206
Training Epoch: 70 [33536/50176]	Loss: 0.4018
Training Epoch: 70 [33792/50176]	Loss: 0.4361
Training Epoch: 70 [34048/50176]	Loss: 0.5668
Training Epoch: 70 [34304/50176]	Loss: 0.3533
Training Epoch: 70 [34560/50176]	Loss: 0.4844
Training Epoch: 70 [34816/50176]	Loss: 0.3489
Training Epoch: 70 [35072/50176]	Loss: 0.3311
Training Epoch: 70 [35328/50176]	Loss: 0.3235
Training Epoch: 70 [35584/50176]	Loss: 0.3618
Training Epoch: 70 [35840/50176]	Loss: 0.3606
Training Epoch: 70 [36096/50176]	Loss: 0.4471
Training Epoch: 70 [36352/50176]	Loss: 0.3404
Training Epoch: 70 [36608/50176]	Loss: 0.5501
Training Epoch: 70 [36864/50176]	Loss: 0.3733
Training Epoch: 70 [37120/50176]	Loss: 0.5423
Training Epoch: 70 [37376/50176]	Loss: 0.4123
Training Epoch: 70 [37632/50176]	Loss: 0.4140
Training Epoch: 70 [37888/50176]	Loss: 0.5020
Training Epoch: 70 [38144/50176]	Loss: 0.4219
Training Epoch: 70 [38400/50176]	Loss: 0.4428
Training Epoch: 70 [38656/50176]	Loss: 0.4180
Training Epoch: 70 [38912/50176]	Loss: 0.3960
Training Epoch: 70 [39168/50176]	Loss: 0.5148
Training Epoch: 70 [39424/50176]	Loss: 0.3681
Training Epoch: 70 [39680/50176]	Loss: 0.4369
Training Epoch: 70 [39936/50176]	Loss: 0.3304
Training Epoch: 70 [40192/50176]	Loss: 0.3769
Training Epoch: 70 [40448/50176]	Loss: 0.4253
Training Epoch: 70 [40704/50176]	Loss: 0.3585
Training Epoch: 70 [40960/50176]	Loss: 0.3282
Training Epoch: 70 [41216/50176]	Loss: 0.4275
Training Epoch: 70 [41472/50176]	Loss: 0.5055
Training Epoch: 70 [41728/50176]	Loss: 0.4600
Training Epoch: 70 [41984/50176]	Loss: 0.3981
Training Epoch: 70 [42240/50176]	Loss: 0.4317
Training Epoch: 70 [42496/50176]	Loss: 0.3421
Training Epoch: 70 [42752/50176]	Loss: 0.4004
Training Epoch: 70 [43008/50176]	Loss: 0.4129
Training Epoch: 70 [43264/50176]	Loss: 0.3869
Training Epoch: 70 [43520/50176]	Loss: 0.4372
Training Epoch: 70 [43776/50176]	Loss: 0.3894
Training Epoch: 70 [44032/50176]	Loss: 0.4281
Training Epoch: 70 [44288/50176]	Loss: 0.3865
Training Epoch: 70 [44544/50176]	Loss: 0.3449
Training Epoch: 70 [44800/50176]	Loss: 0.3684
Training Epoch: 70 [45056/50176]	Loss: 0.3093
Training Epoch: 70 [45312/50176]	Loss: 0.3661
Training Epoch: 70 [45568/50176]	Loss: 0.4007
Training Epoch: 70 [45824/50176]	Loss: 0.3759
Training Epoch: 70 [46080/50176]	Loss: 0.4078
Training Epoch: 70 [46336/50176]	Loss: 0.3894
Training Epoch: 70 [46592/50176]	Loss: 0.3570
Training Epoch: 70 [46848/50176]	Loss: 0.3527
Training Epoch: 70 [47104/50176]	Loss: 0.4804
Training Epoch: 70 [47360/50176]	Loss: 0.4616
Training Epoch: 70 [47616/50176]	Loss: 0.3626
Training Epoch: 70 [47872/50176]	Loss: 0.4244
Training Epoch: 70 [48128/50176]	Loss: 0.3596
Training Epoch: 70 [48384/50176]	Loss: 0.3125
Training Epoch: 70 [48640/50176]	Loss: 0.3625
Training Epoch: 70 [48896/50176]	Loss: 0.3857
Training Epoch: 70 [49152/50176]	Loss: 0.3350
Training Epoch: 70 [49408/50176]	Loss: 0.4218
Training Epoch: 70 [49664/50176]	Loss: 0.3648
Training Epoch: 70 [49920/50176]	Loss: 0.5207
Training Epoch: 70 [50176/50176]	Loss: 0.6531
Validation Epoch: 70, Average loss: 0.0094, Accuracy: 0.5855
Training Epoch: 71 [256/50176]	Loss: 0.3670
Training Epoch: 71 [512/50176]	Loss: 0.4051
Training Epoch: 71 [768/50176]	Loss: 0.3486
Training Epoch: 71 [1024/50176]	Loss: 0.3402
Training Epoch: 71 [1280/50176]	Loss: 0.2528
Training Epoch: 71 [1536/50176]	Loss: 0.2456
Training Epoch: 71 [1792/50176]	Loss: 0.3136
Training Epoch: 71 [2048/50176]	Loss: 0.3943
Training Epoch: 71 [2304/50176]	Loss: 0.3639
Training Epoch: 71 [2560/50176]	Loss: 0.3399
Training Epoch: 71 [2816/50176]	Loss: 0.3265
Training Epoch: 71 [3072/50176]	Loss: 0.3448
Training Epoch: 71 [3328/50176]	Loss: 0.2455
Training Epoch: 71 [3584/50176]	Loss: 0.3654
Training Epoch: 71 [3840/50176]	Loss: 0.2854
Training Epoch: 71 [4096/50176]	Loss: 0.3578
Training Epoch: 71 [4352/50176]	Loss: 0.3445
Training Epoch: 71 [4608/50176]	Loss: 0.2806
Training Epoch: 71 [4864/50176]	Loss: 0.4294
Training Epoch: 71 [5120/50176]	Loss: 0.2632
Training Epoch: 71 [5376/50176]	Loss: 0.3476
Training Epoch: 71 [5632/50176]	Loss: 0.3653
Training Epoch: 71 [5888/50176]	Loss: 0.2827
Training Epoch: 71 [6144/50176]	Loss: 0.4024
Training Epoch: 71 [6400/50176]	Loss: 0.3986
Training Epoch: 71 [6656/50176]	Loss: 0.3276
Training Epoch: 71 [6912/50176]	Loss: 0.3082
Training Epoch: 71 [7168/50176]	Loss: 0.3426
Training Epoch: 71 [7424/50176]	Loss: 0.2923
Training Epoch: 71 [7680/50176]	Loss: 0.3598
Training Epoch: 71 [7936/50176]	Loss: 0.4074
Training Epoch: 71 [8192/50176]	Loss: 0.3184
Training Epoch: 71 [8448/50176]	Loss: 0.3459
Training Epoch: 71 [8704/50176]	Loss: 0.4035
Training Epoch: 71 [8960/50176]	Loss: 0.3982
Training Epoch: 71 [9216/50176]	Loss: 0.4011
Training Epoch: 71 [9472/50176]	Loss: 0.4432
Training Epoch: 71 [9728/50176]	Loss: 0.3633
Training Epoch: 71 [9984/50176]	Loss: 0.3824
Training Epoch: 71 [10240/50176]	Loss: 0.4179
Training Epoch: 71 [10496/50176]	Loss: 0.4118
Training Epoch: 71 [10752/50176]	Loss: 0.3478
Training Epoch: 71 [11008/50176]	Loss: 0.3339
Training Epoch: 71 [11264/50176]	Loss: 0.3219
Training Epoch: 71 [11520/50176]	Loss: 0.3146
Training Epoch: 71 [11776/50176]	Loss: 0.2836
Training Epoch: 71 [12032/50176]	Loss: 0.4267
Training Epoch: 71 [12288/50176]	Loss: 0.3876
Training Epoch: 71 [12544/50176]	Loss: 0.3977
Training Epoch: 71 [12800/50176]	Loss: 0.3344
Training Epoch: 71 [13056/50176]	Loss: 0.2340
Training Epoch: 71 [13312/50176]	Loss: 0.3552
Training Epoch: 71 [13568/50176]	Loss: 0.3552
Training Epoch: 71 [13824/50176]	Loss: 0.3049
Training Epoch: 71 [14080/50176]	Loss: 0.3123
Training Epoch: 71 [14336/50176]	Loss: 0.3841
Training Epoch: 71 [14592/50176]	Loss: 0.3538
Training Epoch: 71 [14848/50176]	Loss: 0.3450
Training Epoch: 71 [15104/50176]	Loss: 0.3645
Training Epoch: 71 [15360/50176]	Loss: 0.4485
Training Epoch: 71 [15616/50176]	Loss: 0.3439
Training Epoch: 71 [15872/50176]	Loss: 0.2651
Training Epoch: 71 [16128/50176]	Loss: 0.3396
Training Epoch: 71 [16384/50176]	Loss: 0.3420
Training Epoch: 71 [16640/50176]	Loss: 0.3453
Training Epoch: 71 [16896/50176]	Loss: 0.3621
Training Epoch: 71 [17152/50176]	Loss: 0.3801
Training Epoch: 71 [17408/50176]	Loss: 0.3777
Training Epoch: 71 [17664/50176]	Loss: 0.3492
Training Epoch: 71 [17920/50176]	Loss: 0.3493
Training Epoch: 71 [18176/50176]	Loss: 0.4085
Training Epoch: 71 [18432/50176]	Loss: 0.3684
Training Epoch: 71 [18688/50176]	Loss: 0.5142
Training Epoch: 71 [18944/50176]	Loss: 0.2551
Training Epoch: 71 [19200/50176]	Loss: 0.4154
Training Epoch: 71 [19456/50176]	Loss: 0.4678
Training Epoch: 71 [19712/50176]	Loss: 0.3426
Training Epoch: 71 [19968/50176]	Loss: 0.4856
Training Epoch: 71 [20224/50176]	Loss: 0.3567
Training Epoch: 71 [20480/50176]	Loss: 0.3083
Training Epoch: 71 [20736/50176]	Loss: 0.2789
Training Epoch: 71 [20992/50176]	Loss: 0.4238
Training Epoch: 71 [21248/50176]	Loss: 0.3042
Training Epoch: 71 [21504/50176]	Loss: 0.4043
Training Epoch: 71 [21760/50176]	Loss: 0.3918
Training Epoch: 71 [22016/50176]	Loss: 0.2773
Training Epoch: 71 [22272/50176]	Loss: 0.4401
Training Epoch: 71 [22528/50176]	Loss: 0.3473
Training Epoch: 71 [22784/50176]	Loss: 0.2796
Training Epoch: 71 [23040/50176]	Loss: 0.3794
Training Epoch: 71 [23296/50176]	Loss: 0.3557
Training Epoch: 71 [23552/50176]	Loss: 0.3300
Training Epoch: 71 [23808/50176]	Loss: 0.3698
Training Epoch: 71 [24064/50176]	Loss: 0.3912
Training Epoch: 71 [24320/50176]	Loss: 0.4118
Training Epoch: 71 [24576/50176]	Loss: 0.3551
Training Epoch: 71 [24832/50176]	Loss: 0.3122
Training Epoch: 71 [25088/50176]	Loss: 0.4057
Training Epoch: 71 [25344/50176]	Loss: 0.4417
Training Epoch: 71 [25600/50176]	Loss: 0.3271
Training Epoch: 71 [25856/50176]	Loss: 0.3455
Training Epoch: 71 [26112/50176]	Loss: 0.3438
Training Epoch: 71 [26368/50176]	Loss: 0.2655
Training Epoch: 71 [26624/50176]	Loss: 0.4424
Training Epoch: 71 [26880/50176]	Loss: 0.2670
Training Epoch: 71 [27136/50176]	Loss: 0.5640
Training Epoch: 71 [27392/50176]	Loss: 0.4771
Training Epoch: 71 [27648/50176]	Loss: 0.4158
Training Epoch: 71 [27904/50176]	Loss: 0.4672
Training Epoch: 71 [28160/50176]	Loss: 0.3785
Training Epoch: 71 [28416/50176]	Loss: 0.4085
Training Epoch: 71 [28672/50176]	Loss: 0.3770
Training Epoch: 71 [28928/50176]	Loss: 0.4633
Training Epoch: 71 [29184/50176]	Loss: 0.3289
Training Epoch: 71 [29440/50176]	Loss: 0.5065
Training Epoch: 71 [29696/50176]	Loss: 0.3034
Training Epoch: 71 [29952/50176]	Loss: 0.4033
Training Epoch: 71 [30208/50176]	Loss: 0.4192
Training Epoch: 71 [30464/50176]	Loss: 0.3946
Training Epoch: 71 [30720/50176]	Loss: 0.4123
Training Epoch: 71 [30976/50176]	Loss: 0.4887
Training Epoch: 71 [31232/50176]	Loss: 0.3687
Training Epoch: 71 [31488/50176]	Loss: 0.3819
Training Epoch: 71 [31744/50176]	Loss: 0.4472
Training Epoch: 71 [32000/50176]	Loss: 0.4109
Training Epoch: 71 [32256/50176]	Loss: 0.3844
Training Epoch: 71 [32512/50176]	Loss: 0.4329
Training Epoch: 71 [32768/50176]	Loss: 0.3644
Training Epoch: 71 [33024/50176]	Loss: 0.5538
Training Epoch: 71 [33280/50176]	Loss: 0.4460
Training Epoch: 71 [33536/50176]	Loss: 0.4056
Training Epoch: 71 [33792/50176]	Loss: 0.2974
Training Epoch: 71 [34048/50176]	Loss: 0.4129
Training Epoch: 71 [34304/50176]	Loss: 0.4206
Training Epoch: 71 [34560/50176]	Loss: 0.3382
Training Epoch: 71 [34816/50176]	Loss: 0.3486
Training Epoch: 71 [35072/50176]	Loss: 0.4389
Training Epoch: 71 [35328/50176]	Loss: 0.3610
Training Epoch: 71 [35584/50176]	Loss: 0.4464
Training Epoch: 71 [35840/50176]	Loss: 0.3753
Training Epoch: 71 [36096/50176]	Loss: 0.3203
Training Epoch: 71 [36352/50176]	Loss: 0.3904
Training Epoch: 71 [36608/50176]	Loss: 0.4397
Training Epoch: 71 [36864/50176]	Loss: 0.4050
Training Epoch: 71 [37120/50176]	Loss: 0.3021
Training Epoch: 71 [37376/50176]	Loss: 0.4409
Training Epoch: 71 [37632/50176]	Loss: 0.4189
Training Epoch: 71 [37888/50176]	Loss: 0.4359
Training Epoch: 71 [38144/50176]	Loss: 0.4390
Training Epoch: 71 [38400/50176]	Loss: 0.5417
Training Epoch: 71 [38656/50176]	Loss: 0.3655
Training Epoch: 71 [38912/50176]	Loss: 0.3867
Training Epoch: 71 [39168/50176]	Loss: 0.5136
Training Epoch: 71 [39424/50176]	Loss: 0.3652
Training Epoch: 71 [39680/50176]	Loss: 0.3966
Training Epoch: 71 [39936/50176]	Loss: 0.3088
Training Epoch: 71 [40192/50176]	Loss: 0.4735
Training Epoch: 71 [40448/50176]	Loss: 0.4882
Training Epoch: 71 [40704/50176]	Loss: 0.3741
Training Epoch: 71 [40960/50176]	Loss: 0.4500
Training Epoch: 71 [41216/50176]	Loss: 0.4116
Training Epoch: 71 [41472/50176]	Loss: 0.4340
Training Epoch: 71 [41728/50176]	Loss: 0.3441
Training Epoch: 71 [41984/50176]	Loss: 0.3764
Training Epoch: 71 [42240/50176]	Loss: 0.4130
Training Epoch: 71 [42496/50176]	Loss: 0.4208
Training Epoch: 71 [42752/50176]	Loss: 0.3189
Training Epoch: 71 [43008/50176]	Loss: 0.3397
Training Epoch: 71 [43264/50176]	Loss: 0.3635
Training Epoch: 71 [43520/50176]	Loss: 0.3879
Training Epoch: 71 [43776/50176]	Loss: 0.4341
Training Epoch: 71 [44032/50176]	Loss: 0.3713
Training Epoch: 71 [44288/50176]	Loss: 0.4714
Training Epoch: 71 [44544/50176]	Loss: 0.4569
Training Epoch: 71 [44800/50176]	Loss: 0.4214
Training Epoch: 71 [45056/50176]	Loss: 0.4179
Training Epoch: 71 [45312/50176]	Loss: 0.4696
Training Epoch: 71 [45568/50176]	Loss: 0.3040
Training Epoch: 71 [45824/50176]	Loss: 0.4276
Training Epoch: 71 [46080/50176]	Loss: 0.4689
Training Epoch: 71 [46336/50176]	Loss: 0.4873
Training Epoch: 71 [46592/50176]	Loss: 0.3986
Training Epoch: 71 [46848/50176]	Loss: 0.4532
Training Epoch: 71 [47104/50176]	Loss: 0.3982
Training Epoch: 71 [47360/50176]	Loss: 0.4059
Training Epoch: 71 [47616/50176]	Loss: 0.4264
Training Epoch: 71 [47872/50176]	Loss: 0.2541
Training Epoch: 71 [48128/50176]	Loss: 0.4553
Training Epoch: 71 [48384/50176]	Loss: 0.3785
Training Epoch: 71 [48640/50176]	Loss: 0.3420
Training Epoch: 71 [48896/50176]	Loss: 0.4035
Training Epoch: 71 [49152/50176]	Loss: 0.4044
Training Epoch: 71 [49408/50176]	Loss: 0.4957
Training Epoch: 71 [49664/50176]	Loss: 0.4228
Training Epoch: 71 [49920/50176]	Loss: 0.3887
Training Epoch: 71 [50176/50176]	Loss: 0.6351
Validation Epoch: 71, Average loss: 0.0098, Accuracy: 0.5713
Training Epoch: 72 [256/50176]	Loss: 0.3010
Training Epoch: 72 [512/50176]	Loss: 0.4206
Training Epoch: 72 [768/50176]	Loss: 0.2676
Training Epoch: 72 [1024/50176]	Loss: 0.3318
Training Epoch: 72 [1280/50176]	Loss: 0.2976
Training Epoch: 72 [1536/50176]	Loss: 0.3062
Training Epoch: 72 [1792/50176]	Loss: 0.3220
Training Epoch: 72 [2048/50176]	Loss: 0.4179
Training Epoch: 72 [2304/50176]	Loss: 0.3873
Training Epoch: 72 [2560/50176]	Loss: 0.3634
Training Epoch: 72 [2816/50176]	Loss: 0.3533
Training Epoch: 72 [3072/50176]	Loss: 0.3257
Training Epoch: 72 [3328/50176]	Loss: 0.4586
Training Epoch: 72 [3584/50176]	Loss: 0.3486
Training Epoch: 72 [3840/50176]	Loss: 0.2951
Training Epoch: 72 [4096/50176]	Loss: 0.3091
Training Epoch: 72 [4352/50176]	Loss: 0.3365
Training Epoch: 72 [4608/50176]	Loss: 0.3958
Training Epoch: 72 [4864/50176]	Loss: 0.3828
Training Epoch: 72 [5120/50176]	Loss: 0.4959
Training Epoch: 72 [5376/50176]	Loss: 0.3310
Training Epoch: 72 [5632/50176]	Loss: 0.3442
Training Epoch: 72 [5888/50176]	Loss: 0.3334
Training Epoch: 72 [6144/50176]	Loss: 0.3063
Training Epoch: 72 [6400/50176]	Loss: 0.3481
Training Epoch: 72 [6656/50176]	Loss: 0.3290
Training Epoch: 72 [6912/50176]	Loss: 0.3543
Training Epoch: 72 [7168/50176]	Loss: 0.3710
Training Epoch: 72 [7424/50176]	Loss: 0.3598
Training Epoch: 72 [7680/50176]	Loss: 0.3182
Training Epoch: 72 [7936/50176]	Loss: 0.3427
Training Epoch: 72 [8192/50176]	Loss: 0.2939
Training Epoch: 72 [8448/50176]	Loss: 0.3881
Training Epoch: 72 [8704/50176]	Loss: 0.4052
Training Epoch: 72 [8960/50176]	Loss: 0.2930
Training Epoch: 72 [9216/50176]	Loss: 0.4107
Training Epoch: 72 [9472/50176]	Loss: 0.3379
Training Epoch: 72 [9728/50176]	Loss: 0.3918
Training Epoch: 72 [9984/50176]	Loss: 0.3101
Training Epoch: 72 [10240/50176]	Loss: 0.3467
Training Epoch: 72 [10496/50176]	Loss: 0.5057
Training Epoch: 72 [10752/50176]	Loss: 0.3476
Training Epoch: 72 [11008/50176]	Loss: 0.5199
Training Epoch: 72 [11264/50176]	Loss: 0.3294
Training Epoch: 72 [11520/50176]	Loss: 0.3144
Training Epoch: 72 [11776/50176]	Loss: 0.3272
Training Epoch: 72 [12032/50176]	Loss: 0.2956
Training Epoch: 72 [12288/50176]	Loss: 0.3044
Training Epoch: 72 [12544/50176]	Loss: 0.4440
Training Epoch: 72 [12800/50176]	Loss: 0.3517
Training Epoch: 72 [13056/50176]	Loss: 0.3929
Training Epoch: 72 [13312/50176]	Loss: 0.4373
Training Epoch: 72 [13568/50176]	Loss: 0.4045
Training Epoch: 72 [13824/50176]	Loss: 0.3749
Training Epoch: 72 [14080/50176]	Loss: 0.3106
Training Epoch: 72 [14336/50176]	Loss: 0.2650
Training Epoch: 72 [14592/50176]	Loss: 0.3969
Training Epoch: 72 [14848/50176]	Loss: 0.3591
Training Epoch: 72 [15104/50176]	Loss: 0.3450
Training Epoch: 72 [15360/50176]	Loss: 0.4967
Training Epoch: 72 [15616/50176]	Loss: 0.4116
Training Epoch: 72 [15872/50176]	Loss: 0.3327
Training Epoch: 72 [16128/50176]	Loss: 0.3979
Training Epoch: 72 [16384/50176]	Loss: 0.3614
Training Epoch: 72 [16640/50176]	Loss: 0.3390
Training Epoch: 72 [16896/50176]	Loss: 0.4175
Training Epoch: 72 [17152/50176]	Loss: 0.3436
Training Epoch: 72 [17408/50176]	Loss: 0.3668
Training Epoch: 72 [17664/50176]	Loss: 0.3329
Training Epoch: 72 [17920/50176]	Loss: 0.3488
Training Epoch: 72 [18176/50176]	Loss: 0.3580
Training Epoch: 72 [18432/50176]	Loss: 0.3569
Training Epoch: 72 [18688/50176]	Loss: 0.3533
Training Epoch: 72 [18944/50176]	Loss: 0.4120
Training Epoch: 72 [19200/50176]	Loss: 0.3511
Training Epoch: 72 [19456/50176]	Loss: 0.4045
Training Epoch: 72 [19712/50176]	Loss: 0.4195
Training Epoch: 72 [19968/50176]	Loss: 0.3558
Training Epoch: 72 [20224/50176]	Loss: 0.5133
Training Epoch: 72 [20480/50176]	Loss: 0.4189
Training Epoch: 72 [20736/50176]	Loss: 0.3047
Training Epoch: 72 [20992/50176]	Loss: 0.2945
Training Epoch: 72 [21248/50176]	Loss: 0.3140
Training Epoch: 72 [21504/50176]	Loss: 0.3258
Training Epoch: 72 [21760/50176]	Loss: 0.3246
Training Epoch: 72 [22016/50176]	Loss: 0.3559
Training Epoch: 72 [22272/50176]	Loss: 0.3710
Training Epoch: 72 [22528/50176]	Loss: 0.2739
Training Epoch: 72 [22784/50176]	Loss: 0.3551
Training Epoch: 72 [23040/50176]	Loss: 0.4353
Training Epoch: 72 [23296/50176]	Loss: 0.4154
Training Epoch: 72 [23552/50176]	Loss: 0.5140
Training Epoch: 72 [23808/50176]	Loss: 0.3583
Training Epoch: 72 [24064/50176]	Loss: 0.3985
Training Epoch: 72 [24320/50176]	Loss: 0.5469
Training Epoch: 72 [24576/50176]	Loss: 0.3818
Training Epoch: 72 [24832/50176]	Loss: 0.3332
Training Epoch: 72 [25088/50176]	Loss: 0.3850
Training Epoch: 72 [25344/50176]	Loss: 0.4305
Training Epoch: 72 [25600/50176]	Loss: 0.2981
Training Epoch: 72 [25856/50176]	Loss: 0.5630
Training Epoch: 72 [26112/50176]	Loss: 0.3155
Training Epoch: 72 [26368/50176]	Loss: 0.3908
Training Epoch: 72 [26624/50176]	Loss: 0.4350
Training Epoch: 72 [26880/50176]	Loss: 0.4100
Training Epoch: 72 [27136/50176]	Loss: 0.3133
Training Epoch: 72 [27392/50176]	Loss: 0.4585
Training Epoch: 72 [27648/50176]	Loss: 0.3214
Training Epoch: 72 [27904/50176]	Loss: 0.4511
Training Epoch: 72 [28160/50176]	Loss: 0.3254
Training Epoch: 72 [28416/50176]	Loss: 0.3928
Training Epoch: 72 [28672/50176]	Loss: 0.4638
Training Epoch: 72 [28928/50176]	Loss: 0.4919
Training Epoch: 72 [29184/50176]	Loss: 0.4204
Training Epoch: 72 [29440/50176]	Loss: 0.5348
Training Epoch: 72 [29696/50176]	Loss: 0.4119
Training Epoch: 72 [29952/50176]	Loss: 0.3625
Training Epoch: 72 [30208/50176]	Loss: 0.4127
Training Epoch: 72 [30464/50176]	Loss: 0.2735
Training Epoch: 72 [30720/50176]	Loss: 0.4458
Training Epoch: 72 [30976/50176]	Loss: 0.3518
Training Epoch: 72 [31232/50176]	Loss: 0.3444
Training Epoch: 72 [31488/50176]	Loss: 0.2796
Training Epoch: 72 [31744/50176]	Loss: 0.2627
Training Epoch: 72 [32000/50176]	Loss: 0.4312
Training Epoch: 72 [32256/50176]	Loss: 0.4200
Training Epoch: 72 [32512/50176]	Loss: 0.4007
Training Epoch: 72 [32768/50176]	Loss: 0.4584
Training Epoch: 72 [33024/50176]	Loss: 0.3352
Training Epoch: 72 [33280/50176]	Loss: 0.2881
Training Epoch: 72 [33536/50176]	Loss: 0.3817
Training Epoch: 72 [33792/50176]	Loss: 0.4682
Training Epoch: 72 [34048/50176]	Loss: 0.3895
Training Epoch: 72 [34304/50176]	Loss: 0.4417
Training Epoch: 72 [34560/50176]	Loss: 0.3001
Training Epoch: 72 [34816/50176]	Loss: 0.3930
Training Epoch: 72 [35072/50176]	Loss: 0.3252
Training Epoch: 72 [35328/50176]	Loss: 0.4325
Training Epoch: 72 [35584/50176]	Loss: 0.4586
Training Epoch: 72 [35840/50176]	Loss: 0.3768
Training Epoch: 72 [36096/50176]	Loss: 0.3919
Training Epoch: 72 [36352/50176]	Loss: 0.3079
Training Epoch: 72 [36608/50176]	Loss: 0.3348
Training Epoch: 72 [36864/50176]	Loss: 0.4715
Training Epoch: 72 [37120/50176]	Loss: 0.4184
Training Epoch: 72 [37376/50176]	Loss: 0.4097
Training Epoch: 72 [37632/50176]	Loss: 0.4388
Training Epoch: 72 [37888/50176]	Loss: 0.3626
Training Epoch: 72 [38144/50176]	Loss: 0.3512
Training Epoch: 72 [38400/50176]	Loss: 0.4501
Training Epoch: 72 [38656/50176]	Loss: 0.4538
Training Epoch: 72 [38912/50176]	Loss: 0.3700
Training Epoch: 72 [39168/50176]	Loss: 0.3955
Training Epoch: 72 [39424/50176]	Loss: 0.3691
Training Epoch: 72 [39680/50176]	Loss: 0.4361
Training Epoch: 72 [39936/50176]	Loss: 0.3812
Training Epoch: 72 [40192/50176]	Loss: 0.3577
Training Epoch: 72 [40448/50176]	Loss: 0.3899
Training Epoch: 72 [40704/50176]	Loss: 0.4170
Training Epoch: 72 [40960/50176]	Loss: 0.3754
Training Epoch: 72 [41216/50176]	Loss: 0.3992
Training Epoch: 72 [41472/50176]	Loss: 0.4464
Training Epoch: 72 [41728/50176]	Loss: 0.3753
Training Epoch: 72 [41984/50176]	Loss: 0.3670
Training Epoch: 72 [42240/50176]	Loss: 0.3939
Training Epoch: 72 [42496/50176]	Loss: 0.3670
Training Epoch: 72 [42752/50176]	Loss: 0.3174
Training Epoch: 72 [43008/50176]	Loss: 0.4284
Training Epoch: 72 [43264/50176]	Loss: 0.3506
Training Epoch: 72 [43520/50176]	Loss: 0.2559
Training Epoch: 72 [43776/50176]	Loss: 0.3973
Training Epoch: 72 [44032/50176]	Loss: 0.3633
Training Epoch: 72 [44288/50176]	Loss: 0.4231
Training Epoch: 72 [44544/50176]	Loss: 0.4465
Training Epoch: 72 [44800/50176]	Loss: 0.4958
Training Epoch: 72 [45056/50176]	Loss: 0.3361
Training Epoch: 72 [45312/50176]	Loss: 0.3257
Training Epoch: 72 [45568/50176]	Loss: 0.4340
Training Epoch: 72 [45824/50176]	Loss: 0.3583
Training Epoch: 72 [46080/50176]	Loss: 0.3074
Training Epoch: 72 [46336/50176]	Loss: 0.4153
Training Epoch: 72 [46592/50176]	Loss: 0.5189
Training Epoch: 72 [46848/50176]	Loss: 0.3656
Training Epoch: 72 [47104/50176]	Loss: 0.4617
Training Epoch: 72 [47360/50176]	Loss: 0.3607
Training Epoch: 72 [47616/50176]	Loss: 0.3085
Training Epoch: 72 [47872/50176]	Loss: 0.4561
Training Epoch: 72 [48128/50176]	Loss: 0.3580
Training Epoch: 72 [48384/50176]	Loss: 0.3617
Training Epoch: 72 [48640/50176]	Loss: 0.3371
Training Epoch: 72 [48896/50176]	Loss: 0.5638
Training Epoch: 72 [49152/50176]	Loss: 0.4319
Training Epoch: 72 [49408/50176]	Loss: 0.4829
Training Epoch: 72 [49664/50176]	Loss: 0.4509
Training Epoch: 72 [49920/50176]	Loss: 0.5129
Training Epoch: 72 [50176/50176]	Loss: 0.5019
Validation Epoch: 72, Average loss: 0.0105, Accuracy: 0.5530
Training Epoch: 73 [256/50176]	Loss: 0.3729
Training Epoch: 73 [512/50176]	Loss: 0.3462
Training Epoch: 73 [768/50176]	Loss: 0.3482
Training Epoch: 73 [1024/50176]	Loss: 0.3371
Training Epoch: 73 [1280/50176]	Loss: 0.3472
Training Epoch: 73 [1536/50176]	Loss: 0.3808
Training Epoch: 73 [1792/50176]	Loss: 0.3172
Training Epoch: 73 [2048/50176]	Loss: 0.3554
Training Epoch: 73 [2304/50176]	Loss: 0.3309
Training Epoch: 73 [2560/50176]	Loss: 0.3660
Training Epoch: 73 [2816/50176]	Loss: 0.3516
Training Epoch: 73 [3072/50176]	Loss: 0.3317
Training Epoch: 73 [3328/50176]	Loss: 0.3517
Training Epoch: 73 [3584/50176]	Loss: 0.4316
Training Epoch: 73 [3840/50176]	Loss: 0.3945
Training Epoch: 73 [4096/50176]	Loss: 0.4132
Training Epoch: 73 [4352/50176]	Loss: 0.2463
Training Epoch: 73 [4608/50176]	Loss: 0.4648
Training Epoch: 73 [4864/50176]	Loss: 0.3531
Training Epoch: 73 [5120/50176]	Loss: 0.3694
Training Epoch: 73 [5376/50176]	Loss: 0.2905
Training Epoch: 73 [5632/50176]	Loss: 0.3133
Training Epoch: 73 [5888/50176]	Loss: 0.3794
Training Epoch: 73 [6144/50176]	Loss: 0.3821
Training Epoch: 73 [6400/50176]	Loss: 0.3774
Training Epoch: 73 [6656/50176]	Loss: 0.2574
Training Epoch: 73 [6912/50176]	Loss: 0.4719
Training Epoch: 73 [7168/50176]	Loss: 0.5043
Training Epoch: 73 [7424/50176]	Loss: 0.3651
Training Epoch: 73 [7680/50176]	Loss: 0.3465
Training Epoch: 73 [7936/50176]	Loss: 0.3588
Training Epoch: 73 [8192/50176]	Loss: 0.3206
Training Epoch: 73 [8448/50176]	Loss: 0.3163
Training Epoch: 73 [8704/50176]	Loss: 0.3937
Training Epoch: 73 [8960/50176]	Loss: 0.3987
Training Epoch: 73 [9216/50176]	Loss: 0.3040
Training Epoch: 73 [9472/50176]	Loss: 0.3359
Training Epoch: 73 [9728/50176]	Loss: 0.4250
Training Epoch: 73 [9984/50176]	Loss: 0.3245
Training Epoch: 73 [10240/50176]	Loss: 0.3378
Training Epoch: 73 [10496/50176]	Loss: 0.3803
Training Epoch: 73 [10752/50176]	Loss: 0.2753
Training Epoch: 73 [11008/50176]	Loss: 0.4311
Training Epoch: 73 [11264/50176]	Loss: 0.3015
Training Epoch: 73 [11520/50176]	Loss: 0.3603
Training Epoch: 73 [11776/50176]	Loss: 0.4028
Training Epoch: 73 [12032/50176]	Loss: 0.3476
Training Epoch: 73 [12288/50176]	Loss: 0.3299
Training Epoch: 73 [12544/50176]	Loss: 0.4131
Training Epoch: 73 [12800/50176]	Loss: 0.3666
Training Epoch: 73 [13056/50176]	Loss: 0.3602
Training Epoch: 73 [13312/50176]	Loss: 0.2998
Training Epoch: 73 [13568/50176]	Loss: 0.4023
Training Epoch: 73 [13824/50176]	Loss: 0.3582
Training Epoch: 73 [14080/50176]	Loss: 0.3243
Training Epoch: 73 [14336/50176]	Loss: 0.2514
Training Epoch: 73 [14592/50176]	Loss: 0.3089
Training Epoch: 73 [14848/50176]	Loss: 0.4491
Training Epoch: 73 [15104/50176]	Loss: 0.3634
Training Epoch: 73 [15360/50176]	Loss: 0.4169
Training Epoch: 73 [15616/50176]	Loss: 0.2547
Training Epoch: 73 [15872/50176]	Loss: 0.3982
Training Epoch: 73 [16128/50176]	Loss: 0.2951
Training Epoch: 73 [16384/50176]	Loss: 0.4395
Training Epoch: 73 [16640/50176]	Loss: 0.4728
Training Epoch: 73 [16896/50176]	Loss: 0.3170
Training Epoch: 73 [17152/50176]	Loss: 0.4705
Training Epoch: 73 [17408/50176]	Loss: 0.3185
Training Epoch: 73 [17664/50176]	Loss: 0.3208
Training Epoch: 73 [17920/50176]	Loss: 0.3249
Training Epoch: 73 [18176/50176]	Loss: 0.3088
Training Epoch: 73 [18432/50176]	Loss: 0.3765
Training Epoch: 73 [18688/50176]	Loss: 0.3721
Training Epoch: 73 [18944/50176]	Loss: 0.3270
Training Epoch: 73 [19200/50176]	Loss: 0.3517
Training Epoch: 73 [19456/50176]	Loss: 0.3279
Training Epoch: 73 [19712/50176]	Loss: 0.4357
Training Epoch: 73 [19968/50176]	Loss: 0.3788
Training Epoch: 73 [20224/50176]	Loss: 0.3897
Training Epoch: 73 [20480/50176]	Loss: 0.3363
Training Epoch: 73 [20736/50176]	Loss: 0.3104
Training Epoch: 73 [20992/50176]	Loss: 0.4102
Training Epoch: 73 [21248/50176]	Loss: 0.3276
Training Epoch: 73 [21504/50176]	Loss: 0.3813
Training Epoch: 73 [21760/50176]	Loss: 0.3480
Training Epoch: 73 [22016/50176]	Loss: 0.3500
Training Epoch: 73 [22272/50176]	Loss: 0.3834
Training Epoch: 73 [22528/50176]	Loss: 0.3378
Training Epoch: 73 [22784/50176]	Loss: 0.2227
Training Epoch: 73 [23040/50176]	Loss: 0.3306
Training Epoch: 73 [23296/50176]	Loss: 0.3120
Training Epoch: 73 [23552/50176]	Loss: 0.4348
Training Epoch: 73 [23808/50176]	Loss: 0.3765
Training Epoch: 73 [24064/50176]	Loss: 0.3589
Training Epoch: 73 [24320/50176]	Loss: 0.4060
Training Epoch: 73 [24576/50176]	Loss: 0.2924
Training Epoch: 73 [24832/50176]	Loss: 0.2452
Training Epoch: 73 [25088/50176]	Loss: 0.2774
Training Epoch: 73 [25344/50176]	Loss: 0.3440
Training Epoch: 73 [25600/50176]	Loss: 0.4746
Training Epoch: 73 [25856/50176]	Loss: 0.3435
Training Epoch: 73 [26112/50176]	Loss: 0.4472
Training Epoch: 73 [26368/50176]	Loss: 0.4220
Training Epoch: 73 [26624/50176]	Loss: 0.4166
Training Epoch: 73 [26880/50176]	Loss: 0.3693
Training Epoch: 73 [27136/50176]	Loss: 0.3382
Training Epoch: 73 [27392/50176]	Loss: 0.4052
Training Epoch: 73 [27648/50176]	Loss: 0.3434
Training Epoch: 73 [27904/50176]	Loss: 0.3188
Training Epoch: 73 [28160/50176]	Loss: 0.3317
Training Epoch: 73 [28416/50176]	Loss: 0.3435
Training Epoch: 73 [28672/50176]	Loss: 0.4029
Training Epoch: 73 [28928/50176]	Loss: 0.3872
Training Epoch: 73 [29184/50176]	Loss: 0.3742
Training Epoch: 73 [29440/50176]	Loss: 0.3629
Training Epoch: 73 [29696/50176]	Loss: 0.3855
Training Epoch: 73 [29952/50176]	Loss: 0.3979
Training Epoch: 73 [30208/50176]	Loss: 0.3797
Training Epoch: 73 [30464/50176]	Loss: 0.3709
Training Epoch: 73 [30720/50176]	Loss: 0.4101
Training Epoch: 73 [30976/50176]	Loss: 0.4236
Training Epoch: 73 [31232/50176]	Loss: 0.3204
Training Epoch: 73 [31488/50176]	Loss: 0.3905
Training Epoch: 73 [31744/50176]	Loss: 0.2395
Training Epoch: 73 [32000/50176]	Loss: 0.3979
Training Epoch: 73 [32256/50176]	Loss: 0.3294
Training Epoch: 73 [32512/50176]	Loss: 0.3538
Training Epoch: 73 [32768/50176]	Loss: 0.4502
Training Epoch: 73 [33024/50176]	Loss: 0.3085
Training Epoch: 73 [33280/50176]	Loss: 0.4085
Training Epoch: 73 [33536/50176]	Loss: 0.3116
Training Epoch: 73 [33792/50176]	Loss: 0.3301
Training Epoch: 73 [34048/50176]	Loss: 0.3661
Training Epoch: 73 [34304/50176]	Loss: 0.3163
Training Epoch: 73 [34560/50176]	Loss: 0.3465
Training Epoch: 73 [34816/50176]	Loss: 0.4975
Training Epoch: 73 [35072/50176]	Loss: 0.3803
Training Epoch: 73 [35328/50176]	Loss: 0.2619
Training Epoch: 73 [35584/50176]	Loss: 0.4153
Training Epoch: 73 [35840/50176]	Loss: 0.4171
Training Epoch: 73 [36096/50176]	Loss: 0.3298
Training Epoch: 73 [36352/50176]	Loss: 0.3428
Training Epoch: 73 [36608/50176]	Loss: 0.4291
Training Epoch: 73 [36864/50176]	Loss: 0.3976
Training Epoch: 73 [37120/50176]	Loss: 0.3252
Training Epoch: 73 [37376/50176]	Loss: 0.3444
Training Epoch: 73 [37632/50176]	Loss: 0.3189
Training Epoch: 73 [37888/50176]	Loss: 0.3461
Training Epoch: 73 [38144/50176]	Loss: 0.3637
Training Epoch: 73 [38400/50176]	Loss: 0.3500
Training Epoch: 73 [38656/50176]	Loss: 0.2811
Training Epoch: 73 [38912/50176]	Loss: 0.3078
Training Epoch: 73 [39168/50176]	Loss: 0.5120
Training Epoch: 73 [39424/50176]	Loss: 0.4995
Training Epoch: 73 [39680/50176]	Loss: 0.4184
Training Epoch: 73 [39936/50176]	Loss: 0.4285
Training Epoch: 73 [40192/50176]	Loss: 0.4392
Training Epoch: 73 [40448/50176]	Loss: 0.4659
Training Epoch: 73 [40704/50176]	Loss: 0.3624
Training Epoch: 73 [40960/50176]	Loss: 0.4016
Training Epoch: 73 [41216/50176]	Loss: 0.3029
Training Epoch: 73 [41472/50176]	Loss: 0.4915
Training Epoch: 73 [41728/50176]	Loss: 0.3857
Training Epoch: 73 [41984/50176]	Loss: 0.3260
Training Epoch: 73 [42240/50176]	Loss: 0.4143
Training Epoch: 73 [42496/50176]	Loss: 0.3999
Training Epoch: 73 [42752/50176]	Loss: 0.4251
Training Epoch: 73 [43008/50176]	Loss: 0.2772
Training Epoch: 73 [43264/50176]	Loss: 0.4733
Training Epoch: 73 [43520/50176]	Loss: 0.4159
Training Epoch: 73 [43776/50176]	Loss: 0.4366
Training Epoch: 73 [44032/50176]	Loss: 0.4369
Training Epoch: 73 [44288/50176]	Loss: 0.4885
Training Epoch: 73 [44544/50176]	Loss: 0.5316
Training Epoch: 73 [44800/50176]	Loss: 0.4128
Training Epoch: 73 [45056/50176]	Loss: 0.3141
Training Epoch: 73 [45312/50176]	Loss: 0.4534
Training Epoch: 73 [45568/50176]	Loss: 0.4351
Training Epoch: 73 [45824/50176]	Loss: 0.4567
Training Epoch: 73 [46080/50176]	Loss: 0.4999
Training Epoch: 73 [46336/50176]	Loss: 0.3288
Training Epoch: 73 [46592/50176]	Loss: 0.5015
Training Epoch: 73 [46848/50176]	Loss: 0.3975
Training Epoch: 73 [47104/50176]	Loss: 0.3822
Training Epoch: 73 [47360/50176]	Loss: 0.3824
Training Epoch: 73 [47616/50176]	Loss: 0.4048
Training Epoch: 73 [47872/50176]	Loss: 0.4852
Training Epoch: 73 [48128/50176]	Loss: 0.4604
Training Epoch: 73 [48384/50176]	Loss: 0.3408
Training Epoch: 73 [48640/50176]	Loss: 0.4237
Training Epoch: 73 [48896/50176]	Loss: 0.3758
Training Epoch: 73 [49152/50176]	Loss: 0.3974
Training Epoch: 73 [49408/50176]	Loss: 0.4304
Training Epoch: 73 [49664/50176]	Loss: 0.3819
Training Epoch: 73 [49920/50176]	Loss: 0.4024
Training Epoch: 73 [50176/50176]	Loss: 0.3670
Validation Epoch: 73, Average loss: 0.0100, Accuracy: 0.5643
Training Epoch: 74 [256/50176]	Loss: 0.3328
Training Epoch: 74 [512/50176]	Loss: 0.3263
Training Epoch: 74 [768/50176]	Loss: 0.2562
Training Epoch: 74 [1024/50176]	Loss: 0.4155
Training Epoch: 74 [1280/50176]	Loss: 0.2960
Training Epoch: 74 [1536/50176]	Loss: 0.2981
Training Epoch: 74 [1792/50176]	Loss: 0.3935
Training Epoch: 74 [2048/50176]	Loss: 0.3133
Training Epoch: 74 [2304/50176]	Loss: 0.3413
Training Epoch: 74 [2560/50176]	Loss: 0.3344
Training Epoch: 74 [2816/50176]	Loss: 0.3245
Training Epoch: 74 [3072/50176]	Loss: 0.3352
Training Epoch: 74 [3328/50176]	Loss: 0.3376
Training Epoch: 74 [3584/50176]	Loss: 0.2202
Training Epoch: 74 [3840/50176]	Loss: 0.3278
Training Epoch: 74 [4096/50176]	Loss: 0.2582
Training Epoch: 74 [4352/50176]	Loss: 0.2685
Training Epoch: 74 [4608/50176]	Loss: 0.3911
Training Epoch: 74 [4864/50176]	Loss: 0.3414
Training Epoch: 74 [5120/50176]	Loss: 0.3171
Training Epoch: 74 [5376/50176]	Loss: 0.3855
Training Epoch: 74 [5632/50176]	Loss: 0.2531
Training Epoch: 74 [5888/50176]	Loss: 0.3753
Training Epoch: 74 [6144/50176]	Loss: 0.3308
Training Epoch: 74 [6400/50176]	Loss: 0.3564
Training Epoch: 74 [6656/50176]	Loss: 0.3640
Training Epoch: 74 [6912/50176]	Loss: 0.3213
Training Epoch: 74 [7168/50176]	Loss: 0.3195
Training Epoch: 74 [7424/50176]	Loss: 0.3236
Training Epoch: 74 [7680/50176]	Loss: 0.3287
Training Epoch: 74 [7936/50176]	Loss: 0.3160
Training Epoch: 74 [8192/50176]	Loss: 0.3379
Training Epoch: 74 [8448/50176]	Loss: 0.3379
Training Epoch: 74 [8704/50176]	Loss: 0.3594
Training Epoch: 74 [8960/50176]	Loss: 0.3950
Training Epoch: 74 [9216/50176]	Loss: 0.3440
Training Epoch: 74 [9472/50176]	Loss: 0.3711
Training Epoch: 74 [9728/50176]	Loss: 0.2805
Training Epoch: 74 [9984/50176]	Loss: 0.2981
Training Epoch: 74 [10240/50176]	Loss: 0.2911
Training Epoch: 74 [10496/50176]	Loss: 0.3630
Training Epoch: 74 [10752/50176]	Loss: 0.3172
Training Epoch: 74 [11008/50176]	Loss: 0.3693
Training Epoch: 74 [11264/50176]	Loss: 0.3343
Training Epoch: 74 [11520/50176]	Loss: 0.4373
Training Epoch: 74 [11776/50176]	Loss: 0.2882
Training Epoch: 74 [12032/50176]	Loss: 0.2988
Training Epoch: 74 [12288/50176]	Loss: 0.3887
Training Epoch: 74 [12544/50176]	Loss: 0.3293
Training Epoch: 74 [12800/50176]	Loss: 0.4279
Training Epoch: 74 [13056/50176]	Loss: 0.3996
Training Epoch: 74 [13312/50176]	Loss: 0.2084
Training Epoch: 74 [13568/50176]	Loss: 0.4290
Training Epoch: 74 [13824/50176]	Loss: 0.2900
Training Epoch: 74 [14080/50176]	Loss: 0.3347
Training Epoch: 74 [14336/50176]	Loss: 0.3689
Training Epoch: 74 [14592/50176]	Loss: 0.3475
Training Epoch: 74 [14848/50176]	Loss: 0.2754
Training Epoch: 74 [15104/50176]	Loss: 0.3544
Training Epoch: 74 [15360/50176]	Loss: 0.3466
Training Epoch: 74 [15616/50176]	Loss: 0.3274
Training Epoch: 74 [15872/50176]	Loss: 0.3009
Training Epoch: 74 [16128/50176]	Loss: 0.3784
Training Epoch: 74 [16384/50176]	Loss: 0.3078
Training Epoch: 74 [16640/50176]	Loss: 0.3582
Training Epoch: 74 [16896/50176]	Loss: 0.2733
Training Epoch: 74 [17152/50176]	Loss: 0.2958
Training Epoch: 74 [17408/50176]	Loss: 0.4607
Training Epoch: 74 [17664/50176]	Loss: 0.2680
Training Epoch: 74 [17920/50176]	Loss: 0.4172
Training Epoch: 74 [18176/50176]	Loss: 0.3478
Training Epoch: 74 [18432/50176]	Loss: 0.3034
Training Epoch: 74 [18688/50176]	Loss: 0.3880
Training Epoch: 74 [18944/50176]	Loss: 0.2997
Training Epoch: 74 [19200/50176]	Loss: 0.4010
Training Epoch: 74 [19456/50176]	Loss: 0.2675
Training Epoch: 74 [19712/50176]	Loss: 0.4148
Training Epoch: 74 [19968/50176]	Loss: 0.2753
Training Epoch: 74 [20224/50176]	Loss: 0.3844
Training Epoch: 74 [20480/50176]	Loss: 0.3514
Training Epoch: 74 [20736/50176]	Loss: 0.3639
Training Epoch: 74 [20992/50176]	Loss: 0.2751
Training Epoch: 74 [21248/50176]	Loss: 0.3523
Training Epoch: 74 [21504/50176]	Loss: 0.3455
Training Epoch: 74 [21760/50176]	Loss: 0.2727
Training Epoch: 74 [22016/50176]	Loss: 0.3644
Training Epoch: 74 [22272/50176]	Loss: 0.3343
Training Epoch: 74 [22528/50176]	Loss: 0.3288
Training Epoch: 74 [22784/50176]	Loss: 0.3242
Training Epoch: 74 [23040/50176]	Loss: 0.3508
Training Epoch: 74 [23296/50176]	Loss: 0.3146
Training Epoch: 74 [23552/50176]	Loss: 0.4124
Training Epoch: 74 [23808/50176]	Loss: 0.3712
Training Epoch: 74 [24064/50176]	Loss: 0.3248
Training Epoch: 74 [24320/50176]	Loss: 0.3892
Training Epoch: 74 [24576/50176]	Loss: 0.2667
Training Epoch: 74 [24832/50176]	Loss: 0.4000
Training Epoch: 74 [25088/50176]	Loss: 0.2858
Training Epoch: 74 [25344/50176]	Loss: 0.3975
Training Epoch: 74 [25600/50176]	Loss: 0.4244
Training Epoch: 74 [25856/50176]	Loss: 0.3915
Training Epoch: 74 [26112/50176]	Loss: 0.3753
Training Epoch: 74 [26368/50176]	Loss: 0.2634
Training Epoch: 74 [26624/50176]	Loss: 0.2759
Training Epoch: 74 [26880/50176]	Loss: 0.3030
Training Epoch: 74 [27136/50176]	Loss: 0.3363
Training Epoch: 74 [27392/50176]	Loss: 0.3628
Training Epoch: 74 [27648/50176]	Loss: 0.3163
Training Epoch: 74 [27904/50176]	Loss: 0.4296
Training Epoch: 74 [28160/50176]	Loss: 0.4337
Training Epoch: 74 [28416/50176]	Loss: 0.3735
Training Epoch: 74 [28672/50176]	Loss: 0.3159
Training Epoch: 74 [28928/50176]	Loss: 0.4110
Training Epoch: 74 [29184/50176]	Loss: 0.3723
Training Epoch: 74 [29440/50176]	Loss: 0.4251
Training Epoch: 74 [29696/50176]	Loss: 0.3918
Training Epoch: 74 [29952/50176]	Loss: 0.3042
Training Epoch: 74 [30208/50176]	Loss: 0.4690
Training Epoch: 74 [30464/50176]	Loss: 0.4016
Training Epoch: 74 [30720/50176]	Loss: 0.3126
Training Epoch: 74 [30976/50176]	Loss: 0.3490
Training Epoch: 74 [31232/50176]	Loss: 0.3051
Training Epoch: 74 [31488/50176]	Loss: 0.3770
Training Epoch: 74 [31744/50176]	Loss: 0.2867
Training Epoch: 74 [32000/50176]	Loss: 0.3316
Training Epoch: 74 [32256/50176]	Loss: 0.3368
Training Epoch: 74 [32512/50176]	Loss: 0.3167
Training Epoch: 74 [32768/50176]	Loss: 0.3389
Training Epoch: 74 [33024/50176]	Loss: 0.3180
Training Epoch: 74 [33280/50176]	Loss: 0.3371
Training Epoch: 74 [33536/50176]	Loss: 0.3884
Training Epoch: 74 [33792/50176]	Loss: 0.3351
Training Epoch: 74 [34048/50176]	Loss: 0.3728
Training Epoch: 74 [34304/50176]	Loss: 0.3184
Training Epoch: 74 [34560/50176]	Loss: 0.3758
Training Epoch: 74 [34816/50176]	Loss: 0.3954
Training Epoch: 74 [35072/50176]	Loss: 0.3312
Training Epoch: 74 [35328/50176]	Loss: 0.3706
Training Epoch: 74 [35584/50176]	Loss: 0.3453
Training Epoch: 74 [35840/50176]	Loss: 0.3850
Training Epoch: 74 [36096/50176]	Loss: 0.3368
Training Epoch: 74 [36352/50176]	Loss: 0.3293
Training Epoch: 74 [36608/50176]	Loss: 0.3789
Training Epoch: 74 [36864/50176]	Loss: 0.3636
Training Epoch: 74 [37120/50176]	Loss: 0.6062
Training Epoch: 74 [37376/50176]	Loss: 0.3146
Training Epoch: 74 [37632/50176]	Loss: 0.3847
Training Epoch: 74 [37888/50176]	Loss: 0.3558
Training Epoch: 74 [38144/50176]	Loss: 0.4459
Training Epoch: 74 [38400/50176]	Loss: 0.4175
Training Epoch: 74 [38656/50176]	Loss: 0.3442
Training Epoch: 74 [38912/50176]	Loss: 0.3452
Training Epoch: 74 [39168/50176]	Loss: 0.4272
Training Epoch: 74 [39424/50176]	Loss: 0.4578
Training Epoch: 74 [39680/50176]	Loss: 0.3898
Training Epoch: 74 [39936/50176]	Loss: 0.3049
Training Epoch: 74 [40192/50176]	Loss: 0.2625
Training Epoch: 74 [40448/50176]	Loss: 0.3035
Training Epoch: 74 [40704/50176]	Loss: 0.3491
Training Epoch: 74 [40960/50176]	Loss: 0.4305
Training Epoch: 74 [41216/50176]	Loss: 0.4276
Training Epoch: 74 [41472/50176]	Loss: 0.3342
Training Epoch: 74 [41728/50176]	Loss: 0.4044
Training Epoch: 74 [41984/50176]	Loss: 0.4514
Training Epoch: 74 [42240/50176]	Loss: 0.4139
Training Epoch: 74 [42496/50176]	Loss: 0.3924
Training Epoch: 74 [42752/50176]	Loss: 0.3967
Training Epoch: 74 [43008/50176]	Loss: 0.3099
Training Epoch: 74 [43264/50176]	Loss: 0.3729
Training Epoch: 74 [43520/50176]	Loss: 0.3869
Training Epoch: 74 [43776/50176]	Loss: 0.4456
Training Epoch: 74 [44032/50176]	Loss: 0.3893
Training Epoch: 74 [44288/50176]	Loss: 0.3315
Training Epoch: 74 [44544/50176]	Loss: 0.4190
Training Epoch: 74 [44800/50176]	Loss: 0.4027
Training Epoch: 74 [45056/50176]	Loss: 0.4369
Training Epoch: 74 [45312/50176]	Loss: 0.3262
Training Epoch: 74 [45568/50176]	Loss: 0.4603
Training Epoch: 74 [45824/50176]	Loss: 0.4131
Training Epoch: 74 [46080/50176]	Loss: 0.4518
Training Epoch: 74 [46336/50176]	Loss: 0.3862
Training Epoch: 74 [46592/50176]	Loss: 0.3720
Training Epoch: 74 [46848/50176]	Loss: 0.4469
Training Epoch: 74 [47104/50176]	Loss: 0.4171
Training Epoch: 74 [47360/50176]	Loss: 0.3661
Training Epoch: 74 [47616/50176]	Loss: 0.4762
Training Epoch: 74 [47872/50176]	Loss: 0.4174
Training Epoch: 74 [48128/50176]	Loss: 0.3588
Training Epoch: 74 [48384/50176]	Loss: 0.4322
Training Epoch: 74 [48640/50176]	Loss: 0.4236
Training Epoch: 74 [48896/50176]	Loss: 0.4244
Training Epoch: 74 [49152/50176]	Loss: 0.2916
Training Epoch: 74 [49408/50176]	Loss: 0.3865
Training Epoch: 74 [49664/50176]	Loss: 0.5136
Training Epoch: 74 [49920/50176]	Loss: 0.4112
Training Epoch: 74 [50176/50176]	Loss: 0.6596
Validation Epoch: 74, Average loss: 0.0099, Accuracy: 0.5692
Training Epoch: 75 [256/50176]	Loss: 0.2725
Training Epoch: 75 [512/50176]	Loss: 0.3241
Training Epoch: 75 [768/50176]	Loss: 0.3264
Training Epoch: 75 [1024/50176]	Loss: 0.3179
Training Epoch: 75 [1280/50176]	Loss: 0.3154
Training Epoch: 75 [1536/50176]	Loss: 0.3225
Training Epoch: 75 [1792/50176]	Loss: 0.3706
Training Epoch: 75 [2048/50176]	Loss: 0.3663
Training Epoch: 75 [2304/50176]	Loss: 0.3434
Training Epoch: 75 [2560/50176]	Loss: 0.2857
Training Epoch: 75 [2816/50176]	Loss: 0.3336
Training Epoch: 75 [3072/50176]	Loss: 0.3480
Training Epoch: 75 [3328/50176]	Loss: 0.3463
Training Epoch: 75 [3584/50176]	Loss: 0.2801
Training Epoch: 75 [3840/50176]	Loss: 0.4022
Training Epoch: 75 [4096/50176]	Loss: 0.3673
Training Epoch: 75 [4352/50176]	Loss: 0.3095
Training Epoch: 75 [4608/50176]	Loss: 0.3492
Training Epoch: 75 [4864/50176]	Loss: 0.4913
Training Epoch: 75 [5120/50176]	Loss: 0.3681
Training Epoch: 75 [5376/50176]	Loss: 0.3547
Training Epoch: 75 [5632/50176]	Loss: 0.3417
Training Epoch: 75 [5888/50176]	Loss: 0.1831
Training Epoch: 75 [6144/50176]	Loss: 0.3314
Training Epoch: 75 [6400/50176]	Loss: 0.3506
Training Epoch: 75 [6656/50176]	Loss: 0.2508
Training Epoch: 75 [6912/50176]	Loss: 0.4193
Training Epoch: 75 [7168/50176]	Loss: 0.2958
Training Epoch: 75 [7424/50176]	Loss: 0.3646
Training Epoch: 75 [7680/50176]	Loss: 0.3342
Training Epoch: 75 [7936/50176]	Loss: 0.3003
Training Epoch: 75 [8192/50176]	Loss: 0.2705
Training Epoch: 75 [8448/50176]	Loss: 0.3666
Training Epoch: 75 [8704/50176]	Loss: 0.3453
Training Epoch: 75 [8960/50176]	Loss: 0.2684
Training Epoch: 75 [9216/50176]	Loss: 0.3567
Training Epoch: 75 [9472/50176]	Loss: 0.2863
Training Epoch: 75 [9728/50176]	Loss: 0.2776
Training Epoch: 75 [9984/50176]	Loss: 0.3848
Training Epoch: 75 [10240/50176]	Loss: 0.3135
Training Epoch: 75 [10496/50176]	Loss: 0.3800
Training Epoch: 75 [10752/50176]	Loss: 0.3883
Training Epoch: 75 [11008/50176]	Loss: 0.3801
Training Epoch: 75 [11264/50176]	Loss: 0.3275
Training Epoch: 75 [11520/50176]	Loss: 0.2297
Training Epoch: 75 [11776/50176]	Loss: 0.2222
Training Epoch: 75 [12032/50176]	Loss: 0.3674
Training Epoch: 75 [12288/50176]	Loss: 0.3457
Training Epoch: 75 [12544/50176]	Loss: 0.3500
Training Epoch: 75 [12800/50176]	Loss: 0.3504
Training Epoch: 75 [13056/50176]	Loss: 0.3743
Training Epoch: 75 [13312/50176]	Loss: 0.3540
Training Epoch: 75 [13568/50176]	Loss: 0.3181
Training Epoch: 75 [13824/50176]	Loss: 0.3511
Training Epoch: 75 [14080/50176]	Loss: 0.2975
Training Epoch: 75 [14336/50176]	Loss: 0.5096
Training Epoch: 75 [14592/50176]	Loss: 0.3548
Training Epoch: 75 [14848/50176]	Loss: 0.3308
Training Epoch: 75 [15104/50176]	Loss: 0.4202
Training Epoch: 75 [15360/50176]	Loss: 0.3347
Training Epoch: 75 [15616/50176]	Loss: 0.3497
Training Epoch: 75 [15872/50176]	Loss: 0.3480
Training Epoch: 75 [16128/50176]	Loss: 0.2733
Training Epoch: 75 [16384/50176]	Loss: 0.3181
Training Epoch: 75 [16640/50176]	Loss: 0.3246
Training Epoch: 75 [16896/50176]	Loss: 0.3556
Training Epoch: 75 [17152/50176]	Loss: 0.4686
Training Epoch: 75 [17408/50176]	Loss: 0.3742
Training Epoch: 75 [17664/50176]	Loss: 0.2897
Training Epoch: 75 [17920/50176]	Loss: 0.3688
Training Epoch: 75 [18176/50176]	Loss: 0.3056
Training Epoch: 75 [18432/50176]	Loss: 0.2685
Training Epoch: 75 [18688/50176]	Loss: 0.3038
Training Epoch: 75 [18944/50176]	Loss: 0.5319
Training Epoch: 75 [19200/50176]	Loss: 0.2577
Training Epoch: 75 [19456/50176]	Loss: 0.4243
Training Epoch: 75 [19712/50176]	Loss: 0.3773
Training Epoch: 75 [19968/50176]	Loss: 0.3730
Training Epoch: 75 [20224/50176]	Loss: 0.3412
Training Epoch: 75 [20480/50176]	Loss: 0.4266
Training Epoch: 75 [20736/50176]	Loss: 0.3752
Training Epoch: 75 [20992/50176]	Loss: 0.2949
Training Epoch: 75 [21248/50176]	Loss: 0.3002
Training Epoch: 75 [21504/50176]	Loss: 0.3782
Training Epoch: 75 [21760/50176]	Loss: 0.3093
Training Epoch: 75 [22016/50176]	Loss: 0.4179
Training Epoch: 75 [22272/50176]	Loss: 0.3676
Training Epoch: 75 [22528/50176]	Loss: 0.3439
Training Epoch: 75 [22784/50176]	Loss: 0.3383
Training Epoch: 75 [23040/50176]	Loss: 0.3139
Training Epoch: 75 [23296/50176]	Loss: 0.3763
Training Epoch: 75 [23552/50176]	Loss: 0.4060
Training Epoch: 75 [23808/50176]	Loss: 0.3896
Training Epoch: 75 [24064/50176]	Loss: 0.3423
Training Epoch: 75 [24320/50176]	Loss: 0.3312
Training Epoch: 75 [24576/50176]	Loss: 0.3456
Training Epoch: 75 [24832/50176]	Loss: 0.2729
Training Epoch: 75 [25088/50176]	Loss: 0.3119
Training Epoch: 75 [25344/50176]	Loss: 0.4058
Training Epoch: 75 [25600/50176]	Loss: 0.2796
Training Epoch: 75 [25856/50176]	Loss: 0.3522
Training Epoch: 75 [26112/50176]	Loss: 0.2866
Training Epoch: 75 [26368/50176]	Loss: 0.4282
Training Epoch: 75 [26624/50176]	Loss: 0.4518
Training Epoch: 75 [26880/50176]	Loss: 0.3980
Training Epoch: 75 [27136/50176]	Loss: 0.3223
Training Epoch: 75 [27392/50176]	Loss: 0.3965
Training Epoch: 75 [27648/50176]	Loss: 0.3269
Training Epoch: 75 [27904/50176]	Loss: 0.3805
Training Epoch: 75 [28160/50176]	Loss: 0.3084
Training Epoch: 75 [28416/50176]	Loss: 0.3008
Training Epoch: 75 [28672/50176]	Loss: 0.2994
Training Epoch: 75 [28928/50176]	Loss: 0.3795
Training Epoch: 75 [29184/50176]	Loss: 0.3155
Training Epoch: 75 [29440/50176]	Loss: 0.3914
Training Epoch: 75 [29696/50176]	Loss: 0.2737
Training Epoch: 75 [29952/50176]	Loss: 0.3961
Training Epoch: 75 [30208/50176]	Loss: 0.4033
Training Epoch: 75 [30464/50176]	Loss: 0.3907
Training Epoch: 75 [30720/50176]	Loss: 0.2420
Training Epoch: 75 [30976/50176]	Loss: 0.3525
Training Epoch: 75 [31232/50176]	Loss: 0.4703
Training Epoch: 75 [31488/50176]	Loss: 0.2999
Training Epoch: 75 [31744/50176]	Loss: 0.4433
Training Epoch: 75 [32000/50176]	Loss: 0.3260
Training Epoch: 75 [32256/50176]	Loss: 0.3846
Training Epoch: 75 [32512/50176]	Loss: 0.3407
Training Epoch: 75 [32768/50176]	Loss: 0.3758
Training Epoch: 75 [33024/50176]	Loss: 0.2844
Training Epoch: 75 [33280/50176]	Loss: 0.4099
Training Epoch: 75 [33536/50176]	Loss: 0.3048
Training Epoch: 75 [33792/50176]	Loss: 0.3274
Training Epoch: 75 [34048/50176]	Loss: 0.3557
Training Epoch: 75 [34304/50176]	Loss: 0.3663
Training Epoch: 75 [34560/50176]	Loss: 0.4428
Training Epoch: 75 [34816/50176]	Loss: 0.3832
Training Epoch: 75 [35072/50176]	Loss: 0.3815
Training Epoch: 75 [35328/50176]	Loss: 0.3334
Training Epoch: 75 [35584/50176]	Loss: 0.2829
Training Epoch: 75 [35840/50176]	Loss: 0.3266
Training Epoch: 75 [36096/50176]	Loss: 0.3831
Training Epoch: 75 [36352/50176]	Loss: 0.3813
Training Epoch: 75 [36608/50176]	Loss: 0.4644
Training Epoch: 75 [36864/50176]	Loss: 0.3752
Training Epoch: 75 [37120/50176]	Loss: 0.3522
Training Epoch: 75 [37376/50176]	Loss: 0.3967
Training Epoch: 75 [37632/50176]	Loss: 0.3517
Training Epoch: 75 [37888/50176]	Loss: 0.4137
Training Epoch: 75 [38144/50176]	Loss: 0.3009
Training Epoch: 75 [38400/50176]	Loss: 0.3087
Training Epoch: 75 [38656/50176]	Loss: 0.3231
Training Epoch: 75 [38912/50176]	Loss: 0.4149
Training Epoch: 75 [39168/50176]	Loss: 0.3889
Training Epoch: 75 [39424/50176]	Loss: 0.4054
Training Epoch: 75 [39680/50176]	Loss: 0.3746
Training Epoch: 75 [39936/50176]	Loss: 0.4805
Training Epoch: 75 [40192/50176]	Loss: 0.4218
Training Epoch: 75 [40448/50176]	Loss: 0.3732
Training Epoch: 75 [40704/50176]	Loss: 0.3479
Training Epoch: 75 [40960/50176]	Loss: 0.4154
Training Epoch: 75 [41216/50176]	Loss: 0.4011
Training Epoch: 75 [41472/50176]	Loss: 0.2767
Training Epoch: 75 [41728/50176]	Loss: 0.4182
Training Epoch: 75 [41984/50176]	Loss: 0.3820
Training Epoch: 75 [42240/50176]	Loss: 0.4905
Training Epoch: 75 [42496/50176]	Loss: 0.3478
Training Epoch: 75 [42752/50176]	Loss: 0.3280
Training Epoch: 75 [43008/50176]	Loss: 0.3605
Training Epoch: 75 [43264/50176]	Loss: 0.4205
Training Epoch: 75 [43520/50176]	Loss: 0.4724
Training Epoch: 75 [43776/50176]	Loss: 0.3771
Training Epoch: 75 [44032/50176]	Loss: 0.4602
Training Epoch: 75 [44288/50176]	Loss: 0.3002
Training Epoch: 75 [44544/50176]	Loss: 0.3016
Training Epoch: 75 [44800/50176]	Loss: 0.3280
Training Epoch: 75 [45056/50176]	Loss: 0.3296
Training Epoch: 75 [45312/50176]	Loss: 0.4438
Training Epoch: 75 [45568/50176]	Loss: 0.4116
Training Epoch: 75 [45824/50176]	Loss: 0.3716
Training Epoch: 75 [46080/50176]	Loss: 0.3802
Training Epoch: 75 [46336/50176]	Loss: 0.4260
Training Epoch: 75 [46592/50176]	Loss: 0.3363
Training Epoch: 75 [46848/50176]	Loss: 0.2604
Training Epoch: 75 [47104/50176]	Loss: 0.2865
Training Epoch: 75 [47360/50176]	Loss: 0.3438
Training Epoch: 75 [47616/50176]	Loss: 0.3222
Training Epoch: 75 [47872/50176]	Loss: 0.3877
Training Epoch: 75 [48128/50176]	Loss: 0.3973
Training Epoch: 75 [48384/50176]	Loss: 0.4846
Training Epoch: 75 [48640/50176]	Loss: 0.3296
Training Epoch: 75 [48896/50176]	Loss: 0.4373
Training Epoch: 75 [49152/50176]	Loss: 0.3367
Training Epoch: 75 [49408/50176]	Loss: 0.3941
Training Epoch: 75 [49664/50176]	Loss: 0.3986
Training Epoch: 75 [49920/50176]	Loss: 0.3819
Training Epoch: 75 [50176/50176]	Loss: 0.5202
Validation Epoch: 75, Average loss: 0.0111, Accuracy: 0.5469
Training Epoch: 76 [256/50176]	Loss: 0.3168
Training Epoch: 76 [512/50176]	Loss: 0.3021
Training Epoch: 76 [768/50176]	Loss: 0.3416
Training Epoch: 76 [1024/50176]	Loss: 0.2567
Training Epoch: 76 [1280/50176]	Loss: 0.2654
Training Epoch: 76 [1536/50176]	Loss: 0.4124
Training Epoch: 76 [1792/50176]	Loss: 0.3565
Training Epoch: 76 [2048/50176]	Loss: 0.4320
Training Epoch: 76 [2304/50176]	Loss: 0.3329
Training Epoch: 76 [2560/50176]	Loss: 0.4004
Training Epoch: 76 [2816/50176]	Loss: 0.3653
Training Epoch: 76 [3072/50176]	Loss: 0.2632
Training Epoch: 76 [3328/50176]	Loss: 0.2786
Training Epoch: 76 [3584/50176]	Loss: 0.2321
Training Epoch: 76 [3840/50176]	Loss: 0.5127
Training Epoch: 76 [4096/50176]	Loss: 0.3023
Training Epoch: 76 [4352/50176]	Loss: 0.3037
Training Epoch: 76 [4608/50176]	Loss: 0.3202
Training Epoch: 76 [4864/50176]	Loss: 0.2544
Training Epoch: 76 [5120/50176]	Loss: 0.3120
Training Epoch: 76 [5376/50176]	Loss: 0.2569
Training Epoch: 76 [5632/50176]	Loss: 0.2906
Training Epoch: 76 [5888/50176]	Loss: 0.4140
Training Epoch: 76 [6144/50176]	Loss: 0.2955
Training Epoch: 76 [6400/50176]	Loss: 0.3741
Training Epoch: 76 [6656/50176]	Loss: 0.4690
Training Epoch: 76 [6912/50176]	Loss: 0.3944
Training Epoch: 76 [7168/50176]	Loss: 0.3226
Training Epoch: 76 [7424/50176]	Loss: 0.3780
Training Epoch: 76 [7680/50176]	Loss: 0.3003
Training Epoch: 76 [7936/50176]	Loss: 0.3369
Training Epoch: 76 [8192/50176]	Loss: 0.4248
Training Epoch: 76 [8448/50176]	Loss: 0.3377
Training Epoch: 76 [8704/50176]	Loss: 0.4817
Training Epoch: 76 [8960/50176]	Loss: 0.3533
Training Epoch: 76 [9216/50176]	Loss: 0.3252
Training Epoch: 76 [9472/50176]	Loss: 0.3399
Training Epoch: 76 [9728/50176]	Loss: 0.2735
Training Epoch: 76 [9984/50176]	Loss: 0.3652
Training Epoch: 76 [10240/50176]	Loss: 0.3283
Training Epoch: 76 [10496/50176]	Loss: 0.3618
Training Epoch: 76 [10752/50176]	Loss: 0.4731
Training Epoch: 76 [11008/50176]	Loss: 0.3087
Training Epoch: 76 [11264/50176]	Loss: 0.3611
Training Epoch: 76 [11520/50176]	Loss: 0.3353
Training Epoch: 76 [11776/50176]	Loss: 0.3280
Training Epoch: 76 [12032/50176]	Loss: 0.2318
Training Epoch: 76 [12288/50176]	Loss: 0.3136
Training Epoch: 76 [12544/50176]	Loss: 0.3606
Training Epoch: 76 [12800/50176]	Loss: 0.3599
Training Epoch: 76 [13056/50176]	Loss: 0.3455
Training Epoch: 76 [13312/50176]	Loss: 0.3496
Training Epoch: 76 [13568/50176]	Loss: 0.2819
Training Epoch: 76 [13824/50176]	Loss: 0.2999
Training Epoch: 76 [14080/50176]	Loss: 0.3523
Training Epoch: 76 [14336/50176]	Loss: 0.3572
Training Epoch: 76 [14592/50176]	Loss: 0.3430
Training Epoch: 76 [14848/50176]	Loss: 0.3518
Training Epoch: 76 [15104/50176]	Loss: 0.3275
Training Epoch: 76 [15360/50176]	Loss: 0.3963
Training Epoch: 76 [15616/50176]	Loss: 0.3452
Training Epoch: 76 [15872/50176]	Loss: 0.2786
Training Epoch: 76 [16128/50176]	Loss: 0.3593
Training Epoch: 76 [16384/50176]	Loss: 0.1801
Training Epoch: 76 [16640/50176]	Loss: 0.3425
Training Epoch: 76 [16896/50176]	Loss: 0.3360
Training Epoch: 76 [17152/50176]	Loss: 0.2883
Training Epoch: 76 [17408/50176]	Loss: 0.3057
Training Epoch: 76 [17664/50176]	Loss: 0.2135
Training Epoch: 76 [17920/50176]	Loss: 0.2883
Training Epoch: 76 [18176/50176]	Loss: 0.3034
Training Epoch: 76 [18432/50176]	Loss: 0.3986
Training Epoch: 76 [18688/50176]	Loss: 0.2441
Training Epoch: 76 [18944/50176]	Loss: 0.2942
Training Epoch: 76 [19200/50176]	Loss: 0.2806
Training Epoch: 76 [19456/50176]	Loss: 0.2341
Training Epoch: 76 [19712/50176]	Loss: 0.3474
Training Epoch: 76 [19968/50176]	Loss: 0.3560
Training Epoch: 76 [20224/50176]	Loss: 0.3544
Training Epoch: 76 [20480/50176]	Loss: 0.3153
Training Epoch: 76 [20736/50176]	Loss: 0.3019
Training Epoch: 76 [20992/50176]	Loss: 0.2974
Training Epoch: 76 [21248/50176]	Loss: 0.3014
Training Epoch: 76 [21504/50176]	Loss: 0.2811
Training Epoch: 76 [21760/50176]	Loss: 0.3294
Training Epoch: 76 [22016/50176]	Loss: 0.3469
Training Epoch: 76 [22272/50176]	Loss: 0.3087
Training Epoch: 76 [22528/50176]	Loss: 0.4229
Training Epoch: 76 [22784/50176]	Loss: 0.3511
Training Epoch: 76 [23040/50176]	Loss: 0.3358
Training Epoch: 76 [23296/50176]	Loss: 0.3693
Training Epoch: 76 [23552/50176]	Loss: 0.2969
Training Epoch: 76 [23808/50176]	Loss: 0.3517
Training Epoch: 76 [24064/50176]	Loss: 0.2806
Training Epoch: 76 [24320/50176]	Loss: 0.4358
Training Epoch: 76 [24576/50176]	Loss: 0.3195
Training Epoch: 76 [24832/50176]	Loss: 0.4209
Training Epoch: 76 [25088/50176]	Loss: 0.3986
Training Epoch: 76 [25344/50176]	Loss: 0.3697
Training Epoch: 76 [25600/50176]	Loss: 0.2973
Training Epoch: 76 [25856/50176]	Loss: 0.4613
Training Epoch: 76 [26112/50176]	Loss: 0.3891
Training Epoch: 76 [26368/50176]	Loss: 0.3893
Training Epoch: 76 [26624/50176]	Loss: 0.3215
Training Epoch: 76 [26880/50176]	Loss: 0.3086
Training Epoch: 76 [27136/50176]	Loss: 0.3467
Training Epoch: 76 [27392/50176]	Loss: 0.2799
Training Epoch: 76 [27648/50176]	Loss: 0.3263
Training Epoch: 76 [27904/50176]	Loss: 0.4135
Training Epoch: 76 [28160/50176]	Loss: 0.3658
Training Epoch: 76 [28416/50176]	Loss: 0.3813
Training Epoch: 76 [28672/50176]	Loss: 0.3237
Training Epoch: 76 [28928/50176]	Loss: 0.3701
Training Epoch: 76 [29184/50176]	Loss: 0.3358
Training Epoch: 76 [29440/50176]	Loss: 0.2929
Training Epoch: 76 [29696/50176]	Loss: 0.3487
Training Epoch: 76 [29952/50176]	Loss: 0.4227
Training Epoch: 76 [30208/50176]	Loss: 0.4502
Training Epoch: 76 [30464/50176]	Loss: 0.3271
Training Epoch: 76 [30720/50176]	Loss: 0.3479
Training Epoch: 76 [30976/50176]	Loss: 0.4083
Training Epoch: 76 [31232/50176]	Loss: 0.3394
Training Epoch: 76 [31488/50176]	Loss: 0.4261
Training Epoch: 76 [31744/50176]	Loss: 0.3632
Training Epoch: 76 [32000/50176]	Loss: 0.4242
Training Epoch: 76 [32256/50176]	Loss: 0.3280
Training Epoch: 76 [32512/50176]	Loss: 0.3375
Training Epoch: 76 [32768/50176]	Loss: 0.4142
Training Epoch: 76 [33024/50176]	Loss: 0.4044
Training Epoch: 76 [33280/50176]	Loss: 0.4024
Training Epoch: 76 [33536/50176]	Loss: 0.3567
Training Epoch: 76 [33792/50176]	Loss: 0.4391
Training Epoch: 76 [34048/50176]	Loss: 0.4166
Training Epoch: 76 [34304/50176]	Loss: 0.3233
Training Epoch: 76 [34560/50176]	Loss: 0.4194
Training Epoch: 76 [34816/50176]	Loss: 0.3619
Training Epoch: 76 [35072/50176]	Loss: 0.4421
Training Epoch: 76 [35328/50176]	Loss: 0.4383
Training Epoch: 76 [35584/50176]	Loss: 0.3427
Training Epoch: 76 [35840/50176]	Loss: 0.3290
Training Epoch: 76 [36096/50176]	Loss: 0.3466
Training Epoch: 76 [36352/50176]	Loss: 0.2892
Training Epoch: 76 [36608/50176]	Loss: 0.3606
Training Epoch: 76 [36864/50176]	Loss: 0.3791
Training Epoch: 76 [37120/50176]	Loss: 0.3580
Training Epoch: 76 [37376/50176]	Loss: 0.3507
Training Epoch: 76 [37632/50176]	Loss: 0.3920
Training Epoch: 76 [37888/50176]	Loss: 0.4368
Training Epoch: 76 [38144/50176]	Loss: 0.2982
Training Epoch: 76 [38400/50176]	Loss: 0.4175
Training Epoch: 76 [38656/50176]	Loss: 0.4170
Training Epoch: 76 [38912/50176]	Loss: 0.3150
Training Epoch: 76 [39168/50176]	Loss: 0.4105
Training Epoch: 76 [39424/50176]	Loss: 0.3211
Training Epoch: 76 [39680/50176]	Loss: 0.3994
Training Epoch: 76 [39936/50176]	Loss: 0.4360
Training Epoch: 76 [40192/50176]	Loss: 0.4348
Training Epoch: 76 [40448/50176]	Loss: 0.4589
Training Epoch: 76 [40704/50176]	Loss: 0.3600
Training Epoch: 76 [40960/50176]	Loss: 0.2980
Training Epoch: 76 [41216/50176]	Loss: 0.3657
Training Epoch: 76 [41472/50176]	Loss: 0.2844
Training Epoch: 76 [41728/50176]	Loss: 0.3434
Training Epoch: 76 [41984/50176]	Loss: 0.3570
Training Epoch: 76 [42240/50176]	Loss: 0.4041
Training Epoch: 76 [42496/50176]	Loss: 0.3439
Training Epoch: 76 [42752/50176]	Loss: 0.3440
Training Epoch: 76 [43008/50176]	Loss: 0.4081
Training Epoch: 76 [43264/50176]	Loss: 0.2921
Training Epoch: 76 [43520/50176]	Loss: 0.3734
Training Epoch: 76 [43776/50176]	Loss: 0.4191
Training Epoch: 76 [44032/50176]	Loss: 0.4173
Training Epoch: 76 [44288/50176]	Loss: 0.2292
Training Epoch: 76 [44544/50176]	Loss: 0.3378
Training Epoch: 76 [44800/50176]	Loss: 0.3630
Training Epoch: 76 [45056/50176]	Loss: 0.4123
Training Epoch: 76 [45312/50176]	Loss: 0.4604
Training Epoch: 76 [45568/50176]	Loss: 0.4424
Training Epoch: 76 [45824/50176]	Loss: 0.3350
Training Epoch: 76 [46080/50176]	Loss: 0.3467
Training Epoch: 76 [46336/50176]	Loss: 0.3505
Training Epoch: 76 [46592/50176]	Loss: 0.4086
Training Epoch: 76 [46848/50176]	Loss: 0.3233
Training Epoch: 76 [47104/50176]	Loss: 0.4060
Training Epoch: 76 [47360/50176]	Loss: 0.3501
Training Epoch: 76 [47616/50176]	Loss: 0.2946
Training Epoch: 76 [47872/50176]	Loss: 0.3883
Training Epoch: 76 [48128/50176]	Loss: 0.3573
Training Epoch: 76 [48384/50176]	Loss: 0.4139
Training Epoch: 76 [48640/50176]	Loss: 0.3808
Training Epoch: 76 [48896/50176]	Loss: 0.3134
Training Epoch: 76 [49152/50176]	Loss: 0.3675
Training Epoch: 76 [49408/50176]	Loss: 0.4274
Training Epoch: 76 [49664/50176]	Loss: 0.3098
Training Epoch: 76 [49920/50176]	Loss: 0.3565
Training Epoch: 76 [50176/50176]	Loss: 0.5772
Validation Epoch: 76, Average loss: 0.0095, Accuracy: 0.5760
Training Epoch: 77 [256/50176]	Loss: 0.3038
Training Epoch: 77 [512/50176]	Loss: 0.4053
Training Epoch: 77 [768/50176]	Loss: 0.3089
Training Epoch: 77 [1024/50176]	Loss: 0.3127
Training Epoch: 77 [1280/50176]	Loss: 0.2574
Training Epoch: 77 [1536/50176]	Loss: 0.3037
Training Epoch: 77 [1792/50176]	Loss: 0.3148
Training Epoch: 77 [2048/50176]	Loss: 0.3728
Training Epoch: 77 [2304/50176]	Loss: 0.4157
Training Epoch: 77 [2560/50176]	Loss: 0.3239
Training Epoch: 77 [2816/50176]	Loss: 0.3429
Training Epoch: 77 [3072/50176]	Loss: 0.3136
Training Epoch: 77 [3328/50176]	Loss: 0.4372
Training Epoch: 77 [3584/50176]	Loss: 0.3410
Training Epoch: 77 [3840/50176]	Loss: 0.2754
Training Epoch: 77 [4096/50176]	Loss: 0.3153
Training Epoch: 77 [4352/50176]	Loss: 0.4174
Training Epoch: 77 [4608/50176]	Loss: 0.3455
Training Epoch: 77 [4864/50176]	Loss: 0.3052
Training Epoch: 77 [5120/50176]	Loss: 0.3887
Training Epoch: 77 [5376/50176]	Loss: 0.2754
Training Epoch: 77 [5632/50176]	Loss: 0.2427
Training Epoch: 77 [5888/50176]	Loss: 0.2380
Training Epoch: 77 [6144/50176]	Loss: 0.3762
Training Epoch: 77 [6400/50176]	Loss: 0.3104
Training Epoch: 77 [6656/50176]	Loss: 0.2970
Training Epoch: 77 [6912/50176]	Loss: 0.3466
Training Epoch: 77 [7168/50176]	Loss: 0.3375
Training Epoch: 77 [7424/50176]	Loss: 0.3539
Training Epoch: 77 [7680/50176]	Loss: 0.3471
Training Epoch: 77 [7936/50176]	Loss: 0.2783
Training Epoch: 77 [8192/50176]	Loss: 0.3371
Training Epoch: 77 [8448/50176]	Loss: 0.3481
Training Epoch: 77 [8704/50176]	Loss: 0.3494
Training Epoch: 77 [8960/50176]	Loss: 0.3282
Training Epoch: 77 [9216/50176]	Loss: 0.3742
Training Epoch: 77 [9472/50176]	Loss: 0.3596
Training Epoch: 77 [9728/50176]	Loss: 0.2868
Training Epoch: 77 [9984/50176]	Loss: 0.3143
Training Epoch: 77 [10240/50176]	Loss: 0.2464
Training Epoch: 77 [10496/50176]	Loss: 0.3047
Training Epoch: 77 [10752/50176]	Loss: 0.3843
Training Epoch: 77 [11008/50176]	Loss: 0.3253
Training Epoch: 77 [11264/50176]	Loss: 0.3055
Training Epoch: 77 [11520/50176]	Loss: 0.2934
Training Epoch: 77 [11776/50176]	Loss: 0.1963
Training Epoch: 77 [12032/50176]	Loss: 0.3349
Training Epoch: 77 [12288/50176]	Loss: 0.3416
Training Epoch: 77 [12544/50176]	Loss: 0.3409
Training Epoch: 77 [12800/50176]	Loss: 0.2949
Training Epoch: 77 [13056/50176]	Loss: 0.3329
Training Epoch: 77 [13312/50176]	Loss: 0.2808
Training Epoch: 77 [13568/50176]	Loss: 0.3648
Training Epoch: 77 [13824/50176]	Loss: 0.3103
Training Epoch: 77 [14080/50176]	Loss: 0.3634
Training Epoch: 77 [14336/50176]	Loss: 0.2965
Training Epoch: 77 [14592/50176]	Loss: 0.3258
Training Epoch: 77 [14848/50176]	Loss: 0.2618
Training Epoch: 77 [15104/50176]	Loss: 0.2920
Training Epoch: 77 [15360/50176]	Loss: 0.3320
Training Epoch: 77 [15616/50176]	Loss: 0.2813
Training Epoch: 77 [15872/50176]	Loss: 0.2525
Training Epoch: 77 [16128/50176]	Loss: 0.3272
Training Epoch: 77 [16384/50176]	Loss: 0.3364
Training Epoch: 77 [16640/50176]	Loss: 0.3952
Training Epoch: 77 [16896/50176]	Loss: 0.3530
Training Epoch: 77 [17152/50176]	Loss: 0.3198
Training Epoch: 77 [17408/50176]	Loss: 0.4632
Training Epoch: 77 [17664/50176]	Loss: 0.2517
Training Epoch: 77 [17920/50176]	Loss: 0.2690
Training Epoch: 77 [18176/50176]	Loss: 0.3516
Training Epoch: 77 [18432/50176]	Loss: 0.3346
Training Epoch: 77 [18688/50176]	Loss: 0.3304
Training Epoch: 77 [18944/50176]	Loss: 0.3899
Training Epoch: 77 [19200/50176]	Loss: 0.4015
Training Epoch: 77 [19456/50176]	Loss: 0.3905
Training Epoch: 77 [19712/50176]	Loss: 0.2876
Training Epoch: 77 [19968/50176]	Loss: 0.3555
Training Epoch: 77 [20224/50176]	Loss: 0.3127
Training Epoch: 77 [20480/50176]	Loss: 0.4293
Training Epoch: 77 [20736/50176]	Loss: 0.3502
Training Epoch: 77 [20992/50176]	Loss: 0.2817
Training Epoch: 77 [21248/50176]	Loss: 0.3561
Training Epoch: 77 [21504/50176]	Loss: 0.3598
Training Epoch: 77 [21760/50176]	Loss: 0.3189
Training Epoch: 77 [22016/50176]	Loss: 0.3663
Training Epoch: 77 [22272/50176]	Loss: 0.4356
Training Epoch: 77 [22528/50176]	Loss: 0.3472
Training Epoch: 77 [22784/50176]	Loss: 0.3829
Training Epoch: 77 [23040/50176]	Loss: 0.2966
Training Epoch: 77 [23296/50176]	Loss: 0.3658
Training Epoch: 77 [23552/50176]	Loss: 0.2444
Training Epoch: 77 [23808/50176]	Loss: 0.2911
Training Epoch: 77 [24064/50176]	Loss: 0.3573
Training Epoch: 77 [24320/50176]	Loss: 0.3856
Training Epoch: 77 [24576/50176]	Loss: 0.3908
Training Epoch: 77 [24832/50176]	Loss: 0.3404
Training Epoch: 77 [25088/50176]	Loss: 0.4525
Training Epoch: 77 [25344/50176]	Loss: 0.5171
Training Epoch: 77 [25600/50176]	Loss: 0.3310
Training Epoch: 77 [25856/50176]	Loss: 0.4652
Training Epoch: 77 [26112/50176]	Loss: 0.3096
Training Epoch: 77 [26368/50176]	Loss: 0.2840
Training Epoch: 77 [26624/50176]	Loss: 0.3056
Training Epoch: 77 [26880/50176]	Loss: 0.2931
Training Epoch: 77 [27136/50176]	Loss: 0.3883
Training Epoch: 77 [27392/50176]	Loss: 0.3365
Training Epoch: 77 [27648/50176]	Loss: 0.4125
Training Epoch: 77 [27904/50176]	Loss: 0.3482
Training Epoch: 77 [28160/50176]	Loss: 0.4004
Training Epoch: 77 [28416/50176]	Loss: 0.4134
Training Epoch: 77 [28672/50176]	Loss: 0.3341
Training Epoch: 77 [28928/50176]	Loss: 0.3345
Training Epoch: 77 [29184/50176]	Loss: 0.4401
Training Epoch: 77 [29440/50176]	Loss: 0.3949
Training Epoch: 77 [29696/50176]	Loss: 0.3606
Training Epoch: 77 [29952/50176]	Loss: 0.3903
Training Epoch: 77 [30208/50176]	Loss: 0.3778
Training Epoch: 77 [30464/50176]	Loss: 0.3512
Training Epoch: 77 [30720/50176]	Loss: 0.3904
Training Epoch: 77 [30976/50176]	Loss: 0.3926
Training Epoch: 77 [31232/50176]	Loss: 0.3717
Training Epoch: 77 [31488/50176]	Loss: 0.3372
Training Epoch: 77 [31744/50176]	Loss: 0.3309
Training Epoch: 77 [32000/50176]	Loss: 0.3942
Training Epoch: 77 [32256/50176]	Loss: 0.3380
Training Epoch: 77 [32512/50176]	Loss: 0.3888
Training Epoch: 77 [32768/50176]	Loss: 0.3171
Training Epoch: 77 [33024/50176]	Loss: 0.3494
Training Epoch: 77 [33280/50176]	Loss: 0.3648
Training Epoch: 77 [33536/50176]	Loss: 0.2459
Training Epoch: 77 [33792/50176]	Loss: 0.3535
Training Epoch: 77 [34048/50176]	Loss: 0.3082
Training Epoch: 77 [34304/50176]	Loss: 0.2877
Training Epoch: 77 [34560/50176]	Loss: 0.3729
Training Epoch: 77 [34816/50176]	Loss: 0.3731
Training Epoch: 77 [35072/50176]	Loss: 0.3223
Training Epoch: 77 [35328/50176]	Loss: 0.3704
Training Epoch: 77 [35584/50176]	Loss: 0.3465
Training Epoch: 77 [35840/50176]	Loss: 0.3761
Training Epoch: 77 [36096/50176]	Loss: 0.3587
Training Epoch: 77 [36352/50176]	Loss: 0.4168
Training Epoch: 77 [36608/50176]	Loss: 0.4090
Training Epoch: 77 [36864/50176]	Loss: 0.3449
Training Epoch: 77 [37120/50176]	Loss: 0.3553
Training Epoch: 77 [37376/50176]	Loss: 0.4165
Training Epoch: 77 [37632/50176]	Loss: 0.3270
Training Epoch: 77 [37888/50176]	Loss: 0.3851
Training Epoch: 77 [38144/50176]	Loss: 0.3356
Training Epoch: 77 [38400/50176]	Loss: 0.3505
Training Epoch: 77 [38656/50176]	Loss: 0.3221
Training Epoch: 77 [38912/50176]	Loss: 0.3935
Training Epoch: 77 [39168/50176]	Loss: 0.2838
Training Epoch: 77 [39424/50176]	Loss: 0.3528
Training Epoch: 77 [39680/50176]	Loss: 0.3859
Training Epoch: 77 [39936/50176]	Loss: 0.3606
Training Epoch: 77 [40192/50176]	Loss: 0.2609
Training Epoch: 77 [40448/50176]	Loss: 0.3060
Training Epoch: 77 [40704/50176]	Loss: 0.3272
Training Epoch: 77 [40960/50176]	Loss: 0.3510
Training Epoch: 77 [41216/50176]	Loss: 0.4383
Training Epoch: 77 [41472/50176]	Loss: 0.3912
Training Epoch: 77 [41728/50176]	Loss: 0.4292
Training Epoch: 77 [41984/50176]	Loss: 0.3394
Training Epoch: 77 [42240/50176]	Loss: 0.4536
Training Epoch: 77 [42496/50176]	Loss: 0.4235
Training Epoch: 77 [42752/50176]	Loss: 0.3319
Training Epoch: 77 [43008/50176]	Loss: 0.4471
Training Epoch: 77 [43264/50176]	Loss: 0.3340
Training Epoch: 77 [43520/50176]	Loss: 0.3331
Training Epoch: 77 [43776/50176]	Loss: 0.4040
Training Epoch: 77 [44032/50176]	Loss: 0.3409
Training Epoch: 77 [44288/50176]	Loss: 0.3476
Training Epoch: 77 [44544/50176]	Loss: 0.4906
Training Epoch: 77 [44800/50176]	Loss: 0.3120
Training Epoch: 77 [45056/50176]	Loss: 0.2999
Training Epoch: 77 [45312/50176]	Loss: 0.3564
Training Epoch: 77 [45568/50176]	Loss: 0.4662
Training Epoch: 77 [45824/50176]	Loss: 0.3035
Training Epoch: 77 [46080/50176]	Loss: 0.4131
Training Epoch: 77 [46336/50176]	Loss: 0.3954
Training Epoch: 77 [46592/50176]	Loss: 0.3792
Training Epoch: 77 [46848/50176]	Loss: 0.4392
Training Epoch: 77 [47104/50176]	Loss: 0.3840
Training Epoch: 77 [47360/50176]	Loss: 0.4670
Training Epoch: 77 [47616/50176]	Loss: 0.4231
Training Epoch: 77 [47872/50176]	Loss: 0.4651
Training Epoch: 77 [48128/50176]	Loss: 0.3493
Training Epoch: 77 [48384/50176]	Loss: 0.4413
Training Epoch: 77 [48640/50176]	Loss: 0.3027
Training Epoch: 77 [48896/50176]	Loss: 0.3803
Training Epoch: 77 [49152/50176]	Loss: 0.3333
Training Epoch: 77 [49408/50176]	Loss: 0.4134
Training Epoch: 77 [49664/50176]	Loss: 0.3894
Training Epoch: 77 [49920/50176]	Loss: 0.3869
Training Epoch: 77 [50176/50176]	Loss: 0.4148
Validation Epoch: 77, Average loss: 0.0114, Accuracy: 0.5501
Training Epoch: 78 [256/50176]	Loss: 0.3743
Training Epoch: 78 [512/50176]	Loss: 0.3710
Training Epoch: 78 [768/50176]	Loss: 0.2848
Training Epoch: 78 [1024/50176]	Loss: 0.2599
Training Epoch: 78 [1280/50176]	Loss: 0.3828
Training Epoch: 78 [1536/50176]	Loss: 0.3012
Training Epoch: 78 [1792/50176]	Loss: 0.3072
Training Epoch: 78 [2048/50176]	Loss: 0.3325
Training Epoch: 78 [2304/50176]	Loss: 0.2687
Training Epoch: 78 [2560/50176]	Loss: 0.3064
Training Epoch: 78 [2816/50176]	Loss: 0.3531
Training Epoch: 78 [3072/50176]	Loss: 0.3838
Training Epoch: 78 [3328/50176]	Loss: 0.3074
Training Epoch: 78 [3584/50176]	Loss: 0.3060
Training Epoch: 78 [3840/50176]	Loss: 0.3266
Training Epoch: 78 [4096/50176]	Loss: 0.2900
Training Epoch: 78 [4352/50176]	Loss: 0.3767
Training Epoch: 78 [4608/50176]	Loss: 0.3293
Training Epoch: 78 [4864/50176]	Loss: 0.2530
Training Epoch: 78 [5120/50176]	Loss: 0.3918
Training Epoch: 78 [5376/50176]	Loss: 0.3360
Training Epoch: 78 [5632/50176]	Loss: 0.2346
Training Epoch: 78 [5888/50176]	Loss: 0.3372
Training Epoch: 78 [6144/50176]	Loss: 0.3361
Training Epoch: 78 [6400/50176]	Loss: 0.3738
Training Epoch: 78 [6656/50176]	Loss: 0.2915
Training Epoch: 78 [6912/50176]	Loss: 0.3365
Training Epoch: 78 [7168/50176]	Loss: 0.2786
Training Epoch: 78 [7424/50176]	Loss: 0.3868
Training Epoch: 78 [7680/50176]	Loss: 0.4978
Training Epoch: 78 [7936/50176]	Loss: 0.4448
Training Epoch: 78 [8192/50176]	Loss: 0.3146
Training Epoch: 78 [8448/50176]	Loss: 0.3389
Training Epoch: 78 [8704/50176]	Loss: 0.3143
Training Epoch: 78 [8960/50176]	Loss: 0.3724
Training Epoch: 78 [9216/50176]	Loss: 0.3124
Training Epoch: 78 [9472/50176]	Loss: 0.4151
Training Epoch: 78 [9728/50176]	Loss: 0.3779
Training Epoch: 78 [9984/50176]	Loss: 0.2606
Training Epoch: 78 [10240/50176]	Loss: 0.2848
Training Epoch: 78 [10496/50176]	Loss: 0.2933
Training Epoch: 78 [10752/50176]	Loss: 0.3541
Training Epoch: 78 [11008/50176]	Loss: 0.3445
Training Epoch: 78 [11264/50176]	Loss: 0.3714
Training Epoch: 78 [11520/50176]	Loss: 0.3289
Training Epoch: 78 [11776/50176]	Loss: 0.2702
Training Epoch: 78 [12032/50176]	Loss: 0.2780
Training Epoch: 78 [12288/50176]	Loss: 0.2882
Training Epoch: 78 [12544/50176]	Loss: 0.2911
Training Epoch: 78 [12800/50176]	Loss: 0.3442
Training Epoch: 78 [13056/50176]	Loss: 0.3911
Training Epoch: 78 [13312/50176]	Loss: 0.3561
Training Epoch: 78 [13568/50176]	Loss: 0.3752
Training Epoch: 78 [13824/50176]	Loss: 0.3121
Training Epoch: 78 [14080/50176]	Loss: 0.2592
Training Epoch: 78 [14336/50176]	Loss: 0.3718
Training Epoch: 78 [14592/50176]	Loss: 0.3171
Training Epoch: 78 [14848/50176]	Loss: 0.2998
Training Epoch: 78 [15104/50176]	Loss: 0.2409
Training Epoch: 78 [15360/50176]	Loss: 0.5056
Training Epoch: 78 [15616/50176]	Loss: 0.2789
Training Epoch: 78 [15872/50176]	Loss: 0.2519
Training Epoch: 78 [16128/50176]	Loss: 0.3371
Training Epoch: 78 [16384/50176]	Loss: 0.3942
Training Epoch: 78 [16640/50176]	Loss: 0.2555
Training Epoch: 78 [16896/50176]	Loss: 0.2922
Training Epoch: 78 [17152/50176]	Loss: 0.3165
Training Epoch: 78 [17408/50176]	Loss: 0.3853
Training Epoch: 78 [17664/50176]	Loss: 0.3459
Training Epoch: 78 [17920/50176]	Loss: 0.2904
Training Epoch: 78 [18176/50176]	Loss: 0.4167
Training Epoch: 78 [18432/50176]	Loss: 0.3284
Training Epoch: 78 [18688/50176]	Loss: 0.2492
Training Epoch: 78 [18944/50176]	Loss: 0.3650
Training Epoch: 78 [19200/50176]	Loss: 0.3125
Training Epoch: 78 [19456/50176]	Loss: 0.4335
Training Epoch: 78 [19712/50176]	Loss: 0.2868
Training Epoch: 78 [19968/50176]	Loss: 0.3405
Training Epoch: 78 [20224/50176]	Loss: 0.3226
Training Epoch: 78 [20480/50176]	Loss: 0.4690
Training Epoch: 78 [20736/50176]	Loss: 0.3790
Training Epoch: 78 [20992/50176]	Loss: 0.3358
Training Epoch: 78 [21248/50176]	Loss: 0.3712
Training Epoch: 78 [21504/50176]	Loss: 0.4043
Training Epoch: 78 [21760/50176]	Loss: 0.2744
Training Epoch: 78 [22016/50176]	Loss: 0.2554
Training Epoch: 78 [22272/50176]	Loss: 0.3420
Training Epoch: 78 [22528/50176]	Loss: 0.3212
Training Epoch: 78 [22784/50176]	Loss: 0.3317
Training Epoch: 78 [23040/50176]	Loss: 0.3303
Training Epoch: 78 [23296/50176]	Loss: 0.3190
Training Epoch: 78 [23552/50176]	Loss: 0.2650
Training Epoch: 78 [23808/50176]	Loss: 0.3201
Training Epoch: 78 [24064/50176]	Loss: 0.3184
Training Epoch: 78 [24320/50176]	Loss: 0.3087
Training Epoch: 78 [24576/50176]	Loss: 0.3543
Training Epoch: 78 [24832/50176]	Loss: 0.3603
Training Epoch: 78 [25088/50176]	Loss: 0.3812
Training Epoch: 78 [25344/50176]	Loss: 0.2994
Training Epoch: 78 [25600/50176]	Loss: 0.2661
Training Epoch: 78 [25856/50176]	Loss: 0.3042
Training Epoch: 78 [26112/50176]	Loss: 0.2739
Training Epoch: 78 [26368/50176]	Loss: 0.3482
Training Epoch: 78 [26624/50176]	Loss: 0.3285
Training Epoch: 78 [26880/50176]	Loss: 0.3538
Training Epoch: 78 [27136/50176]	Loss: 0.3710
Training Epoch: 78 [27392/50176]	Loss: 0.2563
Training Epoch: 78 [27648/50176]	Loss: 0.2506
Training Epoch: 78 [27904/50176]	Loss: 0.3210
Training Epoch: 78 [28160/50176]	Loss: 0.3778
Training Epoch: 78 [28416/50176]	Loss: 0.3041
Training Epoch: 78 [28672/50176]	Loss: 0.3777
Training Epoch: 78 [28928/50176]	Loss: 0.3029
Training Epoch: 78 [29184/50176]	Loss: 0.3555
Training Epoch: 78 [29440/50176]	Loss: 0.3408
Training Epoch: 78 [29696/50176]	Loss: 0.3422
Training Epoch: 78 [29952/50176]	Loss: 0.3424
Training Epoch: 78 [30208/50176]	Loss: 0.3017
Training Epoch: 78 [30464/50176]	Loss: 0.3212
Training Epoch: 78 [30720/50176]	Loss: 0.3560
Training Epoch: 78 [30976/50176]	Loss: 0.3618
Training Epoch: 78 [31232/50176]	Loss: 0.2696
Training Epoch: 78 [31488/50176]	Loss: 0.2712
Training Epoch: 78 [31744/50176]	Loss: 0.3826
Training Epoch: 78 [32000/50176]	Loss: 0.3780
Training Epoch: 78 [32256/50176]	Loss: 0.3090
Training Epoch: 78 [32512/50176]	Loss: 0.4003
Training Epoch: 78 [32768/50176]	Loss: 0.3949
Training Epoch: 78 [33024/50176]	Loss: 0.3001
Training Epoch: 78 [33280/50176]	Loss: 0.4006
Training Epoch: 78 [33536/50176]	Loss: 0.3242
Training Epoch: 78 [33792/50176]	Loss: 0.4072
Training Epoch: 78 [34048/50176]	Loss: 0.4557
Training Epoch: 78 [34304/50176]	Loss: 0.3891
Training Epoch: 78 [34560/50176]	Loss: 0.3342
Training Epoch: 78 [34816/50176]	Loss: 0.3237
Training Epoch: 78 [35072/50176]	Loss: 0.3870
Training Epoch: 78 [35328/50176]	Loss: 0.3532
Training Epoch: 78 [35584/50176]	Loss: 0.3350
Training Epoch: 78 [35840/50176]	Loss: 0.3725
Training Epoch: 78 [36096/50176]	Loss: 0.3707
Training Epoch: 78 [36352/50176]	Loss: 0.2740
Training Epoch: 78 [36608/50176]	Loss: 0.3724
Training Epoch: 78 [36864/50176]	Loss: 0.2897
Training Epoch: 78 [37120/50176]	Loss: 0.3238
Training Epoch: 78 [37376/50176]	Loss: 0.2840
Training Epoch: 78 [37632/50176]	Loss: 0.4154
Training Epoch: 78 [37888/50176]	Loss: 0.2948
Training Epoch: 78 [38144/50176]	Loss: 0.3637
Training Epoch: 78 [38400/50176]	Loss: 0.3603
Training Epoch: 78 [38656/50176]	Loss: 0.3498
Training Epoch: 78 [38912/50176]	Loss: 0.3046
Training Epoch: 78 [39168/50176]	Loss: 0.3350
Training Epoch: 78 [39424/50176]	Loss: 0.4214
Training Epoch: 78 [39680/50176]	Loss: 0.3647
Training Epoch: 78 [39936/50176]	Loss: 0.4623
Training Epoch: 78 [40192/50176]	Loss: 0.3676
Training Epoch: 78 [40448/50176]	Loss: 0.4687
Training Epoch: 78 [40704/50176]	Loss: 0.3664
Training Epoch: 78 [40960/50176]	Loss: 0.4071
Training Epoch: 78 [41216/50176]	Loss: 0.3225
Training Epoch: 78 [41472/50176]	Loss: 0.3646
Training Epoch: 78 [41728/50176]	Loss: 0.3478
Training Epoch: 78 [41984/50176]	Loss: 0.2507
Training Epoch: 78 [42240/50176]	Loss: 0.4779
Training Epoch: 78 [42496/50176]	Loss: 0.3030
Training Epoch: 78 [42752/50176]	Loss: 0.3414
Training Epoch: 78 [43008/50176]	Loss: 0.3323
Training Epoch: 78 [43264/50176]	Loss: 0.4220
Training Epoch: 78 [43520/50176]	Loss: 0.3920
Training Epoch: 78 [43776/50176]	Loss: 0.3462
Training Epoch: 78 [44032/50176]	Loss: 0.3349
Training Epoch: 78 [44288/50176]	Loss: 0.4122
Training Epoch: 78 [44544/50176]	Loss: 0.4159
Training Epoch: 78 [44800/50176]	Loss: 0.3390
Training Epoch: 78 [45056/50176]	Loss: 0.4278
Training Epoch: 78 [45312/50176]	Loss: 0.3320
Training Epoch: 78 [45568/50176]	Loss: 0.4194
Training Epoch: 78 [45824/50176]	Loss: 0.2897
Training Epoch: 78 [46080/50176]	Loss: 0.3714
Training Epoch: 78 [46336/50176]	Loss: 0.3710
Training Epoch: 78 [46592/50176]	Loss: 0.5149
Training Epoch: 78 [46848/50176]	Loss: 0.4145
Training Epoch: 78 [47104/50176]	Loss: 0.4243
Training Epoch: 78 [47360/50176]	Loss: 0.4932
Training Epoch: 78 [47616/50176]	Loss: 0.4609
Training Epoch: 78 [47872/50176]	Loss: 0.3344
Training Epoch: 78 [48128/50176]	Loss: 0.3004
Training Epoch: 78 [48384/50176]	Loss: 0.3205
Training Epoch: 78 [48640/50176]	Loss: 0.3903
Training Epoch: 78 [48896/50176]	Loss: 0.3890
Training Epoch: 78 [49152/50176]	Loss: 0.3402
Training Epoch: 78 [49408/50176]	Loss: 0.4713
Training Epoch: 78 [49664/50176]	Loss: 0.4105
Training Epoch: 78 [49920/50176]	Loss: 0.4558
Training Epoch: 78 [50176/50176]	Loss: 0.5351
Validation Epoch: 78, Average loss: 0.0101, Accuracy: 0.5770
Training Epoch: 79 [256/50176]	Loss: 0.3104
Training Epoch: 79 [512/50176]	Loss: 0.2785
Training Epoch: 79 [768/50176]	Loss: 0.3352
Training Epoch: 79 [1024/50176]	Loss: 0.2972
Training Epoch: 79 [1280/50176]	Loss: 0.2854
Training Epoch: 79 [1536/50176]	Loss: 0.3045
Training Epoch: 79 [1792/50176]	Loss: 0.2991
Training Epoch: 79 [2048/50176]	Loss: 0.3749
Training Epoch: 79 [2304/50176]	Loss: 0.2750
Training Epoch: 79 [2560/50176]	Loss: 0.2510
Training Epoch: 79 [2816/50176]	Loss: 0.2518
Training Epoch: 79 [3072/50176]	Loss: 0.3523
Training Epoch: 79 [3328/50176]	Loss: 0.4030
Training Epoch: 79 [3584/50176]	Loss: 0.3413
Training Epoch: 79 [3840/50176]	Loss: 0.3735
Training Epoch: 79 [4096/50176]	Loss: 0.3086
Training Epoch: 79 [4352/50176]	Loss: 0.2891
Training Epoch: 79 [4608/50176]	Loss: 0.2913
Training Epoch: 79 [4864/50176]	Loss: 0.2784
Training Epoch: 79 [5120/50176]	Loss: 0.2728
Training Epoch: 79 [5376/50176]	Loss: 0.3268
Training Epoch: 79 [5632/50176]	Loss: 0.3654
Training Epoch: 79 [5888/50176]	Loss: 0.3264
Training Epoch: 79 [6144/50176]	Loss: 0.3958
Training Epoch: 79 [6400/50176]	Loss: 0.3479
Training Epoch: 79 [6656/50176]	Loss: 0.4051
Training Epoch: 79 [6912/50176]	Loss: 0.4080
Training Epoch: 79 [7168/50176]	Loss: 0.3376
Training Epoch: 79 [7424/50176]	Loss: 0.2264
Training Epoch: 79 [7680/50176]	Loss: 0.3506
Training Epoch: 79 [7936/50176]	Loss: 0.2986
Training Epoch: 79 [8192/50176]	Loss: 0.3848
Training Epoch: 79 [8448/50176]	Loss: 0.3628
Training Epoch: 79 [8704/50176]	Loss: 0.2827
Training Epoch: 79 [8960/50176]	Loss: 0.2800
Training Epoch: 79 [9216/50176]	Loss: 0.3461
Training Epoch: 79 [9472/50176]	Loss: 0.3583
Training Epoch: 79 [9728/50176]	Loss: 0.2600
Training Epoch: 79 [9984/50176]	Loss: 0.4001
Training Epoch: 79 [10240/50176]	Loss: 0.3223
Training Epoch: 79 [10496/50176]	Loss: 0.3404
Training Epoch: 79 [10752/50176]	Loss: 0.3211
Training Epoch: 79 [11008/50176]	Loss: 0.2914
Training Epoch: 79 [11264/50176]	Loss: 0.3007
Training Epoch: 79 [11520/50176]	Loss: 0.3419
Training Epoch: 79 [11776/50176]	Loss: 0.2894
Training Epoch: 79 [12032/50176]	Loss: 0.3926
Training Epoch: 79 [12288/50176]	Loss: 0.2733
Training Epoch: 79 [12544/50176]	Loss: 0.3447
Training Epoch: 79 [12800/50176]	Loss: 0.3476
Training Epoch: 79 [13056/50176]	Loss: 0.3327
Training Epoch: 79 [13312/50176]	Loss: 0.2186
Training Epoch: 79 [13568/50176]	Loss: 0.2906
Training Epoch: 79 [13824/50176]	Loss: 0.3587
Training Epoch: 79 [14080/50176]	Loss: 0.3445
Training Epoch: 79 [14336/50176]	Loss: 0.2747
Training Epoch: 79 [14592/50176]	Loss: 0.2880
Training Epoch: 79 [14848/50176]	Loss: 0.2776
Training Epoch: 79 [15104/50176]	Loss: 0.2865
Training Epoch: 79 [15360/50176]	Loss: 0.3332
Training Epoch: 79 [15616/50176]	Loss: 0.2745
Training Epoch: 79 [15872/50176]	Loss: 0.2547
Training Epoch: 79 [16128/50176]	Loss: 0.2467
Training Epoch: 79 [16384/50176]	Loss: 0.2541
Training Epoch: 79 [16640/50176]	Loss: 0.2886
Training Epoch: 79 [16896/50176]	Loss: 0.2945
Training Epoch: 79 [17152/50176]	Loss: 0.3270
Training Epoch: 79 [17408/50176]	Loss: 0.3522
Training Epoch: 79 [17664/50176]	Loss: 0.2880
Training Epoch: 79 [17920/50176]	Loss: 0.4167
Training Epoch: 79 [18176/50176]	Loss: 0.3027
Training Epoch: 79 [18432/50176]	Loss: 0.4448
Training Epoch: 79 [18688/50176]	Loss: 0.3461
Training Epoch: 79 [18944/50176]	Loss: 0.3491
Training Epoch: 79 [19200/50176]	Loss: 0.2719
Training Epoch: 79 [19456/50176]	Loss: 0.3138
Training Epoch: 79 [19712/50176]	Loss: 0.3952
Training Epoch: 79 [19968/50176]	Loss: 0.4168
Training Epoch: 79 [20224/50176]	Loss: 0.3354
Training Epoch: 79 [20480/50176]	Loss: 0.2816
Training Epoch: 79 [20736/50176]	Loss: 0.3216
Training Epoch: 79 [20992/50176]	Loss: 0.3509
Training Epoch: 79 [21248/50176]	Loss: 0.2899
Training Epoch: 79 [21504/50176]	Loss: 0.3206
Training Epoch: 79 [21760/50176]	Loss: 0.2983
Training Epoch: 79 [22016/50176]	Loss: 0.3277
Training Epoch: 79 [22272/50176]	Loss: 0.2975
Training Epoch: 79 [22528/50176]	Loss: 0.3269
Training Epoch: 79 [22784/50176]	Loss: 0.4190
Training Epoch: 79 [23040/50176]	Loss: 0.3146
Training Epoch: 79 [23296/50176]	Loss: 0.2851
Training Epoch: 79 [23552/50176]	Loss: 0.3287
Training Epoch: 79 [23808/50176]	Loss: 0.3331
Training Epoch: 79 [24064/50176]	Loss: 0.3616
Training Epoch: 79 [24320/50176]	Loss: 0.2906
Training Epoch: 79 [24576/50176]	Loss: 0.3891
Training Epoch: 79 [24832/50176]	Loss: 0.4850
Training Epoch: 79 [25088/50176]	Loss: 0.3218
Training Epoch: 79 [25344/50176]	Loss: 0.2328
Training Epoch: 79 [25600/50176]	Loss: 0.3814
Training Epoch: 79 [25856/50176]	Loss: 0.2688
Training Epoch: 79 [26112/50176]	Loss: 0.3028
Training Epoch: 79 [26368/50176]	Loss: 0.3567
Training Epoch: 79 [26624/50176]	Loss: 0.3522
Training Epoch: 79 [26880/50176]	Loss: 0.4391
Training Epoch: 79 [27136/50176]	Loss: 0.3927
Training Epoch: 79 [27392/50176]	Loss: 0.3298
Training Epoch: 79 [27648/50176]	Loss: 0.3842
Training Epoch: 79 [27904/50176]	Loss: 0.3612
Training Epoch: 79 [28160/50176]	Loss: 0.2731
Training Epoch: 79 [28416/50176]	Loss: 0.2680
Training Epoch: 79 [28672/50176]	Loss: 0.2740
Training Epoch: 79 [28928/50176]	Loss: 0.4641
Training Epoch: 79 [29184/50176]	Loss: 0.2816
Training Epoch: 79 [29440/50176]	Loss: 0.3447
Training Epoch: 79 [29696/50176]	Loss: 0.3098
Training Epoch: 79 [29952/50176]	Loss: 0.3742
Training Epoch: 79 [30208/50176]	Loss: 0.3982
Training Epoch: 79 [30464/50176]	Loss: 0.3401
Training Epoch: 79 [30720/50176]	Loss: 0.3975
Training Epoch: 79 [30976/50176]	Loss: 0.4138
Training Epoch: 79 [31232/50176]	Loss: 0.2606
Training Epoch: 79 [31488/50176]	Loss: 0.3938
Training Epoch: 79 [31744/50176]	Loss: 0.3464
Training Epoch: 79 [32000/50176]	Loss: 0.3086
Training Epoch: 79 [32256/50176]	Loss: 0.3683
Training Epoch: 79 [32512/50176]	Loss: 0.2619
Training Epoch: 79 [32768/50176]	Loss: 0.2888
Training Epoch: 79 [33024/50176]	Loss: 0.3371
Training Epoch: 79 [33280/50176]	Loss: 0.3853
Training Epoch: 79 [33536/50176]	Loss: 0.2787
Training Epoch: 79 [33792/50176]	Loss: 0.2861
Training Epoch: 79 [34048/50176]	Loss: 0.3036
Training Epoch: 79 [34304/50176]	Loss: 0.2908
Training Epoch: 79 [34560/50176]	Loss: 0.4413
Training Epoch: 79 [34816/50176]	Loss: 0.3480
Training Epoch: 79 [35072/50176]	Loss: 0.3324
Training Epoch: 79 [35328/50176]	Loss: 0.3655
Training Epoch: 79 [35584/50176]	Loss: 0.4125
Training Epoch: 79 [35840/50176]	Loss: 0.3284
Training Epoch: 79 [36096/50176]	Loss: 0.3388
Training Epoch: 79 [36352/50176]	Loss: 0.3210
Training Epoch: 79 [36608/50176]	Loss: 0.4518
Training Epoch: 79 [36864/50176]	Loss: 0.3106
Training Epoch: 79 [37120/50176]	Loss: 0.4496
Training Epoch: 79 [37376/50176]	Loss: 0.3141
Training Epoch: 79 [37632/50176]	Loss: 0.4270
Training Epoch: 79 [37888/50176]	Loss: 0.2572
Training Epoch: 79 [38144/50176]	Loss: 0.3398
Training Epoch: 79 [38400/50176]	Loss: 0.2002
Training Epoch: 79 [38656/50176]	Loss: 0.3337
Training Epoch: 79 [38912/50176]	Loss: 0.2996
Training Epoch: 79 [39168/50176]	Loss: 0.3565
Training Epoch: 79 [39424/50176]	Loss: 0.3654
Training Epoch: 79 [39680/50176]	Loss: 0.4746
Training Epoch: 79 [39936/50176]	Loss: 0.4042
Training Epoch: 79 [40192/50176]	Loss: 0.4492
Training Epoch: 79 [40448/50176]	Loss: 0.2670
Training Epoch: 79 [40704/50176]	Loss: 0.4450
Training Epoch: 79 [40960/50176]	Loss: 0.3158
Training Epoch: 79 [41216/50176]	Loss: 0.3105
Training Epoch: 79 [41472/50176]	Loss: 0.3155
Training Epoch: 79 [41728/50176]	Loss: 0.4743
Training Epoch: 79 [41984/50176]	Loss: 0.2676
Training Epoch: 79 [42240/50176]	Loss: 0.3690
Training Epoch: 79 [42496/50176]	Loss: 0.3618
Training Epoch: 79 [42752/50176]	Loss: 0.2712
Training Epoch: 79 [43008/50176]	Loss: 0.3071
Training Epoch: 79 [43264/50176]	Loss: 0.4596
Training Epoch: 79 [43520/50176]	Loss: 0.3394
Training Epoch: 79 [43776/50176]	Loss: 0.3725
Training Epoch: 79 [44032/50176]	Loss: 0.3175
Training Epoch: 79 [44288/50176]	Loss: 0.3106
Training Epoch: 79 [44544/50176]	Loss: 0.3186
Training Epoch: 79 [44800/50176]	Loss: 0.2792
Training Epoch: 79 [45056/50176]	Loss: 0.4931
Training Epoch: 79 [45312/50176]	Loss: 0.4167
Training Epoch: 79 [45568/50176]	Loss: 0.2926
Training Epoch: 79 [45824/50176]	Loss: 0.3384
Training Epoch: 79 [46080/50176]	Loss: 0.3550
Training Epoch: 79 [46336/50176]	Loss: 0.3804
Training Epoch: 79 [46592/50176]	Loss: 0.5029
Training Epoch: 79 [46848/50176]	Loss: 0.3368
Training Epoch: 79 [47104/50176]	Loss: 0.3590
Training Epoch: 79 [47360/50176]	Loss: 0.3322
Training Epoch: 79 [47616/50176]	Loss: 0.3697
Training Epoch: 79 [47872/50176]	Loss: 0.4015
Training Epoch: 79 [48128/50176]	Loss: 0.3987
Training Epoch: 79 [48384/50176]	Loss: 0.4185
Training Epoch: 79 [48640/50176]	Loss: 0.3216
Training Epoch: 79 [48896/50176]	Loss: 0.4002
Training Epoch: 79 [49152/50176]	Loss: 0.3211
Training Epoch: 79 [49408/50176]	Loss: 0.4044
Training Epoch: 79 [49664/50176]	Loss: 0.3020
Training Epoch: 79 [49920/50176]	Loss: 0.4198
Training Epoch: 79 [50176/50176]	Loss: 0.3557
Validation Epoch: 79, Average loss: 0.0098, Accuracy: 0.5884
Training Epoch: 80 [256/50176]	Loss: 0.2201
Training Epoch: 80 [512/50176]	Loss: 0.3247
Training Epoch: 80 [768/50176]	Loss: 0.3692
Training Epoch: 80 [1024/50176]	Loss: 0.2080
Training Epoch: 80 [1280/50176]	Loss: 0.2770
Training Epoch: 80 [1536/50176]	Loss: 0.3185
Training Epoch: 80 [1792/50176]	Loss: 0.3115
Training Epoch: 80 [2048/50176]	Loss: 0.2962
Training Epoch: 80 [2304/50176]	Loss: 0.2152
Training Epoch: 80 [2560/50176]	Loss: 0.2452
Training Epoch: 80 [2816/50176]	Loss: 0.3443
Training Epoch: 80 [3072/50176]	Loss: 0.3366
Training Epoch: 80 [3328/50176]	Loss: 0.2828
Training Epoch: 80 [3584/50176]	Loss: 0.3523
Training Epoch: 80 [3840/50176]	Loss: 0.3253
Training Epoch: 80 [4096/50176]	Loss: 0.3861
Training Epoch: 80 [4352/50176]	Loss: 0.2871
Training Epoch: 80 [4608/50176]	Loss: 0.2903
Training Epoch: 80 [4864/50176]	Loss: 0.2171
Training Epoch: 80 [5120/50176]	Loss: 0.2921
Training Epoch: 80 [5376/50176]	Loss: 0.2373
Training Epoch: 80 [5632/50176]	Loss: 0.3020
Training Epoch: 80 [5888/50176]	Loss: 0.3304
Training Epoch: 80 [6144/50176]	Loss: 0.3410
Training Epoch: 80 [6400/50176]	Loss: 0.3277
Training Epoch: 80 [6656/50176]	Loss: 0.3006
Training Epoch: 80 [6912/50176]	Loss: 0.3181
Training Epoch: 80 [7168/50176]	Loss: 0.3148
Training Epoch: 80 [7424/50176]	Loss: 0.2826
Training Epoch: 80 [7680/50176]	Loss: 0.2559
Training Epoch: 80 [7936/50176]	Loss: 0.3311
Training Epoch: 80 [8192/50176]	Loss: 0.2547
Training Epoch: 80 [8448/50176]	Loss: 0.3059
Training Epoch: 80 [8704/50176]	Loss: 0.4047
Training Epoch: 80 [8960/50176]	Loss: 0.2912
Training Epoch: 80 [9216/50176]	Loss: 0.2826
Training Epoch: 80 [9472/50176]	Loss: 0.3190
Training Epoch: 80 [9728/50176]	Loss: 0.3069
Training Epoch: 80 [9984/50176]	Loss: 0.2779
Training Epoch: 80 [10240/50176]	Loss: 0.2746
Training Epoch: 80 [10496/50176]	Loss: 0.2825
Training Epoch: 80 [10752/50176]	Loss: 0.2584
Training Epoch: 80 [11008/50176]	Loss: 0.3136
Training Epoch: 80 [11264/50176]	Loss: 0.2422
Training Epoch: 80 [11520/50176]	Loss: 0.2961
Training Epoch: 80 [11776/50176]	Loss: 0.2466
Training Epoch: 80 [12032/50176]	Loss: 0.3087
Training Epoch: 80 [12288/50176]	Loss: 0.2938
Training Epoch: 80 [12544/50176]	Loss: 0.3484
Training Epoch: 80 [12800/50176]	Loss: 0.3402
Training Epoch: 80 [13056/50176]	Loss: 0.2212
Training Epoch: 80 [13312/50176]	Loss: 0.3202
Training Epoch: 80 [13568/50176]	Loss: 0.3706
Training Epoch: 80 [13824/50176]	Loss: 0.3746
Training Epoch: 80 [14080/50176]	Loss: 0.3080
Training Epoch: 80 [14336/50176]	Loss: 0.2987
Training Epoch: 80 [14592/50176]	Loss: 0.2499
Training Epoch: 80 [14848/50176]	Loss: 0.3367
Training Epoch: 80 [15104/50176]	Loss: 0.3355
Training Epoch: 80 [15360/50176]	Loss: 0.3018
Training Epoch: 80 [15616/50176]	Loss: 0.2946
Training Epoch: 80 [15872/50176]	Loss: 0.3032
Training Epoch: 80 [16128/50176]	Loss: 0.3778
Training Epoch: 80 [16384/50176]	Loss: 0.3253
Training Epoch: 80 [16640/50176]	Loss: 0.2805
Training Epoch: 80 [16896/50176]	Loss: 0.3542
Training Epoch: 80 [17152/50176]	Loss: 0.3338
Training Epoch: 80 [17408/50176]	Loss: 0.2924
Training Epoch: 80 [17664/50176]	Loss: 0.3008
Training Epoch: 80 [17920/50176]	Loss: 0.3885
Training Epoch: 80 [18176/50176]	Loss: 0.3111
Training Epoch: 80 [18432/50176]	Loss: 0.2895
Training Epoch: 80 [18688/50176]	Loss: 0.4189
Training Epoch: 80 [18944/50176]	Loss: 0.2913
Training Epoch: 80 [19200/50176]	Loss: 0.3526
Training Epoch: 80 [19456/50176]	Loss: 0.2936
Training Epoch: 80 [19712/50176]	Loss: 0.3434
Training Epoch: 80 [19968/50176]	Loss: 0.3090
Training Epoch: 80 [20224/50176]	Loss: 0.3607
Training Epoch: 80 [20480/50176]	Loss: 0.3055
Training Epoch: 80 [20736/50176]	Loss: 0.2911
Training Epoch: 80 [20992/50176]	Loss: 0.3856
Training Epoch: 80 [21248/50176]	Loss: 0.2566
Training Epoch: 80 [21504/50176]	Loss: 0.3323
Training Epoch: 80 [21760/50176]	Loss: 0.3745
Training Epoch: 80 [22016/50176]	Loss: 0.3093
Training Epoch: 80 [22272/50176]	Loss: 0.4044
Training Epoch: 80 [22528/50176]	Loss: 0.3129
Training Epoch: 80 [22784/50176]	Loss: 0.3570
Training Epoch: 80 [23040/50176]	Loss: 0.3236
Training Epoch: 80 [23296/50176]	Loss: 0.3453
Training Epoch: 80 [23552/50176]	Loss: 0.3042
Training Epoch: 80 [23808/50176]	Loss: 0.3951
Training Epoch: 80 [24064/50176]	Loss: 0.3196
Training Epoch: 80 [24320/50176]	Loss: 0.3097
Training Epoch: 80 [24576/50176]	Loss: 0.3574
Training Epoch: 80 [24832/50176]	Loss: 0.2893
Training Epoch: 80 [25088/50176]	Loss: 0.3069
Training Epoch: 80 [25344/50176]	Loss: 0.3639
Training Epoch: 80 [25600/50176]	Loss: 0.3177
Training Epoch: 80 [25856/50176]	Loss: 0.3064
Training Epoch: 80 [26112/50176]	Loss: 0.3019
Training Epoch: 80 [26368/50176]	Loss: 0.2886
Training Epoch: 80 [26624/50176]	Loss: 0.3271
Training Epoch: 80 [26880/50176]	Loss: 0.4191
Training Epoch: 80 [27136/50176]	Loss: 0.2969
Training Epoch: 80 [27392/50176]	Loss: 0.3137
Training Epoch: 80 [27648/50176]	Loss: 0.3471
Training Epoch: 80 [27904/50176]	Loss: 0.3385
Training Epoch: 80 [28160/50176]	Loss: 0.3303
Training Epoch: 80 [28416/50176]	Loss: 0.4250
Training Epoch: 80 [28672/50176]	Loss: 0.3194
Training Epoch: 80 [28928/50176]	Loss: 0.3108
Training Epoch: 80 [29184/50176]	Loss: 0.3947
Training Epoch: 80 [29440/50176]	Loss: 0.2788
Training Epoch: 80 [29696/50176]	Loss: 0.3263
Training Epoch: 80 [29952/50176]	Loss: 0.2962
Training Epoch: 80 [30208/50176]	Loss: 0.4224
Training Epoch: 80 [30464/50176]	Loss: 0.3819
Training Epoch: 80 [30720/50176]	Loss: 0.3047
Training Epoch: 80 [30976/50176]	Loss: 0.3268
Training Epoch: 80 [31232/50176]	Loss: 0.3696
Training Epoch: 80 [31488/50176]	Loss: 0.2286
Training Epoch: 80 [31744/50176]	Loss: 0.3196
Training Epoch: 80 [32000/50176]	Loss: 0.2344
Training Epoch: 80 [32256/50176]	Loss: 0.3057
Training Epoch: 80 [32512/50176]	Loss: 0.2911
Training Epoch: 80 [32768/50176]	Loss: 0.2911
Training Epoch: 80 [33024/50176]	Loss: 0.4288
Training Epoch: 80 [33280/50176]	Loss: 0.3191
Training Epoch: 80 [33536/50176]	Loss: 0.2616
Training Epoch: 80 [33792/50176]	Loss: 0.3537
Training Epoch: 80 [34048/50176]	Loss: 0.2523
Training Epoch: 80 [34304/50176]	Loss: 0.3718
Training Epoch: 80 [34560/50176]	Loss: 0.3417
Training Epoch: 80 [34816/50176]	Loss: 0.3855
Training Epoch: 80 [35072/50176]	Loss: 0.3335
Training Epoch: 80 [35328/50176]	Loss: 0.3159
Training Epoch: 80 [35584/50176]	Loss: 0.3812
Training Epoch: 80 [35840/50176]	Loss: 0.3978
Training Epoch: 80 [36096/50176]	Loss: 0.2734
Training Epoch: 80 [36352/50176]	Loss: 0.3611
Training Epoch: 80 [36608/50176]	Loss: 0.3461
Training Epoch: 80 [36864/50176]	Loss: 0.4007
Training Epoch: 80 [37120/50176]	Loss: 0.2969
Training Epoch: 80 [37376/50176]	Loss: 0.4080
Training Epoch: 80 [37632/50176]	Loss: 0.3754
Training Epoch: 80 [37888/50176]	Loss: 0.5049
Training Epoch: 80 [38144/50176]	Loss: 0.2972
Training Epoch: 80 [38400/50176]	Loss: 0.4215
Training Epoch: 80 [38656/50176]	Loss: 0.2897
Training Epoch: 80 [38912/50176]	Loss: 0.2916
Training Epoch: 80 [39168/50176]	Loss: 0.3380
Training Epoch: 80 [39424/50176]	Loss: 0.3551
Training Epoch: 80 [39680/50176]	Loss: 0.3220
Training Epoch: 80 [39936/50176]	Loss: 0.3128
Training Epoch: 80 [40192/50176]	Loss: 0.3881
Training Epoch: 80 [40448/50176]	Loss: 0.3931
Training Epoch: 80 [40704/50176]	Loss: 0.3990
Training Epoch: 80 [40960/50176]	Loss: 0.3959
Training Epoch: 80 [41216/50176]	Loss: 0.3596
Training Epoch: 80 [41472/50176]	Loss: 0.3412
Training Epoch: 80 [41728/50176]	Loss: 0.3447
Training Epoch: 80 [41984/50176]	Loss: 0.3091
Training Epoch: 80 [42240/50176]	Loss: 0.4195
Training Epoch: 80 [42496/50176]	Loss: 0.3633
Training Epoch: 80 [42752/50176]	Loss: 0.2943
Training Epoch: 80 [43008/50176]	Loss: 0.3811
Training Epoch: 80 [43264/50176]	Loss: 0.2655
Training Epoch: 80 [43520/50176]	Loss: 0.2976
Training Epoch: 80 [43776/50176]	Loss: 0.3278
Training Epoch: 80 [44032/50176]	Loss: 0.3893
Training Epoch: 80 [44288/50176]	Loss: 0.3684
Training Epoch: 80 [44544/50176]	Loss: 0.2547
Training Epoch: 80 [44800/50176]	Loss: 0.3379
Training Epoch: 80 [45056/50176]	Loss: 0.3610
Training Epoch: 80 [45312/50176]	Loss: 0.3273
Training Epoch: 80 [45568/50176]	Loss: 0.3194
Training Epoch: 80 [45824/50176]	Loss: 0.3855
Training Epoch: 80 [46080/50176]	Loss: 0.4031
Training Epoch: 80 [46336/50176]	Loss: 0.3338
Training Epoch: 80 [46592/50176]	Loss: 0.3905
Training Epoch: 80 [46848/50176]	Loss: 0.3925
Training Epoch: 80 [47104/50176]	Loss: 0.3266
Training Epoch: 80 [47360/50176]	Loss: 0.4251
Training Epoch: 80 [47616/50176]	Loss: 0.3144
Training Epoch: 80 [47872/50176]	Loss: 0.3094
Training Epoch: 80 [48128/50176]	Loss: 0.3691
Training Epoch: 80 [48384/50176]	Loss: 0.3181
Training Epoch: 80 [48640/50176]	Loss: 0.4073
Training Epoch: 80 [48896/50176]	Loss: 0.3161
Training Epoch: 80 [49152/50176]	Loss: 0.2606
Training Epoch: 80 [49408/50176]	Loss: 0.3987
Training Epoch: 80 [49664/50176]	Loss: 0.3407
Training Epoch: 80 [49920/50176]	Loss: 0.2710
Training Epoch: 80 [50176/50176]	Loss: 0.3778
Validation Epoch: 80, Average loss: 0.0104, Accuracy: 0.5705
Training Epoch: 81 [256/50176]	Loss: 0.4190
Training Epoch: 81 [512/50176]	Loss: 0.2997
Training Epoch: 81 [768/50176]	Loss: 0.3814
Training Epoch: 81 [1024/50176]	Loss: 0.3781
Training Epoch: 81 [1280/50176]	Loss: 0.2873
Training Epoch: 81 [1536/50176]	Loss: 0.3371
Training Epoch: 81 [1792/50176]	Loss: 0.2814
Training Epoch: 81 [2048/50176]	Loss: 0.3599
Training Epoch: 81 [2304/50176]	Loss: 0.3681
Training Epoch: 81 [2560/50176]	Loss: 0.3250
Training Epoch: 81 [2816/50176]	Loss: 0.2814
Training Epoch: 81 [3072/50176]	Loss: 0.2926
Training Epoch: 81 [3328/50176]	Loss: 0.3568
Training Epoch: 81 [3584/50176]	Loss: 0.2323
Training Epoch: 81 [3840/50176]	Loss: 0.2703
Training Epoch: 81 [4096/50176]	Loss: 0.3275
Training Epoch: 81 [4352/50176]	Loss: 0.3245
Training Epoch: 81 [4608/50176]	Loss: 0.3675
Training Epoch: 81 [4864/50176]	Loss: 0.3097
Training Epoch: 81 [5120/50176]	Loss: 0.2608
Training Epoch: 81 [5376/50176]	Loss: 0.2908
Training Epoch: 81 [5632/50176]	Loss: 0.3289
Training Epoch: 81 [5888/50176]	Loss: 0.3276
Training Epoch: 81 [6144/50176]	Loss: 0.2647
Training Epoch: 81 [6400/50176]	Loss: 0.2851
Training Epoch: 81 [6656/50176]	Loss: 0.3268
Training Epoch: 81 [6912/50176]	Loss: 0.2750
Training Epoch: 81 [7168/50176]	Loss: 0.3490
Training Epoch: 81 [7424/50176]	Loss: 0.3571
Training Epoch: 81 [7680/50176]	Loss: 0.2160
Training Epoch: 81 [7936/50176]	Loss: 0.2847
Training Epoch: 81 [8192/50176]	Loss: 0.2527
Training Epoch: 81 [8448/50176]	Loss: 0.2579
Training Epoch: 81 [8704/50176]	Loss: 0.3482
Training Epoch: 81 [8960/50176]	Loss: 0.2853
Training Epoch: 81 [9216/50176]	Loss: 0.2423
Training Epoch: 81 [9472/50176]	Loss: 0.3152
Training Epoch: 81 [9728/50176]	Loss: 0.2701
Training Epoch: 81 [9984/50176]	Loss: 0.3118
Training Epoch: 81 [10240/50176]	Loss: 0.3493
Training Epoch: 81 [10496/50176]	Loss: 0.3481
Training Epoch: 81 [10752/50176]	Loss: 0.2914
Training Epoch: 81 [11008/50176]	Loss: 0.3713
Training Epoch: 81 [11264/50176]	Loss: 0.2449
Training Epoch: 81 [11520/50176]	Loss: 0.3057
Training Epoch: 81 [11776/50176]	Loss: 0.2336
Training Epoch: 81 [12032/50176]	Loss: 0.3185
Training Epoch: 81 [12288/50176]	Loss: 0.2529
Training Epoch: 81 [12544/50176]	Loss: 0.2208
Training Epoch: 81 [12800/50176]	Loss: 0.3080
Training Epoch: 81 [13056/50176]	Loss: 0.3288
Training Epoch: 81 [13312/50176]	Loss: 0.3206
Training Epoch: 81 [13568/50176]	Loss: 0.3008
Training Epoch: 81 [13824/50176]	Loss: 0.2403
Training Epoch: 81 [14080/50176]	Loss: 0.2677
Training Epoch: 81 [14336/50176]	Loss: 0.3640
Training Epoch: 81 [14592/50176]	Loss: 0.2565
Training Epoch: 81 [14848/50176]	Loss: 0.3179
Training Epoch: 81 [15104/50176]	Loss: 0.3057
Training Epoch: 81 [15360/50176]	Loss: 0.3344
Training Epoch: 81 [15616/50176]	Loss: 0.2452
Training Epoch: 81 [15872/50176]	Loss: 0.3457
Training Epoch: 81 [16128/50176]	Loss: 0.3068
Training Epoch: 81 [16384/50176]	Loss: 0.3398
Training Epoch: 81 [16640/50176]	Loss: 0.2625
Training Epoch: 81 [16896/50176]	Loss: 0.4351
Training Epoch: 81 [17152/50176]	Loss: 0.3108
Training Epoch: 81 [17408/50176]	Loss: 0.2848
Training Epoch: 81 [17664/50176]	Loss: 0.2824
Training Epoch: 81 [17920/50176]	Loss: 0.3140
Training Epoch: 81 [18176/50176]	Loss: 0.3979
Training Epoch: 81 [18432/50176]	Loss: 0.3209
Training Epoch: 81 [18688/50176]	Loss: 0.2214
Training Epoch: 81 [18944/50176]	Loss: 0.2803
Training Epoch: 81 [19200/50176]	Loss: 0.3194
Training Epoch: 81 [19456/50176]	Loss: 0.2944
Training Epoch: 81 [19712/50176]	Loss: 0.2756
Training Epoch: 81 [19968/50176]	Loss: 0.2181
Training Epoch: 81 [20224/50176]	Loss: 0.3526
Training Epoch: 81 [20480/50176]	Loss: 0.3383
Training Epoch: 81 [20736/50176]	Loss: 0.2685
Training Epoch: 81 [20992/50176]	Loss: 0.3350
Training Epoch: 81 [21248/50176]	Loss: 0.3289
Training Epoch: 81 [21504/50176]	Loss: 0.3350
Training Epoch: 81 [21760/50176]	Loss: 0.3306
Training Epoch: 81 [22016/50176]	Loss: 0.3418
Training Epoch: 81 [22272/50176]	Loss: 0.3008
Training Epoch: 81 [22528/50176]	Loss: 0.4397
Training Epoch: 81 [22784/50176]	Loss: 0.2604
Training Epoch: 81 [23040/50176]	Loss: 0.3204
Training Epoch: 81 [23296/50176]	Loss: 0.4232
Training Epoch: 81 [23552/50176]	Loss: 0.3288
Training Epoch: 81 [23808/50176]	Loss: 0.2913
Training Epoch: 81 [24064/50176]	Loss: 0.3199
Training Epoch: 81 [24320/50176]	Loss: 0.3642
Training Epoch: 81 [24576/50176]	Loss: 0.2954
Training Epoch: 81 [24832/50176]	Loss: 0.2867
Training Epoch: 81 [25088/50176]	Loss: 0.2955
Training Epoch: 81 [25344/50176]	Loss: 0.3767
Training Epoch: 81 [25600/50176]	Loss: 0.3145
Training Epoch: 81 [25856/50176]	Loss: 0.2925
Training Epoch: 81 [26112/50176]	Loss: 0.3091
Training Epoch: 81 [26368/50176]	Loss: 0.3050
Training Epoch: 81 [26624/50176]	Loss: 0.2979
Training Epoch: 81 [26880/50176]	Loss: 0.3652
Training Epoch: 81 [27136/50176]	Loss: 0.3044
Training Epoch: 81 [27392/50176]	Loss: 0.2670
Training Epoch: 81 [27648/50176]	Loss: 0.3684
Training Epoch: 81 [27904/50176]	Loss: 0.3241
Training Epoch: 81 [28160/50176]	Loss: 0.3307
Training Epoch: 81 [28416/50176]	Loss: 0.3281
Training Epoch: 81 [28672/50176]	Loss: 0.3386
Training Epoch: 81 [28928/50176]	Loss: 0.2701
Training Epoch: 81 [29184/50176]	Loss: 0.2131
Training Epoch: 81 [29440/50176]	Loss: 0.2604
Training Epoch: 81 [29696/50176]	Loss: 0.2677
Training Epoch: 81 [29952/50176]	Loss: 0.2964
Training Epoch: 81 [30208/50176]	Loss: 0.2508
Training Epoch: 81 [30464/50176]	Loss: 0.3348
Training Epoch: 81 [30720/50176]	Loss: 0.3966
Training Epoch: 81 [30976/50176]	Loss: 0.2561
Training Epoch: 81 [31232/50176]	Loss: 0.2981
Training Epoch: 81 [31488/50176]	Loss: 0.2491
Training Epoch: 81 [31744/50176]	Loss: 0.3612
Training Epoch: 81 [32000/50176]	Loss: 0.2906
Training Epoch: 81 [32256/50176]	Loss: 0.3058
Training Epoch: 81 [32512/50176]	Loss: 0.3606
Training Epoch: 81 [32768/50176]	Loss: 0.3535
Training Epoch: 81 [33024/50176]	Loss: 0.3565
Training Epoch: 81 [33280/50176]	Loss: 0.2979
Training Epoch: 81 [33536/50176]	Loss: 0.3702
Training Epoch: 81 [33792/50176]	Loss: 0.2926
Training Epoch: 81 [34048/50176]	Loss: 0.3914
Training Epoch: 81 [34304/50176]	Loss: 0.3064
Training Epoch: 81 [34560/50176]	Loss: 0.2772
Training Epoch: 81 [34816/50176]	Loss: 0.3009
Training Epoch: 81 [35072/50176]	Loss: 0.3244
Training Epoch: 81 [35328/50176]	Loss: 0.2861
Training Epoch: 81 [35584/50176]	Loss: 0.2701
Training Epoch: 81 [35840/50176]	Loss: 0.3477
Training Epoch: 81 [36096/50176]	Loss: 0.3072
Training Epoch: 81 [36352/50176]	Loss: 0.3987
Training Epoch: 81 [36608/50176]	Loss: 0.2793
Training Epoch: 81 [36864/50176]	Loss: 0.3398
Training Epoch: 81 [37120/50176]	Loss: 0.4152
Training Epoch: 81 [37376/50176]	Loss: 0.3188
Training Epoch: 81 [37632/50176]	Loss: 0.2607
Training Epoch: 81 [37888/50176]	Loss: 0.3361
Training Epoch: 81 [38144/50176]	Loss: 0.3469
Training Epoch: 81 [38400/50176]	Loss: 0.3090
Training Epoch: 81 [38656/50176]	Loss: 0.2718
Training Epoch: 81 [38912/50176]	Loss: 0.3173
Training Epoch: 81 [39168/50176]	Loss: 0.3599
Training Epoch: 81 [39424/50176]	Loss: 0.2931
Training Epoch: 81 [39680/50176]	Loss: 0.3972
Training Epoch: 81 [39936/50176]	Loss: 0.3904
Training Epoch: 81 [40192/50176]	Loss: 0.2749
Training Epoch: 81 [40448/50176]	Loss: 0.3594
Training Epoch: 81 [40704/50176]	Loss: 0.3310
Training Epoch: 81 [40960/50176]	Loss: 0.2704
Training Epoch: 81 [41216/50176]	Loss: 0.3186
Training Epoch: 81 [41472/50176]	Loss: 0.3542
Training Epoch: 81 [41728/50176]	Loss: 0.3117
Training Epoch: 81 [41984/50176]	Loss: 0.1963
Training Epoch: 81 [42240/50176]	Loss: 0.4455
Training Epoch: 81 [42496/50176]	Loss: 0.4042
Training Epoch: 81 [42752/50176]	Loss: 0.3534
Training Epoch: 81 [43008/50176]	Loss: 0.4572
Training Epoch: 81 [43264/50176]	Loss: 0.4500
Training Epoch: 81 [43520/50176]	Loss: 0.3645
Training Epoch: 81 [43776/50176]	Loss: 0.4934
Training Epoch: 81 [44032/50176]	Loss: 0.3035
Training Epoch: 81 [44288/50176]	Loss: 0.3392
Training Epoch: 81 [44544/50176]	Loss: 0.3673
Training Epoch: 81 [44800/50176]	Loss: 0.3168
Training Epoch: 81 [45056/50176]	Loss: 0.4209
Training Epoch: 81 [45312/50176]	Loss: 0.2818
Training Epoch: 81 [45568/50176]	Loss: 0.3916
Training Epoch: 81 [45824/50176]	Loss: 0.3587
Training Epoch: 81 [46080/50176]	Loss: 0.3153
Training Epoch: 81 [46336/50176]	Loss: 0.3941
Training Epoch: 81 [46592/50176]	Loss: 0.3687
Training Epoch: 81 [46848/50176]	Loss: 0.3431
Training Epoch: 81 [47104/50176]	Loss: 0.3139
Training Epoch: 81 [47360/50176]	Loss: 0.3379
Training Epoch: 81 [47616/50176]	Loss: 0.4240
Training Epoch: 81 [47872/50176]	Loss: 0.3258
Training Epoch: 81 [48128/50176]	Loss: 0.3897
Training Epoch: 81 [48384/50176]	Loss: 0.4208
Training Epoch: 81 [48640/50176]	Loss: 0.4419
Training Epoch: 81 [48896/50176]	Loss: 0.5905
Training Epoch: 81 [49152/50176]	Loss: 0.2890
Training Epoch: 81 [49408/50176]	Loss: 0.4455
Training Epoch: 81 [49664/50176]	Loss: 0.3300
Training Epoch: 81 [49920/50176]	Loss: 0.3696
Training Epoch: 81 [50176/50176]	Loss: 0.5387
Validation Epoch: 81, Average loss: 0.0104, Accuracy: 0.5653
Training Epoch: 82 [256/50176]	Loss: 0.2613
Training Epoch: 82 [512/50176]	Loss: 0.3609
Training Epoch: 82 [768/50176]	Loss: 0.3044
Training Epoch: 82 [1024/50176]	Loss: 0.4423
Training Epoch: 82 [1280/50176]	Loss: 0.2470
Training Epoch: 82 [1536/50176]	Loss: 0.2417
Training Epoch: 82 [1792/50176]	Loss: 0.2791
Training Epoch: 82 [2048/50176]	Loss: 0.2429
Training Epoch: 82 [2304/50176]	Loss: 0.4165
Training Epoch: 82 [2560/50176]	Loss: 0.3415
Training Epoch: 82 [2816/50176]	Loss: 0.3187
Training Epoch: 82 [3072/50176]	Loss: 0.2925
Training Epoch: 82 [3328/50176]	Loss: 0.2552
Training Epoch: 82 [3584/50176]	Loss: 0.2255
Training Epoch: 82 [3840/50176]	Loss: 0.3024
Training Epoch: 82 [4096/50176]	Loss: 0.2302
Training Epoch: 82 [4352/50176]	Loss: 0.3361
Training Epoch: 82 [4608/50176]	Loss: 0.3776
Training Epoch: 82 [4864/50176]	Loss: 0.3814
Training Epoch: 82 [5120/50176]	Loss: 0.3133
Training Epoch: 82 [5376/50176]	Loss: 0.2438
Training Epoch: 82 [5632/50176]	Loss: 0.3160
Training Epoch: 82 [5888/50176]	Loss: 0.3108
Training Epoch: 82 [6144/50176]	Loss: 0.2538
Training Epoch: 82 [6400/50176]	Loss: 0.3239
Training Epoch: 82 [6656/50176]	Loss: 0.2907
Training Epoch: 82 [6912/50176]	Loss: 0.2938
Training Epoch: 82 [7168/50176]	Loss: 0.2559
Training Epoch: 82 [7424/50176]	Loss: 0.2790
Training Epoch: 82 [7680/50176]	Loss: 0.3345
Training Epoch: 82 [7936/50176]	Loss: 0.3522
Training Epoch: 82 [8192/50176]	Loss: 0.2055
Training Epoch: 82 [8448/50176]	Loss: 0.2701
Training Epoch: 82 [8704/50176]	Loss: 0.2548
Training Epoch: 82 [8960/50176]	Loss: 0.3116
Training Epoch: 82 [9216/50176]	Loss: 0.2920
Training Epoch: 82 [9472/50176]	Loss: 0.3186
Training Epoch: 82 [9728/50176]	Loss: 0.2693
Training Epoch: 82 [9984/50176]	Loss: 0.3248
Training Epoch: 82 [10240/50176]	Loss: 0.2603
Training Epoch: 82 [10496/50176]	Loss: 0.3145
Training Epoch: 82 [10752/50176]	Loss: 0.2590
Training Epoch: 82 [11008/50176]	Loss: 0.4059
Training Epoch: 82 [11264/50176]	Loss: 0.2463
Training Epoch: 82 [11520/50176]	Loss: 0.2785
Training Epoch: 82 [11776/50176]	Loss: 0.2865
Training Epoch: 82 [12032/50176]	Loss: 0.2886
Training Epoch: 82 [12288/50176]	Loss: 0.2578
Training Epoch: 82 [12544/50176]	Loss: 0.3063
Training Epoch: 82 [12800/50176]	Loss: 0.2799
Training Epoch: 82 [13056/50176]	Loss: 0.4036
Training Epoch: 82 [13312/50176]	Loss: 0.2424
Training Epoch: 82 [13568/50176]	Loss: 0.3404
Training Epoch: 82 [13824/50176]	Loss: 0.2936
Training Epoch: 82 [14080/50176]	Loss: 0.2914
Training Epoch: 82 [14336/50176]	Loss: 0.3769
Training Epoch: 82 [14592/50176]	Loss: 0.3707
Training Epoch: 82 [14848/50176]	Loss: 0.3264
Training Epoch: 82 [15104/50176]	Loss: 0.3074
Training Epoch: 82 [15360/50176]	Loss: 0.2781
Training Epoch: 82 [15616/50176]	Loss: 0.2220
Training Epoch: 82 [15872/50176]	Loss: 0.3070
Training Epoch: 82 [16128/50176]	Loss: 0.2806
Training Epoch: 82 [16384/50176]	Loss: 0.2555
Training Epoch: 82 [16640/50176]	Loss: 0.3021
Training Epoch: 82 [16896/50176]	Loss: 0.2816
Training Epoch: 82 [17152/50176]	Loss: 0.2834
Training Epoch: 82 [17408/50176]	Loss: 0.2649
Training Epoch: 82 [17664/50176]	Loss: 0.3129
Training Epoch: 82 [17920/50176]	Loss: 0.3939
Training Epoch: 82 [18176/50176]	Loss: 0.3140
Training Epoch: 82 [18432/50176]	Loss: 0.2985
Training Epoch: 82 [18688/50176]	Loss: 0.3179
Training Epoch: 82 [18944/50176]	Loss: 0.3803
Training Epoch: 82 [19200/50176]	Loss: 0.3020
Training Epoch: 82 [19456/50176]	Loss: 0.2313
Training Epoch: 82 [19712/50176]	Loss: 0.2952
Training Epoch: 82 [19968/50176]	Loss: 0.3340
Training Epoch: 82 [20224/50176]	Loss: 0.3189
Training Epoch: 82 [20480/50176]	Loss: 0.3630
Training Epoch: 82 [20736/50176]	Loss: 0.3707
Training Epoch: 82 [20992/50176]	Loss: 0.2889
Training Epoch: 82 [21248/50176]	Loss: 0.2719
Training Epoch: 82 [21504/50176]	Loss: 0.3062
Training Epoch: 82 [21760/50176]	Loss: 0.3915
Training Epoch: 82 [22016/50176]	Loss: 0.3629
Training Epoch: 82 [22272/50176]	Loss: 0.2609
Training Epoch: 82 [22528/50176]	Loss: 0.3492
Training Epoch: 82 [22784/50176]	Loss: 0.3262
Training Epoch: 82 [23040/50176]	Loss: 0.3312
Training Epoch: 82 [23296/50176]	Loss: 0.3448
Training Epoch: 82 [23552/50176]	Loss: 0.2970
Training Epoch: 82 [23808/50176]	Loss: 0.3432
Training Epoch: 82 [24064/50176]	Loss: 0.3028
Training Epoch: 82 [24320/50176]	Loss: 0.3793
Training Epoch: 82 [24576/50176]	Loss: 0.2212
Training Epoch: 82 [24832/50176]	Loss: 0.3503
Training Epoch: 82 [25088/50176]	Loss: 0.2835
Training Epoch: 82 [25344/50176]	Loss: 0.2436
Training Epoch: 82 [25600/50176]	Loss: 0.2686
Training Epoch: 82 [25856/50176]	Loss: 0.2852
Training Epoch: 82 [26112/50176]	Loss: 0.2496
Training Epoch: 82 [26368/50176]	Loss: 0.3207
Training Epoch: 82 [26624/50176]	Loss: 0.3492
Training Epoch: 82 [26880/50176]	Loss: 0.3795
Training Epoch: 82 [27136/50176]	Loss: 0.2618
Training Epoch: 82 [27392/50176]	Loss: 0.4046
Training Epoch: 82 [27648/50176]	Loss: 0.3015
Training Epoch: 82 [27904/50176]	Loss: 0.3491
Training Epoch: 82 [28160/50176]	Loss: 0.3449
Training Epoch: 82 [28416/50176]	Loss: 0.4044
Training Epoch: 82 [28672/50176]	Loss: 0.3346
Training Epoch: 82 [28928/50176]	Loss: 0.2975
Training Epoch: 82 [29184/50176]	Loss: 0.2228
Training Epoch: 82 [29440/50176]	Loss: 0.3189
Training Epoch: 82 [29696/50176]	Loss: 0.3636
Training Epoch: 82 [29952/50176]	Loss: 0.3554
Training Epoch: 82 [30208/50176]	Loss: 0.2853
Training Epoch: 82 [30464/50176]	Loss: 0.3117
Training Epoch: 82 [30720/50176]	Loss: 0.2638
Training Epoch: 82 [30976/50176]	Loss: 0.4247
Training Epoch: 82 [31232/50176]	Loss: 0.4338
Training Epoch: 82 [31488/50176]	Loss: 0.3155
Training Epoch: 82 [31744/50176]	Loss: 0.3989
Training Epoch: 82 [32000/50176]	Loss: 0.2755
Training Epoch: 82 [32256/50176]	Loss: 0.2835
Training Epoch: 82 [32512/50176]	Loss: 0.3040
Training Epoch: 82 [32768/50176]	Loss: 0.3877
Training Epoch: 82 [33024/50176]	Loss: 0.2571
Training Epoch: 82 [33280/50176]	Loss: 0.2516
Training Epoch: 82 [33536/50176]	Loss: 0.3367
Training Epoch: 82 [33792/50176]	Loss: 0.3469
Training Epoch: 82 [34048/50176]	Loss: 0.2724
Training Epoch: 82 [34304/50176]	Loss: 0.2369
Training Epoch: 82 [34560/50176]	Loss: 0.3672
Training Epoch: 82 [34816/50176]	Loss: 0.4491
Training Epoch: 82 [35072/50176]	Loss: 0.3385
Training Epoch: 82 [35328/50176]	Loss: 0.4076
Training Epoch: 82 [35584/50176]	Loss: 0.3047
Training Epoch: 82 [35840/50176]	Loss: 0.3113
Training Epoch: 82 [36096/50176]	Loss: 0.3568
Training Epoch: 82 [36352/50176]	Loss: 0.3452
Training Epoch: 82 [36608/50176]	Loss: 0.4217
Training Epoch: 82 [36864/50176]	Loss: 0.4408
Training Epoch: 82 [37120/50176]	Loss: 0.3800
Training Epoch: 82 [37376/50176]	Loss: 0.2586
Training Epoch: 82 [37632/50176]	Loss: 0.3388
Training Epoch: 82 [37888/50176]	Loss: 0.3818
Training Epoch: 82 [38144/50176]	Loss: 0.4301
Training Epoch: 82 [38400/50176]	Loss: 0.3046
Training Epoch: 82 [38656/50176]	Loss: 0.3406
Training Epoch: 82 [38912/50176]	Loss: 0.3771
Training Epoch: 82 [39168/50176]	Loss: 0.3653
Training Epoch: 82 [39424/50176]	Loss: 0.2980
Training Epoch: 82 [39680/50176]	Loss: 0.2838
Training Epoch: 82 [39936/50176]	Loss: 0.4154
Training Epoch: 82 [40192/50176]	Loss: 0.3129
Training Epoch: 82 [40448/50176]	Loss: 0.3688
Training Epoch: 82 [40704/50176]	Loss: 0.3215
Training Epoch: 82 [40960/50176]	Loss: 0.3465
Training Epoch: 82 [41216/50176]	Loss: 0.3301
Training Epoch: 82 [41472/50176]	Loss: 0.3584
Training Epoch: 82 [41728/50176]	Loss: 0.2787
Training Epoch: 82 [41984/50176]	Loss: 0.4253
Training Epoch: 82 [42240/50176]	Loss: 0.4462
Training Epoch: 82 [42496/50176]	Loss: 0.3512
Training Epoch: 82 [42752/50176]	Loss: 0.3587
Training Epoch: 82 [43008/50176]	Loss: 0.3584
Training Epoch: 82 [43264/50176]	Loss: 0.3765
Training Epoch: 82 [43520/50176]	Loss: 0.4565
Training Epoch: 82 [43776/50176]	Loss: 0.3002
Training Epoch: 82 [44032/50176]	Loss: 0.3153
Training Epoch: 82 [44288/50176]	Loss: 0.3045
Training Epoch: 82 [44544/50176]	Loss: 0.3570
Training Epoch: 82 [44800/50176]	Loss: 0.4264
Training Epoch: 82 [45056/50176]	Loss: 0.3776
Training Epoch: 82 [45312/50176]	Loss: 0.3344
Training Epoch: 82 [45568/50176]	Loss: 0.3395
Training Epoch: 82 [45824/50176]	Loss: 0.3566
Training Epoch: 82 [46080/50176]	Loss: 0.4425
Training Epoch: 82 [46336/50176]	Loss: 0.2570
Training Epoch: 82 [46592/50176]	Loss: 0.3486
Training Epoch: 82 [46848/50176]	Loss: 0.3711
Training Epoch: 82 [47104/50176]	Loss: 0.2978
Training Epoch: 82 [47360/50176]	Loss: 0.4197
Training Epoch: 82 [47616/50176]	Loss: 0.3456
Training Epoch: 82 [47872/50176]	Loss: 0.3822
Training Epoch: 82 [48128/50176]	Loss: 0.3141
Training Epoch: 82 [48384/50176]	Loss: 0.2916
Training Epoch: 82 [48640/50176]	Loss: 0.3072
Training Epoch: 82 [48896/50176]	Loss: 0.3150
Training Epoch: 82 [49152/50176]	Loss: 0.3180
Training Epoch: 82 [49408/50176]	Loss: 0.3937
Training Epoch: 82 [49664/50176]	Loss: 0.3259
Training Epoch: 82 [49920/50176]	Loss: 0.2880
Training Epoch: 82 [50176/50176]	Loss: 0.5856
Validation Epoch: 82, Average loss: 0.0102, Accuracy: 0.5800
Training Epoch: 83 [256/50176]	Loss: 0.2689
Training Epoch: 83 [512/50176]	Loss: 0.2737
Training Epoch: 83 [768/50176]	Loss: 0.2684
Training Epoch: 83 [1024/50176]	Loss: 0.2416
Training Epoch: 83 [1280/50176]	Loss: 0.2492
Training Epoch: 83 [1536/50176]	Loss: 0.3346
Training Epoch: 83 [1792/50176]	Loss: 0.3078
Training Epoch: 83 [2048/50176]	Loss: 0.3379
Training Epoch: 83 [2304/50176]	Loss: 0.3311
Training Epoch: 83 [2560/50176]	Loss: 0.3494
Training Epoch: 83 [2816/50176]	Loss: 0.3410
Training Epoch: 83 [3072/50176]	Loss: 0.3859
Training Epoch: 83 [3328/50176]	Loss: 0.3489
Training Epoch: 83 [3584/50176]	Loss: 0.3308
Training Epoch: 83 [3840/50176]	Loss: 0.3570
Training Epoch: 83 [4096/50176]	Loss: 0.2764
Training Epoch: 83 [4352/50176]	Loss: 0.4414
Training Epoch: 83 [4608/50176]	Loss: 0.3005
Training Epoch: 83 [4864/50176]	Loss: 0.2893
Training Epoch: 83 [5120/50176]	Loss: 0.3202
Training Epoch: 83 [5376/50176]	Loss: 0.2677
Training Epoch: 83 [5632/50176]	Loss: 0.2843
Training Epoch: 83 [5888/50176]	Loss: 0.2617
Training Epoch: 83 [6144/50176]	Loss: 0.2807
Training Epoch: 83 [6400/50176]	Loss: 0.3565
Training Epoch: 83 [6656/50176]	Loss: 0.3626
Training Epoch: 83 [6912/50176]	Loss: 0.3314
Training Epoch: 83 [7168/50176]	Loss: 0.3264
Training Epoch: 83 [7424/50176]	Loss: 0.2479
Training Epoch: 83 [7680/50176]	Loss: 0.3600
Training Epoch: 83 [7936/50176]	Loss: 0.3303
Training Epoch: 83 [8192/50176]	Loss: 0.2654
Training Epoch: 83 [8448/50176]	Loss: 0.2305
Training Epoch: 83 [8704/50176]	Loss: 0.2788
Training Epoch: 83 [8960/50176]	Loss: 0.2828
Training Epoch: 83 [9216/50176]	Loss: 0.2737
Training Epoch: 83 [9472/50176]	Loss: 0.2211
Training Epoch: 83 [9728/50176]	Loss: 0.2685
Training Epoch: 83 [9984/50176]	Loss: 0.3162
Training Epoch: 83 [10240/50176]	Loss: 0.2586
Training Epoch: 83 [10496/50176]	Loss: 0.2603
Training Epoch: 83 [10752/50176]	Loss: 0.3390
Training Epoch: 83 [11008/50176]	Loss: 0.3506
Training Epoch: 83 [11264/50176]	Loss: 0.2622
Training Epoch: 83 [11520/50176]	Loss: 0.3144
Training Epoch: 83 [11776/50176]	Loss: 0.3178
Training Epoch: 83 [12032/50176]	Loss: 0.3234
Training Epoch: 83 [12288/50176]	Loss: 0.3641
Training Epoch: 83 [12544/50176]	Loss: 0.3151
Training Epoch: 83 [12800/50176]	Loss: 0.2725
Training Epoch: 83 [13056/50176]	Loss: 0.2749
Training Epoch: 83 [13312/50176]	Loss: 0.2744
Training Epoch: 83 [13568/50176]	Loss: 0.2888
Training Epoch: 83 [13824/50176]	Loss: 0.2748
Training Epoch: 83 [14080/50176]	Loss: 0.2081
Training Epoch: 83 [14336/50176]	Loss: 0.3326
Training Epoch: 83 [14592/50176]	Loss: 0.2935
Training Epoch: 83 [14848/50176]	Loss: 0.3499
Training Epoch: 83 [15104/50176]	Loss: 0.3886
Training Epoch: 83 [15360/50176]	Loss: 0.3105
Training Epoch: 83 [15616/50176]	Loss: 0.2784
Training Epoch: 83 [15872/50176]	Loss: 0.2876
Training Epoch: 83 [16128/50176]	Loss: 0.2599
Training Epoch: 83 [16384/50176]	Loss: 0.3215
Training Epoch: 83 [16640/50176]	Loss: 0.2842
Training Epoch: 83 [16896/50176]	Loss: 0.3939
Training Epoch: 83 [17152/50176]	Loss: 0.4612
Training Epoch: 83 [17408/50176]	Loss: 0.3095
Training Epoch: 83 [17664/50176]	Loss: 0.2445
Training Epoch: 83 [17920/50176]	Loss: 0.3408
Training Epoch: 83 [18176/50176]	Loss: 0.2735
Training Epoch: 83 [18432/50176]	Loss: 0.2628
Training Epoch: 83 [18688/50176]	Loss: 0.3080
Training Epoch: 83 [18944/50176]	Loss: 0.3336
Training Epoch: 83 [19200/50176]	Loss: 0.2719
Training Epoch: 83 [19456/50176]	Loss: 0.3711
Training Epoch: 83 [19712/50176]	Loss: 0.2704
Training Epoch: 83 [19968/50176]	Loss: 0.3213
Training Epoch: 83 [20224/50176]	Loss: 0.3339
Training Epoch: 83 [20480/50176]	Loss: 0.3350
Training Epoch: 83 [20736/50176]	Loss: 0.3466
Training Epoch: 83 [20992/50176]	Loss: 0.4482
Training Epoch: 83 [21248/50176]	Loss: 0.2977
Training Epoch: 83 [21504/50176]	Loss: 0.4589
Training Epoch: 83 [21760/50176]	Loss: 0.3645
Training Epoch: 83 [22016/50176]	Loss: 0.3160
Training Epoch: 83 [22272/50176]	Loss: 0.2957
Training Epoch: 83 [22528/50176]	Loss: 0.2986
Training Epoch: 83 [22784/50176]	Loss: 0.3833
Training Epoch: 83 [23040/50176]	Loss: 0.2606
Training Epoch: 83 [23296/50176]	Loss: 0.3004
Training Epoch: 83 [23552/50176]	Loss: 0.2792
Training Epoch: 83 [23808/50176]	Loss: 0.2882
Training Epoch: 83 [24064/50176]	Loss: 0.3690
Training Epoch: 83 [24320/50176]	Loss: 0.3152
Training Epoch: 83 [24576/50176]	Loss: 0.3730
Training Epoch: 83 [24832/50176]	Loss: 0.4223
Training Epoch: 83 [25088/50176]	Loss: 0.2873
Training Epoch: 83 [25344/50176]	Loss: 0.2504
Training Epoch: 83 [25600/50176]	Loss: 0.3029
Training Epoch: 83 [25856/50176]	Loss: 0.3757
Training Epoch: 83 [26112/50176]	Loss: 0.3568
Training Epoch: 83 [26368/50176]	Loss: 0.2882
Training Epoch: 83 [26624/50176]	Loss: 0.2660
Training Epoch: 83 [26880/50176]	Loss: 0.2835
Training Epoch: 83 [27136/50176]	Loss: 0.2243
Training Epoch: 83 [27392/50176]	Loss: 0.3152
Training Epoch: 83 [27648/50176]	Loss: 0.3661
Training Epoch: 83 [27904/50176]	Loss: 0.3618
Training Epoch: 83 [28160/50176]	Loss: 0.3079
Training Epoch: 83 [28416/50176]	Loss: 0.3251
Training Epoch: 83 [28672/50176]	Loss: 0.3743
Training Epoch: 83 [28928/50176]	Loss: 0.2832
Training Epoch: 83 [29184/50176]	Loss: 0.3519
Training Epoch: 83 [29440/50176]	Loss: 0.3072
Training Epoch: 83 [29696/50176]	Loss: 0.3877
Training Epoch: 83 [29952/50176]	Loss: 0.3232
Training Epoch: 83 [30208/50176]	Loss: 0.3308
Training Epoch: 83 [30464/50176]	Loss: 0.2757
Training Epoch: 83 [30720/50176]	Loss: 0.3366
Training Epoch: 83 [30976/50176]	Loss: 0.3449
Training Epoch: 83 [31232/50176]	Loss: 0.2384
Training Epoch: 83 [31488/50176]	Loss: 0.4310
Training Epoch: 83 [31744/50176]	Loss: 0.4221
Training Epoch: 83 [32000/50176]	Loss: 0.3467
Training Epoch: 83 [32256/50176]	Loss: 0.4407
Training Epoch: 83 [32512/50176]	Loss: 0.3304
Training Epoch: 83 [32768/50176]	Loss: 0.2561
Training Epoch: 83 [33024/50176]	Loss: 0.4295
Training Epoch: 83 [33280/50176]	Loss: 0.2993
Training Epoch: 83 [33536/50176]	Loss: 0.2484
Training Epoch: 83 [33792/50176]	Loss: 0.3957
Training Epoch: 83 [34048/50176]	Loss: 0.3573
Training Epoch: 83 [34304/50176]	Loss: 0.2760
Training Epoch: 83 [34560/50176]	Loss: 0.3432
Training Epoch: 83 [34816/50176]	Loss: 0.3924
Training Epoch: 83 [35072/50176]	Loss: 0.3016
Training Epoch: 83 [35328/50176]	Loss: 0.3920
Training Epoch: 83 [35584/50176]	Loss: 0.3239
Training Epoch: 83 [35840/50176]	Loss: 0.4495
Training Epoch: 83 [36096/50176]	Loss: 0.3175
Training Epoch: 83 [36352/50176]	Loss: 0.3028
Training Epoch: 83 [36608/50176]	Loss: 0.3887
Training Epoch: 83 [36864/50176]	Loss: 0.3831
Training Epoch: 83 [37120/50176]	Loss: 0.2639
Training Epoch: 83 [37376/50176]	Loss: 0.3629
Training Epoch: 83 [37632/50176]	Loss: 0.2680
Training Epoch: 83 [37888/50176]	Loss: 0.3471
Training Epoch: 83 [38144/50176]	Loss: 0.3547
Training Epoch: 83 [38400/50176]	Loss: 0.2971
Training Epoch: 83 [38656/50176]	Loss: 0.3300
Training Epoch: 83 [38912/50176]	Loss: 0.3735
Training Epoch: 83 [39168/50176]	Loss: 0.3122
Training Epoch: 83 [39424/50176]	Loss: 0.2709
Training Epoch: 83 [39680/50176]	Loss: 0.2596
Training Epoch: 83 [39936/50176]	Loss: 0.3694
Training Epoch: 83 [40192/50176]	Loss: 0.3795
Training Epoch: 83 [40448/50176]	Loss: 0.4969
Training Epoch: 83 [40704/50176]	Loss: 0.3776
Training Epoch: 83 [40960/50176]	Loss: 0.2492
Training Epoch: 83 [41216/50176]	Loss: 0.3102
Training Epoch: 83 [41472/50176]	Loss: 0.3729
Training Epoch: 83 [41728/50176]	Loss: 0.2553
Training Epoch: 83 [41984/50176]	Loss: 0.3072
Training Epoch: 83 [42240/50176]	Loss: 0.3612
Training Epoch: 83 [42496/50176]	Loss: 0.4140
Training Epoch: 83 [42752/50176]	Loss: 0.3305
Training Epoch: 83 [43008/50176]	Loss: 0.3049
Training Epoch: 83 [43264/50176]	Loss: 0.3718
Training Epoch: 83 [43520/50176]	Loss: 0.3029
Training Epoch: 83 [43776/50176]	Loss: 0.2704
Training Epoch: 83 [44032/50176]	Loss: 0.3787
Training Epoch: 83 [44288/50176]	Loss: 0.3794
Training Epoch: 83 [44544/50176]	Loss: 0.3578
Training Epoch: 83 [44800/50176]	Loss: 0.3961
Training Epoch: 83 [45056/50176]	Loss: 0.3315
Training Epoch: 83 [45312/50176]	Loss: 0.4402
Training Epoch: 83 [45568/50176]	Loss: 0.2612
Training Epoch: 83 [45824/50176]	Loss: 0.2982
Training Epoch: 83 [46080/50176]	Loss: 0.2543
Training Epoch: 83 [46336/50176]	Loss: 0.3252
Training Epoch: 83 [46592/50176]	Loss: 0.3526
Training Epoch: 83 [46848/50176]	Loss: 0.3374
Training Epoch: 83 [47104/50176]	Loss: 0.3881
Training Epoch: 83 [47360/50176]	Loss: 0.3325
Training Epoch: 83 [47616/50176]	Loss: 0.3778
Training Epoch: 83 [47872/50176]	Loss: 0.3628
Training Epoch: 83 [48128/50176]	Loss: 0.3771
Training Epoch: 83 [48384/50176]	Loss: 0.2324
Training Epoch: 83 [48640/50176]	Loss: 0.3318
Training Epoch: 83 [48896/50176]	Loss: 0.3736
Training Epoch: 83 [49152/50176]	Loss: 0.4138
Training Epoch: 83 [49408/50176]	Loss: 0.3272
Training Epoch: 83 [49664/50176]	Loss: 0.2827
Training Epoch: 83 [49920/50176]	Loss: 0.3547
Training Epoch: 83 [50176/50176]	Loss: 0.9187
Validation Epoch: 83, Average loss: 0.0103, Accuracy: 0.5786
Training Epoch: 84 [256/50176]	Loss: 0.3584
Training Epoch: 84 [512/50176]	Loss: 0.2588
Training Epoch: 84 [768/50176]	Loss: 0.2831
Training Epoch: 84 [1024/50176]	Loss: 0.3174
Training Epoch: 84 [1280/50176]	Loss: 0.3686
Training Epoch: 84 [1536/50176]	Loss: 0.2673
Training Epoch: 84 [1792/50176]	Loss: 0.2868
Training Epoch: 84 [2048/50176]	Loss: 0.2845
Training Epoch: 84 [2304/50176]	Loss: 0.2440
Training Epoch: 84 [2560/50176]	Loss: 0.3102
Training Epoch: 84 [2816/50176]	Loss: 0.2878
Training Epoch: 84 [3072/50176]	Loss: 0.2582
Training Epoch: 84 [3328/50176]	Loss: 0.3002
Training Epoch: 84 [3584/50176]	Loss: 0.2365
Training Epoch: 84 [3840/50176]	Loss: 0.3026
Training Epoch: 84 [4096/50176]	Loss: 0.2505
Training Epoch: 84 [4352/50176]	Loss: 0.3279
Training Epoch: 84 [4608/50176]	Loss: 0.2005
Training Epoch: 84 [4864/50176]	Loss: 0.2755
Training Epoch: 84 [5120/50176]	Loss: 0.3641
Training Epoch: 84 [5376/50176]	Loss: 0.4461
Training Epoch: 84 [5632/50176]	Loss: 0.3239
Training Epoch: 84 [5888/50176]	Loss: 0.3303
Training Epoch: 84 [6144/50176]	Loss: 0.3271
Training Epoch: 84 [6400/50176]	Loss: 0.2085
Training Epoch: 84 [6656/50176]	Loss: 0.2899
Training Epoch: 84 [6912/50176]	Loss: 0.3125
Training Epoch: 84 [7168/50176]	Loss: 0.3963
Training Epoch: 84 [7424/50176]	Loss: 0.3024
Training Epoch: 84 [7680/50176]	Loss: 0.4200
Training Epoch: 84 [7936/50176]	Loss: 0.2447
Training Epoch: 84 [8192/50176]	Loss: 0.2494
Training Epoch: 84 [8448/50176]	Loss: 0.2891
Training Epoch: 84 [8704/50176]	Loss: 0.3539
Training Epoch: 84 [8960/50176]	Loss: 0.3959
Training Epoch: 84 [9216/50176]	Loss: 0.3479
Training Epoch: 84 [9472/50176]	Loss: 0.3433
Training Epoch: 84 [9728/50176]	Loss: 0.2345
Training Epoch: 84 [9984/50176]	Loss: 0.2947
Training Epoch: 84 [10240/50176]	Loss: 0.2210
Training Epoch: 84 [10496/50176]	Loss: 0.2983
Training Epoch: 84 [10752/50176]	Loss: 0.2549
Training Epoch: 84 [11008/50176]	Loss: 0.3229
Training Epoch: 84 [11264/50176]	Loss: 0.3032
Training Epoch: 84 [11520/50176]	Loss: 0.2343
Training Epoch: 84 [11776/50176]	Loss: 0.2751
Training Epoch: 84 [12032/50176]	Loss: 0.3630
Training Epoch: 84 [12288/50176]	Loss: 0.2827
Training Epoch: 84 [12544/50176]	Loss: 0.3496
Training Epoch: 84 [12800/50176]	Loss: 0.2453
Training Epoch: 84 [13056/50176]	Loss: 0.2157
Training Epoch: 84 [13312/50176]	Loss: 0.2650
Training Epoch: 84 [13568/50176]	Loss: 0.2899
Training Epoch: 84 [13824/50176]	Loss: 0.3634
Training Epoch: 84 [14080/50176]	Loss: 0.4087
Training Epoch: 84 [14336/50176]	Loss: 0.2983
Training Epoch: 84 [14592/50176]	Loss: 0.3245
Training Epoch: 84 [14848/50176]	Loss: 0.2535
Training Epoch: 84 [15104/50176]	Loss: 0.3212
Training Epoch: 84 [15360/50176]	Loss: 0.3216
Training Epoch: 84 [15616/50176]	Loss: 0.3568
Training Epoch: 84 [15872/50176]	Loss: 0.3139
Training Epoch: 84 [16128/50176]	Loss: 0.3386
Training Epoch: 84 [16384/50176]	Loss: 0.2371
Training Epoch: 84 [16640/50176]	Loss: 0.4006
Training Epoch: 84 [16896/50176]	Loss: 0.2130
Training Epoch: 84 [17152/50176]	Loss: 0.2305
Training Epoch: 84 [17408/50176]	Loss: 0.3052
Training Epoch: 84 [17664/50176]	Loss: 0.3930
Training Epoch: 84 [17920/50176]	Loss: 0.2687
Training Epoch: 84 [18176/50176]	Loss: 0.2763
Training Epoch: 84 [18432/50176]	Loss: 0.3505
Training Epoch: 84 [18688/50176]	Loss: 0.4583
Training Epoch: 84 [18944/50176]	Loss: 0.2650
Training Epoch: 84 [19200/50176]	Loss: 0.2971
Training Epoch: 84 [19456/50176]	Loss: 0.3525
Training Epoch: 84 [19712/50176]	Loss: 0.3047
Training Epoch: 84 [19968/50176]	Loss: 0.2852
Training Epoch: 84 [20224/50176]	Loss: 0.3749
Training Epoch: 84 [20480/50176]	Loss: 0.2804
Training Epoch: 84 [20736/50176]	Loss: 0.3025
Training Epoch: 84 [20992/50176]	Loss: 0.2988
Training Epoch: 84 [21248/50176]	Loss: 0.3909
Training Epoch: 84 [21504/50176]	Loss: 0.3115
Training Epoch: 84 [21760/50176]	Loss: 0.4566
Training Epoch: 84 [22016/50176]	Loss: 0.3029
Training Epoch: 84 [22272/50176]	Loss: 0.3763
Training Epoch: 84 [22528/50176]	Loss: 0.3069
Training Epoch: 84 [22784/50176]	Loss: 0.3253
Training Epoch: 84 [23040/50176]	Loss: 0.3491
Training Epoch: 84 [23296/50176]	Loss: 0.2364
Training Epoch: 84 [23552/50176]	Loss: 0.3671
Training Epoch: 84 [23808/50176]	Loss: 0.3550
Training Epoch: 84 [24064/50176]	Loss: 0.3777
Training Epoch: 84 [24320/50176]	Loss: 0.3713
Training Epoch: 84 [24576/50176]	Loss: 0.3303
Training Epoch: 84 [24832/50176]	Loss: 0.4299
Training Epoch: 84 [25088/50176]	Loss: 0.3637
Training Epoch: 84 [25344/50176]	Loss: 0.3242
Training Epoch: 84 [25600/50176]	Loss: 0.2486
Training Epoch: 84 [25856/50176]	Loss: 0.4056
Training Epoch: 84 [26112/50176]	Loss: 0.3437
Training Epoch: 84 [26368/50176]	Loss: 0.2618
Training Epoch: 84 [26624/50176]	Loss: 0.3446
Training Epoch: 84 [26880/50176]	Loss: 0.2608
Training Epoch: 84 [27136/50176]	Loss: 0.3261
Training Epoch: 84 [27392/50176]	Loss: 0.2371
Training Epoch: 84 [27648/50176]	Loss: 0.2418
Training Epoch: 84 [27904/50176]	Loss: 0.3444
Training Epoch: 84 [28160/50176]	Loss: 0.3158
Training Epoch: 84 [28416/50176]	Loss: 0.3163
Training Epoch: 84 [28672/50176]	Loss: 0.3381
Training Epoch: 84 [28928/50176]	Loss: 0.2968
Training Epoch: 84 [29184/50176]	Loss: 0.2402
Training Epoch: 84 [29440/50176]	Loss: 0.4314
Training Epoch: 84 [29696/50176]	Loss: 0.2853
Training Epoch: 84 [29952/50176]	Loss: 0.3130
Training Epoch: 84 [30208/50176]	Loss: 0.2959
Training Epoch: 84 [30464/50176]	Loss: 0.3177
Training Epoch: 84 [30720/50176]	Loss: 0.3925
Training Epoch: 84 [30976/50176]	Loss: 0.3976
Training Epoch: 84 [31232/50176]	Loss: 0.3001
Training Epoch: 84 [31488/50176]	Loss: 0.4224
Training Epoch: 84 [31744/50176]	Loss: 0.3964
Training Epoch: 84 [32000/50176]	Loss: 0.3099
Training Epoch: 84 [32256/50176]	Loss: 0.3703
Training Epoch: 84 [32512/50176]	Loss: 0.3269
Training Epoch: 84 [32768/50176]	Loss: 0.2903
Training Epoch: 84 [33024/50176]	Loss: 0.3405
Training Epoch: 84 [33280/50176]	Loss: 0.3430
Training Epoch: 84 [33536/50176]	Loss: 0.3141
Training Epoch: 84 [33792/50176]	Loss: 0.3116
Training Epoch: 84 [34048/50176]	Loss: 0.3419
Training Epoch: 84 [34304/50176]	Loss: 0.3306
Training Epoch: 84 [34560/50176]	Loss: 0.2797
Training Epoch: 84 [34816/50176]	Loss: 0.3150
Training Epoch: 84 [35072/50176]	Loss: 0.2649
Training Epoch: 84 [35328/50176]	Loss: 0.2406
Training Epoch: 84 [35584/50176]	Loss: 0.2740
Training Epoch: 84 [35840/50176]	Loss: 0.2608
Training Epoch: 84 [36096/50176]	Loss: 0.3261
Training Epoch: 84 [36352/50176]	Loss: 0.4253
Training Epoch: 84 [36608/50176]	Loss: 0.3945
Training Epoch: 84 [36864/50176]	Loss: 0.3828
Training Epoch: 84 [37120/50176]	Loss: 0.2858
Training Epoch: 84 [37376/50176]	Loss: 0.3069
Training Epoch: 84 [37632/50176]	Loss: 0.3982
Training Epoch: 84 [37888/50176]	Loss: 0.3467
Training Epoch: 84 [38144/50176]	Loss: 0.3181
Training Epoch: 84 [38400/50176]	Loss: 0.4074
Training Epoch: 84 [38656/50176]	Loss: 0.3630
Training Epoch: 84 [38912/50176]	Loss: 0.3343
Training Epoch: 84 [39168/50176]	Loss: 0.2795
Training Epoch: 84 [39424/50176]	Loss: 0.2190
Training Epoch: 84 [39680/50176]	Loss: 0.3174
Training Epoch: 84 [39936/50176]	Loss: 0.2876
Training Epoch: 84 [40192/50176]	Loss: 0.2935
Training Epoch: 84 [40448/50176]	Loss: 0.2780
Training Epoch: 84 [40704/50176]	Loss: 0.2960
Training Epoch: 84 [40960/50176]	Loss: 0.2373
Training Epoch: 84 [41216/50176]	Loss: 0.2681
Training Epoch: 84 [41472/50176]	Loss: 0.3372
Training Epoch: 84 [41728/50176]	Loss: 0.3532
Training Epoch: 84 [41984/50176]	Loss: 0.3759
Training Epoch: 84 [42240/50176]	Loss: 0.3357
Training Epoch: 84 [42496/50176]	Loss: 0.3259
Training Epoch: 84 [42752/50176]	Loss: 0.2591
Training Epoch: 84 [43008/50176]	Loss: 0.3136
Training Epoch: 84 [43264/50176]	Loss: 0.2600
Training Epoch: 84 [43520/50176]	Loss: 0.3657
Training Epoch: 84 [43776/50176]	Loss: 0.2792
Training Epoch: 84 [44032/50176]	Loss: 0.3873
Training Epoch: 84 [44288/50176]	Loss: 0.3252
Training Epoch: 84 [44544/50176]	Loss: 0.3615
Training Epoch: 84 [44800/50176]	Loss: 0.2589
Training Epoch: 84 [45056/50176]	Loss: 0.3814
Training Epoch: 84 [45312/50176]	Loss: 0.3105
Training Epoch: 84 [45568/50176]	Loss: 0.2844
Training Epoch: 84 [45824/50176]	Loss: 0.3427
Training Epoch: 84 [46080/50176]	Loss: 0.3625
Training Epoch: 84 [46336/50176]	Loss: 0.3351
Training Epoch: 84 [46592/50176]	Loss: 0.3482
Training Epoch: 84 [46848/50176]	Loss: 0.3298
Training Epoch: 84 [47104/50176]	Loss: 0.3797
Training Epoch: 84 [47360/50176]	Loss: 0.3147
Training Epoch: 84 [47616/50176]	Loss: 0.3873
Training Epoch: 84 [47872/50176]	Loss: 0.2794
Training Epoch: 84 [48128/50176]	Loss: 0.3008
Training Epoch: 84 [48384/50176]	Loss: 0.3092
Training Epoch: 84 [48640/50176]	Loss: 0.3731
Training Epoch: 84 [48896/50176]	Loss: 0.3435
Training Epoch: 84 [49152/50176]	Loss: 0.2591
Training Epoch: 84 [49408/50176]	Loss: 0.3766
Training Epoch: 84 [49664/50176]	Loss: 0.3980
Training Epoch: 84 [49920/50176]	Loss: 0.2206
Training Epoch: 84 [50176/50176]	Loss: 0.2779
Validation Epoch: 84, Average loss: 0.0103, Accuracy: 0.5852
Training Epoch: 85 [256/50176]	Loss: 0.2694
Training Epoch: 85 [512/50176]	Loss: 0.3248
Training Epoch: 85 [768/50176]	Loss: 0.3348
Training Epoch: 85 [1024/50176]	Loss: 0.2721
Training Epoch: 85 [1280/50176]	Loss: 0.2995
Training Epoch: 85 [1536/50176]	Loss: 0.2543
Training Epoch: 85 [1792/50176]	Loss: 0.2626
Training Epoch: 85 [2048/50176]	Loss: 0.3301
Training Epoch: 85 [2304/50176]	Loss: 0.3528
Training Epoch: 85 [2560/50176]	Loss: 0.3162
Training Epoch: 85 [2816/50176]	Loss: 0.3484
Training Epoch: 85 [3072/50176]	Loss: 0.2895
Training Epoch: 85 [3328/50176]	Loss: 0.2105
Training Epoch: 85 [3584/50176]	Loss: 0.3227
Training Epoch: 85 [3840/50176]	Loss: 0.3025
Training Epoch: 85 [4096/50176]	Loss: 0.3492
Training Epoch: 85 [4352/50176]	Loss: 0.3346
Training Epoch: 85 [4608/50176]	Loss: 0.3456
Training Epoch: 85 [4864/50176]	Loss: 0.2986
Training Epoch: 85 [5120/50176]	Loss: 0.2903
Training Epoch: 85 [5376/50176]	Loss: 0.3527
Training Epoch: 85 [5632/50176]	Loss: 0.3221
Training Epoch: 85 [5888/50176]	Loss: 0.2063
Training Epoch: 85 [6144/50176]	Loss: 0.1988
Training Epoch: 85 [6400/50176]	Loss: 0.3692
Training Epoch: 85 [6656/50176]	Loss: 0.3833
Training Epoch: 85 [6912/50176]	Loss: 0.2887
Training Epoch: 85 [7168/50176]	Loss: 0.3332
Training Epoch: 85 [7424/50176]	Loss: 0.2660
Training Epoch: 85 [7680/50176]	Loss: 0.2869
Training Epoch: 85 [7936/50176]	Loss: 0.3663
Training Epoch: 85 [8192/50176]	Loss: 0.2901
Training Epoch: 85 [8448/50176]	Loss: 0.2294
Training Epoch: 85 [8704/50176]	Loss: 0.3789
Training Epoch: 85 [8960/50176]	Loss: 0.3572
Training Epoch: 85 [9216/50176]	Loss: 0.3124
Training Epoch: 85 [9472/50176]	Loss: 0.2391
Training Epoch: 85 [9728/50176]	Loss: 0.3018
Training Epoch: 85 [9984/50176]	Loss: 0.3363
Training Epoch: 85 [10240/50176]	Loss: 0.3250
Training Epoch: 85 [10496/50176]	Loss: 0.2997
Training Epoch: 85 [10752/50176]	Loss: 0.3012
Training Epoch: 85 [11008/50176]	Loss: 0.2760
Training Epoch: 85 [11264/50176]	Loss: 0.3521
Training Epoch: 85 [11520/50176]	Loss: 0.4376
Training Epoch: 85 [11776/50176]	Loss: 0.3379
Training Epoch: 85 [12032/50176]	Loss: 0.2512
Training Epoch: 85 [12288/50176]	Loss: 0.3179
Training Epoch: 85 [12544/50176]	Loss: 0.3094
Training Epoch: 85 [12800/50176]	Loss: 0.1877
Training Epoch: 85 [13056/50176]	Loss: 0.3712
Training Epoch: 85 [13312/50176]	Loss: 0.2248
Training Epoch: 85 [13568/50176]	Loss: 0.3369
Training Epoch: 85 [13824/50176]	Loss: 0.2856
Training Epoch: 85 [14080/50176]	Loss: 0.3338
Training Epoch: 85 [14336/50176]	Loss: 0.2631
Training Epoch: 85 [14592/50176]	Loss: 0.2633
Training Epoch: 85 [14848/50176]	Loss: 0.2677
Training Epoch: 85 [15104/50176]	Loss: 0.2782
Training Epoch: 85 [15360/50176]	Loss: 0.2495
Training Epoch: 85 [15616/50176]	Loss: 0.2651
Training Epoch: 85 [15872/50176]	Loss: 0.2679
Training Epoch: 85 [16128/50176]	Loss: 0.3638
Training Epoch: 85 [16384/50176]	Loss: 0.2632
Training Epoch: 85 [16640/50176]	Loss: 0.1972
Training Epoch: 85 [16896/50176]	Loss: 0.2700
Training Epoch: 85 [17152/50176]	Loss: 0.3071
Training Epoch: 85 [17408/50176]	Loss: 0.2004
Training Epoch: 85 [17664/50176]	Loss: 0.2629
Training Epoch: 85 [17920/50176]	Loss: 0.2365
Training Epoch: 85 [18176/50176]	Loss: 0.3524
Training Epoch: 85 [18432/50176]	Loss: 0.2423
Training Epoch: 85 [18688/50176]	Loss: 0.2843
Training Epoch: 85 [18944/50176]	Loss: 0.2862
Training Epoch: 85 [19200/50176]	Loss: 0.3156
Training Epoch: 85 [19456/50176]	Loss: 0.3067
Training Epoch: 85 [19712/50176]	Loss: 0.2063
Training Epoch: 85 [19968/50176]	Loss: 0.2947
Training Epoch: 85 [20224/50176]	Loss: 0.2308
Training Epoch: 85 [20480/50176]	Loss: 0.2694
Training Epoch: 85 [20736/50176]	Loss: 0.3337
Training Epoch: 85 [20992/50176]	Loss: 0.3375
Training Epoch: 85 [21248/50176]	Loss: 0.3037
Training Epoch: 85 [21504/50176]	Loss: 0.2981
Training Epoch: 85 [21760/50176]	Loss: 0.3196
Training Epoch: 85 [22016/50176]	Loss: 0.3309
Training Epoch: 85 [22272/50176]	Loss: 0.3402
Training Epoch: 85 [22528/50176]	Loss: 0.3548
Training Epoch: 85 [22784/50176]	Loss: 0.3152
Training Epoch: 85 [23040/50176]	Loss: 0.3008
Training Epoch: 85 [23296/50176]	Loss: 0.3196
Training Epoch: 85 [23552/50176]	Loss: 0.2372
Training Epoch: 85 [23808/50176]	Loss: 0.3728
Training Epoch: 85 [24064/50176]	Loss: 0.2906
Training Epoch: 85 [24320/50176]	Loss: 0.2968
Training Epoch: 85 [24576/50176]	Loss: 0.2578
Training Epoch: 85 [24832/50176]	Loss: 0.3326
Training Epoch: 85 [25088/50176]	Loss: 0.2555
Training Epoch: 85 [25344/50176]	Loss: 0.2512
Training Epoch: 85 [25600/50176]	Loss: 0.2561
Training Epoch: 85 [25856/50176]	Loss: 0.2758
Training Epoch: 85 [26112/50176]	Loss: 0.3661
Training Epoch: 85 [26368/50176]	Loss: 0.2627
Training Epoch: 85 [26624/50176]	Loss: 0.2933
Training Epoch: 85 [26880/50176]	Loss: 0.3736
Training Epoch: 85 [27136/50176]	Loss: 0.3174
Training Epoch: 85 [27392/50176]	Loss: 0.2970
Training Epoch: 85 [27648/50176]	Loss: 0.3280
Training Epoch: 85 [27904/50176]	Loss: 0.2540
Training Epoch: 85 [28160/50176]	Loss: 0.3370
Training Epoch: 85 [28416/50176]	Loss: 0.3227
Training Epoch: 85 [28672/50176]	Loss: 0.3969
Training Epoch: 85 [28928/50176]	Loss: 0.2635
Training Epoch: 85 [29184/50176]	Loss: 0.3839
Training Epoch: 85 [29440/50176]	Loss: 0.2971
Training Epoch: 85 [29696/50176]	Loss: 0.3747
Training Epoch: 85 [29952/50176]	Loss: 0.3533
Training Epoch: 85 [30208/50176]	Loss: 0.2528
Training Epoch: 85 [30464/50176]	Loss: 0.3267
Training Epoch: 85 [30720/50176]	Loss: 0.3444
Training Epoch: 85 [30976/50176]	Loss: 0.2717
Training Epoch: 85 [31232/50176]	Loss: 0.2945
Training Epoch: 85 [31488/50176]	Loss: 0.2995
Training Epoch: 85 [31744/50176]	Loss: 0.3308
Training Epoch: 85 [32000/50176]	Loss: 0.2492
Training Epoch: 85 [32256/50176]	Loss: 0.3162
Training Epoch: 85 [32512/50176]	Loss: 0.3446
Training Epoch: 85 [32768/50176]	Loss: 0.2375
Training Epoch: 85 [33024/50176]	Loss: 0.3981
Training Epoch: 85 [33280/50176]	Loss: 0.3457
Training Epoch: 85 [33536/50176]	Loss: 0.2645
Training Epoch: 85 [33792/50176]	Loss: 0.3193
Training Epoch: 85 [34048/50176]	Loss: 0.2678
Training Epoch: 85 [34304/50176]	Loss: 0.2898
Training Epoch: 85 [34560/50176]	Loss: 0.3194
Training Epoch: 85 [34816/50176]	Loss: 0.3739
Training Epoch: 85 [35072/50176]	Loss: 0.2324
Training Epoch: 85 [35328/50176]	Loss: 0.3782
Training Epoch: 85 [35584/50176]	Loss: 0.3530
Training Epoch: 85 [35840/50176]	Loss: 0.2942
Training Epoch: 85 [36096/50176]	Loss: 0.2936
Training Epoch: 85 [36352/50176]	Loss: 0.2958
Training Epoch: 85 [36608/50176]	Loss: 0.2989
Training Epoch: 85 [36864/50176]	Loss: 0.2666
Training Epoch: 85 [37120/50176]	Loss: 0.4081
Training Epoch: 85 [37376/50176]	Loss: 0.3138
Training Epoch: 85 [37632/50176]	Loss: 0.3292
Training Epoch: 85 [37888/50176]	Loss: 0.3727
Training Epoch: 85 [38144/50176]	Loss: 0.2716
Training Epoch: 85 [38400/50176]	Loss: 0.2967
Training Epoch: 85 [38656/50176]	Loss: 0.2359
Training Epoch: 85 [38912/50176]	Loss: 0.3237
Training Epoch: 85 [39168/50176]	Loss: 0.4806
Training Epoch: 85 [39424/50176]	Loss: 0.2738
Training Epoch: 85 [39680/50176]	Loss: 0.3674
Training Epoch: 85 [39936/50176]	Loss: 0.2496
Training Epoch: 85 [40192/50176]	Loss: 0.2520
Training Epoch: 85 [40448/50176]	Loss: 0.3434
Training Epoch: 85 [40704/50176]	Loss: 0.3290
Training Epoch: 85 [40960/50176]	Loss: 0.3727
Training Epoch: 85 [41216/50176]	Loss: 0.2959
Training Epoch: 85 [41472/50176]	Loss: 0.3955
Training Epoch: 85 [41728/50176]	Loss: 0.2597
Training Epoch: 85 [41984/50176]	Loss: 0.3153
Training Epoch: 85 [42240/50176]	Loss: 0.3178
Training Epoch: 85 [42496/50176]	Loss: 0.3086
Training Epoch: 85 [42752/50176]	Loss: 0.3349
Training Epoch: 85 [43008/50176]	Loss: 0.3391
Training Epoch: 85 [43264/50176]	Loss: 0.3195
Training Epoch: 85 [43520/50176]	Loss: 0.2848
Training Epoch: 85 [43776/50176]	Loss: 0.3375
Training Epoch: 85 [44032/50176]	Loss: 0.3674
Training Epoch: 85 [44288/50176]	Loss: 0.3598
Training Epoch: 85 [44544/50176]	Loss: 0.3291
Training Epoch: 85 [44800/50176]	Loss: 0.2967
Training Epoch: 85 [45056/50176]	Loss: 0.4803
Training Epoch: 85 [45312/50176]	Loss: 0.3969
Training Epoch: 85 [45568/50176]	Loss: 0.3245
Training Epoch: 85 [45824/50176]	Loss: 0.3053
Training Epoch: 85 [46080/50176]	Loss: 0.2944
Training Epoch: 85 [46336/50176]	Loss: 0.4061
Training Epoch: 85 [46592/50176]	Loss: 0.3968
Training Epoch: 85 [46848/50176]	Loss: 0.3443
Training Epoch: 85 [47104/50176]	Loss: 0.3034
Training Epoch: 85 [47360/50176]	Loss: 0.4086
Training Epoch: 85 [47616/50176]	Loss: 0.3070
Training Epoch: 85 [47872/50176]	Loss: 0.3530
Training Epoch: 85 [48128/50176]	Loss: 0.2802
Training Epoch: 85 [48384/50176]	Loss: 0.3240
Training Epoch: 85 [48640/50176]	Loss: 0.3702
Training Epoch: 85 [48896/50176]	Loss: 0.2977
Training Epoch: 85 [49152/50176]	Loss: 0.4261
Training Epoch: 85 [49408/50176]	Loss: 0.3354
Training Epoch: 85 [49664/50176]	Loss: 0.3239
Training Epoch: 85 [49920/50176]	Loss: 0.3224
Training Epoch: 85 [50176/50176]	Loss: 0.5130
Validation Epoch: 85, Average loss: 0.0107, Accuracy: 0.5668
Training Epoch: 86 [256/50176]	Loss: 0.3244
Training Epoch: 86 [512/50176]	Loss: 0.2076
Training Epoch: 86 [768/50176]	Loss: 0.3389
Training Epoch: 86 [1024/50176]	Loss: 0.3108
Training Epoch: 86 [1280/50176]	Loss: 0.3060
Training Epoch: 86 [1536/50176]	Loss: 0.3212
Training Epoch: 86 [1792/50176]	Loss: 0.3358
Training Epoch: 86 [2048/50176]	Loss: 0.2741
Training Epoch: 86 [2304/50176]	Loss: 0.2887
Training Epoch: 86 [2560/50176]	Loss: 0.3177
Training Epoch: 86 [2816/50176]	Loss: 0.3100
Training Epoch: 86 [3072/50176]	Loss: 0.2774
Training Epoch: 86 [3328/50176]	Loss: 0.3533
Training Epoch: 86 [3584/50176]	Loss: 0.2970
Training Epoch: 86 [3840/50176]	Loss: 0.3822
Training Epoch: 86 [4096/50176]	Loss: 0.2632
Training Epoch: 86 [4352/50176]	Loss: 0.2553
Training Epoch: 86 [4608/50176]	Loss: 0.3785
Training Epoch: 86 [4864/50176]	Loss: 0.3041
Training Epoch: 86 [5120/50176]	Loss: 0.3032
Training Epoch: 86 [5376/50176]	Loss: 0.3238
Training Epoch: 86 [5632/50176]	Loss: 0.2622
Training Epoch: 86 [5888/50176]	Loss: 0.3579
Training Epoch: 86 [6144/50176]	Loss: 0.3243
Training Epoch: 86 [6400/50176]	Loss: 0.3585
Training Epoch: 86 [6656/50176]	Loss: 0.2624
Training Epoch: 86 [6912/50176]	Loss: 0.2544
Training Epoch: 86 [7168/50176]	Loss: 0.3034
Training Epoch: 86 [7424/50176]	Loss: 0.3217
Training Epoch: 86 [7680/50176]	Loss: 0.3049
Training Epoch: 86 [7936/50176]	Loss: 0.2571
Training Epoch: 86 [8192/50176]	Loss: 0.1916
Training Epoch: 86 [8448/50176]	Loss: 0.2715
Training Epoch: 86 [8704/50176]	Loss: 0.3713
Training Epoch: 86 [8960/50176]	Loss: 0.2611
Training Epoch: 86 [9216/50176]	Loss: 0.3592
Training Epoch: 86 [9472/50176]	Loss: 0.2547
Training Epoch: 86 [9728/50176]	Loss: 0.2620
Training Epoch: 86 [9984/50176]	Loss: 0.1976
Training Epoch: 86 [10240/50176]	Loss: 0.3561
Training Epoch: 86 [10496/50176]	Loss: 0.2939
Training Epoch: 86 [10752/50176]	Loss: 0.2792
Training Epoch: 86 [11008/50176]	Loss: 0.3928
Training Epoch: 86 [11264/50176]	Loss: 0.2644
Training Epoch: 86 [11520/50176]	Loss: 0.2643
Training Epoch: 86 [11776/50176]	Loss: 0.2666
Training Epoch: 86 [12032/50176]	Loss: 0.3425
Training Epoch: 86 [12288/50176]	Loss: 0.2377
Training Epoch: 86 [12544/50176]	Loss: 0.2675
Training Epoch: 86 [12800/50176]	Loss: 0.3528
Training Epoch: 86 [13056/50176]	Loss: 0.2739
Training Epoch: 86 [13312/50176]	Loss: 0.3174
Training Epoch: 86 [13568/50176]	Loss: 0.2318
Training Epoch: 86 [13824/50176]	Loss: 0.3623
Training Epoch: 86 [14080/50176]	Loss: 0.2944
Training Epoch: 86 [14336/50176]	Loss: 0.3129
Training Epoch: 86 [14592/50176]	Loss: 0.2258
Training Epoch: 86 [14848/50176]	Loss: 0.3615
Training Epoch: 86 [15104/50176]	Loss: 0.3491
Training Epoch: 86 [15360/50176]	Loss: 0.2982
Training Epoch: 86 [15616/50176]	Loss: 0.3361
Training Epoch: 86 [15872/50176]	Loss: 0.2111
Training Epoch: 86 [16128/50176]	Loss: 0.3340
Training Epoch: 86 [16384/50176]	Loss: 0.2412
Training Epoch: 86 [16640/50176]	Loss: 0.3223
Training Epoch: 86 [16896/50176]	Loss: 0.3087
Training Epoch: 86 [17152/50176]	Loss: 0.3519
Training Epoch: 86 [17408/50176]	Loss: 0.2581
Training Epoch: 86 [17664/50176]	Loss: 0.3052
Training Epoch: 86 [17920/50176]	Loss: 0.3079
Training Epoch: 86 [18176/50176]	Loss: 0.2073
Training Epoch: 86 [18432/50176]	Loss: 0.2890
Training Epoch: 86 [18688/50176]	Loss: 0.3547
Training Epoch: 86 [18944/50176]	Loss: 0.3467
Training Epoch: 86 [19200/50176]	Loss: 0.2867
Training Epoch: 86 [19456/50176]	Loss: 0.2340
Training Epoch: 86 [19712/50176]	Loss: 0.2876
Training Epoch: 86 [19968/50176]	Loss: 0.2979
Training Epoch: 86 [20224/50176]	Loss: 0.2790
Training Epoch: 86 [20480/50176]	Loss: 0.3215
Training Epoch: 86 [20736/50176]	Loss: 0.2317
Training Epoch: 86 [20992/50176]	Loss: 0.2143
Training Epoch: 86 [21248/50176]	Loss: 0.3550
Training Epoch: 86 [21504/50176]	Loss: 0.3227
Training Epoch: 86 [21760/50176]	Loss: 0.3661
Training Epoch: 86 [22016/50176]	Loss: 0.4008
Training Epoch: 86 [22272/50176]	Loss: 0.3379
Training Epoch: 86 [22528/50176]	Loss: 0.3561
Training Epoch: 86 [22784/50176]	Loss: 0.3732
Training Epoch: 86 [23040/50176]	Loss: 0.3328
Training Epoch: 86 [23296/50176]	Loss: 0.3601
Training Epoch: 86 [23552/50176]	Loss: 0.3200
Training Epoch: 86 [23808/50176]	Loss: 0.2714
Training Epoch: 86 [24064/50176]	Loss: 0.3593
Training Epoch: 86 [24320/50176]	Loss: 0.3591
Training Epoch: 86 [24576/50176]	Loss: 0.2190
Training Epoch: 86 [24832/50176]	Loss: 0.3918
Training Epoch: 86 [25088/50176]	Loss: 0.3404
Training Epoch: 86 [25344/50176]	Loss: 0.2668
Training Epoch: 86 [25600/50176]	Loss: 0.3164
Training Epoch: 86 [25856/50176]	Loss: 0.3006
Training Epoch: 86 [26112/50176]	Loss: 0.4145
Training Epoch: 86 [26368/50176]	Loss: 0.3237
Training Epoch: 86 [26624/50176]	Loss: 0.2969
Training Epoch: 86 [26880/50176]	Loss: 0.3662
Training Epoch: 86 [27136/50176]	Loss: 0.3166
Training Epoch: 86 [27392/50176]	Loss: 0.3631
Training Epoch: 86 [27648/50176]	Loss: 0.3662
Training Epoch: 86 [27904/50176]	Loss: 0.2442
Training Epoch: 86 [28160/50176]	Loss: 0.2632
Training Epoch: 86 [28416/50176]	Loss: 0.3299
Training Epoch: 86 [28672/50176]	Loss: 0.3753
Training Epoch: 86 [28928/50176]	Loss: 0.2653
Training Epoch: 86 [29184/50176]	Loss: 0.2613
Training Epoch: 86 [29440/50176]	Loss: 0.2687
Training Epoch: 86 [29696/50176]	Loss: 0.2820
Training Epoch: 86 [29952/50176]	Loss: 0.3428
Training Epoch: 86 [30208/50176]	Loss: 0.2953
Training Epoch: 86 [30464/50176]	Loss: 0.3282
Training Epoch: 86 [30720/50176]	Loss: 0.2274
Training Epoch: 86 [30976/50176]	Loss: 0.2544
Training Epoch: 86 [31232/50176]	Loss: 0.2922
Training Epoch: 86 [31488/50176]	Loss: 0.3789
Training Epoch: 86 [31744/50176]	Loss: 0.3083
Training Epoch: 86 [32000/50176]	Loss: 0.3985
Training Epoch: 86 [32256/50176]	Loss: 0.3204
Training Epoch: 86 [32512/50176]	Loss: 0.3151
Training Epoch: 86 [32768/50176]	Loss: 0.2202
Training Epoch: 86 [33024/50176]	Loss: 0.3407
Training Epoch: 86 [33280/50176]	Loss: 0.3363
Training Epoch: 86 [33536/50176]	Loss: 0.3333
Training Epoch: 86 [33792/50176]	Loss: 0.2900
Training Epoch: 86 [34048/50176]	Loss: 0.2927
Training Epoch: 86 [34304/50176]	Loss: 0.3528
Training Epoch: 86 [34560/50176]	Loss: 0.3028
Training Epoch: 86 [34816/50176]	Loss: 0.4285
Training Epoch: 86 [35072/50176]	Loss: 0.3623
Training Epoch: 86 [35328/50176]	Loss: 0.2995
Training Epoch: 86 [35584/50176]	Loss: 0.2672
Training Epoch: 86 [35840/50176]	Loss: 0.2586
Training Epoch: 86 [36096/50176]	Loss: 0.3434
Training Epoch: 86 [36352/50176]	Loss: 0.3302
Training Epoch: 86 [36608/50176]	Loss: 0.3430
Training Epoch: 86 [36864/50176]	Loss: 0.3442
Training Epoch: 86 [37120/50176]	Loss: 0.2833
Training Epoch: 86 [37376/50176]	Loss: 0.2630
Training Epoch: 86 [37632/50176]	Loss: 0.2733
Training Epoch: 86 [37888/50176]	Loss: 0.4032
Training Epoch: 86 [38144/50176]	Loss: 0.4154
Training Epoch: 86 [38400/50176]	Loss: 0.3238
Training Epoch: 86 [38656/50176]	Loss: 0.2908
Training Epoch: 86 [38912/50176]	Loss: 0.3600
Training Epoch: 86 [39168/50176]	Loss: 0.2965
Training Epoch: 86 [39424/50176]	Loss: 0.3608
Training Epoch: 86 [39680/50176]	Loss: 0.3292
Training Epoch: 86 [39936/50176]	Loss: 0.2667
Training Epoch: 86 [40192/50176]	Loss: 0.4425
Training Epoch: 86 [40448/50176]	Loss: 0.3757
Training Epoch: 86 [40704/50176]	Loss: 0.3197
Training Epoch: 86 [40960/50176]	Loss: 0.3558
Training Epoch: 86 [41216/50176]	Loss: 0.3607
Training Epoch: 86 [41472/50176]	Loss: 0.3300
Training Epoch: 86 [41728/50176]	Loss: 0.4393
Training Epoch: 86 [41984/50176]	Loss: 0.3576
Training Epoch: 86 [42240/50176]	Loss: 0.3340
Training Epoch: 86 [42496/50176]	Loss: 0.3408
Training Epoch: 86 [42752/50176]	Loss: 0.2828
Training Epoch: 86 [43008/50176]	Loss: 0.3059
Training Epoch: 86 [43264/50176]	Loss: 0.3473
Training Epoch: 86 [43520/50176]	Loss: 0.2939
Training Epoch: 86 [43776/50176]	Loss: 0.2597
Training Epoch: 86 [44032/50176]	Loss: 0.2883
Training Epoch: 86 [44288/50176]	Loss: 0.3972
Training Epoch: 86 [44544/50176]	Loss: 0.3980
Training Epoch: 86 [44800/50176]	Loss: 0.3406
Training Epoch: 86 [45056/50176]	Loss: 0.3399
Training Epoch: 86 [45312/50176]	Loss: 0.3364
Training Epoch: 86 [45568/50176]	Loss: 0.4116
Training Epoch: 86 [45824/50176]	Loss: 0.2453
Training Epoch: 86 [46080/50176]	Loss: 0.3267
Training Epoch: 86 [46336/50176]	Loss: 0.3484
Training Epoch: 86 [46592/50176]	Loss: 0.3938
Training Epoch: 86 [46848/50176]	Loss: 0.3671
Training Epoch: 86 [47104/50176]	Loss: 0.2754
Training Epoch: 86 [47360/50176]	Loss: 0.3046
Training Epoch: 86 [47616/50176]	Loss: 0.3806
Training Epoch: 86 [47872/50176]	Loss: 0.2985
Training Epoch: 86 [48128/50176]	Loss: 0.3489
Training Epoch: 86 [48384/50176]	Loss: 0.2933
Training Epoch: 86 [48640/50176]	Loss: 0.3251
Training Epoch: 86 [48896/50176]	Loss: 0.3636
Training Epoch: 86 [49152/50176]	Loss: 0.3080
Training Epoch: 86 [49408/50176]	Loss: 0.3734
Training Epoch: 86 [49664/50176]	Loss: 0.3832
Training Epoch: 86 [49920/50176]	Loss: 0.2789
Training Epoch: 86 [50176/50176]	Loss: 0.3608
Validation Epoch: 86, Average loss: 0.0103, Accuracy: 0.5725
Training Epoch: 87 [256/50176]	Loss: 0.2676
Training Epoch: 87 [512/50176]	Loss: 0.2847
Training Epoch: 87 [768/50176]	Loss: 0.2438
Training Epoch: 87 [1024/50176]	Loss: 0.3356
Training Epoch: 87 [1280/50176]	Loss: 0.2279
Training Epoch: 87 [1536/50176]	Loss: 0.2344
Training Epoch: 87 [1792/50176]	Loss: 0.2781
Training Epoch: 87 [2048/50176]	Loss: 0.3224
Training Epoch: 87 [2304/50176]	Loss: 0.2573
Training Epoch: 87 [2560/50176]	Loss: 0.2142
Training Epoch: 87 [2816/50176]	Loss: 0.2786
Training Epoch: 87 [3072/50176]	Loss: 0.3241
Training Epoch: 87 [3328/50176]	Loss: 0.3158
Training Epoch: 87 [3584/50176]	Loss: 0.2568
Training Epoch: 87 [3840/50176]	Loss: 0.2184
Training Epoch: 87 [4096/50176]	Loss: 0.2634
Training Epoch: 87 [4352/50176]	Loss: 0.2544
Training Epoch: 87 [4608/50176]	Loss: 0.3183
Training Epoch: 87 [4864/50176]	Loss: 0.2517
Training Epoch: 87 [5120/50176]	Loss: 0.2904
Training Epoch: 87 [5376/50176]	Loss: 0.2574
Training Epoch: 87 [5632/50176]	Loss: 0.3008
Training Epoch: 87 [5888/50176]	Loss: 0.3700
Training Epoch: 87 [6144/50176]	Loss: 0.2570
Training Epoch: 87 [6400/50176]	Loss: 0.2627
Training Epoch: 87 [6656/50176]	Loss: 0.3276
Training Epoch: 87 [6912/50176]	Loss: 0.4011
Training Epoch: 87 [7168/50176]	Loss: 0.3512
Training Epoch: 87 [7424/50176]	Loss: 0.2679
Training Epoch: 87 [7680/50176]	Loss: 0.2664
Training Epoch: 87 [7936/50176]	Loss: 0.3054
Training Epoch: 87 [8192/50176]	Loss: 0.3560
Training Epoch: 87 [8448/50176]	Loss: 0.3089
Training Epoch: 87 [8704/50176]	Loss: 0.2441
Training Epoch: 87 [8960/50176]	Loss: 0.2660
Training Epoch: 87 [9216/50176]	Loss: 0.3062
Training Epoch: 87 [9472/50176]	Loss: 0.2800
Training Epoch: 87 [9728/50176]	Loss: 0.2667
Training Epoch: 87 [9984/50176]	Loss: 0.3262
Training Epoch: 87 [10240/50176]	Loss: 0.3347
Training Epoch: 87 [10496/50176]	Loss: 0.2436
Training Epoch: 87 [10752/50176]	Loss: 0.2557
Training Epoch: 87 [11008/50176]	Loss: 0.2848
Training Epoch: 87 [11264/50176]	Loss: 0.2506
Training Epoch: 87 [11520/50176]	Loss: 0.3213
Training Epoch: 87 [11776/50176]	Loss: 0.2761
Training Epoch: 87 [12032/50176]	Loss: 0.2997
Training Epoch: 87 [12288/50176]	Loss: 0.2820
Training Epoch: 87 [12544/50176]	Loss: 0.2924
Training Epoch: 87 [12800/50176]	Loss: 0.2877
Training Epoch: 87 [13056/50176]	Loss: 0.3255
Training Epoch: 87 [13312/50176]	Loss: 0.3873
Training Epoch: 87 [13568/50176]	Loss: 0.2670
Training Epoch: 87 [13824/50176]	Loss: 0.2700
Training Epoch: 87 [14080/50176]	Loss: 0.3155
Training Epoch: 87 [14336/50176]	Loss: 0.3003
Training Epoch: 87 [14592/50176]	Loss: 0.2488
Training Epoch: 87 [14848/50176]	Loss: 0.2772
Training Epoch: 87 [15104/50176]	Loss: 0.3087
Training Epoch: 87 [15360/50176]	Loss: 0.2300
Training Epoch: 87 [15616/50176]	Loss: 0.4021
Training Epoch: 87 [15872/50176]	Loss: 0.3019
Training Epoch: 87 [16128/50176]	Loss: 0.2149
Training Epoch: 87 [16384/50176]	Loss: 0.3163
Training Epoch: 87 [16640/50176]	Loss: 0.2174
Training Epoch: 87 [16896/50176]	Loss: 0.3167
Training Epoch: 87 [17152/50176]	Loss: 0.2621
Training Epoch: 87 [17408/50176]	Loss: 0.3493
Training Epoch: 87 [17664/50176]	Loss: 0.2325
Training Epoch: 87 [17920/50176]	Loss: 0.2453
Training Epoch: 87 [18176/50176]	Loss: 0.2789
Training Epoch: 87 [18432/50176]	Loss: 0.2131
Training Epoch: 87 [18688/50176]	Loss: 0.3130
Training Epoch: 87 [18944/50176]	Loss: 0.2368
Training Epoch: 87 [19200/50176]	Loss: 0.3038
Training Epoch: 87 [19456/50176]	Loss: 0.2208
Training Epoch: 87 [19712/50176]	Loss: 0.3377
Training Epoch: 87 [19968/50176]	Loss: 0.2588
Training Epoch: 87 [20224/50176]	Loss: 0.2773
Training Epoch: 87 [20480/50176]	Loss: 0.3000
Training Epoch: 87 [20736/50176]	Loss: 0.2730
Training Epoch: 87 [20992/50176]	Loss: 0.2264
Training Epoch: 87 [21248/50176]	Loss: 0.2565
Training Epoch: 87 [21504/50176]	Loss: 0.2702
Training Epoch: 87 [21760/50176]	Loss: 0.3583
Training Epoch: 87 [22016/50176]	Loss: 0.2836
Training Epoch: 87 [22272/50176]	Loss: 0.3134
Training Epoch: 87 [22528/50176]	Loss: 0.2918
Training Epoch: 87 [22784/50176]	Loss: 0.2620
Training Epoch: 87 [23040/50176]	Loss: 0.3240
Training Epoch: 87 [23296/50176]	Loss: 0.3959
Training Epoch: 87 [23552/50176]	Loss: 0.1777
Training Epoch: 87 [23808/50176]	Loss: 0.4766
Training Epoch: 87 [24064/50176]	Loss: 0.3891
Training Epoch: 87 [24320/50176]	Loss: 0.2883
Training Epoch: 87 [24576/50176]	Loss: 0.2978
Training Epoch: 87 [24832/50176]	Loss: 0.3317
Training Epoch: 87 [25088/50176]	Loss: 0.3407
Training Epoch: 87 [25344/50176]	Loss: 0.2897
Training Epoch: 87 [25600/50176]	Loss: 0.2649
Training Epoch: 87 [25856/50176]	Loss: 0.2623
Training Epoch: 87 [26112/50176]	Loss: 0.2936
Training Epoch: 87 [26368/50176]	Loss: 0.2833
Training Epoch: 87 [26624/50176]	Loss: 0.2335
Training Epoch: 87 [26880/50176]	Loss: 0.2071
Training Epoch: 87 [27136/50176]	Loss: 0.3134
Training Epoch: 87 [27392/50176]	Loss: 0.2350
Training Epoch: 87 [27648/50176]	Loss: 0.3707
Training Epoch: 87 [27904/50176]	Loss: 0.3686
Training Epoch: 87 [28160/50176]	Loss: 0.3296
Training Epoch: 87 [28416/50176]	Loss: 0.2909
Training Epoch: 87 [28672/50176]	Loss: 0.2555
Training Epoch: 87 [28928/50176]	Loss: 0.3051
Training Epoch: 87 [29184/50176]	Loss: 0.3459
Training Epoch: 87 [29440/50176]	Loss: 0.2639
Training Epoch: 87 [29696/50176]	Loss: 0.2802
Training Epoch: 87 [29952/50176]	Loss: 0.3508
Training Epoch: 87 [30208/50176]	Loss: 0.2835
Training Epoch: 87 [30464/50176]	Loss: 0.2460
Training Epoch: 87 [30720/50176]	Loss: 0.4280
Training Epoch: 87 [30976/50176]	Loss: 0.4177
Training Epoch: 87 [31232/50176]	Loss: 0.2899
Training Epoch: 87 [31488/50176]	Loss: 0.2497
Training Epoch: 87 [31744/50176]	Loss: 0.2406
Training Epoch: 87 [32000/50176]	Loss: 0.2760
Training Epoch: 87 [32256/50176]	Loss: 0.2654
Training Epoch: 87 [32512/50176]	Loss: 0.3640
Training Epoch: 87 [32768/50176]	Loss: 0.2965
Training Epoch: 87 [33024/50176]	Loss: 0.3513
Training Epoch: 87 [33280/50176]	Loss: 0.1829
Training Epoch: 87 [33536/50176]	Loss: 0.3674
Training Epoch: 87 [33792/50176]	Loss: 0.3013
Training Epoch: 87 [34048/50176]	Loss: 0.3179
Training Epoch: 87 [34304/50176]	Loss: 0.1833
Training Epoch: 87 [34560/50176]	Loss: 0.3231
Training Epoch: 87 [34816/50176]	Loss: 0.2757
Training Epoch: 87 [35072/50176]	Loss: 0.3960
Training Epoch: 87 [35328/50176]	Loss: 0.3865
Training Epoch: 87 [35584/50176]	Loss: 0.3996
Training Epoch: 87 [35840/50176]	Loss: 0.2601
Training Epoch: 87 [36096/50176]	Loss: 0.3667
Training Epoch: 87 [36352/50176]	Loss: 0.3408
Training Epoch: 87 [36608/50176]	Loss: 0.3415
Training Epoch: 87 [36864/50176]	Loss: 0.3290
Training Epoch: 87 [37120/50176]	Loss: 0.3162
Training Epoch: 87 [37376/50176]	Loss: 0.2339
Training Epoch: 87 [37632/50176]	Loss: 0.2855
Training Epoch: 87 [37888/50176]	Loss: 0.2734
Training Epoch: 87 [38144/50176]	Loss: 0.3420
Training Epoch: 87 [38400/50176]	Loss: 0.2680
Training Epoch: 87 [38656/50176]	Loss: 0.3458
Training Epoch: 87 [38912/50176]	Loss: 0.2289
Training Epoch: 87 [39168/50176]	Loss: 0.3777
Training Epoch: 87 [39424/50176]	Loss: 0.3365
Training Epoch: 87 [39680/50176]	Loss: 0.2471
Training Epoch: 87 [39936/50176]	Loss: 0.3235
Training Epoch: 87 [40192/50176]	Loss: 0.3244
Training Epoch: 87 [40448/50176]	Loss: 0.2531
Training Epoch: 87 [40704/50176]	Loss: 0.4295
Training Epoch: 87 [40960/50176]	Loss: 0.3756
Training Epoch: 87 [41216/50176]	Loss: 0.2924
Training Epoch: 87 [41472/50176]	Loss: 0.3618
Training Epoch: 87 [41728/50176]	Loss: 0.2916
Training Epoch: 87 [41984/50176]	Loss: 0.2654
Training Epoch: 87 [42240/50176]	Loss: 0.2381
Training Epoch: 87 [42496/50176]	Loss: 0.3236
Training Epoch: 87 [42752/50176]	Loss: 0.3500
Training Epoch: 87 [43008/50176]	Loss: 0.2696
Training Epoch: 87 [43264/50176]	Loss: 0.2782
Training Epoch: 87 [43520/50176]	Loss: 0.2876
Training Epoch: 87 [43776/50176]	Loss: 0.2942
Training Epoch: 87 [44032/50176]	Loss: 0.2272
Training Epoch: 87 [44288/50176]	Loss: 0.3254
Training Epoch: 87 [44544/50176]	Loss: 0.3737
Training Epoch: 87 [44800/50176]	Loss: 0.3000
Training Epoch: 87 [45056/50176]	Loss: 0.3035
Training Epoch: 87 [45312/50176]	Loss: 0.3635
Training Epoch: 87 [45568/50176]	Loss: 0.2813
Training Epoch: 87 [45824/50176]	Loss: 0.3334
Training Epoch: 87 [46080/50176]	Loss: 0.2933
Training Epoch: 87 [46336/50176]	Loss: 0.3377
Training Epoch: 87 [46592/50176]	Loss: 0.1980
Training Epoch: 87 [46848/50176]	Loss: 0.2577
Training Epoch: 87 [47104/50176]	Loss: 0.2592
Training Epoch: 87 [47360/50176]	Loss: 0.3977
Training Epoch: 87 [47616/50176]	Loss: 0.3199
Training Epoch: 87 [47872/50176]	Loss: 0.2694
Training Epoch: 87 [48128/50176]	Loss: 0.3329
Training Epoch: 87 [48384/50176]	Loss: 0.3614
Training Epoch: 87 [48640/50176]	Loss: 0.3955
Training Epoch: 87 [48896/50176]	Loss: 0.3509
Training Epoch: 87 [49152/50176]	Loss: 0.2956
Training Epoch: 87 [49408/50176]	Loss: 0.3325
Training Epoch: 87 [49664/50176]	Loss: 0.3299
Training Epoch: 87 [49920/50176]	Loss: 0.2791
Training Epoch: 87 [50176/50176]	Loss: 0.3018
Validation Epoch: 87, Average loss: 0.0100, Accuracy: 0.5830
Training Epoch: 88 [256/50176]	Loss: 0.4194
Training Epoch: 88 [512/50176]	Loss: 0.2140
Training Epoch: 88 [768/50176]	Loss: 0.2264
Training Epoch: 88 [1024/50176]	Loss: 0.2874
Training Epoch: 88 [1280/50176]	Loss: 0.2365
Training Epoch: 88 [1536/50176]	Loss: 0.2463
Training Epoch: 88 [1792/50176]	Loss: 0.2660
Training Epoch: 88 [2048/50176]	Loss: 0.2175
Training Epoch: 88 [2304/50176]	Loss: 0.3326
Training Epoch: 88 [2560/50176]	Loss: 0.2303
Training Epoch: 88 [2816/50176]	Loss: 0.2167
Training Epoch: 88 [3072/50176]	Loss: 0.2966
Training Epoch: 88 [3328/50176]	Loss: 0.2215
Training Epoch: 88 [3584/50176]	Loss: 0.2602
Training Epoch: 88 [3840/50176]	Loss: 0.3043
Training Epoch: 88 [4096/50176]	Loss: 0.3546
Training Epoch: 88 [4352/50176]	Loss: 0.3486
Training Epoch: 88 [4608/50176]	Loss: 0.2516
Training Epoch: 88 [4864/50176]	Loss: 0.1812
Training Epoch: 88 [5120/50176]	Loss: 0.3534
Training Epoch: 88 [5376/50176]	Loss: 0.2998
Training Epoch: 88 [5632/50176]	Loss: 0.3619
Training Epoch: 88 [5888/50176]	Loss: 0.3707
Training Epoch: 88 [6144/50176]	Loss: 0.2742
Training Epoch: 88 [6400/50176]	Loss: 0.3031
Training Epoch: 88 [6656/50176]	Loss: 0.2334
Training Epoch: 88 [6912/50176]	Loss: 0.2069
Training Epoch: 88 [7168/50176]	Loss: 0.3085
Training Epoch: 88 [7424/50176]	Loss: 0.2508
Training Epoch: 88 [7680/50176]	Loss: 0.2237
Training Epoch: 88 [7936/50176]	Loss: 0.2708
Training Epoch: 88 [8192/50176]	Loss: 0.2444
Training Epoch: 88 [8448/50176]	Loss: 0.2221
Training Epoch: 88 [8704/50176]	Loss: 0.3164
Training Epoch: 88 [8960/50176]	Loss: 0.3453
Training Epoch: 88 [9216/50176]	Loss: 0.3174
Training Epoch: 88 [9472/50176]	Loss: 0.2974
Training Epoch: 88 [9728/50176]	Loss: 0.3180
Training Epoch: 88 [9984/50176]	Loss: 0.3429
Training Epoch: 88 [10240/50176]	Loss: 0.3299
Training Epoch: 88 [10496/50176]	Loss: 0.2800
Training Epoch: 88 [10752/50176]	Loss: 0.3277
Training Epoch: 88 [11008/50176]	Loss: 0.3793
Training Epoch: 88 [11264/50176]	Loss: 0.2151
Training Epoch: 88 [11520/50176]	Loss: 0.2946
Training Epoch: 88 [11776/50176]	Loss: 0.1918
Training Epoch: 88 [12032/50176]	Loss: 0.2832
Training Epoch: 88 [12288/50176]	Loss: 0.3277
Training Epoch: 88 [12544/50176]	Loss: 0.2607
Training Epoch: 88 [12800/50176]	Loss: 0.2932
Training Epoch: 88 [13056/50176]	Loss: 0.2194
Training Epoch: 88 [13312/50176]	Loss: 0.3923
Training Epoch: 88 [13568/50176]	Loss: 0.4148
Training Epoch: 88 [13824/50176]	Loss: 0.3553
Training Epoch: 88 [14080/50176]	Loss: 0.3583
Training Epoch: 88 [14336/50176]	Loss: 0.3239
Training Epoch: 88 [14592/50176]	Loss: 0.2734
Training Epoch: 88 [14848/50176]	Loss: 0.3237
Training Epoch: 88 [15104/50176]	Loss: 0.3308
Training Epoch: 88 [15360/50176]	Loss: 0.3225
Training Epoch: 88 [15616/50176]	Loss: 0.3144
Training Epoch: 88 [15872/50176]	Loss: 0.2517
Training Epoch: 88 [16128/50176]	Loss: 0.2727
Training Epoch: 88 [16384/50176]	Loss: 0.2823
Training Epoch: 88 [16640/50176]	Loss: 0.2773
Training Epoch: 88 [16896/50176]	Loss: 0.2868
Training Epoch: 88 [17152/50176]	Loss: 0.2165
Training Epoch: 88 [17408/50176]	Loss: 0.3174
Training Epoch: 88 [17664/50176]	Loss: 0.2251
Training Epoch: 88 [17920/50176]	Loss: 0.3498
Training Epoch: 88 [18176/50176]	Loss: 0.3175
Training Epoch: 88 [18432/50176]	Loss: 0.2880
Training Epoch: 88 [18688/50176]	Loss: 0.2837
Training Epoch: 88 [18944/50176]	Loss: 0.3456
Training Epoch: 88 [19200/50176]	Loss: 0.3012
Training Epoch: 88 [19456/50176]	Loss: 0.2718
Training Epoch: 88 [19712/50176]	Loss: 0.3092
Training Epoch: 88 [19968/50176]	Loss: 0.3010
Training Epoch: 88 [20224/50176]	Loss: 0.2531
Training Epoch: 88 [20480/50176]	Loss: 0.2463
Training Epoch: 88 [20736/50176]	Loss: 0.3605
Training Epoch: 88 [20992/50176]	Loss: 0.3245
Training Epoch: 88 [21248/50176]	Loss: 0.2742
Training Epoch: 88 [21504/50176]	Loss: 0.3799
Training Epoch: 88 [21760/50176]	Loss: 0.2028
Training Epoch: 88 [22016/50176]	Loss: 0.3680
Training Epoch: 88 [22272/50176]	Loss: 0.2014
Training Epoch: 88 [22528/50176]	Loss: 0.3263
Training Epoch: 88 [22784/50176]	Loss: 0.3105
Training Epoch: 88 [23040/50176]	Loss: 0.3369
Training Epoch: 88 [23296/50176]	Loss: 0.3099
Training Epoch: 88 [23552/50176]	Loss: 0.2400
Training Epoch: 88 [23808/50176]	Loss: 0.3062
Training Epoch: 88 [24064/50176]	Loss: 0.2698
Training Epoch: 88 [24320/50176]	Loss: 0.3185
Training Epoch: 88 [24576/50176]	Loss: 0.2575
Training Epoch: 88 [24832/50176]	Loss: 0.2871
Training Epoch: 88 [25088/50176]	Loss: 0.2501
Training Epoch: 88 [25344/50176]	Loss: 0.2607
Training Epoch: 88 [25600/50176]	Loss: 0.3117
Training Epoch: 88 [25856/50176]	Loss: 0.2632
Training Epoch: 88 [26112/50176]	Loss: 0.3511
Training Epoch: 88 [26368/50176]	Loss: 0.2420
Training Epoch: 88 [26624/50176]	Loss: 0.3496
Training Epoch: 88 [26880/50176]	Loss: 0.3284
Training Epoch: 88 [27136/50176]	Loss: 0.2595
Training Epoch: 88 [27392/50176]	Loss: 0.3220
Training Epoch: 88 [27648/50176]	Loss: 0.3223
Training Epoch: 88 [27904/50176]	Loss: 0.3401
Training Epoch: 88 [28160/50176]	Loss: 0.3091
Training Epoch: 88 [28416/50176]	Loss: 0.2744
Training Epoch: 88 [28672/50176]	Loss: 0.1745
Training Epoch: 88 [28928/50176]	Loss: 0.3361
Training Epoch: 88 [29184/50176]	Loss: 0.4129
Training Epoch: 88 [29440/50176]	Loss: 0.2875
Training Epoch: 88 [29696/50176]	Loss: 0.2980
Training Epoch: 88 [29952/50176]	Loss: 0.2777
Training Epoch: 88 [30208/50176]	Loss: 0.3071
Training Epoch: 88 [30464/50176]	Loss: 0.2659
Training Epoch: 88 [30720/50176]	Loss: 0.2842
Training Epoch: 88 [30976/50176]	Loss: 0.2426
Training Epoch: 88 [31232/50176]	Loss: 0.2497
Training Epoch: 88 [31488/50176]	Loss: 0.2902
Training Epoch: 88 [31744/50176]	Loss: 0.3544
Training Epoch: 88 [32000/50176]	Loss: 0.3688
Training Epoch: 88 [32256/50176]	Loss: 0.3179
Training Epoch: 88 [32512/50176]	Loss: 0.3496
Training Epoch: 88 [32768/50176]	Loss: 0.2842
Training Epoch: 88 [33024/50176]	Loss: 0.2910
Training Epoch: 88 [33280/50176]	Loss: 0.3012
Training Epoch: 88 [33536/50176]	Loss: 0.2961
Training Epoch: 88 [33792/50176]	Loss: 0.2714
Training Epoch: 88 [34048/50176]	Loss: 0.3970
Training Epoch: 88 [34304/50176]	Loss: 0.2892
Training Epoch: 88 [34560/50176]	Loss: 0.2827
Training Epoch: 88 [34816/50176]	Loss: 0.2461
Training Epoch: 88 [35072/50176]	Loss: 0.3080
Training Epoch: 88 [35328/50176]	Loss: 0.2712
Training Epoch: 88 [35584/50176]	Loss: 0.2672
Training Epoch: 88 [35840/50176]	Loss: 0.2099
Training Epoch: 88 [36096/50176]	Loss: 0.3621
Training Epoch: 88 [36352/50176]	Loss: 0.3734
Training Epoch: 88 [36608/50176]	Loss: 0.2978
Training Epoch: 88 [36864/50176]	Loss: 0.2391
Training Epoch: 88 [37120/50176]	Loss: 0.2408
Training Epoch: 88 [37376/50176]	Loss: 0.2324
Training Epoch: 88 [37632/50176]	Loss: 0.2160
Training Epoch: 88 [37888/50176]	Loss: 0.3546
Training Epoch: 88 [38144/50176]	Loss: 0.2591
Training Epoch: 88 [38400/50176]	Loss: 0.2764
Training Epoch: 88 [38656/50176]	Loss: 0.3106
Training Epoch: 88 [38912/50176]	Loss: 0.2989
Training Epoch: 88 [39168/50176]	Loss: 0.2393
Training Epoch: 88 [39424/50176]	Loss: 0.3452
Training Epoch: 88 [39680/50176]	Loss: 0.3601
Training Epoch: 88 [39936/50176]	Loss: 0.2767
Training Epoch: 88 [40192/50176]	Loss: 0.3603
Training Epoch: 88 [40448/50176]	Loss: 0.3146
Training Epoch: 88 [40704/50176]	Loss: 0.2929
Training Epoch: 88 [40960/50176]	Loss: 0.5044
Training Epoch: 88 [41216/50176]	Loss: 0.3268
Training Epoch: 88 [41472/50176]	Loss: 0.3112
Training Epoch: 88 [41728/50176]	Loss: 0.3641
Training Epoch: 88 [41984/50176]	Loss: 0.3002
Training Epoch: 88 [42240/50176]	Loss: 0.2719
Training Epoch: 88 [42496/50176]	Loss: 0.3456
Training Epoch: 88 [42752/50176]	Loss: 0.2761
Training Epoch: 88 [43008/50176]	Loss: 0.2637
Training Epoch: 88 [43264/50176]	Loss: 0.3514
Training Epoch: 88 [43520/50176]	Loss: 0.3889
Training Epoch: 88 [43776/50176]	Loss: 0.3544
Training Epoch: 88 [44032/50176]	Loss: 0.3203
Training Epoch: 88 [44288/50176]	Loss: 0.2471
Training Epoch: 88 [44544/50176]	Loss: 0.3283
Training Epoch: 88 [44800/50176]	Loss: 0.2598
Training Epoch: 88 [45056/50176]	Loss: 0.3555
Training Epoch: 88 [45312/50176]	Loss: 0.2961
Training Epoch: 88 [45568/50176]	Loss: 0.2667
Training Epoch: 88 [45824/50176]	Loss: 0.2803
Training Epoch: 88 [46080/50176]	Loss: 0.3207
Training Epoch: 88 [46336/50176]	Loss: 0.2925
Training Epoch: 88 [46592/50176]	Loss: 0.3598
Training Epoch: 88 [46848/50176]	Loss: 0.2571
Training Epoch: 88 [47104/50176]	Loss: 0.1880
Training Epoch: 88 [47360/50176]	Loss: 0.3407
Training Epoch: 88 [47616/50176]	Loss: 0.3231
Training Epoch: 88 [47872/50176]	Loss: 0.2119
Training Epoch: 88 [48128/50176]	Loss: 0.3411
Training Epoch: 88 [48384/50176]	Loss: 0.3393
Training Epoch: 88 [48640/50176]	Loss: 0.2826
Training Epoch: 88 [48896/50176]	Loss: 0.3632
Training Epoch: 88 [49152/50176]	Loss: 0.3387
Training Epoch: 88 [49408/50176]	Loss: 0.4115
Training Epoch: 88 [49664/50176]	Loss: 0.3838
Training Epoch: 88 [49920/50176]	Loss: 0.3717
Training Epoch: 88 [50176/50176]	Loss: 0.6875
Validation Epoch: 88, Average loss: 0.0107, Accuracy: 0.5776
Training Epoch: 89 [256/50176]	Loss: 0.2611
Training Epoch: 89 [512/50176]	Loss: 0.3200
Training Epoch: 89 [768/50176]	Loss: 0.2746
Training Epoch: 89 [1024/50176]	Loss: 0.2767
Training Epoch: 89 [1280/50176]	Loss: 0.2672
Training Epoch: 89 [1536/50176]	Loss: 0.3061
Training Epoch: 89 [1792/50176]	Loss: 0.3764
Training Epoch: 89 [2048/50176]	Loss: 0.2911
Training Epoch: 89 [2304/50176]	Loss: 0.3123
Training Epoch: 89 [2560/50176]	Loss: 0.2926
Training Epoch: 89 [2816/50176]	Loss: 0.2365
Training Epoch: 89 [3072/50176]	Loss: 0.2818
Training Epoch: 89 [3328/50176]	Loss: 0.2501
Training Epoch: 89 [3584/50176]	Loss: 0.3121
Training Epoch: 89 [3840/50176]	Loss: 0.2404
Training Epoch: 89 [4096/50176]	Loss: 0.3576
Training Epoch: 89 [4352/50176]	Loss: 0.2908
Training Epoch: 89 [4608/50176]	Loss: 0.2920
Training Epoch: 89 [4864/50176]	Loss: 0.2236
Training Epoch: 89 [5120/50176]	Loss: 0.3056
Training Epoch: 89 [5376/50176]	Loss: 0.3013
Training Epoch: 89 [5632/50176]	Loss: 0.2938
Training Epoch: 89 [5888/50176]	Loss: 0.2744
Training Epoch: 89 [6144/50176]	Loss: 0.2986
Training Epoch: 89 [6400/50176]	Loss: 0.2560
Training Epoch: 89 [6656/50176]	Loss: 0.3057
Training Epoch: 89 [6912/50176]	Loss: 0.2245
Training Epoch: 89 [7168/50176]	Loss: 0.2858
Training Epoch: 89 [7424/50176]	Loss: 0.1864
Training Epoch: 89 [7680/50176]	Loss: 0.2601
Training Epoch: 89 [7936/50176]	Loss: 0.3405
Training Epoch: 89 [8192/50176]	Loss: 0.2981
Training Epoch: 89 [8448/50176]	Loss: 0.2182
Training Epoch: 89 [8704/50176]	Loss: 0.1836
Training Epoch: 89 [8960/50176]	Loss: 0.2687
Training Epoch: 89 [9216/50176]	Loss: 0.3231
Training Epoch: 89 [9472/50176]	Loss: 0.4368
Training Epoch: 89 [9728/50176]	Loss: 0.3308
Training Epoch: 89 [9984/50176]	Loss: 0.3102
Training Epoch: 89 [10240/50176]	Loss: 0.1867
Training Epoch: 89 [10496/50176]	Loss: 0.3068
Training Epoch: 89 [10752/50176]	Loss: 0.2055
Training Epoch: 89 [11008/50176]	Loss: 0.2216
Training Epoch: 89 [11264/50176]	Loss: 0.2256
Training Epoch: 89 [11520/50176]	Loss: 0.3377
Training Epoch: 89 [11776/50176]	Loss: 0.2700
Training Epoch: 89 [12032/50176]	Loss: 0.3468
Training Epoch: 89 [12288/50176]	Loss: 0.3274
Training Epoch: 89 [12544/50176]	Loss: 0.3246
Training Epoch: 89 [12800/50176]	Loss: 0.2532
Training Epoch: 89 [13056/50176]	Loss: 0.2810
Training Epoch: 89 [13312/50176]	Loss: 0.2731
Training Epoch: 89 [13568/50176]	Loss: 0.2443
Training Epoch: 89 [13824/50176]	Loss: 0.2396
Training Epoch: 89 [14080/50176]	Loss: 0.2279
Training Epoch: 89 [14336/50176]	Loss: 0.3192
Training Epoch: 89 [14592/50176]	Loss: 0.2319
Training Epoch: 89 [14848/50176]	Loss: 0.3704
Training Epoch: 89 [15104/50176]	Loss: 0.3068
Training Epoch: 89 [15360/50176]	Loss: 0.2588
Training Epoch: 89 [15616/50176]	Loss: 0.2751
Training Epoch: 89 [15872/50176]	Loss: 0.2685
Training Epoch: 89 [16128/50176]	Loss: 0.2981
Training Epoch: 89 [16384/50176]	Loss: 0.2298
Training Epoch: 89 [16640/50176]	Loss: 0.2906
Training Epoch: 89 [16896/50176]	Loss: 0.2753
Training Epoch: 89 [17152/50176]	Loss: 0.2835
Training Epoch: 89 [17408/50176]	Loss: 0.2109
Training Epoch: 89 [17664/50176]	Loss: 0.2891
Training Epoch: 89 [17920/50176]	Loss: 0.3077
Training Epoch: 89 [18176/50176]	Loss: 0.3324
Training Epoch: 89 [18432/50176]	Loss: 0.2617
Training Epoch: 89 [18688/50176]	Loss: 0.3157
Training Epoch: 89 [18944/50176]	Loss: 0.3372
Training Epoch: 89 [19200/50176]	Loss: 0.2189
Training Epoch: 89 [19456/50176]	Loss: 0.2088
Training Epoch: 89 [19712/50176]	Loss: 0.3353
Training Epoch: 89 [19968/50176]	Loss: 0.2616
Training Epoch: 89 [20224/50176]	Loss: 0.2494
Training Epoch: 89 [20480/50176]	Loss: 0.2082
Training Epoch: 89 [20736/50176]	Loss: 0.3240
Training Epoch: 89 [20992/50176]	Loss: 0.2895
Training Epoch: 89 [21248/50176]	Loss: 0.3369
Training Epoch: 89 [21504/50176]	Loss: 0.2557
Training Epoch: 89 [21760/50176]	Loss: 0.3192
Training Epoch: 89 [22016/50176]	Loss: 0.4804
Training Epoch: 89 [22272/50176]	Loss: 0.3252
Training Epoch: 89 [22528/50176]	Loss: 0.2541
Training Epoch: 89 [22784/50176]	Loss: 0.3082
Training Epoch: 89 [23040/50176]	Loss: 0.2453
Training Epoch: 89 [23296/50176]	Loss: 0.2913
Training Epoch: 89 [23552/50176]	Loss: 0.3436
Training Epoch: 89 [23808/50176]	Loss: 0.2964
Training Epoch: 89 [24064/50176]	Loss: 0.3857
Training Epoch: 89 [24320/50176]	Loss: 0.2739
Training Epoch: 89 [24576/50176]	Loss: 0.3282
Training Epoch: 89 [24832/50176]	Loss: 0.3107
Training Epoch: 89 [25088/50176]	Loss: 0.2496
Training Epoch: 89 [25344/50176]	Loss: 0.3075
Training Epoch: 89 [25600/50176]	Loss: 0.2668
Training Epoch: 89 [25856/50176]	Loss: 0.3293
Training Epoch: 89 [26112/50176]	Loss: 0.3418
Training Epoch: 89 [26368/50176]	Loss: 0.3045
Training Epoch: 89 [26624/50176]	Loss: 0.3016
Training Epoch: 89 [26880/50176]	Loss: 0.3573
Training Epoch: 89 [27136/50176]	Loss: 0.2782
Training Epoch: 89 [27392/50176]	Loss: 0.2680
Training Epoch: 89 [27648/50176]	Loss: 0.3239
Training Epoch: 89 [27904/50176]	Loss: 0.3400
Training Epoch: 89 [28160/50176]	Loss: 0.2640
Training Epoch: 89 [28416/50176]	Loss: 0.2780
Training Epoch: 89 [28672/50176]	Loss: 0.3612
Training Epoch: 89 [28928/50176]	Loss: 0.2515
Training Epoch: 89 [29184/50176]	Loss: 0.2674
Training Epoch: 89 [29440/50176]	Loss: 0.3684
Training Epoch: 89 [29696/50176]	Loss: 0.2971
Training Epoch: 89 [29952/50176]	Loss: 0.3377
Training Epoch: 89 [30208/50176]	Loss: 0.1992
Training Epoch: 89 [30464/50176]	Loss: 0.4097
Training Epoch: 89 [30720/50176]	Loss: 0.3990
Training Epoch: 89 [30976/50176]	Loss: 0.3298
Training Epoch: 89 [31232/50176]	Loss: 0.2840
Training Epoch: 89 [31488/50176]	Loss: 0.2971
Training Epoch: 89 [31744/50176]	Loss: 0.3246
Training Epoch: 89 [32000/50176]	Loss: 0.2823
Training Epoch: 89 [32256/50176]	Loss: 0.3267
Training Epoch: 89 [32512/50176]	Loss: 0.2879
Training Epoch: 89 [32768/50176]	Loss: 0.3340
Training Epoch: 89 [33024/50176]	Loss: 0.3245
Training Epoch: 89 [33280/50176]	Loss: 0.2513
Training Epoch: 89 [33536/50176]	Loss: 0.3514
Training Epoch: 89 [33792/50176]	Loss: 0.2815
Training Epoch: 89 [34048/50176]	Loss: 0.3117
Training Epoch: 89 [34304/50176]	Loss: 0.2482
Training Epoch: 89 [34560/50176]	Loss: 0.4288
Training Epoch: 89 [34816/50176]	Loss: 0.3404
Training Epoch: 89 [35072/50176]	Loss: 0.2659
Training Epoch: 89 [35328/50176]	Loss: 0.3702
Training Epoch: 89 [35584/50176]	Loss: 0.2793
Training Epoch: 89 [35840/50176]	Loss: 0.3990
Training Epoch: 89 [36096/50176]	Loss: 0.3900
Training Epoch: 89 [36352/50176]	Loss: 0.2966
Training Epoch: 89 [36608/50176]	Loss: 0.3477
Training Epoch: 89 [36864/50176]	Loss: 0.3014
Training Epoch: 89 [37120/50176]	Loss: 0.2811
Training Epoch: 89 [37376/50176]	Loss: 0.3613
Training Epoch: 89 [37632/50176]	Loss: 0.2651
Training Epoch: 89 [37888/50176]	Loss: 0.2851
Training Epoch: 89 [38144/50176]	Loss: 0.4552
Training Epoch: 89 [38400/50176]	Loss: 0.3234
Training Epoch: 89 [38656/50176]	Loss: 0.2956
Training Epoch: 89 [38912/50176]	Loss: 0.3492
Training Epoch: 89 [39168/50176]	Loss: 0.2891
Training Epoch: 89 [39424/50176]	Loss: 0.3766
Training Epoch: 89 [39680/50176]	Loss: 0.2632
Training Epoch: 89 [39936/50176]	Loss: 0.3335
Training Epoch: 89 [40192/50176]	Loss: 0.3327
Training Epoch: 89 [40448/50176]	Loss: 0.3315
Training Epoch: 89 [40704/50176]	Loss: 0.2979
Training Epoch: 89 [40960/50176]	Loss: 0.3027
Training Epoch: 89 [41216/50176]	Loss: 0.3583
Training Epoch: 89 [41472/50176]	Loss: 0.2567
Training Epoch: 89 [41728/50176]	Loss: 0.3235
Training Epoch: 89 [41984/50176]	Loss: 0.3303
Training Epoch: 89 [42240/50176]	Loss: 0.3496
Training Epoch: 89 [42496/50176]	Loss: 0.2994
Training Epoch: 89 [42752/50176]	Loss: 0.3141
Training Epoch: 89 [43008/50176]	Loss: 0.3394
Training Epoch: 89 [43264/50176]	Loss: 0.3950
Training Epoch: 89 [43520/50176]	Loss: 0.3595
Training Epoch: 89 [43776/50176]	Loss: 0.2114
Training Epoch: 89 [44032/50176]	Loss: 0.3507
Training Epoch: 89 [44288/50176]	Loss: 0.2252
Training Epoch: 89 [44544/50176]	Loss: 0.2567
Training Epoch: 89 [44800/50176]	Loss: 0.2429
Training Epoch: 89 [45056/50176]	Loss: 0.3958
Training Epoch: 89 [45312/50176]	Loss: 0.2551
Training Epoch: 89 [45568/50176]	Loss: 0.2241
Training Epoch: 89 [45824/50176]	Loss: 0.3516
Training Epoch: 89 [46080/50176]	Loss: 0.2701
Training Epoch: 89 [46336/50176]	Loss: 0.3347
Training Epoch: 89 [46592/50176]	Loss: 0.3107
Training Epoch: 89 [46848/50176]	Loss: 0.4126
Training Epoch: 89 [47104/50176]	Loss: 0.4058
Training Epoch: 89 [47360/50176]	Loss: 0.2531
Training Epoch: 89 [47616/50176]	Loss: 0.3512
Training Epoch: 89 [47872/50176]	Loss: 0.2927
Training Epoch: 89 [48128/50176]	Loss: 0.3514
Training Epoch: 89 [48384/50176]	Loss: 0.3532
Training Epoch: 89 [48640/50176]	Loss: 0.2367
Training Epoch: 89 [48896/50176]	Loss: 0.3217
Training Epoch: 89 [49152/50176]	Loss: 0.2621
Training Epoch: 89 [49408/50176]	Loss: 0.3966
Training Epoch: 89 [49664/50176]	Loss: 0.3115
Training Epoch: 89 [49920/50176]	Loss: 0.3703
Training Epoch: 89 [50176/50176]	Loss: 0.4994
Validation Epoch: 89, Average loss: 0.0123, Accuracy: 0.5540
Training Epoch: 90 [256/50176]	Loss: 0.2091
Training Epoch: 90 [512/50176]	Loss: 0.2544
Training Epoch: 90 [768/50176]	Loss: 0.2533
Training Epoch: 90 [1024/50176]	Loss: 0.3483
Training Epoch: 90 [1280/50176]	Loss: 0.2235
Training Epoch: 90 [1536/50176]	Loss: 0.3526
Training Epoch: 90 [1792/50176]	Loss: 0.3216
Training Epoch: 90 [2048/50176]	Loss: 0.2437
Training Epoch: 90 [2304/50176]	Loss: 0.2448
Training Epoch: 90 [2560/50176]	Loss: 0.3090
Training Epoch: 90 [2816/50176]	Loss: 0.3126
Training Epoch: 90 [3072/50176]	Loss: 0.2112
Training Epoch: 90 [3328/50176]	Loss: 0.3360
Training Epoch: 90 [3584/50176]	Loss: 0.3200
Training Epoch: 90 [3840/50176]	Loss: 0.3060
Training Epoch: 90 [4096/50176]	Loss: 0.2594
Training Epoch: 90 [4352/50176]	Loss: 0.2885
Training Epoch: 90 [4608/50176]	Loss: 0.3520
Training Epoch: 90 [4864/50176]	Loss: 0.3135
Training Epoch: 90 [5120/50176]	Loss: 0.2974
Training Epoch: 90 [5376/50176]	Loss: 0.1927
Training Epoch: 90 [5632/50176]	Loss: 0.2282
Training Epoch: 90 [5888/50176]	Loss: 0.2552
Training Epoch: 90 [6144/50176]	Loss: 0.3942
Training Epoch: 90 [6400/50176]	Loss: 0.4031
Training Epoch: 90 [6656/50176]	Loss: 0.2642
Training Epoch: 90 [6912/50176]	Loss: 0.2510
Training Epoch: 90 [7168/50176]	Loss: 0.3335
Training Epoch: 90 [7424/50176]	Loss: 0.2513
Training Epoch: 90 [7680/50176]	Loss: 0.2011
Training Epoch: 90 [7936/50176]	Loss: 0.3037
Training Epoch: 90 [8192/50176]	Loss: 0.2797
Training Epoch: 90 [8448/50176]	Loss: 0.3234
Training Epoch: 90 [8704/50176]	Loss: 0.3186
Training Epoch: 90 [8960/50176]	Loss: 0.3659
Training Epoch: 90 [9216/50176]	Loss: 0.2193
Training Epoch: 90 [9472/50176]	Loss: 0.3426
Training Epoch: 90 [9728/50176]	Loss: 0.2874
Training Epoch: 90 [9984/50176]	Loss: 0.2497
Training Epoch: 90 [10240/50176]	Loss: 0.2962
Training Epoch: 90 [10496/50176]	Loss: 0.3047
Training Epoch: 90 [10752/50176]	Loss: 0.2550
Training Epoch: 90 [11008/50176]	Loss: 0.2264
Training Epoch: 90 [11264/50176]	Loss: 0.2129
Training Epoch: 90 [11520/50176]	Loss: 0.2388
Training Epoch: 90 [11776/50176]	Loss: 0.3144
Training Epoch: 90 [12032/50176]	Loss: 0.3330
Training Epoch: 90 [12288/50176]	Loss: 0.3324
Training Epoch: 90 [12544/50176]	Loss: 0.3081
Training Epoch: 90 [12800/50176]	Loss: 0.3058
Training Epoch: 90 [13056/50176]	Loss: 0.2611
Training Epoch: 90 [13312/50176]	Loss: 0.2668
Training Epoch: 90 [13568/50176]	Loss: 0.2822
Training Epoch: 90 [13824/50176]	Loss: 0.2321
Training Epoch: 90 [14080/50176]	Loss: 0.1984
Training Epoch: 90 [14336/50176]	Loss: 0.3122
Training Epoch: 90 [14592/50176]	Loss: 0.2923
Training Epoch: 90 [14848/50176]	Loss: 0.2233
Training Epoch: 90 [15104/50176]	Loss: 0.2189
Training Epoch: 90 [15360/50176]	Loss: 0.2557
Training Epoch: 90 [15616/50176]	Loss: 0.2371
Training Epoch: 90 [15872/50176]	Loss: 0.3248
Training Epoch: 90 [16128/50176]	Loss: 0.2464
Training Epoch: 90 [16384/50176]	Loss: 0.3458
Training Epoch: 90 [16640/50176]	Loss: 0.3136
Training Epoch: 90 [16896/50176]	Loss: 0.2848
Training Epoch: 90 [17152/50176]	Loss: 0.2540
Training Epoch: 90 [17408/50176]	Loss: 0.2255
Training Epoch: 90 [17664/50176]	Loss: 0.2261
Training Epoch: 90 [17920/50176]	Loss: 0.2210
Training Epoch: 90 [18176/50176]	Loss: 0.2584
Training Epoch: 90 [18432/50176]	Loss: 0.2533
Training Epoch: 90 [18688/50176]	Loss: 0.2634
Training Epoch: 90 [18944/50176]	Loss: 0.2430
Training Epoch: 90 [19200/50176]	Loss: 0.3085
Training Epoch: 90 [19456/50176]	Loss: 0.3512
Training Epoch: 90 [19712/50176]	Loss: 0.2074
Training Epoch: 90 [19968/50176]	Loss: 0.3250
Training Epoch: 90 [20224/50176]	Loss: 0.2087
Training Epoch: 90 [20480/50176]	Loss: 0.1805
Training Epoch: 90 [20736/50176]	Loss: 0.2908
Training Epoch: 90 [20992/50176]	Loss: 0.3548
Training Epoch: 90 [21248/50176]	Loss: 0.3839
Training Epoch: 90 [21504/50176]	Loss: 0.2669
Training Epoch: 90 [21760/50176]	Loss: 0.3281
Training Epoch: 90 [22016/50176]	Loss: 0.2395
Training Epoch: 90 [22272/50176]	Loss: 0.2612
Training Epoch: 90 [22528/50176]	Loss: 0.2272
Training Epoch: 90 [22784/50176]	Loss: 0.2985
Training Epoch: 90 [23040/50176]	Loss: 0.2745
Training Epoch: 90 [23296/50176]	Loss: 0.3475
Training Epoch: 90 [23552/50176]	Loss: 0.2906
Training Epoch: 90 [23808/50176]	Loss: 0.2281
Training Epoch: 90 [24064/50176]	Loss: 0.3060
Training Epoch: 90 [24320/50176]	Loss: 0.3318
Training Epoch: 90 [24576/50176]	Loss: 0.1679
Training Epoch: 90 [24832/50176]	Loss: 0.2695
Training Epoch: 90 [25088/50176]	Loss: 0.3691
Training Epoch: 90 [25344/50176]	Loss: 0.3149
Training Epoch: 90 [25600/50176]	Loss: 0.3210
Training Epoch: 90 [25856/50176]	Loss: 0.2687
Training Epoch: 90 [26112/50176]	Loss: 0.3826
Training Epoch: 90 [26368/50176]	Loss: 0.3314
Training Epoch: 90 [26624/50176]	Loss: 0.2820
Training Epoch: 90 [26880/50176]	Loss: 0.3189
Training Epoch: 90 [27136/50176]	Loss: 0.2432
Training Epoch: 90 [27392/50176]	Loss: 0.4058
Training Epoch: 90 [27648/50176]	Loss: 0.3117
Training Epoch: 90 [27904/50176]	Loss: 0.3176
Training Epoch: 90 [28160/50176]	Loss: 0.2818
Training Epoch: 90 [28416/50176]	Loss: 0.2910
Training Epoch: 90 [28672/50176]	Loss: 0.2818
Training Epoch: 90 [28928/50176]	Loss: 0.2191
Training Epoch: 90 [29184/50176]	Loss: 0.4036
Training Epoch: 90 [29440/50176]	Loss: 0.3207
Training Epoch: 90 [29696/50176]	Loss: 0.3027
Training Epoch: 90 [29952/50176]	Loss: 0.2886
Training Epoch: 90 [30208/50176]	Loss: 0.3070
Training Epoch: 90 [30464/50176]	Loss: 0.3121
Training Epoch: 90 [30720/50176]	Loss: 0.2503
Training Epoch: 90 [30976/50176]	Loss: 0.2051
Training Epoch: 90 [31232/50176]	Loss: 0.2599
Training Epoch: 90 [31488/50176]	Loss: 0.2942
Training Epoch: 90 [31744/50176]	Loss: 0.2071
Training Epoch: 90 [32000/50176]	Loss: 0.3367
Training Epoch: 90 [32256/50176]	Loss: 0.3304
Training Epoch: 90 [32512/50176]	Loss: 0.3334
Training Epoch: 90 [32768/50176]	Loss: 0.3975
Training Epoch: 90 [33024/50176]	Loss: 0.2910
Training Epoch: 90 [33280/50176]	Loss: 0.3204
Training Epoch: 90 [33536/50176]	Loss: 0.3270
Training Epoch: 90 [33792/50176]	Loss: 0.3045
Training Epoch: 90 [34048/50176]	Loss: 0.2659
Training Epoch: 90 [34304/50176]	Loss: 0.2253
Training Epoch: 90 [34560/50176]	Loss: 0.2367
Training Epoch: 90 [34816/50176]	Loss: 0.3650
Training Epoch: 90 [35072/50176]	Loss: 0.2448
Training Epoch: 90 [35328/50176]	Loss: 0.4111
Training Epoch: 90 [35584/50176]	Loss: 0.2875
Training Epoch: 90 [35840/50176]	Loss: 0.3045
Training Epoch: 90 [36096/50176]	Loss: 0.2259
Training Epoch: 90 [36352/50176]	Loss: 0.3802
Training Epoch: 90 [36608/50176]	Loss: 0.2751
Training Epoch: 90 [36864/50176]	Loss: 0.2693
Training Epoch: 90 [37120/50176]	Loss: 0.2804
Training Epoch: 90 [37376/50176]	Loss: 0.3445
Training Epoch: 90 [37632/50176]	Loss: 0.2641
Training Epoch: 90 [37888/50176]	Loss: 0.3635
Training Epoch: 90 [38144/50176]	Loss: 0.3129
Training Epoch: 90 [38400/50176]	Loss: 0.4299
Training Epoch: 90 [38656/50176]	Loss: 0.2827
Training Epoch: 90 [38912/50176]	Loss: 0.3558
Training Epoch: 90 [39168/50176]	Loss: 0.3098
Training Epoch: 90 [39424/50176]	Loss: 0.2684
Training Epoch: 90 [39680/50176]	Loss: 0.2699
Training Epoch: 90 [39936/50176]	Loss: 0.2774
Training Epoch: 90 [40192/50176]	Loss: 0.3766
Training Epoch: 90 [40448/50176]	Loss: 0.3553
Training Epoch: 90 [40704/50176]	Loss: 0.2087
Training Epoch: 90 [40960/50176]	Loss: 0.2547
Training Epoch: 90 [41216/50176]	Loss: 0.3634
Training Epoch: 90 [41472/50176]	Loss: 0.2515
Training Epoch: 90 [41728/50176]	Loss: 0.3193
Training Epoch: 90 [41984/50176]	Loss: 0.2604
Training Epoch: 90 [42240/50176]	Loss: 0.3436
Training Epoch: 90 [42496/50176]	Loss: 0.3382
Training Epoch: 90 [42752/50176]	Loss: 0.3504
Training Epoch: 90 [43008/50176]	Loss: 0.2718
Training Epoch: 90 [43264/50176]	Loss: 0.2574
Training Epoch: 90 [43520/50176]	Loss: 0.2150
Training Epoch: 90 [43776/50176]	Loss: 0.3665
Training Epoch: 90 [44032/50176]	Loss: 0.2747
Training Epoch: 90 [44288/50176]	Loss: 0.3650
Training Epoch: 90 [44544/50176]	Loss: 0.3132
Training Epoch: 90 [44800/50176]	Loss: 0.3295
Training Epoch: 90 [45056/50176]	Loss: 0.3661
Training Epoch: 90 [45312/50176]	Loss: 0.2739
Training Epoch: 90 [45568/50176]	Loss: 0.3081
Training Epoch: 90 [45824/50176]	Loss: 0.3882
Training Epoch: 90 [46080/50176]	Loss: 0.3930
Training Epoch: 90 [46336/50176]	Loss: 0.2853
Training Epoch: 90 [46592/50176]	Loss: 0.3607
Training Epoch: 90 [46848/50176]	Loss: 0.2985
Training Epoch: 90 [47104/50176]	Loss: 0.3741
Training Epoch: 90 [47360/50176]	Loss: 0.3375
Training Epoch: 90 [47616/50176]	Loss: 0.3692
Training Epoch: 90 [47872/50176]	Loss: 0.2930
Training Epoch: 90 [48128/50176]	Loss: 0.3205
Training Epoch: 90 [48384/50176]	Loss: 0.2795
Training Epoch: 90 [48640/50176]	Loss: 0.3262
Training Epoch: 90 [48896/50176]	Loss: 0.2640
Training Epoch: 90 [49152/50176]	Loss: 0.2884
Training Epoch: 90 [49408/50176]	Loss: 0.3483
Training Epoch: 90 [49664/50176]	Loss: 0.2443
Training Epoch: 90 [49920/50176]	Loss: 0.2182
Training Epoch: 90 [50176/50176]	Loss: 0.3632
Validation Epoch: 90, Average loss: 0.0112, Accuracy: 0.5625
Training Epoch: 91 [256/50176]	Loss: 0.3474
Training Epoch: 91 [512/50176]	Loss: 0.2768
Training Epoch: 91 [768/50176]	Loss: 0.2448
Training Epoch: 91 [1024/50176]	Loss: 0.2343
Training Epoch: 91 [1280/50176]	Loss: 0.3666
Training Epoch: 91 [1536/50176]	Loss: 0.2647
Training Epoch: 91 [1792/50176]	Loss: 0.2751
Training Epoch: 91 [2048/50176]	Loss: 0.4062
Training Epoch: 91 [2304/50176]	Loss: 0.2518
Training Epoch: 91 [2560/50176]	Loss: 0.2656
Training Epoch: 91 [2816/50176]	Loss: 0.3145
Training Epoch: 91 [3072/50176]	Loss: 0.2755
Training Epoch: 91 [3328/50176]	Loss: 0.2601
Training Epoch: 91 [3584/50176]	Loss: 0.2195
Training Epoch: 91 [3840/50176]	Loss: 0.3089
Training Epoch: 91 [4096/50176]	Loss: 0.2644
Training Epoch: 91 [4352/50176]	Loss: 0.2658
Training Epoch: 91 [4608/50176]	Loss: 0.2364
Training Epoch: 91 [4864/50176]	Loss: 0.2372
Training Epoch: 91 [5120/50176]	Loss: 0.3119
Training Epoch: 91 [5376/50176]	Loss: 0.3543
Training Epoch: 91 [5632/50176]	Loss: 0.3301
Training Epoch: 91 [5888/50176]	Loss: 0.3118
Training Epoch: 91 [6144/50176]	Loss: 0.2484
Training Epoch: 91 [6400/50176]	Loss: 0.2702
Training Epoch: 91 [6656/50176]	Loss: 0.2928
Training Epoch: 91 [6912/50176]	Loss: 0.2863
Training Epoch: 91 [7168/50176]	Loss: 0.2486
Training Epoch: 91 [7424/50176]	Loss: 0.2627
Training Epoch: 91 [7680/50176]	Loss: 0.2194
Training Epoch: 91 [7936/50176]	Loss: 0.2333
Training Epoch: 91 [8192/50176]	Loss: 0.2473
Training Epoch: 91 [8448/50176]	Loss: 0.2717
Training Epoch: 91 [8704/50176]	Loss: 0.2647
Training Epoch: 91 [8960/50176]	Loss: 0.2983
Training Epoch: 91 [9216/50176]	Loss: 0.2685
Training Epoch: 91 [9472/50176]	Loss: 0.2966
Training Epoch: 91 [9728/50176]	Loss: 0.2407
Training Epoch: 91 [9984/50176]	Loss: 0.4408
Training Epoch: 91 [10240/50176]	Loss: 0.3398
Training Epoch: 91 [10496/50176]	Loss: 0.3058
Training Epoch: 91 [10752/50176]	Loss: 0.3013
Training Epoch: 91 [11008/50176]	Loss: 0.2577
Training Epoch: 91 [11264/50176]	Loss: 0.2346
Training Epoch: 91 [11520/50176]	Loss: 0.2216
Training Epoch: 91 [11776/50176]	Loss: 0.2508
Training Epoch: 91 [12032/50176]	Loss: 0.3491
Training Epoch: 91 [12288/50176]	Loss: 0.3341
Training Epoch: 91 [12544/50176]	Loss: 0.2373
Training Epoch: 91 [12800/50176]	Loss: 0.2759
Training Epoch: 91 [13056/50176]	Loss: 0.2503
Training Epoch: 91 [13312/50176]	Loss: 0.2670
Training Epoch: 91 [13568/50176]	Loss: 0.2119
Training Epoch: 91 [13824/50176]	Loss: 0.2945
Training Epoch: 91 [14080/50176]	Loss: 0.2933
Training Epoch: 91 [14336/50176]	Loss: 0.2547
Training Epoch: 91 [14592/50176]	Loss: 0.2620
Training Epoch: 91 [14848/50176]	Loss: 0.2201
Training Epoch: 91 [15104/50176]	Loss: 0.3056
Training Epoch: 91 [15360/50176]	Loss: 0.3161
Training Epoch: 91 [15616/50176]	Loss: 0.2895
Training Epoch: 91 [15872/50176]	Loss: 0.2230
Training Epoch: 91 [16128/50176]	Loss: 0.2599
Training Epoch: 91 [16384/50176]	Loss: 0.3492
Training Epoch: 91 [16640/50176]	Loss: 0.2447
Training Epoch: 91 [16896/50176]	Loss: 0.2460
Training Epoch: 91 [17152/50176]	Loss: 0.3403
Training Epoch: 91 [17408/50176]	Loss: 0.2840
Training Epoch: 91 [17664/50176]	Loss: 0.2941
Training Epoch: 91 [17920/50176]	Loss: 0.2688
Training Epoch: 91 [18176/50176]	Loss: 0.2560
Training Epoch: 91 [18432/50176]	Loss: 0.3380
Training Epoch: 91 [18688/50176]	Loss: 0.2833
Training Epoch: 91 [18944/50176]	Loss: 0.3568
Training Epoch: 91 [19200/50176]	Loss: 0.2959
Training Epoch: 91 [19456/50176]	Loss: 0.2645
Training Epoch: 91 [19712/50176]	Loss: 0.2397
Training Epoch: 91 [19968/50176]	Loss: 0.1418
Training Epoch: 91 [20224/50176]	Loss: 0.2653
Training Epoch: 91 [20480/50176]	Loss: 0.2244
Training Epoch: 91 [20736/50176]	Loss: 0.3477
Training Epoch: 91 [20992/50176]	Loss: 0.3836
Training Epoch: 91 [21248/50176]	Loss: 0.2947
Training Epoch: 91 [21504/50176]	Loss: 0.2282
Training Epoch: 91 [21760/50176]	Loss: 0.3096
Training Epoch: 91 [22016/50176]	Loss: 0.2188
Training Epoch: 91 [22272/50176]	Loss: 0.3580
Training Epoch: 91 [22528/50176]	Loss: 0.2422
Training Epoch: 91 [22784/50176]	Loss: 0.2973
Training Epoch: 91 [23040/50176]	Loss: 0.2986
Training Epoch: 91 [23296/50176]	Loss: 0.2818
Training Epoch: 91 [23552/50176]	Loss: 0.3300
Training Epoch: 91 [23808/50176]	Loss: 0.2850
Training Epoch: 91 [24064/50176]	Loss: 0.3377
Training Epoch: 91 [24320/50176]	Loss: 0.2668
Training Epoch: 91 [24576/50176]	Loss: 0.2839
Training Epoch: 91 [24832/50176]	Loss: 0.2344
Training Epoch: 91 [25088/50176]	Loss: 0.2636
Training Epoch: 91 [25344/50176]	Loss: 0.3409
Training Epoch: 91 [25600/50176]	Loss: 0.2798
Training Epoch: 91 [25856/50176]	Loss: 0.2727
Training Epoch: 91 [26112/50176]	Loss: 0.2889
Training Epoch: 91 [26368/50176]	Loss: 0.3700
Training Epoch: 91 [26624/50176]	Loss: 0.2287
Training Epoch: 91 [26880/50176]	Loss: 0.2546
Training Epoch: 91 [27136/50176]	Loss: 0.2562
Training Epoch: 91 [27392/50176]	Loss: 0.2865
Training Epoch: 91 [27648/50176]	Loss: 0.2859
Training Epoch: 91 [27904/50176]	Loss: 0.3616
Training Epoch: 91 [28160/50176]	Loss: 0.2670
Training Epoch: 91 [28416/50176]	Loss: 0.2340
Training Epoch: 91 [28672/50176]	Loss: 0.3250
Training Epoch: 91 [28928/50176]	Loss: 0.3910
Training Epoch: 91 [29184/50176]	Loss: 0.3921
Training Epoch: 91 [29440/50176]	Loss: 0.2620
Training Epoch: 91 [29696/50176]	Loss: 0.3409
Training Epoch: 91 [29952/50176]	Loss: 0.2158
Training Epoch: 91 [30208/50176]	Loss: 0.3155
Training Epoch: 91 [30464/50176]	Loss: 0.2990
Training Epoch: 91 [30720/50176]	Loss: 0.2797
Training Epoch: 91 [30976/50176]	Loss: 0.2261
Training Epoch: 91 [31232/50176]	Loss: 0.2758
Training Epoch: 91 [31488/50176]	Loss: 0.3257
Training Epoch: 91 [31744/50176]	Loss: 0.2616
Training Epoch: 91 [32000/50176]	Loss: 0.3570
Training Epoch: 91 [32256/50176]	Loss: 0.2559
Training Epoch: 91 [32512/50176]	Loss: 0.2716
Training Epoch: 91 [32768/50176]	Loss: 0.3451
Training Epoch: 91 [33024/50176]	Loss: 0.3648
Training Epoch: 91 [33280/50176]	Loss: 0.3101
Training Epoch: 91 [33536/50176]	Loss: 0.2706
Training Epoch: 91 [33792/50176]	Loss: 0.3070
Training Epoch: 91 [34048/50176]	Loss: 0.3566
Training Epoch: 91 [34304/50176]	Loss: 0.2873
Training Epoch: 91 [34560/50176]	Loss: 0.4296
Training Epoch: 91 [34816/50176]	Loss: 0.2871
Training Epoch: 91 [35072/50176]	Loss: 0.3031
Training Epoch: 91 [35328/50176]	Loss: 0.3842
Training Epoch: 91 [35584/50176]	Loss: 0.2353
Training Epoch: 91 [35840/50176]	Loss: 0.3020
Training Epoch: 91 [36096/50176]	Loss: 0.2757
Training Epoch: 91 [36352/50176]	Loss: 0.2931
Training Epoch: 91 [36608/50176]	Loss: 0.3337
Training Epoch: 91 [36864/50176]	Loss: 0.3763
Training Epoch: 91 [37120/50176]	Loss: 0.3364
Training Epoch: 91 [37376/50176]	Loss: 0.2289
Training Epoch: 91 [37632/50176]	Loss: 0.3892
Training Epoch: 91 [37888/50176]	Loss: 0.2839
Training Epoch: 91 [38144/50176]	Loss: 0.3705
Training Epoch: 91 [38400/50176]	Loss: 0.3341
Training Epoch: 91 [38656/50176]	Loss: 0.3981
Training Epoch: 91 [38912/50176]	Loss: 0.2752
Training Epoch: 91 [39168/50176]	Loss: 0.3085
Training Epoch: 91 [39424/50176]	Loss: 0.2733
Training Epoch: 91 [39680/50176]	Loss: 0.1894
Training Epoch: 91 [39936/50176]	Loss: 0.2844
Training Epoch: 91 [40192/50176]	Loss: 0.2679
Training Epoch: 91 [40448/50176]	Loss: 0.3730
Training Epoch: 91 [40704/50176]	Loss: 0.3617
Training Epoch: 91 [40960/50176]	Loss: 0.3030
Training Epoch: 91 [41216/50176]	Loss: 0.3128
Training Epoch: 91 [41472/50176]	Loss: 0.3074
Training Epoch: 91 [41728/50176]	Loss: 0.3213
Training Epoch: 91 [41984/50176]	Loss: 0.3899
Training Epoch: 91 [42240/50176]	Loss: 0.2989
Training Epoch: 91 [42496/50176]	Loss: 0.2296
Training Epoch: 91 [42752/50176]	Loss: 0.3498
Training Epoch: 91 [43008/50176]	Loss: 0.3309
Training Epoch: 91 [43264/50176]	Loss: 0.2821
Training Epoch: 91 [43520/50176]	Loss: 0.3062
Training Epoch: 91 [43776/50176]	Loss: 0.3157
Training Epoch: 91 [44032/50176]	Loss: 0.3839
Training Epoch: 91 [44288/50176]	Loss: 0.3931
Training Epoch: 91 [44544/50176]	Loss: 0.2524
Training Epoch: 91 [44800/50176]	Loss: 0.2884
Training Epoch: 91 [45056/50176]	Loss: 0.3362
Training Epoch: 91 [45312/50176]	Loss: 0.3426
Training Epoch: 91 [45568/50176]	Loss: 0.3329
Training Epoch: 91 [45824/50176]	Loss: 0.2979
Training Epoch: 91 [46080/50176]	Loss: 0.2271
Training Epoch: 91 [46336/50176]	Loss: 0.3141
Training Epoch: 91 [46592/50176]	Loss: 0.2515
Training Epoch: 91 [46848/50176]	Loss: 0.4080
Training Epoch: 91 [47104/50176]	Loss: 0.2753
Training Epoch: 91 [47360/50176]	Loss: 0.3677
Training Epoch: 91 [47616/50176]	Loss: 0.3108
Training Epoch: 91 [47872/50176]	Loss: 0.2963
Training Epoch: 91 [48128/50176]	Loss: 0.2189
Training Epoch: 91 [48384/50176]	Loss: 0.3208
Training Epoch: 91 [48640/50176]	Loss: 0.2274
Training Epoch: 91 [48896/50176]	Loss: 0.2623
Training Epoch: 91 [49152/50176]	Loss: 0.2859
Training Epoch: 91 [49408/50176]	Loss: 0.2951
Training Epoch: 91 [49664/50176]	Loss: 0.2998
Training Epoch: 91 [49920/50176]	Loss: 0.3836
Training Epoch: 91 [50176/50176]	Loss: 0.4151
Validation Epoch: 91, Average loss: 0.0113, Accuracy: 0.5746
Training Epoch: 92 [256/50176]	Loss: 0.2759
Training Epoch: 92 [512/50176]	Loss: 0.2314
Training Epoch: 92 [768/50176]	Loss: 0.2597
Training Epoch: 92 [1024/50176]	Loss: 0.2746
Training Epoch: 92 [1280/50176]	Loss: 0.3035
Training Epoch: 92 [1536/50176]	Loss: 0.1911
Training Epoch: 92 [1792/50176]	Loss: 0.2368
Training Epoch: 92 [2048/50176]	Loss: 0.3172
Training Epoch: 92 [2304/50176]	Loss: 0.3798
Training Epoch: 92 [2560/50176]	Loss: 0.2712
Training Epoch: 92 [2816/50176]	Loss: 0.2514
Training Epoch: 92 [3072/50176]	Loss: 0.3646
Training Epoch: 92 [3328/50176]	Loss: 0.2650
Training Epoch: 92 [3584/50176]	Loss: 0.2596
Training Epoch: 92 [3840/50176]	Loss: 0.2729
Training Epoch: 92 [4096/50176]	Loss: 0.3521
Training Epoch: 92 [4352/50176]	Loss: 0.2447
Training Epoch: 92 [4608/50176]	Loss: 0.2473
Training Epoch: 92 [4864/50176]	Loss: 0.3110
Training Epoch: 92 [5120/50176]	Loss: 0.3148
Training Epoch: 92 [5376/50176]	Loss: 0.3132
Training Epoch: 92 [5632/50176]	Loss: 0.2932
Training Epoch: 92 [5888/50176]	Loss: 0.2322
Training Epoch: 92 [6144/50176]	Loss: 0.2871
Training Epoch: 92 [6400/50176]	Loss: 0.2507
Training Epoch: 92 [6656/50176]	Loss: 0.3569
Training Epoch: 92 [6912/50176]	Loss: 0.1773
Training Epoch: 92 [7168/50176]	Loss: 0.3092
Training Epoch: 92 [7424/50176]	Loss: 0.2659
Training Epoch: 92 [7680/50176]	Loss: 0.2012
Training Epoch: 92 [7936/50176]	Loss: 0.1989
Training Epoch: 92 [8192/50176]	Loss: 0.2960
Training Epoch: 92 [8448/50176]	Loss: 0.2640
Training Epoch: 92 [8704/50176]	Loss: 0.2625
Training Epoch: 92 [8960/50176]	Loss: 0.3420
Training Epoch: 92 [9216/50176]	Loss: 0.3589
Training Epoch: 92 [9472/50176]	Loss: 0.3650
Training Epoch: 92 [9728/50176]	Loss: 0.3471
Training Epoch: 92 [9984/50176]	Loss: 0.2462
Training Epoch: 92 [10240/50176]	Loss: 0.3293
Training Epoch: 92 [10496/50176]	Loss: 0.3517
Training Epoch: 92 [10752/50176]	Loss: 0.2327
Training Epoch: 92 [11008/50176]	Loss: 0.2564
Training Epoch: 92 [11264/50176]	Loss: 0.2674
Training Epoch: 92 [11520/50176]	Loss: 0.2348
Training Epoch: 92 [11776/50176]	Loss: 0.2946
Training Epoch: 92 [12032/50176]	Loss: 0.3024
Training Epoch: 92 [12288/50176]	Loss: 0.2662
Training Epoch: 92 [12544/50176]	Loss: 0.1720
Training Epoch: 92 [12800/50176]	Loss: 0.2944
Training Epoch: 92 [13056/50176]	Loss: 0.2430
Training Epoch: 92 [13312/50176]	Loss: 0.2319
Training Epoch: 92 [13568/50176]	Loss: 0.2844
Training Epoch: 92 [13824/50176]	Loss: 0.1990
Training Epoch: 92 [14080/50176]	Loss: 0.2089
Training Epoch: 92 [14336/50176]	Loss: 0.3016
Training Epoch: 92 [14592/50176]	Loss: 0.2828
Training Epoch: 92 [14848/50176]	Loss: 0.2223
Training Epoch: 92 [15104/50176]	Loss: 0.2823
Training Epoch: 92 [15360/50176]	Loss: 0.2664
Training Epoch: 92 [15616/50176]	Loss: 0.1960
Training Epoch: 92 [15872/50176]	Loss: 0.3323
Training Epoch: 92 [16128/50176]	Loss: 0.3100
Training Epoch: 92 [16384/50176]	Loss: 0.3019
Training Epoch: 92 [16640/50176]	Loss: 0.3047
Training Epoch: 92 [16896/50176]	Loss: 0.2428
Training Epoch: 92 [17152/50176]	Loss: 0.3987
Training Epoch: 92 [17408/50176]	Loss: 0.2966
Training Epoch: 92 [17664/50176]	Loss: 0.2621
Training Epoch: 92 [17920/50176]	Loss: 0.2394
Training Epoch: 92 [18176/50176]	Loss: 0.2756
Training Epoch: 92 [18432/50176]	Loss: 0.1972
Training Epoch: 92 [18688/50176]	Loss: 0.3653
Training Epoch: 92 [18944/50176]	Loss: 0.2559
Training Epoch: 92 [19200/50176]	Loss: 0.2435
Training Epoch: 92 [19456/50176]	Loss: 0.1662
Training Epoch: 92 [19712/50176]	Loss: 0.3538
Training Epoch: 92 [19968/50176]	Loss: 0.3958
Training Epoch: 92 [20224/50176]	Loss: 0.2417
Training Epoch: 92 [20480/50176]	Loss: 0.2487
Training Epoch: 92 [20736/50176]	Loss: 0.2629
Training Epoch: 92 [20992/50176]	Loss: 0.3921
Training Epoch: 92 [21248/50176]	Loss: 0.3168
Training Epoch: 92 [21504/50176]	Loss: 0.3359
Training Epoch: 92 [21760/50176]	Loss: 0.3727
Training Epoch: 92 [22016/50176]	Loss: 0.3232
Training Epoch: 92 [22272/50176]	Loss: 0.2516
Training Epoch: 92 [22528/50176]	Loss: 0.2454
Training Epoch: 92 [22784/50176]	Loss: 0.2463
Training Epoch: 92 [23040/50176]	Loss: 0.3223
Training Epoch: 92 [23296/50176]	Loss: 0.3369
Training Epoch: 92 [23552/50176]	Loss: 0.2883
Training Epoch: 92 [23808/50176]	Loss: 0.2379
Training Epoch: 92 [24064/50176]	Loss: 0.3427
Training Epoch: 92 [24320/50176]	Loss: 0.3377
Training Epoch: 92 [24576/50176]	Loss: 0.3206
Training Epoch: 92 [24832/50176]	Loss: 0.2124
Training Epoch: 92 [25088/50176]	Loss: 0.1989
Training Epoch: 92 [25344/50176]	Loss: 0.3107
Training Epoch: 92 [25600/50176]	Loss: 0.2744
Training Epoch: 92 [25856/50176]	Loss: 0.2801
Training Epoch: 92 [26112/50176]	Loss: 0.2649
Training Epoch: 92 [26368/50176]	Loss: 0.3085
Training Epoch: 92 [26624/50176]	Loss: 0.2080
Training Epoch: 92 [26880/50176]	Loss: 0.3337
Training Epoch: 92 [27136/50176]	Loss: 0.3920
Training Epoch: 92 [27392/50176]	Loss: 0.2844
Training Epoch: 92 [27648/50176]	Loss: 0.2433
Training Epoch: 92 [27904/50176]	Loss: 0.3412
Training Epoch: 92 [28160/50176]	Loss: 0.2636
Training Epoch: 92 [28416/50176]	Loss: 0.2922
Training Epoch: 92 [28672/50176]	Loss: 0.2179
Training Epoch: 92 [28928/50176]	Loss: 0.1956
Training Epoch: 92 [29184/50176]	Loss: 0.3870
Training Epoch: 92 [29440/50176]	Loss: 0.2228
Training Epoch: 92 [29696/50176]	Loss: 0.2747
Training Epoch: 92 [29952/50176]	Loss: 0.3410
Training Epoch: 92 [30208/50176]	Loss: 0.3302
Training Epoch: 92 [30464/50176]	Loss: 0.2681
Training Epoch: 92 [30720/50176]	Loss: 0.2506
Training Epoch: 92 [30976/50176]	Loss: 0.3595
Training Epoch: 92 [31232/50176]	Loss: 0.2642
Training Epoch: 92 [31488/50176]	Loss: 0.2566
Training Epoch: 92 [31744/50176]	Loss: 0.2385
Training Epoch: 92 [32000/50176]	Loss: 0.2753
Training Epoch: 92 [32256/50176]	Loss: 0.2481
Training Epoch: 92 [32512/50176]	Loss: 0.2835
Training Epoch: 92 [32768/50176]	Loss: 0.2327
Training Epoch: 92 [33024/50176]	Loss: 0.3791
Training Epoch: 92 [33280/50176]	Loss: 0.2576
Training Epoch: 92 [33536/50176]	Loss: 0.2209
Training Epoch: 92 [33792/50176]	Loss: 0.2615
Training Epoch: 92 [34048/50176]	Loss: 0.2717
Training Epoch: 92 [34304/50176]	Loss: 0.3284
Training Epoch: 92 [34560/50176]	Loss: 0.3004
Training Epoch: 92 [34816/50176]	Loss: 0.3604
Training Epoch: 92 [35072/50176]	Loss: 0.2694
Training Epoch: 92 [35328/50176]	Loss: 0.2799
Training Epoch: 92 [35584/50176]	Loss: 0.2832
Training Epoch: 92 [35840/50176]	Loss: 0.3272
Training Epoch: 92 [36096/50176]	Loss: 0.3626
Training Epoch: 92 [36352/50176]	Loss: 0.3395
Training Epoch: 92 [36608/50176]	Loss: 0.2779
Training Epoch: 92 [36864/50176]	Loss: 0.2181
Training Epoch: 92 [37120/50176]	Loss: 0.2274
Training Epoch: 92 [37376/50176]	Loss: 0.2367
Training Epoch: 92 [37632/50176]	Loss: 0.2968
Training Epoch: 92 [37888/50176]	Loss: 0.2138
Training Epoch: 92 [38144/50176]	Loss: 0.2777
Training Epoch: 92 [38400/50176]	Loss: 0.3386
Training Epoch: 92 [38656/50176]	Loss: 0.2676
Training Epoch: 92 [38912/50176]	Loss: 0.2170
Training Epoch: 92 [39168/50176]	Loss: 0.3526
Training Epoch: 92 [39424/50176]	Loss: 0.3602
Training Epoch: 92 [39680/50176]	Loss: 0.2367
Training Epoch: 92 [39936/50176]	Loss: 0.2242
Training Epoch: 92 [40192/50176]	Loss: 0.3753
Training Epoch: 92 [40448/50176]	Loss: 0.3913
Training Epoch: 92 [40704/50176]	Loss: 0.4236
Training Epoch: 92 [40960/50176]	Loss: 0.2967
Training Epoch: 92 [41216/50176]	Loss: 0.3512
Training Epoch: 92 [41472/50176]	Loss: 0.3548
Training Epoch: 92 [41728/50176]	Loss: 0.3063
Training Epoch: 92 [41984/50176]	Loss: 0.4242
Training Epoch: 92 [42240/50176]	Loss: 0.3395
Training Epoch: 92 [42496/50176]	Loss: 0.2657
Training Epoch: 92 [42752/50176]	Loss: 0.3550
Training Epoch: 92 [43008/50176]	Loss: 0.2238
Training Epoch: 92 [43264/50176]	Loss: 0.2551
Training Epoch: 92 [43520/50176]	Loss: 0.2879
Training Epoch: 92 [43776/50176]	Loss: 0.2714
Training Epoch: 92 [44032/50176]	Loss: 0.2846
Training Epoch: 92 [44288/50176]	Loss: 0.3522
Training Epoch: 92 [44544/50176]	Loss: 0.2793
Training Epoch: 92 [44800/50176]	Loss: 0.3780
Training Epoch: 92 [45056/50176]	Loss: 0.2599
Training Epoch: 92 [45312/50176]	Loss: 0.2892
Training Epoch: 92 [45568/50176]	Loss: 0.4089
Training Epoch: 92 [45824/50176]	Loss: 0.3184
Training Epoch: 92 [46080/50176]	Loss: 0.2975
Training Epoch: 92 [46336/50176]	Loss: 0.2803
Training Epoch: 92 [46592/50176]	Loss: 0.2628
Training Epoch: 92 [46848/50176]	Loss: 0.2952
Training Epoch: 92 [47104/50176]	Loss: 0.3369
Training Epoch: 92 [47360/50176]	Loss: 0.4051
Training Epoch: 92 [47616/50176]	Loss: 0.2982
Training Epoch: 92 [47872/50176]	Loss: 0.3153
Training Epoch: 92 [48128/50176]	Loss: 0.3102
Training Epoch: 92 [48384/50176]	Loss: 0.2758
Training Epoch: 92 [48640/50176]	Loss: 0.2598
Training Epoch: 92 [48896/50176]	Loss: 0.2553
Training Epoch: 92 [49152/50176]	Loss: 0.4242
Training Epoch: 92 [49408/50176]	Loss: 0.3336
Training Epoch: 92 [49664/50176]	Loss: 0.2937
Training Epoch: 92 [49920/50176]	Loss: 0.2966
Training Epoch: 92 [50176/50176]	Loss: 0.3439
Validation Epoch: 92, Average loss: 0.0113, Accuracy: 0.5659
Training Epoch: 93 [256/50176]	Loss: 0.2115
Training Epoch: 93 [512/50176]	Loss: 0.2222
Training Epoch: 93 [768/50176]	Loss: 0.3088
Training Epoch: 93 [1024/50176]	Loss: 0.2631
Training Epoch: 93 [1280/50176]	Loss: 0.3299
Training Epoch: 93 [1536/50176]	Loss: 0.3021
Training Epoch: 93 [1792/50176]	Loss: 0.1761
Training Epoch: 93 [2048/50176]	Loss: 0.1912
Training Epoch: 93 [2304/50176]	Loss: 0.2737
Training Epoch: 93 [2560/50176]	Loss: 0.3043
Training Epoch: 93 [2816/50176]	Loss: 0.3576
Training Epoch: 93 [3072/50176]	Loss: 0.3208
Training Epoch: 93 [3328/50176]	Loss: 0.2929
Training Epoch: 93 [3584/50176]	Loss: 0.2361
Training Epoch: 93 [3840/50176]	Loss: 0.1690
Training Epoch: 93 [4096/50176]	Loss: 0.2874
Training Epoch: 93 [4352/50176]	Loss: 0.2252
Training Epoch: 93 [4608/50176]	Loss: 0.3028
Training Epoch: 93 [4864/50176]	Loss: 0.3013
Training Epoch: 93 [5120/50176]	Loss: 0.1910
Training Epoch: 93 [5376/50176]	Loss: 0.2535
Training Epoch: 93 [5632/50176]	Loss: 0.2734
Training Epoch: 93 [5888/50176]	Loss: 0.2101
Training Epoch: 93 [6144/50176]	Loss: 0.2595
Training Epoch: 93 [6400/50176]	Loss: 0.2334
Training Epoch: 93 [6656/50176]	Loss: 0.3185
Training Epoch: 93 [6912/50176]	Loss: 0.2579
Training Epoch: 93 [7168/50176]	Loss: 0.2650
Training Epoch: 93 [7424/50176]	Loss: 0.2957
Training Epoch: 93 [7680/50176]	Loss: 0.2103
Training Epoch: 93 [7936/50176]	Loss: 0.3074
Training Epoch: 93 [8192/50176]	Loss: 0.3569
Training Epoch: 93 [8448/50176]	Loss: 0.2255
Training Epoch: 93 [8704/50176]	Loss: 0.3187
Training Epoch: 93 [8960/50176]	Loss: 0.2338
Training Epoch: 93 [9216/50176]	Loss: 0.2859
Training Epoch: 93 [9472/50176]	Loss: 0.3497
Training Epoch: 93 [9728/50176]	Loss: 0.3057
Training Epoch: 93 [9984/50176]	Loss: 0.2967
Training Epoch: 93 [10240/50176]	Loss: 0.1877
Training Epoch: 93 [10496/50176]	Loss: 0.1839
Training Epoch: 93 [10752/50176]	Loss: 0.1898
Training Epoch: 93 [11008/50176]	Loss: 0.2470
Training Epoch: 93 [11264/50176]	Loss: 0.2106
Training Epoch: 93 [11520/50176]	Loss: 0.2960
Training Epoch: 93 [11776/50176]	Loss: 0.1821
Training Epoch: 93 [12032/50176]	Loss: 0.2767
Training Epoch: 93 [12288/50176]	Loss: 0.3328
Training Epoch: 93 [12544/50176]	Loss: 0.2590
Training Epoch: 93 [12800/50176]	Loss: 0.2955
Training Epoch: 93 [13056/50176]	Loss: 0.1840
Training Epoch: 93 [13312/50176]	Loss: 0.2500
Training Epoch: 93 [13568/50176]	Loss: 0.2632
Training Epoch: 93 [13824/50176]	Loss: 0.2590
Training Epoch: 93 [14080/50176]	Loss: 0.2333
Training Epoch: 93 [14336/50176]	Loss: 0.1960
Training Epoch: 93 [14592/50176]	Loss: 0.2558
Training Epoch: 93 [14848/50176]	Loss: 0.2301
Training Epoch: 93 [15104/50176]	Loss: 0.2100
Training Epoch: 93 [15360/50176]	Loss: 0.2641
Training Epoch: 93 [15616/50176]	Loss: 0.2627
Training Epoch: 93 [15872/50176]	Loss: 0.3491
Training Epoch: 93 [16128/50176]	Loss: 0.2361
Training Epoch: 93 [16384/50176]	Loss: 0.2859
Training Epoch: 93 [16640/50176]	Loss: 0.2753
Training Epoch: 93 [16896/50176]	Loss: 0.3391
Training Epoch: 93 [17152/50176]	Loss: 0.2277
Training Epoch: 93 [17408/50176]	Loss: 0.3233
Training Epoch: 93 [17664/50176]	Loss: 0.2184
Training Epoch: 93 [17920/50176]	Loss: 0.1922
Training Epoch: 93 [18176/50176]	Loss: 0.2728
Training Epoch: 93 [18432/50176]	Loss: 0.2348
Training Epoch: 93 [18688/50176]	Loss: 0.3326
Training Epoch: 93 [18944/50176]	Loss: 0.3111
Training Epoch: 93 [19200/50176]	Loss: 0.3321
Training Epoch: 93 [19456/50176]	Loss: 0.3192
Training Epoch: 93 [19712/50176]	Loss: 0.2971
Training Epoch: 93 [19968/50176]	Loss: 0.2663
Training Epoch: 93 [20224/50176]	Loss: 0.2653
Training Epoch: 93 [20480/50176]	Loss: 0.2657
Training Epoch: 93 [20736/50176]	Loss: 0.2217
Training Epoch: 93 [20992/50176]	Loss: 0.2056
Training Epoch: 93 [21248/50176]	Loss: 0.2629
Training Epoch: 93 [21504/50176]	Loss: 0.2010
Training Epoch: 93 [21760/50176]	Loss: 0.2380
Training Epoch: 93 [22016/50176]	Loss: 0.2522
Training Epoch: 93 [22272/50176]	Loss: 0.2686
Training Epoch: 93 [22528/50176]	Loss: 0.2565
Training Epoch: 93 [22784/50176]	Loss: 0.2291
Training Epoch: 93 [23040/50176]	Loss: 0.2498
Training Epoch: 93 [23296/50176]	Loss: 0.2340
Training Epoch: 93 [23552/50176]	Loss: 0.2826
Training Epoch: 93 [23808/50176]	Loss: 0.2937
Training Epoch: 93 [24064/50176]	Loss: 0.3293
Training Epoch: 93 [24320/50176]	Loss: 0.2595
Training Epoch: 93 [24576/50176]	Loss: 0.2467
Training Epoch: 93 [24832/50176]	Loss: 0.2841
Training Epoch: 93 [25088/50176]	Loss: 0.1888
Training Epoch: 93 [25344/50176]	Loss: 0.3154
Training Epoch: 93 [25600/50176]	Loss: 0.2159
Training Epoch: 93 [25856/50176]	Loss: 0.2740
Training Epoch: 93 [26112/50176]	Loss: 0.3104
Training Epoch: 93 [26368/50176]	Loss: 0.2386
Training Epoch: 93 [26624/50176]	Loss: 0.2809
Training Epoch: 93 [26880/50176]	Loss: 0.1983
Training Epoch: 93 [27136/50176]	Loss: 0.2808
Training Epoch: 93 [27392/50176]	Loss: 0.4057
Training Epoch: 93 [27648/50176]	Loss: 0.3383
Training Epoch: 93 [27904/50176]	Loss: 0.2364
Training Epoch: 93 [28160/50176]	Loss: 0.3107
Training Epoch: 93 [28416/50176]	Loss: 0.2181
Training Epoch: 93 [28672/50176]	Loss: 0.2764
Training Epoch: 93 [28928/50176]	Loss: 0.3055
Training Epoch: 93 [29184/50176]	Loss: 0.2275
Training Epoch: 93 [29440/50176]	Loss: 0.2348
Training Epoch: 93 [29696/50176]	Loss: 0.2653
Training Epoch: 93 [29952/50176]	Loss: 0.3090
Training Epoch: 93 [30208/50176]	Loss: 0.2528
Training Epoch: 93 [30464/50176]	Loss: 0.2820
Training Epoch: 93 [30720/50176]	Loss: 0.2521
Training Epoch: 93 [30976/50176]	Loss: 0.2471
Training Epoch: 93 [31232/50176]	Loss: 0.3820
Training Epoch: 93 [31488/50176]	Loss: 0.3326
Training Epoch: 93 [31744/50176]	Loss: 0.3003
Training Epoch: 93 [32000/50176]	Loss: 0.3143
Training Epoch: 93 [32256/50176]	Loss: 0.3824
Training Epoch: 93 [32512/50176]	Loss: 0.2766
Training Epoch: 93 [32768/50176]	Loss: 0.2358
Training Epoch: 93 [33024/50176]	Loss: 0.2704
Training Epoch: 93 [33280/50176]	Loss: 0.2324
Training Epoch: 93 [33536/50176]	Loss: 0.2824
Training Epoch: 93 [33792/50176]	Loss: 0.2761
Training Epoch: 93 [34048/50176]	Loss: 0.2426
Training Epoch: 93 [34304/50176]	Loss: 0.2717
Training Epoch: 93 [34560/50176]	Loss: 0.3566
Training Epoch: 93 [34816/50176]	Loss: 0.2960
Training Epoch: 93 [35072/50176]	Loss: 0.2401
Training Epoch: 93 [35328/50176]	Loss: 0.4173
Training Epoch: 93 [35584/50176]	Loss: 0.2703
Training Epoch: 93 [35840/50176]	Loss: 0.2661
Training Epoch: 93 [36096/50176]	Loss: 0.2951
Training Epoch: 93 [36352/50176]	Loss: 0.2575
Training Epoch: 93 [36608/50176]	Loss: 0.3762
Training Epoch: 93 [36864/50176]	Loss: 0.2620
Training Epoch: 93 [37120/50176]	Loss: 0.3346
Training Epoch: 93 [37376/50176]	Loss: 0.2408
Training Epoch: 93 [37632/50176]	Loss: 0.2983
Training Epoch: 93 [37888/50176]	Loss: 0.3076
Training Epoch: 93 [38144/50176]	Loss: 0.3861
Training Epoch: 93 [38400/50176]	Loss: 0.3305
Training Epoch: 93 [38656/50176]	Loss: 0.3548
Training Epoch: 93 [38912/50176]	Loss: 0.3471
Training Epoch: 93 [39168/50176]	Loss: 0.3530
Training Epoch: 93 [39424/50176]	Loss: 0.2836
Training Epoch: 93 [39680/50176]	Loss: 0.2544
Training Epoch: 93 [39936/50176]	Loss: 0.2690
Training Epoch: 93 [40192/50176]	Loss: 0.3270
Training Epoch: 93 [40448/50176]	Loss: 0.3087
Training Epoch: 93 [40704/50176]	Loss: 0.2916
Training Epoch: 93 [40960/50176]	Loss: 0.3061
Training Epoch: 93 [41216/50176]	Loss: 0.3154
Training Epoch: 93 [41472/50176]	Loss: 0.2944
Training Epoch: 93 [41728/50176]	Loss: 0.2872
Training Epoch: 93 [41984/50176]	Loss: 0.3678
Training Epoch: 93 [42240/50176]	Loss: 0.2798
Training Epoch: 93 [42496/50176]	Loss: 0.2552
Training Epoch: 93 [42752/50176]	Loss: 0.2941
Training Epoch: 93 [43008/50176]	Loss: 0.4043
Training Epoch: 93 [43264/50176]	Loss: 0.2963
Training Epoch: 93 [43520/50176]	Loss: 0.3051
Training Epoch: 93 [43776/50176]	Loss: 0.2924
Training Epoch: 93 [44032/50176]	Loss: 0.3406
Training Epoch: 93 [44288/50176]	Loss: 0.3020
Training Epoch: 93 [44544/50176]	Loss: 0.2528
Training Epoch: 93 [44800/50176]	Loss: 0.3739
Training Epoch: 93 [45056/50176]	Loss: 0.3727
Training Epoch: 93 [45312/50176]	Loss: 0.2379
Training Epoch: 93 [45568/50176]	Loss: 0.3636
Training Epoch: 93 [45824/50176]	Loss: 0.2424
Training Epoch: 93 [46080/50176]	Loss: 0.3507
Training Epoch: 93 [46336/50176]	Loss: 0.2423
Training Epoch: 93 [46592/50176]	Loss: 0.2983
Training Epoch: 93 [46848/50176]	Loss: 0.4624
Training Epoch: 93 [47104/50176]	Loss: 0.2510
Training Epoch: 93 [47360/50176]	Loss: 0.2094
Training Epoch: 93 [47616/50176]	Loss: 0.3590
Training Epoch: 93 [47872/50176]	Loss: 0.3743
Training Epoch: 93 [48128/50176]	Loss: 0.2851
Training Epoch: 93 [48384/50176]	Loss: 0.3884
Training Epoch: 93 [48640/50176]	Loss: 0.2772
Training Epoch: 93 [48896/50176]	Loss: 0.3938
Training Epoch: 93 [49152/50176]	Loss: 0.3665
Training Epoch: 93 [49408/50176]	Loss: 0.3252
Training Epoch: 93 [49664/50176]	Loss: 0.3796
Training Epoch: 93 [49920/50176]	Loss: 0.2932
Training Epoch: 93 [50176/50176]	Loss: 0.4514
Validation Epoch: 93, Average loss: 0.0115, Accuracy: 0.5714
Training Epoch: 94 [256/50176]	Loss: 0.2144
Training Epoch: 94 [512/50176]	Loss: 0.2557
Training Epoch: 94 [768/50176]	Loss: 0.2390
Training Epoch: 94 [1024/50176]	Loss: 0.2928
Training Epoch: 94 [1280/50176]	Loss: 0.1732
Training Epoch: 94 [1536/50176]	Loss: 0.2061
Training Epoch: 94 [1792/50176]	Loss: 0.2678
Training Epoch: 94 [2048/50176]	Loss: 0.2936
Training Epoch: 94 [2304/50176]	Loss: 0.2168
Training Epoch: 94 [2560/50176]	Loss: 0.3333
Training Epoch: 94 [2816/50176]	Loss: 0.2997
Training Epoch: 94 [3072/50176]	Loss: 0.2718
Training Epoch: 94 [3328/50176]	Loss: 0.3754
Training Epoch: 94 [3584/50176]	Loss: 0.2227
Training Epoch: 94 [3840/50176]	Loss: 0.2654
Training Epoch: 94 [4096/50176]	Loss: 0.2515
Training Epoch: 94 [4352/50176]	Loss: 0.2585
Training Epoch: 94 [4608/50176]	Loss: 0.2017
Training Epoch: 94 [4864/50176]	Loss: 0.3223
Training Epoch: 94 [5120/50176]	Loss: 0.3300
Training Epoch: 94 [5376/50176]	Loss: 0.2403
Training Epoch: 94 [5632/50176]	Loss: 0.3223
Training Epoch: 94 [5888/50176]	Loss: 0.2765
Training Epoch: 94 [6144/50176]	Loss: 0.2067
Training Epoch: 94 [6400/50176]	Loss: 0.3231
Training Epoch: 94 [6656/50176]	Loss: 0.2048
Training Epoch: 94 [6912/50176]	Loss: 0.2558
Training Epoch: 94 [7168/50176]	Loss: 0.2080
Training Epoch: 94 [7424/50176]	Loss: 0.2473
Training Epoch: 94 [7680/50176]	Loss: 0.2638
Training Epoch: 94 [7936/50176]	Loss: 0.2899
Training Epoch: 94 [8192/50176]	Loss: 0.3502
Training Epoch: 94 [8448/50176]	Loss: 0.2129
Training Epoch: 94 [8704/50176]	Loss: 0.2503
Training Epoch: 94 [8960/50176]	Loss: 0.1767
Training Epoch: 94 [9216/50176]	Loss: 0.3021
Training Epoch: 94 [9472/50176]	Loss: 0.2440
Training Epoch: 94 [9728/50176]	Loss: 0.3344
Training Epoch: 94 [9984/50176]	Loss: 0.2757
Training Epoch: 94 [10240/50176]	Loss: 0.2399
Training Epoch: 94 [10496/50176]	Loss: 0.3215
Training Epoch: 94 [10752/50176]	Loss: 0.2564
Training Epoch: 94 [11008/50176]	Loss: 0.3006
Training Epoch: 94 [11264/50176]	Loss: 0.1773
Training Epoch: 94 [11520/50176]	Loss: 0.2620
Training Epoch: 94 [11776/50176]	Loss: 0.1832
Training Epoch: 94 [12032/50176]	Loss: 0.2864
Training Epoch: 94 [12288/50176]	Loss: 0.2681
Training Epoch: 94 [12544/50176]	Loss: 0.3370
Training Epoch: 94 [12800/50176]	Loss: 0.2606
Training Epoch: 94 [13056/50176]	Loss: 0.1822
Training Epoch: 94 [13312/50176]	Loss: 0.2171
Training Epoch: 94 [13568/50176]	Loss: 0.3401
Training Epoch: 94 [13824/50176]	Loss: 0.3510
Training Epoch: 94 [14080/50176]	Loss: 0.2551
Training Epoch: 94 [14336/50176]	Loss: 0.3537
Training Epoch: 94 [14592/50176]	Loss: 0.3034
Training Epoch: 94 [14848/50176]	Loss: 0.2291
Training Epoch: 94 [15104/50176]	Loss: 0.3293
Training Epoch: 94 [15360/50176]	Loss: 0.3174
Training Epoch: 94 [15616/50176]	Loss: 0.2581
Training Epoch: 94 [15872/50176]	Loss: 0.2537
Training Epoch: 94 [16128/50176]	Loss: 0.2730
Training Epoch: 94 [16384/50176]	Loss: 0.2795
Training Epoch: 94 [16640/50176]	Loss: 0.3328
Training Epoch: 94 [16896/50176]	Loss: 0.1982
Training Epoch: 94 [17152/50176]	Loss: 0.2799
Training Epoch: 94 [17408/50176]	Loss: 0.2674
Training Epoch: 94 [17664/50176]	Loss: 0.2829
Training Epoch: 94 [17920/50176]	Loss: 0.2842
Training Epoch: 94 [18176/50176]	Loss: 0.2723
Training Epoch: 94 [18432/50176]	Loss: 0.2902
Training Epoch: 94 [18688/50176]	Loss: 0.2688
Training Epoch: 94 [18944/50176]	Loss: 0.3231
Training Epoch: 94 [19200/50176]	Loss: 0.2503
Training Epoch: 94 [19456/50176]	Loss: 0.2872
Training Epoch: 94 [19712/50176]	Loss: 0.3295
Training Epoch: 94 [19968/50176]	Loss: 0.3165
Training Epoch: 94 [20224/50176]	Loss: 0.2268
Training Epoch: 94 [20480/50176]	Loss: 0.3156
Training Epoch: 94 [20736/50176]	Loss: 0.3125
Training Epoch: 94 [20992/50176]	Loss: 0.3114
Training Epoch: 94 [21248/50176]	Loss: 0.2258
Training Epoch: 94 [21504/50176]	Loss: 0.2528
Training Epoch: 94 [21760/50176]	Loss: 0.2583
Training Epoch: 94 [22016/50176]	Loss: 0.2861
Training Epoch: 94 [22272/50176]	Loss: 0.2048
Training Epoch: 94 [22528/50176]	Loss: 0.2603
Training Epoch: 94 [22784/50176]	Loss: 0.2737
Training Epoch: 94 [23040/50176]	Loss: 0.3147
Training Epoch: 94 [23296/50176]	Loss: 0.2837
Training Epoch: 94 [23552/50176]	Loss: 0.2735
Training Epoch: 94 [23808/50176]	Loss: 0.2127
Training Epoch: 94 [24064/50176]	Loss: 0.2918
Training Epoch: 94 [24320/50176]	Loss: 0.2522
Training Epoch: 94 [24576/50176]	Loss: 0.3612
Training Epoch: 94 [24832/50176]	Loss: 0.2844
Training Epoch: 94 [25088/50176]	Loss: 0.2627
Training Epoch: 94 [25344/50176]	Loss: 0.3276
Training Epoch: 94 [25600/50176]	Loss: 0.2187
Training Epoch: 94 [25856/50176]	Loss: 0.2812
Training Epoch: 94 [26112/50176]	Loss: 0.3036
Training Epoch: 94 [26368/50176]	Loss: 0.2291
Training Epoch: 94 [26624/50176]	Loss: 0.2224
Training Epoch: 94 [26880/50176]	Loss: 0.2747
Training Epoch: 94 [27136/50176]	Loss: 0.3015
Training Epoch: 94 [27392/50176]	Loss: 0.2605
Training Epoch: 94 [27648/50176]	Loss: 0.2450
Training Epoch: 94 [27904/50176]	Loss: 0.2034
Training Epoch: 94 [28160/50176]	Loss: 0.3139
Training Epoch: 94 [28416/50176]	Loss: 0.3443
Training Epoch: 94 [28672/50176]	Loss: 0.2126
Training Epoch: 94 [28928/50176]	Loss: 0.2721
Training Epoch: 94 [29184/50176]	Loss: 0.2321
Training Epoch: 94 [29440/50176]	Loss: 0.2405
Training Epoch: 94 [29696/50176]	Loss: 0.2969
Training Epoch: 94 [29952/50176]	Loss: 0.2481
Training Epoch: 94 [30208/50176]	Loss: 0.3164
Training Epoch: 94 [30464/50176]	Loss: 0.2594
Training Epoch: 94 [30720/50176]	Loss: 0.2772
Training Epoch: 94 [30976/50176]	Loss: 0.3356
Training Epoch: 94 [31232/50176]	Loss: 0.3828
Training Epoch: 94 [31488/50176]	Loss: 0.2510
Training Epoch: 94 [31744/50176]	Loss: 0.2616
Training Epoch: 94 [32000/50176]	Loss: 0.2573
Training Epoch: 94 [32256/50176]	Loss: 0.3942
Training Epoch: 94 [32512/50176]	Loss: 0.3188
Training Epoch: 94 [32768/50176]	Loss: 0.1580
Training Epoch: 94 [33024/50176]	Loss: 0.3590
Training Epoch: 94 [33280/50176]	Loss: 0.3152
Training Epoch: 94 [33536/50176]	Loss: 0.2289
Training Epoch: 94 [33792/50176]	Loss: 0.2909
Training Epoch: 94 [34048/50176]	Loss: 0.2491
Training Epoch: 94 [34304/50176]	Loss: 0.2767
Training Epoch: 94 [34560/50176]	Loss: 0.2877
Training Epoch: 94 [34816/50176]	Loss: 0.3041
Training Epoch: 94 [35072/50176]	Loss: 0.2318
Training Epoch: 94 [35328/50176]	Loss: 0.3152
Training Epoch: 94 [35584/50176]	Loss: 0.3309
Training Epoch: 94 [35840/50176]	Loss: 0.2503
Training Epoch: 94 [36096/50176]	Loss: 0.3158
Training Epoch: 94 [36352/50176]	Loss: 0.2670
Training Epoch: 94 [36608/50176]	Loss: 0.3352
Training Epoch: 94 [36864/50176]	Loss: 0.2231
Training Epoch: 94 [37120/50176]	Loss: 0.4384
Training Epoch: 94 [37376/50176]	Loss: 0.3708
Training Epoch: 94 [37632/50176]	Loss: 0.3269
Training Epoch: 94 [37888/50176]	Loss: 0.3254
Training Epoch: 94 [38144/50176]	Loss: 0.3095
Training Epoch: 94 [38400/50176]	Loss: 0.2498
Training Epoch: 94 [38656/50176]	Loss: 0.2611
Training Epoch: 94 [38912/50176]	Loss: 0.2251
Training Epoch: 94 [39168/50176]	Loss: 0.2599
Training Epoch: 94 [39424/50176]	Loss: 0.2404
Training Epoch: 94 [39680/50176]	Loss: 0.2884
Training Epoch: 94 [39936/50176]	Loss: 0.4521
Training Epoch: 94 [40192/50176]	Loss: 0.2926
Training Epoch: 94 [40448/50176]	Loss: 0.3932
Training Epoch: 94 [40704/50176]	Loss: 0.2700
Training Epoch: 94 [40960/50176]	Loss: 0.2998
Training Epoch: 94 [41216/50176]	Loss: 0.3135
Training Epoch: 94 [41472/50176]	Loss: 0.2533
Training Epoch: 94 [41728/50176]	Loss: 0.3213
Training Epoch: 94 [41984/50176]	Loss: 0.3124
Training Epoch: 94 [42240/50176]	Loss: 0.3504
Training Epoch: 94 [42496/50176]	Loss: 0.2856
Training Epoch: 94 [42752/50176]	Loss: 0.3691
Training Epoch: 94 [43008/50176]	Loss: 0.2776
Training Epoch: 94 [43264/50176]	Loss: 0.3269
Training Epoch: 94 [43520/50176]	Loss: 0.2508
Training Epoch: 94 [43776/50176]	Loss: 0.3410
Training Epoch: 94 [44032/50176]	Loss: 0.2913
Training Epoch: 94 [44288/50176]	Loss: 0.2569
Training Epoch: 94 [44544/50176]	Loss: 0.3524
Training Epoch: 94 [44800/50176]	Loss: 0.3407
Training Epoch: 94 [45056/50176]	Loss: 0.3482
Training Epoch: 94 [45312/50176]	Loss: 0.3109
Training Epoch: 94 [45568/50176]	Loss: 0.2765
Training Epoch: 94 [45824/50176]	Loss: 0.2850
Training Epoch: 94 [46080/50176]	Loss: 0.3498
Training Epoch: 94 [46336/50176]	Loss: 0.3263
Training Epoch: 94 [46592/50176]	Loss: 0.2498
Training Epoch: 94 [46848/50176]	Loss: 0.4036
Training Epoch: 94 [47104/50176]	Loss: 0.3168
Training Epoch: 94 [47360/50176]	Loss: 0.2868
Training Epoch: 94 [47616/50176]	Loss: 0.3270
Training Epoch: 94 [47872/50176]	Loss: 0.3329
Training Epoch: 94 [48128/50176]	Loss: 0.2031
Training Epoch: 94 [48384/50176]	Loss: 0.4010
Training Epoch: 94 [48640/50176]	Loss: 0.3055
Training Epoch: 94 [48896/50176]	Loss: 0.2369
Training Epoch: 94 [49152/50176]	Loss: 0.2163
Training Epoch: 94 [49408/50176]	Loss: 0.4748
Training Epoch: 94 [49664/50176]	Loss: 0.1864
Training Epoch: 94 [49920/50176]	Loss: 0.3136
Training Epoch: 94 [50176/50176]	Loss: 0.2600
Validation Epoch: 94, Average loss: 0.0099, Accuracy: 0.5938
Training Epoch: 95 [256/50176]	Loss: 0.2840
Training Epoch: 95 [512/50176]	Loss: 0.3910
Training Epoch: 95 [768/50176]	Loss: 0.2821
Training Epoch: 95 [1024/50176]	Loss: 0.2078
Training Epoch: 95 [1280/50176]	Loss: 0.2158
Training Epoch: 95 [1536/50176]	Loss: 0.2407
Training Epoch: 95 [1792/50176]	Loss: 0.2719
Training Epoch: 95 [2048/50176]	Loss: 0.3201
Training Epoch: 95 [2304/50176]	Loss: 0.3561
Training Epoch: 95 [2560/50176]	Loss: 0.1422
Training Epoch: 95 [2816/50176]	Loss: 0.2255
Training Epoch: 95 [3072/50176]	Loss: 0.3237
Training Epoch: 95 [3328/50176]	Loss: 0.2362
Training Epoch: 95 [3584/50176]	Loss: 0.3211
Training Epoch: 95 [3840/50176]	Loss: 0.2687
Training Epoch: 95 [4096/50176]	Loss: 0.1995
Training Epoch: 95 [4352/50176]	Loss: 0.2341
Training Epoch: 95 [4608/50176]	Loss: 0.2386
Training Epoch: 95 [4864/50176]	Loss: 0.2852
Training Epoch: 95 [5120/50176]	Loss: 0.2539
Training Epoch: 95 [5376/50176]	Loss: 0.2016
Training Epoch: 95 [5632/50176]	Loss: 0.2393
Training Epoch: 95 [5888/50176]	Loss: 0.2078
Training Epoch: 95 [6144/50176]	Loss: 0.2158
Training Epoch: 95 [6400/50176]	Loss: 0.2629
Training Epoch: 95 [6656/50176]	Loss: 0.2003
Training Epoch: 95 [6912/50176]	Loss: 0.2452
Training Epoch: 95 [7168/50176]	Loss: 0.2215
Training Epoch: 95 [7424/50176]	Loss: 0.2852
Training Epoch: 95 [7680/50176]	Loss: 0.2570
Training Epoch: 95 [7936/50176]	Loss: 0.2541
Training Epoch: 95 [8192/50176]	Loss: 0.2625
Training Epoch: 95 [8448/50176]	Loss: 0.2659
Training Epoch: 95 [8704/50176]	Loss: 0.2313
Training Epoch: 95 [8960/50176]	Loss: 0.3251
Training Epoch: 95 [9216/50176]	Loss: 0.2299
Training Epoch: 95 [9472/50176]	Loss: 0.2156
Training Epoch: 95 [9728/50176]	Loss: 0.2718
Training Epoch: 95 [9984/50176]	Loss: 0.3836
Training Epoch: 95 [10240/50176]	Loss: 0.3925
Training Epoch: 95 [10496/50176]	Loss: 0.3108
Training Epoch: 95 [10752/50176]	Loss: 0.2296
Training Epoch: 95 [11008/50176]	Loss: 0.2483
Training Epoch: 95 [11264/50176]	Loss: 0.2120
Training Epoch: 95 [11520/50176]	Loss: 0.1533
Training Epoch: 95 [11776/50176]	Loss: 0.2503
Training Epoch: 95 [12032/50176]	Loss: 0.2871
Training Epoch: 95 [12288/50176]	Loss: 0.2211
Training Epoch: 95 [12544/50176]	Loss: 0.2042
Training Epoch: 95 [12800/50176]	Loss: 0.2423
Training Epoch: 95 [13056/50176]	Loss: 0.3517
Training Epoch: 95 [13312/50176]	Loss: 0.3268
Training Epoch: 95 [13568/50176]	Loss: 0.1883
Training Epoch: 95 [13824/50176]	Loss: 0.2915
Training Epoch: 95 [14080/50176]	Loss: 0.2397
Training Epoch: 95 [14336/50176]	Loss: 0.3317
Training Epoch: 95 [14592/50176]	Loss: 0.1639
Training Epoch: 95 [14848/50176]	Loss: 0.2044
Training Epoch: 95 [15104/50176]	Loss: 0.2279
Training Epoch: 95 [15360/50176]	Loss: 0.2149
Training Epoch: 95 [15616/50176]	Loss: 0.2972
Training Epoch: 95 [15872/50176]	Loss: 0.2789
Training Epoch: 95 [16128/50176]	Loss: 0.3217
Training Epoch: 95 [16384/50176]	Loss: 0.2547
Training Epoch: 95 [16640/50176]	Loss: 0.2509
Training Epoch: 95 [16896/50176]	Loss: 0.3206
Training Epoch: 95 [17152/50176]	Loss: 0.3287
Training Epoch: 95 [17408/50176]	Loss: 0.2847
Training Epoch: 95 [17664/50176]	Loss: 0.3371
Training Epoch: 95 [17920/50176]	Loss: 0.3327
Training Epoch: 95 [18176/50176]	Loss: 0.2387
Training Epoch: 95 [18432/50176]	Loss: 0.2024
Training Epoch: 95 [18688/50176]	Loss: 0.3124
Training Epoch: 95 [18944/50176]	Loss: 0.1410
Training Epoch: 95 [19200/50176]	Loss: 0.2776
Training Epoch: 95 [19456/50176]	Loss: 0.1986
Training Epoch: 95 [19712/50176]	Loss: 0.3508
Training Epoch: 95 [19968/50176]	Loss: 0.2249
Training Epoch: 95 [20224/50176]	Loss: 0.2867
Training Epoch: 95 [20480/50176]	Loss: 0.2541
Training Epoch: 95 [20736/50176]	Loss: 0.2133
Training Epoch: 95 [20992/50176]	Loss: 0.2709
Training Epoch: 95 [21248/50176]	Loss: 0.2701
Training Epoch: 95 [21504/50176]	Loss: 0.2319
Training Epoch: 95 [21760/50176]	Loss: 0.2404
Training Epoch: 95 [22016/50176]	Loss: 0.2822
Training Epoch: 95 [22272/50176]	Loss: 0.2952
Training Epoch: 95 [22528/50176]	Loss: 0.2829
Training Epoch: 95 [22784/50176]	Loss: 0.3259
Training Epoch: 95 [23040/50176]	Loss: 0.3347
Training Epoch: 95 [23296/50176]	Loss: 0.2941
Training Epoch: 95 [23552/50176]	Loss: 0.3529
Training Epoch: 95 [23808/50176]	Loss: 0.3512
Training Epoch: 95 [24064/50176]	Loss: 0.3452
Training Epoch: 95 [24320/50176]	Loss: 0.3159
Training Epoch: 95 [24576/50176]	Loss: 0.2335
Training Epoch: 95 [24832/50176]	Loss: 0.3221
Training Epoch: 95 [25088/50176]	Loss: 0.2986
Training Epoch: 95 [25344/50176]	Loss: 0.3156
Training Epoch: 95 [25600/50176]	Loss: 0.1544
Training Epoch: 95 [25856/50176]	Loss: 0.3199
Training Epoch: 95 [26112/50176]	Loss: 0.2912
Training Epoch: 95 [26368/50176]	Loss: 0.3743
Training Epoch: 95 [26624/50176]	Loss: 0.3270
Training Epoch: 95 [26880/50176]	Loss: 0.3017
Training Epoch: 95 [27136/50176]	Loss: 0.2514
Training Epoch: 95 [27392/50176]	Loss: 0.2269
Training Epoch: 95 [27648/50176]	Loss: 0.3129
Training Epoch: 95 [27904/50176]	Loss: 0.2531
Training Epoch: 95 [28160/50176]	Loss: 0.2597
Training Epoch: 95 [28416/50176]	Loss: 0.2266
Training Epoch: 95 [28672/50176]	Loss: 0.2036
Training Epoch: 95 [28928/50176]	Loss: 0.3153
Training Epoch: 95 [29184/50176]	Loss: 0.4235
Training Epoch: 95 [29440/50176]	Loss: 0.1914
Training Epoch: 95 [29696/50176]	Loss: 0.2671
Training Epoch: 95 [29952/50176]	Loss: 0.2214
Training Epoch: 95 [30208/50176]	Loss: 0.2512
Training Epoch: 95 [30464/50176]	Loss: 0.3480
Training Epoch: 95 [30720/50176]	Loss: 0.1675
Training Epoch: 95 [30976/50176]	Loss: 0.3510
Training Epoch: 95 [31232/50176]	Loss: 0.2631
Training Epoch: 95 [31488/50176]	Loss: 0.2511
Training Epoch: 95 [31744/50176]	Loss: 0.2583
Training Epoch: 95 [32000/50176]	Loss: 0.2233
Training Epoch: 95 [32256/50176]	Loss: 0.2010
Training Epoch: 95 [32512/50176]	Loss: 0.2819
Training Epoch: 95 [32768/50176]	Loss: 0.2786
Training Epoch: 95 [33024/50176]	Loss: 0.3056
Training Epoch: 95 [33280/50176]	Loss: 0.2460
Training Epoch: 95 [33536/50176]	Loss: 0.2410
Training Epoch: 95 [33792/50176]	Loss: 0.1981
Training Epoch: 95 [34048/50176]	Loss: 0.2405
Training Epoch: 95 [34304/50176]	Loss: 0.3541
Training Epoch: 95 [34560/50176]	Loss: 0.3036
Training Epoch: 95 [34816/50176]	Loss: 0.2333
Training Epoch: 95 [35072/50176]	Loss: 0.2648
Training Epoch: 95 [35328/50176]	Loss: 0.3339
Training Epoch: 95 [35584/50176]	Loss: 0.2717
Training Epoch: 95 [35840/50176]	Loss: 0.2826
Training Epoch: 95 [36096/50176]	Loss: 0.2498
Training Epoch: 95 [36352/50176]	Loss: 0.3411
Training Epoch: 95 [36608/50176]	Loss: 0.2420
Training Epoch: 95 [36864/50176]	Loss: 0.2719
Training Epoch: 95 [37120/50176]	Loss: 0.3195
Training Epoch: 95 [37376/50176]	Loss: 0.2904
Training Epoch: 95 [37632/50176]	Loss: 0.3319
Training Epoch: 95 [37888/50176]	Loss: 0.3599
Training Epoch: 95 [38144/50176]	Loss: 0.3715
Training Epoch: 95 [38400/50176]	Loss: 0.2716
Training Epoch: 95 [38656/50176]	Loss: 0.2868
Training Epoch: 95 [38912/50176]	Loss: 0.2566
Training Epoch: 95 [39168/50176]	Loss: 0.3358
Training Epoch: 95 [39424/50176]	Loss: 0.2245
Training Epoch: 95 [39680/50176]	Loss: 0.2835
Training Epoch: 95 [39936/50176]	Loss: 0.3313
Training Epoch: 95 [40192/50176]	Loss: 0.3599
Training Epoch: 95 [40448/50176]	Loss: 0.2660
Training Epoch: 95 [40704/50176]	Loss: 0.2464
Training Epoch: 95 [40960/50176]	Loss: 0.3816
Training Epoch: 95 [41216/50176]	Loss: 0.1687
Training Epoch: 95 [41472/50176]	Loss: 0.2998
Training Epoch: 95 [41728/50176]	Loss: 0.2924
Training Epoch: 95 [41984/50176]	Loss: 0.2580
Training Epoch: 95 [42240/50176]	Loss: 0.2257
Training Epoch: 95 [42496/50176]	Loss: 0.2725
Training Epoch: 95 [42752/50176]	Loss: 0.2226
Training Epoch: 95 [43008/50176]	Loss: 0.3267
Training Epoch: 95 [43264/50176]	Loss: 0.2514
Training Epoch: 95 [43520/50176]	Loss: 0.2612
Training Epoch: 95 [43776/50176]	Loss: 0.2772
Training Epoch: 95 [44032/50176]	Loss: 0.3510
Training Epoch: 95 [44288/50176]	Loss: 0.3052
Training Epoch: 95 [44544/50176]	Loss: 0.2727
Training Epoch: 95 [44800/50176]	Loss: 0.3197
Training Epoch: 95 [45056/50176]	Loss: 0.3023
Training Epoch: 95 [45312/50176]	Loss: 0.2964
Training Epoch: 95 [45568/50176]	Loss: 0.3435
Training Epoch: 95 [45824/50176]	Loss: 0.3354
Training Epoch: 95 [46080/50176]	Loss: 0.2685
Training Epoch: 95 [46336/50176]	Loss: 0.3638
Training Epoch: 95 [46592/50176]	Loss: 0.3217
Training Epoch: 95 [46848/50176]	Loss: 0.2582
Training Epoch: 95 [47104/50176]	Loss: 0.3014
Training Epoch: 95 [47360/50176]	Loss: 0.2499
Training Epoch: 95 [47616/50176]	Loss: 0.3917
Training Epoch: 95 [47872/50176]	Loss: 0.2806
Training Epoch: 95 [48128/50176]	Loss: 0.3004
Training Epoch: 95 [48384/50176]	Loss: 0.3139
Training Epoch: 95 [48640/50176]	Loss: 0.2511
Training Epoch: 95 [48896/50176]	Loss: 0.3719
Training Epoch: 95 [49152/50176]	Loss: 0.3029
Training Epoch: 95 [49408/50176]	Loss: 0.2736
Training Epoch: 95 [49664/50176]	Loss: 0.2559
Training Epoch: 95 [49920/50176]	Loss: 0.3772
Training Epoch: 95 [50176/50176]	Loss: 0.5759
Validation Epoch: 95, Average loss: 0.0110, Accuracy: 0.5817
Training Epoch: 96 [256/50176]	Loss: 0.2442
Training Epoch: 96 [512/50176]	Loss: 0.2771
Training Epoch: 96 [768/50176]	Loss: 0.2398
Training Epoch: 96 [1024/50176]	Loss: 0.2933
Training Epoch: 96 [1280/50176]	Loss: 0.3004
Training Epoch: 96 [1536/50176]	Loss: 0.2299
Training Epoch: 96 [1792/50176]	Loss: 0.2596
Training Epoch: 96 [2048/50176]	Loss: 0.3842
Training Epoch: 96 [2304/50176]	Loss: 0.3363
Training Epoch: 96 [2560/50176]	Loss: 0.2147
Training Epoch: 96 [2816/50176]	Loss: 0.1675
Training Epoch: 96 [3072/50176]	Loss: 0.2500
Training Epoch: 96 [3328/50176]	Loss: 0.2366
Training Epoch: 96 [3584/50176]	Loss: 0.3648
Training Epoch: 96 [3840/50176]	Loss: 0.2071
Training Epoch: 96 [4096/50176]	Loss: 0.2638
Training Epoch: 96 [4352/50176]	Loss: 0.4080
Training Epoch: 96 [4608/50176]	Loss: 0.3324
Training Epoch: 96 [4864/50176]	Loss: 0.2413
Training Epoch: 96 [5120/50176]	Loss: 0.1699
Training Epoch: 96 [5376/50176]	Loss: 0.2291
Training Epoch: 96 [5632/50176]	Loss: 0.3142
Training Epoch: 96 [5888/50176]	Loss: 0.2671
Training Epoch: 96 [6144/50176]	Loss: 0.3169
Training Epoch: 96 [6400/50176]	Loss: 0.3554
Training Epoch: 96 [6656/50176]	Loss: 0.2809
Training Epoch: 96 [6912/50176]	Loss: 0.2559
Training Epoch: 96 [7168/50176]	Loss: 0.2527
Training Epoch: 96 [7424/50176]	Loss: 0.2573
Training Epoch: 96 [7680/50176]	Loss: 0.2154
Training Epoch: 96 [7936/50176]	Loss: 0.2350
Training Epoch: 96 [8192/50176]	Loss: 0.2449
Training Epoch: 96 [8448/50176]	Loss: 0.2469
Training Epoch: 96 [8704/50176]	Loss: 0.2509
Training Epoch: 96 [8960/50176]	Loss: 0.2694
Training Epoch: 96 [9216/50176]	Loss: 0.4039
Training Epoch: 96 [9472/50176]	Loss: 0.3378
Training Epoch: 96 [9728/50176]	Loss: 0.2639
Training Epoch: 96 [9984/50176]	Loss: 0.2433
Training Epoch: 96 [10240/50176]	Loss: 0.3027
Training Epoch: 96 [10496/50176]	Loss: 0.4033
Training Epoch: 96 [10752/50176]	Loss: 0.2679
Training Epoch: 96 [11008/50176]	Loss: 0.3323
Training Epoch: 96 [11264/50176]	Loss: 0.3318
Training Epoch: 96 [11520/50176]	Loss: 0.3100
Training Epoch: 96 [11776/50176]	Loss: 0.1974
Training Epoch: 96 [12032/50176]	Loss: 0.1699
Training Epoch: 96 [12288/50176]	Loss: 0.2023
Training Epoch: 96 [12544/50176]	Loss: 0.2937
Training Epoch: 96 [12800/50176]	Loss: 0.3384
Training Epoch: 96 [13056/50176]	Loss: 0.2563
Training Epoch: 96 [13312/50176]	Loss: 0.2799
Training Epoch: 96 [13568/50176]	Loss: 0.3559
Training Epoch: 96 [13824/50176]	Loss: 0.2710
Training Epoch: 96 [14080/50176]	Loss: 0.2899
Training Epoch: 96 [14336/50176]	Loss: 0.2732
Training Epoch: 96 [14592/50176]	Loss: 0.2525
Training Epoch: 96 [14848/50176]	Loss: 0.2414
Training Epoch: 96 [15104/50176]	Loss: 0.2515
Training Epoch: 96 [15360/50176]	Loss: 0.2901
Training Epoch: 96 [15616/50176]	Loss: 0.2243
Training Epoch: 96 [15872/50176]	Loss: 0.2119
Training Epoch: 96 [16128/50176]	Loss: 0.2939
Training Epoch: 96 [16384/50176]	Loss: 0.2701
Training Epoch: 96 [16640/50176]	Loss: 0.2755
Training Epoch: 96 [16896/50176]	Loss: 0.2886
Training Epoch: 96 [17152/50176]	Loss: 0.2430
Training Epoch: 96 [17408/50176]	Loss: 0.2805
Training Epoch: 96 [17664/50176]	Loss: 0.3033
Training Epoch: 96 [17920/50176]	Loss: 0.3512
Training Epoch: 96 [18176/50176]	Loss: 0.3438
Training Epoch: 96 [18432/50176]	Loss: 0.2807
Training Epoch: 96 [18688/50176]	Loss: 0.2831
Training Epoch: 96 [18944/50176]	Loss: 0.2110
Training Epoch: 96 [19200/50176]	Loss: 0.2619
Training Epoch: 96 [19456/50176]	Loss: 0.2959
Training Epoch: 96 [19712/50176]	Loss: 0.3002
Training Epoch: 96 [19968/50176]	Loss: 0.2555
Training Epoch: 96 [20224/50176]	Loss: 0.1779
Training Epoch: 96 [20480/50176]	Loss: 0.2954
Training Epoch: 96 [20736/50176]	Loss: 0.2376
Training Epoch: 96 [20992/50176]	Loss: 0.2885
Training Epoch: 96 [21248/50176]	Loss: 0.2558
Training Epoch: 96 [21504/50176]	Loss: 0.2495
Training Epoch: 96 [21760/50176]	Loss: 0.2601
Training Epoch: 96 [22016/50176]	Loss: 0.2831
Training Epoch: 96 [22272/50176]	Loss: 0.2723
Training Epoch: 96 [22528/50176]	Loss: 0.2490
Training Epoch: 96 [22784/50176]	Loss: 0.2536
Training Epoch: 96 [23040/50176]	Loss: 0.2573
Training Epoch: 96 [23296/50176]	Loss: 0.2964
Training Epoch: 96 [23552/50176]	Loss: 0.2746
Training Epoch: 96 [23808/50176]	Loss: 0.2337
Training Epoch: 96 [24064/50176]	Loss: 0.2742
Training Epoch: 96 [24320/50176]	Loss: 0.3062
Training Epoch: 96 [24576/50176]	Loss: 0.3269
Training Epoch: 96 [24832/50176]	Loss: 0.3392
Training Epoch: 96 [25088/50176]	Loss: 0.2421
Training Epoch: 96 [25344/50176]	Loss: 0.2358
Training Epoch: 96 [25600/50176]	Loss: 0.2970
Training Epoch: 96 [25856/50176]	Loss: 0.2448
Training Epoch: 96 [26112/50176]	Loss: 0.2771
Training Epoch: 96 [26368/50176]	Loss: 0.3363
Training Epoch: 96 [26624/50176]	Loss: 0.3147
Training Epoch: 96 [26880/50176]	Loss: 0.2269
Training Epoch: 96 [27136/50176]	Loss: 0.3187
Training Epoch: 96 [27392/50176]	Loss: 0.3078
Training Epoch: 96 [27648/50176]	Loss: 0.2682
Training Epoch: 96 [27904/50176]	Loss: 0.2518
Training Epoch: 96 [28160/50176]	Loss: 0.3009
Training Epoch: 96 [28416/50176]	Loss: 0.2445
Training Epoch: 96 [28672/50176]	Loss: 0.2899
Training Epoch: 96 [28928/50176]	Loss: 0.2887
Training Epoch: 96 [29184/50176]	Loss: 0.2756
Training Epoch: 96 [29440/50176]	Loss: 0.2444
Training Epoch: 96 [29696/50176]	Loss: 0.2271
Training Epoch: 96 [29952/50176]	Loss: 0.2545
Training Epoch: 96 [30208/50176]	Loss: 0.2802
Training Epoch: 96 [30464/50176]	Loss: 0.2521
Training Epoch: 96 [30720/50176]	Loss: 0.2459
Training Epoch: 96 [30976/50176]	Loss: 0.2774
Training Epoch: 96 [31232/50176]	Loss: 0.3104
Training Epoch: 96 [31488/50176]	Loss: 0.1780
Training Epoch: 96 [31744/50176]	Loss: 0.1874
Training Epoch: 96 [32000/50176]	Loss: 0.3205
Training Epoch: 96 [32256/50176]	Loss: 0.2114
Training Epoch: 96 [32512/50176]	Loss: 0.3079
Training Epoch: 96 [32768/50176]	Loss: 0.3266
Training Epoch: 96 [33024/50176]	Loss: 0.3110
Training Epoch: 96 [33280/50176]	Loss: 0.3041
Training Epoch: 96 [33536/50176]	Loss: 0.2925
Training Epoch: 96 [33792/50176]	Loss: 0.2764
Training Epoch: 96 [34048/50176]	Loss: 0.1948
Training Epoch: 96 [34304/50176]	Loss: 0.2323
Training Epoch: 96 [34560/50176]	Loss: 0.3352
Training Epoch: 96 [34816/50176]	Loss: 0.3001
Training Epoch: 96 [35072/50176]	Loss: 0.4082
Training Epoch: 96 [35328/50176]	Loss: 0.2520
Training Epoch: 96 [35584/50176]	Loss: 0.3102
Training Epoch: 96 [35840/50176]	Loss: 0.2918
Training Epoch: 96 [36096/50176]	Loss: 0.2876
Training Epoch: 96 [36352/50176]	Loss: 0.2491
Training Epoch: 96 [36608/50176]	Loss: 0.3099
Training Epoch: 96 [36864/50176]	Loss: 0.2644
Training Epoch: 96 [37120/50176]	Loss: 0.3089
Training Epoch: 96 [37376/50176]	Loss: 0.2699
Training Epoch: 96 [37632/50176]	Loss: 0.1795
Training Epoch: 96 [37888/50176]	Loss: 0.2625
Training Epoch: 96 [38144/50176]	Loss: 0.2908
Training Epoch: 96 [38400/50176]	Loss: 0.3119
Training Epoch: 96 [38656/50176]	Loss: 0.3010
Training Epoch: 96 [38912/50176]	Loss: 0.2784
Training Epoch: 96 [39168/50176]	Loss: 0.2309
Training Epoch: 96 [39424/50176]	Loss: 0.2859
Training Epoch: 96 [39680/50176]	Loss: 0.2197
Training Epoch: 96 [39936/50176]	Loss: 0.2557
Training Epoch: 96 [40192/50176]	Loss: 0.2821
Training Epoch: 96 [40448/50176]	Loss: 0.3005
Training Epoch: 96 [40704/50176]	Loss: 0.2956
Training Epoch: 96 [40960/50176]	Loss: 0.2503
Training Epoch: 96 [41216/50176]	Loss: 0.2546
Training Epoch: 96 [41472/50176]	Loss: 0.3216
Training Epoch: 96 [41728/50176]	Loss: 0.3249
Training Epoch: 96 [41984/50176]	Loss: 0.2418
Training Epoch: 96 [42240/50176]	Loss: 0.2629
Training Epoch: 96 [42496/50176]	Loss: 0.3607
Training Epoch: 96 [42752/50176]	Loss: 0.2169
Training Epoch: 96 [43008/50176]	Loss: 0.3028
Training Epoch: 96 [43264/50176]	Loss: 0.2674
Training Epoch: 96 [43520/50176]	Loss: 0.2603
Training Epoch: 96 [43776/50176]	Loss: 0.3361
Training Epoch: 96 [44032/50176]	Loss: 0.2481
Training Epoch: 96 [44288/50176]	Loss: 0.2462
Training Epoch: 96 [44544/50176]	Loss: 0.2557
Training Epoch: 96 [44800/50176]	Loss: 0.3138
Training Epoch: 96 [45056/50176]	Loss: 0.3367
Training Epoch: 96 [45312/50176]	Loss: 0.3348
Training Epoch: 96 [45568/50176]	Loss: 0.3149
Training Epoch: 96 [45824/50176]	Loss: 0.3046
Training Epoch: 96 [46080/50176]	Loss: 0.2235
Training Epoch: 96 [46336/50176]	Loss: 0.2875
Training Epoch: 96 [46592/50176]	Loss: 0.2911
Training Epoch: 96 [46848/50176]	Loss: 0.2972
Training Epoch: 96 [47104/50176]	Loss: 0.2037
Training Epoch: 96 [47360/50176]	Loss: 0.2385
Training Epoch: 96 [47616/50176]	Loss: 0.2394
Training Epoch: 96 [47872/50176]	Loss: 0.3048
Training Epoch: 96 [48128/50176]	Loss: 0.3599
Training Epoch: 96 [48384/50176]	Loss: 0.2547
Training Epoch: 96 [48640/50176]	Loss: 0.2338
Training Epoch: 96 [48896/50176]	Loss: 0.2632
Training Epoch: 96 [49152/50176]	Loss: 0.2581
Training Epoch: 96 [49408/50176]	Loss: 0.2879
Training Epoch: 96 [49664/50176]	Loss: 0.2885
Training Epoch: 96 [49920/50176]	Loss: 0.3109
Training Epoch: 96 [50176/50176]	Loss: 0.4098
Validation Epoch: 96, Average loss: 0.0111, Accuracy: 0.5734
Training Epoch: 97 [256/50176]	Loss: 0.2317
Training Epoch: 97 [512/50176]	Loss: 0.2444
Training Epoch: 97 [768/50176]	Loss: 0.2382
Training Epoch: 97 [1024/50176]	Loss: 0.3124
Training Epoch: 97 [1280/50176]	Loss: 0.2308
Training Epoch: 97 [1536/50176]	Loss: 0.2024
Training Epoch: 97 [1792/50176]	Loss: 0.2317
Training Epoch: 97 [2048/50176]	Loss: 0.2570
Training Epoch: 97 [2304/50176]	Loss: 0.2899
Training Epoch: 97 [2560/50176]	Loss: 0.3277
Training Epoch: 97 [2816/50176]	Loss: 0.2672
Training Epoch: 97 [3072/50176]	Loss: 0.1982
Training Epoch: 97 [3328/50176]	Loss: 0.2828
Training Epoch: 97 [3584/50176]	Loss: 0.2582
Training Epoch: 97 [3840/50176]	Loss: 0.3051
Training Epoch: 97 [4096/50176]	Loss: 0.2016
Training Epoch: 97 [4352/50176]	Loss: 0.2051
Training Epoch: 97 [4608/50176]	Loss: 0.2561
Training Epoch: 97 [4864/50176]	Loss: 0.2250
Training Epoch: 97 [5120/50176]	Loss: 0.2498
Training Epoch: 97 [5376/50176]	Loss: 0.2270
Training Epoch: 97 [5632/50176]	Loss: 0.2815
Training Epoch: 97 [5888/50176]	Loss: 0.1897
Training Epoch: 97 [6144/50176]	Loss: 0.3064
Training Epoch: 97 [6400/50176]	Loss: 0.3339
Training Epoch: 97 [6656/50176]	Loss: 0.2719
Training Epoch: 97 [6912/50176]	Loss: 0.2503
Training Epoch: 97 [7168/50176]	Loss: 0.2727
Training Epoch: 97 [7424/50176]	Loss: 0.2399
Training Epoch: 97 [7680/50176]	Loss: 0.2967
Training Epoch: 97 [7936/50176]	Loss: 0.2254
Training Epoch: 97 [8192/50176]	Loss: 0.2847
Training Epoch: 97 [8448/50176]	Loss: 0.2932
Training Epoch: 97 [8704/50176]	Loss: 0.2212
Training Epoch: 97 [8960/50176]	Loss: 0.1752
Training Epoch: 97 [9216/50176]	Loss: 0.2849
Training Epoch: 97 [9472/50176]	Loss: 0.2860
Training Epoch: 97 [9728/50176]	Loss: 0.3076
Training Epoch: 97 [9984/50176]	Loss: 0.2405
Training Epoch: 97 [10240/50176]	Loss: 0.2226
Training Epoch: 97 [10496/50176]	Loss: 0.1888
Training Epoch: 97 [10752/50176]	Loss: 0.2807
Training Epoch: 97 [11008/50176]	Loss: 0.2215
Training Epoch: 97 [11264/50176]	Loss: 0.2208
Training Epoch: 97 [11520/50176]	Loss: 0.2224
Training Epoch: 97 [11776/50176]	Loss: 0.2365
Training Epoch: 97 [12032/50176]	Loss: 0.2142
Training Epoch: 97 [12288/50176]	Loss: 0.2970
Training Epoch: 97 [12544/50176]	Loss: 0.2113
Training Epoch: 97 [12800/50176]	Loss: 0.2457
Training Epoch: 97 [13056/50176]	Loss: 0.2723
Training Epoch: 97 [13312/50176]	Loss: 0.2852
Training Epoch: 97 [13568/50176]	Loss: 0.2548
Training Epoch: 97 [13824/50176]	Loss: 0.2107
Training Epoch: 97 [14080/50176]	Loss: 0.2655
Training Epoch: 97 [14336/50176]	Loss: 0.2090
Training Epoch: 97 [14592/50176]	Loss: 0.2696
Training Epoch: 97 [14848/50176]	Loss: 0.3366
Training Epoch: 97 [15104/50176]	Loss: 0.2730
Training Epoch: 97 [15360/50176]	Loss: 0.2057
Training Epoch: 97 [15616/50176]	Loss: 0.2629
Training Epoch: 97 [15872/50176]	Loss: 0.2463
Training Epoch: 97 [16128/50176]	Loss: 0.3119
Training Epoch: 97 [16384/50176]	Loss: 0.2610
Training Epoch: 97 [16640/50176]	Loss: 0.3456
Training Epoch: 97 [16896/50176]	Loss: 0.1762
Training Epoch: 97 [17152/50176]	Loss: 0.2215
Training Epoch: 97 [17408/50176]	Loss: 0.3520
Training Epoch: 97 [17664/50176]	Loss: 0.2315
Training Epoch: 97 [17920/50176]	Loss: 0.1905
Training Epoch: 97 [18176/50176]	Loss: 0.3123
Training Epoch: 97 [18432/50176]	Loss: 0.2283
Training Epoch: 97 [18688/50176]	Loss: 0.3321
Training Epoch: 97 [18944/50176]	Loss: 0.2744
Training Epoch: 97 [19200/50176]	Loss: 0.2566
Training Epoch: 97 [19456/50176]	Loss: 0.2470
Training Epoch: 97 [19712/50176]	Loss: 0.2182
Training Epoch: 97 [19968/50176]	Loss: 0.2806
Training Epoch: 97 [20224/50176]	Loss: 0.2433
Training Epoch: 97 [20480/50176]	Loss: 0.1964
Training Epoch: 97 [20736/50176]	Loss: 0.2188
Training Epoch: 97 [20992/50176]	Loss: 0.2356
Training Epoch: 97 [21248/50176]	Loss: 0.2655
Training Epoch: 97 [21504/50176]	Loss: 0.2045
Training Epoch: 97 [21760/50176]	Loss: 0.2559
Training Epoch: 97 [22016/50176]	Loss: 0.2426
Training Epoch: 97 [22272/50176]	Loss: 0.2846
Training Epoch: 97 [22528/50176]	Loss: 0.3204
Training Epoch: 97 [22784/50176]	Loss: 0.2217
Training Epoch: 97 [23040/50176]	Loss: 0.1823
Training Epoch: 97 [23296/50176]	Loss: 0.3080
Training Epoch: 97 [23552/50176]	Loss: 0.3064
Training Epoch: 97 [23808/50176]	Loss: 0.2747
Training Epoch: 97 [24064/50176]	Loss: 0.2351
Training Epoch: 97 [24320/50176]	Loss: 0.3168
Training Epoch: 97 [24576/50176]	Loss: 0.2469
Training Epoch: 97 [24832/50176]	Loss: 0.1859
Training Epoch: 97 [25088/50176]	Loss: 0.2919
Training Epoch: 97 [25344/50176]	Loss: 0.2378
Training Epoch: 97 [25600/50176]	Loss: 0.3691
Training Epoch: 97 [25856/50176]	Loss: 0.2036
Training Epoch: 97 [26112/50176]	Loss: 0.2487
Training Epoch: 97 [26368/50176]	Loss: 0.4040
Training Epoch: 97 [26624/50176]	Loss: 0.2902
Training Epoch: 97 [26880/50176]	Loss: 0.2684
Training Epoch: 97 [27136/50176]	Loss: 0.2189
Training Epoch: 97 [27392/50176]	Loss: 0.3420
Training Epoch: 97 [27648/50176]	Loss: 0.2491
Training Epoch: 97 [27904/50176]	Loss: 0.2787
Training Epoch: 97 [28160/50176]	Loss: 0.2391
Training Epoch: 97 [28416/50176]	Loss: 0.2497
Training Epoch: 97 [28672/50176]	Loss: 0.2981
Training Epoch: 97 [28928/50176]	Loss: 0.1939
Training Epoch: 97 [29184/50176]	Loss: 0.2366
Training Epoch: 97 [29440/50176]	Loss: 0.2568
Training Epoch: 97 [29696/50176]	Loss: 0.2425
Training Epoch: 97 [29952/50176]	Loss: 0.2490
Training Epoch: 97 [30208/50176]	Loss: 0.2363
Training Epoch: 97 [30464/50176]	Loss: 0.2514
Training Epoch: 97 [30720/50176]	Loss: 0.2519
Training Epoch: 97 [30976/50176]	Loss: 0.2809
Training Epoch: 97 [31232/50176]	Loss: 0.2505
Training Epoch: 97 [31488/50176]	Loss: 0.2294
Training Epoch: 97 [31744/50176]	Loss: 0.3283
Training Epoch: 97 [32000/50176]	Loss: 0.3247
Training Epoch: 97 [32256/50176]	Loss: 0.2814
Training Epoch: 97 [32512/50176]	Loss: 0.2954
Training Epoch: 97 [32768/50176]	Loss: 0.2948
Training Epoch: 97 [33024/50176]	Loss: 0.3049
Training Epoch: 97 [33280/50176]	Loss: 0.2731
Training Epoch: 97 [33536/50176]	Loss: 0.3084
Training Epoch: 97 [33792/50176]	Loss: 0.1670
Training Epoch: 97 [34048/50176]	Loss: 0.2711
Training Epoch: 97 [34304/50176]	Loss: 0.2420
Training Epoch: 97 [34560/50176]	Loss: 0.3614
Training Epoch: 97 [34816/50176]	Loss: 0.3448
Training Epoch: 97 [35072/50176]	Loss: 0.3460
Training Epoch: 97 [35328/50176]	Loss: 0.2528
Training Epoch: 97 [35584/50176]	Loss: 0.2199
Training Epoch: 97 [35840/50176]	Loss: 0.2451
Training Epoch: 97 [36096/50176]	Loss: 0.2615
Training Epoch: 97 [36352/50176]	Loss: 0.3221
Training Epoch: 97 [36608/50176]	Loss: 0.3025
Training Epoch: 97 [36864/50176]	Loss: 0.2324
Training Epoch: 97 [37120/50176]	Loss: 0.2429
Training Epoch: 97 [37376/50176]	Loss: 0.3116
Training Epoch: 97 [37632/50176]	Loss: 0.2920
Training Epoch: 97 [37888/50176]	Loss: 0.2834
Training Epoch: 97 [38144/50176]	Loss: 0.2830
Training Epoch: 97 [38400/50176]	Loss: 0.2850
Training Epoch: 97 [38656/50176]	Loss: 0.2809
Training Epoch: 97 [38912/50176]	Loss: 0.2257
Training Epoch: 97 [39168/50176]	Loss: 0.2897
Training Epoch: 97 [39424/50176]	Loss: 0.2727
Training Epoch: 97 [39680/50176]	Loss: 0.3424
Training Epoch: 97 [39936/50176]	Loss: 0.3892
Training Epoch: 97 [40192/50176]	Loss: 0.4193
Training Epoch: 97 [40448/50176]	Loss: 0.2481
Training Epoch: 97 [40704/50176]	Loss: 0.2666
Training Epoch: 97 [40960/50176]	Loss: 0.2890
Training Epoch: 97 [41216/50176]	Loss: 0.2103
Training Epoch: 97 [41472/50176]	Loss: 0.4021
Training Epoch: 97 [41728/50176]	Loss: 0.2626
Training Epoch: 97 [41984/50176]	Loss: 0.2780
Training Epoch: 97 [42240/50176]	Loss: 0.2540
Training Epoch: 97 [42496/50176]	Loss: 0.1842
Training Epoch: 97 [42752/50176]	Loss: 0.2815
Training Epoch: 97 [43008/50176]	Loss: 0.2593
Training Epoch: 97 [43264/50176]	Loss: 0.3172
Training Epoch: 97 [43520/50176]	Loss: 0.4068
Training Epoch: 97 [43776/50176]	Loss: 0.3191
Training Epoch: 97 [44032/50176]	Loss: 0.2215
Training Epoch: 97 [44288/50176]	Loss: 0.2562
Training Epoch: 97 [44544/50176]	Loss: 0.2926
Training Epoch: 97 [44800/50176]	Loss: 0.2732
Training Epoch: 97 [45056/50176]	Loss: 0.3243
Training Epoch: 97 [45312/50176]	Loss: 0.2316
Training Epoch: 97 [45568/50176]	Loss: 0.3450
Training Epoch: 97 [45824/50176]	Loss: 0.3216
Training Epoch: 97 [46080/50176]	Loss: 0.3241
Training Epoch: 97 [46336/50176]	Loss: 0.3841
Training Epoch: 97 [46592/50176]	Loss: 0.3136
Training Epoch: 97 [46848/50176]	Loss: 0.3453
Training Epoch: 97 [47104/50176]	Loss: 0.2341
Training Epoch: 97 [47360/50176]	Loss: 0.2458
Training Epoch: 97 [47616/50176]	Loss: 0.2575
Training Epoch: 97 [47872/50176]	Loss: 0.3308
Training Epoch: 97 [48128/50176]	Loss: 0.2460
Training Epoch: 97 [48384/50176]	Loss: 0.2916
Training Epoch: 97 [48640/50176]	Loss: 0.2675
Training Epoch: 97 [48896/50176]	Loss: 0.2269
Training Epoch: 97 [49152/50176]	Loss: 0.3378
Training Epoch: 97 [49408/50176]	Loss: 0.3183
Training Epoch: 97 [49664/50176]	Loss: 0.3262
Training Epoch: 97 [49920/50176]	Loss: 0.3306
Training Epoch: 97 [50176/50176]	Loss: 0.5345
Validation Epoch: 97, Average loss: 0.0105, Accuracy: 0.5885
Training Epoch: 98 [256/50176]	Loss: 0.2334
Training Epoch: 98 [512/50176]	Loss: 0.1474
Training Epoch: 98 [768/50176]	Loss: 0.3142
Training Epoch: 98 [1024/50176]	Loss: 0.2754
Training Epoch: 98 [1280/50176]	Loss: 0.2384
Training Epoch: 98 [1536/50176]	Loss: 0.2621
Training Epoch: 98 [1792/50176]	Loss: 0.2904
Training Epoch: 98 [2048/50176]	Loss: 0.2492
Training Epoch: 98 [2304/50176]	Loss: 0.2386
Training Epoch: 98 [2560/50176]	Loss: 0.2198
Training Epoch: 98 [2816/50176]	Loss: 0.2732
Training Epoch: 98 [3072/50176]	Loss: 0.2521
Training Epoch: 98 [3328/50176]	Loss: 0.2491
Training Epoch: 98 [3584/50176]	Loss: 0.2641
Training Epoch: 98 [3840/50176]	Loss: 0.1650
Training Epoch: 98 [4096/50176]	Loss: 0.3011
Training Epoch: 98 [4352/50176]	Loss: 0.2913
Training Epoch: 98 [4608/50176]	Loss: 0.2554
Training Epoch: 98 [4864/50176]	Loss: 0.3237
Training Epoch: 98 [5120/50176]	Loss: 0.2444
Training Epoch: 98 [5376/50176]	Loss: 0.2019
Training Epoch: 98 [5632/50176]	Loss: 0.2161
Training Epoch: 98 [5888/50176]	Loss: 0.2748
Training Epoch: 98 [6144/50176]	Loss: 0.2768
Training Epoch: 98 [6400/50176]	Loss: 0.1960
Training Epoch: 98 [6656/50176]	Loss: 0.2852
Training Epoch: 98 [6912/50176]	Loss: 0.1857
Training Epoch: 98 [7168/50176]	Loss: 0.3088
Training Epoch: 98 [7424/50176]	Loss: 0.2565
Training Epoch: 98 [7680/50176]	Loss: 0.2568
Training Epoch: 98 [7936/50176]	Loss: 0.2101
Training Epoch: 98 [8192/50176]	Loss: 0.2143
Training Epoch: 98 [8448/50176]	Loss: 0.2414
Training Epoch: 98 [8704/50176]	Loss: 0.3027
Training Epoch: 98 [8960/50176]	Loss: 0.2311
Training Epoch: 98 [9216/50176]	Loss: 0.2071
Training Epoch: 98 [9472/50176]	Loss: 0.2298
Training Epoch: 98 [9728/50176]	Loss: 0.2994
Training Epoch: 98 [9984/50176]	Loss: 0.3424
Training Epoch: 98 [10240/50176]	Loss: 0.2540
Training Epoch: 98 [10496/50176]	Loss: 0.1889
Training Epoch: 98 [10752/50176]	Loss: 0.3057
Training Epoch: 98 [11008/50176]	Loss: 0.2354
Training Epoch: 98 [11264/50176]	Loss: 0.2779
Training Epoch: 98 [11520/50176]	Loss: 0.2749
Training Epoch: 98 [11776/50176]	Loss: 0.1992
Training Epoch: 98 [12032/50176]	Loss: 0.1540
Training Epoch: 98 [12288/50176]	Loss: 0.3347
Training Epoch: 98 [12544/50176]	Loss: 0.3234
Training Epoch: 98 [12800/50176]	Loss: 0.3054
Training Epoch: 98 [13056/50176]	Loss: 0.2496
Training Epoch: 98 [13312/50176]	Loss: 0.3412
Training Epoch: 98 [13568/50176]	Loss: 0.1790
Training Epoch: 98 [13824/50176]	Loss: 0.3167
Training Epoch: 98 [14080/50176]	Loss: 0.4043
Training Epoch: 98 [14336/50176]	Loss: 0.2857
Training Epoch: 98 [14592/50176]	Loss: 0.3360
Training Epoch: 98 [14848/50176]	Loss: 0.3043
Training Epoch: 98 [15104/50176]	Loss: 0.3093
Training Epoch: 98 [15360/50176]	Loss: 0.2352
Training Epoch: 98 [15616/50176]	Loss: 0.1838
Training Epoch: 98 [15872/50176]	Loss: 0.2091
Training Epoch: 98 [16128/50176]	Loss: 0.1874
Training Epoch: 98 [16384/50176]	Loss: 0.2452
Training Epoch: 98 [16640/50176]	Loss: 0.2139
Training Epoch: 98 [16896/50176]	Loss: 0.2368
Training Epoch: 98 [17152/50176]	Loss: 0.1924
Training Epoch: 98 [17408/50176]	Loss: 0.3043
Training Epoch: 98 [17664/50176]	Loss: 0.3139
Training Epoch: 98 [17920/50176]	Loss: 0.2445
Training Epoch: 98 [18176/50176]	Loss: 0.2084
Training Epoch: 98 [18432/50176]	Loss: 0.2327
Training Epoch: 98 [18688/50176]	Loss: 0.2670
Training Epoch: 98 [18944/50176]	Loss: 0.3128
Training Epoch: 98 [19200/50176]	Loss: 0.4182
Training Epoch: 98 [19456/50176]	Loss: 0.3117
Training Epoch: 98 [19712/50176]	Loss: 0.2250
Training Epoch: 98 [19968/50176]	Loss: 0.2676
Training Epoch: 98 [20224/50176]	Loss: 0.3081
Training Epoch: 98 [20480/50176]	Loss: 0.1548
Training Epoch: 98 [20736/50176]	Loss: 0.2225
Training Epoch: 98 [20992/50176]	Loss: 0.2372
Training Epoch: 98 [21248/50176]	Loss: 0.1994
Training Epoch: 98 [21504/50176]	Loss: 0.2800
Training Epoch: 98 [21760/50176]	Loss: 0.2945
Training Epoch: 98 [22016/50176]	Loss: 0.2824
Training Epoch: 98 [22272/50176]	Loss: 0.2578
Training Epoch: 98 [22528/50176]	Loss: 0.2295
Training Epoch: 98 [22784/50176]	Loss: 0.2192
Training Epoch: 98 [23040/50176]	Loss: 0.2031
Training Epoch: 98 [23296/50176]	Loss: 0.1889
Training Epoch: 98 [23552/50176]	Loss: 0.3150
Training Epoch: 98 [23808/50176]	Loss: 0.2654
Training Epoch: 98 [24064/50176]	Loss: 0.1777
Training Epoch: 98 [24320/50176]	Loss: 0.2024
Training Epoch: 98 [24576/50176]	Loss: 0.2092
Training Epoch: 98 [24832/50176]	Loss: 0.2900
Training Epoch: 98 [25088/50176]	Loss: 0.2085
Training Epoch: 98 [25344/50176]	Loss: 0.2740
Training Epoch: 98 [25600/50176]	Loss: 0.3062
Training Epoch: 98 [25856/50176]	Loss: 0.1536
Training Epoch: 98 [26112/50176]	Loss: 0.2043
Training Epoch: 98 [26368/50176]	Loss: 0.3131
Training Epoch: 98 [26624/50176]	Loss: 0.3088
Training Epoch: 98 [26880/50176]	Loss: 0.2109
Training Epoch: 98 [27136/50176]	Loss: 0.2913
Training Epoch: 98 [27392/50176]	Loss: 0.2850
Training Epoch: 98 [27648/50176]	Loss: 0.3127
Training Epoch: 98 [27904/50176]	Loss: 0.2216
Training Epoch: 98 [28160/50176]	Loss: 0.3290
Training Epoch: 98 [28416/50176]	Loss: 0.2329
Training Epoch: 98 [28672/50176]	Loss: 0.2929
Training Epoch: 98 [28928/50176]	Loss: 0.2257
Training Epoch: 98 [29184/50176]	Loss: 0.2517
Training Epoch: 98 [29440/50176]	Loss: 0.2542
Training Epoch: 98 [29696/50176]	Loss: 0.2708
Training Epoch: 98 [29952/50176]	Loss: 0.2601
Training Epoch: 98 [30208/50176]	Loss: 0.2627
Training Epoch: 98 [30464/50176]	Loss: 0.3901
Training Epoch: 98 [30720/50176]	Loss: 0.2535
Training Epoch: 98 [30976/50176]	Loss: 0.2846
Training Epoch: 98 [31232/50176]	Loss: 0.2262
Training Epoch: 98 [31488/50176]	Loss: 0.2756
Training Epoch: 98 [31744/50176]	Loss: 0.2769
Training Epoch: 98 [32000/50176]	Loss: 0.2619
Training Epoch: 98 [32256/50176]	Loss: 0.1829
Training Epoch: 98 [32512/50176]	Loss: 0.2338
Training Epoch: 98 [32768/50176]	Loss: 0.2745
Training Epoch: 98 [33024/50176]	Loss: 0.2046
Training Epoch: 98 [33280/50176]	Loss: 0.2750
Training Epoch: 98 [33536/50176]	Loss: 0.2512
Training Epoch: 98 [33792/50176]	Loss: 0.3355
Training Epoch: 98 [34048/50176]	Loss: 0.3243
Training Epoch: 98 [34304/50176]	Loss: 0.2175
Training Epoch: 98 [34560/50176]	Loss: 0.3100
Training Epoch: 98 [34816/50176]	Loss: 0.2296
Training Epoch: 98 [35072/50176]	Loss: 0.2956
Training Epoch: 98 [35328/50176]	Loss: 0.3012
Training Epoch: 98 [35584/50176]	Loss: 0.2015
Training Epoch: 98 [35840/50176]	Loss: 0.3503
Training Epoch: 98 [36096/50176]	Loss: 0.2571
Training Epoch: 98 [36352/50176]	Loss: 0.2898
Training Epoch: 98 [36608/50176]	Loss: 0.1737
Training Epoch: 98 [36864/50176]	Loss: 0.2532
Training Epoch: 98 [37120/50176]	Loss: 0.2251
Training Epoch: 98 [37376/50176]	Loss: 0.2432
Training Epoch: 98 [37632/50176]	Loss: 0.2430
Training Epoch: 98 [37888/50176]	Loss: 0.1712
Training Epoch: 98 [38144/50176]	Loss: 0.2017
Training Epoch: 98 [38400/50176]	Loss: 0.2042
Training Epoch: 98 [38656/50176]	Loss: 0.2027
Training Epoch: 98 [38912/50176]	Loss: 0.2186
Training Epoch: 98 [39168/50176]	Loss: 0.2832
Training Epoch: 98 [39424/50176]	Loss: 0.2627
Training Epoch: 98 [39680/50176]	Loss: 0.1847
Training Epoch: 98 [39936/50176]	Loss: 0.2927
Training Epoch: 98 [40192/50176]	Loss: 0.2389
Training Epoch: 98 [40448/50176]	Loss: 0.3368
Training Epoch: 98 [40704/50176]	Loss: 0.1788
Training Epoch: 98 [40960/50176]	Loss: 0.2222
Training Epoch: 98 [41216/50176]	Loss: 0.3091
Training Epoch: 98 [41472/50176]	Loss: 0.1615
Training Epoch: 98 [41728/50176]	Loss: 0.2990
Training Epoch: 98 [41984/50176]	Loss: 0.2120
Training Epoch: 98 [42240/50176]	Loss: 0.2703
Training Epoch: 98 [42496/50176]	Loss: 0.2713
Training Epoch: 98 [42752/50176]	Loss: 0.2218
Training Epoch: 98 [43008/50176]	Loss: 0.3604
Training Epoch: 98 [43264/50176]	Loss: 0.3065
Training Epoch: 98 [43520/50176]	Loss: 0.3073
Training Epoch: 98 [43776/50176]	Loss: 0.2722
Training Epoch: 98 [44032/50176]	Loss: 0.2791
Training Epoch: 98 [44288/50176]	Loss: 0.2684
Training Epoch: 98 [44544/50176]	Loss: 0.2400
Training Epoch: 98 [44800/50176]	Loss: 0.2452
Training Epoch: 98 [45056/50176]	Loss: 0.2054
Training Epoch: 98 [45312/50176]	Loss: 0.2233
Training Epoch: 98 [45568/50176]	Loss: 0.3560
Training Epoch: 98 [45824/50176]	Loss: 0.4069
Training Epoch: 98 [46080/50176]	Loss: 0.2940
Training Epoch: 98 [46336/50176]	Loss: 0.3478
Training Epoch: 98 [46592/50176]	Loss: 0.2580
Training Epoch: 98 [46848/50176]	Loss: 0.3313
Training Epoch: 98 [47104/50176]	Loss: 0.2334
Training Epoch: 98 [47360/50176]	Loss: 0.3506
Training Epoch: 98 [47616/50176]	Loss: 0.2864
Training Epoch: 98 [47872/50176]	Loss: 0.3481
Training Epoch: 98 [48128/50176]	Loss: 0.2906
Training Epoch: 98 [48384/50176]	Loss: 0.2766
Training Epoch: 98 [48640/50176]	Loss: 0.2512
Training Epoch: 98 [48896/50176]	Loss: 0.3052
Training Epoch: 98 [49152/50176]	Loss: 0.2425
Training Epoch: 98 [49408/50176]	Loss: 0.2116
Training Epoch: 98 [49664/50176]	Loss: 0.2326
Training Epoch: 98 [49920/50176]	Loss: 0.2461
Training Epoch: 98 [50176/50176]	Loss: 0.3231
Validation Epoch: 98, Average loss: 0.0101, Accuracy: 0.5885
Training Epoch: 99 [256/50176]	Loss: 0.1503
Training Epoch: 99 [512/50176]	Loss: 0.1598
Training Epoch: 99 [768/50176]	Loss: 0.1582
Training Epoch: 99 [1024/50176]	Loss: 0.2590
Training Epoch: 99 [1280/50176]	Loss: 0.2529
Training Epoch: 99 [1536/50176]	Loss: 0.4060
Training Epoch: 99 [1792/50176]	Loss: 0.3698
Training Epoch: 99 [2048/50176]	Loss: 0.3551
Training Epoch: 99 [2304/50176]	Loss: 0.3002
Training Epoch: 99 [2560/50176]	Loss: 0.2457
Training Epoch: 99 [2816/50176]	Loss: 0.3479
Training Epoch: 99 [3072/50176]	Loss: 0.2203
Training Epoch: 99 [3328/50176]	Loss: 0.2607
Training Epoch: 99 [3584/50176]	Loss: 0.3470
Training Epoch: 99 [3840/50176]	Loss: 0.2104
Training Epoch: 99 [4096/50176]	Loss: 0.1939
Training Epoch: 99 [4352/50176]	Loss: 0.1889
Training Epoch: 99 [4608/50176]	Loss: 0.2029
Training Epoch: 99 [4864/50176]	Loss: 0.2600
Training Epoch: 99 [5120/50176]	Loss: 0.2435
Training Epoch: 99 [5376/50176]	Loss: 0.2217
Training Epoch: 99 [5632/50176]	Loss: 0.3469
Training Epoch: 99 [5888/50176]	Loss: 0.2732
Training Epoch: 99 [6144/50176]	Loss: 0.2827
Training Epoch: 99 [6400/50176]	Loss: 0.2719
Training Epoch: 99 [6656/50176]	Loss: 0.3444
Training Epoch: 99 [6912/50176]	Loss: 0.2357
Training Epoch: 99 [7168/50176]	Loss: 0.2478
Training Epoch: 99 [7424/50176]	Loss: 0.2805
Training Epoch: 99 [7680/50176]	Loss: 0.3166
Training Epoch: 99 [7936/50176]	Loss: 0.3122
Training Epoch: 99 [8192/50176]	Loss: 0.2260
Training Epoch: 99 [8448/50176]	Loss: 0.2166
Training Epoch: 99 [8704/50176]	Loss: 0.1763
Training Epoch: 99 [8960/50176]	Loss: 0.3034
Training Epoch: 99 [9216/50176]	Loss: 0.3083
Training Epoch: 99 [9472/50176]	Loss: 0.2413
Training Epoch: 99 [9728/50176]	Loss: 0.2617
Training Epoch: 99 [9984/50176]	Loss: 0.2979
Training Epoch: 99 [10240/50176]	Loss: 0.2634
Training Epoch: 99 [10496/50176]	Loss: 0.2229
Training Epoch: 99 [10752/50176]	Loss: 0.2966
Training Epoch: 99 [11008/50176]	Loss: 0.2216
Training Epoch: 99 [11264/50176]	Loss: 0.2369
Training Epoch: 99 [11520/50176]	Loss: 0.2788
Training Epoch: 99 [11776/50176]	Loss: 0.1950
Training Epoch: 99 [12032/50176]	Loss: 0.2481
Training Epoch: 99 [12288/50176]	Loss: 0.2154
Training Epoch: 99 [12544/50176]	Loss: 0.2628
Training Epoch: 99 [12800/50176]	Loss: 0.3317
Training Epoch: 99 [13056/50176]	Loss: 0.3078
Training Epoch: 99 [13312/50176]	Loss: 0.2427
Training Epoch: 99 [13568/50176]	Loss: 0.3080
Training Epoch: 99 [13824/50176]	Loss: 0.3273
Training Epoch: 99 [14080/50176]	Loss: 0.2098
Training Epoch: 99 [14336/50176]	Loss: 0.2885
Training Epoch: 99 [14592/50176]	Loss: 0.3025
Training Epoch: 99 [14848/50176]	Loss: 0.2930
Training Epoch: 99 [15104/50176]	Loss: 0.3285
Training Epoch: 99 [15360/50176]	Loss: 0.2306
Training Epoch: 99 [15616/50176]	Loss: 0.2916
Training Epoch: 99 [15872/50176]	Loss: 0.2507
Training Epoch: 99 [16128/50176]	Loss: 0.2497
Training Epoch: 99 [16384/50176]	Loss: 0.1925
Training Epoch: 99 [16640/50176]	Loss: 0.2746
Training Epoch: 99 [16896/50176]	Loss: 0.2334
Training Epoch: 99 [17152/50176]	Loss: 0.3237
Training Epoch: 99 [17408/50176]	Loss: 0.3268
Training Epoch: 99 [17664/50176]	Loss: 0.2672
Training Epoch: 99 [17920/50176]	Loss: 0.2184
Training Epoch: 99 [18176/50176]	Loss: 0.3177
Training Epoch: 99 [18432/50176]	Loss: 0.3403
Training Epoch: 99 [18688/50176]	Loss: 0.2680
Training Epoch: 99 [18944/50176]	Loss: 0.1964
Training Epoch: 99 [19200/50176]	Loss: 0.3341
Training Epoch: 99 [19456/50176]	Loss: 0.3039
Training Epoch: 99 [19712/50176]	Loss: 0.3054
Training Epoch: 99 [19968/50176]	Loss: 0.2842
Training Epoch: 99 [20224/50176]	Loss: 0.2362
Training Epoch: 99 [20480/50176]	Loss: 0.2755
Training Epoch: 99 [20736/50176]	Loss: 0.3034
Training Epoch: 99 [20992/50176]	Loss: 0.2364
Training Epoch: 99 [21248/50176]	Loss: 0.3041
Training Epoch: 99 [21504/50176]	Loss: 0.2248
Training Epoch: 99 [21760/50176]	Loss: 0.2707
Training Epoch: 99 [22016/50176]	Loss: 0.2392
Training Epoch: 99 [22272/50176]	Loss: 0.2602
Training Epoch: 99 [22528/50176]	Loss: 0.2869
Training Epoch: 99 [22784/50176]	Loss: 0.2714
Training Epoch: 99 [23040/50176]	Loss: 0.3108
Training Epoch: 99 [23296/50176]	Loss: 0.2637
Training Epoch: 99 [23552/50176]	Loss: 0.3398
Training Epoch: 99 [23808/50176]	Loss: 0.2205
Training Epoch: 99 [24064/50176]	Loss: 0.2563
Training Epoch: 99 [24320/50176]	Loss: 0.3348
Training Epoch: 99 [24576/50176]	Loss: 0.3175
Training Epoch: 99 [24832/50176]	Loss: 0.2016
Training Epoch: 99 [25088/50176]	Loss: 0.2344
Training Epoch: 99 [25344/50176]	Loss: 0.2986
Training Epoch: 99 [25600/50176]	Loss: 0.3584
Training Epoch: 99 [25856/50176]	Loss: 0.3286
Training Epoch: 99 [26112/50176]	Loss: 0.2107
Training Epoch: 99 [26368/50176]	Loss: 0.2210
Training Epoch: 99 [26624/50176]	Loss: 0.3117
Training Epoch: 99 [26880/50176]	Loss: 0.2280
Training Epoch: 99 [27136/50176]	Loss: 0.3240
Training Epoch: 99 [27392/50176]	Loss: 0.1863
Training Epoch: 99 [27648/50176]	Loss: 0.2919
Training Epoch: 99 [27904/50176]	Loss: 0.3024
Training Epoch: 99 [28160/50176]	Loss: 0.2871
Training Epoch: 99 [28416/50176]	Loss: 0.2676
Training Epoch: 99 [28672/50176]	Loss: 0.3039
Training Epoch: 99 [28928/50176]	Loss: 0.2332
Training Epoch: 99 [29184/50176]	Loss: 0.2234
Training Epoch: 99 [29440/50176]	Loss: 0.3199
Training Epoch: 99 [29696/50176]	Loss: 0.2707
Training Epoch: 99 [29952/50176]	Loss: 0.3548
Training Epoch: 99 [30208/50176]	Loss: 0.2027
Training Epoch: 99 [30464/50176]	Loss: 0.2599
Training Epoch: 99 [30720/50176]	Loss: 0.2896
Training Epoch: 99 [30976/50176]	Loss: 0.3018
Training Epoch: 99 [31232/50176]	Loss: 0.2458
Training Epoch: 99 [31488/50176]	Loss: 0.2463
Training Epoch: 99 [31744/50176]	Loss: 0.3150
Training Epoch: 99 [32000/50176]	Loss: 0.2191
Training Epoch: 99 [32256/50176]	Loss: 0.2418
Training Epoch: 99 [32512/50176]	Loss: 0.2639
Training Epoch: 99 [32768/50176]	Loss: 0.2691
Training Epoch: 99 [33024/50176]	Loss: 0.3604
Training Epoch: 99 [33280/50176]	Loss: 0.2465
Training Epoch: 99 [33536/50176]	Loss: 0.2350
Training Epoch: 99 [33792/50176]	Loss: 0.2648
Training Epoch: 99 [34048/50176]	Loss: 0.2742
Training Epoch: 99 [34304/50176]	Loss: 0.2560
Training Epoch: 99 [34560/50176]	Loss: 0.3307
Training Epoch: 99 [34816/50176]	Loss: 0.2565
Training Epoch: 99 [35072/50176]	Loss: 0.3097
Training Epoch: 99 [35328/50176]	Loss: 0.2985
Training Epoch: 99 [35584/50176]	Loss: 0.2824
Training Epoch: 99 [35840/50176]	Loss: 0.3599
Training Epoch: 99 [36096/50176]	Loss: 0.1631
Training Epoch: 99 [36352/50176]	Loss: 0.3154
Training Epoch: 99 [36608/50176]	Loss: 0.3593
Training Epoch: 99 [36864/50176]	Loss: 0.2345
Training Epoch: 99 [37120/50176]	Loss: 0.3161
Training Epoch: 99 [37376/50176]	Loss: 0.2638
Training Epoch: 99 [37632/50176]	Loss: 0.3211
Training Epoch: 99 [37888/50176]	Loss: 0.2903
Training Epoch: 99 [38144/50176]	Loss: 0.2846
Training Epoch: 99 [38400/50176]	Loss: 0.2407
Training Epoch: 99 [38656/50176]	Loss: 0.1964
Training Epoch: 99 [38912/50176]	Loss: 0.2651
Training Epoch: 99 [39168/50176]	Loss: 0.3698
Training Epoch: 99 [39424/50176]	Loss: 0.2556
Training Epoch: 99 [39680/50176]	Loss: 0.2723
Training Epoch: 99 [39936/50176]	Loss: 0.3539
Training Epoch: 99 [40192/50176]	Loss: 0.3142
Training Epoch: 99 [40448/50176]	Loss: 0.2339
Training Epoch: 99 [40704/50176]	Loss: 0.3617
Training Epoch: 99 [40960/50176]	Loss: 0.3808
Training Epoch: 99 [41216/50176]	Loss: 0.3759
Training Epoch: 99 [41472/50176]	Loss: 0.2561
Training Epoch: 99 [41728/50176]	Loss: 0.2294
Training Epoch: 99 [41984/50176]	Loss: 0.2696
Training Epoch: 99 [42240/50176]	Loss: 0.2166
Training Epoch: 99 [42496/50176]	Loss: 0.4062
Training Epoch: 99 [42752/50176]	Loss: 0.3153
Training Epoch: 99 [43008/50176]	Loss: 0.2673
Training Epoch: 99 [43264/50176]	Loss: 0.3626
Training Epoch: 99 [43520/50176]	Loss: 0.3313
Training Epoch: 99 [43776/50176]	Loss: 0.3226
Training Epoch: 99 [44032/50176]	Loss: 0.4175
Training Epoch: 99 [44288/50176]	Loss: 0.1902
Training Epoch: 99 [44544/50176]	Loss: 0.2731
Training Epoch: 99 [44800/50176]	Loss: 0.3679
Training Epoch: 99 [45056/50176]	Loss: 0.2627
Training Epoch: 99 [45312/50176]	Loss: 0.2726
Training Epoch: 99 [45568/50176]	Loss: 0.2764
Training Epoch: 99 [45824/50176]	Loss: 0.2460
Training Epoch: 99 [46080/50176]	Loss: 0.2774
Training Epoch: 99 [46336/50176]	Loss: 0.2487
Training Epoch: 99 [46592/50176]	Loss: 0.2526
Training Epoch: 99 [46848/50176]	Loss: 0.3815
Training Epoch: 99 [47104/50176]	Loss: 0.2651
Training Epoch: 99 [47360/50176]	Loss: 0.3729
Training Epoch: 99 [47616/50176]	Loss: 0.2955
Training Epoch: 99 [47872/50176]	Loss: 0.2453
Training Epoch: 99 [48128/50176]	Loss: 0.2885
Training Epoch: 99 [48384/50176]	Loss: 0.2416
Training Epoch: 99 [48640/50176]	Loss: 0.3209
Training Epoch: 99 [48896/50176]	Loss: 0.3382
Training Epoch: 99 [49152/50176]	Loss: 0.3461
Training Epoch: 99 [49408/50176]	Loss: 0.2604
Training Epoch: 99 [49664/50176]	Loss: 0.2473
Training Epoch: 99 [49920/50176]	Loss: 0.2434
Training Epoch: 99 [50176/50176]	Loss: 0.4219
Validation Epoch: 99, Average loss: 0.0100, Accuracy: 0.5901
[Training Loop] Training done
Stopped Zeus monitor 0.
