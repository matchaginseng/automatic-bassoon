[Power Profiler] Batch sizes: [8, 16, 32, 64, 128, 256, 512, 1024, 2048]
[Power Profiler] Learning rates: [0.001, 0.0505]

[Power Profiler] with batch size 8 and learning rate 2.7950849718747372e-06
[run job] Launching job with BS 8: and LR: 2.7950849718747372e-06 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs8+lr0.00000+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '2.7950849718747372e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '2.7950849718747372e-06', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs8+lr0.00000+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 2.7950849718747372e-06
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.7089
Training Epoch: 0 [24/50000]	Loss: 4.7207
Training Epoch: 0 [32/50000]	Loss: 4.6539
Training Epoch: 0 [40/50000]	Loss: 4.6484
Training Epoch: 0 [48/50000]	Loss: 4.7948
Training Epoch: 0 [56/50000]	Loss: 4.6474
Training Epoch: 0 [64/50000]	Loss: 4.8011
Training Epoch: 0 [72/50000]	Loss: 4.6768
Training Epoch: 0 [80/50000]	Loss: 4.6270
Training Epoch: 0 [88/50000]	Loss: 4.4709
Training Epoch: 0 [96/50000]	Loss: 4.6920
Training Epoch: 0 [104/50000]	Loss: 4.7182
Training Epoch: 0 [112/50000]	Loss: 4.7027
Training Epoch: 0 [120/50000]	Loss: 4.5433
Training Epoch: 0 [128/50000]	Loss: 4.6368
Training Epoch: 0 [136/50000]	Loss: 4.9090
Training Epoch: 0 [144/50000]	Loss: 4.6464
Training Epoch: 0 [152/50000]	Loss: 4.5364
Training Epoch: 0 [160/50000]	Loss: 4.7189
Training Epoch: 0 [168/50000]	Loss: 4.7222
Training Epoch: 0 [176/50000]	Loss: 4.7126
Training Epoch: 0 [184/50000]	Loss: 4.5494
Training Epoch: 0 [192/50000]	Loss: 4.5906
Training Epoch: 0 [200/50000]	Loss: 4.7444
Training Epoch: 0 [208/50000]	Loss: 4.7540
Training Epoch: 0 [216/50000]	Loss: 4.6313
Training Epoch: 0 [224/50000]	Loss: 4.4866
Training Epoch: 0 [232/50000]	Loss: 4.5783
Training Epoch: 0 [240/50000]	Loss: 4.5500
Training Epoch: 0 [248/50000]	Loss: 4.5849
Training Epoch: 0 [256/50000]	Loss: 4.7236
Training Epoch: 0 [264/50000]	Loss: 4.6672
Training Epoch: 0 [272/50000]	Loss: 4.6852
Training Epoch: 0 [280/50000]	Loss: 4.6924
Training Epoch: 0 [288/50000]	Loss: 4.5876
Training Epoch: 0 [296/50000]	Loss: 4.6167
Training Epoch: 0 [304/50000]	Loss: 4.6078
Training Epoch: 0 [312/50000]	Loss: 4.5831
Training Epoch: 0 [320/50000]	Loss: 4.5181
Training Epoch: 0 [328/50000]	Loss: 4.6173
Training Epoch: 0 [336/50000]	Loss: 4.5953
Training Epoch: 0 [344/50000]	Loss: 4.7234
Training Epoch: 0 [352/50000]	Loss: 4.4947
Training Epoch: 0 [360/50000]	Loss: 4.5064
Training Epoch: 0 [368/50000]	Loss: 4.5520
Training Epoch: 0 [376/50000]	Loss: 4.5009
Training Epoch: 0 [384/50000]	Loss: 4.5319
Training Epoch: 0 [392/50000]	Loss: 4.5919
Training Epoch: 0 [400/50000]	Loss: 4.5755
Profile done with power limit 175W
epoch 1 train time consumed: 3.55s
Validation Epoch: 0, Average loss: 0.5780, Accuracy: 0.0100
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 175000, 'lr': 2.7950849718747372e-06, 'energy': 82.58275456563551, 'time': 2.3174024310001187, 'accuracy': 0.01, 'total_cost': 4088.4873560764627}
[run job] Launching job with BS 8: and LR: 2.7950849718747372e-06 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs8+lr0.00000+pl150', 'ZEUS_COST_THRESH': '8176.974712152925', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '2.7950849718747372e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '2.7950849718747372e-06', '--power_limit', '150']
[run job] cost_ub=8176.974712152925
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs8+lr0.00000+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 2.7950849718747372e-06
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.7089
Training Epoch: 0 [24/50000]	Loss: 4.7207
Training Epoch: 0 [32/50000]	Loss: 4.6539
Training Epoch: 0 [40/50000]	Loss: 4.6484
Training Epoch: 0 [48/50000]	Loss: 4.7948
Training Epoch: 0 [56/50000]	Loss: 4.6474
Training Epoch: 0 [64/50000]	Loss: 4.8011
Training Epoch: 0 [72/50000]	Loss: 4.6768
Training Epoch: 0 [80/50000]	Loss: 4.6270
Training Epoch: 0 [88/50000]	Loss: 4.4709
Training Epoch: 0 [96/50000]	Loss: 4.6920
Training Epoch: 0 [104/50000]	Loss: 4.7182
Training Epoch: 0 [112/50000]	Loss: 4.7027
Training Epoch: 0 [120/50000]	Loss: 4.5433
Training Epoch: 0 [128/50000]	Loss: 4.6368
Training Epoch: 0 [136/50000]	Loss: 4.9089
Training Epoch: 0 [144/50000]	Loss: 4.6464
Training Epoch: 0 [152/50000]	Loss: 4.5364
Training Epoch: 0 [160/50000]	Loss: 4.7188
Training Epoch: 0 [168/50000]	Loss: 4.7222
Training Epoch: 0 [176/50000]	Loss: 4.7126
Training Epoch: 0 [184/50000]	Loss: 4.5495
Training Epoch: 0 [192/50000]	Loss: 4.5907
Training Epoch: 0 [200/50000]	Loss: 4.7444
Training Epoch: 0 [208/50000]	Loss: 4.7541
Training Epoch: 0 [216/50000]	Loss: 4.6312
Training Epoch: 0 [224/50000]	Loss: 4.4867
Training Epoch: 0 [232/50000]	Loss: 4.5783
Training Epoch: 0 [240/50000]	Loss: 4.5502
Training Epoch: 0 [248/50000]	Loss: 4.5849
Training Epoch: 0 [256/50000]	Loss: 4.7237
Training Epoch: 0 [264/50000]	Loss: 4.6671
Training Epoch: 0 [272/50000]	Loss: 4.6852
Training Epoch: 0 [280/50000]	Loss: 4.6924
Training Epoch: 0 [288/50000]	Loss: 4.5877
Training Epoch: 0 [296/50000]	Loss: 4.6166
Training Epoch: 0 [304/50000]	Loss: 4.6077
Training Epoch: 0 [312/50000]	Loss: 4.5830
Training Epoch: 0 [320/50000]	Loss: 4.5180
Training Epoch: 0 [328/50000]	Loss: 4.6172
Training Epoch: 0 [336/50000]	Loss: 4.5952
Training Epoch: 0 [344/50000]	Loss: 4.7232
Training Epoch: 0 [352/50000]	Loss: 4.4948
Training Epoch: 0 [360/50000]	Loss: 4.5063
Training Epoch: 0 [368/50000]	Loss: 4.5518
Training Epoch: 0 [376/50000]	Loss: 4.5009
Training Epoch: 0 [384/50000]	Loss: 4.5319
Training Epoch: 0 [392/50000]	Loss: 4.5919
Training Epoch: 0 [400/50000]	Loss: 4.5756
Profile done with power limit 150W
epoch 1 train time consumed: 3.39s
Validation Epoch: 0, Average loss: 0.5780, Accuracy: 0.0100
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 150000, 'lr': 2.7950849718747372e-06, 'energy': 80.25333204630772, 'time': 2.374768388999655, 'accuracy': 0.01, 'total_cost': 4187.946013567919}
[run job] Launching job with BS 8: and LR: 2.7950849718747372e-06 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs8+lr0.00000+pl125', 'ZEUS_COST_THRESH': '8176.974712152925', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '2.7950849718747372e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '2.7950849718747372e-06', '--power_limit', '125']
[run job] cost_ub=8176.974712152925
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs8+lr0.00000+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 2.7950849718747372e-06
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.7089
Training Epoch: 0 [24/50000]	Loss: 4.7207
Training Epoch: 0 [32/50000]	Loss: 4.6539
Training Epoch: 0 [40/50000]	Loss: 4.6484
Training Epoch: 0 [48/50000]	Loss: 4.7948
Training Epoch: 0 [56/50000]	Loss: 4.6474
Training Epoch: 0 [64/50000]	Loss: 4.8011
Training Epoch: 0 [72/50000]	Loss: 4.6768
Training Epoch: 0 [80/50000]	Loss: 4.6270
Training Epoch: 0 [88/50000]	Loss: 4.4709
Training Epoch: 0 [96/50000]	Loss: 4.6920
Training Epoch: 0 [104/50000]	Loss: 4.7182
Training Epoch: 0 [112/50000]	Loss: 4.7027
Training Epoch: 0 [120/50000]	Loss: 4.5433
Training Epoch: 0 [128/50000]	Loss: 4.6367
Training Epoch: 0 [136/50000]	Loss: 4.9089
Training Epoch: 0 [144/50000]	Loss: 4.6464
Training Epoch: 0 [152/50000]	Loss: 4.5364
Training Epoch: 0 [160/50000]	Loss: 4.7188
Training Epoch: 0 [168/50000]	Loss: 4.7222
Training Epoch: 0 [176/50000]	Loss: 4.7125
Training Epoch: 0 [184/50000]	Loss: 4.5494
Training Epoch: 0 [192/50000]	Loss: 4.5906
Training Epoch: 0 [200/50000]	Loss: 4.7444
Training Epoch: 0 [208/50000]	Loss: 4.7540
Training Epoch: 0 [216/50000]	Loss: 4.6313
Training Epoch: 0 [224/50000]	Loss: 4.4865
Training Epoch: 0 [232/50000]	Loss: 4.5783
Training Epoch: 0 [240/50000]	Loss: 4.5500
Training Epoch: 0 [248/50000]	Loss: 4.5849
Training Epoch: 0 [256/50000]	Loss: 4.7234
Training Epoch: 0 [264/50000]	Loss: 4.6672
Training Epoch: 0 [272/50000]	Loss: 4.6851
Training Epoch: 0 [280/50000]	Loss: 4.6925
Training Epoch: 0 [288/50000]	Loss: 4.5876
Training Epoch: 0 [296/50000]	Loss: 4.6169
Training Epoch: 0 [304/50000]	Loss: 4.6079
Training Epoch: 0 [312/50000]	Loss: 4.5834
Training Epoch: 0 [320/50000]	Loss: 4.5181
Training Epoch: 0 [328/50000]	Loss: 4.6173
Training Epoch: 0 [336/50000]	Loss: 4.5953
Training Epoch: 0 [344/50000]	Loss: 4.7236
Training Epoch: 0 [352/50000]	Loss: 4.4945
Training Epoch: 0 [360/50000]	Loss: 4.5065
Training Epoch: 0 [368/50000]	Loss: 4.5521
Training Epoch: 0 [376/50000]	Loss: 4.5009
Training Epoch: 0 [384/50000]	Loss: 4.5320
Training Epoch: 0 [392/50000]	Loss: 4.5918
Training Epoch: 0 [400/50000]	Loss: 4.5756
Profile done with power limit 125W
epoch 1 train time consumed: 3.33s
Validation Epoch: 0, Average loss: 0.5780, Accuracy: 0.0099
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 125000, 'lr': 2.7950849718747372e-06, 'energy': 81.83520515653255, 'time': 2.3189265480004906, 'accuracy': 0.0099, 'total_cost': 4132.177314205526}
[run job] Launching job with BS 8: and LR: 2.7950849718747372e-06 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs8+lr0.00000+pl100', 'ZEUS_COST_THRESH': '8176.974712152925', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '2.7950849718747372e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '2.7950849718747372e-06', '--power_limit', '100']
[run job] cost_ub=8176.974712152925
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs8+lr0.00000+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 2.7950849718747372e-06
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.7089
Training Epoch: 0 [24/50000]	Loss: 4.7207
Training Epoch: 0 [32/50000]	Loss: 4.6539
Training Epoch: 0 [40/50000]	Loss: 4.6484
Training Epoch: 0 [48/50000]	Loss: 4.7948
Training Epoch: 0 [56/50000]	Loss: 4.6474
Training Epoch: 0 [64/50000]	Loss: 4.8011
Training Epoch: 0 [72/50000]	Loss: 4.6768
Training Epoch: 0 [80/50000]	Loss: 4.6270
Training Epoch: 0 [88/50000]	Loss: 4.4709
Training Epoch: 0 [96/50000]	Loss: 4.6920
Training Epoch: 0 [104/50000]	Loss: 4.7182
Training Epoch: 0 [112/50000]	Loss: 4.7027
Training Epoch: 0 [120/50000]	Loss: 4.5433
Training Epoch: 0 [128/50000]	Loss: 4.6368
Training Epoch: 0 [136/50000]	Loss: 4.9090
Training Epoch: 0 [144/50000]	Loss: 4.6464
Training Epoch: 0 [152/50000]	Loss: 4.5364
Training Epoch: 0 [160/50000]	Loss: 4.7189
Training Epoch: 0 [168/50000]	Loss: 4.7222
Training Epoch: 0 [176/50000]	Loss: 4.7126
Training Epoch: 0 [184/50000]	Loss: 4.5494
Training Epoch: 0 [192/50000]	Loss: 4.5906
Training Epoch: 0 [200/50000]	Loss: 4.7444
Training Epoch: 0 [208/50000]	Loss: 4.7540
Training Epoch: 0 [216/50000]	Loss: 4.6312
Training Epoch: 0 [224/50000]	Loss: 4.4866
Training Epoch: 0 [232/50000]	Loss: 4.5783
Training Epoch: 0 [240/50000]	Loss: 4.5500
Training Epoch: 0 [248/50000]	Loss: 4.5849
Training Epoch: 0 [256/50000]	Loss: 4.7237
Training Epoch: 0 [264/50000]	Loss: 4.6671
Training Epoch: 0 [272/50000]	Loss: 4.6851
Training Epoch: 0 [280/50000]	Loss: 4.6924
Training Epoch: 0 [288/50000]	Loss: 4.5876
Training Epoch: 0 [296/50000]	Loss: 4.6167
Training Epoch: 0 [304/50000]	Loss: 4.6077
Training Epoch: 0 [312/50000]	Loss: 4.5831
Training Epoch: 0 [320/50000]	Loss: 4.5181
Training Epoch: 0 [328/50000]	Loss: 4.6172
Training Epoch: 0 [336/50000]	Loss: 4.5952
Training Epoch: 0 [344/50000]	Loss: 4.7233
Training Epoch: 0 [352/50000]	Loss: 4.4946
Training Epoch: 0 [360/50000]	Loss: 4.5064
Training Epoch: 0 [368/50000]	Loss: 4.5520
Training Epoch: 0 [376/50000]	Loss: 4.5009
Training Epoch: 0 [384/50000]	Loss: 4.5319
Training Epoch: 0 [392/50000]	Loss: 4.5918
Training Epoch: 0 [400/50000]	Loss: 4.5755
Profile done with power limit 100W
epoch 1 train time consumed: 3.26s
Validation Epoch: 0, Average loss: 0.5780, Accuracy: 0.0100
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 100000, 'lr': 2.7950849718747372e-06, 'energy': 83.44478981885383, 'time': 2.274567181999373, 'accuracy': 0.01, 'total_cost': 4013.870484426445}

[Power Profiler] with batch size 8 and learning rate 1.986281072758838e-05
[run job] Launching job with BS 8: and LR: 1.986281072758838e-05 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs8+lr0.00002+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '1.986281072758838e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '1.986281072758838e-05', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs8+lr0.00002+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 1.986281072758838e-05
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.7096
Training Epoch: 0 [24/50000]	Loss: 4.7167
Training Epoch: 0 [32/50000]	Loss: 4.6502
Training Epoch: 0 [40/50000]	Loss: 4.6448
Training Epoch: 0 [48/50000]	Loss: 4.8098
Training Epoch: 0 [56/50000]	Loss: 4.6578
Training Epoch: 0 [64/50000]	Loss: 4.7890
Training Epoch: 0 [72/50000]	Loss: 4.6600
Training Epoch: 0 [80/50000]	Loss: 4.6227
Training Epoch: 0 [88/50000]	Loss: 4.4602
Training Epoch: 0 [96/50000]	Loss: 4.7170
Training Epoch: 0 [104/50000]	Loss: 4.7404
Training Epoch: 0 [112/50000]	Loss: 4.6726
Training Epoch: 0 [120/50000]	Loss: 4.5439
Training Epoch: 0 [128/50000]	Loss: 4.6422
Training Epoch: 0 [136/50000]	Loss: 4.9032
Training Epoch: 0 [144/50000]	Loss: 4.6280
Training Epoch: 0 [152/50000]	Loss: 4.5301
Training Epoch: 0 [160/50000]	Loss: 4.7353
Training Epoch: 0 [168/50000]	Loss: 4.7110
Training Epoch: 0 [176/50000]	Loss: 4.7189
Training Epoch: 0 [184/50000]	Loss: 4.5589
Training Epoch: 0 [192/50000]	Loss: 4.5551
Training Epoch: 0 [200/50000]	Loss: 4.7562
Training Epoch: 0 [208/50000]	Loss: 4.7401
Training Epoch: 0 [216/50000]	Loss: 4.6226
Training Epoch: 0 [224/50000]	Loss: 4.4874
Training Epoch: 0 [232/50000]	Loss: 4.5703
Training Epoch: 0 [240/50000]	Loss: 4.5783
Training Epoch: 0 [248/50000]	Loss: 4.6019
Training Epoch: 0 [256/50000]	Loss: 4.7255
Training Epoch: 0 [264/50000]	Loss: 4.6215
Training Epoch: 0 [272/50000]	Loss: 4.6927
Training Epoch: 0 [280/50000]	Loss: 4.6834
Training Epoch: 0 [288/50000]	Loss: 4.5841
Training Epoch: 0 [296/50000]	Loss: 4.6279
Training Epoch: 0 [304/50000]	Loss: 4.6163
Training Epoch: 0 [312/50000]	Loss: 4.5544
Training Epoch: 0 [320/50000]	Loss: 4.5422
Training Epoch: 0 [328/50000]	Loss: 4.6352
Training Epoch: 0 [336/50000]	Loss: 4.6070
Training Epoch: 0 [344/50000]	Loss: 4.6575
Training Epoch: 0 [352/50000]	Loss: 4.4462
Training Epoch: 0 [360/50000]	Loss: 4.5468
Training Epoch: 0 [368/50000]	Loss: 4.5795
Training Epoch: 0 [376/50000]	Loss: 4.5336
Training Epoch: 0 [384/50000]	Loss: 4.5060
Training Epoch: 0 [392/50000]	Loss: 4.5885
Training Epoch: 0 [400/50000]	Loss: 4.5512
Profile done with power limit 175W
epoch 1 train time consumed: 3.17s
Validation Epoch: 0, Average loss: 0.5779, Accuracy: 0.0112
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 175000, 'lr': 1.986281072758838e-05, 'energy': 88.005798735016, 'time': 2.173690054999497, 'accuracy': 0.0112, 'total_cost': 3427.821353342077}
[run job] Launching job with BS 8: and LR: 1.986281072758838e-05 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs8+lr0.00002+pl150', 'ZEUS_COST_THRESH': '6855.642706684154', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '1.986281072758838e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '1.986281072758838e-05', '--power_limit', '150']
[run job] cost_ub=6855.642706684154
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs8+lr0.00002+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 1.986281072758838e-05
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.7096
Training Epoch: 0 [24/50000]	Loss: 4.7167
Training Epoch: 0 [32/50000]	Loss: 4.6502
Training Epoch: 0 [40/50000]	Loss: 4.6448
Training Epoch: 0 [48/50000]	Loss: 4.8098
Training Epoch: 0 [56/50000]	Loss: 4.6576
Training Epoch: 0 [64/50000]	Loss: 4.7892
Training Epoch: 0 [72/50000]	Loss: 4.6600
Training Epoch: 0 [80/50000]	Loss: 4.6228
Training Epoch: 0 [88/50000]	Loss: 4.4605
Training Epoch: 0 [96/50000]	Loss: 4.7172
Training Epoch: 0 [104/50000]	Loss: 4.7404
Training Epoch: 0 [112/50000]	Loss: 4.6725
Training Epoch: 0 [120/50000]	Loss: 4.5436
Training Epoch: 0 [128/50000]	Loss: 4.6426
Training Epoch: 0 [136/50000]	Loss: 4.9038
Training Epoch: 0 [144/50000]	Loss: 4.6277
Training Epoch: 0 [152/50000]	Loss: 4.5327
Training Epoch: 0 [160/50000]	Loss: 4.7364
Training Epoch: 0 [168/50000]	Loss: 4.7123
Training Epoch: 0 [176/50000]	Loss: 4.7189
Training Epoch: 0 [184/50000]	Loss: 4.5588
Training Epoch: 0 [192/50000]	Loss: 4.5586
Training Epoch: 0 [200/50000]	Loss: 4.7563
Training Epoch: 0 [208/50000]	Loss: 4.7383
Training Epoch: 0 [216/50000]	Loss: 4.6275
Training Epoch: 0 [224/50000]	Loss: 4.4869
Training Epoch: 0 [232/50000]	Loss: 4.5733
Training Epoch: 0 [240/50000]	Loss: 4.5794
Training Epoch: 0 [248/50000]	Loss: 4.6013
Training Epoch: 0 [256/50000]	Loss: 4.7278
Training Epoch: 0 [264/50000]	Loss: 4.6228
Training Epoch: 0 [272/50000]	Loss: 4.6952
Training Epoch: 0 [280/50000]	Loss: 4.6853
Training Epoch: 0 [288/50000]	Loss: 4.5831
Training Epoch: 0 [296/50000]	Loss: 4.6335
Training Epoch: 0 [304/50000]	Loss: 4.6155
Training Epoch: 0 [312/50000]	Loss: 4.5494
Training Epoch: 0 [320/50000]	Loss: 4.5471
Training Epoch: 0 [328/50000]	Loss: 4.6347
Training Epoch: 0 [336/50000]	Loss: 4.6055
Training Epoch: 0 [344/50000]	Loss: 4.6543
Training Epoch: 0 [352/50000]	Loss: 4.4490
Training Epoch: 0 [360/50000]	Loss: 4.5427
Training Epoch: 0 [368/50000]	Loss: 4.5812
Training Epoch: 0 [376/50000]	Loss: 4.5382
Training Epoch: 0 [384/50000]	Loss: 4.5068
Training Epoch: 0 [392/50000]	Loss: 4.5920
Training Epoch: 0 [400/50000]	Loss: 4.5576
Profile done with power limit 150W
epoch 1 train time consumed: 3.22s
Validation Epoch: 0, Average loss: 0.5779, Accuracy: 0.0110
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 150000, 'lr': 1.986281072758838e-05, 'energy': 88.97610251265722, 'time': 2.242281455999546, 'accuracy': 0.011, 'total_cost': 3599.6208990947894}
[run job] Launching job with BS 8: and LR: 1.986281072758838e-05 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs8+lr0.00002+pl125', 'ZEUS_COST_THRESH': '6855.642706684154', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '1.986281072758838e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '1.986281072758838e-05', '--power_limit', '125']
[run job] cost_ub=6855.642706684154
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs8+lr0.00002+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 1.986281072758838e-05
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.7096
Training Epoch: 0 [24/50000]	Loss: 4.7167
Training Epoch: 0 [32/50000]	Loss: 4.6502
Training Epoch: 0 [40/50000]	Loss: 4.6448
Training Epoch: 0 [48/50000]	Loss: 4.8098
Training Epoch: 0 [56/50000]	Loss: 4.6578
Training Epoch: 0 [64/50000]	Loss: 4.7890
Training Epoch: 0 [72/50000]	Loss: 4.6600
Training Epoch: 0 [80/50000]	Loss: 4.6228
Training Epoch: 0 [88/50000]	Loss: 4.4601
Training Epoch: 0 [96/50000]	Loss: 4.7170
Training Epoch: 0 [104/50000]	Loss: 4.7405
Training Epoch: 0 [112/50000]	Loss: 4.6729
Training Epoch: 0 [120/50000]	Loss: 4.5441
Training Epoch: 0 [128/50000]	Loss: 4.6417
Training Epoch: 0 [136/50000]	Loss: 4.9028
Training Epoch: 0 [144/50000]	Loss: 4.6287
Training Epoch: 0 [152/50000]	Loss: 4.5301
Training Epoch: 0 [160/50000]	Loss: 4.7351
Training Epoch: 0 [168/50000]	Loss: 4.7115
Training Epoch: 0 [176/50000]	Loss: 4.7199
Training Epoch: 0 [184/50000]	Loss: 4.5599
Training Epoch: 0 [192/50000]	Loss: 4.5559
Training Epoch: 0 [200/50000]	Loss: 4.7575
Training Epoch: 0 [208/50000]	Loss: 4.7361
Training Epoch: 0 [216/50000]	Loss: 4.6239
Training Epoch: 0 [224/50000]	Loss: 4.4847
Training Epoch: 0 [232/50000]	Loss: 4.5694
Training Epoch: 0 [240/50000]	Loss: 4.5773
Training Epoch: 0 [248/50000]	Loss: 4.6047
Training Epoch: 0 [256/50000]	Loss: 4.7288
Training Epoch: 0 [264/50000]	Loss: 4.6163
Training Epoch: 0 [272/50000]	Loss: 4.6961
Training Epoch: 0 [280/50000]	Loss: 4.6855
Training Epoch: 0 [288/50000]	Loss: 4.5877
Training Epoch: 0 [296/50000]	Loss: 4.6334
Training Epoch: 0 [304/50000]	Loss: 4.6126
Training Epoch: 0 [312/50000]	Loss: 4.5521
Training Epoch: 0 [320/50000]	Loss: 4.5454
Training Epoch: 0 [328/50000]	Loss: 4.6368
Training Epoch: 0 [336/50000]	Loss: 4.6056
Training Epoch: 0 [344/50000]	Loss: 4.6513
Training Epoch: 0 [352/50000]	Loss: 4.4461
Training Epoch: 0 [360/50000]	Loss: 4.5430
Training Epoch: 0 [368/50000]	Loss: 4.5817
Training Epoch: 0 [376/50000]	Loss: 4.5395
Training Epoch: 0 [384/50000]	Loss: 4.5075
Training Epoch: 0 [392/50000]	Loss: 4.5910
Training Epoch: 0 [400/50000]	Loss: 4.5442
Profile done with power limit 125W
epoch 1 train time consumed: 3.26s
Validation Epoch: 0, Average loss: 0.5779, Accuracy: 0.0111
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 125000, 'lr': 1.986281072758838e-05, 'energy': 85.82851240929077, 'time': 2.255532666000363, 'accuracy': 0.0111, 'total_cost': 3586.9491625804967}
[run job] Launching job with BS 8: and LR: 1.986281072758838e-05 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs8+lr0.00002+pl100', 'ZEUS_COST_THRESH': '6855.642706684154', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '1.986281072758838e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '1.986281072758838e-05', '--power_limit', '100']
[run job] cost_ub=6855.642706684154
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs8+lr0.00002+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 1.986281072758838e-05
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.7096
Training Epoch: 0 [24/50000]	Loss: 4.7167
Training Epoch: 0 [32/50000]	Loss: 4.6503
Training Epoch: 0 [40/50000]	Loss: 4.6448
Training Epoch: 0 [48/50000]	Loss: 4.8098
Training Epoch: 0 [56/50000]	Loss: 4.6577
Training Epoch: 0 [64/50000]	Loss: 4.7892
Training Epoch: 0 [72/50000]	Loss: 4.6593
Training Epoch: 0 [80/50000]	Loss: 4.6227
Training Epoch: 0 [88/50000]	Loss: 4.4606
Training Epoch: 0 [96/50000]	Loss: 4.7175
Training Epoch: 0 [104/50000]	Loss: 4.7394
Training Epoch: 0 [112/50000]	Loss: 4.6731
Training Epoch: 0 [120/50000]	Loss: 4.5433
Training Epoch: 0 [128/50000]	Loss: 4.6427
Training Epoch: 0 [136/50000]	Loss: 4.9043
Training Epoch: 0 [144/50000]	Loss: 4.6291
Training Epoch: 0 [152/50000]	Loss: 4.5328
Training Epoch: 0 [160/50000]	Loss: 4.7351
Training Epoch: 0 [168/50000]	Loss: 4.7125
Training Epoch: 0 [176/50000]	Loss: 4.7171
Training Epoch: 0 [184/50000]	Loss: 4.5591
Training Epoch: 0 [192/50000]	Loss: 4.5554
Training Epoch: 0 [200/50000]	Loss: 4.7581
Training Epoch: 0 [208/50000]	Loss: 4.7399
Training Epoch: 0 [216/50000]	Loss: 4.6257
Training Epoch: 0 [224/50000]	Loss: 4.4844
Training Epoch: 0 [232/50000]	Loss: 4.5683
Training Epoch: 0 [240/50000]	Loss: 4.5786
Training Epoch: 0 [248/50000]	Loss: 4.6026
Training Epoch: 0 [256/50000]	Loss: 4.7303
Training Epoch: 0 [264/50000]	Loss: 4.6217
Training Epoch: 0 [272/50000]	Loss: 4.6958
Training Epoch: 0 [280/50000]	Loss: 4.6867
Training Epoch: 0 [288/50000]	Loss: 4.5861
Training Epoch: 0 [296/50000]	Loss: 4.6331
Training Epoch: 0 [304/50000]	Loss: 4.6146
Training Epoch: 0 [312/50000]	Loss: 4.5525
Training Epoch: 0 [320/50000]	Loss: 4.5455
Training Epoch: 0 [328/50000]	Loss: 4.6354
Training Epoch: 0 [336/50000]	Loss: 4.6103
Training Epoch: 0 [344/50000]	Loss: 4.6513
Training Epoch: 0 [352/50000]	Loss: 4.4492
Training Epoch: 0 [360/50000]	Loss: 4.5432
Training Epoch: 0 [368/50000]	Loss: 4.5831
Training Epoch: 0 [376/50000]	Loss: 4.5352
Training Epoch: 0 [384/50000]	Loss: 4.4972
Training Epoch: 0 [392/50000]	Loss: 4.5907
Training Epoch: 0 [400/50000]	Loss: 4.5501
Profile done with power limit 100W
epoch 1 train time consumed: 3.22s
Validation Epoch: 0, Average loss: 0.5778, Accuracy: 0.0119
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 100000, 'lr': 1.986281072758838e-05, 'energy': 87.56180252209676, 'time': 2.2456502539998837, 'accuracy': 0.0119, 'total_cost': 3331.859382780365}

[Power Profiler] with batch size 16 and learning rate 3.952847075210474e-06
[run job] Launching job with BS 16: and LR: 3.952847075210474e-06 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs16+lr0.00000+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '3.952847075210474e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '3.952847075210474e-06', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs16+lr0.00000+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 3.952847075210474e-06
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6918
Training Epoch: 0 [48/50000]	Loss: 4.7064
Training Epoch: 0 [64/50000]	Loss: 4.7042
Training Epoch: 0 [80/50000]	Loss: 4.6476
Training Epoch: 0 [96/50000]	Loss: 4.5760
Training Epoch: 0 [112/50000]	Loss: 4.6947
Training Epoch: 0 [128/50000]	Loss: 4.5323
Training Epoch: 0 [144/50000]	Loss: 4.7544
Training Epoch: 0 [160/50000]	Loss: 4.6350
Training Epoch: 0 [176/50000]	Loss: 4.7401
Training Epoch: 0 [192/50000]	Loss: 4.5593
Training Epoch: 0 [208/50000]	Loss: 4.7149
Training Epoch: 0 [224/50000]	Loss: 4.5512
Training Epoch: 0 [240/50000]	Loss: 4.5625
Training Epoch: 0 [256/50000]	Loss: 4.6453
Training Epoch: 0 [272/50000]	Loss: 4.6703
Training Epoch: 0 [288/50000]	Loss: 4.6431
Training Epoch: 0 [304/50000]	Loss: 4.5666
Training Epoch: 0 [320/50000]	Loss: 4.5935
Training Epoch: 0 [336/50000]	Loss: 4.6313
Training Epoch: 0 [352/50000]	Loss: 4.5795
Training Epoch: 0 [368/50000]	Loss: 4.5633
Training Epoch: 0 [384/50000]	Loss: 4.5325
Training Epoch: 0 [400/50000]	Loss: 4.6196
Training Epoch: 0 [416/50000]	Loss: 4.6691
Training Epoch: 0 [432/50000]	Loss: 4.5570
Training Epoch: 0 [448/50000]	Loss: 4.5724
Training Epoch: 0 [464/50000]	Loss: 4.6587
Training Epoch: 0 [480/50000]	Loss: 4.6578
Training Epoch: 0 [496/50000]	Loss: 4.6314
Training Epoch: 0 [512/50000]	Loss: 4.5159
Training Epoch: 0 [528/50000]	Loss: 4.6299
Training Epoch: 0 [544/50000]	Loss: 4.5586
Training Epoch: 0 [560/50000]	Loss: 4.6873
Training Epoch: 0 [576/50000]	Loss: 4.6672
Training Epoch: 0 [592/50000]	Loss: 4.6891
Training Epoch: 0 [608/50000]	Loss: 4.5985
Training Epoch: 0 [624/50000]	Loss: 4.6768
Training Epoch: 0 [640/50000]	Loss: 4.7070
Training Epoch: 0 [656/50000]	Loss: 4.5821
Training Epoch: 0 [672/50000]	Loss: 4.6034
Training Epoch: 0 [688/50000]	Loss: 4.5553
Training Epoch: 0 [704/50000]	Loss: 4.6235
Training Epoch: 0 [720/50000]	Loss: 4.6435
Training Epoch: 0 [736/50000]	Loss: 4.5843
Training Epoch: 0 [752/50000]	Loss: 4.5950
Training Epoch: 0 [768/50000]	Loss: 4.6595
Training Epoch: 0 [784/50000]	Loss: 4.6655
Training Epoch: 0 [800/50000]	Loss: 4.6466
Profile done with power limit 175W
epoch 1 train time consumed: 3.62s
Validation Epoch: 0, Average loss: 0.2890, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 175000, 'lr': 3.952847075210474e-06, 'energy': 107.1587775034141, 'time': 2.4688133199997537, 'accuracy': 0.0098, 'total_cost': 8904.666981634562}
[run job] Launching job with BS 16: and LR: 3.952847075210474e-06 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs16+lr0.00000+pl150', 'ZEUS_COST_THRESH': '17809.333963269124', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '3.952847075210474e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '3.952847075210474e-06', '--power_limit', '150']
[run job] cost_ub=17809.333963269124
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs16+lr0.00000+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 3.952847075210474e-06
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6918
Training Epoch: 0 [48/50000]	Loss: 4.7064
Training Epoch: 0 [64/50000]	Loss: 4.7042
Training Epoch: 0 [80/50000]	Loss: 4.6476
Training Epoch: 0 [96/50000]	Loss: 4.5760
Training Epoch: 0 [112/50000]	Loss: 4.6947
Training Epoch: 0 [128/50000]	Loss: 4.5323
Training Epoch: 0 [144/50000]	Loss: 4.7544
Training Epoch: 0 [160/50000]	Loss: 4.6350
Training Epoch: 0 [176/50000]	Loss: 4.7401
Training Epoch: 0 [192/50000]	Loss: 4.5593
Training Epoch: 0 [208/50000]	Loss: 4.7149
Training Epoch: 0 [224/50000]	Loss: 4.5512
Training Epoch: 0 [240/50000]	Loss: 4.5625
Training Epoch: 0 [256/50000]	Loss: 4.6452
Training Epoch: 0 [272/50000]	Loss: 4.6703
Training Epoch: 0 [288/50000]	Loss: 4.6431
Training Epoch: 0 [304/50000]	Loss: 4.5667
Training Epoch: 0 [320/50000]	Loss: 4.5935
Training Epoch: 0 [336/50000]	Loss: 4.6312
Training Epoch: 0 [352/50000]	Loss: 4.5797
Training Epoch: 0 [368/50000]	Loss: 4.5634
Training Epoch: 0 [384/50000]	Loss: 4.5326
Training Epoch: 0 [400/50000]	Loss: 4.6196
Training Epoch: 0 [416/50000]	Loss: 4.6692
Training Epoch: 0 [432/50000]	Loss: 4.5570
Training Epoch: 0 [448/50000]	Loss: 4.5725
Training Epoch: 0 [464/50000]	Loss: 4.6587
Training Epoch: 0 [480/50000]	Loss: 4.6577
Training Epoch: 0 [496/50000]	Loss: 4.6315
Training Epoch: 0 [512/50000]	Loss: 4.5158
Training Epoch: 0 [528/50000]	Loss: 4.6299
Training Epoch: 0 [544/50000]	Loss: 4.5585
Training Epoch: 0 [560/50000]	Loss: 4.6871
Training Epoch: 0 [576/50000]	Loss: 4.6671
Training Epoch: 0 [592/50000]	Loss: 4.6891
Training Epoch: 0 [608/50000]	Loss: 4.5987
Training Epoch: 0 [624/50000]	Loss: 4.6767
Training Epoch: 0 [640/50000]	Loss: 4.7069
Training Epoch: 0 [656/50000]	Loss: 4.5821
Training Epoch: 0 [672/50000]	Loss: 4.6037
Training Epoch: 0 [688/50000]	Loss: 4.5555
Training Epoch: 0 [704/50000]	Loss: 4.6231
Training Epoch: 0 [720/50000]	Loss: 4.6434
Training Epoch: 0 [736/50000]	Loss: 4.5841
Training Epoch: 0 [752/50000]	Loss: 4.5949
Training Epoch: 0 [768/50000]	Loss: 4.6596
Training Epoch: 0 [784/50000]	Loss: 4.6657
Training Epoch: 0 [800/50000]	Loss: 4.6469
Profile done with power limit 150W
epoch 1 train time consumed: 3.65s
Validation Epoch: 0, Average loss: 0.2890, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 150000, 'lr': 3.952847075210474e-06, 'energy': 107.13288977471339, 'time': 2.524389973000325, 'accuracy': 0.0098, 'total_cost': 9103.133895225417}
[run job] Launching job with BS 16: and LR: 3.952847075210474e-06 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs16+lr0.00000+pl125', 'ZEUS_COST_THRESH': '17809.333963269124', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '3.952847075210474e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '3.952847075210474e-06', '--power_limit', '125']
[run job] cost_ub=17809.333963269124
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs16+lr0.00000+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 3.952847075210474e-06
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6918
Training Epoch: 0 [48/50000]	Loss: 4.7064
Training Epoch: 0 [64/50000]	Loss: 4.7042
Training Epoch: 0 [80/50000]	Loss: 4.6476
Training Epoch: 0 [96/50000]	Loss: 4.5760
Training Epoch: 0 [112/50000]	Loss: 4.6947
Training Epoch: 0 [128/50000]	Loss: 4.5323
Training Epoch: 0 [144/50000]	Loss: 4.7544
Training Epoch: 0 [160/50000]	Loss: 4.6350
Training Epoch: 0 [176/50000]	Loss: 4.7401
Training Epoch: 0 [192/50000]	Loss: 4.5593
Training Epoch: 0 [208/50000]	Loss: 4.7149
Training Epoch: 0 [224/50000]	Loss: 4.5512
Training Epoch: 0 [240/50000]	Loss: 4.5625
Training Epoch: 0 [256/50000]	Loss: 4.6453
Training Epoch: 0 [272/50000]	Loss: 4.6703
Training Epoch: 0 [288/50000]	Loss: 4.6432
Training Epoch: 0 [304/50000]	Loss: 4.5666
Training Epoch: 0 [320/50000]	Loss: 4.5935
Training Epoch: 0 [336/50000]	Loss: 4.6313
Training Epoch: 0 [352/50000]	Loss: 4.5795
Training Epoch: 0 [368/50000]	Loss: 4.5633
Training Epoch: 0 [384/50000]	Loss: 4.5327
Training Epoch: 0 [400/50000]	Loss: 4.6197
Training Epoch: 0 [416/50000]	Loss: 4.6693
Training Epoch: 0 [432/50000]	Loss: 4.5570
Training Epoch: 0 [448/50000]	Loss: 4.5725
Training Epoch: 0 [464/50000]	Loss: 4.6587
Training Epoch: 0 [480/50000]	Loss: 4.6578
Training Epoch: 0 [496/50000]	Loss: 4.6315
Training Epoch: 0 [512/50000]	Loss: 4.5159
Training Epoch: 0 [528/50000]	Loss: 4.6300
Training Epoch: 0 [544/50000]	Loss: 4.5584
Training Epoch: 0 [560/50000]	Loss: 4.6872
Training Epoch: 0 [576/50000]	Loss: 4.6672
Training Epoch: 0 [592/50000]	Loss: 4.6890
Training Epoch: 0 [608/50000]	Loss: 4.5987
Training Epoch: 0 [624/50000]	Loss: 4.6768
Training Epoch: 0 [640/50000]	Loss: 4.7071
Training Epoch: 0 [656/50000]	Loss: 4.5824
Training Epoch: 0 [672/50000]	Loss: 4.6037
Training Epoch: 0 [688/50000]	Loss: 4.5555
Training Epoch: 0 [704/50000]	Loss: 4.6232
Training Epoch: 0 [720/50000]	Loss: 4.6434
Training Epoch: 0 [736/50000]	Loss: 4.5843
Training Epoch: 0 [752/50000]	Loss: 4.5950
Training Epoch: 0 [768/50000]	Loss: 4.6595
Training Epoch: 0 [784/50000]	Loss: 4.6657
Training Epoch: 0 [800/50000]	Loss: 4.6467
Profile done with power limit 125W
epoch 1 train time consumed: 3.69s
Validation Epoch: 0, Average loss: 0.2890, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 125000, 'lr': 3.952847075210474e-06, 'energy': 107.34882969263539, 'time': 2.533772251999835, 'accuracy': 0.0098, 'total_cost': 9136.8183119934}
[run job] Launching job with BS 16: and LR: 3.952847075210474e-06 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs16+lr0.00000+pl100', 'ZEUS_COST_THRESH': '17809.333963269124', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '3.952847075210474e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '3.952847075210474e-06', '--power_limit', '100']
[run job] cost_ub=17809.333963269124
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs16+lr0.00000+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 3.952847075210474e-06
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6918
Training Epoch: 0 [48/50000]	Loss: 4.7064
Training Epoch: 0 [64/50000]	Loss: 4.7042
Training Epoch: 0 [80/50000]	Loss: 4.6476
Training Epoch: 0 [96/50000]	Loss: 4.5760
Training Epoch: 0 [112/50000]	Loss: 4.6947
Training Epoch: 0 [128/50000]	Loss: 4.5323
Training Epoch: 0 [144/50000]	Loss: 4.7544
Training Epoch: 0 [160/50000]	Loss: 4.6350
Training Epoch: 0 [176/50000]	Loss: 4.7401
Training Epoch: 0 [192/50000]	Loss: 4.5593
Training Epoch: 0 [208/50000]	Loss: 4.7149
Training Epoch: 0 [224/50000]	Loss: 4.5512
Training Epoch: 0 [240/50000]	Loss: 4.5625
Training Epoch: 0 [256/50000]	Loss: 4.6452
Training Epoch: 0 [272/50000]	Loss: 4.6703
Training Epoch: 0 [288/50000]	Loss: 4.6432
Training Epoch: 0 [304/50000]	Loss: 4.5667
Training Epoch: 0 [320/50000]	Loss: 4.5936
Training Epoch: 0 [336/50000]	Loss: 4.6313
Training Epoch: 0 [352/50000]	Loss: 4.5796
Training Epoch: 0 [368/50000]	Loss: 4.5633
Training Epoch: 0 [384/50000]	Loss: 4.5326
Training Epoch: 0 [400/50000]	Loss: 4.6196
Training Epoch: 0 [416/50000]	Loss: 4.6693
Training Epoch: 0 [432/50000]	Loss: 4.5571
Training Epoch: 0 [448/50000]	Loss: 4.5725
Training Epoch: 0 [464/50000]	Loss: 4.6588
Training Epoch: 0 [480/50000]	Loss: 4.6578
Training Epoch: 0 [496/50000]	Loss: 4.6314
Training Epoch: 0 [512/50000]	Loss: 4.5158
Training Epoch: 0 [528/50000]	Loss: 4.6300
Training Epoch: 0 [544/50000]	Loss: 4.5585
Training Epoch: 0 [560/50000]	Loss: 4.6871
Training Epoch: 0 [576/50000]	Loss: 4.6670
Training Epoch: 0 [592/50000]	Loss: 4.6890
Training Epoch: 0 [608/50000]	Loss: 4.5987
Training Epoch: 0 [624/50000]	Loss: 4.6767
Training Epoch: 0 [640/50000]	Loss: 4.7071
Training Epoch: 0 [656/50000]	Loss: 4.5823
Training Epoch: 0 [672/50000]	Loss: 4.6038
Training Epoch: 0 [688/50000]	Loss: 4.5556
Training Epoch: 0 [704/50000]	Loss: 4.6232
Training Epoch: 0 [720/50000]	Loss: 4.6435
Training Epoch: 0 [736/50000]	Loss: 4.5843
Training Epoch: 0 [752/50000]	Loss: 4.5949
Training Epoch: 0 [768/50000]	Loss: 4.6596
Training Epoch: 0 [784/50000]	Loss: 4.6657
Training Epoch: 0 [800/50000]	Loss: 4.6469
Profile done with power limit 100W
epoch 1 train time consumed: 3.78s
Validation Epoch: 0, Average loss: 0.2890, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 100000, 'lr': 3.952847075210474e-06, 'energy': 98.15100052294723, 'time': 2.6601100509997195, 'accuracy': 0.0098, 'total_cost': 9580.51630501773}

[Power Profiler] with batch size 16 and learning rate 2.8090256317805292e-05
[run job] Launching job with BS 16: and LR: 2.8090256317805292e-05 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs16+lr0.00003+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '2.8090256317805292e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '2.8090256317805292e-05', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs16+lr0.00003+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 2.8090256317805292e-05
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6895
Training Epoch: 0 [48/50000]	Loss: 4.7097
Training Epoch: 0 [64/50000]	Loss: 4.7043
Training Epoch: 0 [80/50000]	Loss: 4.6403
Training Epoch: 0 [96/50000]	Loss: 4.5767
Training Epoch: 0 [112/50000]	Loss: 4.6870
Training Epoch: 0 [128/50000]	Loss: 4.5303
Training Epoch: 0 [144/50000]	Loss: 4.7566
Training Epoch: 0 [160/50000]	Loss: 4.6345
Training Epoch: 0 [176/50000]	Loss: 4.7392
Training Epoch: 0 [192/50000]	Loss: 4.5460
Training Epoch: 0 [208/50000]	Loss: 4.7261
Training Epoch: 0 [224/50000]	Loss: 4.5474
Training Epoch: 0 [240/50000]	Loss: 4.5771
Training Epoch: 0 [256/50000]	Loss: 4.6511
Training Epoch: 0 [272/50000]	Loss: 4.6415
Training Epoch: 0 [288/50000]	Loss: 4.6340
Training Epoch: 0 [304/50000]	Loss: 4.5883
Training Epoch: 0 [320/50000]	Loss: 4.5700
Training Epoch: 0 [336/50000]	Loss: 4.6514
Training Epoch: 0 [352/50000]	Loss: 4.5544
Training Epoch: 0 [368/50000]	Loss: 4.6009
Training Epoch: 0 [384/50000]	Loss: 4.5356
Training Epoch: 0 [400/50000]	Loss: 4.6212
Training Epoch: 0 [416/50000]	Loss: 4.6307
Training Epoch: 0 [432/50000]	Loss: 4.5555
Training Epoch: 0 [448/50000]	Loss: 4.5497
Training Epoch: 0 [464/50000]	Loss: 4.6705
Training Epoch: 0 [480/50000]	Loss: 4.6464
Training Epoch: 0 [496/50000]	Loss: 4.6553
Training Epoch: 0 [512/50000]	Loss: 4.5214
Training Epoch: 0 [528/50000]	Loss: 4.5865
Training Epoch: 0 [544/50000]	Loss: 4.5222
Training Epoch: 0 [560/50000]	Loss: 4.6589
Training Epoch: 0 [576/50000]	Loss: 4.6397
Training Epoch: 0 [592/50000]	Loss: 4.6739
Training Epoch: 0 [608/50000]	Loss: 4.6004
Training Epoch: 0 [624/50000]	Loss: 4.6940
Training Epoch: 0 [640/50000]	Loss: 4.6755
Training Epoch: 0 [656/50000]	Loss: 4.6258
Training Epoch: 0 [672/50000]	Loss: 4.5987
Training Epoch: 0 [688/50000]	Loss: 4.5540
Training Epoch: 0 [704/50000]	Loss: 4.6058
Training Epoch: 0 [720/50000]	Loss: 4.6677
Training Epoch: 0 [736/50000]	Loss: 4.5624
Training Epoch: 0 [752/50000]	Loss: 4.6064
Training Epoch: 0 [768/50000]	Loss: 4.6426
Training Epoch: 0 [784/50000]	Loss: 4.6700
Training Epoch: 0 [800/50000]	Loss: 4.6229
Profile done with power limit 175W
epoch 1 train time consumed: 3.61s
Validation Epoch: 0, Average loss: 0.2888, Accuracy: 0.0121
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 175000, 'lr': 2.8090256317805292e-05, 'energy': 107.24327810978069, 'time': 2.4729733569993186, 'accuracy': 0.0121, 'total_cost': 7224.133365277225}
[run job] Launching job with BS 16: and LR: 2.8090256317805292e-05 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs16+lr0.00003+pl150', 'ZEUS_COST_THRESH': '14448.26673055445', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '2.8090256317805292e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '2.8090256317805292e-05', '--power_limit', '150']
[run job] cost_ub=14448.26673055445
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs16+lr0.00003+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 2.8090256317805292e-05
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6895
Training Epoch: 0 [48/50000]	Loss: 4.7097
Training Epoch: 0 [64/50000]	Loss: 4.7043
Training Epoch: 0 [80/50000]	Loss: 4.6404
Training Epoch: 0 [96/50000]	Loss: 4.5765
Training Epoch: 0 [112/50000]	Loss: 4.6869
Training Epoch: 0 [128/50000]	Loss: 4.5299
Training Epoch: 0 [144/50000]	Loss: 4.7580
Training Epoch: 0 [160/50000]	Loss: 4.6342
Training Epoch: 0 [176/50000]	Loss: 4.7383
Training Epoch: 0 [192/50000]	Loss: 4.5447
Training Epoch: 0 [208/50000]	Loss: 4.7257
Training Epoch: 0 [224/50000]	Loss: 4.5478
Training Epoch: 0 [240/50000]	Loss: 4.5759
Training Epoch: 0 [256/50000]	Loss: 4.6536
Training Epoch: 0 [272/50000]	Loss: 4.6436
Training Epoch: 0 [288/50000]	Loss: 4.6355
Training Epoch: 0 [304/50000]	Loss: 4.5897
Training Epoch: 0 [320/50000]	Loss: 4.5740
Training Epoch: 0 [336/50000]	Loss: 4.6515
Training Epoch: 0 [352/50000]	Loss: 4.5509
Training Epoch: 0 [368/50000]	Loss: 4.6023
Training Epoch: 0 [384/50000]	Loss: 4.5306
Training Epoch: 0 [400/50000]	Loss: 4.6214
Training Epoch: 0 [416/50000]	Loss: 4.6339
Training Epoch: 0 [432/50000]	Loss: 4.5525
Training Epoch: 0 [448/50000]	Loss: 4.5506
Training Epoch: 0 [464/50000]	Loss: 4.6741
Training Epoch: 0 [480/50000]	Loss: 4.6397
Training Epoch: 0 [496/50000]	Loss: 4.6538
Training Epoch: 0 [512/50000]	Loss: 4.5255
Training Epoch: 0 [528/50000]	Loss: 4.5878
Training Epoch: 0 [544/50000]	Loss: 4.5211
Training Epoch: 0 [560/50000]	Loss: 4.6601
Training Epoch: 0 [576/50000]	Loss: 4.6422
Training Epoch: 0 [592/50000]	Loss: 4.6821
Training Epoch: 0 [608/50000]	Loss: 4.5948
Training Epoch: 0 [624/50000]	Loss: 4.6977
Training Epoch: 0 [640/50000]	Loss: 4.6712
Training Epoch: 0 [656/50000]	Loss: 4.6278
Training Epoch: 0 [672/50000]	Loss: 4.5928
Training Epoch: 0 [688/50000]	Loss: 4.5604
Training Epoch: 0 [704/50000]	Loss: 4.6037
Training Epoch: 0 [720/50000]	Loss: 4.6663
Training Epoch: 0 [736/50000]	Loss: 4.5701
Training Epoch: 0 [752/50000]	Loss: 4.6076
Training Epoch: 0 [768/50000]	Loss: 4.6424
Training Epoch: 0 [784/50000]	Loss: 4.6801
Training Epoch: 0 [800/50000]	Loss: 4.6155
Profile done with power limit 150W
epoch 1 train time consumed: 3.63s
Validation Epoch: 0, Average loss: 0.2888, Accuracy: 0.0127
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 150000, 'lr': 2.8090256317805292e-05, 'energy': 108.31975337951117, 'time': 2.4756612219998715, 'accuracy': 0.0127, 'total_cost': 6890.921322600912}
[run job] Launching job with BS 16: and LR: 2.8090256317805292e-05 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs16+lr0.00003+pl125', 'ZEUS_COST_THRESH': '13781.842645201825', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '2.8090256317805292e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '2.8090256317805292e-05', '--power_limit', '125']
[run job] cost_ub=13781.842645201825
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs16+lr0.00003+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 2.8090256317805292e-05
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6895
Training Epoch: 0 [48/50000]	Loss: 4.7097
Training Epoch: 0 [64/50000]	Loss: 4.7043
Training Epoch: 0 [80/50000]	Loss: 4.6404
Training Epoch: 0 [96/50000]	Loss: 4.5764
Training Epoch: 0 [112/50000]	Loss: 4.6868
Training Epoch: 0 [128/50000]	Loss: 4.5300
Training Epoch: 0 [144/50000]	Loss: 4.7581
Training Epoch: 0 [160/50000]	Loss: 4.6347
Training Epoch: 0 [176/50000]	Loss: 4.7382
Training Epoch: 0 [192/50000]	Loss: 4.5440
Training Epoch: 0 [208/50000]	Loss: 4.7243
Training Epoch: 0 [224/50000]	Loss: 4.5491
Training Epoch: 0 [240/50000]	Loss: 4.5750
Training Epoch: 0 [256/50000]	Loss: 4.6569
Training Epoch: 0 [272/50000]	Loss: 4.6428
Training Epoch: 0 [288/50000]	Loss: 4.6348
Training Epoch: 0 [304/50000]	Loss: 4.5895
Training Epoch: 0 [320/50000]	Loss: 4.5770
Training Epoch: 0 [336/50000]	Loss: 4.6537
Training Epoch: 0 [352/50000]	Loss: 4.5516
Training Epoch: 0 [368/50000]	Loss: 4.6037
Training Epoch: 0 [384/50000]	Loss: 4.5288
Training Epoch: 0 [400/50000]	Loss: 4.6203
Training Epoch: 0 [416/50000]	Loss: 4.6352
Training Epoch: 0 [432/50000]	Loss: 4.5508
Training Epoch: 0 [448/50000]	Loss: 4.5512
Training Epoch: 0 [464/50000]	Loss: 4.6691
Training Epoch: 0 [480/50000]	Loss: 4.6381
Training Epoch: 0 [496/50000]	Loss: 4.6564
Training Epoch: 0 [512/50000]	Loss: 4.5225
Training Epoch: 0 [528/50000]	Loss: 4.5899
Training Epoch: 0 [544/50000]	Loss: 4.5272
Training Epoch: 0 [560/50000]	Loss: 4.6618
Training Epoch: 0 [576/50000]	Loss: 4.6489
Training Epoch: 0 [592/50000]	Loss: 4.6793
Training Epoch: 0 [608/50000]	Loss: 4.5917
Training Epoch: 0 [624/50000]	Loss: 4.7033
Training Epoch: 0 [640/50000]	Loss: 4.6652
Training Epoch: 0 [656/50000]	Loss: 4.6224
Training Epoch: 0 [672/50000]	Loss: 4.5922
Training Epoch: 0 [688/50000]	Loss: 4.5658
Training Epoch: 0 [704/50000]	Loss: 4.5964
Training Epoch: 0 [720/50000]	Loss: 4.6596
Training Epoch: 0 [736/50000]	Loss: 4.5650
Training Epoch: 0 [752/50000]	Loss: 4.6100
Training Epoch: 0 [768/50000]	Loss: 4.6292
Training Epoch: 0 [784/50000]	Loss: 4.6719
Training Epoch: 0 [800/50000]	Loss: 4.6231
Profile done with power limit 125W
epoch 1 train time consumed: 3.70s
Validation Epoch: 0, Average loss: 0.2888, Accuracy: 0.0128
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 125000, 'lr': 2.8090256317805292e-05, 'energy': 108.0251306464466, 'time': 2.530810377999842, 'accuracy': 0.0128, 'total_cost': 6987.700333997346}
[run job] Launching job with BS 16: and LR: 2.8090256317805292e-05 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs16+lr0.00003+pl100', 'ZEUS_COST_THRESH': '13781.842645201825', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '2.8090256317805292e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '2.8090256317805292e-05', '--power_limit', '100']
[run job] cost_ub=13781.842645201825
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs16+lr0.00003+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 2.8090256317805292e-05
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6895
Training Epoch: 0 [48/50000]	Loss: 4.7097
Training Epoch: 0 [64/50000]	Loss: 4.7043
Training Epoch: 0 [80/50000]	Loss: 4.6404
Training Epoch: 0 [96/50000]	Loss: 4.5764
Training Epoch: 0 [112/50000]	Loss: 4.6868
Training Epoch: 0 [128/50000]	Loss: 4.5299
Training Epoch: 0 [144/50000]	Loss: 4.7582
Training Epoch: 0 [160/50000]	Loss: 4.6345
Training Epoch: 0 [176/50000]	Loss: 4.7384
Training Epoch: 0 [192/50000]	Loss: 4.5438
Training Epoch: 0 [208/50000]	Loss: 4.7243
Training Epoch: 0 [224/50000]	Loss: 4.5493
Training Epoch: 0 [240/50000]	Loss: 4.5754
Training Epoch: 0 [256/50000]	Loss: 4.6559
Training Epoch: 0 [272/50000]	Loss: 4.6419
Training Epoch: 0 [288/50000]	Loss: 4.6350
Training Epoch: 0 [304/50000]	Loss: 4.5897
Training Epoch: 0 [320/50000]	Loss: 4.5754
Training Epoch: 0 [336/50000]	Loss: 4.6510
Training Epoch: 0 [352/50000]	Loss: 4.5531
Training Epoch: 0 [368/50000]	Loss: 4.6043
Training Epoch: 0 [384/50000]	Loss: 4.5300
Training Epoch: 0 [400/50000]	Loss: 4.6185
Training Epoch: 0 [416/50000]	Loss: 4.6370
Training Epoch: 0 [432/50000]	Loss: 4.5513
Training Epoch: 0 [448/50000]	Loss: 4.5531
Training Epoch: 0 [464/50000]	Loss: 4.6677
Training Epoch: 0 [480/50000]	Loss: 4.6419
Training Epoch: 0 [496/50000]	Loss: 4.6538
Training Epoch: 0 [512/50000]	Loss: 4.5257
Training Epoch: 0 [528/50000]	Loss: 4.5947
Training Epoch: 0 [544/50000]	Loss: 4.5267
Training Epoch: 0 [560/50000]	Loss: 4.6572
Training Epoch: 0 [576/50000]	Loss: 4.6494
Training Epoch: 0 [592/50000]	Loss: 4.6823
Training Epoch: 0 [608/50000]	Loss: 4.5918
Training Epoch: 0 [624/50000]	Loss: 4.7044
Training Epoch: 0 [640/50000]	Loss: 4.6650
Training Epoch: 0 [656/50000]	Loss: 4.6273
Training Epoch: 0 [672/50000]	Loss: 4.5934
Training Epoch: 0 [688/50000]	Loss: 4.5587
Training Epoch: 0 [704/50000]	Loss: 4.5933
Training Epoch: 0 [720/50000]	Loss: 4.6598
Training Epoch: 0 [736/50000]	Loss: 4.5617
Training Epoch: 0 [752/50000]	Loss: 4.6053
Training Epoch: 0 [768/50000]	Loss: 4.6455
Training Epoch: 0 [784/50000]	Loss: 4.6664
Training Epoch: 0 [800/50000]	Loss: 4.6188
Profile done with power limit 100W
epoch 1 train time consumed: 3.75s
Validation Epoch: 0, Average loss: 0.2888, Accuracy: 0.0122
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 100000, 'lr': 2.8090256317805292e-05, 'energy': 98.45412548403087, 'time': 2.586913416000243, 'accuracy': 0.0122, 'total_cost': 7486.032997039406}

[Power Profiler] with batch size 32 and learning rate 5.5901699437494744e-06
[run job] Launching job with BS 32: and LR: 5.5901699437494744e-06 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs32+lr0.00001+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '5.5901699437494744e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '5.5901699437494744e-06', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs32+lr0.00001+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 5.5901699437494744e-06
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.7211
Training Epoch: 0 [96/50016]	Loss: 4.6174
Training Epoch: 0 [128/50016]	Loss: 4.6092
Training Epoch: 0 [160/50016]	Loss: 4.7020
Training Epoch: 0 [192/50016]	Loss: 4.6796
Training Epoch: 0 [224/50016]	Loss: 4.6792
Training Epoch: 0 [256/50016]	Loss: 4.6048
Training Epoch: 0 [288/50016]	Loss: 4.6617
Training Epoch: 0 [320/50016]	Loss: 4.5821
Training Epoch: 0 [352/50016]	Loss: 4.6366
Training Epoch: 0 [384/50016]	Loss: 4.5436
Training Epoch: 0 [416/50016]	Loss: 4.6239
Training Epoch: 0 [448/50016]	Loss: 4.5971
Training Epoch: 0 [480/50016]	Loss: 4.6603
Training Epoch: 0 [512/50016]	Loss: 4.5791
Training Epoch: 0 [544/50016]	Loss: 4.5724
Training Epoch: 0 [576/50016]	Loss: 4.6890
Training Epoch: 0 [608/50016]	Loss: 4.6526
Training Epoch: 0 [640/50016]	Loss: 4.6916
Training Epoch: 0 [672/50016]	Loss: 4.5529
Training Epoch: 0 [704/50016]	Loss: 4.6154
Training Epoch: 0 [736/50016]	Loss: 4.5819
Training Epoch: 0 [768/50016]	Loss: 4.6602
Training Epoch: 0 [800/50016]	Loss: 4.6366
Training Epoch: 0 [832/50016]	Loss: 4.6445
Training Epoch: 0 [864/50016]	Loss: 4.6731
Training Epoch: 0 [896/50016]	Loss: 4.5872
Training Epoch: 0 [928/50016]	Loss: 4.5464
Training Epoch: 0 [960/50016]	Loss: 4.5818
Training Epoch: 0 [992/50016]	Loss: 4.6312
Training Epoch: 0 [1024/50016]	Loss: 4.6220
Training Epoch: 0 [1056/50016]	Loss: 4.5937
Training Epoch: 0 [1088/50016]	Loss: 4.6206
Training Epoch: 0 [1120/50016]	Loss: 4.6326
Training Epoch: 0 [1152/50016]	Loss: 4.6603
Training Epoch: 0 [1184/50016]	Loss: 4.6220
Training Epoch: 0 [1216/50016]	Loss: 4.6653
Training Epoch: 0 [1248/50016]	Loss: 4.6865
Training Epoch: 0 [1280/50016]	Loss: 4.6510
Training Epoch: 0 [1312/50016]	Loss: 4.6725
Training Epoch: 0 [1344/50016]	Loss: 4.6582
Training Epoch: 0 [1376/50016]	Loss: 4.6183
Training Epoch: 0 [1408/50016]	Loss: 4.6627
Training Epoch: 0 [1440/50016]	Loss: 4.6447
Training Epoch: 0 [1472/50016]	Loss: 4.6158
Training Epoch: 0 [1504/50016]	Loss: 4.6803
Training Epoch: 0 [1536/50016]	Loss: 4.6187
Training Epoch: 0 [1568/50016]	Loss: 4.6161
Training Epoch: 0 [1600/50016]	Loss: 4.5984
Profile done with power limit 175W
epoch 1 train time consumed: 4.65s
Validation Epoch: 0, Average loss: 0.1445, Accuracy: 0.0100
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 175000, 'lr': 5.5901699437494744e-06, 'energy': 124.94669736723206, 'time': 3.2907728649997807, 'accuracy': 0.009984025559105431, 'total_cost': 23265.056472348144}
[run job] Launching job with BS 32: and LR: 5.5901699437494744e-06 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs32+lr0.00001+pl150', 'ZEUS_COST_THRESH': '46530.11294469629', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '5.5901699437494744e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '5.5901699437494744e-06', '--power_limit', '150']
[run job] cost_ub=46530.11294469629
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs32+lr0.00001+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 5.5901699437494744e-06
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.7211
Training Epoch: 0 [96/50016]	Loss: 4.6174
Training Epoch: 0 [128/50016]	Loss: 4.6092
Training Epoch: 0 [160/50016]	Loss: 4.7020
Training Epoch: 0 [192/50016]	Loss: 4.6796
Training Epoch: 0 [224/50016]	Loss: 4.6792
Training Epoch: 0 [256/50016]	Loss: 4.6047
Training Epoch: 0 [288/50016]	Loss: 4.6617
Training Epoch: 0 [320/50016]	Loss: 4.5821
Training Epoch: 0 [352/50016]	Loss: 4.6366
Training Epoch: 0 [384/50016]	Loss: 4.5437
Training Epoch: 0 [416/50016]	Loss: 4.6239
Training Epoch: 0 [448/50016]	Loss: 4.5971
Training Epoch: 0 [480/50016]	Loss: 4.6603
Training Epoch: 0 [512/50016]	Loss: 4.5791
Training Epoch: 0 [544/50016]	Loss: 4.5725
Training Epoch: 0 [576/50016]	Loss: 4.6889
Training Epoch: 0 [608/50016]	Loss: 4.6526
Training Epoch: 0 [640/50016]	Loss: 4.6915
Training Epoch: 0 [672/50016]	Loss: 4.5530
Training Epoch: 0 [704/50016]	Loss: 4.6154
Training Epoch: 0 [736/50016]	Loss: 4.5818
Training Epoch: 0 [768/50016]	Loss: 4.6604
Training Epoch: 0 [800/50016]	Loss: 4.6367
Training Epoch: 0 [832/50016]	Loss: 4.6445
Training Epoch: 0 [864/50016]	Loss: 4.6733
Training Epoch: 0 [896/50016]	Loss: 4.5872
Training Epoch: 0 [928/50016]	Loss: 4.5462
Training Epoch: 0 [960/50016]	Loss: 4.5818
Training Epoch: 0 [992/50016]	Loss: 4.6312
Training Epoch: 0 [1024/50016]	Loss: 4.6219
Training Epoch: 0 [1056/50016]	Loss: 4.5939
Training Epoch: 0 [1088/50016]	Loss: 4.6208
Training Epoch: 0 [1120/50016]	Loss: 4.6326
Training Epoch: 0 [1152/50016]	Loss: 4.6604
Training Epoch: 0 [1184/50016]	Loss: 4.6220
Training Epoch: 0 [1216/50016]	Loss: 4.6652
Training Epoch: 0 [1248/50016]	Loss: 4.6862
Training Epoch: 0 [1280/50016]	Loss: 4.6507
Training Epoch: 0 [1312/50016]	Loss: 4.6728
Training Epoch: 0 [1344/50016]	Loss: 4.6582
Training Epoch: 0 [1376/50016]	Loss: 4.6181
Training Epoch: 0 [1408/50016]	Loss: 4.6629
Training Epoch: 0 [1440/50016]	Loss: 4.6446
Training Epoch: 0 [1472/50016]	Loss: 4.6155
Training Epoch: 0 [1504/50016]	Loss: 4.6803
Training Epoch: 0 [1536/50016]	Loss: 4.6186
Training Epoch: 0 [1568/50016]	Loss: 4.6164
Training Epoch: 0 [1600/50016]	Loss: 4.5987
Profile done with power limit 150W
epoch 1 train time consumed: 4.68s
Validation Epoch: 0, Average loss: 0.1445, Accuracy: 0.0100
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 150000, 'lr': 5.5901699437494744e-06, 'energy': 125.68911631565382, 'time': 3.276060543999847, 'accuracy': 0.009984025559105431, 'total_cost': 23163.127835427207}
[run job] Launching job with BS 32: and LR: 5.5901699437494744e-06 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs32+lr0.00001+pl125', 'ZEUS_COST_THRESH': '46326.255670854414', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '5.5901699437494744e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '5.5901699437494744e-06', '--power_limit', '125']
[run job] cost_ub=46326.255670854414
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs32+lr0.00001+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 5.5901699437494744e-06
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.7211
Training Epoch: 0 [96/50016]	Loss: 4.6174
Training Epoch: 0 [128/50016]	Loss: 4.6092
Training Epoch: 0 [160/50016]	Loss: 4.7020
Training Epoch: 0 [192/50016]	Loss: 4.6796
Training Epoch: 0 [224/50016]	Loss: 4.6792
Training Epoch: 0 [256/50016]	Loss: 4.6047
Training Epoch: 0 [288/50016]	Loss: 4.6618
Training Epoch: 0 [320/50016]	Loss: 4.5821
Training Epoch: 0 [352/50016]	Loss: 4.6366
Training Epoch: 0 [384/50016]	Loss: 4.5437
Training Epoch: 0 [416/50016]	Loss: 4.6240
Training Epoch: 0 [448/50016]	Loss: 4.5971
Training Epoch: 0 [480/50016]	Loss: 4.6603
Training Epoch: 0 [512/50016]	Loss: 4.5791
Training Epoch: 0 [544/50016]	Loss: 4.5724
Training Epoch: 0 [576/50016]	Loss: 4.6890
Training Epoch: 0 [608/50016]	Loss: 4.6527
Training Epoch: 0 [640/50016]	Loss: 4.6915
Training Epoch: 0 [672/50016]	Loss: 4.5529
Training Epoch: 0 [704/50016]	Loss: 4.6154
Training Epoch: 0 [736/50016]	Loss: 4.5818
Training Epoch: 0 [768/50016]	Loss: 4.6602
Training Epoch: 0 [800/50016]	Loss: 4.6367
Training Epoch: 0 [832/50016]	Loss: 4.6443
Training Epoch: 0 [864/50016]	Loss: 4.6733
Training Epoch: 0 [896/50016]	Loss: 4.5872
Training Epoch: 0 [928/50016]	Loss: 4.5462
Training Epoch: 0 [960/50016]	Loss: 4.5819
Training Epoch: 0 [992/50016]	Loss: 4.6313
Training Epoch: 0 [1024/50016]	Loss: 4.6219
Training Epoch: 0 [1056/50016]	Loss: 4.5941
Training Epoch: 0 [1088/50016]	Loss: 4.6208
Training Epoch: 0 [1120/50016]	Loss: 4.6327
Training Epoch: 0 [1152/50016]	Loss: 4.6605
Training Epoch: 0 [1184/50016]	Loss: 4.6219
Training Epoch: 0 [1216/50016]	Loss: 4.6651
Training Epoch: 0 [1248/50016]	Loss: 4.6862
Training Epoch: 0 [1280/50016]	Loss: 4.6510
Training Epoch: 0 [1312/50016]	Loss: 4.6729
Training Epoch: 0 [1344/50016]	Loss: 4.6585
Training Epoch: 0 [1376/50016]	Loss: 4.6182
Training Epoch: 0 [1408/50016]	Loss: 4.6629
Training Epoch: 0 [1440/50016]	Loss: 4.6448
Training Epoch: 0 [1472/50016]	Loss: 4.6156
Training Epoch: 0 [1504/50016]	Loss: 4.6804
Training Epoch: 0 [1536/50016]	Loss: 4.6186
Training Epoch: 0 [1568/50016]	Loss: 4.6165
Training Epoch: 0 [1600/50016]	Loss: 4.5989
Profile done with power limit 125W
epoch 1 train time consumed: 4.80s
Validation Epoch: 0, Average loss: 0.1445, Accuracy: 0.0100
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 125000, 'lr': 5.5901699437494744e-06, 'energy': 120.8300199180341, 'time': 3.4144483119998768, 'accuracy': 0.009984025559105431, 'total_cost': 24125.297266688038}
[run job] Launching job with BS 32: and LR: 5.5901699437494744e-06 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs32+lr0.00001+pl100', 'ZEUS_COST_THRESH': '46326.255670854414', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '5.5901699437494744e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '5.5901699437494744e-06', '--power_limit', '100']
[run job] cost_ub=46326.255670854414
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs32+lr0.00001+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 5.5901699437494744e-06
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.7211
Training Epoch: 0 [96/50016]	Loss: 4.6174
Training Epoch: 0 [128/50016]	Loss: 4.6092
Training Epoch: 0 [160/50016]	Loss: 4.7020
Training Epoch: 0 [192/50016]	Loss: 4.6796
Training Epoch: 0 [224/50016]	Loss: 4.6792
Training Epoch: 0 [256/50016]	Loss: 4.6047
Training Epoch: 0 [288/50016]	Loss: 4.6617
Training Epoch: 0 [320/50016]	Loss: 4.5821
Training Epoch: 0 [352/50016]	Loss: 4.6366
Training Epoch: 0 [384/50016]	Loss: 4.5436
Training Epoch: 0 [416/50016]	Loss: 4.6239
Training Epoch: 0 [448/50016]	Loss: 4.5971
Training Epoch: 0 [480/50016]	Loss: 4.6603
Training Epoch: 0 [512/50016]	Loss: 4.5791
Training Epoch: 0 [544/50016]	Loss: 4.5725
Training Epoch: 0 [576/50016]	Loss: 4.6890
Training Epoch: 0 [608/50016]	Loss: 4.6526
Training Epoch: 0 [640/50016]	Loss: 4.6914
Training Epoch: 0 [672/50016]	Loss: 4.5530
Training Epoch: 0 [704/50016]	Loss: 4.6154
Training Epoch: 0 [736/50016]	Loss: 4.5819
Training Epoch: 0 [768/50016]	Loss: 4.6603
Training Epoch: 0 [800/50016]	Loss: 4.6366
Training Epoch: 0 [832/50016]	Loss: 4.6444
Training Epoch: 0 [864/50016]	Loss: 4.6733
Training Epoch: 0 [896/50016]	Loss: 4.5873
Training Epoch: 0 [928/50016]	Loss: 4.5462
Training Epoch: 0 [960/50016]	Loss: 4.5818
Training Epoch: 0 [992/50016]	Loss: 4.6312
Training Epoch: 0 [1024/50016]	Loss: 4.6219
Training Epoch: 0 [1056/50016]	Loss: 4.5937
Training Epoch: 0 [1088/50016]	Loss: 4.6207
Training Epoch: 0 [1120/50016]	Loss: 4.6325
Training Epoch: 0 [1152/50016]	Loss: 4.6604
Training Epoch: 0 [1184/50016]	Loss: 4.6220
Training Epoch: 0 [1216/50016]	Loss: 4.6654
Training Epoch: 0 [1248/50016]	Loss: 4.6865
Training Epoch: 0 [1280/50016]	Loss: 4.6509
Training Epoch: 0 [1312/50016]	Loss: 4.6727
Training Epoch: 0 [1344/50016]	Loss: 4.6578
Training Epoch: 0 [1376/50016]	Loss: 4.6182
Training Epoch: 0 [1408/50016]	Loss: 4.6629
Training Epoch: 0 [1440/50016]	Loss: 4.6448
Training Epoch: 0 [1472/50016]	Loss: 4.6157
Training Epoch: 0 [1504/50016]	Loss: 4.6805
Training Epoch: 0 [1536/50016]	Loss: 4.6186
Training Epoch: 0 [1568/50016]	Loss: 4.6165
Training Epoch: 0 [1600/50016]	Loss: 4.5988
Profile done with power limit 100W
epoch 1 train time consumed: 5.22s
Validation Epoch: 0, Average loss: 0.1445, Accuracy: 0.0100
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 100000, 'lr': 5.5901699437494744e-06, 'energy': 98.70453284623137, 'time': 3.811435232999429, 'accuracy': 0.009984025559105431, 'total_cost': 26872.31550080339}

[Power Profiler] with batch size 32 and learning rate 3.972562145517676e-05
[run job] Launching job with BS 32: and LR: 3.972562145517676e-05 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs32+lr0.00004+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '3.972562145517676e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '3.972562145517676e-05', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs32+lr0.00004+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 3.972562145517676e-05
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.7213
Training Epoch: 0 [96/50016]	Loss: 4.6203
Training Epoch: 0 [128/50016]	Loss: 4.6127
Training Epoch: 0 [160/50016]	Loss: 4.7056
Training Epoch: 0 [192/50016]	Loss: 4.6787
Training Epoch: 0 [224/50016]	Loss: 4.6725
Training Epoch: 0 [256/50016]	Loss: 4.6066
Training Epoch: 0 [288/50016]	Loss: 4.6457
Training Epoch: 0 [320/50016]	Loss: 4.5911
Training Epoch: 0 [352/50016]	Loss: 4.6254
Training Epoch: 0 [384/50016]	Loss: 4.5538
Training Epoch: 0 [416/50016]	Loss: 4.6104
Training Epoch: 0 [448/50016]	Loss: 4.5851
Training Epoch: 0 [480/50016]	Loss: 4.6508
Training Epoch: 0 [512/50016]	Loss: 4.5738
Training Epoch: 0 [544/50016]	Loss: 4.5482
Training Epoch: 0 [576/50016]	Loss: 4.6693
Training Epoch: 0 [608/50016]	Loss: 4.6417
Training Epoch: 0 [640/50016]	Loss: 4.7005
Training Epoch: 0 [672/50016]	Loss: 4.5684
Training Epoch: 0 [704/50016]	Loss: 4.5917
Training Epoch: 0 [736/50016]	Loss: 4.5760
Training Epoch: 0 [768/50016]	Loss: 4.6603
Training Epoch: 0 [800/50016]	Loss: 4.6333
Training Epoch: 0 [832/50016]	Loss: 4.6531
Training Epoch: 0 [864/50016]	Loss: 4.6526
Training Epoch: 0 [896/50016]	Loss: 4.5633
Training Epoch: 0 [928/50016]	Loss: 4.5439
Training Epoch: 0 [960/50016]	Loss: 4.5705
Training Epoch: 0 [992/50016]	Loss: 4.6068
Training Epoch: 0 [1024/50016]	Loss: 4.6244
Training Epoch: 0 [1056/50016]	Loss: 4.5772
Training Epoch: 0 [1088/50016]	Loss: 4.6015
Training Epoch: 0 [1120/50016]	Loss: 4.5924
Training Epoch: 0 [1152/50016]	Loss: 4.6383
Training Epoch: 0 [1184/50016]	Loss: 4.6123
Training Epoch: 0 [1216/50016]	Loss: 4.6186
Training Epoch: 0 [1248/50016]	Loss: 4.6647
Training Epoch: 0 [1280/50016]	Loss: 4.6620
Training Epoch: 0 [1312/50016]	Loss: 4.6418
Training Epoch: 0 [1344/50016]	Loss: 4.6400
Training Epoch: 0 [1376/50016]	Loss: 4.6230
Training Epoch: 0 [1408/50016]	Loss: 4.6302
Training Epoch: 0 [1440/50016]	Loss: 4.6459
Training Epoch: 0 [1472/50016]	Loss: 4.6165
Training Epoch: 0 [1504/50016]	Loss: 4.6924
Training Epoch: 0 [1536/50016]	Loss: 4.5938
Training Epoch: 0 [1568/50016]	Loss: 4.6143
Training Epoch: 0 [1600/50016]	Loss: 4.5858
Profile done with power limit 175W
epoch 1 train time consumed: 4.79s
Validation Epoch: 0, Average loss: 0.1443, Accuracy: 0.0123
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 175000, 'lr': 3.972562145517676e-05, 'energy': 123.72049772801523, 'time': 3.3889381489998414, 'accuracy': 0.01228035143769968, 'total_cost': 19472.46204068901}
[run job] Launching job with BS 32: and LR: 3.972562145517676e-05 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs32+lr0.00004+pl150', 'ZEUS_COST_THRESH': '38944.92408137802', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '3.972562145517676e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '3.972562145517676e-05', '--power_limit', '150']
[run job] cost_ub=38944.92408137802
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs32+lr0.00004+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 3.972562145517676e-05
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.7213
Training Epoch: 0 [96/50016]	Loss: 4.6203
Training Epoch: 0 [128/50016]	Loss: 4.6127
Training Epoch: 0 [160/50016]	Loss: 4.7055
Training Epoch: 0 [192/50016]	Loss: 4.6788
Training Epoch: 0 [224/50016]	Loss: 4.6723
Training Epoch: 0 [256/50016]	Loss: 4.6064
Training Epoch: 0 [288/50016]	Loss: 4.6453
Training Epoch: 0 [320/50016]	Loss: 4.5917
Training Epoch: 0 [352/50016]	Loss: 4.6259
Training Epoch: 0 [384/50016]	Loss: 4.5551
Training Epoch: 0 [416/50016]	Loss: 4.6096
Training Epoch: 0 [448/50016]	Loss: 4.5842
Training Epoch: 0 [480/50016]	Loss: 4.6532
Training Epoch: 0 [512/50016]	Loss: 4.5757
Training Epoch: 0 [544/50016]	Loss: 4.5474
Training Epoch: 0 [576/50016]	Loss: 4.6680
Training Epoch: 0 [608/50016]	Loss: 4.6415
Training Epoch: 0 [640/50016]	Loss: 4.6956
Training Epoch: 0 [672/50016]	Loss: 4.5687
Training Epoch: 0 [704/50016]	Loss: 4.5930
Training Epoch: 0 [736/50016]	Loss: 4.5747
Training Epoch: 0 [768/50016]	Loss: 4.6579
Training Epoch: 0 [800/50016]	Loss: 4.6399
Training Epoch: 0 [832/50016]	Loss: 4.6540
Training Epoch: 0 [864/50016]	Loss: 4.6559
Training Epoch: 0 [896/50016]	Loss: 4.5608
Training Epoch: 0 [928/50016]	Loss: 4.5448
Training Epoch: 0 [960/50016]	Loss: 4.5750
Training Epoch: 0 [992/50016]	Loss: 4.6117
Training Epoch: 0 [1024/50016]	Loss: 4.6208
Training Epoch: 0 [1056/50016]	Loss: 4.5805
Training Epoch: 0 [1088/50016]	Loss: 4.6083
Training Epoch: 0 [1120/50016]	Loss: 4.5945
Training Epoch: 0 [1152/50016]	Loss: 4.6389
Training Epoch: 0 [1184/50016]	Loss: 4.6140
Training Epoch: 0 [1216/50016]	Loss: 4.6224
Training Epoch: 0 [1248/50016]	Loss: 4.6614
Training Epoch: 0 [1280/50016]	Loss: 4.6627
Training Epoch: 0 [1312/50016]	Loss: 4.6299
Training Epoch: 0 [1344/50016]	Loss: 4.6459
Training Epoch: 0 [1376/50016]	Loss: 4.6305
Training Epoch: 0 [1408/50016]	Loss: 4.6339
Training Epoch: 0 [1440/50016]	Loss: 4.6411
Training Epoch: 0 [1472/50016]	Loss: 4.6226
Training Epoch: 0 [1504/50016]	Loss: 4.6987
Training Epoch: 0 [1536/50016]	Loss: 4.5953
Training Epoch: 0 [1568/50016]	Loss: 4.6228
Training Epoch: 0 [1600/50016]	Loss: 4.5905
Profile done with power limit 150W
epoch 1 train time consumed: 4.75s
Validation Epoch: 0, Average loss: 0.1443, Accuracy: 0.0115
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 150000, 'lr': 3.972562145517676e-05, 'energy': 124.35639360394443, 'time': 3.3918819579994306, 'accuracy': 0.011481629392971246, 'total_cost': 20845.895718639207}
[run job] Launching job with BS 32: and LR: 3.972562145517676e-05 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs32+lr0.00004+pl125', 'ZEUS_COST_THRESH': '38944.92408137802', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '3.972562145517676e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '3.972562145517676e-05', '--power_limit', '125']
[run job] cost_ub=38944.92408137802
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs32+lr0.00004+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 3.972562145517676e-05
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.7213
Training Epoch: 0 [96/50016]	Loss: 4.6203
Training Epoch: 0 [128/50016]	Loss: 4.6127
Training Epoch: 0 [160/50016]	Loss: 4.7055
Training Epoch: 0 [192/50016]	Loss: 4.6788
Training Epoch: 0 [224/50016]	Loss: 4.6722
Training Epoch: 0 [256/50016]	Loss: 4.6062
Training Epoch: 0 [288/50016]	Loss: 4.6456
Training Epoch: 0 [320/50016]	Loss: 4.5915
Training Epoch: 0 [352/50016]	Loss: 4.6259
Training Epoch: 0 [384/50016]	Loss: 4.5555
Training Epoch: 0 [416/50016]	Loss: 4.6094
Training Epoch: 0 [448/50016]	Loss: 4.5832
Training Epoch: 0 [480/50016]	Loss: 4.6515
Training Epoch: 0 [512/50016]	Loss: 4.5769
Training Epoch: 0 [544/50016]	Loss: 4.5458
Training Epoch: 0 [576/50016]	Loss: 4.6664
Training Epoch: 0 [608/50016]	Loss: 4.6425
Training Epoch: 0 [640/50016]	Loss: 4.6971
Training Epoch: 0 [672/50016]	Loss: 4.5693
Training Epoch: 0 [704/50016]	Loss: 4.5938
Training Epoch: 0 [736/50016]	Loss: 4.5738
Training Epoch: 0 [768/50016]	Loss: 4.6617
Training Epoch: 0 [800/50016]	Loss: 4.6383
Training Epoch: 0 [832/50016]	Loss: 4.6591
Training Epoch: 0 [864/50016]	Loss: 4.6560
Training Epoch: 0 [896/50016]	Loss: 4.5596
Training Epoch: 0 [928/50016]	Loss: 4.5508
Training Epoch: 0 [960/50016]	Loss: 4.5778
Training Epoch: 0 [992/50016]	Loss: 4.6120
Training Epoch: 0 [1024/50016]	Loss: 4.6210
Training Epoch: 0 [1056/50016]	Loss: 4.5733
Training Epoch: 0 [1088/50016]	Loss: 4.6038
Training Epoch: 0 [1120/50016]	Loss: 4.6018
Training Epoch: 0 [1152/50016]	Loss: 4.6383
Training Epoch: 0 [1184/50016]	Loss: 4.6111
Training Epoch: 0 [1216/50016]	Loss: 4.6256
Training Epoch: 0 [1248/50016]	Loss: 4.6642
Training Epoch: 0 [1280/50016]	Loss: 4.6623
Training Epoch: 0 [1312/50016]	Loss: 4.6379
Training Epoch: 0 [1344/50016]	Loss: 4.6426
Training Epoch: 0 [1376/50016]	Loss: 4.6240
Training Epoch: 0 [1408/50016]	Loss: 4.6325
Training Epoch: 0 [1440/50016]	Loss: 4.6442
Training Epoch: 0 [1472/50016]	Loss: 4.6125
Training Epoch: 0 [1504/50016]	Loss: 4.7057
Training Epoch: 0 [1536/50016]	Loss: 4.5936
Training Epoch: 0 [1568/50016]	Loss: 4.6197
Training Epoch: 0 [1600/50016]	Loss: 4.5922
Profile done with power limit 125W
epoch 1 train time consumed: 4.65s
Validation Epoch: 0, Average loss: 0.1443, Accuracy: 0.0113
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 125000, 'lr': 3.972562145517676e-05, 'energy': 121.97757032684703, 'time': 3.278242682000382, 'accuracy': 0.011281948881789138, 'total_cost': 20506.616172332877}
[run job] Launching job with BS 32: and LR: 3.972562145517676e-05 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs32+lr0.00004+pl100', 'ZEUS_COST_THRESH': '38944.92408137802', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '3.972562145517676e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '3.972562145517676e-05', '--power_limit', '100']
[run job] cost_ub=38944.92408137802
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs32+lr0.00004+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 3.972562145517676e-05
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.7213
Training Epoch: 0 [96/50016]	Loss: 4.6203
Training Epoch: 0 [128/50016]	Loss: 4.6127
Training Epoch: 0 [160/50016]	Loss: 4.7057
Training Epoch: 0 [192/50016]	Loss: 4.6784
Training Epoch: 0 [224/50016]	Loss: 4.6724
Training Epoch: 0 [256/50016]	Loss: 4.6074
Training Epoch: 0 [288/50016]	Loss: 4.6457
Training Epoch: 0 [320/50016]	Loss: 4.5910
Training Epoch: 0 [352/50016]	Loss: 4.6235
Training Epoch: 0 [384/50016]	Loss: 4.5541
Training Epoch: 0 [416/50016]	Loss: 4.6114
Training Epoch: 0 [448/50016]	Loss: 4.5866
Training Epoch: 0 [480/50016]	Loss: 4.6512
Training Epoch: 0 [512/50016]	Loss: 4.5729
Training Epoch: 0 [544/50016]	Loss: 4.5489
Training Epoch: 0 [576/50016]	Loss: 4.6689
Training Epoch: 0 [608/50016]	Loss: 4.6422
Training Epoch: 0 [640/50016]	Loss: 4.6983
Training Epoch: 0 [672/50016]	Loss: 4.5696
Training Epoch: 0 [704/50016]	Loss: 4.5952
Training Epoch: 0 [736/50016]	Loss: 4.5715
Training Epoch: 0 [768/50016]	Loss: 4.6635
Training Epoch: 0 [800/50016]	Loss: 4.6346
Training Epoch: 0 [832/50016]	Loss: 4.6550
Training Epoch: 0 [864/50016]	Loss: 4.6471
Training Epoch: 0 [896/50016]	Loss: 4.5618
Training Epoch: 0 [928/50016]	Loss: 4.5459
Training Epoch: 0 [960/50016]	Loss: 4.5656
Training Epoch: 0 [992/50016]	Loss: 4.6128
Training Epoch: 0 [1024/50016]	Loss: 4.6217
Training Epoch: 0 [1056/50016]	Loss: 4.5808
Training Epoch: 0 [1088/50016]	Loss: 4.6045
Training Epoch: 0 [1120/50016]	Loss: 4.5937
Training Epoch: 0 [1152/50016]	Loss: 4.6371
Training Epoch: 0 [1184/50016]	Loss: 4.6070
Training Epoch: 0 [1216/50016]	Loss: 4.6154
Training Epoch: 0 [1248/50016]	Loss: 4.6671
Training Epoch: 0 [1280/50016]	Loss: 4.6624
Training Epoch: 0 [1312/50016]	Loss: 4.6425
Training Epoch: 0 [1344/50016]	Loss: 4.6415
Training Epoch: 0 [1376/50016]	Loss: 4.6320
Training Epoch: 0 [1408/50016]	Loss: 4.6336
Training Epoch: 0 [1440/50016]	Loss: 4.6542
Training Epoch: 0 [1472/50016]	Loss: 4.6110
Training Epoch: 0 [1504/50016]	Loss: 4.7027
Training Epoch: 0 [1536/50016]	Loss: 4.6011
Training Epoch: 0 [1568/50016]	Loss: 4.6133
Training Epoch: 0 [1600/50016]	Loss: 4.6031
Profile done with power limit 100W
epoch 1 train time consumed: 5.23s
Validation Epoch: 0, Average loss: 0.1442, Accuracy: 0.0118
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 100000, 'lr': 3.972562145517676e-05, 'energy': 98.44390240441486, 'time': 3.7920772500001476, 'accuracy': 0.01178115015974441, 'total_cost': 22657.81244530416}

[Power Profiler] with batch size 64 and learning rate 7.905694150420949e-06
[run job] Launching job with BS 64: and LR: 7.905694150420949e-06 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs64+lr0.00001+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '7.905694150420949e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '7.905694150420949e-06', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs64+lr0.00001+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 7.905694150420949e-06
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.6162
Training Epoch: 0 [192/50048]	Loss: 4.6779
Training Epoch: 0 [256/50048]	Loss: 4.6288
Training Epoch: 0 [320/50048]	Loss: 4.6309
Training Epoch: 0 [384/50048]	Loss: 4.5915
Training Epoch: 0 [448/50048]	Loss: 4.6345
Training Epoch: 0 [512/50048]	Loss: 4.6123
Training Epoch: 0 [576/50048]	Loss: 4.6162
Training Epoch: 0 [640/50048]	Loss: 4.6717
Training Epoch: 0 [704/50048]	Loss: 4.5960
Training Epoch: 0 [768/50048]	Loss: 4.6104
Training Epoch: 0 [832/50048]	Loss: 4.6240
Training Epoch: 0 [896/50048]	Loss: 4.6299
Training Epoch: 0 [960/50048]	Loss: 4.5668
Training Epoch: 0 [1024/50048]	Loss: 4.6412
Training Epoch: 0 [1088/50048]	Loss: 4.6023
Training Epoch: 0 [1152/50048]	Loss: 4.6546
Training Epoch: 0 [1216/50048]	Loss: 4.6473
Training Epoch: 0 [1280/50048]	Loss: 4.6652
Training Epoch: 0 [1344/50048]	Loss: 4.6829
Training Epoch: 0 [1408/50048]	Loss: 4.6709
Training Epoch: 0 [1472/50048]	Loss: 4.6407
Training Epoch: 0 [1536/50048]	Loss: 4.6341
Training Epoch: 0 [1600/50048]	Loss: 4.6055
Training Epoch: 0 [1664/50048]	Loss: 4.6173
Training Epoch: 0 [1728/50048]	Loss: 4.6175
Training Epoch: 0 [1792/50048]	Loss: 4.6420
Training Epoch: 0 [1856/50048]	Loss: 4.7114
Training Epoch: 0 [1920/50048]	Loss: 4.6232
Training Epoch: 0 [1984/50048]	Loss: 4.6073
Training Epoch: 0 [2048/50048]	Loss: 4.6790
Training Epoch: 0 [2112/50048]	Loss: 4.6432
Training Epoch: 0 [2176/50048]	Loss: 4.6380
Training Epoch: 0 [2240/50048]	Loss: 4.6214
Training Epoch: 0 [2304/50048]	Loss: 4.6044
Training Epoch: 0 [2368/50048]	Loss: 4.6454
Training Epoch: 0 [2432/50048]	Loss: 4.6104
Training Epoch: 0 [2496/50048]	Loss: 4.6143
Training Epoch: 0 [2560/50048]	Loss: 4.5971
Training Epoch: 0 [2624/50048]	Loss: 4.6183
Training Epoch: 0 [2688/50048]	Loss: 4.5845
Training Epoch: 0 [2752/50048]	Loss: 4.6190
Training Epoch: 0 [2816/50048]	Loss: 4.6071
Training Epoch: 0 [2880/50048]	Loss: 4.6380
Training Epoch: 0 [2944/50048]	Loss: 4.6238
Training Epoch: 0 [3008/50048]	Loss: 4.6407
Training Epoch: 0 [3072/50048]	Loss: 4.6281
Training Epoch: 0 [3136/50048]	Loss: 4.6963
Training Epoch: 0 [3200/50048]	Loss: 4.5706
Profile done with power limit 175W
epoch 1 train time consumed: 7.29s
Validation Epoch: 0, Average loss: 0.0722, Accuracy: 0.0102
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 175000, 'lr': 7.905694150420949e-06, 'energy': 135.99288131081485, 'time': 5.384508112999356, 'accuracy': 0.010151273885350318, 'total_cost': 74616.81746115503}
[run job] Launching job with BS 64: and LR: 7.905694150420949e-06 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs64+lr0.00001+pl150', 'ZEUS_COST_THRESH': '149233.63492231007', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '7.905694150420949e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '7.905694150420949e-06', '--power_limit', '150']
[run job] cost_ub=149233.63492231007
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs64+lr0.00001+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 7.905694150420949e-06
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.6162
Training Epoch: 0 [192/50048]	Loss: 4.6779
Training Epoch: 0 [256/50048]	Loss: 4.6288
Training Epoch: 0 [320/50048]	Loss: 4.6309
Training Epoch: 0 [384/50048]	Loss: 4.5916
Training Epoch: 0 [448/50048]	Loss: 4.6345
Training Epoch: 0 [512/50048]	Loss: 4.6123
Training Epoch: 0 [576/50048]	Loss: 4.6162
Training Epoch: 0 [640/50048]	Loss: 4.6717
Training Epoch: 0 [704/50048]	Loss: 4.5961
Training Epoch: 0 [768/50048]	Loss: 4.6104
Training Epoch: 0 [832/50048]	Loss: 4.6240
Training Epoch: 0 [896/50048]	Loss: 4.6298
Training Epoch: 0 [960/50048]	Loss: 4.5668
Training Epoch: 0 [1024/50048]	Loss: 4.6412
Training Epoch: 0 [1088/50048]	Loss: 4.6022
Training Epoch: 0 [1152/50048]	Loss: 4.6546
Training Epoch: 0 [1216/50048]	Loss: 4.6472
Training Epoch: 0 [1280/50048]	Loss: 4.6653
Training Epoch: 0 [1344/50048]	Loss: 4.6830
Training Epoch: 0 [1408/50048]	Loss: 4.6709
Training Epoch: 0 [1472/50048]	Loss: 4.6405
Training Epoch: 0 [1536/50048]	Loss: 4.6341
Training Epoch: 0 [1600/50048]	Loss: 4.6055
Training Epoch: 0 [1664/50048]	Loss: 4.6174
Training Epoch: 0 [1728/50048]	Loss: 4.6175
Training Epoch: 0 [1792/50048]	Loss: 4.6421
Training Epoch: 0 [1856/50048]	Loss: 4.7114
Training Epoch: 0 [1920/50048]	Loss: 4.6231
Training Epoch: 0 [1984/50048]	Loss: 4.6071
Training Epoch: 0 [2048/50048]	Loss: 4.6790
Training Epoch: 0 [2112/50048]	Loss: 4.6431
Training Epoch: 0 [2176/50048]	Loss: 4.6380
Training Epoch: 0 [2240/50048]	Loss: 4.6213
Training Epoch: 0 [2304/50048]	Loss: 4.6046
Training Epoch: 0 [2368/50048]	Loss: 4.6454
Training Epoch: 0 [2432/50048]	Loss: 4.6099
Training Epoch: 0 [2496/50048]	Loss: 4.6144
Training Epoch: 0 [2560/50048]	Loss: 4.5967
Training Epoch: 0 [2624/50048]	Loss: 4.6181
Training Epoch: 0 [2688/50048]	Loss: 4.5844
Training Epoch: 0 [2752/50048]	Loss: 4.6193
Training Epoch: 0 [2816/50048]	Loss: 4.6068
Training Epoch: 0 [2880/50048]	Loss: 4.6375
Training Epoch: 0 [2944/50048]	Loss: 4.6234
Training Epoch: 0 [3008/50048]	Loss: 4.6403
Training Epoch: 0 [3072/50048]	Loss: 4.6280
Training Epoch: 0 [3136/50048]	Loss: 4.6962
Training Epoch: 0 [3200/50048]	Loss: 4.5707
Profile done with power limit 150W
epoch 1 train time consumed: 7.41s
Validation Epoch: 0, Average loss: 0.0722, Accuracy: 0.0101
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 150000, 'lr': 7.905694150420949e-06, 'energy': 134.45144340231482, 'time': 5.468797582000661, 'accuracy': 0.010051751592356687, 'total_cost': 76523.54644351103}
[run job] Launching job with BS 64: and LR: 7.905694150420949e-06 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs64+lr0.00001+pl125', 'ZEUS_COST_THRESH': '149233.63492231007', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '7.905694150420949e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '7.905694150420949e-06', '--power_limit', '125']
[run job] cost_ub=149233.63492231007
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs64+lr0.00001+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 7.905694150420949e-06
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.6162
Training Epoch: 0 [192/50048]	Loss: 4.6779
Training Epoch: 0 [256/50048]	Loss: 4.6288
Training Epoch: 0 [320/50048]	Loss: 4.6309
Training Epoch: 0 [384/50048]	Loss: 4.5915
Training Epoch: 0 [448/50048]	Loss: 4.6346
Training Epoch: 0 [512/50048]	Loss: 4.6122
Training Epoch: 0 [576/50048]	Loss: 4.6162
Training Epoch: 0 [640/50048]	Loss: 4.6716
Training Epoch: 0 [704/50048]	Loss: 4.5960
Training Epoch: 0 [768/50048]	Loss: 4.6105
Training Epoch: 0 [832/50048]	Loss: 4.6240
Training Epoch: 0 [896/50048]	Loss: 4.6298
Training Epoch: 0 [960/50048]	Loss: 4.5668
Training Epoch: 0 [1024/50048]	Loss: 4.6412
Training Epoch: 0 [1088/50048]	Loss: 4.6023
Training Epoch: 0 [1152/50048]	Loss: 4.6546
Training Epoch: 0 [1216/50048]	Loss: 4.6472
Training Epoch: 0 [1280/50048]	Loss: 4.6653
Training Epoch: 0 [1344/50048]	Loss: 4.6830
Training Epoch: 0 [1408/50048]	Loss: 4.6710
Training Epoch: 0 [1472/50048]	Loss: 4.6405
Training Epoch: 0 [1536/50048]	Loss: 4.6339
Training Epoch: 0 [1600/50048]	Loss: 4.6054
Training Epoch: 0 [1664/50048]	Loss: 4.6173
Training Epoch: 0 [1728/50048]	Loss: 4.6174
Training Epoch: 0 [1792/50048]	Loss: 4.6420
Training Epoch: 0 [1856/50048]	Loss: 4.7114
Training Epoch: 0 [1920/50048]	Loss: 4.6231
Training Epoch: 0 [1984/50048]	Loss: 4.6072
Training Epoch: 0 [2048/50048]	Loss: 4.6790
Training Epoch: 0 [2112/50048]	Loss: 4.6430
Training Epoch: 0 [2176/50048]	Loss: 4.6384
Training Epoch: 0 [2240/50048]	Loss: 4.6215
Training Epoch: 0 [2304/50048]	Loss: 4.6048
Training Epoch: 0 [2368/50048]	Loss: 4.6453
Training Epoch: 0 [2432/50048]	Loss: 4.6103
Training Epoch: 0 [2496/50048]	Loss: 4.6149
Training Epoch: 0 [2560/50048]	Loss: 4.5973
Training Epoch: 0 [2624/50048]	Loss: 4.6176
Training Epoch: 0 [2688/50048]	Loss: 4.5840
Training Epoch: 0 [2752/50048]	Loss: 4.6191
Training Epoch: 0 [2816/50048]	Loss: 4.6072
Training Epoch: 0 [2880/50048]	Loss: 4.6374
Training Epoch: 0 [2944/50048]	Loss: 4.6238
Training Epoch: 0 [3008/50048]	Loss: 4.6407
Training Epoch: 0 [3072/50048]	Loss: 4.6283
Training Epoch: 0 [3136/50048]	Loss: 4.6961
Training Epoch: 0 [3200/50048]	Loss: 4.5708
Profile done with power limit 125W
epoch 1 train time consumed: 7.60s
Validation Epoch: 0, Average loss: 0.0722, Accuracy: 0.0101
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 125000, 'lr': 7.905694150420949e-06, 'energy': 123.00376541853791, 'time': 5.609262044999923, 'accuracy': 0.010051751592356687, 'total_cost': 78441.6390231705}
[run job] Launching job with BS 64: and LR: 7.905694150420949e-06 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs64+lr0.00001+pl100', 'ZEUS_COST_THRESH': '149233.63492231007', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '7.905694150420949e-06', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '7.905694150420949e-06', '--power_limit', '100']
[run job] cost_ub=149233.63492231007
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs64+lr0.00001+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 7.905694150420949e-06
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.6162
Training Epoch: 0 [192/50048]	Loss: 4.6779
Training Epoch: 0 [256/50048]	Loss: 4.6289
Training Epoch: 0 [320/50048]	Loss: 4.6309
Training Epoch: 0 [384/50048]	Loss: 4.5915
Training Epoch: 0 [448/50048]	Loss: 4.6346
Training Epoch: 0 [512/50048]	Loss: 4.6123
Training Epoch: 0 [576/50048]	Loss: 4.6162
Training Epoch: 0 [640/50048]	Loss: 4.6716
Training Epoch: 0 [704/50048]	Loss: 4.5960
Training Epoch: 0 [768/50048]	Loss: 4.6104
Training Epoch: 0 [832/50048]	Loss: 4.6241
Training Epoch: 0 [896/50048]	Loss: 4.6299
Training Epoch: 0 [960/50048]	Loss: 4.5667
Training Epoch: 0 [1024/50048]	Loss: 4.6412
Training Epoch: 0 [1088/50048]	Loss: 4.6022
Training Epoch: 0 [1152/50048]	Loss: 4.6546
Training Epoch: 0 [1216/50048]	Loss: 4.6472
Training Epoch: 0 [1280/50048]	Loss: 4.6653
Training Epoch: 0 [1344/50048]	Loss: 4.6830
Training Epoch: 0 [1408/50048]	Loss: 4.6709
Training Epoch: 0 [1472/50048]	Loss: 4.6406
Training Epoch: 0 [1536/50048]	Loss: 4.6341
Training Epoch: 0 [1600/50048]	Loss: 4.6054
Training Epoch: 0 [1664/50048]	Loss: 4.6174
Training Epoch: 0 [1728/50048]	Loss: 4.6176
Training Epoch: 0 [1792/50048]	Loss: 4.6420
Training Epoch: 0 [1856/50048]	Loss: 4.7114
Training Epoch: 0 [1920/50048]	Loss: 4.6232
Training Epoch: 0 [1984/50048]	Loss: 4.6072
Training Epoch: 0 [2048/50048]	Loss: 4.6790
Training Epoch: 0 [2112/50048]	Loss: 4.6430
Training Epoch: 0 [2176/50048]	Loss: 4.6383
Training Epoch: 0 [2240/50048]	Loss: 4.6213
Training Epoch: 0 [2304/50048]	Loss: 4.6046
Training Epoch: 0 [2368/50048]	Loss: 4.6452
Training Epoch: 0 [2432/50048]	Loss: 4.6101
Training Epoch: 0 [2496/50048]	Loss: 4.6145
Training Epoch: 0 [2560/50048]	Loss: 4.5968
Training Epoch: 0 [2624/50048]	Loss: 4.6183
Training Epoch: 0 [2688/50048]	Loss: 4.5846
Training Epoch: 0 [2752/50048]	Loss: 4.6192
Training Epoch: 0 [2816/50048]	Loss: 4.6070
Training Epoch: 0 [2880/50048]	Loss: 4.6377
Training Epoch: 0 [2944/50048]	Loss: 4.6237
Training Epoch: 0 [3008/50048]	Loss: 4.6406
Training Epoch: 0 [3072/50048]	Loss: 4.6281
Training Epoch: 0 [3136/50048]	Loss: 4.6963
Training Epoch: 0 [3200/50048]	Loss: 4.5705
Profile done with power limit 100W
epoch 1 train time consumed: 8.77s
Validation Epoch: 0, Average loss: 0.0722, Accuracy: 0.0101
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 100000, 'lr': 7.905694150420949e-06, 'energy': 97.56247761580696, 'time': 6.632539956999608, 'accuracy': 0.010051751592356687, 'total_cost': 92599.1885128214}

[Power Profiler] with batch size 64 and learning rate 5.6180512635610584e-05
[run job] Launching job with BS 64: and LR: 5.6180512635610584e-05 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs64+lr0.00006+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '5.6180512635610584e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '5.6180512635610584e-05', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs64+lr0.00006+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 5.6180512635610584e-05
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.6164
Training Epoch: 0 [192/50048]	Loss: 4.6779
Training Epoch: 0 [256/50048]	Loss: 4.6292
Training Epoch: 0 [320/50048]	Loss: 4.6168
Training Epoch: 0 [384/50048]	Loss: 4.5954
Training Epoch: 0 [448/50048]	Loss: 4.6216
Training Epoch: 0 [512/50048]	Loss: 4.6063
Training Epoch: 0 [576/50048]	Loss: 4.5993
Training Epoch: 0 [640/50048]	Loss: 4.6680
Training Epoch: 0 [704/50048]	Loss: 4.5988
Training Epoch: 0 [768/50048]	Loss: 4.5957
Training Epoch: 0 [832/50048]	Loss: 4.6348
Training Epoch: 0 [896/50048]	Loss: 4.6080
Training Epoch: 0 [960/50048]	Loss: 4.5650
Training Epoch: 0 [1024/50048]	Loss: 4.6278
Training Epoch: 0 [1088/50048]	Loss: 4.5944
Training Epoch: 0 [1152/50048]	Loss: 4.6065
Training Epoch: 0 [1216/50048]	Loss: 4.6207
Training Epoch: 0 [1280/50048]	Loss: 4.6542
Training Epoch: 0 [1344/50048]	Loss: 4.6492
Training Epoch: 0 [1408/50048]	Loss: 4.6570
Training Epoch: 0 [1472/50048]	Loss: 4.6440
Training Epoch: 0 [1536/50048]	Loss: 4.6300
Training Epoch: 0 [1600/50048]	Loss: 4.5930
Training Epoch: 0 [1664/50048]	Loss: 4.6383
Training Epoch: 0 [1728/50048]	Loss: 4.5963
Training Epoch: 0 [1792/50048]	Loss: 4.6217
Training Epoch: 0 [1856/50048]	Loss: 4.6831
Training Epoch: 0 [1920/50048]	Loss: 4.6173
Training Epoch: 0 [1984/50048]	Loss: 4.6032
Training Epoch: 0 [2048/50048]	Loss: 4.6255
Training Epoch: 0 [2112/50048]	Loss: 4.6671
Training Epoch: 0 [2176/50048]	Loss: 4.6109
Training Epoch: 0 [2240/50048]	Loss: 4.6089
Training Epoch: 0 [2304/50048]	Loss: 4.5937
Training Epoch: 0 [2368/50048]	Loss: 4.6138
Training Epoch: 0 [2432/50048]	Loss: 4.6170
Training Epoch: 0 [2496/50048]	Loss: 4.5815
Training Epoch: 0 [2560/50048]	Loss: 4.5863
Training Epoch: 0 [2624/50048]	Loss: 4.6078
Training Epoch: 0 [2688/50048]	Loss: 4.5812
Training Epoch: 0 [2752/50048]	Loss: 4.5958
Training Epoch: 0 [2816/50048]	Loss: 4.5817
Training Epoch: 0 [2880/50048]	Loss: 4.5867
Training Epoch: 0 [2944/50048]	Loss: 4.6251
Training Epoch: 0 [3008/50048]	Loss: 4.5691
Training Epoch: 0 [3072/50048]	Loss: 4.5987
Training Epoch: 0 [3136/50048]	Loss: 4.6463
Training Epoch: 0 [3200/50048]	Loss: 4.5257
Profile done with power limit 175W
epoch 1 train time consumed: 7.30s
Validation Epoch: 0, Average loss: 0.0720, Accuracy: 0.0111
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 175000, 'lr': 5.6180512635610584e-05, 'energy': 135.97671070460044, 'time': 5.346900024000206, 'accuracy': 0.011146496815286623, 'total_cost': 67482.65066798984}
[run job] Launching job with BS 64: and LR: 5.6180512635610584e-05 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs64+lr0.00006+pl150', 'ZEUS_COST_THRESH': '134965.3013359797', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '5.6180512635610584e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '5.6180512635610584e-05', '--power_limit', '150']
[run job] cost_ub=134965.3013359797
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs64+lr0.00006+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 5.6180512635610584e-05
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.6164
Training Epoch: 0 [192/50048]	Loss: 4.6779
Training Epoch: 0 [256/50048]	Loss: 4.6292
Training Epoch: 0 [320/50048]	Loss: 4.6168
Training Epoch: 0 [384/50048]	Loss: 4.5952
Training Epoch: 0 [448/50048]	Loss: 4.6214
Training Epoch: 0 [512/50048]	Loss: 4.6068
Training Epoch: 0 [576/50048]	Loss: 4.6004
Training Epoch: 0 [640/50048]	Loss: 4.6686
Training Epoch: 0 [704/50048]	Loss: 4.5971
Training Epoch: 0 [768/50048]	Loss: 4.6001
Training Epoch: 0 [832/50048]	Loss: 4.6349
Training Epoch: 0 [896/50048]	Loss: 4.6085
Training Epoch: 0 [960/50048]	Loss: 4.5646
Training Epoch: 0 [1024/50048]	Loss: 4.6302
Training Epoch: 0 [1088/50048]	Loss: 4.5919
Training Epoch: 0 [1152/50048]	Loss: 4.6074
Training Epoch: 0 [1216/50048]	Loss: 4.6196
Training Epoch: 0 [1280/50048]	Loss: 4.6548
Training Epoch: 0 [1344/50048]	Loss: 4.6526
Training Epoch: 0 [1408/50048]	Loss: 4.6579
Training Epoch: 0 [1472/50048]	Loss: 4.6429
Training Epoch: 0 [1536/50048]	Loss: 4.6267
Training Epoch: 0 [1600/50048]	Loss: 4.5949
Training Epoch: 0 [1664/50048]	Loss: 4.6364
Training Epoch: 0 [1728/50048]	Loss: 4.5977
Training Epoch: 0 [1792/50048]	Loss: 4.6259
Training Epoch: 0 [1856/50048]	Loss: 4.6843
Training Epoch: 0 [1920/50048]	Loss: 4.6171
Training Epoch: 0 [1984/50048]	Loss: 4.5983
Training Epoch: 0 [2048/50048]	Loss: 4.6185
Training Epoch: 0 [2112/50048]	Loss: 4.6651
Training Epoch: 0 [2176/50048]	Loss: 4.6147
Training Epoch: 0 [2240/50048]	Loss: 4.6107
Training Epoch: 0 [2304/50048]	Loss: 4.5933
Training Epoch: 0 [2368/50048]	Loss: 4.6263
Training Epoch: 0 [2432/50048]	Loss: 4.6126
Training Epoch: 0 [2496/50048]	Loss: 4.5692
Training Epoch: 0 [2560/50048]	Loss: 4.5824
Training Epoch: 0 [2624/50048]	Loss: 4.6016
Training Epoch: 0 [2688/50048]	Loss: 4.5840
Training Epoch: 0 [2752/50048]	Loss: 4.5865
Training Epoch: 0 [2816/50048]	Loss: 4.5796
Training Epoch: 0 [2880/50048]	Loss: 4.5941
Training Epoch: 0 [2944/50048]	Loss: 4.6230
Training Epoch: 0 [3008/50048]	Loss: 4.5671
Training Epoch: 0 [3072/50048]	Loss: 4.6028
Training Epoch: 0 [3136/50048]	Loss: 4.6595
Training Epoch: 0 [3200/50048]	Loss: 4.5390
Profile done with power limit 150W
epoch 1 train time consumed: 7.32s
Validation Epoch: 0, Average loss: 0.0720, Accuracy: 0.0119
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 150000, 'lr': 5.6180512635610584e-05, 'energy': 136.05120836833643, 'time': 5.377354012000069, 'accuracy': 0.01194267515923567, 'total_cost': 63340.66626950343}
[run job] Launching job with BS 64: and LR: 5.6180512635610584e-05 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs64+lr0.00006+pl125', 'ZEUS_COST_THRESH': '126681.33253900686', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '5.6180512635610584e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '5.6180512635610584e-05', '--power_limit', '125']
[run job] cost_ub=126681.33253900686
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs64+lr0.00006+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 5.6180512635610584e-05
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.6164
Training Epoch: 0 [192/50048]	Loss: 4.6779
Training Epoch: 0 [256/50048]	Loss: 4.6292
Training Epoch: 0 [320/50048]	Loss: 4.6166
Training Epoch: 0 [384/50048]	Loss: 4.5951
Training Epoch: 0 [448/50048]	Loss: 4.6212
Training Epoch: 0 [512/50048]	Loss: 4.6054
Training Epoch: 0 [576/50048]	Loss: 4.5993
Training Epoch: 0 [640/50048]	Loss: 4.6679
Training Epoch: 0 [704/50048]	Loss: 4.5986
Training Epoch: 0 [768/50048]	Loss: 4.5947
Training Epoch: 0 [832/50048]	Loss: 4.6362
Training Epoch: 0 [896/50048]	Loss: 4.6079
Training Epoch: 0 [960/50048]	Loss: 4.5676
Training Epoch: 0 [1024/50048]	Loss: 4.6273
Training Epoch: 0 [1088/50048]	Loss: 4.5957
Training Epoch: 0 [1152/50048]	Loss: 4.6088
Training Epoch: 0 [1216/50048]	Loss: 4.6212
Training Epoch: 0 [1280/50048]	Loss: 4.6510
Training Epoch: 0 [1344/50048]	Loss: 4.6515
Training Epoch: 0 [1408/50048]	Loss: 4.6577
Training Epoch: 0 [1472/50048]	Loss: 4.6424
Training Epoch: 0 [1536/50048]	Loss: 4.6295
Training Epoch: 0 [1600/50048]	Loss: 4.5928
Training Epoch: 0 [1664/50048]	Loss: 4.6356
Training Epoch: 0 [1728/50048]	Loss: 4.5948
Training Epoch: 0 [1792/50048]	Loss: 4.6170
Training Epoch: 0 [1856/50048]	Loss: 4.6826
Training Epoch: 0 [1920/50048]	Loss: 4.6186
Training Epoch: 0 [1984/50048]	Loss: 4.6030
Training Epoch: 0 [2048/50048]	Loss: 4.6252
Training Epoch: 0 [2112/50048]	Loss: 4.6665
Training Epoch: 0 [2176/50048]	Loss: 4.6120
Training Epoch: 0 [2240/50048]	Loss: 4.6061
Training Epoch: 0 [2304/50048]	Loss: 4.5944
Training Epoch: 0 [2368/50048]	Loss: 4.6188
Training Epoch: 0 [2432/50048]	Loss: 4.6202
Training Epoch: 0 [2496/50048]	Loss: 4.5856
Training Epoch: 0 [2560/50048]	Loss: 4.5853
Training Epoch: 0 [2624/50048]	Loss: 4.6075
Training Epoch: 0 [2688/50048]	Loss: 4.5812
Training Epoch: 0 [2752/50048]	Loss: 4.5959
Training Epoch: 0 [2816/50048]	Loss: 4.5848
Training Epoch: 0 [2880/50048]	Loss: 4.5830
Training Epoch: 0 [2944/50048]	Loss: 4.6255
Training Epoch: 0 [3008/50048]	Loss: 4.5653
Training Epoch: 0 [3072/50048]	Loss: 4.6022
Training Epoch: 0 [3136/50048]	Loss: 4.6577
Training Epoch: 0 [3200/50048]	Loss: 4.5374
Profile done with power limit 125W
epoch 1 train time consumed: 7.59s
Validation Epoch: 0, Average loss: 0.0720, Accuracy: 0.0116
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 125000, 'lr': 5.6180512635610584e-05, 'energy': 122.88531410260008, 'time': 5.633039928000471, 'accuracy': 0.011644108280254776, 'total_cost': 67999.86525646591}
[run job] Launching job with BS 64: and LR: 5.6180512635610584e-05 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs64+lr0.00006+pl100', 'ZEUS_COST_THRESH': '126681.33253900686', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '5.6180512635610584e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '5.6180512635610584e-05', '--power_limit', '100']
[run job] cost_ub=126681.33253900686
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs64+lr0.00006+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 5.6180512635610584e-05
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.6164
Training Epoch: 0 [192/50048]	Loss: 4.6779
Training Epoch: 0 [256/50048]	Loss: 4.6292
Training Epoch: 0 [320/50048]	Loss: 4.6168
Training Epoch: 0 [384/50048]	Loss: 4.5953
Training Epoch: 0 [448/50048]	Loss: 4.6213
Training Epoch: 0 [512/50048]	Loss: 4.6064
Training Epoch: 0 [576/50048]	Loss: 4.5996
Training Epoch: 0 [640/50048]	Loss: 4.6675
Training Epoch: 0 [704/50048]	Loss: 4.5971
Training Epoch: 0 [768/50048]	Loss: 4.5991
Training Epoch: 0 [832/50048]	Loss: 4.6337
Training Epoch: 0 [896/50048]	Loss: 4.6092
Training Epoch: 0 [960/50048]	Loss: 4.5639
Training Epoch: 0 [1024/50048]	Loss: 4.6290
Training Epoch: 0 [1088/50048]	Loss: 4.5940
Training Epoch: 0 [1152/50048]	Loss: 4.6081
Training Epoch: 0 [1216/50048]	Loss: 4.6224
Training Epoch: 0 [1280/50048]	Loss: 4.6568
Training Epoch: 0 [1344/50048]	Loss: 4.6499
Training Epoch: 0 [1408/50048]	Loss: 4.6587
Training Epoch: 0 [1472/50048]	Loss: 4.6401
Training Epoch: 0 [1536/50048]	Loss: 4.6302
Training Epoch: 0 [1600/50048]	Loss: 4.5901
Training Epoch: 0 [1664/50048]	Loss: 4.6382
Training Epoch: 0 [1728/50048]	Loss: 4.6011
Training Epoch: 0 [1792/50048]	Loss: 4.6221
Training Epoch: 0 [1856/50048]	Loss: 4.6801
Training Epoch: 0 [1920/50048]	Loss: 4.6184
Training Epoch: 0 [1984/50048]	Loss: 4.5967
Training Epoch: 0 [2048/50048]	Loss: 4.6216
Training Epoch: 0 [2112/50048]	Loss: 4.6629
Training Epoch: 0 [2176/50048]	Loss: 4.6100
Training Epoch: 0 [2240/50048]	Loss: 4.6113
Training Epoch: 0 [2304/50048]	Loss: 4.5939
Training Epoch: 0 [2368/50048]	Loss: 4.6253
Training Epoch: 0 [2432/50048]	Loss: 4.6166
Training Epoch: 0 [2496/50048]	Loss: 4.5750
Training Epoch: 0 [2560/50048]	Loss: 4.5761
Training Epoch: 0 [2624/50048]	Loss: 4.5962
Training Epoch: 0 [2688/50048]	Loss: 4.5843
Training Epoch: 0 [2752/50048]	Loss: 4.5934
Training Epoch: 0 [2816/50048]	Loss: 4.5793
Training Epoch: 0 [2880/50048]	Loss: 4.6003
Training Epoch: 0 [2944/50048]	Loss: 4.6312
Training Epoch: 0 [3008/50048]	Loss: 4.5651
Training Epoch: 0 [3072/50048]	Loss: 4.6040
Training Epoch: 0 [3136/50048]	Loss: 4.6458
Training Epoch: 0 [3200/50048]	Loss: 4.5382
Profile done with power limit 100W
epoch 1 train time consumed: 8.78s
Validation Epoch: 0, Average loss: 0.0720, Accuracy: 0.0114
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 100000, 'lr': 5.6180512635610584e-05, 'energy': 98.09147957815537, 'time': 6.611415551999926, 'accuracy': 0.011445063694267515, 'total_cost': 81069.56823438176}

[Power Profiler] with batch size 128 and learning rate 1.1180339887498949e-05
[run job] Launching job with BS 128: and LR: 1.1180339887498949e-05 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs128+lr0.00001+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '1.1180339887498949e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '1.1180339887498949e-05', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs128+lr0.00001+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 1.1180339887498949e-05
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.6650
Training Epoch: 0 [384/50048]	Loss: 4.5993
Training Epoch: 0 [512/50048]	Loss: 4.6184
Training Epoch: 0 [640/50048]	Loss: 4.6456
Training Epoch: 0 [768/50048]	Loss: 4.6004
Training Epoch: 0 [896/50048]	Loss: 4.6326
Training Epoch: 0 [1024/50048]	Loss: 4.5937
Training Epoch: 0 [1152/50048]	Loss: 4.6232
Training Epoch: 0 [1280/50048]	Loss: 4.6559
Training Epoch: 0 [1408/50048]	Loss: 4.6570
Training Epoch: 0 [1536/50048]	Loss: 4.6246
Training Epoch: 0 [1664/50048]	Loss: 4.6041
Training Epoch: 0 [1792/50048]	Loss: 4.6385
Training Epoch: 0 [1920/50048]	Loss: 4.6654
Training Epoch: 0 [2048/50048]	Loss: 4.6491
Training Epoch: 0 [2176/50048]	Loss: 4.6444
Training Epoch: 0 [2304/50048]	Loss: 4.6232
Training Epoch: 0 [2432/50048]	Loss: 4.6341
Training Epoch: 0 [2560/50048]	Loss: 4.6155
Training Epoch: 0 [2688/50048]	Loss: 4.6046
Training Epoch: 0 [2816/50048]	Loss: 4.6217
Training Epoch: 0 [2944/50048]	Loss: 4.6169
Training Epoch: 0 [3072/50048]	Loss: 4.6302
Training Epoch: 0 [3200/50048]	Loss: 4.6351
Training Epoch: 0 [3328/50048]	Loss: 4.6171
Training Epoch: 0 [3456/50048]	Loss: 4.6279
Training Epoch: 0 [3584/50048]	Loss: 4.6221
Training Epoch: 0 [3712/50048]	Loss: 4.6031
Training Epoch: 0 [3840/50048]	Loss: 4.6293
Training Epoch: 0 [3968/50048]	Loss: 4.6061
Training Epoch: 0 [4096/50048]	Loss: 4.6191
Training Epoch: 0 [4224/50048]	Loss: 4.6156
Training Epoch: 0 [4352/50048]	Loss: 4.6309
Training Epoch: 0 [4480/50048]	Loss: 4.6347
Training Epoch: 0 [4608/50048]	Loss: 4.6748
Training Epoch: 0 [4736/50048]	Loss: 4.5949
Training Epoch: 0 [4864/50048]	Loss: 4.6222
Training Epoch: 0 [4992/50048]	Loss: 4.6214
Training Epoch: 0 [5120/50048]	Loss: 4.6398
Training Epoch: 0 [5248/50048]	Loss: 4.6111
Training Epoch: 0 [5376/50048]	Loss: 4.6021
Training Epoch: 0 [5504/50048]	Loss: 4.6009
Training Epoch: 0 [5632/50048]	Loss: 4.6427
Training Epoch: 0 [5760/50048]	Loss: 4.6187
Training Epoch: 0 [5888/50048]	Loss: 4.6381
Training Epoch: 0 [6016/50048]	Loss: 4.6416
Training Epoch: 0 [6144/50048]	Loss: 4.6044
Training Epoch: 0 [6272/50048]	Loss: 4.6109
Training Epoch: 0 [6400/50048]	Loss: 4.6388
Profile done with power limit 175W
epoch 1 train time consumed: 11.71s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0105
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 175000, 'lr': 1.1180339887498949e-05, 'energy': 139.3572357461117, 'time': 8.822673620999922, 'accuracy': 0.010482594936708861, 'total_cost': 236285.92708639023}
[run job] Launching job with BS 128: and LR: 1.1180339887498949e-05 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs128+lr0.00001+pl150', 'ZEUS_COST_THRESH': '472571.85417278047', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '1.1180339887498949e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '1.1180339887498949e-05', '--power_limit', '150']
[run job] cost_ub=472571.85417278047
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs128+lr0.00001+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 1.1180339887498949e-05
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.6650
Training Epoch: 0 [384/50048]	Loss: 4.5993
Training Epoch: 0 [512/50048]	Loss: 4.6184
Training Epoch: 0 [640/50048]	Loss: 4.6456
Training Epoch: 0 [768/50048]	Loss: 4.6004
Training Epoch: 0 [896/50048]	Loss: 4.6326
Training Epoch: 0 [1024/50048]	Loss: 4.5937
Training Epoch: 0 [1152/50048]	Loss: 4.6232
Training Epoch: 0 [1280/50048]	Loss: 4.6559
Training Epoch: 0 [1408/50048]	Loss: 4.6570
Training Epoch: 0 [1536/50048]	Loss: 4.6246
Training Epoch: 0 [1664/50048]	Loss: 4.6041
Training Epoch: 0 [1792/50048]	Loss: 4.6385
Training Epoch: 0 [1920/50048]	Loss: 4.6654
Training Epoch: 0 [2048/50048]	Loss: 4.6490
Training Epoch: 0 [2176/50048]	Loss: 4.6444
Training Epoch: 0 [2304/50048]	Loss: 4.6231
Training Epoch: 0 [2432/50048]	Loss: 4.6341
Training Epoch: 0 [2560/50048]	Loss: 4.6153
Training Epoch: 0 [2688/50048]	Loss: 4.6046
Training Epoch: 0 [2816/50048]	Loss: 4.6215
Training Epoch: 0 [2944/50048]	Loss: 4.6169
Training Epoch: 0 [3072/50048]	Loss: 4.6301
Training Epoch: 0 [3200/50048]	Loss: 4.6352
Training Epoch: 0 [3328/50048]	Loss: 4.6171
Training Epoch: 0 [3456/50048]	Loss: 4.6281
Training Epoch: 0 [3584/50048]	Loss: 4.6218
Training Epoch: 0 [3712/50048]	Loss: 4.6031
Training Epoch: 0 [3840/50048]	Loss: 4.6290
Training Epoch: 0 [3968/50048]	Loss: 4.6062
Training Epoch: 0 [4096/50048]	Loss: 4.6188
Training Epoch: 0 [4224/50048]	Loss: 4.6155
Training Epoch: 0 [4352/50048]	Loss: 4.6308
Training Epoch: 0 [4480/50048]	Loss: 4.6351
Training Epoch: 0 [4608/50048]	Loss: 4.6749
Training Epoch: 0 [4736/50048]	Loss: 4.5952
Training Epoch: 0 [4864/50048]	Loss: 4.6217
Training Epoch: 0 [4992/50048]	Loss: 4.6211
Training Epoch: 0 [5120/50048]	Loss: 4.6407
Training Epoch: 0 [5248/50048]	Loss: 4.6112
Training Epoch: 0 [5376/50048]	Loss: 4.6028
Training Epoch: 0 [5504/50048]	Loss: 4.6003
Training Epoch: 0 [5632/50048]	Loss: 4.6426
Training Epoch: 0 [5760/50048]	Loss: 4.6190
Training Epoch: 0 [5888/50048]	Loss: 4.6384
Training Epoch: 0 [6016/50048]	Loss: 4.6422
Training Epoch: 0 [6144/50048]	Loss: 4.6038
Training Epoch: 0 [6272/50048]	Loss: 4.6106
Training Epoch: 0 [6400/50048]	Loss: 4.6390
Profile done with power limit 150W
epoch 1 train time consumed: 11.65s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0106
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 150000, 'lr': 1.1180339887498949e-05, 'energy': 139.73605271788264, 'time': 8.753814968999905, 'accuracy': 0.010581487341772151, 'total_cost': 232259.59389633522}
[run job] Launching job with BS 128: and LR: 1.1180339887498949e-05 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs128+lr0.00001+pl125', 'ZEUS_COST_THRESH': '464519.18779267045', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '1.1180339887498949e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '1.1180339887498949e-05', '--power_limit', '125']
[run job] cost_ub=464519.18779267045
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs128+lr0.00001+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 1.1180339887498949e-05
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.6650
Training Epoch: 0 [384/50048]	Loss: 4.5993
Training Epoch: 0 [512/50048]	Loss: 4.6184
Training Epoch: 0 [640/50048]	Loss: 4.6456
Training Epoch: 0 [768/50048]	Loss: 4.6004
Training Epoch: 0 [896/50048]	Loss: 4.6326
Training Epoch: 0 [1024/50048]	Loss: 4.5937
Training Epoch: 0 [1152/50048]	Loss: 4.6232
Training Epoch: 0 [1280/50048]	Loss: 4.6559
Training Epoch: 0 [1408/50048]	Loss: 4.6570
Training Epoch: 0 [1536/50048]	Loss: 4.6245
Training Epoch: 0 [1664/50048]	Loss: 4.6041
Training Epoch: 0 [1792/50048]	Loss: 4.6386
Training Epoch: 0 [1920/50048]	Loss: 4.6654
Training Epoch: 0 [2048/50048]	Loss: 4.6490
Training Epoch: 0 [2176/50048]	Loss: 4.6444
Training Epoch: 0 [2304/50048]	Loss: 4.6230
Training Epoch: 0 [2432/50048]	Loss: 4.6339
Training Epoch: 0 [2560/50048]	Loss: 4.6155
Training Epoch: 0 [2688/50048]	Loss: 4.6045
Training Epoch: 0 [2816/50048]	Loss: 4.6215
Training Epoch: 0 [2944/50048]	Loss: 4.6168
Training Epoch: 0 [3072/50048]	Loss: 4.6299
Training Epoch: 0 [3200/50048]	Loss: 4.6352
Training Epoch: 0 [3328/50048]	Loss: 4.6173
Training Epoch: 0 [3456/50048]	Loss: 4.6282
Training Epoch: 0 [3584/50048]	Loss: 4.6213
Training Epoch: 0 [3712/50048]	Loss: 4.6031
Training Epoch: 0 [3840/50048]	Loss: 4.6292
Training Epoch: 0 [3968/50048]	Loss: 4.6063
Training Epoch: 0 [4096/50048]	Loss: 4.6187
Training Epoch: 0 [4224/50048]	Loss: 4.6152
Training Epoch: 0 [4352/50048]	Loss: 4.6303
Training Epoch: 0 [4480/50048]	Loss: 4.6350
Training Epoch: 0 [4608/50048]	Loss: 4.6749
Training Epoch: 0 [4736/50048]	Loss: 4.5950
Training Epoch: 0 [4864/50048]	Loss: 4.6212
Training Epoch: 0 [4992/50048]	Loss: 4.6207
Training Epoch: 0 [5120/50048]	Loss: 4.6405
Training Epoch: 0 [5248/50048]	Loss: 4.6115
Training Epoch: 0 [5376/50048]	Loss: 4.6036
Training Epoch: 0 [5504/50048]	Loss: 4.5999
Training Epoch: 0 [5632/50048]	Loss: 4.6424
Training Epoch: 0 [5760/50048]	Loss: 4.6180
Training Epoch: 0 [5888/50048]	Loss: 4.6387
Training Epoch: 0 [6016/50048]	Loss: 4.6421
Training Epoch: 0 [6144/50048]	Loss: 4.6045
Training Epoch: 0 [6272/50048]	Loss: 4.6113
Training Epoch: 0 [6400/50048]	Loss: 4.6385
Profile done with power limit 125W
epoch 1 train time consumed: 12.24s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0107
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 125000, 'lr': 1.1180339887498949e-05, 'energy': 122.5357405834043, 'time': 9.28990251200048, 'accuracy': 0.010680379746835443, 'total_cost': 244046.82233333375}
[run job] Launching job with BS 128: and LR: 1.1180339887498949e-05 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs128+lr0.00001+pl100', 'ZEUS_COST_THRESH': '464519.18779267045', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '1.1180339887498949e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '1.1180339887498949e-05', '--power_limit', '100']
[run job] cost_ub=464519.18779267045
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs128+lr0.00001+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 1.1180339887498949e-05
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.6650
Training Epoch: 0 [384/50048]	Loss: 4.5993
Training Epoch: 0 [512/50048]	Loss: 4.6184
Training Epoch: 0 [640/50048]	Loss: 4.6456
Training Epoch: 0 [768/50048]	Loss: 4.6004
Training Epoch: 0 [896/50048]	Loss: 4.6326
Training Epoch: 0 [1024/50048]	Loss: 4.5937
Training Epoch: 0 [1152/50048]	Loss: 4.6233
Training Epoch: 0 [1280/50048]	Loss: 4.6559
Training Epoch: 0 [1408/50048]	Loss: 4.6569
Training Epoch: 0 [1536/50048]	Loss: 4.6246
Training Epoch: 0 [1664/50048]	Loss: 4.6041
Training Epoch: 0 [1792/50048]	Loss: 4.6384
Training Epoch: 0 [1920/50048]	Loss: 4.6654
Training Epoch: 0 [2048/50048]	Loss: 4.6491
Training Epoch: 0 [2176/50048]	Loss: 4.6445
Training Epoch: 0 [2304/50048]	Loss: 4.6230
Training Epoch: 0 [2432/50048]	Loss: 4.6343
Training Epoch: 0 [2560/50048]	Loss: 4.6157
Training Epoch: 0 [2688/50048]	Loss: 4.6045
Training Epoch: 0 [2816/50048]	Loss: 4.6217
Training Epoch: 0 [2944/50048]	Loss: 4.6168
Training Epoch: 0 [3072/50048]	Loss: 4.6299
Training Epoch: 0 [3200/50048]	Loss: 4.6351
Training Epoch: 0 [3328/50048]	Loss: 4.6169
Training Epoch: 0 [3456/50048]	Loss: 4.6281
Training Epoch: 0 [3584/50048]	Loss: 4.6218
Training Epoch: 0 [3712/50048]	Loss: 4.6030
Training Epoch: 0 [3840/50048]	Loss: 4.6294
Training Epoch: 0 [3968/50048]	Loss: 4.6060
Training Epoch: 0 [4096/50048]	Loss: 4.6189
Training Epoch: 0 [4224/50048]	Loss: 4.6155
Training Epoch: 0 [4352/50048]	Loss: 4.6308
Training Epoch: 0 [4480/50048]	Loss: 4.6347
Training Epoch: 0 [4608/50048]	Loss: 4.6747
Training Epoch: 0 [4736/50048]	Loss: 4.5950
Training Epoch: 0 [4864/50048]	Loss: 4.6218
Training Epoch: 0 [4992/50048]	Loss: 4.6214
Training Epoch: 0 [5120/50048]	Loss: 4.6397
Training Epoch: 0 [5248/50048]	Loss: 4.6115
Training Epoch: 0 [5376/50048]	Loss: 4.6027
Training Epoch: 0 [5504/50048]	Loss: 4.6005
Training Epoch: 0 [5632/50048]	Loss: 4.6423
Training Epoch: 0 [5760/50048]	Loss: 4.6189
Training Epoch: 0 [5888/50048]	Loss: 4.6385
Training Epoch: 0 [6016/50048]	Loss: 4.6420
Training Epoch: 0 [6144/50048]	Loss: 4.6041
Training Epoch: 0 [6272/50048]	Loss: 4.6114
Training Epoch: 0 [6400/50048]	Loss: 4.6381
Profile done with power limit 100W
epoch 1 train time consumed: 14.38s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0105
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 100000, 'lr': 1.1180339887498949e-05, 'energy': 98.59729827143344, 'time': 11.041151051999805, 'accuracy': 0.010482594936708861, 'total_cost': 295238.1040514115}

[Power Profiler] with batch size 128 and learning rate 7.945124291035352e-05
[run job] Launching job with BS 128: and LR: 7.945124291035352e-05 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs128+lr0.00008+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '7.945124291035352e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '7.945124291035352e-05', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs128+lr0.00008+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 7.945124291035352e-05
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.6607
Training Epoch: 0 [384/50048]	Loss: 4.5988
Training Epoch: 0 [512/50048]	Loss: 4.6158
Training Epoch: 0 [640/50048]	Loss: 4.6344
Training Epoch: 0 [768/50048]	Loss: 4.5996
Training Epoch: 0 [896/50048]	Loss: 4.6319
Training Epoch: 0 [1024/50048]	Loss: 4.5903
Training Epoch: 0 [1152/50048]	Loss: 4.6049
Training Epoch: 0 [1280/50048]	Loss: 4.6372
Training Epoch: 0 [1408/50048]	Loss: 4.6384
Training Epoch: 0 [1536/50048]	Loss: 4.6232
Training Epoch: 0 [1664/50048]	Loss: 4.6063
Training Epoch: 0 [1792/50048]	Loss: 4.6151
Training Epoch: 0 [1920/50048]	Loss: 4.6451
Training Epoch: 0 [2048/50048]	Loss: 4.6160
Training Epoch: 0 [2176/50048]	Loss: 4.6462
Training Epoch: 0 [2304/50048]	Loss: 4.5970
Training Epoch: 0 [2432/50048]	Loss: 4.6244
Training Epoch: 0 [2560/50048]	Loss: 4.6063
Training Epoch: 0 [2688/50048]	Loss: 4.5898
Training Epoch: 0 [2816/50048]	Loss: 4.6004
Training Epoch: 0 [2944/50048]	Loss: 4.5977
Training Epoch: 0 [3072/50048]	Loss: 4.5815
Training Epoch: 0 [3200/50048]	Loss: 4.5975
Training Epoch: 0 [3328/50048]	Loss: 4.5876
Training Epoch: 0 [3456/50048]	Loss: 4.5852
Training Epoch: 0 [3584/50048]	Loss: 4.5968
Training Epoch: 0 [3712/50048]	Loss: 4.5861
Training Epoch: 0 [3840/50048]	Loss: 4.6141
Training Epoch: 0 [3968/50048]	Loss: 4.6082
Training Epoch: 0 [4096/50048]	Loss: 4.5817
Training Epoch: 0 [4224/50048]	Loss: 4.6011
Training Epoch: 0 [4352/50048]	Loss: 4.5808
Training Epoch: 0 [4480/50048]	Loss: 4.5702
Training Epoch: 0 [4608/50048]	Loss: 4.6081
Training Epoch: 0 [4736/50048]	Loss: 4.5586
Training Epoch: 0 [4864/50048]	Loss: 4.5735
Training Epoch: 0 [4992/50048]	Loss: 4.5866
Training Epoch: 0 [5120/50048]	Loss: 4.5895
Training Epoch: 0 [5248/50048]	Loss: 4.5560
Training Epoch: 0 [5376/50048]	Loss: 4.5783
Training Epoch: 0 [5504/50048]	Loss: 4.5688
Training Epoch: 0 [5632/50048]	Loss: 4.5885
Training Epoch: 0 [5760/50048]	Loss: 4.5621
Training Epoch: 0 [5888/50048]	Loss: 4.5805
Training Epoch: 0 [6016/50048]	Loss: 4.5754
Training Epoch: 0 [6144/50048]	Loss: 4.5546
Training Epoch: 0 [6272/50048]	Loss: 4.5827
Training Epoch: 0 [6400/50048]	Loss: 4.5757
Profile done with power limit 175W
epoch 1 train time consumed: 11.64s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0111
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 175000, 'lr': 7.945124291035352e-05, 'energy': 140.0925384269126, 'time': 8.771167259999856, 'accuracy': 0.011075949367088608, 'total_cost': 222331.16512774403}
[run job] Launching job with BS 128: and LR: 7.945124291035352e-05 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs128+lr0.00008+pl150', 'ZEUS_COST_THRESH': '444662.33025548805', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '7.945124291035352e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '7.945124291035352e-05', '--power_limit', '150']
[run job] cost_ub=444662.33025548805
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs128+lr0.00008+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 7.945124291035352e-05
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.6607
Training Epoch: 0 [384/50048]	Loss: 4.5988
Training Epoch: 0 [512/50048]	Loss: 4.6157
Training Epoch: 0 [640/50048]	Loss: 4.6344
Training Epoch: 0 [768/50048]	Loss: 4.5994
Training Epoch: 0 [896/50048]	Loss: 4.6318
Training Epoch: 0 [1024/50048]	Loss: 4.5906
Training Epoch: 0 [1152/50048]	Loss: 4.6049
Training Epoch: 0 [1280/50048]	Loss: 4.6369
Training Epoch: 0 [1408/50048]	Loss: 4.6386
Training Epoch: 0 [1536/50048]	Loss: 4.6238
Training Epoch: 0 [1664/50048]	Loss: 4.6055
Training Epoch: 0 [1792/50048]	Loss: 4.6139
Training Epoch: 0 [1920/50048]	Loss: 4.6485
Training Epoch: 0 [2048/50048]	Loss: 4.6174
Training Epoch: 0 [2176/50048]	Loss: 4.6426
Training Epoch: 0 [2304/50048]	Loss: 4.5972
Training Epoch: 0 [2432/50048]	Loss: 4.6260
Training Epoch: 0 [2560/50048]	Loss: 4.6063
Training Epoch: 0 [2688/50048]	Loss: 4.5897
Training Epoch: 0 [2816/50048]	Loss: 4.6011
Training Epoch: 0 [2944/50048]	Loss: 4.5940
Training Epoch: 0 [3072/50048]	Loss: 4.5769
Training Epoch: 0 [3200/50048]	Loss: 4.5931
Training Epoch: 0 [3328/50048]	Loss: 4.5857
Training Epoch: 0 [3456/50048]	Loss: 4.5864
Training Epoch: 0 [3584/50048]	Loss: 4.5994
Training Epoch: 0 [3712/50048]	Loss: 4.5871
Training Epoch: 0 [3840/50048]	Loss: 4.6148
Training Epoch: 0 [3968/50048]	Loss: 4.5911
Training Epoch: 0 [4096/50048]	Loss: 4.5802
Training Epoch: 0 [4224/50048]	Loss: 4.6003
Training Epoch: 0 [4352/50048]	Loss: 4.5804
Training Epoch: 0 [4480/50048]	Loss: 4.5761
Training Epoch: 0 [4608/50048]	Loss: 4.6083
Training Epoch: 0 [4736/50048]	Loss: 4.5721
Training Epoch: 0 [4864/50048]	Loss: 4.5763
Training Epoch: 0 [4992/50048]	Loss: 4.5912
Training Epoch: 0 [5120/50048]	Loss: 4.5893
Training Epoch: 0 [5248/50048]	Loss: 4.5635
Training Epoch: 0 [5376/50048]	Loss: 4.5856
Training Epoch: 0 [5504/50048]	Loss: 4.5643
Training Epoch: 0 [5632/50048]	Loss: 4.5909
Training Epoch: 0 [5760/50048]	Loss: 4.5634
Training Epoch: 0 [5888/50048]	Loss: 4.5782
Training Epoch: 0 [6016/50048]	Loss: 4.5724
Training Epoch: 0 [6144/50048]	Loss: 4.5478
Training Epoch: 0 [6272/50048]	Loss: 4.5932
Training Epoch: 0 [6400/50048]	Loss: 4.5684
Profile done with power limit 150W
epoch 1 train time consumed: 11.64s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0120
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 150000, 'lr': 7.945124291035352e-05, 'energy': 139.8006288385306, 'time': 8.792504326000198, 'accuracy': 0.011965981012658227, 'total_cost': 206291.37797819957}
[run job] Launching job with BS 128: and LR: 7.945124291035352e-05 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs128+lr0.00008+pl125', 'ZEUS_COST_THRESH': '412582.75595639914', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '7.945124291035352e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '7.945124291035352e-05', '--power_limit', '125']
[run job] cost_ub=412582.75595639914
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs128+lr0.00008+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 7.945124291035352e-05
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.6607
Training Epoch: 0 [384/50048]	Loss: 4.5988
Training Epoch: 0 [512/50048]	Loss: 4.6157
Training Epoch: 0 [640/50048]	Loss: 4.6344
Training Epoch: 0 [768/50048]	Loss: 4.5992
Training Epoch: 0 [896/50048]	Loss: 4.6320
Training Epoch: 0 [1024/50048]	Loss: 4.5911
Training Epoch: 0 [1152/50048]	Loss: 4.6048
Training Epoch: 0 [1280/50048]	Loss: 4.6366
Training Epoch: 0 [1408/50048]	Loss: 4.6394
Training Epoch: 0 [1536/50048]	Loss: 4.6234
Training Epoch: 0 [1664/50048]	Loss: 4.6073
Training Epoch: 0 [1792/50048]	Loss: 4.6146
Training Epoch: 0 [1920/50048]	Loss: 4.6474
Training Epoch: 0 [2048/50048]	Loss: 4.6168
Training Epoch: 0 [2176/50048]	Loss: 4.6431
Training Epoch: 0 [2304/50048]	Loss: 4.6018
Training Epoch: 0 [2432/50048]	Loss: 4.6312
Training Epoch: 0 [2560/50048]	Loss: 4.6044
Training Epoch: 0 [2688/50048]	Loss: 4.5901
Training Epoch: 0 [2816/50048]	Loss: 4.6040
Training Epoch: 0 [2944/50048]	Loss: 4.5966
Training Epoch: 0 [3072/50048]	Loss: 4.5743
Training Epoch: 0 [3200/50048]	Loss: 4.5935
Training Epoch: 0 [3328/50048]	Loss: 4.5868
Training Epoch: 0 [3456/50048]	Loss: 4.5858
Training Epoch: 0 [3584/50048]	Loss: 4.5980
Training Epoch: 0 [3712/50048]	Loss: 4.5859
Training Epoch: 0 [3840/50048]	Loss: 4.6174
Training Epoch: 0 [3968/50048]	Loss: 4.6000
Training Epoch: 0 [4096/50048]	Loss: 4.5795
Training Epoch: 0 [4224/50048]	Loss: 4.5950
Training Epoch: 0 [4352/50048]	Loss: 4.5783
Training Epoch: 0 [4480/50048]	Loss: 4.5719
Training Epoch: 0 [4608/50048]	Loss: 4.6035
Training Epoch: 0 [4736/50048]	Loss: 4.5602
Training Epoch: 0 [4864/50048]	Loss: 4.5779
Training Epoch: 0 [4992/50048]	Loss: 4.5893
Training Epoch: 0 [5120/50048]	Loss: 4.5885
Training Epoch: 0 [5248/50048]	Loss: 4.5614
Training Epoch: 0 [5376/50048]	Loss: 4.5860
Training Epoch: 0 [5504/50048]	Loss: 4.5578
Training Epoch: 0 [5632/50048]	Loss: 4.5858
Training Epoch: 0 [5760/50048]	Loss: 4.5592
Training Epoch: 0 [5888/50048]	Loss: 4.5761
Training Epoch: 0 [6016/50048]	Loss: 4.5762
Training Epoch: 0 [6144/50048]	Loss: 4.5489
Training Epoch: 0 [6272/50048]	Loss: 4.5814
Training Epoch: 0 [6400/50048]	Loss: 4.5696
Profile done with power limit 125W
epoch 1 train time consumed: 12.33s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0101
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 125000, 'lr': 7.945124291035352e-05, 'energy': 122.78657509880935, 'time': 9.327858290999757, 'accuracy': 0.010087025316455696, 'total_cost': 259456.69013667092}
[run job] Launching job with BS 128: and LR: 7.945124291035352e-05 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs128+lr0.00008+pl100', 'ZEUS_COST_THRESH': '412582.75595639914', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '7.945124291035352e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '7.945124291035352e-05', '--power_limit', '100']
[run job] cost_ub=412582.75595639914
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs128+lr0.00008+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 7.945124291035352e-05
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 4.6607
Training Epoch: 0 [384/50048]	Loss: 4.5987
Training Epoch: 0 [512/50048]	Loss: 4.6158
Training Epoch: 0 [640/50048]	Loss: 4.6341
Training Epoch: 0 [768/50048]	Loss: 4.5994
Training Epoch: 0 [896/50048]	Loss: 4.6318
Training Epoch: 0 [1024/50048]	Loss: 4.5899
Training Epoch: 0 [1152/50048]	Loss: 4.6045
Training Epoch: 0 [1280/50048]	Loss: 4.6362
Training Epoch: 0 [1408/50048]	Loss: 4.6390
Training Epoch: 0 [1536/50048]	Loss: 4.6221
Training Epoch: 0 [1664/50048]	Loss: 4.6073
Training Epoch: 0 [1792/50048]	Loss: 4.6155
Training Epoch: 0 [1920/50048]	Loss: 4.6484
Training Epoch: 0 [2048/50048]	Loss: 4.6186
Training Epoch: 0 [2176/50048]	Loss: 4.6461
Training Epoch: 0 [2304/50048]	Loss: 4.5990
Training Epoch: 0 [2432/50048]	Loss: 4.6254
Training Epoch: 0 [2560/50048]	Loss: 4.6045
Training Epoch: 0 [2688/50048]	Loss: 4.5928
Training Epoch: 0 [2816/50048]	Loss: 4.6029
Training Epoch: 0 [2944/50048]	Loss: 4.5921
Training Epoch: 0 [3072/50048]	Loss: 4.5784
Training Epoch: 0 [3200/50048]	Loss: 4.5914
Training Epoch: 0 [3328/50048]	Loss: 4.5862
Training Epoch: 0 [3456/50048]	Loss: 4.5833
Training Epoch: 0 [3584/50048]	Loss: 4.6051
Training Epoch: 0 [3712/50048]	Loss: 4.5893
Training Epoch: 0 [3840/50048]	Loss: 4.6170
Training Epoch: 0 [3968/50048]	Loss: 4.5952
Training Epoch: 0 [4096/50048]	Loss: 4.5816
Training Epoch: 0 [4224/50048]	Loss: 4.6047
Training Epoch: 0 [4352/50048]	Loss: 4.5741
Training Epoch: 0 [4480/50048]	Loss: 4.5791
Training Epoch: 0 [4608/50048]	Loss: 4.6130
Training Epoch: 0 [4736/50048]	Loss: 4.5671
Training Epoch: 0 [4864/50048]	Loss: 4.5758
Training Epoch: 0 [4992/50048]	Loss: 4.5939
Training Epoch: 0 [5120/50048]	Loss: 4.5882
Training Epoch: 0 [5248/50048]	Loss: 4.5574
Training Epoch: 0 [5376/50048]	Loss: 4.5839
Training Epoch: 0 [5504/50048]	Loss: 4.5655
Training Epoch: 0 [5632/50048]	Loss: 4.5853
Training Epoch: 0 [5760/50048]	Loss: 4.5674
Training Epoch: 0 [5888/50048]	Loss: 4.5751
Training Epoch: 0 [6016/50048]	Loss: 4.5768
Training Epoch: 0 [6144/50048]	Loss: 4.5459
Training Epoch: 0 [6272/50048]	Loss: 4.5848
Training Epoch: 0 [6400/50048]	Loss: 4.5697
Profile done with power limit 100W
epoch 1 train time consumed: 14.51s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0114
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 100000, 'lr': 7.945124291035352e-05, 'energy': 98.33831803576037, 'time': 11.13747637600045, 'accuracy': 0.011372626582278481, 'total_cost': 274500.3624742786}

[Power Profiler] with batch size 256 and learning rate 1.5811388300841898e-05
[run job] Launching job with BS 256: and LR: 1.5811388300841898e-05 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs256+lr0.00002+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '1.5811388300841898e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '1.5811388300841898e-05', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs256+lr0.00002+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 1.5811388300841898e-05
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 4.6081
Training Epoch: 0 [768/50176]	Loss: 4.6183
Training Epoch: 0 [1024/50176]	Loss: 4.6252
Training Epoch: 0 [1280/50176]	Loss: 4.6463
Training Epoch: 0 [1536/50176]	Loss: 4.6373
Training Epoch: 0 [1792/50176]	Loss: 4.6180
Training Epoch: 0 [2048/50176]	Loss: 4.6552
Training Epoch: 0 [2304/50176]	Loss: 4.6236
Training Epoch: 0 [2560/50176]	Loss: 4.6212
Training Epoch: 0 [2816/50176]	Loss: 4.6124
Training Epoch: 0 [3072/50176]	Loss: 4.6335
Training Epoch: 0 [3328/50176]	Loss: 4.6279
Training Epoch: 0 [3584/50176]	Loss: 4.6130
Training Epoch: 0 [3840/50176]	Loss: 4.6175
Training Epoch: 0 [4096/50176]	Loss: 4.6026
Training Epoch: 0 [4352/50176]	Loss: 4.6236
Training Epoch: 0 [4608/50176]	Loss: 4.6450
Training Epoch: 0 [4864/50176]	Loss: 4.6140
Training Epoch: 0 [5120/50176]	Loss: 4.6248
Training Epoch: 0 [5376/50176]	Loss: 4.6068
Training Epoch: 0 [5632/50176]	Loss: 4.6264
Training Epoch: 0 [5888/50176]	Loss: 4.6324
Training Epoch: 0 [6144/50176]	Loss: 4.6158
Training Epoch: 0 [6400/50176]	Loss: 4.6291
Training Epoch: 0 [6656/50176]	Loss: 4.6314
Training Epoch: 0 [6912/50176]	Loss: 4.6187
Training Epoch: 0 [7168/50176]	Loss: 4.6387
Training Epoch: 0 [7424/50176]	Loss: 4.6270
Training Epoch: 0 [7680/50176]	Loss: 4.6144
Training Epoch: 0 [7936/50176]	Loss: 4.6214
Training Epoch: 0 [8192/50176]	Loss: 4.6352
Training Epoch: 0 [8448/50176]	Loss: 4.6319
Training Epoch: 0 [8704/50176]	Loss: 4.6125
Training Epoch: 0 [8960/50176]	Loss: 4.6052
Training Epoch: 0 [9216/50176]	Loss: 4.6276
Training Epoch: 0 [9472/50176]	Loss: 4.6133
Training Epoch: 0 [9728/50176]	Loss: 4.6109
Training Epoch: 0 [9984/50176]	Loss: 4.6054
Training Epoch: 0 [10240/50176]	Loss: 4.6280
Training Epoch: 0 [10496/50176]	Loss: 4.6200
Training Epoch: 0 [10752/50176]	Loss: 4.6113
Training Epoch: 0 [11008/50176]	Loss: 4.6097
Training Epoch: 0 [11264/50176]	Loss: 4.6056
Training Epoch: 0 [11520/50176]	Loss: 4.5854
Training Epoch: 0 [11776/50176]	Loss: 4.6069
Training Epoch: 0 [12032/50176]	Loss: 4.6278
Training Epoch: 0 [12288/50176]	Loss: 4.6267
Training Epoch: 0 [12544/50176]	Loss: 4.6077
Training Epoch: 0 [12800/50176]	Loss: 4.5771
Profile done with power limit 175W
epoch 1 train time consumed: 12.97s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0110
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 175000, 'lr': 1.5811388300841898e-05, 'energy': 147.91544706034037, 'time': 9.752818318000209, 'accuracy': 0.01103515625, 'total_cost': 494898.991809991}
[run job] Launching job with BS 256: and LR: 1.5811388300841898e-05 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs256+lr0.00002+pl150', 'ZEUS_COST_THRESH': '989797.983619982', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '1.5811388300841898e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '1.5811388300841898e-05', '--power_limit', '150']
[run job] cost_ub=989797.983619982
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs256+lr0.00002+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 1.5811388300841898e-05
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 4.6081
Training Epoch: 0 [768/50176]	Loss: 4.6183
Training Epoch: 0 [1024/50176]	Loss: 4.6252
Training Epoch: 0 [1280/50176]	Loss: 4.6463
Training Epoch: 0 [1536/50176]	Loss: 4.6374
Training Epoch: 0 [1792/50176]	Loss: 4.6180
Training Epoch: 0 [2048/50176]	Loss: 4.6553
Training Epoch: 0 [2304/50176]	Loss: 4.6236
Training Epoch: 0 [2560/50176]	Loss: 4.6212
Training Epoch: 0 [2816/50176]	Loss: 4.6125
Training Epoch: 0 [3072/50176]	Loss: 4.6335
Training Epoch: 0 [3328/50176]	Loss: 4.6279
Training Epoch: 0 [3584/50176]	Loss: 4.6130
Training Epoch: 0 [3840/50176]	Loss: 4.6174
Training Epoch: 0 [4096/50176]	Loss: 4.6026
Training Epoch: 0 [4352/50176]	Loss: 4.6235
Training Epoch: 0 [4608/50176]	Loss: 4.6449
Training Epoch: 0 [4864/50176]	Loss: 4.6139
Training Epoch: 0 [5120/50176]	Loss: 4.6245
Training Epoch: 0 [5376/50176]	Loss: 4.6066
Training Epoch: 0 [5632/50176]	Loss: 4.6263
Training Epoch: 0 [5888/50176]	Loss: 4.6324
Training Epoch: 0 [6144/50176]	Loss: 4.6159
Training Epoch: 0 [6400/50176]	Loss: 4.6291
Training Epoch: 0 [6656/50176]	Loss: 4.6315
Training Epoch: 0 [6912/50176]	Loss: 4.6184
Training Epoch: 0 [7168/50176]	Loss: 4.6386
Training Epoch: 0 [7424/50176]	Loss: 4.6272
Training Epoch: 0 [7680/50176]	Loss: 4.6146
Training Epoch: 0 [7936/50176]	Loss: 4.6216
Training Epoch: 0 [8192/50176]	Loss: 4.6348
Training Epoch: 0 [8448/50176]	Loss: 4.6319
Training Epoch: 0 [8704/50176]	Loss: 4.6128
Training Epoch: 0 [8960/50176]	Loss: 4.6050
Training Epoch: 0 [9216/50176]	Loss: 4.6276
Training Epoch: 0 [9472/50176]	Loss: 4.6128
Training Epoch: 0 [9728/50176]	Loss: 4.6109
Training Epoch: 0 [9984/50176]	Loss: 4.6049
Training Epoch: 0 [10240/50176]	Loss: 4.6281
Training Epoch: 0 [10496/50176]	Loss: 4.6204
Training Epoch: 0 [10752/50176]	Loss: 4.6114
Training Epoch: 0 [11008/50176]	Loss: 4.6094
Training Epoch: 0 [11264/50176]	Loss: 4.6051
Training Epoch: 0 [11520/50176]	Loss: 4.5857
Training Epoch: 0 [11776/50176]	Loss: 4.6073
Training Epoch: 0 [12032/50176]	Loss: 4.6274
Training Epoch: 0 [12288/50176]	Loss: 4.6262
Training Epoch: 0 [12544/50176]	Loss: 4.6074
Training Epoch: 0 [12800/50176]	Loss: 4.5770
Profile done with power limit 150W
epoch 1 train time consumed: 12.99s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0111
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 150000, 'lr': 1.5811388300841898e-05, 'energy': 145.4251832151443, 'time': 9.736000805000003, 'accuracy': 0.0111328125, 'total_cost': 489686.2579888597}
[run job] Launching job with BS 256: and LR: 1.5811388300841898e-05 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs256+lr0.00002+pl125', 'ZEUS_COST_THRESH': '979372.5159777194', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '1.5811388300841898e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '1.5811388300841898e-05', '--power_limit', '125']
[run job] cost_ub=979372.5159777194
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs256+lr0.00002+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 1.5811388300841898e-05
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 4.6081
Training Epoch: 0 [768/50176]	Loss: 4.6183
Training Epoch: 0 [1024/50176]	Loss: 4.6252
Training Epoch: 0 [1280/50176]	Loss: 4.6463
Training Epoch: 0 [1536/50176]	Loss: 4.6374
Training Epoch: 0 [1792/50176]	Loss: 4.6179
Training Epoch: 0 [2048/50176]	Loss: 4.6552
Training Epoch: 0 [2304/50176]	Loss: 4.6236
Training Epoch: 0 [2560/50176]	Loss: 4.6212
Training Epoch: 0 [2816/50176]	Loss: 4.6125
Training Epoch: 0 [3072/50176]	Loss: 4.6335
Training Epoch: 0 [3328/50176]	Loss: 4.6279
Training Epoch: 0 [3584/50176]	Loss: 4.6130
Training Epoch: 0 [3840/50176]	Loss: 4.6173
Training Epoch: 0 [4096/50176]	Loss: 4.6026
Training Epoch: 0 [4352/50176]	Loss: 4.6237
Training Epoch: 0 [4608/50176]	Loss: 4.6450
Training Epoch: 0 [4864/50176]	Loss: 4.6138
Training Epoch: 0 [5120/50176]	Loss: 4.6248
Training Epoch: 0 [5376/50176]	Loss: 4.6067
Training Epoch: 0 [5632/50176]	Loss: 4.6262
Training Epoch: 0 [5888/50176]	Loss: 4.6325
Training Epoch: 0 [6144/50176]	Loss: 4.6162
Training Epoch: 0 [6400/50176]	Loss: 4.6293
Training Epoch: 0 [6656/50176]	Loss: 4.6313
Training Epoch: 0 [6912/50176]	Loss: 4.6183
Training Epoch: 0 [7168/50176]	Loss: 4.6388
Training Epoch: 0 [7424/50176]	Loss: 4.6272
Training Epoch: 0 [7680/50176]	Loss: 4.6143
Training Epoch: 0 [7936/50176]	Loss: 4.6212
Training Epoch: 0 [8192/50176]	Loss: 4.6346
Training Epoch: 0 [8448/50176]	Loss: 4.6317
Training Epoch: 0 [8704/50176]	Loss: 4.6124
Training Epoch: 0 [8960/50176]	Loss: 4.6046
Training Epoch: 0 [9216/50176]	Loss: 4.6274
Training Epoch: 0 [9472/50176]	Loss: 4.6131
Training Epoch: 0 [9728/50176]	Loss: 4.6107
Training Epoch: 0 [9984/50176]	Loss: 4.6054
Training Epoch: 0 [10240/50176]	Loss: 4.6278
Training Epoch: 0 [10496/50176]	Loss: 4.6199
Training Epoch: 0 [10752/50176]	Loss: 4.6112
Training Epoch: 0 [11008/50176]	Loss: 4.6096
Training Epoch: 0 [11264/50176]	Loss: 4.6053
Training Epoch: 0 [11520/50176]	Loss: 4.5851
Training Epoch: 0 [11776/50176]	Loss: 4.6063
Training Epoch: 0 [12032/50176]	Loss: 4.6282
Training Epoch: 0 [12288/50176]	Loss: 4.6272
Training Epoch: 0 [12544/50176]	Loss: 4.6081
Training Epoch: 0 [12800/50176]	Loss: 4.5770
Profile done with power limit 125W
epoch 1 train time consumed: 14.12s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0111
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 125000, 'lr': 1.5811388300841898e-05, 'energy': 122.16036590797935, 'time': 10.70808712500002, 'accuracy': 0.0111328125, 'total_cost': 538145.8393158102}
[run job] Launching job with BS 256: and LR: 1.5811388300841898e-05 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs256+lr0.00002+pl100', 'ZEUS_COST_THRESH': '979372.5159777194', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '1.5811388300841898e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '1.5811388300841898e-05', '--power_limit', '100']
[run job] cost_ub=979372.5159777194
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs256+lr0.00002+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 1.5811388300841898e-05
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 4.6081
Training Epoch: 0 [768/50176]	Loss: 4.6183
Training Epoch: 0 [1024/50176]	Loss: 4.6252
Training Epoch: 0 [1280/50176]	Loss: 4.6463
Training Epoch: 0 [1536/50176]	Loss: 4.6374
Training Epoch: 0 [1792/50176]	Loss: 4.6180
Training Epoch: 0 [2048/50176]	Loss: 4.6552
Training Epoch: 0 [2304/50176]	Loss: 4.6236
Training Epoch: 0 [2560/50176]	Loss: 4.6212
Training Epoch: 0 [2816/50176]	Loss: 4.6124
Training Epoch: 0 [3072/50176]	Loss: 4.6335
Training Epoch: 0 [3328/50176]	Loss: 4.6279
Training Epoch: 0 [3584/50176]	Loss: 4.6129
Training Epoch: 0 [3840/50176]	Loss: 4.6174
Training Epoch: 0 [4096/50176]	Loss: 4.6027
Training Epoch: 0 [4352/50176]	Loss: 4.6236
Training Epoch: 0 [4608/50176]	Loss: 4.6449
Training Epoch: 0 [4864/50176]	Loss: 4.6139
Training Epoch: 0 [5120/50176]	Loss: 4.6247
Training Epoch: 0 [5376/50176]	Loss: 4.6067
Training Epoch: 0 [5632/50176]	Loss: 4.6265
Training Epoch: 0 [5888/50176]	Loss: 4.6324
Training Epoch: 0 [6144/50176]	Loss: 4.6161
Training Epoch: 0 [6400/50176]	Loss: 4.6291
Training Epoch: 0 [6656/50176]	Loss: 4.6312
Training Epoch: 0 [6912/50176]	Loss: 4.6184
Training Epoch: 0 [7168/50176]	Loss: 4.6386
Training Epoch: 0 [7424/50176]	Loss: 4.6272
Training Epoch: 0 [7680/50176]	Loss: 4.6146
Training Epoch: 0 [7936/50176]	Loss: 4.6213
Training Epoch: 0 [8192/50176]	Loss: 4.6350
Training Epoch: 0 [8448/50176]	Loss: 4.6316
Training Epoch: 0 [8704/50176]	Loss: 4.6125
Training Epoch: 0 [8960/50176]	Loss: 4.6050
Training Epoch: 0 [9216/50176]	Loss: 4.6279
Training Epoch: 0 [9472/50176]	Loss: 4.6129
Training Epoch: 0 [9728/50176]	Loss: 4.6109
Training Epoch: 0 [9984/50176]	Loss: 4.6054
Training Epoch: 0 [10240/50176]	Loss: 4.6283
Training Epoch: 0 [10496/50176]	Loss: 4.6206
Training Epoch: 0 [10752/50176]	Loss: 4.6112
Training Epoch: 0 [11008/50176]	Loss: 4.6099
Training Epoch: 0 [11264/50176]	Loss: 4.6052
Training Epoch: 0 [11520/50176]	Loss: 4.5857
Training Epoch: 0 [11776/50176]	Loss: 4.6069
Training Epoch: 0 [12032/50176]	Loss: 4.6273
Training Epoch: 0 [12288/50176]	Loss: 4.6268
Training Epoch: 0 [12544/50176]	Loss: 4.6078
Training Epoch: 0 [12800/50176]	Loss: 4.5773
Profile done with power limit 100W
epoch 1 train time consumed: 17.79s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0113
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 100000, 'lr': 1.5811388300841898e-05, 'energy': 98.48512490409874, 'time': 13.148589262999849, 'accuracy': 0.011328125, 'total_cost': 648822.6674232619}

[Power Profiler] with batch size 256 and learning rate 0.00011236102527122117
[run job] Launching job with BS 256: and LR: 0.00011236102527122117 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs256+lr0.00011+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.00011236102527122117', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00011236102527122117', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs256+lr0.00011+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.00011236102527122117
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 4.6078
Training Epoch: 0 [768/50176]	Loss: 4.6151
Training Epoch: 0 [1024/50176]	Loss: 4.6201
Training Epoch: 0 [1280/50176]	Loss: 4.6292
Training Epoch: 0 [1536/50176]	Loss: 4.6265
Training Epoch: 0 [1792/50176]	Loss: 4.6096
Training Epoch: 0 [2048/50176]	Loss: 4.6370
Training Epoch: 0 [2304/50176]	Loss: 4.6191
Training Epoch: 0 [2560/50176]	Loss: 4.6109
Training Epoch: 0 [2816/50176]	Loss: 4.6078
Training Epoch: 0 [3072/50176]	Loss: 4.6026
Training Epoch: 0 [3328/50176]	Loss: 4.5981
Training Epoch: 0 [3584/50176]	Loss: 4.5841
Training Epoch: 0 [3840/50176]	Loss: 4.6092
Training Epoch: 0 [4096/50176]	Loss: 4.5906
Training Epoch: 0 [4352/50176]	Loss: 4.5926
Training Epoch: 0 [4608/50176]	Loss: 4.5836
Training Epoch: 0 [4864/50176]	Loss: 4.5734
Training Epoch: 0 [5120/50176]	Loss: 4.5767
Training Epoch: 0 [5376/50176]	Loss: 4.5781
Training Epoch: 0 [5632/50176]	Loss: 4.5911
Training Epoch: 0 [5888/50176]	Loss: 4.5842
Training Epoch: 0 [6144/50176]	Loss: 4.5612
Training Epoch: 0 [6400/50176]	Loss: 4.5843
Training Epoch: 0 [6656/50176]	Loss: 4.5869
Training Epoch: 0 [6912/50176]	Loss: 4.5745
Training Epoch: 0 [7168/50176]	Loss: 4.5808
Training Epoch: 0 [7424/50176]	Loss: 4.5680
Training Epoch: 0 [7680/50176]	Loss: 4.5489
Training Epoch: 0 [7936/50176]	Loss: 4.5736
Training Epoch: 0 [8192/50176]	Loss: 4.5872
Training Epoch: 0 [8448/50176]	Loss: 4.5701
Training Epoch: 0 [8704/50176]	Loss: 4.5649
Training Epoch: 0 [8960/50176]	Loss: 4.5493
Training Epoch: 0 [9216/50176]	Loss: 4.5841
Training Epoch: 0 [9472/50176]	Loss: 4.5525
Training Epoch: 0 [9728/50176]	Loss: 4.5462
Training Epoch: 0 [9984/50176]	Loss: 4.5580
Training Epoch: 0 [10240/50176]	Loss: 4.5625
Training Epoch: 0 [10496/50176]	Loss: 4.5686
Training Epoch: 0 [10752/50176]	Loss: 4.5457
Training Epoch: 0 [11008/50176]	Loss: 4.5361
Training Epoch: 0 [11264/50176]	Loss: 4.5466
Training Epoch: 0 [11520/50176]	Loss: 4.5530
Training Epoch: 0 [11776/50176]	Loss: 4.5469
Training Epoch: 0 [12032/50176]	Loss: 4.5626
Training Epoch: 0 [12288/50176]	Loss: 4.5322
Training Epoch: 0 [12544/50176]	Loss: 4.5380
Training Epoch: 0 [12800/50176]	Loss: 4.5082
Profile done with power limit 175W
epoch 1 train time consumed: 13.05s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0120
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 175000, 'lr': 0.00011236102527122117, 'energy': 146.9971672481358, 'time': 9.793276755000079, 'accuracy': 0.01201171875, 'total_cost': 456533.15249774395}
[run job] Launching job with BS 256: and LR: 0.00011236102527122117 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs256+lr0.00011+pl150', 'ZEUS_COST_THRESH': '913066.3049954879', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.00011236102527122117', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00011236102527122117', '--power_limit', '150']
[run job] cost_ub=913066.3049954879
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs256+lr0.00011+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.00011236102527122117
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 4.6078
Training Epoch: 0 [768/50176]	Loss: 4.6151
Training Epoch: 0 [1024/50176]	Loss: 4.6200
Training Epoch: 0 [1280/50176]	Loss: 4.6292
Training Epoch: 0 [1536/50176]	Loss: 4.6262
Training Epoch: 0 [1792/50176]	Loss: 4.6094
Training Epoch: 0 [2048/50176]	Loss: 4.6381
Training Epoch: 0 [2304/50176]	Loss: 4.6195
Training Epoch: 0 [2560/50176]	Loss: 4.6097
Training Epoch: 0 [2816/50176]	Loss: 4.6075
Training Epoch: 0 [3072/50176]	Loss: 4.6030
Training Epoch: 0 [3328/50176]	Loss: 4.5965
Training Epoch: 0 [3584/50176]	Loss: 4.5813
Training Epoch: 0 [3840/50176]	Loss: 4.6091
Training Epoch: 0 [4096/50176]	Loss: 4.5926
Training Epoch: 0 [4352/50176]	Loss: 4.5944
Training Epoch: 0 [4608/50176]	Loss: 4.5882
Training Epoch: 0 [4864/50176]	Loss: 4.5766
Training Epoch: 0 [5120/50176]	Loss: 4.5765
Training Epoch: 0 [5376/50176]	Loss: 4.5737
Training Epoch: 0 [5632/50176]	Loss: 4.5844
Training Epoch: 0 [5888/50176]	Loss: 4.5833
Training Epoch: 0 [6144/50176]	Loss: 4.5640
Training Epoch: 0 [6400/50176]	Loss: 4.5862
Training Epoch: 0 [6656/50176]	Loss: 4.5912
Training Epoch: 0 [6912/50176]	Loss: 4.5764
Training Epoch: 0 [7168/50176]	Loss: 4.5797
Training Epoch: 0 [7424/50176]	Loss: 4.5705
Training Epoch: 0 [7680/50176]	Loss: 4.5579
Training Epoch: 0 [7936/50176]	Loss: 4.5752
Training Epoch: 0 [8192/50176]	Loss: 4.5859
Training Epoch: 0 [8448/50176]	Loss: 4.5666
Training Epoch: 0 [8704/50176]	Loss: 4.5735
Training Epoch: 0 [8960/50176]	Loss: 4.5490
Training Epoch: 0 [9216/50176]	Loss: 4.5858
Training Epoch: 0 [9472/50176]	Loss: 4.5524
Training Epoch: 0 [9728/50176]	Loss: 4.5543
Training Epoch: 0 [9984/50176]	Loss: 4.5643
Training Epoch: 0 [10240/50176]	Loss: 4.5657
Training Epoch: 0 [10496/50176]	Loss: 4.5737
Training Epoch: 0 [10752/50176]	Loss: 4.5555
Training Epoch: 0 [11008/50176]	Loss: 4.5320
Training Epoch: 0 [11264/50176]	Loss: 4.5533
Training Epoch: 0 [11520/50176]	Loss: 4.5542
Training Epoch: 0 [11776/50176]	Loss: 4.5470
Training Epoch: 0 [12032/50176]	Loss: 4.5551
Training Epoch: 0 [12288/50176]	Loss: 4.5292
Training Epoch: 0 [12544/50176]	Loss: 4.5348
Training Epoch: 0 [12800/50176]	Loss: 4.5180
Profile done with power limit 150W
epoch 1 train time consumed: 13.02s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0110
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 150000, 'lr': 0.00011236102527122117, 'energy': 145.38528202126685, 'time': 9.738714594999692, 'accuracy': 0.01103515625, 'total_cost': 494156.5368700153}
[run job] Launching job with BS 256: and LR: 0.00011236102527122117 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs256+lr0.00011+pl125', 'ZEUS_COST_THRESH': '913066.3049954879', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.00011236102527122117', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00011236102527122117', '--power_limit', '125']
[run job] cost_ub=913066.3049954879
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs256+lr0.00011+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.00011236102527122117
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 4.6078
Training Epoch: 0 [768/50176]	Loss: 4.6151
Training Epoch: 0 [1024/50176]	Loss: 4.6202
Training Epoch: 0 [1280/50176]	Loss: 4.6294
Training Epoch: 0 [1536/50176]	Loss: 4.6267
Training Epoch: 0 [1792/50176]	Loss: 4.6099
Training Epoch: 0 [2048/50176]	Loss: 4.6364
Training Epoch: 0 [2304/50176]	Loss: 4.6180
Training Epoch: 0 [2560/50176]	Loss: 4.6105
Training Epoch: 0 [2816/50176]	Loss: 4.6084
Training Epoch: 0 [3072/50176]	Loss: 4.6017
Training Epoch: 0 [3328/50176]	Loss: 4.5979
Training Epoch: 0 [3584/50176]	Loss: 4.5834
Training Epoch: 0 [3840/50176]	Loss: 4.6077
Training Epoch: 0 [4096/50176]	Loss: 4.5897
Training Epoch: 0 [4352/50176]	Loss: 4.5895
Training Epoch: 0 [4608/50176]	Loss: 4.5839
Training Epoch: 0 [4864/50176]	Loss: 4.5746
Training Epoch: 0 [5120/50176]	Loss: 4.5764
Training Epoch: 0 [5376/50176]	Loss: 4.5785
Training Epoch: 0 [5632/50176]	Loss: 4.5896
Training Epoch: 0 [5888/50176]	Loss: 4.5863
Training Epoch: 0 [6144/50176]	Loss: 4.5624
Training Epoch: 0 [6400/50176]	Loss: 4.5869
Training Epoch: 0 [6656/50176]	Loss: 4.5876
Training Epoch: 0 [6912/50176]	Loss: 4.5730
Training Epoch: 0 [7168/50176]	Loss: 4.5791
Training Epoch: 0 [7424/50176]	Loss: 4.5710
Training Epoch: 0 [7680/50176]	Loss: 4.5529
Training Epoch: 0 [7936/50176]	Loss: 4.5707
Training Epoch: 0 [8192/50176]	Loss: 4.5907
Training Epoch: 0 [8448/50176]	Loss: 4.5718
Training Epoch: 0 [8704/50176]	Loss: 4.5731
Training Epoch: 0 [8960/50176]	Loss: 4.5484
Training Epoch: 0 [9216/50176]	Loss: 4.5817
Training Epoch: 0 [9472/50176]	Loss: 4.5517
Training Epoch: 0 [9728/50176]	Loss: 4.5533
Training Epoch: 0 [9984/50176]	Loss: 4.5585
Training Epoch: 0 [10240/50176]	Loss: 4.5616
Training Epoch: 0 [10496/50176]	Loss: 4.5652
Training Epoch: 0 [10752/50176]	Loss: 4.5480
Training Epoch: 0 [11008/50176]	Loss: 4.5382
Training Epoch: 0 [11264/50176]	Loss: 4.5479
Training Epoch: 0 [11520/50176]	Loss: 4.5480
Training Epoch: 0 [11776/50176]	Loss: 4.5442
Training Epoch: 0 [12032/50176]	Loss: 4.5574
Training Epoch: 0 [12288/50176]	Loss: 4.5282
Training Epoch: 0 [12544/50176]	Loss: 4.5357
Training Epoch: 0 [12800/50176]	Loss: 4.5073
Profile done with power limit 125W
epoch 1 train time consumed: 14.15s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0114
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 125000, 'lr': 0.00011236102527122117, 'energy': 122.7362195355823, 'time': 10.705316474999563, 'accuracy': 0.01142578125, 'total_cost': 524218.33787722926}
[run job] Launching job with BS 256: and LR: 0.00011236102527122117 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs256+lr0.00011+pl100', 'ZEUS_COST_THRESH': '913066.3049954879', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.00011236102527122117', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00011236102527122117', '--power_limit', '100']
[run job] cost_ub=913066.3049954879
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs256+lr0.00011+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.00011236102527122117
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 4.6078
Training Epoch: 0 [768/50176]	Loss: 4.6151
Training Epoch: 0 [1024/50176]	Loss: 4.6199
Training Epoch: 0 [1280/50176]	Loss: 4.6292
Training Epoch: 0 [1536/50176]	Loss: 4.6263
Training Epoch: 0 [1792/50176]	Loss: 4.6097
Training Epoch: 0 [2048/50176]	Loss: 4.6386
Training Epoch: 0 [2304/50176]	Loss: 4.6181
Training Epoch: 0 [2560/50176]	Loss: 4.6105
Training Epoch: 0 [2816/50176]	Loss: 4.6074
Training Epoch: 0 [3072/50176]	Loss: 4.5993
Training Epoch: 0 [3328/50176]	Loss: 4.5983
Training Epoch: 0 [3584/50176]	Loss: 4.5803
Training Epoch: 0 [3840/50176]	Loss: 4.6094
Training Epoch: 0 [4096/50176]	Loss: 4.5869
Training Epoch: 0 [4352/50176]	Loss: 4.5934
Training Epoch: 0 [4608/50176]	Loss: 4.5841
Training Epoch: 0 [4864/50176]	Loss: 4.5760
Training Epoch: 0 [5120/50176]	Loss: 4.5807
Training Epoch: 0 [5376/50176]	Loss: 4.5796
Training Epoch: 0 [5632/50176]	Loss: 4.5840
Training Epoch: 0 [5888/50176]	Loss: 4.5855
Training Epoch: 0 [6144/50176]	Loss: 4.5656
Training Epoch: 0 [6400/50176]	Loss: 4.5828
Training Epoch: 0 [6656/50176]	Loss: 4.5914
Training Epoch: 0 [6912/50176]	Loss: 4.5748
Training Epoch: 0 [7168/50176]	Loss: 4.5787
Training Epoch: 0 [7424/50176]	Loss: 4.5689
Training Epoch: 0 [7680/50176]	Loss: 4.5534
Training Epoch: 0 [7936/50176]	Loss: 4.5757
Training Epoch: 0 [8192/50176]	Loss: 4.5871
Training Epoch: 0 [8448/50176]	Loss: 4.5696
Training Epoch: 0 [8704/50176]	Loss: 4.5693
Training Epoch: 0 [8960/50176]	Loss: 4.5490
Training Epoch: 0 [9216/50176]	Loss: 4.5859
Training Epoch: 0 [9472/50176]	Loss: 4.5451
Training Epoch: 0 [9728/50176]	Loss: 4.5509
Training Epoch: 0 [9984/50176]	Loss: 4.5553
Training Epoch: 0 [10240/50176]	Loss: 4.5622
Training Epoch: 0 [10496/50176]	Loss: 4.5716
Training Epoch: 0 [10752/50176]	Loss: 4.5444
Training Epoch: 0 [11008/50176]	Loss: 4.5358
Training Epoch: 0 [11264/50176]	Loss: 4.5534
Training Epoch: 0 [11520/50176]	Loss: 4.5476
Training Epoch: 0 [11776/50176]	Loss: 4.5386
Training Epoch: 0 [12032/50176]	Loss: 4.5600
Training Epoch: 0 [12288/50176]	Loss: 4.5308
Training Epoch: 0 [12544/50176]	Loss: 4.5391
Training Epoch: 0 [12800/50176]	Loss: 4.5115
Profile done with power limit 100W
epoch 1 train time consumed: 17.17s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0104
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 100000, 'lr': 0.00011236102527122117, 'energy': 98.55861873738141, 'time': 13.168525174000024, 'accuracy': 0.01044921875, 'total_cost': 704462.1019109669}

[Power Profiler] with batch size 512 and learning rate 2.2360679774997898e-05
[run job] Launching job with BS 512: and LR: 2.2360679774997898e-05 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs512+lr0.00002+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '2.2360679774997898e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '2.2360679774997898e-05', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs512+lr0.00002+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 2.2360679774997898e-05
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 4.6304
Training Epoch: 0 [1536/50176]	Loss: 4.6415
Training Epoch: 0 [2048/50176]	Loss: 4.6382
Training Epoch: 0 [2560/50176]	Loss: 4.6258
Training Epoch: 0 [3072/50176]	Loss: 4.6255
Training Epoch: 0 [3584/50176]	Loss: 4.6194
Training Epoch: 0 [4096/50176]	Loss: 4.6173
Training Epoch: 0 [4608/50176]	Loss: 4.6336
Training Epoch: 0 [5120/50176]	Loss: 4.6190
Training Epoch: 0 [5632/50176]	Loss: 4.6101
Training Epoch: 0 [6144/50176]	Loss: 4.6293
Training Epoch: 0 [6656/50176]	Loss: 4.6329
Training Epoch: 0 [7168/50176]	Loss: 4.6280
Training Epoch: 0 [7680/50176]	Loss: 4.6234
Training Epoch: 0 [8192/50176]	Loss: 4.6376
Training Epoch: 0 [8704/50176]	Loss: 4.6249
Training Epoch: 0 [9216/50176]	Loss: 4.6212
Training Epoch: 0 [9728/50176]	Loss: 4.6141
Training Epoch: 0 [10240/50176]	Loss: 4.6179
Training Epoch: 0 [10752/50176]	Loss: 4.6107
Training Epoch: 0 [11264/50176]	Loss: 4.6094
Training Epoch: 0 [11776/50176]	Loss: 4.6014
Training Epoch: 0 [12288/50176]	Loss: 4.6242
Training Epoch: 0 [12800/50176]	Loss: 4.6047
Training Epoch: 0 [13312/50176]	Loss: 4.6079
Training Epoch: 0 [13824/50176]	Loss: 4.6107
Training Epoch: 0 [14336/50176]	Loss: 4.6202
Training Epoch: 0 [14848/50176]	Loss: 4.6120
Training Epoch: 0 [15360/50176]	Loss: 4.5932
Training Epoch: 0 [15872/50176]	Loss: 4.6118
Training Epoch: 0 [16384/50176]	Loss: 4.6037
Training Epoch: 0 [16896/50176]	Loss: 4.5959
Training Epoch: 0 [17408/50176]	Loss: 4.6102
Training Epoch: 0 [17920/50176]	Loss: 4.6006
Training Epoch: 0 [18432/50176]	Loss: 4.5985
Training Epoch: 0 [18944/50176]	Loss: 4.5973
Training Epoch: 0 [19456/50176]	Loss: 4.6006
Training Epoch: 0 [19968/50176]	Loss: 4.5965
Training Epoch: 0 [20480/50176]	Loss: 4.5963
Training Epoch: 0 [20992/50176]	Loss: 4.5939
Training Epoch: 0 [21504/50176]	Loss: 4.6141
Training Epoch: 0 [22016/50176]	Loss: 4.5956
Training Epoch: 0 [22528/50176]	Loss: 4.6120
Training Epoch: 0 [23040/50176]	Loss: 4.5997
Training Epoch: 0 [23552/50176]	Loss: 4.5924
Training Epoch: 0 [24064/50176]	Loss: 4.6060
Training Epoch: 0 [24576/50176]	Loss: 4.6042
Training Epoch: 0 [25088/50176]	Loss: 4.5915
Training Epoch: 0 [25600/50176]	Loss: 4.5938
Profile done with power limit 175W
epoch 1 train time consumed: 24.37s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0106
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 175000, 'lr': 2.2360679774997898e-05, 'energy': 147.88957520096696, 'time': 18.652101215000584, 'accuracy': 0.01064453125, 'total_cost': 1959203.6221730108}
[run job] Launching job with BS 512: and LR: 2.2360679774997898e-05 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs512+lr0.00002+pl150', 'ZEUS_COST_THRESH': '3918407.2443460217', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '2.2360679774997898e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '2.2360679774997898e-05', '--power_limit', '150']
[run job] cost_ub=3918407.2443460217
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs512+lr0.00002+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 2.2360679774997898e-05
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 4.6304
Training Epoch: 0 [1536/50176]	Loss: 4.6415
Training Epoch: 0 [2048/50176]	Loss: 4.6382
Training Epoch: 0 [2560/50176]	Loss: 4.6258
Training Epoch: 0 [3072/50176]	Loss: 4.6255
Training Epoch: 0 [3584/50176]	Loss: 4.6194
Training Epoch: 0 [4096/50176]	Loss: 4.6173
Training Epoch: 0 [4608/50176]	Loss: 4.6336
Training Epoch: 0 [5120/50176]	Loss: 4.6190
Training Epoch: 0 [5632/50176]	Loss: 4.6101
Training Epoch: 0 [6144/50176]	Loss: 4.6292
Training Epoch: 0 [6656/50176]	Loss: 4.6329
Training Epoch: 0 [7168/50176]	Loss: 4.6279
Training Epoch: 0 [7680/50176]	Loss: 4.6232
Training Epoch: 0 [8192/50176]	Loss: 4.6376
Training Epoch: 0 [8704/50176]	Loss: 4.6249
Training Epoch: 0 [9216/50176]	Loss: 4.6214
Training Epoch: 0 [9728/50176]	Loss: 4.6145
Training Epoch: 0 [10240/50176]	Loss: 4.6176
Training Epoch: 0 [10752/50176]	Loss: 4.6105
Training Epoch: 0 [11264/50176]	Loss: 4.6096
Training Epoch: 0 [11776/50176]	Loss: 4.6019
Training Epoch: 0 [12288/50176]	Loss: 4.6242
Training Epoch: 0 [12800/50176]	Loss: 4.6045
Training Epoch: 0 [13312/50176]	Loss: 4.6074
Training Epoch: 0 [13824/50176]	Loss: 4.6106
Training Epoch: 0 [14336/50176]	Loss: 4.6203
Training Epoch: 0 [14848/50176]	Loss: 4.6117
Training Epoch: 0 [15360/50176]	Loss: 4.5928
Training Epoch: 0 [15872/50176]	Loss: 4.6109
Training Epoch: 0 [16384/50176]	Loss: 4.6037
Training Epoch: 0 [16896/50176]	Loss: 4.5960
Training Epoch: 0 [17408/50176]	Loss: 4.6092
Training Epoch: 0 [17920/50176]	Loss: 4.6004
Training Epoch: 0 [18432/50176]	Loss: 4.5977
Training Epoch: 0 [18944/50176]	Loss: 4.5980
Training Epoch: 0 [19456/50176]	Loss: 4.6003
Training Epoch: 0 [19968/50176]	Loss: 4.5967
Training Epoch: 0 [20480/50176]	Loss: 4.5979
Training Epoch: 0 [20992/50176]	Loss: 4.5936
Training Epoch: 0 [21504/50176]	Loss: 4.6145
Training Epoch: 0 [22016/50176]	Loss: 4.5955
Training Epoch: 0 [22528/50176]	Loss: 4.6131
Training Epoch: 0 [23040/50176]	Loss: 4.5989
Training Epoch: 0 [23552/50176]	Loss: 4.5924
Training Epoch: 0 [24064/50176]	Loss: 4.6070
Training Epoch: 0 [24576/50176]	Loss: 4.6026
Training Epoch: 0 [25088/50176]	Loss: 4.5904
Training Epoch: 0 [25600/50176]	Loss: 4.5928
Profile done with power limit 150W
epoch 1 train time consumed: 24.48s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0109
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 150000, 'lr': 2.2360679774997898e-05, 'energy': 146.00962981095824, 'time': 18.67574999000044, 'accuracy': 0.0109375, 'total_cost': 1909094.2469052568}
[run job] Launching job with BS 512: and LR: 2.2360679774997898e-05 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs512+lr0.00002+pl125', 'ZEUS_COST_THRESH': '3818188.4938105135', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '2.2360679774997898e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '2.2360679774997898e-05', '--power_limit', '125']
[run job] cost_ub=3818188.4938105135
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs512+lr0.00002+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 2.2360679774997898e-05
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 4.6304
Training Epoch: 0 [1536/50176]	Loss: 4.6415
Training Epoch: 0 [2048/50176]	Loss: 4.6382
Training Epoch: 0 [2560/50176]	Loss: 4.6258
Training Epoch: 0 [3072/50176]	Loss: 4.6255
Training Epoch: 0 [3584/50176]	Loss: 4.6194
Training Epoch: 0 [4096/50176]	Loss: 4.6173
Training Epoch: 0 [4608/50176]	Loss: 4.6336
Training Epoch: 0 [5120/50176]	Loss: 4.6191
Training Epoch: 0 [5632/50176]	Loss: 4.6102
Training Epoch: 0 [6144/50176]	Loss: 4.6294
Training Epoch: 0 [6656/50176]	Loss: 4.6328
Training Epoch: 0 [7168/50176]	Loss: 4.6280
Training Epoch: 0 [7680/50176]	Loss: 4.6232
Training Epoch: 0 [8192/50176]	Loss: 4.6377
Training Epoch: 0 [8704/50176]	Loss: 4.6249
Training Epoch: 0 [9216/50176]	Loss: 4.6211
Training Epoch: 0 [9728/50176]	Loss: 4.6142
Training Epoch: 0 [10240/50176]	Loss: 4.6181
Training Epoch: 0 [10752/50176]	Loss: 4.6105
Training Epoch: 0 [11264/50176]	Loss: 4.6096
Training Epoch: 0 [11776/50176]	Loss: 4.6017
Training Epoch: 0 [12288/50176]	Loss: 4.6242
Training Epoch: 0 [12800/50176]	Loss: 4.6054
Training Epoch: 0 [13312/50176]	Loss: 4.6070
Training Epoch: 0 [13824/50176]	Loss: 4.6111
Training Epoch: 0 [14336/50176]	Loss: 4.6203
Training Epoch: 0 [14848/50176]	Loss: 4.6120
Training Epoch: 0 [15360/50176]	Loss: 4.5925
Training Epoch: 0 [15872/50176]	Loss: 4.6113
Training Epoch: 0 [16384/50176]	Loss: 4.6034
Training Epoch: 0 [16896/50176]	Loss: 4.5964
Training Epoch: 0 [17408/50176]	Loss: 4.6094
Training Epoch: 0 [17920/50176]	Loss: 4.6002
Training Epoch: 0 [18432/50176]	Loss: 4.5981
Training Epoch: 0 [18944/50176]	Loss: 4.5976
Training Epoch: 0 [19456/50176]	Loss: 4.6012
Training Epoch: 0 [19968/50176]	Loss: 4.5966
Training Epoch: 0 [20480/50176]	Loss: 4.5969
Training Epoch: 0 [20992/50176]	Loss: 4.5921
Training Epoch: 0 [21504/50176]	Loss: 4.6148
Training Epoch: 0 [22016/50176]	Loss: 4.5963
Training Epoch: 0 [22528/50176]	Loss: 4.6123
Training Epoch: 0 [23040/50176]	Loss: 4.5990
Training Epoch: 0 [23552/50176]	Loss: 4.5912
Training Epoch: 0 [24064/50176]	Loss: 4.6056
Training Epoch: 0 [24576/50176]	Loss: 4.6019
Training Epoch: 0 [25088/50176]	Loss: 4.5904
Training Epoch: 0 [25600/50176]	Loss: 4.5924
Profile done with power limit 125W
epoch 1 train time consumed: 26.63s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0109
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 125000, 'lr': 2.2360679774997898e-05, 'energy': 122.93025455644637, 'time': 20.508353145, 'accuracy': 0.0109375, 'total_cost': 2095556.2743774103}
[run job] Launching job with BS 512: and LR: 2.2360679774997898e-05 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs512+lr0.00002+pl100', 'ZEUS_COST_THRESH': '3818188.4938105135', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '2.2360679774997898e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '2.2360679774997898e-05', '--power_limit', '100']
[run job] cost_ub=3818188.4938105135
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs512+lr0.00002+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 2.2360679774997898e-05
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 4.6304
Training Epoch: 0 [1536/50176]	Loss: 4.6415
Training Epoch: 0 [2048/50176]	Loss: 4.6382
Training Epoch: 0 [2560/50176]	Loss: 4.6258
Training Epoch: 0 [3072/50176]	Loss: 4.6255
Training Epoch: 0 [3584/50176]	Loss: 4.6194
Training Epoch: 0 [4096/50176]	Loss: 4.6173
Training Epoch: 0 [4608/50176]	Loss: 4.6336
Training Epoch: 0 [5120/50176]	Loss: 4.6191
Training Epoch: 0 [5632/50176]	Loss: 4.6103
Training Epoch: 0 [6144/50176]	Loss: 4.6292
Training Epoch: 0 [6656/50176]	Loss: 4.6329
Training Epoch: 0 [7168/50176]	Loss: 4.6279
Training Epoch: 0 [7680/50176]	Loss: 4.6232
Training Epoch: 0 [8192/50176]	Loss: 4.6377
Training Epoch: 0 [8704/50176]	Loss: 4.6249
Training Epoch: 0 [9216/50176]	Loss: 4.6213
Training Epoch: 0 [9728/50176]	Loss: 4.6144
Training Epoch: 0 [10240/50176]	Loss: 4.6177
Training Epoch: 0 [10752/50176]	Loss: 4.6106
Training Epoch: 0 [11264/50176]	Loss: 4.6094
Training Epoch: 0 [11776/50176]	Loss: 4.6016
Training Epoch: 0 [12288/50176]	Loss: 4.6241
Training Epoch: 0 [12800/50176]	Loss: 4.6049
Training Epoch: 0 [13312/50176]	Loss: 4.6075
Training Epoch: 0 [13824/50176]	Loss: 4.6109
Training Epoch: 0 [14336/50176]	Loss: 4.6203
Training Epoch: 0 [14848/50176]	Loss: 4.6122
Training Epoch: 0 [15360/50176]	Loss: 4.5925
Training Epoch: 0 [15872/50176]	Loss: 4.6108
Training Epoch: 0 [16384/50176]	Loss: 4.6032
Training Epoch: 0 [16896/50176]	Loss: 4.5962
Training Epoch: 0 [17408/50176]	Loss: 4.6091
Training Epoch: 0 [17920/50176]	Loss: 4.6006
Training Epoch: 0 [18432/50176]	Loss: 4.5977
Training Epoch: 0 [18944/50176]	Loss: 4.5977
Training Epoch: 0 [19456/50176]	Loss: 4.6002
Training Epoch: 0 [19968/50176]	Loss: 4.5966
Training Epoch: 0 [20480/50176]	Loss: 4.5977
Training Epoch: 0 [20992/50176]	Loss: 4.5917
Training Epoch: 0 [21504/50176]	Loss: 4.6142
Training Epoch: 0 [22016/50176]	Loss: 4.5964
Training Epoch: 0 [22528/50176]	Loss: 4.6126
Training Epoch: 0 [23040/50176]	Loss: 4.5996
Training Epoch: 0 [23552/50176]	Loss: 4.5921
Training Epoch: 0 [24064/50176]	Loss: 4.6060
Training Epoch: 0 [24576/50176]	Loss: 4.6019
Training Epoch: 0 [25088/50176]	Loss: 4.5896
Training Epoch: 0 [25600/50176]	Loss: 4.5933
Profile done with power limit 100W
epoch 1 train time consumed: 51.72s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0107
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 100000, 'lr': 2.2360679774997898e-05, 'energy': 96.48015361701886, 'time': 40.361692316999324, 'accuracy': 0.0107421875, 'total_cost': 4195713.758289908}

[Power Profiler] with batch size 512 and learning rate 0.00015890248582070703
[run job] Launching job with BS 512: and LR: 0.00015890248582070703 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs512+lr0.00016+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.00015890248582070703', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00015890248582070703', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs512+lr0.00016+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.00015890248582070703
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 4.6246
Training Epoch: 0 [1536/50176]	Loss: 4.6326
Training Epoch: 0 [2048/50176]	Loss: 4.6282
Training Epoch: 0 [2560/50176]	Loss: 4.6154
Training Epoch: 0 [3072/50176]	Loss: 4.6038
Training Epoch: 0 [3584/50176]	Loss: 4.5893
Training Epoch: 0 [4096/50176]	Loss: 4.6103
Training Epoch: 0 [4608/50176]	Loss: 4.5945
Training Epoch: 0 [5120/50176]	Loss: 4.5832
Training Epoch: 0 [5632/50176]	Loss: 4.5765
Training Epoch: 0 [6144/50176]	Loss: 4.5798
Training Epoch: 0 [6656/50176]	Loss: 4.5975
Training Epoch: 0 [7168/50176]	Loss: 4.5861
Training Epoch: 0 [7680/50176]	Loss: 4.5679
Training Epoch: 0 [8192/50176]	Loss: 4.5884
Training Epoch: 0 [8704/50176]	Loss: 4.5739
Training Epoch: 0 [9216/50176]	Loss: 4.5742
Training Epoch: 0 [9728/50176]	Loss: 4.5576
Training Epoch: 0 [10240/50176]	Loss: 4.5578
Training Epoch: 0 [10752/50176]	Loss: 4.5602
Training Epoch: 0 [11264/50176]	Loss: 4.5505
Training Epoch: 0 [11776/50176]	Loss: 4.5631
Training Epoch: 0 [12288/50176]	Loss: 4.5485
Training Epoch: 0 [12800/50176]	Loss: 4.5414
Training Epoch: 0 [13312/50176]	Loss: 4.5503
Training Epoch: 0 [13824/50176]	Loss: 4.5553
Training Epoch: 0 [14336/50176]	Loss: 4.5564
Training Epoch: 0 [14848/50176]	Loss: 4.5508
Training Epoch: 0 [15360/50176]	Loss: 4.5226
Training Epoch: 0 [15872/50176]	Loss: 4.5237
Training Epoch: 0 [16384/50176]	Loss: 4.5290
Training Epoch: 0 [16896/50176]	Loss: 4.5125
Training Epoch: 0 [17408/50176]	Loss: 4.5629
Training Epoch: 0 [17920/50176]	Loss: 4.5114
Training Epoch: 0 [18432/50176]	Loss: 4.5218
Training Epoch: 0 [18944/50176]	Loss: 4.4938
Training Epoch: 0 [19456/50176]	Loss: 4.5057
Training Epoch: 0 [19968/50176]	Loss: 4.5221
Training Epoch: 0 [20480/50176]	Loss: 4.5137
Training Epoch: 0 [20992/50176]	Loss: 4.5098
Training Epoch: 0 [21504/50176]	Loss: 4.5296
Training Epoch: 0 [22016/50176]	Loss: 4.4934
Training Epoch: 0 [22528/50176]	Loss: 4.5186
Training Epoch: 0 [23040/50176]	Loss: 4.4815
Training Epoch: 0 [23552/50176]	Loss: 4.4999
Training Epoch: 0 [24064/50176]	Loss: 4.5132
Training Epoch: 0 [24576/50176]	Loss: 4.5128
Training Epoch: 0 [25088/50176]	Loss: 4.4919
Training Epoch: 0 [25600/50176]	Loss: 4.4800
Profile done with power limit 175W
epoch 1 train time consumed: 24.57s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0136
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 175000, 'lr': 0.00015890248582070703, 'energy': 147.76453761990356, 'time': 18.60461743199994, 'accuracy': 0.01357421875, 'total_cost': 1532447.4559881538}
[run job] Launching job with BS 512: and LR: 0.00015890248582070703 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs512+lr0.00016+pl150', 'ZEUS_COST_THRESH': '3064894.9119763076', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.00015890248582070703', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00015890248582070703', '--power_limit', '150']
[run job] cost_ub=3064894.9119763076
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs512+lr0.00016+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.00015890248582070703
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 4.6246
Training Epoch: 0 [1536/50176]	Loss: 4.6326
Training Epoch: 0 [2048/50176]	Loss: 4.6282
Training Epoch: 0 [2560/50176]	Loss: 4.6151
Training Epoch: 0 [3072/50176]	Loss: 4.6038
Training Epoch: 0 [3584/50176]	Loss: 4.5885
Training Epoch: 0 [4096/50176]	Loss: 4.6092
Training Epoch: 0 [4608/50176]	Loss: 4.5915
Training Epoch: 0 [5120/50176]	Loss: 4.5841
Training Epoch: 0 [5632/50176]	Loss: 4.5744
Training Epoch: 0 [6144/50176]	Loss: 4.5777
Training Epoch: 0 [6656/50176]	Loss: 4.5958
Training Epoch: 0 [7168/50176]	Loss: 4.5845
Training Epoch: 0 [7680/50176]	Loss: 4.5650
Training Epoch: 0 [8192/50176]	Loss: 4.5877
Training Epoch: 0 [8704/50176]	Loss: 4.5767
Training Epoch: 0 [9216/50176]	Loss: 4.5732
Training Epoch: 0 [9728/50176]	Loss: 4.5577
Training Epoch: 0 [10240/50176]	Loss: 4.5578
Training Epoch: 0 [10752/50176]	Loss: 4.5596
Training Epoch: 0 [11264/50176]	Loss: 4.5569
Training Epoch: 0 [11776/50176]	Loss: 4.5576
Training Epoch: 0 [12288/50176]	Loss: 4.5481
Training Epoch: 0 [12800/50176]	Loss: 4.5499
Training Epoch: 0 [13312/50176]	Loss: 4.5472
Training Epoch: 0 [13824/50176]	Loss: 4.5544
Training Epoch: 0 [14336/50176]	Loss: 4.5594
Training Epoch: 0 [14848/50176]	Loss: 4.5517
Training Epoch: 0 [15360/50176]	Loss: 4.5200
Training Epoch: 0 [15872/50176]	Loss: 4.5244
Training Epoch: 0 [16384/50176]	Loss: 4.5297
Training Epoch: 0 [16896/50176]	Loss: 4.5126
Training Epoch: 0 [17408/50176]	Loss: 4.5593
Training Epoch: 0 [17920/50176]	Loss: 4.5170
Training Epoch: 0 [18432/50176]	Loss: 4.5181
Training Epoch: 0 [18944/50176]	Loss: 4.4956
Training Epoch: 0 [19456/50176]	Loss: 4.4997
Training Epoch: 0 [19968/50176]	Loss: 4.5262
Training Epoch: 0 [20480/50176]	Loss: 4.5170
Training Epoch: 0 [20992/50176]	Loss: 4.5070
Training Epoch: 0 [21504/50176]	Loss: 4.5331
Training Epoch: 0 [22016/50176]	Loss: 4.4946
Training Epoch: 0 [22528/50176]	Loss: 4.5160
Training Epoch: 0 [23040/50176]	Loss: 4.4818
Training Epoch: 0 [23552/50176]	Loss: 4.5019
Training Epoch: 0 [24064/50176]	Loss: 4.5149
Training Epoch: 0 [24576/50176]	Loss: 4.5148
Training Epoch: 0 [25088/50176]	Loss: 4.4920
Training Epoch: 0 [25600/50176]	Loss: 4.4747
Profile done with power limit 150W
epoch 1 train time consumed: 24.36s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0129
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 150000, 'lr': 0.00015890248582070703, 'energy': 145.88568289989595, 'time': 18.62459855799989, 'accuracy': 0.012890625, 'total_cost': 1615406.39454099}
[run job] Launching job with BS 512: and LR: 0.00015890248582070703 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs512+lr0.00016+pl125', 'ZEUS_COST_THRESH': '3064894.9119763076', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.00015890248582070703', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00015890248582070703', '--power_limit', '125']
[run job] cost_ub=3064894.9119763076
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs512+lr0.00016+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.00015890248582070703
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 4.6246
Training Epoch: 0 [1536/50176]	Loss: 4.6326
Training Epoch: 0 [2048/50176]	Loss: 4.6283
Training Epoch: 0 [2560/50176]	Loss: 4.6151
Training Epoch: 0 [3072/50176]	Loss: 4.6038
Training Epoch: 0 [3584/50176]	Loss: 4.5877
Training Epoch: 0 [4096/50176]	Loss: 4.6084
Training Epoch: 0 [4608/50176]	Loss: 4.5917
Training Epoch: 0 [5120/50176]	Loss: 4.5833
Training Epoch: 0 [5632/50176]	Loss: 4.5756
Training Epoch: 0 [6144/50176]	Loss: 4.5774
Training Epoch: 0 [6656/50176]	Loss: 4.5981
Training Epoch: 0 [7168/50176]	Loss: 4.5840
Training Epoch: 0 [7680/50176]	Loss: 4.5655
Training Epoch: 0 [8192/50176]	Loss: 4.5880
Training Epoch: 0 [8704/50176]	Loss: 4.5718
Training Epoch: 0 [9216/50176]	Loss: 4.5718
Training Epoch: 0 [9728/50176]	Loss: 4.5604
Training Epoch: 0 [10240/50176]	Loss: 4.5592
Training Epoch: 0 [10752/50176]	Loss: 4.5582
Training Epoch: 0 [11264/50176]	Loss: 4.5529
Training Epoch: 0 [11776/50176]	Loss: 4.5620
Training Epoch: 0 [12288/50176]	Loss: 4.5476
Training Epoch: 0 [12800/50176]	Loss: 4.5457
Training Epoch: 0 [13312/50176]	Loss: 4.5485
Training Epoch: 0 [13824/50176]	Loss: 4.5547
Training Epoch: 0 [14336/50176]	Loss: 4.5524
Training Epoch: 0 [14848/50176]	Loss: 4.5489
Training Epoch: 0 [15360/50176]	Loss: 4.5236
Training Epoch: 0 [15872/50176]	Loss: 4.5230
Training Epoch: 0 [16384/50176]	Loss: 4.5282
Training Epoch: 0 [16896/50176]	Loss: 4.5144
Training Epoch: 0 [17408/50176]	Loss: 4.5552
Training Epoch: 0 [17920/50176]	Loss: 4.5214
Training Epoch: 0 [18432/50176]	Loss: 4.5187
Training Epoch: 0 [18944/50176]	Loss: 4.4971
Training Epoch: 0 [19456/50176]	Loss: 4.5065
Training Epoch: 0 [19968/50176]	Loss: 4.5243
Training Epoch: 0 [20480/50176]	Loss: 4.5199
Training Epoch: 0 [20992/50176]	Loss: 4.5090
Training Epoch: 0 [21504/50176]	Loss: 4.5275
Training Epoch: 0 [22016/50176]	Loss: 4.4946
Training Epoch: 0 [22528/50176]	Loss: 4.5185
Training Epoch: 0 [23040/50176]	Loss: 4.4852
Training Epoch: 0 [23552/50176]	Loss: 4.5043
Training Epoch: 0 [24064/50176]	Loss: 4.5160
Training Epoch: 0 [24576/50176]	Loss: 4.5116
Training Epoch: 0 [25088/50176]	Loss: 4.4907
Training Epoch: 0 [25600/50176]	Loss: 4.4709
Profile done with power limit 125W
epoch 1 train time consumed: 26.69s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0136
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 125000, 'lr': 0.00015890248582070703, 'energy': 122.39753444756845, 'time': 20.527446312000393, 'accuracy': 0.01357421875, 'total_cost': 1690065.6025737054}
[run job] Launching job with BS 512: and LR: 0.00015890248582070703 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs512+lr0.00016+pl100', 'ZEUS_COST_THRESH': '3064894.9119763076', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.00015890248582070703', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00015890248582070703', '--power_limit', '100']
[run job] cost_ub=3064894.9119763076
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs512+lr0.00016+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.00015890248582070703
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 4.6246
Training Epoch: 0 [1536/50176]	Loss: 4.6326
Training Epoch: 0 [2048/50176]	Loss: 4.6282
Training Epoch: 0 [2560/50176]	Loss: 4.6155
Training Epoch: 0 [3072/50176]	Loss: 4.6032
Training Epoch: 0 [3584/50176]	Loss: 4.5883
Training Epoch: 0 [4096/50176]	Loss: 4.6094
Training Epoch: 0 [4608/50176]	Loss: 4.5942
Training Epoch: 0 [5120/50176]	Loss: 4.5842
Training Epoch: 0 [5632/50176]	Loss: 4.5764
Training Epoch: 0 [6144/50176]	Loss: 4.5774
Training Epoch: 0 [6656/50176]	Loss: 4.5972
Training Epoch: 0 [7168/50176]	Loss: 4.5879
Training Epoch: 0 [7680/50176]	Loss: 4.5677
Training Epoch: 0 [8192/50176]	Loss: 4.5879
Training Epoch: 0 [8704/50176]	Loss: 4.5782
Training Epoch: 0 [9216/50176]	Loss: 4.5718
Training Epoch: 0 [9728/50176]	Loss: 4.5550
Training Epoch: 0 [10240/50176]	Loss: 4.5589
Training Epoch: 0 [10752/50176]	Loss: 4.5600
Training Epoch: 0 [11264/50176]	Loss: 4.5562
Training Epoch: 0 [11776/50176]	Loss: 4.5596
Training Epoch: 0 [12288/50176]	Loss: 4.5452
Training Epoch: 0 [12800/50176]	Loss: 4.5472
Training Epoch: 0 [13312/50176]	Loss: 4.5537
Training Epoch: 0 [13824/50176]	Loss: 4.5548
Training Epoch: 0 [14336/50176]	Loss: 4.5495
Training Epoch: 0 [14848/50176]	Loss: 4.5499
Training Epoch: 0 [15360/50176]	Loss: 4.5221
Training Epoch: 0 [15872/50176]	Loss: 4.5276
Training Epoch: 0 [16384/50176]	Loss: 4.5268
Training Epoch: 0 [16896/50176]	Loss: 4.5115
Training Epoch: 0 [17408/50176]	Loss: 4.5571
Training Epoch: 0 [17920/50176]	Loss: 4.5205
Training Epoch: 0 [18432/50176]	Loss: 4.5300
Training Epoch: 0 [18944/50176]	Loss: 4.4922
Training Epoch: 0 [19456/50176]	Loss: 4.5046
Training Epoch: 0 [19968/50176]	Loss: 4.5225
Training Epoch: 0 [20480/50176]	Loss: 4.5141
Training Epoch: 0 [20992/50176]	Loss: 4.5079
Training Epoch: 0 [21504/50176]	Loss: 4.5368
Training Epoch: 0 [22016/50176]	Loss: 4.4938
Training Epoch: 0 [22528/50176]	Loss: 4.5204
Training Epoch: 0 [23040/50176]	Loss: 4.4851
Training Epoch: 0 [23552/50176]	Loss: 4.5043
Training Epoch: 0 [24064/50176]	Loss: 4.5153
Training Epoch: 0 [24576/50176]	Loss: 4.5113
Training Epoch: 0 [25088/50176]	Loss: 4.4948
Training Epoch: 0 [25600/50176]	Loss: 4.4823
Profile done with power limit 100W
epoch 1 train time consumed: 54.18s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0141
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 100000, 'lr': 0.00015890248582070703, 'energy': 96.62853553482012, 'time': 42.20682161100012, 'accuracy': 0.0140625, 'total_cost': 3351500.645508578}

[Power Profiler] with batch size 1024 and learning rate 3.1622776601683795e-05
[run job] Launching job with BS 1024: and LR: 3.1622776601683795e-05 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs1024+lr0.00003+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '1024', 'ZEUS_LEARNING_RATE': '3.1622776601683795e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '1024', '--epochs', '100', '--seed', '1', '--learning_rate', '3.1622776601683795e-05', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs1024+lr0.00003+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 3.1622776601683795e-05
batch size arg: 1024
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (49) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 47 iterations (9 for warmup + 38 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [1024/50176]	Loss: 4.6235
Training Epoch: 0 [2048/50176]	Loss: 4.6433
Training Epoch: 0 [3072/50176]	Loss: 4.6222
Training Epoch: 0 [4096/50176]	Loss: 4.6211
Training Epoch: 0 [5120/50176]	Loss: 4.6288
Training Epoch: 0 [6144/50176]	Loss: 4.6220
Training Epoch: 0 [7168/50176]	Loss: 4.6363
Training Epoch: 0 [8192/50176]	Loss: 4.6336
Training Epoch: 0 [9216/50176]	Loss: 4.6217
Training Epoch: 0 [10240/50176]	Loss: 4.6179
Training Epoch: 0 [11264/50176]	Loss: 4.6098
Training Epoch: 0 [12288/50176]	Loss: 4.6157
Training Epoch: 0 [13312/50176]	Loss: 4.6094
Training Epoch: 0 [14336/50176]	Loss: 4.6181
Training Epoch: 0 [15360/50176]	Loss: 4.6082
Training Epoch: 0 [16384/50176]	Loss: 4.6092
Training Epoch: 0 [17408/50176]	Loss: 4.6071
Training Epoch: 0 [18432/50176]	Loss: 4.6028
Training Epoch: 0 [19456/50176]	Loss: 4.5999
Training Epoch: 0 [20480/50176]	Loss: 4.6000
Training Epoch: 0 [21504/50176]	Loss: 4.6034
Training Epoch: 0 [22528/50176]	Loss: 4.6090
Training Epoch: 0 [23552/50176]	Loss: 4.5974
Training Epoch: 0 [24576/50176]	Loss: 4.6084
Training Epoch: 0 [25600/50176]	Loss: 4.5961
Training Epoch: 0 [26624/50176]	Loss: 4.6023
Training Epoch: 0 [27648/50176]	Loss: 4.5988
Training Epoch: 0 [28672/50176]	Loss: 4.6014
Training Epoch: 0 [29696/50176]	Loss: 4.5906
Training Epoch: 0 [30720/50176]	Loss: 4.5974
Training Epoch: 0 [31744/50176]	Loss: 4.5845
Training Epoch: 0 [32768/50176]	Loss: 4.5988
Training Epoch: 0 [33792/50176]	Loss: 4.5937
Training Epoch: 0 [34816/50176]	Loss: 4.5932
Training Epoch: 0 [35840/50176]	Loss: 4.5875
Training Epoch: 0 [36864/50176]	Loss: 4.5746
Training Epoch: 0 [37888/50176]	Loss: 4.5845
Training Epoch: 0 [38912/50176]	Loss: 4.5716
Training Epoch: 0 [39936/50176]	Loss: 4.5857
Training Epoch: 0 [40960/50176]	Loss: 4.5746
Training Epoch: 0 [41984/50176]	Loss: 4.5792
Training Epoch: 0 [43008/50176]	Loss: 4.5837
Training Epoch: 0 [44032/50176]	Loss: 4.5679
Training Epoch: 0 [45056/50176]	Loss: 4.5704
Training Epoch: 0 [46080/50176]	Loss: 4.5707
Training Epoch: 0 [47104/50176]	Loss: 4.5690
Training Epoch: 0 [48128/50176]	Loss: 4.5747
Profile done with power limit 175W
epoch 1 train time consumed: 44.16s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0111
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 1024, 'pl': 175000, 'lr': 3.1622776601683795e-05, 'energy': 148.56158686138454, 'time': 34.41040736800005, 'accuracy': 0.0111328125, 'total_cost': 6833105.177281486}
[run job] Launching job with BS 1024: and LR: 3.1622776601683795e-05 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs1024+lr0.00003+pl150', 'ZEUS_COST_THRESH': '13666210.354562972', 'ZEUS_BATCH_SIZE': '1024', 'ZEUS_LEARNING_RATE': '3.1622776601683795e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '1024', '--epochs', '100', '--seed', '1', '--learning_rate', '3.1622776601683795e-05', '--power_limit', '150']
[run job] cost_ub=13666210.354562972
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs1024+lr0.00003+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 3.1622776601683795e-05
batch size arg: 1024
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (49) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 47 iterations (9 for warmup + 38 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [1024/50176]	Loss: 4.6235
Training Epoch: 0 [2048/50176]	Loss: 4.6433
Training Epoch: 0 [3072/50176]	Loss: 4.6222
Training Epoch: 0 [4096/50176]	Loss: 4.6211
Training Epoch: 0 [5120/50176]	Loss: 4.6288
Training Epoch: 0 [6144/50176]	Loss: 4.6219
Training Epoch: 0 [7168/50176]	Loss: 4.6363
Training Epoch: 0 [8192/50176]	Loss: 4.6336
Training Epoch: 0 [9216/50176]	Loss: 4.6217
Training Epoch: 0 [10240/50176]	Loss: 4.6178
Training Epoch: 0 [11264/50176]	Loss: 4.6096
Training Epoch: 0 [12288/50176]	Loss: 4.6156
Training Epoch: 0 [13312/50176]	Loss: 4.6092
Training Epoch: 0 [14336/50176]	Loss: 4.6181
Training Epoch: 0 [15360/50176]	Loss: 4.6082
Training Epoch: 0 [16384/50176]	Loss: 4.6094
Training Epoch: 0 [17408/50176]	Loss: 4.6071
Training Epoch: 0 [18432/50176]	Loss: 4.6029
Training Epoch: 0 [19456/50176]	Loss: 4.5997
Training Epoch: 0 [20480/50176]	Loss: 4.6001
Training Epoch: 0 [21504/50176]	Loss: 4.6034
Training Epoch: 0 [22528/50176]	Loss: 4.6091
Training Epoch: 0 [23552/50176]	Loss: 4.5972
Training Epoch: 0 [24576/50176]	Loss: 4.6086
Training Epoch: 0 [25600/50176]	Loss: 4.5962
Training Epoch: 0 [26624/50176]	Loss: 4.6026
Training Epoch: 0 [27648/50176]	Loss: 4.5991
Training Epoch: 0 [28672/50176]	Loss: 4.6020
Training Epoch: 0 [29696/50176]	Loss: 4.5906
Training Epoch: 0 [30720/50176]	Loss: 4.5979
Training Epoch: 0 [31744/50176]	Loss: 4.5839
Training Epoch: 0 [32768/50176]	Loss: 4.5987
Training Epoch: 0 [33792/50176]	Loss: 4.5928
Training Epoch: 0 [34816/50176]	Loss: 4.5938
Training Epoch: 0 [35840/50176]	Loss: 4.5867
Training Epoch: 0 [36864/50176]	Loss: 4.5744
Training Epoch: 0 [37888/50176]	Loss: 4.5843
Training Epoch: 0 [38912/50176]	Loss: 4.5714
Training Epoch: 0 [39936/50176]	Loss: 4.5854
Training Epoch: 0 [40960/50176]	Loss: 4.5742
Training Epoch: 0 [41984/50176]	Loss: 4.5784
Training Epoch: 0 [43008/50176]	Loss: 4.5835
Training Epoch: 0 [44032/50176]	Loss: 4.5681
Training Epoch: 0 [45056/50176]	Loss: 4.5706
Training Epoch: 0 [46080/50176]	Loss: 4.5719
Training Epoch: 0 [47104/50176]	Loss: 4.5693
Training Epoch: 0 [48128/50176]	Loss: 4.5743
Profile done with power limit 150W
epoch 1 train time consumed: 44.78s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0109
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 1024, 'pl': 150000, 'lr': 3.1622776601683795e-05, 'energy': 145.80795637644056, 'time': 34.82652115499968, 'accuracy': 0.0109375, 'total_cost': 7039031.44416046}
[run job] Launching job with BS 1024: and LR: 3.1622776601683795e-05 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs1024+lr0.00003+pl125', 'ZEUS_COST_THRESH': '13666210.354562972', 'ZEUS_BATCH_SIZE': '1024', 'ZEUS_LEARNING_RATE': '3.1622776601683795e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '1024', '--epochs', '100', '--seed', '1', '--learning_rate', '3.1622776601683795e-05', '--power_limit', '125']
[run job] cost_ub=13666210.354562972
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs1024+lr0.00003+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 3.1622776601683795e-05
batch size arg: 1024
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (49) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 47 iterations (9 for warmup + 38 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [1024/50176]	Loss: 4.6235
Training Epoch: 0 [2048/50176]	Loss: 4.6433
Training Epoch: 0 [3072/50176]	Loss: 4.6222
Training Epoch: 0 [4096/50176]	Loss: 4.6211
Training Epoch: 0 [5120/50176]	Loss: 4.6288
Training Epoch: 0 [6144/50176]	Loss: 4.6219
Training Epoch: 0 [7168/50176]	Loss: 4.6363
Training Epoch: 0 [8192/50176]	Loss: 4.6335
Training Epoch: 0 [9216/50176]	Loss: 4.6217
Training Epoch: 0 [10240/50176]	Loss: 4.6178
Training Epoch: 0 [11264/50176]	Loss: 4.6096
Training Epoch: 0 [12288/50176]	Loss: 4.6158
Training Epoch: 0 [13312/50176]	Loss: 4.6095
Training Epoch: 0 [14336/50176]	Loss: 4.6181
Training Epoch: 0 [15360/50176]	Loss: 4.6081
Training Epoch: 0 [16384/50176]	Loss: 4.6092
Training Epoch: 0 [17408/50176]	Loss: 4.6073
Training Epoch: 0 [18432/50176]	Loss: 4.6030
Training Epoch: 0 [19456/50176]	Loss: 4.5997
Training Epoch: 0 [20480/50176]	Loss: 4.5996
Training Epoch: 0 [21504/50176]	Loss: 4.6033
Training Epoch: 0 [22528/50176]	Loss: 4.6090
Training Epoch: 0 [23552/50176]	Loss: 4.5970
Training Epoch: 0 [24576/50176]	Loss: 4.6080
Training Epoch: 0 [25600/50176]	Loss: 4.5963
Training Epoch: 0 [26624/50176]	Loss: 4.6021
Training Epoch: 0 [27648/50176]	Loss: 4.5988
Training Epoch: 0 [28672/50176]	Loss: 4.6015
Training Epoch: 0 [29696/50176]	Loss: 4.5909
Training Epoch: 0 [30720/50176]	Loss: 4.5988
Training Epoch: 0 [31744/50176]	Loss: 4.5850
Training Epoch: 0 [32768/50176]	Loss: 4.5987
Training Epoch: 0 [33792/50176]	Loss: 4.5931
Training Epoch: 0 [34816/50176]	Loss: 4.5933
Training Epoch: 0 [35840/50176]	Loss: 4.5880
Training Epoch: 0 [36864/50176]	Loss: 4.5746
Training Epoch: 0 [37888/50176]	Loss: 4.5835
Training Epoch: 0 [38912/50176]	Loss: 4.5722
Training Epoch: 0 [39936/50176]	Loss: 4.5858
Training Epoch: 0 [40960/50176]	Loss: 4.5738
Training Epoch: 0 [41984/50176]	Loss: 4.5787
Training Epoch: 0 [43008/50176]	Loss: 4.5831
Training Epoch: 0 [44032/50176]	Loss: 4.5675
Training Epoch: 0 [45056/50176]	Loss: 4.5699
Training Epoch: 0 [46080/50176]	Loss: 4.5716
Training Epoch: 0 [47104/50176]	Loss: 4.5683
Training Epoch: 0 [48128/50176]	Loss: 4.5751
Profile done with power limit 125W
epoch 1 train time consumed: 48.98s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0112
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 1024, 'pl': 125000, 'lr': 3.1622776601683795e-05, 'energy': 122.16435839679788, 'time': 38.26629502899959, 'accuracy': 0.01123046875, 'total_cost': 7530879.961925026}
[run job] Launching job with BS 1024: and LR: 3.1622776601683795e-05 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs1024+lr0.00003+pl100', 'ZEUS_COST_THRESH': '13666210.354562972', 'ZEUS_BATCH_SIZE': '1024', 'ZEUS_LEARNING_RATE': '3.1622776601683795e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '1024', '--epochs', '100', '--seed', '1', '--learning_rate', '3.1622776601683795e-05', '--power_limit', '100']
[run job] cost_ub=13666210.354562972
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs1024+lr0.00003+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 3.1622776601683795e-05
batch size arg: 1024
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (49) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 47 iterations (9 for warmup + 38 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [1024/50176]	Loss: 4.6235
Training Epoch: 0 [2048/50176]	Loss: 4.6433
Training Epoch: 0 [3072/50176]	Loss: 4.6222
Training Epoch: 0 [4096/50176]	Loss: 4.6211
Training Epoch: 0 [5120/50176]	Loss: 4.6287
Training Epoch: 0 [6144/50176]	Loss: 4.6219
Training Epoch: 0 [7168/50176]	Loss: 4.6363
Training Epoch: 0 [8192/50176]	Loss: 4.6336
Training Epoch: 0 [9216/50176]	Loss: 4.6216
Training Epoch: 0 [10240/50176]	Loss: 4.6179
Training Epoch: 0 [11264/50176]	Loss: 4.6095
Training Epoch: 0 [12288/50176]	Loss: 4.6158
Training Epoch: 0 [13312/50176]	Loss: 4.6092
Training Epoch: 0 [14336/50176]	Loss: 4.6181
Training Epoch: 0 [15360/50176]	Loss: 4.6083
Training Epoch: 0 [16384/50176]	Loss: 4.6092
Training Epoch: 0 [17408/50176]	Loss: 4.6070
Training Epoch: 0 [18432/50176]	Loss: 4.6029
Training Epoch: 0 [19456/50176]	Loss: 4.5995
Training Epoch: 0 [20480/50176]	Loss: 4.6003
Training Epoch: 0 [21504/50176]	Loss: 4.6032
Training Epoch: 0 [22528/50176]	Loss: 4.6091
Training Epoch: 0 [23552/50176]	Loss: 4.5972
Training Epoch: 0 [24576/50176]	Loss: 4.6081
Training Epoch: 0 [25600/50176]	Loss: 4.5963
Training Epoch: 0 [26624/50176]	Loss: 4.6024
Training Epoch: 0 [27648/50176]	Loss: 4.5991
Training Epoch: 0 [28672/50176]	Loss: 4.6018
Training Epoch: 0 [29696/50176]	Loss: 4.5902
Training Epoch: 0 [30720/50176]	Loss: 4.5983
Training Epoch: 0 [31744/50176]	Loss: 4.5843
Training Epoch: 0 [32768/50176]	Loss: 4.5990
Training Epoch: 0 [33792/50176]	Loss: 4.5932
Training Epoch: 0 [34816/50176]	Loss: 4.5937
Training Epoch: 0 [35840/50176]	Loss: 4.5878
Training Epoch: 0 [36864/50176]	Loss: 4.5741
Training Epoch: 0 [37888/50176]	Loss: 4.5838
Training Epoch: 0 [38912/50176]	Loss: 4.5729
Training Epoch: 0 [39936/50176]	Loss: 4.5857
Training Epoch: 0 [40960/50176]	Loss: 4.5740
Training Epoch: 0 [41984/50176]	Loss: 4.5789
Training Epoch: 0 [43008/50176]	Loss: 4.5843
Training Epoch: 0 [44032/50176]	Loss: 4.5687
Training Epoch: 0 [45056/50176]	Loss: 4.5707
Training Epoch: 0 [46080/50176]	Loss: 4.5711
Training Epoch: 0 [47104/50176]	Loss: 4.5683
Training Epoch: 0 [48128/50176]	Loss: 4.5748
Profile done with power limit 100W
epoch 1 train time consumed: 89.84s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0106
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 1024, 'pl': 100000, 'lr': 3.1622776601683795e-05, 'energy': 96.73357476804128, 'time': 71.39534332800031, 'accuracy': 0.01064453125, 'total_cost': 14818264.17776669}

[Power Profiler] with batch size 1024 and learning rate 0.00022472205054244234
[run job] Launching job with BS 1024: and LR: 0.00022472205054244234 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs1024+lr0.00022+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '1024', 'ZEUS_LEARNING_RATE': '0.00022472205054244234', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '1024', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00022472205054244234', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs1024+lr0.00022+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.00022472205054244234
batch size arg: 1024
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (49) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 47 iterations (9 for warmup + 38 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [1024/50176]	Loss: 4.6235
Training Epoch: 0 [2048/50176]	Loss: 4.6372
Training Epoch: 0 [3072/50176]	Loss: 4.6091
Training Epoch: 0 [4096/50176]	Loss: 4.6039
Training Epoch: 0 [5120/50176]	Loss: 4.5933
Training Epoch: 0 [6144/50176]	Loss: 4.5895
Training Epoch: 0 [7168/50176]	Loss: 4.5921
Training Epoch: 0 [8192/50176]	Loss: 4.5891
Training Epoch: 0 [9216/50176]	Loss: 4.5822
Training Epoch: 0 [10240/50176]	Loss: 4.5705
Training Epoch: 0 [11264/50176]	Loss: 4.5560
Training Epoch: 0 [12288/50176]	Loss: 4.5544
Training Epoch: 0 [13312/50176]	Loss: 4.5562
Training Epoch: 0 [14336/50176]	Loss: 4.5687
Training Epoch: 0 [15360/50176]	Loss: 4.5505
Training Epoch: 0 [16384/50176]	Loss: 4.5374
Training Epoch: 0 [17408/50176]	Loss: 4.5398
Training Epoch: 0 [18432/50176]	Loss: 4.5273
Training Epoch: 0 [19456/50176]	Loss: 4.5052
Training Epoch: 0 [20480/50176]	Loss: 4.5269
Training Epoch: 0 [21504/50176]	Loss: 4.5276
Training Epoch: 0 [22528/50176]	Loss: 4.5128
Training Epoch: 0 [23552/50176]	Loss: 4.4973
Training Epoch: 0 [24576/50176]	Loss: 4.5150
Training Epoch: 0 [25600/50176]	Loss: 4.4904
Training Epoch: 0 [26624/50176]	Loss: 4.4799
Training Epoch: 0 [27648/50176]	Loss: 4.5032
Training Epoch: 0 [28672/50176]	Loss: 4.4741
Training Epoch: 0 [29696/50176]	Loss: 4.5004
Training Epoch: 0 [30720/50176]	Loss: 4.5016
Training Epoch: 0 [31744/50176]	Loss: 4.5030
Training Epoch: 0 [32768/50176]	Loss: 4.4863
Training Epoch: 0 [33792/50176]	Loss: 4.4968
Training Epoch: 0 [34816/50176]	Loss: 4.4587
Training Epoch: 0 [35840/50176]	Loss: 4.4814
Training Epoch: 0 [36864/50176]	Loss: 4.4601
Training Epoch: 0 [37888/50176]	Loss: 4.4465
Training Epoch: 0 [38912/50176]	Loss: 4.4422
Training Epoch: 0 [39936/50176]	Loss: 4.4584
Training Epoch: 0 [40960/50176]	Loss: 4.4264
Training Epoch: 0 [41984/50176]	Loss: 4.4537
Training Epoch: 0 [43008/50176]	Loss: 4.4668
Training Epoch: 0 [44032/50176]	Loss: 4.4019
Training Epoch: 0 [45056/50176]	Loss: 4.4453
Training Epoch: 0 [46080/50176]	Loss: 4.4055
Training Epoch: 0 [47104/50176]	Loss: 4.4141
Training Epoch: 0 [48128/50176]	Loss: 4.4240
Profile done with power limit 175W
epoch 1 train time consumed: 44.15s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0201
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 1024, 'pl': 175000, 'lr': 0.00022472205054244234, 'energy': 148.81729130372582, 'time': 34.355349280000155, 'accuracy': 0.0201171875, 'total_cost': 3775388.458522991}
[run job] Launching job with BS 1024: and LR: 0.00022472205054244234 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs1024+lr0.00022+pl150', 'ZEUS_COST_THRESH': '7550776.917045982', 'ZEUS_BATCH_SIZE': '1024', 'ZEUS_LEARNING_RATE': '0.00022472205054244234', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '1024', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00022472205054244234', '--power_limit', '150']
[run job] cost_ub=7550776.917045982
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs1024+lr0.00022+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.00022472205054244234
batch size arg: 1024
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (49) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 47 iterations (9 for warmup + 38 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [1024/50176]	Loss: 4.6235
Training Epoch: 0 [2048/50176]	Loss: 4.6372
Training Epoch: 0 [3072/50176]	Loss: 4.6091
Training Epoch: 0 [4096/50176]	Loss: 4.6038
Training Epoch: 0 [5120/50176]	Loss: 4.5936
Training Epoch: 0 [6144/50176]	Loss: 4.5899
Training Epoch: 0 [7168/50176]	Loss: 4.5913
Training Epoch: 0 [8192/50176]	Loss: 4.5889
Training Epoch: 0 [9216/50176]	Loss: 4.5812
Training Epoch: 0 [10240/50176]	Loss: 4.5700
Training Epoch: 0 [11264/50176]	Loss: 4.5561
Training Epoch: 0 [12288/50176]	Loss: 4.5559
Training Epoch: 0 [13312/50176]	Loss: 4.5546
Training Epoch: 0 [14336/50176]	Loss: 4.5700
Training Epoch: 0 [15360/50176]	Loss: 4.5507
Training Epoch: 0 [16384/50176]	Loss: 4.5324
Training Epoch: 0 [17408/50176]	Loss: 4.5375
Training Epoch: 0 [18432/50176]	Loss: 4.5322
Training Epoch: 0 [19456/50176]	Loss: 4.5048
Training Epoch: 0 [20480/50176]	Loss: 4.5257
Training Epoch: 0 [21504/50176]	Loss: 4.5257
Training Epoch: 0 [22528/50176]	Loss: 4.5110
Training Epoch: 0 [23552/50176]	Loss: 4.4997
Training Epoch: 0 [24576/50176]	Loss: 4.5114
Training Epoch: 0 [25600/50176]	Loss: 4.4864
Training Epoch: 0 [26624/50176]	Loss: 4.4821
Training Epoch: 0 [27648/50176]	Loss: 4.5008
Training Epoch: 0 [28672/50176]	Loss: 4.4726
Training Epoch: 0 [29696/50176]	Loss: 4.4990
Training Epoch: 0 [30720/50176]	Loss: 4.5021
Training Epoch: 0 [31744/50176]	Loss: 4.5051
Training Epoch: 0 [32768/50176]	Loss: 4.4901
Training Epoch: 0 [33792/50176]	Loss: 4.4952
Training Epoch: 0 [34816/50176]	Loss: 4.4596
Training Epoch: 0 [35840/50176]	Loss: 4.4779
Training Epoch: 0 [36864/50176]	Loss: 4.4598
Training Epoch: 0 [37888/50176]	Loss: 4.4446
Training Epoch: 0 [38912/50176]	Loss: 4.4420
Training Epoch: 0 [39936/50176]	Loss: 4.4577
Training Epoch: 0 [40960/50176]	Loss: 4.4227
Training Epoch: 0 [41984/50176]	Loss: 4.4539
Training Epoch: 0 [43008/50176]	Loss: 4.4636
Training Epoch: 0 [44032/50176]	Loss: 4.3989
Training Epoch: 0 [45056/50176]	Loss: 4.4460
Training Epoch: 0 [46080/50176]	Loss: 4.4012
Training Epoch: 0 [47104/50176]	Loss: 4.4126
Training Epoch: 0 [48128/50176]	Loss: 4.4221
Profile done with power limit 150W
epoch 1 train time consumed: 44.86s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0164
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 1024, 'pl': 150000, 'lr': 0.00022472205054244234, 'energy': 145.6980417935453, 'time': 34.920249884999976, 'accuracy': 0.01640625, 'total_cost': 4705302.39078133}
[run job] Launching job with BS 1024: and LR: 0.00022472205054244234 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs1024+lr0.00022+pl125', 'ZEUS_COST_THRESH': '7550776.917045982', 'ZEUS_BATCH_SIZE': '1024', 'ZEUS_LEARNING_RATE': '0.00022472205054244234', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '1024', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00022472205054244234', '--power_limit', '125']
[run job] cost_ub=7550776.917045982
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs1024+lr0.00022+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.00022472205054244234
batch size arg: 1024
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (49) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 47 iterations (9 for warmup + 38 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [1024/50176]	Loss: 4.6235
Training Epoch: 0 [2048/50176]	Loss: 4.6372
Training Epoch: 0 [3072/50176]	Loss: 4.6091
Training Epoch: 0 [4096/50176]	Loss: 4.6040
Training Epoch: 0 [5120/50176]	Loss: 4.5934
Training Epoch: 0 [6144/50176]	Loss: 4.5896
Training Epoch: 0 [7168/50176]	Loss: 4.5923
Training Epoch: 0 [8192/50176]	Loss: 4.5889
Training Epoch: 0 [9216/50176]	Loss: 4.5798
Training Epoch: 0 [10240/50176]	Loss: 4.5713
Training Epoch: 0 [11264/50176]	Loss: 4.5546
Training Epoch: 0 [12288/50176]	Loss: 4.5571
Training Epoch: 0 [13312/50176]	Loss: 4.5536
Training Epoch: 0 [14336/50176]	Loss: 4.5682
Training Epoch: 0 [15360/50176]	Loss: 4.5466
Training Epoch: 0 [16384/50176]	Loss: 4.5359
Training Epoch: 0 [17408/50176]	Loss: 4.5396
Training Epoch: 0 [18432/50176]	Loss: 4.5296
Training Epoch: 0 [19456/50176]	Loss: 4.5063
Training Epoch: 0 [20480/50176]	Loss: 4.5272
Training Epoch: 0 [21504/50176]	Loss: 4.5297
Training Epoch: 0 [22528/50176]	Loss: 4.5113
Training Epoch: 0 [23552/50176]	Loss: 4.4962
Training Epoch: 0 [24576/50176]	Loss: 4.5117
Training Epoch: 0 [25600/50176]	Loss: 4.4878
Training Epoch: 0 [26624/50176]	Loss: 4.4797
Training Epoch: 0 [27648/50176]	Loss: 4.5024
Training Epoch: 0 [28672/50176]	Loss: 4.4721
Training Epoch: 0 [29696/50176]	Loss: 4.5027
Training Epoch: 0 [30720/50176]	Loss: 4.5033
Training Epoch: 0 [31744/50176]	Loss: 4.5053
Training Epoch: 0 [32768/50176]	Loss: 4.4910
Training Epoch: 0 [33792/50176]	Loss: 4.4931
Training Epoch: 0 [34816/50176]	Loss: 4.4569
Training Epoch: 0 [35840/50176]	Loss: 4.4785
Training Epoch: 0 [36864/50176]	Loss: 4.4594
Training Epoch: 0 [37888/50176]	Loss: 4.4443
Training Epoch: 0 [38912/50176]	Loss: 4.4384
Training Epoch: 0 [39936/50176]	Loss: 4.4535
Training Epoch: 0 [40960/50176]	Loss: 4.4280
Training Epoch: 0 [41984/50176]	Loss: 4.4489
Training Epoch: 0 [43008/50176]	Loss: 4.4641
Training Epoch: 0 [44032/50176]	Loss: 4.4052
Training Epoch: 0 [45056/50176]	Loss: 4.4423
Training Epoch: 0 [46080/50176]	Loss: 4.4017
Training Epoch: 0 [47104/50176]	Loss: 4.4141
Training Epoch: 0 [48128/50176]	Loss: 4.4238
Profile done with power limit 125W
epoch 1 train time consumed: 48.97s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0187
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 1024, 'pl': 125000, 'lr': 0.00022472205054244234, 'energy': 122.58382874490184, 'time': 38.39685815099983, 'accuracy': 0.01875, 'total_cost': 4526073.65571231}
[run job] Launching job with BS 1024: and LR: 0.00022472205054244234 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs1024+lr0.00022+pl100', 'ZEUS_COST_THRESH': '7550776.917045982', 'ZEUS_BATCH_SIZE': '1024', 'ZEUS_LEARNING_RATE': '0.00022472205054244234', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '1024', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00022472205054244234', '--power_limit', '100']
[run job] cost_ub=7550776.917045982
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs1024+lr0.00022+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.00022472205054244234
batch size arg: 1024
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (49) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 47 iterations (9 for warmup + 38 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [1024/50176]	Loss: 4.6235
Training Epoch: 0 [2048/50176]	Loss: 4.6372
Training Epoch: 0 [3072/50176]	Loss: 4.6091
Training Epoch: 0 [4096/50176]	Loss: 4.6039
Training Epoch: 0 [5120/50176]	Loss: 4.5935
Training Epoch: 0 [6144/50176]	Loss: 4.5888
Training Epoch: 0 [7168/50176]	Loss: 4.5927
Training Epoch: 0 [8192/50176]	Loss: 4.5879
Training Epoch: 0 [9216/50176]	Loss: 4.5808
Training Epoch: 0 [10240/50176]	Loss: 4.5709
Training Epoch: 0 [11264/50176]	Loss: 4.5537
Training Epoch: 0 [12288/50176]	Loss: 4.5564
Training Epoch: 0 [13312/50176]	Loss: 4.5558
Training Epoch: 0 [14336/50176]	Loss: 4.5690
Training Epoch: 0 [15360/50176]	Loss: 4.5482
Training Epoch: 0 [16384/50176]	Loss: 4.5361
Training Epoch: 0 [17408/50176]	Loss: 4.5384
Training Epoch: 0 [18432/50176]	Loss: 4.5323
Training Epoch: 0 [19456/50176]	Loss: 4.5064
Training Epoch: 0 [20480/50176]	Loss: 4.5277
Training Epoch: 0 [21504/50176]	Loss: 4.5270
Training Epoch: 0 [22528/50176]	Loss: 4.5141
Training Epoch: 0 [23552/50176]	Loss: 4.4958
Training Epoch: 0 [24576/50176]	Loss: 4.5151
Training Epoch: 0 [25600/50176]	Loss: 4.4943
Training Epoch: 0 [26624/50176]	Loss: 4.4852
Training Epoch: 0 [27648/50176]	Loss: 4.5023
Training Epoch: 0 [28672/50176]	Loss: 4.4732
Training Epoch: 0 [29696/50176]	Loss: 4.5031
Training Epoch: 0 [30720/50176]	Loss: 4.5037
Training Epoch: 0 [31744/50176]	Loss: 4.5094
Training Epoch: 0 [32768/50176]	Loss: 4.4927
Training Epoch: 0 [33792/50176]	Loss: 4.4941
Training Epoch: 0 [34816/50176]	Loss: 4.4634
Training Epoch: 0 [35840/50176]	Loss: 4.4802
Training Epoch: 0 [36864/50176]	Loss: 4.4575
Training Epoch: 0 [37888/50176]	Loss: 4.4455
Training Epoch: 0 [38912/50176]	Loss: 4.4400
Training Epoch: 0 [39936/50176]	Loss: 4.4588
Training Epoch: 0 [40960/50176]	Loss: 4.4245
Training Epoch: 0 [41984/50176]	Loss: 4.4569
Training Epoch: 0 [43008/50176]	Loss: 4.4644
Training Epoch: 0 [44032/50176]	Loss: 4.4029
Training Epoch: 0 [45056/50176]	Loss: 4.4448
Training Epoch: 0 [46080/50176]	Loss: 4.4070
Training Epoch: 0 [47104/50176]	Loss: 4.4187
Training Epoch: 0 [48128/50176]	Loss: 4.4297
Profile done with power limit 100W
epoch 1 train time consumed: 92.14s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0197
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 1024, 'pl': 100000, 'lr': 0.00022472205054244234, 'energy': 96.89942441105258, 'time': 72.16457277500012, 'accuracy': 0.0197265625, 'total_cost': 8082123.363692388}

[Power Profiler] with batch size 2048 and learning rate 4.4721359549995795e-05
[run job] Launching job with BS 2048: and LR: 4.4721359549995795e-05 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs2048+lr0.00004+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '2048', 'ZEUS_LEARNING_RATE': '4.4721359549995795e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '2048', '--epochs', '100', '--seed', '1', '--learning_rate', '4.4721359549995795e-05', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs2048+lr0.00004+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 4.4721359549995795e-05
batch size arg: 2048
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (25) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 23 iterations (4 for warmup + 19 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [2048/51200]	Loss: 4.6363
Training Epoch: 0 [4096/51200]	Loss: 4.6239
Training Epoch: 0 [6144/51200]	Loss: 4.6273
Training Epoch: 0 [8192/51200]	Loss: 4.6353
Training Epoch: 0 [10240/51200]	Loss: 4.6235
Training Epoch: 0 [12288/51200]	Loss: 4.6174
Training Epoch: 0 [14336/51200]	Loss: 4.6155
Training Epoch: 0 [16384/51200]	Loss: 4.6118
Training Epoch: 0 [18432/51200]	Loss: 4.6086
Training Epoch: 0 [20480/51200]	Loss: 4.6027
Training Epoch: 0 [22528/51200]	Loss: 4.6090
Training Epoch: 0 [24576/51200]	Loss: 4.6079
Training Epoch: 0 [26624/51200]	Loss: 4.6054
Training Epoch: 0 [28672/51200]	Loss: 4.6058
Training Epoch: 0 [30720/51200]	Loss: 4.5958
Training Epoch: 0 [32768/51200]	Loss: 4.5991
Training Epoch: 0 [34816/51200]	Loss: 4.5949
Training Epoch: 0 [36864/51200]	Loss: 4.5893
Training Epoch: 0 [38912/51200]	Loss: 4.5820
Training Epoch: 0 [40960/51200]	Loss: 4.5846
Training Epoch: 0 [43008/51200]	Loss: 4.5849
Training Epoch: 0 [45056/51200]	Loss: 4.5737
Training Epoch: 0 [47104/51200]	Loss: 4.5784
Profile done with power limit 175W
epoch 1 train time consumed: 43.62s
Validation Epoch: 0, Average loss: 0.0023, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 2048, 'pl': 175000, 'lr': 4.4721359549995795e-05, 'energy': 148.88460431793334, 'time': 34.143182105999585, 'accuracy': 0.009765625, 'total_cost': 14820122.467568941}
[run job] Launching job with BS 2048: and LR: 4.4721359549995795e-05 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs2048+lr0.00004+pl150', 'ZEUS_COST_THRESH': '29640244.935137883', 'ZEUS_BATCH_SIZE': '2048', 'ZEUS_LEARNING_RATE': '4.4721359549995795e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '2048', '--epochs', '100', '--seed', '1', '--learning_rate', '4.4721359549995795e-05', '--power_limit', '150']
[run job] cost_ub=29640244.935137883
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs2048+lr0.00004+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 4.4721359549995795e-05
batch size arg: 2048
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (25) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 23 iterations (4 for warmup + 19 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [2048/51200]	Loss: 4.6363
Training Epoch: 0 [4096/51200]	Loss: 4.6239
Training Epoch: 0 [6144/51200]	Loss: 4.6273
Training Epoch: 0 [8192/51200]	Loss: 4.6353
Training Epoch: 0 [10240/51200]	Loss: 4.6235
Training Epoch: 0 [12288/51200]	Loss: 4.6174
Training Epoch: 0 [14336/51200]	Loss: 4.6155
Training Epoch: 0 [16384/51200]	Loss: 4.6118
Training Epoch: 0 [18432/51200]	Loss: 4.6085
Training Epoch: 0 [20480/51200]	Loss: 4.6027
Training Epoch: 0 [22528/51200]	Loss: 4.6088
Training Epoch: 0 [24576/51200]	Loss: 4.6078
Training Epoch: 0 [26624/51200]	Loss: 4.6052
Training Epoch: 0 [28672/51200]	Loss: 4.6056
Training Epoch: 0 [30720/51200]	Loss: 4.5958
Training Epoch: 0 [32768/51200]	Loss: 4.5992
Training Epoch: 0 [34816/51200]	Loss: 4.5951
Training Epoch: 0 [36864/51200]	Loss: 4.5888
Training Epoch: 0 [38912/51200]	Loss: 4.5819
Training Epoch: 0 [40960/51200]	Loss: 4.5850
Training Epoch: 0 [43008/51200]	Loss: 4.5849
Training Epoch: 0 [45056/51200]	Loss: 4.5740
Training Epoch: 0 [47104/51200]	Loss: 4.5781
Profile done with power limit 150W
epoch 1 train time consumed: 44.25s
Validation Epoch: 0, Average loss: 0.0023, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 2048, 'pl': 150000, 'lr': 4.4721359549995795e-05, 'energy': 146.51940353246158, 'time': 34.52738575100011, 'accuracy': 0.009765625, 'total_cost': 14986698.819596842}
[run job] Launching job with BS 2048: and LR: 4.4721359549995795e-05 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs2048+lr0.00004+pl125', 'ZEUS_COST_THRESH': '29640244.935137883', 'ZEUS_BATCH_SIZE': '2048', 'ZEUS_LEARNING_RATE': '4.4721359549995795e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '2048', '--epochs', '100', '--seed', '1', '--learning_rate', '4.4721359549995795e-05', '--power_limit', '125']
[run job] cost_ub=29640244.935137883
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs2048+lr0.00004+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 4.4721359549995795e-05
batch size arg: 2048
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (25) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 23 iterations (4 for warmup + 19 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [2048/51200]	Loss: 4.6363
Training Epoch: 0 [4096/51200]	Loss: 4.6239
Training Epoch: 0 [6144/51200]	Loss: 4.6273
Training Epoch: 0 [8192/51200]	Loss: 4.6353
Training Epoch: 0 [10240/51200]	Loss: 4.6235
Training Epoch: 0 [12288/51200]	Loss: 4.6173
Training Epoch: 0 [14336/51200]	Loss: 4.6156
Training Epoch: 0 [16384/51200]	Loss: 4.6118
Training Epoch: 0 [18432/51200]	Loss: 4.6086
Training Epoch: 0 [20480/51200]	Loss: 4.6026
Training Epoch: 0 [22528/51200]	Loss: 4.6091
Training Epoch: 0 [24576/51200]	Loss: 4.6081
Training Epoch: 0 [26624/51200]	Loss: 4.6052
Training Epoch: 0 [28672/51200]	Loss: 4.6058
Training Epoch: 0 [30720/51200]	Loss: 4.5961
Training Epoch: 0 [32768/51200]	Loss: 4.5987
Training Epoch: 0 [34816/51200]	Loss: 4.5952
Training Epoch: 0 [36864/51200]	Loss: 4.5896
Training Epoch: 0 [38912/51200]	Loss: 4.5818
Training Epoch: 0 [40960/51200]	Loss: 4.5842
Training Epoch: 0 [43008/51200]	Loss: 4.5846
Training Epoch: 0 [45056/51200]	Loss: 4.5743
Training Epoch: 0 [47104/51200]	Loss: 4.5786
Profile done with power limit 125W
epoch 1 train time consumed: 48.30s
Validation Epoch: 0, Average loss: 0.0023, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 2048, 'pl': 125000, 'lr': 4.4721359549995795e-05, 'energy': 122.64921684017943, 'time': 38.1541098609996, 'accuracy': 0.009765625, 'total_cost': 16559038.23745698}
[run job] Launching job with BS 2048: and LR: 4.4721359549995795e-05 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs2048+lr0.00004+pl100', 'ZEUS_COST_THRESH': '29640244.935137883', 'ZEUS_BATCH_SIZE': '2048', 'ZEUS_LEARNING_RATE': '4.4721359549995795e-05', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '2048', '--epochs', '100', '--seed', '1', '--learning_rate', '4.4721359549995795e-05', '--power_limit', '100']
[run job] cost_ub=29640244.935137883
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs2048+lr0.00004+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 4.4721359549995795e-05
batch size arg: 2048
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (25) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 23 iterations (4 for warmup + 19 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [2048/51200]	Loss: 4.6363
Training Epoch: 0 [4096/51200]	Loss: 4.6239
Training Epoch: 0 [6144/51200]	Loss: 4.6273
Training Epoch: 0 [8192/51200]	Loss: 4.6353
Training Epoch: 0 [10240/51200]	Loss: 4.6235
Training Epoch: 0 [12288/51200]	Loss: 4.6173
Training Epoch: 0 [14336/51200]	Loss: 4.6155
Training Epoch: 0 [16384/51200]	Loss: 4.6119
Training Epoch: 0 [18432/51200]	Loss: 4.6086
Training Epoch: 0 [20480/51200]	Loss: 4.6027
Training Epoch: 0 [22528/51200]	Loss: 4.6090
Training Epoch: 0 [24576/51200]	Loss: 4.6078
Training Epoch: 0 [26624/51200]	Loss: 4.6053
Training Epoch: 0 [28672/51200]	Loss: 4.6058
Training Epoch: 0 [30720/51200]	Loss: 4.5960
Training Epoch: 0 [32768/51200]	Loss: 4.5989
Training Epoch: 0 [34816/51200]	Loss: 4.5951
Training Epoch: 0 [36864/51200]	Loss: 4.5893
Training Epoch: 0 [38912/51200]	Loss: 4.5816
Training Epoch: 0 [40960/51200]	Loss: 4.5843
Training Epoch: 0 [43008/51200]	Loss: 4.5849
Training Epoch: 0 [45056/51200]	Loss: 4.5742
Training Epoch: 0 [47104/51200]	Loss: 4.5783
Profile done with power limit 100W
epoch 1 train time consumed: 103.34s
Validation Epoch: 0, Average loss: 0.0023, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 2048, 'pl': 100000, 'lr': 4.4721359549995795e-05, 'energy': 98.11438281162421, 'time': 83.87942498700068, 'accuracy': 0.009765625, 'total_cost': 36395930.84582711}

[Power Profiler] with batch size 2048 and learning rate 0.00031780497164141406
[run job] Launching job with BS 2048: and LR: 0.00031780497164141406 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs2048+lr0.00032+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '2048', 'ZEUS_LEARNING_RATE': '0.00031780497164141406', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '2048', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00031780497164141406', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs2048+lr0.00032+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.00031780497164141406
batch size arg: 2048
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (25) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 23 iterations (4 for warmup + 19 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [2048/51200]	Loss: 4.6363
Training Epoch: 0 [4096/51200]	Loss: 4.6076
Training Epoch: 0 [6144/51200]	Loss: 4.5985
Training Epoch: 0 [8192/51200]	Loss: 4.5952
Training Epoch: 0 [10240/51200]	Loss: 4.5819
Training Epoch: 0 [12288/51200]	Loss: 4.5659
Training Epoch: 0 [14336/51200]	Loss: 4.5708
Training Epoch: 0 [16384/51200]	Loss: 4.5597
Training Epoch: 0 [18432/51200]	Loss: 4.5479
Training Epoch: 0 [20480/51200]	Loss: 4.5285
Training Epoch: 0 [22528/51200]	Loss: 4.5331
Training Epoch: 0 [24576/51200]	Loss: 4.5222
Training Epoch: 0 [26624/51200]	Loss: 4.4988
Training Epoch: 0 [28672/51200]	Loss: 4.4963
Training Epoch: 0 [30720/51200]	Loss: 4.5228
Training Epoch: 0 [32768/51200]	Loss: 4.5124
Training Epoch: 0 [34816/51200]	Loss: 4.5012
Training Epoch: 0 [36864/51200]	Loss: 4.5009
Training Epoch: 0 [38912/51200]	Loss: 4.4726
Training Epoch: 0 [40960/51200]	Loss: 4.4589
Training Epoch: 0 [43008/51200]	Loss: 4.4804
Training Epoch: 0 [45056/51200]	Loss: 4.4560
Training Epoch: 0 [47104/51200]	Loss: 4.4433
Profile done with power limit 175W
epoch 1 train time consumed: 44.11s
Validation Epoch: 0, Average loss: 0.0023, Accuracy: 0.0095
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 2048, 'pl': 175000, 'lr': 0.00031780497164141406, 'energy': 148.03296100706604, 'time': 34.646565788999396, 'accuracy': 0.00947265625, 'total_cost': 15503584.213737933}
[run job] Launching job with BS 2048: and LR: 0.00031780497164141406 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs2048+lr0.00032+pl150', 'ZEUS_COST_THRESH': '31007168.427475866', 'ZEUS_BATCH_SIZE': '2048', 'ZEUS_LEARNING_RATE': '0.00031780497164141406', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '2048', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00031780497164141406', '--power_limit', '150']
[run job] cost_ub=31007168.427475866
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs2048+lr0.00032+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.00031780497164141406
batch size arg: 2048
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (25) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 23 iterations (4 for warmup + 19 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [2048/51200]	Loss: 4.6363
Training Epoch: 0 [4096/51200]	Loss: 4.6076
Training Epoch: 0 [6144/51200]	Loss: 4.5985
Training Epoch: 0 [8192/51200]	Loss: 4.5952
Training Epoch: 0 [10240/51200]	Loss: 4.5820
Training Epoch: 0 [12288/51200]	Loss: 4.5669
Training Epoch: 0 [14336/51200]	Loss: 4.5708
Training Epoch: 0 [16384/51200]	Loss: 4.5589
Training Epoch: 0 [18432/51200]	Loss: 4.5495
Training Epoch: 0 [20480/51200]	Loss: 4.5306
Training Epoch: 0 [22528/51200]	Loss: 4.5357
Training Epoch: 0 [24576/51200]	Loss: 4.5221
Training Epoch: 0 [26624/51200]	Loss: 4.5032
Training Epoch: 0 [28672/51200]	Loss: 4.4963
Training Epoch: 0 [30720/51200]	Loss: 4.5213
Training Epoch: 0 [32768/51200]	Loss: 4.5130
Training Epoch: 0 [34816/51200]	Loss: 4.5024
Training Epoch: 0 [36864/51200]	Loss: 4.4976
Training Epoch: 0 [38912/51200]	Loss: 4.4715
Training Epoch: 0 [40960/51200]	Loss: 4.4629
Training Epoch: 0 [43008/51200]	Loss: 4.4787
Training Epoch: 0 [45056/51200]	Loss: 4.4567
Training Epoch: 0 [47104/51200]	Loss: 4.4412
Profile done with power limit 150W
epoch 1 train time consumed: 44.09s
Validation Epoch: 0, Average loss: 0.0023, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 2048, 'pl': 150000, 'lr': 0.00031780497164141406, 'energy': 146.64282057437924, 'time': 34.50261269699968, 'accuracy': 0.009765625, 'total_cost': 14975956.778362848}
[run job] Launching job with BS 2048: and LR: 0.00031780497164141406 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs2048+lr0.00032+pl125', 'ZEUS_COST_THRESH': '29951913.556725696', 'ZEUS_BATCH_SIZE': '2048', 'ZEUS_LEARNING_RATE': '0.00031780497164141406', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '2048', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00031780497164141406', '--power_limit', '125']
[run job] cost_ub=29951913.556725696
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs2048+lr0.00032+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.00031780497164141406
batch size arg: 2048
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (25) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 23 iterations (4 for warmup + 19 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [2048/51200]	Loss: 4.6363
Training Epoch: 0 [4096/51200]	Loss: 4.6076
Training Epoch: 0 [6144/51200]	Loss: 4.5985
Training Epoch: 0 [8192/51200]	Loss: 4.5953
Training Epoch: 0 [10240/51200]	Loss: 4.5819
Training Epoch: 0 [12288/51200]	Loss: 4.5662
Training Epoch: 0 [14336/51200]	Loss: 4.5706
Training Epoch: 0 [16384/51200]	Loss: 4.5598
Training Epoch: 0 [18432/51200]	Loss: 4.5480
Training Epoch: 0 [20480/51200]	Loss: 4.5299
Training Epoch: 0 [22528/51200]	Loss: 4.5341
Training Epoch: 0 [24576/51200]	Loss: 4.5225
Training Epoch: 0 [26624/51200]	Loss: 4.5031
Training Epoch: 0 [28672/51200]	Loss: 4.4960
Training Epoch: 0 [30720/51200]	Loss: 4.5228
Training Epoch: 0 [32768/51200]	Loss: 4.5134
Training Epoch: 0 [34816/51200]	Loss: 4.5008
Training Epoch: 0 [36864/51200]	Loss: 4.4950
Training Epoch: 0 [38912/51200]	Loss: 4.4666
Training Epoch: 0 [40960/51200]	Loss: 4.4575
Training Epoch: 0 [43008/51200]	Loss: 4.4782
Training Epoch: 0 [45056/51200]	Loss: 4.4542
Training Epoch: 0 [47104/51200]	Loss: 4.4415
Profile done with power limit 125W
epoch 1 train time consumed: 48.44s
Validation Epoch: 0, Average loss: 0.0023, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 2048, 'pl': 125000, 'lr': 0.00031780497164141406, 'energy': 122.84791434650147, 'time': 38.19700901400029, 'accuracy': 0.009765625, 'total_cost': 16577659.507326176}
[run job] Launching job with BS 2048: and LR: 0.00031780497164141406 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351', 'ZEUS_JOB_ID': 'bs2048+lr0.00032+pl100', 'ZEUS_COST_THRESH': '29951913.556725696', 'ZEUS_BATCH_SIZE': '2048', 'ZEUS_LEARNING_RATE': '0.00031780497164141406', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '2048', '--epochs', '100', '--seed', '1', '--learning_rate', '0.00031780497164141406', '--power_limit', '100']
[run job] cost_ub=29951913.556725696
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs1024+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112522121669432351/bs2048+lr0.00032+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.00031780497164141406
batch size arg: 2048
ProfileDataLoader constructor: called constructor of DataLoader!
[Profile DataLoader] The profile window takes 50 iterations (10 for warmup + 40 for profile) and exceeds the number of iterations (25) in one epoch. Scaling the profile window to fit in one epoch...
[Profile DataLoader] Scaling done! New profile window takes 23 iterations (4 for warmup + 19 for profile).
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [2048/51200]	Loss: 4.6363
Training Epoch: 0 [4096/51200]	Loss: 4.6076
Training Epoch: 0 [6144/51200]	Loss: 4.5985
Training Epoch: 0 [8192/51200]	Loss: 4.5952
Training Epoch: 0 [10240/51200]	Loss: 4.5816
Training Epoch: 0 [12288/51200]	Loss: 4.5661
Training Epoch: 0 [14336/51200]	Loss: 4.5707
Training Epoch: 0 [16384/51200]	Loss: 4.5595
Training Epoch: 0 [18432/51200]	Loss: 4.5478
Training Epoch: 0 [20480/51200]	Loss: 4.5305
Training Epoch: 0 [22528/51200]	Loss: 4.5357
Training Epoch: 0 [24576/51200]	Loss: 4.5226
Training Epoch: 0 [26624/51200]	Loss: 4.5001
Training Epoch: 0 [28672/51200]	Loss: 4.4983
Training Epoch: 0 [30720/51200]	Loss: 4.5219
Training Epoch: 0 [32768/51200]	Loss: 4.5124
Training Epoch: 0 [34816/51200]	Loss: 4.5011
Training Epoch: 0 [36864/51200]	Loss: 4.5002
Training Epoch: 0 [38912/51200]	Loss: 4.4718
Training Epoch: 0 [40960/51200]	Loss: 4.4607
Training Epoch: 0 [43008/51200]	Loss: 4.4793
Training Epoch: 0 [45056/51200]	Loss: 4.4574
Training Epoch: 0 [47104/51200]	Loss: 4.4408
Profile done with power limit 100W
epoch 1 train time consumed: 105.52s
Validation Epoch: 0, Average loss: 0.0023, Accuracy: 0.0098
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 2048, 'pl': 100000, 'lr': 0.00031780497164141406, 'energy': 98.38904983615055, 'time': 85.1129923260005, 'accuracy': 0.009765625, 'total_cost': 36931130.21999755}
[Power Profiler]
[HistoryEntry(bs=8, pl=175, energy=82.58275456563551, time=2.3174024310001187, accuracy=0.01, total_cost=4088.4873560764627), HistoryEntry(bs=8, pl=150, energy=80.25333204630772, time=2.374768388999655, accuracy=0.01, total_cost=4187.946013567919), HistoryEntry(bs=8, pl=125, energy=81.83520515653255, time=2.3189265480004906, accuracy=0.0099, total_cost=4132.177314205526), HistoryEntry(bs=8, pl=100, energy=83.44478981885383, time=2.274567181999373, accuracy=0.01, total_cost=4013.870484426445), HistoryEntry(bs=8, pl=175, energy=88.005798735016, time=2.173690054999497, accuracy=0.0112, total_cost=3427.821353342077), HistoryEntry(bs=8, pl=150, energy=88.97610251265722, time=2.242281455999546, accuracy=0.011, total_cost=3599.6208990947894), HistoryEntry(bs=8, pl=125, energy=85.82851240929077, time=2.255532666000363, accuracy=0.0111, total_cost=3586.9491625804967), HistoryEntry(bs=8, pl=100, energy=87.56180252209676, time=2.2456502539998837, accuracy=0.0119, total_cost=3331.859382780365), HistoryEntry(bs=16, pl=175, energy=107.1587775034141, time=2.4688133199997537, accuracy=0.0098, total_cost=8904.666981634562), HistoryEntry(bs=16, pl=150, energy=107.13288977471339, time=2.524389973000325, accuracy=0.0098, total_cost=9103.133895225417), HistoryEntry(bs=16, pl=125, energy=107.34882969263539, time=2.533772251999835, accuracy=0.0098, total_cost=9136.8183119934), HistoryEntry(bs=16, pl=100, energy=98.15100052294723, time=2.6601100509997195, accuracy=0.0098, total_cost=9580.51630501773), HistoryEntry(bs=16, pl=175, energy=107.24327810978069, time=2.4729733569993186, accuracy=0.0121, total_cost=7224.133365277225), HistoryEntry(bs=16, pl=150, energy=108.31975337951117, time=2.4756612219998715, accuracy=0.0127, total_cost=6890.921322600912), HistoryEntry(bs=16, pl=125, energy=108.0251306464466, time=2.530810377999842, accuracy=0.0128, total_cost=6987.700333997346), HistoryEntry(bs=16, pl=100, energy=98.45412548403087, time=2.586913416000243, accuracy=0.0122, total_cost=7486.032997039406), HistoryEntry(bs=32, pl=175, energy=124.94669736723206, time=3.2907728649997807, accuracy=0.009984025559105431, total_cost=23265.056472348144), HistoryEntry(bs=32, pl=150, energy=125.68911631565382, time=3.276060543999847, accuracy=0.009984025559105431, total_cost=23163.127835427207), HistoryEntry(bs=32, pl=125, energy=120.8300199180341, time=3.4144483119998768, accuracy=0.009984025559105431, total_cost=24125.297266688038), HistoryEntry(bs=32, pl=100, energy=98.70453284623137, time=3.811435232999429, accuracy=0.009984025559105431, total_cost=26872.31550080339), HistoryEntry(bs=32, pl=175, energy=123.72049772801523, time=3.3889381489998414, accuracy=0.01228035143769968, total_cost=19472.46204068901), HistoryEntry(bs=32, pl=150, energy=124.35639360394443, time=3.3918819579994306, accuracy=0.011481629392971246, total_cost=20845.895718639207), HistoryEntry(bs=32, pl=125, energy=121.97757032684703, time=3.278242682000382, accuracy=0.011281948881789138, total_cost=20506.616172332877), HistoryEntry(bs=32, pl=100, energy=98.44390240441486, time=3.7920772500001476, accuracy=0.01178115015974441, total_cost=22657.81244530416), HistoryEntry(bs=64, pl=175, energy=135.99288131081485, time=5.384508112999356, accuracy=0.010151273885350318, total_cost=74616.81746115503), HistoryEntry(bs=64, pl=150, energy=134.45144340231482, time=5.468797582000661, accuracy=0.010051751592356687, total_cost=76523.54644351103), HistoryEntry(bs=64, pl=125, energy=123.00376541853791, time=5.609262044999923, accuracy=0.010051751592356687, total_cost=78441.6390231705), HistoryEntry(bs=64, pl=100, energy=97.56247761580696, time=6.632539956999608, accuracy=0.010051751592356687, total_cost=92599.1885128214), HistoryEntry(bs=64, pl=175, energy=135.97671070460044, time=5.346900024000206, accuracy=0.011146496815286623, total_cost=67482.65066798984), HistoryEntry(bs=64, pl=150, energy=136.05120836833643, time=5.377354012000069, accuracy=0.01194267515923567, total_cost=63340.66626950343), HistoryEntry(bs=64, pl=125, energy=122.88531410260008, time=5.633039928000471, accuracy=0.011644108280254776, total_cost=67999.86525646591), HistoryEntry(bs=64, pl=100, energy=98.09147957815537, time=6.611415551999926, accuracy=0.011445063694267515, total_cost=81069.56823438176), HistoryEntry(bs=128, pl=175, energy=139.3572357461117, time=8.822673620999922, accuracy=0.010482594936708861, total_cost=236285.92708639023), HistoryEntry(bs=128, pl=150, energy=139.73605271788264, time=8.753814968999905, accuracy=0.010581487341772151, total_cost=232259.59389633522), HistoryEntry(bs=128, pl=125, energy=122.5357405834043, time=9.28990251200048, accuracy=0.010680379746835443, total_cost=244046.82233333375), HistoryEntry(bs=128, pl=100, energy=98.59729827143344, time=11.041151051999805, accuracy=0.010482594936708861, total_cost=295238.1040514115), HistoryEntry(bs=128, pl=175, energy=140.0925384269126, time=8.771167259999856, accuracy=0.011075949367088608, total_cost=222331.16512774403), HistoryEntry(bs=128, pl=150, energy=139.8006288385306, time=8.792504326000198, accuracy=0.011965981012658227, total_cost=206291.37797819957), HistoryEntry(bs=128, pl=125, energy=122.78657509880935, time=9.327858290999757, accuracy=0.010087025316455696, total_cost=259456.69013667092), HistoryEntry(bs=128, pl=100, energy=98.33831803576037, time=11.13747637600045, accuracy=0.011372626582278481, total_cost=274500.3624742786), HistoryEntry(bs=256, pl=175, energy=147.91544706034037, time=9.752818318000209, accuracy=0.01103515625, total_cost=494898.991809991), HistoryEntry(bs=256, pl=150, energy=145.4251832151443, time=9.736000805000003, accuracy=0.0111328125, total_cost=489686.2579888597), HistoryEntry(bs=256, pl=125, energy=122.16036590797935, time=10.70808712500002, accuracy=0.0111328125, total_cost=538145.8393158102), HistoryEntry(bs=256, pl=100, energy=98.48512490409874, time=13.148589262999849, accuracy=0.011328125, total_cost=648822.6674232619), HistoryEntry(bs=256, pl=175, energy=146.9971672481358, time=9.793276755000079, accuracy=0.01201171875, total_cost=456533.15249774395), HistoryEntry(bs=256, pl=150, energy=145.38528202126685, time=9.738714594999692, accuracy=0.01103515625, total_cost=494156.5368700153), HistoryEntry(bs=256, pl=125, energy=122.7362195355823, time=10.705316474999563, accuracy=0.01142578125, total_cost=524218.33787722926), HistoryEntry(bs=256, pl=100, energy=98.55861873738141, time=13.168525174000024, accuracy=0.01044921875, total_cost=704462.1019109669), HistoryEntry(bs=512, pl=175, energy=147.88957520096696, time=18.652101215000584, accuracy=0.01064453125, total_cost=1959203.6221730108), HistoryEntry(bs=512, pl=150, energy=146.00962981095824, time=18.67574999000044, accuracy=0.0109375, total_cost=1909094.2469052568), HistoryEntry(bs=512, pl=125, energy=122.93025455644637, time=20.508353145, accuracy=0.0109375, total_cost=2095556.2743774103), HistoryEntry(bs=512, pl=100, energy=96.48015361701886, time=40.361692316999324, accuracy=0.0107421875, total_cost=4195713.758289908), HistoryEntry(bs=512, pl=175, energy=147.76453761990356, time=18.60461743199994, accuracy=0.01357421875, total_cost=1532447.4559881538), HistoryEntry(bs=512, pl=150, energy=145.88568289989595, time=18.62459855799989, accuracy=0.012890625, total_cost=1615406.39454099), HistoryEntry(bs=512, pl=125, energy=122.39753444756845, time=20.527446312000393, accuracy=0.01357421875, total_cost=1690065.6025737054), HistoryEntry(bs=512, pl=100, energy=96.62853553482012, time=42.20682161100012, accuracy=0.0140625, total_cost=3351500.645508578), HistoryEntry(bs=1024, pl=175, energy=148.56158686138454, time=34.41040736800005, accuracy=0.0111328125, total_cost=6833105.177281486), HistoryEntry(bs=1024, pl=150, energy=145.80795637644056, time=34.82652115499968, accuracy=0.0109375, total_cost=7039031.44416046), HistoryEntry(bs=1024, pl=125, energy=122.16435839679788, time=38.26629502899959, accuracy=0.01123046875, total_cost=7530879.961925026), HistoryEntry(bs=1024, pl=100, energy=96.73357476804128, time=71.39534332800031, accuracy=0.01064453125, total_cost=14818264.17776669), HistoryEntry(bs=1024, pl=175, energy=148.81729130372582, time=34.355349280000155, accuracy=0.0201171875, total_cost=3775388.458522991), HistoryEntry(bs=1024, pl=150, energy=145.6980417935453, time=34.920249884999976, accuracy=0.01640625, total_cost=4705302.39078133), HistoryEntry(bs=1024, pl=125, energy=122.58382874490184, time=38.39685815099983, accuracy=0.01875, total_cost=4526073.65571231), HistoryEntry(bs=1024, pl=100, energy=96.89942441105258, time=72.16457277500012, accuracy=0.0197265625, total_cost=8082123.363692388), HistoryEntry(bs=2048, pl=175, energy=148.88460431793334, time=34.143182105999585, accuracy=0.009765625, total_cost=14820122.467568941), HistoryEntry(bs=2048, pl=150, energy=146.51940353246158, time=34.52738575100011, accuracy=0.009765625, total_cost=14986698.819596842), HistoryEntry(bs=2048, pl=125, energy=122.64921684017943, time=38.1541098609996, accuracy=0.009765625, total_cost=16559038.23745698), HistoryEntry(bs=2048, pl=100, energy=98.11438281162421, time=83.87942498700068, accuracy=0.009765625, total_cost=36395930.84582711), HistoryEntry(bs=2048, pl=175, energy=148.03296100706604, time=34.646565788999396, accuracy=0.00947265625, total_cost=15503584.213737933), HistoryEntry(bs=2048, pl=150, energy=146.64282057437924, time=34.50261269699968, accuracy=0.009765625, total_cost=14975956.778362848), HistoryEntry(bs=2048, pl=125, energy=122.84791434650147, time=38.19700901400029, accuracy=0.009765625, total_cost=16577659.507326176), HistoryEntry(bs=2048, pl=100, energy=98.38904983615055, time=85.1129923260005, accuracy=0.009765625, total_cost=36931130.21999755)]
