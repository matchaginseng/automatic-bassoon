[Power Profiler] Batch sizes: [8, 16, 32, 64, 128, 256, 512]
[Power Profiler] Learning rates factors: [0.8, 1.0, 1.2]

[Power Profiler] with batch size 8 and learning rate 0.0008
[run job] Launching job with BS 8: and LR: 0.0008 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00080+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0008', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00080+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0008
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8076
Training Epoch: 0 [24/50000]	Loss: 4.7133
Training Epoch: 0 [32/50000]	Loss: 4.7900
Training Epoch: 0 [40/50000]	Loss: 4.7448
Training Epoch: 0 [48/50000]	Loss: 5.5634
Training Epoch: 0 [56/50000]	Loss: 5.4205
Training Epoch: 0 [64/50000]	Loss: 5.0032
Training Epoch: 0 [72/50000]	Loss: 5.2349
Training Epoch: 0 [80/50000]	Loss: 5.0745
Training Epoch: 0 [88/50000]	Loss: 4.8223
Training Epoch: 0 [96/50000]	Loss: 5.9079
Training Epoch: 0 [104/50000]	Loss: 5.8223
Training Epoch: 0 [112/50000]	Loss: 5.0360
Training Epoch: 0 [120/50000]	Loss: 5.6502
Training Epoch: 0 [128/50000]	Loss: 5.8230
Training Epoch: 0 [136/50000]	Loss: 5.5681
Training Epoch: 0 [144/50000]	Loss: 5.0551
Training Epoch: 0 [152/50000]	Loss: 5.1709
Training Epoch: 0 [160/50000]	Loss: 6.0310
Training Epoch: 0 [168/50000]	Loss: 5.4475
Training Epoch: 0 [176/50000]	Loss: 5.0421
Training Epoch: 0 [184/50000]	Loss: 5.4683
Training Epoch: 0 [192/50000]	Loss: 4.5063
Training Epoch: 0 [200/50000]	Loss: 5.6406
Training Epoch: 0 [208/50000]	Loss: 5.2547
Training Epoch: 0 [216/50000]	Loss: 4.8728
Training Epoch: 0 [224/50000]	Loss: 5.3231
Training Epoch: 0 [232/50000]	Loss: 4.8043
Training Epoch: 0 [240/50000]	Loss: 5.2156
Training Epoch: 0 [248/50000]	Loss: 5.1373
Training Epoch: 0 [256/50000]	Loss: 5.3538
Training Epoch: 0 [264/50000]	Loss: 4.7286
Training Epoch: 0 [272/50000]	Loss: 5.5395
Training Epoch: 0 [280/50000]	Loss: 5.0332
Training Epoch: 0 [288/50000]	Loss: 5.3546
Training Epoch: 0 [296/50000]	Loss: 5.5320
Training Epoch: 0 [304/50000]	Loss: 5.1925
Training Epoch: 0 [312/50000]	Loss: 4.8324
Training Epoch: 0 [320/50000]	Loss: 5.0601
Training Epoch: 0 [328/50000]	Loss: 5.2649
Training Epoch: 0 [336/50000]	Loss: 5.0607
Training Epoch: 0 [344/50000]	Loss: 4.4595
Training Epoch: 0 [352/50000]	Loss: 4.7711
Training Epoch: 0 [360/50000]	Loss: 5.5779
Training Epoch: 0 [368/50000]	Loss: 4.9723
Training Epoch: 0 [376/50000]	Loss: 4.8201
Training Epoch: 0 [384/50000]	Loss: 4.6454
Training Epoch: 0 [392/50000]	Loss: 4.6721
Training Epoch: 0 [400/50000]	Loss: 4.6644
Profile done with power limit 175W
epoch 1 train time consumed: 6.28s
Validation Epoch: 0, Average loss: 0.6235, Accuracy: 0.0115
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 175000, 'lr': 0.0008, 'energy': 97.06921267349081, 'time': 2.180380848000027, 'accuracy': 0.0115, 'total_cost': 5.158386962119354}
[run job] Launching job with BS 8: and LR: 0.0008 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00080+pl150', 'ZEUS_COST_THRESH': '10.316773924238708', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0008', '--power_limit', '150']
[run job] cost_ub=10.316773924238708
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00080+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0008
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8076
Training Epoch: 0 [24/50000]	Loss: 4.7136
Training Epoch: 0 [32/50000]	Loss: 4.7937
Training Epoch: 0 [40/50000]	Loss: 4.7622
Training Epoch: 0 [48/50000]	Loss: 5.5875
Training Epoch: 0 [56/50000]	Loss: 5.3805
Training Epoch: 0 [64/50000]	Loss: 4.9843
Training Epoch: 0 [72/50000]	Loss: 5.2081
Training Epoch: 0 [80/50000]	Loss: 5.1358
Training Epoch: 0 [88/50000]	Loss: 4.8473
Training Epoch: 0 [96/50000]	Loss: 5.8643
Training Epoch: 0 [104/50000]	Loss: 5.8257
Training Epoch: 0 [112/50000]	Loss: 4.9646
Training Epoch: 0 [120/50000]	Loss: 5.5938
Training Epoch: 0 [128/50000]	Loss: 5.8390
Training Epoch: 0 [136/50000]	Loss: 5.6412
Training Epoch: 0 [144/50000]	Loss: 5.0960
Training Epoch: 0 [152/50000]	Loss: 5.1412
Training Epoch: 0 [160/50000]	Loss: 6.0594
Training Epoch: 0 [168/50000]	Loss: 5.4404
Training Epoch: 0 [176/50000]	Loss: 5.0381
Training Epoch: 0 [184/50000]	Loss: 5.3099
Training Epoch: 0 [192/50000]	Loss: 4.4807
Training Epoch: 0 [200/50000]	Loss: 5.6920
Training Epoch: 0 [208/50000]	Loss: 5.3796
Training Epoch: 0 [216/50000]	Loss: 4.8663
Training Epoch: 0 [224/50000]	Loss: 5.1927
Training Epoch: 0 [232/50000]	Loss: 4.8599
Training Epoch: 0 [240/50000]	Loss: 5.1809
Training Epoch: 0 [248/50000]	Loss: 5.0285
Training Epoch: 0 [256/50000]	Loss: 5.2560
Training Epoch: 0 [264/50000]	Loss: 4.7052
Training Epoch: 0 [272/50000]	Loss: 5.5243
Training Epoch: 0 [280/50000]	Loss: 5.0327
Training Epoch: 0 [288/50000]	Loss: 5.3365
Training Epoch: 0 [296/50000]	Loss: 5.4468
Training Epoch: 0 [304/50000]	Loss: 5.2080
Training Epoch: 0 [312/50000]	Loss: 4.6519
Training Epoch: 0 [320/50000]	Loss: 5.0101
Training Epoch: 0 [328/50000]	Loss: 5.3606
Training Epoch: 0 [336/50000]	Loss: 5.0324
Training Epoch: 0 [344/50000]	Loss: 4.5361
Training Epoch: 0 [352/50000]	Loss: 4.8559
Training Epoch: 0 [360/50000]	Loss: 5.5665
Training Epoch: 0 [368/50000]	Loss: 5.0472
Training Epoch: 0 [376/50000]	Loss: 4.8135
Training Epoch: 0 [384/50000]	Loss: 4.6038
Training Epoch: 0 [392/50000]	Loss: 4.8285
Training Epoch: 0 [400/50000]	Loss: 4.5563
Profile done with power limit 150W
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.6240, Accuracy: 0.0101
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 150000, 'lr': 0.0008, 'energy': 95.98314935374884, 'time': 2.242382289000034, 'accuracy': 0.0101, 'total_cost': 6.016314997309875}
[run job] Launching job with BS 8: and LR: 0.0008 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00080+pl125', 'ZEUS_COST_THRESH': '10.316773924238708', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0008', '--power_limit', '125']
[run job] cost_ub=10.316773924238708
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00080+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0008
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8076
Training Epoch: 0 [24/50000]	Loss: 4.7133
Training Epoch: 0 [32/50000]	Loss: 4.7880
Training Epoch: 0 [40/50000]	Loss: 4.7575
Training Epoch: 0 [48/50000]	Loss: 5.6126
Training Epoch: 0 [56/50000]	Loss: 5.4235
Training Epoch: 0 [64/50000]	Loss: 4.9210
Training Epoch: 0 [72/50000]	Loss: 5.1506
Training Epoch: 0 [80/50000]	Loss: 5.1277
Training Epoch: 0 [88/50000]	Loss: 4.8595
Training Epoch: 0 [96/50000]	Loss: 5.9121
Training Epoch: 0 [104/50000]	Loss: 5.8032
Training Epoch: 0 [112/50000]	Loss: 4.9824
Training Epoch: 0 [120/50000]	Loss: 5.6125
Training Epoch: 0 [128/50000]	Loss: 5.8915
Training Epoch: 0 [136/50000]	Loss: 5.6220
Training Epoch: 0 [144/50000]	Loss: 5.0515
Training Epoch: 0 [152/50000]	Loss: 5.1529
Training Epoch: 0 [160/50000]	Loss: 5.9629
Training Epoch: 0 [168/50000]	Loss: 5.4825
Training Epoch: 0 [176/50000]	Loss: 5.0533
Training Epoch: 0 [184/50000]	Loss: 5.3729
Training Epoch: 0 [192/50000]	Loss: 4.5666
Training Epoch: 0 [200/50000]	Loss: 5.8419
Training Epoch: 0 [208/50000]	Loss: 5.3443
Training Epoch: 0 [216/50000]	Loss: 4.8352
Training Epoch: 0 [224/50000]	Loss: 5.1743
Training Epoch: 0 [232/50000]	Loss: 4.8451
Training Epoch: 0 [240/50000]	Loss: 5.2505
Training Epoch: 0 [248/50000]	Loss: 5.1295
Training Epoch: 0 [256/50000]	Loss: 5.3685
Training Epoch: 0 [264/50000]	Loss: 4.7138
Training Epoch: 0 [272/50000]	Loss: 5.5551
Training Epoch: 0 [280/50000]	Loss: 5.0044
Training Epoch: 0 [288/50000]	Loss: 5.2685
Training Epoch: 0 [296/50000]	Loss: 5.5257
Training Epoch: 0 [304/50000]	Loss: 5.1732
Training Epoch: 0 [312/50000]	Loss: 4.6199
Training Epoch: 0 [320/50000]	Loss: 5.0593
Training Epoch: 0 [328/50000]	Loss: 5.3014
Training Epoch: 0 [336/50000]	Loss: 5.0999
Training Epoch: 0 [344/50000]	Loss: 4.5021
Training Epoch: 0 [352/50000]	Loss: 4.7748
Training Epoch: 0 [360/50000]	Loss: 5.4970
Training Epoch: 0 [368/50000]	Loss: 5.0672
Training Epoch: 0 [376/50000]	Loss: 4.9095
Training Epoch: 0 [384/50000]	Loss: 4.5718
Training Epoch: 0 [392/50000]	Loss: 4.8217
Training Epoch: 0 [400/50000]	Loss: 4.5812
Profile done with power limit 125W
epoch 1 train time consumed: 3.20s
Validation Epoch: 0, Average loss: 0.6241, Accuracy: 0.0133
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 125000, 'lr': 0.0008, 'energy': 96.9676319806666, 'time': 2.238269564999996, 'accuracy': 0.0133, 'total_cost': 4.576968972386811}
[run job] Launching job with BS 8: and LR: 0.0008 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00080+pl100', 'ZEUS_COST_THRESH': '9.153937944773622', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0008', '--power_limit', '100']
[run job] cost_ub=9.153937944773622
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00080+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0008
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8076
Training Epoch: 0 [24/50000]	Loss: 4.7134
Training Epoch: 0 [32/50000]	Loss: 4.7920
Training Epoch: 0 [40/50000]	Loss: 4.7744
Training Epoch: 0 [48/50000]	Loss: 5.6040
Training Epoch: 0 [56/50000]	Loss: 5.3955
Training Epoch: 0 [64/50000]	Loss: 4.9416
Training Epoch: 0 [72/50000]	Loss: 5.1870
Training Epoch: 0 [80/50000]	Loss: 5.1301
Training Epoch: 0 [88/50000]	Loss: 4.8955
Training Epoch: 0 [96/50000]	Loss: 5.9174
Training Epoch: 0 [104/50000]	Loss: 5.7724
Training Epoch: 0 [112/50000]	Loss: 4.9957
Training Epoch: 0 [120/50000]	Loss: 5.6490
Training Epoch: 0 [128/50000]	Loss: 5.8171
Training Epoch: 0 [136/50000]	Loss: 5.6351
Training Epoch: 0 [144/50000]	Loss: 5.1471
Training Epoch: 0 [152/50000]	Loss: 5.1313
Training Epoch: 0 [160/50000]	Loss: 6.0533
Training Epoch: 0 [168/50000]	Loss: 5.5114
Training Epoch: 0 [176/50000]	Loss: 5.0689
Training Epoch: 0 [184/50000]	Loss: 5.4580
Training Epoch: 0 [192/50000]	Loss: 4.4951
Training Epoch: 0 [200/50000]	Loss: 5.7156
Training Epoch: 0 [208/50000]	Loss: 5.3715
Training Epoch: 0 [216/50000]	Loss: 4.9033
Training Epoch: 0 [224/50000]	Loss: 5.2275
Training Epoch: 0 [232/50000]	Loss: 4.7904
Training Epoch: 0 [240/50000]	Loss: 5.1717
Training Epoch: 0 [248/50000]	Loss: 5.0655
Training Epoch: 0 [256/50000]	Loss: 5.3106
Training Epoch: 0 [264/50000]	Loss: 4.7144
Training Epoch: 0 [272/50000]	Loss: 5.5934
Training Epoch: 0 [280/50000]	Loss: 5.0258
Training Epoch: 0 [288/50000]	Loss: 5.2573
Training Epoch: 0 [296/50000]	Loss: 5.4381
Training Epoch: 0 [304/50000]	Loss: 5.1905
Training Epoch: 0 [312/50000]	Loss: 4.6728
Training Epoch: 0 [320/50000]	Loss: 5.0818
Training Epoch: 0 [328/50000]	Loss: 5.3172
Training Epoch: 0 [336/50000]	Loss: 5.0383
Training Epoch: 0 [344/50000]	Loss: 4.4834
Training Epoch: 0 [352/50000]	Loss: 4.7765
Training Epoch: 0 [360/50000]	Loss: 5.5524
Training Epoch: 0 [368/50000]	Loss: 4.9062
Training Epoch: 0 [376/50000]	Loss: 4.8868
Training Epoch: 0 [384/50000]	Loss: 4.6296
Training Epoch: 0 [392/50000]	Loss: 4.7718
Training Epoch: 0 [400/50000]	Loss: 4.5303
Profile done with power limit 100W
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.6276, Accuracy: 0.0102
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 100000, 'lr': 0.0008, 'energy': 97.48630493993177, 'time': 2.1913649130000294, 'accuracy': 0.0102, 'total_cost': 5.8540875286116965}

[Power Profiler] with batch size 8 and learning rate 0.001
[run job] Launching job with BS 8: and LR: 0.001 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00100+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.001', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.001', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00100+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.001
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8286
Training Epoch: 0 [24/50000]	Loss: 4.7007
Training Epoch: 0 [32/50000]	Loss: 4.8804
Training Epoch: 0 [40/50000]	Loss: 4.8747
Training Epoch: 0 [48/50000]	Loss: 5.8861
Training Epoch: 0 [56/50000]	Loss: 5.6911
Training Epoch: 0 [64/50000]	Loss: 5.1279
Training Epoch: 0 [72/50000]	Loss: 5.4216
Training Epoch: 0 [80/50000]	Loss: 5.3272
Training Epoch: 0 [88/50000]	Loss: 5.1296
Training Epoch: 0 [96/50000]	Loss: 6.2302
Training Epoch: 0 [104/50000]	Loss: 6.1194
Training Epoch: 0 [112/50000]	Loss: 5.1775
Training Epoch: 0 [120/50000]	Loss: 5.8250
Training Epoch: 0 [128/50000]	Loss: 6.0945
Training Epoch: 0 [136/50000]	Loss: 5.7882
Training Epoch: 0 [144/50000]	Loss: 5.1330
Training Epoch: 0 [152/50000]	Loss: 5.3961
Training Epoch: 0 [160/50000]	Loss: 6.1865
Training Epoch: 0 [168/50000]	Loss: 5.4870
Training Epoch: 0 [176/50000]	Loss: 5.0809
Training Epoch: 0 [184/50000]	Loss: 5.5889
Training Epoch: 0 [192/50000]	Loss: 4.5064
Training Epoch: 0 [200/50000]	Loss: 5.7860
Training Epoch: 0 [208/50000]	Loss: 5.4626
Training Epoch: 0 [216/50000]	Loss: 4.9134
Training Epoch: 0 [224/50000]	Loss: 5.2883
Training Epoch: 0 [232/50000]	Loss: 4.7517
Training Epoch: 0 [240/50000]	Loss: 5.2540
Training Epoch: 0 [248/50000]	Loss: 5.0655
Training Epoch: 0 [256/50000]	Loss: 5.4159
Training Epoch: 0 [264/50000]	Loss: 4.7880
Training Epoch: 0 [272/50000]	Loss: 5.6434
Training Epoch: 0 [280/50000]	Loss: 5.1083
Training Epoch: 0 [288/50000]	Loss: 5.5662
Training Epoch: 0 [296/50000]	Loss: 5.6341
Training Epoch: 0 [304/50000]	Loss: 5.1002
Training Epoch: 0 [312/50000]	Loss: 4.7208
Training Epoch: 0 [320/50000]	Loss: 5.1512
Training Epoch: 0 [328/50000]	Loss: 5.3559
Training Epoch: 0 [336/50000]	Loss: 5.0876
Training Epoch: 0 [344/50000]	Loss: 4.4841
Training Epoch: 0 [352/50000]	Loss: 4.8584
Training Epoch: 0 [360/50000]	Loss: 5.5408
Training Epoch: 0 [368/50000]	Loss: 5.0572
Training Epoch: 0 [376/50000]	Loss: 5.0344
Training Epoch: 0 [384/50000]	Loss: 4.5638
Training Epoch: 0 [392/50000]	Loss: 4.9911
Training Epoch: 0 [400/50000]	Loss: 4.5816
Profile done with power limit 175W
epoch 1 train time consumed: 3.22s
Validation Epoch: 0, Average loss: 0.6300, Accuracy: 0.0122
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 175000, 'lr': 0.001, 'energy': 97.34030920608366, 'time': 2.227547700000059, 'accuracy': 0.0122, 'total_cost': 4.972549421223905}
[run job] Launching job with BS 8: and LR: 0.001 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00100+pl150', 'ZEUS_COST_THRESH': '9.94509884244781', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.001', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.001', '--power_limit', '150']
[run job] cost_ub=9.94509884244781
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00100+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.001
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8286
Training Epoch: 0 [24/50000]	Loss: 4.7008
Training Epoch: 0 [32/50000]	Loss: 4.8785
Training Epoch: 0 [40/50000]	Loss: 4.8681
Training Epoch: 0 [48/50000]	Loss: 5.8974
Training Epoch: 0 [56/50000]	Loss: 5.6971
Training Epoch: 0 [64/50000]	Loss: 5.1051
Training Epoch: 0 [72/50000]	Loss: 5.4177
Training Epoch: 0 [80/50000]	Loss: 5.2782
Training Epoch: 0 [88/50000]	Loss: 5.1825
Training Epoch: 0 [96/50000]	Loss: 6.2814
Training Epoch: 0 [104/50000]	Loss: 6.1249
Training Epoch: 0 [112/50000]	Loss: 5.1759
Training Epoch: 0 [120/50000]	Loss: 5.8149
Training Epoch: 0 [128/50000]	Loss: 6.0854
Training Epoch: 0 [136/50000]	Loss: 5.7593
Training Epoch: 0 [144/50000]	Loss: 5.1850
Training Epoch: 0 [152/50000]	Loss: 5.2543
Training Epoch: 0 [160/50000]	Loss: 6.1891
Training Epoch: 0 [168/50000]	Loss: 5.4828
Training Epoch: 0 [176/50000]	Loss: 5.1409
Training Epoch: 0 [184/50000]	Loss: 5.5906
Training Epoch: 0 [192/50000]	Loss: 4.5049
Training Epoch: 0 [200/50000]	Loss: 5.9905
Training Epoch: 0 [208/50000]	Loss: 5.4007
Training Epoch: 0 [216/50000]	Loss: 4.9559
Training Epoch: 0 [224/50000]	Loss: 5.2896
Training Epoch: 0 [232/50000]	Loss: 4.8345
Training Epoch: 0 [240/50000]	Loss: 5.2816
Training Epoch: 0 [248/50000]	Loss: 5.1579
Training Epoch: 0 [256/50000]	Loss: 5.4351
Training Epoch: 0 [264/50000]	Loss: 4.9599
Training Epoch: 0 [272/50000]	Loss: 5.6891
Training Epoch: 0 [280/50000]	Loss: 5.2139
Training Epoch: 0 [288/50000]	Loss: 5.5769
Training Epoch: 0 [296/50000]	Loss: 5.6734
Training Epoch: 0 [304/50000]	Loss: 5.1910
Training Epoch: 0 [312/50000]	Loss: 4.7423
Training Epoch: 0 [320/50000]	Loss: 5.1125
Training Epoch: 0 [328/50000]	Loss: 5.4070
Training Epoch: 0 [336/50000]	Loss: 5.0419
Training Epoch: 0 [344/50000]	Loss: 4.5578
Training Epoch: 0 [352/50000]	Loss: 4.7402
Training Epoch: 0 [360/50000]	Loss: 5.5479
Training Epoch: 0 [368/50000]	Loss: 4.9805
Training Epoch: 0 [376/50000]	Loss: 4.8443
Training Epoch: 0 [384/50000]	Loss: 4.5902
Training Epoch: 0 [392/50000]	Loss: 4.7244
Training Epoch: 0 [400/50000]	Loss: 4.5742
Profile done with power limit 150W
epoch 1 train time consumed: 3.26s
Validation Epoch: 0, Average loss: 0.6247, Accuracy: 0.0117
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 150000, 'lr': 0.001, 'energy': 95.12453371430512, 'time': 2.283929243999978, 'accuracy': 0.0117, 'total_cost': 5.273036940785978}
[run job] Launching job with BS 8: and LR: 0.001 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00100+pl125', 'ZEUS_COST_THRESH': '9.94509884244781', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.001', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.001', '--power_limit', '125']
[run job] cost_ub=9.94509884244781
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00100+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.001
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8286
Training Epoch: 0 [24/50000]	Loss: 4.7004
Training Epoch: 0 [32/50000]	Loss: 4.8894
Training Epoch: 0 [40/50000]	Loss: 4.8607
Training Epoch: 0 [48/50000]	Loss: 5.9000
Training Epoch: 0 [56/50000]	Loss: 5.6970
Training Epoch: 0 [64/50000]	Loss: 5.0936
Training Epoch: 0 [72/50000]	Loss: 5.3835
Training Epoch: 0 [80/50000]	Loss: 5.4060
Training Epoch: 0 [88/50000]	Loss: 5.1306
Training Epoch: 0 [96/50000]	Loss: 6.2837
Training Epoch: 0 [104/50000]	Loss: 6.1350
Training Epoch: 0 [112/50000]	Loss: 5.1757
Training Epoch: 0 [120/50000]	Loss: 5.8921
Training Epoch: 0 [128/50000]	Loss: 6.0587
Training Epoch: 0 [136/50000]	Loss: 5.7996
Training Epoch: 0 [144/50000]	Loss: 5.1870
Training Epoch: 0 [152/50000]	Loss: 5.3434
Training Epoch: 0 [160/50000]	Loss: 6.1120
Training Epoch: 0 [168/50000]	Loss: 5.5603
Training Epoch: 0 [176/50000]	Loss: 5.0554
Training Epoch: 0 [184/50000]	Loss: 5.5712
Training Epoch: 0 [192/50000]	Loss: 4.4833
Training Epoch: 0 [200/50000]	Loss: 5.9368
Training Epoch: 0 [208/50000]	Loss: 5.4192
Training Epoch: 0 [216/50000]	Loss: 4.9318
Training Epoch: 0 [224/50000]	Loss: 5.3515
Training Epoch: 0 [232/50000]	Loss: 4.8766
Training Epoch: 0 [240/50000]	Loss: 5.3223
Training Epoch: 0 [248/50000]	Loss: 5.0545
Training Epoch: 0 [256/50000]	Loss: 5.3871
Training Epoch: 0 [264/50000]	Loss: 4.7631
Training Epoch: 0 [272/50000]	Loss: 5.6675
Training Epoch: 0 [280/50000]	Loss: 5.2563
Training Epoch: 0 [288/50000]	Loss: 5.5510
Training Epoch: 0 [296/50000]	Loss: 5.6703
Training Epoch: 0 [304/50000]	Loss: 5.2250
Training Epoch: 0 [312/50000]	Loss: 4.7996
Training Epoch: 0 [320/50000]	Loss: 5.0610
Training Epoch: 0 [328/50000]	Loss: 5.3264
Training Epoch: 0 [336/50000]	Loss: 5.0876
Training Epoch: 0 [344/50000]	Loss: 4.6122
Training Epoch: 0 [352/50000]	Loss: 4.7435
Training Epoch: 0 [360/50000]	Loss: 5.4811
Training Epoch: 0 [368/50000]	Loss: 4.9649
Training Epoch: 0 [376/50000]	Loss: 4.8402
Training Epoch: 0 [384/50000]	Loss: 4.7261
Training Epoch: 0 [392/50000]	Loss: 4.9862
Training Epoch: 0 [400/50000]	Loss: 4.4911
Profile done with power limit 125W
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.6273, Accuracy: 0.0101
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 125000, 'lr': 0.001, 'energy': 97.67314956625486, 'time': 2.2052223619999722, 'accuracy': 0.0101, 'total_cost': 5.95351412812345}
[run job] Launching job with BS 8: and LR: 0.001 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00100+pl100', 'ZEUS_COST_THRESH': '9.94509884244781', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.001', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.001', '--power_limit', '100']
[run job] cost_ub=9.94509884244781
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00100+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.001
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8286
Training Epoch: 0 [24/50000]	Loss: 4.7003
Training Epoch: 0 [32/50000]	Loss: 4.8852
Training Epoch: 0 [40/50000]	Loss: 4.8945
Training Epoch: 0 [48/50000]	Loss: 5.9009
Training Epoch: 0 [56/50000]	Loss: 5.7368
Training Epoch: 0 [64/50000]	Loss: 5.0686
Training Epoch: 0 [72/50000]	Loss: 5.4143
Training Epoch: 0 [80/50000]	Loss: 5.3805
Training Epoch: 0 [88/50000]	Loss: 5.1481
Training Epoch: 0 [96/50000]	Loss: 6.2761
Training Epoch: 0 [104/50000]	Loss: 6.0976
Training Epoch: 0 [112/50000]	Loss: 5.2356
Training Epoch: 0 [120/50000]	Loss: 5.8580
Training Epoch: 0 [128/50000]	Loss: 6.0265
Training Epoch: 0 [136/50000]	Loss: 5.8125
Training Epoch: 0 [144/50000]	Loss: 5.1643
Training Epoch: 0 [152/50000]	Loss: 5.2224
Training Epoch: 0 [160/50000]	Loss: 6.2115
Training Epoch: 0 [168/50000]	Loss: 5.5318
Training Epoch: 0 [176/50000]	Loss: 5.1526
Training Epoch: 0 [184/50000]	Loss: 5.5611
Training Epoch: 0 [192/50000]	Loss: 4.5842
Training Epoch: 0 [200/50000]	Loss: 5.8784
Training Epoch: 0 [208/50000]	Loss: 5.4695
Training Epoch: 0 [216/50000]	Loss: 4.8882
Training Epoch: 0 [224/50000]	Loss: 5.3930
Training Epoch: 0 [232/50000]	Loss: 4.7953
Training Epoch: 0 [240/50000]	Loss: 5.2995
Training Epoch: 0 [248/50000]	Loss: 5.0494
Training Epoch: 0 [256/50000]	Loss: 5.4084
Training Epoch: 0 [264/50000]	Loss: 4.8739
Training Epoch: 0 [272/50000]	Loss: 5.7960
Training Epoch: 0 [280/50000]	Loss: 5.2584
Training Epoch: 0 [288/50000]	Loss: 5.6065
Training Epoch: 0 [296/50000]	Loss: 5.6701
Training Epoch: 0 [304/50000]	Loss: 5.2103
Training Epoch: 0 [312/50000]	Loss: 4.7450
Training Epoch: 0 [320/50000]	Loss: 5.1518
Training Epoch: 0 [328/50000]	Loss: 5.4109
Training Epoch: 0 [336/50000]	Loss: 5.1000
Training Epoch: 0 [344/50000]	Loss: 4.6258
Training Epoch: 0 [352/50000]	Loss: 4.8148
Training Epoch: 0 [360/50000]	Loss: 5.4882
Training Epoch: 0 [368/50000]	Loss: 4.9802
Training Epoch: 0 [376/50000]	Loss: 4.9501
Training Epoch: 0 [384/50000]	Loss: 4.5523
Training Epoch: 0 [392/50000]	Loss: 4.7567
Training Epoch: 0 [400/50000]	Loss: 4.5668
Profile done with power limit 100W
epoch 1 train time consumed: 3.15s
Validation Epoch: 0, Average loss: 0.6297, Accuracy: 0.0118
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 100000, 'lr': 0.001, 'energy': 97.89078810054312, 'time': 2.189029348999952, 'accuracy': 0.0118, 'total_cost': 5.062423256134033}

[Power Profiler] with batch size 8 and learning rate 0.0012
[run job] Launching job with BS 8: and LR: 0.0012 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00120+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0012', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0012', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00120+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0012
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8424
Training Epoch: 0 [24/50000]	Loss: 4.7711
Training Epoch: 0 [32/50000]	Loss: 4.9708
Training Epoch: 0 [40/50000]	Loss: 4.9894
Training Epoch: 0 [48/50000]	Loss: 6.2381
Training Epoch: 0 [56/50000]	Loss: 5.9411
Training Epoch: 0 [64/50000]	Loss: 5.2271
Training Epoch: 0 [72/50000]	Loss: 5.6259
Training Epoch: 0 [80/50000]	Loss: 5.5846
Training Epoch: 0 [88/50000]	Loss: 5.3438
Training Epoch: 0 [96/50000]	Loss: 6.6003
Training Epoch: 0 [104/50000]	Loss: 6.3166
Training Epoch: 0 [112/50000]	Loss: 5.5219
Training Epoch: 0 [120/50000]	Loss: 6.1270
Training Epoch: 0 [128/50000]	Loss: 6.3784
Training Epoch: 0 [136/50000]	Loss: 5.9735
Training Epoch: 0 [144/50000]	Loss: 5.3028
Training Epoch: 0 [152/50000]	Loss: 5.3417
Training Epoch: 0 [160/50000]	Loss: 6.2661
Training Epoch: 0 [168/50000]	Loss: 5.6986
Training Epoch: 0 [176/50000]	Loss: 5.1333
Training Epoch: 0 [184/50000]	Loss: 5.6588
Training Epoch: 0 [192/50000]	Loss: 4.5831
Training Epoch: 0 [200/50000]	Loss: 6.0336
Training Epoch: 0 [208/50000]	Loss: 5.6643
Training Epoch: 0 [216/50000]	Loss: 4.8291
Training Epoch: 0 [224/50000]	Loss: 5.4131
Training Epoch: 0 [232/50000]	Loss: 4.9412
Training Epoch: 0 [240/50000]	Loss: 5.2313
Training Epoch: 0 [248/50000]	Loss: 5.0094
Training Epoch: 0 [256/50000]	Loss: 5.4166
Training Epoch: 0 [264/50000]	Loss: 5.0237
Training Epoch: 0 [272/50000]	Loss: 5.9178
Training Epoch: 0 [280/50000]	Loss: 5.3602
Training Epoch: 0 [288/50000]	Loss: 5.6354
Training Epoch: 0 [296/50000]	Loss: 5.6516
Training Epoch: 0 [304/50000]	Loss: 5.2345
Training Epoch: 0 [312/50000]	Loss: 4.7581
Training Epoch: 0 [320/50000]	Loss: 5.0948
Training Epoch: 0 [328/50000]	Loss: 5.4359
Training Epoch: 0 [336/50000]	Loss: 5.0290
Training Epoch: 0 [344/50000]	Loss: 4.5568
Training Epoch: 0 [352/50000]	Loss: 4.8119
Training Epoch: 0 [360/50000]	Loss: 5.5109
Training Epoch: 0 [368/50000]	Loss: 4.9780
Training Epoch: 0 [376/50000]	Loss: 4.9602
Training Epoch: 0 [384/50000]	Loss: 4.6093
Training Epoch: 0 [392/50000]	Loss: 4.8511
Training Epoch: 0 [400/50000]	Loss: 4.5156
Profile done with power limit 175W
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.6323, Accuracy: 0.0142
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 175000, 'lr': 0.0012, 'energy': 97.41434316929761, 'time': 2.228714114000013, 'accuracy': 0.0142, 'total_cost': 4.275589376601808}
[run job] Launching job with BS 8: and LR: 0.0012 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00120+pl150', 'ZEUS_COST_THRESH': '8.551178753203615', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0012', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0012', '--power_limit', '150']
[run job] cost_ub=8.551178753203615
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00120+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0012
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8424
Training Epoch: 0 [24/50000]	Loss: 4.7710
Training Epoch: 0 [32/50000]	Loss: 4.9693
Training Epoch: 0 [40/50000]	Loss: 4.9667
Training Epoch: 0 [48/50000]	Loss: 6.1872
Training Epoch: 0 [56/50000]	Loss: 5.9724
Training Epoch: 0 [64/50000]	Loss: 5.2691
Training Epoch: 0 [72/50000]	Loss: 5.6599
Training Epoch: 0 [80/50000]	Loss: 5.5374
Training Epoch: 0 [88/50000]	Loss: 5.3059
Training Epoch: 0 [96/50000]	Loss: 6.6070
Training Epoch: 0 [104/50000]	Loss: 6.3864
Training Epoch: 0 [112/50000]	Loss: 5.3812
Training Epoch: 0 [120/50000]	Loss: 6.1289
Training Epoch: 0 [128/50000]	Loss: 6.3188
Training Epoch: 0 [136/50000]	Loss: 5.8665
Training Epoch: 0 [144/50000]	Loss: 5.2543
Training Epoch: 0 [152/50000]	Loss: 5.3257
Training Epoch: 0 [160/50000]	Loss: 6.3405
Training Epoch: 0 [168/50000]	Loss: 5.7170
Training Epoch: 0 [176/50000]	Loss: 5.0012
Training Epoch: 0 [184/50000]	Loss: 5.6624
Training Epoch: 0 [192/50000]	Loss: 4.6505
Training Epoch: 0 [200/50000]	Loss: 6.0777
Training Epoch: 0 [208/50000]	Loss: 5.7658
Training Epoch: 0 [216/50000]	Loss: 4.9443
Training Epoch: 0 [224/50000]	Loss: 5.3356
Training Epoch: 0 [232/50000]	Loss: 4.8246
Training Epoch: 0 [240/50000]	Loss: 5.2846
Training Epoch: 0 [248/50000]	Loss: 4.9016
Training Epoch: 0 [256/50000]	Loss: 5.5539
Training Epoch: 0 [264/50000]	Loss: 5.0248
Training Epoch: 0 [272/50000]	Loss: 5.9276
Training Epoch: 0 [280/50000]	Loss: 5.3689
Training Epoch: 0 [288/50000]	Loss: 5.7231
Training Epoch: 0 [296/50000]	Loss: 5.6798
Training Epoch: 0 [304/50000]	Loss: 5.2557
Training Epoch: 0 [312/50000]	Loss: 4.8040
Training Epoch: 0 [320/50000]	Loss: 5.2786
Training Epoch: 0 [328/50000]	Loss: 5.4319
Training Epoch: 0 [336/50000]	Loss: 5.0654
Training Epoch: 0 [344/50000]	Loss: 4.6215
Training Epoch: 0 [352/50000]	Loss: 4.8652
Training Epoch: 0 [360/50000]	Loss: 5.5621
Training Epoch: 0 [368/50000]	Loss: 4.9146
Training Epoch: 0 [376/50000]	Loss: 4.9558
Training Epoch: 0 [384/50000]	Loss: 4.6097
Training Epoch: 0 [392/50000]	Loss: 4.7942
Training Epoch: 0 [400/50000]	Loss: 4.5340
Profile done with power limit 150W
epoch 1 train time consumed: 3.18s
Validation Epoch: 0, Average loss: 0.6375, Accuracy: 0.0117
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 150000, 'lr': 0.0012, 'energy': 99.28335179485042, 'time': 2.2040448610000567, 'accuracy': 0.0117, 'total_cost': 5.166947110951375}
[run job] Launching job with BS 8: and LR: 0.0012 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00120+pl125', 'ZEUS_COST_THRESH': '8.551178753203615', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0012', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0012', '--power_limit', '125']
[run job] cost_ub=8.551178753203615
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00120+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0012
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8424
Training Epoch: 0 [24/50000]	Loss: 4.7710
Training Epoch: 0 [32/50000]	Loss: 4.9687
Training Epoch: 0 [40/50000]	Loss: 4.9728
Training Epoch: 0 [48/50000]	Loss: 6.1969
Training Epoch: 0 [56/50000]	Loss: 5.9840
Training Epoch: 0 [64/50000]	Loss: 5.2791
Training Epoch: 0 [72/50000]	Loss: 5.6443
Training Epoch: 0 [80/50000]	Loss: 5.6175
Training Epoch: 0 [88/50000]	Loss: 5.3746
Training Epoch: 0 [96/50000]	Loss: 6.5992
Training Epoch: 0 [104/50000]	Loss: 6.3849
Training Epoch: 0 [112/50000]	Loss: 5.4019
Training Epoch: 0 [120/50000]	Loss: 6.0773
Training Epoch: 0 [128/50000]	Loss: 6.2907
Training Epoch: 0 [136/50000]	Loss: 5.8967
Training Epoch: 0 [144/50000]	Loss: 5.2759
Training Epoch: 0 [152/50000]	Loss: 5.3780
Training Epoch: 0 [160/50000]	Loss: 6.2885
Training Epoch: 0 [168/50000]	Loss: 5.6394
Training Epoch: 0 [176/50000]	Loss: 5.1534
Training Epoch: 0 [184/50000]	Loss: 5.6376
Training Epoch: 0 [192/50000]	Loss: 4.5949
Training Epoch: 0 [200/50000]	Loss: 6.1506
Training Epoch: 0 [208/50000]	Loss: 5.8098
Training Epoch: 0 [216/50000]	Loss: 5.0094
Training Epoch: 0 [224/50000]	Loss: 5.4189
Training Epoch: 0 [232/50000]	Loss: 4.8042
Training Epoch: 0 [240/50000]	Loss: 5.3226
Training Epoch: 0 [248/50000]	Loss: 5.0864
Training Epoch: 0 [256/50000]	Loss: 5.4569
Training Epoch: 0 [264/50000]	Loss: 4.9605
Training Epoch: 0 [272/50000]	Loss: 5.9028
Training Epoch: 0 [280/50000]	Loss: 5.2713
Training Epoch: 0 [288/50000]	Loss: 5.7012
Training Epoch: 0 [296/50000]	Loss: 5.6618
Training Epoch: 0 [304/50000]	Loss: 5.2180
Training Epoch: 0 [312/50000]	Loss: 4.7836
Training Epoch: 0 [320/50000]	Loss: 5.2692
Training Epoch: 0 [328/50000]	Loss: 5.3791
Training Epoch: 0 [336/50000]	Loss: 5.1045
Training Epoch: 0 [344/50000]	Loss: 4.6061
Training Epoch: 0 [352/50000]	Loss: 4.7901
Training Epoch: 0 [360/50000]	Loss: 5.5117
Training Epoch: 0 [368/50000]	Loss: 4.8749
Training Epoch: 0 [376/50000]	Loss: 4.8174
Training Epoch: 0 [384/50000]	Loss: 4.6714
Training Epoch: 0 [392/50000]	Loss: 4.7420
Training Epoch: 0 [400/50000]	Loss: 4.5770
Profile done with power limit 125W
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.6473, Accuracy: 0.0109
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 125000, 'lr': 0.0012, 'energy': 96.05392124180507, 'time': 2.2358609709999655, 'accuracy': 0.0109, 'total_cost': 5.559989757257345}
[run job] Launching job with BS 8: and LR: 0.0012 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs8+lr0.00120+pl100', 'ZEUS_COST_THRESH': '8.551178753203615', 'ZEUS_BATCH_SIZE': '8', 'ZEUS_LEARNING_RATE': '0.0012', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '8', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0012', '--power_limit', '100']
[run job] cost_ub=8.551178753203615
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs8+lr0.00120+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0012
batch size arg: 8
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [8/50000]	Loss: 4.6550
Training Epoch: 0 [16/50000]	Loss: 4.8424
Training Epoch: 0 [24/50000]	Loss: 4.7713
Training Epoch: 0 [32/50000]	Loss: 4.9706
Training Epoch: 0 [40/50000]	Loss: 4.9514
Training Epoch: 0 [48/50000]	Loss: 6.2145
Training Epoch: 0 [56/50000]	Loss: 6.0007
Training Epoch: 0 [64/50000]	Loss: 5.2733
Training Epoch: 0 [72/50000]	Loss: 5.6432
Training Epoch: 0 [80/50000]	Loss: 5.5807
Training Epoch: 0 [88/50000]	Loss: 5.3786
Training Epoch: 0 [96/50000]	Loss: 6.5182
Training Epoch: 0 [104/50000]	Loss: 6.3483
Training Epoch: 0 [112/50000]	Loss: 5.3408
Training Epoch: 0 [120/50000]	Loss: 6.1348
Training Epoch: 0 [128/50000]	Loss: 6.2838
Training Epoch: 0 [136/50000]	Loss: 5.8340
Training Epoch: 0 [144/50000]	Loss: 5.3164
Training Epoch: 0 [152/50000]	Loss: 5.4141
Training Epoch: 0 [160/50000]	Loss: 6.3359
Training Epoch: 0 [168/50000]	Loss: 5.6403
Training Epoch: 0 [176/50000]	Loss: 5.1229
Training Epoch: 0 [184/50000]	Loss: 5.6491
Training Epoch: 0 [192/50000]	Loss: 4.6267
Training Epoch: 0 [200/50000]	Loss: 6.0624
Training Epoch: 0 [208/50000]	Loss: 5.6822
Training Epoch: 0 [216/50000]	Loss: 4.9019
Training Epoch: 0 [224/50000]	Loss: 5.4157
Training Epoch: 0 [232/50000]	Loss: 4.8642
Training Epoch: 0 [240/50000]	Loss: 5.3919
Training Epoch: 0 [248/50000]	Loss: 4.9256
Training Epoch: 0 [256/50000]	Loss: 5.3115
Training Epoch: 0 [264/50000]	Loss: 4.9901
Training Epoch: 0 [272/50000]	Loss: 5.9002
Training Epoch: 0 [280/50000]	Loss: 5.3205
Training Epoch: 0 [288/50000]	Loss: 5.6923
Training Epoch: 0 [296/50000]	Loss: 5.6965
Training Epoch: 0 [304/50000]	Loss: 5.2099
Training Epoch: 0 [312/50000]	Loss: 4.8433
Training Epoch: 0 [320/50000]	Loss: 5.1080
Training Epoch: 0 [328/50000]	Loss: 5.4051
Training Epoch: 0 [336/50000]	Loss: 5.0127
Training Epoch: 0 [344/50000]	Loss: 4.7241
Training Epoch: 0 [352/50000]	Loss: 4.7706
Training Epoch: 0 [360/50000]	Loss: 5.5541
Training Epoch: 0 [368/50000]	Loss: 4.8561
Training Epoch: 0 [376/50000]	Loss: 4.9643
Training Epoch: 0 [384/50000]	Loss: 4.6417
Training Epoch: 0 [392/50000]	Loss: 4.9163
Training Epoch: 0 [400/50000]	Loss: 4.5035
Profile done with power limit 100W
epoch 1 train time consumed: 3.21s
Validation Epoch: 0, Average loss: 0.6329, Accuracy: 0.0124
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 8, 'pl': 100000, 'lr': 0.0012, 'energy': 95.78115449898867, 'time': 2.240701842999897, 'accuracy': 0.0124, 'total_cost': 4.893063160770352}

[Power Profiler] with batch size 16 and learning rate 0.0011313708498984763
[run job] Launching job with BS 16: and LR: 0.0011313708498984763 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00113+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0011313708498984763', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0011313708498984763', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00113+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0011313708498984763
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6490
Training Epoch: 0 [48/50000]	Loss: 4.9717
Training Epoch: 0 [64/50000]	Loss: 5.0852
Training Epoch: 0 [80/50000]	Loss: 4.9977
Training Epoch: 0 [96/50000]	Loss: 5.2276
Training Epoch: 0 [112/50000]	Loss: 5.3510
Training Epoch: 0 [128/50000]	Loss: 5.5345
Training Epoch: 0 [144/50000]	Loss: 5.3235
Training Epoch: 0 [160/50000]	Loss: 5.4972
Training Epoch: 0 [176/50000]	Loss: 5.2351
Training Epoch: 0 [192/50000]	Loss: 4.9777
Training Epoch: 0 [208/50000]	Loss: 5.5583
Training Epoch: 0 [224/50000]	Loss: 5.0281
Training Epoch: 0 [240/50000]	Loss: 5.0652
Training Epoch: 0 [256/50000]	Loss: 5.2933
Training Epoch: 0 [272/50000]	Loss: 4.9948
Training Epoch: 0 [288/50000]	Loss: 5.3733
Training Epoch: 0 [304/50000]	Loss: 5.4669
Training Epoch: 0 [320/50000]	Loss: 4.9509
Training Epoch: 0 [336/50000]	Loss: 5.2957
Training Epoch: 0 [352/50000]	Loss: 4.7324
Training Epoch: 0 [368/50000]	Loss: 5.6236
Training Epoch: 0 [384/50000]	Loss: 4.8962
Training Epoch: 0 [400/50000]	Loss: 4.9146
Training Epoch: 0 [416/50000]	Loss: 4.7214
Training Epoch: 0 [432/50000]	Loss: 4.5495
Training Epoch: 0 [448/50000]	Loss: 5.0564
Training Epoch: 0 [464/50000]	Loss: 4.9276
Training Epoch: 0 [480/50000]	Loss: 5.0820
Training Epoch: 0 [496/50000]	Loss: 4.9375
Training Epoch: 0 [512/50000]	Loss: 4.8918
Training Epoch: 0 [528/50000]	Loss: 4.6930
Training Epoch: 0 [544/50000]	Loss: 4.8509
Training Epoch: 0 [560/50000]	Loss: 4.5908
Training Epoch: 0 [576/50000]	Loss: 4.6714
Training Epoch: 0 [592/50000]	Loss: 5.0289
Training Epoch: 0 [608/50000]	Loss: 4.9648
Training Epoch: 0 [624/50000]	Loss: 5.7111
Training Epoch: 0 [640/50000]	Loss: 4.9257
Training Epoch: 0 [656/50000]	Loss: 5.2028
Training Epoch: 0 [672/50000]	Loss: 4.6313
Training Epoch: 0 [688/50000]	Loss: 5.0211
Training Epoch: 0 [704/50000]	Loss: 4.5036
Training Epoch: 0 [720/50000]	Loss: 4.9814
Training Epoch: 0 [736/50000]	Loss: 4.9384
Training Epoch: 0 [752/50000]	Loss: 4.9577
Training Epoch: 0 [768/50000]	Loss: 5.1187
Training Epoch: 0 [784/50000]	Loss: 4.7865
Training Epoch: 0 [800/50000]	Loss: 4.9468
Profile done with power limit 175W
epoch 1 train time consumed: 3.51s
Validation Epoch: 0, Average loss: 0.3012, Accuracy: 0.0103
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 175000, 'lr': 0.0011313708498984763, 'energy': 116.12608123203319, 'time': 2.398267027999964, 'accuracy': 0.0103, 'total_cost': 13.5572443030995}
[run job] Launching job with BS 16: and LR: 0.0011313708498984763 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00113+pl150', 'ZEUS_COST_THRESH': '27.114488606199', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0011313708498984763', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0011313708498984763', '--power_limit', '150']
[run job] cost_ub=27.114488606199
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00113+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0011313708498984763
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6490
Training Epoch: 0 [48/50000]	Loss: 4.9716
Training Epoch: 0 [64/50000]	Loss: 5.0877
Training Epoch: 0 [80/50000]	Loss: 4.9992
Training Epoch: 0 [96/50000]	Loss: 5.2424
Training Epoch: 0 [112/50000]	Loss: 5.3226
Training Epoch: 0 [128/50000]	Loss: 5.5221
Training Epoch: 0 [144/50000]	Loss: 5.2883
Training Epoch: 0 [160/50000]	Loss: 5.5032
Training Epoch: 0 [176/50000]	Loss: 5.2756
Training Epoch: 0 [192/50000]	Loss: 4.9699
Training Epoch: 0 [208/50000]	Loss: 5.5945
Training Epoch: 0 [224/50000]	Loss: 4.9985
Training Epoch: 0 [240/50000]	Loss: 5.0350
Training Epoch: 0 [256/50000]	Loss: 5.3052
Training Epoch: 0 [272/50000]	Loss: 4.9803
Training Epoch: 0 [288/50000]	Loss: 5.3066
Training Epoch: 0 [304/50000]	Loss: 5.4695
Training Epoch: 0 [320/50000]	Loss: 4.9186
Training Epoch: 0 [336/50000]	Loss: 5.2570
Training Epoch: 0 [352/50000]	Loss: 4.7920
Training Epoch: 0 [368/50000]	Loss: 5.6198
Training Epoch: 0 [384/50000]	Loss: 4.8267
Training Epoch: 0 [400/50000]	Loss: 4.8335
Training Epoch: 0 [416/50000]	Loss: 4.7902
Training Epoch: 0 [432/50000]	Loss: 4.5668
Training Epoch: 0 [448/50000]	Loss: 4.9863
Training Epoch: 0 [464/50000]	Loss: 5.0235
Training Epoch: 0 [480/50000]	Loss: 5.1059
Training Epoch: 0 [496/50000]	Loss: 4.9541
Training Epoch: 0 [512/50000]	Loss: 4.9099
Training Epoch: 0 [528/50000]	Loss: 4.6967
Training Epoch: 0 [544/50000]	Loss: 4.7865
Training Epoch: 0 [560/50000]	Loss: 4.7057
Training Epoch: 0 [576/50000]	Loss: 4.6093
Training Epoch: 0 [592/50000]	Loss: 5.0178
Training Epoch: 0 [608/50000]	Loss: 4.9307
Training Epoch: 0 [624/50000]	Loss: 5.6909
Training Epoch: 0 [640/50000]	Loss: 5.0398
Training Epoch: 0 [656/50000]	Loss: 5.1246
Training Epoch: 0 [672/50000]	Loss: 4.6359
Training Epoch: 0 [688/50000]	Loss: 5.0162
Training Epoch: 0 [704/50000]	Loss: 4.4178
Training Epoch: 0 [720/50000]	Loss: 4.9580
Training Epoch: 0 [736/50000]	Loss: 4.8690
Training Epoch: 0 [752/50000]	Loss: 4.9266
Training Epoch: 0 [768/50000]	Loss: 5.2061
Training Epoch: 0 [784/50000]	Loss: 4.7763
Training Epoch: 0 [800/50000]	Loss: 4.9599
Profile done with power limit 150W
epoch 1 train time consumed: 3.56s
Validation Epoch: 0, Average loss: 0.3021, Accuracy: 0.0101
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 150000, 'lr': 0.0011313708498984763, 'energy': 115.2645611281345, 'time': 2.4296023320000586, 'accuracy': 0.0101, 'total_cost': 13.964900091364141}
[run job] Launching job with BS 16: and LR: 0.0011313708498984763 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00113+pl125', 'ZEUS_COST_THRESH': '27.114488606199', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0011313708498984763', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0011313708498984763', '--power_limit', '125']
[run job] cost_ub=27.114488606199
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00113+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0011313708498984763
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6490
Training Epoch: 0 [48/50000]	Loss: 4.9716
Training Epoch: 0 [64/50000]	Loss: 5.0815
Training Epoch: 0 [80/50000]	Loss: 5.0080
Training Epoch: 0 [96/50000]	Loss: 5.2254
Training Epoch: 0 [112/50000]	Loss: 5.3009
Training Epoch: 0 [128/50000]	Loss: 5.5579
Training Epoch: 0 [144/50000]	Loss: 5.3045
Training Epoch: 0 [160/50000]	Loss: 5.5310
Training Epoch: 0 [176/50000]	Loss: 5.2771
Training Epoch: 0 [192/50000]	Loss: 4.9089
Training Epoch: 0 [208/50000]	Loss: 5.5072
Training Epoch: 0 [224/50000]	Loss: 5.0172
Training Epoch: 0 [240/50000]	Loss: 5.0139
Training Epoch: 0 [256/50000]	Loss: 5.2639
Training Epoch: 0 [272/50000]	Loss: 5.0044
Training Epoch: 0 [288/50000]	Loss: 5.3540
Training Epoch: 0 [304/50000]	Loss: 5.4732
Training Epoch: 0 [320/50000]	Loss: 4.9320
Training Epoch: 0 [336/50000]	Loss: 5.2439
Training Epoch: 0 [352/50000]	Loss: 4.7672
Training Epoch: 0 [368/50000]	Loss: 5.5554
Training Epoch: 0 [384/50000]	Loss: 4.8767
Training Epoch: 0 [400/50000]	Loss: 4.8360
Training Epoch: 0 [416/50000]	Loss: 4.8243
Training Epoch: 0 [432/50000]	Loss: 4.5676
Training Epoch: 0 [448/50000]	Loss: 5.0410
Training Epoch: 0 [464/50000]	Loss: 4.9487
Training Epoch: 0 [480/50000]	Loss: 5.1213
Training Epoch: 0 [496/50000]	Loss: 5.0064
Training Epoch: 0 [512/50000]	Loss: 4.9883
Training Epoch: 0 [528/50000]	Loss: 4.6720
Training Epoch: 0 [544/50000]	Loss: 4.7599
Training Epoch: 0 [560/50000]	Loss: 4.6303
Training Epoch: 0 [576/50000]	Loss: 4.6516
Training Epoch: 0 [592/50000]	Loss: 5.0213
Training Epoch: 0 [608/50000]	Loss: 4.9430
Training Epoch: 0 [624/50000]	Loss: 5.6792
Training Epoch: 0 [640/50000]	Loss: 4.9729
Training Epoch: 0 [656/50000]	Loss: 5.1754
Training Epoch: 0 [672/50000]	Loss: 4.6902
Training Epoch: 0 [688/50000]	Loss: 5.0187
Training Epoch: 0 [704/50000]	Loss: 4.4689
Training Epoch: 0 [720/50000]	Loss: 4.9648
Training Epoch: 0 [736/50000]	Loss: 4.9131
Training Epoch: 0 [752/50000]	Loss: 4.9509
Training Epoch: 0 [768/50000]	Loss: 5.2984
Training Epoch: 0 [784/50000]	Loss: 4.7860
Training Epoch: 0 [800/50000]	Loss: 4.9334
Profile done with power limit 125W
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.3021, Accuracy: 0.0107
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 125000, 'lr': 0.0011313708498984763, 'energy': 115.88892358227561, 'time': 2.438761488999944, 'accuracy': 0.0107, 'total_cost': 13.25997578334769}
[run job] Launching job with BS 16: and LR: 0.0011313708498984763 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00113+pl100', 'ZEUS_COST_THRESH': '26.51995156669538', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0011313708498984763', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0011313708498984763', '--power_limit', '100']
[run job] cost_ub=26.51995156669538
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00113+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0011313708498984763
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6490
Training Epoch: 0 [48/50000]	Loss: 4.9717
Training Epoch: 0 [64/50000]	Loss: 5.0816
Training Epoch: 0 [80/50000]	Loss: 5.0114
Training Epoch: 0 [96/50000]	Loss: 5.2052
Training Epoch: 0 [112/50000]	Loss: 5.2787
Training Epoch: 0 [128/50000]	Loss: 5.5171
Training Epoch: 0 [144/50000]	Loss: 5.3362
Training Epoch: 0 [160/50000]	Loss: 5.4730
Training Epoch: 0 [176/50000]	Loss: 5.2551
Training Epoch: 0 [192/50000]	Loss: 4.9260
Training Epoch: 0 [208/50000]	Loss: 5.5828
Training Epoch: 0 [224/50000]	Loss: 5.0222
Training Epoch: 0 [240/50000]	Loss: 5.0405
Training Epoch: 0 [256/50000]	Loss: 5.3547
Training Epoch: 0 [272/50000]	Loss: 5.0237
Training Epoch: 0 [288/50000]	Loss: 5.3356
Training Epoch: 0 [304/50000]	Loss: 5.4814
Training Epoch: 0 [320/50000]	Loss: 4.9313
Training Epoch: 0 [336/50000]	Loss: 5.2923
Training Epoch: 0 [352/50000]	Loss: 4.7045
Training Epoch: 0 [368/50000]	Loss: 5.6275
Training Epoch: 0 [384/50000]	Loss: 4.9017
Training Epoch: 0 [400/50000]	Loss: 4.8578
Training Epoch: 0 [416/50000]	Loss: 4.7191
Training Epoch: 0 [432/50000]	Loss: 4.5798
Training Epoch: 0 [448/50000]	Loss: 4.9952
Training Epoch: 0 [464/50000]	Loss: 4.9730
Training Epoch: 0 [480/50000]	Loss: 5.1585
Training Epoch: 0 [496/50000]	Loss: 4.9490
Training Epoch: 0 [512/50000]	Loss: 4.9059
Training Epoch: 0 [528/50000]	Loss: 4.6322
Training Epoch: 0 [544/50000]	Loss: 4.8081
Training Epoch: 0 [560/50000]	Loss: 4.5881
Training Epoch: 0 [576/50000]	Loss: 4.6746
Training Epoch: 0 [592/50000]	Loss: 5.0076
Training Epoch: 0 [608/50000]	Loss: 4.9967
Training Epoch: 0 [624/50000]	Loss: 5.6510
Training Epoch: 0 [640/50000]	Loss: 4.9326
Training Epoch: 0 [656/50000]	Loss: 5.1537
Training Epoch: 0 [672/50000]	Loss: 4.6228
Training Epoch: 0 [688/50000]	Loss: 4.9994
Training Epoch: 0 [704/50000]	Loss: 4.4605
Training Epoch: 0 [720/50000]	Loss: 4.9525
Training Epoch: 0 [736/50000]	Loss: 4.8776
Training Epoch: 0 [752/50000]	Loss: 4.9289
Training Epoch: 0 [768/50000]	Loss: 5.2801
Training Epoch: 0 [784/50000]	Loss: 4.8577
Training Epoch: 0 [800/50000]	Loss: 4.9313
Profile done with power limit 100W
epoch 1 train time consumed: 3.87s
Validation Epoch: 0, Average loss: 0.3020, Accuracy: 0.0126
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 100000, 'lr': 0.0011313708498984763, 'energy': 98.79022100671412, 'time': 2.748906297999838, 'accuracy': 0.0126, 'total_cost': 11.9464073469226}

[Power Profiler] with batch size 16 and learning rate 0.0014142135623730952
[run job] Launching job with BS 16: and LR: 0.0014142135623730952 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00141+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0014142135623730952', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0014142135623730952', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00141+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0014142135623730952
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6726
Training Epoch: 0 [48/50000]	Loss: 5.0945
Training Epoch: 0 [64/50000]	Loss: 5.2559
Training Epoch: 0 [80/50000]	Loss: 5.2064
Training Epoch: 0 [96/50000]	Loss: 5.4773
Training Epoch: 0 [112/50000]	Loss: 5.5667
Training Epoch: 0 [128/50000]	Loss: 5.7544
Training Epoch: 0 [144/50000]	Loss: 5.4134
Training Epoch: 0 [160/50000]	Loss: 5.6155
Training Epoch: 0 [176/50000]	Loss: 5.2647
Training Epoch: 0 [192/50000]	Loss: 5.0442
Training Epoch: 0 [208/50000]	Loss: 5.8185
Training Epoch: 0 [224/50000]	Loss: 5.1606
Training Epoch: 0 [240/50000]	Loss: 5.0516
Training Epoch: 0 [256/50000]	Loss: 5.3582
Training Epoch: 0 [272/50000]	Loss: 5.1485
Training Epoch: 0 [288/50000]	Loss: 5.5690
Training Epoch: 0 [304/50000]	Loss: 5.5868
Training Epoch: 0 [320/50000]	Loss: 5.0507
Training Epoch: 0 [336/50000]	Loss: 5.3814
Training Epoch: 0 [352/50000]	Loss: 4.7653
Training Epoch: 0 [368/50000]	Loss: 5.5968
Training Epoch: 0 [384/50000]	Loss: 5.1267
Training Epoch: 0 [400/50000]	Loss: 4.8200
Training Epoch: 0 [416/50000]	Loss: 4.8346
Training Epoch: 0 [432/50000]	Loss: 4.6073
Training Epoch: 0 [448/50000]	Loss: 5.1827
Training Epoch: 0 [464/50000]	Loss: 5.0818
Training Epoch: 0 [480/50000]	Loss: 5.3265
Training Epoch: 0 [496/50000]	Loss: 4.8559
Training Epoch: 0 [512/50000]	Loss: 4.9873
Training Epoch: 0 [528/50000]	Loss: 4.7669
Training Epoch: 0 [544/50000]	Loss: 4.8121
Training Epoch: 0 [560/50000]	Loss: 4.6109
Training Epoch: 0 [576/50000]	Loss: 4.6679
Training Epoch: 0 [592/50000]	Loss: 5.0592
Training Epoch: 0 [608/50000]	Loss: 4.9717
Training Epoch: 0 [624/50000]	Loss: 5.6939
Training Epoch: 0 [640/50000]	Loss: 4.9227
Training Epoch: 0 [656/50000]	Loss: 5.1763
Training Epoch: 0 [672/50000]	Loss: 4.6805
Training Epoch: 0 [688/50000]	Loss: 5.0148
Training Epoch: 0 [704/50000]	Loss: 4.5253
Training Epoch: 0 [720/50000]	Loss: 4.9884
Training Epoch: 0 [736/50000]	Loss: 4.9968
Training Epoch: 0 [752/50000]	Loss: 4.9467
Training Epoch: 0 [768/50000]	Loss: 5.2676
Training Epoch: 0 [784/50000]	Loss: 4.8165
Training Epoch: 0 [800/50000]	Loss: 4.9498
Profile done with power limit 175W
epoch 1 train time consumed: 3.56s
Validation Epoch: 0, Average loss: 0.3029, Accuracy: 0.0118
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 175000, 'lr': 0.0014142135623730952, 'energy': 115.31197773457963, 'time': 2.42680802100017, 'accuracy': 0.0118, 'total_cost': 11.94121078235086}
[run job] Launching job with BS 16: and LR: 0.0014142135623730952 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00141+pl150', 'ZEUS_COST_THRESH': '23.88242156470172', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0014142135623730952', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0014142135623730952', '--power_limit', '150']
[run job] cost_ub=23.88242156470172
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00141+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0014142135623730952
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6726
Training Epoch: 0 [48/50000]	Loss: 5.0936
Training Epoch: 0 [64/50000]	Loss: 5.2517
Training Epoch: 0 [80/50000]	Loss: 5.2394
Training Epoch: 0 [96/50000]	Loss: 5.4868
Training Epoch: 0 [112/50000]	Loss: 5.5671
Training Epoch: 0 [128/50000]	Loss: 5.7873
Training Epoch: 0 [144/50000]	Loss: 5.4070
Training Epoch: 0 [160/50000]	Loss: 5.6495
Training Epoch: 0 [176/50000]	Loss: 5.3276
Training Epoch: 0 [192/50000]	Loss: 5.0294
Training Epoch: 0 [208/50000]	Loss: 5.7747
Training Epoch: 0 [224/50000]	Loss: 5.0860
Training Epoch: 0 [240/50000]	Loss: 5.1277
Training Epoch: 0 [256/50000]	Loss: 5.2570
Training Epoch: 0 [272/50000]	Loss: 5.1088
Training Epoch: 0 [288/50000]	Loss: 5.6569
Training Epoch: 0 [304/50000]	Loss: 5.6352
Training Epoch: 0 [320/50000]	Loss: 5.0826
Training Epoch: 0 [336/50000]	Loss: 5.3802
Training Epoch: 0 [352/50000]	Loss: 4.8797
Training Epoch: 0 [368/50000]	Loss: 5.6916
Training Epoch: 0 [384/50000]	Loss: 4.9424
Training Epoch: 0 [400/50000]	Loss: 4.9434
Training Epoch: 0 [416/50000]	Loss: 4.8302
Training Epoch: 0 [432/50000]	Loss: 4.5397
Training Epoch: 0 [448/50000]	Loss: 5.1374
Training Epoch: 0 [464/50000]	Loss: 4.9670
Training Epoch: 0 [480/50000]	Loss: 5.3066
Training Epoch: 0 [496/50000]	Loss: 5.0416
Training Epoch: 0 [512/50000]	Loss: 5.0380
Training Epoch: 0 [528/50000]	Loss: 4.7766
Training Epoch: 0 [544/50000]	Loss: 4.9176
Training Epoch: 0 [560/50000]	Loss: 4.6558
Training Epoch: 0 [576/50000]	Loss: 4.6840
Training Epoch: 0 [592/50000]	Loss: 5.0062
Training Epoch: 0 [608/50000]	Loss: 4.9260
Training Epoch: 0 [624/50000]	Loss: 5.7071
Training Epoch: 0 [640/50000]	Loss: 4.9657
Training Epoch: 0 [656/50000]	Loss: 5.2433
Training Epoch: 0 [672/50000]	Loss: 4.6755
Training Epoch: 0 [688/50000]	Loss: 5.0212
Training Epoch: 0 [704/50000]	Loss: 4.5063
Training Epoch: 0 [720/50000]	Loss: 4.9513
Training Epoch: 0 [736/50000]	Loss: 4.8960
Training Epoch: 0 [752/50000]	Loss: 4.9636
Training Epoch: 0 [768/50000]	Loss: 5.2659
Training Epoch: 0 [784/50000]	Loss: 4.8112
Training Epoch: 0 [800/50000]	Loss: 5.0291
Profile done with power limit 150W
epoch 1 train time consumed: 3.50s
Validation Epoch: 0, Average loss: 0.3041, Accuracy: 0.0109
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 150000, 'lr': 0.0014142135623730952, 'energy': 115.69260051704765, 'time': 2.4260206649998963, 'accuracy': 0.0109, 'total_cost': 12.9399313030627}
[run job] Launching job with BS 16: and LR: 0.0014142135623730952 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00141+pl125', 'ZEUS_COST_THRESH': '23.88242156470172', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0014142135623730952', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0014142135623730952', '--power_limit', '125']
[run job] cost_ub=23.88242156470172
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00141+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0014142135623730952
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6726
Training Epoch: 0 [48/50000]	Loss: 5.0938
Training Epoch: 0 [64/50000]	Loss: 5.2503
Training Epoch: 0 [80/50000]	Loss: 5.2394
Training Epoch: 0 [96/50000]	Loss: 5.4499
Training Epoch: 0 [112/50000]	Loss: 5.5738
Training Epoch: 0 [128/50000]	Loss: 5.7755
Training Epoch: 0 [144/50000]	Loss: 5.4228
Training Epoch: 0 [160/50000]	Loss: 5.6542
Training Epoch: 0 [176/50000]	Loss: 5.3033
Training Epoch: 0 [192/50000]	Loss: 5.0913
Training Epoch: 0 [208/50000]	Loss: 5.6960
Training Epoch: 0 [224/50000]	Loss: 5.0969
Training Epoch: 0 [240/50000]	Loss: 5.0490
Training Epoch: 0 [256/50000]	Loss: 5.3094
Training Epoch: 0 [272/50000]	Loss: 5.1875
Training Epoch: 0 [288/50000]	Loss: 5.5344
Training Epoch: 0 [304/50000]	Loss: 5.6565
Training Epoch: 0 [320/50000]	Loss: 5.0379
Training Epoch: 0 [336/50000]	Loss: 5.4169
Training Epoch: 0 [352/50000]	Loss: 4.8608
Training Epoch: 0 [368/50000]	Loss: 5.7196
Training Epoch: 0 [384/50000]	Loss: 4.9357
Training Epoch: 0 [400/50000]	Loss: 4.9295
Training Epoch: 0 [416/50000]	Loss: 4.8061
Training Epoch: 0 [432/50000]	Loss: 4.5644
Training Epoch: 0 [448/50000]	Loss: 5.1637
Training Epoch: 0 [464/50000]	Loss: 4.9285
Training Epoch: 0 [480/50000]	Loss: 5.2844
Training Epoch: 0 [496/50000]	Loss: 4.9225
Training Epoch: 0 [512/50000]	Loss: 5.0096
Training Epoch: 0 [528/50000]	Loss: 4.7360
Training Epoch: 0 [544/50000]	Loss: 4.8813
Training Epoch: 0 [560/50000]	Loss: 4.6849
Training Epoch: 0 [576/50000]	Loss: 4.6196
Training Epoch: 0 [592/50000]	Loss: 5.0011
Training Epoch: 0 [608/50000]	Loss: 5.0606
Training Epoch: 0 [624/50000]	Loss: 5.7076
Training Epoch: 0 [640/50000]	Loss: 5.0596
Training Epoch: 0 [656/50000]	Loss: 5.1230
Training Epoch: 0 [672/50000]	Loss: 4.6992
Training Epoch: 0 [688/50000]	Loss: 4.9993
Training Epoch: 0 [704/50000]	Loss: 4.5100
Training Epoch: 0 [720/50000]	Loss: 5.0186
Training Epoch: 0 [736/50000]	Loss: 4.9747
Training Epoch: 0 [752/50000]	Loss: 4.9246
Training Epoch: 0 [768/50000]	Loss: 5.3902
Training Epoch: 0 [784/50000]	Loss: 4.8775
Training Epoch: 0 [800/50000]	Loss: 5.0563
Profile done with power limit 125W
epoch 1 train time consumed: 3.60s
Validation Epoch: 0, Average loss: 0.3021, Accuracy: 0.0123
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 125000, 'lr': 0.0014142135623730952, 'energy': 114.34343975165451, 'time': 2.4709513320001406, 'accuracy': 0.0123, 'total_cost': 11.625261103412246}
[run job] Launching job with BS 16: and LR: 0.0014142135623730952 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00141+pl100', 'ZEUS_COST_THRESH': '23.250522206824492', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0014142135623730952', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0014142135623730952', '--power_limit', '100']
[run job] cost_ub=23.250522206824492
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00141+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0014142135623730952
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6726
Training Epoch: 0 [48/50000]	Loss: 5.0944
Training Epoch: 0 [64/50000]	Loss: 5.2559
Training Epoch: 0 [80/50000]	Loss: 5.1959
Training Epoch: 0 [96/50000]	Loss: 5.4944
Training Epoch: 0 [112/50000]	Loss: 5.5776
Training Epoch: 0 [128/50000]	Loss: 5.7618
Training Epoch: 0 [144/50000]	Loss: 5.4318
Training Epoch: 0 [160/50000]	Loss: 5.6539
Training Epoch: 0 [176/50000]	Loss: 5.3118
Training Epoch: 0 [192/50000]	Loss: 5.0320
Training Epoch: 0 [208/50000]	Loss: 5.8152
Training Epoch: 0 [224/50000]	Loss: 5.1683
Training Epoch: 0 [240/50000]	Loss: 5.0412
Training Epoch: 0 [256/50000]	Loss: 5.3185
Training Epoch: 0 [272/50000]	Loss: 5.1966
Training Epoch: 0 [288/50000]	Loss: 5.5495
Training Epoch: 0 [304/50000]	Loss: 5.6253
Training Epoch: 0 [320/50000]	Loss: 5.0770
Training Epoch: 0 [336/50000]	Loss: 5.3861
Training Epoch: 0 [352/50000]	Loss: 4.8887
Training Epoch: 0 [368/50000]	Loss: 5.7068
Training Epoch: 0 [384/50000]	Loss: 5.0052
Training Epoch: 0 [400/50000]	Loss: 4.9169
Training Epoch: 0 [416/50000]	Loss: 4.9306
Training Epoch: 0 [432/50000]	Loss: 4.6308
Training Epoch: 0 [448/50000]	Loss: 5.1626
Training Epoch: 0 [464/50000]	Loss: 5.0361
Training Epoch: 0 [480/50000]	Loss: 5.2299
Training Epoch: 0 [496/50000]	Loss: 4.8767
Training Epoch: 0 [512/50000]	Loss: 5.0501
Training Epoch: 0 [528/50000]	Loss: 4.7283
Training Epoch: 0 [544/50000]	Loss: 4.8722
Training Epoch: 0 [560/50000]	Loss: 4.6487
Training Epoch: 0 [576/50000]	Loss: 4.6586
Training Epoch: 0 [592/50000]	Loss: 5.0047
Training Epoch: 0 [608/50000]	Loss: 4.9602
Training Epoch: 0 [624/50000]	Loss: 5.7664
Training Epoch: 0 [640/50000]	Loss: 4.9966
Training Epoch: 0 [656/50000]	Loss: 5.2390
Training Epoch: 0 [672/50000]	Loss: 4.7326
Training Epoch: 0 [688/50000]	Loss: 5.0498
Training Epoch: 0 [704/50000]	Loss: 4.5036
Training Epoch: 0 [720/50000]	Loss: 4.9802
Training Epoch: 0 [736/50000]	Loss: 4.9698
Training Epoch: 0 [752/50000]	Loss: 4.9576
Training Epoch: 0 [768/50000]	Loss: 5.2691
Training Epoch: 0 [784/50000]	Loss: 4.7636
Training Epoch: 0 [800/50000]	Loss: 4.9655
Profile done with power limit 100W
epoch 1 train time consumed: 4.22s
Validation Epoch: 0, Average loss: 0.3038, Accuracy: 0.0125
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 100000, 'lr': 0.0014142135623730952, 'energy': 98.70944011900627, 'time': 2.7478723839999475, 'accuracy': 0.0125, 'total_cost': 12.033897787889673}

[Power Profiler] with batch size 16 and learning rate 0.0016970562748477142
[run job] Launching job with BS 16: and LR: 0.0016970562748477142 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00170+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0016970562748477142', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016970562748477142', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00170+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0016970562748477142
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6893
Training Epoch: 0 [48/50000]	Loss: 5.2462
Training Epoch: 0 [64/50000]	Loss: 5.4410
Training Epoch: 0 [80/50000]	Loss: 5.3685
Training Epoch: 0 [96/50000]	Loss: 5.7124
Training Epoch: 0 [112/50000]	Loss: 5.7630
Training Epoch: 0 [128/50000]	Loss: 5.9740
Training Epoch: 0 [144/50000]	Loss: 5.5404
Training Epoch: 0 [160/50000]	Loss: 5.7926
Training Epoch: 0 [176/50000]	Loss: 5.3674
Training Epoch: 0 [192/50000]	Loss: 5.1130
Training Epoch: 0 [208/50000]	Loss: 5.8837
Training Epoch: 0 [224/50000]	Loss: 5.1907
Training Epoch: 0 [240/50000]	Loss: 5.1076
Training Epoch: 0 [256/50000]	Loss: 5.4149
Training Epoch: 0 [272/50000]	Loss: 5.3149
Training Epoch: 0 [288/50000]	Loss: 5.8384
Training Epoch: 0 [304/50000]	Loss: 5.8316
Training Epoch: 0 [320/50000]	Loss: 5.1272
Training Epoch: 0 [336/50000]	Loss: 5.4688
Training Epoch: 0 [352/50000]	Loss: 4.9223
Training Epoch: 0 [368/50000]	Loss: 5.8278
Training Epoch: 0 [384/50000]	Loss: 4.9928
Training Epoch: 0 [400/50000]	Loss: 5.0053
Training Epoch: 0 [416/50000]	Loss: 4.8411
Training Epoch: 0 [432/50000]	Loss: 4.5363
Training Epoch: 0 [448/50000]	Loss: 5.2673
Training Epoch: 0 [464/50000]	Loss: 4.9468
Training Epoch: 0 [480/50000]	Loss: 5.4098
Training Epoch: 0 [496/50000]	Loss: 5.0003
Training Epoch: 0 [512/50000]	Loss: 5.1383
Training Epoch: 0 [528/50000]	Loss: 4.8447
Training Epoch: 0 [544/50000]	Loss: 5.0230
Training Epoch: 0 [560/50000]	Loss: 4.7582
Training Epoch: 0 [576/50000]	Loss: 4.6019
Training Epoch: 0 [592/50000]	Loss: 5.0240
Training Epoch: 0 [608/50000]	Loss: 5.0165
Training Epoch: 0 [624/50000]	Loss: 5.7784
Training Epoch: 0 [640/50000]	Loss: 4.9890
Training Epoch: 0 [656/50000]	Loss: 5.2546
Training Epoch: 0 [672/50000]	Loss: 4.6860
Training Epoch: 0 [688/50000]	Loss: 5.0703
Training Epoch: 0 [704/50000]	Loss: 4.5654
Training Epoch: 0 [720/50000]	Loss: 5.0540
Training Epoch: 0 [736/50000]	Loss: 5.1329
Training Epoch: 0 [752/50000]	Loss: 4.9946
Training Epoch: 0 [768/50000]	Loss: 5.4605
Training Epoch: 0 [784/50000]	Loss: 4.8873
Training Epoch: 0 [800/50000]	Loss: 5.0933
Profile done with power limit 175W
epoch 1 train time consumed: 3.46s
Validation Epoch: 0, Average loss: 0.3029, Accuracy: 0.0127
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 175000, 'lr': 0.0016970562748477142, 'energy': 116.39662378160118, 'time': 2.3688248989999465, 'accuracy': 0.0127, 'total_cost': 10.870355557454754}
[run job] Launching job with BS 16: and LR: 0.0016970562748477142 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00170+pl150', 'ZEUS_COST_THRESH': '21.74071111490951', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0016970562748477142', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016970562748477142', '--power_limit', '150']
[run job] cost_ub=21.74071111490951
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00170+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0016970562748477142
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6893
Training Epoch: 0 [48/50000]	Loss: 5.2458
Training Epoch: 0 [64/50000]	Loss: 5.4561
Training Epoch: 0 [80/50000]	Loss: 5.3768
Training Epoch: 0 [96/50000]	Loss: 5.7674
Training Epoch: 0 [112/50000]	Loss: 5.7968
Training Epoch: 0 [128/50000]	Loss: 5.9863
Training Epoch: 0 [144/50000]	Loss: 5.5020
Training Epoch: 0 [160/50000]	Loss: 5.7478
Training Epoch: 0 [176/50000]	Loss: 5.4268
Training Epoch: 0 [192/50000]	Loss: 5.2125
Training Epoch: 0 [208/50000]	Loss: 5.9026
Training Epoch: 0 [224/50000]	Loss: 5.2454
Training Epoch: 0 [240/50000]	Loss: 5.1351
Training Epoch: 0 [256/50000]	Loss: 5.4918
Training Epoch: 0 [272/50000]	Loss: 5.3213
Training Epoch: 0 [288/50000]	Loss: 5.7285
Training Epoch: 0 [304/50000]	Loss: 5.6680
Training Epoch: 0 [320/50000]	Loss: 5.0368
Training Epoch: 0 [336/50000]	Loss: 5.4072
Training Epoch: 0 [352/50000]	Loss: 5.0050
Training Epoch: 0 [368/50000]	Loss: 5.7246
Training Epoch: 0 [384/50000]	Loss: 4.9207
Training Epoch: 0 [400/50000]	Loss: 4.9291
Training Epoch: 0 [416/50000]	Loss: 4.8827
Training Epoch: 0 [432/50000]	Loss: 4.6121
Training Epoch: 0 [448/50000]	Loss: 5.2332
Training Epoch: 0 [464/50000]	Loss: 4.9708
Training Epoch: 0 [480/50000]	Loss: 5.4851
Training Epoch: 0 [496/50000]	Loss: 4.9460
Training Epoch: 0 [512/50000]	Loss: 5.1898
Training Epoch: 0 [528/50000]	Loss: 4.9311
Training Epoch: 0 [544/50000]	Loss: 4.9830
Training Epoch: 0 [560/50000]	Loss: 4.7655
Training Epoch: 0 [576/50000]	Loss: 4.6207
Training Epoch: 0 [592/50000]	Loss: 5.0487
Training Epoch: 0 [608/50000]	Loss: 5.0384
Training Epoch: 0 [624/50000]	Loss: 5.7949
Training Epoch: 0 [640/50000]	Loss: 5.0708
Training Epoch: 0 [656/50000]	Loss: 5.2282
Training Epoch: 0 [672/50000]	Loss: 4.6876
Training Epoch: 0 [688/50000]	Loss: 5.0360
Training Epoch: 0 [704/50000]	Loss: 4.5840
Training Epoch: 0 [720/50000]	Loss: 4.9982
Training Epoch: 0 [736/50000]	Loss: 5.0352
Training Epoch: 0 [752/50000]	Loss: 5.0022
Training Epoch: 0 [768/50000]	Loss: 5.3399
Training Epoch: 0 [784/50000]	Loss: 4.8871
Training Epoch: 0 [800/50000]	Loss: 5.1662
Profile done with power limit 150W
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.3057, Accuracy: 0.0150
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 150000, 'lr': 0.0016970562748477142, 'energy': 114.53153742968385, 'time': 2.4240193299999646, 'accuracy': 0.015, 'total_cost': 9.357733911655492}
[run job] Launching job with BS 16: and LR: 0.0016970562748477142 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00170+pl125', 'ZEUS_COST_THRESH': '18.715467823310984', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0016970562748477142', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016970562748477142', '--power_limit', '125']
[run job] cost_ub=18.715467823310984
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00170+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0016970562748477142
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6893
Training Epoch: 0 [48/50000]	Loss: 5.2466
Training Epoch: 0 [64/50000]	Loss: 5.4491
Training Epoch: 0 [80/50000]	Loss: 5.3885
Training Epoch: 0 [96/50000]	Loss: 5.7406
Training Epoch: 0 [112/50000]	Loss: 5.7712
Training Epoch: 0 [128/50000]	Loss: 5.9962
Training Epoch: 0 [144/50000]	Loss: 5.5006
Training Epoch: 0 [160/50000]	Loss: 5.7635
Training Epoch: 0 [176/50000]	Loss: 5.3877
Training Epoch: 0 [192/50000]	Loss: 5.1369
Training Epoch: 0 [208/50000]	Loss: 5.9636
Training Epoch: 0 [224/50000]	Loss: 5.1964
Training Epoch: 0 [240/50000]	Loss: 5.1330
Training Epoch: 0 [256/50000]	Loss: 5.3904
Training Epoch: 0 [272/50000]	Loss: 5.2407
Training Epoch: 0 [288/50000]	Loss: 5.8686
Training Epoch: 0 [304/50000]	Loss: 5.7848
Training Epoch: 0 [320/50000]	Loss: 5.0845
Training Epoch: 0 [336/50000]	Loss: 5.3693
Training Epoch: 0 [352/50000]	Loss: 4.9423
Training Epoch: 0 [368/50000]	Loss: 5.7560
Training Epoch: 0 [384/50000]	Loss: 4.9622
Training Epoch: 0 [400/50000]	Loss: 4.8648
Training Epoch: 0 [416/50000]	Loss: 5.0327
Training Epoch: 0 [432/50000]	Loss: 4.4984
Training Epoch: 0 [448/50000]	Loss: 5.2149
Training Epoch: 0 [464/50000]	Loss: 4.8931
Training Epoch: 0 [480/50000]	Loss: 5.5291
Training Epoch: 0 [496/50000]	Loss: 4.6744
Training Epoch: 0 [512/50000]	Loss: 5.1745
Training Epoch: 0 [528/50000]	Loss: 4.9721
Training Epoch: 0 [544/50000]	Loss: 5.1300
Training Epoch: 0 [560/50000]	Loss: 4.7157
Training Epoch: 0 [576/50000]	Loss: 4.6138
Training Epoch: 0 [592/50000]	Loss: 5.1192
Training Epoch: 0 [608/50000]	Loss: 5.0645
Training Epoch: 0 [624/50000]	Loss: 5.8352
Training Epoch: 0 [640/50000]	Loss: 4.9696
Training Epoch: 0 [656/50000]	Loss: 5.1342
Training Epoch: 0 [672/50000]	Loss: 4.6968
Training Epoch: 0 [688/50000]	Loss: 4.9927
Training Epoch: 0 [704/50000]	Loss: 4.7615
Training Epoch: 0 [720/50000]	Loss: 4.9973
Training Epoch: 0 [736/50000]	Loss: 4.9274
Training Epoch: 0 [752/50000]	Loss: 4.9641
Training Epoch: 0 [768/50000]	Loss: 5.1654
Training Epoch: 0 [784/50000]	Loss: 4.9279
Training Epoch: 0 [800/50000]	Loss: 5.0058
Profile done with power limit 125W
epoch 1 train time consumed: 3.52s
Validation Epoch: 0, Average loss: 0.3192, Accuracy: 0.0133
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 125000, 'lr': 0.0016970562748477142, 'energy': 115.64280038288454, 'time': 2.4238679730001422, 'accuracy': 0.0133, 'total_cost': 10.59368083355109}
[run job] Launching job with BS 16: and LR: 0.0016970562748477142 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs16+lr0.00170+pl100', 'ZEUS_COST_THRESH': '18.715467823310984', 'ZEUS_BATCH_SIZE': '16', 'ZEUS_LEARNING_RATE': '0.0016970562748477142', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '16', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016970562748477142', '--power_limit', '100']
[run job] cost_ub=18.715467823310984
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs16+lr0.00170+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0016970562748477142
batch size arg: 16
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [16/50000]	Loss: 4.6697
Training Epoch: 0 [32/50000]	Loss: 4.6893
Training Epoch: 0 [48/50000]	Loss: 5.2466
Training Epoch: 0 [64/50000]	Loss: 5.4450
Training Epoch: 0 [80/50000]	Loss: 5.3662
Training Epoch: 0 [96/50000]	Loss: 5.7314
Training Epoch: 0 [112/50000]	Loss: 5.7463
Training Epoch: 0 [128/50000]	Loss: 5.9918
Training Epoch: 0 [144/50000]	Loss: 5.5426
Training Epoch: 0 [160/50000]	Loss: 5.8252
Training Epoch: 0 [176/50000]	Loss: 5.3608
Training Epoch: 0 [192/50000]	Loss: 5.1265
Training Epoch: 0 [208/50000]	Loss: 5.9064
Training Epoch: 0 [224/50000]	Loss: 5.2431
Training Epoch: 0 [240/50000]	Loss: 5.1598
Training Epoch: 0 [256/50000]	Loss: 5.4341
Training Epoch: 0 [272/50000]	Loss: 5.3447
Training Epoch: 0 [288/50000]	Loss: 5.7734
Training Epoch: 0 [304/50000]	Loss: 5.7641
Training Epoch: 0 [320/50000]	Loss: 5.0670
Training Epoch: 0 [336/50000]	Loss: 5.4060
Training Epoch: 0 [352/50000]	Loss: 4.9693
Training Epoch: 0 [368/50000]	Loss: 5.7317
Training Epoch: 0 [384/50000]	Loss: 4.9784
Training Epoch: 0 [400/50000]	Loss: 4.9153
Training Epoch: 0 [416/50000]	Loss: 4.8617
Training Epoch: 0 [432/50000]	Loss: 4.4880
Training Epoch: 0 [448/50000]	Loss: 5.2821
Training Epoch: 0 [464/50000]	Loss: 4.9657
Training Epoch: 0 [480/50000]	Loss: 5.3732
Training Epoch: 0 [496/50000]	Loss: 4.7365
Training Epoch: 0 [512/50000]	Loss: 5.2292
Training Epoch: 0 [528/50000]	Loss: 4.7825
Training Epoch: 0 [544/50000]	Loss: 5.1241
Training Epoch: 0 [560/50000]	Loss: 4.6674
Training Epoch: 0 [576/50000]	Loss: 4.5101
Training Epoch: 0 [592/50000]	Loss: 5.0620
Training Epoch: 0 [608/50000]	Loss: 5.1776
Training Epoch: 0 [624/50000]	Loss: 5.8059
Training Epoch: 0 [640/50000]	Loss: 5.0619
Training Epoch: 0 [656/50000]	Loss: 5.2777
Training Epoch: 0 [672/50000]	Loss: 4.7415
Training Epoch: 0 [688/50000]	Loss: 5.1721
Training Epoch: 0 [704/50000]	Loss: 4.6206
Training Epoch: 0 [720/50000]	Loss: 5.0042
Training Epoch: 0 [736/50000]	Loss: 5.0602
Training Epoch: 0 [752/50000]	Loss: 4.9190
Training Epoch: 0 [768/50000]	Loss: 5.1965
Training Epoch: 0 [784/50000]	Loss: 4.7968
Training Epoch: 0 [800/50000]	Loss: 5.0323
Profile done with power limit 100W
epoch 1 train time consumed: 3.89s
Validation Epoch: 0, Average loss: 0.3095, Accuracy: 0.0164
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 16, 'pl': 100000, 'lr': 0.0016970562748477142, 'energy': 98.9862196523038, 'time': 2.7549078560000453, 'accuracy': 0.0164, 'total_cost': 9.204960840925436}

[Power Profiler] with batch size 32 and learning rate 0.0016
[run job] Launching job with BS 32: and LR: 0.0016 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00160+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0016', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00160+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0016
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.8917
Training Epoch: 0 [96/50016]	Loss: 4.9878
Training Epoch: 0 [128/50016]	Loss: 5.1889
Training Epoch: 0 [160/50016]	Loss: 5.3137
Training Epoch: 0 [192/50016]	Loss: 5.1139
Training Epoch: 0 [224/50016]	Loss: 5.1166
Training Epoch: 0 [256/50016]	Loss: 5.1531
Training Epoch: 0 [288/50016]	Loss: 5.0965
Training Epoch: 0 [320/50016]	Loss: 5.1575
Training Epoch: 0 [352/50016]	Loss: 4.9281
Training Epoch: 0 [384/50016]	Loss: 5.3223
Training Epoch: 0 [416/50016]	Loss: 5.0031
Training Epoch: 0 [448/50016]	Loss: 4.8090
Training Epoch: 0 [480/50016]	Loss: 5.0919
Training Epoch: 0 [512/50016]	Loss: 5.0053
Training Epoch: 0 [544/50016]	Loss: 4.7828
Training Epoch: 0 [576/50016]	Loss: 4.6566
Training Epoch: 0 [608/50016]	Loss: 5.1163
Training Epoch: 0 [640/50016]	Loss: 5.4387
Training Epoch: 0 [672/50016]	Loss: 4.9345
Training Epoch: 0 [704/50016]	Loss: 4.6731
Training Epoch: 0 [736/50016]	Loss: 5.0147
Training Epoch: 0 [768/50016]	Loss: 5.3186
Training Epoch: 0 [800/50016]	Loss: 4.9096
Training Epoch: 0 [832/50016]	Loss: 5.0682
Training Epoch: 0 [864/50016]	Loss: 5.2146
Training Epoch: 0 [896/50016]	Loss: 4.5515
Training Epoch: 0 [928/50016]	Loss: 4.7863
Training Epoch: 0 [960/50016]	Loss: 4.7654
Training Epoch: 0 [992/50016]	Loss: 4.8209
Training Epoch: 0 [1024/50016]	Loss: 4.7407
Training Epoch: 0 [1056/50016]	Loss: 4.9297
Training Epoch: 0 [1088/50016]	Loss: 4.7094
Training Epoch: 0 [1120/50016]	Loss: 4.9214
Training Epoch: 0 [1152/50016]	Loss: 4.6549
Training Epoch: 0 [1184/50016]	Loss: 4.7584
Training Epoch: 0 [1216/50016]	Loss: 4.5884
Training Epoch: 0 [1248/50016]	Loss: 4.7898
Training Epoch: 0 [1280/50016]	Loss: 4.7331
Training Epoch: 0 [1312/50016]	Loss: 4.7095
Training Epoch: 0 [1344/50016]	Loss: 4.7501
Training Epoch: 0 [1376/50016]	Loss: 4.6315
Training Epoch: 0 [1408/50016]	Loss: 4.6270
Training Epoch: 0 [1440/50016]	Loss: 4.8664
Training Epoch: 0 [1472/50016]	Loss: 4.4064
Training Epoch: 0 [1504/50016]	Loss: 4.7226
Training Epoch: 0 [1536/50016]	Loss: 4.6786
Training Epoch: 0 [1568/50016]	Loss: 4.6361
Training Epoch: 0 [1600/50016]	Loss: 5.1153
Profile done with power limit 175W
epoch 1 train time consumed: 4.65s
Validation Epoch: 0, Average loss: 0.2205, Accuracy: 0.0207
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 175000, 'lr': 0.0016, 'energy': 129.40896109734103, 'time': 3.292198983999924, 'accuracy': 0.020666932907348244, 'total_cost': 19.390478182074503}
[run job] Launching job with BS 32: and LR: 0.0016 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00160+pl150', 'ZEUS_COST_THRESH': '38.780956364149006', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0016', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016', '--power_limit', '150']
[run job] cost_ub=38.780956364149006
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00160+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0016
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.8917
Training Epoch: 0 [96/50016]	Loss: 4.9884
Training Epoch: 0 [128/50016]	Loss: 5.1701
Training Epoch: 0 [160/50016]	Loss: 5.3075
Training Epoch: 0 [192/50016]	Loss: 5.1110
Training Epoch: 0 [224/50016]	Loss: 5.1385
Training Epoch: 0 [256/50016]	Loss: 5.1626
Training Epoch: 0 [288/50016]	Loss: 4.9988
Training Epoch: 0 [320/50016]	Loss: 5.1438
Training Epoch: 0 [352/50016]	Loss: 4.9114
Training Epoch: 0 [384/50016]	Loss: 5.3485
Training Epoch: 0 [416/50016]	Loss: 5.0124
Training Epoch: 0 [448/50016]	Loss: 4.8526
Training Epoch: 0 [480/50016]	Loss: 5.1050
Training Epoch: 0 [512/50016]	Loss: 4.9913
Training Epoch: 0 [544/50016]	Loss: 4.8082
Training Epoch: 0 [576/50016]	Loss: 4.6601
Training Epoch: 0 [608/50016]	Loss: 5.0931
Training Epoch: 0 [640/50016]	Loss: 5.3790
Training Epoch: 0 [672/50016]	Loss: 4.8876
Training Epoch: 0 [704/50016]	Loss: 4.6623
Training Epoch: 0 [736/50016]	Loss: 5.0004
Training Epoch: 0 [768/50016]	Loss: 5.3674
Training Epoch: 0 [800/50016]	Loss: 4.9427
Training Epoch: 0 [832/50016]	Loss: 5.1034
Training Epoch: 0 [864/50016]	Loss: 5.1758
Training Epoch: 0 [896/50016]	Loss: 4.5820
Training Epoch: 0 [928/50016]	Loss: 4.8547
Training Epoch: 0 [960/50016]	Loss: 4.7659
Training Epoch: 0 [992/50016]	Loss: 4.7790
Training Epoch: 0 [1024/50016]	Loss: 4.6934
Training Epoch: 0 [1056/50016]	Loss: 4.9484
Training Epoch: 0 [1088/50016]	Loss: 4.7401
Training Epoch: 0 [1120/50016]	Loss: 4.8889
Training Epoch: 0 [1152/50016]	Loss: 4.7011
Training Epoch: 0 [1184/50016]	Loss: 4.7874
Training Epoch: 0 [1216/50016]	Loss: 4.7803
Training Epoch: 0 [1248/50016]	Loss: 4.8190
Training Epoch: 0 [1280/50016]	Loss: 4.7608
Training Epoch: 0 [1312/50016]	Loss: 4.8454
Training Epoch: 0 [1344/50016]	Loss: 4.7485
Training Epoch: 0 [1376/50016]	Loss: 4.7059
Training Epoch: 0 [1408/50016]	Loss: 4.6792
Training Epoch: 0 [1440/50016]	Loss: 4.8416
Training Epoch: 0 [1472/50016]	Loss: 4.4208
Training Epoch: 0 [1504/50016]	Loss: 4.8682
Training Epoch: 0 [1536/50016]	Loss: 4.7274
Training Epoch: 0 [1568/50016]	Loss: 4.8599
Training Epoch: 0 [1600/50016]	Loss: 5.2630
Profile done with power limit 150W
epoch 1 train time consumed: 4.54s
Validation Epoch: 0, Average loss: 0.1508, Accuracy: 0.0140
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 150000, 'lr': 0.0016, 'energy': 131.01049105045487, 'time': 3.193597031000081, 'accuracy': 0.013977635782747603, 'total_cost': 27.95784875658896}
[run job] Launching job with BS 32: and LR: 0.0016 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00160+pl125', 'ZEUS_COST_THRESH': '38.780956364149006', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0016', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016', '--power_limit', '125']
[run job] cost_ub=38.780956364149006
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00160+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0016
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.8917
Training Epoch: 0 [96/50016]	Loss: 4.9883
Training Epoch: 0 [128/50016]	Loss: 5.1700
Training Epoch: 0 [160/50016]	Loss: 5.2886
Training Epoch: 0 [192/50016]	Loss: 5.1144
Training Epoch: 0 [224/50016]	Loss: 5.1191
Training Epoch: 0 [256/50016]	Loss: 5.1473
Training Epoch: 0 [288/50016]	Loss: 5.0836
Training Epoch: 0 [320/50016]	Loss: 5.1347
Training Epoch: 0 [352/50016]	Loss: 4.9447
Training Epoch: 0 [384/50016]	Loss: 5.3759
Training Epoch: 0 [416/50016]	Loss: 4.9459
Training Epoch: 0 [448/50016]	Loss: 4.8742
Training Epoch: 0 [480/50016]	Loss: 5.1040
Training Epoch: 0 [512/50016]	Loss: 4.9657
Training Epoch: 0 [544/50016]	Loss: 4.8034
Training Epoch: 0 [576/50016]	Loss: 4.6851
Training Epoch: 0 [608/50016]	Loss: 5.1222
Training Epoch: 0 [640/50016]	Loss: 5.4076
Training Epoch: 0 [672/50016]	Loss: 4.9488
Training Epoch: 0 [704/50016]	Loss: 4.6926
Training Epoch: 0 [736/50016]	Loss: 5.0057
Training Epoch: 0 [768/50016]	Loss: 5.2884
Training Epoch: 0 [800/50016]	Loss: 4.8271
Training Epoch: 0 [832/50016]	Loss: 5.1027
Training Epoch: 0 [864/50016]	Loss: 5.2184
Training Epoch: 0 [896/50016]	Loss: 4.5501
Training Epoch: 0 [928/50016]	Loss: 4.7829
Training Epoch: 0 [960/50016]	Loss: 4.6946
Training Epoch: 0 [992/50016]	Loss: 4.8320
Training Epoch: 0 [1024/50016]	Loss: 4.7402
Training Epoch: 0 [1056/50016]	Loss: 4.9586
Training Epoch: 0 [1088/50016]	Loss: 4.7517
Training Epoch: 0 [1120/50016]	Loss: 4.9944
Training Epoch: 0 [1152/50016]	Loss: 4.6960
Training Epoch: 0 [1184/50016]	Loss: 4.7068
Training Epoch: 0 [1216/50016]	Loss: 4.7067
Training Epoch: 0 [1248/50016]	Loss: 4.8729
Training Epoch: 0 [1280/50016]	Loss: 4.6915
Training Epoch: 0 [1312/50016]	Loss: 4.8479
Training Epoch: 0 [1344/50016]	Loss: 4.6884
Training Epoch: 0 [1376/50016]	Loss: 4.7511
Training Epoch: 0 [1408/50016]	Loss: 4.6563
Training Epoch: 0 [1440/50016]	Loss: 4.9598
Training Epoch: 0 [1472/50016]	Loss: 4.3988
Training Epoch: 0 [1504/50016]	Loss: 4.8787
Training Epoch: 0 [1536/50016]	Loss: 4.7197
Training Epoch: 0 [1568/50016]	Loss: 4.8450
Training Epoch: 0 [1600/50016]	Loss: 5.1796
Profile done with power limit 125W
epoch 1 train time consumed: 4.79s
Validation Epoch: 0, Average loss: 0.1541, Accuracy: 0.0184
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 125000, 'lr': 0.0016, 'energy': 123.08962782683443, 'time': 3.4081493000001046, 'accuracy': 0.018370607028753993, 'total_cost': 22.113781214915182}
[run job] Launching job with BS 32: and LR: 0.0016 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00160+pl100', 'ZEUS_COST_THRESH': '38.780956364149006', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0016', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0016', '--power_limit', '100']
[run job] cost_ub=38.780956364149006
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00160+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0016
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 4.8917
Training Epoch: 0 [96/50016]	Loss: 4.9879
Training Epoch: 0 [128/50016]	Loss: 5.1722
Training Epoch: 0 [160/50016]	Loss: 5.3188
Training Epoch: 0 [192/50016]	Loss: 5.1263
Training Epoch: 0 [224/50016]	Loss: 5.1164
Training Epoch: 0 [256/50016]	Loss: 5.1326
Training Epoch: 0 [288/50016]	Loss: 5.0490
Training Epoch: 0 [320/50016]	Loss: 5.1474
Training Epoch: 0 [352/50016]	Loss: 4.9450
Training Epoch: 0 [384/50016]	Loss: 5.3307
Training Epoch: 0 [416/50016]	Loss: 4.9540
Training Epoch: 0 [448/50016]	Loss: 4.8408
Training Epoch: 0 [480/50016]	Loss: 5.1047
Training Epoch: 0 [512/50016]	Loss: 5.0020
Training Epoch: 0 [544/50016]	Loss: 4.8076
Training Epoch: 0 [576/50016]	Loss: 4.6916
Training Epoch: 0 [608/50016]	Loss: 5.0728
Training Epoch: 0 [640/50016]	Loss: 5.4562
Training Epoch: 0 [672/50016]	Loss: 4.8794
Training Epoch: 0 [704/50016]	Loss: 4.6860
Training Epoch: 0 [736/50016]	Loss: 5.0181
Training Epoch: 0 [768/50016]	Loss: 5.2887
Training Epoch: 0 [800/50016]	Loss: 4.9074
Training Epoch: 0 [832/50016]	Loss: 5.0506
Training Epoch: 0 [864/50016]	Loss: 5.2297
Training Epoch: 0 [896/50016]	Loss: 4.5601
Training Epoch: 0 [928/50016]	Loss: 4.8578
Training Epoch: 0 [960/50016]	Loss: 4.7246
Training Epoch: 0 [992/50016]	Loss: 4.8618
Training Epoch: 0 [1024/50016]	Loss: 4.7877
Training Epoch: 0 [1056/50016]	Loss: 5.0190
Training Epoch: 0 [1088/50016]	Loss: 4.6632
Training Epoch: 0 [1120/50016]	Loss: 4.8930
Training Epoch: 0 [1152/50016]	Loss: 4.7047
Training Epoch: 0 [1184/50016]	Loss: 4.7530
Training Epoch: 0 [1216/50016]	Loss: 4.6947
Training Epoch: 0 [1248/50016]	Loss: 4.8331
Training Epoch: 0 [1280/50016]	Loss: 4.6864
Training Epoch: 0 [1312/50016]	Loss: 4.8745
Training Epoch: 0 [1344/50016]	Loss: 4.7487
Training Epoch: 0 [1376/50016]	Loss: 4.7535
Training Epoch: 0 [1408/50016]	Loss: 4.6532
Training Epoch: 0 [1440/50016]	Loss: 4.8330
Training Epoch: 0 [1472/50016]	Loss: 4.4900
Training Epoch: 0 [1504/50016]	Loss: 4.7465
Training Epoch: 0 [1536/50016]	Loss: 4.7214
Training Epoch: 0 [1568/50016]	Loss: 4.8761
Training Epoch: 0 [1600/50016]	Loss: 5.1831
Profile done with power limit 100W
epoch 1 train time consumed: 5.59s
Validation Epoch: 0, Average loss: 0.1525, Accuracy: 0.0135
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 100000, 'lr': 0.0016, 'energy': 99.36257958973064, 'time': 4.11309332299993, 'accuracy': 0.013478434504792332, 'total_cost': 33.47919659552689}

[Power Profiler] with batch size 32 and learning rate 0.002
[run job] Launching job with BS 32: and LR: 0.002 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00200+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.002', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.002', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00200+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.002
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.0068
Training Epoch: 0 [96/50016]	Loss: 5.1686
Training Epoch: 0 [128/50016]	Loss: 5.4293
Training Epoch: 0 [160/50016]	Loss: 5.5271
Training Epoch: 0 [192/50016]	Loss: 5.2496
Training Epoch: 0 [224/50016]	Loss: 5.2564
Training Epoch: 0 [256/50016]	Loss: 5.1980
Training Epoch: 0 [288/50016]	Loss: 5.1891
Training Epoch: 0 [320/50016]	Loss: 5.2334
Training Epoch: 0 [352/50016]	Loss: 4.9989
Training Epoch: 0 [384/50016]	Loss: 5.5429
Training Epoch: 0 [416/50016]	Loss: 5.0344
Training Epoch: 0 [448/50016]	Loss: 4.9246
Training Epoch: 0 [480/50016]	Loss: 5.1438
Training Epoch: 0 [512/50016]	Loss: 4.9865
Training Epoch: 0 [544/50016]	Loss: 4.9409
Training Epoch: 0 [576/50016]	Loss: 4.6547
Training Epoch: 0 [608/50016]	Loss: 5.1505
Training Epoch: 0 [640/50016]	Loss: 5.5923
Training Epoch: 0 [672/50016]	Loss: 4.9233
Training Epoch: 0 [704/50016]	Loss: 4.7344
Training Epoch: 0 [736/50016]	Loss: 5.1208
Training Epoch: 0 [768/50016]	Loss: 5.3022
Training Epoch: 0 [800/50016]	Loss: 4.9506
Training Epoch: 0 [832/50016]	Loss: 5.0747
Training Epoch: 0 [864/50016]	Loss: 5.3271
Training Epoch: 0 [896/50016]	Loss: 4.6218
Training Epoch: 0 [928/50016]	Loss: 4.7555
Training Epoch: 0 [960/50016]	Loss: 4.7807
Training Epoch: 0 [992/50016]	Loss: 4.7947
Training Epoch: 0 [1024/50016]	Loss: 4.7360
Training Epoch: 0 [1056/50016]	Loss: 4.9015
Training Epoch: 0 [1088/50016]	Loss: 4.6506
Training Epoch: 0 [1120/50016]	Loss: 4.8271
Training Epoch: 0 [1152/50016]	Loss: 4.6279
Training Epoch: 0 [1184/50016]	Loss: 4.5948
Training Epoch: 0 [1216/50016]	Loss: 4.5562
Training Epoch: 0 [1248/50016]	Loss: 4.6519
Training Epoch: 0 [1280/50016]	Loss: 4.5203
Training Epoch: 0 [1312/50016]	Loss: 4.5901
Training Epoch: 0 [1344/50016]	Loss: 4.5929
Training Epoch: 0 [1376/50016]	Loss: 4.5646
Training Epoch: 0 [1408/50016]	Loss: 4.6016
Training Epoch: 0 [1440/50016]	Loss: 4.8387
Training Epoch: 0 [1472/50016]	Loss: 4.3047
Training Epoch: 0 [1504/50016]	Loss: 4.7094
Training Epoch: 0 [1536/50016]	Loss: 4.5433
Training Epoch: 0 [1568/50016]	Loss: 4.6292
Training Epoch: 0 [1600/50016]	Loss: 4.9359
Profile done with power limit 175W
epoch 1 train time consumed: 4.63s
Validation Epoch: 0, Average loss: 0.2726, Accuracy: 0.0254
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 175000, 'lr': 0.002, 'energy': 130.17930452274314, 'time': 3.25805347000005, 'accuracy': 0.025359424920127795, 'total_cost': 15.678153964901956}
[run job] Launching job with BS 32: and LR: 0.002 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00200+pl150', 'ZEUS_COST_THRESH': '31.35630792980391', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.002', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.002', '--power_limit', '150']
[run job] cost_ub=31.35630792980391
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00200+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.002
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.0068
Training Epoch: 0 [96/50016]	Loss: 5.1681
Training Epoch: 0 [128/50016]	Loss: 5.4385
Training Epoch: 0 [160/50016]	Loss: 5.5293
Training Epoch: 0 [192/50016]	Loss: 5.2527
Training Epoch: 0 [224/50016]	Loss: 5.2706
Training Epoch: 0 [256/50016]	Loss: 5.1861
Training Epoch: 0 [288/50016]	Loss: 5.2364
Training Epoch: 0 [320/50016]	Loss: 5.2889
Training Epoch: 0 [352/50016]	Loss: 5.0362
Training Epoch: 0 [384/50016]	Loss: 5.5551
Training Epoch: 0 [416/50016]	Loss: 5.0701
Training Epoch: 0 [448/50016]	Loss: 4.9132
Training Epoch: 0 [480/50016]	Loss: 5.1797
Training Epoch: 0 [512/50016]	Loss: 4.9781
Training Epoch: 0 [544/50016]	Loss: 4.9242
Training Epoch: 0 [576/50016]	Loss: 4.7032
Training Epoch: 0 [608/50016]	Loss: 5.1425
Training Epoch: 0 [640/50016]	Loss: 5.5568
Training Epoch: 0 [672/50016]	Loss: 5.0273
Training Epoch: 0 [704/50016]	Loss: 4.7708
Training Epoch: 0 [736/50016]	Loss: 5.1010
Training Epoch: 0 [768/50016]	Loss: 5.3244
Training Epoch: 0 [800/50016]	Loss: 4.9288
Training Epoch: 0 [832/50016]	Loss: 5.1188
Training Epoch: 0 [864/50016]	Loss: 5.3140
Training Epoch: 0 [896/50016]	Loss: 4.7052
Training Epoch: 0 [928/50016]	Loss: 4.7872
Training Epoch: 0 [960/50016]	Loss: 4.6564
Training Epoch: 0 [992/50016]	Loss: 4.7132
Training Epoch: 0 [1024/50016]	Loss: 4.6989
Training Epoch: 0 [1056/50016]	Loss: 4.8665
Training Epoch: 0 [1088/50016]	Loss: 4.6948
Training Epoch: 0 [1120/50016]	Loss: 4.9031
Training Epoch: 0 [1152/50016]	Loss: 4.6402
Training Epoch: 0 [1184/50016]	Loss: 4.5655
Training Epoch: 0 [1216/50016]	Loss: 4.4634
Training Epoch: 0 [1248/50016]	Loss: 4.7408
Training Epoch: 0 [1280/50016]	Loss: 4.6029
Training Epoch: 0 [1312/50016]	Loss: 4.5858
Training Epoch: 0 [1344/50016]	Loss: 4.7170
Training Epoch: 0 [1376/50016]	Loss: 4.6767
Training Epoch: 0 [1408/50016]	Loss: 4.5695
Training Epoch: 0 [1440/50016]	Loss: 4.7634
Training Epoch: 0 [1472/50016]	Loss: 4.2950
Training Epoch: 0 [1504/50016]	Loss: 4.7983
Training Epoch: 0 [1536/50016]	Loss: 4.4533
Training Epoch: 0 [1568/50016]	Loss: 4.8537
Training Epoch: 0 [1600/50016]	Loss: 4.9077
Profile done with power limit 150W
epoch 1 train time consumed: 4.64s
Validation Epoch: 0, Average loss: 0.1854, Accuracy: 0.0312
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 150000, 'lr': 0.002, 'energy': 129.76808258680018, 'time': 3.2715427319999435, 'accuracy': 0.031150159744408944, 'total_cost': 12.799200466379688}
[run job] Launching job with BS 32: and LR: 0.002 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00200+pl125', 'ZEUS_COST_THRESH': '25.598400932759375', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.002', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.002', '--power_limit', '125']
[run job] cost_ub=25.598400932759375
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00200+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.002
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.0068
Training Epoch: 0 [96/50016]	Loss: 5.1680
Training Epoch: 0 [128/50016]	Loss: 5.4390
Training Epoch: 0 [160/50016]	Loss: 5.5029
Training Epoch: 0 [192/50016]	Loss: 5.2563
Training Epoch: 0 [224/50016]	Loss: 5.2625
Training Epoch: 0 [256/50016]	Loss: 5.1868
Training Epoch: 0 [288/50016]	Loss: 5.2005
Training Epoch: 0 [320/50016]	Loss: 5.2383
Training Epoch: 0 [352/50016]	Loss: 5.0180
Training Epoch: 0 [384/50016]	Loss: 5.5117
Training Epoch: 0 [416/50016]	Loss: 4.9948
Training Epoch: 0 [448/50016]	Loss: 4.8870
Training Epoch: 0 [480/50016]	Loss: 5.1180
Training Epoch: 0 [512/50016]	Loss: 4.8498
Training Epoch: 0 [544/50016]	Loss: 4.8943
Training Epoch: 0 [576/50016]	Loss: 4.5301
Training Epoch: 0 [608/50016]	Loss: 5.2281
Training Epoch: 0 [640/50016]	Loss: 5.6304
Training Epoch: 0 [672/50016]	Loss: 5.1860
Training Epoch: 0 [704/50016]	Loss: 4.6775
Training Epoch: 0 [736/50016]	Loss: 4.9006
Training Epoch: 0 [768/50016]	Loss: 5.0238
Training Epoch: 0 [800/50016]	Loss: 4.6946
Training Epoch: 0 [832/50016]	Loss: 4.9899
Training Epoch: 0 [864/50016]	Loss: 5.2505
Training Epoch: 0 [896/50016]	Loss: 4.8703
Training Epoch: 0 [928/50016]	Loss: 4.9206
Training Epoch: 0 [960/50016]	Loss: 4.6787
Training Epoch: 0 [992/50016]	Loss: 4.6952
Training Epoch: 0 [1024/50016]	Loss: 4.8576
Training Epoch: 0 [1056/50016]	Loss: 4.7908
Training Epoch: 0 [1088/50016]	Loss: 4.6273
Training Epoch: 0 [1120/50016]	Loss: 4.7180
Training Epoch: 0 [1152/50016]	Loss: 4.7302
Training Epoch: 0 [1184/50016]	Loss: 4.7280
Training Epoch: 0 [1216/50016]	Loss: 4.5442
Training Epoch: 0 [1248/50016]	Loss: 4.5075
Training Epoch: 0 [1280/50016]	Loss: 4.8075
Training Epoch: 0 [1312/50016]	Loss: 4.5185
Training Epoch: 0 [1344/50016]	Loss: 4.8236
Training Epoch: 0 [1376/50016]	Loss: 4.5685
Training Epoch: 0 [1408/50016]	Loss: 4.5871
Training Epoch: 0 [1440/50016]	Loss: 4.6720
Training Epoch: 0 [1472/50016]	Loss: 4.4629
Training Epoch: 0 [1504/50016]	Loss: 4.6410
Training Epoch: 0 [1536/50016]	Loss: 4.4968
Training Epoch: 0 [1568/50016]	Loss: 4.6857
Training Epoch: 0 [1600/50016]	Loss: 4.7009
Profile done with power limit 125W
epoch 1 train time consumed: 4.71s
Validation Epoch: 0, Average loss: 0.1570, Accuracy: 0.0246
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 125000, 'lr': 0.002, 'energy': 123.48572481994759, 'time': 3.3633146150000357, 'accuracy': 0.02456070287539936, 'total_cost': 16.344487534669526}
[run job] Launching job with BS 32: and LR: 0.002 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00200+pl100', 'ZEUS_COST_THRESH': '25.598400932759375', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.002', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.002', '--power_limit', '100']
[run job] cost_ub=25.598400932759375
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00200+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.002
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.0068
Training Epoch: 0 [96/50016]	Loss: 5.1676
Training Epoch: 0 [128/50016]	Loss: 5.4251
Training Epoch: 0 [160/50016]	Loss: 5.5362
Training Epoch: 0 [192/50016]	Loss: 5.2431
Training Epoch: 0 [224/50016]	Loss: 5.2371
Training Epoch: 0 [256/50016]	Loss: 5.1650
Training Epoch: 0 [288/50016]	Loss: 5.2225
Training Epoch: 0 [320/50016]	Loss: 5.2665
Training Epoch: 0 [352/50016]	Loss: 5.0046
Training Epoch: 0 [384/50016]	Loss: 5.5418
Training Epoch: 0 [416/50016]	Loss: 4.9998
Training Epoch: 0 [448/50016]	Loss: 4.8994
Training Epoch: 0 [480/50016]	Loss: 5.2299
Training Epoch: 0 [512/50016]	Loss: 5.0052
Training Epoch: 0 [544/50016]	Loss: 4.8675
Training Epoch: 0 [576/50016]	Loss: 4.6453
Training Epoch: 0 [608/50016]	Loss: 5.1881
Training Epoch: 0 [640/50016]	Loss: 5.5270
Training Epoch: 0 [672/50016]	Loss: 4.9151
Training Epoch: 0 [704/50016]	Loss: 4.7170
Training Epoch: 0 [736/50016]	Loss: 5.0938
Training Epoch: 0 [768/50016]	Loss: 5.3551
Training Epoch: 0 [800/50016]	Loss: 4.8990
Training Epoch: 0 [832/50016]	Loss: 5.1127
Training Epoch: 0 [864/50016]	Loss: 5.3052
Training Epoch: 0 [896/50016]	Loss: 4.6488
Training Epoch: 0 [928/50016]	Loss: 4.7468
Training Epoch: 0 [960/50016]	Loss: 4.6538
Training Epoch: 0 [992/50016]	Loss: 4.7115
Training Epoch: 0 [1024/50016]	Loss: 4.6278
Training Epoch: 0 [1056/50016]	Loss: 4.8927
Training Epoch: 0 [1088/50016]	Loss: 4.6431
Training Epoch: 0 [1120/50016]	Loss: 4.9167
Training Epoch: 0 [1152/50016]	Loss: 4.5957
Training Epoch: 0 [1184/50016]	Loss: 4.5510
Training Epoch: 0 [1216/50016]	Loss: 4.5233
Training Epoch: 0 [1248/50016]	Loss: 4.7744
Training Epoch: 0 [1280/50016]	Loss: 4.7166
Training Epoch: 0 [1312/50016]	Loss: 4.4810
Training Epoch: 0 [1344/50016]	Loss: 4.6342
Training Epoch: 0 [1376/50016]	Loss: 4.5946
Training Epoch: 0 [1408/50016]	Loss: 4.6255
Training Epoch: 0 [1440/50016]	Loss: 4.9266
Training Epoch: 0 [1472/50016]	Loss: 4.2472
Training Epoch: 0 [1504/50016]	Loss: 4.6337
Training Epoch: 0 [1536/50016]	Loss: 4.5518
Training Epoch: 0 [1568/50016]	Loss: 4.6183
Training Epoch: 0 [1600/50016]	Loss: 4.9015
Profile done with power limit 100W
epoch 1 train time consumed: 5.55s
Validation Epoch: 0, Average loss: 0.1914, Accuracy: 0.0235
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 100000, 'lr': 0.002, 'energy': 98.86800287545782, 'time': 4.0426507229999515, 'accuracy': 0.023462460063897763, 'total_cost': 18.869266075175126}

[Power Profiler] with batch size 32 and learning rate 0.0024
[run job] Launching job with BS 32: and LR: 0.0024 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00240+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0024', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0024', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00240+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0024
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.1427
Training Epoch: 0 [96/50016]	Loss: 5.3795
Training Epoch: 0 [128/50016]	Loss: 5.6551
Training Epoch: 0 [160/50016]	Loss: 5.6380
Training Epoch: 0 [192/50016]	Loss: 5.3344
Training Epoch: 0 [224/50016]	Loss: 5.3698
Training Epoch: 0 [256/50016]	Loss: 5.2960
Training Epoch: 0 [288/50016]	Loss: 5.3273
Training Epoch: 0 [320/50016]	Loss: 5.3549
Training Epoch: 0 [352/50016]	Loss: 5.2030
Training Epoch: 0 [384/50016]	Loss: 5.6231
Training Epoch: 0 [416/50016]	Loss: 5.0719
Training Epoch: 0 [448/50016]	Loss: 4.9635
Training Epoch: 0 [480/50016]	Loss: 5.2728
Training Epoch: 0 [512/50016]	Loss: 5.0818
Training Epoch: 0 [544/50016]	Loss: 5.0517
Training Epoch: 0 [576/50016]	Loss: 4.6622
Training Epoch: 0 [608/50016]	Loss: 5.1887
Training Epoch: 0 [640/50016]	Loss: 5.5979
Training Epoch: 0 [672/50016]	Loss: 5.0444
Training Epoch: 0 [704/50016]	Loss: 4.6444
Training Epoch: 0 [736/50016]	Loss: 5.1170
Training Epoch: 0 [768/50016]	Loss: 5.4873
Training Epoch: 0 [800/50016]	Loss: 4.8343
Training Epoch: 0 [832/50016]	Loss: 5.0550
Training Epoch: 0 [864/50016]	Loss: 5.3779
Training Epoch: 0 [896/50016]	Loss: 4.8109
Training Epoch: 0 [928/50016]	Loss: 4.8512
Training Epoch: 0 [960/50016]	Loss: 4.7644
Training Epoch: 0 [992/50016]	Loss: 4.7155
Training Epoch: 0 [1024/50016]	Loss: 4.7208
Training Epoch: 0 [1056/50016]	Loss: 4.9250
Training Epoch: 0 [1088/50016]	Loss: 4.6655
Training Epoch: 0 [1120/50016]	Loss: 4.8265
Training Epoch: 0 [1152/50016]	Loss: 4.7002
Training Epoch: 0 [1184/50016]	Loss: 4.7089
Training Epoch: 0 [1216/50016]	Loss: 4.5834
Training Epoch: 0 [1248/50016]	Loss: 4.6170
Training Epoch: 0 [1280/50016]	Loss: 4.7477
Training Epoch: 0 [1312/50016]	Loss: 4.5794
Training Epoch: 0 [1344/50016]	Loss: 4.7026
Training Epoch: 0 [1376/50016]	Loss: 4.5541
Training Epoch: 0 [1408/50016]	Loss: 4.6174
Training Epoch: 0 [1440/50016]	Loss: 4.6434
Training Epoch: 0 [1472/50016]	Loss: 4.4302
Training Epoch: 0 [1504/50016]	Loss: 5.0058
Training Epoch: 0 [1536/50016]	Loss: 4.4752
Training Epoch: 0 [1568/50016]	Loss: 4.9018
Training Epoch: 0 [1600/50016]	Loss: 4.7448
Profile done with power limit 175W
epoch 1 train time consumed: 4.67s
Validation Epoch: 0, Average loss: 0.2118, Accuracy: 0.0248
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 175000, 'lr': 0.0024, 'energy': 129.9685534585697, 'time': 3.2875678709999647, 'accuracy': 0.02476038338658147, 'total_cost': 16.19173776850366}
[run job] Launching job with BS 32: and LR: 0.0024 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00240+pl150', 'ZEUS_COST_THRESH': '32.38347553700732', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0024', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0024', '--power_limit', '150']
[run job] cost_ub=32.38347553700732
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00240+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0024
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.1427
Training Epoch: 0 [96/50016]	Loss: 5.3804
Training Epoch: 0 [128/50016]	Loss: 5.6623
Training Epoch: 0 [160/50016]	Loss: 5.6386
Training Epoch: 0 [192/50016]	Loss: 5.3093
Training Epoch: 0 [224/50016]	Loss: 5.3944
Training Epoch: 0 [256/50016]	Loss: 5.2815
Training Epoch: 0 [288/50016]	Loss: 5.4482
Training Epoch: 0 [320/50016]	Loss: 5.3416
Training Epoch: 0 [352/50016]	Loss: 5.1716
Training Epoch: 0 [384/50016]	Loss: 5.5682
Training Epoch: 0 [416/50016]	Loss: 5.0295
Training Epoch: 0 [448/50016]	Loss: 4.8869
Training Epoch: 0 [480/50016]	Loss: 5.1576
Training Epoch: 0 [512/50016]	Loss: 4.7769
Training Epoch: 0 [544/50016]	Loss: 4.9869
Training Epoch: 0 [576/50016]	Loss: 4.4224
Training Epoch: 0 [608/50016]	Loss: 5.2543
Training Epoch: 0 [640/50016]	Loss: 5.5262
Training Epoch: 0 [672/50016]	Loss: 5.0649
Training Epoch: 0 [704/50016]	Loss: 4.6681
Training Epoch: 0 [736/50016]	Loss: 4.9418
Training Epoch: 0 [768/50016]	Loss: 5.3782
Training Epoch: 0 [800/50016]	Loss: 4.5366
Training Epoch: 0 [832/50016]	Loss: 4.7197
Training Epoch: 0 [864/50016]	Loss: 5.1377
Training Epoch: 0 [896/50016]	Loss: 4.9530
Training Epoch: 0 [928/50016]	Loss: 5.0616
Training Epoch: 0 [960/50016]	Loss: 5.0084
Training Epoch: 0 [992/50016]	Loss: 5.0060
Training Epoch: 0 [1024/50016]	Loss: 4.9753
Training Epoch: 0 [1056/50016]	Loss: 4.9891
Training Epoch: 0 [1088/50016]	Loss: 4.7330
Training Epoch: 0 [1120/50016]	Loss: 4.6895
Training Epoch: 0 [1152/50016]	Loss: 4.6746
Training Epoch: 0 [1184/50016]	Loss: 4.8909
Training Epoch: 0 [1216/50016]	Loss: 4.8785
Training Epoch: 0 [1248/50016]	Loss: 4.6805
Training Epoch: 0 [1280/50016]	Loss: 4.9641
Training Epoch: 0 [1312/50016]	Loss: 4.7255
Training Epoch: 0 [1344/50016]	Loss: 4.5941
Training Epoch: 0 [1376/50016]	Loss: 4.5223
Training Epoch: 0 [1408/50016]	Loss: 4.9200
Training Epoch: 0 [1440/50016]	Loss: 4.7183
Training Epoch: 0 [1472/50016]	Loss: 4.7295
Training Epoch: 0 [1504/50016]	Loss: 4.7673
Training Epoch: 0 [1536/50016]	Loss: 4.6479
Training Epoch: 0 [1568/50016]	Loss: 4.5413
Training Epoch: 0 [1600/50016]	Loss: 4.5058
Profile done with power limit 150W
epoch 1 train time consumed: 4.62s
Validation Epoch: 0, Average loss: 0.1994, Accuracy: 0.0142
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 150000, 'lr': 0.0024, 'energy': 130.290800961541, 'time': 3.279302641999948, 'accuracy': 0.014177316293929713, 'total_cost': 28.237238890871424}
[run job] Launching job with BS 32: and LR: 0.0024 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00240+pl125', 'ZEUS_COST_THRESH': '32.38347553700732', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0024', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0024', '--power_limit', '125']
[run job] cost_ub=32.38347553700732
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00240+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0024
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.1427
Training Epoch: 0 [96/50016]	Loss: 5.3805
Training Epoch: 0 [128/50016]	Loss: 5.6521
Training Epoch: 0 [160/50016]	Loss: 5.6816
Training Epoch: 0 [192/50016]	Loss: 5.3455
Training Epoch: 0 [224/50016]	Loss: 5.3298
Training Epoch: 0 [256/50016]	Loss: 5.2482
Training Epoch: 0 [288/50016]	Loss: 5.4043
Training Epoch: 0 [320/50016]	Loss: 5.3921
Training Epoch: 0 [352/50016]	Loss: 5.1159
Training Epoch: 0 [384/50016]	Loss: 5.4696
Training Epoch: 0 [416/50016]	Loss: 5.0355
Training Epoch: 0 [448/50016]	Loss: 4.8731
Training Epoch: 0 [480/50016]	Loss: 5.1187
Training Epoch: 0 [512/50016]	Loss: 4.7355
Training Epoch: 0 [544/50016]	Loss: 4.8439
Training Epoch: 0 [576/50016]	Loss: 4.4088
Training Epoch: 0 [608/50016]	Loss: 5.3096
Training Epoch: 0 [640/50016]	Loss: 5.5129
Training Epoch: 0 [672/50016]	Loss: 5.0935
Training Epoch: 0 [704/50016]	Loss: 4.6466
Training Epoch: 0 [736/50016]	Loss: 4.7955
Training Epoch: 0 [768/50016]	Loss: 5.1166
Training Epoch: 0 [800/50016]	Loss: 4.6651
Training Epoch: 0 [832/50016]	Loss: 4.6759
Training Epoch: 0 [864/50016]	Loss: 5.5454
Training Epoch: 0 [896/50016]	Loss: 5.0039
Training Epoch: 0 [928/50016]	Loss: 4.9458
Training Epoch: 0 [960/50016]	Loss: 4.7042
Training Epoch: 0 [992/50016]	Loss: 4.8709
Training Epoch: 0 [1024/50016]	Loss: 5.0722
Training Epoch: 0 [1056/50016]	Loss: 4.9440
Training Epoch: 0 [1088/50016]	Loss: 4.5714
Training Epoch: 0 [1120/50016]	Loss: 4.7291
Training Epoch: 0 [1152/50016]	Loss: 4.8081
Training Epoch: 0 [1184/50016]	Loss: 4.7321
Training Epoch: 0 [1216/50016]	Loss: 4.5926
Training Epoch: 0 [1248/50016]	Loss: 4.6357
Training Epoch: 0 [1280/50016]	Loss: 4.7594
Training Epoch: 0 [1312/50016]	Loss: 4.4646
Training Epoch: 0 [1344/50016]	Loss: 4.8878
Training Epoch: 0 [1376/50016]	Loss: 4.5950
Training Epoch: 0 [1408/50016]	Loss: 4.5813
Training Epoch: 0 [1440/50016]	Loss: 4.6423
Training Epoch: 0 [1472/50016]	Loss: 4.5678
Training Epoch: 0 [1504/50016]	Loss: 4.6147
Training Epoch: 0 [1536/50016]	Loss: 4.5853
Training Epoch: 0 [1568/50016]	Loss: 4.7735
Training Epoch: 0 [1600/50016]	Loss: 4.8388
Profile done with power limit 125W
epoch 1 train time consumed: 4.67s
Validation Epoch: 0, Average loss: 0.1507, Accuracy: 0.0154
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 125000, 'lr': 0.0024, 'energy': 123.6526186286231, 'time': 3.2953076109999984, 'accuracy': 0.015375399361022364, 'total_cost': 25.59510543139236}
[run job] Launching job with BS 32: and LR: 0.0024 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs32+lr0.00240+pl100', 'ZEUS_COST_THRESH': '32.38347553700732', 'ZEUS_BATCH_SIZE': '32', 'ZEUS_LEARNING_RATE': '0.0024', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '32', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0024', '--power_limit', '100']
[run job] cost_ub=32.38347553700732
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs32+lr0.00240+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0024
batch size arg: 32
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [32/50016]	Loss: 4.6525
Training Epoch: 0 [64/50016]	Loss: 5.1427
Training Epoch: 0 [96/50016]	Loss: 5.3794
Training Epoch: 0 [128/50016]	Loss: 5.6695
Training Epoch: 0 [160/50016]	Loss: 5.6715
Training Epoch: 0 [192/50016]	Loss: 5.3447
Training Epoch: 0 [224/50016]	Loss: 5.3862
Training Epoch: 0 [256/50016]	Loss: 5.2927
Training Epoch: 0 [288/50016]	Loss: 5.4245
Training Epoch: 0 [320/50016]	Loss: 5.3418
Training Epoch: 0 [352/50016]	Loss: 5.1940
Training Epoch: 0 [384/50016]	Loss: 5.5706
Training Epoch: 0 [416/50016]	Loss: 5.0719
Training Epoch: 0 [448/50016]	Loss: 4.9241
Training Epoch: 0 [480/50016]	Loss: 5.2013
Training Epoch: 0 [512/50016]	Loss: 4.8858
Training Epoch: 0 [544/50016]	Loss: 4.9918
Training Epoch: 0 [576/50016]	Loss: 4.5523
Training Epoch: 0 [608/50016]	Loss: 5.3068
Training Epoch: 0 [640/50016]	Loss: 5.6418
Training Epoch: 0 [672/50016]	Loss: 5.1688
Training Epoch: 0 [704/50016]	Loss: 4.7171
Training Epoch: 0 [736/50016]	Loss: 4.9471
Training Epoch: 0 [768/50016]	Loss: 4.9899
Training Epoch: 0 [800/50016]	Loss: 4.6737
Training Epoch: 0 [832/50016]	Loss: 4.9003
Training Epoch: 0 [864/50016]	Loss: 5.2267
Training Epoch: 0 [896/50016]	Loss: 4.8607
Training Epoch: 0 [928/50016]	Loss: 4.8935
Training Epoch: 0 [960/50016]	Loss: 4.7027
Training Epoch: 0 [992/50016]	Loss: 4.7556
Training Epoch: 0 [1024/50016]	Loss: 4.8646
Training Epoch: 0 [1056/50016]	Loss: 4.8187
Training Epoch: 0 [1088/50016]	Loss: 4.6713
Training Epoch: 0 [1120/50016]	Loss: 4.7025
Training Epoch: 0 [1152/50016]	Loss: 4.6859
Training Epoch: 0 [1184/50016]	Loss: 4.7694
Training Epoch: 0 [1216/50016]	Loss: 4.5574
Training Epoch: 0 [1248/50016]	Loss: 4.4815
Training Epoch: 0 [1280/50016]	Loss: 4.7524
Training Epoch: 0 [1312/50016]	Loss: 4.5545
Training Epoch: 0 [1344/50016]	Loss: 4.7234
Training Epoch: 0 [1376/50016]	Loss: 4.5387
Training Epoch: 0 [1408/50016]	Loss: 4.6391
Training Epoch: 0 [1440/50016]	Loss: 4.6440
Training Epoch: 0 [1472/50016]	Loss: 4.5529
Training Epoch: 0 [1504/50016]	Loss: 5.0052
Training Epoch: 0 [1536/50016]	Loss: 4.5310
Training Epoch: 0 [1568/50016]	Loss: 4.9417
Training Epoch: 0 [1600/50016]	Loss: 4.6835
Profile done with power limit 100W
epoch 1 train time consumed: 5.55s
Validation Epoch: 0, Average loss: 0.1544, Accuracy: 0.0209
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 32, 'pl': 100000, 'lr': 0.0024, 'energy': 98.9455504749466, 'time': 4.0192913579999185, 'accuracy': 0.020866613418530352, 'total_cost': 21.100017195649954}

[Power Profiler] with batch size 64 and learning rate 0.0022627416997969526
[run job] Launching job with BS 64: and LR: 0.0022627416997969526 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00226+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0022627416997969526', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0022627416997969526', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00226+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0022627416997969526
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.8843
Training Epoch: 0 [192/50048]	Loss: 5.0511
Training Epoch: 0 [256/50048]	Loss: 5.2169
Training Epoch: 0 [320/50048]	Loss: 5.1154
Training Epoch: 0 [384/50048]	Loss: 5.1309
Training Epoch: 0 [448/50048]	Loss: 4.9216
Training Epoch: 0 [512/50048]	Loss: 5.1484
Training Epoch: 0 [576/50048]	Loss: 4.7252
Training Epoch: 0 [640/50048]	Loss: 5.3264
Training Epoch: 0 [704/50048]	Loss: 4.8767
Training Epoch: 0 [768/50048]	Loss: 5.2156
Training Epoch: 0 [832/50048]	Loss: 5.1042
Training Epoch: 0 [896/50048]	Loss: 5.0056
Training Epoch: 0 [960/50048]	Loss: 4.8460
Training Epoch: 0 [1024/50048]	Loss: 5.0447
Training Epoch: 0 [1088/50048]	Loss: 5.0353
Training Epoch: 0 [1152/50048]	Loss: 4.8341
Training Epoch: 0 [1216/50048]	Loss: 4.8490
Training Epoch: 0 [1280/50048]	Loss: 4.8550
Training Epoch: 0 [1344/50048]	Loss: 4.8737
Training Epoch: 0 [1408/50048]	Loss: 4.6921
Training Epoch: 0 [1472/50048]	Loss: 4.7024
Training Epoch: 0 [1536/50048]	Loss: 4.8540
Training Epoch: 0 [1600/50048]	Loss: 5.0366
Training Epoch: 0 [1664/50048]	Loss: 4.7897
Training Epoch: 0 [1728/50048]	Loss: 4.6977
Training Epoch: 0 [1792/50048]	Loss: 4.6061
Training Epoch: 0 [1856/50048]	Loss: 4.6417
Training Epoch: 0 [1920/50048]	Loss: 4.6737
Training Epoch: 0 [1984/50048]	Loss: 4.5926
Training Epoch: 0 [2048/50048]	Loss: 4.6345
Training Epoch: 0 [2112/50048]	Loss: 4.5461
Training Epoch: 0 [2176/50048]	Loss: 4.7216
Training Epoch: 0 [2240/50048]	Loss: 4.6396
Training Epoch: 0 [2304/50048]	Loss: 4.7772
Training Epoch: 0 [2368/50048]	Loss: 4.6509
Training Epoch: 0 [2432/50048]	Loss: 4.5712
Training Epoch: 0 [2496/50048]	Loss: 4.4889
Training Epoch: 0 [2560/50048]	Loss: 4.7665
Training Epoch: 0 [2624/50048]	Loss: 4.5065
Training Epoch: 0 [2688/50048]	Loss: 4.5848
Training Epoch: 0 [2752/50048]	Loss: 4.4416
Training Epoch: 0 [2816/50048]	Loss: 4.4999
Training Epoch: 0 [2880/50048]	Loss: 4.4632
Training Epoch: 0 [2944/50048]	Loss: 4.5033
Training Epoch: 0 [3008/50048]	Loss: 4.4293
Training Epoch: 0 [3072/50048]	Loss: 4.6716
Training Epoch: 0 [3136/50048]	Loss: 4.6362
Training Epoch: 0 [3200/50048]	Loss: 4.4104
Profile done with power limit 175W
epoch 1 train time consumed: 7.19s
Validation Epoch: 0, Average loss: 0.0734, Accuracy: 0.0157
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 175000, 'lr': 0.0022627416997969526, 'energy': 140.89273903349454, 'time': 5.2876487409998845, 'accuracy': 0.01572452229299363, 'total_cost': 84.89811437181832}
[run job] Launching job with BS 64: and LR: 0.0022627416997969526 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00226+pl150', 'ZEUS_COST_THRESH': '169.79622874363665', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0022627416997969526', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0022627416997969526', '--power_limit', '150']
[run job] cost_ub=169.79622874363665
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00226+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0022627416997969526
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.8843
Training Epoch: 0 [192/50048]	Loss: 5.0515
Training Epoch: 0 [256/50048]	Loss: 5.1997
Training Epoch: 0 [320/50048]	Loss: 5.1260
Training Epoch: 0 [384/50048]	Loss: 5.1617
Training Epoch: 0 [448/50048]	Loss: 4.8880
Training Epoch: 0 [512/50048]	Loss: 5.1205
Training Epoch: 0 [576/50048]	Loss: 4.7100
Training Epoch: 0 [640/50048]	Loss: 5.2993
Training Epoch: 0 [704/50048]	Loss: 4.8443
Training Epoch: 0 [768/50048]	Loss: 5.1980
Training Epoch: 0 [832/50048]	Loss: 5.0283
Training Epoch: 0 [896/50048]	Loss: 4.9972
Training Epoch: 0 [960/50048]	Loss: 4.8590
Training Epoch: 0 [1024/50048]	Loss: 4.9770
Training Epoch: 0 [1088/50048]	Loss: 4.9818
Training Epoch: 0 [1152/50048]	Loss: 4.7666
Training Epoch: 0 [1216/50048]	Loss: 4.7973
Training Epoch: 0 [1280/50048]	Loss: 4.6645
Training Epoch: 0 [1344/50048]	Loss: 4.7184
Training Epoch: 0 [1408/50048]	Loss: 4.6079
Training Epoch: 0 [1472/50048]	Loss: 4.6766
Training Epoch: 0 [1536/50048]	Loss: 4.8818
Training Epoch: 0 [1600/50048]	Loss: 4.9314
Training Epoch: 0 [1664/50048]	Loss: 4.8480
Training Epoch: 0 [1728/50048]	Loss: 4.6451
Training Epoch: 0 [1792/50048]	Loss: 4.5404
Training Epoch: 0 [1856/50048]	Loss: 4.6733
Training Epoch: 0 [1920/50048]	Loss: 4.6821
Training Epoch: 0 [1984/50048]	Loss: 4.6134
Training Epoch: 0 [2048/50048]	Loss: 4.5105
Training Epoch: 0 [2112/50048]	Loss: 4.5945
Training Epoch: 0 [2176/50048]	Loss: 4.6617
Training Epoch: 0 [2240/50048]	Loss: 4.6015
Training Epoch: 0 [2304/50048]	Loss: 4.6302
Training Epoch: 0 [2368/50048]	Loss: 4.6229
Training Epoch: 0 [2432/50048]	Loss: 4.5690
Training Epoch: 0 [2496/50048]	Loss: 4.4671
Training Epoch: 0 [2560/50048]	Loss: 4.5910
Training Epoch: 0 [2624/50048]	Loss: 4.4115
Training Epoch: 0 [2688/50048]	Loss: 4.5211
Training Epoch: 0 [2752/50048]	Loss: 4.4445
Training Epoch: 0 [2816/50048]	Loss: 4.4120
Training Epoch: 0 [2880/50048]	Loss: 4.3660
Training Epoch: 0 [2944/50048]	Loss: 4.4685
Training Epoch: 0 [3008/50048]	Loss: 4.2503
Training Epoch: 0 [3072/50048]	Loss: 4.6090
Training Epoch: 0 [3136/50048]	Loss: 4.7380
Training Epoch: 0 [3200/50048]	Loss: 4.5223
Profile done with power limit 150W
epoch 1 train time consumed: 7.22s
Validation Epoch: 0, Average loss: 0.0720, Accuracy: 0.0229
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 150000, 'lr': 0.0022627416997969526, 'energy': 141.18079688987459, 'time': 5.289481872999886, 'accuracy': 0.02289012738853503, 'total_cost': 58.394733062854485}
[run job] Launching job with BS 64: and LR: 0.0022627416997969526 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00226+pl125', 'ZEUS_COST_THRESH': '116.78946612570897', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0022627416997969526', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0022627416997969526', '--power_limit', '125']
[run job] cost_ub=116.78946612570897
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00226+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0022627416997969526
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.8843
Training Epoch: 0 [192/50048]	Loss: 5.0513
Training Epoch: 0 [256/50048]	Loss: 5.2028
Training Epoch: 0 [320/50048]	Loss: 5.1288
Training Epoch: 0 [384/50048]	Loss: 5.1600
Training Epoch: 0 [448/50048]	Loss: 4.8887
Training Epoch: 0 [512/50048]	Loss: 5.1541
Training Epoch: 0 [576/50048]	Loss: 4.6973
Training Epoch: 0 [640/50048]	Loss: 5.3069
Training Epoch: 0 [704/50048]	Loss: 4.8956
Training Epoch: 0 [768/50048]	Loss: 5.2075
Training Epoch: 0 [832/50048]	Loss: 5.0937
Training Epoch: 0 [896/50048]	Loss: 4.9579
Training Epoch: 0 [960/50048]	Loss: 4.9211
Training Epoch: 0 [1024/50048]	Loss: 5.0488
Training Epoch: 0 [1088/50048]	Loss: 5.0402
Training Epoch: 0 [1152/50048]	Loss: 4.8187
Training Epoch: 0 [1216/50048]	Loss: 4.8646
Training Epoch: 0 [1280/50048]	Loss: 4.8967
Training Epoch: 0 [1344/50048]	Loss: 4.8416
Training Epoch: 0 [1408/50048]	Loss: 4.8243
Training Epoch: 0 [1472/50048]	Loss: 4.7566
Training Epoch: 0 [1536/50048]	Loss: 4.8633
Training Epoch: 0 [1600/50048]	Loss: 5.0704
Training Epoch: 0 [1664/50048]	Loss: 4.7684
Training Epoch: 0 [1728/50048]	Loss: 4.7540
Training Epoch: 0 [1792/50048]	Loss: 4.7766
Training Epoch: 0 [1856/50048]	Loss: 4.7278
Training Epoch: 0 [1920/50048]	Loss: 4.6949
Training Epoch: 0 [1984/50048]	Loss: 4.6574
Training Epoch: 0 [2048/50048]	Loss: 4.4951
Training Epoch: 0 [2112/50048]	Loss: 4.6104
Training Epoch: 0 [2176/50048]	Loss: 4.6063
Training Epoch: 0 [2240/50048]	Loss: 4.6196
Training Epoch: 0 [2304/50048]	Loss: 4.5975
Training Epoch: 0 [2368/50048]	Loss: 4.5913
Training Epoch: 0 [2432/50048]	Loss: 4.6671
Training Epoch: 0 [2496/50048]	Loss: 4.5596
Training Epoch: 0 [2560/50048]	Loss: 4.4306
Training Epoch: 0 [2624/50048]	Loss: 4.3943
Training Epoch: 0 [2688/50048]	Loss: 4.6338
Training Epoch: 0 [2752/50048]	Loss: 4.4254
Training Epoch: 0 [2816/50048]	Loss: 4.3273
Training Epoch: 0 [2880/50048]	Loss: 4.3154
Training Epoch: 0 [2944/50048]	Loss: 4.4471
Training Epoch: 0 [3008/50048]	Loss: 4.2282
Training Epoch: 0 [3072/50048]	Loss: 4.6212
Training Epoch: 0 [3136/50048]	Loss: 4.4905
Training Epoch: 0 [3200/50048]	Loss: 4.2600
Profile done with power limit 125W
epoch 1 train time consumed: 7.79s
Validation Epoch: 0, Average loss: 0.0775, Accuracy: 0.0322
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 125000, 'lr': 0.0022627416997969526, 'energy': 123.22366880039353, 'time': 5.796638918999861, 'accuracy': 0.032245222929936306, 'total_cost': 42.84757400243753}
[run job] Launching job with BS 64: and LR: 0.0022627416997969526 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00226+pl100', 'ZEUS_COST_THRESH': '85.69514800487507', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0022627416997969526', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0022627416997969526', '--power_limit', '100']
[run job] cost_ub=85.69514800487507
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00226+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0022627416997969526
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 4.8843
Training Epoch: 0 [192/50048]	Loss: 5.0510
Training Epoch: 0 [256/50048]	Loss: 5.2125
Training Epoch: 0 [320/50048]	Loss: 5.1121
Training Epoch: 0 [384/50048]	Loss: 5.1663
Training Epoch: 0 [448/50048]	Loss: 4.9028
Training Epoch: 0 [512/50048]	Loss: 5.1123
Training Epoch: 0 [576/50048]	Loss: 4.7101
Training Epoch: 0 [640/50048]	Loss: 5.2988
Training Epoch: 0 [704/50048]	Loss: 4.8531
Training Epoch: 0 [768/50048]	Loss: 5.2101
Training Epoch: 0 [832/50048]	Loss: 5.0680
Training Epoch: 0 [896/50048]	Loss: 5.0086
Training Epoch: 0 [960/50048]	Loss: 4.8937
Training Epoch: 0 [1024/50048]	Loss: 4.9841
Training Epoch: 0 [1088/50048]	Loss: 5.0193
Training Epoch: 0 [1152/50048]	Loss: 4.8083
Training Epoch: 0 [1216/50048]	Loss: 4.8341
Training Epoch: 0 [1280/50048]	Loss: 4.7831
Training Epoch: 0 [1344/50048]	Loss: 4.7729
Training Epoch: 0 [1408/50048]	Loss: 4.6128
Training Epoch: 0 [1472/50048]	Loss: 4.6244
Training Epoch: 0 [1536/50048]	Loss: 4.7966
Training Epoch: 0 [1600/50048]	Loss: 4.8643
Training Epoch: 0 [1664/50048]	Loss: 4.8294
Training Epoch: 0 [1728/50048]	Loss: 4.7134
Training Epoch: 0 [1792/50048]	Loss: 4.7040
Training Epoch: 0 [1856/50048]	Loss: 4.8791
Training Epoch: 0 [1920/50048]	Loss: 4.6254
Training Epoch: 0 [1984/50048]	Loss: 4.6493
Training Epoch: 0 [2048/50048]	Loss: 4.5553
Training Epoch: 0 [2112/50048]	Loss: 4.8338
Training Epoch: 0 [2176/50048]	Loss: 4.6371
Training Epoch: 0 [2240/50048]	Loss: 4.6604
Training Epoch: 0 [2304/50048]	Loss: 4.6835
Training Epoch: 0 [2368/50048]	Loss: 4.5975
Training Epoch: 0 [2432/50048]	Loss: 4.5949
Training Epoch: 0 [2496/50048]	Loss: 4.5538
Training Epoch: 0 [2560/50048]	Loss: 4.6720
Training Epoch: 0 [2624/50048]	Loss: 4.5252
Training Epoch: 0 [2688/50048]	Loss: 4.5598
Training Epoch: 0 [2752/50048]	Loss: 4.4643
Training Epoch: 0 [2816/50048]	Loss: 4.3949
Training Epoch: 0 [2880/50048]	Loss: 4.3582
Training Epoch: 0 [2944/50048]	Loss: 4.5169
Training Epoch: 0 [3008/50048]	Loss: 4.3816
Training Epoch: 0 [3072/50048]	Loss: 4.5979
Training Epoch: 0 [3136/50048]	Loss: 4.5333
Training Epoch: 0 [3200/50048]	Loss: 4.4753
Profile done with power limit 100W
epoch 1 train time consumed: 15.50s
Validation Epoch: 0, Average loss: 0.0718, Accuracy: 0.0206
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 100000, 'lr': 0.0022627416997969526, 'energy': 99.05516183652752, 'time': 12.049748545000057, 'accuracy': 0.020601114649681528, 'total_cost': 128.1145677876729}

[Power Profiler] with batch size 64 and learning rate 0.0028284271247461905
[run job] Launching job with BS 64: and LR: 0.0028284271247461905 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00283+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0028284271247461905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0028284271247461905', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00283+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0028284271247461905
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.0397
Training Epoch: 0 [192/50048]	Loss: 5.2128
Training Epoch: 0 [256/50048]	Loss: 5.3832
Training Epoch: 0 [320/50048]	Loss: 5.2509
Training Epoch: 0 [384/50048]	Loss: 5.2706
Training Epoch: 0 [448/50048]	Loss: 5.0079
Training Epoch: 0 [512/50048]	Loss: 5.2109
Training Epoch: 0 [576/50048]	Loss: 4.6833
Training Epoch: 0 [640/50048]	Loss: 5.5179
Training Epoch: 0 [704/50048]	Loss: 4.8420
Training Epoch: 0 [768/50048]	Loss: 5.1852
Training Epoch: 0 [832/50048]	Loss: 5.0150
Training Epoch: 0 [896/50048]	Loss: 4.9061
Training Epoch: 0 [960/50048]	Loss: 4.9021
Training Epoch: 0 [1024/50048]	Loss: 4.7455
Training Epoch: 0 [1088/50048]	Loss: 4.8309
Training Epoch: 0 [1152/50048]	Loss: 4.7992
Training Epoch: 0 [1216/50048]	Loss: 4.7416
Training Epoch: 0 [1280/50048]	Loss: 4.6575
Training Epoch: 0 [1344/50048]	Loss: 4.8846
Training Epoch: 0 [1408/50048]	Loss: 4.5485
Training Epoch: 0 [1472/50048]	Loss: 4.5682
Training Epoch: 0 [1536/50048]	Loss: 4.6685
Training Epoch: 0 [1600/50048]	Loss: 4.8796
Training Epoch: 0 [1664/50048]	Loss: 4.8905
Training Epoch: 0 [1728/50048]	Loss: 4.8082
Training Epoch: 0 [1792/50048]	Loss: 4.7792
Training Epoch: 0 [1856/50048]	Loss: 4.9138
Training Epoch: 0 [1920/50048]	Loss: 4.6143
Training Epoch: 0 [1984/50048]	Loss: 4.5535
Training Epoch: 0 [2048/50048]	Loss: 4.5879
Training Epoch: 0 [2112/50048]	Loss: 4.7480
Training Epoch: 0 [2176/50048]	Loss: 4.6167
Training Epoch: 0 [2240/50048]	Loss: 4.5760
Training Epoch: 0 [2304/50048]	Loss: 4.6588
Training Epoch: 0 [2368/50048]	Loss: 4.5956
Training Epoch: 0 [2432/50048]	Loss: 4.6413
Training Epoch: 0 [2496/50048]	Loss: 4.7941
Training Epoch: 0 [2560/50048]	Loss: 4.5458
Training Epoch: 0 [2624/50048]	Loss: 4.5621
Training Epoch: 0 [2688/50048]	Loss: 4.6514
Training Epoch: 0 [2752/50048]	Loss: 4.5758
Training Epoch: 0 [2816/50048]	Loss: 4.4828
Training Epoch: 0 [2880/50048]	Loss: 4.4255
Training Epoch: 0 [2944/50048]	Loss: 4.6107
Training Epoch: 0 [3008/50048]	Loss: 4.5576
Training Epoch: 0 [3072/50048]	Loss: 4.6047
Training Epoch: 0 [3136/50048]	Loss: 4.6311
Training Epoch: 0 [3200/50048]	Loss: 4.5761
Profile done with power limit 175W
epoch 1 train time consumed: 7.19s
Validation Epoch: 0, Average loss: 0.0788, Accuracy: 0.0223
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 175000, 'lr': 0.0028284271247461905, 'energy': 140.74453648235547, 'time': 5.303924341999846, 'accuracy': 0.022292993630573247, 'total_cost': 60.03963414907953}
[run job] Launching job with BS 64: and LR: 0.0028284271247461905 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00283+pl150', 'ZEUS_COST_THRESH': '120.07926829815906', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0028284271247461905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0028284271247461905', '--power_limit', '150']
[run job] cost_ub=120.07926829815906
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00283+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0028284271247461905
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.0397
Training Epoch: 0 [192/50048]	Loss: 5.2125
Training Epoch: 0 [256/50048]	Loss: 5.3858
Training Epoch: 0 [320/50048]	Loss: 5.2716
Training Epoch: 0 [384/50048]	Loss: 5.2589
Training Epoch: 0 [448/50048]	Loss: 4.9937
Training Epoch: 0 [512/50048]	Loss: 5.1762
Training Epoch: 0 [576/50048]	Loss: 4.7107
Training Epoch: 0 [640/50048]	Loss: 5.4666
Training Epoch: 0 [704/50048]	Loss: 4.8640
Training Epoch: 0 [768/50048]	Loss: 5.1529
Training Epoch: 0 [832/50048]	Loss: 4.9797
Training Epoch: 0 [896/50048]	Loss: 5.0138
Training Epoch: 0 [960/50048]	Loss: 4.8154
Training Epoch: 0 [1024/50048]	Loss: 4.8071
Training Epoch: 0 [1088/50048]	Loss: 4.7627
Training Epoch: 0 [1152/50048]	Loss: 4.7587
Training Epoch: 0 [1216/50048]	Loss: 4.7511
Training Epoch: 0 [1280/50048]	Loss: 4.5423
Training Epoch: 0 [1344/50048]	Loss: 4.7989
Training Epoch: 0 [1408/50048]	Loss: 4.6024
Training Epoch: 0 [1472/50048]	Loss: 4.6444
Training Epoch: 0 [1536/50048]	Loss: 4.7381
Training Epoch: 0 [1600/50048]	Loss: 4.9258
Training Epoch: 0 [1664/50048]	Loss: 4.9682
Training Epoch: 0 [1728/50048]	Loss: 4.6295
Training Epoch: 0 [1792/50048]	Loss: 4.5586
Training Epoch: 0 [1856/50048]	Loss: 4.7104
Training Epoch: 0 [1920/50048]	Loss: 4.6385
Training Epoch: 0 [1984/50048]	Loss: 4.5691
Training Epoch: 0 [2048/50048]	Loss: 4.6168
Training Epoch: 0 [2112/50048]	Loss: 4.6492
Training Epoch: 0 [2176/50048]	Loss: 4.7231
Training Epoch: 0 [2240/50048]	Loss: 4.6497
Training Epoch: 0 [2304/50048]	Loss: 4.6211
Training Epoch: 0 [2368/50048]	Loss: 4.6920
Training Epoch: 0 [2432/50048]	Loss: 4.5465
Training Epoch: 0 [2496/50048]	Loss: 4.6340
Training Epoch: 0 [2560/50048]	Loss: 4.5701
Training Epoch: 0 [2624/50048]	Loss: 4.4956
Training Epoch: 0 [2688/50048]	Loss: 4.4746
Training Epoch: 0 [2752/50048]	Loss: 4.6223
Training Epoch: 0 [2816/50048]	Loss: 4.4732
Training Epoch: 0 [2880/50048]	Loss: 4.4711
Training Epoch: 0 [2944/50048]	Loss: 4.4894
Training Epoch: 0 [3008/50048]	Loss: 4.3809
Training Epoch: 0 [3072/50048]	Loss: 4.5792
Training Epoch: 0 [3136/50048]	Loss: 4.7418
Training Epoch: 0 [3200/50048]	Loss: 4.5150
Profile done with power limit 150W
epoch 1 train time consumed: 7.22s
Validation Epoch: 0, Average loss: 0.0737, Accuracy: 0.0206
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 150000, 'lr': 0.0028284271247461905, 'energy': 141.20441949984854, 'time': 5.273835460999862, 'accuracy': 0.020601114649681528, 'total_cost': 64.69594441006188}
[run job] Launching job with BS 64: and LR: 0.0028284271247461905 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00283+pl125', 'ZEUS_COST_THRESH': '120.07926829815906', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0028284271247461905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0028284271247461905', '--power_limit', '125']
[run job] cost_ub=120.07926829815906
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00283+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0028284271247461905
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.0397
Training Epoch: 0 [192/50048]	Loss: 5.2123
Training Epoch: 0 [256/50048]	Loss: 5.4025
Training Epoch: 0 [320/50048]	Loss: 5.2517
Training Epoch: 0 [384/50048]	Loss: 5.2580
Training Epoch: 0 [448/50048]	Loss: 5.0120
Training Epoch: 0 [512/50048]	Loss: 5.1763
Training Epoch: 0 [576/50048]	Loss: 4.6979
Training Epoch: 0 [640/50048]	Loss: 5.4958
Training Epoch: 0 [704/50048]	Loss: 4.8842
Training Epoch: 0 [768/50048]	Loss: 5.1368
Training Epoch: 0 [832/50048]	Loss: 4.9272
Training Epoch: 0 [896/50048]	Loss: 4.9800
Training Epoch: 0 [960/50048]	Loss: 4.8792
Training Epoch: 0 [1024/50048]	Loss: 4.7618
Training Epoch: 0 [1088/50048]	Loss: 4.8046
Training Epoch: 0 [1152/50048]	Loss: 4.7858
Training Epoch: 0 [1216/50048]	Loss: 4.7140
Training Epoch: 0 [1280/50048]	Loss: 4.6545
Training Epoch: 0 [1344/50048]	Loss: 4.7654
Training Epoch: 0 [1408/50048]	Loss: 4.5491
Training Epoch: 0 [1472/50048]	Loss: 4.5603
Training Epoch: 0 [1536/50048]	Loss: 4.6849
Training Epoch: 0 [1600/50048]	Loss: 4.8358
Training Epoch: 0 [1664/50048]	Loss: 4.8989
Training Epoch: 0 [1728/50048]	Loss: 4.7255
Training Epoch: 0 [1792/50048]	Loss: 4.7859
Training Epoch: 0 [1856/50048]	Loss: 4.8061
Training Epoch: 0 [1920/50048]	Loss: 4.6992
Training Epoch: 0 [1984/50048]	Loss: 4.4956
Training Epoch: 0 [2048/50048]	Loss: 4.5549
Training Epoch: 0 [2112/50048]	Loss: 4.6988
Training Epoch: 0 [2176/50048]	Loss: 4.6414
Training Epoch: 0 [2240/50048]	Loss: 4.6016
Training Epoch: 0 [2304/50048]	Loss: 4.6775
Training Epoch: 0 [2368/50048]	Loss: 4.6021
Training Epoch: 0 [2432/50048]	Loss: 4.5448
Training Epoch: 0 [2496/50048]	Loss: 4.5404
Training Epoch: 0 [2560/50048]	Loss: 4.5440
Training Epoch: 0 [2624/50048]	Loss: 4.4771
Training Epoch: 0 [2688/50048]	Loss: 4.5066
Training Epoch: 0 [2752/50048]	Loss: 4.5113
Training Epoch: 0 [2816/50048]	Loss: 4.4541
Training Epoch: 0 [2880/50048]	Loss: 4.4641
Training Epoch: 0 [2944/50048]	Loss: 4.4753
Training Epoch: 0 [3008/50048]	Loss: 4.4484
Training Epoch: 0 [3072/50048]	Loss: 4.6476
Training Epoch: 0 [3136/50048]	Loss: 4.7509
Training Epoch: 0 [3200/50048]	Loss: 4.5119
Profile done with power limit 125W
epoch 1 train time consumed: 7.78s
Validation Epoch: 0, Average loss: 0.0717, Accuracy: 0.0195
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 125000, 'lr': 0.0028284271247461905, 'energy': 123.69159510274207, 'time': 5.781752718000007, 'accuracy': 0.01950636942675159, 'total_cost': 70.75861680337147}
[run job] Launching job with BS 64: and LR: 0.0028284271247461905 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00283+pl100', 'ZEUS_COST_THRESH': '120.07926829815906', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0028284271247461905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0028284271247461905', '--power_limit', '100']
[run job] cost_ub=120.07926829815906
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00283+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0028284271247461905
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.0397
Training Epoch: 0 [192/50048]	Loss: 5.2126
Training Epoch: 0 [256/50048]	Loss: 5.3831
Training Epoch: 0 [320/50048]	Loss: 5.2575
Training Epoch: 0 [384/50048]	Loss: 5.2444
Training Epoch: 0 [448/50048]	Loss: 4.9882
Training Epoch: 0 [512/50048]	Loss: 5.1540
Training Epoch: 0 [576/50048]	Loss: 4.7165
Training Epoch: 0 [640/50048]	Loss: 5.4962
Training Epoch: 0 [704/50048]	Loss: 4.8137
Training Epoch: 0 [768/50048]	Loss: 5.0925
Training Epoch: 0 [832/50048]	Loss: 4.8533
Training Epoch: 0 [896/50048]	Loss: 4.9910
Training Epoch: 0 [960/50048]	Loss: 4.8852
Training Epoch: 0 [1024/50048]	Loss: 4.7606
Training Epoch: 0 [1088/50048]	Loss: 4.8256
Training Epoch: 0 [1152/50048]	Loss: 4.8182
Training Epoch: 0 [1216/50048]	Loss: 4.6964
Training Epoch: 0 [1280/50048]	Loss: 4.6442
Training Epoch: 0 [1344/50048]	Loss: 4.8518
Training Epoch: 0 [1408/50048]	Loss: 4.5640
Training Epoch: 0 [1472/50048]	Loss: 4.5739
Training Epoch: 0 [1536/50048]	Loss: 4.7661
Training Epoch: 0 [1600/50048]	Loss: 4.8996
Training Epoch: 0 [1664/50048]	Loss: 4.9392
Training Epoch: 0 [1728/50048]	Loss: 4.7699
Training Epoch: 0 [1792/50048]	Loss: 4.6250
Training Epoch: 0 [1856/50048]	Loss: 4.7846
Training Epoch: 0 [1920/50048]	Loss: 4.6413
Training Epoch: 0 [1984/50048]	Loss: 4.5364
Training Epoch: 0 [2048/50048]	Loss: 4.5926
Training Epoch: 0 [2112/50048]	Loss: 4.8533
Training Epoch: 0 [2176/50048]	Loss: 4.6837
Training Epoch: 0 [2240/50048]	Loss: 4.5639
Training Epoch: 0 [2304/50048]	Loss: 4.6431
Training Epoch: 0 [2368/50048]	Loss: 4.6062
Training Epoch: 0 [2432/50048]	Loss: 4.5457
Training Epoch: 0 [2496/50048]	Loss: 4.5848
Training Epoch: 0 [2560/50048]	Loss: 4.5468
Training Epoch: 0 [2624/50048]	Loss: 4.5216
Training Epoch: 0 [2688/50048]	Loss: 4.5286
Training Epoch: 0 [2752/50048]	Loss: 4.4996
Training Epoch: 0 [2816/50048]	Loss: 4.4255
Training Epoch: 0 [2880/50048]	Loss: 4.4230
Training Epoch: 0 [2944/50048]	Loss: 4.5964
Training Epoch: 0 [3008/50048]	Loss: 4.4972
Training Epoch: 0 [3072/50048]	Loss: 4.6018
Training Epoch: 0 [3136/50048]	Loss: 4.5973
Training Epoch: 0 [3200/50048]	Loss: 4.4656
Profile done with power limit 100W
epoch 1 train time consumed: 15.40s
Validation Epoch: 0, Average loss: 0.0772, Accuracy: 0.0201
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 100000, 'lr': 0.0028284271247461905, 'energy': 98.14576287360254, 'time': 11.975796710000168, 'accuracy': 0.020103503184713375, 'total_cost': 130.0470198664079}

[Power Profiler] with batch size 64 and learning rate 0.0033941125496954284
[run job] Launching job with BS 64: and LR: 0.0033941125496954284 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00339+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0033941125496954284', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0033941125496954284', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00339+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0033941125496954284
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.2218
Training Epoch: 0 [192/50048]	Loss: 5.4184
Training Epoch: 0 [256/50048]	Loss: 5.5501
Training Epoch: 0 [320/50048]	Loss: 5.3710
Training Epoch: 0 [384/50048]	Loss: 5.3690
Training Epoch: 0 [448/50048]	Loss: 5.0935
Training Epoch: 0 [512/50048]	Loss: 5.2191
Training Epoch: 0 [576/50048]	Loss: 4.7812
Training Epoch: 0 [640/50048]	Loss: 5.5477
Training Epoch: 0 [704/50048]	Loss: 4.7454
Training Epoch: 0 [768/50048]	Loss: 4.9937
Training Epoch: 0 [832/50048]	Loss: 4.7658
Training Epoch: 0 [896/50048]	Loss: 4.9378
Training Epoch: 0 [960/50048]	Loss: 4.8938
Training Epoch: 0 [1024/50048]	Loss: 4.7594
Training Epoch: 0 [1088/50048]	Loss: 4.8071
Training Epoch: 0 [1152/50048]	Loss: 4.9250
Training Epoch: 0 [1216/50048]	Loss: 4.8609
Training Epoch: 0 [1280/50048]	Loss: 4.6459
Training Epoch: 0 [1344/50048]	Loss: 4.7892
Training Epoch: 0 [1408/50048]	Loss: 4.5914
Training Epoch: 0 [1472/50048]	Loss: 4.6244
Training Epoch: 0 [1536/50048]	Loss: 4.7666
Training Epoch: 0 [1600/50048]	Loss: 4.8262
Training Epoch: 0 [1664/50048]	Loss: 4.8234
Training Epoch: 0 [1728/50048]	Loss: 4.6346
Training Epoch: 0 [1792/50048]	Loss: 4.5709
Training Epoch: 0 [1856/50048]	Loss: 4.7893
Training Epoch: 0 [1920/50048]	Loss: 4.6631
Training Epoch: 0 [1984/50048]	Loss: 4.5320
Training Epoch: 0 [2048/50048]	Loss: 4.5293
Training Epoch: 0 [2112/50048]	Loss: 4.7652
Training Epoch: 0 [2176/50048]	Loss: 4.6224
Training Epoch: 0 [2240/50048]	Loss: 4.5520
Training Epoch: 0 [2304/50048]	Loss: 4.5733
Training Epoch: 0 [2368/50048]	Loss: 4.6130
Training Epoch: 0 [2432/50048]	Loss: 4.5419
Training Epoch: 0 [2496/50048]	Loss: 4.5211
Training Epoch: 0 [2560/50048]	Loss: 4.5329
Training Epoch: 0 [2624/50048]	Loss: 4.4605
Training Epoch: 0 [2688/50048]	Loss: 4.5033
Training Epoch: 0 [2752/50048]	Loss: 4.4722
Training Epoch: 0 [2816/50048]	Loss: 4.4548
Training Epoch: 0 [2880/50048]	Loss: 4.4381
Training Epoch: 0 [2944/50048]	Loss: 4.6051
Training Epoch: 0 [3008/50048]	Loss: 4.4952
Training Epoch: 0 [3072/50048]	Loss: 4.6782
Training Epoch: 0 [3136/50048]	Loss: 4.6045
Training Epoch: 0 [3200/50048]	Loss: 4.4965
Profile done with power limit 175W
epoch 1 train time consumed: 7.22s
Validation Epoch: 0, Average loss: 0.0761, Accuracy: 0.0218
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 175000, 'lr': 0.0033941125496954284, 'energy': 140.74987937560394, 'time': 5.305631545000097, 'accuracy': 0.021795382165605094, 'total_cost': 61.43120804731883}
[run job] Launching job with BS 64: and LR: 0.0033941125496954284 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00339+pl150', 'ZEUS_COST_THRESH': '122.86241609463767', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0033941125496954284', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0033941125496954284', '--power_limit', '150']
[run job] cost_ub=122.86241609463767
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00339+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0033941125496954284
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.2218
Training Epoch: 0 [192/50048]	Loss: 5.4181
Training Epoch: 0 [256/50048]	Loss: 5.5603
Training Epoch: 0 [320/50048]	Loss: 5.3546
Training Epoch: 0 [384/50048]	Loss: 5.3643
Training Epoch: 0 [448/50048]	Loss: 5.0926
Training Epoch: 0 [512/50048]	Loss: 5.2615
Training Epoch: 0 [576/50048]	Loss: 4.8009
Training Epoch: 0 [640/50048]	Loss: 5.5603
Training Epoch: 0 [704/50048]	Loss: 4.8436
Training Epoch: 0 [768/50048]	Loss: 5.0301
Training Epoch: 0 [832/50048]	Loss: 4.7829
Training Epoch: 0 [896/50048]	Loss: 4.9155
Training Epoch: 0 [960/50048]	Loss: 4.8873
Training Epoch: 0 [1024/50048]	Loss: 4.7595
Training Epoch: 0 [1088/50048]	Loss: 4.8298
Training Epoch: 0 [1152/50048]	Loss: 4.9167
Training Epoch: 0 [1216/50048]	Loss: 4.8482
Training Epoch: 0 [1280/50048]	Loss: 4.7812
Training Epoch: 0 [1344/50048]	Loss: 4.7781
Training Epoch: 0 [1408/50048]	Loss: 4.5623
Training Epoch: 0 [1472/50048]	Loss: 4.5772
Training Epoch: 0 [1536/50048]	Loss: 4.6652
Training Epoch: 0 [1600/50048]	Loss: 4.7381
Training Epoch: 0 [1664/50048]	Loss: 4.8903
Training Epoch: 0 [1728/50048]	Loss: 4.8178
Training Epoch: 0 [1792/50048]	Loss: 4.7260
Training Epoch: 0 [1856/50048]	Loss: 4.7568
Training Epoch: 0 [1920/50048]	Loss: 4.7508
Training Epoch: 0 [1984/50048]	Loss: 4.6399
Training Epoch: 0 [2048/50048]	Loss: 4.5696
Training Epoch: 0 [2112/50048]	Loss: 4.5657
Training Epoch: 0 [2176/50048]	Loss: 4.5650
Training Epoch: 0 [2240/50048]	Loss: 4.6279
Training Epoch: 0 [2304/50048]	Loss: 4.6023
Training Epoch: 0 [2368/50048]	Loss: 4.6228
Training Epoch: 0 [2432/50048]	Loss: 4.7562
Training Epoch: 0 [2496/50048]	Loss: 4.7337
Training Epoch: 0 [2560/50048]	Loss: 4.4908
Training Epoch: 0 [2624/50048]	Loss: 4.5546
Training Epoch: 0 [2688/50048]	Loss: 4.5569
Training Epoch: 0 [2752/50048]	Loss: 4.5586
Training Epoch: 0 [2816/50048]	Loss: 4.6062
Training Epoch: 0 [2880/50048]	Loss: 4.4494
Training Epoch: 0 [2944/50048]	Loss: 4.6425
Training Epoch: 0 [3008/50048]	Loss: 4.6150
Training Epoch: 0 [3072/50048]	Loss: 4.6120
Training Epoch: 0 [3136/50048]	Loss: 4.5531
Training Epoch: 0 [3200/50048]	Loss: 4.6666
Profile done with power limit 150W
epoch 1 train time consumed: 7.20s
Validation Epoch: 0, Average loss: 0.0777, Accuracy: 0.0224
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 150000, 'lr': 0.0033941125496954284, 'energy': 141.1678749675045, 'time': 5.279714154999965, 'accuracy': 0.022392515923566877, 'total_cost': 59.57972895870061}
[run job] Launching job with BS 64: and LR: 0.0033941125496954284 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00339+pl125', 'ZEUS_COST_THRESH': '119.15945791740123', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0033941125496954284', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0033941125496954284', '--power_limit', '125']
[run job] cost_ub=119.15945791740123
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00339+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0033941125496954284
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.2218
Training Epoch: 0 [192/50048]	Loss: 5.4177
Training Epoch: 0 [256/50048]	Loss: 5.5550
Training Epoch: 0 [320/50048]	Loss: 5.3950
Training Epoch: 0 [384/50048]	Loss: 5.3464
Training Epoch: 0 [448/50048]	Loss: 5.1297
Training Epoch: 0 [512/50048]	Loss: 5.2820
Training Epoch: 0 [576/50048]	Loss: 4.7845
Training Epoch: 0 [640/50048]	Loss: 5.5815
Training Epoch: 0 [704/50048]	Loss: 4.9238
Training Epoch: 0 [768/50048]	Loss: 5.1106
Training Epoch: 0 [832/50048]	Loss: 4.8492
Training Epoch: 0 [896/50048]	Loss: 4.9888
Training Epoch: 0 [960/50048]	Loss: 4.6780
Training Epoch: 0 [1024/50048]	Loss: 4.7761
Training Epoch: 0 [1088/50048]	Loss: 4.7989
Training Epoch: 0 [1152/50048]	Loss: 4.7831
Training Epoch: 0 [1216/50048]	Loss: 4.6033
Training Epoch: 0 [1280/50048]	Loss: 4.6067
Training Epoch: 0 [1344/50048]	Loss: 4.7552
Training Epoch: 0 [1408/50048]	Loss: 4.5619
Training Epoch: 0 [1472/50048]	Loss: 4.6847
Training Epoch: 0 [1536/50048]	Loss: 4.8291
Training Epoch: 0 [1600/50048]	Loss: 4.9000
Training Epoch: 0 [1664/50048]	Loss: 4.8095
Training Epoch: 0 [1728/50048]	Loss: 4.6152
Training Epoch: 0 [1792/50048]	Loss: 4.5652
Training Epoch: 0 [1856/50048]	Loss: 4.7614
Training Epoch: 0 [1920/50048]	Loss: 4.5599
Training Epoch: 0 [1984/50048]	Loss: 4.5344
Training Epoch: 0 [2048/50048]	Loss: 4.5731
Training Epoch: 0 [2112/50048]	Loss: 4.6757
Training Epoch: 0 [2176/50048]	Loss: 4.6512
Training Epoch: 0 [2240/50048]	Loss: 4.6132
Training Epoch: 0 [2304/50048]	Loss: 4.5938
Training Epoch: 0 [2368/50048]	Loss: 4.6195
Training Epoch: 0 [2432/50048]	Loss: 4.5265
Training Epoch: 0 [2496/50048]	Loss: 4.6227
Training Epoch: 0 [2560/50048]	Loss: 4.5743
Training Epoch: 0 [2624/50048]	Loss: 4.5137
Training Epoch: 0 [2688/50048]	Loss: 4.4815
Training Epoch: 0 [2752/50048]	Loss: 4.5685
Training Epoch: 0 [2816/50048]	Loss: 4.5003
Training Epoch: 0 [2880/50048]	Loss: 4.4527
Training Epoch: 0 [2944/50048]	Loss: 4.5153
Training Epoch: 0 [3008/50048]	Loss: 4.5069
Training Epoch: 0 [3072/50048]	Loss: 4.6850
Training Epoch: 0 [3136/50048]	Loss: 4.6303
Training Epoch: 0 [3200/50048]	Loss: 4.4729
Profile done with power limit 125W
epoch 1 train time consumed: 7.76s
Validation Epoch: 0, Average loss: 0.0731, Accuracy: 0.0179
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 125000, 'lr': 0.0033941125496954284, 'energy': 123.51617684760323, 'time': 5.766914885000006, 'accuracy': 0.017914012738853503, 'total_cost': 76.80540756950575}
[run job] Launching job with BS 64: and LR: 0.0033941125496954284 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs64+lr0.00339+pl100', 'ZEUS_COST_THRESH': '119.15945791740123', 'ZEUS_BATCH_SIZE': '64', 'ZEUS_LEARNING_RATE': '0.0033941125496954284', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '64', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0033941125496954284', '--power_limit', '100']
[run job] cost_ub=119.15945791740123
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs64+lr0.00339+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0033941125496954284
batch size arg: 64
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [64/50048]	Loss: 4.6943
Training Epoch: 0 [128/50048]	Loss: 5.2218
Training Epoch: 0 [192/50048]	Loss: 5.4167
Training Epoch: 0 [256/50048]	Loss: 5.5685
Training Epoch: 0 [320/50048]	Loss: 5.3550
Training Epoch: 0 [384/50048]	Loss: 5.3746
Training Epoch: 0 [448/50048]	Loss: 5.1152
Training Epoch: 0 [512/50048]	Loss: 5.3052
Training Epoch: 0 [576/50048]	Loss: 4.8136
Training Epoch: 0 [640/50048]	Loss: 5.5499
Training Epoch: 0 [704/50048]	Loss: 4.8109
Training Epoch: 0 [768/50048]	Loss: 5.1615
Training Epoch: 0 [832/50048]	Loss: 4.9216
Training Epoch: 0 [896/50048]	Loss: 4.9028
Training Epoch: 0 [960/50048]	Loss: 4.8241
Training Epoch: 0 [1024/50048]	Loss: 4.7053
Training Epoch: 0 [1088/50048]	Loss: 4.7599
Training Epoch: 0 [1152/50048]	Loss: 4.9172
Training Epoch: 0 [1216/50048]	Loss: 4.7836
Training Epoch: 0 [1280/50048]	Loss: 4.6739
Training Epoch: 0 [1344/50048]	Loss: 4.7005
Training Epoch: 0 [1408/50048]	Loss: 4.6063
Training Epoch: 0 [1472/50048]	Loss: 4.6280
Training Epoch: 0 [1536/50048]	Loss: 4.8382
Training Epoch: 0 [1600/50048]	Loss: 4.6066
Training Epoch: 0 [1664/50048]	Loss: 4.6579
Training Epoch: 0 [1728/50048]	Loss: 4.7082
Training Epoch: 0 [1792/50048]	Loss: 4.7550
Training Epoch: 0 [1856/50048]	Loss: 4.6376
Training Epoch: 0 [1920/50048]	Loss: 4.6898
Training Epoch: 0 [1984/50048]	Loss: 4.7392
Training Epoch: 0 [2048/50048]	Loss: 4.6960
Training Epoch: 0 [2112/50048]	Loss: 4.7218
Training Epoch: 0 [2176/50048]	Loss: 4.5745
Training Epoch: 0 [2240/50048]	Loss: 4.6268
Training Epoch: 0 [2304/50048]	Loss: 4.6454
Training Epoch: 0 [2368/50048]	Loss: 4.6234
Training Epoch: 0 [2432/50048]	Loss: 4.6787
Training Epoch: 0 [2496/50048]	Loss: 4.6659
Training Epoch: 0 [2560/50048]	Loss: 4.4779
Training Epoch: 0 [2624/50048]	Loss: 4.5168
Training Epoch: 0 [2688/50048]	Loss: 4.5477
Training Epoch: 0 [2752/50048]	Loss: 4.5398
Training Epoch: 0 [2816/50048]	Loss: 4.5365
Training Epoch: 0 [2880/50048]	Loss: 4.4808
Training Epoch: 0 [2944/50048]	Loss: 4.5900
Training Epoch: 0 [3008/50048]	Loss: 4.5731
Training Epoch: 0 [3072/50048]	Loss: 4.6239
Training Epoch: 0 [3136/50048]	Loss: 4.5080
Training Epoch: 0 [3200/50048]	Loss: 4.5551
Profile done with power limit 100W
epoch 1 train time consumed: 15.93s
Validation Epoch: 0, Average loss: 0.0805, Accuracy: 0.0219
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 64, 'pl': 100000, 'lr': 0.0033941125496954284, 'energy': 98.54980060823367, 'time': 12.529872716, 'accuracy': 0.021894904458598725, 'total_cost': 125.1161210760313}

[Power Profiler] with batch size 128 and learning rate 0.0032
[run job] Launching job with BS 128: and LR: 0.0032 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00320+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0032', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0032', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00320+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0032
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.1136
Training Epoch: 0 [384/50048]	Loss: 5.3871
Training Epoch: 0 [512/50048]	Loss: 5.1004
Training Epoch: 0 [640/50048]	Loss: 5.0185
Training Epoch: 0 [768/50048]	Loss: 5.0069
Training Epoch: 0 [896/50048]	Loss: 5.0198
Training Epoch: 0 [1024/50048]	Loss: 5.0334
Training Epoch: 0 [1152/50048]	Loss: 4.9382
Training Epoch: 0 [1280/50048]	Loss: 4.9058
Training Epoch: 0 [1408/50048]	Loss: 4.6542
Training Epoch: 0 [1536/50048]	Loss: 4.8000
Training Epoch: 0 [1664/50048]	Loss: 4.8888
Training Epoch: 0 [1792/50048]	Loss: 4.5646
Training Epoch: 0 [1920/50048]	Loss: 4.7467
Training Epoch: 0 [2048/50048]	Loss: 4.5231
Training Epoch: 0 [2176/50048]	Loss: 4.7324
Training Epoch: 0 [2304/50048]	Loss: 4.6746
Training Epoch: 0 [2432/50048]	Loss: 4.6012
Training Epoch: 0 [2560/50048]	Loss: 4.6345
Training Epoch: 0 [2688/50048]	Loss: 4.5649
Training Epoch: 0 [2816/50048]	Loss: 4.6510
Training Epoch: 0 [2944/50048]	Loss: 4.5185
Training Epoch: 0 [3072/50048]	Loss: 4.6179
Training Epoch: 0 [3200/50048]	Loss: 4.6671
Training Epoch: 0 [3328/50048]	Loss: 4.6101
Training Epoch: 0 [3456/50048]	Loss: 4.6178
Training Epoch: 0 [3584/50048]	Loss: 4.5477
Training Epoch: 0 [3712/50048]	Loss: 4.7712
Training Epoch: 0 [3840/50048]	Loss: 4.5584
Training Epoch: 0 [3968/50048]	Loss: 4.5547
Training Epoch: 0 [4096/50048]	Loss: 4.7743
Training Epoch: 0 [4224/50048]	Loss: 4.5383
Training Epoch: 0 [4352/50048]	Loss: 4.4705
Training Epoch: 0 [4480/50048]	Loss: 4.4631
Training Epoch: 0 [4608/50048]	Loss: 4.5654
Training Epoch: 0 [4736/50048]	Loss: 4.4325
Training Epoch: 0 [4864/50048]	Loss: 4.5172
Training Epoch: 0 [4992/50048]	Loss: 4.4406
Training Epoch: 0 [5120/50048]	Loss: 4.4732
Training Epoch: 0 [5248/50048]	Loss: 4.4533
Training Epoch: 0 [5376/50048]	Loss: 4.4910
Training Epoch: 0 [5504/50048]	Loss: 4.5196
Training Epoch: 0 [5632/50048]	Loss: 4.4712
Training Epoch: 0 [5760/50048]	Loss: 4.4083
Training Epoch: 0 [5888/50048]	Loss: 4.3415
Training Epoch: 0 [6016/50048]	Loss: 4.3430
Training Epoch: 0 [6144/50048]	Loss: 4.2032
Training Epoch: 0 [6272/50048]	Loss: 4.5572
Training Epoch: 0 [6400/50048]	Loss: 4.2956
Profile done with power limit 175W
epoch 1 train time consumed: 11.45s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0317
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 175000, 'lr': 0.0032, 'energy': 144.27467077667583, 'time': 8.646741140000131, 'accuracy': 0.03174446202531646, 'total_cost': 139.0119744280006}
[run job] Launching job with BS 128: and LR: 0.0032 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00320+pl150', 'ZEUS_COST_THRESH': '278.0239488560012', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0032', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0032', '--power_limit', '150']
[run job] cost_ub=278.0239488560012
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00320+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0032
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.1136
Training Epoch: 0 [384/50048]	Loss: 5.3868
Training Epoch: 0 [512/50048]	Loss: 5.1035
Training Epoch: 0 [640/50048]	Loss: 5.0029
Training Epoch: 0 [768/50048]	Loss: 5.0131
Training Epoch: 0 [896/50048]	Loss: 5.0196
Training Epoch: 0 [1024/50048]	Loss: 5.0350
Training Epoch: 0 [1152/50048]	Loss: 4.9148
Training Epoch: 0 [1280/50048]	Loss: 4.9125
Training Epoch: 0 [1408/50048]	Loss: 4.6452
Training Epoch: 0 [1536/50048]	Loss: 4.8031
Training Epoch: 0 [1664/50048]	Loss: 4.8611
Training Epoch: 0 [1792/50048]	Loss: 4.5580
Training Epoch: 0 [1920/50048]	Loss: 4.6181
Training Epoch: 0 [2048/50048]	Loss: 4.5382
Training Epoch: 0 [2176/50048]	Loss: 4.7083
Training Epoch: 0 [2304/50048]	Loss: 4.6279
Training Epoch: 0 [2432/50048]	Loss: 4.5777
Training Epoch: 0 [2560/50048]	Loss: 4.5985
Training Epoch: 0 [2688/50048]	Loss: 4.5834
Training Epoch: 0 [2816/50048]	Loss: 4.6709
Training Epoch: 0 [2944/50048]	Loss: 4.4979
Training Epoch: 0 [3072/50048]	Loss: 4.5723
Training Epoch: 0 [3200/50048]	Loss: 4.6578
Training Epoch: 0 [3328/50048]	Loss: 4.5690
Training Epoch: 0 [3456/50048]	Loss: 4.5198
Training Epoch: 0 [3584/50048]	Loss: 4.5066
Training Epoch: 0 [3712/50048]	Loss: 4.8584
Training Epoch: 0 [3840/50048]	Loss: 4.5459
Training Epoch: 0 [3968/50048]	Loss: 4.5824
Training Epoch: 0 [4096/50048]	Loss: 4.7221
Training Epoch: 0 [4224/50048]	Loss: 4.5021
Training Epoch: 0 [4352/50048]	Loss: 4.4547
Training Epoch: 0 [4480/50048]	Loss: 4.4272
Training Epoch: 0 [4608/50048]	Loss: 4.4429
Training Epoch: 0 [4736/50048]	Loss: 4.3510
Training Epoch: 0 [4864/50048]	Loss: 4.4511
Training Epoch: 0 [4992/50048]	Loss: 4.4035
Training Epoch: 0 [5120/50048]	Loss: 4.4361
Training Epoch: 0 [5248/50048]	Loss: 4.3678
Training Epoch: 0 [5376/50048]	Loss: 4.3346
Training Epoch: 0 [5504/50048]	Loss: 4.4054
Training Epoch: 0 [5632/50048]	Loss: 4.4727
Training Epoch: 0 [5760/50048]	Loss: 4.3587
Training Epoch: 0 [5888/50048]	Loss: 4.2340
Training Epoch: 0 [6016/50048]	Loss: 4.2458
Training Epoch: 0 [6144/50048]	Loss: 4.1001
Training Epoch: 0 [6272/50048]	Loss: 4.4386
Training Epoch: 0 [6400/50048]	Loss: 4.2754
Profile done with power limit 150W
epoch 1 train time consumed: 11.50s
Validation Epoch: 0, Average loss: 0.0356, Accuracy: 0.0380
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 150000, 'lr': 0.0032, 'energy': 144.20649641552933, 'time': 8.663041862, 'accuracy': 0.0379746835443038, 'total_cost': 116.39953114052462}
[run job] Launching job with BS 128: and LR: 0.0032 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00320+pl125', 'ZEUS_COST_THRESH': '232.79906228104923', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0032', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0032', '--power_limit', '125']
[run job] cost_ub=232.79906228104923
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00320+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0032
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.1136
Training Epoch: 0 [384/50048]	Loss: 5.3867
Training Epoch: 0 [512/50048]	Loss: 5.1025
Training Epoch: 0 [640/50048]	Loss: 4.9993
Training Epoch: 0 [768/50048]	Loss: 4.9941
Training Epoch: 0 [896/50048]	Loss: 5.0424
Training Epoch: 0 [1024/50048]	Loss: 5.0406
Training Epoch: 0 [1152/50048]	Loss: 4.9194
Training Epoch: 0 [1280/50048]	Loss: 4.9159
Training Epoch: 0 [1408/50048]	Loss: 4.6673
Training Epoch: 0 [1536/50048]	Loss: 4.7744
Training Epoch: 0 [1664/50048]	Loss: 4.8255
Training Epoch: 0 [1792/50048]	Loss: 4.5690
Training Epoch: 0 [1920/50048]	Loss: 4.7906
Training Epoch: 0 [2048/50048]	Loss: 4.5259
Training Epoch: 0 [2176/50048]	Loss: 4.7359
Training Epoch: 0 [2304/50048]	Loss: 4.6263
Training Epoch: 0 [2432/50048]	Loss: 4.5932
Training Epoch: 0 [2560/50048]	Loss: 4.6383
Training Epoch: 0 [2688/50048]	Loss: 4.5812
Training Epoch: 0 [2816/50048]	Loss: 4.6411
Training Epoch: 0 [2944/50048]	Loss: 4.4657
Training Epoch: 0 [3072/50048]	Loss: 4.5780
Training Epoch: 0 [3200/50048]	Loss: 4.5867
Training Epoch: 0 [3328/50048]	Loss: 4.5309
Training Epoch: 0 [3456/50048]	Loss: 4.5216
Training Epoch: 0 [3584/50048]	Loss: 4.5071
Training Epoch: 0 [3712/50048]	Loss: 4.7382
Training Epoch: 0 [3840/50048]	Loss: 4.5292
Training Epoch: 0 [3968/50048]	Loss: 4.5469
Training Epoch: 0 [4096/50048]	Loss: 4.7207
Training Epoch: 0 [4224/50048]	Loss: 4.5102
Training Epoch: 0 [4352/50048]	Loss: 4.4266
Training Epoch: 0 [4480/50048]	Loss: 4.4152
Training Epoch: 0 [4608/50048]	Loss: 4.5079
Training Epoch: 0 [4736/50048]	Loss: 4.3792
Training Epoch: 0 [4864/50048]	Loss: 4.4601
Training Epoch: 0 [4992/50048]	Loss: 4.3616
Training Epoch: 0 [5120/50048]	Loss: 4.4151
Training Epoch: 0 [5248/50048]	Loss: 4.3994
Training Epoch: 0 [5376/50048]	Loss: 4.3859
Training Epoch: 0 [5504/50048]	Loss: 4.4371
Training Epoch: 0 [5632/50048]	Loss: 4.4567
Training Epoch: 0 [5760/50048]	Loss: 4.4165
Training Epoch: 0 [5888/50048]	Loss: 4.2447
Training Epoch: 0 [6016/50048]	Loss: 4.2817
Training Epoch: 0 [6144/50048]	Loss: 4.1698
Training Epoch: 0 [6272/50048]	Loss: 4.4651
Training Epoch: 0 [6400/50048]	Loss: 4.3247
Profile done with power limit 125W
epoch 1 train time consumed: 12.68s
Validation Epoch: 0, Average loss: 0.0366, Accuracy: 0.0374
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 125000, 'lr': 0.0032, 'energy': 123.11963137299509, 'time': 9.672366398999884, 'accuracy': 0.03738132911392405, 'total_cost': 123.30248697523743}
[run job] Launching job with BS 128: and LR: 0.0032 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00320+pl100', 'ZEUS_COST_THRESH': '232.79906228104923', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0032', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0032', '--power_limit', '100']
[run job] cost_ub=232.79906228104923
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00320+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0032
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.1136
Training Epoch: 0 [384/50048]	Loss: 5.3869
Training Epoch: 0 [512/50048]	Loss: 5.0968
Training Epoch: 0 [640/50048]	Loss: 4.9926
Training Epoch: 0 [768/50048]	Loss: 5.0166
Training Epoch: 0 [896/50048]	Loss: 5.0247
Training Epoch: 0 [1024/50048]	Loss: 5.0285
Training Epoch: 0 [1152/50048]	Loss: 4.9457
Training Epoch: 0 [1280/50048]	Loss: 4.9210
Training Epoch: 0 [1408/50048]	Loss: 4.6803
Training Epoch: 0 [1536/50048]	Loss: 4.8011
Training Epoch: 0 [1664/50048]	Loss: 4.7857
Training Epoch: 0 [1792/50048]	Loss: 4.5776
Training Epoch: 0 [1920/50048]	Loss: 4.7798
Training Epoch: 0 [2048/50048]	Loss: 4.5175
Training Epoch: 0 [2176/50048]	Loss: 4.7198
Training Epoch: 0 [2304/50048]	Loss: 4.6645
Training Epoch: 0 [2432/50048]	Loss: 4.5906
Training Epoch: 0 [2560/50048]	Loss: 4.6116
Training Epoch: 0 [2688/50048]	Loss: 4.5767
Training Epoch: 0 [2816/50048]	Loss: 4.5644
Training Epoch: 0 [2944/50048]	Loss: 4.4993
Training Epoch: 0 [3072/50048]	Loss: 4.6241
Training Epoch: 0 [3200/50048]	Loss: 4.5977
Training Epoch: 0 [3328/50048]	Loss: 4.6169
Training Epoch: 0 [3456/50048]	Loss: 4.5060
Training Epoch: 0 [3584/50048]	Loss: 4.5519
Training Epoch: 0 [3712/50048]	Loss: 4.7844
Training Epoch: 0 [3840/50048]	Loss: 4.5323
Training Epoch: 0 [3968/50048]	Loss: 4.5361
Training Epoch: 0 [4096/50048]	Loss: 4.6324
Training Epoch: 0 [4224/50048]	Loss: 4.5747
Training Epoch: 0 [4352/50048]	Loss: 4.4678
Training Epoch: 0 [4480/50048]	Loss: 4.4384
Training Epoch: 0 [4608/50048]	Loss: 4.4850
Training Epoch: 0 [4736/50048]	Loss: 4.4229
Training Epoch: 0 [4864/50048]	Loss: 4.5029
Training Epoch: 0 [4992/50048]	Loss: 4.4284
Training Epoch: 0 [5120/50048]	Loss: 4.4538
Training Epoch: 0 [5248/50048]	Loss: 4.4549
Training Epoch: 0 [5376/50048]	Loss: 4.4017
Training Epoch: 0 [5504/50048]	Loss: 4.4127
Training Epoch: 0 [5632/50048]	Loss: 4.4150
Training Epoch: 0 [5760/50048]	Loss: 4.3885
Training Epoch: 0 [5888/50048]	Loss: 4.2566
Training Epoch: 0 [6016/50048]	Loss: 4.2581
Training Epoch: 0 [6144/50048]	Loss: 4.1199
Training Epoch: 0 [6272/50048]	Loss: 4.4822
Training Epoch: 0 [6400/50048]	Loss: 4.2965
Profile done with power limit 100W
epoch 1 train time consumed: 26.47s
Validation Epoch: 0, Average loss: 0.0384, Accuracy: 0.0340
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 100000, 'lr': 0.0032, 'energy': 98.91903479683887, 'time': 20.795609867000167, 'accuracy': 0.03401898734177215, 'total_cost': 267.65524132495335}

[Power Profiler] with batch size 128 and learning rate 0.004
[run job] Launching job with BS 128: and LR: 0.004 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00400+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.004', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00400+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.004
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.3519
Training Epoch: 0 [384/50048]	Loss: 5.6626
Training Epoch: 0 [512/50048]	Loss: 5.2132
Training Epoch: 0 [640/50048]	Loss: 5.0113
Training Epoch: 0 [768/50048]	Loss: 4.9935
Training Epoch: 0 [896/50048]	Loss: 5.0356
Training Epoch: 0 [1024/50048]	Loss: 4.9137
Training Epoch: 0 [1152/50048]	Loss: 4.8182
Training Epoch: 0 [1280/50048]	Loss: 4.7956
Training Epoch: 0 [1408/50048]	Loss: 4.6266
Training Epoch: 0 [1536/50048]	Loss: 4.8079
Training Epoch: 0 [1664/50048]	Loss: 4.9905
Training Epoch: 0 [1792/50048]	Loss: 4.5668
Training Epoch: 0 [1920/50048]	Loss: 4.7321
Training Epoch: 0 [2048/50048]	Loss: 4.5303
Training Epoch: 0 [2176/50048]	Loss: 4.6990
Training Epoch: 0 [2304/50048]	Loss: 4.6218
Training Epoch: 0 [2432/50048]	Loss: 4.5363
Training Epoch: 0 [2560/50048]	Loss: 4.5756
Training Epoch: 0 [2688/50048]	Loss: 4.5737
Training Epoch: 0 [2816/50048]	Loss: 4.5966
Training Epoch: 0 [2944/50048]	Loss: 4.5517
Training Epoch: 0 [3072/50048]	Loss: 4.6213
Training Epoch: 0 [3200/50048]	Loss: 4.6115
Training Epoch: 0 [3328/50048]	Loss: 4.5520
Training Epoch: 0 [3456/50048]	Loss: 4.5044
Training Epoch: 0 [3584/50048]	Loss: 4.5485
Training Epoch: 0 [3712/50048]	Loss: 4.7830
Training Epoch: 0 [3840/50048]	Loss: 4.6125
Training Epoch: 0 [3968/50048]	Loss: 4.5675
Training Epoch: 0 [4096/50048]	Loss: 4.6058
Training Epoch: 0 [4224/50048]	Loss: 4.5702
Training Epoch: 0 [4352/50048]	Loss: 4.5737
Training Epoch: 0 [4480/50048]	Loss: 4.5039
Training Epoch: 0 [4608/50048]	Loss: 4.5159
Training Epoch: 0 [4736/50048]	Loss: 4.4764
Training Epoch: 0 [4864/50048]	Loss: 4.4904
Training Epoch: 0 [4992/50048]	Loss: 4.4667
Training Epoch: 0 [5120/50048]	Loss: 4.5099
Training Epoch: 0 [5248/50048]	Loss: 4.4450
Training Epoch: 0 [5376/50048]	Loss: 4.4872
Training Epoch: 0 [5504/50048]	Loss: 4.4766
Training Epoch: 0 [5632/50048]	Loss: 4.4852
Training Epoch: 0 [5760/50048]	Loss: 4.4360
Training Epoch: 0 [5888/50048]	Loss: 4.3945
Training Epoch: 0 [6016/50048]	Loss: 4.3530
Training Epoch: 0 [6144/50048]	Loss: 4.2236
Training Epoch: 0 [6272/50048]	Loss: 4.6143
Training Epoch: 0 [6400/50048]	Loss: 4.3865
Profile done with power limit 175W
epoch 1 train time consumed: 11.52s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0313
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 175000, 'lr': 0.004, 'energy': 144.19208573122665, 'time': 8.66300266799999, 'accuracy': 0.03134889240506329, 'total_cost': 140.99432133620576}
[run job] Launching job with BS 128: and LR: 0.004 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00400+pl150', 'ZEUS_COST_THRESH': '281.9886426724115', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.004', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004', '--power_limit', '150']
[run job] cost_ub=281.9886426724115
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00400+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.004
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.3519
Training Epoch: 0 [384/50048]	Loss: 5.6623
Training Epoch: 0 [512/50048]	Loss: 5.2282
Training Epoch: 0 [640/50048]	Loss: 5.0021
Training Epoch: 0 [768/50048]	Loss: 4.9892
Training Epoch: 0 [896/50048]	Loss: 5.0424
Training Epoch: 0 [1024/50048]	Loss: 4.9238
Training Epoch: 0 [1152/50048]	Loss: 4.8086
Training Epoch: 0 [1280/50048]	Loss: 4.7904
Training Epoch: 0 [1408/50048]	Loss: 4.6364
Training Epoch: 0 [1536/50048]	Loss: 4.8688
Training Epoch: 0 [1664/50048]	Loss: 5.0706
Training Epoch: 0 [1792/50048]	Loss: 4.5736
Training Epoch: 0 [1920/50048]	Loss: 4.7634
Training Epoch: 0 [2048/50048]	Loss: 4.5071
Training Epoch: 0 [2176/50048]	Loss: 4.6944
Training Epoch: 0 [2304/50048]	Loss: 4.6031
Training Epoch: 0 [2432/50048]	Loss: 4.5304
Training Epoch: 0 [2560/50048]	Loss: 4.5882
Training Epoch: 0 [2688/50048]	Loss: 4.5999
Training Epoch: 0 [2816/50048]	Loss: 4.5177
Training Epoch: 0 [2944/50048]	Loss: 4.5453
Training Epoch: 0 [3072/50048]	Loss: 4.7048
Training Epoch: 0 [3200/50048]	Loss: 4.6087
Training Epoch: 0 [3328/50048]	Loss: 4.5277
Training Epoch: 0 [3456/50048]	Loss: 4.4836
Training Epoch: 0 [3584/50048]	Loss: 4.5390
Training Epoch: 0 [3712/50048]	Loss: 4.7364
Training Epoch: 0 [3840/50048]	Loss: 4.5710
Training Epoch: 0 [3968/50048]	Loss: 4.5567
Training Epoch: 0 [4096/50048]	Loss: 4.6002
Training Epoch: 0 [4224/50048]	Loss: 4.5765
Training Epoch: 0 [4352/50048]	Loss: 4.5236
Training Epoch: 0 [4480/50048]	Loss: 4.4496
Training Epoch: 0 [4608/50048]	Loss: 4.4358
Training Epoch: 0 [4736/50048]	Loss: 4.3835
Training Epoch: 0 [4864/50048]	Loss: 4.4587
Training Epoch: 0 [4992/50048]	Loss: 4.3962
Training Epoch: 0 [5120/50048]	Loss: 4.4374
Training Epoch: 0 [5248/50048]	Loss: 4.4345
Training Epoch: 0 [5376/50048]	Loss: 4.3611
Training Epoch: 0 [5504/50048]	Loss: 4.4365
Training Epoch: 0 [5632/50048]	Loss: 4.4575
Training Epoch: 0 [5760/50048]	Loss: 4.3989
Training Epoch: 0 [5888/50048]	Loss: 4.3308
Training Epoch: 0 [6016/50048]	Loss: 4.2647
Training Epoch: 0 [6144/50048]	Loss: 4.2096
Training Epoch: 0 [6272/50048]	Loss: 4.4737
Training Epoch: 0 [6400/50048]	Loss: 4.3258
Profile done with power limit 150W
epoch 1 train time consumed: 11.49s
Validation Epoch: 0, Average loss: 0.0376, Accuracy: 0.0297
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 150000, 'lr': 0.004, 'energy': 144.4445676085219, 'time': 8.66324461399995, 'accuracy': 0.02966772151898734, 'total_cost': 149.10601054027592}
[run job] Launching job with BS 128: and LR: 0.004 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00400+pl125', 'ZEUS_COST_THRESH': '281.9886426724115', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.004', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004', '--power_limit', '125']
[run job] cost_ub=281.9886426724115
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00400+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.004
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.3519
Training Epoch: 0 [384/50048]	Loss: 5.6625
Training Epoch: 0 [512/50048]	Loss: 5.2269
Training Epoch: 0 [640/50048]	Loss: 5.0138
Training Epoch: 0 [768/50048]	Loss: 4.9815
Training Epoch: 0 [896/50048]	Loss: 5.0146
Training Epoch: 0 [1024/50048]	Loss: 4.8886
Training Epoch: 0 [1152/50048]	Loss: 4.8235
Training Epoch: 0 [1280/50048]	Loss: 4.7840
Training Epoch: 0 [1408/50048]	Loss: 4.6241
Training Epoch: 0 [1536/50048]	Loss: 4.8228
Training Epoch: 0 [1664/50048]	Loss: 4.9528
Training Epoch: 0 [1792/50048]	Loss: 4.5830
Training Epoch: 0 [1920/50048]	Loss: 4.7291
Training Epoch: 0 [2048/50048]	Loss: 4.5251
Training Epoch: 0 [2176/50048]	Loss: 4.6836
Training Epoch: 0 [2304/50048]	Loss: 4.6192
Training Epoch: 0 [2432/50048]	Loss: 4.5506
Training Epoch: 0 [2560/50048]	Loss: 4.6032
Training Epoch: 0 [2688/50048]	Loss: 4.5427
Training Epoch: 0 [2816/50048]	Loss: 4.6182
Training Epoch: 0 [2944/50048]	Loss: 4.5361
Training Epoch: 0 [3072/50048]	Loss: 4.5827
Training Epoch: 0 [3200/50048]	Loss: 4.5911
Training Epoch: 0 [3328/50048]	Loss: 4.5370
Training Epoch: 0 [3456/50048]	Loss: 4.5084
Training Epoch: 0 [3584/50048]	Loss: 4.5437
Training Epoch: 0 [3712/50048]	Loss: 4.8162
Training Epoch: 0 [3840/50048]	Loss: 4.5868
Training Epoch: 0 [3968/50048]	Loss: 4.5587
Training Epoch: 0 [4096/50048]	Loss: 4.6327
Training Epoch: 0 [4224/50048]	Loss: 4.5505
Training Epoch: 0 [4352/50048]	Loss: 4.5367
Training Epoch: 0 [4480/50048]	Loss: 4.4903
Training Epoch: 0 [4608/50048]	Loss: 4.4980
Training Epoch: 0 [4736/50048]	Loss: 4.4750
Training Epoch: 0 [4864/50048]	Loss: 4.4832
Training Epoch: 0 [4992/50048]	Loss: 4.4765
Training Epoch: 0 [5120/50048]	Loss: 4.4758
Training Epoch: 0 [5248/50048]	Loss: 4.4407
Training Epoch: 0 [5376/50048]	Loss: 4.4429
Training Epoch: 0 [5504/50048]	Loss: 4.5190
Training Epoch: 0 [5632/50048]	Loss: 4.5277
Training Epoch: 0 [5760/50048]	Loss: 4.4456
Training Epoch: 0 [5888/50048]	Loss: 4.3683
Training Epoch: 0 [6016/50048]	Loss: 4.3552
Training Epoch: 0 [6144/50048]	Loss: 4.2491
Training Epoch: 0 [6272/50048]	Loss: 4.4739
Training Epoch: 0 [6400/50048]	Loss: 4.3399
Profile done with power limit 125W
epoch 1 train time consumed: 12.68s
Validation Epoch: 0, Average loss: 0.0369, Accuracy: 0.0347
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 125000, 'lr': 0.004, 'energy': 123.35715631504716, 'time': 9.658206722000159, 'accuracy': 0.03471123417721519, 'total_cost': 132.69854479150018}
[run job] Launching job with BS 128: and LR: 0.004 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00400+pl100', 'ZEUS_COST_THRESH': '265.39708958300037', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.004', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004', '--power_limit', '100']
[run job] cost_ub=265.39708958300037
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00400+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.004
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.3519
Training Epoch: 0 [384/50048]	Loss: 5.6641
Training Epoch: 0 [512/50048]	Loss: 5.2230
Training Epoch: 0 [640/50048]	Loss: 5.0199
Training Epoch: 0 [768/50048]	Loss: 4.9848
Training Epoch: 0 [896/50048]	Loss: 5.0314
Training Epoch: 0 [1024/50048]	Loss: 4.9354
Training Epoch: 0 [1152/50048]	Loss: 4.8352
Training Epoch: 0 [1280/50048]	Loss: 4.8161
Training Epoch: 0 [1408/50048]	Loss: 4.6058
Training Epoch: 0 [1536/50048]	Loss: 4.7742
Training Epoch: 0 [1664/50048]	Loss: 4.9978
Training Epoch: 0 [1792/50048]	Loss: 4.5761
Training Epoch: 0 [1920/50048]	Loss: 4.6160
Training Epoch: 0 [2048/50048]	Loss: 4.5609
Training Epoch: 0 [2176/50048]	Loss: 4.6927
Training Epoch: 0 [2304/50048]	Loss: 4.6109
Training Epoch: 0 [2432/50048]	Loss: 4.5593
Training Epoch: 0 [2560/50048]	Loss: 4.5728
Training Epoch: 0 [2688/50048]	Loss: 4.5106
Training Epoch: 0 [2816/50048]	Loss: 4.6569
Training Epoch: 0 [2944/50048]	Loss: 4.5367
Training Epoch: 0 [3072/50048]	Loss: 4.5610
Training Epoch: 0 [3200/50048]	Loss: 4.6315
Training Epoch: 0 [3328/50048]	Loss: 4.5348
Training Epoch: 0 [3456/50048]	Loss: 4.5333
Training Epoch: 0 [3584/50048]	Loss: 4.5572
Training Epoch: 0 [3712/50048]	Loss: 4.7212
Training Epoch: 0 [3840/50048]	Loss: 4.5974
Training Epoch: 0 [3968/50048]	Loss: 4.5603
Training Epoch: 0 [4096/50048]	Loss: 4.6365
Training Epoch: 0 [4224/50048]	Loss: 4.5557
Training Epoch: 0 [4352/50048]	Loss: 4.5542
Training Epoch: 0 [4480/50048]	Loss: 4.4902
Training Epoch: 0 [4608/50048]	Loss: 4.5341
Training Epoch: 0 [4736/50048]	Loss: 4.4824
Training Epoch: 0 [4864/50048]	Loss: 4.5155
Training Epoch: 0 [4992/50048]	Loss: 4.5461
Training Epoch: 0 [5120/50048]	Loss: 4.5047
Training Epoch: 0 [5248/50048]	Loss: 4.4507
Training Epoch: 0 [5376/50048]	Loss: 4.4665
Training Epoch: 0 [5504/50048]	Loss: 4.4982
Training Epoch: 0 [5632/50048]	Loss: 4.5657
Training Epoch: 0 [5760/50048]	Loss: 4.4382
Training Epoch: 0 [5888/50048]	Loss: 4.3919
Training Epoch: 0 [6016/50048]	Loss: 4.4160
Training Epoch: 0 [6144/50048]	Loss: 4.2673
Training Epoch: 0 [6272/50048]	Loss: 4.5504
Training Epoch: 0 [6400/50048]	Loss: 4.4180
Profile done with power limit 100W
epoch 1 train time consumed: 26.46s
Validation Epoch: 0, Average loss: 0.0369, Accuracy: 0.0290
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 100000, 'lr': 0.004, 'energy': 99.0257256197786, 'time': 20.996372585000245, 'accuracy': 0.028975474683544302, 'total_cost': 317.4010124777079}

[Power Profiler] with batch size 128 and learning rate 0.0048
[run job] Launching job with BS 128: and LR: 0.0048 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00480+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0048', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0048', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00480+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0048
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.6144
Training Epoch: 0 [384/50048]	Loss: 5.9819
Training Epoch: 0 [512/50048]	Loss: 5.3665
Training Epoch: 0 [640/50048]	Loss: 5.0796
Training Epoch: 0 [768/50048]	Loss: 5.0440
Training Epoch: 0 [896/50048]	Loss: 5.0926
Training Epoch: 0 [1024/50048]	Loss: 4.9322
Training Epoch: 0 [1152/50048]	Loss: 4.8149
Training Epoch: 0 [1280/50048]	Loss: 4.7636
Training Epoch: 0 [1408/50048]	Loss: 4.5482
Training Epoch: 0 [1536/50048]	Loss: 4.7262
Training Epoch: 0 [1664/50048]	Loss: 4.9981
Training Epoch: 0 [1792/50048]	Loss: 4.6123
Training Epoch: 0 [1920/50048]	Loss: 4.8238
Training Epoch: 0 [2048/50048]	Loss: 4.5579
Training Epoch: 0 [2176/50048]	Loss: 4.7721
Training Epoch: 0 [2304/50048]	Loss: 4.5790
Training Epoch: 0 [2432/50048]	Loss: 4.5916
Training Epoch: 0 [2560/50048]	Loss: 4.6738
Training Epoch: 0 [2688/50048]	Loss: 4.5561
Training Epoch: 0 [2816/50048]	Loss: 4.6640
Training Epoch: 0 [2944/50048]	Loss: 4.6083
Training Epoch: 0 [3072/50048]	Loss: 4.6038
Training Epoch: 0 [3200/50048]	Loss: 4.5539
Training Epoch: 0 [3328/50048]	Loss: 4.5386
Training Epoch: 0 [3456/50048]	Loss: 4.5294
Training Epoch: 0 [3584/50048]	Loss: 4.6145
Training Epoch: 0 [3712/50048]	Loss: 4.8306
Training Epoch: 0 [3840/50048]	Loss: 4.5603
Training Epoch: 0 [3968/50048]	Loss: 4.5573
Training Epoch: 0 [4096/50048]	Loss: 4.6565
Training Epoch: 0 [4224/50048]	Loss: 4.6737
Training Epoch: 0 [4352/50048]	Loss: 4.5666
Training Epoch: 0 [4480/50048]	Loss: 4.4859
Training Epoch: 0 [4608/50048]	Loss: 4.5565
Training Epoch: 0 [4736/50048]	Loss: 4.4578
Training Epoch: 0 [4864/50048]	Loss: 4.5949
Training Epoch: 0 [4992/50048]	Loss: 4.4649
Training Epoch: 0 [5120/50048]	Loss: 4.4871
Training Epoch: 0 [5248/50048]	Loss: 4.4459
Training Epoch: 0 [5376/50048]	Loss: 4.5083
Training Epoch: 0 [5504/50048]	Loss: 4.4603
Training Epoch: 0 [5632/50048]	Loss: 4.4883
Training Epoch: 0 [5760/50048]	Loss: 4.4803
Training Epoch: 0 [5888/50048]	Loss: 4.4444
Training Epoch: 0 [6016/50048]	Loss: 4.4215
Training Epoch: 0 [6144/50048]	Loss: 4.3254
Training Epoch: 0 [6272/50048]	Loss: 4.5916
Training Epoch: 0 [6400/50048]	Loss: 4.3852
Profile done with power limit 175W
epoch 1 train time consumed: 11.49s
Validation Epoch: 0, Average loss: 0.0359, Accuracy: 0.0252
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 175000, 'lr': 0.0048, 'energy': 144.32507224193986, 'time': 8.638574053999946, 'accuracy': 0.02521756329113924, 'total_cost': 174.8538584223536}
[run job] Launching job with BS 128: and LR: 0.0048 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00480+pl150', 'ZEUS_COST_THRESH': '349.7077168447072', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0048', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0048', '--power_limit', '150']
[run job] cost_ub=349.7077168447072
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00480+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0048
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.6144
Training Epoch: 0 [384/50048]	Loss: 5.9812
Training Epoch: 0 [512/50048]	Loss: 5.3765
Training Epoch: 0 [640/50048]	Loss: 5.1087
Training Epoch: 0 [768/50048]	Loss: 5.0733
Training Epoch: 0 [896/50048]	Loss: 5.0288
Training Epoch: 0 [1024/50048]	Loss: 4.9711
Training Epoch: 0 [1152/50048]	Loss: 4.7803
Training Epoch: 0 [1280/50048]	Loss: 4.7589
Training Epoch: 0 [1408/50048]	Loss: 4.5570
Training Epoch: 0 [1536/50048]	Loss: 4.6799
Training Epoch: 0 [1664/50048]	Loss: 5.0694
Training Epoch: 0 [1792/50048]	Loss: 4.6355
Training Epoch: 0 [1920/50048]	Loss: 4.5723
Training Epoch: 0 [2048/50048]	Loss: 4.5324
Training Epoch: 0 [2176/50048]	Loss: 4.7690
Training Epoch: 0 [2304/50048]	Loss: 4.5816
Training Epoch: 0 [2432/50048]	Loss: 4.6109
Training Epoch: 0 [2560/50048]	Loss: 4.5425
Training Epoch: 0 [2688/50048]	Loss: 4.4977
Training Epoch: 0 [2816/50048]	Loss: 4.7794
Training Epoch: 0 [2944/50048]	Loss: 4.6279
Training Epoch: 0 [3072/50048]	Loss: 4.6145
Training Epoch: 0 [3200/50048]	Loss: 4.7051
Training Epoch: 0 [3328/50048]	Loss: 4.5847
Training Epoch: 0 [3456/50048]	Loss: 4.4864
Training Epoch: 0 [3584/50048]	Loss: 4.5812
Training Epoch: 0 [3712/50048]	Loss: 4.7585
Training Epoch: 0 [3840/50048]	Loss: 4.6228
Training Epoch: 0 [3968/50048]	Loss: 4.6379
Training Epoch: 0 [4096/50048]	Loss: 4.6119
Training Epoch: 0 [4224/50048]	Loss: 4.5655
Training Epoch: 0 [4352/50048]	Loss: 4.5491
Training Epoch: 0 [4480/50048]	Loss: 4.5163
Training Epoch: 0 [4608/50048]	Loss: 4.5209
Training Epoch: 0 [4736/50048]	Loss: 4.4711
Training Epoch: 0 [4864/50048]	Loss: 4.5273
Training Epoch: 0 [4992/50048]	Loss: 4.5377
Training Epoch: 0 [5120/50048]	Loss: 4.5180
Training Epoch: 0 [5248/50048]	Loss: 4.4733
Training Epoch: 0 [5376/50048]	Loss: 4.5254
Training Epoch: 0 [5504/50048]	Loss: 4.5593
Training Epoch: 0 [5632/50048]	Loss: 4.5077
Training Epoch: 0 [5760/50048]	Loss: 4.4803
Training Epoch: 0 [5888/50048]	Loss: 4.4378
Training Epoch: 0 [6016/50048]	Loss: 4.4571
Training Epoch: 0 [6144/50048]	Loss: 4.3871
Training Epoch: 0 [6272/50048]	Loss: 4.5854
Training Epoch: 0 [6400/50048]	Loss: 4.3897
Profile done with power limit 150W
epoch 1 train time consumed: 11.47s
Validation Epoch: 0, Average loss: 0.0366, Accuracy: 0.0265
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 150000, 'lr': 0.0048, 'energy': 144.34276112585013, 'time': 8.650591868999982, 'accuracy': 0.026503164556962024, 'total_cost': 166.61282401246197}
[run job] Launching job with BS 128: and LR: 0.0048 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00480+pl125', 'ZEUS_COST_THRESH': '333.22564802492394', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0048', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0048', '--power_limit', '125']
[run job] cost_ub=333.22564802492394
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00480+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0048
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.6144
Training Epoch: 0 [384/50048]	Loss: 5.9819
Training Epoch: 0 [512/50048]	Loss: 5.3664
Training Epoch: 0 [640/50048]	Loss: 5.1142
Training Epoch: 0 [768/50048]	Loss: 5.1449
Training Epoch: 0 [896/50048]	Loss: 5.0354
Training Epoch: 0 [1024/50048]	Loss: 4.9025
Training Epoch: 0 [1152/50048]	Loss: 4.8139
Training Epoch: 0 [1280/50048]	Loss: 4.8000
Training Epoch: 0 [1408/50048]	Loss: 4.5576
Training Epoch: 0 [1536/50048]	Loss: 4.7320
Training Epoch: 0 [1664/50048]	Loss: 4.9730
Training Epoch: 0 [1792/50048]	Loss: 4.6107
Training Epoch: 0 [1920/50048]	Loss: 4.6037
Training Epoch: 0 [2048/50048]	Loss: 4.6018
Training Epoch: 0 [2176/50048]	Loss: 4.7409
Training Epoch: 0 [2304/50048]	Loss: 4.5789
Training Epoch: 0 [2432/50048]	Loss: 4.5683
Training Epoch: 0 [2560/50048]	Loss: 4.6408
Training Epoch: 0 [2688/50048]	Loss: 4.5369
Training Epoch: 0 [2816/50048]	Loss: 4.6921
Training Epoch: 0 [2944/50048]	Loss: 4.5736
Training Epoch: 0 [3072/50048]	Loss: 4.5642
Training Epoch: 0 [3200/50048]	Loss: 4.5833
Training Epoch: 0 [3328/50048]	Loss: 4.5313
Training Epoch: 0 [3456/50048]	Loss: 4.5205
Training Epoch: 0 [3584/50048]	Loss: 4.5416
Training Epoch: 0 [3712/50048]	Loss: 4.7213
Training Epoch: 0 [3840/50048]	Loss: 4.5830
Training Epoch: 0 [3968/50048]	Loss: 4.5836
Training Epoch: 0 [4096/50048]	Loss: 4.6531
Training Epoch: 0 [4224/50048]	Loss: 4.5295
Training Epoch: 0 [4352/50048]	Loss: 4.6296
Training Epoch: 0 [4480/50048]	Loss: 4.4598
Training Epoch: 0 [4608/50048]	Loss: 4.5005
Training Epoch: 0 [4736/50048]	Loss: 4.4084
Training Epoch: 0 [4864/50048]	Loss: 4.4601
Training Epoch: 0 [4992/50048]	Loss: 4.4792
Training Epoch: 0 [5120/50048]	Loss: 4.4912
Training Epoch: 0 [5248/50048]	Loss: 4.4250
Training Epoch: 0 [5376/50048]	Loss: 4.4146
Training Epoch: 0 [5504/50048]	Loss: 4.4624
Training Epoch: 0 [5632/50048]	Loss: 4.5456
Training Epoch: 0 [5760/50048]	Loss: 4.4414
Training Epoch: 0 [5888/50048]	Loss: 4.4159
Training Epoch: 0 [6016/50048]	Loss: 4.3475
Training Epoch: 0 [6144/50048]	Loss: 4.2711
Training Epoch: 0 [6272/50048]	Loss: 4.5192
Training Epoch: 0 [6400/50048]	Loss: 4.3530
Profile done with power limit 125W
epoch 1 train time consumed: 12.69s
Validation Epoch: 0, Average loss: 0.0349, Accuracy: 0.0307
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 125000, 'lr': 0.0048, 'energy': 123.20228838139138, 'time': 9.663978663999842, 'accuracy': 0.030656645569620253, 'total_cost': 150.26075316915893}
[run job] Launching job with BS 128: and LR: 0.0048 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs128+lr0.00480+pl100', 'ZEUS_COST_THRESH': '300.52150633831786', 'ZEUS_BATCH_SIZE': '128', 'ZEUS_LEARNING_RATE': '0.0048', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '128', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0048', '--power_limit', '100']
[run job] cost_ub=300.52150633831786
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs128+lr0.00480+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0048
batch size arg: 128
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [128/50048]	Loss: 4.6535
Training Epoch: 0 [256/50048]	Loss: 5.6144
Training Epoch: 0 [384/50048]	Loss: 5.9806
Training Epoch: 0 [512/50048]	Loss: 5.3723
Training Epoch: 0 [640/50048]	Loss: 5.1043
Training Epoch: 0 [768/50048]	Loss: 5.1341
Training Epoch: 0 [896/50048]	Loss: 5.0404
Training Epoch: 0 [1024/50048]	Loss: 4.9211
Training Epoch: 0 [1152/50048]	Loss: 4.7592
Training Epoch: 0 [1280/50048]	Loss: 4.7680
Training Epoch: 0 [1408/50048]	Loss: 4.5470
Training Epoch: 0 [1536/50048]	Loss: 4.7294
Training Epoch: 0 [1664/50048]	Loss: 4.9904
Training Epoch: 0 [1792/50048]	Loss: 4.6250
Training Epoch: 0 [1920/50048]	Loss: 4.5854
Training Epoch: 0 [2048/50048]	Loss: 4.5782
Training Epoch: 0 [2176/50048]	Loss: 4.6795
Training Epoch: 0 [2304/50048]	Loss: 4.5814
Training Epoch: 0 [2432/50048]	Loss: 4.6232
Training Epoch: 0 [2560/50048]	Loss: 4.5548
Training Epoch: 0 [2688/50048]	Loss: 4.4879
Training Epoch: 0 [2816/50048]	Loss: 4.7928
Training Epoch: 0 [2944/50048]	Loss: 4.6552
Training Epoch: 0 [3072/50048]	Loss: 4.6335
Training Epoch: 0 [3200/50048]	Loss: 4.7288
Training Epoch: 0 [3328/50048]	Loss: 4.6128
Training Epoch: 0 [3456/50048]	Loss: 4.5235
Training Epoch: 0 [3584/50048]	Loss: 4.5760
Training Epoch: 0 [3712/50048]	Loss: 4.7327
Training Epoch: 0 [3840/50048]	Loss: 4.6418
Training Epoch: 0 [3968/50048]	Loss: 4.5942
Training Epoch: 0 [4096/50048]	Loss: 4.6047
Training Epoch: 0 [4224/50048]	Loss: 4.5477
Training Epoch: 0 [4352/50048]	Loss: 4.5899
Training Epoch: 0 [4480/50048]	Loss: 4.5420
Training Epoch: 0 [4608/50048]	Loss: 4.5842
Training Epoch: 0 [4736/50048]	Loss: 4.5104
Training Epoch: 0 [4864/50048]	Loss: 4.5314
Training Epoch: 0 [4992/50048]	Loss: 4.5877
Training Epoch: 0 [5120/50048]	Loss: 4.5324
Training Epoch: 0 [5248/50048]	Loss: 4.4885
Training Epoch: 0 [5376/50048]	Loss: 4.5378
Training Epoch: 0 [5504/50048]	Loss: 4.5424
Training Epoch: 0 [5632/50048]	Loss: 4.6583
Training Epoch: 0 [5760/50048]	Loss: 4.5049
Training Epoch: 0 [5888/50048]	Loss: 4.5030
Training Epoch: 0 [6016/50048]	Loss: 4.5310
Training Epoch: 0 [6144/50048]	Loss: 4.4218
Training Epoch: 0 [6272/50048]	Loss: 4.6384
Training Epoch: 0 [6400/50048]	Loss: 4.4657
Profile done with power limit 100W
epoch 1 train time consumed: 26.49s
Validation Epoch: 0, Average loss: 0.0382, Accuracy: 0.0253
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 128, 'pl': 100000, 'lr': 0.0048, 'energy': 98.84710091157461, 'time': 20.89193062000004, 'accuracy': 0.02531645569620253, 'total_cost': 361.23271738002603}

[Power Profiler] with batch size 256 and learning rate 0.004525483399593905
[run job] Launching job with BS 256: and LR: 0.004525483399593905 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00453+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.004525483399593905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004525483399593905', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00453+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.004525483399593905
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.5787
Training Epoch: 0 [768/50176]	Loss: 5.3858
Training Epoch: 0 [1024/50176]	Loss: 5.1447
Training Epoch: 0 [1280/50176]	Loss: 4.9762
Training Epoch: 0 [1536/50176]	Loss: 4.9442
Training Epoch: 0 [1792/50176]	Loss: 4.7022
Training Epoch: 0 [2048/50176]	Loss: 4.6195
Training Epoch: 0 [2304/50176]	Loss: 4.7966
Training Epoch: 0 [2560/50176]	Loss: 4.6557
Training Epoch: 0 [2816/50176]	Loss: 4.6399
Training Epoch: 0 [3072/50176]	Loss: 4.6356
Training Epoch: 0 [3328/50176]	Loss: 4.6059
Training Epoch: 0 [3584/50176]	Loss: 4.5457
Training Epoch: 0 [3840/50176]	Loss: 4.7181
Training Epoch: 0 [4096/50176]	Loss: 4.7017
Training Epoch: 0 [4352/50176]	Loss: 4.6352
Training Epoch: 0 [4608/50176]	Loss: 4.4830
Training Epoch: 0 [4864/50176]	Loss: 4.4799
Training Epoch: 0 [5120/50176]	Loss: 4.5287
Training Epoch: 0 [5376/50176]	Loss: 4.5005
Training Epoch: 0 [5632/50176]	Loss: 4.6409
Training Epoch: 0 [5888/50176]	Loss: 4.4413
Training Epoch: 0 [6144/50176]	Loss: 4.3973
Training Epoch: 0 [6400/50176]	Loss: 4.5175
Training Epoch: 0 [6656/50176]	Loss: 4.4831
Training Epoch: 0 [6912/50176]	Loss: 4.3763
Training Epoch: 0 [7168/50176]	Loss: 4.4657
Training Epoch: 0 [7424/50176]	Loss: 4.4713
Training Epoch: 0 [7680/50176]	Loss: 4.3504
Training Epoch: 0 [7936/50176]	Loss: 4.4651
Training Epoch: 0 [8192/50176]	Loss: 4.2944
Training Epoch: 0 [8448/50176]	Loss: 4.4484
Training Epoch: 0 [8704/50176]	Loss: 4.3074
Training Epoch: 0 [8960/50176]	Loss: 4.3102
Training Epoch: 0 [9216/50176]	Loss: 4.3440
Training Epoch: 0 [9472/50176]	Loss: 4.2900
Training Epoch: 0 [9728/50176]	Loss: 4.3433
Training Epoch: 0 [9984/50176]	Loss: 4.3271
Training Epoch: 0 [10240/50176]	Loss: 4.3285
Training Epoch: 0 [10496/50176]	Loss: 4.2519
Training Epoch: 0 [10752/50176]	Loss: 4.2244
Training Epoch: 0 [11008/50176]	Loss: 4.2117
Training Epoch: 0 [11264/50176]	Loss: 4.3183
Training Epoch: 0 [11520/50176]	Loss: 4.1942
Training Epoch: 0 [11776/50176]	Loss: 4.2437
Training Epoch: 0 [12032/50176]	Loss: 4.3091
Training Epoch: 0 [12288/50176]	Loss: 4.2106
Training Epoch: 0 [12544/50176]	Loss: 4.1551
Training Epoch: 0 [12800/50176]	Loss: 4.1789
Profile done with power limit 175W
epoch 1 train time consumed: 12.71s
Validation Epoch: 0, Average loss: 0.0192, Accuracy: 0.0382
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 175000, 'lr': 0.004525483399593905, 'energy': 154.83767195724607, 'time': 9.485337706000337, 'accuracy': 0.03818359375, 'total_cost': 261.2764095558587}
[run job] Launching job with BS 256: and LR: 0.004525483399593905 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00453+pl150', 'ZEUS_COST_THRESH': '522.5528191117174', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.004525483399593905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004525483399593905', '--power_limit', '150']
[run job] cost_ub=522.5528191117174
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00453+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.004525483399593905
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.5787
Training Epoch: 0 [768/50176]	Loss: 5.3860
Training Epoch: 0 [1024/50176]	Loss: 5.1507
Training Epoch: 0 [1280/50176]	Loss: 4.9884
Training Epoch: 0 [1536/50176]	Loss: 4.9814
Training Epoch: 0 [1792/50176]	Loss: 4.7458
Training Epoch: 0 [2048/50176]	Loss: 4.6193
Training Epoch: 0 [2304/50176]	Loss: 4.7712
Training Epoch: 0 [2560/50176]	Loss: 4.6991
Training Epoch: 0 [2816/50176]	Loss: 4.6424
Training Epoch: 0 [3072/50176]	Loss: 4.6397
Training Epoch: 0 [3328/50176]	Loss: 4.6210
Training Epoch: 0 [3584/50176]	Loss: 4.5871
Training Epoch: 0 [3840/50176]	Loss: 4.6813
Training Epoch: 0 [4096/50176]	Loss: 4.6941
Training Epoch: 0 [4352/50176]	Loss: 4.6208
Training Epoch: 0 [4608/50176]	Loss: 4.5059
Training Epoch: 0 [4864/50176]	Loss: 4.5741
Training Epoch: 0 [5120/50176]	Loss: 4.5336
Training Epoch: 0 [5376/50176]	Loss: 4.5087
Training Epoch: 0 [5632/50176]	Loss: 4.6646
Training Epoch: 0 [5888/50176]	Loss: 4.4463
Training Epoch: 0 [6144/50176]	Loss: 4.4457
Training Epoch: 0 [6400/50176]	Loss: 4.5316
Training Epoch: 0 [6656/50176]	Loss: 4.5162
Training Epoch: 0 [6912/50176]	Loss: 4.3984
Training Epoch: 0 [7168/50176]	Loss: 4.4661
Training Epoch: 0 [7424/50176]	Loss: 4.4206
Training Epoch: 0 [7680/50176]	Loss: 4.3549
Training Epoch: 0 [7936/50176]	Loss: 4.3619
Training Epoch: 0 [8192/50176]	Loss: 4.3007
Training Epoch: 0 [8448/50176]	Loss: 4.4023
Training Epoch: 0 [8704/50176]	Loss: 4.3172
Training Epoch: 0 [8960/50176]	Loss: 4.2976
Training Epoch: 0 [9216/50176]	Loss: 4.3326
Training Epoch: 0 [9472/50176]	Loss: 4.3186
Training Epoch: 0 [9728/50176]	Loss: 4.2725
Training Epoch: 0 [9984/50176]	Loss: 4.2742
Training Epoch: 0 [10240/50176]	Loss: 4.3267
Training Epoch: 0 [10496/50176]	Loss: 4.2592
Training Epoch: 0 [10752/50176]	Loss: 4.1861
Training Epoch: 0 [11008/50176]	Loss: 4.1740
Training Epoch: 0 [11264/50176]	Loss: 4.2898
Training Epoch: 0 [11520/50176]	Loss: 4.2297
Training Epoch: 0 [11776/50176]	Loss: 4.2746
Training Epoch: 0 [12032/50176]	Loss: 4.2414
Training Epoch: 0 [12288/50176]	Loss: 4.1863
Training Epoch: 0 [12544/50176]	Loss: 4.1778
Training Epoch: 0 [12800/50176]	Loss: 4.1525
Profile done with power limit 150W
epoch 1 train time consumed: 12.99s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0416
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 150000, 'lr': 0.004525483399593905, 'energy': 146.89528996314885, 'time': 9.764853021999897, 'accuracy': 0.0416015625, 'total_cost': 240.93208354616746}
[run job] Launching job with BS 256: and LR: 0.004525483399593905 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00453+pl125', 'ZEUS_COST_THRESH': '481.8641670923349', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.004525483399593905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004525483399593905', '--power_limit', '125']
[run job] cost_ub=481.8641670923349
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00453+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.004525483399593905
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.5787
Training Epoch: 0 [768/50176]	Loss: 5.3857
Training Epoch: 0 [1024/50176]	Loss: 5.1448
Training Epoch: 0 [1280/50176]	Loss: 4.9781
Training Epoch: 0 [1536/50176]	Loss: 4.9677
Training Epoch: 0 [1792/50176]	Loss: 4.7349
Training Epoch: 0 [2048/50176]	Loss: 4.6351
Training Epoch: 0 [2304/50176]	Loss: 4.7743
Training Epoch: 0 [2560/50176]	Loss: 4.7021
Training Epoch: 0 [2816/50176]	Loss: 4.6821
Training Epoch: 0 [3072/50176]	Loss: 4.6281
Training Epoch: 0 [3328/50176]	Loss: 4.6200
Training Epoch: 0 [3584/50176]	Loss: 4.5411
Training Epoch: 0 [3840/50176]	Loss: 4.7043
Training Epoch: 0 [4096/50176]	Loss: 4.7249
Training Epoch: 0 [4352/50176]	Loss: 4.6307
Training Epoch: 0 [4608/50176]	Loss: 4.5243
Training Epoch: 0 [4864/50176]	Loss: 4.5768
Training Epoch: 0 [5120/50176]	Loss: 4.5405
Training Epoch: 0 [5376/50176]	Loss: 4.5035
Training Epoch: 0 [5632/50176]	Loss: 4.6696
Training Epoch: 0 [5888/50176]	Loss: 4.4762
Training Epoch: 0 [6144/50176]	Loss: 4.4352
Training Epoch: 0 [6400/50176]	Loss: 4.5739
Training Epoch: 0 [6656/50176]	Loss: 4.5112
Training Epoch: 0 [6912/50176]	Loss: 4.4410
Training Epoch: 0 [7168/50176]	Loss: 4.4614
Training Epoch: 0 [7424/50176]	Loss: 4.3917
Training Epoch: 0 [7680/50176]	Loss: 4.3627
Training Epoch: 0 [7936/50176]	Loss: 4.4018
Training Epoch: 0 [8192/50176]	Loss: 4.3380
Training Epoch: 0 [8448/50176]	Loss: 4.4060
Training Epoch: 0 [8704/50176]	Loss: 4.3472
Training Epoch: 0 [8960/50176]	Loss: 4.3308
Training Epoch: 0 [9216/50176]	Loss: 4.3082
Training Epoch: 0 [9472/50176]	Loss: 4.3285
Training Epoch: 0 [9728/50176]	Loss: 4.3246
Training Epoch: 0 [9984/50176]	Loss: 4.2834
Training Epoch: 0 [10240/50176]	Loss: 4.3201
Training Epoch: 0 [10496/50176]	Loss: 4.2608
Training Epoch: 0 [10752/50176]	Loss: 4.1865
Training Epoch: 0 [11008/50176]	Loss: 4.1714
Training Epoch: 0 [11264/50176]	Loss: 4.3390
Training Epoch: 0 [11520/50176]	Loss: 4.1963
Training Epoch: 0 [11776/50176]	Loss: 4.2620
Training Epoch: 0 [12032/50176]	Loss: 4.2535
Training Epoch: 0 [12288/50176]	Loss: 4.1668
Training Epoch: 0 [12544/50176]	Loss: 4.1763
Training Epoch: 0 [12800/50176]	Loss: 4.1467
Profile done with power limit 125W
epoch 1 train time consumed: 14.66s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0302
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 125000, 'lr': 0.004525483399593905, 'energy': 122.96890305987783, 'time': 11.124008482000136, 'accuracy': 0.03017578125, 'total_cost': 350.2657531217602}
[run job] Launching job with BS 256: and LR: 0.004525483399593905 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00453+pl100', 'ZEUS_COST_THRESH': '481.8641670923349', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.004525483399593905', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.004525483399593905', '--power_limit', '100']
[run job] cost_ub=481.8641670923349
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00453+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.004525483399593905
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.5787
Training Epoch: 0 [768/50176]	Loss: 5.3854
Training Epoch: 0 [1024/50176]	Loss: 5.1421
Training Epoch: 0 [1280/50176]	Loss: 4.9788
Training Epoch: 0 [1536/50176]	Loss: 4.9691
Training Epoch: 0 [1792/50176]	Loss: 4.7106
Training Epoch: 0 [2048/50176]	Loss: 4.6032
Training Epoch: 0 [2304/50176]	Loss: 4.7641
Training Epoch: 0 [2560/50176]	Loss: 4.7014
Training Epoch: 0 [2816/50176]	Loss: 4.6740
Training Epoch: 0 [3072/50176]	Loss: 4.6425
Training Epoch: 0 [3328/50176]	Loss: 4.6132
Training Epoch: 0 [3584/50176]	Loss: 4.5948
Training Epoch: 0 [3840/50176]	Loss: 4.6758
Training Epoch: 0 [4096/50176]	Loss: 4.6759
Training Epoch: 0 [4352/50176]	Loss: 4.6138
Training Epoch: 0 [4608/50176]	Loss: 4.4971
Training Epoch: 0 [4864/50176]	Loss: 4.5714
Training Epoch: 0 [5120/50176]	Loss: 4.5483
Training Epoch: 0 [5376/50176]	Loss: 4.5058
Training Epoch: 0 [5632/50176]	Loss: 4.6177
Training Epoch: 0 [5888/50176]	Loss: 4.4614
Training Epoch: 0 [6144/50176]	Loss: 4.3968
Training Epoch: 0 [6400/50176]	Loss: 4.5386
Training Epoch: 0 [6656/50176]	Loss: 4.5154
Training Epoch: 0 [6912/50176]	Loss: 4.4105
Training Epoch: 0 [7168/50176]	Loss: 4.5048
Training Epoch: 0 [7424/50176]	Loss: 4.3953
Training Epoch: 0 [7680/50176]	Loss: 4.3478
Training Epoch: 0 [7936/50176]	Loss: 4.4057
Training Epoch: 0 [8192/50176]	Loss: 4.3035
Training Epoch: 0 [8448/50176]	Loss: 4.3741
Training Epoch: 0 [8704/50176]	Loss: 4.3318
Training Epoch: 0 [8960/50176]	Loss: 4.3123
Training Epoch: 0 [9216/50176]	Loss: 4.2973
Training Epoch: 0 [9472/50176]	Loss: 4.2998
Training Epoch: 0 [9728/50176]	Loss: 4.3060
Training Epoch: 0 [9984/50176]	Loss: 4.2526
Training Epoch: 0 [10240/50176]	Loss: 4.3049
Training Epoch: 0 [10496/50176]	Loss: 4.2195
Training Epoch: 0 [10752/50176]	Loss: 4.1863
Training Epoch: 0 [11008/50176]	Loss: 4.1972
Training Epoch: 0 [11264/50176]	Loss: 4.2512
Training Epoch: 0 [11520/50176]	Loss: 4.2095
Training Epoch: 0 [11776/50176]	Loss: 4.3141
Training Epoch: 0 [12032/50176]	Loss: 4.2750
Training Epoch: 0 [12288/50176]	Loss: 4.1807
Training Epoch: 0 [12544/50176]	Loss: 4.1660
Training Epoch: 0 [12800/50176]	Loss: 4.1177
Profile done with power limit 100W
epoch 1 train time consumed: 36.76s
Validation Epoch: 0, Average loss: 0.0177, Accuracy: 0.0396
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 100000, 'lr': 0.004525483399593905, 'energy': 103.23046630168689, 'time': 28.92220561499971, 'accuracy': 0.0396484375, 'total_cost': 647.1932244684538}

[Power Profiler] with batch size 256 and learning rate 0.005656854249492381
[run job] Launching job with BS 256: and LR: 0.005656854249492381 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00566+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.005656854249492381', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.005656854249492381', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00566+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.005656854249492381
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.9997
Training Epoch: 0 [768/50176]	Loss: 5.7261
Training Epoch: 0 [1024/50176]	Loss: 5.1670
Training Epoch: 0 [1280/50176]	Loss: 4.8456
Training Epoch: 0 [1536/50176]	Loss: 4.7931
Training Epoch: 0 [1792/50176]	Loss: 4.6399
Training Epoch: 0 [2048/50176]	Loss: 4.6494
Training Epoch: 0 [2304/50176]	Loss: 4.7598
Training Epoch: 0 [2560/50176]	Loss: 4.6660
Training Epoch: 0 [2816/50176]	Loss: 4.6342
Training Epoch: 0 [3072/50176]	Loss: 4.6489
Training Epoch: 0 [3328/50176]	Loss: 4.5859
Training Epoch: 0 [3584/50176]	Loss: 4.6617
Training Epoch: 0 [3840/50176]	Loss: 4.6868
Training Epoch: 0 [4096/50176]	Loss: 4.6249
Training Epoch: 0 [4352/50176]	Loss: 4.6209
Training Epoch: 0 [4608/50176]	Loss: 4.5760
Training Epoch: 0 [4864/50176]	Loss: 4.5516
Training Epoch: 0 [5120/50176]	Loss: 4.5268
Training Epoch: 0 [5376/50176]	Loss: 4.5327
Training Epoch: 0 [5632/50176]	Loss: 4.6196
Training Epoch: 0 [5888/50176]	Loss: 4.6018
Training Epoch: 0 [6144/50176]	Loss: 4.5098
Training Epoch: 0 [6400/50176]	Loss: 4.6925
Training Epoch: 0 [6656/50176]	Loss: 4.5557
Training Epoch: 0 [6912/50176]	Loss: 4.4918
Training Epoch: 0 [7168/50176]	Loss: 4.5931
Training Epoch: 0 [7424/50176]	Loss: 4.5251
Training Epoch: 0 [7680/50176]	Loss: 4.4671
Training Epoch: 0 [7936/50176]	Loss: 4.5179
Training Epoch: 0 [8192/50176]	Loss: 4.4638
Training Epoch: 0 [8448/50176]	Loss: 4.4577
Training Epoch: 0 [8704/50176]	Loss: 4.3975
Training Epoch: 0 [8960/50176]	Loss: 4.4419
Training Epoch: 0 [9216/50176]	Loss: 4.3738
Training Epoch: 0 [9472/50176]	Loss: 4.4148
Training Epoch: 0 [9728/50176]	Loss: 4.3637
Training Epoch: 0 [9984/50176]	Loss: 4.4103
Training Epoch: 0 [10240/50176]	Loss: 4.4447
Training Epoch: 0 [10496/50176]	Loss: 4.3380
Training Epoch: 0 [10752/50176]	Loss: 4.3708
Training Epoch: 0 [11008/50176]	Loss: 4.3241
Training Epoch: 0 [11264/50176]	Loss: 4.3645
Training Epoch: 0 [11520/50176]	Loss: 4.3243
Training Epoch: 0 [11776/50176]	Loss: 4.3741
Training Epoch: 0 [12032/50176]	Loss: 4.3373
Training Epoch: 0 [12288/50176]	Loss: 4.3089
Training Epoch: 0 [12544/50176]	Loss: 4.2895
Training Epoch: 0 [12800/50176]	Loss: 4.3057
Profile done with power limit 175W
epoch 1 train time consumed: 12.73s
Validation Epoch: 0, Average loss: 0.0196, Accuracy: 0.0214
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 175000, 'lr': 0.005656854249492381, 'energy': 154.37150324769297, 'time': 9.506530696000027, 'accuracy': 0.02138671875, 'total_cost': 466.8612887539825}
[run job] Launching job with BS 256: and LR: 0.005656854249492381 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00566+pl150', 'ZEUS_COST_THRESH': '933.722577507965', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.005656854249492381', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.005656854249492381', '--power_limit', '150']
[run job] cost_ub=933.722577507965
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00566+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.005656854249492381
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.9997
Training Epoch: 0 [768/50176]	Loss: 5.7261
Training Epoch: 0 [1024/50176]	Loss: 5.1717
Training Epoch: 0 [1280/50176]	Loss: 4.8494
Training Epoch: 0 [1536/50176]	Loss: 4.7866
Training Epoch: 0 [1792/50176]	Loss: 4.6367
Training Epoch: 0 [2048/50176]	Loss: 4.6540
Training Epoch: 0 [2304/50176]	Loss: 4.7814
Training Epoch: 0 [2560/50176]	Loss: 4.6573
Training Epoch: 0 [2816/50176]	Loss: 4.6391
Training Epoch: 0 [3072/50176]	Loss: 4.6448
Training Epoch: 0 [3328/50176]	Loss: 4.5697
Training Epoch: 0 [3584/50176]	Loss: 4.5854
Training Epoch: 0 [3840/50176]	Loss: 4.6445
Training Epoch: 0 [4096/50176]	Loss: 4.6286
Training Epoch: 0 [4352/50176]	Loss: 4.6097
Training Epoch: 0 [4608/50176]	Loss: 4.5812
Training Epoch: 0 [4864/50176]	Loss: 4.5694
Training Epoch: 0 [5120/50176]	Loss: 4.5490
Training Epoch: 0 [5376/50176]	Loss: 4.5278
Training Epoch: 0 [5632/50176]	Loss: 4.6225
Training Epoch: 0 [5888/50176]	Loss: 4.6319
Training Epoch: 0 [6144/50176]	Loss: 4.5362
Training Epoch: 0 [6400/50176]	Loss: 4.6508
Training Epoch: 0 [6656/50176]	Loss: 4.6192
Training Epoch: 0 [6912/50176]	Loss: 4.4994
Training Epoch: 0 [7168/50176]	Loss: 4.5211
Training Epoch: 0 [7424/50176]	Loss: 4.5265
Training Epoch: 0 [7680/50176]	Loss: 4.4641
Training Epoch: 0 [7936/50176]	Loss: 4.5109
Training Epoch: 0 [8192/50176]	Loss: 4.4472
Training Epoch: 0 [8448/50176]	Loss: 4.4751
Training Epoch: 0 [8704/50176]	Loss: 4.4499
Training Epoch: 0 [8960/50176]	Loss: 4.4296
Training Epoch: 0 [9216/50176]	Loss: 4.3968
Training Epoch: 0 [9472/50176]	Loss: 4.4560
Training Epoch: 0 [9728/50176]	Loss: 4.3869
Training Epoch: 0 [9984/50176]	Loss: 4.4336
Training Epoch: 0 [10240/50176]	Loss: 4.4412
Training Epoch: 0 [10496/50176]	Loss: 4.3756
Training Epoch: 0 [10752/50176]	Loss: 4.3815
Training Epoch: 0 [11008/50176]	Loss: 4.3296
Training Epoch: 0 [11264/50176]	Loss: 4.3824
Training Epoch: 0 [11520/50176]	Loss: 4.3656
Training Epoch: 0 [11776/50176]	Loss: 4.4207
Training Epoch: 0 [12032/50176]	Loss: 4.3823
Training Epoch: 0 [12288/50176]	Loss: 4.3585
Training Epoch: 0 [12544/50176]	Loss: 4.2779
Training Epoch: 0 [12800/50176]	Loss: 4.3385
Profile done with power limit 150W
epoch 1 train time consumed: 13.01s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0269
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 150000, 'lr': 0.005656854249492381, 'energy': 147.00740542613758, 'time': 9.767149691999748, 'accuracy': 0.02685546875, 'total_cost': 373.4435064317076}
[run job] Launching job with BS 256: and LR: 0.005656854249492381 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00566+pl125', 'ZEUS_COST_THRESH': '746.8870128634152', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.005656854249492381', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.005656854249492381', '--power_limit', '125']
[run job] cost_ub=746.8870128634152
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00566+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.005656854249492381
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.9997
Training Epoch: 0 [768/50176]	Loss: 5.7262
Training Epoch: 0 [1024/50176]	Loss: 5.1633
Training Epoch: 0 [1280/50176]	Loss: 4.8504
Training Epoch: 0 [1536/50176]	Loss: 4.7900
Training Epoch: 0 [1792/50176]	Loss: 4.6447
Training Epoch: 0 [2048/50176]	Loss: 4.6485
Training Epoch: 0 [2304/50176]	Loss: 4.7735
Training Epoch: 0 [2560/50176]	Loss: 4.6514
Training Epoch: 0 [2816/50176]	Loss: 4.6387
Training Epoch: 0 [3072/50176]	Loss: 4.6404
Training Epoch: 0 [3328/50176]	Loss: 4.5689
Training Epoch: 0 [3584/50176]	Loss: 4.6042
Training Epoch: 0 [3840/50176]	Loss: 4.6994
Training Epoch: 0 [4096/50176]	Loss: 4.6050
Training Epoch: 0 [4352/50176]	Loss: 4.6048
Training Epoch: 0 [4608/50176]	Loss: 4.6323
Training Epoch: 0 [4864/50176]	Loss: 4.5224
Training Epoch: 0 [5120/50176]	Loss: 4.5610
Training Epoch: 0 [5376/50176]	Loss: 4.5351
Training Epoch: 0 [5632/50176]	Loss: 4.6306
Training Epoch: 0 [5888/50176]	Loss: 4.6552
Training Epoch: 0 [6144/50176]	Loss: 4.5220
Training Epoch: 0 [6400/50176]	Loss: 4.6158
Training Epoch: 0 [6656/50176]	Loss: 4.6123
Training Epoch: 0 [6912/50176]	Loss: 4.4837
Training Epoch: 0 [7168/50176]	Loss: 4.5146
Training Epoch: 0 [7424/50176]	Loss: 4.5105
Training Epoch: 0 [7680/50176]	Loss: 4.4822
Training Epoch: 0 [7936/50176]	Loss: 4.4964
Training Epoch: 0 [8192/50176]	Loss: 4.4544
Training Epoch: 0 [8448/50176]	Loss: 4.4705
Training Epoch: 0 [8704/50176]	Loss: 4.4256
Training Epoch: 0 [8960/50176]	Loss: 4.3891
Training Epoch: 0 [9216/50176]	Loss: 4.4050
Training Epoch: 0 [9472/50176]	Loss: 4.4295
Training Epoch: 0 [9728/50176]	Loss: 4.3506
Training Epoch: 0 [9984/50176]	Loss: 4.4392
Training Epoch: 0 [10240/50176]	Loss: 4.4233
Training Epoch: 0 [10496/50176]	Loss: 4.3512
Training Epoch: 0 [10752/50176]	Loss: 4.3718
Training Epoch: 0 [11008/50176]	Loss: 4.3127
Training Epoch: 0 [11264/50176]	Loss: 4.4005
Training Epoch: 0 [11520/50176]	Loss: 4.3103
Training Epoch: 0 [11776/50176]	Loss: 4.3769
Training Epoch: 0 [12032/50176]	Loss: 4.2885
Training Epoch: 0 [12288/50176]	Loss: 4.3544
Training Epoch: 0 [12544/50176]	Loss: 4.2681
Training Epoch: 0 [12800/50176]	Loss: 4.3068
Profile done with power limit 125W
epoch 1 train time consumed: 14.64s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0321
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 125000, 'lr': 0.005656854249492381, 'energy': 122.91217166725176, 'time': 11.11393583500012, 'accuracy': 0.03212890625, 'total_cost': 328.61254367598804}
[run job] Launching job with BS 256: and LR: 0.005656854249492381 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00566+pl100', 'ZEUS_COST_THRESH': '657.2250873519761', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.005656854249492381', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.005656854249492381', '--power_limit', '100']
[run job] cost_ub=657.2250873519761
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00566+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.005656854249492381
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 5.9997
Training Epoch: 0 [768/50176]	Loss: 5.7265
Training Epoch: 0 [1024/50176]	Loss: 5.1682
Training Epoch: 0 [1280/50176]	Loss: 4.8516
Training Epoch: 0 [1536/50176]	Loss: 4.7877
Training Epoch: 0 [1792/50176]	Loss: 4.6456
Training Epoch: 0 [2048/50176]	Loss: 4.6472
Training Epoch: 0 [2304/50176]	Loss: 4.7694
Training Epoch: 0 [2560/50176]	Loss: 4.6586
Training Epoch: 0 [2816/50176]	Loss: 4.6468
Training Epoch: 0 [3072/50176]	Loss: 4.6296
Training Epoch: 0 [3328/50176]	Loss: 4.5727
Training Epoch: 0 [3584/50176]	Loss: 4.5938
Training Epoch: 0 [3840/50176]	Loss: 4.7458
Training Epoch: 0 [4096/50176]	Loss: 4.6183
Training Epoch: 0 [4352/50176]	Loss: 4.5782
Training Epoch: 0 [4608/50176]	Loss: 4.7002
Training Epoch: 0 [4864/50176]	Loss: 4.5279
Training Epoch: 0 [5120/50176]	Loss: 4.5758
Training Epoch: 0 [5376/50176]	Loss: 4.5685
Training Epoch: 0 [5632/50176]	Loss: 4.7757
Training Epoch: 0 [5888/50176]	Loss: 4.5532
Training Epoch: 0 [6144/50176]	Loss: 4.5553
Training Epoch: 0 [6400/50176]	Loss: 4.7056
Training Epoch: 0 [6656/50176]	Loss: 4.5947
Training Epoch: 0 [6912/50176]	Loss: 4.4871
Training Epoch: 0 [7168/50176]	Loss: 4.5199
Training Epoch: 0 [7424/50176]	Loss: 4.5110
Training Epoch: 0 [7680/50176]	Loss: 4.4699
Training Epoch: 0 [7936/50176]	Loss: 4.4951
Training Epoch: 0 [8192/50176]	Loss: 4.4528
Training Epoch: 0 [8448/50176]	Loss: 4.4369
Training Epoch: 0 [8704/50176]	Loss: 4.4148
Training Epoch: 0 [8960/50176]	Loss: 4.4212
Training Epoch: 0 [9216/50176]	Loss: 4.4042
Training Epoch: 0 [9472/50176]	Loss: 4.4313
Training Epoch: 0 [9728/50176]	Loss: 4.3744
Training Epoch: 0 [9984/50176]	Loss: 4.4221
Training Epoch: 0 [10240/50176]	Loss: 4.4212
Training Epoch: 0 [10496/50176]	Loss: 4.3488
Training Epoch: 0 [10752/50176]	Loss: 4.3462
Training Epoch: 0 [11008/50176]	Loss: 4.3097
Training Epoch: 0 [11264/50176]	Loss: 4.3668
Training Epoch: 0 [11520/50176]	Loss: 4.3195
Training Epoch: 0 [11776/50176]	Loss: 4.3721
Training Epoch: 0 [12032/50176]	Loss: 4.3346
Training Epoch: 0 [12288/50176]	Loss: 4.3583
Training Epoch: 0 [12544/50176]	Loss: 4.2834
Training Epoch: 0 [12800/50176]	Loss: 4.3126
Profile done with power limit 100W
epoch 1 train time consumed: 36.61s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0253
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 100000, 'lr': 0.005656854249492381, 'energy': 103.4132666654794, 'time': 28.820854689000043, 'accuracy': 0.02529296875, 'total_cost': 1011.6281840114226}

[Power Profiler] with batch size 256 and learning rate 0.006788225099390857
[run job] Launching job with BS 256: and LR: 0.006788225099390857 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00679+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.006788225099390857', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.006788225099390857', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00679+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.006788225099390857
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 6.4519
Training Epoch: 0 [768/50176]	Loss: 6.0629
Training Epoch: 0 [1024/50176]	Loss: 5.2296
Training Epoch: 0 [1280/50176]	Loss: 4.8752
Training Epoch: 0 [1536/50176]	Loss: 4.8117
Training Epoch: 0 [1792/50176]	Loss: 4.6213
Training Epoch: 0 [2048/50176]	Loss: 4.6157
Training Epoch: 0 [2304/50176]	Loss: 4.7200
Training Epoch: 0 [2560/50176]	Loss: 4.6390
Training Epoch: 0 [2816/50176]	Loss: 4.5903
Training Epoch: 0 [3072/50176]	Loss: 4.6902
Training Epoch: 0 [3328/50176]	Loss: 4.5459
Training Epoch: 0 [3584/50176]	Loss: 4.6383
Training Epoch: 0 [3840/50176]	Loss: 4.7670
Training Epoch: 0 [4096/50176]	Loss: 4.5918
Training Epoch: 0 [4352/50176]	Loss: 4.6034
Training Epoch: 0 [4608/50176]	Loss: 4.6528
Training Epoch: 0 [4864/50176]	Loss: 4.5101
Training Epoch: 0 [5120/50176]	Loss: 4.4916
Training Epoch: 0 [5376/50176]	Loss: 4.5100
Training Epoch: 0 [5632/50176]	Loss: 4.5799
Training Epoch: 0 [5888/50176]	Loss: 4.5351
Training Epoch: 0 [6144/50176]	Loss: 4.4947
Training Epoch: 0 [6400/50176]	Loss: 4.7264
Training Epoch: 0 [6656/50176]	Loss: 4.5831
Training Epoch: 0 [6912/50176]	Loss: 4.4293
Training Epoch: 0 [7168/50176]	Loss: 4.5014
Training Epoch: 0 [7424/50176]	Loss: 4.5495
Training Epoch: 0 [7680/50176]	Loss: 4.4805
Training Epoch: 0 [7936/50176]	Loss: 4.4802
Training Epoch: 0 [8192/50176]	Loss: 4.4623
Training Epoch: 0 [8448/50176]	Loss: 4.4527
Training Epoch: 0 [8704/50176]	Loss: 4.4263
Training Epoch: 0 [8960/50176]	Loss: 4.4121
Training Epoch: 0 [9216/50176]	Loss: 4.3688
Training Epoch: 0 [9472/50176]	Loss: 4.4110
Training Epoch: 0 [9728/50176]	Loss: 4.3757
Training Epoch: 0 [9984/50176]	Loss: 4.4119
Training Epoch: 0 [10240/50176]	Loss: 4.4149
Training Epoch: 0 [10496/50176]	Loss: 4.3470
Training Epoch: 0 [10752/50176]	Loss: 4.3563
Training Epoch: 0 [11008/50176]	Loss: 4.3157
Training Epoch: 0 [11264/50176]	Loss: 4.4775
Training Epoch: 0 [11520/50176]	Loss: 4.3510
Training Epoch: 0 [11776/50176]	Loss: 4.3896
Training Epoch: 0 [12032/50176]	Loss: 4.3094
Training Epoch: 0 [12288/50176]	Loss: 4.4567
Training Epoch: 0 [12544/50176]	Loss: 4.3062
Training Epoch: 0 [12800/50176]	Loss: 4.3699
Profile done with power limit 175W
epoch 1 train time consumed: 12.66s
Validation Epoch: 0, Average loss: 0.0187, Accuracy: 0.0265
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 175000, 'lr': 0.006788225099390857, 'energy': 154.40490943520513, 'time': 9.504084484000032, 'accuracy': 0.02646484375, 'total_cost': 377.2202230549371}
[run job] Launching job with BS 256: and LR: 0.006788225099390857 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00679+pl150', 'ZEUS_COST_THRESH': '754.4404461098742', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.006788225099390857', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.006788225099390857', '--power_limit', '150']
[run job] cost_ub=754.4404461098742
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00679+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.006788225099390857
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 6.4519
Training Epoch: 0 [768/50176]	Loss: 6.0641
Training Epoch: 0 [1024/50176]	Loss: 5.2546
Training Epoch: 0 [1280/50176]	Loss: 4.8732
Training Epoch: 0 [1536/50176]	Loss: 4.8130
Training Epoch: 0 [1792/50176]	Loss: 4.6268
Training Epoch: 0 [2048/50176]	Loss: 4.6104
Training Epoch: 0 [2304/50176]	Loss: 4.7245
Training Epoch: 0 [2560/50176]	Loss: 4.6404
Training Epoch: 0 [2816/50176]	Loss: 4.5831
Training Epoch: 0 [3072/50176]	Loss: 4.7241
Training Epoch: 0 [3328/50176]	Loss: 4.5572
Training Epoch: 0 [3584/50176]	Loss: 4.6548
Training Epoch: 0 [3840/50176]	Loss: 4.7073
Training Epoch: 0 [4096/50176]	Loss: 4.5943
Training Epoch: 0 [4352/50176]	Loss: 4.5714
Training Epoch: 0 [4608/50176]	Loss: 4.6696
Training Epoch: 0 [4864/50176]	Loss: 4.5157
Training Epoch: 0 [5120/50176]	Loss: 4.5178
Training Epoch: 0 [5376/50176]	Loss: 4.5151
Training Epoch: 0 [5632/50176]	Loss: 4.5858
Training Epoch: 0 [5888/50176]	Loss: 4.6647
Training Epoch: 0 [6144/50176]	Loss: 4.4545
Training Epoch: 0 [6400/50176]	Loss: 4.7474
Training Epoch: 0 [6656/50176]	Loss: 4.5656
Training Epoch: 0 [6912/50176]	Loss: 4.4465
Training Epoch: 0 [7168/50176]	Loss: 4.4940
Training Epoch: 0 [7424/50176]	Loss: 4.4829
Training Epoch: 0 [7680/50176]	Loss: 4.4943
Training Epoch: 0 [7936/50176]	Loss: 4.4976
Training Epoch: 0 [8192/50176]	Loss: 4.4700
Training Epoch: 0 [8448/50176]	Loss: 4.4682
Training Epoch: 0 [8704/50176]	Loss: 4.4186
Training Epoch: 0 [8960/50176]	Loss: 4.4042
Training Epoch: 0 [9216/50176]	Loss: 4.3974
Training Epoch: 0 [9472/50176]	Loss: 4.4033
Training Epoch: 0 [9728/50176]	Loss: 4.4005
Training Epoch: 0 [9984/50176]	Loss: 4.4268
Training Epoch: 0 [10240/50176]	Loss: 4.3867
Training Epoch: 0 [10496/50176]	Loss: 4.3810
Training Epoch: 0 [10752/50176]	Loss: 4.3480
Training Epoch: 0 [11008/50176]	Loss: 4.3053
Training Epoch: 0 [11264/50176]	Loss: 4.5144
Training Epoch: 0 [11520/50176]	Loss: 4.3627
Training Epoch: 0 [11776/50176]	Loss: 4.3717
Training Epoch: 0 [12032/50176]	Loss: 4.3193
Training Epoch: 0 [12288/50176]	Loss: 4.3323
Training Epoch: 0 [12544/50176]	Loss: 4.2764
Training Epoch: 0 [12800/50176]	Loss: 4.3520
Profile done with power limit 150W
epoch 1 train time consumed: 13.04s
Validation Epoch: 0, Average loss: 0.0188, Accuracy: 0.0267
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 150000, 'lr': 0.006788225099390857, 'energy': 147.11409435022975, 'time': 9.764007869000125, 'accuracy': 0.02666015625, 'total_cost': 376.18294708248385}
[run job] Launching job with BS 256: and LR: 0.006788225099390857 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00679+pl125', 'ZEUS_COST_THRESH': '752.3658941649677', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.006788225099390857', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.006788225099390857', '--power_limit', '125']
[run job] cost_ub=752.3658941649677
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00679+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.006788225099390857
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 6.4519
Training Epoch: 0 [768/50176]	Loss: 6.0645
Training Epoch: 0 [1024/50176]	Loss: 5.2368
Training Epoch: 0 [1280/50176]	Loss: 4.8744
Training Epoch: 0 [1536/50176]	Loss: 4.8075
Training Epoch: 0 [1792/50176]	Loss: 4.6226
Training Epoch: 0 [2048/50176]	Loss: 4.6049
Training Epoch: 0 [2304/50176]	Loss: 4.7208
Training Epoch: 0 [2560/50176]	Loss: 4.6377
Training Epoch: 0 [2816/50176]	Loss: 4.5943
Training Epoch: 0 [3072/50176]	Loss: 4.7088
Training Epoch: 0 [3328/50176]	Loss: 4.5525
Training Epoch: 0 [3584/50176]	Loss: 4.6515
Training Epoch: 0 [3840/50176]	Loss: 4.7850
Training Epoch: 0 [4096/50176]	Loss: 4.6082
Training Epoch: 0 [4352/50176]	Loss: 4.5859
Training Epoch: 0 [4608/50176]	Loss: 4.6301
Training Epoch: 0 [4864/50176]	Loss: 4.5569
Training Epoch: 0 [5120/50176]	Loss: 4.5343
Training Epoch: 0 [5376/50176]	Loss: 4.5175
Training Epoch: 0 [5632/50176]	Loss: 4.5647
Training Epoch: 0 [5888/50176]	Loss: 4.5711
Training Epoch: 0 [6144/50176]	Loss: 4.4555
Training Epoch: 0 [6400/50176]	Loss: 4.7610
Training Epoch: 0 [6656/50176]	Loss: 4.5491
Training Epoch: 0 [6912/50176]	Loss: 4.4545
Training Epoch: 0 [7168/50176]	Loss: 4.5159
Training Epoch: 0 [7424/50176]	Loss: 4.5458
Training Epoch: 0 [7680/50176]	Loss: 4.4870
Training Epoch: 0 [7936/50176]	Loss: 4.5422
Training Epoch: 0 [8192/50176]	Loss: 4.4523
Training Epoch: 0 [8448/50176]	Loss: 4.4803
Training Epoch: 0 [8704/50176]	Loss: 4.5281
Training Epoch: 0 [8960/50176]	Loss: 4.4384
Training Epoch: 0 [9216/50176]	Loss: 4.3740
Training Epoch: 0 [9472/50176]	Loss: 4.4409
Training Epoch: 0 [9728/50176]	Loss: 4.3600
Training Epoch: 0 [9984/50176]	Loss: 4.3948
Training Epoch: 0 [10240/50176]	Loss: 4.4239
Training Epoch: 0 [10496/50176]	Loss: 4.3559
Training Epoch: 0 [10752/50176]	Loss: 4.3525
Training Epoch: 0 [11008/50176]	Loss: 4.3163
Training Epoch: 0 [11264/50176]	Loss: 4.4413
Training Epoch: 0 [11520/50176]	Loss: 4.3244
Training Epoch: 0 [11776/50176]	Loss: 4.3938
Training Epoch: 0 [12032/50176]	Loss: 4.3960
Training Epoch: 0 [12288/50176]	Loss: 4.3599
Training Epoch: 0 [12544/50176]	Loss: 4.2901
Training Epoch: 0 [12800/50176]	Loss: 4.3344
Profile done with power limit 125W
epoch 1 train time consumed: 14.62s
Validation Epoch: 0, Average loss: 0.0178, Accuracy: 0.0314
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 125000, 'lr': 0.006788225099390857, 'energy': 123.04611985048531, 'time': 11.10592756899996, 'accuracy': 0.0314453125, 'total_cost': 335.66521681135464}
[run job] Launching job with BS 256: and LR: 0.006788225099390857 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs256+lr0.00679+pl100', 'ZEUS_COST_THRESH': '671.3304336227093', 'ZEUS_BATCH_SIZE': '256', 'ZEUS_LEARNING_RATE': '0.006788225099390857', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '256', '--epochs', '100', '--seed', '1', '--learning_rate', '0.006788225099390857', '--power_limit', '100']
[run job] cost_ub=671.3304336227093
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs256+lr0.00679+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.006788225099390857
batch size arg: 256
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [256/50176]	Loss: 4.6571
Training Epoch: 0 [512/50176]	Loss: 6.4519
Training Epoch: 0 [768/50176]	Loss: 6.0642
Training Epoch: 0 [1024/50176]	Loss: 5.2629
Training Epoch: 0 [1280/50176]	Loss: 4.8814
Training Epoch: 0 [1536/50176]	Loss: 4.8027
Training Epoch: 0 [1792/50176]	Loss: 4.6198
Training Epoch: 0 [2048/50176]	Loss: 4.6190
Training Epoch: 0 [2304/50176]	Loss: 4.7281
Training Epoch: 0 [2560/50176]	Loss: 4.6140
Training Epoch: 0 [2816/50176]	Loss: 4.5957
Training Epoch: 0 [3072/50176]	Loss: 4.7183
Training Epoch: 0 [3328/50176]	Loss: 4.6181
Training Epoch: 0 [3584/50176]	Loss: 4.6730
Training Epoch: 0 [3840/50176]	Loss: 4.6187
Training Epoch: 0 [4096/50176]	Loss: 4.6173
Training Epoch: 0 [4352/50176]	Loss: 4.6308
Training Epoch: 0 [4608/50176]	Loss: 4.6108
Training Epoch: 0 [4864/50176]	Loss: 4.5088
Training Epoch: 0 [5120/50176]	Loss: 4.5192
Training Epoch: 0 [5376/50176]	Loss: 4.5166
Training Epoch: 0 [5632/50176]	Loss: 4.5966
Training Epoch: 0 [5888/50176]	Loss: 4.5043
Training Epoch: 0 [6144/50176]	Loss: 4.5368
Training Epoch: 0 [6400/50176]	Loss: 4.7733
Training Epoch: 0 [6656/50176]	Loss: 4.5369
Training Epoch: 0 [6912/50176]	Loss: 4.4591
Training Epoch: 0 [7168/50176]	Loss: 4.4939
Training Epoch: 0 [7424/50176]	Loss: 4.5109
Training Epoch: 0 [7680/50176]	Loss: 4.4623
Training Epoch: 0 [7936/50176]	Loss: 4.4710
Training Epoch: 0 [8192/50176]	Loss: 4.4948
Training Epoch: 0 [8448/50176]	Loss: 4.5142
Training Epoch: 0 [8704/50176]	Loss: 4.4269
Training Epoch: 0 [8960/50176]	Loss: 4.4209
Training Epoch: 0 [9216/50176]	Loss: 4.4046
Training Epoch: 0 [9472/50176]	Loss: 4.5620
Training Epoch: 0 [9728/50176]	Loss: 4.3885
Training Epoch: 0 [9984/50176]	Loss: 4.3900
Training Epoch: 0 [10240/50176]	Loss: 4.4202
Training Epoch: 0 [10496/50176]	Loss: 4.3559
Training Epoch: 0 [10752/50176]	Loss: 4.3420
Training Epoch: 0 [11008/50176]	Loss: 4.3201
Training Epoch: 0 [11264/50176]	Loss: 4.3598
Training Epoch: 0 [11520/50176]	Loss: 4.3647
Training Epoch: 0 [11776/50176]	Loss: 4.4144
Training Epoch: 0 [12032/50176]	Loss: 4.3420
Training Epoch: 0 [12288/50176]	Loss: 4.3827
Training Epoch: 0 [12544/50176]	Loss: 4.3631
Training Epoch: 0 [12800/50176]	Loss: 4.3409
Profile done with power limit 100W
epoch 1 train time consumed: 36.77s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0239
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 256, 'pl': 100000, 'lr': 0.006788225099390857, 'energy': 103.0294214214699, 'time': 28.971577270000125, 'accuracy': 0.02392578125, 'total_cost': 1073.5461379152746}

[Power Profiler] with batch size 512 and learning rate 0.0064
[run job] Launching job with BS 512: and LR: 0.0064 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00640+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0064', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0064', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00640+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0064
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 5.9978
Training Epoch: 0 [1536/50176]	Loss: 5.4431
Training Epoch: 0 [2048/50176]	Loss: 4.6747
Training Epoch: 0 [2560/50176]	Loss: 4.8386
Training Epoch: 0 [3072/50176]	Loss: 4.7377
Training Epoch: 0 [3584/50176]	Loss: 4.6539
Training Epoch: 0 [4096/50176]	Loss: 4.7369
Training Epoch: 0 [4608/50176]	Loss: 4.6273
Training Epoch: 0 [5120/50176]	Loss: 4.5777
Training Epoch: 0 [5632/50176]	Loss: 4.6782
Training Epoch: 0 [6144/50176]	Loss: 4.6368
Training Epoch: 0 [6656/50176]	Loss: 4.6708
Training Epoch: 0 [7168/50176]	Loss: 4.6099
Training Epoch: 0 [7680/50176]	Loss: 4.6174
Training Epoch: 0 [8192/50176]	Loss: 4.6034
Training Epoch: 0 [8704/50176]	Loss: 4.5913
Training Epoch: 0 [9216/50176]	Loss: 4.6029
Training Epoch: 0 [9728/50176]	Loss: 4.6799
Training Epoch: 0 [10240/50176]	Loss: 4.5456
Training Epoch: 0 [10752/50176]	Loss: 4.5541
Training Epoch: 0 [11264/50176]	Loss: 4.5149
Training Epoch: 0 [11776/50176]	Loss: 4.5644
Training Epoch: 0 [12288/50176]	Loss: 4.5941
Training Epoch: 0 [12800/50176]	Loss: 4.5406
Training Epoch: 0 [13312/50176]	Loss: 4.5160
Training Epoch: 0 [13824/50176]	Loss: 4.4650
Training Epoch: 0 [14336/50176]	Loss: 4.4617
Training Epoch: 0 [14848/50176]	Loss: 4.3957
Training Epoch: 0 [15360/50176]	Loss: 4.4089
Training Epoch: 0 [15872/50176]	Loss: 4.3872
Training Epoch: 0 [16384/50176]	Loss: 4.3098
Training Epoch: 0 [16896/50176]	Loss: 4.3347
Training Epoch: 0 [17408/50176]	Loss: 4.3992
Training Epoch: 0 [17920/50176]	Loss: 4.3776
Training Epoch: 0 [18432/50176]	Loss: 4.2912
Training Epoch: 0 [18944/50176]	Loss: 4.2489
Training Epoch: 0 [19456/50176]	Loss: 4.2832
Training Epoch: 0 [19968/50176]	Loss: 4.2919
Training Epoch: 0 [20480/50176]	Loss: 4.2576
Training Epoch: 0 [20992/50176]	Loss: 4.1830
Training Epoch: 0 [21504/50176]	Loss: 4.2590
Training Epoch: 0 [22016/50176]	Loss: 4.3052
Training Epoch: 0 [22528/50176]	Loss: 4.2114
Training Epoch: 0 [23040/50176]	Loss: 4.1898
Training Epoch: 0 [23552/50176]	Loss: 4.2054
Training Epoch: 0 [24064/50176]	Loss: 4.2309
Training Epoch: 0 [24576/50176]	Loss: 4.2887
Training Epoch: 0 [25088/50176]	Loss: 4.2037
Training Epoch: 0 [25600/50176]	Loss: 4.2175
Profile done with power limit 175W
epoch 1 train time consumed: 23.76s
Validation Epoch: 0, Average loss: 0.0088, Accuracy: 0.0444
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 175000, 'lr': 0.0064, 'energy': 155.2678766283314, 'time': 18.130308626999977, 'accuracy': 0.04443359375, 'total_cost': 859.4369726457916}
[run job] Launching job with BS 512: and LR: 0.0064 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00640+pl150', 'ZEUS_COST_THRESH': '1718.8739452915831', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0064', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0064', '--power_limit', '150']
[run job] cost_ub=1718.8739452915831
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00640+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0064
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 5.9978
Training Epoch: 0 [1536/50176]	Loss: 5.4426
Training Epoch: 0 [2048/50176]	Loss: 4.6730
Training Epoch: 0 [2560/50176]	Loss: 4.8400
Training Epoch: 0 [3072/50176]	Loss: 4.7392
Training Epoch: 0 [3584/50176]	Loss: 4.6645
Training Epoch: 0 [4096/50176]	Loss: 4.7302
Training Epoch: 0 [4608/50176]	Loss: 4.6600
Training Epoch: 0 [5120/50176]	Loss: 4.5718
Training Epoch: 0 [5632/50176]	Loss: 4.6830
Training Epoch: 0 [6144/50176]	Loss: 4.6343
Training Epoch: 0 [6656/50176]	Loss: 4.6689
Training Epoch: 0 [7168/50176]	Loss: 4.6121
Training Epoch: 0 [7680/50176]	Loss: 4.5929
Training Epoch: 0 [8192/50176]	Loss: 4.6029
Training Epoch: 0 [8704/50176]	Loss: 4.6168
Training Epoch: 0 [9216/50176]	Loss: 4.5788
Training Epoch: 0 [9728/50176]	Loss: 4.6417
Training Epoch: 0 [10240/50176]	Loss: 4.5417
Training Epoch: 0 [10752/50176]	Loss: 4.5468
Training Epoch: 0 [11264/50176]	Loss: 4.5498
Training Epoch: 0 [11776/50176]	Loss: 4.6235
Training Epoch: 0 [12288/50176]	Loss: 4.5293
Training Epoch: 0 [12800/50176]	Loss: 4.5853
Training Epoch: 0 [13312/50176]	Loss: 4.5194
Training Epoch: 0 [13824/50176]	Loss: 4.4953
Training Epoch: 0 [14336/50176]	Loss: 4.5335
Training Epoch: 0 [14848/50176]	Loss: 4.4641
Training Epoch: 0 [15360/50176]	Loss: 4.4585
Training Epoch: 0 [15872/50176]	Loss: 4.4629
Training Epoch: 0 [16384/50176]	Loss: 4.4145
Training Epoch: 0 [16896/50176]	Loss: 4.4350
Training Epoch: 0 [17408/50176]	Loss: 4.4544
Training Epoch: 0 [17920/50176]	Loss: 4.4603
Training Epoch: 0 [18432/50176]	Loss: 4.3778
Training Epoch: 0 [18944/50176]	Loss: 4.3273
Training Epoch: 0 [19456/50176]	Loss: 4.3428
Training Epoch: 0 [19968/50176]	Loss: 4.3147
Training Epoch: 0 [20480/50176]	Loss: 4.2729
Training Epoch: 0 [20992/50176]	Loss: 4.2412
Training Epoch: 0 [21504/50176]	Loss: 4.2978
Training Epoch: 0 [22016/50176]	Loss: 4.2930
Training Epoch: 0 [22528/50176]	Loss: 4.3039
Training Epoch: 0 [23040/50176]	Loss: 4.2707
Training Epoch: 0 [23552/50176]	Loss: 4.2560
Training Epoch: 0 [24064/50176]	Loss: 4.2927
Training Epoch: 0 [24576/50176]	Loss: 4.2355
Training Epoch: 0 [25088/50176]	Loss: 4.2663
Training Epoch: 0 [25600/50176]	Loss: 4.1940
Profile done with power limit 150W
epoch 1 train time consumed: 24.43s
Validation Epoch: 0, Average loss: 0.0083, Accuracy: 0.0407
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 150000, 'lr': 0.0064, 'energy': 147.06060958898496, 'time': 18.67529945699971, 'accuracy': 0.04072265625, 'total_cost': 941.9395413111207}
[run job] Launching job with BS 512: and LR: 0.0064 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00640+pl125', 'ZEUS_COST_THRESH': '1718.8739452915831', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0064', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0064', '--power_limit', '125']
[run job] cost_ub=1718.8739452915831
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00640+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0064
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 5.9978
Training Epoch: 0 [1536/50176]	Loss: 5.4434
Training Epoch: 0 [2048/50176]	Loss: 4.6756
Training Epoch: 0 [2560/50176]	Loss: 4.8412
Training Epoch: 0 [3072/50176]	Loss: 4.7389
Training Epoch: 0 [3584/50176]	Loss: 4.6596
Training Epoch: 0 [4096/50176]	Loss: 4.7296
Training Epoch: 0 [4608/50176]	Loss: 4.6725
Training Epoch: 0 [5120/50176]	Loss: 4.5760
Training Epoch: 0 [5632/50176]	Loss: 4.6782
Training Epoch: 0 [6144/50176]	Loss: 4.6386
Training Epoch: 0 [6656/50176]	Loss: 4.6535
Training Epoch: 0 [7168/50176]	Loss: 4.6072
Training Epoch: 0 [7680/50176]	Loss: 4.5954
Training Epoch: 0 [8192/50176]	Loss: 4.6044
Training Epoch: 0 [8704/50176]	Loss: 4.5911
Training Epoch: 0 [9216/50176]	Loss: 4.5751
Training Epoch: 0 [9728/50176]	Loss: 4.6645
Training Epoch: 0 [10240/50176]	Loss: 4.5386
Training Epoch: 0 [10752/50176]	Loss: 4.5534
Training Epoch: 0 [11264/50176]	Loss: 4.5131
Training Epoch: 0 [11776/50176]	Loss: 4.5634
Training Epoch: 0 [12288/50176]	Loss: 4.5421
Training Epoch: 0 [12800/50176]	Loss: 4.4976
Training Epoch: 0 [13312/50176]	Loss: 4.4886
Training Epoch: 0 [13824/50176]	Loss: 4.4690
Training Epoch: 0 [14336/50176]	Loss: 4.4851
Training Epoch: 0 [14848/50176]	Loss: 4.4388
Training Epoch: 0 [15360/50176]	Loss: 4.3802
Training Epoch: 0 [15872/50176]	Loss: 4.3734
Training Epoch: 0 [16384/50176]	Loss: 4.2872
Training Epoch: 0 [16896/50176]	Loss: 4.3455
Training Epoch: 0 [17408/50176]	Loss: 4.3843
Training Epoch: 0 [17920/50176]	Loss: 4.4007
Training Epoch: 0 [18432/50176]	Loss: 4.3127
Training Epoch: 0 [18944/50176]	Loss: 4.2883
Training Epoch: 0 [19456/50176]	Loss: 4.2823
Training Epoch: 0 [19968/50176]	Loss: 4.2890
Training Epoch: 0 [20480/50176]	Loss: 4.2508
Training Epoch: 0 [20992/50176]	Loss: 4.1993
Training Epoch: 0 [21504/50176]	Loss: 4.2581
Training Epoch: 0 [22016/50176]	Loss: 4.2952
Training Epoch: 0 [22528/50176]	Loss: 4.2921
Training Epoch: 0 [23040/50176]	Loss: 4.2328
Training Epoch: 0 [23552/50176]	Loss: 4.2230
Training Epoch: 0 [24064/50176]	Loss: 4.2654
Training Epoch: 0 [24576/50176]	Loss: 4.2287
Training Epoch: 0 [25088/50176]	Loss: 4.2521
Training Epoch: 0 [25600/50176]	Loss: 4.2242
Profile done with power limit 125W
epoch 1 train time consumed: 27.74s
Validation Epoch: 0, Average loss: 0.0123, Accuracy: 0.0387
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 125000, 'lr': 0.0064, 'energy': 123.08272827567056, 'time': 21.39033102700023, 'accuracy': 0.038671875, 'total_cost': 1051.5090878166075}
[run job] Launching job with BS 512: and LR: 0.0064 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00640+pl100', 'ZEUS_COST_THRESH': '1718.8739452915831', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0064', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0064', '--power_limit', '100']
[run job] cost_ub=1718.8739452915831
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00640+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0064
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 5.9978
Training Epoch: 0 [1536/50176]	Loss: 5.4437
Training Epoch: 0 [2048/50176]	Loss: 4.6777
Training Epoch: 0 [2560/50176]	Loss: 4.8394
Training Epoch: 0 [3072/50176]	Loss: 4.7368
Training Epoch: 0 [3584/50176]	Loss: 4.6577
Training Epoch: 0 [4096/50176]	Loss: 4.7113
Training Epoch: 0 [4608/50176]	Loss: 4.6574
Training Epoch: 0 [5120/50176]	Loss: 4.5760
Training Epoch: 0 [5632/50176]	Loss: 4.6863
Training Epoch: 0 [6144/50176]	Loss: 4.6492
Training Epoch: 0 [6656/50176]	Loss: 4.6555
Training Epoch: 0 [7168/50176]	Loss: 4.6204
Training Epoch: 0 [7680/50176]	Loss: 4.6176
Training Epoch: 0 [8192/50176]	Loss: 4.5969
Training Epoch: 0 [8704/50176]	Loss: 4.6020
Training Epoch: 0 [9216/50176]	Loss: 4.6319
Training Epoch: 0 [9728/50176]	Loss: 4.7620
Training Epoch: 0 [10240/50176]	Loss: 4.5489
Training Epoch: 0 [10752/50176]	Loss: 4.5428
Training Epoch: 0 [11264/50176]	Loss: 4.5105
Training Epoch: 0 [11776/50176]	Loss: 4.5668
Training Epoch: 0 [12288/50176]	Loss: 4.5841
Training Epoch: 0 [12800/50176]	Loss: 4.5386
Training Epoch: 0 [13312/50176]	Loss: 4.5073
Training Epoch: 0 [13824/50176]	Loss: 4.4719
Training Epoch: 0 [14336/50176]	Loss: 4.4610
Training Epoch: 0 [14848/50176]	Loss: 4.4391
Training Epoch: 0 [15360/50176]	Loss: 4.4444
Training Epoch: 0 [15872/50176]	Loss: 4.4014
Training Epoch: 0 [16384/50176]	Loss: 4.3314
Training Epoch: 0 [16896/50176]	Loss: 4.3474
Training Epoch: 0 [17408/50176]	Loss: 4.3822
Training Epoch: 0 [17920/50176]	Loss: 4.4056
Training Epoch: 0 [18432/50176]	Loss: 4.3120
Training Epoch: 0 [18944/50176]	Loss: 4.2628
Training Epoch: 0 [19456/50176]	Loss: 4.2943
Training Epoch: 0 [19968/50176]	Loss: 4.2764
Training Epoch: 0 [20480/50176]	Loss: 4.2529
Training Epoch: 0 [20992/50176]	Loss: 4.2043
Training Epoch: 0 [21504/50176]	Loss: 4.2261
Training Epoch: 0 [22016/50176]	Loss: 4.3508
Training Epoch: 0 [22528/50176]	Loss: 4.2540
Training Epoch: 0 [23040/50176]	Loss: 4.2386
Training Epoch: 0 [23552/50176]	Loss: 4.2486
Training Epoch: 0 [24064/50176]	Loss: 4.2537
Training Epoch: 0 [24576/50176]	Loss: 4.2190
Training Epoch: 0 [25088/50176]	Loss: 4.2196
Training Epoch: 0 [25600/50176]	Loss: 4.2147
Profile done with power limit 100W
epoch 1 train time consumed: 66.11s
Validation Epoch: 0, Average loss: 0.0089, Accuracy: 0.0371
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 100000, 'lr': 0.0064, 'energy': 103.7810616993493, 'time': 52.073935949000315, 'accuracy': 0.037109375, 'total_cost': 2494.904773486783}

[Power Profiler] with batch size 512 and learning rate 0.008
[run job] Launching job with BS 512: and LR: 0.008 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00800+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.008', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00800+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.008
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 6.5227
Training Epoch: 0 [1536/50176]	Loss: 5.2992
Training Epoch: 0 [2048/50176]	Loss: 4.6769
Training Epoch: 0 [2560/50176]	Loss: 4.8504
Training Epoch: 0 [3072/50176]	Loss: 4.6668
Training Epoch: 0 [3584/50176]	Loss: 4.6915
Training Epoch: 0 [4096/50176]	Loss: 4.6781
Training Epoch: 0 [4608/50176]	Loss: 4.6823
Training Epoch: 0 [5120/50176]	Loss: 4.7352
Training Epoch: 0 [5632/50176]	Loss: 4.6701
Training Epoch: 0 [6144/50176]	Loss: 4.6208
Training Epoch: 0 [6656/50176]	Loss: 4.6685
Training Epoch: 0 [7168/50176]	Loss: 4.6294
Training Epoch: 0 [7680/50176]	Loss: 4.6120
Training Epoch: 0 [8192/50176]	Loss: 4.6538
Training Epoch: 0 [8704/50176]	Loss: 4.6360
Training Epoch: 0 [9216/50176]	Loss: 4.5985
Training Epoch: 0 [9728/50176]	Loss: 4.5935
Training Epoch: 0 [10240/50176]	Loss: 4.8725
Training Epoch: 0 [10752/50176]	Loss: 4.5757
Training Epoch: 0 [11264/50176]	Loss: 4.6036
Training Epoch: 0 [11776/50176]	Loss: 4.5953
Training Epoch: 0 [12288/50176]	Loss: 4.6156
Training Epoch: 0 [12800/50176]	Loss: 4.5934
Training Epoch: 0 [13312/50176]	Loss: 4.5492
Training Epoch: 0 [13824/50176]	Loss: 4.5861
Training Epoch: 0 [14336/50176]	Loss: 4.4921
Training Epoch: 0 [14848/50176]	Loss: 4.5060
Training Epoch: 0 [15360/50176]	Loss: 4.5419
Training Epoch: 0 [15872/50176]	Loss: 4.4582
Training Epoch: 0 [16384/50176]	Loss: 4.4640
Training Epoch: 0 [16896/50176]	Loss: 4.4560
Training Epoch: 0 [17408/50176]	Loss: 4.4915
Training Epoch: 0 [17920/50176]	Loss: 4.4843
Training Epoch: 0 [18432/50176]	Loss: 4.4164
Training Epoch: 0 [18944/50176]	Loss: 4.4110
Training Epoch: 0 [19456/50176]	Loss: 4.4087
Training Epoch: 0 [19968/50176]	Loss: 4.4453
Training Epoch: 0 [20480/50176]	Loss: 4.4275
Training Epoch: 0 [20992/50176]	Loss: 4.3590
Training Epoch: 0 [21504/50176]	Loss: 4.4223
Training Epoch: 0 [22016/50176]	Loss: 4.4516
Training Epoch: 0 [22528/50176]	Loss: 4.3372
Training Epoch: 0 [23040/50176]	Loss: 4.3521
Training Epoch: 0 [23552/50176]	Loss: 4.4245
Training Epoch: 0 [24064/50176]	Loss: 4.4034
Training Epoch: 0 [24576/50176]	Loss: 4.3644
Training Epoch: 0 [25088/50176]	Loss: 4.3718
Training Epoch: 0 [25600/50176]	Loss: 4.3859
Profile done with power limit 175W
epoch 1 train time consumed: 23.73s
Validation Epoch: 0, Average loss: 0.0104, Accuracy: 0.0215
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 175000, 'lr': 0.008, 'energy': 155.49126038999586, 'time': 18.120286468999893, 'accuracy': 0.021484375, 'total_cost': 1777.690928020056}
[run job] Launching job with BS 512: and LR: 0.008 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00800+pl150', 'ZEUS_COST_THRESH': '3555.381856040112', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.008', '--power_limit', '150']
[run job] cost_ub=3555.381856040112
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00800+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.008
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 6.5227
Training Epoch: 0 [1536/50176]	Loss: 5.2995
Training Epoch: 0 [2048/50176]	Loss: 4.6761
Training Epoch: 0 [2560/50176]	Loss: 4.8389
Training Epoch: 0 [3072/50176]	Loss: 4.6682
Training Epoch: 0 [3584/50176]	Loss: 4.6503
Training Epoch: 0 [4096/50176]	Loss: 4.6810
Training Epoch: 0 [4608/50176]	Loss: 4.6508
Training Epoch: 0 [5120/50176]	Loss: 4.7684
Training Epoch: 0 [5632/50176]	Loss: 5.5476
Training Epoch: 0 [6144/50176]	Loss: 5.4268
Training Epoch: 0 [6656/50176]	Loss: 5.1500
Training Epoch: 0 [7168/50176]	Loss: 4.9028
Training Epoch: 0 [7680/50176]	Loss: 4.7141
Training Epoch: 0 [8192/50176]	Loss: 4.6565
Training Epoch: 0 [8704/50176]	Loss: 4.7296
Training Epoch: 0 [9216/50176]	Loss: 4.7718
Training Epoch: 0 [9728/50176]	Loss: 4.6664
Training Epoch: 0 [10240/50176]	Loss: 4.6145
Training Epoch: 0 [10752/50176]	Loss: 4.6370
Training Epoch: 0 [11264/50176]	Loss: 4.5956
Training Epoch: 0 [11776/50176]	Loss: 4.6445
Training Epoch: 0 [12288/50176]	Loss: 4.6380
Training Epoch: 0 [12800/50176]	Loss: 4.6335
Training Epoch: 0 [13312/50176]	Loss: 4.6629
Training Epoch: 0 [13824/50176]	Loss: 4.6142
Training Epoch: 0 [14336/50176]	Loss: 4.6530
Training Epoch: 0 [14848/50176]	Loss: 4.6227
Training Epoch: 0 [15360/50176]	Loss: 4.5941
Training Epoch: 0 [15872/50176]	Loss: 4.6376
Training Epoch: 0 [16384/50176]	Loss: 4.6346
Training Epoch: 0 [16896/50176]	Loss: 4.5779
Training Epoch: 0 [17408/50176]	Loss: 4.5884
Training Epoch: 0 [17920/50176]	Loss: 4.5847
Training Epoch: 0 [18432/50176]	Loss: 4.6201
Training Epoch: 0 [18944/50176]	Loss: 4.6851
Training Epoch: 0 [19456/50176]	Loss: 4.6216
Training Epoch: 0 [19968/50176]	Loss: 4.5755
Training Epoch: 0 [20480/50176]	Loss: 4.5827
Training Epoch: 0 [20992/50176]	Loss: 4.5573
Training Epoch: 0 [21504/50176]	Loss: 4.5677
Training Epoch: 0 [22016/50176]	Loss: 4.5439
Training Epoch: 0 [22528/50176]	Loss: 4.5453
Training Epoch: 0 [23040/50176]	Loss: 4.5614
Training Epoch: 0 [23552/50176]	Loss: 4.5564
Training Epoch: 0 [24064/50176]	Loss: 4.5332
Training Epoch: 0 [24576/50176]	Loss: 4.5175
Training Epoch: 0 [25088/50176]	Loss: 4.4966
Training Epoch: 0 [25600/50176]	Loss: 4.4781
Profile done with power limit 150W
epoch 1 train time consumed: 24.47s
Validation Epoch: 0, Average loss: 0.0089, Accuracy: 0.0220
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 150000, 'lr': 0.008, 'energy': 147.04288415088283, 'time': 18.718716961000155, 'accuracy': 0.02197265625, 'total_cost': 1749.6902233758296}
[run job] Launching job with BS 512: and LR: 0.008 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00800+pl125', 'ZEUS_COST_THRESH': '3499.3804467516593', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.008', '--power_limit', '125']
[run job] cost_ub=3499.3804467516593
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00800+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.008
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 6.5227
Training Epoch: 0 [1536/50176]	Loss: 5.2998
Training Epoch: 0 [2048/50176]	Loss: 4.6781
Training Epoch: 0 [2560/50176]	Loss: 4.8548
Training Epoch: 0 [3072/50176]	Loss: 4.6674
Training Epoch: 0 [3584/50176]	Loss: 4.6770
Training Epoch: 0 [4096/50176]	Loss: 4.6936
Training Epoch: 0 [4608/50176]	Loss: 4.6495
Training Epoch: 0 [5120/50176]	Loss: 4.7996
Training Epoch: 0 [5632/50176]	Loss: 4.6600
Training Epoch: 0 [6144/50176]	Loss: 4.7264
Training Epoch: 0 [6656/50176]	Loss: 4.6799
Training Epoch: 0 [7168/50176]	Loss: 4.6680
Training Epoch: 0 [7680/50176]	Loss: 4.6396
Training Epoch: 0 [8192/50176]	Loss: 4.7154
Training Epoch: 0 [8704/50176]	Loss: 4.7362
Training Epoch: 0 [9216/50176]	Loss: 4.7659
Training Epoch: 0 [9728/50176]	Loss: 4.6708
Training Epoch: 0 [10240/50176]	Loss: 4.5803
Training Epoch: 0 [10752/50176]	Loss: 4.5510
Training Epoch: 0 [11264/50176]	Loss: 4.5683
Training Epoch: 0 [11776/50176]	Loss: 4.5770
Training Epoch: 0 [12288/50176]	Loss: 4.5819
Training Epoch: 0 [12800/50176]	Loss: 4.5630
Training Epoch: 0 [13312/50176]	Loss: 4.5789
Training Epoch: 0 [13824/50176]	Loss: 4.6365
Training Epoch: 0 [14336/50176]	Loss: 4.5474
Training Epoch: 0 [14848/50176]	Loss: 4.5258
Training Epoch: 0 [15360/50176]	Loss: 4.4932
Training Epoch: 0 [15872/50176]	Loss: 4.4698
Training Epoch: 0 [16384/50176]	Loss: 4.4951
Training Epoch: 0 [16896/50176]	Loss: 4.4647
Training Epoch: 0 [17408/50176]	Loss: 4.5216
Training Epoch: 0 [17920/50176]	Loss: 4.5635
Training Epoch: 0 [18432/50176]	Loss: 4.4416
Training Epoch: 0 [18944/50176]	Loss: 4.4446
Training Epoch: 0 [19456/50176]	Loss: 4.4178
Training Epoch: 0 [19968/50176]	Loss: 4.4330
Training Epoch: 0 [20480/50176]	Loss: 4.4278
Training Epoch: 0 [20992/50176]	Loss: 4.3625
Training Epoch: 0 [21504/50176]	Loss: 4.4207
Training Epoch: 0 [22016/50176]	Loss: 4.4260
Training Epoch: 0 [22528/50176]	Loss: 4.3511
Training Epoch: 0 [23040/50176]	Loss: 4.3666
Training Epoch: 0 [23552/50176]	Loss: 4.4344
Training Epoch: 0 [24064/50176]	Loss: 4.4212
Training Epoch: 0 [24576/50176]	Loss: 4.3674
Training Epoch: 0 [25088/50176]	Loss: 4.3588
Training Epoch: 0 [25600/50176]	Loss: 4.3920
Profile done with power limit 125W
epoch 1 train time consumed: 27.75s
Validation Epoch: 0, Average loss: 0.0088, Accuracy: 0.0209
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 125000, 'lr': 0.008, 'energy': 122.95409134718817, 'time': 21.38370377000001, 'accuracy': 0.0208984375, 'total_cost': 1944.3408816106185}
[run job] Launching job with BS 512: and LR: 0.008 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00800+pl100', 'ZEUS_COST_THRESH': '3499.3804467516593', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.008', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.008', '--power_limit', '100']
[run job] cost_ub=3499.3804467516593
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00800+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.008
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 6.5227
Training Epoch: 0 [1536/50176]	Loss: 5.2997
Training Epoch: 0 [2048/50176]	Loss: 4.6793
Training Epoch: 0 [2560/50176]	Loss: 4.8343
Training Epoch: 0 [3072/50176]	Loss: 4.6691
Training Epoch: 0 [3584/50176]	Loss: 4.7061
Training Epoch: 0 [4096/50176]	Loss: 4.6852
Training Epoch: 0 [4608/50176]	Loss: 4.7154
Training Epoch: 0 [5120/50176]	Loss: 4.6892
Training Epoch: 0 [5632/50176]	Loss: 4.6845
Training Epoch: 0 [6144/50176]	Loss: 4.6140
Training Epoch: 0 [6656/50176]	Loss: 4.7042
Training Epoch: 0 [7168/50176]	Loss: 4.6317
Training Epoch: 0 [7680/50176]	Loss: 4.6119
Training Epoch: 0 [8192/50176]	Loss: 4.6504
Training Epoch: 0 [8704/50176]	Loss: 4.6287
Training Epoch: 0 [9216/50176]	Loss: 4.5876
Training Epoch: 0 [9728/50176]	Loss: 4.5919
Training Epoch: 0 [10240/50176]	Loss: 4.5698
Training Epoch: 0 [10752/50176]	Loss: 4.5384
Training Epoch: 0 [11264/50176]	Loss: 4.6424
Training Epoch: 0 [11776/50176]	Loss: 4.5970
Training Epoch: 0 [12288/50176]	Loss: 4.5981
Training Epoch: 0 [12800/50176]	Loss: 4.5508
Training Epoch: 0 [13312/50176]	Loss: 4.5185
Training Epoch: 0 [13824/50176]	Loss: 4.5343
Training Epoch: 0 [14336/50176]	Loss: 4.5534
Training Epoch: 0 [14848/50176]	Loss: 4.4869
Training Epoch: 0 [15360/50176]	Loss: 4.5364
Training Epoch: 0 [15872/50176]	Loss: 4.4830
Training Epoch: 0 [16384/50176]	Loss: 4.4627
Training Epoch: 0 [16896/50176]	Loss: 4.4661
Training Epoch: 0 [17408/50176]	Loss: 4.5451
Training Epoch: 0 [17920/50176]	Loss: 4.5432
Training Epoch: 0 [18432/50176]	Loss: 4.4224
Training Epoch: 0 [18944/50176]	Loss: 4.4237
Training Epoch: 0 [19456/50176]	Loss: 4.4216
Training Epoch: 0 [19968/50176]	Loss: 4.3794
Training Epoch: 0 [20480/50176]	Loss: 4.4084
Training Epoch: 0 [20992/50176]	Loss: 4.3408
Training Epoch: 0 [21504/50176]	Loss: 4.3877
Training Epoch: 0 [22016/50176]	Loss: 4.4086
Training Epoch: 0 [22528/50176]	Loss: 4.3602
Training Epoch: 0 [23040/50176]	Loss: 4.4081
Training Epoch: 0 [23552/50176]	Loss: 4.4387
Training Epoch: 0 [24064/50176]	Loss: 4.4014
Training Epoch: 0 [24576/50176]	Loss: 4.3437
Training Epoch: 0 [25088/50176]	Loss: 4.3591
Training Epoch: 0 [25600/50176]	Loss: 4.3697
Profile done with power limit 100W
epoch 1 train time consumed: 66.15s
Validation Epoch: 0, Average loss: 0.0089, Accuracy: 0.0252
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 100000, 'lr': 0.008, 'energy': 103.7209801337447, 'time': 52.08827555200014, 'accuracy': 0.0251953125, 'total_cost': 3674.8856738693376}

[Power Profiler] with batch size 512 and learning rate 0.0096
[run job] Launching job with BS 512: and LR: 0.0096 and PL: 175
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00960+pl175', 'ZEUS_COST_THRESH': 'inf', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0096', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '175', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0096', '--power_limit', '175']
[run job] cost_ub=inf
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00960+pl175.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 175
learning rate arg: 0.0096
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 7.0369
Training Epoch: 0 [1536/50176]	Loss: 5.2668
Training Epoch: 0 [2048/50176]	Loss: 4.6927
Training Epoch: 0 [2560/50176]	Loss: 4.8758
Training Epoch: 0 [3072/50176]	Loss: 4.6884
Training Epoch: 0 [3584/50176]	Loss: 4.5823
Training Epoch: 0 [4096/50176]	Loss: 4.6819
Training Epoch: 0 [4608/50176]	Loss: 4.7465
Training Epoch: 0 [5120/50176]	Loss: 4.7449
Training Epoch: 0 [5632/50176]	Loss: 4.6861
Training Epoch: 0 [6144/50176]	Loss: 4.6040
Training Epoch: 0 [6656/50176]	Loss: 4.6763
Training Epoch: 0 [7168/50176]	Loss: 4.6145
Training Epoch: 0 [7680/50176]	Loss: 4.6036
Training Epoch: 0 [8192/50176]	Loss: 4.5680
Training Epoch: 0 [8704/50176]	Loss: 4.6106
Training Epoch: 0 [9216/50176]	Loss: 4.6109
Training Epoch: 0 [9728/50176]	Loss: 4.6476
Training Epoch: 0 [10240/50176]	Loss: 4.5704
Training Epoch: 0 [10752/50176]	Loss: 4.5864
Training Epoch: 0 [11264/50176]	Loss: 4.5746
Training Epoch: 0 [11776/50176]	Loss: 4.5542
Training Epoch: 0 [12288/50176]	Loss: 4.5352
Training Epoch: 0 [12800/50176]	Loss: 4.5344
Training Epoch: 0 [13312/50176]	Loss: 4.5383
Training Epoch: 0 [13824/50176]	Loss: 4.6154
Training Epoch: 0 [14336/50176]	Loss: 4.5375
Training Epoch: 0 [14848/50176]	Loss: 4.4908
Training Epoch: 0 [15360/50176]	Loss: 4.4864
Training Epoch: 0 [15872/50176]	Loss: 4.4739
Training Epoch: 0 [16384/50176]	Loss: 4.5003
Training Epoch: 0 [16896/50176]	Loss: 4.4591
Training Epoch: 0 [17408/50176]	Loss: 4.5061
Training Epoch: 0 [17920/50176]	Loss: 4.5203
Training Epoch: 0 [18432/50176]	Loss: 4.4718
Training Epoch: 0 [18944/50176]	Loss: 4.4583
Training Epoch: 0 [19456/50176]	Loss: 4.4822
Training Epoch: 0 [19968/50176]	Loss: 4.5155
Training Epoch: 0 [20480/50176]	Loss: 4.4473
Training Epoch: 0 [20992/50176]	Loss: 4.4178
Training Epoch: 0 [21504/50176]	Loss: 4.4103
Training Epoch: 0 [22016/50176]	Loss: 4.4440
Training Epoch: 0 [22528/50176]	Loss: 4.3601
Training Epoch: 0 [23040/50176]	Loss: 4.3896
Training Epoch: 0 [23552/50176]	Loss: 4.5425
Training Epoch: 0 [24064/50176]	Loss: 4.3847
Training Epoch: 0 [24576/50176]	Loss: 4.3540
Training Epoch: 0 [25088/50176]	Loss: 4.3558
Training Epoch: 0 [25600/50176]	Loss: 4.3802
Profile done with power limit 175W
epoch 1 train time consumed: 23.73s
Validation Epoch: 0, Average loss: 0.0162, Accuracy: 0.0184
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 175000, 'lr': 0.0096, 'energy': 155.4247928662042, 'time': 18.08044887799997, 'accuracy': 0.018359375, 'total_cost': 2075.285653648679}
[run job] Launching job with BS 512: and LR: 0.0096 and PL: 150
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00960+pl150', 'ZEUS_COST_THRESH': '4150.571307297358', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0096', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '150', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0096', '--power_limit', '150']
[run job] cost_ub=4150.571307297358
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00960+pl150.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 150
learning rate arg: 0.0096
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 7.0369
Training Epoch: 0 [1536/50176]	Loss: 5.2667
Training Epoch: 0 [2048/50176]	Loss: 4.6944
Training Epoch: 0 [2560/50176]	Loss: 4.8658
Training Epoch: 0 [3072/50176]	Loss: 4.6943
Training Epoch: 0 [3584/50176]	Loss: 4.5846
Training Epoch: 0 [4096/50176]	Loss: 4.7045
Training Epoch: 0 [4608/50176]	Loss: 4.7016
Training Epoch: 0 [5120/50176]	Loss: 4.6273
Training Epoch: 0 [5632/50176]	Loss: 4.6291
Training Epoch: 0 [6144/50176]	Loss: 4.5963
Training Epoch: 0 [6656/50176]	Loss: 4.7183
Training Epoch: 0 [7168/50176]	Loss: 4.5759
Training Epoch: 0 [7680/50176]	Loss: 4.5695
Training Epoch: 0 [8192/50176]	Loss: 4.6703
Training Epoch: 0 [8704/50176]	Loss: 4.6347
Training Epoch: 0 [9216/50176]	Loss: 4.5888
Training Epoch: 0 [9728/50176]	Loss: 4.5881
Training Epoch: 0 [10240/50176]	Loss: 4.5540
Training Epoch: 0 [10752/50176]	Loss: 4.5423
Training Epoch: 0 [11264/50176]	Loss: 4.5291
Training Epoch: 0 [11776/50176]	Loss: 4.5501
Training Epoch: 0 [12288/50176]	Loss: 4.7236
Training Epoch: 0 [12800/50176]	Loss: 4.6989
Training Epoch: 0 [13312/50176]	Loss: 4.5670
Training Epoch: 0 [13824/50176]	Loss: 4.5240
Training Epoch: 0 [14336/50176]	Loss: 4.5192
Training Epoch: 0 [14848/50176]	Loss: 4.4723
Training Epoch: 0 [15360/50176]	Loss: 4.5410
Training Epoch: 0 [15872/50176]	Loss: 4.5226
Training Epoch: 0 [16384/50176]	Loss: 4.4629
Training Epoch: 0 [16896/50176]	Loss: 4.4867
Training Epoch: 0 [17408/50176]	Loss: 4.5212
Training Epoch: 0 [17920/50176]	Loss: 4.6478
Training Epoch: 0 [18432/50176]	Loss: 4.4461
Training Epoch: 0 [18944/50176]	Loss: 4.4738
Training Epoch: 0 [19456/50176]	Loss: 4.4205
Training Epoch: 0 [19968/50176]	Loss: 4.4049
Training Epoch: 0 [20480/50176]	Loss: 4.4251
Training Epoch: 0 [20992/50176]	Loss: 4.3525
Training Epoch: 0 [21504/50176]	Loss: 4.3920
Training Epoch: 0 [22016/50176]	Loss: 4.4453
Training Epoch: 0 [22528/50176]	Loss: 4.3992
Training Epoch: 0 [23040/50176]	Loss: 4.3906
Training Epoch: 0 [23552/50176]	Loss: 4.4620
Training Epoch: 0 [24064/50176]	Loss: 4.4443
Training Epoch: 0 [24576/50176]	Loss: 4.3255
Training Epoch: 0 [25088/50176]	Loss: 4.4194
Training Epoch: 0 [25600/50176]	Loss: 4.3665
Profile done with power limit 150W
epoch 1 train time consumed: 24.47s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0214
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 150000, 'lr': 0.0096, 'energy': 146.98597036373786, 'time': 18.734387884999705, 'accuracy': 0.02138671875, 'total_cost': 1798.8139222282468}
[run job] Launching job with BS 512: and LR: 0.0096 and PL: 125
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00960+pl125', 'ZEUS_COST_THRESH': '3597.6278444564937', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0096', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '125', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0096', '--power_limit', '125']
[run job] cost_ub=3597.6278444564937
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00960+pl125.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 125
learning rate arg: 0.0096
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 7.0369
Training Epoch: 0 [1536/50176]	Loss: 5.2666
Training Epoch: 0 [2048/50176]	Loss: 4.6938
Training Epoch: 0 [2560/50176]	Loss: 4.8553
Training Epoch: 0 [3072/50176]	Loss: 4.6914
Training Epoch: 0 [3584/50176]	Loss: 4.5812
Training Epoch: 0 [4096/50176]	Loss: 4.6629
Training Epoch: 0 [4608/50176]	Loss: 4.7840
Training Epoch: 0 [5120/50176]	Loss: 4.6852
Training Epoch: 0 [5632/50176]	Loss: 4.6428
Training Epoch: 0 [6144/50176]	Loss: 4.6096
Training Epoch: 0 [6656/50176]	Loss: 4.6894
Training Epoch: 0 [7168/50176]	Loss: 4.5824
Training Epoch: 0 [7680/50176]	Loss: 4.6059
Training Epoch: 0 [8192/50176]	Loss: 4.5874
Training Epoch: 0 [8704/50176]	Loss: 4.7280
Training Epoch: 0 [9216/50176]	Loss: 4.5968
Training Epoch: 0 [9728/50176]	Loss: 4.5756
Training Epoch: 0 [10240/50176]	Loss: 4.5759
Training Epoch: 0 [10752/50176]	Loss: 4.5277
Training Epoch: 0 [11264/50176]	Loss: 4.5311
Training Epoch: 0 [11776/50176]	Loss: 4.5457
Training Epoch: 0 [12288/50176]	Loss: 4.5743
Training Epoch: 0 [12800/50176]	Loss: 4.5607
Training Epoch: 0 [13312/50176]	Loss: 4.5794
Training Epoch: 0 [13824/50176]	Loss: 4.5634
Training Epoch: 0 [14336/50176]	Loss: 4.5174
Training Epoch: 0 [14848/50176]	Loss: 4.4656
Training Epoch: 0 [15360/50176]	Loss: 4.4834
Training Epoch: 0 [15872/50176]	Loss: 4.4785
Training Epoch: 0 [16384/50176]	Loss: 4.4352
Training Epoch: 0 [16896/50176]	Loss: 4.4701
Training Epoch: 0 [17408/50176]	Loss: 4.4808
Training Epoch: 0 [17920/50176]	Loss: 4.5493
Training Epoch: 0 [18432/50176]	Loss: 4.4324
Training Epoch: 0 [18944/50176]	Loss: 4.4213
Training Epoch: 0 [19456/50176]	Loss: 4.4112
Training Epoch: 0 [19968/50176]	Loss: 4.3895
Training Epoch: 0 [20480/50176]	Loss: 4.4258
Training Epoch: 0 [20992/50176]	Loss: 4.3333
Training Epoch: 0 [21504/50176]	Loss: 4.3946
Training Epoch: 0 [22016/50176]	Loss: 4.3904
Training Epoch: 0 [22528/50176]	Loss: 4.4364
Training Epoch: 0 [23040/50176]	Loss: 4.3970
Training Epoch: 0 [23552/50176]	Loss: 4.4793
Training Epoch: 0 [24064/50176]	Loss: 4.4075
Training Epoch: 0 [24576/50176]	Loss: 4.4263
Training Epoch: 0 [25088/50176]	Loss: 4.4138
Training Epoch: 0 [25600/50176]	Loss: 4.3871
Profile done with power limit 125W
epoch 1 train time consumed: 27.82s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0206
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 125000, 'lr': 0.0096, 'energy': 122.96940099486118, 'time': 21.383818178000183, 'accuracy': 0.02060546875, 'total_cost': 1972.0974155277647}
[run job] Launching job with BS 512: and LR: 0.0096 and PL: 100
[run job] zeus_env={'ZEUS_LOG_DIR': '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728', 'ZEUS_JOB_ID': 'bs512+lr0.00960+pl100', 'ZEUS_COST_THRESH': '3597.6278444564937', 'ZEUS_BATCH_SIZE': '512', 'ZEUS_LEARNING_RATE': '0.0096', 'ZEUS_ETA_KNOB': '0.5', 'ZEUS_POWER_LIMIT': '100', 'ZEUS_MONITOR_PATH': '/workspace/zeus/zeus_monitor/zeus_monitor', 'ZEUS_PROFILE_PARAMS': '10,40', 'ZEUS_LOG_PREFIX': '/workspace/zeus_logs'}
[run job] cwd=/workspace/zeus/zeus2
[run job] command=['python', 'train_lr.py', '--zeus', '--arch', 'shufflenetv2', '--batch_size', '512', '--epochs', '100', '--seed', '1', '--learning_rate', '0.0096', '--power_limit', '100']
[run job] cost_ub=3597.6278444564937
[run job] Job output logged to '/workspace/zeus_logs/cifar100+shufflenetv2+bs8+adam+lr0.001+tm0.5+me100+eta0.5+beta2.0+2022112711481669567728/bs512+lr0.00960+pl100.train.log'
Files already downloaded and verified
Files already downloaded and verified
power limit arg: 100
learning rate arg: 0.0096
batch size arg: 512
ProfileDataLoader constructor: called constructor of DataLoader!
ProfileDataLoader constructor: called constructor of DataLoader!
Launching Zeus monitor 0...
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Training Epoch: 0 [512/50176]	Loss: 4.6290
Training Epoch: 0 [1024/50176]	Loss: 7.0369
Training Epoch: 0 [1536/50176]	Loss: 5.2666
Training Epoch: 0 [2048/50176]	Loss: 4.6943
Training Epoch: 0 [2560/50176]	Loss: 4.8495
Training Epoch: 0 [3072/50176]	Loss: 4.6954
Training Epoch: 0 [3584/50176]	Loss: 4.5798
Training Epoch: 0 [4096/50176]	Loss: 4.6748
Training Epoch: 0 [4608/50176]	Loss: 4.7911
Training Epoch: 0 [5120/50176]	Loss: 4.8102
Training Epoch: 0 [5632/50176]	Loss: 4.6402
Training Epoch: 0 [6144/50176]	Loss: 4.5764
Training Epoch: 0 [6656/50176]	Loss: 4.6581
Training Epoch: 0 [7168/50176]	Loss: 4.5612
Training Epoch: 0 [7680/50176]	Loss: 4.5519
Training Epoch: 0 [8192/50176]	Loss: 4.5939
Training Epoch: 0 [8704/50176]	Loss: 4.6245
Training Epoch: 0 [9216/50176]	Loss: 4.6115
Training Epoch: 0 [9728/50176]	Loss: 4.6489
Training Epoch: 0 [10240/50176]	Loss: 4.5947
Training Epoch: 0 [10752/50176]	Loss: 4.5494
Training Epoch: 0 [11264/50176]	Loss: 4.7100
Training Epoch: 0 [11776/50176]	Loss: 4.5612
Training Epoch: 0 [12288/50176]	Loss: 4.6526
Training Epoch: 0 [12800/50176]	Loss: 4.5914
Training Epoch: 0 [13312/50176]	Loss: 4.5591
Training Epoch: 0 [13824/50176]	Loss: 4.5762
Training Epoch: 0 [14336/50176]	Loss: 4.5293
Training Epoch: 0 [14848/50176]	Loss: 4.5049
Training Epoch: 0 [15360/50176]	Loss: 4.4939
Training Epoch: 0 [15872/50176]	Loss: 4.5009
Training Epoch: 0 [16384/50176]	Loss: 4.4397
Training Epoch: 0 [16896/50176]	Loss: 4.4736
Training Epoch: 0 [17408/50176]	Loss: 4.5013
Training Epoch: 0 [17920/50176]	Loss: 4.5502
Training Epoch: 0 [18432/50176]	Loss: 4.4380
Training Epoch: 0 [18944/50176]	Loss: 4.4453
Training Epoch: 0 [19456/50176]	Loss: 4.4365
Training Epoch: 0 [19968/50176]	Loss: 4.4112
Training Epoch: 0 [20480/50176]	Loss: 4.4266
Training Epoch: 0 [20992/50176]	Loss: 4.3437
Training Epoch: 0 [21504/50176]	Loss: 4.3997
Training Epoch: 0 [22016/50176]	Loss: 4.4158
Training Epoch: 0 [22528/50176]	Loss: 4.3644
Training Epoch: 0 [23040/50176]	Loss: 4.3844
Training Epoch: 0 [23552/50176]	Loss: 4.4577
Training Epoch: 0 [24064/50176]	Loss: 4.4109
Training Epoch: 0 [24576/50176]	Loss: 4.3809
Training Epoch: 0 [25088/50176]	Loss: 4.3986
Training Epoch: 0 [25600/50176]	Loss: 4.4198
Profile done with power limit 100W
epoch 1 train time consumed: 66.06s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0236
Stopped Zeus monitor 0.

[run job] Job terminated with exit code 0.
[run job] stats={'bs': 512, 'pl': 100000, 'lr': 0.0096, 'energy': 103.88993239093958, 'time': 52.09024767499977, 'accuracy': 0.0236328125, 'total_cost': 3920.376630745957}
[Power Profiler]
[HistoryEntry(bs=8, pl=175, lr=0.0008, energy=97.06921267349081, time=2.180380848000027, accuracy=0.0115, total_cost=5.158386962119354), HistoryEntry(bs=8, pl=150, lr=0.0008, energy=95.98314935374884, time=2.242382289000034, accuracy=0.0101, total_cost=6.016314997309875), HistoryEntry(bs=8, pl=125, lr=0.0008, energy=96.9676319806666, time=2.238269564999996, accuracy=0.0133, total_cost=4.576968972386811), HistoryEntry(bs=8, pl=100, lr=0.0008, energy=97.48630493993177, time=2.1913649130000294, accuracy=0.0102, total_cost=5.8540875286116965), HistoryEntry(bs=8, pl=175, lr=0.001, energy=97.34030920608366, time=2.227547700000059, accuracy=0.0122, total_cost=4.972549421223905), HistoryEntry(bs=8, pl=150, lr=0.001, energy=95.12453371430512, time=2.283929243999978, accuracy=0.0117, total_cost=5.273036940785978), HistoryEntry(bs=8, pl=125, lr=0.001, energy=97.67314956625486, time=2.2052223619999722, accuracy=0.0101, total_cost=5.95351412812345), HistoryEntry(bs=8, pl=100, lr=0.001, energy=97.89078810054312, time=2.189029348999952, accuracy=0.0118, total_cost=5.062423256134033), HistoryEntry(bs=8, pl=175, lr=0.0012, energy=97.41434316929761, time=2.228714114000013, accuracy=0.0142, total_cost=4.275589376601808), HistoryEntry(bs=8, pl=150, lr=0.0012, energy=99.28335179485042, time=2.2040448610000567, accuracy=0.0117, total_cost=5.166947110951375), HistoryEntry(bs=8, pl=125, lr=0.0012, energy=96.05392124180507, time=2.2358609709999655, accuracy=0.0109, total_cost=5.559989757257345), HistoryEntry(bs=8, pl=100, lr=0.0012, energy=95.78115449898867, time=2.240701842999897, accuracy=0.0124, total_cost=4.893063160770352), HistoryEntry(bs=16, pl=175, lr=0.0011313708498984763, energy=116.12608123203319, time=2.398267027999964, accuracy=0.0103, total_cost=13.5572443030995), HistoryEntry(bs=16, pl=150, lr=0.0011313708498984763, energy=115.2645611281345, time=2.4296023320000586, accuracy=0.0101, total_cost=13.964900091364141), HistoryEntry(bs=16, pl=125, lr=0.0011313708498984763, energy=115.88892358227561, time=2.438761488999944, accuracy=0.0107, total_cost=13.25997578334769), HistoryEntry(bs=16, pl=100, lr=0.0011313708498984763, energy=98.79022100671412, time=2.748906297999838, accuracy=0.0126, total_cost=11.9464073469226), HistoryEntry(bs=16, pl=175, lr=0.0014142135623730952, energy=115.31197773457963, time=2.42680802100017, accuracy=0.0118, total_cost=11.94121078235086), HistoryEntry(bs=16, pl=150, lr=0.0014142135623730952, energy=115.69260051704765, time=2.4260206649998963, accuracy=0.0109, total_cost=12.9399313030627), HistoryEntry(bs=16, pl=125, lr=0.0014142135623730952, energy=114.34343975165451, time=2.4709513320001406, accuracy=0.0123, total_cost=11.625261103412246), HistoryEntry(bs=16, pl=100, lr=0.0014142135623730952, energy=98.70944011900627, time=2.7478723839999475, accuracy=0.0125, total_cost=12.033897787889673), HistoryEntry(bs=16, pl=175, lr=0.0016970562748477142, energy=116.39662378160118, time=2.3688248989999465, accuracy=0.0127, total_cost=10.870355557454754), HistoryEntry(bs=16, pl=150, lr=0.0016970562748477142, energy=114.53153742968385, time=2.4240193299999646, accuracy=0.015, total_cost=9.357733911655492), HistoryEntry(bs=16, pl=125, lr=0.0016970562748477142, energy=115.64280038288454, time=2.4238679730001422, accuracy=0.0133, total_cost=10.59368083355109), HistoryEntry(bs=16, pl=100, lr=0.0016970562748477142, energy=98.9862196523038, time=2.7549078560000453, accuracy=0.0164, total_cost=9.204960840925436), HistoryEntry(bs=32, pl=175, lr=0.0016, energy=129.40896109734103, time=3.292198983999924, accuracy=0.020666932907348244, total_cost=19.390478182074503), HistoryEntry(bs=32, pl=150, lr=0.0016, energy=131.01049105045487, time=3.193597031000081, accuracy=0.013977635782747603, total_cost=27.95784875658896), HistoryEntry(bs=32, pl=125, lr=0.0016, energy=123.08962782683443, time=3.4081493000001046, accuracy=0.018370607028753993, total_cost=22.113781214915182), HistoryEntry(bs=32, pl=100, lr=0.0016, energy=99.36257958973064, time=4.11309332299993, accuracy=0.013478434504792332, total_cost=33.47919659552689), HistoryEntry(bs=32, pl=175, lr=0.002, energy=130.17930452274314, time=3.25805347000005, accuracy=0.025359424920127795, total_cost=15.678153964901956), HistoryEntry(bs=32, pl=150, lr=0.002, energy=129.76808258680018, time=3.2715427319999435, accuracy=0.031150159744408944, total_cost=12.799200466379688), HistoryEntry(bs=32, pl=125, lr=0.002, energy=123.48572481994759, time=3.3633146150000357, accuracy=0.02456070287539936, total_cost=16.344487534669526), HistoryEntry(bs=32, pl=100, lr=0.002, energy=98.86800287545782, time=4.0426507229999515, accuracy=0.023462460063897763, total_cost=18.869266075175126), HistoryEntry(bs=32, pl=175, lr=0.0024, energy=129.9685534585697, time=3.2875678709999647, accuracy=0.02476038338658147, total_cost=16.19173776850366), HistoryEntry(bs=32, pl=150, lr=0.0024, energy=130.290800961541, time=3.279302641999948, accuracy=0.014177316293929713, total_cost=28.237238890871424), HistoryEntry(bs=32, pl=125, lr=0.0024, energy=123.6526186286231, time=3.2953076109999984, accuracy=0.015375399361022364, total_cost=25.59510543139236), HistoryEntry(bs=32, pl=100, lr=0.0024, energy=98.9455504749466, time=4.0192913579999185, accuracy=0.020866613418530352, total_cost=21.100017195649954), HistoryEntry(bs=64, pl=175, lr=0.0022627416997969526, energy=140.89273903349454, time=5.2876487409998845, accuracy=0.01572452229299363, total_cost=84.89811437181832), HistoryEntry(bs=64, pl=150, lr=0.0022627416997969526, energy=141.18079688987459, time=5.289481872999886, accuracy=0.02289012738853503, total_cost=58.394733062854485), HistoryEntry(bs=64, pl=125, lr=0.0022627416997969526, energy=123.22366880039353, time=5.796638918999861, accuracy=0.032245222929936306, total_cost=42.84757400243753), HistoryEntry(bs=64, pl=100, lr=0.0022627416997969526, energy=99.05516183652752, time=12.049748545000057, accuracy=0.020601114649681528, total_cost=128.1145677876729), HistoryEntry(bs=64, pl=175, lr=0.0028284271247461905, energy=140.74453648235547, time=5.303924341999846, accuracy=0.022292993630573247, total_cost=60.03963414907953), HistoryEntry(bs=64, pl=150, lr=0.0028284271247461905, energy=141.20441949984854, time=5.273835460999862, accuracy=0.020601114649681528, total_cost=64.69594441006188), HistoryEntry(bs=64, pl=125, lr=0.0028284271247461905, energy=123.69159510274207, time=5.781752718000007, accuracy=0.01950636942675159, total_cost=70.75861680337147), HistoryEntry(bs=64, pl=100, lr=0.0028284271247461905, energy=98.14576287360254, time=11.975796710000168, accuracy=0.020103503184713375, total_cost=130.0470198664079), HistoryEntry(bs=64, pl=175, lr=0.0033941125496954284, energy=140.74987937560394, time=5.305631545000097, accuracy=0.021795382165605094, total_cost=61.43120804731883), HistoryEntry(bs=64, pl=150, lr=0.0033941125496954284, energy=141.1678749675045, time=5.279714154999965, accuracy=0.022392515923566877, total_cost=59.57972895870061), HistoryEntry(bs=64, pl=125, lr=0.0033941125496954284, energy=123.51617684760323, time=5.766914885000006, accuracy=0.017914012738853503, total_cost=76.80540756950575), HistoryEntry(bs=64, pl=100, lr=0.0033941125496954284, energy=98.54980060823367, time=12.529872716, accuracy=0.021894904458598725, total_cost=125.1161210760313), HistoryEntry(bs=128, pl=175, lr=0.0032, energy=144.27467077667583, time=8.646741140000131, accuracy=0.03174446202531646, total_cost=139.0119744280006), HistoryEntry(bs=128, pl=150, lr=0.0032, energy=144.20649641552933, time=8.663041862, accuracy=0.0379746835443038, total_cost=116.39953114052462), HistoryEntry(bs=128, pl=125, lr=0.0032, energy=123.11963137299509, time=9.672366398999884, accuracy=0.03738132911392405, total_cost=123.30248697523743), HistoryEntry(bs=128, pl=100, lr=0.0032, energy=98.91903479683887, time=20.795609867000167, accuracy=0.03401898734177215, total_cost=267.65524132495335), HistoryEntry(bs=128, pl=175, lr=0.004, energy=144.19208573122665, time=8.66300266799999, accuracy=0.03134889240506329, total_cost=140.99432133620576), HistoryEntry(bs=128, pl=150, lr=0.004, energy=144.4445676085219, time=8.66324461399995, accuracy=0.02966772151898734, total_cost=149.10601054027592), HistoryEntry(bs=128, pl=125, lr=0.004, energy=123.35715631504716, time=9.658206722000159, accuracy=0.03471123417721519, total_cost=132.69854479150018), HistoryEntry(bs=128, pl=100, lr=0.004, energy=99.0257256197786, time=20.996372585000245, accuracy=0.028975474683544302, total_cost=317.4010124777079), HistoryEntry(bs=128, pl=175, lr=0.0048, energy=144.32507224193986, time=8.638574053999946, accuracy=0.02521756329113924, total_cost=174.8538584223536), HistoryEntry(bs=128, pl=150, lr=0.0048, energy=144.34276112585013, time=8.650591868999982, accuracy=0.026503164556962024, total_cost=166.61282401246197), HistoryEntry(bs=128, pl=125, lr=0.0048, energy=123.20228838139138, time=9.663978663999842, accuracy=0.030656645569620253, total_cost=150.26075316915893), HistoryEntry(bs=128, pl=100, lr=0.0048, energy=98.84710091157461, time=20.89193062000004, accuracy=0.02531645569620253, total_cost=361.23271738002603), HistoryEntry(bs=256, pl=175, lr=0.004525483399593905, energy=154.83767195724607, time=9.485337706000337, accuracy=0.03818359375, total_cost=261.2764095558587), HistoryEntry(bs=256, pl=150, lr=0.004525483399593905, energy=146.89528996314885, time=9.764853021999897, accuracy=0.0416015625, total_cost=240.93208354616746), HistoryEntry(bs=256, pl=125, lr=0.004525483399593905, energy=122.96890305987783, time=11.124008482000136, accuracy=0.03017578125, total_cost=350.2657531217602), HistoryEntry(bs=256, pl=100, lr=0.004525483399593905, energy=103.23046630168689, time=28.92220561499971, accuracy=0.0396484375, total_cost=647.1932244684538), HistoryEntry(bs=256, pl=175, lr=0.005656854249492381, energy=154.37150324769297, time=9.506530696000027, accuracy=0.02138671875, total_cost=466.8612887539825), HistoryEntry(bs=256, pl=150, lr=0.005656854249492381, energy=147.00740542613758, time=9.767149691999748, accuracy=0.02685546875, total_cost=373.4435064317076), HistoryEntry(bs=256, pl=125, lr=0.005656854249492381, energy=122.91217166725176, time=11.11393583500012, accuracy=0.03212890625, total_cost=328.61254367598804), HistoryEntry(bs=256, pl=100, lr=0.005656854249492381, energy=103.4132666654794, time=28.820854689000043, accuracy=0.02529296875, total_cost=1011.6281840114226), HistoryEntry(bs=256, pl=175, lr=0.006788225099390857, energy=154.40490943520513, time=9.504084484000032, accuracy=0.02646484375, total_cost=377.2202230549371), HistoryEntry(bs=256, pl=150, lr=0.006788225099390857, energy=147.11409435022975, time=9.764007869000125, accuracy=0.02666015625, total_cost=376.18294708248385), HistoryEntry(bs=256, pl=125, lr=0.006788225099390857, energy=123.04611985048531, time=11.10592756899996, accuracy=0.0314453125, total_cost=335.66521681135464), HistoryEntry(bs=256, pl=100, lr=0.006788225099390857, energy=103.0294214214699, time=28.971577270000125, accuracy=0.02392578125, total_cost=1073.5461379152746), HistoryEntry(bs=512, pl=175, lr=0.0064, energy=155.2678766283314, time=18.130308626999977, accuracy=0.04443359375, total_cost=859.4369726457916), HistoryEntry(bs=512, pl=150, lr=0.0064, energy=147.06060958898496, time=18.67529945699971, accuracy=0.04072265625, total_cost=941.9395413111207), HistoryEntry(bs=512, pl=125, lr=0.0064, energy=123.08272827567056, time=21.39033102700023, accuracy=0.038671875, total_cost=1051.5090878166075), HistoryEntry(bs=512, pl=100, lr=0.0064, energy=103.7810616993493, time=52.073935949000315, accuracy=0.037109375, total_cost=2494.904773486783), HistoryEntry(bs=512, pl=175, lr=0.008, energy=155.49126038999586, time=18.120286468999893, accuracy=0.021484375, total_cost=1777.690928020056), HistoryEntry(bs=512, pl=150, lr=0.008, energy=147.04288415088283, time=18.718716961000155, accuracy=0.02197265625, total_cost=1749.6902233758296), HistoryEntry(bs=512, pl=125, lr=0.008, energy=122.95409134718817, time=21.38370377000001, accuracy=0.0208984375, total_cost=1944.3408816106185), HistoryEntry(bs=512, pl=100, lr=0.008, energy=103.7209801337447, time=52.08827555200014, accuracy=0.0251953125, total_cost=3674.8856738693376), HistoryEntry(bs=512, pl=175, lr=0.0096, energy=155.4247928662042, time=18.08044887799997, accuracy=0.018359375, total_cost=2075.285653648679), HistoryEntry(bs=512, pl=150, lr=0.0096, energy=146.98597036373786, time=18.734387884999705, accuracy=0.02138671875, total_cost=1798.8139222282468), HistoryEntry(bs=512, pl=125, lr=0.0096, energy=122.96940099486118, time=21.383818178000183, accuracy=0.02060546875, total_cost=1972.0974155277647), HistoryEntry(bs=512, pl=100, lr=0.0096, energy=103.88993239093958, time=52.09024767499977, accuracy=0.0236328125, total_cost=3920.376630745957)]
optimized batch size: 16, learning rate factor: 0.0011313708498984763, power limit: 100
