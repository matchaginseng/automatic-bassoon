Files already downloaded and verified
Files already downloaded and verified
Job(cifar100,shufflenetv2,adam,0.6,bs1024~100)
[Training Loop] Testing batch sizes: [128, 256, 512, 1024]
[Training Loop] Testing power limits: [175, 150, 125, 100]
[Training Loop] Testing learning rates: [0.001, 0.005, 0.01]
[Training Loop] Testing dropout rates: [0.0, 0.25, 0.5]
[Training Loop] Reprofiling at accuracy thresholds [0.5, 0.4, 0.3]
[Training Loop] Model's accuracy 0.0 surpasses threshold 0.0! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
Launching Zeus monitor 0...
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6458
Profiling... [256/50048]	Loss: 4.7206
Profiling... [384/50048]	Loss: 4.7229
Profiling... [512/50048]	Loss: 4.6895
Profiling... [640/50048]	Loss: 4.5746
Profiling... [768/50048]	Loss: 4.7630
Profiling... [896/50048]	Loss: 4.7070
Profiling... [1024/50048]	Loss: 4.7406
Profiling... [1152/50048]	Loss: 4.6448
Profiling... [1280/50048]	Loss: 4.6172
Profiling... [1408/50048]	Loss: 4.7118
Profiling... [1536/50048]	Loss: 4.6834
Profiling... [1664/50048]	Loss: 4.6473
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 147.71383257755426,
                        "time": 2.1464470330000154,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 32061.098874704683
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6856
Profiling... [256/50048]	Loss: 4.6632
Profiling... [384/50048]	Loss: 4.7280
Profiling... [512/50048]	Loss: 4.6070
Profiling... [640/50048]	Loss: 4.5868
Profiling... [768/50048]	Loss: 4.6499
Profiling... [896/50048]	Loss: 4.7021
Profiling... [1024/50048]	Loss: 4.7140
Profiling... [1152/50048]	Loss: 4.6967
Profiling... [1280/50048]	Loss: 4.6745
Profiling... [1408/50048]	Loss: 4.7520
Profiling... [1536/50048]	Loss: 4.7234
Profiling... [1664/50048]	Loss: 4.5614
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 139.8558232637903,
                        "time": 2.170142967000004,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 30690.641111990903
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6092
Profiling... [256/50048]	Loss: 4.6417
Profiling... [384/50048]	Loss: 4.7462
Profiling... [512/50048]	Loss: 4.6468
Profiling... [640/50048]	Loss: 4.7152
Profiling... [768/50048]	Loss: 4.7847
Profiling... [896/50048]	Loss: 4.7587
Profiling... [1024/50048]	Loss: 4.6686
Profiling... [1152/50048]	Loss: 4.7601
Profiling... [1280/50048]	Loss: 4.7407
Profiling... [1408/50048]	Loss: 4.6863
Profiling... [1536/50048]	Loss: 4.6257
Profiling... [1664/50048]	Loss: 4.6324
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 136.32047469790894,
                        "time": 2.468151560000024,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 34022.79397191863
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6606
Profiling... [256/50048]	Loss: 4.6705
Profiling... [384/50048]	Loss: 4.6932
Profiling... [512/50048]	Loss: 4.7111
Profiling... [640/50048]	Loss: 4.7681
Profiling... [768/50048]	Loss: 4.7990
Profiling... [896/50048]	Loss: 4.6487
Profiling... [1024/50048]	Loss: 4.7165
Profiling... [1152/50048]	Loss: 4.5404
Profiling... [1280/50048]	Loss: 4.6237
Profiling... [1408/50048]	Loss: 4.6479
Profiling... [1536/50048]	Loss: 4.5607
Profiling... [1664/50048]	Loss: 4.5460
Profile done
epoch 1 train time consumed: 7.19s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 124.62834504636295,
                        "time": 5.544114716000024,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 69869.2524830455
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6450
Profiling... [256/50048]	Loss: 4.6495
Profiling... [384/50048]	Loss: 4.7771
Profiling... [512/50048]	Loss: 4.7813
Profiling... [640/50048]	Loss: 4.6958
Profiling... [768/50048]	Loss: 4.7593
Profiling... [896/50048]	Loss: 4.7841
Profiling... [1024/50048]	Loss: 4.6745
Profiling... [1152/50048]	Loss: 4.6084
Profiling... [1280/50048]	Loss: 4.6339
Profiling... [1408/50048]	Loss: 4.6126
Profiling... [1536/50048]	Loss: 4.7038
Profiling... [1664/50048]	Loss: 4.5160
Profile done
epoch 1 train time consumed: 3.17s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.71767770798633,
                        "time": 2.172836380999968,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26084.317499068904
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6193
Profiling... [256/50048]	Loss: 4.5910
Profiling... [384/50048]	Loss: 4.6143
Profiling... [512/50048]	Loss: 4.7558
Profiling... [640/50048]	Loss: 4.9058
Profiling... [768/50048]	Loss: 4.7949
Profiling... [896/50048]	Loss: 4.7266
Profiling... [1024/50048]	Loss: 4.6956
Profiling... [1152/50048]	Loss: 4.7034
Profiling... [1280/50048]	Loss: 4.6191
Profiling... [1408/50048]	Loss: 4.7013
Profiling... [1536/50048]	Loss: 4.6211
Profiling... [1664/50048]	Loss: 4.6244
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.86388010366677,
                        "time": 2.19728332599999,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 27076.84898775356
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.7066
Profiling... [256/50048]	Loss: 4.6211
Profiling... [384/50048]	Loss: 4.6862
Profiling... [512/50048]	Loss: 4.7365
Profiling... [640/50048]	Loss: 4.8080
Profiling... [768/50048]	Loss: 4.6198
Profiling... [896/50048]	Loss: 4.7966
Profiling... [1024/50048]	Loss: 4.6700
Profiling... [1152/50048]	Loss: 4.6780
Profiling... [1280/50048]	Loss: 4.6632
Profiling... [1408/50048]	Loss: 4.6755
Profiling... [1536/50048]	Loss: 4.6438
Profiling... [1664/50048]	Loss: 4.6329
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 123.2252308260252,
                        "time": 2.4712519210000323,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 30793.12189845054
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6225
Profiling... [256/50048]	Loss: 4.6746
Profiling... [384/50048]	Loss: 4.7376
Profiling... [512/50048]	Loss: 4.7072
Profiling... [640/50048]	Loss: 4.7293
Profiling... [768/50048]	Loss: 4.6731
Profiling... [896/50048]	Loss: 4.6721
Profiling... [1024/50048]	Loss: 4.6417
Profiling... [1152/50048]	Loss: 4.6622
Profiling... [1280/50048]	Loss: 4.7269
Profiling... [1408/50048]	Loss: 4.7196
Profiling... [1536/50048]	Loss: 4.6863
Profiling... [1664/50048]	Loss: 4.6558
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 120.64643951837044,
                        "time": 5.432105788999991,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 66270.4301822313
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6415
Profiling... [256/50048]	Loss: 4.6584
Profiling... [384/50048]	Loss: 4.7265
Profiling... [512/50048]	Loss: 4.7193
Profiling... [640/50048]	Loss: 4.7188
Profiling... [768/50048]	Loss: 4.8019
Profiling... [896/50048]	Loss: 4.7174
Profiling... [1024/50048]	Loss: 4.6716
Profiling... [1152/50048]	Loss: 4.7127
Profiling... [1280/50048]	Loss: 4.6305
Profiling... [1408/50048]	Loss: 4.7350
Profiling... [1536/50048]	Loss: 4.6539
Profiling... [1664/50048]	Loss: 4.6156
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.53145206984931,
                        "time": 2.1691815990000123,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25999.59466736749
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6713
Profiling... [256/50048]	Loss: 4.7087
Profiling... [384/50048]	Loss: 4.6918
Profiling... [512/50048]	Loss: 4.7061
Profiling... [640/50048]	Loss: 4.6591
Profiling... [768/50048]	Loss: 4.7563
Profiling... [896/50048]	Loss: 4.8068
Profiling... [1024/50048]	Loss: 4.7446
Profiling... [1152/50048]	Loss: 4.5801
Profiling... [1280/50048]	Loss: 4.6505
Profiling... [1408/50048]	Loss: 4.6225
Profiling... [1536/50048]	Loss: 4.6132
Profiling... [1664/50048]	Loss: 4.5338
Profile done
epoch 1 train time consumed: 3.18s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 120.37552393589488,
                        "time": 2.195759907999957,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26727.60857557919
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6445
Profiling... [256/50048]	Loss: 4.6082
Profiling... [384/50048]	Loss: 4.8205
Profiling... [512/50048]	Loss: 4.7105
Profiling... [640/50048]	Loss: 4.7599
Profiling... [768/50048]	Loss: 4.6928
Profiling... [896/50048]	Loss: 4.6154
Profiling... [1024/50048]	Loss: 4.6512
Profiling... [1152/50048]	Loss: 4.7354
Profiling... [1280/50048]	Loss: 4.6494
Profiling... [1408/50048]	Loss: 4.7324
Profiling... [1536/50048]	Loss: 4.5290
Profiling... [1664/50048]	Loss: 4.5474
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.2048976333039,
                        "time": 2.497884247000002,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 30614.666946740133
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.5834
Profiling... [256/50048]	Loss: 4.6784
Profiling... [384/50048]	Loss: 4.7783
Profiling... [512/50048]	Loss: 4.7491
Profiling... [640/50048]	Loss: 4.8532
Profiling... [768/50048]	Loss: 4.7142
Profiling... [896/50048]	Loss: 4.8060
Profiling... [1024/50048]	Loss: 4.6987
Profiling... [1152/50048]	Loss: 4.7018
Profiling... [1280/50048]	Loss: 4.6135
Profiling... [1408/50048]	Loss: 4.6774
Profiling... [1536/50048]	Loss: 4.6128
Profiling... [1664/50048]	Loss: 4.5854
Profile done
epoch 1 train time consumed: 7.28s
Validation Epoch: 0, Average loss: 0.0365, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.76684453019335,
                        "time": 5.548138402999996,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 67192.52394749896
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6398
Profiling... [256/50048]	Loss: 5.7568
Profiling... [384/50048]	Loss: 5.9269
Profiling... [512/50048]	Loss: 5.5529
Profiling... [640/50048]	Loss: 5.1501
Profiling... [768/50048]	Loss: 4.8953
Profiling... [896/50048]	Loss: 4.7663
Profiling... [1024/50048]	Loss: 4.7715
Profiling... [1152/50048]	Loss: 4.8253
Profiling... [1280/50048]	Loss: 4.7291
Profiling... [1408/50048]	Loss: 4.5289
Profiling... [1536/50048]	Loss: 4.8928
Profiling... [1664/50048]	Loss: 4.7954
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.39320035140229,
                        "time": 2.174952947999998,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26038.363609686407
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6507
Profiling... [256/50048]	Loss: 5.8664
Profiling... [384/50048]	Loss: 5.4190
Profiling... [512/50048]	Loss: 5.4526
Profiling... [640/50048]	Loss: 5.5158
Profiling... [768/50048]	Loss: 5.1134
Profiling... [896/50048]	Loss: 4.9109
Profiling... [1024/50048]	Loss: 4.6731
Profiling... [1152/50048]	Loss: 4.7734
Profiling... [1280/50048]	Loss: 5.1067
Profiling... [1408/50048]	Loss: 4.7700
Profiling... [1536/50048]	Loss: 4.6552
Profiling... [1664/50048]	Loss: 4.6169
Profile done
epoch 1 train time consumed: 3.18s
Validation Epoch: 0, Average loss: 0.0372, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.62847636849294,
                        "time": 2.2026834420000228,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26645.49051263624
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6240
Profiling... [256/50048]	Loss: 5.8399
Profiling... [384/50048]	Loss: 6.1604
Profiling... [512/50048]	Loss: 5.2440
Profiling... [640/50048]	Loss: 5.2331
Profiling... [768/50048]	Loss: 5.0790
Profiling... [896/50048]	Loss: 5.1321
Profiling... [1024/50048]	Loss: 4.8645
Profiling... [1152/50048]	Loss: 4.8688
Profiling... [1280/50048]	Loss: 4.9640
Profiling... [1408/50048]	Loss: 4.8281
Profiling... [1536/50048]	Loss: 4.8032
Profiling... [1664/50048]	Loss: 4.7358
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 120.33907888920727,
                        "time": 2.4870111839999822,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 30263.66229824954
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6559
Profiling... [256/50048]	Loss: 5.4579
Profiling... [384/50048]	Loss: 6.0577
Profiling... [512/50048]	Loss: 5.4036
Profiling... [640/50048]	Loss: 4.9696
Profiling... [768/50048]	Loss: 5.0036
Profiling... [896/50048]	Loss: 4.9209
Profiling... [1024/50048]	Loss: 5.1586
Profiling... [1152/50048]	Loss: 4.6230
Profiling... [1280/50048]	Loss: 4.9633
Profiling... [1408/50048]	Loss: 4.9560
Profiling... [1536/50048]	Loss: 5.1057
Profiling... [1664/50048]	Loss: 4.6412
Profile done
epoch 1 train time consumed: 7.22s
Validation Epoch: 0, Average loss: 0.0368, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.3290610151785,
                        "time": 5.443097437999995,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 65679.43270011185
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6276
Profiling... [256/50048]	Loss: 5.8432
Profiling... [384/50048]	Loss: 5.7421
Profiling... [512/50048]	Loss: 5.4568
Profiling... [640/50048]	Loss: 5.2046
Profiling... [768/50048]	Loss: 4.8689
Profiling... [896/50048]	Loss: 4.8370
Profiling... [1024/50048]	Loss: 4.7445
Profiling... [1152/50048]	Loss: 4.8048
Profiling... [1280/50048]	Loss: 4.8449
Profiling... [1408/50048]	Loss: 4.8302
Profiling... [1536/50048]	Loss: 4.8248
Profiling... [1664/50048]	Loss: 4.6600
Profile done
epoch 1 train time consumed: 3.13s
Validation Epoch: 0, Average loss: 0.0375, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.421386056773,
                        "time": 2.1650441859999887,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25925.90737541356
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6929
Profiling... [256/50048]	Loss: 5.9426
Profiling... [384/50048]	Loss: 5.7433
Profiling... [512/50048]	Loss: 5.4923
Profiling... [640/50048]	Loss: 5.3073
Profiling... [768/50048]	Loss: 4.9634
Profiling... [896/50048]	Loss: 5.0541
Profiling... [1024/50048]	Loss: 4.8615
Profiling... [1152/50048]	Loss: 4.8429
Profiling... [1280/50048]	Loss: 4.9772
Profiling... [1408/50048]	Loss: 4.6153
Profiling... [1536/50048]	Loss: 4.9100
Profiling... [1664/50048]	Loss: 4.6784
Profile done
epoch 1 train time consumed: 3.22s
Validation Epoch: 0, Average loss: 0.0373, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.42450177085318,
                        "time": 2.197263358999976,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26534.604122529217
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6638
Profiling... [256/50048]	Loss: 5.5927
Profiling... [384/50048]	Loss: 5.8012
Profiling... [512/50048]	Loss: 5.7946
Profiling... [640/50048]	Loss: 5.7431
Profiling... [768/50048]	Loss: 5.0818
Profiling... [896/50048]	Loss: 4.8055
Profiling... [1024/50048]	Loss: 4.8449
Profiling... [1152/50048]	Loss: 4.8653
Profiling... [1280/50048]	Loss: 4.8164
Profiling... [1408/50048]	Loss: 4.6957
Profiling... [1536/50048]	Loss: 4.7498
Profiling... [1664/50048]	Loss: 4.6212
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0366, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.93851868432412,
                        "time": 2.4863560750000033,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 30154.98150403121
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6783
Profiling... [256/50048]	Loss: 5.7207
Profiling... [384/50048]	Loss: 6.0762
Profiling... [512/50048]	Loss: 5.5888
Profiling... [640/50048]	Loss: 5.2108
Profiling... [768/50048]	Loss: 4.8765
Profiling... [896/50048]	Loss: 4.9699
Profiling... [1024/50048]	Loss: 4.8477
Profiling... [1152/50048]	Loss: 4.8245
Profiling... [1280/50048]	Loss: 4.8466
Profiling... [1408/50048]	Loss: 4.9090
Profiling... [1536/50048]	Loss: 4.9285
Profiling... [1664/50048]	Loss: 4.7941
Profile done
epoch 1 train time consumed: 7.15s
Validation Epoch: 0, Average loss: 0.0372, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.2187467891864,
                        "time": 5.4487511060000315,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 65686.87231418604
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6659
Profiling... [256/50048]	Loss: 5.7914
Profiling... [384/50048]	Loss: 5.5759
Profiling... [512/50048]	Loss: 5.0498
Profiling... [640/50048]	Loss: 5.0514
Profiling... [768/50048]	Loss: 5.0585
Profiling... [896/50048]	Loss: 5.1200
Profiling... [1024/50048]	Loss: 4.8230
Profiling... [1152/50048]	Loss: 4.8450
Profiling... [1280/50048]	Loss: 4.6478
Profiling... [1408/50048]	Loss: 4.8160
Profiling... [1536/50048]	Loss: 4.7269
Profiling... [1664/50048]	Loss: 4.6486
Profile done
epoch 1 train time consumed: 3.18s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.37754455390171,
                        "time": 2.176912709000021,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26058.379410904043
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6548
Profiling... [256/50048]	Loss: 5.7487
Profiling... [384/50048]	Loss: 5.8580
Profiling... [512/50048]	Loss: 5.2906
Profiling... [640/50048]	Loss: 5.4730
Profiling... [768/50048]	Loss: 5.3206
Profiling... [896/50048]	Loss: 5.0691
Profiling... [1024/50048]	Loss: 5.2099
Profiling... [1152/50048]	Loss: 5.0759
Profiling... [1280/50048]	Loss: 5.0816
Profiling... [1408/50048]	Loss: 4.7694
Profiling... [1536/50048]	Loss: 4.8062
Profiling... [1664/50048]	Loss: 4.7364
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.16934192646096,
                        "time": 2.1995489960000327,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26505.45370198811
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6715
Profiling... [256/50048]	Loss: 5.5628
Profiling... [384/50048]	Loss: 5.9710
Profiling... [512/50048]	Loss: 5.3415
Profiling... [640/50048]	Loss: 5.3464
Profiling... [768/50048]	Loss: 5.2083
Profiling... [896/50048]	Loss: 4.7105
Profiling... [1024/50048]	Loss: 4.8628
Profiling... [1152/50048]	Loss: 4.6539
Profiling... [1280/50048]	Loss: 4.6715
Profiling... [1408/50048]	Loss: 4.8215
Profiling... [1536/50048]	Loss: 4.6986
Profiling... [1664/50048]	Loss: 4.9292
Profile done
epoch 1 train time consumed: 3.49s
Validation Epoch: 0, Average loss: 0.0364, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.60274315474517,
                        "time": 2.497604076000016,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 30206.596615068018
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6291
Profiling... [256/50048]	Loss: 5.8223
Profiling... [384/50048]	Loss: 5.8402
Profiling... [512/50048]	Loss: 5.2201
Profiling... [640/50048]	Loss: 4.9819
Profiling... [768/50048]	Loss: 4.9972
Profiling... [896/50048]	Loss: 4.9657
Profiling... [1024/50048]	Loss: 4.7236
Profiling... [1152/50048]	Loss: 4.7434
Profiling... [1280/50048]	Loss: 4.8162
Profiling... [1408/50048]	Loss: 4.9435
Profiling... [1536/50048]	Loss: 4.6195
Profiling... [1664/50048]	Loss: 4.6619
Profile done
epoch 1 train time consumed: 7.15s
Validation Epoch: 0, Average loss: 0.0370, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.02675487156031,
                        "time": 5.441198692,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 65490.188301684415
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6254
Profiling... [256/50048]	Loss: 7.7701
Profiling... [384/50048]	Loss: 6.6138
Profiling... [512/50048]	Loss: 6.1756
Profiling... [640/50048]	Loss: 5.5433
Profiling... [768/50048]	Loss: 4.6549
Profiling... [896/50048]	Loss: 5.3042
Profiling... [1024/50048]	Loss: 5.2183
Profiling... [1152/50048]	Loss: 4.6357
Profiling... [1280/50048]	Loss: 4.8529
Profiling... [1408/50048]	Loss: 4.7059
Profiling... [1536/50048]	Loss: 4.8275
Profiling... [1664/50048]	Loss: 4.7186
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.33333288154658,
                        "time": 2.462365959999943,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 29464.34264940559
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.6386
Profiling... [256/50048]	Loss: 7.4288
Profiling... [384/50048]	Loss: 7.5123
Profiling... [512/50048]	Loss: 5.0886
Profiling... [640/50048]	Loss: 5.5017
Profiling... [768/50048]	Loss: 5.3106
Profiling... [896/50048]	Loss: 4.7267
Profiling... [1024/50048]	Loss: 4.7049
Profiling... [1152/50048]	Loss: 5.2484
Profiling... [1280/50048]	Loss: 4.9985
Profiling... [1408/50048]	Loss: 4.7832
Profiling... [1536/50048]	Loss: 4.5732
Profiling... [1664/50048]	Loss: 4.6458
Profile done
epoch 1 train time consumed: 3.26s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.95016695587465,
                        "time": 2.2079221689999713,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26557.419698715843
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6088
Profiling... [256/50048]	Loss: 7.6196
Profiling... [384/50048]	Loss: 6.5540
Profiling... [512/50048]	Loss: 5.4853
Profiling... [640/50048]	Loss: 5.3141
Profiling... [768/50048]	Loss: 5.0349
Profiling... [896/50048]	Loss: 4.9630
Profiling... [1024/50048]	Loss: 5.3135
Profiling... [1152/50048]	Loss: 4.9205
Profiling... [1280/50048]	Loss: 5.3513
Profiling... [1408/50048]	Loss: 4.7733
Profiling... [1536/50048]	Loss: 4.5861
Profiling... [1664/50048]	Loss: 4.7463
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 0, Average loss: 0.0443, Accuracy: 0.0109
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.29618947343808,
                        "time": 2.497444808999944,
                        "accuracy": 0.010878164556962026,
                        "total_cost": 27388.411673110135
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6096
Profiling... [256/50048]	Loss: 7.2179
Profiling... [384/50048]	Loss: 7.7393
Profiling... [512/50048]	Loss: 5.9336
Profiling... [640/50048]	Loss: 5.4253
Profiling... [768/50048]	Loss: 5.8388
Profiling... [896/50048]	Loss: 5.0767
Profiling... [1024/50048]	Loss: 4.8775
Profiling... [1152/50048]	Loss: 4.9756
Profiling... [1280/50048]	Loss: 4.7645
Profiling... [1408/50048]	Loss: 4.7732
Profiling... [1536/50048]	Loss: 4.6264
Profiling... [1664/50048]	Loss: 4.6977
Profile done
epoch 1 train time consumed: 7.36s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.81780218430495,
                        "time": 5.4992426659999865,
                        "accuracy": 0.009988132911392405,
                        "total_cost": 65418.42535024769
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6649
Profiling... [256/50048]	Loss: 7.6572
Profiling... [384/50048]	Loss: 6.4933
Profiling... [512/50048]	Loss: 5.3771
Profiling... [640/50048]	Loss: 4.7271
Profiling... [768/50048]	Loss: 5.5877
Profiling... [896/50048]	Loss: 5.6147
Profiling... [1024/50048]	Loss: 5.2054
Profiling... [1152/50048]	Loss: 5.4760
Profiling... [1280/50048]	Loss: 4.8623
Profiling... [1408/50048]	Loss: 5.1221
Profiling... [1536/50048]	Loss: 5.0351
Profiling... [1664/50048]	Loss: 4.8590
Profile done
epoch 1 train time consumed: 3.20s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.27484808840087,
                        "time": 2.172484159000078,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 25982.80764996497
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.5781
Profiling... [256/50048]	Loss: 7.5921
Profiling... [384/50048]	Loss: 6.5521
Profiling... [512/50048]	Loss: 5.8221
Profiling... [640/50048]	Loss: 5.9294
Profiling... [768/50048]	Loss: 6.4547
Profiling... [896/50048]	Loss: 5.4298
Profiling... [1024/50048]	Loss: 5.5830
Profiling... [1152/50048]	Loss: 5.0673
Profiling... [1280/50048]	Loss: 4.7634
Profiling... [1408/50048]	Loss: 5.0716
Profiling... [1536/50048]	Loss: 4.8184
Profiling... [1664/50048]	Loss: 4.7240
Profile done
epoch 1 train time consumed: 3.19s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.86151362541044,
                        "time": 2.2112436219999836,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26577.547966119404
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6191
Profiling... [256/50048]	Loss: 7.8793
Profiling... [384/50048]	Loss: 7.2398
Profiling... [512/50048]	Loss: 5.3372
Profiling... [640/50048]	Loss: 5.3407
Profiling... [768/50048]	Loss: 5.1515
Profiling... [896/50048]	Loss: 4.7749
Profiling... [1024/50048]	Loss: 4.6065
Profiling... [1152/50048]	Loss: 4.7020
Profiling... [1280/50048]	Loss: 4.9747
Profiling... [1408/50048]	Loss: 4.8386
Profiling... [1536/50048]	Loss: 4.6426
Profiling... [1664/50048]	Loss: 4.7088
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.16890538441777,
                        "time": 2.505925305000005,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 30197.301337836987
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6443
Profiling... [256/50048]	Loss: 7.8929
Profiling... [384/50048]	Loss: 8.7709
Profiling... [512/50048]	Loss: 6.2004
Profiling... [640/50048]	Loss: 5.6598
Profiling... [768/50048]	Loss: 5.3556
Profiling... [896/50048]	Loss: 5.0386
Profiling... [1024/50048]	Loss: 4.7613
Profiling... [1152/50048]	Loss: 4.8170
Profiling... [1280/50048]	Loss: 4.6916
Profiling... [1408/50048]	Loss: 4.8124
Profiling... [1536/50048]	Loss: 4.6478
Profiling... [1664/50048]	Loss: 4.5702
Profile done
epoch 1 train time consumed: 7.16s
Validation Epoch: 0, Average loss: 0.0361, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.73873043722683,
                        "time": 5.4464291900000035,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 65394.51508161468
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 4.6129
Profiling... [256/50048]	Loss: 7.4839
Profiling... [384/50048]	Loss: 7.3568
Profiling... [512/50048]	Loss: 5.9497
Profiling... [640/50048]	Loss: 4.9012
Profiling... [768/50048]	Loss: 4.7339
Profiling... [896/50048]	Loss: 4.9701
Profiling... [1024/50048]	Loss: 4.7094
Profiling... [1152/50048]	Loss: 5.2158
Profiling... [1280/50048]	Loss: 4.8570
Profiling... [1408/50048]	Loss: 4.5740
Profiling... [1536/50048]	Loss: 4.7697
Profiling... [1664/50048]	Loss: 4.6482
Profile done
epoch 1 train time consumed: 3.16s
Validation Epoch: 0, Average loss: 0.0367, Accuracy: 0.0107
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.28046711219014,
                        "time": 2.1763332909999917,
                        "accuracy": 0.010680379746835443,
                        "total_cost": 24101.925619972546
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 4.5937
Profiling... [256/50048]	Loss: 7.6144
Profiling... [384/50048]	Loss: 6.8956
Profiling... [512/50048]	Loss: 5.3047
Profiling... [640/50048]	Loss: 4.7485
Profiling... [768/50048]	Loss: 5.0819
Profiling... [896/50048]	Loss: 5.4586
Profiling... [1024/50048]	Loss: 5.2079
Profiling... [1152/50048]	Loss: 5.0350
Profiling... [1280/50048]	Loss: 5.0269
Profiling... [1408/50048]	Loss: 4.8448
Profiling... [1536/50048]	Loss: 5.3232
Profiling... [1664/50048]	Loss: 4.8468
Profile done
epoch 1 train time consumed: 3.17s
Validation Epoch: 0, Average loss: 0.0669, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.79946365648141,
                        "time": 2.210990145999972,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 26560.628526170127
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 4.6419
Profiling... [256/50048]	Loss: 7.6987
Profiling... [384/50048]	Loss: 7.1714
Profiling... [512/50048]	Loss: 5.5043
Profiling... [640/50048]	Loss: 5.2176
Profiling... [768/50048]	Loss: 5.9128
Profiling... [896/50048]	Loss: 4.5854
Profiling... [1024/50048]	Loss: 4.7965
Profiling... [1152/50048]	Loss: 4.7805
Profiling... [1280/50048]	Loss: 4.6933
Profiling... [1408/50048]	Loss: 4.6895
Profiling... [1536/50048]	Loss: 4.9098
Profiling... [1664/50048]	Loss: 4.8285
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 0, Average loss: 0.0360, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.08755868316105,
                        "time": 2.5089683159999367,
                        "accuracy": 0.009889240506329115,
                        "total_cost": 30213.332497537154
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 4.6464
Profiling... [256/50048]	Loss: 7.4616
Profiling... [384/50048]	Loss: 6.6092
Profiling... [512/50048]	Loss: 5.2438
Profiling... [640/50048]	Loss: 6.2548
Profiling... [768/50048]	Loss: 5.2459
Profiling... [896/50048]	Loss: 5.0619
Profiling... [1024/50048]	Loss: 5.0375
Profiling... [1152/50048]	Loss: 4.9108
Profiling... [1280/50048]	Loss: 4.6334
Profiling... [1408/50048]	Loss: 4.6560
Profiling... [1536/50048]	Loss: 4.5782
Profiling... [1664/50048]	Loss: 4.9847
Profile done
epoch 1 train time consumed: 7.10s
Validation Epoch: 0, Average loss: 0.0493, Accuracy: 0.0066
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.7406188981925,
                        "time": 5.462847618999945,
                        "accuracy": 0.006625791139240506,
                        "total_cost": 97899.54038619528
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6406
Profiling... [512/50176]	Loss: 4.6457
Profiling... [768/50176]	Loss: 4.6816
Profiling... [1024/50176]	Loss: 4.6204
Profiling... [1280/50176]	Loss: 4.6396
Profiling... [1536/50176]	Loss: 4.6725
Profiling... [1792/50176]	Loss: 4.5930
Profiling... [2048/50176]	Loss: 4.7021
Profiling... [2304/50176]	Loss: 4.5891
Profiling... [2560/50176]	Loss: 4.5050
Profiling... [2816/50176]	Loss: 4.6130
Profiling... [3072/50176]	Loss: 4.5806
Profiling... [3328/50176]	Loss: 4.6894
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.34779408255135,
                        "time": 2.446199880999984,
                        "accuracy": 0.009765625,
                        "total_cost": 29645.041643658005
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6185
Profiling... [512/50176]	Loss: 4.6394
Profiling... [768/50176]	Loss: 4.6964
Profiling... [1024/50176]	Loss: 4.6710
Profiling... [1280/50176]	Loss: 4.6654
Profiling... [1536/50176]	Loss: 4.7082
Profiling... [1792/50176]	Loss: 4.6969
Profiling... [2048/50176]	Loss: 4.6245
Profiling... [2304/50176]	Loss: 4.6506
Profiling... [2560/50176]	Loss: 4.5813
Profiling... [2816/50176]	Loss: 4.6630
Profiling... [3072/50176]	Loss: 4.5625
Profiling... [3328/50176]	Loss: 4.5868
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.81199826155687,
                        "time": 2.534695806000059,
                        "accuracy": 0.009765625,
                        "total_cost": 30837.992826475976
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6358
Profiling... [512/50176]	Loss: 4.6806
Profiling... [768/50176]	Loss: 4.6821
Profiling... [1024/50176]	Loss: 4.7699
Profiling... [1280/50176]	Loss: 4.7082
Profiling... [1536/50176]	Loss: 4.6014
Profiling... [1792/50176]	Loss: 4.6465
Profiling... [2048/50176]	Loss: 4.6815
Profiling... [2304/50176]	Loss: 4.6969
Profiling... [2560/50176]	Loss: 4.6380
Profiling... [2816/50176]	Loss: 4.6179
Profiling... [3072/50176]	Loss: 4.5749
Profiling... [3328/50176]	Loss: 4.5862
Profile done
epoch 1 train time consumed: 4.09s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.05928950257358,
                        "time": 2.9000207459999956,
                        "accuracy": 0.009765625,
                        "total_cost": 35356.09953909584
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6341
Profiling... [512/50176]	Loss: 4.5806
Profiling... [768/50176]	Loss: 4.7217
Profiling... [1024/50176]	Loss: 4.7974
Profiling... [1280/50176]	Loss: 4.7843
Profiling... [1536/50176]	Loss: 4.6152
Profiling... [1792/50176]	Loss: 4.6103
Profiling... [2048/50176]	Loss: 4.6109
Profiling... [2304/50176]	Loss: 4.6275
Profiling... [2560/50176]	Loss: 4.5735
Profiling... [2816/50176]	Loss: 4.5884
Profiling... [3072/50176]	Loss: 4.6557
Profiling... [3328/50176]	Loss: 4.5768
Profile done
epoch 1 train time consumed: 9.45s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.64737477964522,
                        "time": 7.114697030000002,
                        "accuracy": 0.009765625,
                        "total_cost": 86439.94879611279
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6227
Profiling... [512/50176]	Loss: 4.6480
Profiling... [768/50176]	Loss: 4.6985
Profiling... [1024/50176]	Loss: 4.6836
Profiling... [1280/50176]	Loss: 4.6696
Profiling... [1536/50176]	Loss: 4.6106
Profiling... [1792/50176]	Loss: 4.6102
Profiling... [2048/50176]	Loss: 4.5705
Profiling... [2304/50176]	Loss: 4.5946
Profiling... [2560/50176]	Loss: 4.6629
Profiling... [2816/50176]	Loss: 4.5903
Profiling... [3072/50176]	Loss: 4.5996
Profiling... [3328/50176]	Loss: 4.5946
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.3966712565434,
                        "time": 2.426071666999974,
                        "accuracy": 0.009765625,
                        "total_cost": 29413.254103307274
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6291
Profiling... [512/50176]	Loss: 4.6907
Profiling... [768/50176]	Loss: 4.6625
Profiling... [1024/50176]	Loss: 4.6380
Profiling... [1280/50176]	Loss: 4.6794
Profiling... [1536/50176]	Loss: 4.6584
Profiling... [1792/50176]	Loss: 4.6277
Profiling... [2048/50176]	Loss: 4.6833
Profiling... [2304/50176]	Loss: 4.5467
Profiling... [2560/50176]	Loss: 4.5830
Profiling... [2816/50176]	Loss: 4.5569
Profiling... [3072/50176]	Loss: 4.6544
Profiling... [3328/50176]	Loss: 4.5495
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.83011094295495,
                        "time": 2.519011298999999,
                        "accuracy": 0.009765625,
                        "total_cost": 30651.841753776836
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6445
Profiling... [512/50176]	Loss: 4.6749
Profiling... [768/50176]	Loss: 4.6836
Profiling... [1024/50176]	Loss: 4.6696
Profiling... [1280/50176]	Loss: 4.6231
Profiling... [1536/50176]	Loss: 4.6799
Profiling... [1792/50176]	Loss: 4.6506
Profiling... [2048/50176]	Loss: 4.6447
Profiling... [2304/50176]	Loss: 4.5529
Profiling... [2560/50176]	Loss: 4.6157
Profiling... [2816/50176]	Loss: 4.6868
Profiling... [3072/50176]	Loss: 4.5588
Profiling... [3328/50176]	Loss: 4.5224
Profile done
epoch 1 train time consumed: 4.04s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.04234953201242,
                        "time": 2.8857397250000076,
                        "accuracy": 0.009765625,
                        "total_cost": 35176.984268990906
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6357
Profiling... [512/50176]	Loss: 4.6815
Profiling... [768/50176]	Loss: 4.7103
Profiling... [1024/50176]	Loss: 4.6353
Profiling... [1280/50176]	Loss: 4.6986
Profiling... [1536/50176]	Loss: 4.6377
Profiling... [1792/50176]	Loss: 4.6958
Profiling... [2048/50176]	Loss: 4.6432
Profiling... [2304/50176]	Loss: 4.5782
Profiling... [2560/50176]	Loss: 4.5865
Profiling... [2816/50176]	Loss: 4.5748
Profiling... [3072/50176]	Loss: 4.5797
Profiling... [3328/50176]	Loss: 4.5354
Profile done
epoch 1 train time consumed: 9.40s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.72081118207296,
                        "time": 7.070956625999997,
                        "accuracy": 0.009765625,
                        "total_cost": 85961.69794273007
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6549
Profiling... [512/50176]	Loss: 4.6734
Profiling... [768/50176]	Loss: 4.6392
Profiling... [1024/50176]	Loss: 4.6645
Profiling... [1280/50176]	Loss: 4.6792
Profiling... [1536/50176]	Loss: 4.7344
Profiling... [1792/50176]	Loss: 4.5994
Profiling... [2048/50176]	Loss: 4.6417
Profiling... [2304/50176]	Loss: 4.6216
Profiling... [2560/50176]	Loss: 4.6645
Profiling... [2816/50176]	Loss: 4.5852
Profiling... [3072/50176]	Loss: 4.5888
Profiling... [3328/50176]	Loss: 4.6357
Profile done
epoch 1 train time consumed: 3.54s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.49244298817263,
                        "time": 2.4254394609999963,
                        "accuracy": 0.009765625,
                        "total_cost": 29429.37569830976
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6528
Profiling... [512/50176]	Loss: 4.6564
Profiling... [768/50176]	Loss: 4.7251
Profiling... [1024/50176]	Loss: 4.5922
Profiling... [1280/50176]	Loss: 4.6547
Profiling... [1536/50176]	Loss: 4.6227
Profiling... [1792/50176]	Loss: 4.5795
Profiling... [2048/50176]	Loss: 4.5751
Profiling... [2304/50176]	Loss: 4.6545
Profiling... [2560/50176]	Loss: 4.5802
Profiling... [2816/50176]	Loss: 4.5495
Profiling... [3072/50176]	Loss: 4.5832
Profiling... [3328/50176]	Loss: 4.6232
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.87775934843451,
                        "time": 2.5254527150000285,
                        "accuracy": 0.009765625,
                        "total_cost": 30742.544394201494
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6424
Profiling... [512/50176]	Loss: 4.6433
Profiling... [768/50176]	Loss: 4.6478
Profiling... [1024/50176]	Loss: 4.6166
Profiling... [1280/50176]	Loss: 4.6834
Profiling... [1536/50176]	Loss: 4.7056
Profiling... [1792/50176]	Loss: 4.6784
Profiling... [2048/50176]	Loss: 4.6972
Profiling... [2304/50176]	Loss: 4.6482
Profiling... [2560/50176]	Loss: 4.6272
Profiling... [2816/50176]	Loss: 4.5836
Profiling... [3072/50176]	Loss: 4.6456
Profiling... [3328/50176]	Loss: 4.6330
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.066062739637,
                        "time": 2.897483718000103,
                        "accuracy": 0.009765625,
                        "total_cost": 35327.17856301844
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6466
Profiling... [512/50176]	Loss: 4.6259
Profiling... [768/50176]	Loss: 4.6803
Profiling... [1024/50176]	Loss: 4.6787
Profiling... [1280/50176]	Loss: 4.7386
Profiling... [1536/50176]	Loss: 4.7372
Profiling... [1792/50176]	Loss: 4.6335
Profiling... [2048/50176]	Loss: 4.6244
Profiling... [2304/50176]	Loss: 4.6030
Profiling... [2560/50176]	Loss: 4.6132
Profiling... [2816/50176]	Loss: 4.6573
Profiling... [3072/50176]	Loss: 4.6127
Profiling... [3328/50176]	Loss: 4.5787
Profile done
epoch 1 train time consumed: 9.65s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.71334576525658,
                        "time": 7.315420014999972,
                        "accuracy": 0.009765625,
                        "total_cost": 88928.04973145807
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6301
Profiling... [512/50176]	Loss: 5.6117
Profiling... [768/50176]	Loss: 5.5827
Profiling... [1024/50176]	Loss: 5.2630
Profiling... [1280/50176]	Loss: 5.1469
Profiling... [1536/50176]	Loss: 4.7787
Profiling... [1792/50176]	Loss: 4.6379
Profiling... [2048/50176]	Loss: 4.7433
Profiling... [2304/50176]	Loss: 4.7084
Profiling... [2560/50176]	Loss: 4.6240
Profiling... [2816/50176]	Loss: 4.5842
Profiling... [3072/50176]	Loss: 4.6963
Profiling... [3328/50176]	Loss: 4.6126
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.51657450668402,
                        "time": 2.444459993999999,
                        "accuracy": 0.009765625,
                        "total_cost": 29666.204160768946
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6420
Profiling... [512/50176]	Loss: 5.4549
Profiling... [768/50176]	Loss: 5.2701
Profiling... [1024/50176]	Loss: 4.8792
Profiling... [1280/50176]	Loss: 4.9598
Profiling... [1536/50176]	Loss: 4.7113
Profiling... [1792/50176]	Loss: 4.7112
Profiling... [2048/50176]	Loss: 4.8274
Profiling... [2304/50176]	Loss: 4.5957
Profiling... [2560/50176]	Loss: 4.8129
Profiling... [2816/50176]	Loss: 4.7314
Profiling... [3072/50176]	Loss: 4.5803
Profiling... [3328/50176]	Loss: 4.6673
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.86226083044781,
                        "time": 2.4973410269999476,
                        "accuracy": 0.009765625,
                        "total_cost": 30396.375094665847
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6240
Profiling... [512/50176]	Loss: 5.3734
Profiling... [768/50176]	Loss: 5.7640
Profiling... [1024/50176]	Loss: 5.2349
Profiling... [1280/50176]	Loss: 4.8617
Profiling... [1536/50176]	Loss: 4.8246
Profiling... [1792/50176]	Loss: 4.9434
Profiling... [2048/50176]	Loss: 4.6839
Profiling... [2304/50176]	Loss: 4.6407
Profiling... [2560/50176]	Loss: 4.8095
Profiling... [2816/50176]	Loss: 4.6914
Profiling... [3072/50176]	Loss: 4.5852
Profiling... [3328/50176]	Loss: 4.7513
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.02755079940236,
                        "time": 2.902986735000013,
                        "accuracy": 0.009765625,
                        "total_cost": 35382.825069589024
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6524
Profiling... [512/50176]	Loss: 5.6061
Profiling... [768/50176]	Loss: 5.3847
Profiling... [1024/50176]	Loss: 5.1992
Profiling... [1280/50176]	Loss: 4.8327
Profiling... [1536/50176]	Loss: 4.8594
Profiling... [1792/50176]	Loss: 4.9632
Profiling... [2048/50176]	Loss: 4.7352
Profiling... [2304/50176]	Loss: 4.7293
Profiling... [2560/50176]	Loss: 4.7400
Profiling... [2816/50176]	Loss: 4.8342
Profiling... [3072/50176]	Loss: 4.6687
Profiling... [3328/50176]	Loss: 4.7374
Profile done
epoch 1 train time consumed: 9.89s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.70393070007702,
                        "time": 7.146770247999939,
                        "accuracy": 0.009765625,
                        "total_cost": 86871.0113533908
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6315
Profiling... [512/50176]	Loss: 5.6992
Profiling... [768/50176]	Loss: 5.5352
Profiling... [1024/50176]	Loss: 4.9872
Profiling... [1280/50176]	Loss: 4.9504
Profiling... [1536/50176]	Loss: 4.7176
Profiling... [1792/50176]	Loss: 4.7458
Profiling... [2048/50176]	Loss: 4.7207
Profiling... [2304/50176]	Loss: 4.6008
Profiling... [2560/50176]	Loss: 4.6640
Profiling... [2816/50176]	Loss: 4.8442
Profiling... [3072/50176]	Loss: 4.5619
Profiling... [3328/50176]	Loss: 4.6817
Profile done
epoch 1 train time consumed: 3.73s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.55141633159131,
                        "time": 2.4573715970000194,
                        "accuracy": 0.009765625,
                        "total_cost": 29831.668047603376
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6451
Profiling... [512/50176]	Loss: 5.6384
Profiling... [768/50176]	Loss: 5.4515
Profiling... [1024/50176]	Loss: 5.1966
Profiling... [1280/50176]	Loss: 5.3050
Profiling... [1536/50176]	Loss: 4.8469
Profiling... [1792/50176]	Loss: 4.8582
Profiling... [2048/50176]	Loss: 4.6665
Profiling... [2304/50176]	Loss: 4.6347
Profiling... [2560/50176]	Loss: 4.7322
Profiling... [2816/50176]	Loss: 4.7701
Profiling... [3072/50176]	Loss: 4.5944
Profiling... [3328/50176]	Loss: 4.6723
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 0, Average loss: 0.0185, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.86271383536658,
                        "time": 2.52741993799998,
                        "accuracy": 0.009765625,
                        "total_cost": 30762.597666026657
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6426
Profiling... [512/50176]	Loss: 5.5364
Profiling... [768/50176]	Loss: 5.6542
Profiling... [1024/50176]	Loss: 4.7952
Profiling... [1280/50176]	Loss: 4.9235
Profiling... [1536/50176]	Loss: 4.8761
Profiling... [1792/50176]	Loss: 4.7580
Profiling... [2048/50176]	Loss: 4.6469
Profiling... [2304/50176]	Loss: 4.7014
Profiling... [2560/50176]	Loss: 4.7825
Profiling... [2816/50176]	Loss: 4.9617
Profiling... [3072/50176]	Loss: 4.7488
Profiling... [3328/50176]	Loss: 4.6243
Profile done
epoch 1 train time consumed: 4.16s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.02853284231037,
                        "time": 2.9025953730000538,
                        "accuracy": 0.009765625,
                        "total_cost": 35378.34687314688
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6489
Profiling... [512/50176]	Loss: 5.7914
Profiling... [768/50176]	Loss: 5.5766
Profiling... [1024/50176]	Loss: 5.0954
Profiling... [1280/50176]	Loss: 4.9436
Profiling... [1536/50176]	Loss: 5.0082
Profiling... [1792/50176]	Loss: 4.9412
Profiling... [2048/50176]	Loss: 4.7426
Profiling... [2304/50176]	Loss: 4.8030
Profiling... [2560/50176]	Loss: 4.7388
Profiling... [2816/50176]	Loss: 4.6759
Profiling... [3072/50176]	Loss: 4.6981
Profiling... [3328/50176]	Loss: 4.6986
Profile done
epoch 1 train time consumed: 9.84s
Validation Epoch: 0, Average loss: 0.0187, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.71601302460087,
                        "time": 7.445392294000044,
                        "accuracy": 0.009765625,
                        "total_cost": 90510.05834729187
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6227
Profiling... [512/50176]	Loss: 5.5529
Profiling... [768/50176]	Loss: 5.2504
Profiling... [1024/50176]	Loss: 4.8975
Profiling... [1280/50176]	Loss: 4.8890
Profiling... [1536/50176]	Loss: 4.9472
Profiling... [1792/50176]	Loss: 4.7054
Profiling... [2048/50176]	Loss: 5.2657
Profiling... [2304/50176]	Loss: 4.7786
Profiling... [2560/50176]	Loss: 4.6595
Profiling... [2816/50176]	Loss: 4.5888
Profiling... [3072/50176]	Loss: 4.8109
Profiling... [3328/50176]	Loss: 4.7119
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 0, Average loss: 0.0182, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.57935200593293,
                        "time": 2.436621639000009,
                        "accuracy": 0.009765625,
                        "total_cost": 29586.740739712543
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6549
Profiling... [512/50176]	Loss: 5.6356
Profiling... [768/50176]	Loss: 5.3051
Profiling... [1024/50176]	Loss: 5.0991
Profiling... [1280/50176]	Loss: 4.8508
Profiling... [1536/50176]	Loss: 4.9652
Profiling... [1792/50176]	Loss: 4.6179
Profiling... [2048/50176]	Loss: 4.8812
Profiling... [2304/50176]	Loss: 4.6090
Profiling... [2560/50176]	Loss: 4.8031
Profiling... [2816/50176]	Loss: 4.6276
Profiling... [3072/50176]	Loss: 4.6709
Profiling... [3328/50176]	Loss: 4.5645
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 0, Average loss: 0.0183, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.87737136496922,
                        "time": 2.5071126469999854,
                        "accuracy": 0.009765625,
                        "total_cost": 30519.18962598177
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6688
Profiling... [512/50176]	Loss: 5.7398
Profiling... [768/50176]	Loss: 5.2265
Profiling... [1024/50176]	Loss: 5.1991
Profiling... [1280/50176]	Loss: 4.8087
Profiling... [1536/50176]	Loss: 4.8565
Profiling... [1792/50176]	Loss: 4.8194
Profiling... [2048/50176]	Loss: 4.8082
Profiling... [2304/50176]	Loss: 4.7391
Profiling... [2560/50176]	Loss: 4.6766
Profiling... [2816/50176]	Loss: 4.8282
Profiling... [3072/50176]	Loss: 4.6580
Profiling... [3328/50176]	Loss: 4.5952
Profile done
epoch 1 train time consumed: 4.15s
Validation Epoch: 0, Average loss: 0.0186, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.01831379283394,
                        "time": 2.8889311609999595,
                        "accuracy": 0.009765625,
                        "total_cost": 35208.777261648815
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6258
Profiling... [512/50176]	Loss: 5.6149
Profiling... [768/50176]	Loss: 5.2786
Profiling... [1024/50176]	Loss: 5.0704
Profiling... [1280/50176]	Loss: 5.0590
Profiling... [1536/50176]	Loss: 4.9766
Profiling... [1792/50176]	Loss: 5.0416
Profiling... [2048/50176]	Loss: 4.8662
Profiling... [2304/50176]	Loss: 4.8675
Profiling... [2560/50176]	Loss: 4.6782
Profiling... [2816/50176]	Loss: 4.7134
Profiling... [3072/50176]	Loss: 4.7000
Profiling... [3328/50176]	Loss: 4.6288
Profile done
epoch 1 train time consumed: 9.60s
Validation Epoch: 0, Average loss: 0.0184, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 118.73923368794634,
                        "time": 7.157924059000038,
                        "accuracy": 0.009765625,
                        "total_cost": 87032.46515836712
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6447
Profiling... [512/50176]	Loss: 7.2987
Profiling... [768/50176]	Loss: 6.8862
Profiling... [1024/50176]	Loss: 5.7257
Profiling... [1280/50176]	Loss: 5.5927
Profiling... [1536/50176]	Loss: 4.6161
Profiling... [1792/50176]	Loss: 4.6806
Profiling... [2048/50176]	Loss: 5.0026
Profiling... [2304/50176]	Loss: 4.6884
Profiling... [2560/50176]	Loss: 4.7787
Profiling... [2816/50176]	Loss: 4.6850
Profiling... [3072/50176]	Loss: 4.6241
Profiling... [3328/50176]	Loss: 4.6440
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.58496383021415,
                        "time": 2.4207534450000594,
                        "accuracy": 0.009765625,
                        "total_cost": 29395.45187504111
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6388
Profiling... [512/50176]	Loss: 7.8951
Profiling... [768/50176]	Loss: 5.3830
Profiling... [1024/50176]	Loss: 5.1489
Profiling... [1280/50176]	Loss: 4.8052
Profiling... [1536/50176]	Loss: 5.5198
Profiling... [1792/50176]	Loss: 6.2331
Profiling... [2048/50176]	Loss: 5.3138
Profiling... [2304/50176]	Loss: 4.8836
Profiling... [2560/50176]	Loss: 5.2781
Profiling... [2816/50176]	Loss: 4.8461
Profiling... [3072/50176]	Loss: 4.9252
Profiling... [3328/50176]	Loss: 4.8888
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.88271084507679,
                        "time": 2.5020610889999944,
                        "accuracy": 0.009765625,
                        "total_cost": 30459.06482793516
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6773
Profiling... [512/50176]	Loss: 7.3308
Profiling... [768/50176]	Loss: 7.2231
Profiling... [1024/50176]	Loss: 5.4248
Profiling... [1280/50176]	Loss: 5.2865
Profiling... [1536/50176]	Loss: 4.7269
Profiling... [1792/50176]	Loss: 6.1394
Profiling... [2048/50176]	Loss: 4.9233
Profiling... [2304/50176]	Loss: 4.7265
Profiling... [2560/50176]	Loss: 5.0695
Profiling... [2816/50176]	Loss: 4.7294
Profiling... [3072/50176]	Loss: 4.5552
Profiling... [3328/50176]	Loss: 5.1915
Profile done
epoch 1 train time consumed: 4.13s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.02028470649174,
                        "time": 2.8907979199999545,
                        "accuracy": 0.009765625,
                        "total_cost": 35232.11176625446
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6620
Profiling... [512/50176]	Loss: 7.4411
Profiling... [768/50176]	Loss: 6.2419
Profiling... [1024/50176]	Loss: 5.1913
Profiling... [1280/50176]	Loss: 4.8235
Profiling... [1536/50176]	Loss: 4.9429
Profiling... [1792/50176]	Loss: 4.8761
Profiling... [2048/50176]	Loss: 4.7894
Profiling... [2304/50176]	Loss: 4.9213
Profiling... [2560/50176]	Loss: 4.8320
Profiling... [2816/50176]	Loss: 4.6838
Profiling... [3072/50176]	Loss: 4.6581
Profiling... [3328/50176]	Loss: 4.5666
Profile done
epoch 1 train time consumed: 9.75s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 118.76561486111282,
                        "time": 7.197017431999939,
                        "accuracy": 0.009765625,
                        "total_cost": 87527.23972890829
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6587
Profiling... [512/50176]	Loss: 7.4147
Profiling... [768/50176]	Loss: 5.6701
Profiling... [1024/50176]	Loss: 5.5309
Profiling... [1280/50176]	Loss: 5.3288
Profiling... [1536/50176]	Loss: 4.8675
Profiling... [1792/50176]	Loss: 4.6221
Profiling... [2048/50176]	Loss: 4.9280
Profiling... [2304/50176]	Loss: 5.5465
Profiling... [2560/50176]	Loss: 4.9297
Profiling... [2816/50176]	Loss: 5.5141
Profiling... [3072/50176]	Loss: 5.1885
Profiling... [3328/50176]	Loss: 5.1308
Profile done
epoch 1 train time consumed: 3.93s
Validation Epoch: 0, Average loss: 0.0181, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.63886191110689,
                        "time": 2.438232049000021,
                        "accuracy": 0.009765625,
                        "total_cost": 29621.15331773939
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6044
Profiling... [512/50176]	Loss: 7.3810
Profiling... [768/50176]	Loss: 5.8834
Profiling... [1024/50176]	Loss: 5.0725
Profiling... [1280/50176]	Loss: 4.9734
Profiling... [1536/50176]	Loss: 4.8522
Profiling... [1792/50176]	Loss: 5.5199
Profiling... [2048/50176]	Loss: 5.1752
Profiling... [2304/50176]	Loss: 5.3431
Profiling... [2560/50176]	Loss: 4.8535
Profiling... [2816/50176]	Loss: 4.7207
Profiling... [3072/50176]	Loss: 4.8384
Profiling... [3328/50176]	Loss: 4.9964
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0093
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.89983206968593,
                        "time": 2.512274504000061,
                        "accuracy": 0.00927734375,
                        "total_cost": 32197.687688198548
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6325
Profiling... [512/50176]	Loss: 7.5349
Profiling... [768/50176]	Loss: 6.2830
Profiling... [1024/50176]	Loss: 5.1383
Profiling... [1280/50176]	Loss: 5.0893
Profiling... [1536/50176]	Loss: 5.1618
Profiling... [1792/50176]	Loss: 5.2083
Profiling... [2048/50176]	Loss: 4.9019
Profiling... [2304/50176]	Loss: 5.1408
Profiling... [2560/50176]	Loss: 4.7172
Profiling... [2816/50176]	Loss: 4.8645
Profiling... [3072/50176]	Loss: 5.0331
Profiling... [3328/50176]	Loss: 4.8893
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 0, Average loss: 0.0190, Accuracy: 0.0101
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.03310607441621,
                        "time": 2.899139439999999,
                        "accuracy": 0.01005859375,
                        "total_cost": 34308.331866573644
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.6582
Profiling... [512/50176]	Loss: 7.4058
Profiling... [768/50176]	Loss: 5.8705
Profiling... [1024/50176]	Loss: 4.9964
Profiling... [1280/50176]	Loss: 5.0901
Profiling... [1536/50176]	Loss: 5.0947
Profiling... [1792/50176]	Loss: 5.1153
Profiling... [2048/50176]	Loss: 4.9793
Profiling... [2304/50176]	Loss: 4.8158
Profiling... [2560/50176]	Loss: 4.6107
Profiling... [2816/50176]	Loss: 4.7739
Profiling... [3072/50176]	Loss: 4.8317
Profiling... [3328/50176]	Loss: 4.6586
Profile done
epoch 1 train time consumed: 9.71s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0139
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 118.78899326438807,
                        "time": 7.253032014999917,
                        "accuracy": 0.0138671875,
                        "total_cost": 62130.86620312996
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 4.6482
Profiling... [512/50176]	Loss: 7.4657
Profiling... [768/50176]	Loss: 6.3012
Profiling... [1024/50176]	Loss: 5.2115
Profiling... [1280/50176]	Loss: 5.2053
Profiling... [1536/50176]	Loss: 5.0559
Profiling... [1792/50176]	Loss: 4.7745
Profiling... [2048/50176]	Loss: 5.0020
Profiling... [2304/50176]	Loss: 4.6844
Profiling... [2560/50176]	Loss: 4.6244
Profiling... [2816/50176]	Loss: 4.7896
Profiling... [3072/50176]	Loss: 4.5844
Profiling... [3328/50176]	Loss: 5.0085
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.6713411177784,
                        "time": 2.4455149450000135,
                        "accuracy": 0.009765625,
                        "total_cost": 29717.7639164643
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 4.6431
Profiling... [512/50176]	Loss: 7.2424
Profiling... [768/50176]	Loss: 6.6839
Profiling... [1024/50176]	Loss: 5.4441
Profiling... [1280/50176]	Loss: 4.9318
Profiling... [1536/50176]	Loss: 4.9326
Profiling... [1792/50176]	Loss: 4.7752
Profiling... [2048/50176]	Loss: 4.6609
Profiling... [2304/50176]	Loss: 4.9753
Profiling... [2560/50176]	Loss: 4.5854
Profiling... [2816/50176]	Loss: 4.5633
Profiling... [3072/50176]	Loss: 4.8831
Profiling... [3328/50176]	Loss: 5.2026
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 0, Average loss: 0.0229, Accuracy: 0.0104
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.92702223258469,
                        "time": 2.5152284739999686,
                        "accuracy": 0.0103515625,
                        "total_cost": 28896.954701034203
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 4.6481
Profiling... [512/50176]	Loss: 7.4928
Profiling... [768/50176]	Loss: 5.8706
Profiling... [1024/50176]	Loss: 5.6219
Profiling... [1280/50176]	Loss: 4.6623
Profiling... [1536/50176]	Loss: 5.1382
Profiling... [1792/50176]	Loss: 5.1914
Profiling... [2048/50176]	Loss: 5.2608
Profiling... [2304/50176]	Loss: 5.8315
Profiling... [2560/50176]	Loss: 5.2004
Profiling... [2816/50176]	Loss: 5.2641
Profiling... [3072/50176]	Loss: 4.8914
Profiling... [3328/50176]	Loss: 4.9630
Profile done
epoch 1 train time consumed: 4.09s
Validation Epoch: 0, Average loss: 0.0444, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.05078107491636,
                        "time": 2.892227888999969,
                        "accuracy": 0.009765625,
                        "total_cost": 35258.571697367326
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 4.5938
Profiling... [512/50176]	Loss: 7.8375
Profiling... [768/50176]	Loss: 6.5412
Profiling... [1024/50176]	Loss: 5.4852
Profiling... [1280/50176]	Loss: 4.6631
Profiling... [1536/50176]	Loss: 4.7269
Profiling... [1792/50176]	Loss: 5.2776
Profiling... [2048/50176]	Loss: 5.4702
Profiling... [2304/50176]	Loss: 4.8612
Profiling... [2560/50176]	Loss: 4.7132
Profiling... [2816/50176]	Loss: 4.8620
Profiling... [3072/50176]	Loss: 4.7826
Profiling... [3328/50176]	Loss: 4.6401
Profile done
epoch 1 train time consumed: 9.86s
Validation Epoch: 0, Average loss: 0.0180, Accuracy: 0.0092
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 118.81709307047801,
                        "time": 7.413864258999979,
                        "accuracy": 0.0091796875,
                        "total_cost": 95961.19690060157
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6299
Profiling... [1024/50176]	Loss: 4.6393
Profiling... [1536/50176]	Loss: 4.6450
Profiling... [2048/50176]	Loss: 4.6358
Profiling... [2560/50176]	Loss: 4.6000
Profiling... [3072/50176]	Loss: 4.6255
Profiling... [3584/50176]	Loss: 4.5704
Profiling... [4096/50176]	Loss: 4.6051
Profiling... [4608/50176]	Loss: 4.5564
Profiling... [5120/50176]	Loss: 4.5515
Profiling... [5632/50176]	Loss: 4.5741
Profiling... [6144/50176]	Loss: 4.5207
Profiling... [6656/50176]	Loss: 4.5123
Profile done
epoch 1 train time consumed: 6.68s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.82431591875363,
                        "time": 4.655174322999983,
                        "accuracy": 0.009765625,
                        "total_cost": 56642.34541189325
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6346
Profiling... [1024/50176]	Loss: 4.6621
Profiling... [1536/50176]	Loss: 4.6394
Profiling... [2048/50176]	Loss: 4.6542
Profiling... [2560/50176]	Loss: 4.5950
Profiling... [3072/50176]	Loss: 4.5756
Profiling... [3584/50176]	Loss: 4.5728
Profiling... [4096/50176]	Loss: 4.5697
Profiling... [4608/50176]	Loss: 4.5643
Profiling... [5120/50176]	Loss: 4.5787
Profiling... [5632/50176]	Loss: 4.5266
Profiling... [6144/50176]	Loss: 4.5284
Profiling... [6656/50176]	Loss: 4.5352
Profile done
epoch 1 train time consumed: 6.85s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.14130775621615,
                        "time": 4.790474615999983,
                        "accuracy": 0.009765625,
                        "total_cost": 58444.12523757521
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6410
Profiling... [1024/50176]	Loss: 4.6609
Profiling... [1536/50176]	Loss: 4.6393
Profiling... [2048/50176]	Loss: 4.6298
Profiling... [2560/50176]	Loss: 4.6247
Profiling... [3072/50176]	Loss: 4.6123
Profiling... [3584/50176]	Loss: 4.5409
Profiling... [4096/50176]	Loss: 4.5468
Profiling... [4608/50176]	Loss: 4.5996
Profiling... [5120/50176]	Loss: 4.5900
Profiling... [5632/50176]	Loss: 4.6195
Profiling... [6144/50176]	Loss: 4.5645
Profiling... [6656/50176]	Loss: 4.4883
Profile done
epoch 1 train time consumed: 7.81s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.23287746435712,
                        "time": 5.594892261999803,
                        "accuracy": 0.009765625,
                        "total_cost": 68310.53859853336
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6112
Profiling... [1024/50176]	Loss: 4.6507
Profiling... [1536/50176]	Loss: 4.6797
Profiling... [2048/50176]	Loss: 4.6548
Profiling... [2560/50176]	Loss: 4.6342
Profiling... [3072/50176]	Loss: 4.5992
Profiling... [3584/50176]	Loss: 4.5802
Profiling... [4096/50176]	Loss: 4.5172
Profiling... [4608/50176]	Loss: 4.5526
Profiling... [5120/50176]	Loss: 4.5660
Profiling... [5632/50176]	Loss: 4.5415
Profiling... [6144/50176]	Loss: 4.5266
Profiling... [6656/50176]	Loss: 4.4985
Profile done
epoch 1 train time consumed: 17.47s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.82466827822267,
                        "time": 13.041779355000017,
                        "accuracy": 0.009765625,
                        "total_cost": 158687.75481504254
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6411
Profiling... [1024/50176]	Loss: 4.6355
Profiling... [1536/50176]	Loss: 4.6628
Profiling... [2048/50176]	Loss: 4.6354
Profiling... [2560/50176]	Loss: 4.6021
Profiling... [3072/50176]	Loss: 4.6140
Profiling... [3584/50176]	Loss: 4.6056
Profiling... [4096/50176]	Loss: 4.5977
Profiling... [4608/50176]	Loss: 4.5853
Profiling... [5120/50176]	Loss: 4.5549
Profiling... [5632/50176]	Loss: 4.5685
Profiling... [6144/50176]	Loss: 4.5366
Profiling... [6656/50176]	Loss: 4.4850
Profile done
epoch 1 train time consumed: 6.61s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.87683772759578,
                        "time": 4.630095026999925,
                        "accuracy": 0.009765625,
                        "total_cost": 56362.09205125307
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6461
Profiling... [1024/50176]	Loss: 4.6510
Profiling... [1536/50176]	Loss: 4.6435
Profiling... [2048/50176]	Loss: 4.6461
Profiling... [2560/50176]	Loss: 4.6236
Profiling... [3072/50176]	Loss: 4.5697
Profiling... [3584/50176]	Loss: 4.5880
Profiling... [4096/50176]	Loss: 4.5761
Profiling... [4608/50176]	Loss: 4.5594
Profiling... [5120/50176]	Loss: 4.5438
Profiling... [5632/50176]	Loss: 4.5925
Profiling... [6144/50176]	Loss: 4.5450
Profiling... [6656/50176]	Loss: 4.5112
Profile done
epoch 1 train time consumed: 6.82s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.17502691839285,
                        "time": 4.795059534000075,
                        "accuracy": 0.009765625,
                        "total_cost": 58516.61814167092
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6331
Profiling... [1024/50176]	Loss: 4.6603
Profiling... [1536/50176]	Loss: 4.6293
Profiling... [2048/50176]	Loss: 4.6296
Profiling... [2560/50176]	Loss: 4.5969
Profiling... [3072/50176]	Loss: 4.5746
Profiling... [3584/50176]	Loss: 4.5793
Profiling... [4096/50176]	Loss: 4.5955
Profiling... [4608/50176]	Loss: 4.5646
Profiling... [5120/50176]	Loss: 4.5559
Profiling... [5632/50176]	Loss: 4.5289
Profiling... [6144/50176]	Loss: 4.5402
Profiling... [6656/50176]	Loss: 4.4975
Profile done
epoch 1 train time consumed: 7.89s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.26853428893472,
                        "time": 5.595832631999883,
                        "accuracy": 0.009765625,
                        "total_cost": 68342.45182922935
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6551
Profiling... [1024/50176]	Loss: 4.6480
Profiling... [1536/50176]	Loss: 4.6762
Profiling... [2048/50176]	Loss: 4.6688
Profiling... [2560/50176]	Loss: 4.6148
Profiling... [3072/50176]	Loss: 4.6252
Profiling... [3584/50176]	Loss: 4.5592
Profiling... [4096/50176]	Loss: 4.6101
Profiling... [4608/50176]	Loss: 4.5561
Profiling... [5120/50176]	Loss: 4.5962
Profiling... [5632/50176]	Loss: 4.5545
Profiling... [6144/50176]	Loss: 4.5640
Profiling... [6656/50176]	Loss: 4.5352
Profile done
epoch 1 train time consumed: 17.46s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 118.887246758563,
                        "time": 13.038652054000067,
                        "accuracy": 0.009765625,
                        "total_cost": 158733.2550802381
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6430
Profiling... [1024/50176]	Loss: 4.6270
Profiling... [1536/50176]	Loss: 4.6695
Profiling... [2048/50176]	Loss: 4.6211
Profiling... [2560/50176]	Loss: 4.6640
Profiling... [3072/50176]	Loss: 4.6335
Profiling... [3584/50176]	Loss: 4.5851
Profiling... [4096/50176]	Loss: 4.5654
Profiling... [4608/50176]	Loss: 4.5697
Profiling... [5120/50176]	Loss: 4.5542
Profiling... [5632/50176]	Loss: 4.5348
Profiling... [6144/50176]	Loss: 4.5154
Profiling... [6656/50176]	Loss: 4.5128
Profile done
epoch 1 train time consumed: 6.73s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.93963528342614,
                        "time": 4.654530095999917,
                        "accuracy": 0.009765625,
                        "total_cost": 56689.47067227756
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6451
Profiling... [1024/50176]	Loss: 4.6299
Profiling... [1536/50176]	Loss: 4.6183
Profiling... [2048/50176]	Loss: 4.6245
Profiling... [2560/50176]	Loss: 4.6152
Profiling... [3072/50176]	Loss: 4.5697
Profiling... [3584/50176]	Loss: 4.5667
Profiling... [4096/50176]	Loss: 4.6027
Profiling... [4608/50176]	Loss: 4.5559
Profiling... [5120/50176]	Loss: 4.5575
Profiling... [5632/50176]	Loss: 4.5265
Profiling... [6144/50176]	Loss: 4.5370
Profiling... [6656/50176]	Loss: 4.5127
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.21008039489934,
                        "time": 4.796800779000023,
                        "accuracy": 0.009765625,
                        "total_cost": 58555.08546589783
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6234
Profiling... [1024/50176]	Loss: 4.6525
Profiling... [1536/50176]	Loss: 4.6587
Profiling... [2048/50176]	Loss: 4.6201
Profiling... [2560/50176]	Loss: 4.6312
Profiling... [3072/50176]	Loss: 4.5792
Profiling... [3584/50176]	Loss: 4.5916
Profiling... [4096/50176]	Loss: 4.5313
Profiling... [4608/50176]	Loss: 4.5695
Profiling... [5120/50176]	Loss: 4.5555
Profiling... [5632/50176]	Loss: 4.5729
Profiling... [6144/50176]	Loss: 4.5987
Profiling... [6656/50176]	Loss: 4.5482
Profile done
epoch 1 train time consumed: 7.81s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.30017153281845,
                        "time": 5.594911211999943,
                        "accuracy": 0.009765625,
                        "total_cost": 68349.3240117742
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6436
Profiling... [1024/50176]	Loss: 4.6439
Profiling... [1536/50176]	Loss: 4.6424
Profiling... [2048/50176]	Loss: 4.6306
Profiling... [2560/50176]	Loss: 4.6725
Profiling... [3072/50176]	Loss: 4.5847
Profiling... [3584/50176]	Loss: 4.5552
Profiling... [4096/50176]	Loss: 4.5837
Profiling... [4608/50176]	Loss: 4.5483
Profiling... [5120/50176]	Loss: 4.4745
Profiling... [5632/50176]	Loss: 4.5473
Profiling... [6144/50176]	Loss: 4.5695
Profiling... [6656/50176]	Loss: 4.5862
Profile done
epoch 1 train time consumed: 18.34s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 118.9104852926141,
                        "time": 13.888742289999982,
                        "accuracy": 0.009765625,
                        "total_cost": 169115.3495867341
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6330
Profiling... [1024/50176]	Loss: 5.4322
Profiling... [1536/50176]	Loss: 5.3073
Profiling... [2048/50176]	Loss: 4.9724
Profiling... [2560/50176]	Loss: 4.6864
Profiling... [3072/50176]	Loss: 4.7850
Profiling... [3584/50176]	Loss: 4.6656
Profiling... [4096/50176]	Loss: 4.7826
Profiling... [4608/50176]	Loss: 4.7529
Profiling... [5120/50176]	Loss: 4.6658
Profiling... [5632/50176]	Loss: 4.6860
Profiling... [6144/50176]	Loss: 4.6583
Profiling... [6656/50176]	Loss: 4.6313
Profile done
epoch 1 train time consumed: 6.66s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.96110718988471,
                        "time": 4.641093046999913,
                        "accuracy": 0.009765625,
                        "total_cost": 56536.01970610026
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6482
Profiling... [1024/50176]	Loss: 5.7230
Profiling... [1536/50176]	Loss: 5.3346
Profiling... [2048/50176]	Loss: 4.7249
Profiling... [2560/50176]	Loss: 4.8937
Profiling... [3072/50176]	Loss: 4.6821
Profiling... [3584/50176]	Loss: 4.7294
Profiling... [4096/50176]	Loss: 4.6001
Profiling... [4608/50176]	Loss: 4.5847
Profiling... [5120/50176]	Loss: 4.5808
Profiling... [5632/50176]	Loss: 4.7242
Profiling... [6144/50176]	Loss: 4.7218
Profiling... [6656/50176]	Loss: 4.6755
Profile done
epoch 1 train time consumed: 6.88s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.21516797409775,
                        "time": 4.8274520890001895,
                        "accuracy": 0.009765625,
                        "total_cost": 58931.76439573163
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6317
Profiling... [1024/50176]	Loss: 5.6207
Profiling... [1536/50176]	Loss: 5.1360
Profiling... [2048/50176]	Loss: 4.9847
Profiling... [2560/50176]	Loss: 4.8690
Profiling... [3072/50176]	Loss: 4.8577
Profiling... [3584/50176]	Loss: 4.7504
Profiling... [4096/50176]	Loss: 4.6942
Profiling... [4608/50176]	Loss: 4.6373
Profiling... [5120/50176]	Loss: 4.6953
Profiling... [5632/50176]	Loss: 4.6133
Profiling... [6144/50176]	Loss: 4.6032
Profiling... [6656/50176]	Loss: 4.9098
Profile done
epoch 1 train time consumed: 7.81s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.30039790468837,
                        "time": 5.595796586999995,
                        "accuracy": 0.009765625,
                        "total_cost": 68360.26976489436
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6233
Profiling... [1024/50176]	Loss: 5.6013
Profiling... [1536/50176]	Loss: 5.0237
Profiling... [2048/50176]	Loss: 4.8874
Profiling... [2560/50176]	Loss: 4.7403
Profiling... [3072/50176]	Loss: 4.7828
Profiling... [3584/50176]	Loss: 4.7587
Profiling... [4096/50176]	Loss: 4.7701
Profiling... [4608/50176]	Loss: 4.6843
Profiling... [5120/50176]	Loss: 4.6138
Profiling... [5632/50176]	Loss: 4.5974
Profiling... [6144/50176]	Loss: 4.6717
Profiling... [6656/50176]	Loss: 4.5671
Profile done
epoch 1 train time consumed: 18.48s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 118.93396890910911,
                        "time": 13.886601702999997,
                        "accuracy": 0.009765625,
                        "total_cost": 169122.678292253
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6423
Profiling... [1024/50176]	Loss: 5.5446
Profiling... [1536/50176]	Loss: 5.3937
Profiling... [2048/50176]	Loss: 5.0045
Profiling... [2560/50176]	Loss: 4.8216
Profiling... [3072/50176]	Loss: 4.7528
Profiling... [3584/50176]	Loss: 4.7085
Profiling... [4096/50176]	Loss: 4.6204
Profiling... [4608/50176]	Loss: 4.6604
Profiling... [5120/50176]	Loss: 4.6213
Profiling... [5632/50176]	Loss: 4.5671
Profiling... [6144/50176]	Loss: 4.6421
Profiling... [6656/50176]	Loss: 4.6921
Profile done
epoch 1 train time consumed: 6.71s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.99601413774187,
                        "time": 4.654143482000109,
                        "accuracy": 0.009765625,
                        "total_cost": 56711.63121491601
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6467
Profiling... [1024/50176]	Loss: 5.6597
Profiling... [1536/50176]	Loss: 5.4419
Profiling... [2048/50176]	Loss: 4.8461
Profiling... [2560/50176]	Loss: 4.8939
Profiling... [3072/50176]	Loss: 5.0096
Profiling... [3584/50176]	Loss: 4.8902
Profiling... [4096/50176]	Loss: 4.7404
Profiling... [4608/50176]	Loss: 4.7648
Profiling... [5120/50176]	Loss: 4.6413
Profiling... [5632/50176]	Loss: 4.4988
Profiling... [6144/50176]	Loss: 4.5060
Profiling... [6656/50176]	Loss: 4.4524
Profile done
epoch 1 train time consumed: 6.84s
Validation Epoch: 0, Average loss: 0.0095, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.24111103699043,
                        "time": 4.7942970410001635,
                        "accuracy": 0.009765625,
                        "total_cost": 58539.75611496604
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6252
Profiling... [1024/50176]	Loss: 5.4413
Profiling... [1536/50176]	Loss: 5.1599
Profiling... [2048/50176]	Loss: 4.9557
Profiling... [2560/50176]	Loss: 4.9557
Profiling... [3072/50176]	Loss: 4.8056
Profiling... [3584/50176]	Loss: 4.7987
Profiling... [4096/50176]	Loss: 4.6930
Profiling... [4608/50176]	Loss: 4.6769
Profiling... [5120/50176]	Loss: 4.6961
Profiling... [5632/50176]	Loss: 4.5729
Profiling... [6144/50176]	Loss: 4.5624
Profiling... [6656/50176]	Loss: 4.6070
Profile done
epoch 1 train time consumed: 7.83s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.30828933357921,
                        "time": 5.599786452999979,
                        "accuracy": 0.009765625,
                        "total_cost": 68413.53649569573
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6454
Profiling... [1024/50176]	Loss: 5.4595
Profiling... [1536/50176]	Loss: 5.2968
Profiling... [2048/50176]	Loss: 4.9772
Profiling... [2560/50176]	Loss: 4.7835
Profiling... [3072/50176]	Loss: 4.7719
Profiling... [3584/50176]	Loss: 4.8330
Profiling... [4096/50176]	Loss: 4.9215
Profiling... [4608/50176]	Loss: 4.7939
Profiling... [5120/50176]	Loss: 4.7433
Profiling... [5632/50176]	Loss: 4.7033
Profiling... [6144/50176]	Loss: 4.6437
Profiling... [6656/50176]	Loss: 4.5952
Profile done
epoch 1 train time consumed: 17.45s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 118.98911058106462,
                        "time": 13.048262328999954,
                        "accuracy": 0.009765625,
                        "total_cost": 158986.3556255862
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6586
Profiling... [1024/50176]	Loss: 5.5324
Profiling... [1536/50176]	Loss: 5.0994
Profiling... [2048/50176]	Loss: 4.8367
Profiling... [2560/50176]	Loss: 4.8585
Profiling... [3072/50176]	Loss: 4.9281
Profiling... [3584/50176]	Loss: 4.9405
Profiling... [4096/50176]	Loss: 4.6713
Profiling... [4608/50176]	Loss: 4.7230
Profiling... [5120/50176]	Loss: 4.6098
Profiling... [5632/50176]	Loss: 4.5786
Profiling... [6144/50176]	Loss: 4.7102
Profiling... [6656/50176]	Loss: 4.5384
Profile done
epoch 1 train time consumed: 6.66s
Validation Epoch: 0, Average loss: 0.0094, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.02873397439242,
                        "time": 4.635615133999863,
                        "accuracy": 0.009765625,
                        "total_cost": 56501.39142067581
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6311
Profiling... [1024/50176]	Loss: 5.6274
Profiling... [1536/50176]	Loss: 5.2274
Profiling... [2048/50176]	Loss: 4.7942
Profiling... [2560/50176]	Loss: 4.7621
Profiling... [3072/50176]	Loss: 4.6760
Profiling... [3584/50176]	Loss: 4.7584
Profiling... [4096/50176]	Loss: 4.7417
Profiling... [4608/50176]	Loss: 4.8841
Profiling... [5120/50176]	Loss: 4.5920
Profiling... [5632/50176]	Loss: 4.6447
Profiling... [6144/50176]	Loss: 4.5730
Profiling... [6656/50176]	Loss: 4.5885
Profile done
epoch 1 train time consumed: 6.93s
Validation Epoch: 0, Average loss: 0.0093, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.25727346323288,
                        "time": 4.796625378000044,
                        "accuracy": 0.009765625,
                        "total_cost": 58576.124355055006
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6277
Profiling... [1024/50176]	Loss: 5.6463
Profiling... [1536/50176]	Loss: 5.1229
Profiling... [2048/50176]	Loss: 4.8337
Profiling... [2560/50176]	Loss: 4.9077
Profiling... [3072/50176]	Loss: 4.6632
Profiling... [3584/50176]	Loss: 4.8046
Profiling... [4096/50176]	Loss: 4.6970
Profiling... [4608/50176]	Loss: 4.6273
Profiling... [5120/50176]	Loss: 4.6078
Profiling... [5632/50176]	Loss: 4.6435
Profiling... [6144/50176]	Loss: 4.6718
Profiling... [6656/50176]	Loss: 4.7241
Profile done
epoch 1 train time consumed: 7.93s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.33232430151602,
                        "time": 5.608971498999836,
                        "accuracy": 0.009765625,
                        "total_cost": 68539.55644586074
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6299
Profiling... [1024/50176]	Loss: 5.6471
Profiling... [1536/50176]	Loss: 5.2647
Profiling... [2048/50176]	Loss: 4.8727
Profiling... [2560/50176]	Loss: 4.7893
Profiling... [3072/50176]	Loss: 4.8778
Profiling... [3584/50176]	Loss: 4.5783
Profiling... [4096/50176]	Loss: 4.5981
Profiling... [4608/50176]	Loss: 4.6322
Profiling... [5120/50176]	Loss: 4.5934
Profiling... [5632/50176]	Loss: 4.5851
Profiling... [6144/50176]	Loss: 4.5945
Profiling... [6656/50176]	Loss: 4.6250
Profile done
epoch 1 train time consumed: 18.29s
Validation Epoch: 0, Average loss: 0.0092, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.01049124518359,
                        "time": 13.865425763999838,
                        "accuracy": 0.009765625,
                        "total_cost": 168973.42786531796
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6488
Profiling... [1024/50176]	Loss: 7.2830
Profiling... [1536/50176]	Loss: 6.2718
Profiling... [2048/50176]	Loss: 5.1635
Profiling... [2560/50176]	Loss: 5.0067
Profiling... [3072/50176]	Loss: 5.0939
Profiling... [3584/50176]	Loss: 4.6971
Profiling... [4096/50176]	Loss: 4.9259
Profiling... [4608/50176]	Loss: 4.7133
Profiling... [5120/50176]	Loss: 4.7030
Profiling... [5632/50176]	Loss: 4.9203
Profiling... [6144/50176]	Loss: 4.6934
Profiling... [6656/50176]	Loss: 4.6952
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.06004236046292,
                        "time": 4.6486754420000125,
                        "accuracy": 0.009765625,
                        "total_cost": 56675.48109256347
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6485
Profiling... [1024/50176]	Loss: 7.4335
Profiling... [1536/50176]	Loss: 5.2839
Profiling... [2048/50176]	Loss: 5.0903
Profiling... [2560/50176]	Loss: 4.8791
Profiling... [3072/50176]	Loss: 4.7290
Profiling... [3584/50176]	Loss: 4.8914
Profiling... [4096/50176]	Loss: 4.8897
Profiling... [4608/50176]	Loss: 4.7880
Profiling... [5120/50176]	Loss: 4.8194
Profiling... [5632/50176]	Loss: 4.9450
Profiling... [6144/50176]	Loss: 4.6757
Profiling... [6656/50176]	Loss: 4.6568
Profile done
epoch 1 train time consumed: 7.48s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.24344500183142,
                        "time": 4.801298335999945,
                        "accuracy": 0.00986328125,
                        "total_cost": 58045.93213502801
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6483
Profiling... [1024/50176]	Loss: 7.1579
Profiling... [1536/50176]	Loss: 5.6793
Profiling... [2048/50176]	Loss: 5.6836
Profiling... [2560/50176]	Loss: 5.3836
Profiling... [3072/50176]	Loss: 5.0078
Profiling... [3584/50176]	Loss: 4.8929
Profiling... [4096/50176]	Loss: 4.7923
Profiling... [4608/50176]	Loss: 6.4311
Profiling... [5120/50176]	Loss: 5.3234
Profiling... [5632/50176]	Loss: 4.6973
Profiling... [6144/50176]	Loss: 4.7142
Profiling... [6656/50176]	Loss: 4.6636
Profile done
epoch 1 train time consumed: 7.97s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.32991827849764,
                        "time": 5.580344466000042,
                        "accuracy": 0.009765625,
                        "total_cost": 68188.36982718992
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6358
Profiling... [1024/50176]	Loss: 7.1965
Profiling... [1536/50176]	Loss: 5.9772
Profiling... [2048/50176]	Loss: 5.0129
Profiling... [2560/50176]	Loss: 5.0276
Profiling... [3072/50176]	Loss: 4.6413
Profiling... [3584/50176]	Loss: 4.9063
Profiling... [4096/50176]	Loss: 4.8429
Profiling... [4608/50176]	Loss: 5.0549
Profiling... [5120/50176]	Loss: 4.9470
Profiling... [5632/50176]	Loss: 4.6332
Profiling... [6144/50176]	Loss: 4.6154
Profiling... [6656/50176]	Loss: 4.7384
Profile done
epoch 1 train time consumed: 18.42s
Validation Epoch: 0, Average loss: 0.0188, Accuracy: 0.0128
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.0278343008958,
                        "time": 13.88192125799992,
                        "accuracy": 0.01279296875,
                        "total_cost": 129159.62319342785
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6336
Profiling... [1024/50176]	Loss: 7.3657
Profiling... [1536/50176]	Loss: 6.2481
Profiling... [2048/50176]	Loss: 5.3808
Profiling... [2560/50176]	Loss: 4.9771
Profiling... [3072/50176]	Loss: 4.9089
Profiling... [3584/50176]	Loss: 4.7776
Profiling... [4096/50176]	Loss: 4.8321
Profiling... [4608/50176]	Loss: 4.6871
Profiling... [5120/50176]	Loss: 4.6846
Profiling... [5632/50176]	Loss: 4.6403
Profiling... [6144/50176]	Loss: 4.7033
Profiling... [6656/50176]	Loss: 4.6401
Profile done
epoch 1 train time consumed: 6.73s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.07105344843332,
                        "time": 4.634261839000146,
                        "accuracy": 0.009765625,
                        "total_cost": 56504.97936646362
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6429
Profiling... [1024/50176]	Loss: 7.3621
Profiling... [1536/50176]	Loss: 6.1217
Profiling... [2048/50176]	Loss: 5.0946
Profiling... [2560/50176]	Loss: 5.0053
Profiling... [3072/50176]	Loss: 4.7239
Profiling... [3584/50176]	Loss: 4.8342
Profiling... [4096/50176]	Loss: 4.9371
Profiling... [4608/50176]	Loss: 4.6448
Profiling... [5120/50176]	Loss: 4.8757
Profiling... [5632/50176]	Loss: 4.5852
Profiling... [6144/50176]	Loss: 4.9236
Profiling... [6656/50176]	Loss: 4.6537
Profile done
epoch 1 train time consumed: 7.01s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.26498292647638,
                        "time": 4.786493798000038,
                        "accuracy": 0.00986328125,
                        "total_cost": 57877.40272499678
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6560
Profiling... [1024/50176]	Loss: 7.4365
Profiling... [1536/50176]	Loss: 6.0969
Profiling... [2048/50176]	Loss: 5.0595
Profiling... [2560/50176]	Loss: 5.0241
Profiling... [3072/50176]	Loss: 4.8367
Profiling... [3584/50176]	Loss: 4.8718
Profiling... [4096/50176]	Loss: 4.9843
Profiling... [4608/50176]	Loss: 4.8883
Profiling... [5120/50176]	Loss: 4.7061
Profiling... [5632/50176]	Loss: 4.7985
Profiling... [6144/50176]	Loss: 4.8303
Profiling... [6656/50176]	Loss: 4.7932
Profile done
epoch 1 train time consumed: 7.90s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.32421248291334,
                        "time": 5.588017712999999,
                        "accuracy": 0.009765625,
                        "total_cost": 68278.86724549584
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6206
Profiling... [1024/50176]	Loss: 7.4108
Profiling... [1536/50176]	Loss: 5.8448
Profiling... [2048/50176]	Loss: 5.2930
Profiling... [2560/50176]	Loss: 5.0791
Profiling... [3072/50176]	Loss: 4.9477
Profiling... [3584/50176]	Loss: 4.7063
Profiling... [4096/50176]	Loss: 4.6867
Profiling... [4608/50176]	Loss: 4.7380
Profiling... [5120/50176]	Loss: 4.6936
Profiling... [5632/50176]	Loss: 4.7046
Profiling... [6144/50176]	Loss: 4.6652
Profiling... [6656/50176]	Loss: 4.8617
Profile done
epoch 1 train time consumed: 17.50s
Validation Epoch: 0, Average loss: 0.0091, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.05869536507949,
                        "time": 13.066115486999934,
                        "accuracy": 0.009765625,
                        "total_cost": 159296.98952925924
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 4.6182
Profiling... [1024/50176]	Loss: 7.0920
Profiling... [1536/50176]	Loss: 5.9418
Profiling... [2048/50176]	Loss: 5.0512
Profiling... [2560/50176]	Loss: 5.1096
Profiling... [3072/50176]	Loss: 4.9602
Profiling... [3584/50176]	Loss: 5.1076
Profiling... [4096/50176]	Loss: 4.7670
Profiling... [4608/50176]	Loss: 5.0918
Profiling... [5120/50176]	Loss: 4.8172
Profiling... [5632/50176]	Loss: 4.7122
Profiling... [6144/50176]	Loss: 4.6566
Profiling... [6656/50176]	Loss: 4.7669
Profile done
epoch 1 train time consumed: 6.73s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0099
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.08477768347325,
                        "time": 4.653445766999994,
                        "accuracy": 0.00986328125,
                        "total_cost": 56183.59048874267
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 4.6317
Profiling... [1024/50176]	Loss: 7.1572
Profiling... [1536/50176]	Loss: 6.2218
Profiling... [2048/50176]	Loss: 5.3375
Profiling... [2560/50176]	Loss: 5.1248
Profiling... [3072/50176]	Loss: 4.8341
Profiling... [3584/50176]	Loss: 4.6521
Profiling... [4096/50176]	Loss: 4.8619
Profiling... [4608/50176]	Loss: 4.9103
Profiling... [5120/50176]	Loss: 4.7152
Profiling... [5632/50176]	Loss: 4.6479
Profiling... [6144/50176]	Loss: 4.7342
Profiling... [6656/50176]	Loss: 4.7261
Profile done
epoch 1 train time consumed: 6.82s
Validation Epoch: 0, Average loss: 0.0152, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.28153595024604,
                        "time": 4.806209018000118,
                        "accuracy": 0.009765625,
                        "total_cost": 58705.10016153372
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 4.6267
Profiling... [1024/50176]	Loss: 7.1520
Profiling... [1536/50176]	Loss: 5.4941
Profiling... [2048/50176]	Loss: 4.8891
Profiling... [2560/50176]	Loss: 4.9743
Profiling... [3072/50176]	Loss: 4.8492
Profiling... [3584/50176]	Loss: 4.9405
Profiling... [4096/50176]	Loss: 5.1897
Profiling... [4608/50176]	Loss: 4.8627
Profiling... [5120/50176]	Loss: 4.7064
Profiling... [5632/50176]	Loss: 5.1035
Profiling... [6144/50176]	Loss: 4.6903
Profiling... [6656/50176]	Loss: 5.0260
Profile done
epoch 1 train time consumed: 7.85s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.33654177831016,
                        "time": 5.6006558680001035,
                        "accuracy": 0.009765625,
                        "total_cost": 68440.36126489931
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 4.6469
Profiling... [1024/50176]	Loss: 7.0025
Profiling... [1536/50176]	Loss: 5.3401
Profiling... [2048/50176]	Loss: 5.3955
Profiling... [2560/50176]	Loss: 5.1421
Profiling... [3072/50176]	Loss: 4.8284
Profiling... [3584/50176]	Loss: 4.9254
Profiling... [4096/50176]	Loss: 4.8052
Profiling... [4608/50176]	Loss: 4.8213
Profiling... [5120/50176]	Loss: 4.8478
Profiling... [5632/50176]	Loss: 4.8106
Profiling... [6144/50176]	Loss: 4.8069
Profiling... [6656/50176]	Loss: 4.8374
Profile done
epoch 1 train time consumed: 18.29s
Validation Epoch: 0, Average loss: 0.0090, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.0611374654713,
                        "time": 13.858473168000046,
                        "accuracy": 0.009765625,
                        "total_cost": 168960.57128108022
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6440
Profiling... [2048/50176]	Loss: 4.6334
Profiling... [3072/50176]	Loss: 4.6167
Profiling... [4096/50176]	Loss: 4.6080
Profiling... [5120/50176]	Loss: 4.5867
Profiling... [6144/50176]	Loss: 4.5451
Profiling... [7168/50176]	Loss: 4.5296
Profiling... [8192/50176]	Loss: 4.5337
Profiling... [9216/50176]	Loss: 4.5190
Profiling... [10240/50176]	Loss: 4.5137
Profiling... [11264/50176]	Loss: 4.4880
Profiling... [12288/50176]	Loss: 4.4906
Profiling... [13312/50176]	Loss: 4.4688
Profile done
epoch 1 train time consumed: 12.90s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.19374636553309,
                        "time": 9.082695094999963,
                        "accuracy": 0.009765625,
                        "total_cost": 110858.28664001505
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6399
Profiling... [2048/50176]	Loss: 4.6230
Profiling... [3072/50176]	Loss: 4.6171
Profiling... [4096/50176]	Loss: 4.6116
Profiling... [5120/50176]	Loss: 4.5933
Profiling... [6144/50176]	Loss: 4.5654
Profiling... [7168/50176]	Loss: 4.5260
Profiling... [8192/50176]	Loss: 4.5242
Profiling... [9216/50176]	Loss: 4.5250
Profiling... [10240/50176]	Loss: 4.4970
Profiling... [11264/50176]	Loss: 4.4865
Profiling... [12288/50176]	Loss: 4.4919
Profiling... [13312/50176]	Loss: 4.4424
Profile done
epoch 1 train time consumed: 13.32s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.47427001413179,
                        "time": 9.443217012000105,
                        "accuracy": 0.009765625,
                        "total_cost": 115529.87741119931
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6386
Profiling... [2048/50176]	Loss: 4.6221
Profiling... [3072/50176]	Loss: 4.6150
Profiling... [4096/50176]	Loss: 4.6030
Profiling... [5120/50176]	Loss: 4.5450
Profiling... [6144/50176]	Loss: 4.5412
Profiling... [7168/50176]	Loss: 4.5363
Profiling... [8192/50176]	Loss: 4.4739
Profiling... [9216/50176]	Loss: 4.5107
Profiling... [10240/50176]	Loss: 4.4929
Profiling... [11264/50176]	Loss: 4.4828
Profiling... [12288/50176]	Loss: 4.4854
Profiling... [13312/50176]	Loss: 4.4827
Profile done
epoch 1 train time consumed: 15.42s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 119.52685131438632,
                        "time": 11.112686162000045,
                        "accuracy": 0.009765625,
                        "total_cost": 136014.27318669495
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6308
Profiling... [2048/50176]	Loss: 4.6267
Profiling... [3072/50176]	Loss: 4.6101
Profiling... [4096/50176]	Loss: 4.5726
Profiling... [5120/50176]	Loss: 4.5793
Profiling... [6144/50176]	Loss: 4.5455
Profiling... [7168/50176]	Loss: 4.5502
Profiling... [8192/50176]	Loss: 4.5454
Profiling... [9216/50176]	Loss: 4.5163
Profiling... [10240/50176]	Loss: 4.5089
Profiling... [11264/50176]	Loss: 4.4949
Profiling... [12288/50176]	Loss: 4.4854
Profiling... [13312/50176]	Loss: 4.5144
Profile done
epoch 1 train time consumed: 38.82s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 118.99486394766201,
                        "time": 29.157335065000098,
                        "accuracy": 0.009765625,
                        "total_cost": 355284.2873995347
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6334
Profiling... [2048/50176]	Loss: 4.6457
Profiling... [3072/50176]	Loss: 4.6017
Profiling... [4096/50176]	Loss: 4.6240
Profiling... [5120/50176]	Loss: 4.5533
Profiling... [6144/50176]	Loss: 4.5534
Profiling... [7168/50176]	Loss: 4.5364
Profiling... [8192/50176]	Loss: 4.5282
Profiling... [9216/50176]	Loss: 4.5326
Profiling... [10240/50176]	Loss: 4.5371
Profiling... [11264/50176]	Loss: 4.4925
Profiling... [12288/50176]	Loss: 4.4767
Profiling... [13312/50176]	Loss: 4.4762
Profile done
epoch 1 train time consumed: 12.89s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.18735590576529,
                        "time": 9.085169653999856,
                        "accuracy": 0.009765625,
                        "total_cost": 110882.54453919122
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6335
Profiling... [2048/50176]	Loss: 4.6328
Profiling... [3072/50176]	Loss: 4.6007
Profiling... [4096/50176]	Loss: 4.5679
Profiling... [5120/50176]	Loss: 4.5742
Profiling... [6144/50176]	Loss: 4.5949
Profiling... [7168/50176]	Loss: 4.5516
Profiling... [8192/50176]	Loss: 4.5561
Profiling... [9216/50176]	Loss: 4.5371
Profiling... [10240/50176]	Loss: 4.5183
Profiling... [11264/50176]	Loss: 4.4860
Profiling... [12288/50176]	Loss: 4.4879
Profiling... [13312/50176]	Loss: 4.4964
Profile done
epoch 1 train time consumed: 13.31s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.44359462377714,
                        "time": 9.440520137000021,
                        "accuracy": 0.009765625,
                        "total_cost": 115467.22921281902
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6275
Profiling... [2048/50176]	Loss: 4.6397
Profiling... [3072/50176]	Loss: 4.6093
Profiling... [4096/50176]	Loss: 4.5915
Profiling... [5120/50176]	Loss: 4.5857
Profiling... [6144/50176]	Loss: 4.5420
Profiling... [7168/50176]	Loss: 4.5449
Profiling... [8192/50176]	Loss: 4.5502
Profiling... [9216/50176]	Loss: 4.5334
Profiling... [10240/50176]	Loss: 4.5461
Profiling... [11264/50176]	Loss: 4.5241
Profiling... [12288/50176]	Loss: 4.4697
Profiling... [13312/50176]	Loss: 4.4760
Profile done
epoch 1 train time consumed: 15.45s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.49747586089335,
                        "time": 11.094295503000012,
                        "accuracy": 0.009765625,
                        "total_cost": 135755.80764808823
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6476
Profiling... [2048/50176]	Loss: 4.6145
Profiling... [3072/50176]	Loss: 4.6272
Profiling... [4096/50176]	Loss: 4.5818
Profiling... [5120/50176]	Loss: 4.5771
Profiling... [6144/50176]	Loss: 4.5663
Profiling... [7168/50176]	Loss: 4.5217
Profiling... [8192/50176]	Loss: 4.5194
Profiling... [9216/50176]	Loss: 4.4985
Profiling... [10240/50176]	Loss: 4.4936
Profiling... [11264/50176]	Loss: 4.5194
Profiling... [12288/50176]	Loss: 4.4666
Profiling... [13312/50176]	Loss: 4.4827
Profile done
epoch 1 train time consumed: 37.98s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 119.03298471211986,
                        "time": 28.416804619999994,
                        "accuracy": 0.009765625,
                        "total_cost": 346371.79595773504
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6355
Profiling... [2048/50176]	Loss: 4.6185
Profiling... [3072/50176]	Loss: 4.6411
Profiling... [4096/50176]	Loss: 4.5977
Profiling... [5120/50176]	Loss: 4.6161
Profiling... [6144/50176]	Loss: 4.5594
Profiling... [7168/50176]	Loss: 4.5257
Profiling... [8192/50176]	Loss: 4.5369
Profiling... [9216/50176]	Loss: 4.5222
Profiling... [10240/50176]	Loss: 4.5060
Profiling... [11264/50176]	Loss: 4.4878
Profiling... [12288/50176]	Loss: 4.4748
Profiling... [13312/50176]	Loss: 4.4637
Profile done
epoch 1 train time consumed: 12.86s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.20750236546623,
                        "time": 9.106453639999927,
                        "accuracy": 0.009765625,
                        "total_cost": 111161.0976083251
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6445
Profiling... [2048/50176]	Loss: 4.6065
Profiling... [3072/50176]	Loss: 4.6346
Profiling... [4096/50176]	Loss: 4.6049
Profiling... [5120/50176]	Loss: 4.5761
Profiling... [6144/50176]	Loss: 4.5814
Profiling... [7168/50176]	Loss: 4.5342
Profiling... [8192/50176]	Loss: 4.5198
Profiling... [9216/50176]	Loss: 4.5131
Profiling... [10240/50176]	Loss: 4.5376
Profiling... [11264/50176]	Loss: 4.4513
Profiling... [12288/50176]	Loss: 4.4569
Profiling... [13312/50176]	Loss: 4.4787
Profile done
epoch 1 train time consumed: 13.30s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.44400734113057,
                        "time": 9.441895116000069,
                        "accuracy": 0.009765625,
                        "total_cost": 115484.44564988899
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6336
Profiling... [2048/50176]	Loss: 4.6244
Profiling... [3072/50176]	Loss: 4.6154
Profiling... [4096/50176]	Loss: 4.6101
Profiling... [5120/50176]	Loss: 4.5801
Profiling... [6144/50176]	Loss: 4.5716
Profiling... [7168/50176]	Loss: 4.5102
Profiling... [8192/50176]	Loss: 4.5279
Profiling... [9216/50176]	Loss: 4.4641
Profiling... [10240/50176]	Loss: 4.5044
Profiling... [11264/50176]	Loss: 4.4956
Profiling... [12288/50176]	Loss: 4.4670
Profiling... [13312/50176]	Loss: 4.4278
Profile done
epoch 1 train time consumed: 15.44s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.48382956019456,
                        "time": 11.040823843999988,
                        "accuracy": 0.009765625,
                        "total_cost": 135086.07123257619
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6274
Profiling... [2048/50176]	Loss: 4.6294
Profiling... [3072/50176]	Loss: 4.6092
Profiling... [4096/50176]	Loss: 4.6204
Profiling... [5120/50176]	Loss: 4.5870
Profiling... [6144/50176]	Loss: 4.5591
Profiling... [7168/50176]	Loss: 4.5368
Profiling... [8192/50176]	Loss: 4.5382
Profiling... [9216/50176]	Loss: 4.5389
Profiling... [10240/50176]	Loss: 4.4939
Profiling... [11264/50176]	Loss: 4.4939
Profiling... [12288/50176]	Loss: 4.4878
Profiling... [13312/50176]	Loss: 4.5110
Profile done
epoch 1 train time consumed: 37.98s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 119.06875822589916,
                        "time": 28.467766113000152,
                        "accuracy": 0.009765625,
                        "total_cost": 347097.24779932265
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6369
Profiling... [2048/50176]	Loss: 5.5303
Profiling... [3072/50176]	Loss: 5.0817
Profiling... [4096/50176]	Loss: 4.7165
Profiling... [5120/50176]	Loss: 4.7886
Profiling... [6144/50176]	Loss: 4.6704
Profiling... [7168/50176]	Loss: 4.6393
Profiling... [8192/50176]	Loss: 4.6554
Profiling... [9216/50176]	Loss: 4.5574
Profiling... [10240/50176]	Loss: 4.6755
Profiling... [11264/50176]	Loss: 4.5354
Profiling... [12288/50176]	Loss: 4.5564
Profiling... [13312/50176]	Loss: 4.6020
Profile done
epoch 1 train time consumed: 12.86s
Validation Epoch: 0, Average loss: 0.0048, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.22092664985144,
                        "time": 9.083501723999916,
                        "accuracy": 0.009765625,
                        "total_cost": 110893.41365870522
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6407
Profiling... [2048/50176]	Loss: 5.4656
Profiling... [3072/50176]	Loss: 5.0786
Profiling... [4096/50176]	Loss: 4.8326
Profiling... [5120/50176]	Loss: 4.6846
Profiling... [6144/50176]	Loss: 4.7904
Profiling... [7168/50176]	Loss: 4.7215
Profiling... [8192/50176]	Loss: 4.6931
Profiling... [9216/50176]	Loss: 4.6619
Profiling... [10240/50176]	Loss: 4.6247
Profiling... [11264/50176]	Loss: 4.5962
Profiling... [12288/50176]	Loss: 4.5771
Profiling... [13312/50176]	Loss: 4.6002
Profile done
epoch 1 train time consumed: 13.29s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.44709389852946,
                        "time": 9.435064140999884,
                        "accuracy": 0.009765625,
                        "total_cost": 115403.87762059893
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6256
Profiling... [2048/50176]	Loss: 5.2770
Profiling... [3072/50176]	Loss: 4.9482
Profiling... [4096/50176]	Loss: 4.8737
Profiling... [5120/50176]	Loss: 4.7625
Profiling... [6144/50176]	Loss: 4.7077
Profiling... [7168/50176]	Loss: 4.7031
Profiling... [8192/50176]	Loss: 4.7243
Profiling... [9216/50176]	Loss: 4.7239
Profiling... [10240/50176]	Loss: 4.6498
Profiling... [11264/50176]	Loss: 4.6683
Profiling... [12288/50176]	Loss: 4.5444
Profiling... [13312/50176]	Loss: 4.5340
Profile done
epoch 1 train time consumed: 15.46s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.49513493157633,
                        "time": 11.085655060000136,
                        "accuracy": 0.009765625,
                        "total_cost": 135647.4211532419
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6326
Profiling... [2048/50176]	Loss: 5.4480
Profiling... [3072/50176]	Loss: 4.9425
Profiling... [4096/50176]	Loss: 4.8299
Profiling... [5120/50176]	Loss: 4.7512
Profiling... [6144/50176]	Loss: 4.7307
Profiling... [7168/50176]	Loss: 4.6310
Profiling... [8192/50176]	Loss: 4.5922
Profiling... [9216/50176]	Loss: 4.6456
Profiling... [10240/50176]	Loss: 4.6146
Profiling... [11264/50176]	Loss: 4.7096
Profiling... [12288/50176]	Loss: 4.5484
Profiling... [13312/50176]	Loss: 4.5948
Profile done
epoch 1 train time consumed: 38.80s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 119.0496017859391,
                        "time": 29.239678718999812,
                        "accuracy": 0.009765625,
                        "total_cost": 356451.5438434023
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6455
Profiling... [2048/50176]	Loss: 5.6775
Profiling... [3072/50176]	Loss: 5.1317
Profiling... [4096/50176]	Loss: 4.7887
Profiling... [5120/50176]	Loss: 4.7765
Profiling... [6144/50176]	Loss: 4.6711
Profiling... [7168/50176]	Loss: 4.6238
Profiling... [8192/50176]	Loss: 4.6022
Profiling... [9216/50176]	Loss: 4.7218
Profiling... [10240/50176]	Loss: 4.7766
Profiling... [11264/50176]	Loss: 4.6670
Profiling... [12288/50176]	Loss: 4.6284
Profiling... [13312/50176]	Loss: 4.5510
Profile done
epoch 1 train time consumed: 12.96s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.20236276859663,
                        "time": 9.069283966000057,
                        "accuracy": 0.009765625,
                        "total_cost": 110702.59992233528
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6310
Profiling... [2048/50176]	Loss: 5.5603
Profiling... [3072/50176]	Loss: 5.0595
Profiling... [4096/50176]	Loss: 4.9032
Profiling... [5120/50176]	Loss: 4.9238
Profiling... [6144/50176]	Loss: 4.7584
Profiling... [7168/50176]	Loss: 4.7404
Profiling... [8192/50176]	Loss: 4.6267
Profiling... [9216/50176]	Loss: 4.6846
Profiling... [10240/50176]	Loss: 4.6513
Profiling... [11264/50176]	Loss: 4.5890
Profiling... [12288/50176]	Loss: 4.5662
Profiling... [13312/50176]	Loss: 4.5247
Profile done
epoch 1 train time consumed: 13.30s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0102
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.3944108536982,
                        "time": 9.407773701999986,
                        "accuracy": 0.01015625,
                        "total_cost": 110595.50509245081
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6366
Profiling... [2048/50176]	Loss: 5.4952
Profiling... [3072/50176]	Loss: 4.8324
Profiling... [4096/50176]	Loss: 4.8203
Profiling... [5120/50176]	Loss: 4.7314
Profiling... [6144/50176]	Loss: 4.7443
Profiling... [7168/50176]	Loss: 4.7193
Profiling... [8192/50176]	Loss: 4.6215
Profiling... [9216/50176]	Loss: 4.5890
Profiling... [10240/50176]	Loss: 4.7492
Profiling... [11264/50176]	Loss: 4.6065
Profiling... [12288/50176]	Loss: 4.5525
Profiling... [13312/50176]	Loss: 4.7964
Profile done
epoch 1 train time consumed: 15.28s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.43854174168628,
                        "time": 11.02665783800012,
                        "accuracy": 0.009765625,
                        "total_cost": 134861.61228341958
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6291
Profiling... [2048/50176]	Loss: 5.2517
Profiling... [3072/50176]	Loss: 5.0138
Profiling... [4096/50176]	Loss: 4.8002
Profiling... [5120/50176]	Loss: 4.6253
Profiling... [6144/50176]	Loss: 4.6748
Profiling... [7168/50176]	Loss: 4.6781
Profiling... [8192/50176]	Loss: 4.7045
Profiling... [9216/50176]	Loss: 4.6698
Profiling... [10240/50176]	Loss: 4.6187
Profiling... [11264/50176]	Loss: 4.5584
Profiling... [12288/50176]	Loss: 4.5472
Profiling... [13312/50176]	Loss: 4.5134
Profile done
epoch 1 train time consumed: 38.43s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 119.04229253570604,
                        "time": 28.738909085999694,
                        "accuracy": 0.009765625,
                        "total_cost": 350325.31175143796
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6333
Profiling... [2048/50176]	Loss: 5.5020
Profiling... [3072/50176]	Loss: 5.1638
Profiling... [4096/50176]	Loss: 4.8330
Profiling... [5120/50176]	Loss: 4.7702
Profiling... [6144/50176]	Loss: 4.6526
Profiling... [7168/50176]	Loss: 4.5957
Profiling... [8192/50176]	Loss: 4.5633
Profiling... [9216/50176]	Loss: 4.6025
Profiling... [10240/50176]	Loss: 4.5111
Profiling... [11264/50176]	Loss: 4.5555
Profiling... [12288/50176]	Loss: 4.5157
Profiling... [13312/50176]	Loss: 4.4845
Profile done
epoch 1 train time consumed: 12.94s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.1799699398153,
                        "time": 9.070391616000052,
                        "accuracy": 0.009765625,
                        "total_cost": 110695.32161405326
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6293
Profiling... [2048/50176]	Loss: 5.4573
Profiling... [3072/50176]	Loss: 4.9531
Profiling... [4096/50176]	Loss: 4.7405
Profiling... [5120/50176]	Loss: 4.7309
Profiling... [6144/50176]	Loss: 4.7693
Profiling... [7168/50176]	Loss: 4.6959
Profiling... [8192/50176]	Loss: 4.6871
Profiling... [9216/50176]	Loss: 4.5807
Profiling... [10240/50176]	Loss: 4.5244
Profiling... [11264/50176]	Loss: 4.5746
Profiling... [12288/50176]	Loss: 4.5190
Profiling... [13312/50176]	Loss: 4.4616
Profile done
epoch 1 train time consumed: 13.39s
Validation Epoch: 0, Average loss: 0.0047, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.38016977096234,
                        "time": 9.418109143000038,
                        "accuracy": 0.009765625,
                        "total_cost": 115131.95196547042
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6463
Profiling... [2048/50176]	Loss: 5.4931
Profiling... [3072/50176]	Loss: 5.0388
Profiling... [4096/50176]	Loss: 4.6473
Profiling... [5120/50176]	Loss: 4.7455
Profiling... [6144/50176]	Loss: 4.8009
Profiling... [7168/50176]	Loss: 4.7778
Profiling... [8192/50176]	Loss: 4.6757
Profiling... [9216/50176]	Loss: 4.7714
Profiling... [10240/50176]	Loss: 4.6508
Profiling... [11264/50176]	Loss: 4.6343
Profiling... [12288/50176]	Loss: 4.5565
Profiling... [13312/50176]	Loss: 4.5304
Profile done
epoch 1 train time consumed: 15.33s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.41889715383775,
                        "time": 11.036794197999825,
                        "accuracy": 0.009765625,
                        "total_cost": 134963.38342287505
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6454
Profiling... [2048/50176]	Loss: 5.5302
Profiling... [3072/50176]	Loss: 5.1685
Profiling... [4096/50176]	Loss: 4.8374
Profiling... [5120/50176]	Loss: 4.7699
Profiling... [6144/50176]	Loss: 4.7582
Profiling... [7168/50176]	Loss: 4.7367
Profiling... [8192/50176]	Loss: 4.6502
Profiling... [9216/50176]	Loss: 4.6043
Profiling... [10240/50176]	Loss: 4.6259
Profiling... [11264/50176]	Loss: 4.5443
Profiling... [12288/50176]	Loss: 4.6130
Profiling... [13312/50176]	Loss: 4.6080
Profile done
epoch 1 train time consumed: 38.94s
Validation Epoch: 0, Average loss: 0.0046, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 119.02782068054299,
                        "time": 29.132817473999694,
                        "accuracy": 0.009765625,
                        "total_cost": 355083.8552795366
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6371
Profiling... [2048/50176]	Loss: 7.1366
Profiling... [3072/50176]	Loss: 5.6525
Profiling... [4096/50176]	Loss: 5.3404
Profiling... [5120/50176]	Loss: 4.7362
Profiling... [6144/50176]	Loss: 4.7396
Profiling... [7168/50176]	Loss: 5.1741
Profiling... [8192/50176]	Loss: 4.8529
Profiling... [9216/50176]	Loss: 5.0794
Profiling... [10240/50176]	Loss: 4.7798
Profiling... [11264/50176]	Loss: 4.7772
Profiling... [12288/50176]	Loss: 4.7261
Profiling... [13312/50176]	Loss: 4.7226
Profile done
epoch 1 train time consumed: 12.88s
Validation Epoch: 0, Average loss: 0.0110, Accuracy: 0.0101
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.16936729589607,
                        "time": 9.075355947999924,
                        "accuracy": 0.01005859375,
                        "total_cost": 107520.44005238783
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6491
Profiling... [2048/50176]	Loss: 6.9009
Profiling... [3072/50176]	Loss: 5.5675
Profiling... [4096/50176]	Loss: 4.9573
Profiling... [5120/50176]	Loss: 5.0047
Profiling... [6144/50176]	Loss: 4.7289
Profiling... [7168/50176]	Loss: 5.2367
Profiling... [8192/50176]	Loss: 4.7162
Profiling... [9216/50176]	Loss: 4.6714
Profiling... [10240/50176]	Loss: 4.6893
Profiling... [11264/50176]	Loss: 4.6563
Profiling... [12288/50176]	Loss: 4.6262
Profiling... [13312/50176]	Loss: 4.6998
Profile done
epoch 1 train time consumed: 13.39s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0101
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.36127646932557,
                        "time": 9.457099447000019,
                        "accuracy": 0.01005859375,
                        "total_cost": 112223.58609435591
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6395
Profiling... [2048/50176]	Loss: 7.1141
Profiling... [3072/50176]	Loss: 5.6578
Profiling... [4096/50176]	Loss: 4.9582
Profiling... [5120/50176]	Loss: 4.8676
Profiling... [6144/50176]	Loss: 4.8413
Profiling... [7168/50176]	Loss: 4.7651
Profiling... [8192/50176]	Loss: 4.6212
Profiling... [9216/50176]	Loss: 5.0301
Profiling... [10240/50176]	Loss: 4.7618
Profiling... [11264/50176]	Loss: 4.9412
Profiling... [12288/50176]	Loss: 4.6589
Profiling... [13312/50176]	Loss: 4.7890
Profile done
epoch 1 train time consumed: 15.36s
Validation Epoch: 0, Average loss: 0.0049, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.40244667180791,
                        "time": 11.026591743999688,
                        "accuracy": 0.009765625,
                        "total_cost": 134820.04814691533
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6466
Profiling... [2048/50176]	Loss: 7.2772
Profiling... [3072/50176]	Loss: 5.6622
Profiling... [4096/50176]	Loss: 5.3737
Profiling... [5120/50176]	Loss: 4.9420
Profiling... [6144/50176]	Loss: 4.7291
Profiling... [7168/50176]	Loss: 4.6196
Profiling... [8192/50176]	Loss: 5.2286
Profiling... [9216/50176]	Loss: 4.7387
Profiling... [10240/50176]	Loss: 4.9298
Profiling... [11264/50176]	Loss: 4.7570
Profiling... [12288/50176]	Loss: 4.6803
Profiling... [13312/50176]	Loss: 4.6333
Profile done
epoch 1 train time consumed: 38.27s
Validation Epoch: 0, Average loss: 0.0098, Accuracy: 0.0127
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 119.04601651101471,
                        "time": 28.695665158999873,
                        "accuracy": 0.0126953125,
                        "total_cost": 269083.93379941204
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6469
Profiling... [2048/50176]	Loss: 7.3380
Profiling... [3072/50176]	Loss: 5.8432
Profiling... [4096/50176]	Loss: 5.1728
Profiling... [5120/50176]	Loss: 4.9513
Profiling... [6144/50176]	Loss: 4.8463
Profiling... [7168/50176]	Loss: 4.9034
Profiling... [8192/50176]	Loss: 4.6967
Profiling... [9216/50176]	Loss: 4.8191
Profiling... [10240/50176]	Loss: 4.7562
Profiling... [11264/50176]	Loss: 4.6512
Profiling... [12288/50176]	Loss: 4.6342
Profiling... [13312/50176]	Loss: 4.6037
Profile done
epoch 1 train time consumed: 12.98s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.19002523869747,
                        "time": 9.036933644999863,
                        "accuracy": 0.009765625,
                        "total_cost": 110296.30456094393
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6412
Profiling... [2048/50176]	Loss: 7.0358
Profiling... [3072/50176]	Loss: 5.5260
Profiling... [4096/50176]	Loss: 5.0507
Profiling... [5120/50176]	Loss: 4.9564
Profiling... [6144/50176]	Loss: 4.7740
Profiling... [7168/50176]	Loss: 4.7693
Profiling... [8192/50176]	Loss: 4.8632
Profiling... [9216/50176]	Loss: 4.6773
Profiling... [10240/50176]	Loss: 4.7977
Profiling... [11264/50176]	Loss: 4.7164
Profiling... [12288/50176]	Loss: 4.7223
Profiling... [13312/50176]	Loss: 4.7152
Profile done
epoch 1 train time consumed: 13.49s
Validation Epoch: 0, Average loss: 0.0107, Accuracy: 0.0115
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.37098739272226,
                        "time": 9.524090404000162,
                        "accuracy": 0.0115234375,
                        "total_cost": 98659.8031657698
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6356
Profiling... [2048/50176]	Loss: 7.1554
Profiling... [3072/50176]	Loss: 5.4032
Profiling... [4096/50176]	Loss: 5.3030
Profiling... [5120/50176]	Loss: 4.9255
Profiling... [6144/50176]	Loss: 4.8908
Profiling... [7168/50176]	Loss: 4.7156
Profiling... [8192/50176]	Loss: 4.6811
Profiling... [9216/50176]	Loss: 5.1103
Profiling... [10240/50176]	Loss: 5.3114
Profiling... [11264/50176]	Loss: 5.1355
Profiling... [12288/50176]	Loss: 4.8451
Profiling... [13312/50176]	Loss: 4.8628
Profile done
epoch 1 train time consumed: 15.46s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0087
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.4101471406624,
                        "time": 11.103108727999825,
                        "accuracy": 0.00869140625,
                        "total_cost": 152544.22688264423
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6503
Profiling... [2048/50176]	Loss: 7.3293
Profiling... [3072/50176]	Loss: 5.4749
Profiling... [4096/50176]	Loss: 4.9765
Profiling... [5120/50176]	Loss: 4.7364
Profiling... [6144/50176]	Loss: 4.6257
Profiling... [7168/50176]	Loss: 4.6569
Profiling... [8192/50176]	Loss: 4.7295
Profiling... [9216/50176]	Loss: 4.6968
Profiling... [10240/50176]	Loss: 4.5888
Profiling... [11264/50176]	Loss: 4.8803
Profiling... [12288/50176]	Loss: 4.6312
Profiling... [13312/50176]	Loss: 4.8712
Profile done
epoch 1 train time consumed: 39.00s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 119.04772682653397,
                        "time": 29.302603063999868,
                        "accuracy": 0.009765625,
                        "total_cost": 357213.008370628
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 4.6399
Profiling... [2048/50176]	Loss: 7.2661
Profiling... [3072/50176]	Loss: 6.0268
Profiling... [4096/50176]	Loss: 5.1769
Profiling... [5120/50176]	Loss: 4.8876
Profiling... [6144/50176]	Loss: 4.7491
Profiling... [7168/50176]	Loss: 4.7800
Profiling... [8192/50176]	Loss: 4.7630
Profiling... [9216/50176]	Loss: 4.6687
Profiling... [10240/50176]	Loss: 4.7266
Profiling... [11264/50176]	Loss: 4.6911
Profiling... [12288/50176]	Loss: 4.7965
Profiling... [13312/50176]	Loss: 4.6236
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 0, Average loss: 0.0045, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.1674210984686,
                        "time": 9.206589303999863,
                        "accuracy": 0.009765625,
                        "total_cost": 112345.65165776983
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 4.6502
Profiling... [2048/50176]	Loss: 7.3271
Profiling... [3072/50176]	Loss: 5.7671
Profiling... [4096/50176]	Loss: 5.2152
Profiling... [5120/50176]	Loss: 4.8589
Profiling... [6144/50176]	Loss: 4.6905
Profiling... [7168/50176]	Loss: 4.7629
Profiling... [8192/50176]	Loss: 4.7597
Profiling... [9216/50176]	Loss: 4.7030
Profiling... [10240/50176]	Loss: 4.7348
Profiling... [11264/50176]	Loss: 4.7346
Profiling... [12288/50176]	Loss: 4.7243
Profiling... [13312/50176]	Loss: 4.6191
Profile done
epoch 1 train time consumed: 13.48s
Validation Epoch: 0, Average loss: 0.0053, Accuracy: 0.0100
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.33847637772035,
                        "time": 9.531107088000226,
                        "accuracy": 0.0099609375,
                        "total_cost": 114188.8299243759
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 4.6275
Profiling... [2048/50176]	Loss: 6.9724
Profiling... [3072/50176]	Loss: 5.6212
Profiling... [4096/50176]	Loss: 5.1045
Profiling... [5120/50176]	Loss: 4.8809
Profiling... [6144/50176]	Loss: 4.6160
Profiling... [7168/50176]	Loss: 4.6847
Profiling... [8192/50176]	Loss: 4.8142
Profiling... [9216/50176]	Loss: 4.6959
Profiling... [10240/50176]	Loss: 5.1557
Profiling... [11264/50176]	Loss: 5.2075
Profiling... [12288/50176]	Loss: 5.1128
Profiling... [13312/50176]	Loss: 4.9088
Profile done
epoch 1 train time consumed: 15.44s
Validation Epoch: 0, Average loss: 0.0049, Accuracy: 0.0098
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.36978875520388,
                        "time": 11.033214051999948,
                        "accuracy": 0.009765625,
                        "total_cost": 134864.1209014457
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 4.6476
Profiling... [2048/50176]	Loss: 6.7551
Profiling... [3072/50176]	Loss: 5.2936
Profiling... [4096/50176]	Loss: 4.8015
Profiling... [5120/50176]	Loss: 4.7619
Profiling... [6144/50176]	Loss: 5.1259
Profiling... [7168/50176]	Loss: 4.6402
Profiling... [8192/50176]	Loss: 4.7549
Profiling... [9216/50176]	Loss: 4.6471
Profiling... [10240/50176]	Loss: 4.7291
Profiling... [11264/50176]	Loss: 4.6113
Profiling... [12288/50176]	Loss: 4.6682
Profiling... [13312/50176]	Loss: 4.6210
Profile done
epoch 1 train time consumed: 38.83s
Validation Epoch: 0, Average loss: 0.0059, Accuracy: 0.0109
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.0,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 119.03660745066837,
                        "time": 29.251567433999753,
                        "accuracy": 0.0109375,
                        "total_cost": 318354.95771042595
                    },
                    
[Training Loop] The optimal parameters are lr: 0.01 dr: 0.5 bs: 128 pl: 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[GPU_0] Set GPU power limit to 175W.
[GPU_0] Set GPU power limit to 175W.
Training Epoch: 0 [128/50048]	Loss: 4.6425
Training Epoch: 0 [256/50048]	Loss: 7.9982
Training Epoch: 0 [384/50048]	Loss: 7.3385
Training Epoch: 0 [512/50048]	Loss: 5.2098
Training Epoch: 0 [640/50048]	Loss: 4.8815
Training Epoch: 0 [768/50048]	Loss: 5.8403
Training Epoch: 0 [896/50048]	Loss: 4.9083
Training Epoch: 0 [1024/50048]	Loss: 4.6652
Training Epoch: 0 [1152/50048]	Loss: 5.0810
Training Epoch: 0 [1280/50048]	Loss: 4.8930
Training Epoch: 0 [1408/50048]	Loss: 4.9576
Training Epoch: 0 [1536/50048]	Loss: 4.8373
Training Epoch: 0 [1664/50048]	Loss: 4.7121
Training Epoch: 0 [1792/50048]	Loss: 4.9506
Training Epoch: 0 [1920/50048]	Loss: 4.5778
Training Epoch: 0 [2048/50048]	Loss: 5.0307
Training Epoch: 0 [2176/50048]	Loss: 4.7317
Training Epoch: 0 [2304/50048]	Loss: 4.6815
Training Epoch: 0 [2432/50048]	Loss: 4.6354
Training Epoch: 0 [2560/50048]	Loss: 4.7356
Training Epoch: 0 [2688/50048]	Loss: 4.5568
Training Epoch: 0 [2816/50048]	Loss: 4.6766
Training Epoch: 0 [2944/50048]	Loss: 4.6712
Training Epoch: 0 [3072/50048]	Loss: 4.5497
Training Epoch: 0 [3200/50048]	Loss: 4.6529
Training Epoch: 0 [3328/50048]	Loss: 4.5784
Training Epoch: 0 [3456/50048]	Loss: 4.6126
Training Epoch: 0 [3584/50048]	Loss: 4.6047
Training Epoch: 0 [3712/50048]	Loss: 4.6054
Training Epoch: 0 [3840/50048]	Loss: 4.7093
Training Epoch: 0 [3968/50048]	Loss: 4.5655
Training Epoch: 0 [4096/50048]	Loss: 4.6675
Training Epoch: 0 [4224/50048]	Loss: 4.6083
Training Epoch: 0 [4352/50048]	Loss: 4.6186
Training Epoch: 0 [4480/50048]	Loss: 4.5889
Training Epoch: 0 [4608/50048]	Loss: 4.5968
Training Epoch: 0 [4736/50048]	Loss: 4.5692
Training Epoch: 0 [4864/50048]	Loss: 4.6605
Training Epoch: 0 [4992/50048]	Loss: 4.5645
Training Epoch: 0 [5120/50048]	Loss: 4.5650
Training Epoch: 0 [5248/50048]	Loss: 4.5406
Training Epoch: 0 [5376/50048]	Loss: 4.5755
Training Epoch: 0 [5504/50048]	Loss: 4.5527
Training Epoch: 0 [5632/50048]	Loss: 4.5971
Training Epoch: 0 [5760/50048]	Loss: 4.5556
Training Epoch: 0 [5888/50048]	Loss: 4.4790
Training Epoch: 0 [6016/50048]	Loss: 4.6094
Training Epoch: 0 [6144/50048]	Loss: 4.6491
Training Epoch: 0 [6272/50048]	Loss: 4.5536
Training Epoch: 0 [6400/50048]	Loss: 4.5460
Training Epoch: 0 [6528/50048]	Loss: 4.5174
Training Epoch: 0 [6656/50048]	Loss: 4.5481
Training Epoch: 0 [6784/50048]	Loss: 4.5946
Training Epoch: 0 [6912/50048]	Loss: 4.5506
Training Epoch: 0 [7040/50048]	Loss: 4.5522
Training Epoch: 0 [7168/50048]	Loss: 4.5328
Training Epoch: 0 [7296/50048]	Loss: 4.5565
Training Epoch: 0 [7424/50048]	Loss: 4.5883
Training Epoch: 0 [7552/50048]	Loss: 4.5086
Training Epoch: 0 [7680/50048]	Loss: 4.5273
Training Epoch: 0 [7808/50048]	Loss: 4.4289
Training Epoch: 0 [7936/50048]	Loss: 4.6385
Training Epoch: 0 [8064/50048]	Loss: 4.5079
Training Epoch: 0 [8192/50048]	Loss: 4.5674
Training Epoch: 0 [8320/50048]	Loss: 4.4421
Training Epoch: 0 [8448/50048]	Loss: 4.5500
Training Epoch: 0 [8576/50048]	Loss: 4.5508
Training Epoch: 0 [8704/50048]	Loss: 4.5140
Training Epoch: 0 [8832/50048]	Loss: 4.5810
Training Epoch: 0 [8960/50048]	Loss: 4.5805
Training Epoch: 0 [9088/50048]	Loss: 4.5302
Training Epoch: 0 [9216/50048]	Loss: 4.4693
Training Epoch: 0 [9344/50048]	Loss: 4.5266
Training Epoch: 0 [9472/50048]	Loss: 4.5146
Training Epoch: 0 [9600/50048]	Loss: 4.4698
Training Epoch: 0 [9728/50048]	Loss: 4.4863
Training Epoch: 0 [9856/50048]	Loss: 4.6003
Training Epoch: 0 [9984/50048]	Loss: 4.4951
Training Epoch: 0 [10112/50048]	Loss: 4.5616
Training Epoch: 0 [10240/50048]	Loss: 4.4401
Training Epoch: 0 [10368/50048]	Loss: 4.5248
Training Epoch: 0 [10496/50048]	Loss: 4.3753
Training Epoch: 0 [10624/50048]	Loss: 4.5601
Training Epoch: 0 [10752/50048]	Loss: 4.5562
Training Epoch: 0 [10880/50048]	Loss: 4.5298
Training Epoch: 0 [11008/50048]	Loss: 4.4776
Training Epoch: 0 [11136/50048]	Loss: 4.3870
Training Epoch: 0 [11264/50048]	Loss: 4.3475
Training Epoch: 0 [11392/50048]	Loss: 4.5506
Training Epoch: 0 [11520/50048]	Loss: 4.3904
Training Epoch: 0 [11648/50048]	Loss: 4.5300
Training Epoch: 0 [11776/50048]	Loss: 4.4159
Training Epoch: 0 [11904/50048]	Loss: 4.4222
Training Epoch: 0 [12032/50048]	Loss: 4.4328
Training Epoch: 0 [12160/50048]	Loss: 4.5156
Training Epoch: 0 [12288/50048]	Loss: 4.3431
Training Epoch: 0 [12416/50048]	Loss: 4.3606
Training Epoch: 0 [12544/50048]	Loss: 4.3512
Training Epoch: 0 [12672/50048]	Loss: 4.3611
Training Epoch: 0 [12800/50048]	Loss: 4.5862
Training Epoch: 0 [12928/50048]	Loss: 4.3795
Training Epoch: 0 [13056/50048]	Loss: 4.3591
Training Epoch: 0 [13184/50048]	Loss: 4.3549
Training Epoch: 0 [13312/50048]	Loss: 4.4456
Training Epoch: 0 [13440/50048]	Loss: 4.3749
Training Epoch: 0 [13568/50048]	Loss: 4.3892
Training Epoch: 0 [13696/50048]	Loss: 4.3741
Training Epoch: 0 [13824/50048]	Loss: 4.3386
Training Epoch: 0 [13952/50048]	Loss: 4.3173
Training Epoch: 0 [14080/50048]	Loss: 4.3795
Training Epoch: 0 [14208/50048]	Loss: 4.3725
Training Epoch: 0 [14336/50048]	Loss: 4.3708
Training Epoch: 0 [14464/50048]	Loss: 4.3889
Training Epoch: 0 [14592/50048]	Loss: 4.4088
Training Epoch: 0 [14720/50048]	Loss: 4.3010
Training Epoch: 0 [14848/50048]	Loss: 4.4251
Training Epoch: 0 [14976/50048]	Loss: 4.3485
Training Epoch: 0 [15104/50048]	Loss: 4.2608
Training Epoch: 0 [15232/50048]	Loss: 4.2148
Training Epoch: 0 [15360/50048]	Loss: 4.3285
Training Epoch: 0 [15488/50048]	Loss: 4.3646
Training Epoch: 0 [15616/50048]	Loss: 4.3056
Training Epoch: 0 [15744/50048]	Loss: 4.2575
Training Epoch: 0 [15872/50048]	Loss: 4.3191
Training Epoch: 0 [16000/50048]	Loss: 4.4089
Training Epoch: 0 [16128/50048]	Loss: 4.4557
Training Epoch: 0 [16256/50048]	Loss: 4.3213
Training Epoch: 0 [16384/50048]	Loss: 4.2698
Training Epoch: 0 [16512/50048]	Loss: 4.4276
Training Epoch: 0 [16640/50048]	Loss: 4.4019
Training Epoch: 0 [16768/50048]	Loss: 4.3274
Training Epoch: 0 [16896/50048]	Loss: 4.3206
Training Epoch: 0 [17024/50048]	Loss: 4.4606
Training Epoch: 0 [17152/50048]	Loss: 4.2705
Training Epoch: 0 [17280/50048]	Loss: 4.3840
Training Epoch: 0 [17408/50048]	Loss: 4.2187
Training Epoch: 0 [17536/50048]	Loss: 4.3655
Training Epoch: 0 [17664/50048]	Loss: 4.3747
Training Epoch: 0 [17792/50048]	Loss: 4.3382
Training Epoch: 0 [17920/50048]	Loss: 4.1970
Training Epoch: 0 [18048/50048]	Loss: 4.2940
Training Epoch: 0 [18176/50048]	Loss: 4.3297
Training Epoch: 0 [18304/50048]	Loss: 4.3378
Training Epoch: 0 [18432/50048]	Loss: 4.2547
Training Epoch: 0 [18560/50048]	Loss: 4.4073
Training Epoch: 0 [18688/50048]	Loss: 4.3359
Training Epoch: 0 [18816/50048]	Loss: 4.3413
Training Epoch: 0 [18944/50048]	Loss: 4.2354
Training Epoch: 0 [19072/50048]	Loss: 4.3006
Training Epoch: 0 [19200/50048]	Loss: 4.2808
Training Epoch: 0 [19328/50048]	Loss: 4.2531
Training Epoch: 0 [19456/50048]	Loss: 4.1047
Training Epoch: 0 [19584/50048]	Loss: 4.4425
Training Epoch: 0 [19712/50048]	Loss: 4.2883
Training Epoch: 0 [19840/50048]	Loss: 4.4191
Training Epoch: 0 [19968/50048]	Loss: 4.2748
Training Epoch: 0 [20096/50048]	Loss: 4.2379
Training Epoch: 0 [20224/50048]	Loss: 4.2386
Training Epoch: 0 [20352/50048]	Loss: 4.3350
Training Epoch: 0 [20480/50048]	Loss: 4.3253
Training Epoch: 0 [20608/50048]	Loss: 4.2306
Training Epoch: 0 [20736/50048]	Loss: 4.4055
Training Epoch: 0 [20864/50048]	Loss: 4.2052
Training Epoch: 0 [20992/50048]	Loss: 4.2903
Training Epoch: 0 [21120/50048]	Loss: 4.1079
Training Epoch: 0 [21248/50048]	Loss: 4.2254
Training Epoch: 0 [21376/50048]	Loss: 4.3534
Training Epoch: 0 [21504/50048]	Loss: 4.2996
Training Epoch: 0 [21632/50048]	Loss: 4.2903
Training Epoch: 0 [21760/50048]	Loss: 4.2879
Training Epoch: 0 [21888/50048]	Loss: 4.4077
Training Epoch: 0 [22016/50048]	Loss: 4.2488
Training Epoch: 0 [22144/50048]	Loss: 4.2572
Training Epoch: 0 [22272/50048]	Loss: 4.2188
Training Epoch: 0 [22400/50048]	Loss: 4.2421
Training Epoch: 0 [22528/50048]	Loss: 4.2159
Training Epoch: 0 [22656/50048]	Loss: 4.2259
Training Epoch: 0 [22784/50048]	Loss: 4.2848
Training Epoch: 0 [22912/50048]	Loss: 4.2377
Training Epoch: 0 [23040/50048]	Loss: 4.3666
Training Epoch: 0 [23168/50048]	Loss: 4.3282
Training Epoch: 0 [23296/50048]	Loss: 4.3179
Training Epoch: 0 [23424/50048]	Loss: 4.2201
Training Epoch: 0 [23552/50048]	Loss: 4.2762
Training Epoch: 0 [23680/50048]	Loss: 4.1812
Training Epoch: 0 [23808/50048]	Loss: 4.1543
Training Epoch: 0 [23936/50048]	Loss: 4.2619
Training Epoch: 0 [24064/50048]	Loss: 4.2239
Training Epoch: 0 [24192/50048]	Loss: 4.3010
Training Epoch: 0 [24320/50048]	Loss: 4.3082
Training Epoch: 0 [24448/50048]	Loss: 4.2604
Training Epoch: 0 [24576/50048]	Loss: 4.1964
Training Epoch: 0 [24704/50048]	Loss: 4.1798
Training Epoch: 0 [24832/50048]	Loss: 4.2560
Training Epoch: 0 [24960/50048]	Loss: 4.1526
Training Epoch: 0 [25088/50048]	Loss: 4.1297
Training Epoch: 0 [25216/50048]	Loss: 4.2898
Training Epoch: 0 [25344/50048]	Loss: 4.3146
Training Epoch: 0 [25472/50048]	Loss: 4.1337
Training Epoch: 0 [25600/50048]	Loss: 4.1195
Training Epoch: 0 [25728/50048]	Loss: 4.2427
Training Epoch: 0 [25856/50048]	Loss: 4.1562
Training Epoch: 0 [25984/50048]	Loss: 4.1383
Training Epoch: 0 [26112/50048]	Loss: 4.2166
Training Epoch: 0 [26240/50048]	Loss: 4.1253
Training Epoch: 0 [26368/50048]	Loss: 4.1806
Training Epoch: 0 [26496/50048]	Loss: 4.1676
Training Epoch: 0 [26624/50048]	Loss: 4.0292
Training Epoch: 0 [26752/50048]	Loss: 4.0843
Training Epoch: 0 [26880/50048]	Loss: 4.0867
Training Epoch: 0 [27008/50048]	Loss: 4.1195
Training Epoch: 0 [27136/50048]	Loss: 4.1776
Training Epoch: 0 [27264/50048]	Loss: 4.1587
Training Epoch: 0 [27392/50048]	Loss: 4.1851
Training Epoch: 0 [27520/50048]	Loss: 4.1263
Training Epoch: 0 [27648/50048]	Loss: 4.0790
Training Epoch: 0 [27776/50048]	Loss: 4.2005
Training Epoch: 0 [27904/50048]	Loss: 4.2699
Training Epoch: 0 [28032/50048]	Loss: 4.2541
Training Epoch: 0 [28160/50048]	Loss: 4.0801
Training Epoch: 0 [28288/50048]	Loss: 4.2115
Training Epoch: 0 [28416/50048]	Loss: 4.0548
Training Epoch: 0 [28544/50048]	Loss: 4.2024
Training Epoch: 0 [28672/50048]	Loss: 4.1245
Training Epoch: 0 [28800/50048]	Loss: 4.1223
Training Epoch: 0 [28928/50048]	Loss: 4.1093
Training Epoch: 0 [29056/50048]	Loss: 4.1037
Training Epoch: 0 [29184/50048]	Loss: 4.1281
Training Epoch: 0 [29312/50048]	Loss: 4.1631
Training Epoch: 0 [29440/50048]	Loss: 4.2481
Training Epoch: 0 [29568/50048]	Loss: 4.1623
Training Epoch: 0 [29696/50048]	Loss: 4.0263
Training Epoch: 0 [29824/50048]	Loss: 4.2361
Training Epoch: 0 [29952/50048]	Loss: 4.1468
Training Epoch: 0 [30080/50048]	Loss: 4.0901
Training Epoch: 0 [30208/50048]	Loss: 4.1467
Training Epoch: 0 [30336/50048]	Loss: 4.0808
Training Epoch: 0 [30464/50048]	Loss: 4.0846
Training Epoch: 0 [30592/50048]	Loss: 4.1377
Training Epoch: 0 [30720/50048]	Loss: 4.2469
Training Epoch: 0 [30848/50048]	Loss: 4.3578
Training Epoch: 0 [30976/50048]	Loss: 4.0767
Training Epoch: 0 [31104/50048]	Loss: 3.9601
Training Epoch: 0 [31232/50048]	Loss: 4.0706
Training Epoch: 0 [31360/50048]	Loss: 4.1646
Training Epoch: 0 [31488/50048]	Loss: 3.9953
Training Epoch: 0 [31616/50048]	Loss: 4.1465
Training Epoch: 0 [31744/50048]	Loss: 4.1506
Training Epoch: 0 [31872/50048]	Loss: 4.1173
Training Epoch: 0 [32000/50048]	Loss: 4.1242
Training Epoch: 0 [32128/50048]	Loss: 4.0003
Training Epoch: 0 [32256/50048]	Loss: 4.0155
Training Epoch: 0 [32384/50048]	Loss: 4.1512
Training Epoch: 0 [32512/50048]	Loss: 4.0759
Training Epoch: 0 [32640/50048]	Loss: 4.2091
Training Epoch: 0 [32768/50048]	Loss: 4.1226
Training Epoch: 0 [32896/50048]	Loss: 4.1313
Training Epoch: 0 [33024/50048]	Loss: 4.0057
Training Epoch: 0 [33152/50048]	Loss: 4.0544
Training Epoch: 0 [33280/50048]	Loss: 4.1928
Training Epoch: 0 [33408/50048]	Loss: 3.9502
Training Epoch: 0 [33536/50048]	Loss: 4.1059
Training Epoch: 0 [33664/50048]	Loss: 4.1124
Training Epoch: 0 [33792/50048]	Loss: 4.0706
Training Epoch: 0 [33920/50048]	Loss: 4.0325
Training Epoch: 0 [34048/50048]	Loss: 3.9791
Training Epoch: 0 [34176/50048]	Loss: 4.1197
Training Epoch: 0 [34304/50048]	Loss: 3.9777
Training Epoch: 0 [34432/50048]	Loss: 3.9569
Training Epoch: 0 [34560/50048]	Loss: 3.9919
Training Epoch: 0 [34688/50048]	Loss: 4.1388
Training Epoch: 0 [34816/50048]	Loss: 4.0139
Training Epoch: 0 [34944/50048]	Loss: 4.1208
Training Epoch: 0 [35072/50048]	Loss: 4.0531
Training Epoch: 0 [35200/50048]	Loss: 4.0342
Training Epoch: 0 [35328/50048]	Loss: 3.9726
Training Epoch: 0 [35456/50048]	Loss: 4.0660
Training Epoch: 0 [35584/50048]	Loss: 4.1275
Training Epoch: 0 [35712/50048]	Loss: 3.9992
Training Epoch: 0 [35840/50048]	Loss: 3.9026
Training Epoch: 0 [35968/50048]	Loss: 3.8531
Training Epoch: 0 [36096/50048]	Loss: 4.0204
Training Epoch: 0 [36224/50048]	Loss: 4.0409
Training Epoch: 0 [36352/50048]	Loss: 3.9544
Training Epoch: 0 [36480/50048]	Loss: 4.1002
Training Epoch: 0 [36608/50048]	Loss: 3.9113
Training Epoch: 0 [36736/50048]	Loss: 4.0513
Training Epoch: 0 [36864/50048]	Loss: 4.1641
Training Epoch: 0 [36992/50048]	Loss: 3.9773
Training Epoch: 0 [37120/50048]	Loss: 4.0920
Training Epoch: 0 [37248/50048]	Loss: 3.9890
Training Epoch: 0 [37376/50048]	Loss: 4.0980
Training Epoch: 0 [37504/50048]	Loss: 3.9347
Training Epoch: 0 [37632/50048]	Loss: 3.9083
Training Epoch: 0 [37760/50048]	Loss: 3.9836
Training Epoch: 0 [37888/50048]	Loss: 4.1496
Training Epoch: 0 [38016/50048]	Loss: 3.9720
Training Epoch: 0 [38144/50048]	Loss: 3.9484
Training Epoch: 0 [38272/50048]	Loss: 3.8765
Training Epoch: 0 [38400/50048]	Loss: 3.9275
Training Epoch: 0 [38528/50048]	Loss: 4.0274
Training Epoch: 0 [38656/50048]	Loss: 4.1319
Training Epoch: 0 [38784/50048]	Loss: 4.0868
Training Epoch: 0 [38912/50048]	Loss: 3.9948
Training Epoch: 0 [39040/50048]	Loss: 3.9606
Training Epoch: 0 [39168/50048]	Loss: 4.0325
Training Epoch: 0 [39296/50048]	Loss: 3.9953
Training Epoch: 0 [39424/50048]	Loss: 3.9698
Training Epoch: 0 [39552/50048]	Loss: 3.9897
Training Epoch: 0 [39680/50048]	Loss: 3.9664
Training Epoch: 0 [39808/50048]	Loss: 3.9831
Training Epoch: 0 [39936/50048]	Loss: 3.9312
Training Epoch: 0 [40064/50048]	Loss: 3.9373
Training Epoch: 0 [40192/50048]	Loss: 3.9376
Training Epoch: 0 [40320/50048]	Loss: 3.9837
Training Epoch: 0 [40448/50048]	Loss: 3.9146
Training Epoch: 0 [40576/50048]	Loss: 3.9319
Training Epoch: 0 [40704/50048]	Loss: 3.8987
Training Epoch: 0 [40832/50048]	Loss: 4.0184
Training Epoch: 0 [40960/50048]	Loss: 3.8247
Training Epoch: 0 [41088/50048]	Loss: 3.7833
Training Epoch: 0 [41216/50048]	Loss: 4.0971
Training Epoch: 0 [41344/50048]	Loss: 4.2077
Training Epoch: 0 [41472/50048]	Loss: 4.0240
Training Epoch: 0 [41600/50048]	Loss: 4.0491
Training Epoch: 0 [41728/50048]	Loss: 4.0678
Training Epoch: 0 [41856/50048]	Loss: 3.9052
Training Epoch: 0 [41984/50048]	Loss: 4.0383
Training Epoch: 0 [42112/50048]	Loss: 3.9056
Training Epoch: 0 [42240/50048]	Loss: 4.0224
Training Epoch: 0 [42368/50048]	Loss: 3.9245
Training Epoch: 0 [42496/50048]	Loss: 4.0012
Training Epoch: 0 [42624/50048]	Loss: 3.9162
Training Epoch: 0 [42752/50048]	Loss: 3.9240
Training Epoch: 0 [42880/50048]	Loss: 3.9053
Training Epoch: 0 [43008/50048]	Loss: 4.0508
Training Epoch: 0 [43136/50048]	Loss: 3.8245
Training Epoch: 0 [43264/50048]	Loss: 3.7532
Training Epoch: 0 [43392/50048]	Loss: 3.9136
Training Epoch: 0 [43520/50048]	Loss: 3.7649
Training Epoch: 0 [43648/50048]	Loss: 3.9527
Training Epoch: 0 [43776/50048]	Loss: 3.8163
Training Epoch: 0 [43904/50048]	Loss: 3.9081
Training Epoch: 0 [44032/50048]	Loss: 4.2782
Training Epoch: 0 [44160/50048]	Loss: 3.8355
Training Epoch: 0 [44288/50048]	Loss: 3.9264
Training Epoch: 0 [44416/50048]	Loss: 4.0021
Training Epoch: 0 [44544/50048]	Loss: 3.9327
Training Epoch: 0 [44672/50048]	Loss: 3.9155
Training Epoch: 0 [44800/50048]	Loss: 3.9795
Training Epoch: 0 [44928/50048]	Loss: 3.8512
Training Epoch: 0 [45056/50048]	Loss: 4.0842
Training Epoch: 0 [45184/50048]	Loss: 3.9369
Training Epoch: 0 [45312/50048]	Loss: 3.8415
Training Epoch: 0 [45440/50048]	Loss: 3.8594
Training Epoch: 0 [45568/50048]	Loss: 3.8305
Training Epoch: 0 [45696/50048]	Loss: 3.9421
Training Epoch: 0 [45824/50048]	Loss: 3.9635
Training Epoch: 0 [45952/50048]	Loss: 3.9893
Training Epoch: 0 [46080/50048]	Loss: 3.8509
Training Epoch: 0 [46208/50048]	Loss: 3.7282
Training Epoch: 0 [46336/50048]	Loss: 4.0040
Training Epoch: 0 [46464/50048]	Loss: 3.8626
Training Epoch: 0 [46592/50048]	Loss: 3.8222
Training Epoch: 0 [46720/50048]	Loss: 4.0288
Training Epoch: 0 [46848/50048]	Loss: 3.7820
Training Epoch: 0 [46976/50048]	Loss: 3.9172
Training Epoch: 0 [47104/50048]	Loss: 3.8353
Training Epoch: 0 [47232/50048]	Loss: 3.8078
Training Epoch: 0 [47360/50048]	Loss: 3.9815
Training Epoch: 0 [47488/50048]	Loss: 3.9737
Training Epoch: 0 [47616/50048]	Loss: 3.7918
Training Epoch: 0 [47744/50048]	Loss: 3.9474
Training Epoch: 0 [47872/50048]	Loss: 3.8664
Training Epoch: 0 [48000/50048]	Loss: 3.8240
Training Epoch: 0 [48128/50048]	Loss: 3.9679
Training Epoch: 0 [48256/50048]	Loss: 3.9245
Training Epoch: 0 [48384/50048]	Loss: 3.8254
Training Epoch: 0 [48512/50048]	Loss: 3.6988
Training Epoch: 0 [48640/50048]	Loss: 3.9915
Training Epoch: 0 [48768/50048]	Loss: 4.0477
Training Epoch: 0 [48896/50048]	Loss: 3.9348
Training Epoch: 0 [49024/50048]	Loss: 3.6069
Training Epoch: 0 [49152/50048]	Loss: 4.0520
Training Epoch: 0 [49280/50048]	Loss: 3.9757
Training Epoch: 0 [49408/50048]	Loss: 3.7186
Training Epoch: 0 [49536/50048]	Loss: 3.7477
Training Epoch: 0 [49664/50048]	Loss: 3.7825
Training Epoch: 0 [49792/50048]	Loss: 3.8707
Training Epoch: 0 [49920/50048]	Loss: 3.8522
Training Epoch: 0 [50048/50048]	Loss: 3.8678
Validation Epoch: 0, Average loss: 0.0304, Accuracy: 0.0993
Training Epoch: 1 [128/50048]	Loss: 3.8242
Training Epoch: 1 [256/50048]	Loss: 3.8484
Training Epoch: 1 [384/50048]	Loss: 3.7628
Training Epoch: 1 [512/50048]	Loss: 4.0036
Training Epoch: 1 [640/50048]	Loss: 3.8091
Training Epoch: 1 [768/50048]	Loss: 3.6202
Training Epoch: 1 [896/50048]	Loss: 3.8942
Training Epoch: 1 [1024/50048]	Loss: 3.9314
Training Epoch: 1 [1152/50048]	Loss: 3.7423
Training Epoch: 1 [1280/50048]	Loss: 3.9538
Training Epoch: 1 [1408/50048]	Loss: 3.8394
Training Epoch: 1 [1536/50048]	Loss: 3.5973
Training Epoch: 1 [1664/50048]	Loss: 3.7475
Training Epoch: 1 [1792/50048]	Loss: 3.7334
Training Epoch: 1 [1920/50048]	Loss: 3.8105
Training Epoch: 1 [2048/50048]	Loss: 3.7277
Training Epoch: 1 [2176/50048]	Loss: 3.8398
Training Epoch: 1 [2304/50048]	Loss: 3.7648
Training Epoch: 1 [2432/50048]	Loss: 3.8406
Training Epoch: 1 [2560/50048]	Loss: 3.6935
Training Epoch: 1 [2688/50048]	Loss: 3.8549
Training Epoch: 1 [2816/50048]	Loss: 3.8178
Training Epoch: 1 [2944/50048]	Loss: 3.7503
Training Epoch: 1 [3072/50048]	Loss: 4.0620
Training Epoch: 1 [3200/50048]	Loss: 3.8243
Training Epoch: 1 [3328/50048]	Loss: 3.7593
Training Epoch: 1 [3456/50048]	Loss: 3.5527
Training Epoch: 1 [3584/50048]	Loss: 3.8181
Training Epoch: 1 [3712/50048]	Loss: 3.8900
Training Epoch: 1 [3840/50048]	Loss: 3.7581
Training Epoch: 1 [3968/50048]	Loss: 3.6961
Training Epoch: 1 [4096/50048]	Loss: 3.6186
Training Epoch: 1 [4224/50048]	Loss: 3.9556
Training Epoch: 1 [4352/50048]	Loss: 3.6809
Training Epoch: 1 [4480/50048]	Loss: 3.8288
Training Epoch: 1 [4608/50048]	Loss: 3.6866
Training Epoch: 1 [4736/50048]	Loss: 3.8768
Training Epoch: 1 [4864/50048]	Loss: 3.7667
Training Epoch: 1 [4992/50048]	Loss: 3.7354
Training Epoch: 1 [5120/50048]	Loss: 3.7876
Training Epoch: 1 [5248/50048]	Loss: 3.9057
Training Epoch: 1 [5376/50048]	Loss: 3.6860
Training Epoch: 1 [5504/50048]	Loss: 3.7550
Training Epoch: 1 [5632/50048]	Loss: 3.8265
Training Epoch: 1 [5760/50048]	Loss: 3.8849
Training Epoch: 1 [5888/50048]	Loss: 3.8482
Training Epoch: 1 [6016/50048]	Loss: 3.5567
Training Epoch: 1 [6144/50048]	Loss: 3.8333
Training Epoch: 1 [6272/50048]	Loss: 3.7489
Training Epoch: 1 [6400/50048]	Loss: 3.8147
Training Epoch: 1 [6528/50048]	Loss: 3.7533
Training Epoch: 1 [6656/50048]	Loss: 3.8466
Training Epoch: 1 [6784/50048]	Loss: 3.6909
Training Epoch: 1 [6912/50048]	Loss: 3.9205
Training Epoch: 1 [7040/50048]	Loss: 3.6577
Training Epoch: 1 [7168/50048]	Loss: 3.6160
Training Epoch: 1 [7296/50048]	Loss: 3.8412
Training Epoch: 1 [7424/50048]	Loss: 3.9534
Training Epoch: 1 [7552/50048]	Loss: 3.8896
Training Epoch: 1 [7680/50048]	Loss: 3.4897
Training Epoch: 1 [7808/50048]	Loss: 3.7151
Training Epoch: 1 [7936/50048]	Loss: 3.7188
Training Epoch: 1 [8064/50048]	Loss: 3.5236
Training Epoch: 1 [8192/50048]	Loss: 3.7353
Training Epoch: 1 [8320/50048]	Loss: 3.9899
Training Epoch: 1 [8448/50048]	Loss: 3.6969
Training Epoch: 1 [8576/50048]	Loss: 3.5963
Training Epoch: 1 [8704/50048]	Loss: 3.6300
Training Epoch: 1 [8832/50048]	Loss: 3.6846
Training Epoch: 1 [8960/50048]	Loss: 3.5451
Training Epoch: 1 [9088/50048]	Loss: 3.4593
Training Epoch: 1 [9216/50048]	Loss: 3.5948
Training Epoch: 1 [9344/50048]	Loss: 3.7293
Training Epoch: 1 [9472/50048]	Loss: 3.8241
Training Epoch: 1 [9600/50048]	Loss: 3.9427
Training Epoch: 1 [9728/50048]	Loss: 4.0185
Training Epoch: 1 [9856/50048]	Loss: 3.8128
Training Epoch: 1 [9984/50048]	Loss: 3.6913
Training Epoch: 1 [10112/50048]	Loss: 3.8739
Training Epoch: 1 [10240/50048]	Loss: 3.7262
Training Epoch: 1 [10368/50048]	Loss: 3.8255
Training Epoch: 1 [10496/50048]	Loss: 3.5606
Training Epoch: 1 [10624/50048]	Loss: 3.7375
Training Epoch: 1 [10752/50048]	Loss: 3.7622
Training Epoch: 1 [10880/50048]	Loss: 3.7362
Training Epoch: 1 [11008/50048]	Loss: 3.7233
Training Epoch: 1 [11136/50048]	Loss: 3.7812
Training Epoch: 1 [11264/50048]	Loss: 3.6366
Training Epoch: 1 [11392/50048]	Loss: 3.8899
Training Epoch: 1 [11520/50048]	Loss: 3.7296
Training Epoch: 1 [11648/50048]	Loss: 3.7729
Training Epoch: 1 [11776/50048]	Loss: 3.6237
Training Epoch: 1 [11904/50048]	Loss: 3.7220
Training Epoch: 1 [12032/50048]	Loss: 3.6260
Training Epoch: 1 [12160/50048]	Loss: 3.6733
Training Epoch: 1 [12288/50048]	Loss: 3.6008
Training Epoch: 1 [12416/50048]	Loss: 3.6368
Training Epoch: 1 [12544/50048]	Loss: 3.7968
Training Epoch: 1 [12672/50048]	Loss: 3.6874
Training Epoch: 1 [12800/50048]	Loss: 3.7781
Training Epoch: 1 [12928/50048]	Loss: 3.6726
Training Epoch: 1 [13056/50048]	Loss: 3.7512
Training Epoch: 1 [13184/50048]	Loss: 3.6515
Training Epoch: 1 [13312/50048]	Loss: 3.6506
Training Epoch: 1 [13440/50048]	Loss: 3.8046
Training Epoch: 1 [13568/50048]	Loss: 3.5807
Training Epoch: 1 [13696/50048]	Loss: 3.5993
Training Epoch: 1 [13824/50048]	Loss: 3.7290
Training Epoch: 1 [13952/50048]	Loss: 3.8018
Training Epoch: 1 [14080/50048]	Loss: 3.7918
Training Epoch: 1 [14208/50048]	Loss: 3.6908
Training Epoch: 1 [14336/50048]	Loss: 3.6021
Training Epoch: 1 [14464/50048]	Loss: 3.9088
Training Epoch: 1 [14592/50048]	Loss: 3.4295
Training Epoch: 1 [14720/50048]	Loss: 3.8509
Training Epoch: 1 [14848/50048]	Loss: 3.7761
Training Epoch: 1 [14976/50048]	Loss: 3.7495
Training Epoch: 1 [15104/50048]	Loss: 3.6882
Training Epoch: 1 [15232/50048]	Loss: 3.7130
Training Epoch: 1 [15360/50048]	Loss: 3.6835
Training Epoch: 1 [15488/50048]	Loss: 3.5869
Training Epoch: 1 [15616/50048]	Loss: 3.6975
Training Epoch: 1 [15744/50048]	Loss: 3.9328
Training Epoch: 1 [15872/50048]	Loss: 3.5835
Training Epoch: 1 [16000/50048]	Loss: 3.8315
Training Epoch: 1 [16128/50048]	Loss: 3.6700
Training Epoch: 1 [16256/50048]	Loss: 3.7037
Training Epoch: 1 [16384/50048]	Loss: 3.4824
Training Epoch: 1 [16512/50048]	Loss: 3.7514
Training Epoch: 1 [16640/50048]	Loss: 3.6043
Training Epoch: 1 [16768/50048]	Loss: 3.6380
Training Epoch: 1 [16896/50048]	Loss: 3.7894
Training Epoch: 1 [17024/50048]	Loss: 3.5133
Training Epoch: 1 [17152/50048]	Loss: 3.7345
Training Epoch: 1 [17280/50048]	Loss: 3.7661
Training Epoch: 1 [17408/50048]	Loss: 3.7178
Training Epoch: 1 [17536/50048]	Loss: 3.8324
Training Epoch: 1 [17664/50048]	Loss: 3.6418
Training Epoch: 1 [17792/50048]	Loss: 3.8041
Training Epoch: 1 [17920/50048]	Loss: 3.6567
Training Epoch: 1 [18048/50048]	Loss: 3.6324
Training Epoch: 1 [18176/50048]	Loss: 3.7512
Training Epoch: 1 [18304/50048]	Loss: 3.5892
Training Epoch: 1 [18432/50048]	Loss: 3.6074
Training Epoch: 1 [18560/50048]	Loss: 3.7998
Training Epoch: 1 [18688/50048]	Loss: 3.5811
Training Epoch: 1 [18816/50048]	Loss: 3.7916
Training Epoch: 1 [18944/50048]	Loss: 3.7618
Training Epoch: 1 [19072/50048]	Loss: 3.6609
Training Epoch: 1 [19200/50048]	Loss: 3.6201
Training Epoch: 1 [19328/50048]	Loss: 3.8876
Training Epoch: 1 [19456/50048]	Loss: 3.7614
Training Epoch: 1 [19584/50048]	Loss: 3.5549
Training Epoch: 1 [19712/50048]	Loss: 3.7951
Training Epoch: 1 [19840/50048]	Loss: 3.6718
Training Epoch: 1 [19968/50048]	Loss: 3.7104
Training Epoch: 1 [20096/50048]	Loss: 3.5665
Training Epoch: 1 [20224/50048]	Loss: 3.6036
Training Epoch: 1 [20352/50048]	Loss: 3.5668
Training Epoch: 1 [20480/50048]	Loss: 3.5007
Training Epoch: 1 [20608/50048]	Loss: 3.5175
Training Epoch: 1 [20736/50048]	Loss: 3.7243
Training Epoch: 1 [20864/50048]	Loss: 3.6343
Training Epoch: 1 [20992/50048]	Loss: 3.8806
Training Epoch: 1 [21120/50048]	Loss: 3.8897
Training Epoch: 1 [21248/50048]	Loss: 3.6406
Training Epoch: 1 [21376/50048]	Loss: 3.5044
Training Epoch: 1 [21504/50048]	Loss: 3.6937
Training Epoch: 1 [21632/50048]	Loss: 3.5269
Training Epoch: 1 [21760/50048]	Loss: 3.6066
Training Epoch: 1 [21888/50048]	Loss: 3.6028
Training Epoch: 1 [22016/50048]	Loss: 3.7934
Training Epoch: 1 [22144/50048]	Loss: 3.8811
Training Epoch: 1 [22272/50048]	Loss: 3.7151
Training Epoch: 1 [22400/50048]	Loss: 3.9275
Training Epoch: 1 [22528/50048]	Loss: 3.8593
Training Epoch: 1 [22656/50048]	Loss: 3.6753
Training Epoch: 1 [22784/50048]	Loss: 3.6088
Training Epoch: 1 [22912/50048]	Loss: 3.4773
Training Epoch: 1 [23040/50048]	Loss: 3.7077
Training Epoch: 1 [23168/50048]	Loss: 3.6668
Training Epoch: 1 [23296/50048]	Loss: 3.6403
Training Epoch: 1 [23424/50048]	Loss: 3.7388
Training Epoch: 1 [23552/50048]	Loss: 3.6698
Training Epoch: 1 [23680/50048]	Loss: 3.6629
Training Epoch: 1 [23808/50048]	Loss: 3.6903
Training Epoch: 1 [23936/50048]	Loss: 3.5938
Training Epoch: 1 [24064/50048]	Loss: 3.4623
Training Epoch: 1 [24192/50048]	Loss: 3.7031
Training Epoch: 1 [24320/50048]	Loss: 3.7308
Training Epoch: 1 [24448/50048]	Loss: 3.6248
Training Epoch: 1 [24576/50048]	Loss: 3.6474
Training Epoch: 1 [24704/50048]	Loss: 3.4511
Training Epoch: 1 [24832/50048]	Loss: 3.4191
Training Epoch: 1 [24960/50048]	Loss: 3.6734
Training Epoch: 1 [25088/50048]	Loss: 3.3981
Training Epoch: 1 [25216/50048]	Loss: 3.3896
Training Epoch: 1 [25344/50048]	Loss: 3.7634
Training Epoch: 1 [25472/50048]	Loss: 3.5854
Training Epoch: 1 [25600/50048]	Loss: 3.5239
Training Epoch: 1 [25728/50048]	Loss: 3.3484
Training Epoch: 1 [25856/50048]	Loss: 3.5924
Training Epoch: 1 [25984/50048]	Loss: 3.8207
Training Epoch: 1 [26112/50048]	Loss: 3.6825
Training Epoch: 1 [26240/50048]	Loss: 3.5387
Training Epoch: 1 [26368/50048]	Loss: 3.5890
Training Epoch: 1 [26496/50048]	Loss: 3.6895
Training Epoch: 1 [26624/50048]	Loss: 3.4776
Training Epoch: 1 [26752/50048]	Loss: 3.5933
Training Epoch: 1 [26880/50048]	Loss: 3.6363
Training Epoch: 1 [27008/50048]	Loss: 3.5378
Training Epoch: 1 [27136/50048]	Loss: 3.4742
Training Epoch: 1 [27264/50048]	Loss: 3.6614
Training Epoch: 1 [27392/50048]	Loss: 3.5614
Training Epoch: 1 [27520/50048]	Loss: 3.7047
Training Epoch: 1 [27648/50048]	Loss: 3.3963
Training Epoch: 1 [27776/50048]	Loss: 3.4479
Training Epoch: 1 [27904/50048]	Loss: 3.6036
Training Epoch: 1 [28032/50048]	Loss: 3.7241
Training Epoch: 1 [28160/50048]	Loss: 3.6780
Training Epoch: 1 [28288/50048]	Loss: 3.7381
Training Epoch: 1 [28416/50048]	Loss: 3.4826
Training Epoch: 1 [28544/50048]	Loss: 3.4780
Training Epoch: 1 [28672/50048]	Loss: 3.6118
Training Epoch: 1 [28800/50048]	Loss: 3.6458
Training Epoch: 1 [28928/50048]	Loss: 3.6100
Training Epoch: 1 [29056/50048]	Loss: 3.4908
Training Epoch: 1 [29184/50048]	Loss: 3.6887
Training Epoch: 1 [29312/50048]	Loss: 3.5165
Training Epoch: 1 [29440/50048]	Loss: 3.5748
Training Epoch: 1 [29568/50048]	Loss: 3.7105
Training Epoch: 1 [29696/50048]	Loss: 3.7336
Training Epoch: 1 [29824/50048]	Loss: 3.4861
Training Epoch: 1 [29952/50048]	Loss: 3.7182
Training Epoch: 1 [30080/50048]	Loss: 3.6038
Training Epoch: 1 [30208/50048]	Loss: 3.5377
Training Epoch: 1 [30336/50048]	Loss: 3.6509
Training Epoch: 1 [30464/50048]	Loss: 3.4156
Training Epoch: 1 [30592/50048]	Loss: 4.0509
Training Epoch: 1 [30720/50048]	Loss: 3.6745
Training Epoch: 1 [30848/50048]	Loss: 3.4842
Training Epoch: 1 [30976/50048]	Loss: 3.5539
Training Epoch: 1 [31104/50048]	Loss: 3.6790
Training Epoch: 1 [31232/50048]	Loss: 3.4134
Training Epoch: 1 [31360/50048]	Loss: 3.4340
Training Epoch: 1 [31488/50048]	Loss: 3.3454
Training Epoch: 1 [31616/50048]	Loss: 3.7091
Training Epoch: 1 [31744/50048]	Loss: 3.5508
Training Epoch: 1 [31872/50048]	Loss: 3.4700
Training Epoch: 1 [32000/50048]	Loss: 3.6656
Training Epoch: 1 [32128/50048]	Loss: 3.5906
Training Epoch: 1 [32256/50048]	Loss: 3.5356
Training Epoch: 1 [32384/50048]	Loss: 3.4977
Training Epoch: 1 [32512/50048]	Loss: 3.5529
Training Epoch: 1 [32640/50048]	Loss: 3.4493
Training Epoch: 1 [32768/50048]	Loss: 3.5348
Training Epoch: 1 [32896/50048]	Loss: 3.4727
Training Epoch: 1 [33024/50048]	Loss: 3.4108
Training Epoch: 1 [33152/50048]	Loss: 3.5189
Training Epoch: 1 [33280/50048]	Loss: 3.5805
Training Epoch: 1 [33408/50048]	Loss: 3.6230
Training Epoch: 1 [33536/50048]	Loss: 3.6455
Training Epoch: 1 [33664/50048]	Loss: 3.7645
Training Epoch: 1 [33792/50048]	Loss: 3.4895
Training Epoch: 1 [33920/50048]	Loss: 3.4377
Training Epoch: 1 [34048/50048]	Loss: 3.5290
Training Epoch: 1 [34176/50048]	Loss: 3.5048
Training Epoch: 1 [34304/50048]	Loss: 3.4361
Training Epoch: 1 [34432/50048]	Loss: 3.6675
Training Epoch: 1 [34560/50048]	Loss: 3.8665
Training Epoch: 1 [34688/50048]	Loss: 3.6329
Training Epoch: 1 [34816/50048]	Loss: 3.4762
Training Epoch: 1 [34944/50048]	Loss: 3.4774
Training Epoch: 1 [35072/50048]	Loss: 3.4926
Training Epoch: 1 [35200/50048]	Loss: 3.6811
Training Epoch: 1 [35328/50048]	Loss: 3.5436
Training Epoch: 1 [35456/50048]	Loss: 3.4927
Training Epoch: 1 [35584/50048]	Loss: 3.7323
Training Epoch: 1 [35712/50048]	Loss: 3.4448
Training Epoch: 1 [35840/50048]	Loss: 3.7853
Training Epoch: 1 [35968/50048]	Loss: 3.6083
Training Epoch: 1 [36096/50048]	Loss: 3.7577
Training Epoch: 1 [36224/50048]	Loss: 3.4105
Training Epoch: 1 [36352/50048]	Loss: 3.6078
Training Epoch: 1 [36480/50048]	Loss: 3.4269
Training Epoch: 1 [36608/50048]	Loss: 3.4864
Training Epoch: 1 [36736/50048]	Loss: 3.4956
Training Epoch: 1 [36864/50048]	Loss: 3.5563
Training Epoch: 1 [36992/50048]	Loss: 3.3837
Training Epoch: 1 [37120/50048]	Loss: 3.2552
Training Epoch: 1 [37248/50048]	Loss: 3.3272
Training Epoch: 1 [37376/50048]	Loss: 3.4447
Training Epoch: 1 [37504/50048]	Loss: 3.5836
Training Epoch: 1 [37632/50048]	Loss: 3.5095
Training Epoch: 1 [37760/50048]	Loss: 3.5374
Training Epoch: 1 [37888/50048]	Loss: 3.5295
Training Epoch: 1 [38016/50048]	Loss: 3.5785
Training Epoch: 1 [38144/50048]	Loss: 3.4809
Training Epoch: 1 [38272/50048]	Loss: 3.2695
Training Epoch: 1 [38400/50048]	Loss: 3.4641
Training Epoch: 1 [38528/50048]	Loss: 3.2954
Training Epoch: 1 [38656/50048]	Loss: 3.5255
Training Epoch: 1 [38784/50048]	Loss: 3.2328
Training Epoch: 1 [38912/50048]	Loss: 3.6251
Training Epoch: 1 [39040/50048]	Loss: 3.4831
Training Epoch: 1 [39168/50048]	Loss: 3.4385
Training Epoch: 1 [39296/50048]	Loss: 3.5927
Training Epoch: 1 [39424/50048]	Loss: 3.4083
Training Epoch: 1 [39552/50048]	Loss: 3.7336
Training Epoch: 1 [39680/50048]	Loss: 3.4935
Training Epoch: 1 [39808/50048]	Loss: 3.4153
Training Epoch: 1 [39936/50048]	Loss: 3.2637
Training Epoch: 1 [40064/50048]	Loss: 3.6543
Training Epoch: 1 [40192/50048]	Loss: 3.4716
Training Epoch: 1 [40320/50048]	Loss: 3.4460
Training Epoch: 1 [40448/50048]	Loss: 3.4282
Training Epoch: 1 [40576/50048]	Loss: 3.3999
Training Epoch: 1 [40704/50048]	Loss: 3.3465
Training Epoch: 1 [40832/50048]	Loss: 3.4663
Training Epoch: 1 [40960/50048]	Loss: 3.3188
Training Epoch: 1 [41088/50048]	Loss: 3.4777
Training Epoch: 1 [41216/50048]	Loss: 3.3185
Training Epoch: 1 [41344/50048]	Loss: 3.3289
Training Epoch: 1 [41472/50048]	Loss: 3.2847
Training Epoch: 1 [41600/50048]	Loss: 3.5789
Training Epoch: 1 [41728/50048]	Loss: 3.2901
Training Epoch: 1 [41856/50048]	Loss: 3.5159
Training Epoch: 1 [41984/50048]	Loss: 3.5726
Training Epoch: 1 [42112/50048]	Loss: 3.8034
Training Epoch: 1 [42240/50048]	Loss: 3.6160
Training Epoch: 1 [42368/50048]	Loss: 3.4540
Training Epoch: 1 [42496/50048]	Loss: 3.3589
Training Epoch: 1 [42624/50048]	Loss: 3.5548
Training Epoch: 1 [42752/50048]	Loss: 3.4481
Training Epoch: 1 [42880/50048]	Loss: 3.4831
Training Epoch: 1 [43008/50048]	Loss: 3.2530
Training Epoch: 1 [43136/50048]	Loss: 3.3464
Training Epoch: 1 [43264/50048]	Loss: 3.5119
Training Epoch: 1 [43392/50048]	Loss: 3.4716
Training Epoch: 1 [43520/50048]	Loss: 3.4220
Training Epoch: 1 [43648/50048]	Loss: 3.4306
Training Epoch: 1 [43776/50048]	Loss: 3.4705
Training Epoch: 1 [43904/50048]	Loss: 3.3725
Training Epoch: 1 [44032/50048]	Loss: 3.4968
Training Epoch: 1 [44160/50048]	Loss: 3.2991
Training Epoch: 1 [44288/50048]	Loss: 3.2931
Training Epoch: 1 [44416/50048]	Loss: 3.2174
Training Epoch: 1 [44544/50048]	Loss: 3.4450
Training Epoch: 1 [44672/50048]	Loss: 3.2235
Training Epoch: 1 [44800/50048]	Loss: 3.4607
Training Epoch: 1 [44928/50048]	Loss: 3.4857
Training Epoch: 1 [45056/50048]	Loss: 3.2702
Training Epoch: 1 [45184/50048]	Loss: 3.2592
Training Epoch: 1 [45312/50048]	Loss: 3.3254
Training Epoch: 1 [45440/50048]	Loss: 3.4017
Training Epoch: 1 [45568/50048]	Loss: 3.3299
Training Epoch: 1 [45696/50048]	Loss: 3.2205
Training Epoch: 1 [45824/50048]	Loss: 3.4851
Training Epoch: 1 [45952/50048]	Loss: 3.4735
Training Epoch: 1 [46080/50048]	Loss: 3.4240
Training Epoch: 1 [46208/50048]	Loss: 3.4037
Training Epoch: 1 [46336/50048]	Loss: 3.3590
Training Epoch: 1 [46464/50048]	Loss: 3.3722
Training Epoch: 1 [46592/50048]	Loss: 3.3650
Training Epoch: 1 [46720/50048]	Loss: 3.2442
Training Epoch: 1 [46848/50048]	Loss: 3.3874
Training Epoch: 1 [46976/50048]	Loss: 3.5508
Training Epoch: 1 [47104/50048]	Loss: 3.3745
Training Epoch: 1 [47232/50048]	Loss: 3.1473
Training Epoch: 1 [47360/50048]	Loss: 3.2488
Training Epoch: 1 [47488/50048]	Loss: 3.4028
Training Epoch: 1 [47616/50048]	Loss: 3.4948
Training Epoch: 1 [47744/50048]	Loss: 3.5016
Training Epoch: 1 [47872/50048]	Loss: 3.2912
Training Epoch: 1 [48000/50048]	Loss: 3.6042
Training Epoch: 1 [48128/50048]	Loss: 3.5425
Training Epoch: 1 [48256/50048]	Loss: 3.5052
Training Epoch: 1 [48384/50048]	Loss: 3.4765
Training Epoch: 1 [48512/50048]	Loss: 3.2726
Training Epoch: 1 [48640/50048]	Loss: 3.3350
Training Epoch: 1 [48768/50048]	Loss: 3.6263
Training Epoch: 1 [48896/50048]	Loss: 3.5011
Training Epoch: 1 [49024/50048]	Loss: 3.6406
Training Epoch: 1 [49152/50048]	Loss: 3.2294
Training Epoch: 1 [49280/50048]	Loss: 3.5344
Training Epoch: 1 [49408/50048]	Loss: 3.3030
Training Epoch: 1 [49536/50048]	Loss: 3.3577
Training Epoch: 1 [49664/50048]	Loss: 3.6710
Training Epoch: 1 [49792/50048]	Loss: 3.3536
Training Epoch: 1 [49920/50048]	Loss: 3.3215
Training Epoch: 1 [50048/50048]	Loss: 3.3795
Validation Epoch: 1, Average loss: 0.0265, Accuracy: 0.1783
Training Epoch: 2 [128/50048]	Loss: 3.2639
Training Epoch: 2 [256/50048]	Loss: 3.2774
Training Epoch: 2 [384/50048]	Loss: 3.3425
Training Epoch: 2 [512/50048]	Loss: 3.3216
Training Epoch: 2 [640/50048]	Loss: 3.4673
Training Epoch: 2 [768/50048]	Loss: 3.5358
Training Epoch: 2 [896/50048]	Loss: 3.3848
Training Epoch: 2 [1024/50048]	Loss: 3.4513
Training Epoch: 2 [1152/50048]	Loss: 3.2068
Training Epoch: 2 [1280/50048]	Loss: 3.2466
Training Epoch: 2 [1408/50048]	Loss: 3.3286
Training Epoch: 2 [1536/50048]	Loss: 3.3338
Training Epoch: 2 [1664/50048]	Loss: 3.3182
Training Epoch: 2 [1792/50048]	Loss: 3.2905
Training Epoch: 2 [1920/50048]	Loss: 3.4345
Training Epoch: 2 [2048/50048]	Loss: 3.5224
Training Epoch: 2 [2176/50048]	Loss: 3.3535
Training Epoch: 2 [2304/50048]	Loss: 3.1890
Training Epoch: 2 [2432/50048]	Loss: 3.2961
Training Epoch: 2 [2560/50048]	Loss: 3.5808
Training Epoch: 2 [2688/50048]	Loss: 3.5266
Training Epoch: 2 [2816/50048]	Loss: 3.2360
Training Epoch: 2 [2944/50048]	Loss: 3.3273
Training Epoch: 2 [3072/50048]	Loss: 3.1364
Training Epoch: 2 [3200/50048]	Loss: 3.3653
Training Epoch: 2 [3328/50048]	Loss: 3.4904
Training Epoch: 2 [3456/50048]	Loss: 3.4192
Training Epoch: 2 [3584/50048]	Loss: 3.1826
Training Epoch: 2 [3712/50048]	Loss: 3.1520
Training Epoch: 2 [3840/50048]	Loss: 3.1981
Training Epoch: 2 [3968/50048]	Loss: 3.2408
Training Epoch: 2 [4096/50048]	Loss: 3.3718
Training Epoch: 2 [4224/50048]	Loss: 3.3031
Training Epoch: 2 [4352/50048]	Loss: 3.4965
Training Epoch: 2 [4480/50048]	Loss: 3.4832
Training Epoch: 2 [4608/50048]	Loss: 3.2742
Training Epoch: 2 [4736/50048]	Loss: 3.4422
Training Epoch: 2 [4864/50048]	Loss: 3.2713
Training Epoch: 2 [4992/50048]	Loss: 3.3383
Training Epoch: 2 [5120/50048]	Loss: 3.1881
Training Epoch: 2 [5248/50048]	Loss: 3.2279
Training Epoch: 2 [5376/50048]	Loss: 3.1653
Training Epoch: 2 [5504/50048]	Loss: 3.4293
Training Epoch: 2 [5632/50048]	Loss: 3.2110
Training Epoch: 2 [5760/50048]	Loss: 3.3505
Training Epoch: 2 [5888/50048]	Loss: 3.2626
Training Epoch: 2 [6016/50048]	Loss: 3.3127
Training Epoch: 2 [6144/50048]	Loss: 3.1882
Training Epoch: 2 [6272/50048]	Loss: 3.5058
Training Epoch: 2 [6400/50048]	Loss: 3.2101
Training Epoch: 2 [6528/50048]	Loss: 3.2729
Training Epoch: 2 [6656/50048]	Loss: 3.3457
Training Epoch: 2 [6784/50048]	Loss: 3.5154
Training Epoch: 2 [6912/50048]	Loss: 3.3381
Training Epoch: 2 [7040/50048]	Loss: 3.1326
Training Epoch: 2 [7168/50048]	Loss: 3.3981
Training Epoch: 2 [7296/50048]	Loss: 3.3204
Training Epoch: 2 [7424/50048]	Loss: 3.3641
Training Epoch: 2 [7552/50048]	Loss: 3.1169
Training Epoch: 2 [7680/50048]	Loss: 3.3288
Training Epoch: 2 [7808/50048]	Loss: 3.1880
Training Epoch: 2 [7936/50048]	Loss: 3.2765
Training Epoch: 2 [8064/50048]	Loss: 3.4502
Training Epoch: 2 [8192/50048]	Loss: 3.3344
Training Epoch: 2 [8320/50048]	Loss: 3.5154
Training Epoch: 2 [8448/50048]	Loss: 3.0010
Training Epoch: 2 [8576/50048]	Loss: 3.0869
Training Epoch: 2 [8704/50048]	Loss: 3.2251
Training Epoch: 2 [8832/50048]	Loss: 3.2981
Training Epoch: 2 [8960/50048]	Loss: 3.2519
Training Epoch: 2 [9088/50048]	Loss: 3.4027
Training Epoch: 2 [9216/50048]	Loss: 3.1749
Training Epoch: 2 [9344/50048]	Loss: 3.3143
Training Epoch: 2 [9472/50048]	Loss: 3.2366
Training Epoch: 2 [9600/50048]	Loss: 3.0223
Training Epoch: 2 [9728/50048]	Loss: 3.3253
Training Epoch: 2 [9856/50048]	Loss: 3.1577
Training Epoch: 2 [9984/50048]	Loss: 3.3372
Training Epoch: 2 [10112/50048]	Loss: 3.3540
Training Epoch: 2 [10240/50048]	Loss: 3.2584
Training Epoch: 2 [10368/50048]	Loss: 3.3911
Training Epoch: 2 [10496/50048]	Loss: 3.2528
Training Epoch: 2 [10624/50048]	Loss: 3.2937
Training Epoch: 2 [10752/50048]	Loss: 2.9576
Training Epoch: 2 [10880/50048]	Loss: 3.2748
Training Epoch: 2 [11008/50048]	Loss: 3.2032
Training Epoch: 2 [11136/50048]	Loss: 3.3362
Training Epoch: 2 [11264/50048]	Loss: 3.1621
Training Epoch: 2 [11392/50048]	Loss: 3.2211
Training Epoch: 2 [11520/50048]	Loss: 3.1551
Training Epoch: 2 [11648/50048]	Loss: 3.0842
Training Epoch: 2 [11776/50048]	Loss: 3.3608
Training Epoch: 2 [11904/50048]	Loss: 3.0479
Training Epoch: 2 [12032/50048]	Loss: 3.6470
Training Epoch: 2 [12160/50048]	Loss: 3.3747
Training Epoch: 2 [12288/50048]	Loss: 3.4409
Training Epoch: 2 [12416/50048]	Loss: 3.0090
Training Epoch: 2 [12544/50048]	Loss: 3.3019
Training Epoch: 2 [12672/50048]	Loss: 3.2945
Training Epoch: 2 [12800/50048]	Loss: 3.4970
Training Epoch: 2 [12928/50048]	Loss: 3.2350
Training Epoch: 2 [13056/50048]	Loss: 3.2695
Training Epoch: 2 [13184/50048]	Loss: 3.3424
Training Epoch: 2 [13312/50048]	Loss: 3.2278
Training Epoch: 2 [13440/50048]	Loss: 3.1950
Training Epoch: 2 [13568/50048]	Loss: 2.9754
Training Epoch: 2 [13696/50048]	Loss: 3.2606
Training Epoch: 2 [13824/50048]	Loss: 3.3771
Training Epoch: 2 [13952/50048]	Loss: 3.1835
Training Epoch: 2 [14080/50048]	Loss: 3.3267
Training Epoch: 2 [14208/50048]	Loss: 3.2606
Training Epoch: 2 [14336/50048]	Loss: 3.1671
Training Epoch: 2 [14464/50048]	Loss: 3.3237
Training Epoch: 2 [14592/50048]	Loss: 2.9736
Training Epoch: 2 [14720/50048]	Loss: 3.4295
Training Epoch: 2 [14848/50048]	Loss: 3.4095
Training Epoch: 2 [14976/50048]	Loss: 3.2828
Training Epoch: 2 [15104/50048]	Loss: 3.1631
Training Epoch: 2 [15232/50048]	Loss: 3.0836
Training Epoch: 2 [15360/50048]	Loss: 3.1486
Training Epoch: 2 [15488/50048]	Loss: 3.2075
Training Epoch: 2 [15616/50048]	Loss: 3.4122
Training Epoch: 2 [15744/50048]	Loss: 3.1266
Training Epoch: 2 [15872/50048]	Loss: 3.4790
Training Epoch: 2 [16000/50048]	Loss: 3.0526
Training Epoch: 2 [16128/50048]	Loss: 3.2602
Training Epoch: 2 [16256/50048]	Loss: 3.5765
Training Epoch: 2 [16384/50048]	Loss: 3.2756
Training Epoch: 2 [16512/50048]	Loss: 3.0788
Training Epoch: 2 [16640/50048]	Loss: 2.9797
Training Epoch: 2 [16768/50048]	Loss: 3.3238
Training Epoch: 2 [16896/50048]	Loss: 3.1983
Training Epoch: 2 [17024/50048]	Loss: 3.0624
Training Epoch: 2 [17152/50048]	Loss: 3.3011
Training Epoch: 2 [17280/50048]	Loss: 3.2123
Training Epoch: 2 [17408/50048]	Loss: 3.2251
Training Epoch: 2 [17536/50048]	Loss: 3.2170
Training Epoch: 2 [17664/50048]	Loss: 2.8695
Training Epoch: 2 [17792/50048]	Loss: 3.1197
Training Epoch: 2 [17920/50048]	Loss: 3.3979
Training Epoch: 2 [18048/50048]	Loss: 3.2433
Training Epoch: 2 [18176/50048]	Loss: 3.3444
Training Epoch: 2 [18304/50048]	Loss: 3.1747
Training Epoch: 2 [18432/50048]	Loss: 3.3759
Training Epoch: 2 [18560/50048]	Loss: 3.0077
Training Epoch: 2 [18688/50048]	Loss: 3.2428
Training Epoch: 2 [18816/50048]	Loss: 3.2338
Training Epoch: 2 [18944/50048]	Loss: 3.3683
Training Epoch: 2 [19072/50048]	Loss: 3.2604
Training Epoch: 2 [19200/50048]	Loss: 3.2140
Training Epoch: 2 [19328/50048]	Loss: 2.9162
Training Epoch: 2 [19456/50048]	Loss: 3.4164
Training Epoch: 2 [19584/50048]	Loss: 3.4297
Training Epoch: 2 [19712/50048]	Loss: 3.4080
Training Epoch: 2 [19840/50048]	Loss: 2.9613
Training Epoch: 2 [19968/50048]	Loss: 3.2222
Training Epoch: 2 [20096/50048]	Loss: 3.4152
Training Epoch: 2 [20224/50048]	Loss: 3.4653
Training Epoch: 2 [20352/50048]	Loss: 3.2964
Training Epoch: 2 [20480/50048]	Loss: 3.3427
Training Epoch: 2 [20608/50048]	Loss: 3.3031
Training Epoch: 2 [20736/50048]	Loss: 3.3951
Training Epoch: 2 [20864/50048]	Loss: 3.2383
Training Epoch: 2 [20992/50048]	Loss: 3.1556
Training Epoch: 2 [21120/50048]	Loss: 3.1535
Training Epoch: 2 [21248/50048]	Loss: 3.1981
Training Epoch: 2 [21376/50048]	Loss: 3.2431
Training Epoch: 2 [21504/50048]	Loss: 3.1300
Training Epoch: 2 [21632/50048]	Loss: 3.0248
Training Epoch: 2 [21760/50048]	Loss: 3.1026
Training Epoch: 2 [21888/50048]	Loss: 3.2947
Training Epoch: 2 [22016/50048]	Loss: 3.1434
Training Epoch: 2 [22144/50048]	Loss: 3.2228
Training Epoch: 2 [22272/50048]	Loss: 3.2366
Training Epoch: 2 [22400/50048]	Loss: 3.1129
Training Epoch: 2 [22528/50048]	Loss: 2.9657
Training Epoch: 2 [22656/50048]	Loss: 3.1750
Training Epoch: 2 [22784/50048]	Loss: 3.0881
Training Epoch: 2 [22912/50048]	Loss: 3.1843
Training Epoch: 2 [23040/50048]	Loss: 3.0514
Training Epoch: 2 [23168/50048]	Loss: 3.1635
Training Epoch: 2 [23296/50048]	Loss: 3.1461
Training Epoch: 2 [23424/50048]	Loss: 3.1672
Training Epoch: 2 [23552/50048]	Loss: 3.1112
Training Epoch: 2 [23680/50048]	Loss: 3.0496
Training Epoch: 2 [23808/50048]	Loss: 3.0926
Training Epoch: 2 [23936/50048]	Loss: 3.2161
Training Epoch: 2 [24064/50048]	Loss: 3.1272
Training Epoch: 2 [24192/50048]	Loss: 2.9857
Training Epoch: 2 [24320/50048]	Loss: 3.1598
Training Epoch: 2 [24448/50048]	Loss: 3.1089
Training Epoch: 2 [24576/50048]	Loss: 3.1491
Training Epoch: 2 [24704/50048]	Loss: 2.9799
Training Epoch: 2 [24832/50048]	Loss: 2.7709
Training Epoch: 2 [24960/50048]	Loss: 3.0569
Training Epoch: 2 [25088/50048]	Loss: 3.1365
Training Epoch: 2 [25216/50048]	Loss: 3.2122
Training Epoch: 2 [25344/50048]	Loss: 3.1847
Training Epoch: 2 [25472/50048]	Loss: 3.0104
Training Epoch: 2 [25600/50048]	Loss: 3.2645
Training Epoch: 2 [25728/50048]	Loss: 3.1606
Training Epoch: 2 [25856/50048]	Loss: 3.2042
Training Epoch: 2 [25984/50048]	Loss: 3.4696
Training Epoch: 2 [26112/50048]	Loss: 3.2819
Training Epoch: 2 [26240/50048]	Loss: 3.2246
Training Epoch: 2 [26368/50048]	Loss: 3.1333
Training Epoch: 2 [26496/50048]	Loss: 3.0510
Training Epoch: 2 [26624/50048]	Loss: 3.2231
Training Epoch: 2 [26752/50048]	Loss: 3.1177
Training Epoch: 2 [26880/50048]	Loss: 3.2259
Training Epoch: 2 [27008/50048]	Loss: 3.1694
Training Epoch: 2 [27136/50048]	Loss: 3.1695
Training Epoch: 2 [27264/50048]	Loss: 3.2318
Training Epoch: 2 [27392/50048]	Loss: 3.3835
Training Epoch: 2 [27520/50048]	Loss: 3.2728
Training Epoch: 2 [27648/50048]	Loss: 3.1646
Training Epoch: 2 [27776/50048]	Loss: 3.1665
Training Epoch: 2 [27904/50048]	Loss: 3.2867
Training Epoch: 2 [28032/50048]	Loss: 3.2370
Training Epoch: 2 [28160/50048]	Loss: 3.0966
Training Epoch: 2 [28288/50048]	Loss: 3.1313
Training Epoch: 2 [28416/50048]	Loss: 3.0560
Training Epoch: 2 [28544/50048]	Loss: 3.0840
Training Epoch: 2 [28672/50048]	Loss: 3.1764
Training Epoch: 2 [28800/50048]	Loss: 2.9709
Training Epoch: 2 [28928/50048]	Loss: 2.8859
Training Epoch: 2 [29056/50048]	Loss: 3.2702
Training Epoch: 2 [29184/50048]	Loss: 3.0728
Training Epoch: 2 [29312/50048]	Loss: 3.3276
Training Epoch: 2 [29440/50048]	Loss: 2.9241
Training Epoch: 2 [29568/50048]	Loss: 3.1151
Training Epoch: 2 [29696/50048]	Loss: 3.1479
Training Epoch: 2 [29824/50048]	Loss: 3.1528
Training Epoch: 2 [29952/50048]	Loss: 3.2792
Training Epoch: 2 [30080/50048]	Loss: 3.3193
Training Epoch: 2 [30208/50048]	Loss: 3.3207
Training Epoch: 2 [30336/50048]	Loss: 3.1663
Training Epoch: 2 [30464/50048]	Loss: 3.2993
Training Epoch: 2 [30592/50048]	Loss: 3.3101
Training Epoch: 2 [30720/50048]	Loss: 2.9581
Training Epoch: 2 [30848/50048]	Loss: 3.4060
Training Epoch: 2 [30976/50048]	Loss: 3.0170
Training Epoch: 2 [31104/50048]	Loss: 3.1185
Training Epoch: 2 [31232/50048]	Loss: 3.0511
Training Epoch: 2 [31360/50048]	Loss: 3.1521
Training Epoch: 2 [31488/50048]	Loss: 3.0807
Training Epoch: 2 [31616/50048]	Loss: 3.1133
Training Epoch: 2 [31744/50048]	Loss: 2.9125
Training Epoch: 2 [31872/50048]	Loss: 2.9820
Training Epoch: 2 [32000/50048]	Loss: 3.2362
Training Epoch: 2 [32128/50048]	Loss: 3.1091
Training Epoch: 2 [32256/50048]	Loss: 2.8233
Training Epoch: 2 [32384/50048]	Loss: 3.0477
Training Epoch: 2 [32512/50048]	Loss: 2.9140
Training Epoch: 2 [32640/50048]	Loss: 2.9374
Training Epoch: 2 [32768/50048]	Loss: 3.1825
Training Epoch: 2 [32896/50048]	Loss: 2.9670
Training Epoch: 2 [33024/50048]	Loss: 3.0528
Training Epoch: 2 [33152/50048]	Loss: 2.9125
Training Epoch: 2 [33280/50048]	Loss: 3.0208
Training Epoch: 2 [33408/50048]	Loss: 2.9221
Training Epoch: 2 [33536/50048]	Loss: 3.2883
Training Epoch: 2 [33664/50048]	Loss: 3.1383
Training Epoch: 2 [33792/50048]	Loss: 3.0739
Training Epoch: 2 [33920/50048]	Loss: 3.0859
Training Epoch: 2 [34048/50048]	Loss: 3.0963
Training Epoch: 2 [34176/50048]	Loss: 3.0539
Training Epoch: 2 [34304/50048]	Loss: 2.7458
Training Epoch: 2 [34432/50048]	Loss: 2.9750
Training Epoch: 2 [34560/50048]	Loss: 3.0071
Training Epoch: 2 [34688/50048]	Loss: 2.9974
Training Epoch: 2 [34816/50048]	Loss: 3.3842
Training Epoch: 2 [34944/50048]	Loss: 2.7884
Training Epoch: 2 [35072/50048]	Loss: 3.1758
Training Epoch: 2 [35200/50048]	Loss: 3.0357
Training Epoch: 2 [35328/50048]	Loss: 3.0698
Training Epoch: 2 [35456/50048]	Loss: 3.0536
Training Epoch: 2 [35584/50048]	Loss: 2.9503
Training Epoch: 2 [35712/50048]	Loss: 3.1212
Training Epoch: 2 [35840/50048]	Loss: 3.1838
Training Epoch: 2 [35968/50048]	Loss: 2.9077
Training Epoch: 2 [36096/50048]	Loss: 2.9027
Training Epoch: 2 [36224/50048]	Loss: 3.2783
Training Epoch: 2 [36352/50048]	Loss: 3.2351
Training Epoch: 2 [36480/50048]	Loss: 3.0117
Training Epoch: 2 [36608/50048]	Loss: 3.1320
Training Epoch: 2 [36736/50048]	Loss: 3.1030
Training Epoch: 2 [36864/50048]	Loss: 3.2895
Training Epoch: 2 [36992/50048]	Loss: 3.0661
Training Epoch: 2 [37120/50048]	Loss: 2.8873
Training Epoch: 2 [37248/50048]	Loss: 3.2187
Training Epoch: 2 [37376/50048]	Loss: 3.1303
Training Epoch: 2 [37504/50048]	Loss: 3.1392
Training Epoch: 2 [37632/50048]	Loss: 3.1795
Training Epoch: 2 [37760/50048]	Loss: 3.1491
Training Epoch: 2 [37888/50048]	Loss: 2.9646
Training Epoch: 2 [38016/50048]	Loss: 2.7780
Training Epoch: 2 [38144/50048]	Loss: 3.0081
Training Epoch: 2 [38272/50048]	Loss: 3.1519
Training Epoch: 2 [38400/50048]	Loss: 2.9074
Training Epoch: 2 [38528/50048]	Loss: 2.8574
Training Epoch: 2 [38656/50048]	Loss: 2.9860
Training Epoch: 2 [38784/50048]	Loss: 3.0188
Training Epoch: 2 [38912/50048]	Loss: 3.2743
Training Epoch: 2 [39040/50048]	Loss: 3.1496
Training Epoch: 2 [39168/50048]	Loss: 3.1503
Training Epoch: 2 [39296/50048]	Loss: 3.2033
Training Epoch: 2 [39424/50048]	Loss: 2.9869
Training Epoch: 2 [39552/50048]	Loss: 3.0629
Training Epoch: 2 [39680/50048]	Loss: 3.0203
Training Epoch: 2 [39808/50048]	Loss: 3.2682
Training Epoch: 2 [39936/50048]	Loss: 2.8410
Training Epoch: 2 [40064/50048]	Loss: 2.9314
Training Epoch: 2 [40192/50048]	Loss: 2.8496
Training Epoch: 2 [40320/50048]	Loss: 3.4056
Training Epoch: 2 [40448/50048]	Loss: 3.1364
Training Epoch: 2 [40576/50048]	Loss: 3.1155
Training Epoch: 2 [40704/50048]	Loss: 3.1667
Training Epoch: 2 [40832/50048]	Loss: 3.1813
Training Epoch: 2 [40960/50048]	Loss: 2.9922
Training Epoch: 2 [41088/50048]	Loss: 2.8551
Training Epoch: 2 [41216/50048]	Loss: 3.0716
Training Epoch: 2 [41344/50048]	Loss: 2.9753
Training Epoch: 2 [41472/50048]	Loss: 3.0803
Training Epoch: 2 [41600/50048]	Loss: 2.9967
Training Epoch: 2 [41728/50048]	Loss: 2.9322
Training Epoch: 2 [41856/50048]	Loss: 3.2224
Training Epoch: 2 [41984/50048]	Loss: 3.1400
Training Epoch: 2 [42112/50048]	Loss: 3.2768
Training Epoch: 2 [42240/50048]	Loss: 2.9149
Training Epoch: 2 [42368/50048]	Loss: 2.9218
Training Epoch: 2 [42496/50048]	Loss: 3.0566
Training Epoch: 2 [42624/50048]	Loss: 3.0560
Training Epoch: 2 [42752/50048]	Loss: 2.9156
Training Epoch: 2 [42880/50048]	Loss: 3.5384
Training Epoch: 2 [43008/50048]	Loss: 3.0429
Training Epoch: 2 [43136/50048]	Loss: 3.0791
Training Epoch: 2 [43264/50048]	Loss: 3.1541
Training Epoch: 2 [43392/50048]	Loss: 3.1054
Training Epoch: 2 [43520/50048]	Loss: 3.1768
Training Epoch: 2 [43648/50048]	Loss: 2.9283
Training Epoch: 2 [43776/50048]	Loss: 2.8543
Training Epoch: 2 [43904/50048]	Loss: 2.9757
Training Epoch: 2 [44032/50048]	Loss: 2.9603
Training Epoch: 2 [44160/50048]	Loss: 3.1492
Training Epoch: 2 [44288/50048]	Loss: 3.2504
Training Epoch: 2 [44416/50048]	Loss: 3.1268
Training Epoch: 2 [44544/50048]	Loss: 3.1229
Training Epoch: 2 [44672/50048]	Loss: 2.8229
Training Epoch: 2 [44800/50048]	Loss: 3.0677
Training Epoch: 2 [44928/50048]	Loss: 2.8274
Training Epoch: 2 [45056/50048]	Loss: 3.0083
Training Epoch: 2 [45184/50048]	Loss: 3.0827
Training Epoch: 2 [45312/50048]	Loss: 3.1670
Training Epoch: 2 [45440/50048]	Loss: 3.1113
Training Epoch: 2 [45568/50048]	Loss: 2.9925
Training Epoch: 2 [45696/50048]	Loss: 3.1341
Training Epoch: 2 [45824/50048]	Loss: 3.0490
Training Epoch: 2 [45952/50048]	Loss: 2.8596
Training Epoch: 2 [46080/50048]	Loss: 2.8502
Training Epoch: 2 [46208/50048]	Loss: 3.1137
Training Epoch: 2 [46336/50048]	Loss: 2.9905
Training Epoch: 2 [46464/50048]	Loss: 2.8868
Training Epoch: 2 [46592/50048]	Loss: 3.0062
Training Epoch: 2 [46720/50048]	Loss: 3.2065
Training Epoch: 2 [46848/50048]	Loss: 3.2079
Training Epoch: 2 [46976/50048]	Loss: 3.2876
Training Epoch: 2 [47104/50048]	Loss: 2.8919
Training Epoch: 2 [47232/50048]	Loss: 3.1282
Training Epoch: 2 [47360/50048]	Loss: 3.0254
Training Epoch: 2 [47488/50048]	Loss: 3.0759
Training Epoch: 2 [47616/50048]	Loss: 3.1363
Training Epoch: 2 [47744/50048]	Loss: 2.9764
Training Epoch: 2 [47872/50048]	Loss: 3.1207
Training Epoch: 2 [48000/50048]	Loss: 3.0177
Training Epoch: 2 [48128/50048]	Loss: 3.1752
Training Epoch: 2 [48256/50048]	Loss: 2.9173
Training Epoch: 2 [48384/50048]	Loss: 3.0501
Training Epoch: 2 [48512/50048]	Loss: 2.7247
Training Epoch: 2 [48640/50048]	Loss: 2.7921
Training Epoch: 2 [48768/50048]	Loss: 3.0672
Training Epoch: 2 [48896/50048]	Loss: 2.8619
Training Epoch: 2 [49024/50048]	Loss: 3.1501
Training Epoch: 2 [49152/50048]	Loss: 2.9027
Training Epoch: 2 [49280/50048]	Loss: 2.5460
Training Epoch: 2 [49408/50048]	Loss: 2.9115
Training Epoch: 2 [49536/50048]	Loss: 2.9804
Training Epoch: 2 [49664/50048]	Loss: 2.8916
Training Epoch: 2 [49792/50048]	Loss: 3.1030
Training Epoch: 2 [49920/50048]	Loss: 2.8528
Training Epoch: 2 [50048/50048]	Loss: 2.7239
Validation Epoch: 2, Average loss: 0.0242, Accuracy: 0.2412
Training Epoch: 3 [128/50048]	Loss: 2.8494
Training Epoch: 3 [256/50048]	Loss: 2.8948
Training Epoch: 3 [384/50048]	Loss: 2.9372
Training Epoch: 3 [512/50048]	Loss: 2.9385
Training Epoch: 3 [640/50048]	Loss: 2.9699
Training Epoch: 3 [768/50048]	Loss: 3.0119
Training Epoch: 3 [896/50048]	Loss: 2.8482
Training Epoch: 3 [1024/50048]	Loss: 2.9589
Training Epoch: 3 [1152/50048]	Loss: 2.9088
Training Epoch: 3 [1280/50048]	Loss: 2.6786
Training Epoch: 3 [1408/50048]	Loss: 2.7485
Training Epoch: 3 [1536/50048]	Loss: 2.9016
Training Epoch: 3 [1664/50048]	Loss: 2.9598
Training Epoch: 3 [1792/50048]	Loss: 2.9328
Training Epoch: 3 [1920/50048]	Loss: 3.0235
Training Epoch: 3 [2048/50048]	Loss: 2.8961
Training Epoch: 3 [2176/50048]	Loss: 2.7817
Training Epoch: 3 [2304/50048]	Loss: 2.5957
Training Epoch: 3 [2432/50048]	Loss: 3.1572
Training Epoch: 3 [2560/50048]	Loss: 3.2016
Training Epoch: 3 [2688/50048]	Loss: 3.1433
Training Epoch: 3 [2816/50048]	Loss: 2.8541
Training Epoch: 3 [2944/50048]	Loss: 2.7641
Training Epoch: 3 [3072/50048]	Loss: 2.6340
Training Epoch: 3 [3200/50048]	Loss: 2.8851
Training Epoch: 3 [3328/50048]	Loss: 2.8862
Training Epoch: 3 [3456/50048]	Loss: 2.9517
Training Epoch: 3 [3584/50048]	Loss: 2.9727
Training Epoch: 3 [3712/50048]	Loss: 2.7319
Training Epoch: 3 [3840/50048]	Loss: 2.7003
Training Epoch: 3 [3968/50048]	Loss: 2.6535
Training Epoch: 3 [4096/50048]	Loss: 3.0383
Training Epoch: 3 [4224/50048]	Loss: 2.8755
Training Epoch: 3 [4352/50048]	Loss: 2.5984
Training Epoch: 3 [4480/50048]	Loss: 2.7752
Training Epoch: 3 [4608/50048]	Loss: 2.6514
Training Epoch: 3 [4736/50048]	Loss: 2.8749
Training Epoch: 3 [4864/50048]	Loss: 2.7038
Training Epoch: 3 [4992/50048]	Loss: 3.0376
Training Epoch: 3 [5120/50048]	Loss: 2.4950
Training Epoch: 3 [5248/50048]	Loss: 3.0325
Training Epoch: 3 [5376/50048]	Loss: 2.7614
Training Epoch: 3 [5504/50048]	Loss: 2.7595
Training Epoch: 3 [5632/50048]	Loss: 2.6802
Training Epoch: 3 [5760/50048]	Loss: 2.8435
Training Epoch: 3 [5888/50048]	Loss: 2.7398
Training Epoch: 3 [6016/50048]	Loss: 2.7160
Training Epoch: 3 [6144/50048]	Loss: 2.7143
Training Epoch: 3 [6272/50048]	Loss: 2.9210
Training Epoch: 3 [6400/50048]	Loss: 2.8679
Training Epoch: 3 [6528/50048]	Loss: 2.7327
Training Epoch: 3 [6656/50048]	Loss: 2.6798
Training Epoch: 3 [6784/50048]	Loss: 2.8385
Training Epoch: 3 [6912/50048]	Loss: 2.8084
Training Epoch: 3 [7040/50048]	Loss: 2.9276
Training Epoch: 3 [7168/50048]	Loss: 2.8900
Training Epoch: 3 [7296/50048]	Loss: 3.0606
Training Epoch: 3 [7424/50048]	Loss: 2.7489
Training Epoch: 3 [7552/50048]	Loss: 2.7410
Training Epoch: 3 [7680/50048]	Loss: 2.8123
Training Epoch: 3 [7808/50048]	Loss: 2.7546
Training Epoch: 3 [7936/50048]	Loss: 3.1309
Training Epoch: 3 [8064/50048]	Loss: 2.8171
Training Epoch: 3 [8192/50048]	Loss: 2.7647
Training Epoch: 3 [8320/50048]	Loss: 3.0690
Training Epoch: 3 [8448/50048]	Loss: 3.0258
Training Epoch: 3 [8576/50048]	Loss: 2.7632
Training Epoch: 3 [8704/50048]	Loss: 2.5713
Training Epoch: 3 [8832/50048]	Loss: 2.8963
Training Epoch: 3 [8960/50048]	Loss: 2.8894
Training Epoch: 3 [9088/50048]	Loss: 3.0531
Training Epoch: 3 [9216/50048]	Loss: 2.9926
Training Epoch: 3 [9344/50048]	Loss: 2.7600
Training Epoch: 3 [9472/50048]	Loss: 2.9324
Training Epoch: 3 [9600/50048]	Loss: 2.9098
Training Epoch: 3 [9728/50048]	Loss: 2.9796
Training Epoch: 3 [9856/50048]	Loss: 2.7196
Training Epoch: 3 [9984/50048]	Loss: 3.0252
Training Epoch: 3 [10112/50048]	Loss: 3.1568
Training Epoch: 3 [10240/50048]	Loss: 2.8773
Training Epoch: 3 [10368/50048]	Loss: 2.6709
Training Epoch: 3 [10496/50048]	Loss: 2.7039
Training Epoch: 3 [10624/50048]	Loss: 2.6495
Training Epoch: 3 [10752/50048]	Loss: 2.9877
Training Epoch: 3 [10880/50048]	Loss: 2.9734
Training Epoch: 3 [11008/50048]	Loss: 2.7314
Training Epoch: 3 [11136/50048]	Loss: 2.8755
Training Epoch: 3 [11264/50048]	Loss: 2.8017
Training Epoch: 3 [11392/50048]	Loss: 2.9868
Training Epoch: 3 [11520/50048]	Loss: 3.0983
Training Epoch: 3 [11648/50048]	Loss: 2.8665
Training Epoch: 3 [11776/50048]	Loss: 2.7622
Training Epoch: 3 [11904/50048]	Loss: 2.8444
Training Epoch: 3 [12032/50048]	Loss: 2.9279
Training Epoch: 3 [12160/50048]	Loss: 3.0503
Training Epoch: 3 [12288/50048]	Loss: 2.9816
Training Epoch: 3 [12416/50048]	Loss: 3.0337
Training Epoch: 3 [12544/50048]	Loss: 2.8367
Training Epoch: 3 [12672/50048]	Loss: 2.5772
Training Epoch: 3 [12800/50048]	Loss: 2.8651
Training Epoch: 3 [12928/50048]	Loss: 2.7286
Training Epoch: 3 [13056/50048]	Loss: 2.9039
Training Epoch: 3 [13184/50048]	Loss: 2.9905
Training Epoch: 3 [13312/50048]	Loss: 2.7480
Training Epoch: 3 [13440/50048]	Loss: 2.8885
Training Epoch: 3 [13568/50048]	Loss: 3.0168
Training Epoch: 3 [13696/50048]	Loss: 2.8469
Training Epoch: 3 [13824/50048]	Loss: 2.7820
Training Epoch: 3 [13952/50048]	Loss: 2.8775
Training Epoch: 3 [14080/50048]	Loss: 2.6933
Training Epoch: 3 [14208/50048]	Loss: 2.7980
Training Epoch: 3 [14336/50048]	Loss: 2.6686
Training Epoch: 3 [14464/50048]	Loss: 2.9649
Training Epoch: 3 [14592/50048]	Loss: 2.6924
Training Epoch: 3 [14720/50048]	Loss: 2.7269
Training Epoch: 3 [14848/50048]	Loss: 2.8593
Training Epoch: 3 [14976/50048]	Loss: 2.9661
Training Epoch: 3 [15104/50048]	Loss: 3.0072
Training Epoch: 3 [15232/50048]	Loss: 2.9606
Training Epoch: 3 [15360/50048]	Loss: 2.8097
Training Epoch: 3 [15488/50048]	Loss: 2.8984
Training Epoch: 3 [15616/50048]	Loss: 2.9042
Training Epoch: 3 [15744/50048]	Loss: 2.8913
Training Epoch: 3 [15872/50048]	Loss: 2.9546
Training Epoch: 3 [16000/50048]	Loss: 2.8456
Training Epoch: 3 [16128/50048]	Loss: 2.7247
Training Epoch: 3 [16256/50048]	Loss: 2.7989
Training Epoch: 3 [16384/50048]	Loss: 2.5872
Training Epoch: 3 [16512/50048]	Loss: 2.8800
Training Epoch: 3 [16640/50048]	Loss: 2.9314
Training Epoch: 3 [16768/50048]	Loss: 2.8055
Training Epoch: 3 [16896/50048]	Loss: 2.6702
Training Epoch: 3 [17024/50048]	Loss: 2.9393
Training Epoch: 3 [17152/50048]	Loss: 2.9407
Training Epoch: 3 [17280/50048]	Loss: 3.0150
Training Epoch: 3 [17408/50048]	Loss: 2.9524
Training Epoch: 3 [17536/50048]	Loss: 2.8269
Training Epoch: 3 [17664/50048]	Loss: 2.9170
Training Epoch: 3 [17792/50048]	Loss: 2.5564
Training Epoch: 3 [17920/50048]	Loss: 2.6087
Training Epoch: 3 [18048/50048]	Loss: 2.8490
Training Epoch: 3 [18176/50048]	Loss: 2.7202
Training Epoch: 3 [18304/50048]	Loss: 2.9511
Training Epoch: 3 [18432/50048]	Loss: 2.8508
Training Epoch: 3 [18560/50048]	Loss: 2.8232
Training Epoch: 3 [18688/50048]	Loss: 2.7102
Training Epoch: 3 [18816/50048]	Loss: 2.6511
Training Epoch: 3 [18944/50048]	Loss: 2.6074
Training Epoch: 3 [19072/50048]	Loss: 2.8413
Training Epoch: 3 [19200/50048]	Loss: 2.9544
Training Epoch: 3 [19328/50048]	Loss: 2.7519
Training Epoch: 3 [19456/50048]	Loss: 2.9256
Training Epoch: 3 [19584/50048]	Loss: 2.9254
Training Epoch: 3 [19712/50048]	Loss: 2.6146
Training Epoch: 3 [19840/50048]	Loss: 2.9521
Training Epoch: 3 [19968/50048]	Loss: 2.8839
Training Epoch: 3 [20096/50048]	Loss: 2.7835
Training Epoch: 3 [20224/50048]	Loss: 2.8682
Training Epoch: 3 [20352/50048]	Loss: 2.9209
Training Epoch: 3 [20480/50048]	Loss: 2.8883
Training Epoch: 3 [20608/50048]	Loss: 2.9089
Training Epoch: 3 [20736/50048]	Loss: 2.8239
Training Epoch: 3 [20864/50048]	Loss: 2.7235
Training Epoch: 3 [20992/50048]	Loss: 2.8272
Training Epoch: 3 [21120/50048]	Loss: 2.9079
Training Epoch: 3 [21248/50048]	Loss: 2.9001
Training Epoch: 3 [21376/50048]	Loss: 3.0210
Training Epoch: 3 [21504/50048]	Loss: 2.7571
Training Epoch: 3 [21632/50048]	Loss: 3.0107
Training Epoch: 3 [21760/50048]	Loss: 2.7103
Training Epoch: 3 [21888/50048]	Loss: 2.8189
Training Epoch: 3 [22016/50048]	Loss: 2.9296
Training Epoch: 3 [22144/50048]	Loss: 2.6447
Training Epoch: 3 [22272/50048]	Loss: 2.7519
Training Epoch: 3 [22400/50048]	Loss: 2.8970
Training Epoch: 3 [22528/50048]	Loss: 2.8070
Training Epoch: 3 [22656/50048]	Loss: 2.7648
Training Epoch: 3 [22784/50048]	Loss: 2.9672
Training Epoch: 3 [22912/50048]	Loss: 3.0305
Training Epoch: 3 [23040/50048]	Loss: 2.7497
Training Epoch: 3 [23168/50048]	Loss: 2.7564
Training Epoch: 3 [23296/50048]	Loss: 2.7188
Training Epoch: 3 [23424/50048]	Loss: 2.8921
Training Epoch: 3 [23552/50048]	Loss: 2.8154
Training Epoch: 3 [23680/50048]	Loss: 2.5620
Training Epoch: 3 [23808/50048]	Loss: 2.9767
Training Epoch: 3 [23936/50048]	Loss: 2.8477
Training Epoch: 3 [24064/50048]	Loss: 2.8783
Training Epoch: 3 [24192/50048]	Loss: 2.7826
Training Epoch: 3 [24320/50048]	Loss: 2.9760
Training Epoch: 3 [24448/50048]	Loss: 2.8698
Training Epoch: 3 [24576/50048]	Loss: 2.6708
Training Epoch: 3 [24704/50048]	Loss: 2.8706
Training Epoch: 3 [24832/50048]	Loss: 3.0809
Training Epoch: 3 [24960/50048]	Loss: 2.9449
Training Epoch: 3 [25088/50048]	Loss: 2.8592
Training Epoch: 3 [25216/50048]	Loss: 2.7330
Training Epoch: 3 [25344/50048]	Loss: 2.9181
Training Epoch: 3 [25472/50048]	Loss: 2.6282
Training Epoch: 3 [25600/50048]	Loss: 2.8049
Training Epoch: 3 [25728/50048]	Loss: 2.5169
Training Epoch: 3 [25856/50048]	Loss: 2.8687
Training Epoch: 3 [25984/50048]	Loss: 3.0031
Training Epoch: 3 [26112/50048]	Loss: 3.0239
Training Epoch: 3 [26240/50048]	Loss: 2.7789
Training Epoch: 3 [26368/50048]	Loss: 2.7819
Training Epoch: 3 [26496/50048]	Loss: 2.9793
Training Epoch: 3 [26624/50048]	Loss: 2.8866
Training Epoch: 3 [26752/50048]	Loss: 2.9406
Training Epoch: 3 [26880/50048]	Loss: 2.7416
Training Epoch: 3 [27008/50048]	Loss: 2.8085
Training Epoch: 3 [27136/50048]	Loss: 2.8287
Training Epoch: 3 [27264/50048]	Loss: 2.6890
Training Epoch: 3 [27392/50048]	Loss: 2.7695
Training Epoch: 3 [27520/50048]	Loss: 2.7048
Training Epoch: 3 [27648/50048]	Loss: 2.7175
Training Epoch: 3 [27776/50048]	Loss: 2.8080
Training Epoch: 3 [27904/50048]	Loss: 2.8702
Training Epoch: 3 [28032/50048]	Loss: 2.7465
Training Epoch: 3 [28160/50048]	Loss: 2.6156
Training Epoch: 3 [28288/50048]	Loss: 2.7613
Training Epoch: 3 [28416/50048]	Loss: 2.7064
Training Epoch: 3 [28544/50048]	Loss: 2.8036
Training Epoch: 3 [28672/50048]	Loss: 3.1240
Training Epoch: 3 [28800/50048]	Loss: 2.9114
Training Epoch: 3 [28928/50048]	Loss: 2.7431
Training Epoch: 3 [29056/50048]	Loss: 2.8248
Training Epoch: 3 [29184/50048]	Loss: 2.9510
Training Epoch: 3 [29312/50048]	Loss: 2.8596
Training Epoch: 3 [29440/50048]	Loss: 2.6741
Training Epoch: 3 [29568/50048]	Loss: 3.0899
Training Epoch: 3 [29696/50048]	Loss: 2.7675
Training Epoch: 3 [29824/50048]	Loss: 2.7846
Training Epoch: 3 [29952/50048]	Loss: 2.8754
Training Epoch: 3 [30080/50048]	Loss: 2.6066
Training Epoch: 3 [30208/50048]	Loss: 2.8720
Training Epoch: 3 [30336/50048]	Loss: 2.8334
Training Epoch: 3 [30464/50048]	Loss: 2.7587
Training Epoch: 3 [30592/50048]	Loss: 2.7303
Training Epoch: 3 [30720/50048]	Loss: 2.9634
Training Epoch: 3 [30848/50048]	Loss: 2.8240
Training Epoch: 3 [30976/50048]	Loss: 2.7823
Training Epoch: 3 [31104/50048]	Loss: 2.6439
Training Epoch: 3 [31232/50048]	Loss: 2.9146
Training Epoch: 3 [31360/50048]	Loss: 2.8710
Training Epoch: 3 [31488/50048]	Loss: 2.9060
Training Epoch: 3 [31616/50048]	Loss: 2.6324
Training Epoch: 3 [31744/50048]	Loss: 2.5960
Training Epoch: 3 [31872/50048]	Loss: 2.8371
Training Epoch: 3 [32000/50048]	Loss: 2.6957
Training Epoch: 3 [32128/50048]	Loss: 2.7286
Training Epoch: 3 [32256/50048]	Loss: 2.8978
Training Epoch: 3 [32384/50048]	Loss: 2.9283
Training Epoch: 3 [32512/50048]	Loss: 2.6685
Training Epoch: 3 [32640/50048]	Loss: 2.8676
Training Epoch: 3 [32768/50048]	Loss: 2.7302
Training Epoch: 3 [32896/50048]	Loss: 2.9924
Training Epoch: 3 [33024/50048]	Loss: 2.8952
Training Epoch: 3 [33152/50048]	Loss: 2.7880
Training Epoch: 3 [33280/50048]	Loss: 2.7113
Training Epoch: 3 [33408/50048]	Loss: 2.8409
Training Epoch: 3 [33536/50048]	Loss: 2.7177
Training Epoch: 3 [33664/50048]	Loss: 3.0106
Training Epoch: 3 [33792/50048]	Loss: 3.0044
Training Epoch: 3 [33920/50048]	Loss: 2.9483
Training Epoch: 3 [34048/50048]	Loss: 2.8420
Training Epoch: 3 [34176/50048]	Loss: 2.8514
Training Epoch: 3 [34304/50048]	Loss: 2.7980
Training Epoch: 3 [34432/50048]	Loss: 2.7836
Training Epoch: 3 [34560/50048]	Loss: 3.0396
Training Epoch: 3 [34688/50048]	Loss: 2.7867
Training Epoch: 3 [34816/50048]	Loss: 2.7169
Training Epoch: 3 [34944/50048]	Loss: 2.5678
Training Epoch: 3 [35072/50048]	Loss: 2.8708
Training Epoch: 3 [35200/50048]	Loss: 2.3423
Training Epoch: 3 [35328/50048]	Loss: 2.7199
Training Epoch: 3 [35456/50048]	Loss: 2.8577
Training Epoch: 3 [35584/50048]	Loss: 2.8835
Training Epoch: 3 [35712/50048]	Loss: 2.6555
Training Epoch: 3 [35840/50048]	Loss: 2.7008
Training Epoch: 3 [35968/50048]	Loss: 2.7214
Training Epoch: 3 [36096/50048]	Loss: 2.5103
Training Epoch: 3 [36224/50048]	Loss: 2.5565
Training Epoch: 3 [36352/50048]	Loss: 3.0621
Training Epoch: 3 [36480/50048]	Loss: 3.0790
Training Epoch: 3 [36608/50048]	Loss: 2.4553
Training Epoch: 3 [36736/50048]	Loss: 3.0768
Training Epoch: 3 [36864/50048]	Loss: 2.8497
Training Epoch: 3 [36992/50048]	Loss: 2.9164
Training Epoch: 3 [37120/50048]	Loss: 2.7828
Training Epoch: 3 [37248/50048]	Loss: 2.9971
Training Epoch: 3 [37376/50048]	Loss: 2.5868
Training Epoch: 3 [37504/50048]	Loss: 2.6641
Training Epoch: 3 [37632/50048]	Loss: 2.6427
Training Epoch: 3 [37760/50048]	Loss: 2.6084
Training Epoch: 3 [37888/50048]	Loss: 2.6357
Training Epoch: 3 [38016/50048]	Loss: 2.8361
Training Epoch: 3 [38144/50048]	Loss: 2.6604
Training Epoch: 3 [38272/50048]	Loss: 2.8675
Training Epoch: 3 [38400/50048]	Loss: 2.9060
Training Epoch: 3 [38528/50048]	Loss: 3.0251
Training Epoch: 3 [38656/50048]	Loss: 2.5592
Training Epoch: 3 [38784/50048]	Loss: 2.9920
Training Epoch: 3 [38912/50048]	Loss: 2.7483
Training Epoch: 3 [39040/50048]	Loss: 2.7141
Training Epoch: 3 [39168/50048]	Loss: 2.9499
Training Epoch: 3 [39296/50048]	Loss: 2.8451
Training Epoch: 3 [39424/50048]	Loss: 2.7421
Training Epoch: 3 [39552/50048]	Loss: 2.7461
Training Epoch: 3 [39680/50048]	Loss: 3.0038
Training Epoch: 3 [39808/50048]	Loss: 2.8766
Training Epoch: 3 [39936/50048]	Loss: 2.6470
Training Epoch: 3 [40064/50048]	Loss: 2.8354
Training Epoch: 3 [40192/50048]	Loss: 2.7948
Training Epoch: 3 [40320/50048]	Loss: 2.9402
Training Epoch: 3 [40448/50048]	Loss: 2.6371
Training Epoch: 3 [40576/50048]	Loss: 2.6341
Training Epoch: 3 [40704/50048]	Loss: 2.8104
Training Epoch: 3 [40832/50048]	Loss: 2.9710
Training Epoch: 3 [40960/50048]	Loss: 2.8715
Training Epoch: 3 [41088/50048]	Loss: 2.9009
Training Epoch: 3 [41216/50048]	Loss: 2.7941
Training Epoch: 3 [41344/50048]	Loss: 2.8030
Training Epoch: 3 [41472/50048]	Loss: 2.8617
Training Epoch: 3 [41600/50048]	Loss: 2.6471
Training Epoch: 3 [41728/50048]	Loss: 2.8039
Training Epoch: 3 [41856/50048]	Loss: 2.8222
Training Epoch: 3 [41984/50048]	Loss: 2.6321
Training Epoch: 3 [42112/50048]	Loss: 2.5173
Training Epoch: 3 [42240/50048]	Loss: 2.5000
Training Epoch: 3 [42368/50048]	Loss: 2.8945
Training Epoch: 3 [42496/50048]	Loss: 2.5401
Training Epoch: 3 [42624/50048]	Loss: 2.7792
Training Epoch: 3 [42752/50048]	Loss: 2.6724
Training Epoch: 3 [42880/50048]	Loss: 2.6418
Training Epoch: 3 [43008/50048]	Loss: 2.5247
Training Epoch: 3 [43136/50048]	Loss: 2.4847
Training Epoch: 3 [43264/50048]	Loss: 2.9378
Training Epoch: 3 [43392/50048]	Loss: 2.7798
Training Epoch: 3 [43520/50048]	Loss: 2.8037
Training Epoch: 3 [43648/50048]	Loss: 2.7811
Training Epoch: 3 [43776/50048]	Loss: 2.4938
Training Epoch: 3 [43904/50048]	Loss: 2.7484
Training Epoch: 3 [44032/50048]	Loss: 2.4868
Training Epoch: 3 [44160/50048]	Loss: 2.7058
Training Epoch: 3 [44288/50048]	Loss: 2.8713
Training Epoch: 3 [44416/50048]	Loss: 2.7545
Training Epoch: 3 [44544/50048]	Loss: 2.8402
Training Epoch: 3 [44672/50048]	Loss: 2.7420
Training Epoch: 3 [44800/50048]	Loss: 2.9026
Training Epoch: 3 [44928/50048]	Loss: 2.4478
Training Epoch: 3 [45056/50048]	Loss: 2.4392
Training Epoch: 3 [45184/50048]	Loss: 2.4663
Training Epoch: 3 [45312/50048]	Loss: 2.7271
Training Epoch: 3 [45440/50048]	Loss: 2.5727
Training Epoch: 3 [45568/50048]	Loss: 2.7074
Training Epoch: 3 [45696/50048]	Loss: 2.5917
Training Epoch: 3 [45824/50048]	Loss: 2.4481
Training Epoch: 3 [45952/50048]	Loss: 2.8156
Training Epoch: 3 [46080/50048]	Loss: 2.5627
Training Epoch: 3 [46208/50048]	Loss: 2.4317
Training Epoch: 3 [46336/50048]	Loss: 2.5852
Training Epoch: 3 [46464/50048]	Loss: 2.7132
Training Epoch: 3 [46592/50048]	Loss: 2.6161
Training Epoch: 3 [46720/50048]	Loss: 2.5863
Training Epoch: 3 [46848/50048]	Loss: 2.6216
Training Epoch: 3 [46976/50048]	Loss: 2.6689
Training Epoch: 3 [47104/50048]	Loss: 2.6248
Training Epoch: 3 [47232/50048]	Loss: 2.7193
Training Epoch: 3 [47360/50048]	Loss: 2.6073
Training Epoch: 3 [47488/50048]	Loss: 2.8518
Training Epoch: 3 [47616/50048]	Loss: 2.7535
Training Epoch: 3 [47744/50048]	Loss: 2.7197
Training Epoch: 3 [47872/50048]	Loss: 2.6711
Training Epoch: 3 [48000/50048]	Loss: 2.9301
Training Epoch: 3 [48128/50048]	Loss: 2.6287
Training Epoch: 3 [48256/50048]	Loss: 2.4175
Training Epoch: 3 [48384/50048]	Loss: 2.7028
Training Epoch: 3 [48512/50048]	Loss: 2.6243
Training Epoch: 3 [48640/50048]	Loss: 2.5812
Training Epoch: 3 [48768/50048]	Loss: 2.4975
Training Epoch: 3 [48896/50048]	Loss: 2.8664
Training Epoch: 3 [49024/50048]	Loss: 2.8725
Training Epoch: 3 [49152/50048]	Loss: 2.9953
Training Epoch: 3 [49280/50048]	Loss: 2.9987
Training Epoch: 3 [49408/50048]	Loss: 2.7125
Training Epoch: 3 [49536/50048]	Loss: 2.6416
Training Epoch: 3 [49664/50048]	Loss: 2.6242
Training Epoch: 3 [49792/50048]	Loss: 2.7912
Training Epoch: 3 [49920/50048]	Loss: 2.4777
Training Epoch: 3 [50048/50048]	Loss: 2.5701
Validation Epoch: 3, Average loss: 0.0220, Accuracy: 0.2899
Training Epoch: 4 [128/50048]	Loss: 2.7420
Training Epoch: 4 [256/50048]	Loss: 2.6455
Training Epoch: 4 [384/50048]	Loss: 2.5098
Training Epoch: 4 [512/50048]	Loss: 2.6297
Training Epoch: 4 [640/50048]	Loss: 2.3899
Training Epoch: 4 [768/50048]	Loss: 2.6490
Training Epoch: 4 [896/50048]	Loss: 3.0325
Training Epoch: 4 [1024/50048]	Loss: 2.7895
Training Epoch: 4 [1152/50048]	Loss: 2.7312
Training Epoch: 4 [1280/50048]	Loss: 2.5322
Training Epoch: 4 [1408/50048]	Loss: 2.7025
Training Epoch: 4 [1536/50048]	Loss: 2.4801
Training Epoch: 4 [1664/50048]	Loss: 2.5618
Training Epoch: 4 [1792/50048]	Loss: 2.6182
Training Epoch: 4 [1920/50048]	Loss: 2.3585
Training Epoch: 4 [2048/50048]	Loss: 2.4304
Training Epoch: 4 [2176/50048]	Loss: 2.7314
Training Epoch: 4 [2304/50048]	Loss: 2.5446
Training Epoch: 4 [2432/50048]	Loss: 2.5714
Training Epoch: 4 [2560/50048]	Loss: 2.5090
Training Epoch: 4 [2688/50048]	Loss: 2.5606
Training Epoch: 4 [2816/50048]	Loss: 2.5518
Training Epoch: 4 [2944/50048]	Loss: 2.5331
Training Epoch: 4 [3072/50048]	Loss: 2.6191
Training Epoch: 4 [3200/50048]	Loss: 2.8233
Training Epoch: 4 [3328/50048]	Loss: 2.7270
Training Epoch: 4 [3456/50048]	Loss: 2.4566
Training Epoch: 4 [3584/50048]	Loss: 2.3618
Training Epoch: 4 [3712/50048]	Loss: 2.6271
Training Epoch: 4 [3840/50048]	Loss: 2.8645
Training Epoch: 4 [3968/50048]	Loss: 2.7610
Training Epoch: 4 [4096/50048]	Loss: 2.6994
Training Epoch: 4 [4224/50048]	Loss: 2.6784
Training Epoch: 4 [4352/50048]	Loss: 2.6080
Training Epoch: 4 [4480/50048]	Loss: 2.5224
Training Epoch: 4 [4608/50048]	Loss: 2.7125
Training Epoch: 4 [4736/50048]	Loss: 2.7015
Training Epoch: 4 [4864/50048]	Loss: 2.3451
Training Epoch: 4 [4992/50048]	Loss: 2.6912
Training Epoch: 4 [5120/50048]	Loss: 2.6201
Training Epoch: 4 [5248/50048]	Loss: 2.6821
Training Epoch: 4 [5376/50048]	Loss: 2.4937
Training Epoch: 4 [5504/50048]	Loss: 3.0689
Training Epoch: 4 [5632/50048]	Loss: 2.3375
Training Epoch: 4 [5760/50048]	Loss: 2.9109
Training Epoch: 4 [5888/50048]	Loss: 2.6144
Training Epoch: 4 [6016/50048]	Loss: 2.6143
Training Epoch: 4 [6144/50048]	Loss: 2.7410
Training Epoch: 4 [6272/50048]	Loss: 2.7034
Training Epoch: 4 [6400/50048]	Loss: 2.6969
Training Epoch: 4 [6528/50048]	Loss: 2.4289
Training Epoch: 4 [6656/50048]	Loss: 2.6626
Training Epoch: 4 [6784/50048]	Loss: 2.6149
Training Epoch: 4 [6912/50048]	Loss: 2.7830
Training Epoch: 4 [7040/50048]	Loss: 2.5637
Training Epoch: 4 [7168/50048]	Loss: 2.8595
Training Epoch: 4 [7296/50048]	Loss: 2.6200
Training Epoch: 4 [7424/50048]	Loss: 2.4414
Training Epoch: 4 [7552/50048]	Loss: 2.6372
Training Epoch: 4 [7680/50048]	Loss: 2.6417
Training Epoch: 4 [7808/50048]	Loss: 2.4637
Training Epoch: 4 [7936/50048]	Loss: 2.4113
Training Epoch: 4 [8064/50048]	Loss: 2.3879
Training Epoch: 4 [8192/50048]	Loss: 2.8630
Training Epoch: 4 [8320/50048]	Loss: 2.5670
Training Epoch: 4 [8448/50048]	Loss: 2.7810
Training Epoch: 4 [8576/50048]	Loss: 2.4081
Training Epoch: 4 [8704/50048]	Loss: 2.6640
Training Epoch: 4 [8832/50048]	Loss: 2.6017
Training Epoch: 4 [8960/50048]	Loss: 2.3937
Training Epoch: 4 [9088/50048]	Loss: 2.4898
Training Epoch: 4 [9216/50048]	Loss: 2.5732
Training Epoch: 4 [9344/50048]	Loss: 2.6126
Training Epoch: 4 [9472/50048]	Loss: 2.5309
Training Epoch: 4 [9600/50048]	Loss: 2.5043
Training Epoch: 4 [9728/50048]	Loss: 2.6908
Training Epoch: 4 [9856/50048]	Loss: 2.5491
Training Epoch: 4 [9984/50048]	Loss: 2.7888
Training Epoch: 4 [10112/50048]	Loss: 2.7971
Training Epoch: 4 [10240/50048]	Loss: 2.6036
Training Epoch: 4 [10368/50048]	Loss: 2.5468
Training Epoch: 4 [10496/50048]	Loss: 2.8083
Training Epoch: 4 [10624/50048]	Loss: 2.5774
Training Epoch: 4 [10752/50048]	Loss: 2.5410
Training Epoch: 4 [10880/50048]	Loss: 2.8203
Training Epoch: 4 [11008/50048]	Loss: 2.7392
Training Epoch: 4 [11136/50048]	Loss: 2.7012
Training Epoch: 4 [11264/50048]	Loss: 2.7123
Training Epoch: 4 [11392/50048]	Loss: 2.7456
Training Epoch: 4 [11520/50048]	Loss: 2.9509
Training Epoch: 4 [11648/50048]	Loss: 2.4608
Training Epoch: 4 [11776/50048]	Loss: 2.4549
Training Epoch: 4 [11904/50048]	Loss: 2.5438
Training Epoch: 4 [12032/50048]	Loss: 2.5112
Training Epoch: 4 [12160/50048]	Loss: 2.7121
Training Epoch: 4 [12288/50048]	Loss: 2.6092
Training Epoch: 4 [12416/50048]	Loss: 2.3648
Training Epoch: 4 [12544/50048]	Loss: 2.4563
Training Epoch: 4 [12672/50048]	Loss: 2.5332
Training Epoch: 4 [12800/50048]	Loss: 2.6177
Training Epoch: 4 [12928/50048]	Loss: 2.5751
Training Epoch: 4 [13056/50048]	Loss: 2.8181
Training Epoch: 4 [13184/50048]	Loss: 2.5512
Training Epoch: 4 [13312/50048]	Loss: 2.6054
Training Epoch: 4 [13440/50048]	Loss: 2.6106
Training Epoch: 4 [13568/50048]	Loss: 2.6738
Training Epoch: 4 [13696/50048]	Loss: 2.4770
Training Epoch: 4 [13824/50048]	Loss: 2.8806
Training Epoch: 4 [13952/50048]	Loss: 2.5432
Training Epoch: 4 [14080/50048]	Loss: 2.4835
Training Epoch: 4 [14208/50048]	Loss: 2.5584
Training Epoch: 4 [14336/50048]	Loss: 2.6033
Training Epoch: 4 [14464/50048]	Loss: 2.5072
Training Epoch: 4 [14592/50048]	Loss: 2.5182
Training Epoch: 4 [14720/50048]	Loss: 2.6077
Training Epoch: 4 [14848/50048]	Loss: 2.6010
Training Epoch: 4 [14976/50048]	Loss: 2.5626
Training Epoch: 4 [15104/50048]	Loss: 2.8964
Training Epoch: 4 [15232/50048]	Loss: 2.3975
Training Epoch: 4 [15360/50048]	Loss: 2.5805
Training Epoch: 4 [15488/50048]	Loss: 2.3504
Training Epoch: 4 [15616/50048]	Loss: 2.7493
Training Epoch: 4 [15744/50048]	Loss: 2.5967
Training Epoch: 4 [15872/50048]	Loss: 2.3038
Training Epoch: 4 [16000/50048]	Loss: 2.3641
Training Epoch: 4 [16128/50048]	Loss: 2.5616
Training Epoch: 4 [16256/50048]	Loss: 2.4838
Training Epoch: 4 [16384/50048]	Loss: 2.4242
Training Epoch: 4 [16512/50048]	Loss: 2.5368
Training Epoch: 4 [16640/50048]	Loss: 2.6615
Training Epoch: 4 [16768/50048]	Loss: 2.5928
Training Epoch: 4 [16896/50048]	Loss: 2.5418
Training Epoch: 4 [17024/50048]	Loss: 2.4736
Training Epoch: 4 [17152/50048]	Loss: 2.5574
Training Epoch: 4 [17280/50048]	Loss: 2.4839
Training Epoch: 4 [17408/50048]	Loss: 2.4284
Training Epoch: 4 [17536/50048]	Loss: 2.7319
Training Epoch: 4 [17664/50048]	Loss: 2.7708
Training Epoch: 4 [17792/50048]	Loss: 2.7984
Training Epoch: 4 [17920/50048]	Loss: 2.7942
Training Epoch: 4 [18048/50048]	Loss: 2.4651
Training Epoch: 4 [18176/50048]	Loss: 2.5512
Training Epoch: 4 [18304/50048]	Loss: 2.5939
Training Epoch: 4 [18432/50048]	Loss: 2.4635
Training Epoch: 4 [18560/50048]	Loss: 2.9023
Training Epoch: 4 [18688/50048]	Loss: 2.5921
Training Epoch: 4 [18816/50048]	Loss: 2.3033
Training Epoch: 4 [18944/50048]	Loss: 2.5118
Training Epoch: 4 [19072/50048]	Loss: 2.6186
Training Epoch: 4 [19200/50048]	Loss: 2.6180
Training Epoch: 4 [19328/50048]	Loss: 2.6567
Training Epoch: 4 [19456/50048]	Loss: 2.5127
Training Epoch: 4 [19584/50048]	Loss: 2.4462
Training Epoch: 4 [19712/50048]	Loss: 2.6570
Training Epoch: 4 [19840/50048]	Loss: 2.7070
Training Epoch: 4 [19968/50048]	Loss: 2.6308
Training Epoch: 4 [20096/50048]	Loss: 2.6200
Training Epoch: 4 [20224/50048]	Loss: 2.5152
Training Epoch: 4 [20352/50048]	Loss: 2.4273
Training Epoch: 4 [20480/50048]	Loss: 2.9275
Training Epoch: 4 [20608/50048]	Loss: 2.3935
Training Epoch: 4 [20736/50048]	Loss: 2.5104
Training Epoch: 4 [20864/50048]	Loss: 2.7572
Training Epoch: 4 [20992/50048]	Loss: 2.5231
Training Epoch: 4 [21120/50048]	Loss: 2.6867
Training Epoch: 4 [21248/50048]	Loss: 2.2789
Training Epoch: 4 [21376/50048]	Loss: 2.6962
Training Epoch: 4 [21504/50048]	Loss: 2.5052
Training Epoch: 4 [21632/50048]	Loss: 2.7000
Training Epoch: 4 [21760/50048]	Loss: 2.6975
Training Epoch: 4 [21888/50048]	Loss: 2.5460
Training Epoch: 4 [22016/50048]	Loss: 2.8582
Training Epoch: 4 [22144/50048]	Loss: 2.7398
Training Epoch: 4 [22272/50048]	Loss: 2.6090
Training Epoch: 4 [22400/50048]	Loss: 2.5282
Training Epoch: 4 [22528/50048]	Loss: 2.5629
Training Epoch: 4 [22656/50048]	Loss: 2.5106
Training Epoch: 4 [22784/50048]	Loss: 2.7153
Training Epoch: 4 [22912/50048]	Loss: 2.4559
Training Epoch: 4 [23040/50048]	Loss: 2.7861
Training Epoch: 4 [23168/50048]	Loss: 2.3383
Training Epoch: 4 [23296/50048]	Loss: 2.6120
Training Epoch: 4 [23424/50048]	Loss: 2.3474
Training Epoch: 4 [23552/50048]	Loss: 2.4622
Training Epoch: 4 [23680/50048]	Loss: 2.4890
Training Epoch: 4 [23808/50048]	Loss: 2.4957
Training Epoch: 4 [23936/50048]	Loss: 2.7448
Training Epoch: 4 [24064/50048]	Loss: 2.6973
Training Epoch: 4 [24192/50048]	Loss: 2.5784
Training Epoch: 4 [24320/50048]	Loss: 2.7675
Training Epoch: 4 [24448/50048]	Loss: 2.5290
Training Epoch: 4 [24576/50048]	Loss: 2.3890
Training Epoch: 4 [24704/50048]	Loss: 2.4794
Training Epoch: 4 [24832/50048]	Loss: 2.5218
Training Epoch: 4 [24960/50048]	Loss: 2.4630
Training Epoch: 4 [25088/50048]	Loss: 2.7615
Training Epoch: 4 [25216/50048]	Loss: 2.6556
Training Epoch: 4 [25344/50048]	Loss: 2.9027
Training Epoch: 4 [25472/50048]	Loss: 2.6082
Training Epoch: 4 [25600/50048]	Loss: 2.8991
Training Epoch: 4 [25728/50048]	Loss: 2.7057
Training Epoch: 4 [25856/50048]	Loss: 2.3814
Training Epoch: 4 [25984/50048]	Loss: 2.4740
Training Epoch: 4 [26112/50048]	Loss: 2.4931
Training Epoch: 4 [26240/50048]	Loss: 2.7113
Training Epoch: 4 [26368/50048]	Loss: 2.4239
Training Epoch: 4 [26496/50048]	Loss: 2.4821
Training Epoch: 4 [26624/50048]	Loss: 2.6365
Training Epoch: 4 [26752/50048]	Loss: 2.4264
Training Epoch: 4 [26880/50048]	Loss: 2.6271
Training Epoch: 4 [27008/50048]	Loss: 2.6558
Training Epoch: 4 [27136/50048]	Loss: 2.3931
Training Epoch: 4 [27264/50048]	Loss: 2.8347
Training Epoch: 4 [27392/50048]	Loss: 2.3618
Training Epoch: 4 [27520/50048]	Loss: 2.5922
Training Epoch: 4 [27648/50048]	Loss: 2.5487
Training Epoch: 4 [27776/50048]	Loss: 2.5187
Training Epoch: 4 [27904/50048]	Loss: 2.5212
Training Epoch: 4 [28032/50048]	Loss: 2.5935
Training Epoch: 4 [28160/50048]	Loss: 2.8006
Training Epoch: 4 [28288/50048]	Loss: 2.6577
Training Epoch: 4 [28416/50048]	Loss: 2.4256
Training Epoch: 4 [28544/50048]	Loss: 2.2031
Training Epoch: 4 [28672/50048]	Loss: 2.6633
Training Epoch: 4 [28800/50048]	Loss: 2.6598
Training Epoch: 4 [28928/50048]	Loss: 2.3633
Training Epoch: 4 [29056/50048]	Loss: 2.3332
Training Epoch: 4 [29184/50048]	Loss: 2.5473
Training Epoch: 4 [29312/50048]	Loss: 2.5615
Training Epoch: 4 [29440/50048]	Loss: 2.6050
Training Epoch: 4 [29568/50048]	Loss: 2.4021
Training Epoch: 4 [29696/50048]	Loss: 2.6945
Training Epoch: 4 [29824/50048]	Loss: 2.6209
Training Epoch: 4 [29952/50048]	Loss: 2.4121
Training Epoch: 4 [30080/50048]	Loss: 2.6811
Training Epoch: 4 [30208/50048]	Loss: 2.5680
Training Epoch: 4 [30336/50048]	Loss: 2.3704
Training Epoch: 4 [30464/50048]	Loss: 2.5573
Training Epoch: 4 [30592/50048]	Loss: 2.5669
Training Epoch: 4 [30720/50048]	Loss: 2.4664
Training Epoch: 4 [30848/50048]	Loss: 2.3770
Training Epoch: 4 [30976/50048]	Loss: 2.6871
Training Epoch: 4 [31104/50048]	Loss: 2.4620
Training Epoch: 4 [31232/50048]	Loss: 2.5907
Training Epoch: 4 [31360/50048]	Loss: 2.7262
Training Epoch: 4 [31488/50048]	Loss: 2.6994
Training Epoch: 4 [31616/50048]	Loss: 2.8000
Training Epoch: 4 [31744/50048]	Loss: 2.5584
Training Epoch: 4 [31872/50048]	Loss: 2.2778
Training Epoch: 4 [32000/50048]	Loss: 2.4984
Training Epoch: 4 [32128/50048]	Loss: 2.6112
Training Epoch: 4 [32256/50048]	Loss: 2.6190
Training Epoch: 4 [32384/50048]	Loss: 2.5388
Training Epoch: 4 [32512/50048]	Loss: 2.6712
Training Epoch: 4 [32640/50048]	Loss: 2.6093
Training Epoch: 4 [32768/50048]	Loss: 2.4321
Training Epoch: 4 [32896/50048]	Loss: 2.6481
Training Epoch: 4 [33024/50048]	Loss: 2.2218
Training Epoch: 4 [33152/50048]	Loss: 2.5728
Training Epoch: 4 [33280/50048]	Loss: 2.5596
Training Epoch: 4 [33408/50048]	Loss: 2.2562
Training Epoch: 4 [33536/50048]	Loss: 2.6537
Training Epoch: 4 [33664/50048]	Loss: 2.8367
Training Epoch: 4 [33792/50048]	Loss: 2.2947
Training Epoch: 4 [33920/50048]	Loss: 2.5878
Training Epoch: 4 [34048/50048]	Loss: 2.4878
Training Epoch: 4 [34176/50048]	Loss: 2.5607
Training Epoch: 4 [34304/50048]	Loss: 2.3749
Training Epoch: 4 [34432/50048]	Loss: 2.3736
Training Epoch: 4 [34560/50048]	Loss: 2.2794
Training Epoch: 4 [34688/50048]	Loss: 2.4861
Training Epoch: 4 [34816/50048]	Loss: 2.5960
Training Epoch: 4 [34944/50048]	Loss: 2.4319
Training Epoch: 4 [35072/50048]	Loss: 2.4063
Training Epoch: 4 [35200/50048]	Loss: 2.4564
Training Epoch: 4 [35328/50048]	Loss: 2.2531
Training Epoch: 4 [35456/50048]	Loss: 2.3143
Training Epoch: 4 [35584/50048]	Loss: 2.2870
Training Epoch: 4 [35712/50048]	Loss: 2.3649
Training Epoch: 4 [35840/50048]	Loss: 2.5849
Training Epoch: 4 [35968/50048]	Loss: 2.6188
Training Epoch: 4 [36096/50048]	Loss: 2.5277
Training Epoch: 4 [36224/50048]	Loss: 2.7049
Training Epoch: 4 [36352/50048]	Loss: 2.3345
Training Epoch: 4 [36480/50048]	Loss: 2.2261
Training Epoch: 4 [36608/50048]	Loss: 2.5282
Training Epoch: 4 [36736/50048]	Loss: 2.4603
Training Epoch: 4 [36864/50048]	Loss: 2.6659
Training Epoch: 4 [36992/50048]	Loss: 2.4382
Training Epoch: 4 [37120/50048]	Loss: 2.5641
Training Epoch: 4 [37248/50048]	Loss: 2.5284
Training Epoch: 4 [37376/50048]	Loss: 2.2244
Training Epoch: 4 [37504/50048]	Loss: 2.4065
Training Epoch: 4 [37632/50048]	Loss: 2.7192
Training Epoch: 4 [37760/50048]	Loss: 2.5668
Training Epoch: 4 [37888/50048]	Loss: 2.6713
Training Epoch: 4 [38016/50048]	Loss: 2.6919
Training Epoch: 4 [38144/50048]	Loss: 2.2328
Training Epoch: 4 [38272/50048]	Loss: 2.5121
Training Epoch: 4 [38400/50048]	Loss: 2.5735
Training Epoch: 4 [38528/50048]	Loss: 2.4499
Training Epoch: 4 [38656/50048]	Loss: 2.3840
Training Epoch: 4 [38784/50048]	Loss: 2.4115
Training Epoch: 4 [38912/50048]	Loss: 2.4786
Training Epoch: 4 [39040/50048]	Loss: 2.6410
Training Epoch: 4 [39168/50048]	Loss: 2.8244
Training Epoch: 4 [39296/50048]	Loss: 2.3105
Training Epoch: 4 [39424/50048]	Loss: 2.5404
Training Epoch: 4 [39552/50048]	Loss: 2.3971
Training Epoch: 4 [39680/50048]	Loss: 2.4352
Training Epoch: 4 [39808/50048]	Loss: 2.3221
Training Epoch: 4 [39936/50048]	Loss: 2.7303
Training Epoch: 4 [40064/50048]	Loss: 2.5024
Training Epoch: 4 [40192/50048]	Loss: 2.7682
Training Epoch: 4 [40320/50048]	Loss: 2.3587
Training Epoch: 4 [40448/50048]	Loss: 2.4059
Training Epoch: 4 [40576/50048]	Loss: 2.7354
Training Epoch: 4 [40704/50048]	Loss: 2.5075
Training Epoch: 4 [40832/50048]	Loss: 2.6120
Training Epoch: 4 [40960/50048]	Loss: 2.3352
Training Epoch: 4 [41088/50048]	Loss: 2.5378
Training Epoch: 4 [41216/50048]	Loss: 2.5172
Training Epoch: 4 [41344/50048]	Loss: 2.0971
Training Epoch: 4 [41472/50048]	Loss: 2.3959
Training Epoch: 4 [41600/50048]	Loss: 2.4476
Training Epoch: 4 [41728/50048]	Loss: 2.1885
Training Epoch: 4 [41856/50048]	Loss: 2.5272
Training Epoch: 4 [41984/50048]	Loss: 2.2695
Training Epoch: 4 [42112/50048]	Loss: 2.5293
Training Epoch: 4 [42240/50048]	Loss: 2.2572
Training Epoch: 4 [42368/50048]	Loss: 2.4852
Training Epoch: 4 [42496/50048]	Loss: 2.4480
Training Epoch: 4 [42624/50048]	Loss: 2.4277
Training Epoch: 4 [42752/50048]	Loss: 2.3608
Training Epoch: 4 [42880/50048]	Loss: 2.3163
Training Epoch: 4 [43008/50048]	Loss: 2.5371
Training Epoch: 4 [43136/50048]	Loss: 2.4754
Training Epoch: 4 [43264/50048]	Loss: 2.3790
Training Epoch: 4 [43392/50048]	Loss: 2.7529
Training Epoch: 4 [43520/50048]	Loss: 2.3928
Training Epoch: 4 [43648/50048]	Loss: 2.4942
Training Epoch: 4 [43776/50048]	Loss: 2.4607
Training Epoch: 4 [43904/50048]	Loss: 2.3581
Training Epoch: 4 [44032/50048]	Loss: 2.5568
Training Epoch: 4 [44160/50048]	Loss: 2.7671
Training Epoch: 4 [44288/50048]	Loss: 2.4573
Training Epoch: 4 [44416/50048]	Loss: 2.4659
Training Epoch: 4 [44544/50048]	Loss: 2.7012
Training Epoch: 4 [44672/50048]	Loss: 2.3612
Training Epoch: 4 [44800/50048]	Loss: 2.5246
Training Epoch: 4 [44928/50048]	Loss: 2.4463
Training Epoch: 4 [45056/50048]	Loss: 2.5179
Training Epoch: 4 [45184/50048]	Loss: 2.5892
Training Epoch: 4 [45312/50048]	Loss: 2.5083
Training Epoch: 4 [45440/50048]	Loss: 2.6251
Training Epoch: 4 [45568/50048]	Loss: 2.7573
Training Epoch: 4 [45696/50048]	Loss: 2.4442
Training Epoch: 4 [45824/50048]	Loss: 2.5927
Training Epoch: 4 [45952/50048]	Loss: 2.7214
Training Epoch: 4 [46080/50048]	Loss: 2.5054
Training Epoch: 4 [46208/50048]	Loss: 2.2867
Training Epoch: 4 [46336/50048]	Loss: 2.5949
Training Epoch: 4 [46464/50048]	Loss: 2.4531
Training Epoch: 4 [46592/50048]	Loss: 2.4515
Training Epoch: 4 [46720/50048]	Loss: 2.4351
Training Epoch: 4 [46848/50048]	Loss: 2.6257
Training Epoch: 4 [46976/50048]	Loss: 2.3816
Training Epoch: 4 [47104/50048]	Loss: 2.5285
Training Epoch: 4 [47232/50048]	Loss: 2.5138
Training Epoch: 4 [47360/50048]	Loss: 2.4843
Training Epoch: 4 [47488/50048]	Loss: 2.3383
Training Epoch: 4 [47616/50048]	Loss: 2.3955
Training Epoch: 4 [47744/50048]	Loss: 2.3359
Training Epoch: 4 [47872/50048]	Loss: 2.4457
Training Epoch: 4 [48000/50048]	Loss: 2.4324
Training Epoch: 4 [48128/50048]	Loss: 2.5760
Training Epoch: 4 [48256/50048]	Loss: 2.4656
Training Epoch: 4 [48384/50048]	Loss: 2.3872
Training Epoch: 4 [48512/50048]	Loss: 2.5902
Training Epoch: 4 [48640/50048]	Loss: 2.5688
Training Epoch: 4 [48768/50048]	Loss: 2.5927
Training Epoch: 4 [48896/50048]	Loss: 2.5268
Training Epoch: 4 [49024/50048]	Loss: 2.4877
Training Epoch: 4 [49152/50048]	Loss: 2.1118
Training Epoch: 4 [49280/50048]	Loss: 2.5876
Training Epoch: 4 [49408/50048]	Loss: 2.2673
Training Epoch: 4 [49536/50048]	Loss: 2.4707
Training Epoch: 4 [49664/50048]	Loss: 2.2335
Training Epoch: 4 [49792/50048]	Loss: 2.3188
Training Epoch: 4 [49920/50048]	Loss: 2.1687
Training Epoch: 4 [50048/50048]	Loss: 2.4392
Validation Epoch: 4, Average loss: 0.0253, Accuracy: 0.2718
Training Epoch: 5 [128/50048]	Loss: 2.6042
Training Epoch: 5 [256/50048]	Loss: 2.3717
Training Epoch: 5 [384/50048]	Loss: 2.4732
Training Epoch: 5 [512/50048]	Loss: 2.1112
Training Epoch: 5 [640/50048]	Loss: 2.5928
Training Epoch: 5 [768/50048]	Loss: 2.6038
Training Epoch: 5 [896/50048]	Loss: 2.4758
Training Epoch: 5 [1024/50048]	Loss: 2.5034
Training Epoch: 5 [1152/50048]	Loss: 2.4002
Training Epoch: 5 [1280/50048]	Loss: 2.3051
Training Epoch: 5 [1408/50048]	Loss: 2.6035
Training Epoch: 5 [1536/50048]	Loss: 2.6117
Training Epoch: 5 [1664/50048]	Loss: 2.4762
Training Epoch: 5 [1792/50048]	Loss: 2.5058
Training Epoch: 5 [1920/50048]	Loss: 2.5559
Training Epoch: 5 [2048/50048]	Loss: 2.1757
Training Epoch: 5 [2176/50048]	Loss: 2.2664
Training Epoch: 5 [2304/50048]	Loss: 2.4558
Training Epoch: 5 [2432/50048]	Loss: 2.3988
Training Epoch: 5 [2560/50048]	Loss: 2.3585
Training Epoch: 5 [2688/50048]	Loss: 2.3644
Training Epoch: 5 [2816/50048]	Loss: 2.5348
Training Epoch: 5 [2944/50048]	Loss: 2.4747
Training Epoch: 5 [3072/50048]	Loss: 2.2257
Training Epoch: 5 [3200/50048]	Loss: 2.3667
Training Epoch: 5 [3328/50048]	Loss: 2.4507
Training Epoch: 5 [3456/50048]	Loss: 2.1866
Training Epoch: 5 [3584/50048]	Loss: 2.6538
Training Epoch: 5 [3712/50048]	Loss: 2.1323
Training Epoch: 5 [3840/50048]	Loss: 2.2927
Training Epoch: 5 [3968/50048]	Loss: 2.5125
Training Epoch: 5 [4096/50048]	Loss: 2.6634
Training Epoch: 5 [4224/50048]	Loss: 2.3683
Training Epoch: 5 [4352/50048]	Loss: 2.4537
Training Epoch: 5 [4480/50048]	Loss: 2.5415
Training Epoch: 5 [4608/50048]	Loss: 2.6473
Training Epoch: 5 [4736/50048]	Loss: 2.5002
Training Epoch: 5 [4864/50048]	Loss: 2.3492
Training Epoch: 5 [4992/50048]	Loss: 2.0384
Training Epoch: 5 [5120/50048]	Loss: 2.3064
Training Epoch: 5 [5248/50048]	Loss: 2.6514
Training Epoch: 5 [5376/50048]	Loss: 2.4578
Training Epoch: 5 [5504/50048]	Loss: 2.4553
Training Epoch: 5 [5632/50048]	Loss: 2.1543
Training Epoch: 5 [5760/50048]	Loss: 2.0705
Training Epoch: 5 [5888/50048]	Loss: 2.4287
Training Epoch: 5 [6016/50048]	Loss: 2.3360
Training Epoch: 5 [6144/50048]	Loss: 2.2662
Training Epoch: 5 [6272/50048]	Loss: 2.4723
Training Epoch: 5 [6400/50048]	Loss: 2.1608
Training Epoch: 5 [6528/50048]	Loss: 2.0182
Training Epoch: 5 [6656/50048]	Loss: 2.3452
Training Epoch: 5 [6784/50048]	Loss: 2.4160
Training Epoch: 5 [6912/50048]	Loss: 2.3796
Training Epoch: 5 [7040/50048]	Loss: 2.3581
Training Epoch: 5 [7168/50048]	Loss: 2.5671
Training Epoch: 5 [7296/50048]	Loss: 2.3483
Training Epoch: 5 [7424/50048]	Loss: 2.5118
Training Epoch: 5 [7552/50048]	Loss: 2.4736
Training Epoch: 5 [7680/50048]	Loss: 2.3350
Training Epoch: 5 [7808/50048]	Loss: 2.4190
Training Epoch: 5 [7936/50048]	Loss: 2.1671
Training Epoch: 5 [8064/50048]	Loss: 2.2893
Training Epoch: 5 [8192/50048]	Loss: 2.6001
Training Epoch: 5 [8320/50048]	Loss: 2.2217
Training Epoch: 5 [8448/50048]	Loss: 2.2544
Training Epoch: 5 [8576/50048]	Loss: 2.3867
Training Epoch: 5 [8704/50048]	Loss: 2.4111
Training Epoch: 5 [8832/50048]	Loss: 2.1145
Training Epoch: 5 [8960/50048]	Loss: 2.6177
Training Epoch: 5 [9088/50048]	Loss: 2.5498
Training Epoch: 5 [9216/50048]	Loss: 2.3607
Training Epoch: 5 [9344/50048]	Loss: 2.2437
Training Epoch: 5 [9472/50048]	Loss: 2.7693
Training Epoch: 5 [9600/50048]	Loss: 2.2905
Training Epoch: 5 [9728/50048]	Loss: 2.0358
Training Epoch: 5 [9856/50048]	Loss: 2.3464
Training Epoch: 5 [9984/50048]	Loss: 2.4807
Training Epoch: 5 [10112/50048]	Loss: 2.2994
Training Epoch: 5 [10240/50048]	Loss: 2.5171
Training Epoch: 5 [10368/50048]	Loss: 2.3699
Training Epoch: 5 [10496/50048]	Loss: 2.5159
Training Epoch: 5 [10624/50048]	Loss: 2.3251
Training Epoch: 5 [10752/50048]	Loss: 2.4044
Training Epoch: 5 [10880/50048]	Loss: 2.5153
Training Epoch: 5 [11008/50048]	Loss: 2.1832
Training Epoch: 5 [11136/50048]	Loss: 2.1525
Training Epoch: 5 [11264/50048]	Loss: 2.6157
Training Epoch: 5 [11392/50048]	Loss: 2.3178
Training Epoch: 5 [11520/50048]	Loss: 2.4015
Training Epoch: 5 [11648/50048]	Loss: 2.5908
Training Epoch: 5 [11776/50048]	Loss: 2.4231
Training Epoch: 5 [11904/50048]	Loss: 2.1228
Training Epoch: 5 [12032/50048]	Loss: 2.7556
Training Epoch: 5 [12160/50048]	Loss: 2.5196
Training Epoch: 5 [12288/50048]	Loss: 2.4843
Training Epoch: 5 [12416/50048]	Loss: 2.4242
Training Epoch: 5 [12544/50048]	Loss: 2.2871
Training Epoch: 5 [12672/50048]	Loss: 2.3584
Training Epoch: 5 [12800/50048]	Loss: 2.3271
Training Epoch: 5 [12928/50048]	Loss: 2.3173
Training Epoch: 5 [13056/50048]	Loss: 2.4208
Training Epoch: 5 [13184/50048]	Loss: 2.3810
Training Epoch: 5 [13312/50048]	Loss: 2.5311
Training Epoch: 5 [13440/50048]	Loss: 2.3142
Training Epoch: 5 [13568/50048]	Loss: 2.4897
Training Epoch: 5 [13696/50048]	Loss: 2.5699
Training Epoch: 5 [13824/50048]	Loss: 2.3366
Training Epoch: 5 [13952/50048]	Loss: 2.0242
Training Epoch: 5 [14080/50048]	Loss: 2.1923
Training Epoch: 5 [14208/50048]	Loss: 2.3738
Training Epoch: 5 [14336/50048]	Loss: 2.2657
Training Epoch: 5 [14464/50048]	Loss: 2.4083
Training Epoch: 5 [14592/50048]	Loss: 2.5443
Training Epoch: 5 [14720/50048]	Loss: 2.1754
Training Epoch: 5 [14848/50048]	Loss: 2.5561
Training Epoch: 5 [14976/50048]	Loss: 2.1674
Training Epoch: 5 [15104/50048]	Loss: 2.5889
Training Epoch: 5 [15232/50048]	Loss: 2.2991
Training Epoch: 5 [15360/50048]	Loss: 2.4666
Training Epoch: 5 [15488/50048]	Loss: 2.3260
Training Epoch: 5 [15616/50048]	Loss: 2.4127
Training Epoch: 5 [15744/50048]	Loss: 2.5128
Training Epoch: 5 [15872/50048]	Loss: 2.3060
Training Epoch: 5 [16000/50048]	Loss: 2.2799
Training Epoch: 5 [16128/50048]	Loss: 2.3959
Training Epoch: 5 [16256/50048]	Loss: 2.2612
Training Epoch: 5 [16384/50048]	Loss: 2.1835
Training Epoch: 5 [16512/50048]	Loss: 2.2029
Training Epoch: 5 [16640/50048]	Loss: 2.2451
Training Epoch: 5 [16768/50048]	Loss: 2.4196
Training Epoch: 5 [16896/50048]	Loss: 2.3568
Training Epoch: 5 [17024/50048]	Loss: 2.3344
Training Epoch: 5 [17152/50048]	Loss: 2.3998
Training Epoch: 5 [17280/50048]	Loss: 2.1525
Training Epoch: 5 [17408/50048]	Loss: 2.1859
Training Epoch: 5 [17536/50048]	Loss: 2.1697
Training Epoch: 5 [17664/50048]	Loss: 2.0978
Training Epoch: 5 [17792/50048]	Loss: 2.1830
Training Epoch: 5 [17920/50048]	Loss: 2.3591
Training Epoch: 5 [18048/50048]	Loss: 2.3197
Training Epoch: 5 [18176/50048]	Loss: 2.1415
Training Epoch: 5 [18304/50048]	Loss: 2.0110
Training Epoch: 5 [18432/50048]	Loss: 2.1974
Training Epoch: 5 [18560/50048]	Loss: 2.3164
Training Epoch: 5 [18688/50048]	Loss: 2.4120
Training Epoch: 5 [18816/50048]	Loss: 2.5026
Training Epoch: 5 [18944/50048]	Loss: 2.2815
Training Epoch: 5 [19072/50048]	Loss: 2.3792
Training Epoch: 5 [19200/50048]	Loss: 2.2768
Training Epoch: 5 [19328/50048]	Loss: 2.3901
Training Epoch: 5 [19456/50048]	Loss: 2.1611
Training Epoch: 5 [19584/50048]	Loss: 2.5764
Training Epoch: 5 [19712/50048]	Loss: 2.4812
Training Epoch: 5 [19840/50048]	Loss: 2.3321
Training Epoch: 5 [19968/50048]	Loss: 2.2904
Training Epoch: 5 [20096/50048]	Loss: 2.7543
Training Epoch: 5 [20224/50048]	Loss: 2.3829
Training Epoch: 5 [20352/50048]	Loss: 2.2766
Training Epoch: 5 [20480/50048]	Loss: 2.2025
Training Epoch: 5 [20608/50048]	Loss: 2.3833
Training Epoch: 5 [20736/50048]	Loss: 2.2367
Training Epoch: 5 [20864/50048]	Loss: 2.5237
Training Epoch: 5 [20992/50048]	Loss: 2.0583
Training Epoch: 5 [21120/50048]	Loss: 2.2212
Training Epoch: 5 [21248/50048]	Loss: 2.4060
Training Epoch: 5 [21376/50048]	Loss: 2.4047
Training Epoch: 5 [21504/50048]	Loss: 2.2952
Training Epoch: 5 [21632/50048]	Loss: 2.2903
Training Epoch: 5 [21760/50048]	Loss: 2.3642
Training Epoch: 5 [21888/50048]	Loss: 2.3059
Training Epoch: 5 [22016/50048]	Loss: 2.2408
Training Epoch: 5 [22144/50048]	Loss: 2.3770
Training Epoch: 5 [22272/50048]	Loss: 2.3086
Training Epoch: 5 [22400/50048]	Loss: 2.3849
Training Epoch: 5 [22528/50048]	Loss: 2.2334
Training Epoch: 5 [22656/50048]	Loss: 2.0735
Training Epoch: 5 [22784/50048]	Loss: 2.3487
Training Epoch: 5 [22912/50048]	Loss: 2.1573
Training Epoch: 5 [23040/50048]	Loss: 2.5193
Training Epoch: 5 [23168/50048]	Loss: 2.3907
Training Epoch: 5 [23296/50048]	Loss: 2.2224
Training Epoch: 5 [23424/50048]	Loss: 2.2790
Training Epoch: 5 [23552/50048]	Loss: 2.4035
Training Epoch: 5 [23680/50048]	Loss: 2.1455
Training Epoch: 5 [23808/50048]	Loss: 2.5306
Training Epoch: 5 [23936/50048]	Loss: 2.5453
Training Epoch: 5 [24064/50048]	Loss: 2.2466
Training Epoch: 5 [24192/50048]	Loss: 2.4595
Training Epoch: 5 [24320/50048]	Loss: 2.2644
Training Epoch: 5 [24448/50048]	Loss: 2.3406
Training Epoch: 5 [24576/50048]	Loss: 2.2941
Training Epoch: 5 [24704/50048]	Loss: 2.2776
Training Epoch: 5 [24832/50048]	Loss: 2.3849
Training Epoch: 5 [24960/50048]	Loss: 2.3617
Training Epoch: 5 [25088/50048]	Loss: 2.4812
Training Epoch: 5 [25216/50048]	Loss: 2.5482
Training Epoch: 5 [25344/50048]	Loss: 2.3517
Training Epoch: 5 [25472/50048]	Loss: 2.3814
Training Epoch: 5 [25600/50048]	Loss: 2.3451
Training Epoch: 5 [25728/50048]	Loss: 2.3142
Training Epoch: 5 [25856/50048]	Loss: 2.2749
Training Epoch: 5 [25984/50048]	Loss: 2.4447
Training Epoch: 5 [26112/50048]	Loss: 2.6845
Training Epoch: 5 [26240/50048]	Loss: 2.1217
Training Epoch: 5 [26368/50048]	Loss: 2.5408
Training Epoch: 5 [26496/50048]	Loss: 2.0491
Training Epoch: 5 [26624/50048]	Loss: 2.4092
Training Epoch: 5 [26752/50048]	Loss: 2.4602
Training Epoch: 5 [26880/50048]	Loss: 2.1683
Training Epoch: 5 [27008/50048]	Loss: 2.2884
Training Epoch: 5 [27136/50048]	Loss: 2.4934
Training Epoch: 5 [27264/50048]	Loss: 2.2379
Training Epoch: 5 [27392/50048]	Loss: 2.3815
Training Epoch: 5 [27520/50048]	Loss: 2.1883
Training Epoch: 5 [27648/50048]	Loss: 2.3601
Training Epoch: 5 [27776/50048]	Loss: 2.3412
Training Epoch: 5 [27904/50048]	Loss: 2.3581
Training Epoch: 5 [28032/50048]	Loss: 2.5250
Training Epoch: 5 [28160/50048]	Loss: 2.4418
Training Epoch: 5 [28288/50048]	Loss: 2.3577
Training Epoch: 5 [28416/50048]	Loss: 2.5181
Training Epoch: 5 [28544/50048]	Loss: 2.3762
Training Epoch: 5 [28672/50048]	Loss: 2.5369
Training Epoch: 5 [28800/50048]	Loss: 2.2314
Training Epoch: 5 [28928/50048]	Loss: 2.0944
Training Epoch: 5 [29056/50048]	Loss: 2.2718
Training Epoch: 5 [29184/50048]	Loss: 2.7415
Training Epoch: 5 [29312/50048]	Loss: 2.4891
Training Epoch: 5 [29440/50048]	Loss: 2.4302
Training Epoch: 5 [29568/50048]	Loss: 2.1619
Training Epoch: 5 [29696/50048]	Loss: 2.4366
Training Epoch: 5 [29824/50048]	Loss: 2.4097
Training Epoch: 5 [29952/50048]	Loss: 2.2201
Training Epoch: 5 [30080/50048]	Loss: 2.1684
Training Epoch: 5 [30208/50048]	Loss: 2.0461
Training Epoch: 5 [30336/50048]	Loss: 2.2953
Training Epoch: 5 [30464/50048]	Loss: 2.1346
Training Epoch: 5 [30592/50048]	Loss: 2.4507
Training Epoch: 5 [30720/50048]	Loss: 2.5793
Training Epoch: 5 [30848/50048]	Loss: 2.7855
Training Epoch: 5 [30976/50048]	Loss: 2.3275
Training Epoch: 5 [31104/50048]	Loss: 2.0720
Training Epoch: 5 [31232/50048]	Loss: 2.3032
Training Epoch: 5 [31360/50048]	Loss: 2.4777
Training Epoch: 5 [31488/50048]	Loss: 2.5682
Training Epoch: 5 [31616/50048]	Loss: 2.3351
Training Epoch: 5 [31744/50048]	Loss: 2.3007
Training Epoch: 5 [31872/50048]	Loss: 2.2662
Training Epoch: 5 [32000/50048]	Loss: 2.3323
Training Epoch: 5 [32128/50048]	Loss: 2.2871
Training Epoch: 5 [32256/50048]	Loss: 2.6183
Training Epoch: 5 [32384/50048]	Loss: 2.5689
Training Epoch: 5 [32512/50048]	Loss: 2.4762
Training Epoch: 5 [32640/50048]	Loss: 2.2814
Training Epoch: 5 [32768/50048]	Loss: 2.1085
Training Epoch: 5 [32896/50048]	Loss: 2.0535
Training Epoch: 5 [33024/50048]	Loss: 2.2338
Training Epoch: 5 [33152/50048]	Loss: 2.1120
Training Epoch: 5 [33280/50048]	Loss: 2.3272
Training Epoch: 5 [33408/50048]	Loss: 2.1940
Training Epoch: 5 [33536/50048]	Loss: 2.4339
Training Epoch: 5 [33664/50048]	Loss: 2.5434
Training Epoch: 5 [33792/50048]	Loss: 2.3835
Training Epoch: 5 [33920/50048]	Loss: 2.0029
Training Epoch: 5 [34048/50048]	Loss: 2.4194
Training Epoch: 5 [34176/50048]	Loss: 2.1541
Training Epoch: 5 [34304/50048]	Loss: 2.3596
Training Epoch: 5 [34432/50048]	Loss: 2.2507
Training Epoch: 5 [34560/50048]	Loss: 2.4886
Training Epoch: 5 [34688/50048]	Loss: 2.4998
Training Epoch: 5 [34816/50048]	Loss: 2.4261
Training Epoch: 5 [34944/50048]	Loss: 2.3672
Training Epoch: 5 [35072/50048]	Loss: 2.2710
Training Epoch: 5 [35200/50048]	Loss: 2.2468
Training Epoch: 5 [35328/50048]	Loss: 2.2927
Training Epoch: 5 [35456/50048]	Loss: 2.3407
Training Epoch: 5 [35584/50048]	Loss: 2.3061
Training Epoch: 5 [35712/50048]	Loss: 2.2671
Training Epoch: 5 [35840/50048]	Loss: 2.6760
Training Epoch: 5 [35968/50048]	Loss: 2.3400
Training Epoch: 5 [36096/50048]	Loss: 2.5346
Training Epoch: 5 [36224/50048]	Loss: 2.2764
Training Epoch: 5 [36352/50048]	Loss: 2.3723
Training Epoch: 5 [36480/50048]	Loss: 2.3856
Training Epoch: 5 [36608/50048]	Loss: 2.3165
Training Epoch: 5 [36736/50048]	Loss: 2.3849
Training Epoch: 5 [36864/50048]	Loss: 2.2154
Training Epoch: 5 [36992/50048]	Loss: 2.3381
Training Epoch: 5 [37120/50048]	Loss: 2.4144
Training Epoch: 5 [37248/50048]	Loss: 2.2282
Training Epoch: 5 [37376/50048]	Loss: 2.3973
Training Epoch: 5 [37504/50048]	Loss: 2.2686
Training Epoch: 5 [37632/50048]	Loss: 2.2576
Training Epoch: 5 [37760/50048]	Loss: 2.0166
Training Epoch: 5 [37888/50048]	Loss: 2.1244
Training Epoch: 5 [38016/50048]	Loss: 2.2453
Training Epoch: 5 [38144/50048]	Loss: 2.1646
Training Epoch: 5 [38272/50048]	Loss: 2.4450
Training Epoch: 5 [38400/50048]	Loss: 2.3951
Training Epoch: 5 [38528/50048]	Loss: 2.2311
Training Epoch: 5 [38656/50048]	Loss: 2.3305
Training Epoch: 5 [38784/50048]	Loss: 2.3887
Training Epoch: 5 [38912/50048]	Loss: 2.3131
Training Epoch: 5 [39040/50048]	Loss: 2.5454
Training Epoch: 5 [39168/50048]	Loss: 2.1112
Training Epoch: 5 [39296/50048]	Loss: 2.1877
Training Epoch: 5 [39424/50048]	Loss: 2.2358
Training Epoch: 5 [39552/50048]	Loss: 2.2536
Training Epoch: 5 [39680/50048]	Loss: 2.6224
Training Epoch: 5 [39808/50048]	Loss: 2.2812
Training Epoch: 5 [39936/50048]	Loss: 2.1394
Training Epoch: 5 [40064/50048]	Loss: 2.2292
Training Epoch: 5 [40192/50048]	Loss: 2.3441
Training Epoch: 5 [40320/50048]	Loss: 2.1681
Training Epoch: 5 [40448/50048]	Loss: 2.3822
Training Epoch: 5 [40576/50048]	Loss: 2.3548
Training Epoch: 5 [40704/50048]	Loss: 2.4131
Training Epoch: 5 [40832/50048]	Loss: 2.4667
Training Epoch: 5 [40960/50048]	Loss: 2.2426
Training Epoch: 5 [41088/50048]	Loss: 2.2918
Training Epoch: 5 [41216/50048]	Loss: 2.3542
Training Epoch: 5 [41344/50048]	Loss: 2.3329
Training Epoch: 5 [41472/50048]	Loss: 2.3414
Training Epoch: 5 [41600/50048]	Loss: 2.5357
Training Epoch: 5 [41728/50048]	Loss: 2.3577
Training Epoch: 5 [41856/50048]	Loss: 2.1388
Training Epoch: 5 [41984/50048]	Loss: 2.3606
Training Epoch: 5 [42112/50048]	Loss: 2.1665
Training Epoch: 5 [42240/50048]	Loss: 2.2309
Training Epoch: 5 [42368/50048]	Loss: 2.3422
Training Epoch: 5 [42496/50048]	Loss: 2.0174
Training Epoch: 5 [42624/50048]	Loss: 2.6363
Training Epoch: 5 [42752/50048]	Loss: 2.1287
Training Epoch: 5 [42880/50048]	Loss: 2.7078
Training Epoch: 5 [43008/50048]	Loss: 2.5756
Training Epoch: 5 [43136/50048]	Loss: 2.2208
Training Epoch: 5 [43264/50048]	Loss: 2.4796
Training Epoch: 5 [43392/50048]	Loss: 2.4151
Training Epoch: 5 [43520/50048]	Loss: 2.3724
Training Epoch: 5 [43648/50048]	Loss: 2.4916
Training Epoch: 5 [43776/50048]	Loss: 2.8552
Training Epoch: 5 [43904/50048]	Loss: 2.3057
Training Epoch: 5 [44032/50048]	Loss: 2.2577
Training Epoch: 5 [44160/50048]	Loss: 2.4491
Training Epoch: 5 [44288/50048]	Loss: 2.4087
Training Epoch: 5 [44416/50048]	Loss: 2.1037
Training Epoch: 5 [44544/50048]	Loss: 2.1768
Training Epoch: 5 [44672/50048]	Loss: 2.4811
Training Epoch: 5 [44800/50048]	Loss: 2.5569
Training Epoch: 5 [44928/50048]	Loss: 2.3698
Training Epoch: 5 [45056/50048]	Loss: 2.0946
Training Epoch: 5 [45184/50048]	Loss: 2.4128
Training Epoch: 5 [45312/50048]	Loss: 2.3289
Training Epoch: 5 [45440/50048]	Loss: 2.4547
Training Epoch: 5 [45568/50048]	Loss: 2.5088
Training Epoch: 5 [45696/50048]	Loss: 2.4743
Training Epoch: 5 [45824/50048]	Loss: 2.6423
Training Epoch: 5 [45952/50048]	Loss: 2.1069
Training Epoch: 5 [46080/50048]	Loss: 2.4014
Training Epoch: 5 [46208/50048]	Loss: 2.2432
Training Epoch: 5 [46336/50048]	Loss: 2.3589
Training Epoch: 5 [46464/50048]	Loss: 2.0822
Training Epoch: 5 [46592/50048]	Loss: 2.3770
Training Epoch: 5 [46720/50048]	Loss: 2.4618
Training Epoch: 5 [46848/50048]	Loss: 2.4507
Training Epoch: 5 [46976/50048]	Loss: 2.0630
Training Epoch: 5 [47104/50048]	Loss: 2.8139
Training Epoch: 5 [47232/50048]	Loss: 2.3461
Training Epoch: 5 [47360/50048]	Loss: 2.3823
Training Epoch: 5 [47488/50048]	Loss: 2.1539
Training Epoch: 5 [47616/50048]	Loss: 2.3745
Training Epoch: 5 [47744/50048]	Loss: 2.4008
Training Epoch: 5 [47872/50048]	Loss: 2.3794
Training Epoch: 5 [48000/50048]	Loss: 2.1401
Training Epoch: 5 [48128/50048]	Loss: 2.0655
Training Epoch: 5 [48256/50048]	Loss: 2.4560
Training Epoch: 5 [48384/50048]	Loss: 2.3570
Training Epoch: 5 [48512/50048]	Loss: 2.2802
Training Epoch: 5 [48640/50048]	Loss: 2.3178
Training Epoch: 5 [48768/50048]	Loss: 2.3656
Training Epoch: 5 [48896/50048]	Loss: 2.2404
Training Epoch: 5 [49024/50048]	Loss: 2.1210
Training Epoch: 5 [49152/50048]	Loss: 2.6416
Training Epoch: 5 [49280/50048]	Loss: 2.4793
Training Epoch: 5 [49408/50048]	Loss: 2.2928
Training Epoch: 5 [49536/50048]	Loss: 2.3258
Training Epoch: 5 [49664/50048]	Loss: 2.3767
Training Epoch: 5 [49792/50048]	Loss: 2.1440
Training Epoch: 5 [49920/50048]	Loss: 2.2043
Training Epoch: 5 [50048/50048]	Loss: 2.2364
Validation Epoch: 5, Average loss: 0.0416, Accuracy: 0.1674
Training Epoch: 6 [128/50048]	Loss: 2.1990
Training Epoch: 6 [256/50048]	Loss: 2.0410
Training Epoch: 6 [384/50048]	Loss: 2.4326
Training Epoch: 6 [512/50048]	Loss: 2.3008
Training Epoch: 6 [640/50048]	Loss: 2.5014
Training Epoch: 6 [768/50048]	Loss: 2.1283
Training Epoch: 6 [896/50048]	Loss: 2.2562
Training Epoch: 6 [1024/50048]	Loss: 2.4238
Training Epoch: 6 [1152/50048]	Loss: 2.3211
Training Epoch: 6 [1280/50048]	Loss: 2.2063
Training Epoch: 6 [1408/50048]	Loss: 2.2270
Training Epoch: 6 [1536/50048]	Loss: 2.0687
Training Epoch: 6 [1664/50048]	Loss: 1.9931
Training Epoch: 6 [1792/50048]	Loss: 2.3770
Training Epoch: 6 [1920/50048]	Loss: 2.0651
Training Epoch: 6 [2048/50048]	Loss: 2.1663
Training Epoch: 6 [2176/50048]	Loss: 2.2797
Training Epoch: 6 [2304/50048]	Loss: 2.1499
Training Epoch: 6 [2432/50048]	Loss: 2.0277
Training Epoch: 6 [2560/50048]	Loss: 2.1949
Training Epoch: 6 [2688/50048]	Loss: 2.4062
Training Epoch: 6 [2816/50048]	Loss: 2.5542
Training Epoch: 6 [2944/50048]	Loss: 2.0880
Training Epoch: 6 [3072/50048]	Loss: 2.3001
Training Epoch: 6 [3200/50048]	Loss: 2.2268
Training Epoch: 6 [3328/50048]	Loss: 2.1557
Training Epoch: 6 [3456/50048]	Loss: 2.2559
Training Epoch: 6 [3584/50048]	Loss: 2.2091
Training Epoch: 6 [3712/50048]	Loss: 2.3322
Training Epoch: 6 [3840/50048]	Loss: 2.2057
Training Epoch: 6 [3968/50048]	Loss: 2.2699
Training Epoch: 6 [4096/50048]	Loss: 2.2139
Training Epoch: 6 [4224/50048]	Loss: 2.2203
Training Epoch: 6 [4352/50048]	Loss: 1.8124
Training Epoch: 6 [4480/50048]	Loss: 2.2753
Training Epoch: 6 [4608/50048]	Loss: 2.2810
Training Epoch: 6 [4736/50048]	Loss: 2.2017
Training Epoch: 6 [4864/50048]	Loss: 2.4594
Training Epoch: 6 [4992/50048]	Loss: 2.2017
Training Epoch: 6 [5120/50048]	Loss: 2.1734
Training Epoch: 6 [5248/50048]	Loss: 2.2999
Training Epoch: 6 [5376/50048]	Loss: 2.3438
Training Epoch: 6 [5504/50048]	Loss: 2.2731
Training Epoch: 6 [5632/50048]	Loss: 2.1706
Training Epoch: 6 [5760/50048]	Loss: 1.9747
Training Epoch: 6 [5888/50048]	Loss: 2.3231
Training Epoch: 6 [6016/50048]	Loss: 2.2418
Training Epoch: 6 [6144/50048]	Loss: 2.1285
Training Epoch: 6 [6272/50048]	Loss: 2.0203
Training Epoch: 6 [6400/50048]	Loss: 2.1465
Training Epoch: 6 [6528/50048]	Loss: 1.9553
Training Epoch: 6 [6656/50048]	Loss: 2.2019
Training Epoch: 6 [6784/50048]	Loss: 2.1606
Training Epoch: 6 [6912/50048]	Loss: 2.1618
Training Epoch: 6 [7040/50048]	Loss: 2.5635
Training Epoch: 6 [7168/50048]	Loss: 1.9500
Training Epoch: 6 [7296/50048]	Loss: 1.9149
Training Epoch: 6 [7424/50048]	Loss: 2.3708
Training Epoch: 6 [7552/50048]	Loss: 2.0164
Training Epoch: 6 [7680/50048]	Loss: 2.3358
Training Epoch: 6 [7808/50048]	Loss: 2.3475
Training Epoch: 6 [7936/50048]	Loss: 2.2354
Training Epoch: 6 [8064/50048]	Loss: 2.2904
Training Epoch: 6 [8192/50048]	Loss: 2.0946
Training Epoch: 6 [8320/50048]	Loss: 2.2526
Training Epoch: 6 [8448/50048]	Loss: 2.3148
Training Epoch: 6 [8576/50048]	Loss: 2.3741
Training Epoch: 6 [8704/50048]	Loss: 2.1426
Training Epoch: 6 [8832/50048]	Loss: 2.1205
Training Epoch: 6 [8960/50048]	Loss: 2.3067
Training Epoch: 6 [9088/50048]	Loss: 2.2946
Training Epoch: 6 [9216/50048]	Loss: 2.3976
Training Epoch: 6 [9344/50048]	Loss: 2.4246
Training Epoch: 6 [9472/50048]	Loss: 2.3503
Training Epoch: 6 [9600/50048]	Loss: 2.2726
Training Epoch: 6 [9728/50048]	Loss: 2.0792
Training Epoch: 6 [9856/50048]	Loss: 2.0162
Training Epoch: 6 [9984/50048]	Loss: 2.2247
Training Epoch: 6 [10112/50048]	Loss: 2.4276
Training Epoch: 6 [10240/50048]	Loss: 2.5151
Training Epoch: 6 [10368/50048]	Loss: 2.0979
Training Epoch: 6 [10496/50048]	Loss: 2.1944
Training Epoch: 6 [10624/50048]	Loss: 2.0975
Training Epoch: 6 [10752/50048]	Loss: 2.1782
Training Epoch: 6 [10880/50048]	Loss: 1.9437
Training Epoch: 6 [11008/50048]	Loss: 2.1786
Training Epoch: 6 [11136/50048]	Loss: 2.3306
Training Epoch: 6 [11264/50048]	Loss: 2.3719
Training Epoch: 6 [11392/50048]	Loss: 2.3983
Training Epoch: 6 [11520/50048]	Loss: 2.2762
Training Epoch: 6 [11648/50048]	Loss: 2.3478
Training Epoch: 6 [11776/50048]	Loss: 2.0438
Training Epoch: 6 [11904/50048]	Loss: 2.3617
Training Epoch: 6 [12032/50048]	Loss: 2.5614
Training Epoch: 6 [12160/50048]	Loss: 2.0699
Training Epoch: 6 [12288/50048]	Loss: 2.3889
Training Epoch: 6 [12416/50048]	Loss: 2.1415
Training Epoch: 6 [12544/50048]	Loss: 2.3243
Training Epoch: 6 [12672/50048]	Loss: 2.1926
Training Epoch: 6 [12800/50048]	Loss: 2.2000
Training Epoch: 6 [12928/50048]	Loss: 2.2951
Training Epoch: 6 [13056/50048]	Loss: 2.4378
Training Epoch: 6 [13184/50048]	Loss: 2.2302
Training Epoch: 6 [13312/50048]	Loss: 2.5639
Training Epoch: 6 [13440/50048]	Loss: 2.1337
Training Epoch: 6 [13568/50048]	Loss: 2.4492
Training Epoch: 6 [13696/50048]	Loss: 2.3879
Training Epoch: 6 [13824/50048]	Loss: 2.2214
Training Epoch: 6 [13952/50048]	Loss: 2.3426
Training Epoch: 6 [14080/50048]	Loss: 1.9349
Training Epoch: 6 [14208/50048]	Loss: 2.0539
Training Epoch: 6 [14336/50048]	Loss: 2.1178
Training Epoch: 6 [14464/50048]	Loss: 2.2698
Training Epoch: 6 [14592/50048]	Loss: 2.4114
Training Epoch: 6 [14720/50048]	Loss: 2.0591
Training Epoch: 6 [14848/50048]	Loss: 2.3442
Training Epoch: 6 [14976/50048]	Loss: 2.2225
Training Epoch: 6 [15104/50048]	Loss: 2.2576
Training Epoch: 6 [15232/50048]	Loss: 2.0864
Training Epoch: 6 [15360/50048]	Loss: 2.2188
Training Epoch: 6 [15488/50048]	Loss: 2.1279
Training Epoch: 6 [15616/50048]	Loss: 2.3391
Training Epoch: 6 [15744/50048]	Loss: 2.3419
Training Epoch: 6 [15872/50048]	Loss: 2.0361
Training Epoch: 6 [16000/50048]	Loss: 2.1466
Training Epoch: 6 [16128/50048]	Loss: 2.1852
Training Epoch: 6 [16256/50048]	Loss: 2.0885
Training Epoch: 6 [16384/50048]	Loss: 2.1855
Training Epoch: 6 [16512/50048]	Loss: 2.0429
Training Epoch: 6 [16640/50048]	Loss: 2.2758
Training Epoch: 6 [16768/50048]	Loss: 2.4077
Training Epoch: 6 [16896/50048]	Loss: 2.0022
Training Epoch: 6 [17024/50048]	Loss: 2.1989
Training Epoch: 6 [17152/50048]	Loss: 1.8604
Training Epoch: 6 [17280/50048]	Loss: 2.2249
Training Epoch: 6 [17408/50048]	Loss: 2.0348
Training Epoch: 6 [17536/50048]	Loss: 2.1349
Training Epoch: 6 [17664/50048]	Loss: 2.0372
Training Epoch: 6 [17792/50048]	Loss: 2.1385
Training Epoch: 6 [17920/50048]	Loss: 2.2644
Training Epoch: 6 [18048/50048]	Loss: 2.0901
Training Epoch: 6 [18176/50048]	Loss: 2.5179
Training Epoch: 6 [18304/50048]	Loss: 2.1459
Training Epoch: 6 [18432/50048]	Loss: 2.1729
Training Epoch: 6 [18560/50048]	Loss: 2.1400
Training Epoch: 6 [18688/50048]	Loss: 2.4009
Training Epoch: 6 [18816/50048]	Loss: 2.3198
Training Epoch: 6 [18944/50048]	Loss: 2.0052
Training Epoch: 6 [19072/50048]	Loss: 2.0777
Training Epoch: 6 [19200/50048]	Loss: 2.1957
Training Epoch: 6 [19328/50048]	Loss: 2.2316
Training Epoch: 6 [19456/50048]	Loss: 2.1117
Training Epoch: 6 [19584/50048]	Loss: 2.0946
Training Epoch: 6 [19712/50048]	Loss: 1.9453
Training Epoch: 6 [19840/50048]	Loss: 1.9542
Training Epoch: 6 [19968/50048]	Loss: 2.2437
Training Epoch: 6 [20096/50048]	Loss: 2.2353
Training Epoch: 6 [20224/50048]	Loss: 2.3933
Training Epoch: 6 [20352/50048]	Loss: 2.2700
Training Epoch: 6 [20480/50048]	Loss: 2.2129
Training Epoch: 6 [20608/50048]	Loss: 1.9241
Training Epoch: 6 [20736/50048]	Loss: 2.3317
Training Epoch: 6 [20864/50048]	Loss: 2.2403
Training Epoch: 6 [20992/50048]	Loss: 2.3083
Training Epoch: 6 [21120/50048]	Loss: 2.5961
Training Epoch: 6 [21248/50048]	Loss: 2.2010
Training Epoch: 6 [21376/50048]	Loss: 2.2135
Training Epoch: 6 [21504/50048]	Loss: 2.2113
Training Epoch: 6 [21632/50048]	Loss: 2.4581
Training Epoch: 6 [21760/50048]	Loss: 2.2092
Training Epoch: 6 [21888/50048]	Loss: 1.9659
Training Epoch: 6 [22016/50048]	Loss: 2.3974
Training Epoch: 6 [22144/50048]	Loss: 1.9657
Training Epoch: 6 [22272/50048]	Loss: 2.0849
Training Epoch: 6 [22400/50048]	Loss: 2.2056
Training Epoch: 6 [22528/50048]	Loss: 2.2188
Training Epoch: 6 [22656/50048]	Loss: 2.2216
Training Epoch: 6 [22784/50048]	Loss: 2.1647
Training Epoch: 6 [22912/50048]	Loss: 2.1561
Training Epoch: 6 [23040/50048]	Loss: 1.9164
Training Epoch: 6 [23168/50048]	Loss: 2.1138
Training Epoch: 6 [23296/50048]	Loss: 2.0532
Training Epoch: 6 [23424/50048]	Loss: 2.0520
Training Epoch: 6 [23552/50048]	Loss: 2.0817
Training Epoch: 6 [23680/50048]	Loss: 2.2740
Training Epoch: 6 [23808/50048]	Loss: 2.1367
Training Epoch: 6 [23936/50048]	Loss: 2.0463
Training Epoch: 6 [24064/50048]	Loss: 2.1109
Training Epoch: 6 [24192/50048]	Loss: 2.2700
Training Epoch: 6 [24320/50048]	Loss: 2.2543
Training Epoch: 6 [24448/50048]	Loss: 2.3132
Training Epoch: 6 [24576/50048]	Loss: 1.9617
Training Epoch: 6 [24704/50048]	Loss: 2.2399
Training Epoch: 6 [24832/50048]	Loss: 1.8926
Training Epoch: 6 [24960/50048]	Loss: 2.1609
Training Epoch: 6 [25088/50048]	Loss: 2.0421
Training Epoch: 6 [25216/50048]	Loss: 2.0177
Training Epoch: 6 [25344/50048]	Loss: 2.0895
Training Epoch: 6 [25472/50048]	Loss: 2.3513
Training Epoch: 6 [25600/50048]	Loss: 1.7795
Training Epoch: 6 [25728/50048]	Loss: 2.2183
Training Epoch: 6 [25856/50048]	Loss: 2.2486
Training Epoch: 6 [25984/50048]	Loss: 2.3230
Training Epoch: 6 [26112/50048]	Loss: 2.1042
Training Epoch: 6 [26240/50048]	Loss: 2.1420
Training Epoch: 6 [26368/50048]	Loss: 2.3237
Training Epoch: 6 [26496/50048]	Loss: 2.2299
Training Epoch: 6 [26624/50048]	Loss: 2.4090
Training Epoch: 6 [26752/50048]	Loss: 2.2636
Training Epoch: 6 [26880/50048]	Loss: 2.1861
Training Epoch: 6 [27008/50048]	Loss: 1.9318
Training Epoch: 6 [27136/50048]	Loss: 2.2853
Training Epoch: 6 [27264/50048]	Loss: 2.1302
Training Epoch: 6 [27392/50048]	Loss: 2.4011
Training Epoch: 6 [27520/50048]	Loss: 2.2440
Training Epoch: 6 [27648/50048]	Loss: 2.2189
Training Epoch: 6 [27776/50048]	Loss: 2.1168
Training Epoch: 6 [27904/50048]	Loss: 2.0277
Training Epoch: 6 [28032/50048]	Loss: 2.0434
Training Epoch: 6 [28160/50048]	Loss: 2.4067
Training Epoch: 6 [28288/50048]	Loss: 2.2250
Training Epoch: 6 [28416/50048]	Loss: 2.1843
Training Epoch: 6 [28544/50048]	Loss: 2.1743
Training Epoch: 6 [28672/50048]	Loss: 2.3788
Training Epoch: 6 [28800/50048]	Loss: 2.1754
Training Epoch: 6 [28928/50048]	Loss: 2.2662
Training Epoch: 6 [29056/50048]	Loss: 2.1729
Training Epoch: 6 [29184/50048]	Loss: 2.2846
Training Epoch: 6 [29312/50048]	Loss: 2.1310
Training Epoch: 6 [29440/50048]	Loss: 2.2114
Training Epoch: 6 [29568/50048]	Loss: 2.2146
Training Epoch: 6 [29696/50048]	Loss: 2.2233
Training Epoch: 6 [29824/50048]	Loss: 2.1436
Training Epoch: 6 [29952/50048]	Loss: 2.5449
Training Epoch: 6 [30080/50048]	Loss: 2.0063
Training Epoch: 6 [30208/50048]	Loss: 2.2065
Training Epoch: 6 [30336/50048]	Loss: 2.3276
Training Epoch: 6 [30464/50048]	Loss: 2.1836
Training Epoch: 6 [30592/50048]	Loss: 2.2517
Training Epoch: 6 [30720/50048]	Loss: 1.9917
Training Epoch: 6 [30848/50048]	Loss: 2.1474
Training Epoch: 6 [30976/50048]	Loss: 2.2969
Training Epoch: 6 [31104/50048]	Loss: 2.2098
Training Epoch: 6 [31232/50048]	Loss: 2.2883
Training Epoch: 6 [31360/50048]	Loss: 2.2360
Training Epoch: 6 [31488/50048]	Loss: 2.0813
Training Epoch: 6 [31616/50048]	Loss: 2.2490
Training Epoch: 6 [31744/50048]	Loss: 2.0162
Training Epoch: 6 [31872/50048]	Loss: 2.1103
Training Epoch: 6 [32000/50048]	Loss: 2.2874
Training Epoch: 6 [32128/50048]	Loss: 2.1341
Training Epoch: 6 [32256/50048]	Loss: 2.1780
Training Epoch: 6 [32384/50048]	Loss: 2.2181
Training Epoch: 6 [32512/50048]	Loss: 2.4155
Training Epoch: 6 [32640/50048]	Loss: 2.1377
Training Epoch: 6 [32768/50048]	Loss: 2.0720
Training Epoch: 6 [32896/50048]	Loss: 2.1611
Training Epoch: 6 [33024/50048]	Loss: 2.1275
Training Epoch: 6 [33152/50048]	Loss: 2.3236
Training Epoch: 6 [33280/50048]	Loss: 2.1506
Training Epoch: 6 [33408/50048]	Loss: 2.1456
Training Epoch: 6 [33536/50048]	Loss: 2.2403
Training Epoch: 6 [33664/50048]	Loss: 1.9615
Training Epoch: 6 [33792/50048]	Loss: 2.5024
Training Epoch: 6 [33920/50048]	Loss: 1.9628
Training Epoch: 6 [34048/50048]	Loss: 2.0778
Training Epoch: 6 [34176/50048]	Loss: 2.2888
Training Epoch: 6 [34304/50048]	Loss: 2.2369
Training Epoch: 6 [34432/50048]	Loss: 2.4796
Training Epoch: 6 [34560/50048]	Loss: 2.1222
Training Epoch: 6 [34688/50048]	Loss: 2.2244
Training Epoch: 6 [34816/50048]	Loss: 2.2378
Training Epoch: 6 [34944/50048]	Loss: 2.1916
Training Epoch: 6 [35072/50048]	Loss: 2.3721
Training Epoch: 6 [35200/50048]	Loss: 2.0890
Training Epoch: 6 [35328/50048]	Loss: 2.2072
Training Epoch: 6 [35456/50048]	Loss: 2.1891
Training Epoch: 6 [35584/50048]	Loss: 2.2723
Training Epoch: 6 [35712/50048]	Loss: 2.0287
Training Epoch: 6 [35840/50048]	Loss: 2.1662
Training Epoch: 6 [35968/50048]	Loss: 2.3440
Training Epoch: 6 [36096/50048]	Loss: 2.1475
Training Epoch: 6 [36224/50048]	Loss: 2.3871
Training Epoch: 6 [36352/50048]	Loss: 2.2736
Training Epoch: 6 [36480/50048]	Loss: 2.1934
Training Epoch: 6 [36608/50048]	Loss: 1.9419
Training Epoch: 6 [36736/50048]	Loss: 2.2829
Training Epoch: 6 [36864/50048]	Loss: 2.1956
Training Epoch: 6 [36992/50048]	Loss: 1.9547
Training Epoch: 6 [37120/50048]	Loss: 2.2443
Training Epoch: 6 [37248/50048]	Loss: 2.1492
Training Epoch: 6 [37376/50048]	Loss: 2.1941
Training Epoch: 6 [37504/50048]	Loss: 2.0577
Training Epoch: 6 [37632/50048]	Loss: 2.1312
Training Epoch: 6 [37760/50048]	Loss: 1.9651
Training Epoch: 6 [37888/50048]	Loss: 2.2516
Training Epoch: 6 [38016/50048]	Loss: 2.2142
Training Epoch: 6 [38144/50048]	Loss: 2.2357
Training Epoch: 6 [38272/50048]	Loss: 2.2984
Training Epoch: 6 [38400/50048]	Loss: 2.1422
Training Epoch: 6 [38528/50048]	Loss: 2.1156
Training Epoch: 6 [38656/50048]	Loss: 2.0193
Training Epoch: 6 [38784/50048]	Loss: 2.2417
Training Epoch: 6 [38912/50048]	Loss: 2.0031
Training Epoch: 6 [39040/50048]	Loss: 2.1920
Training Epoch: 6 [39168/50048]	Loss: 2.2269
Training Epoch: 6 [39296/50048]	Loss: 1.9874
Training Epoch: 6 [39424/50048]	Loss: 2.1243
Training Epoch: 6 [39552/50048]	Loss: 2.1720
Training Epoch: 6 [39680/50048]	Loss: 2.3785
Training Epoch: 6 [39808/50048]	Loss: 1.9595
Training Epoch: 6 [39936/50048]	Loss: 2.3308
Training Epoch: 6 [40064/50048]	Loss: 2.0031
Training Epoch: 6 [40192/50048]	Loss: 2.3657
Training Epoch: 6 [40320/50048]	Loss: 2.3570
Training Epoch: 6 [40448/50048]	Loss: 2.3926
Training Epoch: 6 [40576/50048]	Loss: 2.3606
Training Epoch: 6 [40704/50048]	Loss: 2.0190
Training Epoch: 6 [40832/50048]	Loss: 2.2234
Training Epoch: 6 [40960/50048]	Loss: 2.1709
Training Epoch: 6 [41088/50048]	Loss: 2.0671
Training Epoch: 6 [41216/50048]	Loss: 2.1862
Training Epoch: 6 [41344/50048]	Loss: 2.3037
Training Epoch: 6 [41472/50048]	Loss: 2.3191
Training Epoch: 6 [41600/50048]	Loss: 2.2790
Training Epoch: 6 [41728/50048]	Loss: 2.2215
Training Epoch: 6 [41856/50048]	Loss: 2.1452
Training Epoch: 6 [41984/50048]	Loss: 2.2631
Training Epoch: 6 [42112/50048]	Loss: 2.1361
Training Epoch: 6 [42240/50048]	Loss: 2.0182
Training Epoch: 6 [42368/50048]	Loss: 2.2550
Training Epoch: 6 [42496/50048]	Loss: 2.3541
Training Epoch: 6 [42624/50048]	Loss: 2.2061
Training Epoch: 6 [42752/50048]	Loss: 2.0981
Training Epoch: 6 [42880/50048]	Loss: 2.0151
Training Epoch: 6 [43008/50048]	Loss: 2.2466
Training Epoch: 6 [43136/50048]	Loss: 1.7611
Training Epoch: 6 [43264/50048]	Loss: 2.2986
Training Epoch: 6 [43392/50048]	Loss: 1.9784
Training Epoch: 6 [43520/50048]	Loss: 2.2957
Training Epoch: 6 [43648/50048]	Loss: 2.1301
Training Epoch: 6 [43776/50048]	Loss: 2.1123
Training Epoch: 6 [43904/50048]	Loss: 2.4250
Training Epoch: 6 [44032/50048]	Loss: 2.4207
Training Epoch: 6 [44160/50048]	Loss: 2.1637
Training Epoch: 6 [44288/50048]	Loss: 1.9459
Training Epoch: 6 [44416/50048]	Loss: 2.4615
Training Epoch: 6 [44544/50048]	Loss: 2.1305
Training Epoch: 6 [44672/50048]	Loss: 2.0413
Training Epoch: 6 [44800/50048]	Loss: 2.0042
Training Epoch: 6 [44928/50048]	Loss: 2.1013
Training Epoch: 6 [45056/50048]	Loss: 1.9388
Training Epoch: 6 [45184/50048]	Loss: 2.0281
Training Epoch: 6 [45312/50048]	Loss: 2.2534
Training Epoch: 6 [45440/50048]	Loss: 2.0601
Training Epoch: 6 [45568/50048]	Loss: 2.2292
Training Epoch: 6 [45696/50048]	Loss: 2.3452
Training Epoch: 6 [45824/50048]	Loss: 2.0941
Training Epoch: 6 [45952/50048]	Loss: 2.2400
Training Epoch: 6 [46080/50048]	Loss: 2.1129
Training Epoch: 6 [46208/50048]	Loss: 1.9174
Training Epoch: 6 [46336/50048]	Loss: 2.3427
Training Epoch: 6 [46464/50048]	Loss: 2.2065
Training Epoch: 6 [46592/50048]	Loss: 2.1540
Training Epoch: 6 [46720/50048]	Loss: 2.0133
Training Epoch: 6 [46848/50048]	Loss: 2.1743
Training Epoch: 6 [46976/50048]	Loss: 2.1391
Training Epoch: 6 [47104/50048]	Loss: 2.2797
Training Epoch: 6 [47232/50048]	Loss: 2.1019
Training Epoch: 6 [47360/50048]	Loss: 2.0835
Training Epoch: 6 [47488/50048]	Loss: 2.3024
Training Epoch: 6 [47616/50048]	Loss: 2.1194
Training Epoch: 6 [47744/50048]	Loss: 2.1093
Training Epoch: 6 [47872/50048]	Loss: 2.2558
Training Epoch: 6 [48000/50048]	Loss: 1.9594
Training Epoch: 6 [48128/50048]	Loss: 2.3306
Training Epoch: 6 [48256/50048]	Loss: 1.9114
Training Epoch: 6 [48384/50048]	Loss: 2.3128
Training Epoch: 6 [48512/50048]	Loss: 2.3211
Training Epoch: 6 [48640/50048]	Loss: 2.0190
Training Epoch: 6 [48768/50048]	Loss: 2.3038
Training Epoch: 6 [48896/50048]	Loss: 2.1976
Training Epoch: 6 [49024/50048]	Loss: 2.3815
Training Epoch: 6 [49152/50048]	Loss: 1.9046
Training Epoch: 6 [49280/50048]	Loss: 2.3931
Training Epoch: 6 [49408/50048]	Loss: 2.1324
Training Epoch: 6 [49536/50048]	Loss: 2.1807
Training Epoch: 6 [49664/50048]	Loss: 2.2002
Training Epoch: 6 [49792/50048]	Loss: 2.2289
Training Epoch: 6 [49920/50048]	Loss: 2.0916
Training Epoch: 6 [50048/50048]	Loss: 2.3180
Validation Epoch: 6, Average loss: 0.0215, Accuracy: 0.3296
[Training Loop] Model's accuracy 0.3296083860759494 surpasses threshold 0.3! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0935
Profiling... [256/50048]	Loss: 2.1595
Profiling... [384/50048]	Loss: 2.1423
Profiling... [512/50048]	Loss: 1.9142
Profiling... [640/50048]	Loss: 2.0905
Profiling... [768/50048]	Loss: 2.3387
Profiling... [896/50048]	Loss: 1.9461
Profiling... [1024/50048]	Loss: 2.0902
Profiling... [1152/50048]	Loss: 2.1643
Profiling... [1280/50048]	Loss: 1.9902
Profiling... [1408/50048]	Loss: 1.9847
Profiling... [1536/50048]	Loss: 2.0123
Profiling... [1664/50048]	Loss: 2.1913
Profile done
epoch 1 train time consumed: 3.37s
Validation Epoch: 7, Average loss: 0.0160, Accuracy: 0.4505
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 126.78615355835845,
                        "time": 2.1992623279998043,
                        "accuracy": 0.45045490506329117,
                        "total_cost": 619.0098234000099
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.3178
Profiling... [256/50048]	Loss: 2.1282
Profiling... [384/50048]	Loss: 2.4368
Profiling... [512/50048]	Loss: 2.0855
Profiling... [640/50048]	Loss: 1.9734
Profiling... [768/50048]	Loss: 2.0037
Profiling... [896/50048]	Loss: 1.9676
Profiling... [1024/50048]	Loss: 1.7797
Profiling... [1152/50048]	Loss: 2.2348
Profiling... [1280/50048]	Loss: 2.0610
Profiling... [1408/50048]	Loss: 1.9593
Profiling... [1536/50048]	Loss: 2.3538
Profiling... [1664/50048]	Loss: 1.9648
Profile done
epoch 1 train time consumed: 3.42s
Validation Epoch: 7, Average loss: 0.0160, Accuracy: 0.4482
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 126.82626339388138,
                        "time": 2.2351408360000278,
                        "accuracy": 0.44818037974683544,
                        "total_cost": 632.5010491291175
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1057
Profiling... [256/50048]	Loss: 2.0735
Profiling... [384/50048]	Loss: 2.0732
Profiling... [512/50048]	Loss: 2.0185
Profiling... [640/50048]	Loss: 2.1371
Profiling... [768/50048]	Loss: 1.8894
Profiling... [896/50048]	Loss: 2.0596
Profiling... [1024/50048]	Loss: 2.1660
Profiling... [1152/50048]	Loss: 1.7586
Profiling... [1280/50048]	Loss: 2.3051
Profiling... [1408/50048]	Loss: 1.9298
Profiling... [1536/50048]	Loss: 2.1579
Profiling... [1664/50048]	Loss: 2.0151
Profile done
epoch 1 train time consumed: 3.66s
Validation Epoch: 7, Average loss: 0.0161, Accuracy: 0.4476
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 126.8366169108351,
                        "time": 2.5315365479996217,
                        "accuracy": 0.4475870253164557,
                        "total_cost": 717.3834655001133
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0478
Profiling... [256/50048]	Loss: 1.8987
Profiling... [384/50048]	Loss: 2.0453
Profiling... [512/50048]	Loss: 1.7079
Profiling... [640/50048]	Loss: 1.8552
Profiling... [768/50048]	Loss: 2.0993
Profiling... [896/50048]	Loss: 1.7748
Profiling... [1024/50048]	Loss: 2.4767
Profiling... [1152/50048]	Loss: 2.2232
Profiling... [1280/50048]	Loss: 1.9881
Profiling... [1408/50048]	Loss: 1.9889
Profiling... [1536/50048]	Loss: 2.0616
Profiling... [1664/50048]	Loss: 2.3130
Profile done
epoch 1 train time consumed: 7.35s
Validation Epoch: 7, Average loss: 0.0165, Accuracy: 0.4384
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 126.76172154844677,
                        "time": 5.443565540000236,
                        "accuracy": 0.4383900316455696,
                        "total_cost": 1574.0224215912635
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0788
Profiling... [256/50048]	Loss: 2.2851
Profiling... [384/50048]	Loss: 2.0017
Profiling... [512/50048]	Loss: 1.8744
Profiling... [640/50048]	Loss: 2.1750
Profiling... [768/50048]	Loss: 1.9827
Profiling... [896/50048]	Loss: 2.0031
Profiling... [1024/50048]	Loss: 2.1012
Profiling... [1152/50048]	Loss: 2.0678
Profiling... [1280/50048]	Loss: 1.9264
Profiling... [1408/50048]	Loss: 1.9878
Profiling... [1536/50048]	Loss: 2.2065
Profiling... [1664/50048]	Loss: 2.1082
Profile done
epoch 1 train time consumed: 3.55s
Validation Epoch: 7, Average loss: 0.0162, Accuracy: 0.4448
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 126.6598451592897,
                        "time": 2.185140538000269,
                        "accuracy": 0.44481803797468356,
                        "total_cost": 622.2084955335223
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8995
Profiling... [256/50048]	Loss: 2.1202
Profiling... [384/50048]	Loss: 1.9456
Profiling... [512/50048]	Loss: 2.1055
Profiling... [640/50048]	Loss: 1.7433
Profiling... [768/50048]	Loss: 2.0902
Profiling... [896/50048]	Loss: 1.8011
Profiling... [1024/50048]	Loss: 1.9798
Profiling... [1152/50048]	Loss: 2.1865
Profiling... [1280/50048]	Loss: 1.8140
Profiling... [1408/50048]	Loss: 1.6871
Profiling... [1536/50048]	Loss: 1.9752
Profiling... [1664/50048]	Loss: 1.9249
Profile done
epoch 1 train time consumed: 3.29s
Validation Epoch: 7, Average loss: 0.0163, Accuracy: 0.4406
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 126.69617757144124,
                        "time": 2.2198618509996777,
                        "accuracy": 0.440565664556962,
                        "total_cost": 638.3793243196782
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9525
Profiling... [256/50048]	Loss: 1.9188
Profiling... [384/50048]	Loss: 2.1282
Profiling... [512/50048]	Loss: 2.1783
Profiling... [640/50048]	Loss: 2.4064
Profiling... [768/50048]	Loss: 1.9850
Profiling... [896/50048]	Loss: 2.0682
Profiling... [1024/50048]	Loss: 2.3427
Profiling... [1152/50048]	Loss: 2.2478
Profiling... [1280/50048]	Loss: 2.1043
Profiling... [1408/50048]	Loss: 2.2262
Profiling... [1536/50048]	Loss: 1.8969
Profiling... [1664/50048]	Loss: 1.9786
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 7, Average loss: 0.0165, Accuracy: 0.4351
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 126.70957077331441,
                        "time": 2.5035991730001115,
                        "accuracy": 0.435126582278481,
                        "total_cost": 729.0521644026837
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8167
Profiling... [256/50048]	Loss: 2.0427
Profiling... [384/50048]	Loss: 2.0527
Profiling... [512/50048]	Loss: 2.0620
Profiling... [640/50048]	Loss: 2.1712
Profiling... [768/50048]	Loss: 2.1066
Profiling... [896/50048]	Loss: 2.0402
Profiling... [1024/50048]	Loss: 2.0041
Profiling... [1152/50048]	Loss: 2.0883
Profiling... [1280/50048]	Loss: 1.8323
Profiling... [1408/50048]	Loss: 1.9889
Profiling... [1536/50048]	Loss: 2.2144
Profiling... [1664/50048]	Loss: 1.9834
Profile done
epoch 1 train time consumed: 7.46s
Validation Epoch: 7, Average loss: 0.0165, Accuracy: 0.4405
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 126.63341919709667,
                        "time": 5.462507725999785,
                        "accuracy": 0.4404667721518987,
                        "total_cost": 1570.461325276448
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9877
Profiling... [256/50048]	Loss: 1.9564
Profiling... [384/50048]	Loss: 2.1751
Profiling... [512/50048]	Loss: 2.3107
Profiling... [640/50048]	Loss: 2.0365
Profiling... [768/50048]	Loss: 2.0941
Profiling... [896/50048]	Loss: 2.2396
Profiling... [1024/50048]	Loss: 2.1739
Profiling... [1152/50048]	Loss: 1.9300
Profiling... [1280/50048]	Loss: 1.9841
Profiling... [1408/50048]	Loss: 1.9067
Profiling... [1536/50048]	Loss: 1.6756
Profiling... [1664/50048]	Loss: 2.1440
Profile done
epoch 1 train time consumed: 3.35s
Validation Epoch: 7, Average loss: 0.0162, Accuracy: 0.4436
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 126.54381271163032,
                        "time": 2.175937669999712,
                        "accuracy": 0.44363132911392406,
                        "total_cost": 620.6762933866531
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.2501
Profiling... [256/50048]	Loss: 1.8465
Profiling... [384/50048]	Loss: 2.0838
Profiling... [512/50048]	Loss: 2.0214
Profiling... [640/50048]	Loss: 2.2198
Profiling... [768/50048]	Loss: 2.0690
Profiling... [896/50048]	Loss: 1.8073
Profiling... [1024/50048]	Loss: 2.2074
Profiling... [1152/50048]	Loss: 2.2569
Profiling... [1280/50048]	Loss: 2.0528
Profiling... [1408/50048]	Loss: 2.0233
Profiling... [1536/50048]	Loss: 1.7480
Profiling... [1664/50048]	Loss: 1.6107
Profile done
epoch 1 train time consumed: 3.41s
Validation Epoch: 7, Average loss: 0.0161, Accuracy: 0.4461
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 126.58451270691643,
                        "time": 2.2157957559998067,
                        "accuracy": 0.44610363924050633,
                        "total_cost": 628.7449851537119
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9868
Profiling... [256/50048]	Loss: 2.4255
Profiling... [384/50048]	Loss: 1.9212
Profiling... [512/50048]	Loss: 2.0377
Profiling... [640/50048]	Loss: 1.9879
Profiling... [768/50048]	Loss: 1.8863
Profiling... [896/50048]	Loss: 2.0893
Profiling... [1024/50048]	Loss: 1.8854
Profiling... [1152/50048]	Loss: 1.9908
Profiling... [1280/50048]	Loss: 2.1848
Profiling... [1408/50048]	Loss: 1.9007
Profiling... [1536/50048]	Loss: 2.1363
Profiling... [1664/50048]	Loss: 1.9836
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 7, Average loss: 0.0162, Accuracy: 0.4415
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 126.59225904558663,
                        "time": 2.4950150709996706,
                        "accuracy": 0.44145569620253167,
                        "total_cost": 715.4729158726884
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0752
Profiling... [256/50048]	Loss: 1.9899
Profiling... [384/50048]	Loss: 1.9952
Profiling... [512/50048]	Loss: 1.7593
Profiling... [640/50048]	Loss: 2.2080
Profiling... [768/50048]	Loss: 2.2551
Profiling... [896/50048]	Loss: 2.0099
Profiling... [1024/50048]	Loss: 2.1431
Profiling... [1152/50048]	Loss: 2.1604
Profiling... [1280/50048]	Loss: 2.0526
Profiling... [1408/50048]	Loss: 2.0621
Profiling... [1536/50048]	Loss: 1.8694
Profiling... [1664/50048]	Loss: 2.1526
Profile done
epoch 1 train time consumed: 7.36s
Validation Epoch: 7, Average loss: 0.0162, Accuracy: 0.4413
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 126.51617188626311,
                        "time": 5.494820227999753,
                        "accuracy": 0.44125791139240506,
                        "total_cost": 1575.4587113374478
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0302
Profiling... [256/50048]	Loss: 1.9583
Profiling... [384/50048]	Loss: 2.1240
Profiling... [512/50048]	Loss: 2.1499
Profiling... [640/50048]	Loss: 1.8605
Profiling... [768/50048]	Loss: 1.7253
Profiling... [896/50048]	Loss: 2.0157
Profiling... [1024/50048]	Loss: 2.0879
Profiling... [1152/50048]	Loss: 2.1128
Profiling... [1280/50048]	Loss: 2.0030
Profiling... [1408/50048]	Loss: 2.0115
Profiling... [1536/50048]	Loss: 1.9776
Profiling... [1664/50048]	Loss: 2.3054
Profile done
epoch 1 train time consumed: 3.38s
Validation Epoch: 7, Average loss: 0.0176, Accuracy: 0.4102
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 126.42903547383862,
                        "time": 2.1815111190003336,
                        "accuracy": 0.41020569620253167,
                        "total_cost": 672.3610842168612
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.0791
Profiling... [256/50048]	Loss: 2.2647
Profiling... [384/50048]	Loss: 2.0150
Profiling... [512/50048]	Loss: 2.0566
Profiling... [640/50048]	Loss: 2.1454
Profiling... [768/50048]	Loss: 2.1420
Profiling... [896/50048]	Loss: 1.8061
Profiling... [1024/50048]	Loss: 1.9769
Profiling... [1152/50048]	Loss: 1.8175
Profiling... [1280/50048]	Loss: 1.8612
Profiling... [1408/50048]	Loss: 2.1421
Profiling... [1536/50048]	Loss: 2.1424
Profiling... [1664/50048]	Loss: 2.1077
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 7, Average loss: 0.0180, Accuracy: 0.4053
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 126.46242439981013,
                        "time": 2.2249077419996866,
                        "accuracy": 0.4052610759493671,
                        "total_cost": 694.2863349510067
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1716
Profiling... [256/50048]	Loss: 1.9838
Profiling... [384/50048]	Loss: 2.0129
Profiling... [512/50048]	Loss: 2.3696
Profiling... [640/50048]	Loss: 2.1624
Profiling... [768/50048]	Loss: 2.0237
Profiling... [896/50048]	Loss: 2.1942
Profiling... [1024/50048]	Loss: 1.7676
Profiling... [1152/50048]	Loss: 1.9477
Profiling... [1280/50048]	Loss: 1.9402
Profiling... [1408/50048]	Loss: 1.9118
Profiling... [1536/50048]	Loss: 2.0692
Profiling... [1664/50048]	Loss: 1.9316
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 7, Average loss: 0.0185, Accuracy: 0.3930
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 126.4729807061603,
                        "time": 2.5084833920000165,
                        "accuracy": 0.392998417721519,
                        "total_cost": 807.2688268759155
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.2357
Profiling... [256/50048]	Loss: 1.9772
Profiling... [384/50048]	Loss: 2.0263
Profiling... [512/50048]	Loss: 2.0994
Profiling... [640/50048]	Loss: 2.0823
Profiling... [768/50048]	Loss: 1.9615
Profiling... [896/50048]	Loss: 2.1176
Profiling... [1024/50048]	Loss: 2.0980
Profiling... [1152/50048]	Loss: 1.9965
Profiling... [1280/50048]	Loss: 2.1965
Profiling... [1408/50048]	Loss: 2.1309
Profiling... [1536/50048]	Loss: 2.2273
Profiling... [1664/50048]	Loss: 1.9638
Profile done
epoch 1 train time consumed: 7.35s
Validation Epoch: 7, Average loss: 0.0185, Accuracy: 0.3952
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 126.4031752514831,
                        "time": 5.443030623999675,
                        "accuracy": 0.3951740506329114,
                        "total_cost": 1741.0463889587193
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8370
Profiling... [256/50048]	Loss: 2.2107
Profiling... [384/50048]	Loss: 1.7949
Profiling... [512/50048]	Loss: 1.8733
Profiling... [640/50048]	Loss: 1.9131
Profiling... [768/50048]	Loss: 2.1692
Profiling... [896/50048]	Loss: 2.1944
Profiling... [1024/50048]	Loss: 2.1106
Profiling... [1152/50048]	Loss: 2.1827
Profiling... [1280/50048]	Loss: 2.0814
Profiling... [1408/50048]	Loss: 2.4536
Profiling... [1536/50048]	Loss: 1.9914
Profiling... [1664/50048]	Loss: 1.9039
Profile done
epoch 1 train time consumed: 3.43s
Validation Epoch: 7, Average loss: 0.0169, Accuracy: 0.4193
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 126.31281675139229,
                        "time": 2.1841461340000023,
                        "accuracy": 0.41930379746835444,
                        "total_cost": 657.9612492134078
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.0880
Profiling... [256/50048]	Loss: 1.9145
Profiling... [384/50048]	Loss: 1.7590
Profiling... [512/50048]	Loss: 1.9270
Profiling... [640/50048]	Loss: 2.1218
Profiling... [768/50048]	Loss: 2.1823
Profiling... [896/50048]	Loss: 2.0494
Profiling... [1024/50048]	Loss: 1.8877
Profiling... [1152/50048]	Loss: 2.0813
Profiling... [1280/50048]	Loss: 1.7984
Profiling... [1408/50048]	Loss: 2.1464
Profiling... [1536/50048]	Loss: 2.2271
Profiling... [1664/50048]	Loss: 2.2662
Profile done
epoch 1 train time consumed: 3.33s
Validation Epoch: 7, Average loss: 0.0169, Accuracy: 0.4229
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 126.3471623762391,
                        "time": 2.220042915000249,
                        "accuracy": 0.4228639240506329,
                        "total_cost": 663.3247877399199
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1430
Profiling... [256/50048]	Loss: 2.1533
Profiling... [384/50048]	Loss: 2.2533
Profiling... [512/50048]	Loss: 1.9726
Profiling... [640/50048]	Loss: 2.0626
Profiling... [768/50048]	Loss: 2.0670
Profiling... [896/50048]	Loss: 1.9747
Profiling... [1024/50048]	Loss: 1.9415
Profiling... [1152/50048]	Loss: 2.1924
Profiling... [1280/50048]	Loss: 1.8841
Profiling... [1408/50048]	Loss: 2.4907
Profiling... [1536/50048]	Loss: 2.0042
Profiling... [1664/50048]	Loss: 2.2100
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 7, Average loss: 0.0170, Accuracy: 0.4311
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 126.35926231602618,
                        "time": 2.493240356000115,
                        "accuracy": 0.43107199367088606,
                        "total_cost": 730.838506760544
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8934
Profiling... [256/50048]	Loss: 1.8716
Profiling... [384/50048]	Loss: 2.2143
Profiling... [512/50048]	Loss: 1.9109
Profiling... [640/50048]	Loss: 2.1843
Profiling... [768/50048]	Loss: 1.8372
Profiling... [896/50048]	Loss: 1.8980
Profiling... [1024/50048]	Loss: 2.2328
Profiling... [1152/50048]	Loss: 2.0710
Profiling... [1280/50048]	Loss: 2.3912
Profiling... [1408/50048]	Loss: 2.1941
Profiling... [1536/50048]	Loss: 2.0554
Profiling... [1664/50048]	Loss: 1.8156
Profile done
epoch 1 train time consumed: 7.39s
Validation Epoch: 7, Average loss: 0.0175, Accuracy: 0.4199
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 126.28814331463285,
                        "time": 5.452691374000096,
                        "accuracy": 0.41989715189873417,
                        "total_cost": 1639.9498462334352
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1174
Profiling... [256/50048]	Loss: 2.1180
Profiling... [384/50048]	Loss: 2.0888
Profiling... [512/50048]	Loss: 1.9429
Profiling... [640/50048]	Loss: 2.2862
Profiling... [768/50048]	Loss: 1.9995
Profiling... [896/50048]	Loss: 2.4981
Profiling... [1024/50048]	Loss: 1.9530
Profiling... [1152/50048]	Loss: 1.8956
Profiling... [1280/50048]	Loss: 1.9125
Profiling... [1408/50048]	Loss: 2.3364
Profiling... [1536/50048]	Loss: 1.9042
Profiling... [1664/50048]	Loss: 2.0913
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 7, Average loss: 0.0177, Accuracy: 0.4115
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 126.20513115384207,
                        "time": 2.176031829999829,
                        "accuracy": 0.41149129746835444,
                        "total_cost": 667.3929295459363
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1589
Profiling... [256/50048]	Loss: 2.0334
Profiling... [384/50048]	Loss: 1.9708
Profiling... [512/50048]	Loss: 2.1611
Profiling... [640/50048]	Loss: 2.0387
Profiling... [768/50048]	Loss: 1.8053
Profiling... [896/50048]	Loss: 2.0106
Profiling... [1024/50048]	Loss: 2.2180
Profiling... [1152/50048]	Loss: 2.2154
Profiling... [1280/50048]	Loss: 1.9913
Profiling... [1408/50048]	Loss: 1.9414
Profiling... [1536/50048]	Loss: 2.0774
Profiling... [1664/50048]	Loss: 2.1950
Profile done
epoch 1 train time consumed: 3.32s
Validation Epoch: 7, Average loss: 0.0173, Accuracy: 0.4203
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 126.23624172316457,
                        "time": 2.2104624309999963,
                        "accuracy": 0.42029272151898733,
                        "total_cost": 663.9193482847013
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.1107
Profiling... [256/50048]	Loss: 2.2870
Profiling... [384/50048]	Loss: 2.0910
Profiling... [512/50048]	Loss: 2.1170
Profiling... [640/50048]	Loss: 2.2642
Profiling... [768/50048]	Loss: 2.1151
Profiling... [896/50048]	Loss: 1.7427
Profiling... [1024/50048]	Loss: 2.2691
Profiling... [1152/50048]	Loss: 1.9941
Profiling... [1280/50048]	Loss: 1.9729
Profiling... [1408/50048]	Loss: 2.1286
Profiling... [1536/50048]	Loss: 1.7730
Profiling... [1664/50048]	Loss: 2.0908
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 7, Average loss: 0.0170, Accuracy: 0.4190
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 126.24788684726961,
                        "time": 2.5117917419997866,
                        "accuracy": 0.41900712025316456,
                        "total_cost": 756.8090953592817
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9391
Profiling... [256/50048]	Loss: 1.8920
Profiling... [384/50048]	Loss: 2.1271
Profiling... [512/50048]	Loss: 2.1989
Profiling... [640/50048]	Loss: 2.3244
Profiling... [768/50048]	Loss: 2.1058
Profiling... [896/50048]	Loss: 1.9327
Profiling... [1024/50048]	Loss: 1.8576
Profiling... [1152/50048]	Loss: 1.8401
Profiling... [1280/50048]	Loss: 2.0400
Profiling... [1408/50048]	Loss: 2.0170
Profiling... [1536/50048]	Loss: 2.0642
Profiling... [1664/50048]	Loss: 1.9431
Profile done
epoch 1 train time consumed: 7.43s
Validation Epoch: 7, Average loss: 0.0199, Accuracy: 0.3741
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 126.17619402900607,
                        "time": 5.496399554999698,
                        "accuracy": 0.3741099683544304,
                        "total_cost": 1853.7725144376566
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.3173
Profiling... [256/50048]	Loss: 1.9838
Profiling... [384/50048]	Loss: 1.9513
Profiling... [512/50048]	Loss: 1.7234
Profiling... [640/50048]	Loss: 2.1045
Profiling... [768/50048]	Loss: 2.1327
Profiling... [896/50048]	Loss: 2.4150
Profiling... [1024/50048]	Loss: 2.3057
Profiling... [1152/50048]	Loss: 2.4567
Profiling... [1280/50048]	Loss: 2.4781
Profiling... [1408/50048]	Loss: 2.3194
Profiling... [1536/50048]	Loss: 2.2596
Profiling... [1664/50048]	Loss: 2.0650
Profile done
epoch 1 train time consumed: 3.28s
Validation Epoch: 7, Average loss: 0.0215, Accuracy: 0.3167
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 126.08855483626265,
                        "time": 2.1700032280000414,
                        "accuracy": 0.3166534810126582,
                        "total_cost": 864.075677088838
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.0211
Profiling... [256/50048]	Loss: 1.9200
Profiling... [384/50048]	Loss: 1.9998
Profiling... [512/50048]	Loss: 2.0076
Profiling... [640/50048]	Loss: 2.4692
Profiling... [768/50048]	Loss: 2.3552
Profiling... [896/50048]	Loss: 2.1659
Profiling... [1024/50048]	Loss: 2.0406
Profiling... [1152/50048]	Loss: 2.0817
Profiling... [1280/50048]	Loss: 2.4105
Profiling... [1408/50048]	Loss: 2.1289
Profiling... [1536/50048]	Loss: 2.4209
Profiling... [1664/50048]	Loss: 2.1409
Profile done
epoch 1 train time consumed: 3.45s
Validation Epoch: 7, Average loss: 0.0200, Accuracy: 0.3504
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 126.12299981489458,
                        "time": 2.209797783999875,
                        "accuracy": 0.3503757911392405,
                        "total_cost": 795.4497215579938
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0308
Profiling... [256/50048]	Loss: 1.9279
Profiling... [384/50048]	Loss: 2.3009
Profiling... [512/50048]	Loss: 2.2608
Profiling... [640/50048]	Loss: 2.1948
Profiling... [768/50048]	Loss: 2.1686
Profiling... [896/50048]	Loss: 2.1873
Profiling... [1024/50048]	Loss: 2.0346
Profiling... [1152/50048]	Loss: 2.1293
Profiling... [1280/50048]	Loss: 2.1578
Profiling... [1408/50048]	Loss: 2.2128
Profiling... [1536/50048]	Loss: 2.1676
Profiling... [1664/50048]	Loss: 2.2192
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 7, Average loss: 0.0195, Accuracy: 0.3791
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 126.13239631787751,
                        "time": 2.4905720349997864,
                        "accuracy": 0.37905458860759494,
                        "total_cost": 828.7508670737178
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.1116
Profiling... [256/50048]	Loss: 2.2567
Profiling... [384/50048]	Loss: 1.9911
Profiling... [512/50048]	Loss: 1.9803
Profiling... [640/50048]	Loss: 2.1852
Profiling... [768/50048]	Loss: 2.0819
Profiling... [896/50048]	Loss: 2.1804
Profiling... [1024/50048]	Loss: 2.1249
Profiling... [1152/50048]	Loss: 2.1791
Profiling... [1280/50048]	Loss: 2.0122
Profiling... [1408/50048]	Loss: 2.3299
Profiling... [1536/50048]	Loss: 2.4204
Profiling... [1664/50048]	Loss: 2.3911
Profile done
epoch 1 train time consumed: 7.37s
Validation Epoch: 7, Average loss: 0.0217, Accuracy: 0.3170
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 126.06081606558257,
                        "time": 5.4674124439998195,
                        "accuracy": 0.3169501582278481,
                        "total_cost": 2174.557912548097
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.0179
Profiling... [256/50048]	Loss: 1.8624
Profiling... [384/50048]	Loss: 2.1362
Profiling... [512/50048]	Loss: 1.8789
Profiling... [640/50048]	Loss: 2.2245
Profiling... [768/50048]	Loss: 2.2459
Profiling... [896/50048]	Loss: 2.3535
Profiling... [1024/50048]	Loss: 1.9957
Profiling... [1152/50048]	Loss: 2.1200
Profiling... [1280/50048]	Loss: 2.2555
Profiling... [1408/50048]	Loss: 1.8982
Profiling... [1536/50048]	Loss: 1.8472
Profiling... [1664/50048]	Loss: 2.1560
Profile done
epoch 1 train time consumed: 3.41s
Validation Epoch: 7, Average loss: 0.0234, Accuracy: 0.3059
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 125.97553275888862,
                        "time": 2.1763431250001304,
                        "accuracy": 0.3058742088607595,
                        "total_cost": 896.3357376850371
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1221
Profiling... [256/50048]	Loss: 2.0386
Profiling... [384/50048]	Loss: 2.0906
Profiling... [512/50048]	Loss: 1.7667
Profiling... [640/50048]	Loss: 1.9555
Profiling... [768/50048]	Loss: 2.2304
Profiling... [896/50048]	Loss: 2.1272
Profiling... [1024/50048]	Loss: 2.2281
Profiling... [1152/50048]	Loss: 2.2201
Profiling... [1280/50048]	Loss: 2.4096
Profiling... [1408/50048]	Loss: 2.3600
Profiling... [1536/50048]	Loss: 2.1955
Profiling... [1664/50048]	Loss: 2.3694
Profile done
epoch 1 train time consumed: 3.44s
Validation Epoch: 7, Average loss: 0.0198, Accuracy: 0.3593
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 126.01067125907564,
                        "time": 2.20047377300034,
                        "accuracy": 0.3592761075949367,
                        "total_cost": 771.7829584604177
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0333
Profiling... [256/50048]	Loss: 2.2914
Profiling... [384/50048]	Loss: 2.1546
Profiling... [512/50048]	Loss: 2.3262
Profiling... [640/50048]	Loss: 2.2316
Profiling... [768/50048]	Loss: 2.1997
Profiling... [896/50048]	Loss: 2.1760
Profiling... [1024/50048]	Loss: 2.0591
Profiling... [1152/50048]	Loss: 2.1766
Profiling... [1280/50048]	Loss: 2.0634
Profiling... [1408/50048]	Loss: 1.9191
Profiling... [1536/50048]	Loss: 2.2703
Profiling... [1664/50048]	Loss: 1.8785
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 7, Average loss: 0.0232, Accuracy: 0.2939
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 126.01915075790171,
                        "time": 2.482284583999899,
                        "accuracy": 0.2939082278481013,
                        "total_cost": 1064.33017376012
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.0648
Profiling... [256/50048]	Loss: 2.1270
Profiling... [384/50048]	Loss: 2.0848
Profiling... [512/50048]	Loss: 2.1918
Profiling... [640/50048]	Loss: 2.1791
Profiling... [768/50048]	Loss: 1.9441
Profiling... [896/50048]	Loss: 2.1246
Profiling... [1024/50048]	Loss: 2.0951
Profiling... [1152/50048]	Loss: 2.2586
Profiling... [1280/50048]	Loss: 2.2332
Profiling... [1408/50048]	Loss: 2.0869
Profiling... [1536/50048]	Loss: 2.3209
Profiling... [1664/50048]	Loss: 2.2496
Profile done
epoch 1 train time consumed: 7.35s
Validation Epoch: 7, Average loss: 0.0226, Accuracy: 0.3321
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 125.95239878735056,
                        "time": 5.508764797000367,
                        "accuracy": 0.33208069620253167,
                        "total_cost": 2089.378119450651
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 2.1254
Profiling... [256/50048]	Loss: 2.0799
Profiling... [384/50048]	Loss: 1.8955
Profiling... [512/50048]	Loss: 1.9856
Profiling... [640/50048]	Loss: 2.2156
Profiling... [768/50048]	Loss: 2.3038
Profiling... [896/50048]	Loss: 1.9555
Profiling... [1024/50048]	Loss: 2.2295
Profiling... [1152/50048]	Loss: 2.1991
Profiling... [1280/50048]	Loss: 2.4248
Profiling... [1408/50048]	Loss: 2.1126
Profiling... [1536/50048]	Loss: 2.1625
Profiling... [1664/50048]	Loss: 2.1875
Profile done
epoch 1 train time consumed: 3.39s
Validation Epoch: 7, Average loss: 0.0248, Accuracy: 0.2850
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 125.87390005144408,
                        "time": 2.173192819000178,
                        "accuracy": 0.28500791139240506,
                        "total_cost": 959.791797901067
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 2.1790
Profiling... [256/50048]	Loss: 2.0456
Profiling... [384/50048]	Loss: 2.2940
Profiling... [512/50048]	Loss: 2.0746
Profiling... [640/50048]	Loss: 2.2182
Profiling... [768/50048]	Loss: 2.2281
Profiling... [896/50048]	Loss: 2.3217
Profiling... [1024/50048]	Loss: 1.8995
Profiling... [1152/50048]	Loss: 2.2033
Profiling... [1280/50048]	Loss: 2.1506
Profiling... [1408/50048]	Loss: 2.4699
Profiling... [1536/50048]	Loss: 2.2563
Profiling... [1664/50048]	Loss: 2.0652
Profile done
epoch 1 train time consumed: 3.41s
Validation Epoch: 7, Average loss: 0.0222, Accuracy: 0.3091
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 125.90695240283353,
                        "time": 2.2053282250003576,
                        "accuracy": 0.3091376582278481,
                        "total_cost": 898.1958311047732
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.2510
Profiling... [256/50048]	Loss: 1.8285
Profiling... [384/50048]	Loss: 2.0008
Profiling... [512/50048]	Loss: 2.1059
Profiling... [640/50048]	Loss: 2.2733
Profiling... [768/50048]	Loss: 2.3569
Profiling... [896/50048]	Loss: 2.2461
Profiling... [1024/50048]	Loss: 2.0600
Profiling... [1152/50048]	Loss: 1.9482
Profiling... [1280/50048]	Loss: 2.1617
Profiling... [1408/50048]	Loss: 1.9798
Profiling... [1536/50048]	Loss: 2.2820
Profiling... [1664/50048]	Loss: 1.9533
Profile done
epoch 1 train time consumed: 3.62s
Validation Epoch: 7, Average loss: 0.0362, Accuracy: 0.1912
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 125.91755393100559,
                        "time": 2.4990222400001585,
                        "accuracy": 0.19115901898734178,
                        "total_cost": 1646.120435995956
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 2.3013
Profiling... [256/50048]	Loss: 2.1172
Profiling... [384/50048]	Loss: 2.1514
Profiling... [512/50048]	Loss: 2.3932
Profiling... [640/50048]	Loss: 2.2811
Profiling... [768/50048]	Loss: 2.2034
Profiling... [896/50048]	Loss: 2.0304
Profiling... [1024/50048]	Loss: 2.2567
Profiling... [1152/50048]	Loss: 2.0918
Profiling... [1280/50048]	Loss: 2.1027
Profiling... [1408/50048]	Loss: 2.2588
Profiling... [1536/50048]	Loss: 2.1824
Profiling... [1664/50048]	Loss: 2.2096
Profile done
epoch 1 train time consumed: 7.42s
Validation Epoch: 7, Average loss: 0.0250, Accuracy: 0.2595
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 125.85148234439175,
                        "time": 5.492886407999777,
                        "accuracy": 0.25949367088607594,
                        "total_cost": 2663.9875047265623
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.2551
Profiling... [512/50176]	Loss: 1.9171
Profiling... [768/50176]	Loss: 1.9656
Profiling... [1024/50176]	Loss: 1.8930
Profiling... [1280/50176]	Loss: 2.1250
Profiling... [1536/50176]	Loss: 1.8451
Profiling... [1792/50176]	Loss: 2.0847
Profiling... [2048/50176]	Loss: 2.0717
Profiling... [2304/50176]	Loss: 2.0155
Profiling... [2560/50176]	Loss: 1.8975
Profiling... [2816/50176]	Loss: 2.0961
Profiling... [3072/50176]	Loss: 1.8509
Profiling... [3328/50176]	Loss: 1.8837
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 7, Average loss: 0.0079, Accuracy: 0.4469
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 125.7797756125033,
                        "time": 2.526470397000139,
                        "accuracy": 0.446875,
                        "total_cost": 711.1135767861474
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.2721
Profiling... [512/50176]	Loss: 2.1251
Profiling... [768/50176]	Loss: 1.9005
Profiling... [1024/50176]	Loss: 2.0252
Profiling... [1280/50176]	Loss: 1.9510
Profiling... [1536/50176]	Loss: 2.0032
Profiling... [1792/50176]	Loss: 2.2975
Profiling... [2048/50176]	Loss: 1.9361
Profiling... [2304/50176]	Loss: 2.0102
Profiling... [2560/50176]	Loss: 2.1066
Profiling... [2816/50176]	Loss: 1.8205
Profiling... [3072/50176]	Loss: 1.9236
Profiling... [3328/50176]	Loss: 1.8916
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 7, Average loss: 0.0079, Accuracy: 0.4463
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 125.81686993763485,
                        "time": 2.587727547999748,
                        "accuracy": 0.4462890625,
                        "total_cost": 729.5266850522895
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9951
Profiling... [512/50176]	Loss: 1.9449
Profiling... [768/50176]	Loss: 2.1524
Profiling... [1024/50176]	Loss: 2.0164
Profiling... [1280/50176]	Loss: 2.1128
Profiling... [1536/50176]	Loss: 2.1590
Profiling... [1792/50176]	Loss: 2.0168
Profiling... [2048/50176]	Loss: 1.9857
Profiling... [2304/50176]	Loss: 1.8002
Profiling... [2560/50176]	Loss: 1.8850
Profiling... [2816/50176]	Loss: 1.9341
Profiling... [3072/50176]	Loss: 1.8923
Profiling... [3328/50176]	Loss: 2.0131
Profile done
epoch 1 train time consumed: 4.32s
Validation Epoch: 7, Average loss: 0.0081, Accuracy: 0.4384
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 125.82498498353864,
                        "time": 2.965048854000088,
                        "accuracy": 0.43837890625,
                        "total_cost": 851.038273536957
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9666
Profiling... [512/50176]	Loss: 2.1479
Profiling... [768/50176]	Loss: 2.1771
Profiling... [1024/50176]	Loss: 2.0572
Profiling... [1280/50176]	Loss: 2.0180
Profiling... [1536/50176]	Loss: 1.8798
Profiling... [1792/50176]	Loss: 1.9828
Profiling... [2048/50176]	Loss: 2.0427
Profiling... [2304/50176]	Loss: 1.9752
Profiling... [2560/50176]	Loss: 2.2230
Profiling... [2816/50176]	Loss: 1.9119
Profiling... [3072/50176]	Loss: 1.9107
Profiling... [3328/50176]	Loss: 1.9873
Profile done
epoch 1 train time consumed: 10.04s
Validation Epoch: 7, Average loss: 0.0079, Accuracy: 0.4522
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 125.74146059560645,
                        "time": 7.4380286119999255,
                        "accuracy": 0.45224609375,
                        "total_cost": 2068.0523160953935
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0571
Profiling... [512/50176]	Loss: 2.0463
Profiling... [768/50176]	Loss: 2.0891
Profiling... [1024/50176]	Loss: 2.0668
Profiling... [1280/50176]	Loss: 1.9057
Profiling... [1536/50176]	Loss: 2.1475
Profiling... [1792/50176]	Loss: 1.8386
Profiling... [2048/50176]	Loss: 2.2274
Profiling... [2304/50176]	Loss: 1.9534
Profiling... [2560/50176]	Loss: 1.9907
Profiling... [2816/50176]	Loss: 2.0477
Profiling... [3072/50176]	Loss: 1.8958
Profiling... [3328/50176]	Loss: 1.8664
Profile done
epoch 1 train time consumed: 3.97s
Validation Epoch: 7, Average loss: 0.0080, Accuracy: 0.4460
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 125.68391596208201,
                        "time": 2.5106993059998786,
                        "accuracy": 0.44599609375,
                        "total_cost": 707.5275434098942
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8595
Profiling... [512/50176]	Loss: 1.9644
Profiling... [768/50176]	Loss: 1.9228
Profiling... [1024/50176]	Loss: 2.2390
Profiling... [1280/50176]	Loss: 1.9144
Profiling... [1536/50176]	Loss: 1.9476
Profiling... [1792/50176]	Loss: 1.9415
Profiling... [2048/50176]	Loss: 2.1404
Profiling... [2304/50176]	Loss: 2.0950
Profiling... [2560/50176]	Loss: 1.7843
Profiling... [2816/50176]	Loss: 1.9658
Profiling... [3072/50176]	Loss: 2.0245
Profiling... [3328/50176]	Loss: 2.0117
Profile done
epoch 1 train time consumed: 4.01s
Validation Epoch: 7, Average loss: 0.0080, Accuracy: 0.4404
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 125.71974097008084,
                        "time": 2.5945525629999793,
                        "accuracy": 0.4404296875,
                        "total_cost": 740.6096487390323
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9077
Profiling... [512/50176]	Loss: 2.0667
Profiling... [768/50176]	Loss: 2.0673
Profiling... [1024/50176]	Loss: 2.1133
Profiling... [1280/50176]	Loss: 2.2209
Profiling... [1536/50176]	Loss: 2.0699
Profiling... [1792/50176]	Loss: 2.1215
Profiling... [2048/50176]	Loss: 1.8872
Profiling... [2304/50176]	Loss: 1.8779
Profiling... [2560/50176]	Loss: 2.0071
Profiling... [2816/50176]	Loss: 1.9153
Profiling... [3072/50176]	Loss: 2.1090
Profiling... [3328/50176]	Loss: 2.0126
Profile done
epoch 1 train time consumed: 4.26s
Validation Epoch: 7, Average loss: 0.0078, Accuracy: 0.4495
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 125.72646535889456,
                        "time": 2.956685357999959,
                        "accuracy": 0.44951171875,
                        "total_cost": 826.9719870117015
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0890
Profiling... [512/50176]	Loss: 2.0153
Profiling... [768/50176]	Loss: 1.9426
Profiling... [1024/50176]	Loss: 2.0746
Profiling... [1280/50176]	Loss: 2.0442
Profiling... [1536/50176]	Loss: 1.9760
Profiling... [1792/50176]	Loss: 2.0691
Profiling... [2048/50176]	Loss: 1.9730
Profiling... [2304/50176]	Loss: 2.1479
Profiling... [2560/50176]	Loss: 1.9315
Profiling... [2816/50176]	Loss: 1.8083
Profiling... [3072/50176]	Loss: 1.7828
Profiling... [3328/50176]	Loss: 2.0490
Profile done
epoch 1 train time consumed: 10.15s
Validation Epoch: 7, Average loss: 0.0080, Accuracy: 0.4435
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 125.64428990727575,
                        "time": 7.537183799000104,
                        "accuracy": 0.44345703125,
                        "total_cost": 2135.5036443026092
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.1227
Profiling... [512/50176]	Loss: 2.1516
Profiling... [768/50176]	Loss: 1.9156
Profiling... [1024/50176]	Loss: 2.0029
Profiling... [1280/50176]	Loss: 2.1197
Profiling... [1536/50176]	Loss: 1.9641
Profiling... [1792/50176]	Loss: 1.9748
Profiling... [2048/50176]	Loss: 2.0191
Profiling... [2304/50176]	Loss: 2.0977
Profiling... [2560/50176]	Loss: 2.1242
Profiling... [2816/50176]	Loss: 2.0735
Profiling... [3072/50176]	Loss: 2.0444
Profiling... [3328/50176]	Loss: 2.0762
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 7, Average loss: 0.0080, Accuracy: 0.4415
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 125.58706621911412,
                        "time": 2.5037641409999196,
                        "accuracy": 0.44150390625,
                        "total_cost": 712.2029692637635
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9738
Profiling... [512/50176]	Loss: 2.0286
Profiling... [768/50176]	Loss: 1.9922
Profiling... [1024/50176]	Loss: 2.1386
Profiling... [1280/50176]	Loss: 2.0106
Profiling... [1536/50176]	Loss: 2.0488
Profiling... [1792/50176]	Loss: 2.1024
Profiling... [2048/50176]	Loss: 1.9579
Profiling... [2304/50176]	Loss: 1.8817
Profiling... [2560/50176]	Loss: 1.9530
Profiling... [2816/50176]	Loss: 1.9560
Profiling... [3072/50176]	Loss: 1.9728
Profiling... [3328/50176]	Loss: 1.9855
Profile done
epoch 1 train time consumed: 4.01s
Validation Epoch: 7, Average loss: 0.0080, Accuracy: 0.4431
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 125.6197100616139,
                        "time": 2.5922605370001293,
                        "accuracy": 0.44306640625,
                        "total_cost": 734.9666155424525
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9439
Profiling... [512/50176]	Loss: 2.0744
Profiling... [768/50176]	Loss: 2.1193
Profiling... [1024/50176]	Loss: 2.2002
Profiling... [1280/50176]	Loss: 1.9769
Profiling... [1536/50176]	Loss: 2.0134
Profiling... [1792/50176]	Loss: 1.9185
Profiling... [2048/50176]	Loss: 1.8743
Profiling... [2304/50176]	Loss: 2.0664
Profiling... [2560/50176]	Loss: 2.0079
Profiling... [2816/50176]	Loss: 1.9519
Profiling... [3072/50176]	Loss: 1.8496
Profiling... [3328/50176]	Loss: 1.9114
Profile done
epoch 1 train time consumed: 4.29s
Validation Epoch: 7, Average loss: 0.0081, Accuracy: 0.4420
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 125.62506435601328,
                        "time": 2.949874932000057,
                        "accuracy": 0.4419921875,
                        "total_cost": 838.4271004217636
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9358
Profiling... [512/50176]	Loss: 2.1068
Profiling... [768/50176]	Loss: 1.9741
Profiling... [1024/50176]	Loss: 2.0418
Profiling... [1280/50176]	Loss: 2.0481
Profiling... [1536/50176]	Loss: 1.9409
Profiling... [1792/50176]	Loss: 1.9040
Profiling... [2048/50176]	Loss: 1.9649
Profiling... [2304/50176]	Loss: 1.9640
Profiling... [2560/50176]	Loss: 2.0809
Profiling... [2816/50176]	Loss: 2.0823
Profiling... [3072/50176]	Loss: 1.9914
Profiling... [3328/50176]	Loss: 2.1117
Profile done
epoch 1 train time consumed: 10.41s
Validation Epoch: 7, Average loss: 0.0079, Accuracy: 0.4455
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 125.53755983706932,
                        "time": 7.487363480000113,
                        "accuracy": 0.4455078125,
                        "total_cost": 2109.829086089039
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.1199
Profiling... [512/50176]	Loss: 1.9863
Profiling... [768/50176]	Loss: 2.0695
Profiling... [1024/50176]	Loss: 2.0861
Profiling... [1280/50176]	Loss: 2.0706
Profiling... [1536/50176]	Loss: 2.0149
Profiling... [1792/50176]	Loss: 1.8461
Profiling... [2048/50176]	Loss: 1.9324
Profiling... [2304/50176]	Loss: 1.8551
Profiling... [2560/50176]	Loss: 1.9709
Profiling... [2816/50176]	Loss: 1.9468
Profiling... [3072/50176]	Loss: 2.0578
Profiling... [3328/50176]	Loss: 1.9242
Profile done
epoch 1 train time consumed: 3.84s
Validation Epoch: 7, Average loss: 0.0086, Accuracy: 0.4180
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 125.47967273216942,
                        "time": 2.4417680000001383,
                        "accuracy": 0.41796875,
                        "total_cost": 733.0506156929224
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.1541
Profiling... [512/50176]	Loss: 1.8787
Profiling... [768/50176]	Loss: 2.1301
Profiling... [1024/50176]	Loss: 2.0885
Profiling... [1280/50176]	Loss: 2.3983
Profiling... [1536/50176]	Loss: 1.9691
Profiling... [1792/50176]	Loss: 2.0975
Profiling... [2048/50176]	Loss: 1.9872
Profiling... [2304/50176]	Loss: 2.0297
Profiling... [2560/50176]	Loss: 1.9387
Profiling... [2816/50176]	Loss: 2.0199
Profiling... [3072/50176]	Loss: 1.9828
Profiling... [3328/50176]	Loss: 1.7768
Profile done
epoch 1 train time consumed: 3.85s
Validation Epoch: 7, Average loss: 0.0084, Accuracy: 0.4182
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 125.51611790952498,
                        "time": 2.4945450979998895,
                        "accuracy": 0.4181640625,
                        "total_cost": 748.762614317632
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9143
Profiling... [512/50176]	Loss: 2.0372
Profiling... [768/50176]	Loss: 1.8773
Profiling... [1024/50176]	Loss: 1.8187
Profiling... [1280/50176]	Loss: 2.0907
Profiling... [1536/50176]	Loss: 2.0085
Profiling... [1792/50176]	Loss: 2.0809
Profiling... [2048/50176]	Loss: 1.9277
Profiling... [2304/50176]	Loss: 1.8681
Profiling... [2560/50176]	Loss: 1.9535
Profiling... [2816/50176]	Loss: 1.9710
Profiling... [3072/50176]	Loss: 1.9567
Profiling... [3328/50176]	Loss: 1.7673
Profile done
epoch 1 train time consumed: 4.20s
Validation Epoch: 7, Average loss: 0.0088, Accuracy: 0.4188
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 125.52405952922518,
                        "time": 2.8916930269997465,
                        "accuracy": 0.41884765625,
                        "total_cost": 866.6087591635212
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9526
Profiling... [512/50176]	Loss: 1.9698
Profiling... [768/50176]	Loss: 2.0620
Profiling... [1024/50176]	Loss: 1.9229
Profiling... [1280/50176]	Loss: 1.8165
Profiling... [1536/50176]	Loss: 2.0023
Profiling... [1792/50176]	Loss: 1.9171
Profiling... [2048/50176]	Loss: 1.9109
Profiling... [2304/50176]	Loss: 1.8714
Profiling... [2560/50176]	Loss: 1.9382
Profiling... [2816/50176]	Loss: 1.9117
Profiling... [3072/50176]	Loss: 1.8948
Profiling... [3328/50176]	Loss: 1.9109
Profile done
epoch 1 train time consumed: 10.05s
Validation Epoch: 7, Average loss: 0.0085, Accuracy: 0.4229
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 125.44886238331064,
                        "time": 7.369300316000135,
                        "accuracy": 0.4228515625,
                        "total_cost": 2186.2762803511896
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.2541
Profiling... [512/50176]	Loss: 2.1286
Profiling... [768/50176]	Loss: 2.2384
Profiling... [1024/50176]	Loss: 1.9971
Profiling... [1280/50176]	Loss: 2.0705
Profiling... [1536/50176]	Loss: 1.9377
Profiling... [1792/50176]	Loss: 1.9337
Profiling... [2048/50176]	Loss: 2.2175
Profiling... [2304/50176]	Loss: 1.8794
Profiling... [2560/50176]	Loss: 2.0069
Profiling... [2816/50176]	Loss: 1.9228
Profiling... [3072/50176]	Loss: 1.9246
Profiling... [3328/50176]	Loss: 2.1566
Profile done
epoch 1 train time consumed: 3.81s
Validation Epoch: 7, Average loss: 0.0091, Accuracy: 0.4020
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 125.38709392612846,
                        "time": 2.442855401000088,
                        "accuracy": 0.401953125,
                        "total_cost": 762.035472701321
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.9379
Profiling... [512/50176]	Loss: 1.8448
Profiling... [768/50176]	Loss: 2.0801
Profiling... [1024/50176]	Loss: 1.9688
Profiling... [1280/50176]	Loss: 2.2176
Profiling... [1536/50176]	Loss: 1.9016
Profiling... [1792/50176]	Loss: 2.1519
Profiling... [2048/50176]	Loss: 1.9036
Profiling... [2304/50176]	Loss: 2.1407
Profiling... [2560/50176]	Loss: 2.0173
Profiling... [2816/50176]	Loss: 1.9513
Profiling... [3072/50176]	Loss: 1.8112
Profiling... [3328/50176]	Loss: 1.8378
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 7, Average loss: 0.0086, Accuracy: 0.4164
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 125.4223665463297,
                        "time": 2.5254786599998624,
                        "accuracy": 0.41640625,
                        "total_cost": 760.6790488841998
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.1741
Profiling... [512/50176]	Loss: 1.9976
Profiling... [768/50176]	Loss: 1.9604
Profiling... [1024/50176]	Loss: 1.8241
Profiling... [1280/50176]	Loss: 1.9719
Profiling... [1536/50176]	Loss: 1.9108
Profiling... [1792/50176]	Loss: 1.8041
Profiling... [2048/50176]	Loss: 2.0674
Profiling... [2304/50176]	Loss: 1.8688
Profiling... [2560/50176]	Loss: 1.9256
Profiling... [2816/50176]	Loss: 1.8288
Profiling... [3072/50176]	Loss: 1.8295
Profiling... [3328/50176]	Loss: 1.9340
Profile done
epoch 1 train time consumed: 4.20s
Validation Epoch: 7, Average loss: 0.0082, Accuracy: 0.4350
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 125.43009897668244,
                        "time": 2.8892271760000767,
                        "accuracy": 0.4349609375,
                        "total_cost": 833.1691869498289
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1652
Profiling... [512/50176]	Loss: 2.0291
Profiling... [768/50176]	Loss: 2.0988
Profiling... [1024/50176]	Loss: 2.1222
Profiling... [1280/50176]	Loss: 1.9551
Profiling... [1536/50176]	Loss: 1.8156
Profiling... [1792/50176]	Loss: 2.0571
Profiling... [2048/50176]	Loss: 1.9452
Profiling... [2304/50176]	Loss: 2.0460
Profiling... [2560/50176]	Loss: 1.8670
Profiling... [2816/50176]	Loss: 1.8663
Profiling... [3072/50176]	Loss: 1.8846
Profiling... [3328/50176]	Loss: 1.9908
Profile done
epoch 1 train time consumed: 10.18s
Validation Epoch: 7, Average loss: 0.0099, Accuracy: 0.3624
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 125.3535696089136,
                        "time": 7.4848672090001855,
                        "accuracy": 0.36240234375,
                        "total_cost": 2588.9866301309744
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0142
Profiling... [512/50176]	Loss: 2.0079
Profiling... [768/50176]	Loss: 2.0617
Profiling... [1024/50176]	Loss: 2.2012
Profiling... [1280/50176]	Loss: 1.9724
Profiling... [1536/50176]	Loss: 1.9050
Profiling... [1792/50176]	Loss: 1.9233
Profiling... [2048/50176]	Loss: 1.9020
Profiling... [2304/50176]	Loss: 1.9114
Profiling... [2560/50176]	Loss: 1.8087
Profiling... [2816/50176]	Loss: 1.7992
Profiling... [3072/50176]	Loss: 1.8307
Profiling... [3328/50176]	Loss: 1.8271
Profile done
epoch 1 train time consumed: 3.92s
Validation Epoch: 7, Average loss: 0.0089, Accuracy: 0.3999
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 125.30395685169543,
                        "time": 2.444181413000024,
                        "accuracy": 0.39990234375,
                        "total_cost": 765.8509811178644
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0521
Profiling... [512/50176]	Loss: 2.2152
Profiling... [768/50176]	Loss: 2.2248
Profiling... [1024/50176]	Loss: 1.9834
Profiling... [1280/50176]	Loss: 1.9963
Profiling... [1536/50176]	Loss: 2.1102
Profiling... [1792/50176]	Loss: 1.8150
Profiling... [2048/50176]	Loss: 1.8581
Profiling... [2304/50176]	Loss: 2.1506
Profiling... [2560/50176]	Loss: 1.9807
Profiling... [2816/50176]	Loss: 2.0409
Profiling... [3072/50176]	Loss: 1.8877
Profiling... [3328/50176]	Loss: 1.7773
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 7, Average loss: 0.0083, Accuracy: 0.4268
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 125.33478153191425,
                        "time": 2.531586515000072,
                        "accuracy": 0.4267578125,
                        "total_cost": 743.5033020905133
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 2.0019
Profiling... [512/50176]	Loss: 1.8687
Profiling... [768/50176]	Loss: 2.0404
Profiling... [1024/50176]	Loss: 1.8467
Profiling... [1280/50176]	Loss: 2.0254
Profiling... [1536/50176]	Loss: 1.9075
Profiling... [1792/50176]	Loss: 1.9754
Profiling... [2048/50176]	Loss: 1.9642
Profiling... [2304/50176]	Loss: 2.0412
Profiling... [2560/50176]	Loss: 1.8843
Profiling... [2816/50176]	Loss: 1.8966
Profiling... [3072/50176]	Loss: 2.1400
Profiling... [3328/50176]	Loss: 2.0193
Profile done
epoch 1 train time consumed: 4.22s
Validation Epoch: 7, Average loss: 0.0106, Accuracy: 0.3552
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 125.34148216375014,
                        "time": 2.896170733999952,
                        "accuracy": 0.35517578125,
                        "total_cost": 1022.058235843833
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0931
Profiling... [512/50176]	Loss: 2.0428
Profiling... [768/50176]	Loss: 2.0221
Profiling... [1024/50176]	Loss: 2.0874
Profiling... [1280/50176]	Loss: 1.7738
Profiling... [1536/50176]	Loss: 1.8178
Profiling... [1792/50176]	Loss: 2.2757
Profiling... [2048/50176]	Loss: 1.9597
Profiling... [2304/50176]	Loss: 1.7162
Profiling... [2560/50176]	Loss: 1.8089
Profiling... [2816/50176]	Loss: 2.0622
Profiling... [3072/50176]	Loss: 1.8585
Profiling... [3328/50176]	Loss: 2.0658
Profile done
epoch 1 train time consumed: 10.22s
Validation Epoch: 7, Average loss: 0.0088, Accuracy: 0.4097
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 125.26405656091796,
                        "time": 7.30493104199968,
                        "accuracy": 0.40966796875,
                        "total_cost": 2233.6266562667483
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9466
Profiling... [512/50176]	Loss: 2.0683
Profiling... [768/50176]	Loss: 1.9052
Profiling... [1024/50176]	Loss: 2.0913
Profiling... [1280/50176]	Loss: 2.0782
Profiling... [1536/50176]	Loss: 1.8826
Profiling... [1792/50176]	Loss: 1.9937
Profiling... [2048/50176]	Loss: 2.3334
Profiling... [2304/50176]	Loss: 2.3476
Profiling... [2560/50176]	Loss: 2.0038
Profiling... [2816/50176]	Loss: 2.1288
Profiling... [3072/50176]	Loss: 2.0810
Profiling... [3328/50176]	Loss: 2.1278
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 7, Average loss: 0.0103, Accuracy: 0.3459
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 125.20175506204319,
                        "time": 2.4485020500001156,
                        "accuracy": 0.3458984375,
                        "total_cost": 886.2623264466904
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0852
Profiling... [512/50176]	Loss: 2.0221
Profiling... [768/50176]	Loss: 1.9789
Profiling... [1024/50176]	Loss: 1.9586
Profiling... [1280/50176]	Loss: 1.9864
Profiling... [1536/50176]	Loss: 1.8207
Profiling... [1792/50176]	Loss: 2.2160
Profiling... [2048/50176]	Loss: 2.1312
Profiling... [2304/50176]	Loss: 2.0378
Profiling... [2560/50176]	Loss: 1.9930
Profiling... [2816/50176]	Loss: 1.9015
Profiling... [3072/50176]	Loss: 2.0710
Profiling... [3328/50176]	Loss: 2.0081
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 7, Average loss: 0.0099, Accuracy: 0.3386
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 125.23155148578454,
                        "time": 2.5339417950003735,
                        "accuracy": 0.33857421875,
                        "total_cost": 937.2522914891038
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9714
Profiling... [512/50176]	Loss: 2.0175
Profiling... [768/50176]	Loss: 2.0703
Profiling... [1024/50176]	Loss: 1.9741
Profiling... [1280/50176]	Loss: 2.1251
Profiling... [1536/50176]	Loss: 2.0749
Profiling... [1792/50176]	Loss: 2.0116
Profiling... [2048/50176]	Loss: 2.1623
Profiling... [2304/50176]	Loss: 2.0207
Profiling... [2560/50176]	Loss: 2.3194
Profiling... [2816/50176]	Loss: 2.0803
Profiling... [3072/50176]	Loss: 2.0540
Profiling... [3328/50176]	Loss: 2.0632
Profile done
epoch 1 train time consumed: 4.51s
Validation Epoch: 7, Average loss: 0.0106, Accuracy: 0.3328
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 125.23758379148478,
                        "time": 2.8762580020002133,
                        "accuracy": 0.3328125,
                        "total_cost": 1082.3379606578187
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.1643
Profiling... [512/50176]	Loss: 2.2507
Profiling... [768/50176]	Loss: 2.0602
Profiling... [1024/50176]	Loss: 2.1312
Profiling... [1280/50176]	Loss: 1.9525
Profiling... [1536/50176]	Loss: 2.2714
Profiling... [1792/50176]	Loss: 2.0061
Profiling... [2048/50176]	Loss: 2.0947
Profiling... [2304/50176]	Loss: 2.0293
Profiling... [2560/50176]	Loss: 2.0701
Profiling... [2816/50176]	Loss: 2.2052
Profiling... [3072/50176]	Loss: 2.0900
Profiling... [3328/50176]	Loss: 1.9090
Profile done
epoch 1 train time consumed: 9.95s
Validation Epoch: 7, Average loss: 0.0135, Accuracy: 0.2735
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 125.15623764775472,
                        "time": 7.376442787999622,
                        "accuracy": 0.27353515625,
                        "total_cost": 3375.097516628439
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.1493
Profiling... [512/50176]	Loss: 2.0100
Profiling... [768/50176]	Loss: 2.0200
Profiling... [1024/50176]	Loss: 2.0827
Profiling... [1280/50176]	Loss: 2.1781
Profiling... [1536/50176]	Loss: 2.2419
Profiling... [1792/50176]	Loss: 2.0515
Profiling... [2048/50176]	Loss: 2.1266
Profiling... [2304/50176]	Loss: 2.1866
Profiling... [2560/50176]	Loss: 2.1073
Profiling... [2816/50176]	Loss: 1.8609
Profiling... [3072/50176]	Loss: 2.2216
Profiling... [3328/50176]	Loss: 2.1066
Profile done
epoch 1 train time consumed: 3.90s
Validation Epoch: 7, Average loss: 0.0092, Accuracy: 0.3824
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 125.10496832369185,
                        "time": 2.432238467999923,
                        "accuracy": 0.382421875,
                        "total_cost": 795.6791605992602
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.0014
Profiling... [512/50176]	Loss: 1.8692
Profiling... [768/50176]	Loss: 2.0495
Profiling... [1024/50176]	Loss: 2.1722
Profiling... [1280/50176]	Loss: 2.0353
Profiling... [1536/50176]	Loss: 2.1998
Profiling... [1792/50176]	Loss: 1.9493
Profiling... [2048/50176]	Loss: 2.0030
Profiling... [2304/50176]	Loss: 2.0042
Profiling... [2560/50176]	Loss: 1.9011
Profiling... [2816/50176]	Loss: 1.8563
Profiling... [3072/50176]	Loss: 2.0419
Profiling... [3328/50176]	Loss: 2.0346
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 7, Average loss: 0.0110, Accuracy: 0.3386
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 125.14065463815291,
                        "time": 2.504318101000081,
                        "accuracy": 0.33857421875,
                        "total_cost": 925.6227710968499
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9659
Profiling... [512/50176]	Loss: 2.2072
Profiling... [768/50176]	Loss: 2.0598
Profiling... [1024/50176]	Loss: 2.0917
Profiling... [1280/50176]	Loss: 2.0974
Profiling... [1536/50176]	Loss: 2.0820
Profiling... [1792/50176]	Loss: 1.8963
Profiling... [2048/50176]	Loss: 2.1301
Profiling... [2304/50176]	Loss: 1.9969
Profiling... [2560/50176]	Loss: 1.9733
Profiling... [2816/50176]	Loss: 2.0716
Profiling... [3072/50176]	Loss: 1.9224
Profiling... [3328/50176]	Loss: 2.0988
Profile done
epoch 1 train time consumed: 4.37s
Validation Epoch: 7, Average loss: 0.0095, Accuracy: 0.3625
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 125.14719735335014,
                        "time": 2.8683120539999436,
                        "accuracy": 0.3625,
                        "total_cost": 990.2378336356526
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.9829
Profiling... [512/50176]	Loss: 2.0714
Profiling... [768/50176]	Loss: 2.1110
Profiling... [1024/50176]	Loss: 1.9364
Profiling... [1280/50176]	Loss: 2.0840
Profiling... [1536/50176]	Loss: 2.2409
Profiling... [1792/50176]	Loss: 2.0965
Profiling... [2048/50176]	Loss: 1.9762
Profiling... [2304/50176]	Loss: 2.0555
Profiling... [2560/50176]	Loss: 1.9965
Profiling... [2816/50176]	Loss: 2.0173
Profiling... [3072/50176]	Loss: 2.0775
Profiling... [3328/50176]	Loss: 2.1745
Profile done
epoch 1 train time consumed: 10.17s
Validation Epoch: 7, Average loss: 0.0129, Accuracy: 0.2887
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 125.07051731313133,
                        "time": 7.2595941909999055,
                        "accuracy": 0.288671875,
                        "total_cost": 3145.3053781278877
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 2.0931
Profiling... [512/50176]	Loss: 1.9784
Profiling... [768/50176]	Loss: 2.2691
Profiling... [1024/50176]	Loss: 2.0405
Profiling... [1280/50176]	Loss: 2.0335
Profiling... [1536/50176]	Loss: 2.1358
Profiling... [1792/50176]	Loss: 2.1252
Profiling... [2048/50176]	Loss: 2.1250
Profiling... [2304/50176]	Loss: 2.0395
Profiling... [2560/50176]	Loss: 2.0855
Profiling... [2816/50176]	Loss: 2.0190
Profiling... [3072/50176]	Loss: 2.2129
Profiling... [3328/50176]	Loss: 2.0601
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 7, Average loss: 0.0114, Accuracy: 0.3082
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 125.01565165595653,
                        "time": 2.430729987000177,
                        "accuracy": 0.308203125,
                        "total_cost": 985.9708376561783
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 2.2143
Profiling... [512/50176]	Loss: 2.1708
Profiling... [768/50176]	Loss: 2.1226
Profiling... [1024/50176]	Loss: 2.2866
Profiling... [1280/50176]	Loss: 2.0715
Profiling... [1536/50176]	Loss: 2.0408
Profiling... [1792/50176]	Loss: 2.2739
Profiling... [2048/50176]	Loss: 2.0405
Profiling... [2304/50176]	Loss: 2.0007
Profiling... [2560/50176]	Loss: 2.1726
Profiling... [2816/50176]	Loss: 2.0944
Profiling... [3072/50176]	Loss: 2.1365
Profiling... [3328/50176]	Loss: 2.0698
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 7, Average loss: 0.0113, Accuracy: 0.3156
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 125.05164345986236,
                        "time": 2.5259497130000454,
                        "accuracy": 0.315625,
                        "total_cost": 1000.7894270340541
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8297
Profiling... [512/50176]	Loss: 2.0616
Profiling... [768/50176]	Loss: 2.0899
Profiling... [1024/50176]	Loss: 1.9873
Profiling... [1280/50176]	Loss: 2.2048
Profiling... [1536/50176]	Loss: 2.0596
Profiling... [1792/50176]	Loss: 1.9234
Profiling... [2048/50176]	Loss: 2.2128
Profiling... [2304/50176]	Loss: 1.9370
Profiling... [2560/50176]	Loss: 2.1556
Profiling... [2816/50176]	Loss: 1.9345
Profiling... [3072/50176]	Loss: 1.8339
Profiling... [3328/50176]	Loss: 2.1525
Profile done
epoch 1 train time consumed: 4.42s
Validation Epoch: 7, Average loss: 0.0097, Accuracy: 0.3583
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 125.05629010901973,
                        "time": 2.907666224999957,
                        "accuracy": 0.35830078125,
                        "total_cost": 1014.8511250944779
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.2303
Profiling... [512/50176]	Loss: 1.9697
Profiling... [768/50176]	Loss: 1.9327
Profiling... [1024/50176]	Loss: 2.1771
Profiling... [1280/50176]	Loss: 2.1107
Profiling... [1536/50176]	Loss: 2.0559
Profiling... [1792/50176]	Loss: 1.9871
Profiling... [2048/50176]	Loss: 2.1504
Profiling... [2304/50176]	Loss: 1.9835
Profiling... [2560/50176]	Loss: 2.0220
Profiling... [2816/50176]	Loss: 2.2021
Profiling... [3072/50176]	Loss: 1.9952
Profiling... [3328/50176]	Loss: 1.9792
Profile done
epoch 1 train time consumed: 10.08s
Validation Epoch: 7, Average loss: 0.0107, Accuracy: 0.3206
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 124.98291411401354,
                        "time": 7.152240093999808,
                        "accuracy": 0.32060546875,
                        "total_cost": 2788.1864051677444
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0544
Profiling... [1024/50176]	Loss: 2.0671
Profiling... [1536/50176]	Loss: 2.0666
Profiling... [2048/50176]	Loss: 1.9704
Profiling... [2560/50176]	Loss: 2.0956
Profiling... [3072/50176]	Loss: 1.9783
Profiling... [3584/50176]	Loss: 1.9699
Profiling... [4096/50176]	Loss: 1.8847
Profiling... [4608/50176]	Loss: 2.0747
Profiling... [5120/50176]	Loss: 1.9823
Profiling... [5632/50176]	Loss: 1.9114
Profiling... [6144/50176]	Loss: 1.9173
Profiling... [6656/50176]	Loss: 2.1326
Profile done
epoch 1 train time consumed: 6.86s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4500
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 124.95313703391531,
                        "time": 4.639896863000104,
                        "accuracy": 0.45,
                        "total_cost": 1288.3770412126348
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0839
Profiling... [1024/50176]	Loss: 2.1399
Profiling... [1536/50176]	Loss: 2.0827
Profiling... [2048/50176]	Loss: 1.9205
Profiling... [2560/50176]	Loss: 1.9275
Profiling... [3072/50176]	Loss: 2.0690
Profiling... [3584/50176]	Loss: 2.0743
Profiling... [4096/50176]	Loss: 2.0681
Profiling... [4608/50176]	Loss: 2.0314
Profiling... [5120/50176]	Loss: 1.9476
Profiling... [5632/50176]	Loss: 2.0573
Profiling... [6144/50176]	Loss: 1.9115
Profiling... [6656/50176]	Loss: 1.8092
Profile done
epoch 1 train time consumed: 6.98s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4496
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 125.00078133284455,
                        "time": 4.795722327000021,
                        "accuracy": 0.449609375,
                        "total_cost": 1333.3108054750196
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0111
Profiling... [1024/50176]	Loss: 1.9256
Profiling... [1536/50176]	Loss: 2.0161
Profiling... [2048/50176]	Loss: 1.8609
Profiling... [2560/50176]	Loss: 1.8806
Profiling... [3072/50176]	Loss: 2.0560
Profiling... [3584/50176]	Loss: 2.0136
Profiling... [4096/50176]	Loss: 2.0022
Profiling... [4608/50176]	Loss: 1.9210
Profiling... [5120/50176]	Loss: 1.8373
Profiling... [5632/50176]	Loss: 1.9938
Profiling... [6144/50176]	Loss: 2.1122
Profiling... [6656/50176]	Loss: 1.9597
Profile done
epoch 1 train time consumed: 8.02s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4523
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 125.00178967830428,
                        "time": 5.568486416000269,
                        "accuracy": 0.45234375,
                        "total_cost": 1538.8092966894314
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9791
Profiling... [1024/50176]	Loss: 2.0360
Profiling... [1536/50176]	Loss: 2.0459
Profiling... [2048/50176]	Loss: 2.0031
Profiling... [2560/50176]	Loss: 2.0527
Profiling... [3072/50176]	Loss: 1.9383
Profiling... [3584/50176]	Loss: 2.0540
Profiling... [4096/50176]	Loss: 2.0525
Profiling... [4608/50176]	Loss: 1.9912
Profiling... [5120/50176]	Loss: 1.9437
Profiling... [5632/50176]	Loss: 1.9355
Profiling... [6144/50176]	Loss: 1.9865
Profiling... [6656/50176]	Loss: 1.9117
Profile done
epoch 1 train time consumed: 17.80s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4417
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 124.8803465539262,
                        "time": 13.082881357000133,
                        "accuracy": 0.44169921875,
                        "total_cost": 3698.885323840245
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8928
Profiling... [1024/50176]	Loss: 2.0045
Profiling... [1536/50176]	Loss: 1.9929
Profiling... [2048/50176]	Loss: 1.9688
Profiling... [2560/50176]	Loss: 1.9508
Profiling... [3072/50176]	Loss: 1.8494
Profiling... [3584/50176]	Loss: 1.9542
Profiling... [4096/50176]	Loss: 2.0024
Profiling... [4608/50176]	Loss: 1.9672
Profiling... [5120/50176]	Loss: 1.8664
Profiling... [5632/50176]	Loss: 2.0386
Profiling... [6144/50176]	Loss: 1.9340
Profiling... [6656/50176]	Loss: 1.9974
Profile done
epoch 1 train time consumed: 6.89s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4476
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 124.8541270706194,
                        "time": 4.626486751999892,
                        "accuracy": 0.44755859375,
                        "total_cost": 1290.6376346945783
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0588
Profiling... [1024/50176]	Loss: 1.8729
Profiling... [1536/50176]	Loss: 2.1210
Profiling... [2048/50176]	Loss: 2.0330
Profiling... [2560/50176]	Loss: 1.9478
Profiling... [3072/50176]	Loss: 2.0246
Profiling... [3584/50176]	Loss: 2.0359
Profiling... [4096/50176]	Loss: 1.8913
Profiling... [4608/50176]	Loss: 2.0997
Profiling... [5120/50176]	Loss: 1.9016
Profiling... [5632/50176]	Loss: 1.8068
Profiling... [6144/50176]	Loss: 1.8287
Profiling... [6656/50176]	Loss: 1.9833
Profile done
epoch 1 train time consumed: 7.07s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4429
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 124.90011779793801,
                        "time": 4.788163792999967,
                        "accuracy": 0.44287109375,
                        "total_cost": 1350.375380604794
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9508
Profiling... [1024/50176]	Loss: 1.9518
Profiling... [1536/50176]	Loss: 2.0169
Profiling... [2048/50176]	Loss: 1.8850
Profiling... [2560/50176]	Loss: 1.9897
Profiling... [3072/50176]	Loss: 2.0199
Profiling... [3584/50176]	Loss: 2.1695
Profiling... [4096/50176]	Loss: 1.8646
Profiling... [4608/50176]	Loss: 1.9368
Profiling... [5120/50176]	Loss: 1.9574
Profiling... [5632/50176]	Loss: 1.8786
Profiling... [6144/50176]	Loss: 1.9313
Profiling... [6656/50176]	Loss: 1.9993
Profile done
epoch 1 train time consumed: 8.00s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4512
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 124.90040364176924,
                        "time": 5.567522996000207,
                        "accuracy": 0.451171875,
                        "total_cost": 1541.2881609370227
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0945
Profiling... [1024/50176]	Loss: 2.0164
Profiling... [1536/50176]	Loss: 2.0557
Profiling... [2048/50176]	Loss: 2.1014
Profiling... [2560/50176]	Loss: 2.0485
Profiling... [3072/50176]	Loss: 1.8584
Profiling... [3584/50176]	Loss: 2.0627
Profiling... [4096/50176]	Loss: 1.8970
Profiling... [4608/50176]	Loss: 2.0424
Profiling... [5120/50176]	Loss: 2.0675
Profiling... [5632/50176]	Loss: 2.0253
Profiling... [6144/50176]	Loss: 1.8179
Profiling... [6656/50176]	Loss: 2.0274
Profile done
epoch 1 train time consumed: 17.67s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4473
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 124.78237590913731,
                        "time": 13.063861233000353,
                        "accuracy": 0.447265625,
                        "total_cost": 3644.6790275936273
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0105
Profiling... [1024/50176]	Loss: 2.0284
Profiling... [1536/50176]	Loss: 2.1522
Profiling... [2048/50176]	Loss: 2.0193
Profiling... [2560/50176]	Loss: 2.0069
Profiling... [3072/50176]	Loss: 1.9528
Profiling... [3584/50176]	Loss: 1.8509
Profiling... [4096/50176]	Loss: 2.0024
Profiling... [4608/50176]	Loss: 1.8878
Profiling... [5120/50176]	Loss: 2.0123
Profiling... [5632/50176]	Loss: 1.8832
Profiling... [6144/50176]	Loss: 1.9031
Profiling... [6656/50176]	Loss: 1.9957
Profile done
epoch 1 train time consumed: 7.07s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4423
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 124.76739615039783,
                        "time": 4.6455640040003345,
                        "accuracy": 0.44228515625,
                        "total_cost": 1310.5005136132424
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0143
Profiling... [1024/50176]	Loss: 2.1475
Profiling... [1536/50176]	Loss: 1.9686
Profiling... [2048/50176]	Loss: 1.9495
Profiling... [2560/50176]	Loss: 1.9930
Profiling... [3072/50176]	Loss: 2.0004
Profiling... [3584/50176]	Loss: 1.9206
Profiling... [4096/50176]	Loss: 1.9827
Profiling... [4608/50176]	Loss: 1.9792
Profiling... [5120/50176]	Loss: 1.9969
Profiling... [5632/50176]	Loss: 1.9360
Profiling... [6144/50176]	Loss: 1.9245
Profiling... [6656/50176]	Loss: 1.9745
Profile done
epoch 1 train time consumed: 7.09s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4533
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 124.81308724074097,
                        "time": 4.801916791999702,
                        "accuracy": 0.4533203125,
                        "total_cost": 1322.1160467691104
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.1385
Profiling... [1024/50176]	Loss: 1.8562
Profiling... [1536/50176]	Loss: 1.9475
Profiling... [2048/50176]	Loss: 2.1077
Profiling... [2560/50176]	Loss: 2.0315
Profiling... [3072/50176]	Loss: 1.9158
Profiling... [3584/50176]	Loss: 2.0216
Profiling... [4096/50176]	Loss: 1.9615
Profiling... [4608/50176]	Loss: 1.9487
Profiling... [5120/50176]	Loss: 1.9761
Profiling... [5632/50176]	Loss: 1.8451
Profiling... [6144/50176]	Loss: 1.8932
Profiling... [6656/50176]	Loss: 1.9803
Profile done
epoch 1 train time consumed: 8.04s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4450
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 124.8109926802819,
                        "time": 5.564699121000103,
                        "accuracy": 0.44501953125,
                        "total_cost": 1560.6857058796002
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9039
Profiling... [1024/50176]	Loss: 1.9937
Profiling... [1536/50176]	Loss: 2.0137
Profiling... [2048/50176]	Loss: 1.9575
Profiling... [2560/50176]	Loss: 1.9325
Profiling... [3072/50176]	Loss: 1.9847
Profiling... [3584/50176]	Loss: 1.9605
Profiling... [4096/50176]	Loss: 1.8874
Profiling... [4608/50176]	Loss: 1.9248
Profiling... [5120/50176]	Loss: 1.8800
Profiling... [5632/50176]	Loss: 1.9705
Profiling... [6144/50176]	Loss: 1.9000
Profiling... [6656/50176]	Loss: 1.9548
Profile done
epoch 1 train time consumed: 17.79s
Validation Epoch: 7, Average loss: 0.0040, Accuracy: 0.4494
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 124.69459252475384,
                        "time": 13.072423364000315,
                        "accuracy": 0.4494140625,
                        "total_cost": 3627.0794367612634
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0478
Profiling... [1024/50176]	Loss: 1.9208
Profiling... [1536/50176]	Loss: 1.9976
Profiling... [2048/50176]	Loss: 2.0090
Profiling... [2560/50176]	Loss: 2.0758
Profiling... [3072/50176]	Loss: 1.9719
Profiling... [3584/50176]	Loss: 1.9613
Profiling... [4096/50176]	Loss: 1.9744
Profiling... [4608/50176]	Loss: 1.9966
Profiling... [5120/50176]	Loss: 2.0111
Profiling... [5632/50176]	Loss: 1.9837
Profiling... [6144/50176]	Loss: 2.0638
Profiling... [6656/50176]	Loss: 1.9038
Profile done
epoch 1 train time consumed: 6.87s
Validation Epoch: 7, Average loss: 0.0046, Accuracy: 0.4057
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 124.6700481158849,
                        "time": 4.62380075100009,
                        "accuracy": 0.4056640625,
                        "total_cost": 1421.0020442849705
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.1015
Profiling... [1024/50176]	Loss: 2.0671
Profiling... [1536/50176]	Loss: 1.9554
Profiling... [2048/50176]	Loss: 1.9556
Profiling... [2560/50176]	Loss: 2.0033
Profiling... [3072/50176]	Loss: 1.8502
Profiling... [3584/50176]	Loss: 1.8567
Profiling... [4096/50176]	Loss: 1.9556
Profiling... [4608/50176]	Loss: 1.9061
Profiling... [5120/50176]	Loss: 1.8588
Profiling... [5632/50176]	Loss: 1.9570
Profiling... [6144/50176]	Loss: 2.0208
Profiling... [6656/50176]	Loss: 2.0204
Profile done
epoch 1 train time consumed: 7.04s
Validation Epoch: 7, Average loss: 0.0043, Accuracy: 0.4308
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 124.7145339612269,
                        "time": 4.780803391000518,
                        "accuracy": 0.43076171875,
                        "total_cost": 1384.142649906452
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0131
Profiling... [1024/50176]	Loss: 1.9337
Profiling... [1536/50176]	Loss: 1.9020
Profiling... [2048/50176]	Loss: 2.0100
Profiling... [2560/50176]	Loss: 1.8789
Profiling... [3072/50176]	Loss: 1.9049
Profiling... [3584/50176]	Loss: 1.9933
Profiling... [4096/50176]	Loss: 1.9334
Profiling... [4608/50176]	Loss: 2.0063
Profiling... [5120/50176]	Loss: 1.8321
Profiling... [5632/50176]	Loss: 1.8633
Profiling... [6144/50176]	Loss: 1.8879
Profiling... [6656/50176]	Loss: 1.9663
Profile done
epoch 1 train time consumed: 8.09s
Validation Epoch: 7, Average loss: 0.0042, Accuracy: 0.4293
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 124.71491966522643,
                        "time": 5.575047223999718,
                        "accuracy": 0.429296875,
                        "total_cost": 1619.605469224458
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9688
Profiling... [1024/50176]	Loss: 1.9457
Profiling... [1536/50176]	Loss: 1.9814
Profiling... [2048/50176]	Loss: 1.9439
Profiling... [2560/50176]	Loss: 1.9324
Profiling... [3072/50176]	Loss: 1.8907
Profiling... [3584/50176]	Loss: 1.8924
Profiling... [4096/50176]	Loss: 1.8792
Profiling... [4608/50176]	Loss: 2.0368
Profiling... [5120/50176]	Loss: 2.0075
Profiling... [5632/50176]	Loss: 1.8906
Profiling... [6144/50176]	Loss: 1.8886
Profiling... [6656/50176]	Loss: 1.9952
Profile done
epoch 1 train time consumed: 18.63s
Validation Epoch: 7, Average loss: 0.0046, Accuracy: 0.4013
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 124.58916506610706,
                        "time": 13.972420027000226,
                        "accuracy": 0.40126953125,
                        "total_cost": 4338.261466536182
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1056
Profiling... [1024/50176]	Loss: 1.9334
Profiling... [1536/50176]	Loss: 1.9404
Profiling... [2048/50176]	Loss: 1.9430
Profiling... [2560/50176]	Loss: 2.0479
Profiling... [3072/50176]	Loss: 1.9563
Profiling... [3584/50176]	Loss: 1.8644
Profiling... [4096/50176]	Loss: 1.8355
Profiling... [4608/50176]	Loss: 1.7869
Profiling... [5120/50176]	Loss: 1.8139
Profiling... [5632/50176]	Loss: 1.9759
Profiling... [6144/50176]	Loss: 2.0003
Profiling... [6656/50176]	Loss: 1.8343
Profile done
epoch 1 train time consumed: 6.94s
Validation Epoch: 7, Average loss: 0.0043, Accuracy: 0.4258
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 124.57201141930638,
                        "time": 4.651093309000316,
                        "accuracy": 0.42578125,
                        "total_cost": 1360.7833806703486
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0819
Profiling... [1024/50176]	Loss: 1.9567
Profiling... [1536/50176]	Loss: 1.9732
Profiling... [2048/50176]	Loss: 2.0103
Profiling... [2560/50176]	Loss: 1.9349
Profiling... [3072/50176]	Loss: 1.8925
Profiling... [3584/50176]	Loss: 1.8489
Profiling... [4096/50176]	Loss: 1.8505
Profiling... [4608/50176]	Loss: 1.9104
Profiling... [5120/50176]	Loss: 1.8725
Profiling... [5632/50176]	Loss: 1.9146
Profiling... [6144/50176]	Loss: 1.8684
Profiling... [6656/50176]	Loss: 1.9863
Profile done
epoch 1 train time consumed: 7.08s
Validation Epoch: 7, Average loss: 0.0045, Accuracy: 0.4023
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 124.61817624364296,
                        "time": 4.797271174999878,
                        "accuracy": 0.40234375,
                        "total_cost": 1485.861740799212
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0812
Profiling... [1024/50176]	Loss: 1.8572
Profiling... [1536/50176]	Loss: 1.9722
Profiling... [2048/50176]	Loss: 2.0334
Profiling... [2560/50176]	Loss: 1.9471
Profiling... [3072/50176]	Loss: 2.0573
Profiling... [3584/50176]	Loss: 2.0522
Profiling... [4096/50176]	Loss: 1.8098
Profiling... [4608/50176]	Loss: 1.8643
Profiling... [5120/50176]	Loss: 1.8426
Profiling... [5632/50176]	Loss: 2.0012
Profiling... [6144/50176]	Loss: 1.8656
Profiling... [6656/50176]	Loss: 1.9412
Profile done
epoch 1 train time consumed: 8.06s
Validation Epoch: 7, Average loss: 0.0044, Accuracy: 0.4084
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 124.61792713174876,
                        "time": 5.576413325000431,
                        "accuracy": 0.4083984375,
                        "total_cost": 1701.5762196480405
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0488
Profiling... [1024/50176]	Loss: 1.9780
Profiling... [1536/50176]	Loss: 2.0008
Profiling... [2048/50176]	Loss: 2.0067
Profiling... [2560/50176]	Loss: 2.0841
Profiling... [3072/50176]	Loss: 1.9227
Profiling... [3584/50176]	Loss: 1.9479
Profiling... [4096/50176]	Loss: 2.0256
Profiling... [4608/50176]	Loss: 1.9091
Profiling... [5120/50176]	Loss: 1.9657
Profiling... [5632/50176]	Loss: 2.0258
Profiling... [6144/50176]	Loss: 2.0514
Profiling... [6656/50176]	Loss: 1.9927
Profile done
epoch 1 train time consumed: 18.66s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.4582
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 124.4981127132368,
                        "time": 13.954867239000123,
                        "accuracy": 0.458203125,
                        "total_cost": 3791.669108366061
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0590
Profiling... [1024/50176]	Loss: 1.8955
Profiling... [1536/50176]	Loss: 1.8491
Profiling... [2048/50176]	Loss: 1.9724
Profiling... [2560/50176]	Loss: 1.9424
Profiling... [3072/50176]	Loss: 1.9886
Profiling... [3584/50176]	Loss: 1.9552
Profiling... [4096/50176]	Loss: 1.9197
Profiling... [4608/50176]	Loss: 2.0189
Profiling... [5120/50176]	Loss: 1.8692
Profiling... [5632/50176]	Loss: 1.9248
Profiling... [6144/50176]	Loss: 1.8913
Profiling... [6656/50176]	Loss: 1.8743
Profile done
epoch 1 train time consumed: 6.80s
Validation Epoch: 7, Average loss: 0.0042, Accuracy: 0.4300
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 124.47587428815027,
                        "time": 4.637295976000132,
                        "accuracy": 0.42998046875,
                        "total_cost": 1342.4597461917567
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0351
Profiling... [1024/50176]	Loss: 1.9183
Profiling... [1536/50176]	Loss: 1.9887
Profiling... [2048/50176]	Loss: 2.0030
Profiling... [2560/50176]	Loss: 2.0081
Profiling... [3072/50176]	Loss: 1.9476
Profiling... [3584/50176]	Loss: 1.9215
Profiling... [4096/50176]	Loss: 1.8920
Profiling... [4608/50176]	Loss: 1.9851
Profiling... [5120/50176]	Loss: 1.9865
Profiling... [5632/50176]	Loss: 1.9061
Profiling... [6144/50176]	Loss: 1.9274
Profiling... [6656/50176]	Loss: 1.8497
Profile done
epoch 1 train time consumed: 6.99s
Validation Epoch: 7, Average loss: 0.0043, Accuracy: 0.4225
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 124.5203259135901,
                        "time": 4.788833275999423,
                        "accuracy": 0.4224609375,
                        "total_cost": 1411.5082066570794
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0914
Profiling... [1024/50176]	Loss: 2.1037
Profiling... [1536/50176]	Loss: 1.9108
Profiling... [2048/50176]	Loss: 2.1202
Profiling... [2560/50176]	Loss: 1.9691
Profiling... [3072/50176]	Loss: 1.8660
Profiling... [3584/50176]	Loss: 1.9169
Profiling... [4096/50176]	Loss: 1.9147
Profiling... [4608/50176]	Loss: 1.8071
Profiling... [5120/50176]	Loss: 1.8762
Profiling... [5632/50176]	Loss: 1.9884
Profiling... [6144/50176]	Loss: 1.8494
Profiling... [6656/50176]	Loss: 1.8588
Profile done
epoch 1 train time consumed: 7.99s
Validation Epoch: 7, Average loss: 0.0042, Accuracy: 0.4376
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 124.52236022515797,
                        "time": 5.570316891999937,
                        "accuracy": 0.43759765625,
                        "total_cost": 1585.083915983379
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.1035
Profiling... [1024/50176]	Loss: 1.9441
Profiling... [1536/50176]	Loss: 1.9718
Profiling... [2048/50176]	Loss: 1.9894
Profiling... [2560/50176]	Loss: 1.8231
Profiling... [3072/50176]	Loss: 1.9457
Profiling... [3584/50176]	Loss: 1.9366
Profiling... [4096/50176]	Loss: 2.0756
Profiling... [4608/50176]	Loss: 1.9725
Profiling... [5120/50176]	Loss: 1.9846
Profiling... [5632/50176]	Loss: 2.0329
Profiling... [6144/50176]	Loss: 1.9168
Profiling... [6656/50176]	Loss: 2.1501
Profile done
epoch 1 train time consumed: 17.70s
Validation Epoch: 7, Average loss: 0.0045, Accuracy: 0.4115
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 124.41115102373986,
                        "time": 13.062856477999958,
                        "accuracy": 0.4115234375,
                        "total_cost": 3949.1432613382835
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.1075
Profiling... [1024/50176]	Loss: 2.0158
Profiling... [1536/50176]	Loss: 1.9717
Profiling... [2048/50176]	Loss: 2.1061
Profiling... [2560/50176]	Loss: 2.0010
Profiling... [3072/50176]	Loss: 1.9583
Profiling... [3584/50176]	Loss: 2.0185
Profiling... [4096/50176]	Loss: 1.9759
Profiling... [4608/50176]	Loss: 2.0823
Profiling... [5120/50176]	Loss: 1.7973
Profiling... [5632/50176]	Loss: 2.0659
Profiling... [6144/50176]	Loss: 1.9871
Profiling... [6656/50176]	Loss: 1.8386
Profile done
epoch 1 train time consumed: 6.88s
Validation Epoch: 7, Average loss: 0.0062, Accuracy: 0.3033
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 124.39280518071699,
                        "time": 4.619847280999238,
                        "accuracy": 0.3033203125,
                        "total_cost": 1894.6168097133398
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0553
Profiling... [1024/50176]	Loss: 2.2180
Profiling... [1536/50176]	Loss: 2.0567
Profiling... [2048/50176]	Loss: 2.0135
Profiling... [2560/50176]	Loss: 2.0944
Profiling... [3072/50176]	Loss: 2.0478
Profiling... [3584/50176]	Loss: 1.9663
Profiling... [4096/50176]	Loss: 2.1289
Profiling... [4608/50176]	Loss: 2.0048
Profiling... [5120/50176]	Loss: 1.9938
Profiling... [5632/50176]	Loss: 1.8862
Profiling... [6144/50176]	Loss: 1.9465
Profiling... [6656/50176]	Loss: 2.0372
Profile done
epoch 1 train time consumed: 7.02s
Validation Epoch: 7, Average loss: 0.0052, Accuracy: 0.3652
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 124.43835931819332,
                        "time": 4.775018088999786,
                        "accuracy": 0.365234375,
                        "total_cost": 1626.8879858579255
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.9774
Profiling... [1024/50176]	Loss: 1.9142
Profiling... [1536/50176]	Loss: 2.0775
Profiling... [2048/50176]	Loss: 2.0649
Profiling... [2560/50176]	Loss: 1.9955
Profiling... [3072/50176]	Loss: 2.0277
Profiling... [3584/50176]	Loss: 2.0804
Profiling... [4096/50176]	Loss: 2.1498
Profiling... [4608/50176]	Loss: 2.0780
Profiling... [5120/50176]	Loss: 1.8320
Profiling... [5632/50176]	Loss: 1.9749
Profiling... [6144/50176]	Loss: 1.8914
Profiling... [6656/50176]	Loss: 2.0092
Profile done
epoch 1 train time consumed: 8.02s
Validation Epoch: 7, Average loss: 0.0048, Accuracy: 0.3738
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 124.43811376556458,
                        "time": 5.594149278000259,
                        "accuracy": 0.373828125,
                        "total_cost": 1862.1535880355368
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.9899
Profiling... [1024/50176]	Loss: 1.9903
Profiling... [1536/50176]	Loss: 1.9635
Profiling... [2048/50176]	Loss: 1.9348
Profiling... [2560/50176]	Loss: 2.0405
Profiling... [3072/50176]	Loss: 1.9660
Profiling... [3584/50176]	Loss: 2.0560
Profiling... [4096/50176]	Loss: 1.9419
Profiling... [4608/50176]	Loss: 2.0791
Profiling... [5120/50176]	Loss: 1.9985
Profiling... [5632/50176]	Loss: 1.9767
Profiling... [6144/50176]	Loss: 2.0702
Profiling... [6656/50176]	Loss: 1.9830
Profile done
epoch 1 train time consumed: 18.56s
Validation Epoch: 7, Average loss: 0.0069, Accuracy: 0.2761
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 124.32072853702311,
                        "time": 13.84826291599984,
                        "accuracy": 0.27607421875,
                        "total_cost": 6236.098910229517
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0849
Profiling... [1024/50176]	Loss: 1.9157
Profiling... [1536/50176]	Loss: 2.0225
Profiling... [2048/50176]	Loss: 2.0124
Profiling... [2560/50176]	Loss: 2.0105
Profiling... [3072/50176]	Loss: 1.9841
Profiling... [3584/50176]	Loss: 1.8705
Profiling... [4096/50176]	Loss: 1.8886
Profiling... [4608/50176]	Loss: 1.9794
Profiling... [5120/50176]	Loss: 1.8909
Profiling... [5632/50176]	Loss: 1.9654
Profiling... [6144/50176]	Loss: 1.9521
Profiling... [6656/50176]	Loss: 1.9163
Profile done
epoch 1 train time consumed: 6.95s
Validation Epoch: 7, Average loss: 0.0058, Accuracy: 0.3103
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 124.30000226195742,
                        "time": 4.648284373000024,
                        "accuracy": 0.31025390625,
                        "total_cost": 1862.286812313501
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0796
Profiling... [1024/50176]	Loss: 1.8816
Profiling... [1536/50176]	Loss: 1.9681
Profiling... [2048/50176]	Loss: 2.0341
Profiling... [2560/50176]	Loss: 2.0066
Profiling... [3072/50176]	Loss: 2.2322
Profiling... [3584/50176]	Loss: 2.0006
Profiling... [4096/50176]	Loss: 2.0316
Profiling... [4608/50176]	Loss: 1.9403
Profiling... [5120/50176]	Loss: 2.0178
Profiling... [5632/50176]	Loss: 1.9276
Profiling... [6144/50176]	Loss: 1.9859
Profiling... [6656/50176]	Loss: 1.9177
Profile done
epoch 1 train time consumed: 7.06s
Validation Epoch: 7, Average loss: 0.0052, Accuracy: 0.3450
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 124.34176720732522,
                        "time": 4.808508004999567,
                        "accuracy": 0.34501953125,
                        "total_cost": 1732.9406854331987
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0443
Profiling... [1024/50176]	Loss: 1.8549
Profiling... [1536/50176]	Loss: 1.9858
Profiling... [2048/50176]	Loss: 1.9927
Profiling... [2560/50176]	Loss: 1.9767
Profiling... [3072/50176]	Loss: 2.0137
Profiling... [3584/50176]	Loss: 2.0229
Profiling... [4096/50176]	Loss: 1.9934
Profiling... [4608/50176]	Loss: 1.9936
Profiling... [5120/50176]	Loss: 1.9368
Profiling... [5632/50176]	Loss: 1.9878
Profiling... [6144/50176]	Loss: 2.0444
Profiling... [6656/50176]	Loss: 2.1043
Profile done
epoch 1 train time consumed: 7.96s
Validation Epoch: 7, Average loss: 0.0052, Accuracy: 0.3714
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 124.34503402078442,
                        "time": 5.582521375000397,
                        "accuracy": 0.37138671875,
                        "total_cost": 1869.0997153386509
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0035
Profiling... [1024/50176]	Loss: 2.0032
Profiling... [1536/50176]	Loss: 2.0526
Profiling... [2048/50176]	Loss: 1.9704
Profiling... [2560/50176]	Loss: 2.0549
Profiling... [3072/50176]	Loss: 1.9781
Profiling... [3584/50176]	Loss: 2.0645
Profiling... [4096/50176]	Loss: 2.0072
Profiling... [4608/50176]	Loss: 2.0656
Profiling... [5120/50176]	Loss: 2.0551
Profiling... [5632/50176]	Loss: 1.9904
Profiling... [6144/50176]	Loss: 2.0013
Profiling... [6656/50176]	Loss: 2.0006
Profile done
epoch 1 train time consumed: 18.54s
Validation Epoch: 7, Average loss: 0.0057, Accuracy: 0.3072
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 124.23481029831329,
                        "time": 13.874630985000294,
                        "accuracy": 0.3072265625,
                        "total_cost": 5610.55702460822
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 2.0368
Profiling... [1024/50176]	Loss: 1.9805
Profiling... [1536/50176]	Loss: 1.9920
Profiling... [2048/50176]	Loss: 2.0779
Profiling... [2560/50176]	Loss: 2.0211
Profiling... [3072/50176]	Loss: 1.9235
Profiling... [3584/50176]	Loss: 2.1060
Profiling... [4096/50176]	Loss: 1.9834
Profiling... [4608/50176]	Loss: 1.9383
Profiling... [5120/50176]	Loss: 1.9811
Profiling... [5632/50176]	Loss: 2.1000
Profiling... [6144/50176]	Loss: 1.9344
Profiling... [6656/50176]	Loss: 1.9354
Profile done
epoch 1 train time consumed: 6.96s
Validation Epoch: 7, Average loss: 0.0056, Accuracy: 0.3055
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 124.21847328950135,
                        "time": 4.631711490999805,
                        "accuracy": 0.30546875,
                        "total_cost": 1883.4795052830634
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 2.0216
Profiling... [1024/50176]	Loss: 1.8790
Profiling... [1536/50176]	Loss: 2.0180
Profiling... [2048/50176]	Loss: 1.9689
Profiling... [2560/50176]	Loss: 2.0313
Profiling... [3072/50176]	Loss: 2.0816
Profiling... [3584/50176]	Loss: 1.9609
Profiling... [4096/50176]	Loss: 2.0415
Profiling... [4608/50176]	Loss: 2.0369
Profiling... [5120/50176]	Loss: 2.0632
Profiling... [5632/50176]	Loss: 2.0474
Profiling... [6144/50176]	Loss: 1.9971
Profiling... [6656/50176]	Loss: 1.8625
Profile done
epoch 1 train time consumed: 7.07s
Validation Epoch: 7, Average loss: 0.0056, Accuracy: 0.3354
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 124.26041143087753,
                        "time": 4.792841409000175,
                        "accuracy": 0.33544921875,
                        "total_cost": 1775.4116334644423
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 2.0050
Profiling... [1024/50176]	Loss: 2.0091
Profiling... [1536/50176]	Loss: 1.9156
Profiling... [2048/50176]	Loss: 2.0477
Profiling... [2560/50176]	Loss: 2.0030
Profiling... [3072/50176]	Loss: 1.9578
Profiling... [3584/50176]	Loss: 2.1077
Profiling... [4096/50176]	Loss: 2.0242
Profiling... [4608/50176]	Loss: 2.0231
Profiling... [5120/50176]	Loss: 1.9174
Profiling... [5632/50176]	Loss: 1.9204
Profiling... [6144/50176]	Loss: 2.0024
Profiling... [6656/50176]	Loss: 2.0291
Profile done
epoch 1 train time consumed: 8.02s
Validation Epoch: 7, Average loss: 0.0053, Accuracy: 0.3662
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 124.26290504112714,
                        "time": 5.566339302000415,
                        "accuracy": 0.3662109375,
                        "total_cost": 1888.7734397915722
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 2.0898
Profiling... [1024/50176]	Loss: 1.9578
Profiling... [1536/50176]	Loss: 2.0066
Profiling... [2048/50176]	Loss: 1.8915
Profiling... [2560/50176]	Loss: 2.2274
Profiling... [3072/50176]	Loss: 1.9223
Profiling... [3584/50176]	Loss: 1.9964
Profiling... [4096/50176]	Loss: 1.9502
Profiling... [4608/50176]	Loss: 2.0620
Profiling... [5120/50176]	Loss: 1.9919
Profiling... [5632/50176]	Loss: 2.0157
Profiling... [6144/50176]	Loss: 2.0456
Profiling... [6656/50176]	Loss: 1.8853
Profile done
epoch 1 train time consumed: 18.61s
Validation Epoch: 7, Average loss: 0.0046, Accuracy: 0.3937
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 124.15197704171291,
                        "time": 13.878995670999757,
                        "accuracy": 0.39375,
                        "total_cost": 4376.139052464749
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0239
Profiling... [2048/50176]	Loss: 2.0755
Profiling... [3072/50176]	Loss: 1.9308
Profiling... [4096/50176]	Loss: 1.9767
Profiling... [5120/50176]	Loss: 1.9537
Profiling... [6144/50176]	Loss: 1.9733
Profiling... [7168/50176]	Loss: 1.9720
Profiling... [8192/50176]	Loss: 1.9840
Profiling... [9216/50176]	Loss: 1.9734
Profiling... [10240/50176]	Loss: 1.8944
Profiling... [11264/50176]	Loss: 1.8565
Profiling... [12288/50176]	Loss: 1.9390
Profiling... [13312/50176]	Loss: 1.9149
Profile done
epoch 1 train time consumed: 13.13s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4513
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 124.15674609994397,
                        "time": 9.08379248899928,
                        "accuracy": 0.45126953125,
                        "total_cost": 2499.2028922432632
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.1557
Profiling... [2048/50176]	Loss: 2.0146
Profiling... [3072/50176]	Loss: 1.9860
Profiling... [4096/50176]	Loss: 1.9745
Profiling... [5120/50176]	Loss: 2.0036
Profiling... [6144/50176]	Loss: 1.9383
Profiling... [7168/50176]	Loss: 2.0160
Profiling... [8192/50176]	Loss: 1.9672
Profiling... [9216/50176]	Loss: 1.9324
Profiling... [10240/50176]	Loss: 1.9144
Profiling... [11264/50176]	Loss: 1.9235
Profiling... [12288/50176]	Loss: 1.8307
Profiling... [13312/50176]	Loss: 1.9227
Profile done
epoch 1 train time consumed: 13.53s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4522
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 124.22383688740898,
                        "time": 9.418295288999616,
                        "accuracy": 0.45224609375,
                        "total_cost": 2587.0356735129694
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0357
Profiling... [2048/50176]	Loss: 1.9403
Profiling... [3072/50176]	Loss: 2.0264
Profiling... [4096/50176]	Loss: 1.9376
Profiling... [5120/50176]	Loss: 2.0552
Profiling... [6144/50176]	Loss: 1.9054
Profiling... [7168/50176]	Loss: 1.9169
Profiling... [8192/50176]	Loss: 1.9858
Profiling... [9216/50176]	Loss: 1.8963
Profiling... [10240/50176]	Loss: 2.0204
Profiling... [11264/50176]	Loss: 2.0149
Profiling... [12288/50176]	Loss: 1.9290
Profiling... [13312/50176]	Loss: 1.9964
Profile done
epoch 1 train time consumed: 15.48s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4508
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 124.21600995549487,
                        "time": 11.013326307999705,
                        "accuracy": 0.45078125,
                        "total_cost": 3034.801137619643
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9659
Profiling... [2048/50176]	Loss: 2.0198
Profiling... [3072/50176]	Loss: 1.9706
Profiling... [4096/50176]	Loss: 1.9476
Profiling... [5120/50176]	Loss: 1.9572
Profiling... [6144/50176]	Loss: 2.0461
Profiling... [7168/50176]	Loss: 1.9453
Profiling... [8192/50176]	Loss: 2.0438
Profiling... [9216/50176]	Loss: 1.9515
Profiling... [10240/50176]	Loss: 1.9902
Profiling... [11264/50176]	Loss: 1.9463
Profiling... [12288/50176]	Loss: 1.9127
Profiling... [13312/50176]	Loss: 1.9657
Profile done
epoch 1 train time consumed: 38.52s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4516
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 124.00581272831555,
                        "time": 28.74384888000077,
                        "accuracy": 0.4515625,
                        "total_cost": 7893.490582819384
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.9658
Profiling... [2048/50176]	Loss: 2.0381
Profiling... [3072/50176]	Loss: 1.9726
Profiling... [4096/50176]	Loss: 1.8736
Profiling... [5120/50176]	Loss: 2.0290
Profiling... [6144/50176]	Loss: 2.0445
Profiling... [7168/50176]	Loss: 1.9924
Profiling... [8192/50176]	Loss: 1.9459
Profiling... [9216/50176]	Loss: 2.0150
Profiling... [10240/50176]	Loss: 1.9551
Profiling... [11264/50176]	Loss: 1.9202
Profiling... [12288/50176]	Loss: 1.9133
Profiling... [13312/50176]	Loss: 2.0007
Profile done
epoch 1 train time consumed: 13.15s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4514
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 124.0298854546056,
                        "time": 9.095142797000335,
                        "accuracy": 0.4513671875,
                        "total_cost": 2499.228013346081
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9780
Profiling... [2048/50176]	Loss: 2.0249
Profiling... [3072/50176]	Loss: 2.0658
Profiling... [4096/50176]	Loss: 1.9698
Profiling... [5120/50176]	Loss: 2.0070
Profiling... [6144/50176]	Loss: 1.9837
Profiling... [7168/50176]	Loss: 2.0142
Profiling... [8192/50176]	Loss: 2.0136
Profiling... [9216/50176]	Loss: 1.8403
Profiling... [10240/50176]	Loss: 1.8903
Profiling... [11264/50176]	Loss: 1.9877
Profiling... [12288/50176]	Loss: 2.0013
Profiling... [13312/50176]	Loss: 1.8460
Profile done
epoch 1 train time consumed: 13.51s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4479
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 124.0979692193257,
                        "time": 9.405563027000426,
                        "accuracy": 0.44794921875,
                        "total_cost": 2605.6776575528447
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0219
Profiling... [2048/50176]	Loss: 1.9917
Profiling... [3072/50176]	Loss: 1.9643
Profiling... [4096/50176]	Loss: 2.0328
Profiling... [5120/50176]	Loss: 1.9398
Profiling... [6144/50176]	Loss: 2.0430
Profiling... [7168/50176]	Loss: 2.1079
Profiling... [8192/50176]	Loss: 1.9982
Profiling... [9216/50176]	Loss: 1.8564
Profiling... [10240/50176]	Loss: 1.9736
Profiling... [11264/50176]	Loss: 1.8439
Profiling... [12288/50176]	Loss: 1.8934
Profiling... [13312/50176]	Loss: 1.8876
Profile done
epoch 1 train time consumed: 15.57s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4512
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 124.092147035222,
                        "time": 11.03416744399965,
                        "accuracy": 0.451171875,
                        "total_cost": 3034.8822804437095
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0388
Profiling... [2048/50176]	Loss: 2.0261
Profiling... [3072/50176]	Loss: 2.0121
Profiling... [4096/50176]	Loss: 1.9648
Profiling... [5120/50176]	Loss: 2.1070
Profiling... [6144/50176]	Loss: 1.9964
Profiling... [7168/50176]	Loss: 1.9070
Profiling... [8192/50176]	Loss: 2.0031
Profiling... [9216/50176]	Loss: 1.8945
Profiling... [10240/50176]	Loss: 1.9626
Profiling... [11264/50176]	Loss: 1.9456
Profiling... [12288/50176]	Loss: 1.9065
Profiling... [13312/50176]	Loss: 1.9067
Profile done
epoch 1 train time consumed: 39.19s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4515
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 123.88085904025004,
                        "time": 29.339555644999564,
                        "accuracy": 0.45146484375,
                        "total_cost": 8050.702967193696
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0605
Profiling... [2048/50176]	Loss: 2.0428
Profiling... [3072/50176]	Loss: 1.9539
Profiling... [4096/50176]	Loss: 2.0382
Profiling... [5120/50176]	Loss: 1.9746
Profiling... [6144/50176]	Loss: 1.9364
Profiling... [7168/50176]	Loss: 1.9852
Profiling... [8192/50176]	Loss: 1.9329
Profiling... [9216/50176]	Loss: 2.0715
Profiling... [10240/50176]	Loss: 1.9061
Profiling... [11264/50176]	Loss: 1.9086
Profiling... [12288/50176]	Loss: 1.9614
Profiling... [13312/50176]	Loss: 2.0185
Profile done
epoch 1 train time consumed: 13.13s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4496
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.9045620016098,
                        "time": 9.07096675000048,
                        "accuracy": 0.449609375,
                        "total_cost": 2499.8014378369567
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9763
Profiling... [2048/50176]	Loss: 2.0071
Profiling... [3072/50176]	Loss: 2.0361
Profiling... [4096/50176]	Loss: 1.9494
Profiling... [5120/50176]	Loss: 2.0144
Profiling... [6144/50176]	Loss: 1.9572
Profiling... [7168/50176]	Loss: 1.9572
Profiling... [8192/50176]	Loss: 2.0270
Profiling... [9216/50176]	Loss: 1.9974
Profiling... [10240/50176]	Loss: 1.8859
Profiling... [11264/50176]	Loss: 1.8964
Profiling... [12288/50176]	Loss: 1.9209
Profiling... [13312/50176]	Loss: 1.9498
Profile done
epoch 1 train time consumed: 13.41s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4514
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.97130458396899,
                        "time": 9.39272246300061,
                        "accuracy": 0.4513671875,
                        "total_cost": 2579.780031824613
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0662
Profiling... [2048/50176]	Loss: 2.0260
Profiling... [3072/50176]	Loss: 2.0821
Profiling... [4096/50176]	Loss: 1.9141
Profiling... [5120/50176]	Loss: 1.9685
Profiling... [6144/50176]	Loss: 1.9669
Profiling... [7168/50176]	Loss: 1.9189
Profiling... [8192/50176]	Loss: 1.8450
Profiling... [9216/50176]	Loss: 1.8814
Profiling... [10240/50176]	Loss: 1.9691
Profiling... [11264/50176]	Loss: 1.9128
Profiling... [12288/50176]	Loss: 1.8365
Profiling... [13312/50176]	Loss: 1.9903
Profile done
epoch 1 train time consumed: 15.59s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4550
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.96427370189326,
                        "time": 11.009597342999768,
                        "accuracy": 0.45498046875,
                        "total_cost": 2999.6820349780346
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0028
Profiling... [2048/50176]	Loss: 2.0516
Profiling... [3072/50176]	Loss: 2.1259
Profiling... [4096/50176]	Loss: 1.9084
Profiling... [5120/50176]	Loss: 2.0390
Profiling... [6144/50176]	Loss: 2.0251
Profiling... [7168/50176]	Loss: 2.0443
Profiling... [8192/50176]	Loss: 1.8999
Profiling... [9216/50176]	Loss: 2.0026
Profiling... [10240/50176]	Loss: 1.9588
Profiling... [11264/50176]	Loss: 1.9528
Profiling... [12288/50176]	Loss: 1.9591
Profiling... [13312/50176]	Loss: 1.9172
Profile done
epoch 1 train time consumed: 39.11s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4508
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.75407277921651,
                        "time": 29.37567720700008,
                        "accuracy": 0.45078125,
                        "total_cost": 8064.576099857436
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0412
Profiling... [2048/50176]	Loss: 1.9943
Profiling... [3072/50176]	Loss: 1.9381
Profiling... [4096/50176]	Loss: 1.9536
Profiling... [5120/50176]	Loss: 1.9839
Profiling... [6144/50176]	Loss: 1.8701
Profiling... [7168/50176]	Loss: 1.9187
Profiling... [8192/50176]	Loss: 1.8954
Profiling... [9216/50176]	Loss: 1.8871
Profiling... [10240/50176]	Loss: 1.9232
Profiling... [11264/50176]	Loss: 1.9104
Profiling... [12288/50176]	Loss: 1.8312
Profiling... [13312/50176]	Loss: 1.8960
Profile done
epoch 1 train time consumed: 12.97s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4363
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.77862543590182,
                        "time": 9.064829446999283,
                        "accuracy": 0.436328125,
                        "total_cost": 2571.5329002925428
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9199
Profiling... [2048/50176]	Loss: 1.9408
Profiling... [3072/50176]	Loss: 1.9672
Profiling... [4096/50176]	Loss: 1.8623
Profiling... [5120/50176]	Loss: 1.9759
Profiling... [6144/50176]	Loss: 1.8734
Profiling... [7168/50176]	Loss: 1.8652
Profiling... [8192/50176]	Loss: 1.8181
Profiling... [9216/50176]	Loss: 1.8687
Profiling... [10240/50176]	Loss: 1.8220
Profiling... [11264/50176]	Loss: 1.8659
Profiling... [12288/50176]	Loss: 1.8489
Profiling... [13312/50176]	Loss: 1.8948
Profile done
epoch 1 train time consumed: 13.38s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4493
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.83943879499787,
                        "time": 9.377409663000435,
                        "accuracy": 0.44931640625,
                        "total_cost": 2584.577669239657
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9762
Profiling... [2048/50176]	Loss: 2.0298
Profiling... [3072/50176]	Loss: 1.8772
Profiling... [4096/50176]	Loss: 1.9179
Profiling... [5120/50176]	Loss: 1.9303
Profiling... [6144/50176]	Loss: 2.0283
Profiling... [7168/50176]	Loss: 1.9744
Profiling... [8192/50176]	Loss: 1.8711
Profiling... [9216/50176]	Loss: 1.8779
Profiling... [10240/50176]	Loss: 1.8997
Profiling... [11264/50176]	Loss: 1.8031
Profiling... [12288/50176]	Loss: 1.8082
Profiling... [13312/50176]	Loss: 1.8899
Profile done
epoch 1 train time consumed: 15.47s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4635
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.83220700678709,
                        "time": 11.034174250000433,
                        "accuracy": 0.4634765625,
                        "total_cost": 2948.1235092119964
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9947
Profiling... [2048/50176]	Loss: 2.0122
Profiling... [3072/50176]	Loss: 1.9997
Profiling... [4096/50176]	Loss: 1.8873
Profiling... [5120/50176]	Loss: 2.0010
Profiling... [6144/50176]	Loss: 1.9188
Profiling... [7168/50176]	Loss: 1.9395
Profiling... [8192/50176]	Loss: 1.8821
Profiling... [9216/50176]	Loss: 1.9322
Profiling... [10240/50176]	Loss: 1.8020
Profiling... [11264/50176]	Loss: 1.8829
Profiling... [12288/50176]	Loss: 1.9204
Profiling... [13312/50176]	Loss: 1.9379
Profile done
epoch 1 train time consumed: 38.27s
Validation Epoch: 7, Average loss: 0.0019, Accuracy: 0.4630
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.64183123724251,
                        "time": 28.470141902999785,
                        "accuracy": 0.46298828125,
                        "total_cost": 7603.001248686672
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0598
Profiling... [2048/50176]	Loss: 2.0852
Profiling... [3072/50176]	Loss: 1.9981
Profiling... [4096/50176]	Loss: 1.9759
Profiling... [5120/50176]	Loss: 1.9122
Profiling... [6144/50176]	Loss: 1.8712
Profiling... [7168/50176]	Loss: 1.9452
Profiling... [8192/50176]	Loss: 2.0217
Profiling... [9216/50176]	Loss: 1.8922
Profiling... [10240/50176]	Loss: 1.9892
Profiling... [11264/50176]	Loss: 1.9441
Profiling... [12288/50176]	Loss: 1.8367
Profiling... [13312/50176]	Loss: 1.7832
Profile done
epoch 1 train time consumed: 13.21s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4144
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.67017846561359,
                        "time": 9.164279032000195,
                        "accuracy": 0.41435546875,
                        "total_cost": 2735.2071080783694
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0682
Profiling... [2048/50176]	Loss: 1.9720
Profiling... [3072/50176]	Loss: 1.8684
Profiling... [4096/50176]	Loss: 1.9492
Profiling... [5120/50176]	Loss: 1.9437
Profiling... [6144/50176]	Loss: 1.8929
Profiling... [7168/50176]	Loss: 1.9227
Profiling... [8192/50176]	Loss: 1.8293
Profiling... [9216/50176]	Loss: 1.8489
Profiling... [10240/50176]	Loss: 1.9668
Profiling... [11264/50176]	Loss: 1.9462
Profiling... [12288/50176]	Loss: 1.9512
Profiling... [13312/50176]	Loss: 1.9561
Profile done
epoch 1 train time consumed: 13.59s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4152
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.73384014100363,
                        "time": 9.495548142999723,
                        "accuracy": 0.415234375,
                        "total_cost": 2829.536056539471
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0445
Profiling... [2048/50176]	Loss: 1.9023
Profiling... [3072/50176]	Loss: 2.0197
Profiling... [4096/50176]	Loss: 1.9079
Profiling... [5120/50176]	Loss: 1.9674
Profiling... [6144/50176]	Loss: 1.9453
Profiling... [7168/50176]	Loss: 1.9760
Profiling... [8192/50176]	Loss: 1.8559
Profiling... [9216/50176]	Loss: 1.9221
Profiling... [10240/50176]	Loss: 1.8607
Profiling... [11264/50176]	Loss: 1.8857
Profiling... [12288/50176]	Loss: 1.8198
Profiling... [13312/50176]	Loss: 1.8480
Profile done
epoch 1 train time consumed: 15.57s
Validation Epoch: 7, Average loss: 0.0020, Accuracy: 0.4446
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.73022851467137,
                        "time": 11.043480058000569,
                        "accuracy": 0.44462890625,
                        "total_cost": 3073.1522219235526
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0007
Profiling... [2048/50176]	Loss: 2.0236
Profiling... [3072/50176]	Loss: 2.0077
Profiling... [4096/50176]	Loss: 1.9021
Profiling... [5120/50176]	Loss: 1.9574
Profiling... [6144/50176]	Loss: 1.9392
Profiling... [7168/50176]	Loss: 1.9359
Profiling... [8192/50176]	Loss: 1.9917
Profiling... [9216/50176]	Loss: 1.7593
Profiling... [10240/50176]	Loss: 1.8821
Profiling... [11264/50176]	Loss: 1.9187
Profiling... [12288/50176]	Loss: 1.8103
Profiling... [13312/50176]	Loss: 1.8809
Profile done
epoch 1 train time consumed: 38.46s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4432
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.539663647959,
                        "time": 28.567915644000095,
                        "accuracy": 0.4431640625,
                        "total_cost": 7963.801644640438
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.1376
Profiling... [2048/50176]	Loss: 1.9672
Profiling... [3072/50176]	Loss: 1.9560
Profiling... [4096/50176]	Loss: 1.8787
Profiling... [5120/50176]	Loss: 1.9106
Profiling... [6144/50176]	Loss: 1.9359
Profiling... [7168/50176]	Loss: 1.9682
Profiling... [8192/50176]	Loss: 1.9602
Profiling... [9216/50176]	Loss: 1.8840
Profiling... [10240/50176]	Loss: 1.8699
Profiling... [11264/50176]	Loss: 1.9267
Profiling... [12288/50176]	Loss: 1.8859
Profiling... [13312/50176]	Loss: 1.8072
Profile done
epoch 1 train time consumed: 13.15s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4167
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 123.56606186175944,
                        "time": 9.041418107000027,
                        "accuracy": 0.41669921875,
                        "total_cost": 2681.10036893991
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0568
Profiling... [2048/50176]	Loss: 1.9853
Profiling... [3072/50176]	Loss: 2.0352
Profiling... [4096/50176]	Loss: 1.8118
Profiling... [5120/50176]	Loss: 1.9128
Profiling... [6144/50176]	Loss: 1.8045
Profiling... [7168/50176]	Loss: 1.9982
Profiling... [8192/50176]	Loss: 1.9302
Profiling... [9216/50176]	Loss: 1.9054
Profiling... [10240/50176]	Loss: 1.8514
Profiling... [11264/50176]	Loss: 1.8199
Profiling... [12288/50176]	Loss: 1.8114
Profiling... [13312/50176]	Loss: 1.9280
Profile done
epoch 1 train time consumed: 13.45s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4354
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 123.62920337338262,
                        "time": 9.366976397000144,
                        "accuracy": 0.43544921875,
                        "total_cost": 2659.3958149761993
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0081
Profiling... [2048/50176]	Loss: 1.9433
Profiling... [3072/50176]	Loss: 1.9815
Profiling... [4096/50176]	Loss: 1.8798
Profiling... [5120/50176]	Loss: 1.9414
Profiling... [6144/50176]	Loss: 1.8761
Profiling... [7168/50176]	Loss: 1.9253
Profiling... [8192/50176]	Loss: 1.8841
Profiling... [9216/50176]	Loss: 1.8643
Profiling... [10240/50176]	Loss: 2.0164
Profiling... [11264/50176]	Loss: 1.8756
Profiling... [12288/50176]	Loss: 1.9244
Profiling... [13312/50176]	Loss: 1.9354
Profile done
epoch 1 train time consumed: 15.51s
Validation Epoch: 7, Average loss: 0.0022, Accuracy: 0.4171
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 123.62424777863116,
                        "time": 11.006248836999475,
                        "accuracy": 0.41708984375,
                        "total_cost": 3262.2209668429364
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0441
Profiling... [2048/50176]	Loss: 2.0659
Profiling... [3072/50176]	Loss: 2.0113
Profiling... [4096/50176]	Loss: 1.9074
Profiling... [5120/50176]	Loss: 1.9164
Profiling... [6144/50176]	Loss: 1.9210
Profiling... [7168/50176]	Loss: 1.9732
Profiling... [8192/50176]	Loss: 1.9271
Profiling... [9216/50176]	Loss: 1.8583
Profiling... [10240/50176]	Loss: 1.8592
Profiling... [11264/50176]	Loss: 1.8778
Profiling... [12288/50176]	Loss: 1.9631
Profiling... [13312/50176]	Loss: 1.8831
Profile done
epoch 1 train time consumed: 39.06s
Validation Epoch: 7, Average loss: 0.0021, Accuracy: 0.4347
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 123.43485557808906,
                        "time": 29.238457284000106,
                        "accuracy": 0.43466796875,
                        "total_cost": 8302.992195526667
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0500
Profiling... [2048/50176]	Loss: 1.9299
Profiling... [3072/50176]	Loss: 1.9588
Profiling... [4096/50176]	Loss: 1.9996
Profiling... [5120/50176]	Loss: 2.0112
Profiling... [6144/50176]	Loss: 1.9183
Profiling... [7168/50176]	Loss: 1.9502
Profiling... [8192/50176]	Loss: 1.9517
Profiling... [9216/50176]	Loss: 1.9370
Profiling... [10240/50176]	Loss: 1.8855
Profiling... [11264/50176]	Loss: 1.9223
Profiling... [12288/50176]	Loss: 1.9519
Profiling... [13312/50176]	Loss: 1.9107
Profile done
epoch 1 train time consumed: 13.10s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3547
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 123.46160075785056,
                        "time": 9.058806003999962,
                        "accuracy": 0.3546875,
                        "total_cost": 3153.2396552138516
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.9976
Profiling... [2048/50176]	Loss: 1.9814
Profiling... [3072/50176]	Loss: 1.9720
Profiling... [4096/50176]	Loss: 2.0461
Profiling... [5120/50176]	Loss: 1.9404
Profiling... [6144/50176]	Loss: 1.9373
Profiling... [7168/50176]	Loss: 1.9742
Profiling... [8192/50176]	Loss: 1.9051
Profiling... [9216/50176]	Loss: 1.8966
Profiling... [10240/50176]	Loss: 1.9508
Profiling... [11264/50176]	Loss: 1.9310
Profiling... [12288/50176]	Loss: 1.9436
Profiling... [13312/50176]	Loss: 1.8548
Profile done
epoch 1 train time consumed: 13.55s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3515
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 123.52181009727867,
                        "time": 9.37456621199999,
                        "accuracy": 0.35146484375,
                        "total_cost": 3294.677712365159
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0839
Profiling... [2048/50176]	Loss: 1.9920
Profiling... [3072/50176]	Loss: 1.9284
Profiling... [4096/50176]	Loss: 1.8792
Profiling... [5120/50176]	Loss: 1.9410
Profiling... [6144/50176]	Loss: 1.8960
Profiling... [7168/50176]	Loss: 1.9622
Profiling... [8192/50176]	Loss: 1.9730
Profiling... [9216/50176]	Loss: 1.8729
Profiling... [10240/50176]	Loss: 1.8410
Profiling... [11264/50176]	Loss: 2.0245
Profiling... [12288/50176]	Loss: 1.8762
Profiling... [13312/50176]	Loss: 2.0498
Profile done
epoch 1 train time consumed: 15.51s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3409
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 123.51819484305177,
                        "time": 11.010654345000148,
                        "accuracy": 0.34091796875,
                        "total_cost": 3989.2768155395825
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0215
Profiling... [2048/50176]	Loss: 1.9914
Profiling... [3072/50176]	Loss: 1.9575
Profiling... [4096/50176]	Loss: 1.8835
Profiling... [5120/50176]	Loss: 1.9753
Profiling... [6144/50176]	Loss: 1.9457
Profiling... [7168/50176]	Loss: 1.9923
Profiling... [8192/50176]	Loss: 1.9753
Profiling... [9216/50176]	Loss: 1.8841
Profiling... [10240/50176]	Loss: 1.9282
Profiling... [11264/50176]	Loss: 1.8767
Profiling... [12288/50176]	Loss: 1.8635
Profiling... [13312/50176]	Loss: 1.9095
Profile done
epoch 1 train time consumed: 38.68s
Validation Epoch: 7, Average loss: 0.0025, Accuracy: 0.3521
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 123.33444548135213,
                        "time": 28.89164735600025,
                        "accuracy": 0.35205078125,
                        "total_cost": 10121.651464720515
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0552
Profiling... [2048/50176]	Loss: 2.0710
Profiling... [3072/50176]	Loss: 1.9397
Profiling... [4096/50176]	Loss: 1.9681
Profiling... [5120/50176]	Loss: 2.0061
Profiling... [6144/50176]	Loss: 1.9743
Profiling... [7168/50176]	Loss: 1.8894
Profiling... [8192/50176]	Loss: 1.9565
Profiling... [9216/50176]	Loss: 1.9313
Profiling... [10240/50176]	Loss: 1.9677
Profiling... [11264/50176]	Loss: 1.8262
Profiling... [12288/50176]	Loss: 1.8285
Profiling... [13312/50176]	Loss: 1.9597
Profile done
epoch 1 train time consumed: 13.15s
Validation Epoch: 7, Average loss: 0.0028, Accuracy: 0.3093
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 123.36033463649355,
                        "time": 9.039568926999891,
                        "accuracy": 0.30927734375,
                        "total_cost": 3605.580138148595
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0231
Profiling... [2048/50176]	Loss: 1.8783
Profiling... [3072/50176]	Loss: 2.0012
Profiling... [4096/50176]	Loss: 2.0284
Profiling... [5120/50176]	Loss: 1.8960
Profiling... [6144/50176]	Loss: 1.8240
Profiling... [7168/50176]	Loss: 2.0824
Profiling... [8192/50176]	Loss: 1.9291
Profiling... [9216/50176]	Loss: 1.9021
Profiling... [10240/50176]	Loss: 1.9699
Profiling... [11264/50176]	Loss: 1.8738
Profiling... [12288/50176]	Loss: 1.9242
Profiling... [13312/50176]	Loss: 1.8532
Profile done
epoch 1 train time consumed: 13.50s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3678
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 123.41839896756134,
                        "time": 9.380006417000004,
                        "accuracy": 0.3677734375,
                        "total_cost": 3147.7677729011957
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.9860
Profiling... [2048/50176]	Loss: 1.9737
Profiling... [3072/50176]	Loss: 1.9678
Profiling... [4096/50176]	Loss: 1.9883
Profiling... [5120/50176]	Loss: 1.9040
Profiling... [6144/50176]	Loss: 1.9373
Profiling... [7168/50176]	Loss: 1.9181
Profiling... [8192/50176]	Loss: 1.8900
Profiling... [9216/50176]	Loss: 2.0484
Profiling... [10240/50176]	Loss: 1.8702
Profiling... [11264/50176]	Loss: 1.8845
Profiling... [12288/50176]	Loss: 1.9296
Profiling... [13312/50176]	Loss: 1.8227
Profile done
epoch 1 train time consumed: 15.54s
Validation Epoch: 7, Average loss: 0.0028, Accuracy: 0.3335
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 123.41376254223253,
                        "time": 11.015168326000094,
                        "accuracy": 0.33349609375,
                        "total_cost": 4076.2797334794773
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.9387
Profiling... [2048/50176]	Loss: 1.9845
Profiling... [3072/50176]	Loss: 1.9498
Profiling... [4096/50176]	Loss: 1.9609
Profiling... [5120/50176]	Loss: 1.9812
Profiling... [6144/50176]	Loss: 1.9475
Profiling... [7168/50176]	Loss: 1.9368
Profiling... [8192/50176]	Loss: 1.9723
Profiling... [9216/50176]	Loss: 1.8573
Profiling... [10240/50176]	Loss: 2.0013
Profiling... [11264/50176]	Loss: 1.8785
Profiling... [12288/50176]	Loss: 1.9048
Profiling... [13312/50176]	Loss: 1.9475
Profile done
epoch 1 train time consumed: 38.39s
Validation Epoch: 7, Average loss: 0.0025, Accuracy: 0.3582
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 123.23908748862351,
                        "time": 28.53718240499984,
                        "accuracy": 0.358203125,
                        "total_cost": 9818.162024936502
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 2.0774
Profiling... [2048/50176]	Loss: 2.0090
Profiling... [3072/50176]	Loss: 1.9384
Profiling... [4096/50176]	Loss: 1.8853
Profiling... [5120/50176]	Loss: 1.9573
Profiling... [6144/50176]	Loss: 1.9453
Profiling... [7168/50176]	Loss: 2.0056
Profiling... [8192/50176]	Loss: 1.8840
Profiling... [9216/50176]	Loss: 1.8996
Profiling... [10240/50176]	Loss: 1.9166
Profiling... [11264/50176]	Loss: 1.9201
Profiling... [12288/50176]	Loss: 1.8556
Profiling... [13312/50176]	Loss: 1.9785
Profile done
epoch 1 train time consumed: 13.28s
Validation Epoch: 7, Average loss: 0.0026, Accuracy: 0.3353
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 123.26488404704561,
                        "time": 9.038301390000015,
                        "accuracy": 0.33525390625,
                        "total_cost": 3323.168357029704
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 2.0630
Profiling... [2048/50176]	Loss: 1.8747
Profiling... [3072/50176]	Loss: 1.9902
Profiling... [4096/50176]	Loss: 1.9186
Profiling... [5120/50176]	Loss: 1.8869
Profiling... [6144/50176]	Loss: 1.9841
Profiling... [7168/50176]	Loss: 1.9801
Profiling... [8192/50176]	Loss: 1.9877
Profiling... [9216/50176]	Loss: 1.9888
Profiling... [10240/50176]	Loss: 1.9696
Profiling... [11264/50176]	Loss: 1.8801
Profiling... [12288/50176]	Loss: 1.8548
Profiling... [13312/50176]	Loss: 1.9481
Profile done
epoch 1 train time consumed: 13.44s
Validation Epoch: 7, Average loss: 0.0024, Accuracy: 0.3785
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 123.32393155764811,
                        "time": 9.38237459300035,
                        "accuracy": 0.378515625,
                        "total_cost": 3056.8654125054722
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 2.0799
Profiling... [2048/50176]	Loss: 1.9140
Profiling... [3072/50176]	Loss: 2.0489
Profiling... [4096/50176]	Loss: 1.9562
Profiling... [5120/50176]	Loss: 1.9152
Profiling... [6144/50176]	Loss: 1.9957
Profiling... [7168/50176]	Loss: 1.9766
Profiling... [8192/50176]	Loss: 1.9533
Profiling... [9216/50176]	Loss: 1.8699
Profiling... [10240/50176]	Loss: 1.9817
Profiling... [11264/50176]	Loss: 1.8813
Profiling... [12288/50176]	Loss: 1.9572
Profiling... [13312/50176]	Loss: 1.8984
Profile done
epoch 1 train time consumed: 15.52s
Validation Epoch: 7, Average loss: 0.0023, Accuracy: 0.3915
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 123.32063792109992,
                        "time": 11.017691738999929,
                        "accuracy": 0.39150390625,
                        "total_cost": 3470.485867397406
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 2.0960
Profiling... [2048/50176]	Loss: 1.9848
Profiling... [3072/50176]	Loss: 1.9640
Profiling... [4096/50176]	Loss: 1.9520
Profiling... [5120/50176]	Loss: 1.9196
Profiling... [6144/50176]	Loss: 1.9771
Profiling... [7168/50176]	Loss: 1.9477
Profiling... [8192/50176]	Loss: 1.9848
Profiling... [9216/50176]	Loss: 1.9557
Profiling... [10240/50176]	Loss: 1.8979
Profiling... [11264/50176]	Loss: 1.9056
Profiling... [12288/50176]	Loss: 1.9061
Profiling... [13312/50176]	Loss: 2.0346
Profile done
epoch 1 train time consumed: 39.60s
Validation Epoch: 7, Average loss: 0.0039, Accuracy: 0.2475
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.3,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 123.14217498493953,
                        "time": 29.23053990400058,
                        "accuracy": 0.2474609375,
                        "total_cost": 14545.779613247836
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.0 bs: 128 pl: 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[GPU_0] Set GPU power limit to 175W.
[GPU_0] Set GPU power limit to 175W.
Training Epoch: 7 [128/50048]	Loss: 2.0695
Training Epoch: 7 [256/50048]	Loss: 1.8927
Training Epoch: 7 [384/50048]	Loss: 2.0654
Training Epoch: 7 [512/50048]	Loss: 1.9696
Training Epoch: 7 [640/50048]	Loss: 2.0557
Training Epoch: 7 [768/50048]	Loss: 1.8670
Training Epoch: 7 [896/50048]	Loss: 2.0072
Training Epoch: 7 [1024/50048]	Loss: 1.8237
Training Epoch: 7 [1152/50048]	Loss: 1.6572
Training Epoch: 7 [1280/50048]	Loss: 1.9142
Training Epoch: 7 [1408/50048]	Loss: 1.9476
Training Epoch: 7 [1536/50048]	Loss: 1.9024
Training Epoch: 7 [1664/50048]	Loss: 1.9222
Training Epoch: 7 [1792/50048]	Loss: 1.9721
Training Epoch: 7 [1920/50048]	Loss: 1.9861
Training Epoch: 7 [2048/50048]	Loss: 1.9985
Training Epoch: 7 [2176/50048]	Loss: 1.9738
Training Epoch: 7 [2304/50048]	Loss: 1.9863
Training Epoch: 7 [2432/50048]	Loss: 1.9953
Training Epoch: 7 [2560/50048]	Loss: 1.9214
Training Epoch: 7 [2688/50048]	Loss: 2.0396
Training Epoch: 7 [2816/50048]	Loss: 1.9053
Training Epoch: 7 [2944/50048]	Loss: 1.8596
Training Epoch: 7 [3072/50048]	Loss: 1.7629
Training Epoch: 7 [3200/50048]	Loss: 1.7053
Training Epoch: 7 [3328/50048]	Loss: 1.8853
Training Epoch: 7 [3456/50048]	Loss: 1.8002
Training Epoch: 7 [3584/50048]	Loss: 1.8898
Training Epoch: 7 [3712/50048]	Loss: 1.7195
Training Epoch: 7 [3840/50048]	Loss: 1.7567
Training Epoch: 7 [3968/50048]	Loss: 1.8091
Training Epoch: 7 [4096/50048]	Loss: 1.8347
Training Epoch: 7 [4224/50048]	Loss: 2.1065
Training Epoch: 7 [4352/50048]	Loss: 1.9264
Training Epoch: 7 [4480/50048]	Loss: 2.0314
Training Epoch: 7 [4608/50048]	Loss: 2.0936
Training Epoch: 7 [4736/50048]	Loss: 1.5504
Training Epoch: 7 [4864/50048]	Loss: 2.0736
Training Epoch: 7 [4992/50048]	Loss: 1.8437
Training Epoch: 7 [5120/50048]	Loss: 1.8247
Training Epoch: 7 [5248/50048]	Loss: 1.8674
Training Epoch: 7 [5376/50048]	Loss: 1.8315
Training Epoch: 7 [5504/50048]	Loss: 1.9238
Training Epoch: 7 [5632/50048]	Loss: 1.7630
Training Epoch: 7 [5760/50048]	Loss: 1.8159
Training Epoch: 7 [5888/50048]	Loss: 1.9041
Training Epoch: 7 [6016/50048]	Loss: 1.9337
Training Epoch: 7 [6144/50048]	Loss: 1.8600
Training Epoch: 7 [6272/50048]	Loss: 1.8522
Training Epoch: 7 [6400/50048]	Loss: 1.6535
Training Epoch: 7 [6528/50048]	Loss: 1.7419
Training Epoch: 7 [6656/50048]	Loss: 1.9825
Training Epoch: 7 [6784/50048]	Loss: 1.7884
Training Epoch: 7 [6912/50048]	Loss: 1.9001
Training Epoch: 7 [7040/50048]	Loss: 1.8364
Training Epoch: 7 [7168/50048]	Loss: 1.9683
Training Epoch: 7 [7296/50048]	Loss: 1.8271
Training Epoch: 7 [7424/50048]	Loss: 1.5641
Training Epoch: 7 [7552/50048]	Loss: 1.8984
Training Epoch: 7 [7680/50048]	Loss: 1.7270
Training Epoch: 7 [7808/50048]	Loss: 1.8141
Training Epoch: 7 [7936/50048]	Loss: 1.8815
Training Epoch: 7 [8064/50048]	Loss: 1.8365
Training Epoch: 7 [8192/50048]	Loss: 1.9287
Training Epoch: 7 [8320/50048]	Loss: 1.8850
Training Epoch: 7 [8448/50048]	Loss: 1.8880
Training Epoch: 7 [8576/50048]	Loss: 1.9774
Training Epoch: 7 [8704/50048]	Loss: 2.0221
Training Epoch: 7 [8832/50048]	Loss: 2.1687
Training Epoch: 7 [8960/50048]	Loss: 1.9666
Training Epoch: 7 [9088/50048]	Loss: 1.9489
Training Epoch: 7 [9216/50048]	Loss: 1.7133
Training Epoch: 7 [9344/50048]	Loss: 1.9447
Training Epoch: 7 [9472/50048]	Loss: 1.9803
Training Epoch: 7 [9600/50048]	Loss: 1.7814
Training Epoch: 7 [9728/50048]	Loss: 1.8354
Training Epoch: 7 [9856/50048]	Loss: 1.9055
Training Epoch: 7 [9984/50048]	Loss: 1.7955
Training Epoch: 7 [10112/50048]	Loss: 2.0428
Training Epoch: 7 [10240/50048]	Loss: 1.8157
Training Epoch: 7 [10368/50048]	Loss: 1.6893
Training Epoch: 7 [10496/50048]	Loss: 1.6937
Training Epoch: 7 [10624/50048]	Loss: 1.8578
Training Epoch: 7 [10752/50048]	Loss: 1.7041
Training Epoch: 7 [10880/50048]	Loss: 1.6971
Training Epoch: 7 [11008/50048]	Loss: 1.9169
Training Epoch: 7 [11136/50048]	Loss: 1.8130
Training Epoch: 7 [11264/50048]	Loss: 1.7627
Training Epoch: 7 [11392/50048]	Loss: 1.5393
Training Epoch: 7 [11520/50048]	Loss: 1.8067
Training Epoch: 7 [11648/50048]	Loss: 1.9869
Training Epoch: 7 [11776/50048]	Loss: 1.3565
Training Epoch: 7 [11904/50048]	Loss: 1.7707
Training Epoch: 7 [12032/50048]	Loss: 1.9820
Training Epoch: 7 [12160/50048]	Loss: 1.7664
Training Epoch: 7 [12288/50048]	Loss: 1.7894
Training Epoch: 7 [12416/50048]	Loss: 2.1529
Training Epoch: 7 [12544/50048]	Loss: 1.6852
Training Epoch: 7 [12672/50048]	Loss: 1.5514
Training Epoch: 7 [12800/50048]	Loss: 1.6844
Training Epoch: 7 [12928/50048]	Loss: 1.6961
Training Epoch: 7 [13056/50048]	Loss: 1.7312
Training Epoch: 7 [13184/50048]	Loss: 2.0021
Training Epoch: 7 [13312/50048]	Loss: 1.3977
Training Epoch: 7 [13440/50048]	Loss: 1.7597
Training Epoch: 7 [13568/50048]	Loss: 1.9201
Training Epoch: 7 [13696/50048]	Loss: 1.7424
Training Epoch: 7 [13824/50048]	Loss: 1.7483
Training Epoch: 7 [13952/50048]	Loss: 1.6237
Training Epoch: 7 [14080/50048]	Loss: 1.8946
Training Epoch: 7 [14208/50048]	Loss: 2.1852
Training Epoch: 7 [14336/50048]	Loss: 1.9068
Training Epoch: 7 [14464/50048]	Loss: 1.8262
Training Epoch: 7 [14592/50048]	Loss: 1.6677
Training Epoch: 7 [14720/50048]	Loss: 2.0671
Training Epoch: 7 [14848/50048]	Loss: 1.6924
Training Epoch: 7 [14976/50048]	Loss: 1.8515
Training Epoch: 7 [15104/50048]	Loss: 1.7057
Training Epoch: 7 [15232/50048]	Loss: 1.7368
Training Epoch: 7 [15360/50048]	Loss: 1.9122
Training Epoch: 7 [15488/50048]	Loss: 1.9901
Training Epoch: 7 [15616/50048]	Loss: 1.8479
Training Epoch: 7 [15744/50048]	Loss: 1.4549
Training Epoch: 7 [15872/50048]	Loss: 1.6940
Training Epoch: 7 [16000/50048]	Loss: 1.9408
Training Epoch: 7 [16128/50048]	Loss: 1.6872
Training Epoch: 7 [16256/50048]	Loss: 2.1083
Training Epoch: 7 [16384/50048]	Loss: 1.7440
Training Epoch: 7 [16512/50048]	Loss: 1.8466
Training Epoch: 7 [16640/50048]	Loss: 1.5352
Training Epoch: 7 [16768/50048]	Loss: 2.0255
Training Epoch: 7 [16896/50048]	Loss: 1.9958
Training Epoch: 7 [17024/50048]	Loss: 1.9979
Training Epoch: 7 [17152/50048]	Loss: 1.9063
Training Epoch: 7 [17280/50048]	Loss: 1.7825
Training Epoch: 7 [17408/50048]	Loss: 1.7283
Training Epoch: 7 [17536/50048]	Loss: 1.7956
Training Epoch: 7 [17664/50048]	Loss: 1.8543
Training Epoch: 7 [17792/50048]	Loss: 1.6943
Training Epoch: 7 [17920/50048]	Loss: 1.6856
Training Epoch: 7 [18048/50048]	Loss: 2.0197
Training Epoch: 7 [18176/50048]	Loss: 1.7959
Training Epoch: 7 [18304/50048]	Loss: 1.8774
Training Epoch: 7 [18432/50048]	Loss: 1.7548
Training Epoch: 7 [18560/50048]	Loss: 1.6838
Training Epoch: 7 [18688/50048]	Loss: 1.7234
Training Epoch: 7 [18816/50048]	Loss: 1.6586
Training Epoch: 7 [18944/50048]	Loss: 1.9587
Training Epoch: 7 [19072/50048]	Loss: 1.6537
Training Epoch: 7 [19200/50048]	Loss: 1.9146
Training Epoch: 7 [19328/50048]	Loss: 1.7542
Training Epoch: 7 [19456/50048]	Loss: 1.8799
Training Epoch: 7 [19584/50048]	Loss: 1.6816
Training Epoch: 7 [19712/50048]	Loss: 1.9428
Training Epoch: 7 [19840/50048]	Loss: 1.8075
Training Epoch: 7 [19968/50048]	Loss: 1.5464
Training Epoch: 7 [20096/50048]	Loss: 1.7091
Training Epoch: 7 [20224/50048]	Loss: 1.8900
Training Epoch: 7 [20352/50048]	Loss: 1.9143
Training Epoch: 7 [20480/50048]	Loss: 1.5862
Training Epoch: 7 [20608/50048]	Loss: 1.8688
Training Epoch: 7 [20736/50048]	Loss: 1.7572
Training Epoch: 7 [20864/50048]	Loss: 1.8407
Training Epoch: 7 [20992/50048]	Loss: 2.0194
Training Epoch: 7 [21120/50048]	Loss: 1.8899
Training Epoch: 7 [21248/50048]	Loss: 1.6457
Training Epoch: 7 [21376/50048]	Loss: 2.0359
Training Epoch: 7 [21504/50048]	Loss: 1.9115
Training Epoch: 7 [21632/50048]	Loss: 1.7909
Training Epoch: 7 [21760/50048]	Loss: 1.6847
Training Epoch: 7 [21888/50048]	Loss: 1.7177
Training Epoch: 7 [22016/50048]	Loss: 1.7918
Training Epoch: 7 [22144/50048]	Loss: 1.9332
Training Epoch: 7 [22272/50048]	Loss: 1.7721
Training Epoch: 7 [22400/50048]	Loss: 1.7254
Training Epoch: 7 [22528/50048]	Loss: 1.7748
Training Epoch: 7 [22656/50048]	Loss: 1.5433
Training Epoch: 7 [22784/50048]	Loss: 1.8956
Training Epoch: 7 [22912/50048]	Loss: 1.6132
Training Epoch: 7 [23040/50048]	Loss: 1.7744
Training Epoch: 7 [23168/50048]	Loss: 1.6497
Training Epoch: 7 [23296/50048]	Loss: 1.7587
Training Epoch: 7 [23424/50048]	Loss: 1.7267
Training Epoch: 7 [23552/50048]	Loss: 1.6633
Training Epoch: 7 [23680/50048]	Loss: 1.6984
Training Epoch: 7 [23808/50048]	Loss: 1.8969
Training Epoch: 7 [23936/50048]	Loss: 1.7828
Training Epoch: 7 [24064/50048]	Loss: 1.9533
Training Epoch: 7 [24192/50048]	Loss: 2.0060
Training Epoch: 7 [24320/50048]	Loss: 1.7051
Training Epoch: 7 [24448/50048]	Loss: 1.8213
Training Epoch: 7 [24576/50048]	Loss: 1.7983
Training Epoch: 7 [24704/50048]	Loss: 1.6542
Training Epoch: 7 [24832/50048]	Loss: 1.8654
Training Epoch: 7 [24960/50048]	Loss: 1.8836
Training Epoch: 7 [25088/50048]	Loss: 1.9671
Training Epoch: 7 [25216/50048]	Loss: 1.7913
Training Epoch: 7 [25344/50048]	Loss: 1.7421
Training Epoch: 7 [25472/50048]	Loss: 1.8960
Training Epoch: 7 [25600/50048]	Loss: 1.9272
Training Epoch: 7 [25728/50048]	Loss: 1.8621
Training Epoch: 7 [25856/50048]	Loss: 1.7635
Training Epoch: 7 [25984/50048]	Loss: 1.6520
Training Epoch: 7 [26112/50048]	Loss: 1.8178
Training Epoch: 7 [26240/50048]	Loss: 1.6894
Training Epoch: 7 [26368/50048]	Loss: 2.0842
Training Epoch: 7 [26496/50048]	Loss: 2.1395
Training Epoch: 7 [26624/50048]	Loss: 1.9488
Training Epoch: 7 [26752/50048]	Loss: 1.8183
Training Epoch: 7 [26880/50048]	Loss: 1.8728
Training Epoch: 7 [27008/50048]	Loss: 1.6856
Training Epoch: 7 [27136/50048]	Loss: 1.5305
Training Epoch: 7 [27264/50048]	Loss: 2.0379
Training Epoch: 7 [27392/50048]	Loss: 1.6661
Training Epoch: 7 [27520/50048]	Loss: 1.7154
Training Epoch: 7 [27648/50048]	Loss: 1.5922
Training Epoch: 7 [27776/50048]	Loss: 1.8973
Training Epoch: 7 [27904/50048]	Loss: 1.9860
Training Epoch: 7 [28032/50048]	Loss: 1.7202
Training Epoch: 7 [28160/50048]	Loss: 1.8668
Training Epoch: 7 [28288/50048]	Loss: 1.7923
Training Epoch: 7 [28416/50048]	Loss: 1.9700
Training Epoch: 7 [28544/50048]	Loss: 1.8136
Training Epoch: 7 [28672/50048]	Loss: 1.7984
Training Epoch: 7 [28800/50048]	Loss: 1.7847
Training Epoch: 7 [28928/50048]	Loss: 1.6846
Training Epoch: 7 [29056/50048]	Loss: 1.8392
Training Epoch: 7 [29184/50048]	Loss: 1.9008
Training Epoch: 7 [29312/50048]	Loss: 1.8660
Training Epoch: 7 [29440/50048]	Loss: 2.0568
Training Epoch: 7 [29568/50048]	Loss: 1.7980
Training Epoch: 7 [29696/50048]	Loss: 1.8161
Training Epoch: 7 [29824/50048]	Loss: 2.0222
Training Epoch: 7 [29952/50048]	Loss: 1.8908
Training Epoch: 7 [30080/50048]	Loss: 1.6764
Training Epoch: 7 [30208/50048]	Loss: 1.9230
Training Epoch: 7 [30336/50048]	Loss: 1.8350
Training Epoch: 7 [30464/50048]	Loss: 1.6857
Training Epoch: 7 [30592/50048]	Loss: 1.8163
Training Epoch: 7 [30720/50048]	Loss: 2.0260
Training Epoch: 7 [30848/50048]	Loss: 1.4794
Training Epoch: 7 [30976/50048]	Loss: 1.7094
Training Epoch: 7 [31104/50048]	Loss: 1.6889
Training Epoch: 7 [31232/50048]	Loss: 1.7584
Training Epoch: 7 [31360/50048]	Loss: 1.9348
Training Epoch: 7 [31488/50048]	Loss: 1.7547
Training Epoch: 7 [31616/50048]	Loss: 1.8175
Training Epoch: 7 [31744/50048]	Loss: 2.1567
Training Epoch: 7 [31872/50048]	Loss: 1.9214
Training Epoch: 7 [32000/50048]	Loss: 1.7471
Training Epoch: 7 [32128/50048]	Loss: 1.5834
Training Epoch: 7 [32256/50048]	Loss: 1.7451
Training Epoch: 7 [32384/50048]	Loss: 1.8655
Training Epoch: 7 [32512/50048]	Loss: 1.9342
Training Epoch: 7 [32640/50048]	Loss: 1.7333
Training Epoch: 7 [32768/50048]	Loss: 1.5225
Training Epoch: 7 [32896/50048]	Loss: 1.4755
Training Epoch: 7 [33024/50048]	Loss: 1.8898
Training Epoch: 7 [33152/50048]	Loss: 1.7463
Training Epoch: 7 [33280/50048]	Loss: 1.9734
Training Epoch: 7 [33408/50048]	Loss: 1.6228
Training Epoch: 7 [33536/50048]	Loss: 1.9287
Training Epoch: 7 [33664/50048]	Loss: 1.7488
Training Epoch: 7 [33792/50048]	Loss: 1.7778
Training Epoch: 7 [33920/50048]	Loss: 1.7357
Training Epoch: 7 [34048/50048]	Loss: 1.4462
Training Epoch: 7 [34176/50048]	Loss: 2.0247
Training Epoch: 7 [34304/50048]	Loss: 1.8299
Training Epoch: 7 [34432/50048]	Loss: 1.8891
Training Epoch: 7 [34560/50048]	Loss: 1.9936
Training Epoch: 7 [34688/50048]	Loss: 1.8097
Training Epoch: 7 [34816/50048]	Loss: 1.6108
Training Epoch: 7 [34944/50048]	Loss: 1.8640
Training Epoch: 7 [35072/50048]	Loss: 1.7924
Training Epoch: 7 [35200/50048]	Loss: 1.7851
Training Epoch: 7 [35328/50048]	Loss: 1.7582
Training Epoch: 7 [35456/50048]	Loss: 1.7353
Training Epoch: 7 [35584/50048]	Loss: 1.8047
Training Epoch: 7 [35712/50048]	Loss: 1.9984
Training Epoch: 7 [35840/50048]	Loss: 1.8169
Training Epoch: 7 [35968/50048]	Loss: 1.7554
Training Epoch: 7 [36096/50048]	Loss: 1.8970
Training Epoch: 7 [36224/50048]	Loss: 1.5864
Training Epoch: 7 [36352/50048]	Loss: 1.8365
Training Epoch: 7 [36480/50048]	Loss: 1.6533
Training Epoch: 7 [36608/50048]	Loss: 1.9436
Training Epoch: 7 [36736/50048]	Loss: 1.7284
Training Epoch: 7 [36864/50048]	Loss: 1.8613
Training Epoch: 7 [36992/50048]	Loss: 1.9055
Training Epoch: 7 [37120/50048]	Loss: 1.7771
Training Epoch: 7 [37248/50048]	Loss: 2.0866
Training Epoch: 7 [37376/50048]	Loss: 1.7628
Training Epoch: 7 [37504/50048]	Loss: 1.6730
Training Epoch: 7 [37632/50048]	Loss: 1.5902
Training Epoch: 7 [37760/50048]	Loss: 2.1020
Training Epoch: 7 [37888/50048]	Loss: 1.8762
Training Epoch: 7 [38016/50048]	Loss: 1.6932
Training Epoch: 7 [38144/50048]	Loss: 1.6692
Training Epoch: 7 [38272/50048]	Loss: 1.6021
Training Epoch: 7 [38400/50048]	Loss: 1.8041
Training Epoch: 7 [38528/50048]	Loss: 1.8469
Training Epoch: 7 [38656/50048]	Loss: 1.6332
Training Epoch: 7 [38784/50048]	Loss: 2.1014
Training Epoch: 7 [38912/50048]	Loss: 1.9946
Training Epoch: 7 [39040/50048]	Loss: 1.6588
Training Epoch: 7 [39168/50048]	Loss: 1.7461
Training Epoch: 7 [39296/50048]	Loss: 1.7247
Training Epoch: 7 [39424/50048]	Loss: 1.8557
Training Epoch: 7 [39552/50048]	Loss: 1.8356
Training Epoch: 7 [39680/50048]	Loss: 1.8652
Training Epoch: 7 [39808/50048]	Loss: 1.9434
Training Epoch: 7 [39936/50048]	Loss: 2.0173
Training Epoch: 7 [40064/50048]	Loss: 1.7256
Training Epoch: 7 [40192/50048]	Loss: 1.5052
Training Epoch: 7 [40320/50048]	Loss: 1.6765
Training Epoch: 7 [40448/50048]	Loss: 1.9722
Training Epoch: 7 [40576/50048]	Loss: 1.9213
Training Epoch: 7 [40704/50048]	Loss: 2.0205
Training Epoch: 7 [40832/50048]	Loss: 1.5987
Training Epoch: 7 [40960/50048]	Loss: 1.6009
Training Epoch: 7 [41088/50048]	Loss: 1.8221
Training Epoch: 7 [41216/50048]	Loss: 1.7461
Training Epoch: 7 [41344/50048]	Loss: 1.8193
Training Epoch: 7 [41472/50048]	Loss: 1.7587
Training Epoch: 7 [41600/50048]	Loss: 1.7289
Training Epoch: 7 [41728/50048]	Loss: 1.6286
Training Epoch: 7 [41856/50048]	Loss: 1.8267
Training Epoch: 7 [41984/50048]	Loss: 1.7206
Training Epoch: 7 [42112/50048]	Loss: 1.6480
Training Epoch: 7 [42240/50048]	Loss: 1.6644
Training Epoch: 7 [42368/50048]	Loss: 1.9190
Training Epoch: 7 [42496/50048]	Loss: 1.7437
Training Epoch: 7 [42624/50048]	Loss: 1.7493
Training Epoch: 7 [42752/50048]	Loss: 1.9143
Training Epoch: 7 [42880/50048]	Loss: 2.0616
Training Epoch: 7 [43008/50048]	Loss: 1.6828
Training Epoch: 7 [43136/50048]	Loss: 1.8871
Training Epoch: 7 [43264/50048]	Loss: 1.8376
Training Epoch: 7 [43392/50048]	Loss: 1.7106
Training Epoch: 7 [43520/50048]	Loss: 1.5640
Training Epoch: 7 [43648/50048]	Loss: 1.9230
Training Epoch: 7 [43776/50048]	Loss: 1.7953
Training Epoch: 7 [43904/50048]	Loss: 1.7884
Training Epoch: 7 [44032/50048]	Loss: 1.6829
Training Epoch: 7 [44160/50048]	Loss: 1.6766
Training Epoch: 7 [44288/50048]	Loss: 1.7517
Training Epoch: 7 [44416/50048]	Loss: 1.6760
Training Epoch: 7 [44544/50048]	Loss: 1.7697
Training Epoch: 7 [44672/50048]	Loss: 1.6201
Training Epoch: 7 [44800/50048]	Loss: 1.9388
Training Epoch: 7 [44928/50048]	Loss: 1.8302
Training Epoch: 7 [45056/50048]	Loss: 1.7493
Training Epoch: 7 [45184/50048]	Loss: 1.7821
Training Epoch: 7 [45312/50048]	Loss: 1.8969
Training Epoch: 7 [45440/50048]	Loss: 1.9841
Training Epoch: 7 [45568/50048]	Loss: 1.6887
Training Epoch: 7 [45696/50048]	Loss: 1.7476
Training Epoch: 7 [45824/50048]	Loss: 1.6876
Training Epoch: 7 [45952/50048]	Loss: 1.9556
Training Epoch: 7 [46080/50048]	Loss: 1.9327
Training Epoch: 7 [46208/50048]	Loss: 1.7507
Training Epoch: 7 [46336/50048]	Loss: 1.4994
Training Epoch: 7 [46464/50048]	Loss: 1.8031
Training Epoch: 7 [46592/50048]	Loss: 1.7821
Training Epoch: 7 [46720/50048]	Loss: 1.8654
Training Epoch: 7 [46848/50048]	Loss: 1.4611
Training Epoch: 7 [46976/50048]	Loss: 1.7504
Training Epoch: 7 [47104/50048]	Loss: 1.9190
Training Epoch: 7 [47232/50048]	Loss: 1.8259
Training Epoch: 7 [47360/50048]	Loss: 1.6929
Training Epoch: 7 [47488/50048]	Loss: 1.8675
Training Epoch: 7 [47616/50048]	Loss: 1.8060
Training Epoch: 7 [47744/50048]	Loss: 1.6200
Training Epoch: 7 [47872/50048]	Loss: 1.6994
Training Epoch: 7 [48000/50048]	Loss: 1.7003
Training Epoch: 7 [48128/50048]	Loss: 1.7738
Training Epoch: 7 [48256/50048]	Loss: 1.6437
Training Epoch: 7 [48384/50048]	Loss: 1.6834
Training Epoch: 7 [48512/50048]	Loss: 1.7898
Training Epoch: 7 [48640/50048]	Loss: 1.8517
Training Epoch: 7 [48768/50048]	Loss: 1.8389
Training Epoch: 7 [48896/50048]	Loss: 1.5357
Training Epoch: 7 [49024/50048]	Loss: 1.7630
Training Epoch: 7 [49152/50048]	Loss: 1.8090
Training Epoch: 7 [49280/50048]	Loss: 2.0049
Training Epoch: 7 [49408/50048]	Loss: 1.4777
Training Epoch: 7 [49536/50048]	Loss: 1.9855
Training Epoch: 7 [49664/50048]	Loss: 1.5779
Training Epoch: 7 [49792/50048]	Loss: 1.8464
Training Epoch: 7 [49920/50048]	Loss: 1.5475
Training Epoch: 7 [50048/50048]	Loss: 1.5208
Validation Epoch: 7, Average loss: 0.0142, Accuracy: 0.5067
[Training Loop] Model's accuracy 0.5067246835443038 surpasses threshold 0.4! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7512
Profiling... [256/50048]	Loss: 1.7820
Profiling... [384/50048]	Loss: 1.6893
Profiling... [512/50048]	Loss: 1.7009
Profiling... [640/50048]	Loss: 1.8194
Profiling... [768/50048]	Loss: 1.6827
Profiling... [896/50048]	Loss: 1.7734
Profiling... [1024/50048]	Loss: 1.6302
Profiling... [1152/50048]	Loss: 2.0161
Profiling... [1280/50048]	Loss: 1.6457
Profiling... [1408/50048]	Loss: 1.8670
Profiling... [1536/50048]	Loss: 1.7517
Profiling... [1664/50048]	Loss: 1.5215
Profile done
epoch 1 train time consumed: 3.52s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5082
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 123.60080137511306,
                        "time": 2.171641424999507,
                        "accuracy": 0.5082080696202531,
                        "total_cost": 528.1628460364664
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7316
Profiling... [256/50048]	Loss: 1.6438
Profiling... [384/50048]	Loss: 1.5976
Profiling... [512/50048]	Loss: 1.5248
Profiling... [640/50048]	Loss: 1.7781
Profiling... [768/50048]	Loss: 1.8700
Profiling... [896/50048]	Loss: 1.9809
Profiling... [1024/50048]	Loss: 2.1147
Profiling... [1152/50048]	Loss: 1.9051
Profiling... [1280/50048]	Loss: 1.4729
Profiling... [1408/50048]	Loss: 1.8043
Profiling... [1536/50048]	Loss: 1.5124
Profiling... [1664/50048]	Loss: 1.7403
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 8, Average loss: 0.0142, Accuracy: 0.5046
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 123.62456188136089,
                        "time": 2.2153610149998713,
                        "accuracy": 0.5046479430379747,
                        "total_cost": 542.701181421039
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7982
Profiling... [256/50048]	Loss: 1.8035
Profiling... [384/50048]	Loss: 1.7479
Profiling... [512/50048]	Loss: 1.9713
Profiling... [640/50048]	Loss: 1.5226
Profiling... [768/50048]	Loss: 1.7508
Profiling... [896/50048]	Loss: 1.8439
Profiling... [1024/50048]	Loss: 1.6452
Profiling... [1152/50048]	Loss: 1.6427
Profiling... [1280/50048]	Loss: 1.6748
Profiling... [1408/50048]	Loss: 1.8232
Profiling... [1536/50048]	Loss: 1.6637
Profiling... [1664/50048]	Loss: 1.6768
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 8, Average loss: 0.0142, Accuracy: 0.5079
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 123.6348354521244,
                        "time": 2.531637416000194,
                        "accuracy": 0.5079113924050633,
                        "total_cost": 616.2464162686208
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7649
Profiling... [256/50048]	Loss: 1.6623
Profiling... [384/50048]	Loss: 1.9165
Profiling... [512/50048]	Loss: 1.8914
Profiling... [640/50048]	Loss: 1.8268
Profiling... [768/50048]	Loss: 1.6410
Profiling... [896/50048]	Loss: 1.7037
Profiling... [1024/50048]	Loss: 1.6904
Profiling... [1152/50048]	Loss: 1.6655
Profiling... [1280/50048]	Loss: 1.8343
Profiling... [1408/50048]	Loss: 1.5203
Profiling... [1536/50048]	Loss: 1.8363
Profiling... [1664/50048]	Loss: 1.6631
Profile done
epoch 1 train time consumed: 7.57s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5035
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 123.5968556035434,
                        "time": 5.433141086999967,
                        "accuracy": 0.5034612341772152,
                        "total_cost": 1333.8050853131688
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6192
Profiling... [256/50048]	Loss: 1.3786
Profiling... [384/50048]	Loss: 1.6257
Profiling... [512/50048]	Loss: 1.7222
Profiling... [640/50048]	Loss: 1.7814
Profiling... [768/50048]	Loss: 1.7739
Profiling... [896/50048]	Loss: 1.4654
Profiling... [1024/50048]	Loss: 1.3296
Profiling... [1152/50048]	Loss: 1.8545
Profiling... [1280/50048]	Loss: 1.8592
Profiling... [1408/50048]	Loss: 1.7588
Profiling... [1536/50048]	Loss: 1.6263
Profiling... [1664/50048]	Loss: 1.8755
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5039
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 123.5549804782155,
                        "time": 2.163472338000247,
                        "accuracy": 0.5038568037974683,
                        "total_cost": 530.5233162917207
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7760
Profiling... [256/50048]	Loss: 1.6827
Profiling... [384/50048]	Loss: 1.9008
Profiling... [512/50048]	Loss: 1.3713
Profiling... [640/50048]	Loss: 1.9066
Profiling... [768/50048]	Loss: 1.7081
Profiling... [896/50048]	Loss: 1.7509
Profiling... [1024/50048]	Loss: 1.4490
Profiling... [1152/50048]	Loss: 1.7516
Profiling... [1280/50048]	Loss: 1.8078
Profiling... [1408/50048]	Loss: 1.9218
Profiling... [1536/50048]	Loss: 1.6607
Profiling... [1664/50048]	Loss: 1.6401
Profile done
epoch 1 train time consumed: 3.48s
Validation Epoch: 8, Average loss: 0.0141, Accuracy: 0.5065
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 123.57879283444937,
                        "time": 2.208933036000417,
                        "accuracy": 0.5065268987341772,
                        "total_cost": 538.9196086589745
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7017
Profiling... [256/50048]	Loss: 1.5789
Profiling... [384/50048]	Loss: 1.8065
Profiling... [512/50048]	Loss: 1.8288
Profiling... [640/50048]	Loss: 1.6779
Profiling... [768/50048]	Loss: 1.8067
Profiling... [896/50048]	Loss: 1.8841
Profiling... [1024/50048]	Loss: 1.6049
Profiling... [1152/50048]	Loss: 1.7134
Profiling... [1280/50048]	Loss: 1.7834
Profiling... [1408/50048]	Loss: 1.7458
Profiling... [1536/50048]	Loss: 1.6151
Profiling... [1664/50048]	Loss: 1.9423
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 8, Average loss: 0.0142, Accuracy: 0.5021
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 123.58855243495226,
                        "time": 2.501010848000078,
                        "accuracy": 0.5020767405063291,
                        "total_cost": 615.6355899234206
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8339
Profiling... [256/50048]	Loss: 1.6805
Profiling... [384/50048]	Loss: 1.5717
Profiling... [512/50048]	Loss: 2.0326
Profiling... [640/50048]	Loss: 1.7499
Profiling... [768/50048]	Loss: 1.7008
Profiling... [896/50048]	Loss: 1.5782
Profiling... [1024/50048]	Loss: 1.6180
Profiling... [1152/50048]	Loss: 1.5643
Profiling... [1280/50048]	Loss: 1.8147
Profiling... [1408/50048]	Loss: 1.7719
Profiling... [1536/50048]	Loss: 1.5772
Profiling... [1664/50048]	Loss: 1.5791
Profile done
epoch 1 train time consumed: 7.47s
Validation Epoch: 8, Average loss: 0.0144, Accuracy: 0.5021
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 123.55074213955619,
                        "time": 5.470778946999417,
                        "accuracy": 0.5020767405063291,
                        "total_cost": 1346.2459908052983
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6165
Profiling... [256/50048]	Loss: 1.4531
Profiling... [384/50048]	Loss: 1.8946
Profiling... [512/50048]	Loss: 1.7445
Profiling... [640/50048]	Loss: 1.6796
Profiling... [768/50048]	Loss: 1.8755
Profiling... [896/50048]	Loss: 1.8556
Profiling... [1024/50048]	Loss: 1.9279
Profiling... [1152/50048]	Loss: 1.4829
Profiling... [1280/50048]	Loss: 1.7336
Profiling... [1408/50048]	Loss: 1.9136
Profiling... [1536/50048]	Loss: 1.5648
Profiling... [1664/50048]	Loss: 1.8079
Profile done
epoch 1 train time consumed: 3.46s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.4995
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.50908289109303,
                        "time": 2.1688836229996014,
                        "accuracy": 0.49950553797468356,
                        "total_cost": 536.2839984924626
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6792
Profiling... [256/50048]	Loss: 1.7568
Profiling... [384/50048]	Loss: 1.8226
Profiling... [512/50048]	Loss: 1.5872
Profiling... [640/50048]	Loss: 1.9544
Profiling... [768/50048]	Loss: 1.6389
Profiling... [896/50048]	Loss: 1.6195
Profiling... [1024/50048]	Loss: 1.5253
Profiling... [1152/50048]	Loss: 1.9327
Profiling... [1280/50048]	Loss: 1.9358
Profiling... [1408/50048]	Loss: 1.5101
Profiling... [1536/50048]	Loss: 1.8327
Profiling... [1664/50048]	Loss: 1.4564
Profile done
epoch 1 train time consumed: 3.53s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5027
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.5316136928,
                        "time": 2.2161138779993053,
                        "accuracy": 0.5026700949367089,
                        "total_cost": 544.6119159142183
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6869
Profiling... [256/50048]	Loss: 1.6323
Profiling... [384/50048]	Loss: 1.6408
Profiling... [512/50048]	Loss: 1.7389
Profiling... [640/50048]	Loss: 1.9503
Profiling... [768/50048]	Loss: 1.8981
Profiling... [896/50048]	Loss: 1.9065
Profiling... [1024/50048]	Loss: 1.5732
Profiling... [1152/50048]	Loss: 1.5826
Profiling... [1280/50048]	Loss: 1.6666
Profiling... [1408/50048]	Loss: 1.6754
Profiling... [1536/50048]	Loss: 1.7637
Profiling... [1664/50048]	Loss: 1.5543
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 8, Average loss: 0.0142, Accuracy: 0.5068
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.538936845844,
                        "time": 2.4919807529995524,
                        "accuracy": 0.5068235759493671,
                        "total_cost": 607.4237022009137
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7315
Profiling... [256/50048]	Loss: 1.6537
Profiling... [384/50048]	Loss: 1.9664
Profiling... [512/50048]	Loss: 1.6432
Profiling... [640/50048]	Loss: 1.7302
Profiling... [768/50048]	Loss: 1.6622
Profiling... [896/50048]	Loss: 1.6211
Profiling... [1024/50048]	Loss: 1.6210
Profiling... [1152/50048]	Loss: 1.8430
Profiling... [1280/50048]	Loss: 1.7764
Profiling... [1408/50048]	Loss: 1.6762
Profiling... [1536/50048]	Loss: 1.7526
Profiling... [1664/50048]	Loss: 1.6250
Profile done
epoch 1 train time consumed: 7.55s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5034
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.50382072953776,
                        "time": 5.516116357000101,
                        "accuracy": 0.5033623417721519,
                        "total_cost": 1353.4215596656334
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.9260
Profiling... [256/50048]	Loss: 1.7523
Profiling... [384/50048]	Loss: 1.7153
Profiling... [512/50048]	Loss: 1.9150
Profiling... [640/50048]	Loss: 1.8557
Profiling... [768/50048]	Loss: 2.0413
Profiling... [896/50048]	Loss: 1.5668
Profiling... [1024/50048]	Loss: 1.7027
Profiling... [1152/50048]	Loss: 1.6878
Profiling... [1280/50048]	Loss: 1.7580
Profiling... [1408/50048]	Loss: 1.8624
Profiling... [1536/50048]	Loss: 2.0041
Profiling... [1664/50048]	Loss: 1.7277
Profile done
epoch 1 train time consumed: 3.78s
Validation Epoch: 8, Average loss: 0.0167, Accuracy: 0.4465
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.46416514634662,
                        "time": 2.247611347000202,
                        "accuracy": 0.4464992088607595,
                        "total_cost": 621.5004484305224
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6434
Profiling... [256/50048]	Loss: 1.6511
Profiling... [384/50048]	Loss: 1.6480
Profiling... [512/50048]	Loss: 1.5938
Profiling... [640/50048]	Loss: 1.6967
Profiling... [768/50048]	Loss: 1.8603
Profiling... [896/50048]	Loss: 1.9797
Profiling... [1024/50048]	Loss: 1.8457
Profiling... [1152/50048]	Loss: 1.8297
Profiling... [1280/50048]	Loss: 1.9251
Profiling... [1408/50048]	Loss: 1.9651
Profiling... [1536/50048]	Loss: 1.7876
Profiling... [1664/50048]	Loss: 1.7061
Profile done
epoch 1 train time consumed: 3.51s
Validation Epoch: 8, Average loss: 0.0162, Accuracy: 0.4440
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.48568081122473,
                        "time": 2.187437845000204,
                        "accuracy": 0.4440268987341772,
                        "total_cost": 608.3353334947346
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8604
Profiling... [256/50048]	Loss: 1.6999
Profiling... [384/50048]	Loss: 1.9724
Profiling... [512/50048]	Loss: 1.9506
Profiling... [640/50048]	Loss: 1.7527
Profiling... [768/50048]	Loss: 1.8835
Profiling... [896/50048]	Loss: 1.8794
Profiling... [1024/50048]	Loss: 1.7997
Profiling... [1152/50048]	Loss: 1.7084
Profiling... [1280/50048]	Loss: 1.7837
Profiling... [1408/50048]	Loss: 1.9361
Profiling... [1536/50048]	Loss: 1.9606
Profiling... [1664/50048]	Loss: 1.7069
Profile done
epoch 1 train time consumed: 3.79s
Validation Epoch: 8, Average loss: 0.0183, Accuracy: 0.4147
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.49489441979372,
                        "time": 2.488338269000451,
                        "accuracy": 0.4146558544303797,
                        "total_cost": 741.089432423624
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8261
Profiling... [256/50048]	Loss: 1.6206
Profiling... [384/50048]	Loss: 1.5157
Profiling... [512/50048]	Loss: 1.9297
Profiling... [640/50048]	Loss: 1.7017
Profiling... [768/50048]	Loss: 1.9582
Profiling... [896/50048]	Loss: 2.0527
Profiling... [1024/50048]	Loss: 2.3418
Profiling... [1152/50048]	Loss: 1.8605
Profiling... [1280/50048]	Loss: 1.9567
Profiling... [1408/50048]	Loss: 1.9107
Profiling... [1536/50048]	Loss: 1.7262
Profiling... [1664/50048]	Loss: 1.9365
Profile done
epoch 1 train time consumed: 7.61s
Validation Epoch: 8, Average loss: 0.0165, Accuracy: 0.4420
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.45659006682097,
                        "time": 5.451704742999937,
                        "accuracy": 0.4419501582278481,
                        "total_cost": 1522.9067465904047
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6590
Profiling... [256/50048]	Loss: 1.5039
Profiling... [384/50048]	Loss: 2.0073
Profiling... [512/50048]	Loss: 1.6091
Profiling... [640/50048]	Loss: 1.4567
Profiling... [768/50048]	Loss: 1.7842
Profiling... [896/50048]	Loss: 1.9394
Profiling... [1024/50048]	Loss: 1.8958
Profiling... [1152/50048]	Loss: 2.1115
Profiling... [1280/50048]	Loss: 1.6273
Profiling... [1408/50048]	Loss: 1.9269
Profiling... [1536/50048]	Loss: 2.0463
Profiling... [1664/50048]	Loss: 1.8194
Profile done
epoch 1 train time consumed: 3.74s
Validation Epoch: 8, Average loss: 0.0162, Accuracy: 0.4535
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.41680956129203,
                        "time": 2.1557253900000433,
                        "accuracy": 0.45352056962025317,
                        "total_cost": 586.6387717471151
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6720
Profiling... [256/50048]	Loss: 1.6804
Profiling... [384/50048]	Loss: 1.6606
Profiling... [512/50048]	Loss: 1.8884
Profiling... [640/50048]	Loss: 1.7746
Profiling... [768/50048]	Loss: 1.8612
Profiling... [896/50048]	Loss: 1.8918
Profiling... [1024/50048]	Loss: 2.0051
Profiling... [1152/50048]	Loss: 1.8921
Profiling... [1280/50048]	Loss: 1.9205
Profiling... [1408/50048]	Loss: 1.5129
Profiling... [1536/50048]	Loss: 2.1742
Profiling... [1664/50048]	Loss: 1.8576
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 8, Average loss: 0.0172, Accuracy: 0.4273
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.43729595604766,
                        "time": 2.193457450999631,
                        "accuracy": 0.427314082278481,
                        "total_cost": 633.6193160364614
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7011
Profiling... [256/50048]	Loss: 1.8062
Profiling... [384/50048]	Loss: 1.9174
Profiling... [512/50048]	Loss: 1.8147
Profiling... [640/50048]	Loss: 1.7911
Profiling... [768/50048]	Loss: 1.6679
Profiling... [896/50048]	Loss: 1.9288
Profiling... [1024/50048]	Loss: 1.8890
Profiling... [1152/50048]	Loss: 1.7392
Profiling... [1280/50048]	Loss: 1.7861
Profiling... [1408/50048]	Loss: 1.9324
Profiling... [1536/50048]	Loss: 1.7569
Profiling... [1664/50048]	Loss: 2.0998
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 8, Average loss: 0.0157, Accuracy: 0.4609
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.44656239356905,
                        "time": 2.4734053319998566,
                        "accuracy": 0.4609375,
                        "total_cost": 662.4181925777501
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.9138
Profiling... [256/50048]	Loss: 1.7111
Profiling... [384/50048]	Loss: 1.5190
Profiling... [512/50048]	Loss: 1.9039
Profiling... [640/50048]	Loss: 1.8290
Profiling... [768/50048]	Loss: 1.6623
Profiling... [896/50048]	Loss: 1.8543
Profiling... [1024/50048]	Loss: 1.7982
Profiling... [1152/50048]	Loss: 1.8416
Profiling... [1280/50048]	Loss: 1.8126
Profiling... [1408/50048]	Loss: 1.7702
Profiling... [1536/50048]	Loss: 2.1366
Profiling... [1664/50048]	Loss: 1.8055
Profile done
epoch 1 train time consumed: 7.47s
Validation Epoch: 8, Average loss: 0.0167, Accuracy: 0.4424
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.40986056072677,
                        "time": 5.43116339900007,
                        "accuracy": 0.44244462025316456,
                        "total_cost": 1514.8994632810827
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7638
Profiling... [256/50048]	Loss: 1.7633
Profiling... [384/50048]	Loss: 1.9414
Profiling... [512/50048]	Loss: 1.5406
Profiling... [640/50048]	Loss: 1.6761
Profiling... [768/50048]	Loss: 1.9148
Profiling... [896/50048]	Loss: 1.6800
Profiling... [1024/50048]	Loss: 1.7657
Profiling... [1152/50048]	Loss: 1.9853
Profiling... [1280/50048]	Loss: 1.9578
Profiling... [1408/50048]	Loss: 1.9425
Profiling... [1536/50048]	Loss: 1.9380
Profiling... [1664/50048]	Loss: 1.9628
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 8, Average loss: 0.0162, Accuracy: 0.4454
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 123.36717050914997,
                        "time": 2.1529670200006876,
                        "accuracy": 0.4454113924050633,
                        "total_cost": 596.3148989585252
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8819
Profiling... [256/50048]	Loss: 1.8651
Profiling... [384/50048]	Loss: 1.3524
Profiling... [512/50048]	Loss: 1.8107
Profiling... [640/50048]	Loss: 1.8259
Profiling... [768/50048]	Loss: 1.7398
Profiling... [896/50048]	Loss: 1.6028
Profiling... [1024/50048]	Loss: 1.7203
Profiling... [1152/50048]	Loss: 2.0400
Profiling... [1280/50048]	Loss: 2.1226
Profiling... [1408/50048]	Loss: 1.9245
Profiling... [1536/50048]	Loss: 1.5959
Profiling... [1664/50048]	Loss: 1.7981
Profile done
epoch 1 train time consumed: 3.47s
Validation Epoch: 8, Average loss: 0.0175, Accuracy: 0.4167
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 123.38912636310273,
                        "time": 2.1927464760001385,
                        "accuracy": 0.41673259493670883,
                        "total_cost": 649.2438443662434
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 2.0349
Profiling... [256/50048]	Loss: 1.9992
Profiling... [384/50048]	Loss: 1.9390
Profiling... [512/50048]	Loss: 1.5590
Profiling... [640/50048]	Loss: 1.8488
Profiling... [768/50048]	Loss: 1.8169
Profiling... [896/50048]	Loss: 1.6818
Profiling... [1024/50048]	Loss: 1.8873
Profiling... [1152/50048]	Loss: 1.9852
Profiling... [1280/50048]	Loss: 1.9908
Profiling... [1408/50048]	Loss: 2.0536
Profiling... [1536/50048]	Loss: 1.9155
Profiling... [1664/50048]	Loss: 1.9602
Profile done
epoch 1 train time consumed: 3.89s
Validation Epoch: 8, Average loss: 0.0196, Accuracy: 0.3831
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 123.39806406935995,
                        "time": 2.5081510660002095,
                        "accuracy": 0.3831091772151899,
                        "total_cost": 807.8662802798968
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6833
Profiling... [256/50048]	Loss: 1.4627
Profiling... [384/50048]	Loss: 1.9903
Profiling... [512/50048]	Loss: 2.0158
Profiling... [640/50048]	Loss: 1.7826
Profiling... [768/50048]	Loss: 1.7843
Profiling... [896/50048]	Loss: 1.8291
Profiling... [1024/50048]	Loss: 1.9383
Profiling... [1152/50048]	Loss: 1.9062
Profiling... [1280/50048]	Loss: 1.7311
Profiling... [1408/50048]	Loss: 1.8924
Profiling... [1536/50048]	Loss: 1.6420
Profiling... [1664/50048]	Loss: 1.8530
Profile done
epoch 1 train time consumed: 7.53s
Validation Epoch: 8, Average loss: 0.0174, Accuracy: 0.4310
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 123.36291232200736,
                        "time": 5.478155673999936,
                        "accuracy": 0.4309731012658228,
                        "total_cost": 1568.0821752286793
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5112
Profiling... [256/50048]	Loss: 1.9373
Profiling... [384/50048]	Loss: 1.6546
Profiling... [512/50048]	Loss: 1.7734
Profiling... [640/50048]	Loss: 2.2634
Profiling... [768/50048]	Loss: 1.9333
Profiling... [896/50048]	Loss: 1.9631
Profiling... [1024/50048]	Loss: 2.2147
Profiling... [1152/50048]	Loss: 2.0410
Profiling... [1280/50048]	Loss: 1.9316
Profiling... [1408/50048]	Loss: 2.3016
Profiling... [1536/50048]	Loss: 2.0066
Profiling... [1664/50048]	Loss: 2.0969
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 8, Average loss: 0.0304, Accuracy: 0.2385
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 123.31674402636911,
                        "time": 2.2204493750004985,
                        "accuracy": 0.23852848101265822,
                        "total_cost": 1147.949234565061
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8177
Profiling... [256/50048]	Loss: 1.5494
Profiling... [384/50048]	Loss: 1.6447
Profiling... [512/50048]	Loss: 2.0035
Profiling... [640/50048]	Loss: 2.0715
Profiling... [768/50048]	Loss: 2.0041
Profiling... [896/50048]	Loss: 2.0812
Profiling... [1024/50048]	Loss: 1.9910
Profiling... [1152/50048]	Loss: 1.8391
Profiling... [1280/50048]	Loss: 2.1357
Profiling... [1408/50048]	Loss: 2.0101
Profiling... [1536/50048]	Loss: 2.0066
Profiling... [1664/50048]	Loss: 1.8771
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 8, Average loss: 0.0219, Accuracy: 0.3484
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 123.33748954979532,
                        "time": 2.243166776999715,
                        "accuracy": 0.34839794303797467,
                        "total_cost": 794.1107702995087
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5873
Profiling... [256/50048]	Loss: 1.6019
Profiling... [384/50048]	Loss: 1.7035
Profiling... [512/50048]	Loss: 1.6943
Profiling... [640/50048]	Loss: 1.9794
Profiling... [768/50048]	Loss: 1.9712
Profiling... [896/50048]	Loss: 2.0231
Profiling... [1024/50048]	Loss: 2.0301
Profiling... [1152/50048]	Loss: 2.0470
Profiling... [1280/50048]	Loss: 2.0004
Profiling... [1408/50048]	Loss: 1.8824
Profiling... [1536/50048]	Loss: 2.2224
Profiling... [1664/50048]	Loss: 2.0480
Profile done
epoch 1 train time consumed: 3.82s
Validation Epoch: 8, Average loss: 0.0288, Accuracy: 0.2631
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 123.34549517631501,
                        "time": 2.5117288359997474,
                        "accuracy": 0.26305379746835444,
                        "total_cost": 1177.7455410514976
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5278
Profiling... [256/50048]	Loss: 2.0411
Profiling... [384/50048]	Loss: 1.6592
Profiling... [512/50048]	Loss: 1.8295
Profiling... [640/50048]	Loss: 2.3656
Profiling... [768/50048]	Loss: 2.2543
Profiling... [896/50048]	Loss: 2.0875
Profiling... [1024/50048]	Loss: 2.1279
Profiling... [1152/50048]	Loss: 1.8400
Profiling... [1280/50048]	Loss: 2.0644
Profiling... [1408/50048]	Loss: 2.0093
Profiling... [1536/50048]	Loss: 1.8912
Profiling... [1664/50048]	Loss: 2.1177
Profile done
epoch 1 train time consumed: 7.57s
Validation Epoch: 8, Average loss: 0.0231, Accuracy: 0.2767
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 123.30940091883664,
                        "time": 5.477133377999962,
                        "accuracy": 0.2767009493670886,
                        "total_cost": 2440.837435283736
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6284
Profiling... [256/50048]	Loss: 1.8245
Profiling... [384/50048]	Loss: 1.8047
Profiling... [512/50048]	Loss: 2.0722
Profiling... [640/50048]	Loss: 2.0642
Profiling... [768/50048]	Loss: 1.8124
Profiling... [896/50048]	Loss: 2.0052
Profiling... [1024/50048]	Loss: 2.0789
Profiling... [1152/50048]	Loss: 2.3352
Profiling... [1280/50048]	Loss: 2.0701
Profiling... [1408/50048]	Loss: 2.2616
Profiling... [1536/50048]	Loss: 1.8660
Profiling... [1664/50048]	Loss: 1.7561
Profile done
epoch 1 train time consumed: 3.58s
Validation Epoch: 8, Average loss: 0.0240, Accuracy: 0.3055
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 123.26999365512785,
                        "time": 2.215688047000185,
                        "accuracy": 0.30547863924050633,
                        "total_cost": 894.0980363619441
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7512
Profiling... [256/50048]	Loss: 1.9322
Profiling... [384/50048]	Loss: 1.6449
Profiling... [512/50048]	Loss: 1.9381
Profiling... [640/50048]	Loss: 2.2029
Profiling... [768/50048]	Loss: 2.0182
Profiling... [896/50048]	Loss: 2.2918
Profiling... [1024/50048]	Loss: 2.1403
Profiling... [1152/50048]	Loss: 1.9267
Profiling... [1280/50048]	Loss: 1.9047
Profiling... [1408/50048]	Loss: 2.0084
Profiling... [1536/50048]	Loss: 1.9830
Profiling... [1664/50048]	Loss: 2.0906
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 8, Average loss: 0.0208, Accuracy: 0.3483
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 123.29079129168831,
                        "time": 2.234571039000002,
                        "accuracy": 0.3482990506329114,
                        "total_cost": 790.9927721455801
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8400
Profiling... [256/50048]	Loss: 1.9696
Profiling... [384/50048]	Loss: 1.9443
Profiling... [512/50048]	Loss: 1.8635
Profiling... [640/50048]	Loss: 1.8305
Profiling... [768/50048]	Loss: 1.8074
Profiling... [896/50048]	Loss: 1.9063
Profiling... [1024/50048]	Loss: 1.6911
Profiling... [1152/50048]	Loss: 2.1039
Profiling... [1280/50048]	Loss: 2.0113
Profiling... [1408/50048]	Loss: 2.1359
Profiling... [1536/50048]	Loss: 2.0153
Profiling... [1664/50048]	Loss: 2.0475
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 8, Average loss: 0.0258, Accuracy: 0.2952
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 123.29843206065917,
                        "time": 2.519364036999832,
                        "accuracy": 0.29519382911392406,
                        "total_cost": 1052.3039607044395
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7177
Profiling... [256/50048]	Loss: 1.8101
Profiling... [384/50048]	Loss: 2.1103
Profiling... [512/50048]	Loss: 1.9969
Profiling... [640/50048]	Loss: 2.0738
Profiling... [768/50048]	Loss: 1.8543
Profiling... [896/50048]	Loss: 2.0945
Profiling... [1024/50048]	Loss: 1.6840
Profiling... [1152/50048]	Loss: 2.2459
Profiling... [1280/50048]	Loss: 2.1025
Profiling... [1408/50048]	Loss: 1.9959
Profiling... [1536/50048]	Loss: 2.0697
Profiling... [1664/50048]	Loss: 1.9115
Profile done
epoch 1 train time consumed: 7.53s
Validation Epoch: 8, Average loss: 0.0257, Accuracy: 0.2763
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 123.26376442269857,
                        "time": 5.465607253000599,
                        "accuracy": 0.27630537974683544,
                        "total_cost": 2438.2852244069436
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7109
Profiling... [256/50048]	Loss: 1.7108
Profiling... [384/50048]	Loss: 2.0407
Profiling... [512/50048]	Loss: 1.8411
Profiling... [640/50048]	Loss: 2.1347
Profiling... [768/50048]	Loss: 1.8439
Profiling... [896/50048]	Loss: 2.0634
Profiling... [1024/50048]	Loss: 2.3027
Profiling... [1152/50048]	Loss: 1.9467
Profiling... [1280/50048]	Loss: 2.1288
Profiling... [1408/50048]	Loss: 1.7404
Profiling... [1536/50048]	Loss: 1.9725
Profiling... [1664/50048]	Loss: 2.2762
Profile done
epoch 1 train time consumed: 3.57s
Validation Epoch: 8, Average loss: 0.0244, Accuracy: 0.2559
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 123.21915259065433,
                        "time": 2.2159778989998813,
                        "accuracy": 0.25593354430379744,
                        "total_cost": 1066.8821065138218
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8788
Profiling... [256/50048]	Loss: 1.9551
Profiling... [384/50048]	Loss: 1.8745
Profiling... [512/50048]	Loss: 1.6171
Profiling... [640/50048]	Loss: 2.0467
Profiling... [768/50048]	Loss: 2.0732
Profiling... [896/50048]	Loss: 2.0059
Profiling... [1024/50048]	Loss: 2.2662
Profiling... [1152/50048]	Loss: 1.9158
Profiling... [1280/50048]	Loss: 1.8194
Profiling... [1408/50048]	Loss: 1.8966
Profiling... [1536/50048]	Loss: 1.9527
Profiling... [1664/50048]	Loss: 1.9560
Profile done
epoch 1 train time consumed: 3.50s
Validation Epoch: 8, Average loss: 0.0217, Accuracy: 0.3482
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 123.24054815696498,
                        "time": 2.1938444630004597,
                        "accuracy": 0.3482001582278481,
                        "total_cost": 776.480388657318
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7189
Profiling... [256/50048]	Loss: 1.7268
Profiling... [384/50048]	Loss: 1.7962
Profiling... [512/50048]	Loss: 1.9135
Profiling... [640/50048]	Loss: 1.9967
Profiling... [768/50048]	Loss: 2.1403
Profiling... [896/50048]	Loss: 2.1703
Profiling... [1024/50048]	Loss: 1.9419
Profiling... [1152/50048]	Loss: 1.8550
Profiling... [1280/50048]	Loss: 2.3750
Profiling... [1408/50048]	Loss: 2.0971
Profiling... [1536/50048]	Loss: 1.8568
Profiling... [1664/50048]	Loss: 2.2314
Profile done
epoch 1 train time consumed: 3.77s
Validation Epoch: 8, Average loss: 0.0196, Accuracy: 0.3631
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 123.24888066933758,
                        "time": 2.495363745000759,
                        "accuracy": 0.36313291139240506,
                        "total_cost": 846.9372474527582
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8215
Profiling... [256/50048]	Loss: 1.9270
Profiling... [384/50048]	Loss: 2.0631
Profiling... [512/50048]	Loss: 1.7212
Profiling... [640/50048]	Loss: 2.1781
Profiling... [768/50048]	Loss: 1.9080
Profiling... [896/50048]	Loss: 2.3284
Profiling... [1024/50048]	Loss: 2.0338
Profiling... [1152/50048]	Loss: 2.0124
Profiling... [1280/50048]	Loss: 2.1536
Profiling... [1408/50048]	Loss: 2.0629
Profiling... [1536/50048]	Loss: 1.9592
Profiling... [1664/50048]	Loss: 2.0151
Profile done
epoch 1 train time consumed: 7.66s
Validation Epoch: 8, Average loss: 0.0238, Accuracy: 0.3109
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 123.21360116083589,
                        "time": 5.559977531000186,
                        "accuracy": 0.31091772151898733,
                        "total_cost": 2203.3638051282
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7342
Profiling... [512/50176]	Loss: 1.7007
Profiling... [768/50176]	Loss: 1.9188
Profiling... [1024/50176]	Loss: 1.7490
Profiling... [1280/50176]	Loss: 1.9235
Profiling... [1536/50176]	Loss: 1.6952
Profiling... [1792/50176]	Loss: 1.7793
Profiling... [2048/50176]	Loss: 1.8015
Profiling... [2304/50176]	Loss: 1.8292
Profiling... [2560/50176]	Loss: 1.6079
Profiling... [2816/50176]	Loss: 1.6291
Profiling... [3072/50176]	Loss: 1.7717
Profiling... [3328/50176]	Loss: 1.6391
Profile done
epoch 1 train time consumed: 4.05s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4979
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 123.18148960816542,
                        "time": 2.4193882899999153,
                        "accuracy": 0.49794921875,
                        "total_cost": 598.5025024255884
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7109
Profiling... [512/50176]	Loss: 1.8349
Profiling... [768/50176]	Loss: 1.5944
Profiling... [1024/50176]	Loss: 1.5290
Profiling... [1280/50176]	Loss: 1.5843
Profiling... [1536/50176]	Loss: 1.7479
Profiling... [1792/50176]	Loss: 1.8160
Profiling... [2048/50176]	Loss: 1.6204
Profiling... [2304/50176]	Loss: 1.6490
Profiling... [2560/50176]	Loss: 1.5491
Profiling... [2816/50176]	Loss: 1.7445
Profiling... [3072/50176]	Loss: 1.6505
Profiling... [3328/50176]	Loss: 1.5744
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4968
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 123.20550768893179,
                        "time": 2.4822460649993445,
                        "accuracy": 0.49677734375,
                        "total_cost": 615.6206407049888
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.8845
Profiling... [512/50176]	Loss: 1.8001
Profiling... [768/50176]	Loss: 1.9133
Profiling... [1024/50176]	Loss: 1.7039
Profiling... [1280/50176]	Loss: 1.4917
Profiling... [1536/50176]	Loss: 1.7938
Profiling... [1792/50176]	Loss: 1.8183
Profiling... [2048/50176]	Loss: 1.7016
Profiling... [2304/50176]	Loss: 1.6823
Profiling... [2560/50176]	Loss: 1.8441
Profiling... [2816/50176]	Loss: 1.6160
Profiling... [3072/50176]	Loss: 1.8636
Profiling... [3328/50176]	Loss: 1.6118
Profile done
epoch 1 train time consumed: 4.40s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.5004
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 123.21159860170069,
                        "time": 2.871484937000787,
                        "accuracy": 0.500390625,
                        "total_cost": 707.0481175553015
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8634
Profiling... [512/50176]	Loss: 1.6921
Profiling... [768/50176]	Loss: 1.7883
Profiling... [1024/50176]	Loss: 1.9511
Profiling... [1280/50176]	Loss: 1.7076
Profiling... [1536/50176]	Loss: 1.6933
Profiling... [1792/50176]	Loss: 1.6965
Profiling... [2048/50176]	Loss: 1.5915
Profiling... [2304/50176]	Loss: 1.8079
Profiling... [2560/50176]	Loss: 1.8828
Profiling... [2816/50176]	Loss: 1.6416
Profiling... [3072/50176]	Loss: 1.7100
Profiling... [3328/50176]	Loss: 1.4862
Profile done
epoch 1 train time consumed: 10.35s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4964
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 123.1652672941299,
                        "time": 7.288960803000009,
                        "accuracy": 0.49638671875,
                        "total_cost": 1808.5633069688809
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7123
Profiling... [512/50176]	Loss: 1.6848
Profiling... [768/50176]	Loss: 1.5784
Profiling... [1024/50176]	Loss: 1.4780
Profiling... [1280/50176]	Loss: 1.8895
Profiling... [1536/50176]	Loss: 1.7920
Profiling... [1792/50176]	Loss: 1.6685
Profiling... [2048/50176]	Loss: 1.7280
Profiling... [2304/50176]	Loss: 1.7321
Profiling... [2560/50176]	Loss: 1.6631
Profiling... [2816/50176]	Loss: 1.7649
Profiling... [3072/50176]	Loss: 1.6449
Profiling... [3328/50176]	Loss: 1.5888
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4997
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 123.13171313886413,
                        "time": 2.4315895059999093,
                        "accuracy": 0.49970703125,
                        "total_cost": 599.1626349048959
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7338
Profiling... [512/50176]	Loss: 1.7317
Profiling... [768/50176]	Loss: 1.6667
Profiling... [1024/50176]	Loss: 1.7224
Profiling... [1280/50176]	Loss: 1.6034
Profiling... [1536/50176]	Loss: 1.7059
Profiling... [1792/50176]	Loss: 1.7144
Profiling... [2048/50176]	Loss: 1.8064
Profiling... [2304/50176]	Loss: 1.7561
Profiling... [2560/50176]	Loss: 1.9427
Profiling... [2816/50176]	Loss: 1.9077
Profiling... [3072/50176]	Loss: 1.8263
Profiling... [3328/50176]	Loss: 1.7335
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4988
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 123.15356500935903,
                        "time": 2.5008461730003546,
                        "accuracy": 0.498828125,
                        "total_cost": 617.423329418336
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7488
Profiling... [512/50176]	Loss: 1.5379
Profiling... [768/50176]	Loss: 1.5908
Profiling... [1024/50176]	Loss: 1.7716
Profiling... [1280/50176]	Loss: 1.5576
Profiling... [1536/50176]	Loss: 1.6588
Profiling... [1792/50176]	Loss: 1.7232
Profiling... [2048/50176]	Loss: 1.5252
Profiling... [2304/50176]	Loss: 1.7093
Profiling... [2560/50176]	Loss: 1.6560
Profiling... [2816/50176]	Loss: 1.5833
Profiling... [3072/50176]	Loss: 1.6524
Profiling... [3328/50176]	Loss: 1.7347
Profile done
epoch 1 train time consumed: 4.42s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4938
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 123.16072413801774,
                        "time": 2.8652350460006346,
                        "accuracy": 0.49384765625,
                        "total_cost": 714.5612996742145
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8246
Profiling... [512/50176]	Loss: 1.8208
Profiling... [768/50176]	Loss: 1.6546
Profiling... [1024/50176]	Loss: 1.5911
Profiling... [1280/50176]	Loss: 1.4924
Profiling... [1536/50176]	Loss: 1.5372
Profiling... [1792/50176]	Loss: 1.6976
Profiling... [2048/50176]	Loss: 1.8662
Profiling... [2304/50176]	Loss: 1.7024
Profiling... [2560/50176]	Loss: 1.9416
Profiling... [2816/50176]	Loss: 1.5828
Profiling... [3072/50176]	Loss: 1.9734
Profiling... [3328/50176]	Loss: 1.6490
Profile done
epoch 1 train time consumed: 10.53s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4969
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 123.11343022394881,
                        "time": 7.446148630000607,
                        "accuracy": 0.496875,
                        "total_cost": 1844.9728800940513
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5403
Profiling... [512/50176]	Loss: 1.8031
Profiling... [768/50176]	Loss: 1.6971
Profiling... [1024/50176]	Loss: 1.6357
Profiling... [1280/50176]	Loss: 1.7936
Profiling... [1536/50176]	Loss: 1.7934
Profiling... [1792/50176]	Loss: 1.8627
Profiling... [2048/50176]	Loss: 1.5536
Profiling... [2304/50176]	Loss: 1.6546
Profiling... [2560/50176]	Loss: 1.6745
Profiling... [2816/50176]	Loss: 1.6390
Profiling... [3072/50176]	Loss: 1.6600
Profiling... [3328/50176]	Loss: 1.6022
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 8, Average loss: 0.0070, Accuracy: 0.4996
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.0838808250692,
                        "time": 2.4088021830002617,
                        "accuracy": 0.499609375,
                        "total_cost": 593.4330612262245
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7001
Profiling... [512/50176]	Loss: 1.8171
Profiling... [768/50176]	Loss: 1.8105
Profiling... [1024/50176]	Loss: 1.6623
Profiling... [1280/50176]	Loss: 1.5839
Profiling... [1536/50176]	Loss: 1.6214
Profiling... [1792/50176]	Loss: 1.6564
Profiling... [2048/50176]	Loss: 1.5635
Profiling... [2304/50176]	Loss: 1.7305
Profiling... [2560/50176]	Loss: 1.6198
Profiling... [2816/50176]	Loss: 1.7715
Profiling... [3072/50176]	Loss: 1.5667
Profiling... [3328/50176]	Loss: 1.5409
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4959
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.10751241746902,
                        "time": 2.4794642520000707,
                        "accuracy": 0.4958984375,
                        "total_cost": 615.5306270586287
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6909
Profiling... [512/50176]	Loss: 1.6479
Profiling... [768/50176]	Loss: 1.5697
Profiling... [1024/50176]	Loss: 1.6902
Profiling... [1280/50176]	Loss: 1.7279
Profiling... [1536/50176]	Loss: 1.7487
Profiling... [1792/50176]	Loss: 1.6326
Profiling... [2048/50176]	Loss: 1.6928
Profiling... [2304/50176]	Loss: 1.7804
Profiling... [2560/50176]	Loss: 1.7347
Profiling... [2816/50176]	Loss: 1.7430
Profiling... [3072/50176]	Loss: 1.7461
Profiling... [3328/50176]	Loss: 1.7928
Profile done
epoch 1 train time consumed: 4.44s
Validation Epoch: 8, Average loss: 0.0070, Accuracy: 0.4991
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.11420066153157,
                        "time": 2.863672430999941,
                        "accuracy": 0.49912109375,
                        "total_cost": 706.3591315089008
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.8143
Profiling... [512/50176]	Loss: 1.5081
Profiling... [768/50176]	Loss: 1.6556
Profiling... [1024/50176]	Loss: 1.5628
Profiling... [1280/50176]	Loss: 1.6997
Profiling... [1536/50176]	Loss: 1.7795
Profiling... [1792/50176]	Loss: 1.5959
Profiling... [2048/50176]	Loss: 1.4455
Profiling... [2304/50176]	Loss: 1.6323
Profiling... [2560/50176]	Loss: 1.6683
Profiling... [2816/50176]	Loss: 1.8603
Profiling... [3072/50176]	Loss: 1.6983
Profiling... [3328/50176]	Loss: 1.7820
Profile done
epoch 1 train time consumed: 9.85s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4992
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 123.0749526818654,
                        "time": 7.186512303999734,
                        "accuracy": 0.49921875,
                        "total_cost": 1771.727647974782
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.9251
Profiling... [512/50176]	Loss: 1.7723
Profiling... [768/50176]	Loss: 1.6804
Profiling... [1024/50176]	Loss: 1.7302
Profiling... [1280/50176]	Loss: 1.8034
Profiling... [1536/50176]	Loss: 1.5595
Profiling... [1792/50176]	Loss: 1.6729
Profiling... [2048/50176]	Loss: 1.6544
Profiling... [2304/50176]	Loss: 1.8748
Profiling... [2560/50176]	Loss: 1.7365
Profiling... [2816/50176]	Loss: 1.9408
Profiling... [3072/50176]	Loss: 1.6020
Profiling... [3328/50176]	Loss: 1.6827
Profile done
epoch 1 train time consumed: 4.27s
Validation Epoch: 8, Average loss: 0.0083, Accuracy: 0.4469
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.04694411893102,
                        "time": 2.439558828999907,
                        "accuracy": 0.446875,
                        "total_cost": 671.7320479033204
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8138
Profiling... [512/50176]	Loss: 1.7518
Profiling... [768/50176]	Loss: 1.8277
Profiling... [1024/50176]	Loss: 1.6223
Profiling... [1280/50176]	Loss: 1.6732
Profiling... [1536/50176]	Loss: 1.7357
Profiling... [1792/50176]	Loss: 2.1008
Profiling... [2048/50176]	Loss: 1.8751
Profiling... [2304/50176]	Loss: 1.6871
Profiling... [2560/50176]	Loss: 1.6869
Profiling... [2816/50176]	Loss: 2.0093
Profiling... [3072/50176]	Loss: 2.0291
Profiling... [3328/50176]	Loss: 1.7166
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 8, Average loss: 0.0086, Accuracy: 0.4252
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.06770608475196,
                        "time": 2.4960179709996737,
                        "accuracy": 0.4251953125,
                        "total_cost": 722.4425975703738
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6458
Profiling... [512/50176]	Loss: 1.8911
Profiling... [768/50176]	Loss: 1.5491
Profiling... [1024/50176]	Loss: 1.9371
Profiling... [1280/50176]	Loss: 1.8054
Profiling... [1536/50176]	Loss: 1.7485
Profiling... [1792/50176]	Loss: 1.8597
Profiling... [2048/50176]	Loss: 1.7987
Profiling... [2304/50176]	Loss: 1.9718
Profiling... [2560/50176]	Loss: 1.8626
Profiling... [2816/50176]	Loss: 1.8467
Profiling... [3072/50176]	Loss: 1.6350
Profiling... [3328/50176]	Loss: 1.7164
Profile done
epoch 1 train time consumed: 4.39s
Validation Epoch: 8, Average loss: 0.0087, Accuracy: 0.4302
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.07467106096598,
                        "time": 2.868533856000795,
                        "accuracy": 0.43017578125,
                        "total_cost": 820.6967387347361
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6988
Profiling... [512/50176]	Loss: 1.6561
Profiling... [768/50176]	Loss: 1.8774
Profiling... [1024/50176]	Loss: 1.6590
Profiling... [1280/50176]	Loss: 1.7486
Profiling... [1536/50176]	Loss: 1.6746
Profiling... [1792/50176]	Loss: 1.8839
Profiling... [2048/50176]	Loss: 1.6994
Profiling... [2304/50176]	Loss: 2.1097
Profiling... [2560/50176]	Loss: 2.1159
Profiling... [2816/50176]	Loss: 1.8454
Profiling... [3072/50176]	Loss: 1.8159
Profiling... [3328/50176]	Loss: 1.8149
Profile done
epoch 1 train time consumed: 10.15s
Validation Epoch: 8, Average loss: 0.0078, Accuracy: 0.4585
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 123.03367424091444,
                        "time": 7.107946760000232,
                        "accuracy": 0.45849609375,
                        "total_cost": 1907.359338743836
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7665
Profiling... [512/50176]	Loss: 1.8405
Profiling... [768/50176]	Loss: 1.7644
Profiling... [1024/50176]	Loss: 1.8258
Profiling... [1280/50176]	Loss: 1.6787
Profiling... [1536/50176]	Loss: 1.7484
Profiling... [1792/50176]	Loss: 1.8654
Profiling... [2048/50176]	Loss: 1.5363
Profiling... [2304/50176]	Loss: 1.6914
Profiling... [2560/50176]	Loss: 1.7899
Profiling... [2816/50176]	Loss: 1.8972
Profiling... [3072/50176]	Loss: 1.5914
Profiling... [3328/50176]	Loss: 1.7018
Profile done
epoch 1 train time consumed: 3.88s
Validation Epoch: 8, Average loss: 0.0087, Accuracy: 0.4218
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.00199083475565,
                        "time": 2.4136045039995224,
                        "accuracy": 0.42177734375,
                        "total_cost": 703.8741257179588
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7784
Profiling... [512/50176]	Loss: 1.9816
Profiling... [768/50176]	Loss: 1.6987
Profiling... [1024/50176]	Loss: 1.6953
Profiling... [1280/50176]	Loss: 1.8114
Profiling... [1536/50176]	Loss: 1.9417
Profiling... [1792/50176]	Loss: 1.7730
Profiling... [2048/50176]	Loss: 1.7925
Profiling... [2304/50176]	Loss: 1.9328
Profiling... [2560/50176]	Loss: 1.7807
Profiling... [2816/50176]	Loss: 1.7771
Profiling... [3072/50176]	Loss: 1.6771
Profiling... [3328/50176]	Loss: 1.8870
Profile done
epoch 1 train time consumed: 3.91s
Validation Epoch: 8, Average loss: 0.0093, Accuracy: 0.4136
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.0255772651703,
                        "time": 2.4923191979996773,
                        "accuracy": 0.41357421875,
                        "total_cost": 741.3881092242927
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7895
Profiling... [512/50176]	Loss: 1.7903
Profiling... [768/50176]	Loss: 1.8191
Profiling... [1024/50176]	Loss: 1.8398
Profiling... [1280/50176]	Loss: 1.9204
Profiling... [1536/50176]	Loss: 1.6526
Profiling... [1792/50176]	Loss: 1.6822
Profiling... [2048/50176]	Loss: 1.7890
Profiling... [2304/50176]	Loss: 1.6444
Profiling... [2560/50176]	Loss: 1.9686
Profiling... [2816/50176]	Loss: 1.9397
Profiling... [3072/50176]	Loss: 1.9602
Profiling... [3328/50176]	Loss: 1.8247
Profile done
epoch 1 train time consumed: 4.43s
Validation Epoch: 8, Average loss: 0.0080, Accuracy: 0.4512
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 123.03234134970013,
                        "time": 2.8669765709992134,
                        "accuracy": 0.451171875,
                        "total_cost": 781.8103469432087
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6744
Profiling... [512/50176]	Loss: 1.6934
Profiling... [768/50176]	Loss: 1.8423
Profiling... [1024/50176]	Loss: 1.6325
Profiling... [1280/50176]	Loss: 1.7426
Profiling... [1536/50176]	Loss: 1.6354
Profiling... [1792/50176]	Loss: 1.9762
Profiling... [2048/50176]	Loss: 1.7951
Profiling... [2304/50176]	Loss: 1.6111
Profiling... [2560/50176]	Loss: 1.8052
Profiling... [2816/50176]	Loss: 1.8688
Profiling... [3072/50176]	Loss: 1.7346
Profiling... [3328/50176]	Loss: 1.9068
Profile done
epoch 1 train time consumed: 10.21s
Validation Epoch: 8, Average loss: 0.0094, Accuracy: 0.3907
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.99048342373416,
                        "time": 7.443362087999958,
                        "accuracy": 0.39072265625,
                        "total_cost": 2342.998766151048
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.5344
Profiling... [512/50176]	Loss: 1.5921
Profiling... [768/50176]	Loss: 1.7032
Profiling... [1024/50176]	Loss: 1.8096
Profiling... [1280/50176]	Loss: 1.8672
Profiling... [1536/50176]	Loss: 1.8653
Profiling... [1792/50176]	Loss: 1.7617
Profiling... [2048/50176]	Loss: 1.7752
Profiling... [2304/50176]	Loss: 1.7675
Profiling... [2560/50176]	Loss: 1.9832
Profiling... [2816/50176]	Loss: 1.8598
Profiling... [3072/50176]	Loss: 1.7321
Profiling... [3328/50176]	Loss: 1.7436
Profile done
epoch 1 train time consumed: 4.00s
Validation Epoch: 8, Average loss: 0.0082, Accuracy: 0.4349
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.96073527615106,
                        "time": 2.436567935000312,
                        "accuracy": 0.43486328125,
                        "total_cost": 688.9571912733927
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8269
Profiling... [512/50176]	Loss: 1.7264
Profiling... [768/50176]	Loss: 1.6647
Profiling... [1024/50176]	Loss: 1.7003
Profiling... [1280/50176]	Loss: 1.6530
Profiling... [1536/50176]	Loss: 1.8535
Profiling... [1792/50176]	Loss: 1.8814
Profiling... [2048/50176]	Loss: 1.8552
Profiling... [2304/50176]	Loss: 1.5441
Profiling... [2560/50176]	Loss: 1.8855
Profiling... [2816/50176]	Loss: 1.7311
Profiling... [3072/50176]	Loss: 1.7620
Profiling... [3328/50176]	Loss: 1.6474
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 8, Average loss: 0.0080, Accuracy: 0.4577
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.98505937649922,
                        "time": 2.4876408959999026,
                        "accuracy": 0.45771484375,
                        "total_cost": 668.4132434845375
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7166
Profiling... [512/50176]	Loss: 1.6234
Profiling... [768/50176]	Loss: 1.6658
Profiling... [1024/50176]	Loss: 1.6165
Profiling... [1280/50176]	Loss: 1.8685
Profiling... [1536/50176]	Loss: 1.6702
Profiling... [1792/50176]	Loss: 1.7351
Profiling... [2048/50176]	Loss: 1.6105
Profiling... [2304/50176]	Loss: 1.7049
Profiling... [2560/50176]	Loss: 1.6740
Profiling... [2816/50176]	Loss: 1.8870
Profiling... [3072/50176]	Loss: 1.8700
Profiling... [3328/50176]	Loss: 1.6448
Profile done
epoch 1 train time consumed: 4.44s
Validation Epoch: 8, Average loss: 0.0086, Accuracy: 0.4210
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.99098844614359,
                        "time": 2.872742067999752,
                        "accuracy": 0.42099609375,
                        "total_cost": 839.2509853165546
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7740
Profiling... [512/50176]	Loss: 1.7423
Profiling... [768/50176]	Loss: 1.7811
Profiling... [1024/50176]	Loss: 1.8407
Profiling... [1280/50176]	Loss: 1.7523
Profiling... [1536/50176]	Loss: 1.6670
Profiling... [1792/50176]	Loss: 1.6726
Profiling... [2048/50176]	Loss: 1.8008
Profiling... [2304/50176]	Loss: 1.5816
Profiling... [2560/50176]	Loss: 1.9782
Profiling... [2816/50176]	Loss: 1.8307
Profiling... [3072/50176]	Loss: 1.8443
Profiling... [3328/50176]	Loss: 1.9161
Profile done
epoch 1 train time consumed: 9.79s
Validation Epoch: 8, Average loss: 0.0092, Accuracy: 0.4103
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.95281585111762,
                        "time": 7.056019124999693,
                        "accuracy": 0.41025390625,
                        "total_cost": 2114.6841185453095
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6517
Profiling... [512/50176]	Loss: 1.7622
Profiling... [768/50176]	Loss: 1.7498
Profiling... [1024/50176]	Loss: 1.9558
Profiling... [1280/50176]	Loss: 1.9624
Profiling... [1536/50176]	Loss: 1.9804
Profiling... [1792/50176]	Loss: 1.9875
Profiling... [2048/50176]	Loss: 1.8461
Profiling... [2304/50176]	Loss: 1.9183
Profiling... [2560/50176]	Loss: 2.0237
Profiling... [2816/50176]	Loss: 1.9917
Profiling... [3072/50176]	Loss: 1.7709
Profiling... [3328/50176]	Loss: 2.1233
Profile done
epoch 1 train time consumed: 4.23s
Validation Epoch: 8, Average loss: 0.0108, Accuracy: 0.3525
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.92670499937786,
                        "time": 2.4146814569994604,
                        "accuracy": 0.3525390625,
                        "total_cost": 841.9743135047356
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8350
Profiling... [512/50176]	Loss: 1.8227
Profiling... [768/50176]	Loss: 1.8419
Profiling... [1024/50176]	Loss: 1.8902
Profiling... [1280/50176]	Loss: 2.0180
Profiling... [1536/50176]	Loss: 2.0001
Profiling... [1792/50176]	Loss: 1.9233
Profiling... [2048/50176]	Loss: 2.0547
Profiling... [2304/50176]	Loss: 1.9144
Profiling... [2560/50176]	Loss: 1.8261
Profiling... [2816/50176]	Loss: 1.7464
Profiling... [3072/50176]	Loss: 2.0007
Profiling... [3328/50176]	Loss: 1.9461
Profile done
epoch 1 train time consumed: 3.96s
Validation Epoch: 8, Average loss: 0.0154, Accuracy: 0.2466
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.9488482189432,
                        "time": 2.490513509000266,
                        "accuracy": 0.24658203125,
                        "total_cost": 1241.8008151407073
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7163
Profiling... [512/50176]	Loss: 1.9342
Profiling... [768/50176]	Loss: 1.8354
Profiling... [1024/50176]	Loss: 1.8832
Profiling... [1280/50176]	Loss: 1.8526
Profiling... [1536/50176]	Loss: 1.9573
Profiling... [1792/50176]	Loss: 1.8088
Profiling... [2048/50176]	Loss: 2.0726
Profiling... [2304/50176]	Loss: 2.1207
Profiling... [2560/50176]	Loss: 2.0920
Profiling... [2816/50176]	Loss: 2.1774
Profiling... [3072/50176]	Loss: 1.9384
Profiling... [3328/50176]	Loss: 1.7516
Profile done
epoch 1 train time consumed: 4.44s
Validation Epoch: 8, Average loss: 0.0126, Accuracy: 0.2851
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.95553196313395,
                        "time": 2.8620708930002365,
                        "accuracy": 0.28505859375,
                        "total_cost": 1234.5091741863894
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7307
Profiling... [512/50176]	Loss: 1.7762
Profiling... [768/50176]	Loss: 1.8688
Profiling... [1024/50176]	Loss: 1.8264
Profiling... [1280/50176]	Loss: 1.9420
Profiling... [1536/50176]	Loss: 2.0211
Profiling... [1792/50176]	Loss: 1.8896
Profiling... [2048/50176]	Loss: 1.8033
Profiling... [2304/50176]	Loss: 1.7924
Profiling... [2560/50176]	Loss: 1.8781
Profiling... [2816/50176]	Loss: 1.8729
Profiling... [3072/50176]	Loss: 1.9326
Profiling... [3328/50176]	Loss: 1.9386
Profile done
epoch 1 train time consumed: 10.24s
Validation Epoch: 8, Average loss: 0.0120, Accuracy: 0.3177
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.91232309147179,
                        "time": 7.1943308640002215,
                        "accuracy": 0.31767578125,
                        "total_cost": 2783.56730910831
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8177
Profiling... [512/50176]	Loss: 1.6490
Profiling... [768/50176]	Loss: 1.9913
Profiling... [1024/50176]	Loss: 1.7169
Profiling... [1280/50176]	Loss: 1.8859
Profiling... [1536/50176]	Loss: 1.9963
Profiling... [1792/50176]	Loss: 2.0511
Profiling... [2048/50176]	Loss: 1.8363
Profiling... [2304/50176]	Loss: 2.0964
Profiling... [2560/50176]	Loss: 1.9419
Profiling... [2816/50176]	Loss: 1.9851
Profiling... [3072/50176]	Loss: 1.9219
Profiling... [3328/50176]	Loss: 2.0778
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 8, Average loss: 0.0115, Accuracy: 0.3208
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.88752021428085,
                        "time": 2.419745070999852,
                        "accuracy": 0.32080078125,
                        "total_cost": 926.9194113781504
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6837
Profiling... [512/50176]	Loss: 1.8087
Profiling... [768/50176]	Loss: 1.6967
Profiling... [1024/50176]	Loss: 1.8068
Profiling... [1280/50176]	Loss: 1.9856
Profiling... [1536/50176]	Loss: 1.8748
Profiling... [1792/50176]	Loss: 1.8858
Profiling... [2048/50176]	Loss: 1.8315
Profiling... [2304/50176]	Loss: 1.8230
Profiling... [2560/50176]	Loss: 2.0714
Profiling... [2816/50176]	Loss: 1.9307
Profiling... [3072/50176]	Loss: 1.9505
Profiling... [3328/50176]	Loss: 1.7751
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 8, Average loss: 0.0135, Accuracy: 0.2961
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.90878135872885,
                        "time": 2.5123610430000554,
                        "accuracy": 0.29609375,
                        "total_cost": 1042.8833237050148
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.9440
Profiling... [512/50176]	Loss: 1.6271
Profiling... [768/50176]	Loss: 1.9123
Profiling... [1024/50176]	Loss: 1.8873
Profiling... [1280/50176]	Loss: 1.8090
Profiling... [1536/50176]	Loss: 1.9605
Profiling... [1792/50176]	Loss: 1.9651
Profiling... [2048/50176]	Loss: 1.9294
Profiling... [2304/50176]	Loss: 2.0032
Profiling... [2560/50176]	Loss: 2.1346
Profiling... [2816/50176]	Loss: 1.8434
Profiling... [3072/50176]	Loss: 1.9438
Profiling... [3328/50176]	Loss: 1.9222
Profile done
epoch 1 train time consumed: 4.42s
Validation Epoch: 8, Average loss: 0.0105, Accuracy: 0.3240
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.91345112643049,
                        "time": 2.8692341020005188,
                        "accuracy": 0.3240234375,
                        "total_cost": 1088.4010992770498
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6304
Profiling... [512/50176]	Loss: 1.6321
Profiling... [768/50176]	Loss: 1.8822
Profiling... [1024/50176]	Loss: 1.8229
Profiling... [1280/50176]	Loss: 1.9978
Profiling... [1536/50176]	Loss: 1.8427
Profiling... [1792/50176]	Loss: 1.7596
Profiling... [2048/50176]	Loss: 1.9712
Profiling... [2304/50176]	Loss: 2.0377
Profiling... [2560/50176]	Loss: 1.8902
Profiling... [2816/50176]	Loss: 1.7973
Profiling... [3072/50176]	Loss: 2.0042
Profiling... [3328/50176]	Loss: 1.9137
Profile done
epoch 1 train time consumed: 10.21s
Validation Epoch: 8, Average loss: 0.0113, Accuracy: 0.3268
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.87246499657522,
                        "time": 7.3700063860005685,
                        "accuracy": 0.3267578125,
                        "total_cost": 2771.3824032543694
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6250
Profiling... [512/50176]	Loss: 1.5972
Profiling... [768/50176]	Loss: 1.7967
Profiling... [1024/50176]	Loss: 1.9788
Profiling... [1280/50176]	Loss: 2.1640
Profiling... [1536/50176]	Loss: 1.7770
Profiling... [1792/50176]	Loss: 1.9191
Profiling... [2048/50176]	Loss: 2.0578
Profiling... [2304/50176]	Loss: 1.9819
Profiling... [2560/50176]	Loss: 1.8392
Profiling... [2816/50176]	Loss: 1.7814
Profiling... [3072/50176]	Loss: 1.8769
Profiling... [3328/50176]	Loss: 1.9347
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 8, Average loss: 0.0109, Accuracy: 0.3378
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.84570680791231,
                        "time": 2.413776818999395,
                        "accuracy": 0.33779296875,
                        "total_cost": 877.822059185579
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6810
Profiling... [512/50176]	Loss: 1.6586
Profiling... [768/50176]	Loss: 1.6469
Profiling... [1024/50176]	Loss: 1.9565
Profiling... [1280/50176]	Loss: 1.8937
Profiling... [1536/50176]	Loss: 2.0423
Profiling... [1792/50176]	Loss: 1.8180
Profiling... [2048/50176]	Loss: 1.9685
Profiling... [2304/50176]	Loss: 1.6974
Profiling... [2560/50176]	Loss: 2.1289
Profiling... [2816/50176]	Loss: 1.7193
Profiling... [3072/50176]	Loss: 1.9081
Profiling... [3328/50176]	Loss: 1.9248
Profile done
epoch 1 train time consumed: 4.03s
Validation Epoch: 8, Average loss: 0.0103, Accuracy: 0.3515
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.86840073748924,
                        "time": 2.486174662999474,
                        "accuracy": 0.35146484375,
                        "total_cost": 869.1404282076554
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7293
Profiling... [512/50176]	Loss: 1.8842
Profiling... [768/50176]	Loss: 1.8565
Profiling... [1024/50176]	Loss: 1.9768
Profiling... [1280/50176]	Loss: 1.8496
Profiling... [1536/50176]	Loss: 1.8567
Profiling... [1792/50176]	Loss: 1.8576
Profiling... [2048/50176]	Loss: 1.9757
Profiling... [2304/50176]	Loss: 1.8009
Profiling... [2560/50176]	Loss: 1.8709
Profiling... [2816/50176]	Loss: 1.8111
Profiling... [3072/50176]	Loss: 1.9581
Profiling... [3328/50176]	Loss: 1.9205
Profile done
epoch 1 train time consumed: 4.44s
Validation Epoch: 8, Average loss: 0.0093, Accuracy: 0.3811
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.87639718441507,
                        "time": 2.8694427199998245,
                        "accuracy": 0.3810546875,
                        "total_cost": 925.2918148674571
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5763
Profiling... [512/50176]	Loss: 1.7530
Profiling... [768/50176]	Loss: 1.8235
Profiling... [1024/50176]	Loss: 1.8948
Profiling... [1280/50176]	Loss: 1.8565
Profiling... [1536/50176]	Loss: 1.9367
Profiling... [1792/50176]	Loss: 1.8575
Profiling... [2048/50176]	Loss: 1.7779
Profiling... [2304/50176]	Loss: 2.0196
Profiling... [2560/50176]	Loss: 1.8418
Profiling... [2816/50176]	Loss: 2.0528
Profiling... [3072/50176]	Loss: 2.0316
Profiling... [3328/50176]	Loss: 1.8073
Profile done
epoch 1 train time consumed: 10.55s
Validation Epoch: 8, Average loss: 0.0098, Accuracy: 0.3601
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.83328135462993,
                        "time": 7.744940903000497,
                        "accuracy": 0.36005859375,
                        "total_cost": 2642.1713619027923
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6881
Profiling... [1024/50176]	Loss: 1.6684
Profiling... [1536/50176]	Loss: 1.5891
Profiling... [2048/50176]	Loss: 1.8424
Profiling... [2560/50176]	Loss: 1.5680
Profiling... [3072/50176]	Loss: 1.7492
Profiling... [3584/50176]	Loss: 1.6819
Profiling... [4096/50176]	Loss: 1.7143
Profiling... [4608/50176]	Loss: 1.5654
Profiling... [5120/50176]	Loss: 1.7120
Profiling... [5632/50176]	Loss: 1.7248
Profiling... [6144/50176]	Loss: 1.8577
Profiling... [6656/50176]	Loss: 1.6909
Profile done
epoch 1 train time consumed: 6.98s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.4996
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.81971811317798,
                        "time": 4.598356962000253,
                        "accuracy": 0.499609375,
                        "total_cost": 1130.420953082877
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7068
Profiling... [1024/50176]	Loss: 1.7994
Profiling... [1536/50176]	Loss: 1.6626
Profiling... [2048/50176]	Loss: 1.7501
Profiling... [2560/50176]	Loss: 1.7011
Profiling... [3072/50176]	Loss: 1.6508
Profiling... [3584/50176]	Loss: 1.7606
Profiling... [4096/50176]	Loss: 1.7727
Profiling... [4608/50176]	Loss: 1.6710
Profiling... [5120/50176]	Loss: 1.8433
Profiling... [5632/50176]	Loss: 1.6902
Profiling... [6144/50176]	Loss: 1.6781
Profiling... [6656/50176]	Loss: 1.6029
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.5010
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.849854765147,
                        "time": 4.750225120999858,
                        "accuracy": 0.5009765625,
                        "total_cost": 1164.8538273017218
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7651
Profiling... [1024/50176]	Loss: 1.7202
Profiling... [1536/50176]	Loss: 1.6286
Profiling... [2048/50176]	Loss: 1.7305
Profiling... [2560/50176]	Loss: 1.6030
Profiling... [3072/50176]	Loss: 1.7214
Profiling... [3584/50176]	Loss: 1.8560
Profiling... [4096/50176]	Loss: 1.6867
Profiling... [4608/50176]	Loss: 1.6916
Profiling... [5120/50176]	Loss: 1.7622
Profiling... [5632/50176]	Loss: 1.6551
Profiling... [6144/50176]	Loss: 1.7158
Profiling... [6656/50176]	Loss: 1.7631
Profile done
epoch 1 train time consumed: 8.09s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.5015
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.85211232041597,
                        "time": 5.53619717499987,
                        "accuracy": 0.50146484375,
                        "total_cost": 1356.2935181755772
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7451
Profiling... [1024/50176]	Loss: 1.6243
Profiling... [1536/50176]	Loss: 1.5923
Profiling... [2048/50176]	Loss: 1.7376
Profiling... [2560/50176]	Loss: 1.7689
Profiling... [3072/50176]	Loss: 1.6976
Profiling... [3584/50176]	Loss: 1.7642
Profiling... [4096/50176]	Loss: 1.6748
Profiling... [4608/50176]	Loss: 1.7940
Profiling... [5120/50176]	Loss: 1.6561
Profiling... [5632/50176]	Loss: 1.7285
Profiling... [6144/50176]	Loss: 1.7171
Profiling... [6656/50176]	Loss: 1.6275
Profile done
epoch 1 train time consumed: 18.40s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.4985
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.78075334775816,
                        "time": 13.446568346999811,
                        "accuracy": 0.49853515625,
                        "total_cost": 3311.6617171103585
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6266
Profiling... [1024/50176]	Loss: 1.7532
Profiling... [1536/50176]	Loss: 1.7879
Profiling... [2048/50176]	Loss: 1.6424
Profiling... [2560/50176]	Loss: 1.6197
Profiling... [3072/50176]	Loss: 1.5332
Profiling... [3584/50176]	Loss: 1.9300
Profiling... [4096/50176]	Loss: 1.6902
Profiling... [4608/50176]	Loss: 1.6727
Profiling... [5120/50176]	Loss: 1.7750
Profiling... [5632/50176]	Loss: 1.7620
Profiling... [6144/50176]	Loss: 1.6802
Profiling... [6656/50176]	Loss: 1.6408
Profile done
epoch 1 train time consumed: 6.95s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.4979
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.76478623356307,
                        "time": 4.60502216399982,
                        "accuracy": 0.49794921875,
                        "total_cost": 1135.325732578545
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7841
Profiling... [1024/50176]	Loss: 1.6793
Profiling... [1536/50176]	Loss: 1.7267
Profiling... [2048/50176]	Loss: 1.6716
Profiling... [2560/50176]	Loss: 1.6212
Profiling... [3072/50176]	Loss: 1.6995
Profiling... [3584/50176]	Loss: 1.7424
Profiling... [4096/50176]	Loss: 1.6339
Profiling... [4608/50176]	Loss: 1.6337
Profiling... [5120/50176]	Loss: 1.7713
Profiling... [5632/50176]	Loss: 1.6215
Profiling... [6144/50176]	Loss: 1.7434
Profiling... [6656/50176]	Loss: 1.6378
Profile done
epoch 1 train time consumed: 7.22s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.5000
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.7953349802633,
                        "time": 4.752856820000488,
                        "accuracy": 0.5,
                        "total_cost": 1167.257290650378
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6195
Profiling... [1024/50176]	Loss: 1.7697
Profiling... [1536/50176]	Loss: 1.6513
Profiling... [2048/50176]	Loss: 1.6616
Profiling... [2560/50176]	Loss: 1.6978
Profiling... [3072/50176]	Loss: 1.7091
Profiling... [3584/50176]	Loss: 1.5777
Profiling... [4096/50176]	Loss: 1.6321
Profiling... [4608/50176]	Loss: 1.6460
Profiling... [5120/50176]	Loss: 1.6660
Profiling... [5632/50176]	Loss: 1.6774
Profiling... [6144/50176]	Loss: 1.7129
Profiling... [6656/50176]	Loss: 1.7023
Profile done
epoch 1 train time consumed: 8.16s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.4996
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.79813244787837,
                        "time": 5.545244249999996,
                        "accuracy": 0.499609375,
                        "total_cost": 1362.9560851762149
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7358
Profiling... [1024/50176]	Loss: 1.7036
Profiling... [1536/50176]	Loss: 1.8214
Profiling... [2048/50176]	Loss: 1.6903
Profiling... [2560/50176]	Loss: 1.7433
Profiling... [3072/50176]	Loss: 1.7359
Profiling... [3584/50176]	Loss: 1.7056
Profiling... [4096/50176]	Loss: 1.7472
Profiling... [4608/50176]	Loss: 1.7032
Profiling... [5120/50176]	Loss: 1.5855
Profiling... [5632/50176]	Loss: 1.6448
Profiling... [6144/50176]	Loss: 1.7409
Profiling... [6656/50176]	Loss: 1.6720
Profile done
epoch 1 train time consumed: 17.79s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.5010
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.7320314780953,
                        "time": 13.039228482999533,
                        "accuracy": 0.5009765625,
                        "total_cost": 3194.4228940362364
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.5598
Profiling... [1024/50176]	Loss: 1.6201
Profiling... [1536/50176]	Loss: 1.7554
Profiling... [2048/50176]	Loss: 1.6249
Profiling... [2560/50176]	Loss: 1.6998
Profiling... [3072/50176]	Loss: 1.6399
Profiling... [3584/50176]	Loss: 1.6536
Profiling... [4096/50176]	Loss: 1.6645
Profiling... [4608/50176]	Loss: 1.6220
Profiling... [5120/50176]	Loss: 1.7091
Profiling... [5632/50176]	Loss: 1.7045
Profiling... [6144/50176]	Loss: 1.6268
Profiling... [6656/50176]	Loss: 1.7797
Profile done
epoch 1 train time consumed: 7.07s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.4977
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.72200354478822,
                        "time": 4.615448624999772,
                        "accuracy": 0.49765625,
                        "total_cost": 1138.1693739765346
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6622
Profiling... [1024/50176]	Loss: 1.5768
Profiling... [1536/50176]	Loss: 1.5908
Profiling... [2048/50176]	Loss: 1.6545
Profiling... [2560/50176]	Loss: 1.7090
Profiling... [3072/50176]	Loss: 1.6733
Profiling... [3584/50176]	Loss: 1.7185
Profiling... [4096/50176]	Loss: 1.6237
Profiling... [4608/50176]	Loss: 1.7398
Profiling... [5120/50176]	Loss: 1.8245
Profiling... [5632/50176]	Loss: 1.7842
Profiling... [6144/50176]	Loss: 1.6890
Profiling... [6656/50176]	Loss: 1.7079
Profile done
epoch 1 train time consumed: 7.20s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.4973
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.75344682537181,
                        "time": 4.756765762999748,
                        "accuracy": 0.497265625,
                        "total_cost": 1174.2404135599334
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7891
Profiling... [1024/50176]	Loss: 1.6827
Profiling... [1536/50176]	Loss: 1.6416
Profiling... [2048/50176]	Loss: 1.7300
Profiling... [2560/50176]	Loss: 1.7972
Profiling... [3072/50176]	Loss: 1.8343
Profiling... [3584/50176]	Loss: 1.6308
Profiling... [4096/50176]	Loss: 1.7324
Profiling... [4608/50176]	Loss: 1.6845
Profiling... [5120/50176]	Loss: 1.6852
Profiling... [5632/50176]	Loss: 1.7114
Profiling... [6144/50176]	Loss: 1.5650
Profiling... [6656/50176]	Loss: 1.6839
Profile done
epoch 1 train time consumed: 8.18s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.4990
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.75420334400364,
                        "time": 5.5301816840001266,
                        "accuracy": 0.4990234375,
                        "total_cost": 1360.3630530220055
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7369
Profiling... [1024/50176]	Loss: 1.5627
Profiling... [1536/50176]	Loss: 1.6302
Profiling... [2048/50176]	Loss: 1.8012
Profiling... [2560/50176]	Loss: 1.7373
Profiling... [3072/50176]	Loss: 1.6572
Profiling... [3584/50176]	Loss: 1.7616
Profiling... [4096/50176]	Loss: 1.7357
Profiling... [4608/50176]	Loss: 1.7103
Profiling... [5120/50176]	Loss: 1.6607
Profiling... [5632/50176]	Loss: 1.7368
Profiling... [6144/50176]	Loss: 1.6529
Profiling... [6656/50176]	Loss: 1.6806
Profile done
epoch 1 train time consumed: 17.88s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.5049
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.68718138569064,
                        "time": 13.058833475999563,
                        "accuracy": 0.5048828125,
                        "total_cost": 3173.31355254936
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6127
Profiling... [1024/50176]	Loss: 1.6574
Profiling... [1536/50176]	Loss: 1.7537
Profiling... [2048/50176]	Loss: 1.6413
Profiling... [2560/50176]	Loss: 1.6432
Profiling... [3072/50176]	Loss: 1.8328
Profiling... [3584/50176]	Loss: 1.7810
Profiling... [4096/50176]	Loss: 1.7136
Profiling... [4608/50176]	Loss: 1.7626
Profiling... [5120/50176]	Loss: 1.8881
Profiling... [5632/50176]	Loss: 1.8296
Profiling... [6144/50176]	Loss: 1.6902
Profiling... [6656/50176]	Loss: 1.7281
Profile done
epoch 1 train time consumed: 7.11s
Validation Epoch: 8, Average loss: 0.0039, Accuracy: 0.4666
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.6776570350884,
                        "time": 4.621086943000591,
                        "accuracy": 0.4666015625,
                        "total_cost": 1214.9640393087018
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7065
Profiling... [1024/50176]	Loss: 1.6418
Profiling... [1536/50176]	Loss: 1.7177
Profiling... [2048/50176]	Loss: 1.7159
Profiling... [2560/50176]	Loss: 1.6881
Profiling... [3072/50176]	Loss: 1.8424
Profiling... [3584/50176]	Loss: 1.7763
Profiling... [4096/50176]	Loss: 1.6990
Profiling... [4608/50176]	Loss: 1.8392
Profiling... [5120/50176]	Loss: 1.7518
Profiling... [5632/50176]	Loss: 1.8318
Profiling... [6144/50176]	Loss: 1.6451
Profiling... [6656/50176]	Loss: 1.7506
Profile done
epoch 1 train time consumed: 7.14s
Validation Epoch: 8, Average loss: 0.0039, Accuracy: 0.4661
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.70574220889081,
                        "time": 4.775101455000367,
                        "accuracy": 0.46611328125,
                        "total_cost": 1257.0600146540546
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6950
Profiling... [1024/50176]	Loss: 1.7208
Profiling... [1536/50176]	Loss: 1.7455
Profiling... [2048/50176]	Loss: 1.8052
Profiling... [2560/50176]	Loss: 1.6951
Profiling... [3072/50176]	Loss: 1.8392
Profiling... [3584/50176]	Loss: 1.8338
Profiling... [4096/50176]	Loss: 1.7969
Profiling... [4608/50176]	Loss: 1.7285
Profiling... [5120/50176]	Loss: 1.6401
Profiling... [5632/50176]	Loss: 1.6417
Profiling... [6144/50176]	Loss: 1.6818
Profiling... [6656/50176]	Loss: 1.6736
Profile done
epoch 1 train time consumed: 8.18s
Validation Epoch: 8, Average loss: 0.0041, Accuracy: 0.4346
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.70768719715606,
                        "time": 5.541328745999635,
                        "accuracy": 0.4345703125,
                        "total_cost": 1564.680363251303
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6072
Profiling... [1024/50176]	Loss: 1.8987
Profiling... [1536/50176]	Loss: 1.8099
Profiling... [2048/50176]	Loss: 1.7218
Profiling... [2560/50176]	Loss: 1.6478
Profiling... [3072/50176]	Loss: 1.8015
Profiling... [3584/50176]	Loss: 1.7892
Profiling... [4096/50176]	Loss: 1.6713
Profiling... [4608/50176]	Loss: 1.9070
Profiling... [5120/50176]	Loss: 1.8278
Profiling... [5632/50176]	Loss: 1.7301
Profiling... [6144/50176]	Loss: 1.6762
Profiling... [6656/50176]	Loss: 1.7161
Profile done
epoch 1 train time consumed: 17.86s
Validation Epoch: 8, Average loss: 0.0043, Accuracy: 0.4241
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.64333968246783,
                        "time": 13.04850061199977,
                        "accuracy": 0.42412109375,
                        "total_cost": 3773.2423981903803
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7672
Profiling... [1024/50176]	Loss: 1.6815
Profiling... [1536/50176]	Loss: 1.7758
Profiling... [2048/50176]	Loss: 1.7148
Profiling... [2560/50176]	Loss: 1.7633
Profiling... [3072/50176]	Loss: 1.6528
Profiling... [3584/50176]	Loss: 1.6997
Profiling... [4096/50176]	Loss: 1.8155
Profiling... [4608/50176]	Loss: 1.8148
Profiling... [5120/50176]	Loss: 1.6044
Profiling... [5632/50176]	Loss: 1.8398
Profiling... [6144/50176]	Loss: 1.7128
Profiling... [6656/50176]	Loss: 1.8044
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 8, Average loss: 0.0039, Accuracy: 0.4612
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.63518181641875,
                        "time": 4.601231139999982,
                        "accuracy": 0.46123046875,
                        "total_cost": 1223.4075059319582
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6569
Profiling... [1024/50176]	Loss: 1.7424
Profiling... [1536/50176]	Loss: 1.5759
Profiling... [2048/50176]	Loss: 1.7602
Profiling... [2560/50176]	Loss: 1.5929
Profiling... [3072/50176]	Loss: 1.6495
Profiling... [3584/50176]	Loss: 1.7380
Profiling... [4096/50176]	Loss: 1.6678
Profiling... [4608/50176]	Loss: 1.6688
Profiling... [5120/50176]	Loss: 1.6861
Profiling... [5632/50176]	Loss: 1.7714
Profiling... [6144/50176]	Loss: 1.7005
Profiling... [6656/50176]	Loss: 1.6584
Profile done
epoch 1 train time consumed: 7.26s
Validation Epoch: 8, Average loss: 0.0040, Accuracy: 0.4474
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.66386061572817,
                        "time": 4.7770142879999185,
                        "accuracy": 0.44736328125,
                        "total_cost": 1309.8236698042906
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7856
Profiling... [1024/50176]	Loss: 1.7808
Profiling... [1536/50176]	Loss: 1.6846
Profiling... [2048/50176]	Loss: 1.7394
Profiling... [2560/50176]	Loss: 1.8259
Profiling... [3072/50176]	Loss: 1.7071
Profiling... [3584/50176]	Loss: 1.8105
Profiling... [4096/50176]	Loss: 1.7074
Profiling... [4608/50176]	Loss: 1.7396
Profiling... [5120/50176]	Loss: 1.9482
Profiling... [5632/50176]	Loss: 1.6633
Profiling... [6144/50176]	Loss: 1.7854
Profiling... [6656/50176]	Loss: 1.7910
Profile done
epoch 1 train time consumed: 8.14s
Validation Epoch: 8, Average loss: 0.0039, Accuracy: 0.4597
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.66652726776626,
                        "time": 5.545155941000303,
                        "accuracy": 0.45966796875,
                        "total_cost": 1479.7746823439713
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7542
Profiling... [1024/50176]	Loss: 1.6672
Profiling... [1536/50176]	Loss: 1.8289
Profiling... [2048/50176]	Loss: 1.8099
Profiling... [2560/50176]	Loss: 1.6900
Profiling... [3072/50176]	Loss: 1.8581
Profiling... [3584/50176]	Loss: 1.7419
Profiling... [4096/50176]	Loss: 1.8188
Profiling... [4608/50176]	Loss: 1.7076
Profiling... [5120/50176]	Loss: 1.7335
Profiling... [5632/50176]	Loss: 1.7781
Profiling... [6144/50176]	Loss: 1.6625
Profiling... [6656/50176]	Loss: 1.6825
Profile done
epoch 1 train time consumed: 17.81s
Validation Epoch: 8, Average loss: 0.0043, Accuracy: 0.4292
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.59995625524344,
                        "time": 13.041351488000146,
                        "accuracy": 0.42919921875,
                        "total_cost": 3725.237726654347
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8527
Profiling... [1024/50176]	Loss: 1.6134
Profiling... [1536/50176]	Loss: 1.7200
Profiling... [2048/50176]	Loss: 1.7093
Profiling... [2560/50176]	Loss: 1.7811
Profiling... [3072/50176]	Loss: 1.7304
Profiling... [3584/50176]	Loss: 1.7681
Profiling... [4096/50176]	Loss: 1.8600
Profiling... [4608/50176]	Loss: 1.8688
Profiling... [5120/50176]	Loss: 1.8581
Profiling... [5632/50176]	Loss: 1.7222
Profiling... [6144/50176]	Loss: 1.6448
Profiling... [6656/50176]	Loss: 1.7724
Profile done
epoch 1 train time consumed: 6.97s
Validation Epoch: 8, Average loss: 0.0042, Accuracy: 0.4434
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.59236936153061,
                        "time": 4.5894992969997475,
                        "accuracy": 0.443359375,
                        "total_cost": 1269.0328088861954
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6444
Profiling... [1024/50176]	Loss: 1.7001
Profiling... [1536/50176]	Loss: 1.8001
Profiling... [2048/50176]	Loss: 1.7718
Profiling... [2560/50176]	Loss: 1.7642
Profiling... [3072/50176]	Loss: 1.7497
Profiling... [3584/50176]	Loss: 1.8297
Profiling... [4096/50176]	Loss: 1.7763
Profiling... [4608/50176]	Loss: 1.6897
Profiling... [5120/50176]	Loss: 1.7213
Profiling... [5632/50176]	Loss: 1.7290
Profiling... [6144/50176]	Loss: 1.7521
Profiling... [6656/50176]	Loss: 1.7354
Profile done
epoch 1 train time consumed: 7.13s
Validation Epoch: 8, Average loss: 0.0040, Accuracy: 0.4511
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.62254860840237,
                        "time": 4.750718737999705,
                        "accuracy": 0.45107421875,
                        "total_cost": 1291.462059147482
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.8222
Profiling... [1024/50176]	Loss: 1.7687
Profiling... [1536/50176]	Loss: 1.7298
Profiling... [2048/50176]	Loss: 1.5752
Profiling... [2560/50176]	Loss: 1.6563
Profiling... [3072/50176]	Loss: 1.8119
Profiling... [3584/50176]	Loss: 1.6844
Profiling... [4096/50176]	Loss: 1.8193
Profiling... [4608/50176]	Loss: 1.8227
Profiling... [5120/50176]	Loss: 1.7370
Profiling... [5632/50176]	Loss: 1.8546
Profiling... [6144/50176]	Loss: 1.6780
Profiling... [6656/50176]	Loss: 1.8237
Profile done
epoch 1 train time consumed: 8.15s
Validation Epoch: 8, Average loss: 0.0039, Accuracy: 0.4608
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.62400642325665,
                        "time": 5.533546104000379,
                        "accuracy": 0.46083984375,
                        "total_cost": 1472.4108650822998
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7349
Profiling... [1024/50176]	Loss: 1.8135
Profiling... [1536/50176]	Loss: 1.7786
Profiling... [2048/50176]	Loss: 1.7515
Profiling... [2560/50176]	Loss: 1.7776
Profiling... [3072/50176]	Loss: 1.7271
Profiling... [3584/50176]	Loss: 1.7398
Profiling... [4096/50176]	Loss: 1.8183
Profiling... [4608/50176]	Loss: 1.7365
Profiling... [5120/50176]	Loss: 1.7584
Profiling... [5632/50176]	Loss: 1.5622
Profiling... [6144/50176]	Loss: 1.7128
Profiling... [6656/50176]	Loss: 1.8001
Profile done
epoch 1 train time consumed: 17.90s
Validation Epoch: 8, Average loss: 0.0041, Accuracy: 0.4397
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.55949931022396,
                        "time": 13.04851963999954,
                        "accuracy": 0.43974609375,
                        "total_cost": 3636.6895727950223
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7792
Profiling... [1024/50176]	Loss: 1.6646
Profiling... [1536/50176]	Loss: 1.6812
Profiling... [2048/50176]	Loss: 1.9097
Profiling... [2560/50176]	Loss: 1.9006
Profiling... [3072/50176]	Loss: 1.8789
Profiling... [3584/50176]	Loss: 1.8972
Profiling... [4096/50176]	Loss: 1.8376
Profiling... [4608/50176]	Loss: 1.8030
Profiling... [5120/50176]	Loss: 1.8246
Profiling... [5632/50176]	Loss: 1.7793
Profiling... [6144/50176]	Loss: 1.8542
Profiling... [6656/50176]	Loss: 1.9441
Profile done
epoch 1 train time consumed: 6.96s
Validation Epoch: 8, Average loss: 0.0056, Accuracy: 0.3258
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.55100699060989,
                        "time": 4.601392583999768,
                        "accuracy": 0.32578125,
                        "total_cost": 1730.9323195496856
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6841
Profiling... [1024/50176]	Loss: 1.6773
Profiling... [1536/50176]	Loss: 1.7976
Profiling... [2048/50176]	Loss: 1.7799
Profiling... [2560/50176]	Loss: 1.9859
Profiling... [3072/50176]	Loss: 1.9315
Profiling... [3584/50176]	Loss: 1.8536
Profiling... [4096/50176]	Loss: 1.9073
Profiling... [4608/50176]	Loss: 1.9011
Profiling... [5120/50176]	Loss: 1.8447
Profiling... [5632/50176]	Loss: 1.8404
Profiling... [6144/50176]	Loss: 1.8476
Profiling... [6656/50176]	Loss: 1.7923
Profile done
epoch 1 train time consumed: 7.14s
Validation Epoch: 8, Average loss: 0.0051, Accuracy: 0.3450
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.58060349475781,
                        "time": 4.7674131699996,
                        "accuracy": 0.34501953125,
                        "total_cost": 1693.7950769632184
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7494
Profiling... [1024/50176]	Loss: 1.7467
Profiling... [1536/50176]	Loss: 1.8320
Profiling... [2048/50176]	Loss: 1.8160
Profiling... [2560/50176]	Loss: 1.7562
Profiling... [3072/50176]	Loss: 1.7717
Profiling... [3584/50176]	Loss: 1.7809
Profiling... [4096/50176]	Loss: 1.8281
Profiling... [4608/50176]	Loss: 1.8614
Profiling... [5120/50176]	Loss: 1.8244
Profiling... [5632/50176]	Loss: 1.8955
Profiling... [6144/50176]	Loss: 2.0635
Profiling... [6656/50176]	Loss: 1.8663
Profile done
epoch 1 train time consumed: 8.25s
Validation Epoch: 8, Average loss: 0.0079, Accuracy: 0.2349
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.58362342542642,
                        "time": 5.559810028000356,
                        "accuracy": 0.23486328125,
                        "total_cost": 2901.8655243253575
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6287
Profiling... [1024/50176]	Loss: 1.7987
Profiling... [1536/50176]	Loss: 1.8982
Profiling... [2048/50176]	Loss: 1.9017
Profiling... [2560/50176]	Loss: 1.8460
Profiling... [3072/50176]	Loss: 1.9507
Profiling... [3584/50176]	Loss: 1.8910
Profiling... [4096/50176]	Loss: 1.8447
Profiling... [4608/50176]	Loss: 1.8760
Profiling... [5120/50176]	Loss: 1.7986
Profiling... [5632/50176]	Loss: 1.8311
Profiling... [6144/50176]	Loss: 1.7084
Profiling... [6656/50176]	Loss: 1.8906
Profile done
epoch 1 train time consumed: 17.83s
Validation Epoch: 8, Average loss: 0.0064, Accuracy: 0.2885
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.51972771312907,
                        "time": 13.031713990999378,
                        "accuracy": 0.2884765625,
                        "total_cost": 5534.737505105353
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8843
Profiling... [1024/50176]	Loss: 1.8538
Profiling... [1536/50176]	Loss: 1.7796
Profiling... [2048/50176]	Loss: 1.8458
Profiling... [2560/50176]	Loss: 1.6647
Profiling... [3072/50176]	Loss: 1.8270
Profiling... [3584/50176]	Loss: 1.8469
Profiling... [4096/50176]	Loss: 1.9913
Profiling... [4608/50176]	Loss: 1.7001
Profiling... [5120/50176]	Loss: 1.7596
Profiling... [5632/50176]	Loss: 1.9278
Profiling... [6144/50176]	Loss: 1.8106
Profiling... [6656/50176]	Loss: 1.8596
Profile done
epoch 1 train time consumed: 7.04s
Validation Epoch: 8, Average loss: 0.0049, Accuracy: 0.3640
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.5133970338521,
                        "time": 4.623236434999853,
                        "accuracy": 0.36396484375,
                        "total_cost": 1556.2173398581385
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7712
Profiling... [1024/50176]	Loss: 1.8112
Profiling... [1536/50176]	Loss: 1.6591
Profiling... [2048/50176]	Loss: 1.9426
Profiling... [2560/50176]	Loss: 1.8436
Profiling... [3072/50176]	Loss: 1.7783
Profiling... [3584/50176]	Loss: 1.8686
Profiling... [4096/50176]	Loss: 1.8605
Profiling... [4608/50176]	Loss: 1.8300
Profiling... [5120/50176]	Loss: 1.7977
Profiling... [5632/50176]	Loss: 1.9049
Profiling... [6144/50176]	Loss: 1.9370
Profiling... [6656/50176]	Loss: 1.9196
Profile done
epoch 1 train time consumed: 7.10s
Validation Epoch: 8, Average loss: 0.0057, Accuracy: 0.3236
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.54291534303583,
                        "time": 4.766004608000003,
                        "accuracy": 0.3236328125,
                        "total_cost": 1804.6380856473354
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6785
Profiling... [1024/50176]	Loss: 1.7366
Profiling... [1536/50176]	Loss: 2.0055
Profiling... [2048/50176]	Loss: 1.8994
Profiling... [2560/50176]	Loss: 1.7079
Profiling... [3072/50176]	Loss: 1.7734
Profiling... [3584/50176]	Loss: 1.7290
Profiling... [4096/50176]	Loss: 1.7840
Profiling... [4608/50176]	Loss: 1.7941
Profiling... [5120/50176]	Loss: 1.6652
Profiling... [5632/50176]	Loss: 1.8494
Profiling... [6144/50176]	Loss: 1.8563
Profiling... [6656/50176]	Loss: 1.9336
Profile done
epoch 1 train time consumed: 8.13s
Validation Epoch: 8, Average loss: 0.0053, Accuracy: 0.3293
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.54628539429183,
                        "time": 5.562666476999766,
                        "accuracy": 0.329296875,
                        "total_cost": 2070.1202027613326
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.5747
Profiling... [1024/50176]	Loss: 1.7169
Profiling... [1536/50176]	Loss: 1.8559
Profiling... [2048/50176]	Loss: 1.7871
Profiling... [2560/50176]	Loss: 1.7163
Profiling... [3072/50176]	Loss: 1.7754
Profiling... [3584/50176]	Loss: 1.8568
Profiling... [4096/50176]	Loss: 1.7786
Profiling... [4608/50176]	Loss: 1.9644
Profiling... [5120/50176]	Loss: 1.8067
Profiling... [5632/50176]	Loss: 1.9894
Profiling... [6144/50176]	Loss: 1.8331
Profiling... [6656/50176]	Loss: 1.9156
Profile done
epoch 1 train time consumed: 17.84s
Validation Epoch: 8, Average loss: 0.0216, Accuracy: 0.0758
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.48269738198417,
                        "time": 13.022873779999827,
                        "accuracy": 0.07578125,
                        "total_cost": 21048.43491285107
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7074
Profiling... [1024/50176]	Loss: 1.7115
Profiling... [1536/50176]	Loss: 1.7400
Profiling... [2048/50176]	Loss: 1.7519
Profiling... [2560/50176]	Loss: 1.8433
Profiling... [3072/50176]	Loss: 1.7580
Profiling... [3584/50176]	Loss: 1.9154
Profiling... [4096/50176]	Loss: 1.9347
Profiling... [4608/50176]	Loss: 1.8340
Profiling... [5120/50176]	Loss: 1.8569
Profiling... [5632/50176]	Loss: 1.8159
Profiling... [6144/50176]	Loss: 1.7957
Profiling... [6656/50176]	Loss: 1.8873
Profile done
epoch 1 train time consumed: 6.99s
Validation Epoch: 8, Average loss: 0.0060, Accuracy: 0.2872
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.47618170909615,
                        "time": 4.6157875879998755,
                        "accuracy": 0.28720703125,
                        "total_cost": 1968.350276447013
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6777
Profiling... [1024/50176]	Loss: 1.7824
Profiling... [1536/50176]	Loss: 1.8018
Profiling... [2048/50176]	Loss: 1.9476
Profiling... [2560/50176]	Loss: 1.7660
Profiling... [3072/50176]	Loss: 1.7514
Profiling... [3584/50176]	Loss: 1.8370
Profiling... [4096/50176]	Loss: 1.7578
Profiling... [4608/50176]	Loss: 1.8381
Profiling... [5120/50176]	Loss: 1.8536
Profiling... [5632/50176]	Loss: 1.6914
Profiling... [6144/50176]	Loss: 1.8767
Profiling... [6656/50176]	Loss: 1.9477
Profile done
epoch 1 train time consumed: 7.22s
Validation Epoch: 8, Average loss: 0.0048, Accuracy: 0.3627
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.50541111405232,
                        "time": 4.765536447000159,
                        "accuracy": 0.3626953125,
                        "total_cost": 1609.6265418890814
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7259
Profiling... [1024/50176]	Loss: 1.7273
Profiling... [1536/50176]	Loss: 1.8044
Profiling... [2048/50176]	Loss: 1.7680
Profiling... [2560/50176]	Loss: 1.8658
Profiling... [3072/50176]	Loss: 1.7447
Profiling... [3584/50176]	Loss: 1.9299
Profiling... [4096/50176]	Loss: 1.8820
Profiling... [4608/50176]	Loss: 1.8300
Profiling... [5120/50176]	Loss: 1.9214
Profiling... [5632/50176]	Loss: 1.9400
Profiling... [6144/50176]	Loss: 1.9159
Profiling... [6656/50176]	Loss: 1.8015
Profile done
epoch 1 train time consumed: 8.12s
Validation Epoch: 8, Average loss: 0.0066, Accuracy: 0.2856
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.50711287502578,
                        "time": 5.536424029000045,
                        "accuracy": 0.28564453125,
                        "total_cost": 2374.4593340423476
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6667
Profiling... [1024/50176]	Loss: 1.7858
Profiling... [1536/50176]	Loss: 1.8052
Profiling... [2048/50176]	Loss: 1.8049
Profiling... [2560/50176]	Loss: 1.9512
Profiling... [3072/50176]	Loss: 1.7091
Profiling... [3584/50176]	Loss: 1.9905
Profiling... [4096/50176]	Loss: 1.9197
Profiling... [4608/50176]	Loss: 1.7880
Profiling... [5120/50176]	Loss: 1.7438
Profiling... [5632/50176]	Loss: 1.8481
Profiling... [6144/50176]	Loss: 1.9025
Profiling... [6656/50176]	Loss: 1.6879
Profile done
epoch 1 train time consumed: 17.89s
Validation Epoch: 8, Average loss: 0.0046, Accuracy: 0.3949
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.44234737367086,
                        "time": 13.030254566000622,
                        "accuracy": 0.394921875,
                        "total_cost": 4039.925506627378
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7400
Profiling... [2048/50176]	Loss: 1.7668
Profiling... [3072/50176]	Loss: 1.6900
Profiling... [4096/50176]	Loss: 1.6680
Profiling... [5120/50176]	Loss: 1.7273
Profiling... [6144/50176]	Loss: 1.6611
Profiling... [7168/50176]	Loss: 1.7595
Profiling... [8192/50176]	Loss: 1.7161
Profiling... [9216/50176]	Loss: 1.6300
Profiling... [10240/50176]	Loss: 1.6937
Profiling... [11264/50176]	Loss: 1.6547
Profiling... [12288/50176]	Loss: 1.7186
Profiling... [13312/50176]	Loss: 1.6592
Profile done
epoch 1 train time consumed: 13.17s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5016
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.45088311763489,
                        "time": 9.04296542999964,
                        "accuracy": 0.5015625,
                        "total_cost": 2207.7390213696176
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7205
Profiling... [2048/50176]	Loss: 1.6585
Profiling... [3072/50176]	Loss: 1.6480
Profiling... [4096/50176]	Loss: 1.6038
Profiling... [5120/50176]	Loss: 1.7305
Profiling... [6144/50176]	Loss: 1.6593
Profiling... [7168/50176]	Loss: 1.6657
Profiling... [8192/50176]	Loss: 1.7411
Profiling... [9216/50176]	Loss: 1.7319
Profiling... [10240/50176]	Loss: 1.7681
Profiling... [11264/50176]	Loss: 1.6277
Profiling... [12288/50176]	Loss: 1.6870
Profiling... [13312/50176]	Loss: 1.7726
Profile done
epoch 1 train time consumed: 13.52s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.4997
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.49669500206208,
                        "time": 9.37512660900029,
                        "accuracy": 0.49970703125,
                        "total_cost": 2298.1906457383375
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7474
Profiling... [2048/50176]	Loss: 1.6924
Profiling... [3072/50176]	Loss: 1.6368
Profiling... [4096/50176]	Loss: 1.6268
Profiling... [5120/50176]	Loss: 1.7499
Profiling... [6144/50176]	Loss: 1.6889
Profiling... [7168/50176]	Loss: 1.6172
Profiling... [8192/50176]	Loss: 1.6637
Profiling... [9216/50176]	Loss: 1.6904
Profiling... [10240/50176]	Loss: 1.6797
Profiling... [11264/50176]	Loss: 1.6421
Profiling... [12288/50176]	Loss: 1.6583
Profiling... [13312/50176]	Loss: 1.6629
Profile done
epoch 1 train time consumed: 15.72s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5019
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.49544889401575,
                        "time": 11.0133444410003,
                        "accuracy": 0.50185546875,
                        "total_cost": 2688.1934244633944
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5644
Profiling... [2048/50176]	Loss: 1.6923
Profiling... [3072/50176]	Loss: 1.8154
Profiling... [4096/50176]	Loss: 1.6281
Profiling... [5120/50176]	Loss: 1.7365
Profiling... [6144/50176]	Loss: 1.7151
Profiling... [7168/50176]	Loss: 1.6506
Profiling... [8192/50176]	Loss: 1.7046
Profiling... [9216/50176]	Loss: 1.6704
Profiling... [10240/50176]	Loss: 1.6865
Profiling... [11264/50176]	Loss: 1.5816
Profiling... [12288/50176]	Loss: 1.6648
Profiling... [13312/50176]	Loss: 1.6698
Profile done
epoch 1 train time consumed: 38.08s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5038
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 122.37337802290891,
                        "time": 28.247671455999807,
                        "accuracy": 0.50380859375,
                        "total_cost": 6861.262432270684
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7546
Profiling... [2048/50176]	Loss: 1.7086
Profiling... [3072/50176]	Loss: 1.6842
Profiling... [4096/50176]	Loss: 1.6860
Profiling... [5120/50176]	Loss: 1.6385
Profiling... [6144/50176]	Loss: 1.7796
Profiling... [7168/50176]	Loss: 1.7073
Profiling... [8192/50176]	Loss: 1.6553
Profiling... [9216/50176]	Loss: 1.6874
Profiling... [10240/50176]	Loss: 1.6193
Profiling... [11264/50176]	Loss: 1.7120
Profiling... [12288/50176]	Loss: 1.6954
Profiling... [13312/50176]	Loss: 1.7553
Profile done
epoch 1 train time consumed: 13.16s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5022
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.39819936905002,
                        "time": 9.026377639999737,
                        "accuracy": 0.50224609375,
                        "total_cost": 2199.743081547905
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7640
Profiling... [2048/50176]	Loss: 1.6887
Profiling... [3072/50176]	Loss: 1.7417
Profiling... [4096/50176]	Loss: 1.6809
Profiling... [5120/50176]	Loss: 1.7322
Profiling... [6144/50176]	Loss: 1.7491
Profiling... [7168/50176]	Loss: 1.7203
Profiling... [8192/50176]	Loss: 1.6517
Profiling... [9216/50176]	Loss: 1.7048
Profiling... [10240/50176]	Loss: 1.7876
Profiling... [11264/50176]	Loss: 1.7231
Profiling... [12288/50176]	Loss: 1.7075
Profiling... [13312/50176]	Loss: 1.6648
Profile done
epoch 1 train time consumed: 13.61s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.4958
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.44243906410973,
                        "time": 9.372420956000497,
                        "accuracy": 0.49580078125,
                        "total_cost": 2314.603214006686
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7128
Profiling... [2048/50176]	Loss: 1.6860
Profiling... [3072/50176]	Loss: 1.6934
Profiling... [4096/50176]	Loss: 1.7412
Profiling... [5120/50176]	Loss: 1.6751
Profiling... [6144/50176]	Loss: 1.7372
Profiling... [7168/50176]	Loss: 1.6496
Profiling... [8192/50176]	Loss: 1.6830
Profiling... [9216/50176]	Loss: 1.6452
Profiling... [10240/50176]	Loss: 1.6257
Profiling... [11264/50176]	Loss: 1.6736
Profiling... [12288/50176]	Loss: 1.7560
Profiling... [13312/50176]	Loss: 1.5432
Profile done
epoch 1 train time consumed: 15.70s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5002
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.44128613323375,
                        "time": 10.99803483300002,
                        "accuracy": 0.5001953125,
                        "total_cost": 2692.1754287543963
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6410
Profiling... [2048/50176]	Loss: 1.6773
Profiling... [3072/50176]	Loss: 1.7084
Profiling... [4096/50176]	Loss: 1.6613
Profiling... [5120/50176]	Loss: 1.6639
Profiling... [6144/50176]	Loss: 1.6951
Profiling... [7168/50176]	Loss: 1.6150
Profiling... [8192/50176]	Loss: 1.6900
Profiling... [9216/50176]	Loss: 1.6921
Profiling... [10240/50176]	Loss: 1.7207
Profiling... [11264/50176]	Loss: 1.7338
Profiling... [12288/50176]	Loss: 1.7027
Profiling... [13312/50176]	Loss: 1.6952
Profile done
epoch 1 train time consumed: 38.38s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5001
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 122.31725833613251,
                        "time": 28.496854322000218,
                        "accuracy": 0.50009765625,
                        "total_cost": 6969.952864823559
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7026
Profiling... [2048/50176]	Loss: 1.7330
Profiling... [3072/50176]	Loss: 1.7113
Profiling... [4096/50176]	Loss: 1.6462
Profiling... [5120/50176]	Loss: 1.6545
Profiling... [6144/50176]	Loss: 1.7330
Profiling... [7168/50176]	Loss: 1.7888
Profiling... [8192/50176]	Loss: 1.7139
Profiling... [9216/50176]	Loss: 1.6306
Profiling... [10240/50176]	Loss: 1.6380
Profiling... [11264/50176]	Loss: 1.7112
Profiling... [12288/50176]	Loss: 1.6994
Profiling... [13312/50176]	Loss: 1.7667
Profile done
epoch 1 train time consumed: 13.18s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5017
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.34274874784697,
                        "time": 9.015925564999634,
                        "accuracy": 0.50166015625,
                        "total_cost": 2198.7656432063723
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6751
Profiling... [2048/50176]	Loss: 1.7323
Profiling... [3072/50176]	Loss: 1.6544
Profiling... [4096/50176]	Loss: 1.6214
Profiling... [5120/50176]	Loss: 1.6974
Profiling... [6144/50176]	Loss: 1.7228
Profiling... [7168/50176]	Loss: 1.7160
Profiling... [8192/50176]	Loss: 1.7016
Profiling... [9216/50176]	Loss: 1.7250
Profiling... [10240/50176]	Loss: 1.7479
Profiling... [11264/50176]	Loss: 1.6937
Profiling... [12288/50176]	Loss: 1.6164
Profiling... [13312/50176]	Loss: 1.7195
Profile done
epoch 1 train time consumed: 13.52s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5052
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.38661000559445,
                        "time": 9.371208948999993,
                        "accuracy": 0.50517578125,
                        "total_cost": 2270.3196342554256
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7621
Profiling... [2048/50176]	Loss: 1.7521
Profiling... [3072/50176]	Loss: 1.6537
Profiling... [4096/50176]	Loss: 1.6789
Profiling... [5120/50176]	Loss: 1.5308
Profiling... [6144/50176]	Loss: 1.7073
Profiling... [7168/50176]	Loss: 1.6216
Profiling... [8192/50176]	Loss: 1.7109
Profiling... [9216/50176]	Loss: 1.7319
Profiling... [10240/50176]	Loss: 1.7322
Profiling... [11264/50176]	Loss: 1.6793
Profiling... [12288/50176]	Loss: 1.7436
Profiling... [13312/50176]	Loss: 1.6977
Profile done
epoch 1 train time consumed: 15.62s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5022
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.38648174222355,
                        "time": 10.988789758000166,
                        "accuracy": 0.50224609375,
                        "total_cost": 2677.7297699721944
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6915
Profiling... [2048/50176]	Loss: 1.6637
Profiling... [3072/50176]	Loss: 1.6599
Profiling... [4096/50176]	Loss: 1.6413
Profiling... [5120/50176]	Loss: 1.6766
Profiling... [6144/50176]	Loss: 1.7011
Profiling... [7168/50176]	Loss: 1.7113
Profiling... [8192/50176]	Loss: 1.7440
Profiling... [9216/50176]	Loss: 1.6853
Profiling... [10240/50176]	Loss: 1.5797
Profiling... [11264/50176]	Loss: 1.8307
Profiling... [12288/50176]	Loss: 1.7266
Profiling... [13312/50176]	Loss: 1.6426
Profile done
epoch 1 train time consumed: 38.29s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5018
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 122.2642738303195,
                        "time": 28.364804580000055,
                        "accuracy": 0.5017578125,
                        "total_cost": 6911.705503962886
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6830
Profiling... [2048/50176]	Loss: 1.7602
Profiling... [3072/50176]	Loss: 1.7315
Profiling... [4096/50176]	Loss: 1.8367
Profiling... [5120/50176]	Loss: 1.7215
Profiling... [6144/50176]	Loss: 1.7663
Profiling... [7168/50176]	Loss: 1.6835
Profiling... [8192/50176]	Loss: 1.7592
Profiling... [9216/50176]	Loss: 1.7393
Profiling... [10240/50176]	Loss: 1.7082
Profiling... [11264/50176]	Loss: 1.7117
Profiling... [12288/50176]	Loss: 1.7625
Profiling... [13312/50176]	Loss: 1.6199
Profile done
epoch 1 train time consumed: 13.17s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4666
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.2866273289165,
                        "time": 9.037013392000517,
                        "accuracy": 0.4666015625,
                        "total_cost": 2368.414462465489
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7112
Profiling... [2048/50176]	Loss: 1.6783
Profiling... [3072/50176]	Loss: 1.6570
Profiling... [4096/50176]	Loss: 1.7805
Profiling... [5120/50176]	Loss: 1.7783
Profiling... [6144/50176]	Loss: 1.8124
Profiling... [7168/50176]	Loss: 1.7854
Profiling... [8192/50176]	Loss: 1.6466
Profiling... [9216/50176]	Loss: 1.7276
Profiling... [10240/50176]	Loss: 1.7906
Profiling... [11264/50176]	Loss: 1.7644
Profiling... [12288/50176]	Loss: 1.7310
Profiling... [13312/50176]	Loss: 1.7074
Profile done
epoch 1 train time consumed: 13.62s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4505
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.32827213495382,
                        "time": 9.373046371999408,
                        "accuracy": 0.45048828125,
                        "total_cost": 2545.212861355614
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7295
Profiling... [2048/50176]	Loss: 1.6650
Profiling... [3072/50176]	Loss: 1.6268
Profiling... [4096/50176]	Loss: 1.8057
Profiling... [5120/50176]	Loss: 1.7992
Profiling... [6144/50176]	Loss: 1.6889
Profiling... [7168/50176]	Loss: 1.8155
Profiling... [8192/50176]	Loss: 1.7527
Profiling... [9216/50176]	Loss: 1.6858
Profiling... [10240/50176]	Loss: 1.6764
Profiling... [11264/50176]	Loss: 1.7459
Profiling... [12288/50176]	Loss: 1.7169
Profiling... [13312/50176]	Loss: 1.7505
Profile done
epoch 1 train time consumed: 15.70s
Validation Epoch: 8, Average loss: 0.0022, Accuracy: 0.4271
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.32756989444685,
                        "time": 10.985982659000001,
                        "accuracy": 0.4271484375,
                        "total_cost": 3146.1862987102786
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.8283
Profiling... [2048/50176]	Loss: 1.6362
Profiling... [3072/50176]	Loss: 1.7219
Profiling... [4096/50176]	Loss: 1.7041
Profiling... [5120/50176]	Loss: 1.6379
Profiling... [6144/50176]	Loss: 1.7980
Profiling... [7168/50176]	Loss: 1.6891
Profiling... [8192/50176]	Loss: 1.7217
Profiling... [9216/50176]	Loss: 1.7716
Profiling... [10240/50176]	Loss: 1.7832
Profiling... [11264/50176]	Loss: 1.7861
Profiling... [12288/50176]	Loss: 1.7729
Profiling... [13312/50176]	Loss: 1.8280
Profile done
epoch 1 train time consumed: 39.19s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4602
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 122.20076854005015,
                        "time": 29.23694246099967,
                        "accuracy": 0.46015625,
                        "total_cost": 7764.268851059581
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6879
Profiling... [2048/50176]	Loss: 1.7236
Profiling... [3072/50176]	Loss: 1.6744
Profiling... [4096/50176]	Loss: 1.7086
Profiling... [5120/50176]	Loss: 1.7751
Profiling... [6144/50176]	Loss: 1.7113
Profiling... [7168/50176]	Loss: 1.6985
Profiling... [8192/50176]	Loss: 1.6796
Profiling... [9216/50176]	Loss: 1.7431
Profiling... [10240/50176]	Loss: 1.7282
Profiling... [11264/50176]	Loss: 1.7796
Profiling... [12288/50176]	Loss: 1.5629
Profiling... [13312/50176]	Loss: 1.7441
Profile done
epoch 1 train time consumed: 13.27s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4765
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.22434798512244,
                        "time": 9.047559044000081,
                        "accuracy": 0.47646484375,
                        "total_cost": 2320.909967472931
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7246
Profiling... [2048/50176]	Loss: 1.7597
Profiling... [3072/50176]	Loss: 1.7890
Profiling... [4096/50176]	Loss: 1.6871
Profiling... [5120/50176]	Loss: 1.7083
Profiling... [6144/50176]	Loss: 1.7196
Profiling... [7168/50176]	Loss: 1.7609
Profiling... [8192/50176]	Loss: 1.7104
Profiling... [9216/50176]	Loss: 1.7472
Profiling... [10240/50176]	Loss: 1.7705
Profiling... [11264/50176]	Loss: 1.7984
Profiling... [12288/50176]	Loss: 1.7448
Profiling... [13312/50176]	Loss: 1.6857
Profile done
epoch 1 train time consumed: 13.58s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4590
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.26643951255112,
                        "time": 9.366591211000014,
                        "accuracy": 0.458984375,
                        "total_cost": 2495.1170892005334
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6701
Profiling... [2048/50176]	Loss: 1.7704
Profiling... [3072/50176]	Loss: 1.7112
Profiling... [4096/50176]	Loss: 1.7149
Profiling... [5120/50176]	Loss: 1.7113
Profiling... [6144/50176]	Loss: 1.7153
Profiling... [7168/50176]	Loss: 1.6676
Profiling... [8192/50176]	Loss: 1.6332
Profiling... [9216/50176]	Loss: 1.7280
Profiling... [10240/50176]	Loss: 1.6912
Profiling... [11264/50176]	Loss: 1.6682
Profiling... [12288/50176]	Loss: 1.6898
Profiling... [13312/50176]	Loss: 1.8056
Profile done
epoch 1 train time consumed: 15.67s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4703
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.26705152644958,
                        "time": 10.987642385000072,
                        "accuracy": 0.4703125,
                        "total_cost": 2856.455309269911
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6448
Profiling... [2048/50176]	Loss: 1.7942
Profiling... [3072/50176]	Loss: 1.6622
Profiling... [4096/50176]	Loss: 1.7200
Profiling... [5120/50176]	Loss: 1.7132
Profiling... [6144/50176]	Loss: 1.7415
Profiling... [7168/50176]	Loss: 1.7137
Profiling... [8192/50176]	Loss: 1.7123
Profiling... [9216/50176]	Loss: 1.7427
Profiling... [10240/50176]	Loss: 1.7739
Profiling... [11264/50176]	Loss: 1.8087
Profiling... [12288/50176]	Loss: 1.7085
Profiling... [13312/50176]	Loss: 1.7855
Profile done
epoch 1 train time consumed: 39.14s
Validation Epoch: 8, Average loss: 0.0022, Accuracy: 0.4253
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 122.14440332794521,
                        "time": 29.130400334000115,
                        "accuracy": 0.42529296875,
                        "total_cost": 8366.268969737393
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6910
Profiling... [2048/50176]	Loss: 1.7295
Profiling... [3072/50176]	Loss: 1.7627
Profiling... [4096/50176]	Loss: 1.7250
Profiling... [5120/50176]	Loss: 1.7546
Profiling... [6144/50176]	Loss: 1.7333
Profiling... [7168/50176]	Loss: 1.7006
Profiling... [8192/50176]	Loss: 1.6568
Profiling... [9216/50176]	Loss: 1.6855
Profiling... [10240/50176]	Loss: 1.8436
Profiling... [11264/50176]	Loss: 1.6908
Profiling... [12288/50176]	Loss: 1.6904
Profiling... [13312/50176]	Loss: 1.7259
Profile done
epoch 1 train time consumed: 13.27s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4696
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.16659299193786,
                        "time": 9.038925015000132,
                        "accuracy": 0.46962890625,
                        "total_cost": 2351.334550953585
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7083
Profiling... [2048/50176]	Loss: 1.7107
Profiling... [3072/50176]	Loss: 1.7168
Profiling... [4096/50176]	Loss: 1.6967
Profiling... [5120/50176]	Loss: 1.6691
Profiling... [6144/50176]	Loss: 1.7338
Profiling... [7168/50176]	Loss: 1.8339
Profiling... [8192/50176]	Loss: 1.6998
Profiling... [9216/50176]	Loss: 1.6995
Profiling... [10240/50176]	Loss: 1.7646
Profiling... [11264/50176]	Loss: 1.7261
Profiling... [12288/50176]	Loss: 1.6575
Profiling... [13312/50176]	Loss: 1.7360
Profile done
epoch 1 train time consumed: 13.51s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4611
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.20922309853883,
                        "time": 9.343919478000316,
                        "accuracy": 0.4611328125,
                        "total_cost": 2476.3215914107677
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7296
Profiling... [2048/50176]	Loss: 1.7152
Profiling... [3072/50176]	Loss: 1.6944
Profiling... [4096/50176]	Loss: 1.8088
Profiling... [5120/50176]	Loss: 1.7303
Profiling... [6144/50176]	Loss: 1.7874
Profiling... [7168/50176]	Loss: 1.7355
Profiling... [8192/50176]	Loss: 1.7803
Profiling... [9216/50176]	Loss: 1.7400
Profiling... [10240/50176]	Loss: 1.7448
Profiling... [11264/50176]	Loss: 1.7169
Profiling... [12288/50176]	Loss: 1.6541
Profiling... [13312/50176]	Loss: 1.7937
Profile done
epoch 1 train time consumed: 15.74s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4530
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.209345650166,
                        "time": 10.996977040000274,
                        "accuracy": 0.45302734375,
                        "total_cost": 2966.5612611012134
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7266
Profiling... [2048/50176]	Loss: 1.6880
Profiling... [3072/50176]	Loss: 1.7248
Profiling... [4096/50176]	Loss: 1.6799
Profiling... [5120/50176]	Loss: 1.7044
Profiling... [6144/50176]	Loss: 1.6567
Profiling... [7168/50176]	Loss: 1.7438
Profiling... [8192/50176]	Loss: 1.6932
Profiling... [9216/50176]	Loss: 1.7695
Profiling... [10240/50176]	Loss: 1.7777
Profiling... [11264/50176]	Loss: 1.7449
Profiling... [12288/50176]	Loss: 1.8074
Profiling... [13312/50176]	Loss: 1.6782
Profile done
epoch 1 train time consumed: 38.35s
Validation Epoch: 8, Average loss: 0.0024, Accuracy: 0.3971
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 122.09709966716078,
                        "time": 28.456520804999855,
                        "accuracy": 0.3970703125,
                        "total_cost": 8750.235279573315
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6606
Profiling... [2048/50176]	Loss: 1.7858
Profiling... [3072/50176]	Loss: 1.8269
Profiling... [4096/50176]	Loss: 1.7250
Profiling... [5120/50176]	Loss: 1.7943
Profiling... [6144/50176]	Loss: 1.7846
Profiling... [7168/50176]	Loss: 1.7358
Profiling... [8192/50176]	Loss: 1.7774
Profiling... [9216/50176]	Loss: 1.8155
Profiling... [10240/50176]	Loss: 1.7984
Profiling... [11264/50176]	Loss: 1.8022
Profiling... [12288/50176]	Loss: 1.7895
Profiling... [13312/50176]	Loss: 1.7905
Profile done
epoch 1 train time consumed: 13.39s
Validation Epoch: 8, Average loss: 0.0030, Accuracy: 0.3061
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.11957117335596,
                        "time": 9.175330054999904,
                        "accuracy": 0.3060546875,
                        "total_cost": 3661.069140431293
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.5814
Profiling... [2048/50176]	Loss: 1.7837
Profiling... [3072/50176]	Loss: 1.7907
Profiling... [4096/50176]	Loss: 1.7286
Profiling... [5120/50176]	Loss: 1.8230
Profiling... [6144/50176]	Loss: 1.7869
Profiling... [7168/50176]	Loss: 1.7228
Profiling... [8192/50176]	Loss: 1.7331
Profiling... [9216/50176]	Loss: 1.8151
Profiling... [10240/50176]	Loss: 1.8299
Profiling... [11264/50176]	Loss: 1.7952
Profiling... [12288/50176]	Loss: 1.8969
Profiling... [13312/50176]	Loss: 1.7759
Profile done
epoch 1 train time consumed: 13.73s
Validation Epoch: 8, Average loss: 0.0024, Accuracy: 0.3856
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.16365169956325,
                        "time": 9.480648693999683,
                        "accuracy": 0.38564453125,
                        "total_cost": 3003.2596629482127
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6735
Profiling... [2048/50176]	Loss: 1.7877
Profiling... [3072/50176]	Loss: 1.7458
Profiling... [4096/50176]	Loss: 1.6639
Profiling... [5120/50176]	Loss: 1.8105
Profiling... [6144/50176]	Loss: 1.8004
Profiling... [7168/50176]	Loss: 1.7741
Profiling... [8192/50176]	Loss: 1.7659
Profiling... [9216/50176]	Loss: 1.7814
Profiling... [10240/50176]	Loss: 1.9352
Profiling... [11264/50176]	Loss: 1.7420
Profiling... [12288/50176]	Loss: 1.9154
Profiling... [13312/50176]	Loss: 1.7742
Profile done
epoch 1 train time consumed: 15.70s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4351
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.16508290344788,
                        "time": 11.050973284000065,
                        "accuracy": 0.43505859375,
                        "total_cost": 3103.1292952218705
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7179
Profiling... [2048/50176]	Loss: 1.6976
Profiling... [3072/50176]	Loss: 1.8017
Profiling... [4096/50176]	Loss: 1.8185
Profiling... [5120/50176]	Loss: 1.8743
Profiling... [6144/50176]	Loss: 1.8092
Profiling... [7168/50176]	Loss: 1.8204
Profiling... [8192/50176]	Loss: 1.7970
Profiling... [9216/50176]	Loss: 1.7875
Profiling... [10240/50176]	Loss: 1.7381
Profiling... [11264/50176]	Loss: 1.7865
Profiling... [12288/50176]	Loss: 1.7642
Profiling... [13312/50176]	Loss: 1.7624
Profile done
epoch 1 train time consumed: 39.34s
Validation Epoch: 8, Average loss: 0.0029, Accuracy: 0.3224
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 122.0409659749368,
                        "time": 29.31354788600038,
                        "accuracy": 0.32236328125,
                        "total_cost": 11097.584334940604
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6983
Profiling... [2048/50176]	Loss: 1.7356
Profiling... [3072/50176]	Loss: 1.7730
Profiling... [4096/50176]	Loss: 1.9040
Profiling... [5120/50176]	Loss: 1.8550
Profiling... [6144/50176]	Loss: 1.7890
Profiling... [7168/50176]	Loss: 1.7956
Profiling... [8192/50176]	Loss: 1.7977
Profiling... [9216/50176]	Loss: 1.7880
Profiling... [10240/50176]	Loss: 1.7330
Profiling... [11264/50176]	Loss: 1.7862
Profiling... [12288/50176]	Loss: 1.7238
Profiling... [13312/50176]	Loss: 1.7692
Profile done
epoch 1 train time consumed: 13.18s
Validation Epoch: 8, Average loss: 0.0021, Accuracy: 0.4175
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.06423723505061,
                        "time": 9.051368567999816,
                        "accuracy": 0.41748046875,
                        "total_cost": 2646.4672790425234
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7073
Profiling... [2048/50176]	Loss: 1.8116
Profiling... [3072/50176]	Loss: 1.8354
Profiling... [4096/50176]	Loss: 1.7559
Profiling... [5120/50176]	Loss: 1.8109
Profiling... [6144/50176]	Loss: 1.7509
Profiling... [7168/50176]	Loss: 1.8354
Profiling... [8192/50176]	Loss: 1.8254
Profiling... [9216/50176]	Loss: 1.7198
Profiling... [10240/50176]	Loss: 1.7460
Profiling... [11264/50176]	Loss: 1.7326
Profiling... [12288/50176]	Loss: 1.7801
Profiling... [13312/50176]	Loss: 1.7586
Profile done
epoch 1 train time consumed: 13.60s
Validation Epoch: 8, Average loss: 0.0034, Accuracy: 0.2844
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.10614478710608,
                        "time": 9.349713112000245,
                        "accuracy": 0.284375,
                        "total_cost": 4014.6195093514057
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7336
Profiling... [2048/50176]	Loss: 1.7315
Profiling... [3072/50176]	Loss: 1.9101
Profiling... [4096/50176]	Loss: 1.7119
Profiling... [5120/50176]	Loss: 1.6914
Profiling... [6144/50176]	Loss: 1.8103
Profiling... [7168/50176]	Loss: 1.8350
Profiling... [8192/50176]	Loss: 1.8279
Profiling... [9216/50176]	Loss: 1.7833
Profiling... [10240/50176]	Loss: 1.8256
Profiling... [11264/50176]	Loss: 1.8478
Profiling... [12288/50176]	Loss: 1.8160
Profiling... [13312/50176]	Loss: 1.7624
Profile done
epoch 1 train time consumed: 15.62s
Validation Epoch: 8, Average loss: 0.0024, Accuracy: 0.3788
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 122.10740479548207,
                        "time": 10.985025432999464,
                        "accuracy": 0.37880859375,
                        "total_cost": 3540.978133461185
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6574
Profiling... [2048/50176]	Loss: 1.8221
Profiling... [3072/50176]	Loss: 1.7555
Profiling... [4096/50176]	Loss: 1.7902
Profiling... [5120/50176]	Loss: 1.8066
Profiling... [6144/50176]	Loss: 1.7975
Profiling... [7168/50176]	Loss: 1.7580
Profiling... [8192/50176]	Loss: 1.7872
Profiling... [9216/50176]	Loss: 1.6763
Profiling... [10240/50176]	Loss: 1.7798
Profiling... [11264/50176]	Loss: 1.8535
Profiling... [12288/50176]	Loss: 1.8597
Profiling... [13312/50176]	Loss: 1.8402
Profile done
epoch 1 train time consumed: 38.85s
Validation Epoch: 8, Average loss: 0.0031, Accuracy: 0.2876
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.99159236768595,
                        "time": 28.863249871000335,
                        "accuracy": 0.28759765625,
                        "total_cost": 12243.054615191208
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7605
Profiling... [2048/50176]	Loss: 1.8367
Profiling... [3072/50176]	Loss: 1.6801
Profiling... [4096/50176]	Loss: 1.8060
Profiling... [5120/50176]	Loss: 1.8466
Profiling... [6144/50176]	Loss: 1.7855
Profiling... [7168/50176]	Loss: 1.8028
Profiling... [8192/50176]	Loss: 1.7630
Profiling... [9216/50176]	Loss: 1.8012
Profiling... [10240/50176]	Loss: 1.7540
Profiling... [11264/50176]	Loss: 1.8280
Profiling... [12288/50176]	Loss: 1.8461
Profiling... [13312/50176]	Loss: 1.7830
Profile done
epoch 1 train time consumed: 13.22s
Validation Epoch: 8, Average loss: 0.0030, Accuracy: 0.2840
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.01360328333477,
                        "time": 9.016318397999385,
                        "accuracy": 0.283984375,
                        "total_cost": 3873.852200811152
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6660
Profiling... [2048/50176]	Loss: 1.7733
Profiling... [3072/50176]	Loss: 1.8105
Profiling... [4096/50176]	Loss: 1.8137
Profiling... [5120/50176]	Loss: 1.7888
Profiling... [6144/50176]	Loss: 1.7760
Profiling... [7168/50176]	Loss: 1.8950
Profiling... [8192/50176]	Loss: 1.7838
Profiling... [9216/50176]	Loss: 1.7558
Profiling... [10240/50176]	Loss: 1.8227
Profiling... [11264/50176]	Loss: 1.7574
Profiling... [12288/50176]	Loss: 1.7496
Profiling... [13312/50176]	Loss: 1.7847
Profile done
epoch 1 train time consumed: 13.63s
Validation Epoch: 8, Average loss: 0.0028, Accuracy: 0.3370
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.05581387056652,
                        "time": 9.365604058999452,
                        "accuracy": 0.33701171875,
                        "total_cost": 3391.948594697523
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7570
Profiling... [2048/50176]	Loss: 1.7331
Profiling... [3072/50176]	Loss: 1.8099
Profiling... [4096/50176]	Loss: 1.8356
Profiling... [5120/50176]	Loss: 1.8572
Profiling... [6144/50176]	Loss: 1.7749
Profiling... [7168/50176]	Loss: 1.7683
Profiling... [8192/50176]	Loss: 1.8277
Profiling... [9216/50176]	Loss: 1.8392
Profiling... [10240/50176]	Loss: 1.7551
Profiling... [11264/50176]	Loss: 1.7998
Profiling... [12288/50176]	Loss: 1.8156
Profiling... [13312/50176]	Loss: 1.7625
Profile done
epoch 1 train time consumed: 15.76s
Validation Epoch: 8, Average loss: 0.0030, Accuracy: 0.3145
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 122.05724574666674,
                        "time": 10.984327291999762,
                        "accuracy": 0.314453125,
                        "total_cost": 4263.64576800257
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6781
Profiling... [2048/50176]	Loss: 1.8428
Profiling... [3072/50176]	Loss: 1.8181
Profiling... [4096/50176]	Loss: 1.8889
Profiling... [5120/50176]	Loss: 1.8305
Profiling... [6144/50176]	Loss: 1.8248
Profiling... [7168/50176]	Loss: 1.6992
Profiling... [8192/50176]	Loss: 1.7769
Profiling... [9216/50176]	Loss: 1.7428
Profiling... [10240/50176]	Loss: 1.7453
Profiling... [11264/50176]	Loss: 1.7358
Profiling... [12288/50176]	Loss: 1.8267
Profiling... [13312/50176]	Loss: 1.8040
Profile done
epoch 1 train time consumed: 38.72s
Validation Epoch: 8, Average loss: 0.0024, Accuracy: 0.3810
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.4,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.94529341203983,
                        "time": 28.804139052999744,
                        "accuracy": 0.38095703125,
                        "total_cost": 9220.276567081339
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.0 bs: 128 pl: 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[GPU_0] Set GPU power limit to 175W.
[GPU_0] Set GPU power limit to 175W.
[Training Loop] Model's accuracy 0.5067246835443038 surpasses threshold 0.5! Reprofiling...
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8712
Profiling... [256/50048]	Loss: 1.5994
Profiling... [384/50048]	Loss: 1.5225
Profiling... [512/50048]	Loss: 1.6499
Profiling... [640/50048]	Loss: 1.5822
Profiling... [768/50048]	Loss: 1.7303
Profiling... [896/50048]	Loss: 1.8373
Profiling... [1024/50048]	Loss: 1.5823
Profiling... [1152/50048]	Loss: 1.6914
Profiling... [1280/50048]	Loss: 1.7807
Profiling... [1408/50048]	Loss: 1.7504
Profiling... [1536/50048]	Loss: 1.6790
Profiling... [1664/50048]	Loss: 1.8908
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5040
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.94001194918971,
                        "time": 2.1513234350004495,
                        "accuracy": 0.5039556962025317,
                        "total_cost": 520.546562619067
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6549
Profiling... [256/50048]	Loss: 2.0189
Profiling... [384/50048]	Loss: 1.9015
Profiling... [512/50048]	Loss: 1.5184
Profiling... [640/50048]	Loss: 1.6177
Profiling... [768/50048]	Loss: 1.6078
Profiling... [896/50048]	Loss: 1.7579
Profiling... [1024/50048]	Loss: 1.8922
Profiling... [1152/50048]	Loss: 1.7253
Profiling... [1280/50048]	Loss: 1.8633
Profiling... [1408/50048]	Loss: 1.5316
Profiling... [1536/50048]	Loss: 1.9228
Profiling... [1664/50048]	Loss: 1.9135
Profile done
epoch 1 train time consumed: 3.70s
Validation Epoch: 8, Average loss: 0.0142, Accuracy: 0.5048
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.95659314327283,
                        "time": 2.1851893679995555,
                        "accuracy": 0.5048457278481012,
                        "total_cost": 527.8805702289941
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9572
Profiling... [256/50048]	Loss: 1.5792
Profiling... [384/50048]	Loss: 1.8655
Profiling... [512/50048]	Loss: 1.7480
Profiling... [640/50048]	Loss: 1.4973
Profiling... [768/50048]	Loss: 1.6704
Profiling... [896/50048]	Loss: 1.8255
Profiling... [1024/50048]	Loss: 1.9078
Profiling... [1152/50048]	Loss: 1.7044
Profiling... [1280/50048]	Loss: 1.5355
Profiling... [1408/50048]	Loss: 1.5082
Profiling... [1536/50048]	Loss: 1.9287
Profiling... [1664/50048]	Loss: 1.8479
Profile done
epoch 1 train time consumed: 3.86s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5022
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.96289934836226,
                        "time": 2.4641317420000632,
                        "accuracy": 0.5021756329113924,
                        "total_cost": 598.4612393243829
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5930
Profiling... [256/50048]	Loss: 1.7790
Profiling... [384/50048]	Loss: 1.7278
Profiling... [512/50048]	Loss: 1.8786
Profiling... [640/50048]	Loss: 1.7512
Profiling... [768/50048]	Loss: 1.4541
Profiling... [896/50048]	Loss: 1.4819
Profiling... [1024/50048]	Loss: 1.7192
Profiling... [1152/50048]	Loss: 1.7958
Profiling... [1280/50048]	Loss: 1.3846
Profiling... [1408/50048]	Loss: 1.7134
Profiling... [1536/50048]	Loss: 1.6954
Profiling... [1664/50048]	Loss: 1.6496
Profile done
epoch 1 train time consumed: 7.73s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5057
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.9383249948792,
                        "time": 5.544807647999733,
                        "accuracy": 0.5057357594936709,
                        "total_cost": 1336.9126946704362
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5720
Profiling... [256/50048]	Loss: 1.8438
Profiling... [384/50048]	Loss: 1.8971
Profiling... [512/50048]	Loss: 1.5275
Profiling... [640/50048]	Loss: 1.8908
Profiling... [768/50048]	Loss: 1.7405
Profiling... [896/50048]	Loss: 1.7333
Profiling... [1024/50048]	Loss: 1.9485
Profiling... [1152/50048]	Loss: 1.6200
Profiling... [1280/50048]	Loss: 1.9194
Profiling... [1408/50048]	Loss: 1.6695
Profiling... [1536/50048]	Loss: 1.7406
Profiling... [1664/50048]	Loss: 1.9484
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 8, Average loss: 0.0142, Accuracy: 0.5048
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.91324263942201,
                        "time": 2.155389018000278,
                        "accuracy": 0.5048457278481012,
                        "total_cost": 520.496559322923
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8278
Profiling... [256/50048]	Loss: 1.7225
Profiling... [384/50048]	Loss: 1.9758
Profiling... [512/50048]	Loss: 1.6539
Profiling... [640/50048]	Loss: 1.8241
Profiling... [768/50048]	Loss: 1.8336
Profiling... [896/50048]	Loss: 1.6180
Profiling... [1024/50048]	Loss: 1.8856
Profiling... [1152/50048]	Loss: 1.8954
Profiling... [1280/50048]	Loss: 1.7829
Profiling... [1408/50048]	Loss: 1.9524
Profiling... [1536/50048]	Loss: 1.5277
Profiling... [1664/50048]	Loss: 1.7315
Profile done
epoch 1 train time consumed: 3.56s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5049
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.92910714289008,
                        "time": 2.176763301000392,
                        "accuracy": 0.5049446202531646,
                        "total_cost": 525.6235933740984
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5869
Profiling... [256/50048]	Loss: 1.7830
Profiling... [384/50048]	Loss: 1.6934
Profiling... [512/50048]	Loss: 1.7172
Profiling... [640/50048]	Loss: 1.8484
Profiling... [768/50048]	Loss: 1.5880
Profiling... [896/50048]	Loss: 1.8775
Profiling... [1024/50048]	Loss: 1.5674
Profiling... [1152/50048]	Loss: 1.4908
Profiling... [1280/50048]	Loss: 1.5427
Profiling... [1408/50048]	Loss: 1.8592
Profiling... [1536/50048]	Loss: 1.6427
Profiling... [1664/50048]	Loss: 1.6068
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 8, Average loss: 0.0144, Accuracy: 0.5008
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.93622047856235,
                        "time": 2.4717737169994507,
                        "accuracy": 0.5007911392405063,
                        "total_cost": 601.845203144485
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.6210
Profiling... [256/50048]	Loss: 2.1360
Profiling... [384/50048]	Loss: 1.5066
Profiling... [512/50048]	Loss: 1.9751
Profiling... [640/50048]	Loss: 2.0131
Profiling... [768/50048]	Loss: 1.5611
Profiling... [896/50048]	Loss: 1.7867
Profiling... [1024/50048]	Loss: 1.5304
Profiling... [1152/50048]	Loss: 1.7270
Profiling... [1280/50048]	Loss: 1.5411
Profiling... [1408/50048]	Loss: 1.7264
Profiling... [1536/50048]	Loss: 1.5410
Profiling... [1664/50048]	Loss: 1.7440
Profile done
epoch 1 train time consumed: 7.59s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.4990
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.91099028566215,
                        "time": 5.444298293999964,
                        "accuracy": 0.4990110759493671,
                        "total_cost": 1330.0702698218704
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5952
Profiling... [256/50048]	Loss: 1.9045
Profiling... [384/50048]	Loss: 1.6797
Profiling... [512/50048]	Loss: 1.6786
Profiling... [640/50048]	Loss: 1.7668
Profiling... [768/50048]	Loss: 1.6356
Profiling... [896/50048]	Loss: 1.6191
Profiling... [1024/50048]	Loss: 1.7502
Profiling... [1152/50048]	Loss: 1.6767
Profiling... [1280/50048]	Loss: 1.7283
Profiling... [1408/50048]	Loss: 1.6474
Profiling... [1536/50048]	Loss: 1.6388
Profiling... [1664/50048]	Loss: 1.7341
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5020
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.88325542287944,
                        "time": 2.1557616540003437,
                        "accuracy": 0.5019778481012658,
                        "total_cost": 523.4319587990409
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.6287
Profiling... [256/50048]	Loss: 1.7320
Profiling... [384/50048]	Loss: 1.6621
Profiling... [512/50048]	Loss: 1.8320
Profiling... [640/50048]	Loss: 1.6295
Profiling... [768/50048]	Loss: 1.7237
Profiling... [896/50048]	Loss: 1.7650
Profiling... [1024/50048]	Loss: 1.6000
Profiling... [1152/50048]	Loss: 1.8530
Profiling... [1280/50048]	Loss: 1.7144
Profiling... [1408/50048]	Loss: 1.8240
Profiling... [1536/50048]	Loss: 1.5960
Profiling... [1664/50048]	Loss: 1.8658
Profile done
epoch 1 train time consumed: 3.69s
Validation Epoch: 8, Average loss: 0.0142, Accuracy: 0.5053
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.89921711307589,
                        "time": 2.1966248999997333,
                        "accuracy": 0.5053401898734177,
                        "total_cost": 529.8744508489002
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7980
Profiling... [256/50048]	Loss: 1.7434
Profiling... [384/50048]	Loss: 1.6419
Profiling... [512/50048]	Loss: 1.6019
Profiling... [640/50048]	Loss: 1.7317
Profiling... [768/50048]	Loss: 1.7099
Profiling... [896/50048]	Loss: 1.6958
Profiling... [1024/50048]	Loss: 1.9878
Profiling... [1152/50048]	Loss: 1.5933
Profiling... [1280/50048]	Loss: 2.1105
Profiling... [1408/50048]	Loss: 1.8814
Profiling... [1536/50048]	Loss: 1.7540
Profiling... [1664/50048]	Loss: 1.5080
Profile done
epoch 1 train time consumed: 3.99s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5022
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.9066344765099,
                        "time": 2.465364514000612,
                        "accuracy": 0.5021756329113924,
                        "total_cost": 598.4844165321365
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7261
Profiling... [256/50048]	Loss: 1.5010
Profiling... [384/50048]	Loss: 1.8054
Profiling... [512/50048]	Loss: 1.7608
Profiling... [640/50048]	Loss: 1.9774
Profiling... [768/50048]	Loss: 1.6459
Profiling... [896/50048]	Loss: 1.6892
Profiling... [1024/50048]	Loss: 1.8189
Profiling... [1152/50048]	Loss: 1.5639
Profiling... [1280/50048]	Loss: 1.8944
Profiling... [1408/50048]	Loss: 1.8024
Profiling... [1536/50048]	Loss: 1.5316
Profiling... [1664/50048]	Loss: 1.7038
Profile done
epoch 1 train time consumed: 7.63s
Validation Epoch: 8, Average loss: 0.0143, Accuracy: 0.5033
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.8826932288901,
                        "time": 5.446783346999837,
                        "accuracy": 0.5032634493670886,
                        "total_cost": 1319.127436338759
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7034
Profiling... [256/50048]	Loss: 2.0345
Profiling... [384/50048]	Loss: 1.9077
Profiling... [512/50048]	Loss: 1.8153
Profiling... [640/50048]	Loss: 1.7092
Profiling... [768/50048]	Loss: 1.9423
Profiling... [896/50048]	Loss: 1.9770
Profiling... [1024/50048]	Loss: 2.0827
Profiling... [1152/50048]	Loss: 1.9522
Profiling... [1280/50048]	Loss: 1.9839
Profiling... [1408/50048]	Loss: 1.8815
Profiling... [1536/50048]	Loss: 1.9560
Profiling... [1664/50048]	Loss: 2.0290
Profile done
epoch 1 train time consumed: 3.60s
Validation Epoch: 8, Average loss: 0.0163, Accuracy: 0.4537
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.85436851079618,
                        "time": 2.1573117269999784,
                        "accuracy": 0.4537183544303797,
                        "total_cost": 579.3855496644989
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8196
Profiling... [256/50048]	Loss: 1.7817
Profiling... [384/50048]	Loss: 1.8270
Profiling... [512/50048]	Loss: 1.9406
Profiling... [640/50048]	Loss: 1.6942
Profiling... [768/50048]	Loss: 1.6740
Profiling... [896/50048]	Loss: 1.6915
Profiling... [1024/50048]	Loss: 2.1317
Profiling... [1152/50048]	Loss: 2.0353
Profiling... [1280/50048]	Loss: 2.0102
Profiling... [1408/50048]	Loss: 1.8799
Profiling... [1536/50048]	Loss: 1.9747
Profiling... [1664/50048]	Loss: 1.9487
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 8, Average loss: 0.0173, Accuracy: 0.4243
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.87182312944343,
                        "time": 2.1877038590000666,
                        "accuracy": 0.4243473101265823,
                        "total_cost": 628.3048140074806
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.9586
Profiling... [256/50048]	Loss: 1.7137
Profiling... [384/50048]	Loss: 1.9281
Profiling... [512/50048]	Loss: 1.5910
Profiling... [640/50048]	Loss: 1.7988
Profiling... [768/50048]	Loss: 1.7738
Profiling... [896/50048]	Loss: 2.0661
Profiling... [1024/50048]	Loss: 2.0533
Profiling... [1152/50048]	Loss: 1.5620
Profiling... [1280/50048]	Loss: 1.8725
Profiling... [1408/50048]	Loss: 2.0176
Profiling... [1536/50048]	Loss: 2.0224
Profiling... [1664/50048]	Loss: 1.6845
Profile done
epoch 1 train time consumed: 4.05s
Validation Epoch: 8, Average loss: 0.0161, Accuracy: 0.4516
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.87796838225634,
                        "time": 2.4813077770004384,
                        "accuracy": 0.4516416139240506,
                        "total_cost": 669.5945224453154
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5682
Profiling... [256/50048]	Loss: 1.6111
Profiling... [384/50048]	Loss: 1.5107
Profiling... [512/50048]	Loss: 1.7347
Profiling... [640/50048]	Loss: 1.8148
Profiling... [768/50048]	Loss: 1.9035
Profiling... [896/50048]	Loss: 1.6958
Profiling... [1024/50048]	Loss: 1.7765
Profiling... [1152/50048]	Loss: 1.7699
Profiling... [1280/50048]	Loss: 1.8379
Profiling... [1408/50048]	Loss: 2.0110
Profiling... [1536/50048]	Loss: 1.8303
Profiling... [1664/50048]	Loss: 1.8985
Profile done
epoch 1 train time consumed: 7.65s
Validation Epoch: 8, Average loss: 0.0166, Accuracy: 0.4403
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.85348046520262,
                        "time": 5.429119678999996,
                        "accuracy": 0.44026898734177217,
                        "total_cost": 1502.620324775952
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6778
Profiling... [256/50048]	Loss: 1.6311
Profiling... [384/50048]	Loss: 1.7220
Profiling... [512/50048]	Loss: 1.8193
Profiling... [640/50048]	Loss: 1.7664
Profiling... [768/50048]	Loss: 1.7732
Profiling... [896/50048]	Loss: 1.8876
Profiling... [1024/50048]	Loss: 1.8597
Profiling... [1152/50048]	Loss: 1.8443
Profiling... [1280/50048]	Loss: 1.8255
Profiling... [1408/50048]	Loss: 1.8345
Profiling... [1536/50048]	Loss: 2.1971
Profiling... [1664/50048]	Loss: 1.7012
Profile done
epoch 1 train time consumed: 3.64s
Validation Epoch: 8, Average loss: 0.0154, Accuracy: 0.4676
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.82592544853448,
                        "time": 2.154737135000687,
                        "accuracy": 0.4675632911392405,
                        "total_cost": 561.427405753308
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7746
Profiling... [256/50048]	Loss: 1.6019
Profiling... [384/50048]	Loss: 1.8982
Profiling... [512/50048]	Loss: 1.7285
Profiling... [640/50048]	Loss: 1.9146
Profiling... [768/50048]	Loss: 1.6325
Profiling... [896/50048]	Loss: 2.0747
Profiling... [1024/50048]	Loss: 1.6449
Profiling... [1152/50048]	Loss: 2.1129
Profiling... [1280/50048]	Loss: 1.7900
Profiling... [1408/50048]	Loss: 1.8733
Profiling... [1536/50048]	Loss: 1.8271
Profiling... [1664/50048]	Loss: 1.7633
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 8, Average loss: 0.0165, Accuracy: 0.4408
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.84319638022883,
                        "time": 2.1947191819999716,
                        "accuracy": 0.4407634493670886,
                        "total_cost": 606.700942820612
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.8769
Profiling... [256/50048]	Loss: 1.5551
Profiling... [384/50048]	Loss: 1.8430
Profiling... [512/50048]	Loss: 1.6782
Profiling... [640/50048]	Loss: 1.9136
Profiling... [768/50048]	Loss: 1.8443
Profiling... [896/50048]	Loss: 1.9183
Profiling... [1024/50048]	Loss: 2.0200
Profiling... [1152/50048]	Loss: 1.7618
Profiling... [1280/50048]	Loss: 1.9979
Profiling... [1408/50048]	Loss: 1.9226
Profiling... [1536/50048]	Loss: 1.7570
Profiling... [1664/50048]	Loss: 2.0761
Profile done
epoch 1 train time consumed: 3.95s
Validation Epoch: 8, Average loss: 0.0167, Accuracy: 0.4506
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.84912775103811,
                        "time": 2.481141624999509,
                        "accuracy": 0.45055379746835444,
                        "total_cost": 671.0074236012135
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7779
Profiling... [256/50048]	Loss: 1.9136
Profiling... [384/50048]	Loss: 1.8246
Profiling... [512/50048]	Loss: 1.6684
Profiling... [640/50048]	Loss: 1.5894
Profiling... [768/50048]	Loss: 1.6712
Profiling... [896/50048]	Loss: 1.8772
Profiling... [1024/50048]	Loss: 1.8095
Profiling... [1152/50048]	Loss: 1.8047
Profiling... [1280/50048]	Loss: 1.8408
Profiling... [1408/50048]	Loss: 1.9650
Profiling... [1536/50048]	Loss: 1.7478
Profiling... [1664/50048]	Loss: 1.7470
Profile done
epoch 1 train time consumed: 7.71s
Validation Epoch: 8, Average loss: 0.0179, Accuracy: 0.4212
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.82485535465634,
                        "time": 5.54173038800036,
                        "accuracy": 0.42118275316455694,
                        "total_cost": 1602.9158313348041
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.5950
Profiling... [256/50048]	Loss: 1.7044
Profiling... [384/50048]	Loss: 1.5823
Profiling... [512/50048]	Loss: 1.7918
Profiling... [640/50048]	Loss: 2.1638
Profiling... [768/50048]	Loss: 1.6619
Profiling... [896/50048]	Loss: 1.8158
Profiling... [1024/50048]	Loss: 1.9640
Profiling... [1152/50048]	Loss: 1.9673
Profiling... [1280/50048]	Loss: 1.8811
Profiling... [1408/50048]	Loss: 1.7684
Profiling... [1536/50048]	Loss: 1.6253
Profiling... [1664/50048]	Loss: 1.5802
Profile done
epoch 1 train time consumed: 3.72s
Validation Epoch: 8, Average loss: 0.0167, Accuracy: 0.4344
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.8013223148661,
                        "time": 2.148516782000115,
                        "accuracy": 0.434434335443038,
                        "total_cost": 602.3745448122099
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7097
Profiling... [256/50048]	Loss: 1.8424
Profiling... [384/50048]	Loss: 1.9419
Profiling... [512/50048]	Loss: 1.7035
Profiling... [640/50048]	Loss: 1.6117
Profiling... [768/50048]	Loss: 1.7540
Profiling... [896/50048]	Loss: 1.8817
Profiling... [1024/50048]	Loss: 1.8558
Profiling... [1152/50048]	Loss: 1.5684
Profiling... [1280/50048]	Loss: 1.9726
Profiling... [1408/50048]	Loss: 2.2191
Profiling... [1536/50048]	Loss: 1.6463
Profiling... [1664/50048]	Loss: 1.8473
Profile done
epoch 1 train time consumed: 3.75s
Validation Epoch: 8, Average loss: 0.0167, Accuracy: 0.4470
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.81726606974013,
                        "time": 2.194513460000053,
                        "accuracy": 0.44699367088607594,
                        "total_cost": 598.0613316526936
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.7172
Profiling... [256/50048]	Loss: 1.6540
Profiling... [384/50048]	Loss: 1.9870
Profiling... [512/50048]	Loss: 1.6960
Profiling... [640/50048]	Loss: 1.8787
Profiling... [768/50048]	Loss: 2.0320
Profiling... [896/50048]	Loss: 1.8845
Profiling... [1024/50048]	Loss: 1.6399
Profiling... [1152/50048]	Loss: 1.7655
Profiling... [1280/50048]	Loss: 1.9404
Profiling... [1408/50048]	Loss: 1.6725
Profiling... [1536/50048]	Loss: 1.8092
Profiling... [1664/50048]	Loss: 1.7371
Profile done
epoch 1 train time consumed: 3.87s
Validation Epoch: 8, Average loss: 0.0170, Accuracy: 0.4349
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.82364533170751,
                        "time": 2.4685921600002985,
                        "accuracy": 0.43492879746835444,
                        "total_cost": 691.4531700798486
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7790
Profiling... [256/50048]	Loss: 2.0258
Profiling... [384/50048]	Loss: 1.7163
Profiling... [512/50048]	Loss: 1.7344
Profiling... [640/50048]	Loss: 1.5842
Profiling... [768/50048]	Loss: 1.7359
Profiling... [896/50048]	Loss: 2.0169
Profiling... [1024/50048]	Loss: 1.6857
Profiling... [1152/50048]	Loss: 2.0235
Profiling... [1280/50048]	Loss: 1.7669
Profiling... [1408/50048]	Loss: 1.7828
Profiling... [1536/50048]	Loss: 1.7418
Profiling... [1664/50048]	Loss: 1.8562
Profile done
epoch 1 train time consumed: 7.60s
Validation Epoch: 8, Average loss: 0.0163, Accuracy: 0.4452
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.80104277442433,
                        "time": 5.438255146999836,
                        "accuracy": 0.4452136075949367,
                        "total_cost": 1487.7917846136684
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.6042
Profiling... [256/50048]	Loss: 1.6336
Profiling... [384/50048]	Loss: 1.9882
Profiling... [512/50048]	Loss: 1.8228
Profiling... [640/50048]	Loss: 2.1007
Profiling... [768/50048]	Loss: 1.9276
Profiling... [896/50048]	Loss: 2.1349
Profiling... [1024/50048]	Loss: 1.9601
Profiling... [1152/50048]	Loss: 2.2597
Profiling... [1280/50048]	Loss: 1.7030
Profiling... [1408/50048]	Loss: 1.9901
Profiling... [1536/50048]	Loss: 1.8822
Profiling... [1664/50048]	Loss: 2.0441
Profile done
epoch 1 train time consumed: 3.76s
Validation Epoch: 8, Average loss: 0.0234, Accuracy: 0.3260
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.77309242562369,
                        "time": 2.15779182300048,
                        "accuracy": 0.3260482594936709,
                        "total_cost": 805.8959845562158
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.8035
Profiling... [256/50048]	Loss: 1.9520
Profiling... [384/50048]	Loss: 1.7588
Profiling... [512/50048]	Loss: 2.0483
Profiling... [640/50048]	Loss: 2.0413
Profiling... [768/50048]	Loss: 2.0253
Profiling... [896/50048]	Loss: 2.0496
Profiling... [1024/50048]	Loss: 1.8350
Profiling... [1152/50048]	Loss: 1.8252
Profiling... [1280/50048]	Loss: 1.9602
Profiling... [1408/50048]	Loss: 2.2484
Profiling... [1536/50048]	Loss: 2.0661
Profiling... [1664/50048]	Loss: 2.1339
Profile done
epoch 1 train time consumed: 3.65s
Validation Epoch: 8, Average loss: 0.0317, Accuracy: 0.2183
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.78796206519493,
                        "time": 2.1959067899997535,
                        "accuracy": 0.21825553797468356,
                        "total_cost": 1225.3297914952104
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6436
Profiling... [256/50048]	Loss: 2.0009
Profiling... [384/50048]	Loss: 1.8762
Profiling... [512/50048]	Loss: 1.8929
Profiling... [640/50048]	Loss: 1.9870
Profiling... [768/50048]	Loss: 2.1899
Profiling... [896/50048]	Loss: 2.3032
Profiling... [1024/50048]	Loss: 2.0642
Profiling... [1152/50048]	Loss: 1.9326
Profiling... [1280/50048]	Loss: 1.8091
Profiling... [1408/50048]	Loss: 1.9512
Profiling... [1536/50048]	Loss: 2.0453
Profiling... [1664/50048]	Loss: 1.9193
Profile done
epoch 1 train time consumed: 3.94s
Validation Epoch: 8, Average loss: 0.0243, Accuracy: 0.2997
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.7951662131116,
                        "time": 2.46999194,
                        "accuracy": 0.29974287974683544,
                        "total_cost": 1003.6371143542468
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.5062
Profiling... [256/50048]	Loss: 1.7864
Profiling... [384/50048]	Loss: 1.9951
Profiling... [512/50048]	Loss: 1.7814
Profiling... [640/50048]	Loss: 1.7591
Profiling... [768/50048]	Loss: 1.7097
Profiling... [896/50048]	Loss: 1.9763
Profiling... [1024/50048]	Loss: 2.2960
Profiling... [1152/50048]	Loss: 2.2508
Profiling... [1280/50048]	Loss: 1.9767
Profiling... [1408/50048]	Loss: 2.0066
Profiling... [1536/50048]	Loss: 1.9198
Profiling... [1664/50048]	Loss: 2.0633
Profile done
epoch 1 train time consumed: 7.76s
Validation Epoch: 8, Average loss: 0.0218, Accuracy: 0.3484
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.77032757960423,
                        "time": 5.566757028000211,
                        "accuracy": 0.34839794303797467,
                        "total_cost": 1945.665410492288
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.7184
Profiling... [256/50048]	Loss: 1.7255
Profiling... [384/50048]	Loss: 1.9171
Profiling... [512/50048]	Loss: 1.7459
Profiling... [640/50048]	Loss: 1.8015
Profiling... [768/50048]	Loss: 1.9945
Profiling... [896/50048]	Loss: 2.0450
Profiling... [1024/50048]	Loss: 1.9818
Profiling... [1152/50048]	Loss: 1.9954
Profiling... [1280/50048]	Loss: 2.0853
Profiling... [1408/50048]	Loss: 2.2904
Profiling... [1536/50048]	Loss: 1.8234
Profiling... [1664/50048]	Loss: 1.9969
Profile done
epoch 1 train time consumed: 3.67s
Validation Epoch: 8, Average loss: 0.0210, Accuracy: 0.3337
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.74635816614521,
                        "time": 2.1556832080004824,
                        "accuracy": 0.3336629746835443,
                        "total_cost": 786.5618898317491
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.9328
Profiling... [256/50048]	Loss: 1.8169
Profiling... [384/50048]	Loss: 1.8914
Profiling... [512/50048]	Loss: 1.6311
Profiling... [640/50048]	Loss: 2.2759
Profiling... [768/50048]	Loss: 1.9372
Profiling... [896/50048]	Loss: 1.8612
Profiling... [1024/50048]	Loss: 2.0433
Profiling... [1152/50048]	Loss: 1.8386
Profiling... [1280/50048]	Loss: 2.1383
Profiling... [1408/50048]	Loss: 2.1066
Profiling... [1536/50048]	Loss: 2.3035
Profiling... [1664/50048]	Loss: 1.9525
Profile done
epoch 1 train time consumed: 3.61s
Validation Epoch: 8, Average loss: 0.0263, Accuracy: 0.2922
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.7635268932383,
                        "time": 2.1817446869999912,
                        "accuracy": 0.29222705696202533,
                        "total_cost": 909.0771081619082
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.5758
Profiling... [256/50048]	Loss: 1.7315
Profiling... [384/50048]	Loss: 2.0312
Profiling... [512/50048]	Loss: 1.9550
Profiling... [640/50048]	Loss: 2.1913
Profiling... [768/50048]	Loss: 2.0093
Profiling... [896/50048]	Loss: 2.2410
Profiling... [1024/50048]	Loss: 2.2663
Profiling... [1152/50048]	Loss: 2.1662
Profiling... [1280/50048]	Loss: 2.0037
Profiling... [1408/50048]	Loss: 2.1399
Profiling... [1536/50048]	Loss: 1.8441
Profiling... [1664/50048]	Loss: 1.8309
Profile done
epoch 1 train time consumed: 4.01s
Validation Epoch: 8, Average loss: 0.0315, Accuracy: 0.2376
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.76959844146937,
                        "time": 2.482778388999577,
                        "accuracy": 0.2376384493670886,
                        "total_cost": 1272.2138536622983
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.7574
Profiling... [256/50048]	Loss: 1.9836
Profiling... [384/50048]	Loss: 1.8672
Profiling... [512/50048]	Loss: 2.0293
Profiling... [640/50048]	Loss: 1.9554
Profiling... [768/50048]	Loss: 2.3724
Profiling... [896/50048]	Loss: 1.8504
Profiling... [1024/50048]	Loss: 2.1245
Profiling... [1152/50048]	Loss: 1.8640
Profiling... [1280/50048]	Loss: 2.1896
Profiling... [1408/50048]	Loss: 2.1058
Profiling... [1536/50048]	Loss: 1.9504
Profiling... [1664/50048]	Loss: 2.2984
Profile done
epoch 1 train time consumed: 7.94s
Validation Epoch: 8, Average loss: 0.0248, Accuracy: 0.2848
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.74509577246565,
                        "time": 5.669017731000167,
                        "accuracy": 0.2848101265822785,
                        "total_cost": 2423.28148538299
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [128/50048]	Loss: 1.8956
Profiling... [256/50048]	Loss: 1.9270
Profiling... [384/50048]	Loss: 1.6013
Profiling... [512/50048]	Loss: 1.7672
Profiling... [640/50048]	Loss: 2.0727
Profiling... [768/50048]	Loss: 1.7768
Profiling... [896/50048]	Loss: 1.9891
Profiling... [1024/50048]	Loss: 2.0604
Profiling... [1152/50048]	Loss: 1.7990
Profiling... [1280/50048]	Loss: 1.8793
Profiling... [1408/50048]	Loss: 2.0760
Profiling... [1536/50048]	Loss: 2.0213
Profiling... [1664/50048]	Loss: 2.4588
Profile done
epoch 1 train time consumed: 3.59s
Validation Epoch: 8, Average loss: 0.0307, Accuracy: 0.2490
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.71909331567961,
                        "time": 2.1619101639998917,
                        "accuracy": 0.24901107594936708,
                        "total_cost": 1056.7632142014681
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [128/50048]	Loss: 1.7324
Profiling... [256/50048]	Loss: 1.9396
Profiling... [384/50048]	Loss: 1.8234
Profiling... [512/50048]	Loss: 2.0456
Profiling... [640/50048]	Loss: 2.0758
Profiling... [768/50048]	Loss: 1.9478
Profiling... [896/50048]	Loss: 1.9498
Profiling... [1024/50048]	Loss: 1.9996
Profiling... [1152/50048]	Loss: 2.0038
Profiling... [1280/50048]	Loss: 2.0865
Profiling... [1408/50048]	Loss: 2.1511
Profiling... [1536/50048]	Loss: 1.9515
Profiling... [1664/50048]	Loss: 2.1127
Profile done
epoch 1 train time consumed: 3.63s
Validation Epoch: 8, Average loss: 0.0428, Accuracy: 0.1599
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.73723562445392,
                        "time": 2.192245598000227,
                        "accuracy": 0.15990901898734178,
                        "total_cost": 1668.9360024874597
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [128/50048]	Loss: 1.6444
Profiling... [256/50048]	Loss: 1.8435
Profiling... [384/50048]	Loss: 1.9428
Profiling... [512/50048]	Loss: 2.0929
Profiling... [640/50048]	Loss: 2.2117
Profiling... [768/50048]	Loss: 2.3293
Profiling... [896/50048]	Loss: 2.1058
Profiling... [1024/50048]	Loss: 2.1429
Profiling... [1152/50048]	Loss: 2.0466
Profiling... [1280/50048]	Loss: 1.8068
Profiling... [1408/50048]	Loss: 2.1297
Profiling... [1536/50048]	Loss: 2.0657
Profiling... [1664/50048]	Loss: 2.0324
Profile done
epoch 1 train time consumed: 3.98s
Validation Epoch: 8, Average loss: 0.0238, Accuracy: 0.2879
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.74296820476641,
                        "time": 2.499670031000278,
                        "accuracy": 0.2878757911392405,
                        "total_cost": 1057.1130274698278
                    },
                    
[Training Loop] Profiling with batch size 128 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [128/50048]	Loss: 1.8087
Profiling... [256/50048]	Loss: 1.7408
Profiling... [384/50048]	Loss: 1.8730
Profiling... [512/50048]	Loss: 1.9468
Profiling... [640/50048]	Loss: 1.8567
Profiling... [768/50048]	Loss: 2.0907
Profiling... [896/50048]	Loss: 2.1812
Profiling... [1024/50048]	Loss: 1.9604
Profiling... [1152/50048]	Loss: 2.1923
Profiling... [1280/50048]	Loss: 1.7763
Profiling... [1408/50048]	Loss: 1.8811
Profiling... [1536/50048]	Loss: 2.1201
Profiling... [1664/50048]	Loss: 2.2610
Profile done
epoch 1 train time consumed: 7.60s
Validation Epoch: 8, Average loss: 0.0744, Accuracy: 0.0875
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 128,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.71959924953883,
                        "time": 5.426940222000667,
                        "accuracy": 0.08751977848101265,
                        "total_cost": 7547.608099995747
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7319
Profiling... [512/50176]	Loss: 1.8382
Profiling... [768/50176]	Loss: 1.7330
Profiling... [1024/50176]	Loss: 1.7377
Profiling... [1280/50176]	Loss: 1.7694
Profiling... [1536/50176]	Loss: 1.6597
Profiling... [1792/50176]	Loss: 1.6874
Profiling... [2048/50176]	Loss: 1.7963
Profiling... [2304/50176]	Loss: 1.7472
Profiling... [2560/50176]	Loss: 1.8229
Profiling... [2816/50176]	Loss: 1.7591
Profiling... [3072/50176]	Loss: 1.6225
Profiling... [3328/50176]	Loss: 1.7224
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 8, Average loss: 0.0072, Accuracy: 0.4926
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.69702460763459,
                        "time": 2.4256150769997475,
                        "accuracy": 0.492578125,
                        "total_cost": 599.2757752169522
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6384
Profiling... [512/50176]	Loss: 1.6612
Profiling... [768/50176]	Loss: 1.6895
Profiling... [1024/50176]	Loss: 1.7936
Profiling... [1280/50176]	Loss: 1.8351
Profiling... [1536/50176]	Loss: 1.6932
Profiling... [1792/50176]	Loss: 1.6304
Profiling... [2048/50176]	Loss: 1.5538
Profiling... [2304/50176]	Loss: 1.6035
Profiling... [2560/50176]	Loss: 1.7649
Profiling... [2816/50176]	Loss: 1.6596
Profiling... [3072/50176]	Loss: 1.7425
Profiling... [3328/50176]	Loss: 1.8698
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4972
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.71525495103795,
                        "time": 2.4993645240010665,
                        "accuracy": 0.49716796875,
                        "total_cost": 611.8873486947049
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7447
Profiling... [512/50176]	Loss: 1.7026
Profiling... [768/50176]	Loss: 1.6608
Profiling... [1024/50176]	Loss: 1.7781
Profiling... [1280/50176]	Loss: 1.7190
Profiling... [1536/50176]	Loss: 1.4969
Profiling... [1792/50176]	Loss: 1.6646
Profiling... [2048/50176]	Loss: 1.7714
Profiling... [2304/50176]	Loss: 1.5864
Profiling... [2560/50176]	Loss: 1.7999
Profiling... [2816/50176]	Loss: 1.7289
Profiling... [3072/50176]	Loss: 1.6079
Profiling... [3328/50176]	Loss: 1.6236
Profile done
epoch 1 train time consumed: 4.55s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4975
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.71981434765755,
                        "time": 2.8790340309988096,
                        "accuracy": 0.4974609375,
                        "total_cost": 704.448251786128
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7860
Profiling... [512/50176]	Loss: 1.6414
Profiling... [768/50176]	Loss: 1.6283
Profiling... [1024/50176]	Loss: 1.8895
Profiling... [1280/50176]	Loss: 1.7943
Profiling... [1536/50176]	Loss: 1.6094
Profiling... [1792/50176]	Loss: 1.5586
Profiling... [2048/50176]	Loss: 1.7436
Profiling... [2304/50176]	Loss: 1.7325
Profiling... [2560/50176]	Loss: 1.9214
Profiling... [2816/50176]	Loss: 1.8780
Profiling... [3072/50176]	Loss: 1.5535
Profiling... [3328/50176]	Loss: 1.6988
Profile done
epoch 1 train time consumed: 10.12s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4979
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.69201451177364,
                        "time": 7.31209165799919,
                        "accuracy": 0.4978515625,
                        "total_cost": 1787.3262457756298
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7736
Profiling... [512/50176]	Loss: 1.5887
Profiling... [768/50176]	Loss: 1.7262
Profiling... [1024/50176]	Loss: 1.7163
Profiling... [1280/50176]	Loss: 1.9006
Profiling... [1536/50176]	Loss: 1.7466
Profiling... [1792/50176]	Loss: 1.8183
Profiling... [2048/50176]	Loss: 1.7952
Profiling... [2304/50176]	Loss: 1.6834
Profiling... [2560/50176]	Loss: 1.7033
Profiling... [2816/50176]	Loss: 1.6226
Profiling... [3072/50176]	Loss: 1.6629
Profiling... [3328/50176]	Loss: 1.7629
Profile done
epoch 1 train time consumed: 4.18s
Validation Epoch: 8, Average loss: 0.0070, Accuracy: 0.4991
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.6750693920424,
                        "time": 2.4258064040004683,
                        "accuracy": 0.49912109375,
                        "total_cost": 591.359824768812
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6359
Profiling... [512/50176]	Loss: 1.7623
Profiling... [768/50176]	Loss: 1.6817
Profiling... [1024/50176]	Loss: 1.8581
Profiling... [1280/50176]	Loss: 1.5590
Profiling... [1536/50176]	Loss: 1.7040
Profiling... [1792/50176]	Loss: 1.6461
Profiling... [2048/50176]	Loss: 1.6997
Profiling... [2304/50176]	Loss: 1.6142
Profiling... [2560/50176]	Loss: 1.7858
Profiling... [2816/50176]	Loss: 1.6993
Profiling... [3072/50176]	Loss: 1.7093
Profiling... [3328/50176]	Loss: 1.6948
Profile done
epoch 1 train time consumed: 4.10s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4984
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.69256581419774,
                        "time": 2.4737637889993493,
                        "accuracy": 0.4984375,
                        "total_cost": 603.9647151379712
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5405
Profiling... [512/50176]	Loss: 1.8135
Profiling... [768/50176]	Loss: 1.6789
Profiling... [1024/50176]	Loss: 1.6898
Profiling... [1280/50176]	Loss: 1.7094
Profiling... [1536/50176]	Loss: 1.8194
Profiling... [1792/50176]	Loss: 1.8141
Profiling... [2048/50176]	Loss: 1.7827
Profiling... [2304/50176]	Loss: 1.6870
Profiling... [2560/50176]	Loss: 1.7399
Profiling... [2816/50176]	Loss: 1.7383
Profiling... [3072/50176]	Loss: 1.6981
Profiling... [3328/50176]	Loss: 1.8320
Profile done
epoch 1 train time consumed: 4.48s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4993
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.69863732435442,
                        "time": 2.872946775998571,
                        "accuracy": 0.49931640625,
                        "total_cost": 700.2247540197332
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5813
Profiling... [512/50176]	Loss: 1.7262
Profiling... [768/50176]	Loss: 1.6340
Profiling... [1024/50176]	Loss: 1.6231
Profiling... [1280/50176]	Loss: 1.9355
Profiling... [1536/50176]	Loss: 1.8836
Profiling... [1792/50176]	Loss: 1.8683
Profiling... [2048/50176]	Loss: 1.6366
Profiling... [2304/50176]	Loss: 1.5991
Profiling... [2560/50176]	Loss: 1.7492
Profiling... [2816/50176]	Loss: 1.6075
Profiling... [3072/50176]	Loss: 1.6157
Profiling... [3328/50176]	Loss: 1.5196
Profile done
epoch 1 train time consumed: 10.58s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4978
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.66742972606534,
                        "time": 7.443758837000132,
                        "accuracy": 0.49775390625,
                        "total_cost": 1819.4995435025608
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6433
Profiling... [512/50176]	Loss: 1.6705
Profiling... [768/50176]	Loss: 1.7360
Profiling... [1024/50176]	Loss: 1.7580
Profiling... [1280/50176]	Loss: 1.8908
Profiling... [1536/50176]	Loss: 1.7106
Profiling... [1792/50176]	Loss: 1.6423
Profiling... [2048/50176]	Loss: 1.6389
Profiling... [2304/50176]	Loss: 1.6939
Profiling... [2560/50176]	Loss: 1.8335
Profiling... [2816/50176]	Loss: 1.6708
Profiling... [3072/50176]	Loss: 1.6292
Profiling... [3328/50176]	Loss: 1.5111
Profile done
epoch 1 train time consumed: 4.02s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4976
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.6483165738096,
                        "time": 2.427695884000059,
                        "accuracy": 0.49755859375,
                        "total_cost": 593.5484205306698
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8921
Profiling... [512/50176]	Loss: 1.7220
Profiling... [768/50176]	Loss: 1.8003
Profiling... [1024/50176]	Loss: 1.7405
Profiling... [1280/50176]	Loss: 1.6206
Profiling... [1536/50176]	Loss: 1.8018
Profiling... [1792/50176]	Loss: 1.6987
Profiling... [2048/50176]	Loss: 1.6703
Profiling... [2304/50176]	Loss: 1.6112
Profiling... [2560/50176]	Loss: 1.6423
Profiling... [2816/50176]	Loss: 1.7757
Profiling... [3072/50176]	Loss: 1.5346
Profiling... [3328/50176]	Loss: 1.7267
Profile done
epoch 1 train time consumed: 4.15s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4948
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.66726268944208,
                        "time": 2.4813969120004913,
                        "accuracy": 0.49482421875,
                        "total_cost": 610.1252899298073
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.6899
Profiling... [512/50176]	Loss: 1.6713
Profiling... [768/50176]	Loss: 1.7937
Profiling... [1024/50176]	Loss: 1.7243
Profiling... [1280/50176]	Loss: 1.7411
Profiling... [1536/50176]	Loss: 1.8751
Profiling... [1792/50176]	Loss: 1.6293
Profiling... [2048/50176]	Loss: 1.6350
Profiling... [2304/50176]	Loss: 1.4594
Profiling... [2560/50176]	Loss: 1.6933
Profiling... [2816/50176]	Loss: 1.7325
Profiling... [3072/50176]	Loss: 1.7740
Profiling... [3328/50176]	Loss: 1.6075
Profile done
epoch 1 train time consumed: 4.52s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4997
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.67226513263347,
                        "time": 2.8664156819995696,
                        "accuracy": 0.49970703125,
                        "total_cost": 697.9355242774363
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.6935
Profiling... [512/50176]	Loss: 1.9893
Profiling... [768/50176]	Loss: 1.7244
Profiling... [1024/50176]	Loss: 1.7866
Profiling... [1280/50176]	Loss: 1.7219
Profiling... [1536/50176]	Loss: 1.6757
Profiling... [1792/50176]	Loss: 1.7866
Profiling... [2048/50176]	Loss: 1.6864
Profiling... [2304/50176]	Loss: 1.7236
Profiling... [2560/50176]	Loss: 1.8246
Profiling... [2816/50176]	Loss: 1.7924
Profiling... [3072/50176]	Loss: 1.8192
Profiling... [3328/50176]	Loss: 1.7543
Profile done
epoch 1 train time consumed: 10.35s
Validation Epoch: 8, Average loss: 0.0071, Accuracy: 0.4958
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.6437054668979,
                        "time": 7.352086872000655,
                        "accuracy": 0.49580078125,
                        "total_cost": 1803.819444918824
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.8154
Profiling... [512/50176]	Loss: 1.8001
Profiling... [768/50176]	Loss: 1.8578
Profiling... [1024/50176]	Loss: 1.8652
Profiling... [1280/50176]	Loss: 1.7288
Profiling... [1536/50176]	Loss: 1.6653
Profiling... [1792/50176]	Loss: 1.9108
Profiling... [2048/50176]	Loss: 1.6987
Profiling... [2304/50176]	Loss: 1.7885
Profiling... [2560/50176]	Loss: 1.7636
Profiling... [2816/50176]	Loss: 1.7298
Profiling... [3072/50176]	Loss: 1.7850
Profiling... [3328/50176]	Loss: 1.6945
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 8, Average loss: 0.0086, Accuracy: 0.4346
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.62401967544176,
                        "time": 2.446504379999169,
                        "accuracy": 0.4345703125,
                        "total_cost": 684.7078327493284
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.8213
Profiling... [512/50176]	Loss: 1.7268
Profiling... [768/50176]	Loss: 1.8060
Profiling... [1024/50176]	Loss: 1.8364
Profiling... [1280/50176]	Loss: 1.4628
Profiling... [1536/50176]	Loss: 1.6453
Profiling... [1792/50176]	Loss: 1.7298
Profiling... [2048/50176]	Loss: 1.7446
Profiling... [2304/50176]	Loss: 1.7714
Profiling... [2560/50176]	Loss: 2.0244
Profiling... [2816/50176]	Loss: 1.8369
Profiling... [3072/50176]	Loss: 1.6858
Profiling... [3328/50176]	Loss: 1.9690
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 8, Average loss: 0.0079, Accuracy: 0.4516
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.64138311047225,
                        "time": 2.487612770999476,
                        "accuracy": 0.4515625,
                        "total_cost": 670.1102463283615
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7291
Profiling... [512/50176]	Loss: 1.7588
Profiling... [768/50176]	Loss: 1.6445
Profiling... [1024/50176]	Loss: 1.7753
Profiling... [1280/50176]	Loss: 1.7514
Profiling... [1536/50176]	Loss: 1.6394
Profiling... [1792/50176]	Loss: 1.7632
Profiling... [2048/50176]	Loss: 1.7409
Profiling... [2304/50176]	Loss: 1.7717
Profiling... [2560/50176]	Loss: 1.7591
Profiling... [2816/50176]	Loss: 1.8462
Profiling... [3072/50176]	Loss: 1.7438
Profiling... [3328/50176]	Loss: 1.8892
Profile done
epoch 1 train time consumed: 4.47s
Validation Epoch: 8, Average loss: 0.0117, Accuracy: 0.3204
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.64632359756693,
                        "time": 2.8745457910008554,
                        "accuracy": 0.32041015625,
                        "total_cost": 1091.3447051137725
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 2.0545
Profiling... [512/50176]	Loss: 1.7394
Profiling... [768/50176]	Loss: 1.7674
Profiling... [1024/50176]	Loss: 1.6755
Profiling... [1280/50176]	Loss: 1.7812
Profiling... [1536/50176]	Loss: 1.7122
Profiling... [1792/50176]	Loss: 1.7347
Profiling... [2048/50176]	Loss: 1.8037
Profiling... [2304/50176]	Loss: 1.9780
Profiling... [2560/50176]	Loss: 1.8147
Profiling... [2816/50176]	Loss: 1.8460
Profiling... [3072/50176]	Loss: 1.8707
Profiling... [3328/50176]	Loss: 1.8544
Profile done
epoch 1 train time consumed: 10.37s
Validation Epoch: 8, Average loss: 0.0079, Accuracy: 0.4576
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.61879650220237,
                        "time": 7.3876610190000065,
                        "accuracy": 0.4576171875,
                        "total_cost": 1963.3843890468268
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7643
Profiling... [512/50176]	Loss: 1.8817
Profiling... [768/50176]	Loss: 1.8273
Profiling... [1024/50176]	Loss: 1.6566
Profiling... [1280/50176]	Loss: 1.8705
Profiling... [1536/50176]	Loss: 1.8524
Profiling... [1792/50176]	Loss: 1.9503
Profiling... [2048/50176]	Loss: 1.8568
Profiling... [2304/50176]	Loss: 1.6705
Profiling... [2560/50176]	Loss: 1.9560
Profiling... [2816/50176]	Loss: 1.7366
Profiling... [3072/50176]	Loss: 1.5962
Profiling... [3328/50176]	Loss: 1.5876
Profile done
epoch 1 train time consumed: 4.14s
Validation Epoch: 8, Average loss: 0.0101, Accuracy: 0.3753
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.6008201572166,
                        "time": 2.4223085880003055,
                        "accuracy": 0.37529296875,
                        "total_cost": 784.8660526622415
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6327
Profiling... [512/50176]	Loss: 1.7193
Profiling... [768/50176]	Loss: 1.7071
Profiling... [1024/50176]	Loss: 1.9588
Profiling... [1280/50176]	Loss: 1.8938
Profiling... [1536/50176]	Loss: 1.8400
Profiling... [1792/50176]	Loss: 1.7916
Profiling... [2048/50176]	Loss: 1.6825
Profiling... [2304/50176]	Loss: 1.6645
Profiling... [2560/50176]	Loss: 1.7119
Profiling... [2816/50176]	Loss: 1.7459
Profiling... [3072/50176]	Loss: 1.8981
Profiling... [3328/50176]	Loss: 1.5228
Profile done
epoch 1 train time consumed: 4.11s
Validation Epoch: 8, Average loss: 0.0081, Accuracy: 0.4394
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.61792983782654,
                        "time": 2.5045563809999294,
                        "accuracy": 0.43935546875,
                        "total_cost": 693.285924233372
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7016
Profiling... [512/50176]	Loss: 1.8683
Profiling... [768/50176]	Loss: 1.7918
Profiling... [1024/50176]	Loss: 1.6916
Profiling... [1280/50176]	Loss: 1.5314
Profiling... [1536/50176]	Loss: 1.7523
Profiling... [1792/50176]	Loss: 1.7709
Profiling... [2048/50176]	Loss: 1.8440
Profiling... [2304/50176]	Loss: 1.8545
Profiling... [2560/50176]	Loss: 1.8052
Profiling... [2816/50176]	Loss: 1.7906
Profiling... [3072/50176]	Loss: 1.8538
Profiling... [3328/50176]	Loss: 1.6104
Profile done
epoch 1 train time consumed: 4.62s
Validation Epoch: 8, Average loss: 0.0082, Accuracy: 0.4310
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.62289219757494,
                        "time": 2.859447408000051,
                        "accuracy": 0.43095703125,
                        "total_cost": 806.9812965786837
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.5735
Profiling... [512/50176]	Loss: 1.9576
Profiling... [768/50176]	Loss: 1.8273
Profiling... [1024/50176]	Loss: 1.5869
Profiling... [1280/50176]	Loss: 1.8503
Profiling... [1536/50176]	Loss: 1.7890
Profiling... [1792/50176]	Loss: 1.7844
Profiling... [2048/50176]	Loss: 1.8514
Profiling... [2304/50176]	Loss: 1.8758
Profiling... [2560/50176]	Loss: 1.7453
Profiling... [2816/50176]	Loss: 1.7536
Profiling... [3072/50176]	Loss: 1.7567
Profiling... [3328/50176]	Loss: 1.7033
Profile done
epoch 1 train time consumed: 10.39s
Validation Epoch: 8, Average loss: 0.0080, Accuracy: 0.4532
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.59190479523943,
                        "time": 7.246615346999533,
                        "accuracy": 0.45322265625,
                        "total_cost": 1944.1432399929547
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.7947
Profiling... [512/50176]	Loss: 1.8819
Profiling... [768/50176]	Loss: 1.8862
Profiling... [1024/50176]	Loss: 1.7464
Profiling... [1280/50176]	Loss: 1.8789
Profiling... [1536/50176]	Loss: 1.6285
Profiling... [1792/50176]	Loss: 1.7534
Profiling... [2048/50176]	Loss: 1.6578
Profiling... [2304/50176]	Loss: 1.8060
Profiling... [2560/50176]	Loss: 1.8305
Profiling... [2816/50176]	Loss: 1.7494
Profiling... [3072/50176]	Loss: 1.8735
Profiling... [3328/50176]	Loss: 1.7230
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 8, Average loss: 0.0083, Accuracy: 0.4305
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.57160329731414,
                        "time": 2.41494341500038,
                        "accuracy": 0.43046875,
                        "total_cost": 682.0205713745474
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.5891
Profiling... [512/50176]	Loss: 1.7543
Profiling... [768/50176]	Loss: 1.6248
Profiling... [1024/50176]	Loss: 1.9681
Profiling... [1280/50176]	Loss: 1.7267
Profiling... [1536/50176]	Loss: 1.7298
Profiling... [1792/50176]	Loss: 1.8160
Profiling... [2048/50176]	Loss: 1.7117
Profiling... [2304/50176]	Loss: 1.6422
Profiling... [2560/50176]	Loss: 1.7922
Profiling... [2816/50176]	Loss: 1.8073
Profiling... [3072/50176]	Loss: 1.8699
Profiling... [3328/50176]	Loss: 1.7696
Profile done
epoch 1 train time consumed: 4.11s
Validation Epoch: 8, Average loss: 0.0087, Accuracy: 0.4225
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.58919897489042,
                        "time": 2.510583516999759,
                        "accuracy": 0.4224609375,
                        "total_cost": 722.5753003295455
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7057
Profiling... [512/50176]	Loss: 1.7289
Profiling... [768/50176]	Loss: 1.6532
Profiling... [1024/50176]	Loss: 1.6063
Profiling... [1280/50176]	Loss: 1.7086
Profiling... [1536/50176]	Loss: 1.6445
Profiling... [1792/50176]	Loss: 1.9022
Profiling... [2048/50176]	Loss: 1.5688
Profiling... [2304/50176]	Loss: 1.7226
Profiling... [2560/50176]	Loss: 1.9259
Profiling... [2816/50176]	Loss: 1.7298
Profiling... [3072/50176]	Loss: 1.8674
Profiling... [3328/50176]	Loss: 1.7838
Profile done
epoch 1 train time consumed: 4.46s
Validation Epoch: 8, Average loss: 0.0091, Accuracy: 0.4106
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.59396897119377,
                        "time": 2.860382615001072,
                        "accuracy": 0.41064453125,
                        "total_cost": 846.9740821227177
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7058
Profiling... [512/50176]	Loss: 1.5290
Profiling... [768/50176]	Loss: 1.6575
Profiling... [1024/50176]	Loss: 1.6405
Profiling... [1280/50176]	Loss: 1.8819
Profiling... [1536/50176]	Loss: 1.7298
Profiling... [1792/50176]	Loss: 1.7591
Profiling... [2048/50176]	Loss: 1.9692
Profiling... [2304/50176]	Loss: 1.6909
Profiling... [2560/50176]	Loss: 1.6930
Profiling... [2816/50176]	Loss: 1.6583
Profiling... [3072/50176]	Loss: 1.8568
Profiling... [3328/50176]	Loss: 1.5691
Profile done
epoch 1 train time consumed: 10.66s
Validation Epoch: 8, Average loss: 0.0080, Accuracy: 0.4499
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.56283136073135,
                        "time": 7.456563978001213,
                        "accuracy": 0.44990234375,
                        "total_cost": 2014.7506275094074
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6505
Profiling... [512/50176]	Loss: 1.8205
Profiling... [768/50176]	Loss: 1.7215
Profiling... [1024/50176]	Loss: 1.9374
Profiling... [1280/50176]	Loss: 1.8257
Profiling... [1536/50176]	Loss: 2.0116
Profiling... [1792/50176]	Loss: 1.6798
Profiling... [2048/50176]	Loss: 2.0488
Profiling... [2304/50176]	Loss: 1.7828
Profiling... [2560/50176]	Loss: 1.7928
Profiling... [2816/50176]	Loss: 1.9002
Profiling... [3072/50176]	Loss: 1.9281
Profiling... [3328/50176]	Loss: 2.0888
Profile done
epoch 1 train time consumed: 4.07s
Validation Epoch: 8, Average loss: 0.0099, Accuracy: 0.3637
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.54519177765394,
                        "time": 2.4200791149996803,
                        "accuracy": 0.363671875,
                        "total_cost": 808.830708037929
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7604
Profiling... [512/50176]	Loss: 1.7366
Profiling... [768/50176]	Loss: 1.8642
Profiling... [1024/50176]	Loss: 1.6839
Profiling... [1280/50176]	Loss: 1.8558
Profiling... [1536/50176]	Loss: 1.8639
Profiling... [1792/50176]	Loss: 1.7492
Profiling... [2048/50176]	Loss: 1.7815
Profiling... [2304/50176]	Loss: 2.0449
Profiling... [2560/50176]	Loss: 1.7550
Profiling... [2816/50176]	Loss: 2.0280
Profiling... [3072/50176]	Loss: 1.9707
Profiling... [3328/50176]	Loss: 2.0161
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 8, Average loss: 0.0095, Accuracy: 0.3595
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.5619287272952,
                        "time": 2.5035296969999763,
                        "accuracy": 0.35947265625,
                        "total_cost": 846.6120949731571
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.5783
Profiling... [512/50176]	Loss: 1.9637
Profiling... [768/50176]	Loss: 1.9219
Profiling... [1024/50176]	Loss: 1.8147
Profiling... [1280/50176]	Loss: 2.0509
Profiling... [1536/50176]	Loss: 2.0185
Profiling... [1792/50176]	Loss: 1.9088
Profiling... [2048/50176]	Loss: 1.9562
Profiling... [2304/50176]	Loss: 1.7809
Profiling... [2560/50176]	Loss: 1.8376
Profiling... [2816/50176]	Loss: 2.1781
Profiling... [3072/50176]	Loss: 1.9765
Profiling... [3328/50176]	Loss: 1.9742
Profile done
epoch 1 train time consumed: 4.52s
Validation Epoch: 8, Average loss: 0.0131, Accuracy: 0.2803
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.56686401377682,
                        "time": 2.8779567000001407,
                        "accuracy": 0.2802734375,
                        "total_cost": 1248.29585674331
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7760
Profiling... [512/50176]	Loss: 1.8631
Profiling... [768/50176]	Loss: 1.8073
Profiling... [1024/50176]	Loss: 1.7184
Profiling... [1280/50176]	Loss: 1.7836
Profiling... [1536/50176]	Loss: 1.7285
Profiling... [1792/50176]	Loss: 1.7961
Profiling... [2048/50176]	Loss: 1.8001
Profiling... [2304/50176]	Loss: 1.8858
Profiling... [2560/50176]	Loss: 1.8234
Profiling... [2816/50176]	Loss: 1.9187
Profiling... [3072/50176]	Loss: 1.8978
Profiling... [3328/50176]	Loss: 1.7345
Profile done
epoch 1 train time consumed: 10.28s
Validation Epoch: 8, Average loss: 0.0141, Accuracy: 0.2956
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.53959834402175,
                        "time": 7.332253091000894,
                        "accuracy": 0.29560546875,
                        "total_cost": 3014.690828980006
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6469
Profiling... [512/50176]	Loss: 2.0158
Profiling... [768/50176]	Loss: 1.7656
Profiling... [1024/50176]	Loss: 1.8828
Profiling... [1280/50176]	Loss: 1.8626
Profiling... [1536/50176]	Loss: 1.7941
Profiling... [1792/50176]	Loss: 2.0400
Profiling... [2048/50176]	Loss: 1.9670
Profiling... [2304/50176]	Loss: 2.0010
Profiling... [2560/50176]	Loss: 1.9413
Profiling... [2816/50176]	Loss: 1.8059
Profiling... [3072/50176]	Loss: 1.8692
Profiling... [3328/50176]	Loss: 2.1080
Profile done
epoch 1 train time consumed: 4.08s
Validation Epoch: 8, Average loss: 0.0105, Accuracy: 0.3172
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.520097707083,
                        "time": 2.419089761999203,
                        "accuracy": 0.3171875,
                        "total_cost": 926.7957414474002
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.7329
Profiling... [512/50176]	Loss: 1.6982
Profiling... [768/50176]	Loss: 1.8980
Profiling... [1024/50176]	Loss: 1.9018
Profiling... [1280/50176]	Loss: 2.0446
Profiling... [1536/50176]	Loss: 1.8459
Profiling... [1792/50176]	Loss: 2.0268
Profiling... [2048/50176]	Loss: 1.8650
Profiling... [2304/50176]	Loss: 1.8325
Profiling... [2560/50176]	Loss: 1.8589
Profiling... [2816/50176]	Loss: 1.9219
Profiling... [3072/50176]	Loss: 1.9153
Profiling... [3328/50176]	Loss: 1.9711
Profile done
epoch 1 train time consumed: 4.19s
Validation Epoch: 8, Average loss: 0.0113, Accuracy: 0.3103
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.53600338436883,
                        "time": 2.499449242999617,
                        "accuracy": 0.31025390625,
                        "total_cost": 979.111190985237
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7419
Profiling... [512/50176]	Loss: 1.5417
Profiling... [768/50176]	Loss: 2.0229
Profiling... [1024/50176]	Loss: 1.9861
Profiling... [1280/50176]	Loss: 1.8885
Profiling... [1536/50176]	Loss: 1.8999
Profiling... [1792/50176]	Loss: 1.9176
Profiling... [2048/50176]	Loss: 1.7359
Profiling... [2304/50176]	Loss: 1.9747
Profiling... [2560/50176]	Loss: 1.8559
Profiling... [2816/50176]	Loss: 1.8531
Profiling... [3072/50176]	Loss: 1.8991
Profiling... [3328/50176]	Loss: 1.9002
Profile done
epoch 1 train time consumed: 4.58s
Validation Epoch: 8, Average loss: 0.0121, Accuracy: 0.3291
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.54052839705781,
                        "time": 2.8714376900006755,
                        "accuracy": 0.3291015625,
                        "total_cost": 1060.4509181019437
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7259
Profiling... [512/50176]	Loss: 1.8224
Profiling... [768/50176]	Loss: 1.7334
Profiling... [1024/50176]	Loss: 1.9128
Profiling... [1280/50176]	Loss: 1.9682
Profiling... [1536/50176]	Loss: 1.8744
Profiling... [1792/50176]	Loss: 1.8051
Profiling... [2048/50176]	Loss: 1.9490
Profiling... [2304/50176]	Loss: 1.7091
Profiling... [2560/50176]	Loss: 1.9679
Profiling... [2816/50176]	Loss: 1.9523
Profiling... [3072/50176]	Loss: 1.8995
Profiling... [3328/50176]	Loss: 1.7896
Profile done
epoch 1 train time consumed: 10.27s
Validation Epoch: 8, Average loss: 0.0114, Accuracy: 0.3236
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.51356630255432,
                        "time": 7.37764040500042,
                        "accuracy": 0.3236328125,
                        "total_cost": 2770.063361574075
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [256/50176]	Loss: 1.6729
Profiling... [512/50176]	Loss: 1.7620
Profiling... [768/50176]	Loss: 1.8520
Profiling... [1024/50176]	Loss: 1.8906
Profiling... [1280/50176]	Loss: 1.8850
Profiling... [1536/50176]	Loss: 2.0223
Profiling... [1792/50176]	Loss: 1.7589
Profiling... [2048/50176]	Loss: 1.8705
Profiling... [2304/50176]	Loss: 1.9217
Profiling... [2560/50176]	Loss: 1.9302
Profiling... [2816/50176]	Loss: 1.9497
Profiling... [3072/50176]	Loss: 1.8497
Profiling... [3328/50176]	Loss: 1.8867
Profile done
epoch 1 train time consumed: 4.06s
Validation Epoch: 8, Average loss: 0.0140, Accuracy: 0.2752
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.49574986128651,
                        "time": 2.417554809999274,
                        "accuracy": 0.2751953125,
                        "total_cost": 1067.324264368136
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [256/50176]	Loss: 1.6282
Profiling... [512/50176]	Loss: 1.7456
Profiling... [768/50176]	Loss: 1.8235
Profiling... [1024/50176]	Loss: 1.9305
Profiling... [1280/50176]	Loss: 1.9800
Profiling... [1536/50176]	Loss: 1.8119
Profiling... [1792/50176]	Loss: 1.8962
Profiling... [2048/50176]	Loss: 1.9134
Profiling... [2304/50176]	Loss: 2.0071
Profiling... [2560/50176]	Loss: 1.7024
Profiling... [2816/50176]	Loss: 1.9438
Profiling... [3072/50176]	Loss: 1.9164
Profiling... [3328/50176]	Loss: 2.1024
Profile done
epoch 1 train time consumed: 4.17s
Validation Epoch: 8, Average loss: 0.0169, Accuracy: 0.2168
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.5114846910699,
                        "time": 2.50191019399972,
                        "accuracy": 0.216796875,
                        "total_cost": 1402.2841530194044
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [256/50176]	Loss: 1.7749
Profiling... [512/50176]	Loss: 1.7848
Profiling... [768/50176]	Loss: 1.8576
Profiling... [1024/50176]	Loss: 1.8441
Profiling... [1280/50176]	Loss: 1.8809
Profiling... [1536/50176]	Loss: 1.8784
Profiling... [1792/50176]	Loss: 2.0622
Profiling... [2048/50176]	Loss: 1.8065
Profiling... [2304/50176]	Loss: 1.9886
Profiling... [2560/50176]	Loss: 1.9173
Profiling... [2816/50176]	Loss: 1.9382
Profiling... [3072/50176]	Loss: 1.9839
Profiling... [3328/50176]	Loss: 1.8892
Profile done
epoch 1 train time consumed: 4.57s
Validation Epoch: 8, Average loss: 0.0118, Accuracy: 0.3020
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.5163194562725,
                        "time": 2.8651391019993753,
                        "accuracy": 0.301953125,
                        "total_cost": 1153.03048579217
                    },
                    
[Training Loop] Profiling with batch size 256 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [256/50176]	Loss: 1.7036
Profiling... [512/50176]	Loss: 1.7846
Profiling... [768/50176]	Loss: 1.8163
Profiling... [1024/50176]	Loss: 1.9770
Profiling... [1280/50176]	Loss: 1.7702
Profiling... [1536/50176]	Loss: 1.7773
Profiling... [1792/50176]	Loss: 1.8554
Profiling... [2048/50176]	Loss: 1.8261
Profiling... [2304/50176]	Loss: 1.7885
Profiling... [2560/50176]	Loss: 1.9728
Profiling... [2816/50176]	Loss: 1.9447
Profiling... [3072/50176]	Loss: 1.8344
Profiling... [3328/50176]	Loss: 1.8316
Profile done
epoch 1 train time consumed: 10.36s
Validation Epoch: 8, Average loss: 0.0098, Accuracy: 0.3746
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 256,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.48853086223438,
                        "time": 7.4324650399994425,
                        "accuracy": 0.374609375,
                        "total_cost": 2410.4021913345077
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7007
Profiling... [1024/50176]	Loss: 1.6508
Profiling... [1536/50176]	Loss: 1.7752
Profiling... [2048/50176]	Loss: 1.5735
Profiling... [2560/50176]	Loss: 1.5603
Profiling... [3072/50176]	Loss: 1.6825
Profiling... [3584/50176]	Loss: 1.6988
Profiling... [4096/50176]	Loss: 1.6932
Profiling... [4608/50176]	Loss: 1.6944
Profiling... [5120/50176]	Loss: 1.7654
Profiling... [5632/50176]	Loss: 1.7914
Profiling... [6144/50176]	Loss: 1.5650
Profiling... [6656/50176]	Loss: 1.8199
Profile done
epoch 1 train time consumed: 7.19s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.4986
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.4820173718287,
                        "time": 4.606354285000634,
                        "accuracy": 0.4986328125,
                        "total_cost": 1122.2470668659507
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7090
Profiling... [1024/50176]	Loss: 1.7392
Profiling... [1536/50176]	Loss: 1.7463
Profiling... [2048/50176]	Loss: 1.7861
Profiling... [2560/50176]	Loss: 1.7326
Profiling... [3072/50176]	Loss: 1.6823
Profiling... [3584/50176]	Loss: 1.6711
Profiling... [4096/50176]	Loss: 1.5948
Profiling... [4608/50176]	Loss: 1.6377
Profiling... [5120/50176]	Loss: 1.7799
Profiling... [5632/50176]	Loss: 1.6826
Profiling... [6144/50176]	Loss: 1.6049
Profiling... [6656/50176]	Loss: 1.6422
Profile done
epoch 1 train time consumed: 7.27s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.4999
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.50538793929677,
                        "time": 4.754683103001298,
                        "accuracy": 0.49990234375,
                        "total_cost": 1155.6649457268963
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6513
Profiling... [1024/50176]	Loss: 1.6306
Profiling... [1536/50176]	Loss: 1.7564
Profiling... [2048/50176]	Loss: 1.7238
Profiling... [2560/50176]	Loss: 1.6948
Profiling... [3072/50176]	Loss: 1.8069
Profiling... [3584/50176]	Loss: 1.8380
Profiling... [4096/50176]	Loss: 1.6966
Profiling... [4608/50176]	Loss: 1.6674
Profiling... [5120/50176]	Loss: 1.7321
Profiling... [5632/50176]	Loss: 1.7506
Profiling... [6144/50176]	Loss: 1.7269
Profiling... [6656/50176]	Loss: 1.8041
Profile done
epoch 1 train time consumed: 8.23s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.5003
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.50844031962302,
                        "time": 5.5418963829997665,
                        "accuracy": 0.50029296875,
                        "total_cost": 1345.985708321554
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6200
Profiling... [1024/50176]	Loss: 1.7154
Profiling... [1536/50176]	Loss: 1.7354
Profiling... [2048/50176]	Loss: 1.6524
Profiling... [2560/50176]	Loss: 1.7524
Profiling... [3072/50176]	Loss: 1.7383
Profiling... [3584/50176]	Loss: 1.6922
Profiling... [4096/50176]	Loss: 1.7004
Profiling... [4608/50176]	Loss: 1.7610
Profiling... [5120/50176]	Loss: 1.7786
Profiling... [5632/50176]	Loss: 1.5543
Profiling... [6144/50176]	Loss: 1.5809
Profiling... [6656/50176]	Loss: 1.6496
Profile done
epoch 1 train time consumed: 18.79s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.4981
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.4553250124513,
                        "time": 13.375182734000191,
                        "accuracy": 0.49814453125,
                        "total_cost": 3261.0759812670735
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7317
Profiling... [1024/50176]	Loss: 1.6886
Profiling... [1536/50176]	Loss: 1.7308
Profiling... [2048/50176]	Loss: 1.7193
Profiling... [2560/50176]	Loss: 1.8257
Profiling... [3072/50176]	Loss: 1.6754
Profiling... [3584/50176]	Loss: 1.8237
Profiling... [4096/50176]	Loss: 1.7300
Profiling... [4608/50176]	Loss: 1.7484
Profiling... [5120/50176]	Loss: 1.6722
Profiling... [5632/50176]	Loss: 1.6475
Profiling... [6144/50176]	Loss: 1.7467
Profiling... [6656/50176]	Loss: 1.6508
Profile done
epoch 1 train time consumed: 7.07s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.5034
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.4522580381898,
                        "time": 4.61616935299935,
                        "accuracy": 0.50341796875,
                        "total_cost": 1113.675367608659
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7137
Profiling... [1024/50176]	Loss: 1.7261
Profiling... [1536/50176]	Loss: 1.6628
Profiling... [2048/50176]	Loss: 1.6252
Profiling... [2560/50176]	Loss: 1.7163
Profiling... [3072/50176]	Loss: 1.7099
Profiling... [3584/50176]	Loss: 1.6340
Profiling... [4096/50176]	Loss: 1.7362
Profiling... [4608/50176]	Loss: 1.7398
Profiling... [5120/50176]	Loss: 1.5973
Profiling... [5632/50176]	Loss: 1.7073
Profiling... [6144/50176]	Loss: 1.7199
Profiling... [6656/50176]	Loss: 1.6238
Profile done
epoch 1 train time consumed: 7.33s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.5034
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.47648967250662,
                        "time": 4.756558716000654,
                        "accuracy": 0.50341796875,
                        "total_cost": 1147.7740001526809
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7043
Profiling... [1024/50176]	Loss: 1.7128
Profiling... [1536/50176]	Loss: 1.7218
Profiling... [2048/50176]	Loss: 1.6050
Profiling... [2560/50176]	Loss: 1.5698
Profiling... [3072/50176]	Loss: 1.7225
Profiling... [3584/50176]	Loss: 1.6914
Profiling... [4096/50176]	Loss: 1.7097
Profiling... [4608/50176]	Loss: 1.7619
Profiling... [5120/50176]	Loss: 1.5117
Profiling... [5632/50176]	Loss: 1.6985
Profiling... [6144/50176]	Loss: 1.6271
Profiling... [6656/50176]	Loss: 1.7553
Profile done
epoch 1 train time consumed: 8.27s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.5021
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.47901927508309,
                        "time": 5.53970727900014,
                        "accuracy": 0.50205078125,
                        "total_cost": 1340.4186039676163
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7413
Profiling... [1024/50176]	Loss: 1.6334
Profiling... [1536/50176]	Loss: 1.7469
Profiling... [2048/50176]	Loss: 1.7342
Profiling... [2560/50176]	Loss: 1.6373
Profiling... [3072/50176]	Loss: 1.6782
Profiling... [3584/50176]	Loss: 1.7657
Profiling... [4096/50176]	Loss: 1.6407
Profiling... [4608/50176]	Loss: 1.6522
Profiling... [5120/50176]	Loss: 1.7220
Profiling... [5632/50176]	Loss: 1.7339
Profiling... [6144/50176]	Loss: 1.7607
Profiling... [6656/50176]	Loss: 1.5994
Profile done
epoch 1 train time consumed: 18.03s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.5016
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.43142123189827,
                        "time": 13.017862719001641,
                        "accuracy": 0.5015625,
                        "total_cost": 3151.706061298667
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.8245
Profiling... [1024/50176]	Loss: 1.6890
Profiling... [1536/50176]	Loss: 1.6230
Profiling... [2048/50176]	Loss: 1.6515
Profiling... [2560/50176]	Loss: 1.7432
Profiling... [3072/50176]	Loss: 1.6232
Profiling... [3584/50176]	Loss: 1.7301
Profiling... [4096/50176]	Loss: 1.7627
Profiling... [4608/50176]	Loss: 1.6869
Profiling... [5120/50176]	Loss: 1.7535
Profiling... [5632/50176]	Loss: 1.5714
Profiling... [6144/50176]	Loss: 1.7237
Profiling... [6656/50176]	Loss: 1.6676
Profile done
epoch 1 train time consumed: 7.14s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.4979
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.42847018672643,
                        "time": 4.601533809000102,
                        "accuracy": 0.4978515625,
                        "total_cost": 1122.3369635188853
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.5600
Profiling... [1024/50176]	Loss: 1.6940
Profiling... [1536/50176]	Loss: 1.6311
Profiling... [2048/50176]	Loss: 1.6499
Profiling... [2560/50176]	Loss: 1.6560
Profiling... [3072/50176]	Loss: 1.6348
Profiling... [3584/50176]	Loss: 1.7197
Profiling... [4096/50176]	Loss: 1.7322
Profiling... [4608/50176]	Loss: 1.6613
Profiling... [5120/50176]	Loss: 1.8111
Profiling... [5632/50176]	Loss: 1.7490
Profiling... [6144/50176]	Loss: 1.7521
Profiling... [6656/50176]	Loss: 1.7459
Profile done
epoch 1 train time consumed: 7.28s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.5001
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.45155353220511,
                        "time": 4.766058034001617,
                        "accuracy": 0.50009765625,
                        "total_cost": 1157.4642376743666
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6448
Profiling... [1024/50176]	Loss: 1.6876
Profiling... [1536/50176]	Loss: 1.6994
Profiling... [2048/50176]	Loss: 1.7314
Profiling... [2560/50176]	Loss: 1.6701
Profiling... [3072/50176]	Loss: 1.6735
Profiling... [3584/50176]	Loss: 1.6876
Profiling... [4096/50176]	Loss: 1.6618
Profiling... [4608/50176]	Loss: 1.8085
Profiling... [5120/50176]	Loss: 1.7787
Profiling... [5632/50176]	Loss: 1.7590
Profiling... [6144/50176]	Loss: 1.7515
Profiling... [6656/50176]	Loss: 1.6457
Profile done
epoch 1 train time consumed: 8.23s
Validation Epoch: 8, Average loss: 0.0035, Accuracy: 0.5010
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.4546105014936,
                        "time": 5.541404515999602,
                        "accuracy": 0.5009765625,
                        "total_cost": 1343.4343590114545
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6797
Profiling... [1024/50176]	Loss: 1.7686
Profiling... [1536/50176]	Loss: 1.7212
Profiling... [2048/50176]	Loss: 1.7308
Profiling... [2560/50176]	Loss: 1.6436
Profiling... [3072/50176]	Loss: 1.7364
Profiling... [3584/50176]	Loss: 1.7489
Profiling... [4096/50176]	Loss: 1.6975
Profiling... [4608/50176]	Loss: 1.6010
Profiling... [5120/50176]	Loss: 1.6307
Profiling... [5632/50176]	Loss: 1.6749
Profiling... [6144/50176]	Loss: 1.7061
Profiling... [6656/50176]	Loss: 1.5422
Profile done
epoch 1 train time consumed: 18.07s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.4959
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.40637096924694,
                        "time": 13.043151910000233,
                        "accuracy": 0.4958984375,
                        "total_cost": 3193.238009332768
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7522
Profiling... [1024/50176]	Loss: 1.7504
Profiling... [1536/50176]	Loss: 1.7471
Profiling... [2048/50176]	Loss: 1.6633
Profiling... [2560/50176]	Loss: 1.7315
Profiling... [3072/50176]	Loss: 1.8486
Profiling... [3584/50176]	Loss: 1.7733
Profiling... [4096/50176]	Loss: 1.6497
Profiling... [4608/50176]	Loss: 1.6826
Profiling... [5120/50176]	Loss: 1.7378
Profiling... [5632/50176]	Loss: 1.7285
Profiling... [6144/50176]	Loss: 1.7458
Profiling... [6656/50176]	Loss: 1.7302
Profile done
epoch 1 train time consumed: 7.17s
Validation Epoch: 8, Average loss: 0.0040, Accuracy: 0.4560
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.40491897891563,
                        "time": 4.616596814999866,
                        "accuracy": 0.45595703125,
                        "total_cost": 1229.2332914503743
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7132
Profiling... [1024/50176]	Loss: 1.5651
Profiling... [1536/50176]	Loss: 1.7752
Profiling... [2048/50176]	Loss: 1.6613
Profiling... [2560/50176]	Loss: 1.7523
Profiling... [3072/50176]	Loss: 1.7349
Profiling... [3584/50176]	Loss: 1.7004
Profiling... [4096/50176]	Loss: 1.7352
Profiling... [4608/50176]	Loss: 1.7279
Profiling... [5120/50176]	Loss: 1.7527
Profiling... [5632/50176]	Loss: 1.7207
Profiling... [6144/50176]	Loss: 1.7122
Profiling... [6656/50176]	Loss: 1.8815
Profile done
epoch 1 train time consumed: 7.32s
Validation Epoch: 8, Average loss: 0.0044, Accuracy: 0.4191
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.42776809590362,
                        "time": 4.756243136000194,
                        "accuracy": 0.419140625,
                        "total_cost": 1377.9146044981082
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6917
Profiling... [1024/50176]	Loss: 1.8474
Profiling... [1536/50176]	Loss: 1.6598
Profiling... [2048/50176]	Loss: 1.7525
Profiling... [2560/50176]	Loss: 1.6798
Profiling... [3072/50176]	Loss: 1.7692
Profiling... [3584/50176]	Loss: 1.7449
Profiling... [4096/50176]	Loss: 1.8977
Profiling... [4608/50176]	Loss: 1.8429
Profiling... [5120/50176]	Loss: 1.7674
Profiling... [5632/50176]	Loss: 1.7417
Profiling... [6144/50176]	Loss: 1.7607
Profiling... [6656/50176]	Loss: 1.8156
Profile done
epoch 1 train time consumed: 8.26s
Validation Epoch: 8, Average loss: 0.0042, Accuracy: 0.4351
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.43104631869423,
                        "time": 5.517741519999618,
                        "accuracy": 0.43505859375,
                        "total_cost": 1540.0802000354827
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6871
Profiling... [1024/50176]	Loss: 1.6888
Profiling... [1536/50176]	Loss: 1.7232
Profiling... [2048/50176]	Loss: 1.8435
Profiling... [2560/50176]	Loss: 1.8060
Profiling... [3072/50176]	Loss: 1.7054
Profiling... [3584/50176]	Loss: 1.7208
Profiling... [4096/50176]	Loss: 1.7220
Profiling... [4608/50176]	Loss: 1.7519
Profiling... [5120/50176]	Loss: 1.7647
Profiling... [5632/50176]	Loss: 1.7767
Profiling... [6144/50176]	Loss: 1.7282
Profiling... [6656/50176]	Loss: 1.7303
Profile done
epoch 1 train time consumed: 18.80s
Validation Epoch: 8, Average loss: 0.0038, Accuracy: 0.4679
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.3808449777726,
                        "time": 13.85968113799936,
                        "accuracy": 0.46787109375,
                        "total_cost": 3595.6480965070505
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7425
Profiling... [1024/50176]	Loss: 1.7189
Profiling... [1536/50176]	Loss: 1.7467
Profiling... [2048/50176]	Loss: 1.7475
Profiling... [2560/50176]	Loss: 1.7146
Profiling... [3072/50176]	Loss: 1.7372
Profiling... [3584/50176]	Loss: 1.8632
Profiling... [4096/50176]	Loss: 1.7373
Profiling... [4608/50176]	Loss: 1.7403
Profiling... [5120/50176]	Loss: 1.5507
Profiling... [5632/50176]	Loss: 1.7527
Profiling... [6144/50176]	Loss: 1.7083
Profiling... [6656/50176]	Loss: 1.8328
Profile done
epoch 1 train time consumed: 7.12s
Validation Epoch: 8, Average loss: 0.0043, Accuracy: 0.4205
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.376813742858,
                        "time": 4.602058897999086,
                        "accuracy": 0.4205078125,
                        "total_cost": 1328.3540259935062
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6422
Profiling... [1024/50176]	Loss: 1.5741
Profiling... [1536/50176]	Loss: 1.7570
Profiling... [2048/50176]	Loss: 1.7735
Profiling... [2560/50176]	Loss: 1.7799
Profiling... [3072/50176]	Loss: 1.7085
Profiling... [3584/50176]	Loss: 1.7505
Profiling... [4096/50176]	Loss: 1.8434
Profiling... [4608/50176]	Loss: 1.6843
Profiling... [5120/50176]	Loss: 1.7424
Profiling... [5632/50176]	Loss: 1.6869
Profiling... [6144/50176]	Loss: 1.6867
Profiling... [6656/50176]	Loss: 1.9133
Profile done
epoch 1 train time consumed: 7.30s
Validation Epoch: 8, Average loss: 0.0040, Accuracy: 0.4485
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.40068388228963,
                        "time": 4.766462919998958,
                        "accuracy": 0.44853515625,
                        "total_cost": 1290.0925381753698
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7267
Profiling... [1024/50176]	Loss: 1.7227
Profiling... [1536/50176]	Loss: 1.6705
Profiling... [2048/50176]	Loss: 1.7374
Profiling... [2560/50176]	Loss: 1.9383
Profiling... [3072/50176]	Loss: 1.7548
Profiling... [3584/50176]	Loss: 1.7611
Profiling... [4096/50176]	Loss: 1.7917
Profiling... [4608/50176]	Loss: 1.7645
Profiling... [5120/50176]	Loss: 1.7696
Profiling... [5632/50176]	Loss: 1.8071
Profiling... [6144/50176]	Loss: 1.8326
Profiling... [6656/50176]	Loss: 1.6716
Profile done
epoch 1 train time consumed: 8.24s
Validation Epoch: 8, Average loss: 0.0041, Accuracy: 0.4436
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.40354682735496,
                        "time": 5.547027702999912,
                        "accuracy": 0.4435546875,
                        "total_cost": 1518.254358418397
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7096
Profiling... [1024/50176]	Loss: 1.7975
Profiling... [1536/50176]	Loss: 1.8442
Profiling... [2048/50176]	Loss: 1.7159
Profiling... [2560/50176]	Loss: 1.7479
Profiling... [3072/50176]	Loss: 1.7985
Profiling... [3584/50176]	Loss: 1.7622
Profiling... [4096/50176]	Loss: 1.6657
Profiling... [4608/50176]	Loss: 1.8543
Profiling... [5120/50176]	Loss: 1.7676
Profiling... [5632/50176]	Loss: 1.7285
Profiling... [6144/50176]	Loss: 1.6393
Profiling... [6656/50176]	Loss: 1.7112
Profile done
epoch 1 train time consumed: 18.41s
Validation Epoch: 8, Average loss: 0.0039, Accuracy: 0.4587
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.35504207680846,
                        "time": 13.480574184999568,
                        "accuracy": 0.45869140625,
                        "total_cost": 3566.5277900334772
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6742
Profiling... [1024/50176]	Loss: 1.7957
Profiling... [1536/50176]	Loss: 1.6972
Profiling... [2048/50176]	Loss: 1.7040
Profiling... [2560/50176]	Loss: 1.7694
Profiling... [3072/50176]	Loss: 1.7751
Profiling... [3584/50176]	Loss: 1.7179
Profiling... [4096/50176]	Loss: 1.8339
Profiling... [4608/50176]	Loss: 1.8431
Profiling... [5120/50176]	Loss: 1.7087
Profiling... [5632/50176]	Loss: 1.7769
Profiling... [6144/50176]	Loss: 1.7006
Profiling... [6656/50176]	Loss: 1.8791
Profile done
epoch 1 train time consumed: 7.16s
Validation Epoch: 8, Average loss: 0.0042, Accuracy: 0.4333
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.3519176458216,
                        "time": 4.60118896899985,
                        "accuracy": 0.43330078125,
                        "total_cost": 1288.627043847345
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7455
Profiling... [1024/50176]	Loss: 1.6932
Profiling... [1536/50176]	Loss: 1.6887
Profiling... [2048/50176]	Loss: 1.7929
Profiling... [2560/50176]	Loss: 1.7366
Profiling... [3072/50176]	Loss: 1.7821
Profiling... [3584/50176]	Loss: 1.9731
Profiling... [4096/50176]	Loss: 1.9046
Profiling... [4608/50176]	Loss: 1.7233
Profiling... [5120/50176]	Loss: 1.8885
Profiling... [5632/50176]	Loss: 1.6246
Profiling... [6144/50176]	Loss: 1.7549
Profiling... [6656/50176]	Loss: 1.7933
Profile done
epoch 1 train time consumed: 7.32s
Validation Epoch: 8, Average loss: 0.0043, Accuracy: 0.4197
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.37331333533174,
                        "time": 4.755462686000101,
                        "accuracy": 0.4197265625,
                        "total_cost": 1375.1482851228138
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.5319
Profiling... [1024/50176]	Loss: 1.7664
Profiling... [1536/50176]	Loss: 1.5300
Profiling... [2048/50176]	Loss: 1.6421
Profiling... [2560/50176]	Loss: 1.8161
Profiling... [3072/50176]	Loss: 1.6472
Profiling... [3584/50176]	Loss: 1.7849
Profiling... [4096/50176]	Loss: 1.7695
Profiling... [4608/50176]	Loss: 1.7611
Profiling... [5120/50176]	Loss: 1.7738
Profiling... [5632/50176]	Loss: 1.6036
Profiling... [6144/50176]	Loss: 1.7930
Profiling... [6656/50176]	Loss: 1.6976
Profile done
epoch 1 train time consumed: 8.23s
Validation Epoch: 8, Average loss: 0.0041, Accuracy: 0.4398
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.37548631498629,
                        "time": 5.5651261860002705,
                        "accuracy": 0.43984375,
                        "total_cost": 1535.7042068462895
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.6802
Profiling... [1024/50176]	Loss: 1.6203
Profiling... [1536/50176]	Loss: 1.8084
Profiling... [2048/50176]	Loss: 1.6865
Profiling... [2560/50176]	Loss: 1.6958
Profiling... [3072/50176]	Loss: 1.6249
Profiling... [3584/50176]	Loss: 1.6996
Profiling... [4096/50176]	Loss: 1.8648
Profiling... [4608/50176]	Loss: 1.7294
Profiling... [5120/50176]	Loss: 1.7955
Profiling... [5632/50176]	Loss: 1.7440
Profiling... [6144/50176]	Loss: 1.7972
Profiling... [6656/50176]	Loss: 1.6332
Profile done
epoch 1 train time consumed: 17.98s
Validation Epoch: 8, Average loss: 0.0040, Accuracy: 0.4536
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.33001097516409,
                        "time": 13.047908993001329,
                        "accuracy": 0.45361328125,
                        "total_cost": 3489.983663973228
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.6015
Profiling... [1024/50176]	Loss: 1.7346
Profiling... [1536/50176]	Loss: 1.9260
Profiling... [2048/50176]	Loss: 1.8104
Profiling... [2560/50176]	Loss: 1.9042
Profiling... [3072/50176]	Loss: 1.7719
Profiling... [3584/50176]	Loss: 1.8658
Profiling... [4096/50176]	Loss: 1.9585
Profiling... [4608/50176]	Loss: 1.7946
Profiling... [5120/50176]	Loss: 1.9753
Profiling... [5632/50176]	Loss: 1.8190
Profiling... [6144/50176]	Loss: 1.7473
Profiling... [6656/50176]	Loss: 1.7996
Profile done
epoch 1 train time consumed: 7.17s
Validation Epoch: 8, Average loss: 0.0046, Accuracy: 0.3912
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.32678917596583,
                        "time": 4.600046923998889,
                        "accuracy": 0.3912109375,
                        "total_cost": 1426.618915396667
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7383
Profiling... [1024/50176]	Loss: 1.7552
Profiling... [1536/50176]	Loss: 1.9347
Profiling... [2048/50176]	Loss: 1.7656
Profiling... [2560/50176]	Loss: 1.8414
Profiling... [3072/50176]	Loss: 1.8181
Profiling... [3584/50176]	Loss: 1.9658
Profiling... [4096/50176]	Loss: 1.7778
Profiling... [4608/50176]	Loss: 1.7976
Profiling... [5120/50176]	Loss: 1.9367
Profiling... [5632/50176]	Loss: 1.8354
Profiling... [6144/50176]	Loss: 1.8241
Profiling... [6656/50176]	Loss: 1.8950
Profile done
epoch 1 train time consumed: 7.40s
Validation Epoch: 8, Average loss: 0.0056, Accuracy: 0.3396
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.34815769912845,
                        "time": 4.761415472999943,
                        "accuracy": 0.3396484375,
                        "total_cost": 1701.138388686588
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7634
Profiling... [1024/50176]	Loss: 1.7536
Profiling... [1536/50176]	Loss: 1.8355
Profiling... [2048/50176]	Loss: 2.0057
Profiling... [2560/50176]	Loss: 1.8139
Profiling... [3072/50176]	Loss: 1.7980
Profiling... [3584/50176]	Loss: 1.6601
Profiling... [4096/50176]	Loss: 1.8184
Profiling... [4608/50176]	Loss: 1.7890
Profiling... [5120/50176]	Loss: 1.7723
Profiling... [5632/50176]	Loss: 1.9511
Profiling... [6144/50176]	Loss: 1.7662
Profiling... [6656/50176]	Loss: 1.8999
Profile done
epoch 1 train time consumed: 8.28s
Validation Epoch: 8, Average loss: 0.0054, Accuracy: 0.3467
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.35128184273381,
                        "time": 5.536432511000385,
                        "accuracy": 0.3466796875,
                        "total_cost": 1937.965234971207
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7125
Profiling... [1024/50176]	Loss: 1.8520
Profiling... [1536/50176]	Loss: 1.7885
Profiling... [2048/50176]	Loss: 1.8824
Profiling... [2560/50176]	Loss: 1.8401
Profiling... [3072/50176]	Loss: 1.8252
Profiling... [3584/50176]	Loss: 1.8934
Profiling... [4096/50176]	Loss: 1.8782
Profiling... [4608/50176]	Loss: 1.7551
Profiling... [5120/50176]	Loss: 1.9750
Profiling... [5632/50176]	Loss: 1.8852
Profiling... [6144/50176]	Loss: 1.8791
Profiling... [6656/50176]	Loss: 1.8153
Profile done
epoch 1 train time consumed: 17.95s
Validation Epoch: 8, Average loss: 0.0060, Accuracy: 0.2780
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.30585911113765,
                        "time": 13.040028540999629,
                        "accuracy": 0.27802734375,
                        "total_cost": 5689.48306905412
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7319
Profiling... [1024/50176]	Loss: 1.6977
Profiling... [1536/50176]	Loss: 1.7537
Profiling... [2048/50176]	Loss: 1.9360
Profiling... [2560/50176]	Loss: 1.7768
Profiling... [3072/50176]	Loss: 1.7228
Profiling... [3584/50176]	Loss: 1.7991
Profiling... [4096/50176]	Loss: 1.9708
Profiling... [4608/50176]	Loss: 1.7000
Profiling... [5120/50176]	Loss: 1.9112
Profiling... [5632/50176]	Loss: 1.7897
Profiling... [6144/50176]	Loss: 1.8379
Profiling... [6656/50176]	Loss: 1.8223
Profile done
epoch 1 train time consumed: 7.17s
Validation Epoch: 8, Average loss: 0.0052, Accuracy: 0.3463
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.30232297465088,
                        "time": 4.599430320000465,
                        "accuracy": 0.3462890625,
                        "total_cost": 1611.144106453256
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.6532
Profiling... [1024/50176]	Loss: 1.8153
Profiling... [1536/50176]	Loss: 1.8727
Profiling... [2048/50176]	Loss: 1.7960
Profiling... [2560/50176]	Loss: 1.9215
Profiling... [3072/50176]	Loss: 1.9425
Profiling... [3584/50176]	Loss: 1.9054
Profiling... [4096/50176]	Loss: 1.8291
Profiling... [4608/50176]	Loss: 1.8754
Profiling... [5120/50176]	Loss: 1.8523
Profiling... [5632/50176]	Loss: 1.8085
Profiling... [6144/50176]	Loss: 1.8521
Profiling... [6656/50176]	Loss: 1.8022
Profile done
epoch 1 train time consumed: 7.27s
Validation Epoch: 8, Average loss: 0.0107, Accuracy: 0.1882
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.32434210133607,
                        "time": 4.759062316999916,
                        "accuracy": 0.18818359375,
                        "total_cost": 3068.2276447347044
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.6311
Profiling... [1024/50176]	Loss: 1.6969
Profiling... [1536/50176]	Loss: 1.7532
Profiling... [2048/50176]	Loss: 1.9784
Profiling... [2560/50176]	Loss: 1.9652
Profiling... [3072/50176]	Loss: 1.9785
Profiling... [3584/50176]	Loss: 1.9167
Profiling... [4096/50176]	Loss: 2.0065
Profiling... [4608/50176]	Loss: 1.8785
Profiling... [5120/50176]	Loss: 1.8971
Profiling... [5632/50176]	Loss: 1.9207
Profiling... [6144/50176]	Loss: 1.7401
Profiling... [6656/50176]	Loss: 1.7168
Profile done
epoch 1 train time consumed: 8.17s
Validation Epoch: 8, Average loss: 0.0066, Accuracy: 0.2804
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.32777000144803,
                        "time": 5.533212530999663,
                        "accuracy": 0.28037109375,
                        "total_cost": 2394.442053034425
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7189
Profiling... [1024/50176]	Loss: 1.8790
Profiling... [1536/50176]	Loss: 1.8188
Profiling... [2048/50176]	Loss: 1.8076
Profiling... [2560/50176]	Loss: 1.8795
Profiling... [3072/50176]	Loss: 1.8243
Profiling... [3584/50176]	Loss: 1.8849
Profiling... [4096/50176]	Loss: 1.8413
Profiling... [4608/50176]	Loss: 1.9325
Profiling... [5120/50176]	Loss: 2.0195
Profiling... [5632/50176]	Loss: 1.8787
Profiling... [6144/50176]	Loss: 1.9118
Profiling... [6656/50176]	Loss: 1.8472
Profile done
epoch 1 train time consumed: 17.95s
Validation Epoch: 8, Average loss: 0.0078, Accuracy: 0.2428
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.2829369873452,
                        "time": 13.031229629001245,
                        "accuracy": 0.2427734375,
                        "total_cost": 6510.044172199785
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [512/50176]	Loss: 1.7617
Profiling... [1024/50176]	Loss: 1.7410
Profiling... [1536/50176]	Loss: 1.8661
Profiling... [2048/50176]	Loss: 1.7440
Profiling... [2560/50176]	Loss: 1.7792
Profiling... [3072/50176]	Loss: 1.7073
Profiling... [3584/50176]	Loss: 1.7620
Profiling... [4096/50176]	Loss: 1.9006
Profiling... [4608/50176]	Loss: 1.7328
Profiling... [5120/50176]	Loss: 1.8756
Profiling... [5632/50176]	Loss: 1.8152
Profiling... [6144/50176]	Loss: 1.9521
Profiling... [6656/50176]	Loss: 1.9126
Profile done
epoch 1 train time consumed: 7.14s
Validation Epoch: 8, Average loss: 0.0058, Accuracy: 0.3261
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.27952891078291,
                        "time": 4.604102958999647,
                        "accuracy": 0.32607421875,
                        "total_cost": 1712.4427685965861
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [512/50176]	Loss: 1.7442
Profiling... [1024/50176]	Loss: 1.7036
Profiling... [1536/50176]	Loss: 1.9900
Profiling... [2048/50176]	Loss: 1.8910
Profiling... [2560/50176]	Loss: 1.8158
Profiling... [3072/50176]	Loss: 1.9904
Profiling... [3584/50176]	Loss: 1.8030
Profiling... [4096/50176]	Loss: 1.8253
Profiling... [4608/50176]	Loss: 1.9731
Profiling... [5120/50176]	Loss: 1.8081
Profiling... [5632/50176]	Loss: 1.9745
Profiling... [6144/50176]	Loss: 2.0336
Profiling... [6656/50176]	Loss: 1.8950
Profile done
epoch 1 train time consumed: 7.27s
Validation Epoch: 8, Average loss: 0.0054, Accuracy: 0.3157
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.3017729080412,
                        "time": 4.749539245000051,
                        "accuracy": 0.31572265625,
                        "total_cost": 1824.7899525418543
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [512/50176]	Loss: 1.7126
Profiling... [1024/50176]	Loss: 1.7183
Profiling... [1536/50176]	Loss: 1.8110
Profiling... [2048/50176]	Loss: 1.7865
Profiling... [2560/50176]	Loss: 1.8088
Profiling... [3072/50176]	Loss: 1.8858
Profiling... [3584/50176]	Loss: 1.8484
Profiling... [4096/50176]	Loss: 1.9356
Profiling... [4608/50176]	Loss: 1.9652
Profiling... [5120/50176]	Loss: 1.8944
Profiling... [5632/50176]	Loss: 1.8789
Profiling... [6144/50176]	Loss: 1.7724
Profiling... [6656/50176]	Loss: 1.8133
Profile done
epoch 1 train time consumed: 8.32s
Validation Epoch: 8, Average loss: 0.0061, Accuracy: 0.3089
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.30406347943071,
                        "time": 5.548426184999698,
                        "accuracy": 0.30888671875,
                        "total_cost": 2178.9432866515535
                    },
                    
[Training Loop] Profiling with batch size 512 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [512/50176]	Loss: 1.7271
Profiling... [1024/50176]	Loss: 1.7338
Profiling... [1536/50176]	Loss: 1.8311
Profiling... [2048/50176]	Loss: 1.9954
Profiling... [2560/50176]	Loss: 1.7957
Profiling... [3072/50176]	Loss: 1.8848
Profiling... [3584/50176]	Loss: 1.7727
Profiling... [4096/50176]	Loss: 1.8418
Profiling... [4608/50176]	Loss: 1.7594
Profiling... [5120/50176]	Loss: 1.8426
Profiling... [5632/50176]	Loss: 1.7658
Profiling... [6144/50176]	Loss: 1.8437
Profiling... [6656/50176]	Loss: 1.8571
Profile done
epoch 1 train time consumed: 18.90s
Validation Epoch: 8, Average loss: 0.0073, Accuracy: 0.2778
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 512,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.25543586260736,
                        "time": 13.951351519999662,
                        "accuracy": 0.27783203125,
                        "total_cost": 6088.848725681296
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6534
Profiling... [2048/50176]	Loss: 1.7191
Profiling... [3072/50176]	Loss: 1.6065
Profiling... [4096/50176]	Loss: 1.7219
Profiling... [5120/50176]	Loss: 1.7085
Profiling... [6144/50176]	Loss: 1.5534
Profiling... [7168/50176]	Loss: 1.6748
Profiling... [8192/50176]	Loss: 1.7014
Profiling... [9216/50176]	Loss: 1.6954
Profiling... [10240/50176]	Loss: 1.7625
Profiling... [11264/50176]	Loss: 1.7584
Profiling... [12288/50176]	Loss: 1.7418
Profiling... [13312/50176]	Loss: 1.7248
Profile done
epoch 1 train time consumed: 13.35s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.4999
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.26602939228722,
                        "time": 9.049493960999826,
                        "accuracy": 0.49990234375,
                        "total_cost": 2195.2211554517853
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7213
Profiling... [2048/50176]	Loss: 1.6913
Profiling... [3072/50176]	Loss: 1.6904
Profiling... [4096/50176]	Loss: 1.6363
Profiling... [5120/50176]	Loss: 1.7005
Profiling... [6144/50176]	Loss: 1.7958
Profiling... [7168/50176]	Loss: 1.6690
Profiling... [8192/50176]	Loss: 1.7175
Profiling... [9216/50176]	Loss: 1.7173
Profiling... [10240/50176]	Loss: 1.6049
Profiling... [11264/50176]	Loss: 1.7482
Profiling... [12288/50176]	Loss: 1.6707
Profiling... [13312/50176]	Loss: 1.6863
Profile done
epoch 1 train time consumed: 13.67s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5035
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.30193138596846,
                        "time": 9.36143787400033,
                        "accuracy": 0.503515625,
                        "total_cost": 2255.2636666756757
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6994
Profiling... [2048/50176]	Loss: 1.6831
Profiling... [3072/50176]	Loss: 1.6305
Profiling... [4096/50176]	Loss: 1.7925
Profiling... [5120/50176]	Loss: 1.6711
Profiling... [6144/50176]	Loss: 1.5952
Profiling... [7168/50176]	Loss: 1.7275
Profiling... [8192/50176]	Loss: 1.6520
Profiling... [9216/50176]	Loss: 1.7054
Profiling... [10240/50176]	Loss: 1.5844
Profiling... [11264/50176]	Loss: 1.6759
Profiling... [12288/50176]	Loss: 1.7133
Profiling... [13312/50176]	Loss: 1.6685
Profile done
epoch 1 train time consumed: 15.77s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.4983
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.3032493906893,
                        "time": 11.021757024998806,
                        "accuracy": 0.49833984375,
                        "total_cost": 2682.8578085715467
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7230
Profiling... [2048/50176]	Loss: 1.6627
Profiling... [3072/50176]	Loss: 1.6845
Profiling... [4096/50176]	Loss: 1.6136
Profiling... [5120/50176]	Loss: 1.6920
Profiling... [6144/50176]	Loss: 1.6789
Profiling... [7168/50176]	Loss: 1.6893
Profiling... [8192/50176]	Loss: 1.7554
Profiling... [9216/50176]	Loss: 1.6583
Profiling... [10240/50176]	Loss: 1.6991
Profiling... [11264/50176]	Loss: 1.6916
Profiling... [12288/50176]	Loss: 1.7653
Profiling... [13312/50176]	Loss: 1.6813
Profile done
epoch 1 train time consumed: 38.83s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5041
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.0,
                        "energy": 121.21163763765989,
                        "time": 28.71846214300058,
                        "accuracy": 0.5041015625,
                        "total_cost": 6905.377974876322
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6689
Profiling... [2048/50176]	Loss: 1.7336
Profiling... [3072/50176]	Loss: 1.8547
Profiling... [4096/50176]	Loss: 1.6142
Profiling... [5120/50176]	Loss: 1.6611
Profiling... [6144/50176]	Loss: 1.6528
Profiling... [7168/50176]	Loss: 1.6557
Profiling... [8192/50176]	Loss: 1.6937
Profiling... [9216/50176]	Loss: 1.6249
Profiling... [10240/50176]	Loss: 1.6626
Profiling... [11264/50176]	Loss: 1.6359
Profiling... [12288/50176]	Loss: 1.6867
Profiling... [13312/50176]	Loss: 1.6962
Profile done
epoch 1 train time consumed: 13.29s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5001
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.23394596093719,
                        "time": 9.020854213000348,
                        "accuracy": 0.50009765625,
                        "total_cost": 2186.8403870976485
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7410
Profiling... [2048/50176]	Loss: 1.6699
Profiling... [3072/50176]	Loss: 1.6411
Profiling... [4096/50176]	Loss: 1.7472
Profiling... [5120/50176]	Loss: 1.6933
Profiling... [6144/50176]	Loss: 1.7456
Profiling... [7168/50176]	Loss: 1.7123
Profiling... [8192/50176]	Loss: 1.6770
Profiling... [9216/50176]	Loss: 1.7403
Profiling... [10240/50176]	Loss: 1.7294
Profiling... [11264/50176]	Loss: 1.7418
Profiling... [12288/50176]	Loss: 1.6843
Profiling... [13312/50176]	Loss: 1.6098
Profile done
epoch 1 train time consumed: 13.62s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5012
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.26895389129295,
                        "time": 9.35851082299996,
                        "accuracy": 0.501171875,
                        "total_cost": 2264.486245333596
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7462
Profiling... [2048/50176]	Loss: 1.6814
Profiling... [3072/50176]	Loss: 1.6904
Profiling... [4096/50176]	Loss: 1.6820
Profiling... [5120/50176]	Loss: 1.6893
Profiling... [6144/50176]	Loss: 1.6744
Profiling... [7168/50176]	Loss: 1.6688
Profiling... [8192/50176]	Loss: 1.6595
Profiling... [9216/50176]	Loss: 1.6874
Profiling... [10240/50176]	Loss: 1.6018
Profiling... [11264/50176]	Loss: 1.6743
Profiling... [12288/50176]	Loss: 1.6495
Profiling... [13312/50176]	Loss: 1.7840
Profile done
epoch 1 train time consumed: 15.75s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5007
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.27078433101781,
                        "time": 10.99537733500074,
                        "accuracy": 0.50068359375,
                        "total_cost": 2663.194979975387
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7192
Profiling... [2048/50176]	Loss: 1.7197
Profiling... [3072/50176]	Loss: 1.6830
Profiling... [4096/50176]	Loss: 1.6937
Profiling... [5120/50176]	Loss: 1.6454
Profiling... [6144/50176]	Loss: 1.6929
Profiling... [7168/50176]	Loss: 1.6639
Profiling... [8192/50176]	Loss: 1.6190
Profiling... [9216/50176]	Loss: 1.7780
Profiling... [10240/50176]	Loss: 1.7168
Profiling... [11264/50176]	Loss: 1.5784
Profiling... [12288/50176]	Loss: 1.6561
Profiling... [13312/50176]	Loss: 1.8145
Profile done
epoch 1 train time consumed: 38.49s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5021
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.25,
                        "energy": 121.1839508553025,
                        "time": 28.42216000300141,
                        "accuracy": 0.5021484375,
                        "total_cost": 6859.146387377272
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6981
Profiling... [2048/50176]	Loss: 1.6815
Profiling... [3072/50176]	Loss: 1.6600
Profiling... [4096/50176]	Loss: 1.7286
Profiling... [5120/50176]	Loss: 1.7139
Profiling... [6144/50176]	Loss: 1.7180
Profiling... [7168/50176]	Loss: 1.7629
Profiling... [8192/50176]	Loss: 1.6300
Profiling... [9216/50176]	Loss: 1.6835
Profiling... [10240/50176]	Loss: 1.6321
Profiling... [11264/50176]	Loss: 1.6731
Profiling... [12288/50176]	Loss: 1.6635
Profiling... [13312/50176]	Loss: 1.7165
Profile done
epoch 1 train time consumed: 13.36s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5021
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.20451718204004,
                        "time": 9.03410590599924,
                        "accuracy": 0.50205078125,
                        "total_cost": 2181.0033673920398
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7277
Profiling... [2048/50176]	Loss: 1.7718
Profiling... [3072/50176]	Loss: 1.5933
Profiling... [4096/50176]	Loss: 1.7932
Profiling... [5120/50176]	Loss: 1.7707
Profiling... [6144/50176]	Loss: 1.7281
Profiling... [7168/50176]	Loss: 1.7688
Profiling... [8192/50176]	Loss: 1.7496
Profiling... [9216/50176]	Loss: 1.6614
Profiling... [10240/50176]	Loss: 1.7087
Profiling... [11264/50176]	Loss: 1.6979
Profiling... [12288/50176]	Loss: 1.6792
Profiling... [13312/50176]	Loss: 1.6073
Profile done
epoch 1 train time consumed: 13.66s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5023
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.2389286425196,
                        "time": 9.368371500999274,
                        "accuracy": 0.50234375,
                        "total_cost": 2261.0240973561736
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.8057
Profiling... [2048/50176]	Loss: 1.7275
Profiling... [3072/50176]	Loss: 1.7070
Profiling... [4096/50176]	Loss: 1.7081
Profiling... [5120/50176]	Loss: 1.5769
Profiling... [6144/50176]	Loss: 1.6895
Profiling... [7168/50176]	Loss: 1.7728
Profiling... [8192/50176]	Loss: 1.6972
Profiling... [9216/50176]	Loss: 1.7056
Profiling... [10240/50176]	Loss: 1.6570
Profiling... [11264/50176]	Loss: 1.7120
Profiling... [12288/50176]	Loss: 1.5957
Profiling... [13312/50176]	Loss: 1.6797
Profile done
epoch 1 train time consumed: 15.82s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.5019
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.23970223261423,
                        "time": 10.993530777999695,
                        "accuracy": 0.50185546875,
                        "total_cost": 2655.8491059778116
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.001 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.5706
Profiling... [2048/50176]	Loss: 1.6513
Profiling... [3072/50176]	Loss: 1.6647
Profiling... [4096/50176]	Loss: 1.6009
Profiling... [5120/50176]	Loss: 1.6996
Profiling... [6144/50176]	Loss: 1.7603
Profiling... [7168/50176]	Loss: 1.7674
Profiling... [8192/50176]	Loss: 1.7147
Profiling... [9216/50176]	Loss: 1.6783
Profiling... [10240/50176]	Loss: 1.6991
Profiling... [11264/50176]	Loss: 1.6570
Profiling... [12288/50176]	Loss: 1.6200
Profiling... [13312/50176]	Loss: 1.6817
Profile done
epoch 1 train time consumed: 38.87s
Validation Epoch: 8, Average loss: 0.0018, Accuracy: 0.4994
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.001,
                        "dr": 0.5,
                        "energy": 121.15021224653655,
                        "time": 28.779294091000338,
                        "accuracy": 0.4994140625,
                        "total_cost": 6981.416522347498
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6279
Profiling... [2048/50176]	Loss: 1.7029
Profiling... [3072/50176]	Loss: 1.7039
Profiling... [4096/50176]	Loss: 1.8230
Profiling... [5120/50176]	Loss: 1.7417
Profiling... [6144/50176]	Loss: 1.7802
Profiling... [7168/50176]	Loss: 1.7527
Profiling... [8192/50176]	Loss: 1.7899
Profiling... [9216/50176]	Loss: 1.7896
Profiling... [10240/50176]	Loss: 1.8162
Profiling... [11264/50176]	Loss: 1.7207
Profiling... [12288/50176]	Loss: 1.6893
Profiling... [13312/50176]	Loss: 1.7360
Profile done
epoch 1 train time consumed: 13.32s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4461
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.17288088024749,
                        "time": 9.039106965999963,
                        "accuracy": 0.44609375,
                        "total_cost": 2455.301450995287
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6248
Profiling... [2048/50176]	Loss: 1.6499
Profiling... [3072/50176]	Loss: 1.7197
Profiling... [4096/50176]	Loss: 1.6865
Profiling... [5120/50176]	Loss: 1.6404
Profiling... [6144/50176]	Loss: 1.7624
Profiling... [7168/50176]	Loss: 1.6770
Profiling... [8192/50176]	Loss: 1.8233
Profiling... [9216/50176]	Loss: 1.6665
Profiling... [10240/50176]	Loss: 1.7109
Profiling... [11264/50176]	Loss: 1.7783
Profiling... [12288/50176]	Loss: 1.7349
Profiling... [13312/50176]	Loss: 1.7132
Profile done
epoch 1 train time consumed: 13.69s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4488
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.20836659039405,
                        "time": 9.36388059500132,
                        "accuracy": 0.448828125,
                        "total_cost": 2528.764595283767
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6525
Profiling... [2048/50176]	Loss: 1.7667
Profiling... [3072/50176]	Loss: 1.6378
Profiling... [4096/50176]	Loss: 1.8109
Profiling... [5120/50176]	Loss: 1.7392
Profiling... [6144/50176]	Loss: 1.8431
Profiling... [7168/50176]	Loss: 1.6856
Profiling... [8192/50176]	Loss: 1.6688
Profiling... [9216/50176]	Loss: 1.7443
Profiling... [10240/50176]	Loss: 1.7623
Profiling... [11264/50176]	Loss: 1.7631
Profiling... [12288/50176]	Loss: 1.6338
Profiling... [13312/50176]	Loss: 1.8205
Profile done
epoch 1 train time consumed: 15.74s
Validation Epoch: 8, Average loss: 0.0025, Accuracy: 0.3776
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.21124129552835,
                        "time": 10.974674020999373,
                        "accuracy": 0.37763671875,
                        "total_cost": 3522.5755199397477
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7384
Profiling... [2048/50176]	Loss: 1.7123
Profiling... [3072/50176]	Loss: 1.7585
Profiling... [4096/50176]	Loss: 1.6512
Profiling... [5120/50176]	Loss: 1.7486
Profiling... [6144/50176]	Loss: 1.7079
Profiling... [7168/50176]	Loss: 1.7985
Profiling... [8192/50176]	Loss: 1.7178
Profiling... [9216/50176]	Loss: 1.6628
Profiling... [10240/50176]	Loss: 1.6519
Profiling... [11264/50176]	Loss: 1.7088
Profiling... [12288/50176]	Loss: 1.6494
Profiling... [13312/50176]	Loss: 1.7524
Profile done
epoch 1 train time consumed: 38.95s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4570
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.0,
                        "energy": 121.12188459317436,
                        "time": 28.88273579199995,
                        "accuracy": 0.45703125,
                        "total_cost": 7654.4686853114845
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6498
Profiling... [2048/50176]	Loss: 1.6956
Profiling... [3072/50176]	Loss: 1.7273
Profiling... [4096/50176]	Loss: 1.7244
Profiling... [5120/50176]	Loss: 1.6716
Profiling... [6144/50176]	Loss: 1.7728
Profiling... [7168/50176]	Loss: 1.6968
Profiling... [8192/50176]	Loss: 1.7260
Profiling... [9216/50176]	Loss: 1.6872
Profiling... [10240/50176]	Loss: 1.7046
Profiling... [11264/50176]	Loss: 1.7015
Profiling... [12288/50176]	Loss: 1.7512
Profiling... [13312/50176]	Loss: 1.8074
Profile done
epoch 1 train time consumed: 13.28s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4575
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.14236986561193,
                        "time": 9.027943701999902,
                        "accuracy": 0.45751953125,
                        "total_cost": 2390.425807802263
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7411
Profiling... [2048/50176]	Loss: 1.7531
Profiling... [3072/50176]	Loss: 1.6898
Profiling... [4096/50176]	Loss: 1.7128
Profiling... [5120/50176]	Loss: 1.6805
Profiling... [6144/50176]	Loss: 1.7077
Profiling... [7168/50176]	Loss: 1.6932
Profiling... [8192/50176]	Loss: 1.7575
Profiling... [9216/50176]	Loss: 1.6683
Profiling... [10240/50176]	Loss: 1.7712
Profiling... [11264/50176]	Loss: 1.7857
Profiling... [12288/50176]	Loss: 1.7066
Profiling... [13312/50176]	Loss: 1.7420
Profile done
epoch 1 train time consumed: 13.71s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4799
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.17731392782449,
                        "time": 9.389580241999283,
                        "accuracy": 0.4798828125,
                        "total_cost": 2371.004093077923
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.7151
Profiling... [2048/50176]	Loss: 1.6072
Profiling... [3072/50176]	Loss: 1.7407
Profiling... [4096/50176]	Loss: 1.7976
Profiling... [5120/50176]	Loss: 1.7266
Profiling... [6144/50176]	Loss: 1.8234
Profiling... [7168/50176]	Loss: 1.6490
Profiling... [8192/50176]	Loss: 1.6266
Profiling... [9216/50176]	Loss: 1.7152
Profiling... [10240/50176]	Loss: 1.7172
Profiling... [11264/50176]	Loss: 1.7644
Profiling... [12288/50176]	Loss: 1.6957
Profiling... [13312/50176]	Loss: 1.7765
Profile done
epoch 1 train time consumed: 15.87s
Validation Epoch: 8, Average loss: 0.0023, Accuracy: 0.3991
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.17969685131918,
                        "time": 10.992274296000687,
                        "accuracy": 0.39912109375,
                        "total_cost": 3337.4343971162543
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7224
Profiling... [2048/50176]	Loss: 1.8354
Profiling... [3072/50176]	Loss: 1.7537
Profiling... [4096/50176]	Loss: 1.7125
Profiling... [5120/50176]	Loss: 1.7893
Profiling... [6144/50176]	Loss: 1.7222
Profiling... [7168/50176]	Loss: 1.7304
Profiling... [8192/50176]	Loss: 1.7402
Profiling... [9216/50176]	Loss: 1.7780
Profiling... [10240/50176]	Loss: 1.6705
Profiling... [11264/50176]	Loss: 1.8489
Profiling... [12288/50176]	Loss: 1.6626
Profiling... [13312/50176]	Loss: 1.7654
Profile done
epoch 1 train time consumed: 38.90s
Validation Epoch: 8, Average loss: 0.0024, Accuracy: 0.3978
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.25,
                        "energy": 121.09319853923589,
                        "time": 28.815878628000064,
                        "accuracy": 0.39775390625,
                        "total_cost": 8772.77848678057
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7498
Profiling... [2048/50176]	Loss: 1.6262
Profiling... [3072/50176]	Loss: 1.7506
Profiling... [4096/50176]	Loss: 1.7419
Profiling... [5120/50176]	Loss: 1.7011
Profiling... [6144/50176]	Loss: 1.7522
Profiling... [7168/50176]	Loss: 1.7597
Profiling... [8192/50176]	Loss: 1.7933
Profiling... [9216/50176]	Loss: 1.7310
Profiling... [10240/50176]	Loss: 1.7349
Profiling... [11264/50176]	Loss: 1.6895
Profiling... [12288/50176]	Loss: 1.7483
Profiling... [13312/50176]	Loss: 1.6552
Profile done
epoch 1 train time consumed: 13.33s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4645
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.11314553082275,
                        "time": 9.071632129998761,
                        "accuracy": 0.464453125,
                        "total_cost": 2365.5646678287017
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.6356
Profiling... [2048/50176]	Loss: 1.6521
Profiling... [3072/50176]	Loss: 1.6760
Profiling... [4096/50176]	Loss: 1.8351
Profiling... [5120/50176]	Loss: 1.6319
Profiling... [6144/50176]	Loss: 1.7557
Profiling... [7168/50176]	Loss: 1.7341
Profiling... [8192/50176]	Loss: 1.7395
Profiling... [9216/50176]	Loss: 1.7832
Profiling... [10240/50176]	Loss: 1.7208
Profiling... [11264/50176]	Loss: 1.7339
Profiling... [12288/50176]	Loss: 1.7313
Profiling... [13312/50176]	Loss: 1.6888
Profile done
epoch 1 train time consumed: 13.72s
Validation Epoch: 8, Average loss: 0.0019, Accuracy: 0.4728
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.14730627878943,
                        "time": 9.368478197000513,
                        "accuracy": 0.47275390625,
                        "total_cost": 2400.754139719352
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6393
Profiling... [2048/50176]	Loss: 1.7482
Profiling... [3072/50176]	Loss: 1.7332
Profiling... [4096/50176]	Loss: 1.7038
Profiling... [5120/50176]	Loss: 1.8146
Profiling... [6144/50176]	Loss: 1.8325
Profiling... [7168/50176]	Loss: 1.7503
Profiling... [8192/50176]	Loss: 1.6784
Profiling... [9216/50176]	Loss: 1.7728
Profiling... [10240/50176]	Loss: 1.7407
Profiling... [11264/50176]	Loss: 1.7840
Profiling... [12288/50176]	Loss: 1.7034
Profiling... [13312/50176]	Loss: 1.7713
Profile done
epoch 1 train time consumed: 15.83s
Validation Epoch: 8, Average loss: 0.0025, Accuracy: 0.3808
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.14952154495958,
                        "time": 10.97473837400139,
                        "accuracy": 0.38076171875,
                        "total_cost": 3491.9064538742477
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.005 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.7518
Profiling... [2048/50176]	Loss: 1.6858
Profiling... [3072/50176]	Loss: 1.7637
Profiling... [4096/50176]	Loss: 1.7628
Profiling... [5120/50176]	Loss: 1.7179
Profiling... [6144/50176]	Loss: 1.7530
Profiling... [7168/50176]	Loss: 1.8309
Profiling... [8192/50176]	Loss: 1.6826
Profiling... [9216/50176]	Loss: 1.7009
Profiling... [10240/50176]	Loss: 1.6760
Profiling... [11264/50176]	Loss: 1.7462
Profiling... [12288/50176]	Loss: 1.7125
Profiling... [13312/50176]	Loss: 1.7775
Profile done
epoch 1 train time consumed: 38.77s
Validation Epoch: 8, Average loss: 0.0020, Accuracy: 0.4620
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.005,
                        "dr": 0.5,
                        "energy": 121.06508638267745,
                        "time": 28.7074099750007,
                        "accuracy": 0.46201171875,
                        "total_cost": 7522.460854130437
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7334
Profiling... [2048/50176]	Loss: 1.7464
Profiling... [3072/50176]	Loss: 1.8036
Profiling... [4096/50176]	Loss: 1.7830
Profiling... [5120/50176]	Loss: 1.7483
Profiling... [6144/50176]	Loss: 1.8262
Profiling... [7168/50176]	Loss: 1.7875
Profiling... [8192/50176]	Loss: 1.7636
Profiling... [9216/50176]	Loss: 1.7588
Profiling... [10240/50176]	Loss: 1.7709
Profiling... [11264/50176]	Loss: 1.8548
Profiling... [12288/50176]	Loss: 1.8665
Profiling... [13312/50176]	Loss: 1.7327
Profile done
epoch 1 train time consumed: 13.30s
Validation Epoch: 8, Average loss: 0.0030, Accuracy: 0.3082
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.08666963559612,
                        "time": 9.04550472200026,
                        "accuracy": 0.308203125,
                        "total_cost": 3553.7927850668943
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7127
Profiling... [2048/50176]	Loss: 1.7871
Profiling... [3072/50176]	Loss: 1.7889
Profiling... [4096/50176]	Loss: 1.7252
Profiling... [5120/50176]	Loss: 1.7800
Profiling... [6144/50176]	Loss: 1.7924
Profiling... [7168/50176]	Loss: 1.7039
Profiling... [8192/50176]	Loss: 1.8679
Profiling... [9216/50176]	Loss: 1.7623
Profiling... [10240/50176]	Loss: 1.7895
Profiling... [11264/50176]	Loss: 1.7626
Profiling... [12288/50176]	Loss: 1.7975
Profiling... [13312/50176]	Loss: 1.7965
Profile done
epoch 1 train time consumed: 13.66s
Validation Epoch: 8, Average loss: 0.0036, Accuracy: 0.2401
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.11985870335624,
                        "time": 9.345196968000892,
                        "accuracy": 0.24013671875,
                        "total_cost": 4713.518791341864
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6736
Profiling... [2048/50176]	Loss: 1.7088
Profiling... [3072/50176]	Loss: 1.8391
Profiling... [4096/50176]	Loss: 1.7735
Profiling... [5120/50176]	Loss: 1.8648
Profiling... [6144/50176]	Loss: 1.7161
Profiling... [7168/50176]	Loss: 1.8465
Profiling... [8192/50176]	Loss: 1.7674
Profiling... [9216/50176]	Loss: 1.8290
Profiling... [10240/50176]	Loss: 1.8253
Profiling... [11264/50176]	Loss: 1.7626
Profiling... [12288/50176]	Loss: 1.7934
Profiling... [13312/50176]	Loss: 1.7285
Profile done
epoch 1 train time consumed: 15.85s
Validation Epoch: 8, Average loss: 0.0032, Accuracy: 0.3037
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.12131579359904,
                        "time": 10.970277831000203,
                        "accuracy": 0.3037109375,
                        "total_cost": 4374.997148438536
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.0 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6837
Profiling... [2048/50176]	Loss: 1.7822
Profiling... [3072/50176]	Loss: 1.7069
Profiling... [4096/50176]	Loss: 1.8210
Profiling... [5120/50176]	Loss: 1.6858
Profiling... [6144/50176]	Loss: 1.8126
Profiling... [7168/50176]	Loss: 1.7730
Profiling... [8192/50176]	Loss: 1.7633
Profiling... [9216/50176]	Loss: 1.8294
Profiling... [10240/50176]	Loss: 1.7600
Profiling... [11264/50176]	Loss: 1.7577
Profiling... [12288/50176]	Loss: 1.8972
Profiling... [13312/50176]	Loss: 1.7514
Profile done
epoch 1 train time consumed: 38.44s
Validation Epoch: 8, Average loss: 0.0024, Accuracy: 0.3916
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.0,
                        "energy": 121.03846879545698,
                        "time": 28.450494101000004,
                        "accuracy": 0.3916015625,
                        "total_cost": 8793.64275381108
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.6960
Profiling... [2048/50176]	Loss: 1.6665
Profiling... [3072/50176]	Loss: 1.8117
Profiling... [4096/50176]	Loss: 1.7566
Profiling... [5120/50176]	Loss: 1.8798
Profiling... [6144/50176]	Loss: 1.8356
Profiling... [7168/50176]	Loss: 1.7568
Profiling... [8192/50176]	Loss: 1.7916
Profiling... [9216/50176]	Loss: 1.8124
Profiling... [10240/50176]	Loss: 1.8563
Profiling... [11264/50176]	Loss: 1.7954
Profiling... [12288/50176]	Loss: 1.7586
Profiling... [13312/50176]	Loss: 1.7579
Profile done
epoch 1 train time consumed: 14.04s
Validation Epoch: 8, Average loss: 0.0031, Accuracy: 0.3069
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.05782058034559,
                        "time": 9.500938213999689,
                        "accuracy": 0.30693359375,
                        "total_cost": 3747.2694324627782
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7425
Profiling... [2048/50176]	Loss: 1.8200
Profiling... [3072/50176]	Loss: 1.7551
Profiling... [4096/50176]	Loss: 1.8365
Profiling... [5120/50176]	Loss: 1.8216
Profiling... [6144/50176]	Loss: 1.8005
Profiling... [7168/50176]	Loss: 1.8051
Profiling... [8192/50176]	Loss: 1.7310
Profiling... [9216/50176]	Loss: 1.8131
Profiling... [10240/50176]	Loss: 1.7833
Profiling... [11264/50176]	Loss: 1.7403
Profiling... [12288/50176]	Loss: 1.8200
Profiling... [13312/50176]	Loss: 1.8170
Profile done
epoch 1 train time consumed: 14.31s
Validation Epoch: 8, Average loss: 0.0024, Accuracy: 0.3796
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.0924135231391,
                        "time": 9.739814159000161,
                        "accuracy": 0.37958984375,
                        "total_cost": 3107.0841941623285
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6414
Profiling... [2048/50176]	Loss: 1.7047
Profiling... [3072/50176]	Loss: 1.7854
Profiling... [4096/50176]	Loss: 1.6939
Profiling... [5120/50176]	Loss: 1.8380
Profiling... [6144/50176]	Loss: 1.7457
Profiling... [7168/50176]	Loss: 1.8086
Profiling... [8192/50176]	Loss: 1.8003
Profiling... [9216/50176]	Loss: 1.8337
Profiling... [10240/50176]	Loss: 1.8709
Profiling... [11264/50176]	Loss: 1.8207
Profiling... [12288/50176]	Loss: 1.8044
Profiling... [13312/50176]	Loss: 1.7145
Profile done
epoch 1 train time consumed: 16.13s
Validation Epoch: 8, Average loss: 0.0027, Accuracy: 0.3330
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.09391541743058,
                        "time": 11.281821298998693,
                        "accuracy": 0.3330078125,
                        "total_cost": 4102.486076465592
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.25 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6281
Profiling... [2048/50176]	Loss: 1.7412
Profiling... [3072/50176]	Loss: 1.6735
Profiling... [4096/50176]	Loss: 1.8432
Profiling... [5120/50176]	Loss: 1.7999
Profiling... [6144/50176]	Loss: 1.7895
Profiling... [7168/50176]	Loss: 1.7884
Profiling... [8192/50176]	Loss: 1.7480
Profiling... [9216/50176]	Loss: 1.7524
Profiling... [10240/50176]	Loss: 1.8718
Profiling... [11264/50176]	Loss: 1.7704
Profiling... [12288/50176]	Loss: 1.8168
Profiling... [13312/50176]	Loss: 1.7631
Profile done
epoch 1 train time consumed: 39.54s
Validation Epoch: 8, Average loss: 0.0038, Accuracy: 0.2437
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.25,
                        "energy": 121.00481878971249,
                        "time": 29.300627974000236,
                        "accuracy": 0.24375,
                        "total_cost": 14545.711501204838
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 175W.
Warm-up started with power limit 175W
Profiling... [1024/50176]	Loss: 1.7338
Profiling... [2048/50176]	Loss: 1.7020
Profiling... [3072/50176]	Loss: 1.7265
Profiling... [4096/50176]	Loss: 1.8372
Profiling... [5120/50176]	Loss: 1.8104
Profiling... [6144/50176]	Loss: 1.7755
Profiling... [7168/50176]	Loss: 1.7486
Profiling... [8192/50176]	Loss: 1.8730
Profiling... [9216/50176]	Loss: 1.7137
Profiling... [10240/50176]	Loss: 1.7855
Profiling... [11264/50176]	Loss: 1.8735
Profiling... [12288/50176]	Loss: 1.7951
Profiling... [13312/50176]	Loss: 1.8105
Profile done
epoch 1 train time consumed: 13.40s
Validation Epoch: 8, Average loss: 0.0023, Accuracy: 0.3787
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 175000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.02491453019309,
                        "time": 9.021461913000167,
                        "accuracy": 0.3787109375,
                        "total_cost": 2882.9947826849793
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 150
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 150W.
Warm-up started with power limit 150W
Profiling... [1024/50176]	Loss: 1.7159
Profiling... [2048/50176]	Loss: 1.7789
Profiling... [3072/50176]	Loss: 1.7028
Profiling... [4096/50176]	Loss: 1.6878
Profiling... [5120/50176]	Loss: 1.7631
Profiling... [6144/50176]	Loss: 1.7958
Profiling... [7168/50176]	Loss: 1.8510
Profiling... [8192/50176]	Loss: 1.7613
Profiling... [9216/50176]	Loss: 1.8079
Profiling... [10240/50176]	Loss: 1.7469
Profiling... [11264/50176]	Loss: 1.7289
Profiling... [12288/50176]	Loss: 1.8511
Profiling... [13312/50176]	Loss: 1.8266
Profile done
epoch 1 train time consumed: 13.71s
Validation Epoch: 8, Average loss: 0.0023, Accuracy: 0.3932
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 150000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.05860058084998,
                        "time": 9.366330171998925,
                        "accuracy": 0.3931640625,
                        "total_cost": 2883.973718224518
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 125
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 125W.
Warm-up started with power limit 125W
Profiling... [1024/50176]	Loss: 1.6417
Profiling... [2048/50176]	Loss: 1.7500
Profiling... [3072/50176]	Loss: 1.7434
Profiling... [4096/50176]	Loss: 1.8313
Profiling... [5120/50176]	Loss: 1.7841
Profiling... [6144/50176]	Loss: 1.7754
Profiling... [7168/50176]	Loss: 1.8120
Profiling... [8192/50176]	Loss: 1.8141
Profiling... [9216/50176]	Loss: 1.6616
Profiling... [10240/50176]	Loss: 1.8057
Profiling... [11264/50176]	Loss: 1.8968
Profiling... [12288/50176]	Loss: 1.8290
Profiling... [13312/50176]	Loss: 1.8813
Profile done
epoch 1 train time consumed: 15.73s
Validation Epoch: 8, Average loss: 0.0031, Accuracy: 0.3068
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 125000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 121.06131142016714,
                        "time": 10.964970984001411,
                        "accuracy": 0.3068359375,
                        "total_cost": 4326.200437350307
                    },
                    
[Training Loop] Profiling with batch size 1024 learning rate 0.01 dropout rate 0.5 power limit 100
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[ProfileDataLoader] Profiling for 3 warmup iters and 10 profile iters
[GPU_0] Set GPU power limit to 100W.
Warm-up started with power limit 100W
Profiling... [1024/50176]	Loss: 1.6146
Profiling... [2048/50176]	Loss: 1.7467
Profiling... [3072/50176]	Loss: 1.7286
Profiling... [4096/50176]	Loss: 1.9110
Profiling... [5120/50176]	Loss: 1.7890
Profiling... [6144/50176]	Loss: 1.7801
Profiling... [7168/50176]	Loss: 1.7955
Profiling... [8192/50176]	Loss: 1.8539
Profiling... [9216/50176]	Loss: 1.8545
Profiling... [10240/50176]	Loss: 1.8666
Profiling... [11264/50176]	Loss: 1.7946
Profiling... [12288/50176]	Loss: 1.7888
Profiling... [13312/50176]	Loss: 1.7555
Profile done
epoch 1 train time consumed: 38.54s
Validation Epoch: 8, Average loss: 0.0029, Accuracy: 0.3232
[ProfileDataLoader] writing cost to history_all.py

                    {
                        "profile_threshold": 0.5,
                        "bs": 1024,
                        "pl": 100000,
                        "lr": 0.01,
                        "dr": 0.5,
                        "energy": 120.97965996083657,
                        "time": 28.636331670999425,
                        "accuracy": 0.3232421875,
                        "total_cost": 10717.70270730285
                    },
                    
[Training Loop] The optimal parameters are lr: 0.001 dr: 0.25 bs: 128 pl: 175
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
ProfileDataLoader constructor: called constructor of DataLoader!
Eta knob set to 1.0
[GPU_0] Set GPU power limit to 175W.
[GPU_0] Set GPU power limit to 175W.
Training Epoch: 8 [128/50048]	Loss: 1.6163
Training Epoch: 8 [256/50048]	Loss: 1.7627
Training Epoch: 8 [384/50048]	Loss: 1.7996
Training Epoch: 8 [512/50048]	Loss: 1.8660
Training Epoch: 8 [640/50048]	Loss: 1.7211
Training Epoch: 8 [768/50048]	Loss: 1.9356
Training Epoch: 8 [896/50048]	Loss: 1.6190
Training Epoch: 8 [1024/50048]	Loss: 1.6839
Training Epoch: 8 [1152/50048]	Loss: 1.8387
Training Epoch: 8 [1280/50048]	Loss: 1.7480
Training Epoch: 8 [1408/50048]	Loss: 1.9091
Training Epoch: 8 [1536/50048]	Loss: 1.7887
Training Epoch: 8 [1664/50048]	Loss: 1.8480
Training Epoch: 8 [1792/50048]	Loss: 1.6557
Training Epoch: 8 [1920/50048]	Loss: 1.5955
Training Epoch: 8 [2048/50048]	Loss: 1.7040
Training Epoch: 8 [2176/50048]	Loss: 1.7811
Training Epoch: 8 [2304/50048]	Loss: 1.7588
Training Epoch: 8 [2432/50048]	Loss: 1.8642
Training Epoch: 8 [2560/50048]	Loss: 1.8716
Training Epoch: 8 [2688/50048]	Loss: 1.7593
Training Epoch: 8 [2816/50048]	Loss: 1.8552
Training Epoch: 8 [2944/50048]	Loss: 1.8103
Training Epoch: 8 [3072/50048]	Loss: 1.5399
Training Epoch: 8 [3200/50048]	Loss: 1.7746
Training Epoch: 8 [3328/50048]	Loss: 1.6410
Training Epoch: 8 [3456/50048]	Loss: 1.8028
Training Epoch: 8 [3584/50048]	Loss: 1.9907
Training Epoch: 8 [3712/50048]	Loss: 1.7530
Training Epoch: 8 [3840/50048]	Loss: 1.5770
Training Epoch: 8 [3968/50048]	Loss: 1.4893
Training Epoch: 8 [4096/50048]	Loss: 1.4988
Training Epoch: 8 [4224/50048]	Loss: 1.7288
Training Epoch: 8 [4352/50048]	Loss: 1.7283
Training Epoch: 8 [4480/50048]	Loss: 1.9279
Training Epoch: 8 [4608/50048]	Loss: 1.8232
Training Epoch: 8 [4736/50048]	Loss: 1.6187
Training Epoch: 8 [4864/50048]	Loss: 1.5631
Training Epoch: 8 [4992/50048]	Loss: 1.6029
Training Epoch: 8 [5120/50048]	Loss: 1.9051
Training Epoch: 8 [5248/50048]	Loss: 1.6029
Training Epoch: 8 [5376/50048]	Loss: 1.7039
Training Epoch: 8 [5504/50048]	Loss: 1.8363
Training Epoch: 8 [5632/50048]	Loss: 2.0075
Training Epoch: 8 [5760/50048]	Loss: 1.5275
Training Epoch: 8 [5888/50048]	Loss: 2.1533
Training Epoch: 8 [6016/50048]	Loss: 1.7753
Training Epoch: 8 [6144/50048]	Loss: 1.9931
Training Epoch: 8 [6272/50048]	Loss: 1.9146
Training Epoch: 8 [6400/50048]	Loss: 1.7276
Training Epoch: 8 [6528/50048]	Loss: 1.7298
Training Epoch: 8 [6656/50048]	Loss: 1.6341
Training Epoch: 8 [6784/50048]	Loss: 1.7514
Training Epoch: 8 [6912/50048]	Loss: 1.6391
Training Epoch: 8 [7040/50048]	Loss: 1.7754
Training Epoch: 8 [7168/50048]	Loss: 1.5944
Training Epoch: 8 [7296/50048]	Loss: 1.9751
Training Epoch: 8 [7424/50048]	Loss: 1.7822
Training Epoch: 8 [7552/50048]	Loss: 1.6965
Training Epoch: 8 [7680/50048]	Loss: 1.8099
Training Epoch: 8 [7808/50048]	Loss: 1.8210
Training Epoch: 8 [7936/50048]	Loss: 1.6076
Training Epoch: 8 [8064/50048]	Loss: 1.5914
Training Epoch: 8 [8192/50048]	Loss: 1.8221
Training Epoch: 8 [8320/50048]	Loss: 1.6479
Training Epoch: 8 [8448/50048]	Loss: 1.6069
Training Epoch: 8 [8576/50048]	Loss: 1.4754
Training Epoch: 8 [8704/50048]	Loss: 1.7777
Training Epoch: 8 [8832/50048]	Loss: 1.6421
Training Epoch: 8 [8960/50048]	Loss: 1.8743
Training Epoch: 8 [9088/50048]	Loss: 1.8044
Training Epoch: 8 [9216/50048]	Loss: 1.9125
Training Epoch: 8 [9344/50048]	Loss: 1.4245
Training Epoch: 8 [9472/50048]	Loss: 1.7172
Training Epoch: 8 [9600/50048]	Loss: 2.0053
Training Epoch: 8 [9728/50048]	Loss: 1.7021
Training Epoch: 8 [9856/50048]	Loss: 1.9108
Training Epoch: 8 [9984/50048]	Loss: 1.7617
Training Epoch: 8 [10112/50048]	Loss: 1.7285
Training Epoch: 8 [10240/50048]	Loss: 2.0132
Training Epoch: 8 [10368/50048]	Loss: 1.8634
Training Epoch: 8 [10496/50048]	Loss: 1.9331
Training Epoch: 8 [10624/50048]	Loss: 1.9654
Training Epoch: 8 [10752/50048]	Loss: 1.7455
Training Epoch: 8 [10880/50048]	Loss: 1.7184
Training Epoch: 8 [11008/50048]	Loss: 1.7551
Training Epoch: 8 [11136/50048]	Loss: 1.8088
Training Epoch: 8 [11264/50048]	Loss: 1.7358
Training Epoch: 8 [11392/50048]	Loss: 1.7878
Training Epoch: 8 [11520/50048]	Loss: 2.0877
Training Epoch: 8 [11648/50048]	Loss: 1.7765
Training Epoch: 8 [11776/50048]	Loss: 1.8030
Training Epoch: 8 [11904/50048]	Loss: 2.0086
Training Epoch: 8 [12032/50048]	Loss: 1.5565
Training Epoch: 8 [12160/50048]	Loss: 1.7378
Training Epoch: 8 [12288/50048]	Loss: 1.9275
Training Epoch: 8 [12416/50048]	Loss: 1.7371
Training Epoch: 8 [12544/50048]	Loss: 1.6854
Training Epoch: 8 [12672/50048]	Loss: 1.7076
Training Epoch: 8 [12800/50048]	Loss: 1.7209
Training Epoch: 8 [12928/50048]	Loss: 1.9072
Training Epoch: 8 [13056/50048]	Loss: 1.8466
Training Epoch: 8 [13184/50048]	Loss: 1.6368
Training Epoch: 8 [13312/50048]	Loss: 1.4486
Training Epoch: 8 [13440/50048]	Loss: 1.8956
Training Epoch: 8 [13568/50048]	Loss: 1.7542
Training Epoch: 8 [13696/50048]	Loss: 1.7570
Training Epoch: 8 [13824/50048]	Loss: 1.5750
Training Epoch: 8 [13952/50048]	Loss: 1.8349
Training Epoch: 8 [14080/50048]	Loss: 1.9707
Training Epoch: 8 [14208/50048]	Loss: 1.7121
Training Epoch: 8 [14336/50048]	Loss: 1.9401
Training Epoch: 8 [14464/50048]	Loss: 1.9447
Training Epoch: 8 [14592/50048]	Loss: 1.9312
Training Epoch: 8 [14720/50048]	Loss: 1.5830
Training Epoch: 8 [14848/50048]	Loss: 1.8282
Training Epoch: 8 [14976/50048]	Loss: 1.7633
Training Epoch: 8 [15104/50048]	Loss: 1.5922
Training Epoch: 8 [15232/50048]	Loss: 1.4802
Training Epoch: 8 [15360/50048]	Loss: 2.0353
Training Epoch: 8 [15488/50048]	Loss: 1.5684
Training Epoch: 8 [15616/50048]	Loss: 1.6856
Training Epoch: 8 [15744/50048]	Loss: 1.8270
Training Epoch: 8 [15872/50048]	Loss: 1.7923
Training Epoch: 8 [16000/50048]	Loss: 1.9574
Training Epoch: 8 [16128/50048]	Loss: 2.0036
Training Epoch: 8 [16256/50048]	Loss: 1.7139
Training Epoch: 8 [16384/50048]	Loss: 1.8995
Training Epoch: 8 [16512/50048]	Loss: 2.0253
Training Epoch: 8 [16640/50048]	Loss: 1.4836
Training Epoch: 8 [16768/50048]	Loss: 1.8258
Training Epoch: 8 [16896/50048]	Loss: 1.7296
Training Epoch: 8 [17024/50048]	Loss: 1.6259
Training Epoch: 8 [17152/50048]	Loss: 1.6258
Training Epoch: 8 [17280/50048]	Loss: 1.6906
Training Epoch: 8 [17408/50048]	Loss: 1.9053
Training Epoch: 8 [17536/50048]	Loss: 1.8445
Training Epoch: 8 [17664/50048]	Loss: 1.6381
Training Epoch: 8 [17792/50048]	Loss: 1.5120
Training Epoch: 8 [17920/50048]	Loss: 1.4400
Training Epoch: 8 [18048/50048]	Loss: 1.8047
Training Epoch: 8 [18176/50048]	Loss: 1.4645
Training Epoch: 8 [18304/50048]	Loss: 2.0736
Training Epoch: 8 [18432/50048]	Loss: 1.6575
Training Epoch: 8 [18560/50048]	Loss: 1.6328
Training Epoch: 8 [18688/50048]	Loss: 1.8212
Training Epoch: 8 [18816/50048]	Loss: 1.9923
Training Epoch: 8 [18944/50048]	Loss: 1.6497
Training Epoch: 8 [19072/50048]	Loss: 1.8331
Training Epoch: 8 [19200/50048]	Loss: 1.7119
Training Epoch: 8 [19328/50048]	Loss: 1.9451
Training Epoch: 8 [19456/50048]	Loss: 1.5500
Training Epoch: 8 [19584/50048]	Loss: 1.5076
Training Epoch: 8 [19712/50048]	Loss: 1.7239
Training Epoch: 8 [19840/50048]	Loss: 1.7848
Training Epoch: 8 [19968/50048]	Loss: 1.7128
Training Epoch: 8 [20096/50048]	Loss: 1.7057
Training Epoch: 8 [20224/50048]	Loss: 1.8878
Training Epoch: 8 [20352/50048]	Loss: 1.7027
Training Epoch: 8 [20480/50048]	Loss: 1.5831
Training Epoch: 8 [20608/50048]	Loss: 1.8576
Training Epoch: 8 [20736/50048]	Loss: 1.5348
Training Epoch: 8 [20864/50048]	Loss: 1.6712
Training Epoch: 8 [20992/50048]	Loss: 1.6851
Training Epoch: 8 [21120/50048]	Loss: 1.9671
Training Epoch: 8 [21248/50048]	Loss: 1.7911
Training Epoch: 8 [21376/50048]	Loss: 1.5915
Training Epoch: 8 [21504/50048]	Loss: 1.6466
Training Epoch: 8 [21632/50048]	Loss: 2.0212
Training Epoch: 8 [21760/50048]	Loss: 1.9783
Training Epoch: 8 [21888/50048]	Loss: 1.7681
Training Epoch: 8 [22016/50048]	Loss: 1.7014
Training Epoch: 8 [22144/50048]	Loss: 1.6411
Training Epoch: 8 [22272/50048]	Loss: 1.7067
Training Epoch: 8 [22400/50048]	Loss: 1.9542
Training Epoch: 8 [22528/50048]	Loss: 1.9432
Training Epoch: 8 [22656/50048]	Loss: 1.8552
Training Epoch: 8 [22784/50048]	Loss: 1.7315
Training Epoch: 8 [22912/50048]	Loss: 1.6792
Training Epoch: 8 [23040/50048]	Loss: 1.5352
Training Epoch: 8 [23168/50048]	Loss: 1.5205
Training Epoch: 8 [23296/50048]	Loss: 2.0301
Training Epoch: 8 [23424/50048]	Loss: 1.5647
Training Epoch: 8 [23552/50048]	Loss: 1.5937
Training Epoch: 8 [23680/50048]	Loss: 1.9748
Training Epoch: 8 [23808/50048]	Loss: 1.8222
Training Epoch: 8 [23936/50048]	Loss: 1.5777
Training Epoch: 8 [24064/50048]	Loss: 1.7665
Training Epoch: 8 [24192/50048]	Loss: 2.0031
Training Epoch: 8 [24320/50048]	Loss: 1.6568
Training Epoch: 8 [24448/50048]	Loss: 1.6433
Training Epoch: 8 [24576/50048]	Loss: 1.5693
Training Epoch: 8 [24704/50048]	Loss: 1.6909
Training Epoch: 8 [24832/50048]	Loss: 1.8142
Training Epoch: 8 [24960/50048]	Loss: 1.6429
Training Epoch: 8 [25088/50048]	Loss: 1.7248
Training Epoch: 8 [25216/50048]	Loss: 1.9487
Training Epoch: 8 [25344/50048]	Loss: 1.6518
Training Epoch: 8 [25472/50048]	Loss: 1.9686
Training Epoch: 8 [25600/50048]	Loss: 1.4790
Training Epoch: 8 [25728/50048]	Loss: 1.7302
Training Epoch: 8 [25856/50048]	Loss: 1.8791
Training Epoch: 8 [25984/50048]	Loss: 1.6592
Training Epoch: 8 [26112/50048]	Loss: 1.7189
Training Epoch: 8 [26240/50048]	Loss: 1.6119
Training Epoch: 8 [26368/50048]	Loss: 1.7331
Training Epoch: 8 [26496/50048]	Loss: 1.8302
Training Epoch: 8 [26624/50048]	Loss: 1.8300
Training Epoch: 8 [26752/50048]	Loss: 1.7764
Training Epoch: 8 [26880/50048]	Loss: 1.6837
Training Epoch: 8 [27008/50048]	Loss: 1.6342
Training Epoch: 8 [27136/50048]	Loss: 1.6857
Training Epoch: 8 [27264/50048]	Loss: 1.5763
Training Epoch: 8 [27392/50048]	Loss: 1.7711
Training Epoch: 8 [27520/50048]	Loss: 1.7330
Training Epoch: 8 [27648/50048]	Loss: 1.5795
Training Epoch: 8 [27776/50048]	Loss: 1.5443
Training Epoch: 8 [27904/50048]	Loss: 1.5853
Training Epoch: 8 [28032/50048]	Loss: 1.6241
Training Epoch: 8 [28160/50048]	Loss: 1.8351
Training Epoch: 8 [28288/50048]	Loss: 1.5103
Training Epoch: 8 [28416/50048]	Loss: 1.8288
Training Epoch: 8 [28544/50048]	Loss: 1.8826
Training Epoch: 8 [28672/50048]	Loss: 1.5733
Training Epoch: 8 [28800/50048]	Loss: 2.0226
Training Epoch: 8 [28928/50048]	Loss: 1.6974
Training Epoch: 8 [29056/50048]	Loss: 1.6434
Training Epoch: 8 [29184/50048]	Loss: 1.6498
Training Epoch: 8 [29312/50048]	Loss: 1.8725
Training Epoch: 8 [29440/50048]	Loss: 1.6440
Training Epoch: 8 [29568/50048]	Loss: 1.6033
Training Epoch: 8 [29696/50048]	Loss: 1.7267
Training Epoch: 8 [29824/50048]	Loss: 1.5774
Training Epoch: 8 [29952/50048]	Loss: 1.6296
Training Epoch: 8 [30080/50048]	Loss: 1.8043
Training Epoch: 8 [30208/50048]	Loss: 1.6493
Training Epoch: 8 [30336/50048]	Loss: 1.5126
Training Epoch: 8 [30464/50048]	Loss: 1.7667
Training Epoch: 8 [30592/50048]	Loss: 1.9909
Training Epoch: 8 [30720/50048]	Loss: 1.5876
Training Epoch: 8 [30848/50048]	Loss: 1.6963
Training Epoch: 8 [30976/50048]	Loss: 1.6708
Training Epoch: 8 [31104/50048]	Loss: 1.7630
Training Epoch: 8 [31232/50048]	Loss: 1.7772
Training Epoch: 8 [31360/50048]	Loss: 1.4686
Training Epoch: 8 [31488/50048]	Loss: 1.7733
Training Epoch: 8 [31616/50048]	Loss: 1.8555
Training Epoch: 8 [31744/50048]	Loss: 1.9917
Training Epoch: 8 [31872/50048]	Loss: 1.7214
Training Epoch: 8 [32000/50048]	Loss: 1.6866
Training Epoch: 8 [32128/50048]	Loss: 1.7251
Training Epoch: 8 [32256/50048]	Loss: 1.7203
Training Epoch: 8 [32384/50048]	Loss: 1.5546
Training Epoch: 8 [32512/50048]	Loss: 1.6705
Training Epoch: 8 [32640/50048]	Loss: 1.7746
Training Epoch: 8 [32768/50048]	Loss: 1.6450
Training Epoch: 8 [32896/50048]	Loss: 1.8801
Training Epoch: 8 [33024/50048]	Loss: 1.8648
Training Epoch: 8 [33152/50048]	Loss: 1.6259
Training Epoch: 8 [33280/50048]	Loss: 1.6032
Training Epoch: 8 [33408/50048]	Loss: 1.8974
Training Epoch: 8 [33536/50048]	Loss: 1.5936
Training Epoch: 8 [33664/50048]	Loss: 1.7416
Training Epoch: 8 [33792/50048]	Loss: 1.8553
Training Epoch: 8 [33920/50048]	Loss: 1.8761
Training Epoch: 8 [34048/50048]	Loss: 1.8921
Training Epoch: 8 [34176/50048]	Loss: 1.9575
Training Epoch: 8 [34304/50048]	Loss: 1.9791
Training Epoch: 8 [34432/50048]	Loss: 2.0475
Training Epoch: 8 [34560/50048]	Loss: 1.4944
Training Epoch: 8 [34688/50048]	Loss: 1.9724
Training Epoch: 8 [34816/50048]	Loss: 1.8174
Training Epoch: 8 [34944/50048]	Loss: 1.8400
Training Epoch: 8 [35072/50048]	Loss: 1.7959
Training Epoch: 8 [35200/50048]	Loss: 1.7412
Training Epoch: 8 [35328/50048]	Loss: 1.9030
Training Epoch: 8 [35456/50048]	Loss: 1.5579
Training Epoch: 8 [35584/50048]	Loss: 1.7181
Training Epoch: 8 [35712/50048]	Loss: 1.6707
Training Epoch: 8 [35840/50048]	Loss: 1.7415
Training Epoch: 8 [35968/50048]	Loss: 1.6151
Training Epoch: 8 [36096/50048]	Loss: 1.5359
Training Epoch: 8 [36224/50048]	Loss: 1.5391
Training Epoch: 8 [36352/50048]	Loss: 1.5753
Training Epoch: 8 [36480/50048]	Loss: 1.8514
Training Epoch: 8 [36608/50048]	Loss: 1.6174
Training Epoch: 8 [36736/50048]	Loss: 1.8452
Training Epoch: 8 [36864/50048]	Loss: 1.8076
Training Epoch: 8 [36992/50048]	Loss: 1.7534
Training Epoch: 8 [37120/50048]	Loss: 1.7639
Training Epoch: 8 [37248/50048]	Loss: 1.7060
Training Epoch: 8 [37376/50048]	Loss: 1.9649
Training Epoch: 8 [37504/50048]	Loss: 1.7233
Training Epoch: 8 [37632/50048]	Loss: 2.0074
Training Epoch: 8 [37760/50048]	Loss: 1.7189
Training Epoch: 8 [37888/50048]	Loss: 1.6927
Training Epoch: 8 [38016/50048]	Loss: 1.6725
Training Epoch: 8 [38144/50048]	Loss: 1.6887
Training Epoch: 8 [38272/50048]	Loss: 1.6180
Training Epoch: 8 [38400/50048]	Loss: 1.4570
Training Epoch: 8 [38528/50048]	Loss: 1.9229
Training Epoch: 8 [38656/50048]	Loss: 1.8438
Training Epoch: 8 [38784/50048]	Loss: 1.7521
Training Epoch: 8 [38912/50048]	Loss: 2.0003
Training Epoch: 8 [39040/50048]	Loss: 1.3761
Training Epoch: 8 [39168/50048]	Loss: 1.9743
Training Epoch: 8 [39296/50048]	Loss: 1.5006
Training Epoch: 8 [39424/50048]	Loss: 1.7632
Training Epoch: 8 [39552/50048]	Loss: 2.0002
Training Epoch: 8 [39680/50048]	Loss: 1.7017
Training Epoch: 8 [39808/50048]	Loss: 1.6772
Training Epoch: 8 [39936/50048]	Loss: 1.8840
Training Epoch: 8 [40064/50048]	Loss: 1.8221
Training Epoch: 8 [40192/50048]	Loss: 1.7457
Training Epoch: 8 [40320/50048]	Loss: 1.7377
Training Epoch: 8 [40448/50048]	Loss: 1.6328
Training Epoch: 8 [40576/50048]	Loss: 1.7870
Training Epoch: 8 [40704/50048]	Loss: 1.3261
Training Epoch: 8 [40832/50048]	Loss: 1.6237
Training Epoch: 8 [40960/50048]	Loss: 1.7305
Training Epoch: 8 [41088/50048]	Loss: 1.7612
Training Epoch: 8 [41216/50048]	Loss: 1.7499
Training Epoch: 8 [41344/50048]	Loss: 1.5177
Training Epoch: 8 [41472/50048]	Loss: 1.7936
Training Epoch: 8 [41600/50048]	Loss: 1.7927
Training Epoch: 8 [41728/50048]	Loss: 1.6564
Training Epoch: 8 [41856/50048]	Loss: 1.6481
Training Epoch: 8 [41984/50048]	Loss: 1.5667
Training Epoch: 8 [42112/50048]	Loss: 1.8029
Training Epoch: 8 [42240/50048]	Loss: 1.7829
Training Epoch: 8 [42368/50048]	Loss: 1.7635
Training Epoch: 8 [42496/50048]	Loss: 1.7169
Training Epoch: 8 [42624/50048]	Loss: 1.6181
Training Epoch: 8 [42752/50048]	Loss: 1.5993
Training Epoch: 8 [42880/50048]	Loss: 1.7235
Training Epoch: 8 [43008/50048]	Loss: 1.4337
Training Epoch: 8 [43136/50048]	Loss: 1.8301
Training Epoch: 8 [43264/50048]	Loss: 1.7819
Training Epoch: 8 [43392/50048]	Loss: 1.8304
Training Epoch: 8 [43520/50048]	Loss: 1.6100
Training Epoch: 8 [43648/50048]	Loss: 1.6918
Training Epoch: 8 [43776/50048]	Loss: 1.7524
Training Epoch: 8 [43904/50048]	Loss: 1.6922
Training Epoch: 8 [44032/50048]	Loss: 1.6647
Training Epoch: 8 [44160/50048]	Loss: 1.6452
Training Epoch: 8 [44288/50048]	Loss: 1.6695
Training Epoch: 8 [44416/50048]	Loss: 1.9402
Training Epoch: 8 [44544/50048]	Loss: 1.8206
Training Epoch: 8 [44672/50048]	Loss: 1.6918
Training Epoch: 8 [44800/50048]	Loss: 1.9455
Training Epoch: 8 [44928/50048]	Loss: 1.5182
Training Epoch: 8 [45056/50048]	Loss: 1.6909
Training Epoch: 8 [45184/50048]	Loss: 1.8291
Training Epoch: 8 [45312/50048]	Loss: 1.7135
Training Epoch: 8 [45440/50048]	Loss: 1.5017
Training Epoch: 8 [45568/50048]	Loss: 1.7562
Training Epoch: 8 [45696/50048]	Loss: 1.7599
Training Epoch: 8 [45824/50048]	Loss: 1.7181
Training Epoch: 8 [45952/50048]	Loss: 1.7822
Training Epoch: 8 [46080/50048]	Loss: 1.6875
Training Epoch: 8 [46208/50048]	Loss: 1.6833
Training Epoch: 8 [46336/50048]	Loss: 1.8208
Training Epoch: 8 [46464/50048]	Loss: 1.6567
Training Epoch: 8 [46592/50048]	Loss: 2.0556
Training Epoch: 8 [46720/50048]	Loss: 1.5579
Training Epoch: 8 [46848/50048]	Loss: 1.6156
Training Epoch: 8 [46976/50048]	Loss: 1.7137
Training Epoch: 8 [47104/50048]	Loss: 1.5133
Training Epoch: 8 [47232/50048]	Loss: 1.5812
Training Epoch: 8 [47360/50048]	Loss: 1.9430
Training Epoch: 8 [47488/50048]	Loss: 1.5620
Training Epoch: 8 [47616/50048]	Loss: 1.8863
Training Epoch: 8 [47744/50048]	Loss: 1.7867
Training Epoch: 8 [47872/50048]	Loss: 1.6096
Training Epoch: 8 [48000/50048]	Loss: 1.6476
Training Epoch: 8 [48128/50048]	Loss: 2.0479
Training Epoch: 8 [48256/50048]	Loss: 1.5923
Training Epoch: 8 [48384/50048]	Loss: 1.7037
Training Epoch: 8 [48512/50048]	Loss: 1.5530
Training Epoch: 8 [48640/50048]	Loss: 1.6492
Training Epoch: 8 [48768/50048]	Loss: 2.0143
Training Epoch: 8 [48896/50048]	Loss: 1.6210
Training Epoch: 8 [49024/50048]	Loss: 1.6050
Training Epoch: 8 [49152/50048]	Loss: 1.9213
Training Epoch: 8 [49280/50048]	Loss: 1.7349
Training Epoch: 8 [49408/50048]	Loss: 1.8733
Training Epoch: 8 [49536/50048]	Loss: 1.5730
Training Epoch: 8 [49664/50048]	Loss: 1.4844
Training Epoch: 8 [49792/50048]	Loss: 1.8696
Training Epoch: 8 [49920/50048]	Loss: 1.6820
Training Epoch: 8 [50048/50048]	Loss: 1.5276
Validation Epoch: 8, Average loss: 0.0140, Accuracy: 0.5158
Training Epoch: 9 [128/50048]	Loss: 1.8006
Training Epoch: 9 [256/50048]	Loss: 1.7243
Training Epoch: 9 [384/50048]	Loss: 1.6929
Training Epoch: 9 [512/50048]	Loss: 1.5276
Training Epoch: 9 [640/50048]	Loss: 1.6989
Training Epoch: 9 [768/50048]	Loss: 1.8393
Training Epoch: 9 [896/50048]	Loss: 1.4965
Training Epoch: 9 [1024/50048]	Loss: 1.5938
Training Epoch: 9 [1152/50048]	Loss: 1.5833
Training Epoch: 9 [1280/50048]	Loss: 1.5518
Training Epoch: 9 [1408/50048]	Loss: 1.6975
Training Epoch: 9 [1536/50048]	Loss: 1.5957
Training Epoch: 9 [1664/50048]	Loss: 1.6631
Training Epoch: 9 [1792/50048]	Loss: 1.7873
Training Epoch: 9 [1920/50048]	Loss: 1.9884
Training Epoch: 9 [2048/50048]	Loss: 1.6143
Training Epoch: 9 [2176/50048]	Loss: 1.6530
Training Epoch: 9 [2304/50048]	Loss: 1.7119
Training Epoch: 9 [2432/50048]	Loss: 1.7355
Training Epoch: 9 [2560/50048]	Loss: 1.7739
Training Epoch: 9 [2688/50048]	Loss: 1.6352
Training Epoch: 9 [2816/50048]	Loss: 1.5909
Training Epoch: 9 [2944/50048]	Loss: 1.5190
Training Epoch: 9 [3072/50048]	Loss: 1.5510
Training Epoch: 9 [3200/50048]	Loss: 1.6614
Training Epoch: 9 [3328/50048]	Loss: 1.7214
Training Epoch: 9 [3456/50048]	Loss: 1.9128
Training Epoch: 9 [3584/50048]	Loss: 1.5568
Training Epoch: 9 [3712/50048]	Loss: 1.8220
Training Epoch: 9 [3840/50048]	Loss: 1.7253
Training Epoch: 9 [3968/50048]	Loss: 1.7088
Training Epoch: 9 [4096/50048]	Loss: 1.6567
Training Epoch: 9 [4224/50048]	Loss: 1.4298
Training Epoch: 9 [4352/50048]	Loss: 1.8360
Training Epoch: 9 [4480/50048]	Loss: 1.6569
Training Epoch: 9 [4608/50048]	Loss: 1.5436
Training Epoch: 9 [4736/50048]	Loss: 1.7939
Training Epoch: 9 [4864/50048]	Loss: 1.5335
Training Epoch: 9 [4992/50048]	Loss: 1.8350
Training Epoch: 9 [5120/50048]	Loss: 1.7253
Training Epoch: 9 [5248/50048]	Loss: 1.6740
Training Epoch: 9 [5376/50048]	Loss: 1.7709
Training Epoch: 9 [5504/50048]	Loss: 1.6357
Training Epoch: 9 [5632/50048]	Loss: 1.8596
Training Epoch: 9 [5760/50048]	Loss: 1.9172
Training Epoch: 9 [5888/50048]	Loss: 1.5287
Training Epoch: 9 [6016/50048]	Loss: 1.6311
Training Epoch: 9 [6144/50048]	Loss: 1.8757
Training Epoch: 9 [6272/50048]	Loss: 1.8756
Training Epoch: 9 [6400/50048]	Loss: 1.6200
Training Epoch: 9 [6528/50048]	Loss: 1.3837
Training Epoch: 9 [6656/50048]	Loss: 1.7727
Training Epoch: 9 [6784/50048]	Loss: 1.7290
Training Epoch: 9 [6912/50048]	Loss: 1.6063
Training Epoch: 9 [7040/50048]	Loss: 1.7377
Training Epoch: 9 [7168/50048]	Loss: 1.3056
Training Epoch: 9 [7296/50048]	Loss: 1.9301
Training Epoch: 9 [7424/50048]	Loss: 1.8314
Training Epoch: 9 [7552/50048]	Loss: 1.4771
Training Epoch: 9 [7680/50048]	Loss: 1.9350
Training Epoch: 9 [7808/50048]	Loss: 1.5812
Training Epoch: 9 [7936/50048]	Loss: 1.4881
Training Epoch: 9 [8064/50048]	Loss: 1.6516
Training Epoch: 9 [8192/50048]	Loss: 1.4924
Training Epoch: 9 [8320/50048]	Loss: 1.7631
Training Epoch: 9 [8448/50048]	Loss: 1.7668
Training Epoch: 9 [8576/50048]	Loss: 1.8677
Training Epoch: 9 [8704/50048]	Loss: 1.8951
Training Epoch: 9 [8832/50048]	Loss: 1.8595
Training Epoch: 9 [8960/50048]	Loss: 1.4556
Training Epoch: 9 [9088/50048]	Loss: 1.8270
Training Epoch: 9 [9216/50048]	Loss: 1.9265
Training Epoch: 9 [9344/50048]	Loss: 1.6145
Training Epoch: 9 [9472/50048]	Loss: 1.6213
Training Epoch: 9 [9600/50048]	Loss: 1.8338
Training Epoch: 9 [9728/50048]	Loss: 1.7499
Training Epoch: 9 [9856/50048]	Loss: 1.6080
Training Epoch: 9 [9984/50048]	Loss: 1.6949
Training Epoch: 9 [10112/50048]	Loss: 1.7664
Training Epoch: 9 [10240/50048]	Loss: 1.5228
Training Epoch: 9 [10368/50048]	Loss: 1.5118
Training Epoch: 9 [10496/50048]	Loss: 1.6210
Training Epoch: 9 [10624/50048]	Loss: 1.8177
Training Epoch: 9 [10752/50048]	Loss: 1.5618
Training Epoch: 9 [10880/50048]	Loss: 1.7018
Training Epoch: 9 [11008/50048]	Loss: 1.6328
Training Epoch: 9 [11136/50048]	Loss: 1.7802
Training Epoch: 9 [11264/50048]	Loss: 1.6389
Training Epoch: 9 [11392/50048]	Loss: 1.4462
Training Epoch: 9 [11520/50048]	Loss: 1.9252
Training Epoch: 9 [11648/50048]	Loss: 1.6738
Training Epoch: 9 [11776/50048]	Loss: 1.6482
Training Epoch: 9 [11904/50048]	Loss: 1.7245
Training Epoch: 9 [12032/50048]	Loss: 1.6106
Training Epoch: 9 [12160/50048]	Loss: 1.4773
Training Epoch: 9 [12288/50048]	Loss: 1.7620
Training Epoch: 9 [12416/50048]	Loss: 1.5793
Training Epoch: 9 [12544/50048]	Loss: 1.8265
Training Epoch: 9 [12672/50048]	Loss: 1.6920
Training Epoch: 9 [12800/50048]	Loss: 1.4765
Training Epoch: 9 [12928/50048]	Loss: 1.6973
Training Epoch: 9 [13056/50048]	Loss: 1.5031
Training Epoch: 9 [13184/50048]	Loss: 1.7304
Training Epoch: 9 [13312/50048]	Loss: 1.6600
Training Epoch: 9 [13440/50048]	Loss: 1.5273
Training Epoch: 9 [13568/50048]	Loss: 1.6560
Training Epoch: 9 [13696/50048]	Loss: 1.7021
Training Epoch: 9 [13824/50048]	Loss: 1.4622
Training Epoch: 9 [13952/50048]	Loss: 1.6234
Training Epoch: 9 [14080/50048]	Loss: 1.7760
Training Epoch: 9 [14208/50048]	Loss: 1.5793
Training Epoch: 9 [14336/50048]	Loss: 1.5147
Training Epoch: 9 [14464/50048]	Loss: 1.8225
Training Epoch: 9 [14592/50048]	Loss: 1.6555
Training Epoch: 9 [14720/50048]	Loss: 1.7909
Training Epoch: 9 [14848/50048]	Loss: 1.6911
Training Epoch: 9 [14976/50048]	Loss: 1.9683
Training Epoch: 9 [15104/50048]	Loss: 1.7236
Training Epoch: 9 [15232/50048]	Loss: 1.7048
Training Epoch: 9 [15360/50048]	Loss: 1.9431
Training Epoch: 9 [15488/50048]	Loss: 1.8996
Training Epoch: 9 [15616/50048]	Loss: 2.0376
Training Epoch: 9 [15744/50048]	Loss: 1.6142
Training Epoch: 9 [15872/50048]	Loss: 1.4972
Training Epoch: 9 [16000/50048]	Loss: 1.8277
Training Epoch: 9 [16128/50048]	Loss: 1.5036
Training Epoch: 9 [16256/50048]	Loss: 1.5048
Training Epoch: 9 [16384/50048]	Loss: 1.5233
Training Epoch: 9 [16512/50048]	Loss: 1.3292
Training Epoch: 9 [16640/50048]	Loss: 1.4717
Training Epoch: 9 [16768/50048]	Loss: 1.7371
Training Epoch: 9 [16896/50048]	Loss: 1.5341
Training Epoch: 9 [17024/50048]	Loss: 1.8768
Training Epoch: 9 [17152/50048]	Loss: 1.5630
Training Epoch: 9 [17280/50048]	Loss: 1.4647
Training Epoch: 9 [17408/50048]	Loss: 1.5200
Training Epoch: 9 [17536/50048]	Loss: 1.7953
Training Epoch: 9 [17664/50048]	Loss: 1.7162
Training Epoch: 9 [17792/50048]	Loss: 1.6287
Training Epoch: 9 [17920/50048]	Loss: 1.7874
Training Epoch: 9 [18048/50048]	Loss: 1.9495
Training Epoch: 9 [18176/50048]	Loss: 1.6615
Training Epoch: 9 [18304/50048]	Loss: 1.6499
Training Epoch: 9 [18432/50048]	Loss: 1.8463
Training Epoch: 9 [18560/50048]	Loss: 1.7993
Training Epoch: 9 [18688/50048]	Loss: 1.4177
Training Epoch: 9 [18816/50048]	Loss: 1.5891
Training Epoch: 9 [18944/50048]	Loss: 1.6657
Training Epoch: 9 [19072/50048]	Loss: 1.7206
Training Epoch: 9 [19200/50048]	Loss: 1.6313
Training Epoch: 9 [19328/50048]	Loss: 1.6986
Training Epoch: 9 [19456/50048]	Loss: 1.6172
Training Epoch: 9 [19584/50048]	Loss: 1.5426
Training Epoch: 9 [19712/50048]	Loss: 1.6097
Training Epoch: 9 [19840/50048]	Loss: 1.7223
Training Epoch: 9 [19968/50048]	Loss: 1.7601
Training Epoch: 9 [20096/50048]	Loss: 1.8036
Training Epoch: 9 [20224/50048]	Loss: 1.4589
Training Epoch: 9 [20352/50048]	Loss: 1.8046
Training Epoch: 9 [20480/50048]	Loss: 1.7495
Training Epoch: 9 [20608/50048]	Loss: 1.3443
Training Epoch: 9 [20736/50048]	Loss: 1.6036
Training Epoch: 9 [20864/50048]	Loss: 1.5074
Training Epoch: 9 [20992/50048]	Loss: 1.7847
Training Epoch: 9 [21120/50048]	Loss: 1.8607
Training Epoch: 9 [21248/50048]	Loss: 1.7822
Training Epoch: 9 [21376/50048]	Loss: 1.8908
Training Epoch: 9 [21504/50048]	Loss: 1.6285
Training Epoch: 9 [21632/50048]	Loss: 1.8120
Training Epoch: 9 [21760/50048]	Loss: 1.6976
Training Epoch: 9 [21888/50048]	Loss: 1.6656
Training Epoch: 9 [22016/50048]	Loss: 1.7007
Training Epoch: 9 [22144/50048]	Loss: 1.7094
Training Epoch: 9 [22272/50048]	Loss: 1.7637
Training Epoch: 9 [22400/50048]	Loss: 1.6470
Training Epoch: 9 [22528/50048]	Loss: 1.5550
Training Epoch: 9 [22656/50048]	Loss: 1.7262
Training Epoch: 9 [22784/50048]	Loss: 1.6321
Training Epoch: 9 [22912/50048]	Loss: 1.4889
Training Epoch: 9 [23040/50048]	Loss: 1.8462
Training Epoch: 9 [23168/50048]	Loss: 1.8543
Training Epoch: 9 [23296/50048]	Loss: 1.6415
Training Epoch: 9 [23424/50048]	Loss: 1.7085
Training Epoch: 9 [23552/50048]	Loss: 1.9199
Training Epoch: 9 [23680/50048]	Loss: 1.7471
Training Epoch: 9 [23808/50048]	Loss: 1.5782
Training Epoch: 9 [23936/50048]	Loss: 1.7086
Training Epoch: 9 [24064/50048]	Loss: 1.5631
Training Epoch: 9 [24192/50048]	Loss: 1.5543
Training Epoch: 9 [24320/50048]	Loss: 1.9053
Training Epoch: 9 [24448/50048]	Loss: 1.7250
Training Epoch: 9 [24576/50048]	Loss: 1.6142
Training Epoch: 9 [24704/50048]	Loss: 1.6971
Training Epoch: 9 [24832/50048]	Loss: 1.5208
Training Epoch: 9 [24960/50048]	Loss: 1.6846
Training Epoch: 9 [25088/50048]	Loss: 1.4774
Training Epoch: 9 [25216/50048]	Loss: 1.6111
Training Epoch: 9 [25344/50048]	Loss: 1.5219
Training Epoch: 9 [25472/50048]	Loss: 1.9053
Training Epoch: 9 [25600/50048]	Loss: 1.7047
Training Epoch: 9 [25728/50048]	Loss: 1.7870
Training Epoch: 9 [25856/50048]	Loss: 1.8479
Training Epoch: 9 [25984/50048]	Loss: 2.0217
Training Epoch: 9 [26112/50048]	Loss: 1.7135
Training Epoch: 9 [26240/50048]	Loss: 1.5591
Training Epoch: 9 [26368/50048]	Loss: 1.5587
Training Epoch: 9 [26496/50048]	Loss: 1.4392
Training Epoch: 9 [26624/50048]	Loss: 1.8207
Training Epoch: 9 [26752/50048]	Loss: 1.7033
Training Epoch: 9 [26880/50048]	Loss: 1.5305
Training Epoch: 9 [27008/50048]	Loss: 1.9404
Training Epoch: 9 [27136/50048]	Loss: 1.7453
Training Epoch: 9 [27264/50048]	Loss: 1.5449
Training Epoch: 9 [27392/50048]	Loss: 1.5429
Training Epoch: 9 [27520/50048]	Loss: 1.6100
Training Epoch: 9 [27648/50048]	Loss: 1.9771
Training Epoch: 9 [27776/50048]	Loss: 1.6014
Training Epoch: 9 [27904/50048]	Loss: 1.5436
Training Epoch: 9 [28032/50048]	Loss: 1.8152
Training Epoch: 9 [28160/50048]	Loss: 1.7830
Training Epoch: 9 [28288/50048]	Loss: 1.7012
Training Epoch: 9 [28416/50048]	Loss: 1.5782
Training Epoch: 9 [28544/50048]	Loss: 1.4650
Training Epoch: 9 [28672/50048]	Loss: 1.6340
Training Epoch: 9 [28800/50048]	Loss: 1.5621
Training Epoch: 9 [28928/50048]	Loss: 1.5425
Training Epoch: 9 [29056/50048]	Loss: 2.0930
Training Epoch: 9 [29184/50048]	Loss: 1.9019
Training Epoch: 9 [29312/50048]	Loss: 1.6114
Training Epoch: 9 [29440/50048]	Loss: 1.5582
Training Epoch: 9 [29568/50048]	Loss: 1.6576
Training Epoch: 9 [29696/50048]	Loss: 1.5807
Training Epoch: 9 [29824/50048]	Loss: 1.7293
Training Epoch: 9 [29952/50048]	Loss: 1.7785
Training Epoch: 9 [30080/50048]	Loss: 1.7744
Training Epoch: 9 [30208/50048]	Loss: 1.7508
Training Epoch: 9 [30336/50048]	Loss: 1.6545
Training Epoch: 9 [30464/50048]	Loss: 1.6110
Training Epoch: 9 [30592/50048]	Loss: 1.6736
Training Epoch: 9 [30720/50048]	Loss: 1.7260
Training Epoch: 9 [30848/50048]	Loss: 1.6154
Training Epoch: 9 [30976/50048]	Loss: 1.6693
Training Epoch: 9 [31104/50048]	Loss: 1.8515
Training Epoch: 9 [31232/50048]	Loss: 1.7049
Training Epoch: 9 [31360/50048]	Loss: 1.5302
Training Epoch: 9 [31488/50048]	Loss: 1.5865
Training Epoch: 9 [31616/50048]	Loss: 1.7407
Training Epoch: 9 [31744/50048]	Loss: 2.0266
Training Epoch: 9 [31872/50048]	Loss: 1.6199
Training Epoch: 9 [32000/50048]	Loss: 1.9564
Training Epoch: 9 [32128/50048]	Loss: 1.9970
Training Epoch: 9 [32256/50048]	Loss: 1.6025
Training Epoch: 9 [32384/50048]	Loss: 2.0133
Training Epoch: 9 [32512/50048]	Loss: 1.8399
Training Epoch: 9 [32640/50048]	Loss: 1.8295
Training Epoch: 9 [32768/50048]	Loss: 1.8673
Training Epoch: 9 [32896/50048]	Loss: 1.6293
Training Epoch: 9 [33024/50048]	Loss: 1.8069
Training Epoch: 9 [33152/50048]	Loss: 1.7273
Training Epoch: 9 [33280/50048]	Loss: 1.7810
Training Epoch: 9 [33408/50048]	Loss: 1.5290
Training Epoch: 9 [33536/50048]	Loss: 1.7877
Training Epoch: 9 [33664/50048]	Loss: 1.7208
Training Epoch: 9 [33792/50048]	Loss: 1.6173
Training Epoch: 9 [33920/50048]	Loss: 1.7640
Training Epoch: 9 [34048/50048]	Loss: 1.9542
Training Epoch: 9 [34176/50048]	Loss: 1.6173
Training Epoch: 9 [34304/50048]	Loss: 1.6196
Training Epoch: 9 [34432/50048]	Loss: 1.7801
Training Epoch: 9 [34560/50048]	Loss: 1.7950
Training Epoch: 9 [34688/50048]	Loss: 1.6060
Training Epoch: 9 [34816/50048]	Loss: 1.6369
Training Epoch: 9 [34944/50048]	Loss: 1.6854
Training Epoch: 9 [35072/50048]	Loss: 1.7419
Training Epoch: 9 [35200/50048]	Loss: 1.8857
Training Epoch: 9 [35328/50048]	Loss: 1.7851
Training Epoch: 9 [35456/50048]	Loss: 1.5699
Training Epoch: 9 [35584/50048]	Loss: 1.6973
Training Epoch: 9 [35712/50048]	Loss: 1.5536
Training Epoch: 9 [35840/50048]	Loss: 1.4574
Training Epoch: 9 [35968/50048]	Loss: 1.8977
Training Epoch: 9 [36096/50048]	Loss: 1.6428
Training Epoch: 9 [36224/50048]	Loss: 1.5915
Training Epoch: 9 [36352/50048]	Loss: 1.7365
Training Epoch: 9 [36480/50048]	Loss: 1.5306
Training Epoch: 9 [36608/50048]	Loss: 1.6614
Training Epoch: 9 [36736/50048]	Loss: 1.6690
Training Epoch: 9 [36864/50048]	Loss: 1.7475
Training Epoch: 9 [36992/50048]	Loss: 1.4616
Training Epoch: 9 [37120/50048]	Loss: 1.8024
Training Epoch: 9 [37248/50048]	Loss: 1.7720
Training Epoch: 9 [37376/50048]	Loss: 1.7923
Training Epoch: 9 [37504/50048]	Loss: 1.6995
Training Epoch: 9 [37632/50048]	Loss: 1.7271
Training Epoch: 9 [37760/50048]	Loss: 2.0225
Training Epoch: 9 [37888/50048]	Loss: 1.4384
Training Epoch: 9 [38016/50048]	Loss: 1.8850
Training Epoch: 9 [38144/50048]	Loss: 1.6564
Training Epoch: 9 [38272/50048]	Loss: 1.8102
Training Epoch: 9 [38400/50048]	Loss: 1.9176
Training Epoch: 9 [38528/50048]	Loss: 1.6200
Training Epoch: 9 [38656/50048]	Loss: 1.5937
Training Epoch: 9 [38784/50048]	Loss: 1.7019
Training Epoch: 9 [38912/50048]	Loss: 1.5237
Training Epoch: 9 [39040/50048]	Loss: 2.0308
Training Epoch: 9 [39168/50048]	Loss: 1.7608
Training Epoch: 9 [39296/50048]	Loss: 1.6091
Training Epoch: 9 [39424/50048]	Loss: 1.5638
Training Epoch: 9 [39552/50048]	Loss: 1.7328
Training Epoch: 9 [39680/50048]	Loss: 1.4933
Training Epoch: 9 [39808/50048]	Loss: 1.6684
Training Epoch: 9 [39936/50048]	Loss: 1.5345
Training Epoch: 9 [40064/50048]	Loss: 1.5717
Training Epoch: 9 [40192/50048]	Loss: 1.5473
Training Epoch: 9 [40320/50048]	Loss: 1.5761
Training Epoch: 9 [40448/50048]	Loss: 1.7551
Training Epoch: 9 [40576/50048]	Loss: 1.5587
Training Epoch: 9 [40704/50048]	Loss: 1.7141
Training Epoch: 9 [40832/50048]	Loss: 1.5430
Training Epoch: 9 [40960/50048]	Loss: 1.7174
Training Epoch: 9 [41088/50048]	Loss: 1.6795
Training Epoch: 9 [41216/50048]	Loss: 1.8842
Training Epoch: 9 [41344/50048]	Loss: 1.7543
Training Epoch: 9 [41472/50048]	Loss: 1.7194
Training Epoch: 9 [41600/50048]	Loss: 1.8642
Training Epoch: 9 [41728/50048]	Loss: 1.8047
Training Epoch: 9 [41856/50048]	Loss: 1.7698
Training Epoch: 9 [41984/50048]	Loss: 1.8205
Training Epoch: 9 [42112/50048]	Loss: 1.7719
Training Epoch: 9 [42240/50048]	Loss: 1.4941
Training Epoch: 9 [42368/50048]	Loss: 1.4887
Training Epoch: 9 [42496/50048]	Loss: 1.6079
Training Epoch: 9 [42624/50048]	Loss: 1.6362
Training Epoch: 9 [42752/50048]	Loss: 1.8178
Training Epoch: 9 [42880/50048]	Loss: 1.6081
Training Epoch: 9 [43008/50048]	Loss: 1.5271
Training Epoch: 9 [43136/50048]	Loss: 2.0019
Training Epoch: 9 [43264/50048]	Loss: 1.4521
Training Epoch: 9 [43392/50048]	Loss: 1.6817
Training Epoch: 9 [43520/50048]	Loss: 1.5336
Training Epoch: 9 [43648/50048]	Loss: 1.5841
Training Epoch: 9 [43776/50048]	Loss: 1.8236
Training Epoch: 9 [43904/50048]	Loss: 1.7008
Training Epoch: 9 [44032/50048]	Loss: 2.0231
Training Epoch: 9 [44160/50048]	Loss: 1.6595
Training Epoch: 9 [44288/50048]	Loss: 1.5664
Training Epoch: 9 [44416/50048]	Loss: 1.6568
Training Epoch: 9 [44544/50048]	Loss: 1.5113
Training Epoch: 9 [44672/50048]	Loss: 1.6627
Training Epoch: 9 [44800/50048]	Loss: 1.9569
Training Epoch: 9 [44928/50048]	Loss: 1.4486
Training Epoch: 9 [45056/50048]	Loss: 1.5432
Training Epoch: 9 [45184/50048]	Loss: 1.5991
Training Epoch: 9 [45312/50048]	Loss: 1.6325
Training Epoch: 9 [45440/50048]	Loss: 2.1392
Training Epoch: 9 [45568/50048]	Loss: 1.8891
Training Epoch: 9 [45696/50048]	Loss: 1.6275
Training Epoch: 9 [45824/50048]	Loss: 1.6791
Training Epoch: 9 [45952/50048]	Loss: 1.5956
Training Epoch: 9 [46080/50048]	Loss: 1.7172
Training Epoch: 9 [46208/50048]	Loss: 1.6815
Training Epoch: 9 [46336/50048]	Loss: 1.5906
Training Epoch: 9 [46464/50048]	Loss: 1.8182
Training Epoch: 9 [46592/50048]	Loss: 1.6083
Training Epoch: 9 [46720/50048]	Loss: 1.9942
Training Epoch: 9 [46848/50048]	Loss: 1.6798
Training Epoch: 9 [46976/50048]	Loss: 1.7142
Training Epoch: 9 [47104/50048]	Loss: 1.6622
Training Epoch: 9 [47232/50048]	Loss: 1.4039
Training Epoch: 9 [47360/50048]	Loss: 1.6523
Training Epoch: 9 [47488/50048]	Loss: 1.6909
Training Epoch: 9 [47616/50048]	Loss: 1.6075
Training Epoch: 9 [47744/50048]	Loss: 1.6065
Training Epoch: 9 [47872/50048]	Loss: 1.6642
Training Epoch: 9 [48000/50048]	Loss: 1.6222
Training Epoch: 9 [48128/50048]	Loss: 1.8005
Training Epoch: 9 [48256/50048]	Loss: 1.4937
Training Epoch: 9 [48384/50048]	Loss: 1.7421
Training Epoch: 9 [48512/50048]	Loss: 1.4969
Training Epoch: 9 [48640/50048]	Loss: 1.8559
Training Epoch: 9 [48768/50048]	Loss: 1.7827
Training Epoch: 9 [48896/50048]	Loss: 1.6281
Training Epoch: 9 [49024/50048]	Loss: 1.6293
Training Epoch: 9 [49152/50048]	Loss: 1.7008
Training Epoch: 9 [49280/50048]	Loss: 1.4133
Training Epoch: 9 [49408/50048]	Loss: 1.5541
Training Epoch: 9 [49536/50048]	Loss: 1.5262
Training Epoch: 9 [49664/50048]	Loss: 1.9149
Training Epoch: 9 [49792/50048]	Loss: 1.8603
Training Epoch: 9 [49920/50048]	Loss: 1.7461
Training Epoch: 9 [50048/50048]	Loss: 1.4521
Validation Epoch: 9, Average loss: 0.0137, Accuracy: 0.5224
Training Epoch: 10 [128/50048]	Loss: 1.6624
Training Epoch: 10 [256/50048]	Loss: 1.6310
Training Epoch: 10 [384/50048]	Loss: 1.4882
Training Epoch: 10 [512/50048]	Loss: 1.7592
Training Epoch: 10 [640/50048]	Loss: 1.5875
Training Epoch: 10 [768/50048]	Loss: 1.5559
Training Epoch: 10 [896/50048]	Loss: 1.7495
Training Epoch: 10 [1024/50048]	Loss: 1.5258
Training Epoch: 10 [1152/50048]	Loss: 1.7626
Training Epoch: 10 [1280/50048]	Loss: 1.4097
Training Epoch: 10 [1408/50048]	Loss: 1.6266
Training Epoch: 10 [1536/50048]	Loss: 1.7108
Training Epoch: 10 [1664/50048]	Loss: 1.3764
Training Epoch: 10 [1792/50048]	Loss: 1.7835
Training Epoch: 10 [1920/50048]	Loss: 1.4014
Training Epoch: 10 [2048/50048]	Loss: 1.8060
Training Epoch: 10 [2176/50048]	Loss: 1.6749
Training Epoch: 10 [2304/50048]	Loss: 1.6826
Training Epoch: 10 [2432/50048]	Loss: 1.6505
Training Epoch: 10 [2560/50048]	Loss: 1.9182
Training Epoch: 10 [2688/50048]	Loss: 1.7459
Training Epoch: 10 [2816/50048]	Loss: 1.8345
Training Epoch: 10 [2944/50048]	Loss: 1.8915
Training Epoch: 10 [3072/50048]	Loss: 1.5815
Training Epoch: 10 [3200/50048]	Loss: 1.6766
Training Epoch: 10 [3328/50048]	Loss: 1.8128
Training Epoch: 10 [3456/50048]	Loss: 1.6509
Training Epoch: 10 [3584/50048]	Loss: 1.8005
Training Epoch: 10 [3712/50048]	Loss: 1.5587
Training Epoch: 10 [3840/50048]	Loss: 1.6724
Training Epoch: 10 [3968/50048]	Loss: 1.8458
Training Epoch: 10 [4096/50048]	Loss: 1.7195
Training Epoch: 10 [4224/50048]	Loss: 1.7745
Training Epoch: 10 [4352/50048]	Loss: 1.6038
Training Epoch: 10 [4480/50048]	Loss: 1.6260
Training Epoch: 10 [4608/50048]	Loss: 1.7852
Training Epoch: 10 [4736/50048]	Loss: 1.4879
Training Epoch: 10 [4864/50048]	Loss: 1.5585
Training Epoch: 10 [4992/50048]	Loss: 1.4436
Training Epoch: 10 [5120/50048]	Loss: 1.2470
Training Epoch: 10 [5248/50048]	Loss: 1.8578
Training Epoch: 10 [5376/50048]	Loss: 1.6198
Training Epoch: 10 [5504/50048]	Loss: 1.5097
Training Epoch: 10 [5632/50048]	Loss: 1.8678
Training Epoch: 10 [5760/50048]	Loss: 1.8623
Training Epoch: 10 [5888/50048]	Loss: 1.5815
Training Epoch: 10 [6016/50048]	Loss: 1.7255
Training Epoch: 10 [6144/50048]	Loss: 1.6006
Training Epoch: 10 [6272/50048]	Loss: 1.9639
Training Epoch: 10 [6400/50048]	Loss: 1.6254
Training Epoch: 10 [6528/50048]	Loss: 1.7661
Training Epoch: 10 [6656/50048]	Loss: 1.6011
Training Epoch: 10 [6784/50048]	Loss: 1.8458
Training Epoch: 10 [6912/50048]	Loss: 1.8678
Training Epoch: 10 [7040/50048]	Loss: 1.7123
Training Epoch: 10 [7168/50048]	Loss: 1.7324
Training Epoch: 10 [7296/50048]	Loss: 1.8506
Training Epoch: 10 [7424/50048]	Loss: 1.5438
Training Epoch: 10 [7552/50048]	Loss: 1.3847
Training Epoch: 10 [7680/50048]	Loss: 1.6044
Training Epoch: 10 [7808/50048]	Loss: 1.6341
Training Epoch: 10 [7936/50048]	Loss: 1.7993
Training Epoch: 10 [8064/50048]	Loss: 1.3886
Training Epoch: 10 [8192/50048]	Loss: 1.4080
Training Epoch: 10 [8320/50048]	Loss: 1.7044
Training Epoch: 10 [8448/50048]	Loss: 1.4473
Training Epoch: 10 [8576/50048]	Loss: 1.6284
Training Epoch: 10 [8704/50048]	Loss: 1.6199
Training Epoch: 10 [8832/50048]	Loss: 1.6978
Training Epoch: 10 [8960/50048]	Loss: 1.7092
Training Epoch: 10 [9088/50048]	Loss: 1.5574
Training Epoch: 10 [9216/50048]	Loss: 1.6057
Training Epoch: 10 [9344/50048]	Loss: 1.5770
Training Epoch: 10 [9472/50048]	Loss: 1.5965
Training Epoch: 10 [9600/50048]	Loss: 1.4051
Training Epoch: 10 [9728/50048]	Loss: 1.6034
Training Epoch: 10 [9856/50048]	Loss: 1.7943
Training Epoch: 10 [9984/50048]	Loss: 1.6805
Training Epoch: 10 [10112/50048]	Loss: 1.7997
Training Epoch: 10 [10240/50048]	Loss: 1.6263
Training Epoch: 10 [10368/50048]	Loss: 1.6985
Training Epoch: 10 [10496/50048]	Loss: 1.4834
Training Epoch: 10 [10624/50048]	Loss: 1.4938
Training Epoch: 10 [10752/50048]	Loss: 1.5353
Training Epoch: 10 [10880/50048]	Loss: 1.5421
Training Epoch: 10 [11008/50048]	Loss: 1.6649
Training Epoch: 10 [11136/50048]	Loss: 1.6067
Training Epoch: 10 [11264/50048]	Loss: 1.5506
Training Epoch: 10 [11392/50048]	Loss: 1.6928
Training Epoch: 10 [11520/50048]	Loss: 1.4385
Training Epoch: 10 [11648/50048]	Loss: 1.8020
Training Epoch: 10 [11776/50048]	Loss: 1.6352
Training Epoch: 10 [11904/50048]	Loss: 1.6083
Training Epoch: 10 [12032/50048]	Loss: 1.5247
Training Epoch: 10 [12160/50048]	Loss: 1.6579
Training Epoch: 10 [12288/50048]	Loss: 1.5055
Training Epoch: 10 [12416/50048]	Loss: 1.7235
Training Epoch: 10 [12544/50048]	Loss: 1.8347
Training Epoch: 10 [12672/50048]	Loss: 1.6912
Training Epoch: 10 [12800/50048]	Loss: 1.5499
Training Epoch: 10 [12928/50048]	Loss: 1.7025
Training Epoch: 10 [13056/50048]	Loss: 1.5871
Training Epoch: 10 [13184/50048]	Loss: 1.4150
Training Epoch: 10 [13312/50048]	Loss: 1.5814
Training Epoch: 10 [13440/50048]	Loss: 1.5681
Training Epoch: 10 [13568/50048]	Loss: 1.5331
Training Epoch: 10 [13696/50048]	Loss: 1.7338
Training Epoch: 10 [13824/50048]	Loss: 1.5302
Training Epoch: 10 [13952/50048]	Loss: 1.5875
Training Epoch: 10 [14080/50048]	Loss: 1.5231
Training Epoch: 10 [14208/50048]	Loss: 1.7655
Training Epoch: 10 [14336/50048]	Loss: 1.8892
Training Epoch: 10 [14464/50048]	Loss: 1.7304
Training Epoch: 10 [14592/50048]	Loss: 1.4297
Training Epoch: 10 [14720/50048]	Loss: 1.8019
Training Epoch: 10 [14848/50048]	Loss: 1.6072
Training Epoch: 10 [14976/50048]	Loss: 1.5281
Training Epoch: 10 [15104/50048]	Loss: 1.6445
Training Epoch: 10 [15232/50048]	Loss: 1.4169
Training Epoch: 10 [15360/50048]	Loss: 1.6957
Training Epoch: 10 [15488/50048]	Loss: 1.5943
Training Epoch: 10 [15616/50048]	Loss: 1.3748
Training Epoch: 10 [15744/50048]	Loss: 1.5835
Training Epoch: 10 [15872/50048]	Loss: 1.6220
Training Epoch: 10 [16000/50048]	Loss: 1.4798
Training Epoch: 10 [16128/50048]	Loss: 1.4272
Training Epoch: 10 [16256/50048]	Loss: 1.3875
Training Epoch: 10 [16384/50048]	Loss: 1.4615
Training Epoch: 10 [16512/50048]	Loss: 1.5243
Training Epoch: 10 [16640/50048]	Loss: 1.8734
Training Epoch: 10 [16768/50048]	Loss: 1.7269
Training Epoch: 10 [16896/50048]	Loss: 1.4534
Training Epoch: 10 [17024/50048]	Loss: 1.6347
Training Epoch: 10 [17152/50048]	Loss: 1.6016
Training Epoch: 10 [17280/50048]	Loss: 1.7907
Training Epoch: 10 [17408/50048]	Loss: 1.7688
Training Epoch: 10 [17536/50048]	Loss: 1.4500
Training Epoch: 10 [17664/50048]	Loss: 1.6037
Training Epoch: 10 [17792/50048]	Loss: 1.6212
Training Epoch: 10 [17920/50048]	Loss: 1.5749
Training Epoch: 10 [18048/50048]	Loss: 1.7386
Training Epoch: 10 [18176/50048]	Loss: 1.7457
Training Epoch: 10 [18304/50048]	Loss: 1.6856
Training Epoch: 10 [18432/50048]	Loss: 1.6722
Training Epoch: 10 [18560/50048]	Loss: 1.5268
Training Epoch: 10 [18688/50048]	Loss: 1.7411
Training Epoch: 10 [18816/50048]	Loss: 1.4921
Training Epoch: 10 [18944/50048]	Loss: 1.3881
Training Epoch: 10 [19072/50048]	Loss: 1.6934
Training Epoch: 10 [19200/50048]	Loss: 1.4235
Training Epoch: 10 [19328/50048]	Loss: 1.4876
Training Epoch: 10 [19456/50048]	Loss: 1.6472
Training Epoch: 10 [19584/50048]	Loss: 1.7443
Training Epoch: 10 [19712/50048]	Loss: 1.5844
Training Epoch: 10 [19840/50048]	Loss: 1.6020
Training Epoch: 10 [19968/50048]	Loss: 1.7057
Training Epoch: 10 [20096/50048]	Loss: 1.6397
Training Epoch: 10 [20224/50048]	Loss: 1.8810
Training Epoch: 10 [20352/50048]	Loss: 1.5398
Training Epoch: 10 [20480/50048]	Loss: 1.7512
Training Epoch: 10 [20608/50048]	Loss: 1.4927
Training Epoch: 10 [20736/50048]	Loss: 1.7154
Training Epoch: 10 [20864/50048]	Loss: 1.7991
Training Epoch: 10 [20992/50048]	Loss: 1.4100
Training Epoch: 10 [21120/50048]	Loss: 1.5452
Training Epoch: 10 [21248/50048]	Loss: 1.6534
Training Epoch: 10 [21376/50048]	Loss: 1.5100
Training Epoch: 10 [21504/50048]	Loss: 1.5641
Training Epoch: 10 [21632/50048]	Loss: 1.7036
Training Epoch: 10 [21760/50048]	Loss: 1.6904
Training Epoch: 10 [21888/50048]	Loss: 1.5713
Training Epoch: 10 [22016/50048]	Loss: 1.8287
Training Epoch: 10 [22144/50048]	Loss: 1.5872
Training Epoch: 10 [22272/50048]	Loss: 1.9405
Training Epoch: 10 [22400/50048]	Loss: 1.5191
Training Epoch: 10 [22528/50048]	Loss: 1.7077
Training Epoch: 10 [22656/50048]	Loss: 1.4976
Training Epoch: 10 [22784/50048]	Loss: 1.9065
Training Epoch: 10 [22912/50048]	Loss: 1.8827
Training Epoch: 10 [23040/50048]	Loss: 1.6542
Training Epoch: 10 [23168/50048]	Loss: 1.8228
Training Epoch: 10 [23296/50048]	Loss: 1.4312
Training Epoch: 10 [23424/50048]	Loss: 1.5791
Training Epoch: 10 [23552/50048]	Loss: 1.6424
Training Epoch: 10 [23680/50048]	Loss: 1.5562
Training Epoch: 10 [23808/50048]	Loss: 1.5645
Training Epoch: 10 [23936/50048]	Loss: 1.6893
Training Epoch: 10 [24064/50048]	Loss: 1.6507
Training Epoch: 10 [24192/50048]	Loss: 1.5181
Training Epoch: 10 [24320/50048]	Loss: 1.6171
Training Epoch: 10 [24448/50048]	Loss: 1.4609
Training Epoch: 10 [24576/50048]	Loss: 1.6222
Training Epoch: 10 [24704/50048]	Loss: 1.5368
Training Epoch: 10 [24832/50048]	Loss: 1.6415
Training Epoch: 10 [24960/50048]	Loss: 1.5698
Training Epoch: 10 [25088/50048]	Loss: 1.6082
Training Epoch: 10 [25216/50048]	Loss: 1.5651
Training Epoch: 10 [25344/50048]	Loss: 1.7263
Training Epoch: 10 [25472/50048]	Loss: 1.5860
Training Epoch: 10 [25600/50048]	Loss: 1.8487
Training Epoch: 10 [25728/50048]	Loss: 1.6113
Training Epoch: 10 [25856/50048]	Loss: 1.6299
Training Epoch: 10 [25984/50048]	Loss: 1.7976
Training Epoch: 10 [26112/50048]	Loss: 1.7900
Training Epoch: 10 [26240/50048]	Loss: 1.6393
Training Epoch: 10 [26368/50048]	Loss: 1.6733
Training Epoch: 10 [26496/50048]	Loss: 1.6104
Training Epoch: 10 [26624/50048]	Loss: 1.4527
Training Epoch: 10 [26752/50048]	Loss: 1.5358
Training Epoch: 10 [26880/50048]	Loss: 1.4264
Training Epoch: 10 [27008/50048]	Loss: 1.8375
Training Epoch: 10 [27136/50048]	Loss: 1.6874
Training Epoch: 10 [27264/50048]	Loss: 1.3837
Training Epoch: 10 [27392/50048]	Loss: 1.5679
Training Epoch: 10 [27520/50048]	Loss: 1.8051
Training Epoch: 10 [27648/50048]	Loss: 1.5980
Training Epoch: 10 [27776/50048]	Loss: 1.5821
Training Epoch: 10 [27904/50048]	Loss: 1.6915
Training Epoch: 10 [28032/50048]	Loss: 1.9092
Training Epoch: 10 [28160/50048]	Loss: 1.6105
Training Epoch: 10 [28288/50048]	Loss: 1.5902
Training Epoch: 10 [28416/50048]	Loss: 1.8774
Training Epoch: 10 [28544/50048]	Loss: 1.4787
Training Epoch: 10 [28672/50048]	Loss: 1.6209
Training Epoch: 10 [28800/50048]	Loss: 1.4515
Training Epoch: 10 [28928/50048]	Loss: 1.6010
Training Epoch: 10 [29056/50048]	Loss: 1.5724
Training Epoch: 10 [29184/50048]	Loss: 1.7053
Training Epoch: 10 [29312/50048]	Loss: 1.8051
Training Epoch: 10 [29440/50048]	Loss: 1.7234
Training Epoch: 10 [29568/50048]	Loss: 1.8809
Training Epoch: 10 [29696/50048]	Loss: 1.8343
Training Epoch: 10 [29824/50048]	Loss: 1.7406
Training Epoch: 10 [29952/50048]	Loss: 1.6672
Training Epoch: 10 [30080/50048]	Loss: 1.7120
Training Epoch: 10 [30208/50048]	Loss: 1.6486
Training Epoch: 10 [30336/50048]	Loss: 1.5828
Training Epoch: 10 [30464/50048]	Loss: 1.6392
Training Epoch: 10 [30592/50048]	Loss: 1.6186
Training Epoch: 10 [30720/50048]	Loss: 1.3976
Training Epoch: 10 [30848/50048]	Loss: 1.5299
Training Epoch: 10 [30976/50048]	Loss: 1.6530
Training Epoch: 10 [31104/50048]	Loss: 1.8306
Training Epoch: 10 [31232/50048]	Loss: 1.6025
Training Epoch: 10 [31360/50048]	Loss: 1.6507
Training Epoch: 10 [31488/50048]	Loss: 1.7666
Training Epoch: 10 [31616/50048]	Loss: 1.3808
Training Epoch: 10 [31744/50048]	Loss: 1.8722
Training Epoch: 10 [31872/50048]	Loss: 1.8030
Training Epoch: 10 [32000/50048]	Loss: 1.5191
Training Epoch: 10 [32128/50048]	Loss: 1.7925
Training Epoch: 10 [32256/50048]	Loss: 1.6695
Training Epoch: 10 [32384/50048]	Loss: 1.4665
Training Epoch: 10 [32512/50048]	Loss: 1.4726
Training Epoch: 10 [32640/50048]	Loss: 1.5140
Training Epoch: 10 [32768/50048]	Loss: 1.6317
Training Epoch: 10 [32896/50048]	Loss: 1.4382
Training Epoch: 10 [33024/50048]	Loss: 1.5823
Training Epoch: 10 [33152/50048]	Loss: 1.5791
Training Epoch: 10 [33280/50048]	Loss: 1.5293
Training Epoch: 10 [33408/50048]	Loss: 1.6554
Training Epoch: 10 [33536/50048]	Loss: 1.7058
Training Epoch: 10 [33664/50048]	Loss: 1.7475
Training Epoch: 10 [33792/50048]	Loss: 1.9151
Training Epoch: 10 [33920/50048]	Loss: 1.4686
Training Epoch: 10 [34048/50048]	Loss: 1.6267
Training Epoch: 10 [34176/50048]	Loss: 1.8965
Training Epoch: 10 [34304/50048]	Loss: 1.6058
Training Epoch: 10 [34432/50048]	Loss: 1.5134
Training Epoch: 10 [34560/50048]	Loss: 1.5023
Training Epoch: 10 [34688/50048]	Loss: 1.7083
Training Epoch: 10 [34816/50048]	Loss: 1.7179
Training Epoch: 10 [34944/50048]	Loss: 1.6243
Training Epoch: 10 [35072/50048]	Loss: 1.5806
Training Epoch: 10 [35200/50048]	Loss: 1.4880
Training Epoch: 10 [35328/50048]	Loss: 1.6118
Training Epoch: 10 [35456/50048]	Loss: 1.6891
Training Epoch: 10 [35584/50048]	Loss: 1.7200
Training Epoch: 10 [35712/50048]	Loss: 1.7439
Training Epoch: 10 [35840/50048]	Loss: 1.4791
Training Epoch: 10 [35968/50048]	Loss: 1.5748
Training Epoch: 10 [36096/50048]	Loss: 1.5527
Training Epoch: 10 [36224/50048]	Loss: 1.7992
Training Epoch: 10 [36352/50048]	Loss: 1.6004
Training Epoch: 10 [36480/50048]	Loss: 1.7889
Training Epoch: 10 [36608/50048]	Loss: 1.5371
Training Epoch: 10 [36736/50048]	Loss: 1.6731
Training Epoch: 10 [36864/50048]	Loss: 1.5039
Training Epoch: 10 [36992/50048]	Loss: 1.4624
Training Epoch: 10 [37120/50048]	Loss: 1.7562
Training Epoch: 10 [37248/50048]	Loss: 1.5522
Training Epoch: 10 [37376/50048]	Loss: 1.8275
Training Epoch: 10 [37504/50048]	Loss: 1.8012
Training Epoch: 10 [37632/50048]	Loss: 1.8277
Training Epoch: 10 [37760/50048]	Loss: 1.6436
Training Epoch: 10 [37888/50048]	Loss: 1.5286
Training Epoch: 10 [38016/50048]	Loss: 1.3664
Training Epoch: 10 [38144/50048]	Loss: 1.9798
Training Epoch: 10 [38272/50048]	Loss: 1.7979
Training Epoch: 10 [38400/50048]	Loss: 1.7459
Training Epoch: 10 [38528/50048]	Loss: 1.6496
Training Epoch: 10 [38656/50048]	Loss: 1.6214
Training Epoch: 10 [38784/50048]	Loss: 1.6413
Training Epoch: 10 [38912/50048]	Loss: 1.6733
Training Epoch: 10 [39040/50048]	Loss: 1.4426
Training Epoch: 10 [39168/50048]	Loss: 1.7981
Training Epoch: 10 [39296/50048]	Loss: 1.5396
Training Epoch: 10 [39424/50048]	Loss: 1.4131
Training Epoch: 10 [39552/50048]	Loss: 1.6936
Training Epoch: 10 [39680/50048]	Loss: 1.8766
Training Epoch: 10 [39808/50048]	Loss: 1.5784
Training Epoch: 10 [39936/50048]	Loss: 1.7420
Training Epoch: 10 [40064/50048]	Loss: 1.4136
Training Epoch: 10 [40192/50048]	Loss: 1.7986
Training Epoch: 10 [40320/50048]	Loss: 1.7465
Training Epoch: 10 [40448/50048]	Loss: 1.5893
Training Epoch: 10 [40576/50048]	Loss: 1.7966
Training Epoch: 10 [40704/50048]	Loss: 1.3382
Training Epoch: 10 [40832/50048]	Loss: 1.8018
Training Epoch: 10 [40960/50048]	Loss: 1.3745
Training Epoch: 10 [41088/50048]	Loss: 1.5537
Training Epoch: 10 [41216/50048]	Loss: 1.7414
Training Epoch: 10 [41344/50048]	Loss: 1.5019
Training Epoch: 10 [41472/50048]	Loss: 1.7923
Training Epoch: 10 [41600/50048]	Loss: 1.7837
Training Epoch: 10 [41728/50048]	Loss: 1.6446
Training Epoch: 10 [41856/50048]	Loss: 1.5298
Training Epoch: 10 [41984/50048]	Loss: 1.5957
Training Epoch: 10 [42112/50048]	Loss: 1.5089
Training Epoch: 10 [42240/50048]	Loss: 1.5573
Training Epoch: 10 [42368/50048]	Loss: 1.5261
Training Epoch: 10 [42496/50048]	Loss: 1.7516
Training Epoch: 10 [42624/50048]	Loss: 1.7586
Training Epoch: 10 [42752/50048]	Loss: 1.5709
Training Epoch: 10 [42880/50048]	Loss: 1.6777
Training Epoch: 10 [43008/50048]	Loss: 1.6963
Training Epoch: 10 [43136/50048]	Loss: 1.4952
Training Epoch: 10 [43264/50048]	Loss: 1.8191
Training Epoch: 10 [43392/50048]	Loss: 1.6074
Training Epoch: 10 [43520/50048]	Loss: 1.9110
Training Epoch: 10 [43648/50048]	Loss: 1.6363
Training Epoch: 10 [43776/50048]	Loss: 1.6861
Training Epoch: 10 [43904/50048]	Loss: 1.9419
Training Epoch: 10 [44032/50048]	Loss: 1.5196
Training Epoch: 10 [44160/50048]	Loss: 1.3159
Training Epoch: 10 [44288/50048]	Loss: 1.6902
Training Epoch: 10 [44416/50048]	Loss: 1.8163
Training Epoch: 10 [44544/50048]	Loss: 1.8322
Training Epoch: 10 [44672/50048]	Loss: 1.6037
Training Epoch: 10 [44800/50048]	Loss: 1.4668
Training Epoch: 10 [44928/50048]	Loss: 1.5770
Training Epoch: 10 [45056/50048]	Loss: 1.6634
Training Epoch: 10 [45184/50048]	Loss: 1.5317
Training Epoch: 10 [45312/50048]	Loss: 1.5903
Training Epoch: 10 [45440/50048]	Loss: 1.5755
Training Epoch: 10 [45568/50048]	Loss: 1.7533
Training Epoch: 10 [45696/50048]	Loss: 1.7471
Training Epoch: 10 [45824/50048]	Loss: 1.5696
Training Epoch: 10 [45952/50048]	Loss: 1.7006
Training Epoch: 10 [46080/50048]	Loss: 1.8146
Training Epoch: 10 [46208/50048]	Loss: 1.6782
Training Epoch: 10 [46336/50048]	Loss: 1.6759
Training Epoch: 10 [46464/50048]	Loss: 1.4776
Training Epoch: 10 [46592/50048]	Loss: 1.9792
Training Epoch: 10 [46720/50048]	Loss: 1.5675
Training Epoch: 10 [46848/50048]	Loss: 1.5923
Training Epoch: 10 [46976/50048]	Loss: 1.5801
Training Epoch: 10 [47104/50048]	Loss: 1.7788
Training Epoch: 10 [47232/50048]	Loss: 1.7804
Training Epoch: 10 [47360/50048]	Loss: 1.5720
Training Epoch: 10 [47488/50048]	Loss: 1.6892
Training Epoch: 10 [47616/50048]	Loss: 1.8098
Training Epoch: 10 [47744/50048]	Loss: 1.9504
Training Epoch: 10 [47872/50048]	Loss: 1.5342
Training Epoch: 10 [48000/50048]	Loss: 1.9335
Training Epoch: 10 [48128/50048]	Loss: 1.8691
Training Epoch: 10 [48256/50048]	Loss: 1.7740
Training Epoch: 10 [48384/50048]	Loss: 1.5471
Training Epoch: 10 [48512/50048]	Loss: 1.7041
Training Epoch: 10 [48640/50048]	Loss: 1.5653
Training Epoch: 10 [48768/50048]	Loss: 1.7409
Training Epoch: 10 [48896/50048]	Loss: 1.4213
Training Epoch: 10 [49024/50048]	Loss: 1.7739
Training Epoch: 10 [49152/50048]	Loss: 1.6258
Training Epoch: 10 [49280/50048]	Loss: 1.5682
Training Epoch: 10 [49408/50048]	Loss: 1.6672
Training Epoch: 10 [49536/50048]	Loss: 1.2998
Training Epoch: 10 [49664/50048]	Loss: 1.4892
Training Epoch: 10 [49792/50048]	Loss: 1.6900
Training Epoch: 10 [49920/50048]	Loss: 1.6624
Training Epoch: 10 [50048/50048]	Loss: 1.7894
Validation Epoch: 10, Average loss: 0.0136, Accuracy: 0.5251
Training Epoch: 11 [128/50048]	Loss: 1.5852
Training Epoch: 11 [256/50048]	Loss: 1.6390
Training Epoch: 11 [384/50048]	Loss: 1.8387
Training Epoch: 11 [512/50048]	Loss: 1.5391
Training Epoch: 11 [640/50048]	Loss: 1.4059
Training Epoch: 11 [768/50048]	Loss: 1.5349
Training Epoch: 11 [896/50048]	Loss: 1.6946
Training Epoch: 11 [1024/50048]	Loss: 1.6683
Training Epoch: 11 [1152/50048]	Loss: 1.3612
Training Epoch: 11 [1280/50048]	Loss: 1.4455
Training Epoch: 11 [1408/50048]	Loss: 1.6850
Training Epoch: 11 [1536/50048]	Loss: 1.5044
Training Epoch: 11 [1664/50048]	Loss: 1.5501
Training Epoch: 11 [1792/50048]	Loss: 1.4767
Training Epoch: 11 [1920/50048]	Loss: 1.5754
Training Epoch: 11 [2048/50048]	Loss: 1.5136
Training Epoch: 11 [2176/50048]	Loss: 1.9532
Training Epoch: 11 [2304/50048]	Loss: 1.7258
Training Epoch: 11 [2432/50048]	Loss: 1.4327
Training Epoch: 11 [2560/50048]	Loss: 1.6711
Training Epoch: 11 [2688/50048]	Loss: 1.2812
Training Epoch: 11 [2816/50048]	Loss: 1.4356
Training Epoch: 11 [2944/50048]	Loss: 1.6335
Training Epoch: 11 [3072/50048]	Loss: 1.5391
Training Epoch: 11 [3200/50048]	Loss: 1.6819
Training Epoch: 11 [3328/50048]	Loss: 1.4631
Training Epoch: 11 [3456/50048]	Loss: 1.6378
Training Epoch: 11 [3584/50048]	Loss: 1.6734
Training Epoch: 11 [3712/50048]	Loss: 1.7273
Training Epoch: 11 [3840/50048]	Loss: 1.5763
Training Epoch: 11 [3968/50048]	Loss: 1.6279
Training Epoch: 11 [4096/50048]	Loss: 1.5645
Training Epoch: 11 [4224/50048]	Loss: 1.8696
Training Epoch: 11 [4352/50048]	Loss: 1.8351
Training Epoch: 11 [4480/50048]	Loss: 1.5252
Training Epoch: 11 [4608/50048]	Loss: 1.4720
Training Epoch: 11 [4736/50048]	Loss: 1.5175
Training Epoch: 11 [4864/50048]	Loss: 1.6822
Training Epoch: 11 [4992/50048]	Loss: 1.6050
Training Epoch: 11 [5120/50048]	Loss: 1.4362
Training Epoch: 11 [5248/50048]	Loss: 1.8166
Training Epoch: 11 [5376/50048]	Loss: 1.7613
Training Epoch: 11 [5504/50048]	Loss: 1.4565
Training Epoch: 11 [5632/50048]	Loss: 1.4373
Training Epoch: 11 [5760/50048]	Loss: 1.7168
Training Epoch: 11 [5888/50048]	Loss: 1.6208
Training Epoch: 11 [6016/50048]	Loss: 1.2801
Training Epoch: 11 [6144/50048]	Loss: 1.5514
Training Epoch: 11 [6272/50048]	Loss: 1.7634
Training Epoch: 11 [6400/50048]	Loss: 1.4904
Training Epoch: 11 [6528/50048]	Loss: 1.8246
Training Epoch: 11 [6656/50048]	Loss: 1.5417
Training Epoch: 11 [6784/50048]	Loss: 1.7493
Training Epoch: 11 [6912/50048]	Loss: 1.5538
Training Epoch: 11 [7040/50048]	Loss: 1.3725
Training Epoch: 11 [7168/50048]	Loss: 1.7394
Training Epoch: 11 [7296/50048]	Loss: 1.4917
Training Epoch: 11 [7424/50048]	Loss: 1.7397
Training Epoch: 11 [7552/50048]	Loss: 1.6262
Training Epoch: 11 [7680/50048]	Loss: 1.6665
Training Epoch: 11 [7808/50048]	Loss: 1.4616
Training Epoch: 11 [7936/50048]	Loss: 1.5753
Training Epoch: 11 [8064/50048]	Loss: 1.4840
Training Epoch: 11 [8192/50048]	Loss: 1.8618
Training Epoch: 11 [8320/50048]	Loss: 1.4670
Training Epoch: 11 [8448/50048]	Loss: 1.8065
Training Epoch: 11 [8576/50048]	Loss: 1.4492
Training Epoch: 11 [8704/50048]	Loss: 1.5235
Training Epoch: 11 [8832/50048]	Loss: 1.7650
Training Epoch: 11 [8960/50048]	Loss: 1.5029
Training Epoch: 11 [9088/50048]	Loss: 1.6482
Training Epoch: 11 [9216/50048]	Loss: 1.4218
Training Epoch: 11 [9344/50048]	Loss: 1.7772
Training Epoch: 11 [9472/50048]	Loss: 1.3864
Training Epoch: 11 [9600/50048]	Loss: 1.4750
Training Epoch: 11 [9728/50048]	Loss: 1.3952
Training Epoch: 11 [9856/50048]	Loss: 1.8367
Training Epoch: 11 [9984/50048]	Loss: 1.7530
Training Epoch: 11 [10112/50048]	Loss: 1.4072
Training Epoch: 11 [10240/50048]	Loss: 1.5694
Training Epoch: 11 [10368/50048]	Loss: 1.7023
Training Epoch: 11 [10496/50048]	Loss: 1.5543
Training Epoch: 11 [10624/50048]	Loss: 2.0100
Training Epoch: 11 [10752/50048]	Loss: 1.7476
Training Epoch: 11 [10880/50048]	Loss: 1.3659
Training Epoch: 11 [11008/50048]	Loss: 1.7085
Training Epoch: 11 [11136/50048]	Loss: 1.5521
Training Epoch: 11 [11264/50048]	Loss: 1.8322
Training Epoch: 11 [11392/50048]	Loss: 1.7217
Training Epoch: 11 [11520/50048]	Loss: 1.6975
Training Epoch: 11 [11648/50048]	Loss: 1.5001
Training Epoch: 11 [11776/50048]	Loss: 1.7366
Training Epoch: 11 [11904/50048]	Loss: 1.5462
Training Epoch: 11 [12032/50048]	Loss: 1.8161
Training Epoch: 11 [12160/50048]	Loss: 1.7129
Training Epoch: 11 [12288/50048]	Loss: 1.3211
Training Epoch: 11 [12416/50048]	Loss: 1.6724
Training Epoch: 11 [12544/50048]	Loss: 1.3707
Training Epoch: 11 [12672/50048]	Loss: 1.6105
Training Epoch: 11 [12800/50048]	Loss: 1.7351
Training Epoch: 11 [12928/50048]	Loss: 1.4772
Training Epoch: 11 [13056/50048]	Loss: 1.5115
Training Epoch: 11 [13184/50048]	Loss: 1.7392
Training Epoch: 11 [13312/50048]	Loss: 1.7970
Training Epoch: 11 [13440/50048]	Loss: 1.6690
Training Epoch: 11 [13568/50048]	Loss: 1.6307
Training Epoch: 11 [13696/50048]	Loss: 1.6524
Training Epoch: 11 [13824/50048]	Loss: 1.4859
Training Epoch: 11 [13952/50048]	Loss: 1.4227
Training Epoch: 11 [14080/50048]	Loss: 1.6980
Training Epoch: 11 [14208/50048]	Loss: 1.7107
Training Epoch: 11 [14336/50048]	Loss: 1.7052
Training Epoch: 11 [14464/50048]	Loss: 1.7715
Training Epoch: 11 [14592/50048]	Loss: 1.6729
Training Epoch: 11 [14720/50048]	Loss: 1.4401
Training Epoch: 11 [14848/50048]	Loss: 1.4574
Training Epoch: 11 [14976/50048]	Loss: 1.4265
Training Epoch: 11 [15104/50048]	Loss: 1.6034
Training Epoch: 11 [15232/50048]	Loss: 1.5248
Training Epoch: 11 [15360/50048]	Loss: 1.5144
Training Epoch: 11 [15488/50048]	Loss: 1.6257
Training Epoch: 11 [15616/50048]	Loss: 1.4723
Training Epoch: 11 [15744/50048]	Loss: 1.7845
Training Epoch: 11 [15872/50048]	Loss: 1.8289
Training Epoch: 11 [16000/50048]	Loss: 1.7479
Training Epoch: 11 [16128/50048]	Loss: 1.5668
Training Epoch: 11 [16256/50048]	Loss: 1.3908
Training Epoch: 11 [16384/50048]	Loss: 1.4999
Training Epoch: 11 [16512/50048]	Loss: 1.5418
Training Epoch: 11 [16640/50048]	Loss: 1.5426
Training Epoch: 11 [16768/50048]	Loss: 1.4933
Training Epoch: 11 [16896/50048]	Loss: 1.5314
Training Epoch: 11 [17024/50048]	Loss: 1.4999
Training Epoch: 11 [17152/50048]	Loss: 1.6186
Training Epoch: 11 [17280/50048]	Loss: 1.8449
Training Epoch: 11 [17408/50048]	Loss: 1.8353
Training Epoch: 11 [17536/50048]	Loss: 1.5293
Training Epoch: 11 [17664/50048]	Loss: 1.7289
Training Epoch: 11 [17792/50048]	Loss: 1.5999
Training Epoch: 11 [17920/50048]	Loss: 1.5815
Training Epoch: 11 [18048/50048]	Loss: 1.8042
Training Epoch: 11 [18176/50048]	Loss: 1.5245
Training Epoch: 11 [18304/50048]	Loss: 1.5357
Training Epoch: 11 [18432/50048]	Loss: 1.7584
Training Epoch: 11 [18560/50048]	Loss: 1.5653
Training Epoch: 11 [18688/50048]	Loss: 1.5793
Training Epoch: 11 [18816/50048]	Loss: 1.8609
Training Epoch: 11 [18944/50048]	Loss: 1.8134
Training Epoch: 11 [19072/50048]	Loss: 1.7610
Training Epoch: 11 [19200/50048]	Loss: 1.5995
Training Epoch: 11 [19328/50048]	Loss: 1.3769
Training Epoch: 11 [19456/50048]	Loss: 1.4284
Training Epoch: 11 [19584/50048]	Loss: 1.7696
Training Epoch: 11 [19712/50048]	Loss: 1.8598
Training Epoch: 11 [19840/50048]	Loss: 1.7093
Training Epoch: 11 [19968/50048]	Loss: 1.6804
Training Epoch: 11 [20096/50048]	Loss: 1.8968
Training Epoch: 11 [20224/50048]	Loss: 1.4712
Training Epoch: 11 [20352/50048]	Loss: 1.5504
Training Epoch: 11 [20480/50048]	Loss: 1.4426
Training Epoch: 11 [20608/50048]	Loss: 1.5181
Training Epoch: 11 [20736/50048]	Loss: 1.5308
Training Epoch: 11 [20864/50048]	Loss: 1.6365
Training Epoch: 11 [20992/50048]	Loss: 1.6814
Training Epoch: 11 [21120/50048]	Loss: 1.5150
Training Epoch: 11 [21248/50048]	Loss: 1.6629
Training Epoch: 11 [21376/50048]	Loss: 1.6572
Training Epoch: 11 [21504/50048]	Loss: 1.5822
Training Epoch: 11 [21632/50048]	Loss: 1.7213
Training Epoch: 11 [21760/50048]	Loss: 1.5681
Training Epoch: 11 [21888/50048]	Loss: 1.3441
Training Epoch: 11 [22016/50048]	Loss: 1.8493
Training Epoch: 11 [22144/50048]	Loss: 1.4878
Training Epoch: 11 [22272/50048]	Loss: 1.5812
Training Epoch: 11 [22400/50048]	Loss: 1.4731
Training Epoch: 11 [22528/50048]	Loss: 1.4855
Training Epoch: 11 [22656/50048]	Loss: 1.4663
Training Epoch: 11 [22784/50048]	Loss: 1.6569
Training Epoch: 11 [22912/50048]	Loss: 1.6722
Training Epoch: 11 [23040/50048]	Loss: 1.4121
Training Epoch: 11 [23168/50048]	Loss: 1.7666
Training Epoch: 11 [23296/50048]	Loss: 1.5862
Training Epoch: 11 [23424/50048]	Loss: 1.6459
Training Epoch: 11 [23552/50048]	Loss: 1.6874
Training Epoch: 11 [23680/50048]	Loss: 1.5134
Training Epoch: 11 [23808/50048]	Loss: 1.9355
Training Epoch: 11 [23936/50048]	Loss: 1.5470
Training Epoch: 11 [24064/50048]	Loss: 1.8130
Training Epoch: 11 [24192/50048]	Loss: 1.6706
Training Epoch: 11 [24320/50048]	Loss: 1.5332
Training Epoch: 11 [24448/50048]	Loss: 1.5841
Training Epoch: 11 [24576/50048]	Loss: 1.6358
Training Epoch: 11 [24704/50048]	Loss: 1.4421
Training Epoch: 11 [24832/50048]	Loss: 1.7540
Training Epoch: 11 [24960/50048]	Loss: 1.7967
Training Epoch: 11 [25088/50048]	Loss: 1.4702
Training Epoch: 11 [25216/50048]	Loss: 1.5870
Training Epoch: 11 [25344/50048]	Loss: 1.4079
Training Epoch: 11 [25472/50048]	Loss: 1.5220
Training Epoch: 11 [25600/50048]	Loss: 1.6688
Training Epoch: 11 [25728/50048]	Loss: 1.7129
Training Epoch: 11 [25856/50048]	Loss: 1.5392
Training Epoch: 11 [25984/50048]	Loss: 1.8279
Training Epoch: 11 [26112/50048]	Loss: 1.5392
Training Epoch: 11 [26240/50048]	Loss: 1.6976
Training Epoch: 11 [26368/50048]	Loss: 1.6749
Training Epoch: 11 [26496/50048]	Loss: 1.4446
Training Epoch: 11 [26624/50048]	Loss: 1.5510
Training Epoch: 11 [26752/50048]	Loss: 1.4457
Training Epoch: 11 [26880/50048]	Loss: 1.8819
Training Epoch: 11 [27008/50048]	Loss: 1.7264
Training Epoch: 11 [27136/50048]	Loss: 1.4302
Training Epoch: 11 [27264/50048]	Loss: 1.5157
Training Epoch: 11 [27392/50048]	Loss: 1.5253
Training Epoch: 11 [27520/50048]	Loss: 1.8445
Training Epoch: 11 [27648/50048]	Loss: 1.4533
Training Epoch: 11 [27776/50048]	Loss: 1.4383
Training Epoch: 11 [27904/50048]	Loss: 1.6682
Training Epoch: 11 [28032/50048]	Loss: 1.7746
Training Epoch: 11 [28160/50048]	Loss: 1.5122
Training Epoch: 11 [28288/50048]	Loss: 1.5665
Training Epoch: 11 [28416/50048]	Loss: 1.7156
Training Epoch: 11 [28544/50048]	Loss: 1.8232
Training Epoch: 11 [28672/50048]	Loss: 1.5806
Training Epoch: 11 [28800/50048]	Loss: 1.5088
Training Epoch: 11 [28928/50048]	Loss: 1.6119
Training Epoch: 11 [29056/50048]	Loss: 1.5731
Training Epoch: 11 [29184/50048]	Loss: 1.6449
Training Epoch: 11 [29312/50048]	Loss: 1.4671
Training Epoch: 11 [29440/50048]	Loss: 1.4652
Training Epoch: 11 [29568/50048]	Loss: 1.5519
Training Epoch: 11 [29696/50048]	Loss: 1.7397
Training Epoch: 11 [29824/50048]	Loss: 1.6817
Training Epoch: 11 [29952/50048]	Loss: 1.6681
Training Epoch: 11 [30080/50048]	Loss: 1.6060
Training Epoch: 11 [30208/50048]	Loss: 1.6310
Training Epoch: 11 [30336/50048]	Loss: 1.5518
Training Epoch: 11 [30464/50048]	Loss: 1.5062
Training Epoch: 11 [30592/50048]	Loss: 1.3144
Training Epoch: 11 [30720/50048]	Loss: 1.6924
Training Epoch: 11 [30848/50048]	Loss: 1.5319
Training Epoch: 11 [30976/50048]	Loss: 1.6817
Training Epoch: 11 [31104/50048]	Loss: 1.6389
Training Epoch: 11 [31232/50048]	Loss: 1.3367
Training Epoch: 11 [31360/50048]	Loss: 1.6199
Training Epoch: 11 [31488/50048]	Loss: 1.4424
Training Epoch: 11 [31616/50048]	Loss: 1.7030
Training Epoch: 11 [31744/50048]	Loss: 1.4972
Training Epoch: 11 [31872/50048]	Loss: 1.6137
Training Epoch: 11 [32000/50048]	Loss: 1.8171
Training Epoch: 11 [32128/50048]	Loss: 1.9110
Training Epoch: 11 [32256/50048]	Loss: 1.5086
Training Epoch: 11 [32384/50048]	Loss: 1.9996
Training Epoch: 11 [32512/50048]	Loss: 1.6076
Training Epoch: 11 [32640/50048]	Loss: 1.6960
Training Epoch: 11 [32768/50048]	Loss: 1.5556
Training Epoch: 11 [32896/50048]	Loss: 1.6712
Training Epoch: 11 [33024/50048]	Loss: 1.6322
Training Epoch: 11 [33152/50048]	Loss: 1.6115
Training Epoch: 11 [33280/50048]	Loss: 1.4485
Training Epoch: 11 [33408/50048]	Loss: 1.6830
Training Epoch: 11 [33536/50048]	Loss: 1.7410
Training Epoch: 11 [33664/50048]	Loss: 1.6724
Training Epoch: 11 [33792/50048]	Loss: 1.5924
Training Epoch: 11 [33920/50048]	Loss: 1.5247
Training Epoch: 11 [34048/50048]	Loss: 1.8013
Training Epoch: 11 [34176/50048]	Loss: 1.5644
Training Epoch: 11 [34304/50048]	Loss: 1.6080
Training Epoch: 11 [34432/50048]	Loss: 1.6041
Training Epoch: 11 [34560/50048]	Loss: 1.6845
Training Epoch: 11 [34688/50048]	Loss: 1.4799
Training Epoch: 11 [34816/50048]	Loss: 1.3332
Training Epoch: 11 [34944/50048]	Loss: 1.5292
Training Epoch: 11 [35072/50048]	Loss: 1.7377
Training Epoch: 11 [35200/50048]	Loss: 1.5747
Training Epoch: 11 [35328/50048]	Loss: 1.5351
Training Epoch: 11 [35456/50048]	Loss: 1.6894
Training Epoch: 11 [35584/50048]	Loss: 1.6492
Training Epoch: 11 [35712/50048]	Loss: 1.4126
Training Epoch: 11 [35840/50048]	Loss: 1.7128
Training Epoch: 11 [35968/50048]	Loss: 1.5149
Training Epoch: 11 [36096/50048]	Loss: 1.8588
Training Epoch: 11 [36224/50048]	Loss: 1.7285
Training Epoch: 11 [36352/50048]	Loss: 1.4859
Training Epoch: 11 [36480/50048]	Loss: 1.7059
Training Epoch: 11 [36608/50048]	Loss: 1.6004
Training Epoch: 11 [36736/50048]	Loss: 1.4294
Training Epoch: 11 [36864/50048]	Loss: 1.7083
Training Epoch: 11 [36992/50048]	Loss: 1.5189
Training Epoch: 11 [37120/50048]	Loss: 1.8548
Training Epoch: 11 [37248/50048]	Loss: 1.4165
Training Epoch: 11 [37376/50048]	Loss: 1.4870
Training Epoch: 11 [37504/50048]	Loss: 2.0728
Training Epoch: 11 [37632/50048]	Loss: 1.7225
Training Epoch: 11 [37760/50048]	Loss: 1.4441
Training Epoch: 11 [37888/50048]	Loss: 1.4917
Training Epoch: 11 [38016/50048]	Loss: 1.5573
Training Epoch: 11 [38144/50048]	Loss: 1.6770
Training Epoch: 11 [38272/50048]	Loss: 1.8162
Training Epoch: 11 [38400/50048]	Loss: 1.5929
Training Epoch: 11 [38528/50048]	Loss: 1.5715
Training Epoch: 11 [38656/50048]	Loss: 1.6030
Training Epoch: 11 [38784/50048]	Loss: 1.2819
Training Epoch: 11 [38912/50048]	Loss: 1.4511
Training Epoch: 11 [39040/50048]	Loss: 1.4691
Training Epoch: 11 [39168/50048]	Loss: 1.7243
Training Epoch: 11 [39296/50048]	Loss: 1.6542
Training Epoch: 11 [39424/50048]	Loss: 1.5845
Training Epoch: 11 [39552/50048]	Loss: 1.7361
Training Epoch: 11 [39680/50048]	Loss: 1.6888
Training Epoch: 11 [39808/50048]	Loss: 1.5648
Training Epoch: 11 [39936/50048]	Loss: 1.5220
Training Epoch: 11 [40064/50048]	Loss: 1.6112
Training Epoch: 11 [40192/50048]	Loss: 1.5349
Training Epoch: 11 [40320/50048]	Loss: 1.5758
Training Epoch: 11 [40448/50048]	Loss: 1.7424
Training Epoch: 11 [40576/50048]	Loss: 1.6724
Training Epoch: 11 [40704/50048]	Loss: 1.4307
Training Epoch: 11 [40832/50048]	Loss: 2.0316
Training Epoch: 11 [40960/50048]	Loss: 1.6827
Training Epoch: 11 [41088/50048]	Loss: 1.4540
Training Epoch: 11 [41216/50048]	Loss: 1.5584
Training Epoch: 11 [41344/50048]	Loss: 1.7443
Training Epoch: 11 [41472/50048]	Loss: 1.5072
Training Epoch: 11 [41600/50048]	Loss: 1.7088
Training Epoch: 11 [41728/50048]	Loss: 1.6199
Training Epoch: 11 [41856/50048]	Loss: 1.5235
Training Epoch: 11 [41984/50048]	Loss: 1.1815
Training Epoch: 11 [42112/50048]	Loss: 1.9370
Training Epoch: 11 [42240/50048]	Loss: 1.3380
Training Epoch: 11 [42368/50048]	Loss: 1.6988
Training Epoch: 11 [42496/50048]	Loss: 1.6589
Training Epoch: 11 [42624/50048]	Loss: 1.3908
Training Epoch: 11 [42752/50048]	Loss: 1.3552
Training Epoch: 11 [42880/50048]	Loss: 1.4159
Training Epoch: 11 [43008/50048]	Loss: 1.5568
Training Epoch: 11 [43136/50048]	Loss: 1.8389
Training Epoch: 11 [43264/50048]	Loss: 1.4632
Training Epoch: 11 [43392/50048]	Loss: 1.7123
Training Epoch: 11 [43520/50048]	Loss: 1.7272
Training Epoch: 11 [43648/50048]	Loss: 1.8442
Training Epoch: 11 [43776/50048]	Loss: 1.6788
Training Epoch: 11 [43904/50048]	Loss: 1.6804
Training Epoch: 11 [44032/50048]	Loss: 1.4991
Training Epoch: 11 [44160/50048]	Loss: 1.3858
Training Epoch: 11 [44288/50048]	Loss: 1.6971
Training Epoch: 11 [44416/50048]	Loss: 1.5711
Training Epoch: 11 [44544/50048]	Loss: 1.6020
Training Epoch: 11 [44672/50048]	Loss: 1.9584
Training Epoch: 11 [44800/50048]	Loss: 1.7727
Training Epoch: 11 [44928/50048]	Loss: 1.9063
Training Epoch: 11 [45056/50048]	Loss: 1.5886
Training Epoch: 11 [45184/50048]	Loss: 1.8019
Training Epoch: 11 [45312/50048]	Loss: 1.5340
Training Epoch: 11 [45440/50048]	Loss: 1.9341
Training Epoch: 11 [45568/50048]	Loss: 1.4592
Training Epoch: 11 [45696/50048]	Loss: 1.5763
Training Epoch: 11 [45824/50048]	Loss: 1.6318
Training Epoch: 11 [45952/50048]	Loss: 1.8081
Training Epoch: 11 [46080/50048]	Loss: 1.7839
Training Epoch: 11 [46208/50048]	Loss: 1.4407
Training Epoch: 11 [46336/50048]	Loss: 1.6399
Training Epoch: 11 [46464/50048]	Loss: 1.4067
Training Epoch: 11 [46592/50048]	Loss: 1.4372
Training Epoch: 11 [46720/50048]	Loss: 1.4581
Training Epoch: 11 [46848/50048]	Loss: 1.5753
Training Epoch: 11 [46976/50048]	Loss: 1.5430
Training Epoch: 11 [47104/50048]	Loss: 1.5013
Training Epoch: 11 [47232/50048]	Loss: 1.7559
Training Epoch: 11 [47360/50048]	Loss: 1.6073
Training Epoch: 11 [47488/50048]	Loss: 1.6433
Training Epoch: 11 [47616/50048]	Loss: 1.7356
Training Epoch: 11 [47744/50048]	Loss: 1.4988
Training Epoch: 11 [47872/50048]	Loss: 1.5487
Training Epoch: 11 [48000/50048]	Loss: 1.6459
Training Epoch: 11 [48128/50048]	Loss: 1.5760
Training Epoch: 11 [48256/50048]	Loss: 1.7683
Training Epoch: 11 [48384/50048]	Loss: 1.6990
Training Epoch: 11 [48512/50048]	Loss: 1.7501
Training Epoch: 11 [48640/50048]	Loss: 1.2977
Training Epoch: 11 [48768/50048]	Loss: 1.6739
Training Epoch: 11 [48896/50048]	Loss: 1.7107
Training Epoch: 11 [49024/50048]	Loss: 1.5395
Training Epoch: 11 [49152/50048]	Loss: 1.7330
Training Epoch: 11 [49280/50048]	Loss: 1.6638
Training Epoch: 11 [49408/50048]	Loss: 1.4788
Training Epoch: 11 [49536/50048]	Loss: 1.6277
Training Epoch: 11 [49664/50048]	Loss: 1.4435
Training Epoch: 11 [49792/50048]	Loss: 1.4257
Training Epoch: 11 [49920/50048]	Loss: 1.5115
Training Epoch: 11 [50048/50048]	Loss: 1.5583
Validation Epoch: 11, Average loss: 0.0133, Accuracy: 0.5336
Training Epoch: 12 [128/50048]	Loss: 1.5152
Training Epoch: 12 [256/50048]	Loss: 1.6933
Training Epoch: 12 [384/50048]	Loss: 1.4528
Training Epoch: 12 [512/50048]	Loss: 1.9753
Training Epoch: 12 [640/50048]	Loss: 1.7070
Training Epoch: 12 [768/50048]	Loss: 1.5003
Training Epoch: 12 [896/50048]	Loss: 1.4978
Training Epoch: 12 [1024/50048]	Loss: 1.8430
Training Epoch: 12 [1152/50048]	Loss: 1.6564
Training Epoch: 12 [1280/50048]	Loss: 1.2863
Training Epoch: 12 [1408/50048]	Loss: 1.2604
Training Epoch: 12 [1536/50048]	Loss: 1.6301
Training Epoch: 12 [1664/50048]	Loss: 1.7201
Training Epoch: 12 [1792/50048]	Loss: 1.3934
Training Epoch: 12 [1920/50048]	Loss: 1.7037
Training Epoch: 12 [2048/50048]	Loss: 1.4789
Training Epoch: 12 [2176/50048]	Loss: 1.5282
Training Epoch: 12 [2304/50048]	Loss: 1.3071
Training Epoch: 12 [2432/50048]	Loss: 1.5625
Training Epoch: 12 [2560/50048]	Loss: 1.5924
Training Epoch: 12 [2688/50048]	Loss: 1.4850
Training Epoch: 12 [2816/50048]	Loss: 1.5958
Training Epoch: 12 [2944/50048]	Loss: 1.5191
Training Epoch: 12 [3072/50048]	Loss: 1.4264
Training Epoch: 12 [3200/50048]	Loss: 1.6802
Training Epoch: 12 [3328/50048]	Loss: 1.8826
Training Epoch: 12 [3456/50048]	Loss: 1.5339
Training Epoch: 12 [3584/50048]	Loss: 1.5041
Training Epoch: 12 [3712/50048]	Loss: 1.2594
Training Epoch: 12 [3840/50048]	Loss: 1.9260
Training Epoch: 12 [3968/50048]	Loss: 1.6944
Training Epoch: 12 [4096/50048]	Loss: 1.9554
Training Epoch: 12 [4224/50048]	Loss: 1.4925
Training Epoch: 12 [4352/50048]	Loss: 1.4355
Training Epoch: 12 [4480/50048]	Loss: 1.4208
Training Epoch: 12 [4608/50048]	Loss: 1.4212
Training Epoch: 12 [4736/50048]	Loss: 1.6764
Training Epoch: 12 [4864/50048]	Loss: 1.5819
Training Epoch: 12 [4992/50048]	Loss: 1.7515
Training Epoch: 12 [5120/50048]	Loss: 1.5689
Training Epoch: 12 [5248/50048]	Loss: 1.5443
Training Epoch: 12 [5376/50048]	Loss: 1.5375
Training Epoch: 12 [5504/50048]	Loss: 1.3535
Training Epoch: 12 [5632/50048]	Loss: 1.6892
Training Epoch: 12 [5760/50048]	Loss: 1.5856
Training Epoch: 12 [5888/50048]	Loss: 1.4514
Training Epoch: 12 [6016/50048]	Loss: 1.6935
Training Epoch: 12 [6144/50048]	Loss: 1.7058
Training Epoch: 12 [6272/50048]	Loss: 1.4180
Training Epoch: 12 [6400/50048]	Loss: 2.0223
Training Epoch: 12 [6528/50048]	Loss: 1.4503
Training Epoch: 12 [6656/50048]	Loss: 1.7275
Training Epoch: 12 [6784/50048]	Loss: 1.5426
Training Epoch: 12 [6912/50048]	Loss: 1.4523
Training Epoch: 12 [7040/50048]	Loss: 1.6269
Training Epoch: 12 [7168/50048]	Loss: 1.5368
Training Epoch: 12 [7296/50048]	Loss: 1.5872
Training Epoch: 12 [7424/50048]	Loss: 1.4342
Training Epoch: 12 [7552/50048]	Loss: 1.3492
Training Epoch: 12 [7680/50048]	Loss: 1.5460
Training Epoch: 12 [7808/50048]	Loss: 1.4498
Training Epoch: 12 [7936/50048]	Loss: 1.2071
Training Epoch: 12 [8064/50048]	Loss: 1.6748
Training Epoch: 12 [8192/50048]	Loss: 1.5840
Training Epoch: 12 [8320/50048]	Loss: 1.6541
Training Epoch: 12 [8448/50048]	Loss: 1.5825
Training Epoch: 12 [8576/50048]	Loss: 1.6518
Training Epoch: 12 [8704/50048]	Loss: 1.6169
Training Epoch: 12 [8832/50048]	Loss: 1.7279
Training Epoch: 12 [8960/50048]	Loss: 1.5902
Training Epoch: 12 [9088/50048]	Loss: 1.4910
Training Epoch: 12 [9216/50048]	Loss: 1.6121
Training Epoch: 12 [9344/50048]	Loss: 1.8404
Training Epoch: 12 [9472/50048]	Loss: 1.7078
Training Epoch: 12 [9600/50048]	Loss: 1.4964
Training Epoch: 12 [9728/50048]	Loss: 1.7208
Training Epoch: 12 [9856/50048]	Loss: 1.4123
Training Epoch: 12 [9984/50048]	Loss: 1.5697
Training Epoch: 12 [10112/50048]	Loss: 1.1907
Training Epoch: 12 [10240/50048]	Loss: 1.5088
Training Epoch: 12 [10368/50048]	Loss: 1.4592
Training Epoch: 12 [10496/50048]	Loss: 1.5044
Training Epoch: 12 [10624/50048]	Loss: 1.8677
Training Epoch: 12 [10752/50048]	Loss: 1.5881
Training Epoch: 12 [10880/50048]	Loss: 1.3709
Training Epoch: 12 [11008/50048]	Loss: 1.7395
Training Epoch: 12 [11136/50048]	Loss: 1.4811
Training Epoch: 12 [11264/50048]	Loss: 1.5672
Training Epoch: 12 [11392/50048]	Loss: 1.6545
Training Epoch: 12 [11520/50048]	Loss: 1.7482
Training Epoch: 12 [11648/50048]	Loss: 1.5527
Training Epoch: 12 [11776/50048]	Loss: 1.5793
Training Epoch: 12 [11904/50048]	Loss: 1.7121
Training Epoch: 12 [12032/50048]	Loss: 1.6106
Training Epoch: 12 [12160/50048]	Loss: 1.6724
Training Epoch: 12 [12288/50048]	Loss: 1.6565
Training Epoch: 12 [12416/50048]	Loss: 1.5577
Training Epoch: 12 [12544/50048]	Loss: 1.6289
Training Epoch: 12 [12672/50048]	Loss: 1.5309
Training Epoch: 12 [12800/50048]	Loss: 1.5829
Training Epoch: 12 [12928/50048]	Loss: 1.7111
Training Epoch: 12 [13056/50048]	Loss: 1.4967
Training Epoch: 12 [13184/50048]	Loss: 1.7894
Training Epoch: 12 [13312/50048]	Loss: 1.5282
Training Epoch: 12 [13440/50048]	Loss: 1.3949
Training Epoch: 12 [13568/50048]	Loss: 1.6333
Training Epoch: 12 [13696/50048]	Loss: 1.7663
Training Epoch: 12 [13824/50048]	Loss: 1.5638
Training Epoch: 12 [13952/50048]	Loss: 1.5287
Training Epoch: 12 [14080/50048]	Loss: 1.5451
Training Epoch: 12 [14208/50048]	Loss: 1.4905
Training Epoch: 12 [14336/50048]	Loss: 1.3639
Training Epoch: 12 [14464/50048]	Loss: 1.5640
Training Epoch: 12 [14592/50048]	Loss: 1.8006
Training Epoch: 12 [14720/50048]	Loss: 1.7232
Training Epoch: 12 [14848/50048]	Loss: 1.3925
Training Epoch: 12 [14976/50048]	Loss: 1.3154
Training Epoch: 12 [15104/50048]	Loss: 1.5238
Training Epoch: 12 [15232/50048]	Loss: 1.7491
Training Epoch: 12 [15360/50048]	Loss: 1.3136
Training Epoch: 12 [15488/50048]	Loss: 1.4929
Training Epoch: 12 [15616/50048]	Loss: 1.5088
Training Epoch: 12 [15744/50048]	Loss: 1.4919
Training Epoch: 12 [15872/50048]	Loss: 1.8129
Training Epoch: 12 [16000/50048]	Loss: 1.5645
Training Epoch: 12 [16128/50048]	Loss: 1.5791
Training Epoch: 12 [16256/50048]	Loss: 1.6166
Training Epoch: 12 [16384/50048]	Loss: 1.5307
Training Epoch: 12 [16512/50048]	Loss: 1.5535
Training Epoch: 12 [16640/50048]	Loss: 1.5404
Training Epoch: 12 [16768/50048]	Loss: 1.4602
Training Epoch: 12 [16896/50048]	Loss: 1.6759
Training Epoch: 12 [17024/50048]	Loss: 1.5601
Training Epoch: 12 [17152/50048]	Loss: 1.4857
Training Epoch: 12 [17280/50048]	Loss: 1.7052
Training Epoch: 12 [17408/50048]	Loss: 1.5920
Training Epoch: 12 [17536/50048]	Loss: 1.5844
Training Epoch: 12 [17664/50048]	Loss: 1.5249
Training Epoch: 12 [17792/50048]	Loss: 1.6826
Training Epoch: 12 [17920/50048]	Loss: 1.2699
Training Epoch: 12 [18048/50048]	Loss: 1.6319
Training Epoch: 12 [18176/50048]	Loss: 1.5699
Training Epoch: 12 [18304/50048]	Loss: 1.5371
Training Epoch: 12 [18432/50048]	Loss: 1.4798
Training Epoch: 12 [18560/50048]	Loss: 1.5668
Training Epoch: 12 [18688/50048]	Loss: 1.6141
Training Epoch: 12 [18816/50048]	Loss: 1.6672
Training Epoch: 12 [18944/50048]	Loss: 1.4354
Training Epoch: 12 [19072/50048]	Loss: 1.7868
Training Epoch: 12 [19200/50048]	Loss: 1.4382
Training Epoch: 12 [19328/50048]	Loss: 1.5856
Training Epoch: 12 [19456/50048]	Loss: 1.6042
Training Epoch: 12 [19584/50048]	Loss: 1.4534
Training Epoch: 12 [19712/50048]	Loss: 1.3674
Training Epoch: 12 [19840/50048]	Loss: 1.7767
Training Epoch: 12 [19968/50048]	Loss: 1.7434
Training Epoch: 12 [20096/50048]	Loss: 1.5433
Training Epoch: 12 [20224/50048]	Loss: 1.5654
Training Epoch: 12 [20352/50048]	Loss: 1.6033
Training Epoch: 12 [20480/50048]	Loss: 1.6637
Training Epoch: 12 [20608/50048]	Loss: 1.6053
Training Epoch: 12 [20736/50048]	Loss: 1.7456
Training Epoch: 12 [20864/50048]	Loss: 1.4033
Training Epoch: 12 [20992/50048]	Loss: 1.5389
Training Epoch: 12 [21120/50048]	Loss: 1.5711
Training Epoch: 12 [21248/50048]	Loss: 1.5467
Training Epoch: 12 [21376/50048]	Loss: 1.6011
Training Epoch: 12 [21504/50048]	Loss: 1.6948
Training Epoch: 12 [21632/50048]	Loss: 1.5887
Training Epoch: 12 [21760/50048]	Loss: 1.5089
Training Epoch: 12 [21888/50048]	Loss: 1.4980
Training Epoch: 12 [22016/50048]	Loss: 1.3651
Training Epoch: 12 [22144/50048]	Loss: 1.6092
Training Epoch: 12 [22272/50048]	Loss: 1.6558
Training Epoch: 12 [22400/50048]	Loss: 1.6374
Training Epoch: 12 [22528/50048]	Loss: 1.7304
Training Epoch: 12 [22656/50048]	Loss: 1.3916
Training Epoch: 12 [22784/50048]	Loss: 1.5099
Training Epoch: 12 [22912/50048]	Loss: 1.4936
Training Epoch: 12 [23040/50048]	Loss: 1.4674
Training Epoch: 12 [23168/50048]	Loss: 1.4113
Training Epoch: 12 [23296/50048]	Loss: 1.5513
Training Epoch: 12 [23424/50048]	Loss: 1.4026
Training Epoch: 12 [23552/50048]	Loss: 1.5356
Training Epoch: 12 [23680/50048]	Loss: 1.4850
Training Epoch: 12 [23808/50048]	Loss: 1.1564
Training Epoch: 12 [23936/50048]	Loss: 1.4422
Training Epoch: 12 [24064/50048]	Loss: 1.4497
Training Epoch: 12 [24192/50048]	Loss: 1.5382
Training Epoch: 12 [24320/50048]	Loss: 1.5782
Training Epoch: 12 [24448/50048]	Loss: 1.4951
Training Epoch: 12 [24576/50048]	Loss: 1.2108
Training Epoch: 12 [24704/50048]	Loss: 1.4708
Training Epoch: 12 [24832/50048]	Loss: 1.6690
Training Epoch: 12 [24960/50048]	Loss: 1.7995
Training Epoch: 12 [25088/50048]	Loss: 1.8092
Training Epoch: 12 [25216/50048]	Loss: 1.2466
Training Epoch: 12 [25344/50048]	Loss: 1.5050
Training Epoch: 12 [25472/50048]	Loss: 1.5186
Training Epoch: 12 [25600/50048]	Loss: 1.6185
Training Epoch: 12 [25728/50048]	Loss: 1.6091
Training Epoch: 12 [25856/50048]	Loss: 1.6461
Training Epoch: 12 [25984/50048]	Loss: 1.7129
Training Epoch: 12 [26112/50048]	Loss: 1.5736
Training Epoch: 12 [26240/50048]	Loss: 1.5263
Training Epoch: 12 [26368/50048]	Loss: 1.6067
Training Epoch: 12 [26496/50048]	Loss: 1.5227
Training Epoch: 12 [26624/50048]	Loss: 1.4902
Training Epoch: 12 [26752/50048]	Loss: 1.4891
Training Epoch: 12 [26880/50048]	Loss: 2.1490
Training Epoch: 12 [27008/50048]	Loss: 1.4073
Training Epoch: 12 [27136/50048]	Loss: 1.6012
Training Epoch: 12 [27264/50048]	Loss: 1.4193
Training Epoch: 12 [27392/50048]	Loss: 1.4509
Training Epoch: 12 [27520/50048]	Loss: 1.7111
Training Epoch: 12 [27648/50048]	Loss: 1.5101
Training Epoch: 12 [27776/50048]	Loss: 1.6651
Training Epoch: 12 [27904/50048]	Loss: 1.6487
Training Epoch: 12 [28032/50048]	Loss: 1.3029
Training Epoch: 12 [28160/50048]	Loss: 1.6127
Training Epoch: 12 [28288/50048]	Loss: 1.5775
Training Epoch: 12 [28416/50048]	Loss: 1.3498
Training Epoch: 12 [28544/50048]	Loss: 1.5585
Training Epoch: 12 [28672/50048]	Loss: 1.5147
Training Epoch: 12 [28800/50048]	Loss: 1.4834
Training Epoch: 12 [28928/50048]	Loss: 1.9430
Training Epoch: 12 [29056/50048]	Loss: 1.4565
Training Epoch: 12 [29184/50048]	Loss: 1.5718
Training Epoch: 12 [29312/50048]	Loss: 1.3536
Training Epoch: 12 [29440/50048]	Loss: 1.5596
Training Epoch: 12 [29568/50048]	Loss: 1.7953
Training Epoch: 12 [29696/50048]	Loss: 1.5158
Training Epoch: 12 [29824/50048]	Loss: 1.5572
Training Epoch: 12 [29952/50048]	Loss: 1.5324
Training Epoch: 12 [30080/50048]	Loss: 1.6158
Training Epoch: 12 [30208/50048]	Loss: 1.6659
Training Epoch: 12 [30336/50048]	Loss: 1.7340
Training Epoch: 12 [30464/50048]	Loss: 1.6933
Training Epoch: 12 [30592/50048]	Loss: 1.7291
Training Epoch: 12 [30720/50048]	Loss: 1.5364
Training Epoch: 12 [30848/50048]	Loss: 1.4697
Training Epoch: 12 [30976/50048]	Loss: 2.0075
Training Epoch: 12 [31104/50048]	Loss: 1.6480
Training Epoch: 12 [31232/50048]	Loss: 1.6171
Training Epoch: 12 [31360/50048]	Loss: 1.3407
Training Epoch: 12 [31488/50048]	Loss: 1.4508
Training Epoch: 12 [31616/50048]	Loss: 1.5873
Training Epoch: 12 [31744/50048]	Loss: 1.3919
Training Epoch: 12 [31872/50048]	Loss: 1.6573
Training Epoch: 12 [32000/50048]	Loss: 1.5263
Training Epoch: 12 [32128/50048]	Loss: 1.6111
Training Epoch: 12 [32256/50048]	Loss: 1.4145
Training Epoch: 12 [32384/50048]	Loss: 1.6738
Training Epoch: 12 [32512/50048]	Loss: 1.7078
Training Epoch: 12 [32640/50048]	Loss: 1.6248
Training Epoch: 12 [32768/50048]	Loss: 1.5525
Training Epoch: 12 [32896/50048]	Loss: 1.5307
Training Epoch: 12 [33024/50048]	Loss: 1.7032
Training Epoch: 12 [33152/50048]	Loss: 1.6226
Training Epoch: 12 [33280/50048]	Loss: 1.6318
Training Epoch: 12 [33408/50048]	Loss: 1.5588
Training Epoch: 12 [33536/50048]	Loss: 1.5677
Training Epoch: 12 [33664/50048]	Loss: 1.5802
Training Epoch: 12 [33792/50048]	Loss: 1.6254
Training Epoch: 12 [33920/50048]	Loss: 1.3556
Training Epoch: 12 [34048/50048]	Loss: 1.5838
Training Epoch: 12 [34176/50048]	Loss: 1.4510
Training Epoch: 12 [34304/50048]	Loss: 1.5972
Training Epoch: 12 [34432/50048]	Loss: 1.8952
Training Epoch: 12 [34560/50048]	Loss: 1.4299
Training Epoch: 12 [34688/50048]	Loss: 1.5272
Training Epoch: 12 [34816/50048]	Loss: 1.4887
Training Epoch: 12 [34944/50048]	Loss: 1.7643
Training Epoch: 12 [35072/50048]	Loss: 1.5498
Training Epoch: 12 [35200/50048]	Loss: 1.6092
Training Epoch: 12 [35328/50048]	Loss: 1.4954
Training Epoch: 12 [35456/50048]	Loss: 1.5669
Training Epoch: 12 [35584/50048]	Loss: 1.5686
Training Epoch: 12 [35712/50048]	Loss: 1.8350
Training Epoch: 12 [35840/50048]	Loss: 1.6360
Training Epoch: 12 [35968/50048]	Loss: 1.4423
Training Epoch: 12 [36096/50048]	Loss: 1.4397
Training Epoch: 12 [36224/50048]	Loss: 1.5569
Training Epoch: 12 [36352/50048]	Loss: 1.4226
Training Epoch: 12 [36480/50048]	Loss: 1.4351
Training Epoch: 12 [36608/50048]	Loss: 1.2972
Training Epoch: 12 [36736/50048]	Loss: 1.6583
Training Epoch: 12 [36864/50048]	Loss: 1.3996
Training Epoch: 12 [36992/50048]	Loss: 1.2639
Training Epoch: 12 [37120/50048]	Loss: 1.5590
Training Epoch: 12 [37248/50048]	Loss: 1.6453
Training Epoch: 12 [37376/50048]	Loss: 1.5623
Training Epoch: 12 [37504/50048]	Loss: 1.3586
Training Epoch: 12 [37632/50048]	Loss: 1.7229
Training Epoch: 12 [37760/50048]	Loss: 1.5097
Training Epoch: 12 [37888/50048]	Loss: 1.5547
Training Epoch: 12 [38016/50048]	Loss: 1.5688
Training Epoch: 12 [38144/50048]	Loss: 1.5446
Training Epoch: 12 [38272/50048]	Loss: 1.9028
Training Epoch: 12 [38400/50048]	Loss: 1.7844
Training Epoch: 12 [38528/50048]	Loss: 1.7263
Training Epoch: 12 [38656/50048]	Loss: 1.6566
Training Epoch: 12 [38784/50048]	Loss: 1.7306
Training Epoch: 12 [38912/50048]	Loss: 1.6808
Training Epoch: 12 [39040/50048]	Loss: 1.5593
Training Epoch: 12 [39168/50048]	Loss: 1.4207
Training Epoch: 12 [39296/50048]	Loss: 1.5471
Training Epoch: 12 [39424/50048]	Loss: 1.6386
Training Epoch: 12 [39552/50048]	Loss: 1.5871
Training Epoch: 12 [39680/50048]	Loss: 1.6516
Training Epoch: 12 [39808/50048]	Loss: 1.5865
Training Epoch: 12 [39936/50048]	Loss: 1.5807
Training Epoch: 12 [40064/50048]	Loss: 1.5574
Training Epoch: 12 [40192/50048]	Loss: 1.3250
Training Epoch: 12 [40320/50048]	Loss: 1.5339
Training Epoch: 12 [40448/50048]	Loss: 1.3122
Training Epoch: 12 [40576/50048]	Loss: 1.3364
Training Epoch: 12 [40704/50048]	Loss: 1.5026
Training Epoch: 12 [40832/50048]	Loss: 1.6299
Training Epoch: 12 [40960/50048]	Loss: 1.7987
Training Epoch: 12 [41088/50048]	Loss: 1.4661
Training Epoch: 12 [41216/50048]	Loss: 1.3805
Training Epoch: 12 [41344/50048]	Loss: 1.5238
Training Epoch: 12 [41472/50048]	Loss: 1.5394
Training Epoch: 12 [41600/50048]	Loss: 1.5537
Training Epoch: 12 [41728/50048]	Loss: 1.4215
Training Epoch: 12 [41856/50048]	Loss: 1.5924
Training Epoch: 12 [41984/50048]	Loss: 1.7199
Training Epoch: 12 [42112/50048]	Loss: 1.7551
Training Epoch: 12 [42240/50048]	Loss: 1.4706
Training Epoch: 12 [42368/50048]	Loss: 1.3311
Training Epoch: 12 [42496/50048]	Loss: 1.3251
Training Epoch: 12 [42624/50048]	Loss: 1.2837
Training Epoch: 12 [42752/50048]	Loss: 1.4653
Training Epoch: 12 [42880/50048]	Loss: 1.5981
Training Epoch: 12 [43008/50048]	Loss: 1.6664
Training Epoch: 12 [43136/50048]	Loss: 1.3934
Training Epoch: 12 [43264/50048]	Loss: 1.3767
Training Epoch: 12 [43392/50048]	Loss: 1.8979
Training Epoch: 12 [43520/50048]	Loss: 1.3530
Training Epoch: 12 [43648/50048]	Loss: 1.5061
Training Epoch: 12 [43776/50048]	Loss: 1.6975
Training Epoch: 12 [43904/50048]	Loss: 1.5178
Training Epoch: 12 [44032/50048]	Loss: 1.5637
Training Epoch: 12 [44160/50048]	Loss: 1.5257
Training Epoch: 12 [44288/50048]	Loss: 1.4783
Training Epoch: 12 [44416/50048]	Loss: 1.4923
Training Epoch: 12 [44544/50048]	Loss: 1.6673
Training Epoch: 12 [44672/50048]	Loss: 1.5367
Training Epoch: 12 [44800/50048]	Loss: 1.6261
Training Epoch: 12 [44928/50048]	Loss: 1.6885
Training Epoch: 12 [45056/50048]	Loss: 1.5402
Training Epoch: 12 [45184/50048]	Loss: 1.5080
Training Epoch: 12 [45312/50048]	Loss: 1.5922
Training Epoch: 12 [45440/50048]	Loss: 1.6372
Training Epoch: 12 [45568/50048]	Loss: 1.5326
Training Epoch: 12 [45696/50048]	Loss: 1.7212
Training Epoch: 12 [45824/50048]	Loss: 1.7786
Training Epoch: 12 [45952/50048]	Loss: 1.6388
Training Epoch: 12 [46080/50048]	Loss: 1.3382
Training Epoch: 12 [46208/50048]	Loss: 1.5622
Training Epoch: 12 [46336/50048]	Loss: 1.8615
Training Epoch: 12 [46464/50048]	Loss: 1.6415
Training Epoch: 12 [46592/50048]	Loss: 1.3294
Training Epoch: 12 [46720/50048]	Loss: 1.4400
Training Epoch: 12 [46848/50048]	Loss: 1.4631
Training Epoch: 12 [46976/50048]	Loss: 1.5879
Training Epoch: 12 [47104/50048]	Loss: 1.6518
Training Epoch: 12 [47232/50048]	Loss: 1.6427
Training Epoch: 12 [47360/50048]	Loss: 1.6482
Training Epoch: 12 [47488/50048]	Loss: 1.5005
Training Epoch: 12 [47616/50048]	Loss: 1.7075
Training Epoch: 12 [47744/50048]	Loss: 1.5164
Training Epoch: 12 [47872/50048]	Loss: 1.5132
Training Epoch: 12 [48000/50048]	Loss: 1.6624
Training Epoch: 12 [48128/50048]	Loss: 1.6880
Training Epoch: 12 [48256/50048]	Loss: 1.4626
Training Epoch: 12 [48384/50048]	Loss: 1.4069
Training Epoch: 12 [48512/50048]	Loss: 1.8013
Training Epoch: 12 [48640/50048]	Loss: 1.5601
Training Epoch: 12 [48768/50048]	Loss: 1.5169
Training Epoch: 12 [48896/50048]	Loss: 1.3436
Training Epoch: 12 [49024/50048]	Loss: 1.7417
Training Epoch: 12 [49152/50048]	Loss: 1.5254
Training Epoch: 12 [49280/50048]	Loss: 1.4599
Training Epoch: 12 [49408/50048]	Loss: 1.8321
Training Epoch: 12 [49536/50048]	Loss: 1.5803
Training Epoch: 12 [49664/50048]	Loss: 1.3319
Training Epoch: 12 [49792/50048]	Loss: 1.9041
Training Epoch: 12 [49920/50048]	Loss: 1.6051
Training Epoch: 12 [50048/50048]	Loss: 1.4917
Validation Epoch: 12, Average loss: 0.0131, Accuracy: 0.5416
Training Epoch: 13 [128/50048]	Loss: 1.7755
Training Epoch: 13 [256/50048]	Loss: 1.6646
Training Epoch: 13 [384/50048]	Loss: 1.6709
Training Epoch: 13 [512/50048]	Loss: 1.4369
Training Epoch: 13 [640/50048]	Loss: 1.6719
Training Epoch: 13 [768/50048]	Loss: 1.3180
Training Epoch: 13 [896/50048]	Loss: 1.5579
Training Epoch: 13 [1024/50048]	Loss: 1.5433
Training Epoch: 13 [1152/50048]	Loss: 1.6474
Training Epoch: 13 [1280/50048]	Loss: 1.4828
Training Epoch: 13 [1408/50048]	Loss: 1.5091
Training Epoch: 13 [1536/50048]	Loss: 1.5508
Training Epoch: 13 [1664/50048]	Loss: 1.6115
Training Epoch: 13 [1792/50048]	Loss: 1.2174
Training Epoch: 13 [1920/50048]	Loss: 1.4386
Training Epoch: 13 [2048/50048]	Loss: 1.5456
Training Epoch: 13 [2176/50048]	Loss: 1.5587
Training Epoch: 13 [2304/50048]	Loss: 1.5823
Training Epoch: 13 [2432/50048]	Loss: 1.6323
Training Epoch: 13 [2560/50048]	Loss: 1.4367
Training Epoch: 13 [2688/50048]	Loss: 1.1401
Training Epoch: 13 [2816/50048]	Loss: 1.4747
Training Epoch: 13 [2944/50048]	Loss: 1.4263
Training Epoch: 13 [3072/50048]	Loss: 1.4787
Training Epoch: 13 [3200/50048]	Loss: 1.5479
Training Epoch: 13 [3328/50048]	Loss: 1.6461
Training Epoch: 13 [3456/50048]	Loss: 1.5211
Training Epoch: 13 [3584/50048]	Loss: 1.3764
Training Epoch: 13 [3712/50048]	Loss: 1.5099
Training Epoch: 13 [3840/50048]	Loss: 1.6106
Training Epoch: 13 [3968/50048]	Loss: 1.7035
Training Epoch: 13 [4096/50048]	Loss: 1.7358
Training Epoch: 13 [4224/50048]	Loss: 1.8201
Training Epoch: 13 [4352/50048]	Loss: 1.5166
Training Epoch: 13 [4480/50048]	Loss: 1.5298
Training Epoch: 13 [4608/50048]	Loss: 1.3357
Training Epoch: 13 [4736/50048]	Loss: 1.7853
Training Epoch: 13 [4864/50048]	Loss: 1.6830
Training Epoch: 13 [4992/50048]	Loss: 1.6305
Training Epoch: 13 [5120/50048]	Loss: 1.3967
Training Epoch: 13 [5248/50048]	Loss: 1.5865
Training Epoch: 13 [5376/50048]	Loss: 1.6331
Training Epoch: 13 [5504/50048]	Loss: 1.4790
Training Epoch: 13 [5632/50048]	Loss: 1.6016
Training Epoch: 13 [5760/50048]	Loss: 1.4050
Training Epoch: 13 [5888/50048]	Loss: 1.7597
Training Epoch: 13 [6016/50048]	Loss: 1.3831
Training Epoch: 13 [6144/50048]	Loss: 1.4533
Training Epoch: 13 [6272/50048]	Loss: 1.5625
Training Epoch: 13 [6400/50048]	Loss: 1.3511
Training Epoch: 13 [6528/50048]	Loss: 1.6638
Training Epoch: 13 [6656/50048]	Loss: 1.4499
Training Epoch: 13 [6784/50048]	Loss: 1.3561
Training Epoch: 13 [6912/50048]	Loss: 1.3941
Training Epoch: 13 [7040/50048]	Loss: 1.5422
Training Epoch: 13 [7168/50048]	Loss: 1.8779
Training Epoch: 13 [7296/50048]	Loss: 1.5191
Training Epoch: 13 [7424/50048]	Loss: 1.6055
Training Epoch: 13 [7552/50048]	Loss: 1.6411
Training Epoch: 13 [7680/50048]	Loss: 1.4608
Training Epoch: 13 [7808/50048]	Loss: 1.3792
Training Epoch: 13 [7936/50048]	Loss: 1.3909
Training Epoch: 13 [8064/50048]	Loss: 1.5743
Training Epoch: 13 [8192/50048]	Loss: 1.5909
Training Epoch: 13 [8320/50048]	Loss: 1.3889
Training Epoch: 13 [8448/50048]	Loss: 1.5106
Training Epoch: 13 [8576/50048]	Loss: 1.3399
Training Epoch: 13 [8704/50048]	Loss: 1.2333
Training Epoch: 13 [8832/50048]	Loss: 1.4350
Training Epoch: 13 [8960/50048]	Loss: 1.4464
Training Epoch: 13 [9088/50048]	Loss: 1.7150
Training Epoch: 13 [9216/50048]	Loss: 1.5291
Training Epoch: 13 [9344/50048]	Loss: 1.3387
Training Epoch: 13 [9472/50048]	Loss: 1.6609
Training Epoch: 13 [9600/50048]	Loss: 1.6127
Training Epoch: 13 [9728/50048]	Loss: 1.4383
Training Epoch: 13 [9856/50048]	Loss: 1.5877
Training Epoch: 13 [9984/50048]	Loss: 1.7721
Training Epoch: 13 [10112/50048]	Loss: 1.5574
Training Epoch: 13 [10240/50048]	Loss: 1.4106
Training Epoch: 13 [10368/50048]	Loss: 1.3907
Training Epoch: 13 [10496/50048]	Loss: 1.3325
Training Epoch: 13 [10624/50048]	Loss: 1.5034
Training Epoch: 13 [10752/50048]	Loss: 1.6902
Training Epoch: 13 [10880/50048]	Loss: 1.5894
Training Epoch: 13 [11008/50048]	Loss: 1.6274
Training Epoch: 13 [11136/50048]	Loss: 1.5642
Training Epoch: 13 [11264/50048]	Loss: 1.3683
Training Epoch: 13 [11392/50048]	Loss: 1.4719
Training Epoch: 13 [11520/50048]	Loss: 1.6458
Training Epoch: 13 [11648/50048]	Loss: 1.5622
Training Epoch: 13 [11776/50048]	Loss: 1.5376
Training Epoch: 13 [11904/50048]	Loss: 1.5319
Training Epoch: 13 [12032/50048]	Loss: 1.6637
Training Epoch: 13 [12160/50048]	Loss: 1.3715
Training Epoch: 13 [12288/50048]	Loss: 1.6089
Training Epoch: 13 [12416/50048]	Loss: 1.5062
Training Epoch: 13 [12544/50048]	Loss: 1.6618
Training Epoch: 13 [12672/50048]	Loss: 1.3807
Training Epoch: 13 [12800/50048]	Loss: 1.5998
Training Epoch: 13 [12928/50048]	Loss: 1.3166
Training Epoch: 13 [13056/50048]	Loss: 1.3307
Training Epoch: 13 [13184/50048]	Loss: 1.4527
Training Epoch: 13 [13312/50048]	Loss: 1.6766
Training Epoch: 13 [13440/50048]	Loss: 1.8448
Training Epoch: 13 [13568/50048]	Loss: 1.6961
Training Epoch: 13 [13696/50048]	Loss: 1.5226
Training Epoch: 13 [13824/50048]	Loss: 1.6479
Training Epoch: 13 [13952/50048]	Loss: 1.5779
Training Epoch: 13 [14080/50048]	Loss: 1.2062
Training Epoch: 13 [14208/50048]	Loss: 1.5916
Training Epoch: 13 [14336/50048]	Loss: 1.4551
Training Epoch: 13 [14464/50048]	Loss: 1.4679
Training Epoch: 13 [14592/50048]	Loss: 1.3364
Training Epoch: 13 [14720/50048]	Loss: 1.3082
Training Epoch: 13 [14848/50048]	Loss: 1.6344
Training Epoch: 13 [14976/50048]	Loss: 1.6012
Training Epoch: 13 [15104/50048]	Loss: 1.5076
Training Epoch: 13 [15232/50048]	Loss: 1.6271
Training Epoch: 13 [15360/50048]	Loss: 1.7187
Training Epoch: 13 [15488/50048]	Loss: 1.5921
Training Epoch: 13 [15616/50048]	Loss: 1.4831
Training Epoch: 13 [15744/50048]	Loss: 1.9458
Training Epoch: 13 [15872/50048]	Loss: 1.0797
Training Epoch: 13 [16000/50048]	Loss: 1.3428
Training Epoch: 13 [16128/50048]	Loss: 1.6355
Training Epoch: 13 [16256/50048]	Loss: 1.6005
Training Epoch: 13 [16384/50048]	Loss: 1.4875
Training Epoch: 13 [16512/50048]	Loss: 1.6546
Training Epoch: 13 [16640/50048]	Loss: 1.3702
Training Epoch: 13 [16768/50048]	Loss: 1.5249
Training Epoch: 13 [16896/50048]	Loss: 1.2711
Training Epoch: 13 [17024/50048]	Loss: 1.5108
Training Epoch: 13 [17152/50048]	Loss: 1.5157
Training Epoch: 13 [17280/50048]	Loss: 1.4719
Training Epoch: 13 [17408/50048]	Loss: 1.3353
Training Epoch: 13 [17536/50048]	Loss: 1.4115
Training Epoch: 13 [17664/50048]	Loss: 1.6893
Training Epoch: 13 [17792/50048]	Loss: 1.5372
Training Epoch: 13 [17920/50048]	Loss: 1.5609
Training Epoch: 13 [18048/50048]	Loss: 1.3861
Training Epoch: 13 [18176/50048]	Loss: 1.4053
Training Epoch: 13 [18304/50048]	Loss: 1.5328
Training Epoch: 13 [18432/50048]	Loss: 1.4294
Training Epoch: 13 [18560/50048]	Loss: 1.4068
Training Epoch: 13 [18688/50048]	Loss: 1.3517
Training Epoch: 13 [18816/50048]	Loss: 1.7920
Training Epoch: 13 [18944/50048]	Loss: 1.6191
Training Epoch: 13 [19072/50048]	Loss: 1.3286
Training Epoch: 13 [19200/50048]	Loss: 1.4513
Training Epoch: 13 [19328/50048]	Loss: 1.8372
Training Epoch: 13 [19456/50048]	Loss: 1.4906
Training Epoch: 13 [19584/50048]	Loss: 1.5917
Training Epoch: 13 [19712/50048]	Loss: 1.6598
Training Epoch: 13 [19840/50048]	Loss: 1.7388
Training Epoch: 13 [19968/50048]	Loss: 1.5422
Training Epoch: 13 [20096/50048]	Loss: 1.6619
Training Epoch: 13 [20224/50048]	Loss: 1.5489
Training Epoch: 13 [20352/50048]	Loss: 1.3782
Training Epoch: 13 [20480/50048]	Loss: 1.6031
Training Epoch: 13 [20608/50048]	Loss: 1.3938
Training Epoch: 13 [20736/50048]	Loss: 1.4713
Training Epoch: 13 [20864/50048]	Loss: 1.3491
Training Epoch: 13 [20992/50048]	Loss: 1.5466
Training Epoch: 13 [21120/50048]	Loss: 1.5212
Training Epoch: 13 [21248/50048]	Loss: 1.5139
Training Epoch: 13 [21376/50048]	Loss: 1.6191
Training Epoch: 13 [21504/50048]	Loss: 1.4941
Training Epoch: 13 [21632/50048]	Loss: 1.4454
Training Epoch: 13 [21760/50048]	Loss: 1.6650
Training Epoch: 13 [21888/50048]	Loss: 1.7252
Training Epoch: 13 [22016/50048]	Loss: 1.2403
Training Epoch: 13 [22144/50048]	Loss: 1.5034
Training Epoch: 13 [22272/50048]	Loss: 1.6883
Training Epoch: 13 [22400/50048]	Loss: 1.4354
Training Epoch: 13 [22528/50048]	Loss: 1.5694
Training Epoch: 13 [22656/50048]	Loss: 1.5021
Training Epoch: 13 [22784/50048]	Loss: 1.5409
Training Epoch: 13 [22912/50048]	Loss: 1.5291
Training Epoch: 13 [23040/50048]	Loss: 1.6102
Training Epoch: 13 [23168/50048]	Loss: 1.5237
Training Epoch: 13 [23296/50048]	Loss: 1.4637
Training Epoch: 13 [23424/50048]	Loss: 1.4997
Training Epoch: 13 [23552/50048]	Loss: 1.5522
Training Epoch: 13 [23680/50048]	Loss: 1.4134
Training Epoch: 13 [23808/50048]	Loss: 1.6065
Training Epoch: 13 [23936/50048]	Loss: 1.6167
Training Epoch: 13 [24064/50048]	Loss: 1.5976
Training Epoch: 13 [24192/50048]	Loss: 1.5544
Training Epoch: 13 [24320/50048]	Loss: 1.4076
Training Epoch: 13 [24448/50048]	Loss: 1.4343
Training Epoch: 13 [24576/50048]	Loss: 1.5018
Training Epoch: 13 [24704/50048]	Loss: 1.5325
Training Epoch: 13 [24832/50048]	Loss: 1.5397
Training Epoch: 13 [24960/50048]	Loss: 1.5705
Training Epoch: 13 [25088/50048]	Loss: 1.5476
Training Epoch: 13 [25216/50048]	Loss: 1.6518
Training Epoch: 13 [25344/50048]	Loss: 1.4312
Training Epoch: 13 [25472/50048]	Loss: 1.4911
Training Epoch: 13 [25600/50048]	Loss: 1.4651
Training Epoch: 13 [25728/50048]	Loss: 1.3473
Training Epoch: 13 [25856/50048]	Loss: 1.5127
Training Epoch: 13 [25984/50048]	Loss: 1.5697
Training Epoch: 13 [26112/50048]	Loss: 1.8098
Training Epoch: 13 [26240/50048]	Loss: 1.3547
Training Epoch: 13 [26368/50048]	Loss: 1.5318
Training Epoch: 13 [26496/50048]	Loss: 1.7611
Training Epoch: 13 [26624/50048]	Loss: 1.4309
Training Epoch: 13 [26752/50048]	Loss: 1.3935
Training Epoch: 13 [26880/50048]	Loss: 1.6057
Training Epoch: 13 [27008/50048]	Loss: 1.3759
Training Epoch: 13 [27136/50048]	Loss: 1.5882
Training Epoch: 13 [27264/50048]	Loss: 1.3500
Training Epoch: 13 [27392/50048]	Loss: 1.4997
Training Epoch: 13 [27520/50048]	Loss: 1.3583
Training Epoch: 13 [27648/50048]	Loss: 1.3040
Training Epoch: 13 [27776/50048]	Loss: 1.7646
Training Epoch: 13 [27904/50048]	Loss: 1.6564
Training Epoch: 13 [28032/50048]	Loss: 1.5403
Training Epoch: 13 [28160/50048]	Loss: 1.7708
Training Epoch: 13 [28288/50048]	Loss: 1.3996
Training Epoch: 13 [28416/50048]	Loss: 1.5964
Training Epoch: 13 [28544/50048]	Loss: 1.5778
Training Epoch: 13 [28672/50048]	Loss: 1.6871
Training Epoch: 13 [28800/50048]	Loss: 1.7248
Training Epoch: 13 [28928/50048]	Loss: 1.6551
Training Epoch: 13 [29056/50048]	Loss: 1.3908
Training Epoch: 13 [29184/50048]	Loss: 1.4410
Training Epoch: 13 [29312/50048]	Loss: 1.5838
Training Epoch: 13 [29440/50048]	Loss: 1.4782
Training Epoch: 13 [29568/50048]	Loss: 1.5990
Training Epoch: 13 [29696/50048]	Loss: 1.7477
Training Epoch: 13 [29824/50048]	Loss: 1.5188
Training Epoch: 13 [29952/50048]	Loss: 1.3401
Training Epoch: 13 [30080/50048]	Loss: 1.3208
Training Epoch: 13 [30208/50048]	Loss: 1.7537
Training Epoch: 13 [30336/50048]	Loss: 1.5511
Training Epoch: 13 [30464/50048]	Loss: 1.4101
Training Epoch: 13 [30592/50048]	Loss: 1.6315
Training Epoch: 13 [30720/50048]	Loss: 1.6671
Training Epoch: 13 [30848/50048]	Loss: 1.4180
Training Epoch: 13 [30976/50048]	Loss: 1.6830
Training Epoch: 13 [31104/50048]	Loss: 1.6906
Training Epoch: 13 [31232/50048]	Loss: 1.3955
Training Epoch: 13 [31360/50048]	Loss: 1.5359
Training Epoch: 13 [31488/50048]	Loss: 1.5166
Training Epoch: 13 [31616/50048]	Loss: 1.5837
Training Epoch: 13 [31744/50048]	Loss: 1.5337
Training Epoch: 13 [31872/50048]	Loss: 1.4172
Training Epoch: 13 [32000/50048]	Loss: 1.7048
Training Epoch: 13 [32128/50048]	Loss: 1.5227
Training Epoch: 13 [32256/50048]	Loss: 1.5465
Training Epoch: 13 [32384/50048]	Loss: 1.4579
Training Epoch: 13 [32512/50048]	Loss: 1.3812
Training Epoch: 13 [32640/50048]	Loss: 1.5304
Training Epoch: 13 [32768/50048]	Loss: 1.3525
Training Epoch: 13 [32896/50048]	Loss: 1.2991
Training Epoch: 13 [33024/50048]	Loss: 1.4587
Training Epoch: 13 [33152/50048]	Loss: 1.7886
Training Epoch: 13 [33280/50048]	Loss: 1.5070
Training Epoch: 13 [33408/50048]	Loss: 1.3628
Training Epoch: 13 [33536/50048]	Loss: 1.8690
Training Epoch: 13 [33664/50048]	Loss: 1.5845
Training Epoch: 13 [33792/50048]	Loss: 1.5705
Training Epoch: 13 [33920/50048]	Loss: 1.5012
Training Epoch: 13 [34048/50048]	Loss: 1.6433
Training Epoch: 13 [34176/50048]	Loss: 1.7028
Training Epoch: 13 [34304/50048]	Loss: 1.6726
Training Epoch: 13 [34432/50048]	Loss: 1.5504
Training Epoch: 13 [34560/50048]	Loss: 1.5259
Training Epoch: 13 [34688/50048]	Loss: 1.1773
Training Epoch: 13 [34816/50048]	Loss: 1.5342
Training Epoch: 13 [34944/50048]	Loss: 1.4077
Training Epoch: 13 [35072/50048]	Loss: 1.4867
Training Epoch: 13 [35200/50048]	Loss: 1.5751
Training Epoch: 13 [35328/50048]	Loss: 1.5838
Training Epoch: 13 [35456/50048]	Loss: 1.5006
Training Epoch: 13 [35584/50048]	Loss: 1.5586
Training Epoch: 13 [35712/50048]	Loss: 1.2864
Training Epoch: 13 [35840/50048]	Loss: 1.5408
Training Epoch: 13 [35968/50048]	Loss: 1.3497
Training Epoch: 13 [36096/50048]	Loss: 1.3157
Training Epoch: 13 [36224/50048]	Loss: 1.4845
Training Epoch: 13 [36352/50048]	Loss: 1.3985
Training Epoch: 13 [36480/50048]	Loss: 1.6152
Training Epoch: 13 [36608/50048]	Loss: 1.6658
Training Epoch: 13 [36736/50048]	Loss: 1.8910
Training Epoch: 13 [36864/50048]	Loss: 1.6363
Training Epoch: 13 [36992/50048]	Loss: 1.5511
Training Epoch: 13 [37120/50048]	Loss: 1.4577
Training Epoch: 13 [37248/50048]	Loss: 1.7074
Training Epoch: 13 [37376/50048]	Loss: 1.5376
Training Epoch: 13 [37504/50048]	Loss: 1.4663
Training Epoch: 13 [37632/50048]	Loss: 1.6319
Training Epoch: 13 [37760/50048]	Loss: 1.5757
Training Epoch: 13 [37888/50048]	Loss: 1.5452
Training Epoch: 13 [38016/50048]	Loss: 1.7752
Training Epoch: 13 [38144/50048]	Loss: 1.3475
Training Epoch: 13 [38272/50048]	Loss: 1.6815
Training Epoch: 13 [38400/50048]	Loss: 1.7220
Training Epoch: 13 [38528/50048]	Loss: 1.7151
Training Epoch: 13 [38656/50048]	Loss: 1.3316
Training Epoch: 13 [38784/50048]	Loss: 1.5616
Training Epoch: 13 [38912/50048]	Loss: 1.2743
Training Epoch: 13 [39040/50048]	Loss: 1.7117
Training Epoch: 13 [39168/50048]	Loss: 1.4972
Training Epoch: 13 [39296/50048]	Loss: 1.5247
Training Epoch: 13 [39424/50048]	Loss: 1.4223
Training Epoch: 13 [39552/50048]	Loss: 1.6936
Training Epoch: 13 [39680/50048]	Loss: 1.6644
Training Epoch: 13 [39808/50048]	Loss: 1.4551
Training Epoch: 13 [39936/50048]	Loss: 1.3070
Training Epoch: 13 [40064/50048]	Loss: 1.3783
Training Epoch: 13 [40192/50048]	Loss: 1.5847
Training Epoch: 13 [40320/50048]	Loss: 1.4908
Training Epoch: 13 [40448/50048]	Loss: 1.5328
Training Epoch: 13 [40576/50048]	Loss: 1.4708
Training Epoch: 13 [40704/50048]	Loss: 1.6256
Training Epoch: 13 [40832/50048]	Loss: 1.5146
Training Epoch: 13 [40960/50048]	Loss: 1.5460
Training Epoch: 13 [41088/50048]	Loss: 1.6911
Training Epoch: 13 [41216/50048]	Loss: 1.4238
Training Epoch: 13 [41344/50048]	Loss: 1.5422
Training Epoch: 13 [41472/50048]	Loss: 1.5226
Training Epoch: 13 [41600/50048]	Loss: 1.6699
Training Epoch: 13 [41728/50048]	Loss: 1.7540
Training Epoch: 13 [41856/50048]	Loss: 1.5017
Training Epoch: 13 [41984/50048]	Loss: 1.4623
Training Epoch: 13 [42112/50048]	Loss: 1.4964
Training Epoch: 13 [42240/50048]	Loss: 1.4226
Training Epoch: 13 [42368/50048]	Loss: 1.4965
Training Epoch: 13 [42496/50048]	Loss: 1.5985
Training Epoch: 13 [42624/50048]	Loss: 1.3555
Training Epoch: 13 [42752/50048]	Loss: 1.6269
Training Epoch: 13 [42880/50048]	Loss: 1.3591
Training Epoch: 13 [43008/50048]	Loss: 1.5529
Training Epoch: 13 [43136/50048]	Loss: 1.4153
Training Epoch: 13 [43264/50048]	Loss: 1.3821
Training Epoch: 13 [43392/50048]	Loss: 1.4676
Training Epoch: 13 [43520/50048]	Loss: 1.6181
Training Epoch: 13 [43648/50048]	Loss: 1.4150
Training Epoch: 13 [43776/50048]	Loss: 1.6619
Training Epoch: 13 [43904/50048]	Loss: 1.6302
Training Epoch: 13 [44032/50048]	Loss: 1.4966
Training Epoch: 13 [44160/50048]	Loss: 1.6943
Training Epoch: 13 [44288/50048]	Loss: 1.5765
Training Epoch: 13 [44416/50048]	Loss: 1.3040
Training Epoch: 13 [44544/50048]	Loss: 1.5145
Training Epoch: 13 [44672/50048]	Loss: 1.5713
Training Epoch: 13 [44800/50048]	Loss: 1.4592
Training Epoch: 13 [44928/50048]	Loss: 1.6309
Training Epoch: 13 [45056/50048]	Loss: 1.5171
Training Epoch: 13 [45184/50048]	Loss: 1.5891
Training Epoch: 13 [45312/50048]	Loss: 1.6351
Training Epoch: 13 [45440/50048]	Loss: 1.4193
Training Epoch: 13 [45568/50048]	Loss: 1.6538
Training Epoch: 13 [45696/50048]	Loss: 1.4297
Training Epoch: 13 [45824/50048]	Loss: 1.6502
Training Epoch: 13 [45952/50048]	Loss: 1.5245
Training Epoch: 13 [46080/50048]	Loss: 1.5172
Training Epoch: 13 [46208/50048]	Loss: 1.7524
Training Epoch: 13 [46336/50048]	Loss: 1.5284
Training Epoch: 13 [46464/50048]	Loss: 1.3596
Training Epoch: 13 [46592/50048]	Loss: 1.6642
Training Epoch: 13 [46720/50048]	Loss: 1.8974
Training Epoch: 13 [46848/50048]	Loss: 1.4149
Training Epoch: 13 [46976/50048]	Loss: 1.5388
Training Epoch: 13 [47104/50048]	Loss: 1.2757
Training Epoch: 13 [47232/50048]	Loss: 1.5930
Training Epoch: 13 [47360/50048]	Loss: 1.5063
Training Epoch: 13 [47488/50048]	Loss: 1.5526
Training Epoch: 13 [47616/50048]	Loss: 1.4121
Training Epoch: 13 [47744/50048]	Loss: 1.2190
Training Epoch: 13 [47872/50048]	Loss: 1.2022
Training Epoch: 13 [48000/50048]	Loss: 1.4357
Training Epoch: 13 [48128/50048]	Loss: 1.4387
Training Epoch: 13 [48256/50048]	Loss: 1.4755
Training Epoch: 13 [48384/50048]	Loss: 1.5092
Training Epoch: 13 [48512/50048]	Loss: 1.4873
Training Epoch: 13 [48640/50048]	Loss: 1.4938
Training Epoch: 13 [48768/50048]	Loss: 1.5493
Training Epoch: 13 [48896/50048]	Loss: 1.2441
Training Epoch: 13 [49024/50048]	Loss: 1.6459
Training Epoch: 13 [49152/50048]	Loss: 1.2539
Training Epoch: 13 [49280/50048]	Loss: 1.8055
Training Epoch: 13 [49408/50048]	Loss: 1.5871
Training Epoch: 13 [49536/50048]	Loss: 1.6059
Training Epoch: 13 [49664/50048]	Loss: 1.5941
Training Epoch: 13 [49792/50048]	Loss: 1.5758
Training Epoch: 13 [49920/50048]	Loss: 1.6823
Training Epoch: 13 [50048/50048]	Loss: 2.0524
Validation Epoch: 13, Average loss: 0.0129, Accuracy: 0.5448
Training Epoch: 14 [128/50048]	Loss: 1.2507
Training Epoch: 14 [256/50048]	Loss: 1.5000
Training Epoch: 14 [384/50048]	Loss: 1.3159
Training Epoch: 14 [512/50048]	Loss: 1.6566
Training Epoch: 14 [640/50048]	Loss: 1.6796
Training Epoch: 14 [768/50048]	Loss: 1.5667
Training Epoch: 14 [896/50048]	Loss: 1.5694
Training Epoch: 14 [1024/50048]	Loss: 1.5320
Training Epoch: 14 [1152/50048]	Loss: 1.4875
Training Epoch: 14 [1280/50048]	Loss: 1.5472
Training Epoch: 14 [1408/50048]	Loss: 1.3462
Training Epoch: 14 [1536/50048]	Loss: 1.3406
Training Epoch: 14 [1664/50048]	Loss: 1.5652
Training Epoch: 14 [1792/50048]	Loss: 1.3497
Training Epoch: 14 [1920/50048]	Loss: 1.6134
Training Epoch: 14 [2048/50048]	Loss: 1.6635
Training Epoch: 14 [2176/50048]	Loss: 1.3626
Training Epoch: 14 [2304/50048]	Loss: 1.6176
Training Epoch: 14 [2432/50048]	Loss: 1.4061
Training Epoch: 14 [2560/50048]	Loss: 1.3762
Training Epoch: 14 [2688/50048]	Loss: 1.4507
Training Epoch: 14 [2816/50048]	Loss: 1.4400
Training Epoch: 14 [2944/50048]	Loss: 1.4312
Training Epoch: 14 [3072/50048]	Loss: 1.4782
Training Epoch: 14 [3200/50048]	Loss: 1.6321
Training Epoch: 14 [3328/50048]	Loss: 1.3721
Training Epoch: 14 [3456/50048]	Loss: 1.5284
Training Epoch: 14 [3584/50048]	Loss: 1.3538
Training Epoch: 14 [3712/50048]	Loss: 1.5757
Training Epoch: 14 [3840/50048]	Loss: 1.4598
Training Epoch: 14 [3968/50048]	Loss: 1.1542
Training Epoch: 14 [4096/50048]	Loss: 1.4801
Training Epoch: 14 [4224/50048]	Loss: 1.5361
Training Epoch: 14 [4352/50048]	Loss: 1.4817
Training Epoch: 14 [4480/50048]	Loss: 1.4678
Training Epoch: 14 [4608/50048]	Loss: 1.4268
Training Epoch: 14 [4736/50048]	Loss: 1.2422
Training Epoch: 14 [4864/50048]	Loss: 1.5450
Training Epoch: 14 [4992/50048]	Loss: 1.5938
Training Epoch: 14 [5120/50048]	Loss: 1.5208
Training Epoch: 14 [5248/50048]	Loss: 1.3663
Training Epoch: 14 [5376/50048]	Loss: 1.5190
Training Epoch: 14 [5504/50048]	Loss: 1.2656
Training Epoch: 14 [5632/50048]	Loss: 1.2853
Training Epoch: 14 [5760/50048]	Loss: 1.5112
Training Epoch: 14 [5888/50048]	Loss: 1.2478
Training Epoch: 14 [6016/50048]	Loss: 1.3221
Training Epoch: 14 [6144/50048]	Loss: 1.5913
Training Epoch: 14 [6272/50048]	Loss: 1.3687
Training Epoch: 14 [6400/50048]	Loss: 1.4265
Training Epoch: 14 [6528/50048]	Loss: 1.4825
Training Epoch: 14 [6656/50048]	Loss: 1.4567
Training Epoch: 14 [6784/50048]	Loss: 1.6013
Training Epoch: 14 [6912/50048]	Loss: 1.2626
Training Epoch: 14 [7040/50048]	Loss: 1.5416
Training Epoch: 14 [7168/50048]	Loss: 1.2672
Training Epoch: 14 [7296/50048]	Loss: 1.6614
Training Epoch: 14 [7424/50048]	Loss: 1.6358
Training Epoch: 14 [7552/50048]	Loss: 1.6331
Training Epoch: 14 [7680/50048]	Loss: 1.5403
Training Epoch: 14 [7808/50048]	Loss: 1.3886
Training Epoch: 14 [7936/50048]	Loss: 1.4216
Training Epoch: 14 [8064/50048]	Loss: 1.4410
Training Epoch: 14 [8192/50048]	Loss: 1.7347
Training Epoch: 14 [8320/50048]	Loss: 1.5409
Training Epoch: 14 [8448/50048]	Loss: 1.4428
Training Epoch: 14 [8576/50048]	Loss: 1.3432
Training Epoch: 14 [8704/50048]	Loss: 1.6963
Training Epoch: 14 [8832/50048]	Loss: 1.5417
Training Epoch: 14 [8960/50048]	Loss: 1.4233
Training Epoch: 14 [9088/50048]	Loss: 1.4997
Training Epoch: 14 [9216/50048]	Loss: 1.5068
Training Epoch: 14 [9344/50048]	Loss: 1.4765
Training Epoch: 14 [9472/50048]	Loss: 1.3456
Training Epoch: 14 [9600/50048]	Loss: 1.2465
Training Epoch: 14 [9728/50048]	Loss: 1.3957
Training Epoch: 14 [9856/50048]	Loss: 1.4808
Training Epoch: 14 [9984/50048]	Loss: 1.5601
Training Epoch: 14 [10112/50048]	Loss: 1.5393
Training Epoch: 14 [10240/50048]	Loss: 1.6265
Training Epoch: 14 [10368/50048]	Loss: 1.4536
Training Epoch: 14 [10496/50048]	Loss: 1.4763
Training Epoch: 14 [10624/50048]	Loss: 1.5449
Training Epoch: 14 [10752/50048]	Loss: 1.3171
Training Epoch: 14 [10880/50048]	Loss: 1.4807
Training Epoch: 14 [11008/50048]	Loss: 1.4645
Training Epoch: 14 [11136/50048]	Loss: 1.5523
Training Epoch: 14 [11264/50048]	Loss: 1.5288
Training Epoch: 14 [11392/50048]	Loss: 1.5370
Training Epoch: 14 [11520/50048]	Loss: 1.3335
Training Epoch: 14 [11648/50048]	Loss: 1.3576
Training Epoch: 14 [11776/50048]	Loss: 1.4612
Training Epoch: 14 [11904/50048]	Loss: 1.2409
Training Epoch: 14 [12032/50048]	Loss: 1.4135
Training Epoch: 14 [12160/50048]	Loss: 1.6579
Training Epoch: 14 [12288/50048]	Loss: 1.3570
Training Epoch: 14 [12416/50048]	Loss: 1.3866
Training Epoch: 14 [12544/50048]	Loss: 1.6656
Training Epoch: 14 [12672/50048]	Loss: 1.4736
Training Epoch: 14 [12800/50048]	Loss: 1.3672
Training Epoch: 14 [12928/50048]	Loss: 1.4330
Training Epoch: 14 [13056/50048]	Loss: 1.4889
Training Epoch: 14 [13184/50048]	Loss: 1.7814
Training Epoch: 14 [13312/50048]	Loss: 1.4792
Training Epoch: 14 [13440/50048]	Loss: 1.5809
Training Epoch: 14 [13568/50048]	Loss: 1.4076
Training Epoch: 14 [13696/50048]	Loss: 1.6347
Training Epoch: 14 [13824/50048]	Loss: 1.8706
Training Epoch: 14 [13952/50048]	Loss: 1.3098
Training Epoch: 14 [14080/50048]	Loss: 1.4691
Training Epoch: 14 [14208/50048]	Loss: 1.7245
Training Epoch: 14 [14336/50048]	Loss: 1.6296
Training Epoch: 14 [14464/50048]	Loss: 1.3343
Training Epoch: 14 [14592/50048]	Loss: 1.4969
Training Epoch: 14 [14720/50048]	Loss: 1.5294
Training Epoch: 14 [14848/50048]	Loss: 1.5416
Training Epoch: 14 [14976/50048]	Loss: 1.2805
Training Epoch: 14 [15104/50048]	Loss: 1.3696
Training Epoch: 14 [15232/50048]	Loss: 1.5907
Training Epoch: 14 [15360/50048]	Loss: 1.4205
Training Epoch: 14 [15488/50048]	Loss: 1.3196
Training Epoch: 14 [15616/50048]	Loss: 1.6317
Training Epoch: 14 [15744/50048]	Loss: 1.3928
Training Epoch: 14 [15872/50048]	Loss: 1.6545
Training Epoch: 14 [16000/50048]	Loss: 1.6795
Training Epoch: 14 [16128/50048]	Loss: 1.6911
Training Epoch: 14 [16256/50048]	Loss: 1.4555
Training Epoch: 14 [16384/50048]	Loss: 1.3283
Training Epoch: 14 [16512/50048]	Loss: 1.3803
Training Epoch: 14 [16640/50048]	Loss: 1.2909
Training Epoch: 14 [16768/50048]	Loss: 1.7717
Training Epoch: 14 [16896/50048]	Loss: 1.2863
Training Epoch: 14 [17024/50048]	Loss: 1.4443
Training Epoch: 14 [17152/50048]	Loss: 1.5204
Training Epoch: 14 [17280/50048]	Loss: 1.4606
Training Epoch: 14 [17408/50048]	Loss: 1.2052
Training Epoch: 14 [17536/50048]	Loss: 1.2880
Training Epoch: 14 [17664/50048]	Loss: 1.3738
Training Epoch: 14 [17792/50048]	Loss: 1.7241
Training Epoch: 14 [17920/50048]	Loss: 1.8393
Training Epoch: 14 [18048/50048]	Loss: 1.6119
Training Epoch: 14 [18176/50048]	Loss: 1.5527
Training Epoch: 14 [18304/50048]	Loss: 1.5161
Training Epoch: 14 [18432/50048]	Loss: 1.4123
Training Epoch: 14 [18560/50048]	Loss: 1.4642
Training Epoch: 14 [18688/50048]	Loss: 1.4386
Training Epoch: 14 [18816/50048]	Loss: 1.5352
Training Epoch: 14 [18944/50048]	Loss: 1.5010
Training Epoch: 14 [19072/50048]	Loss: 1.8169
Training Epoch: 14 [19200/50048]	Loss: 1.4987
Training Epoch: 14 [19328/50048]	Loss: 1.4928
Training Epoch: 14 [19456/50048]	Loss: 1.3776
Training Epoch: 14 [19584/50048]	Loss: 1.6226
Training Epoch: 14 [19712/50048]	Loss: 1.5278
Training Epoch: 14 [19840/50048]	Loss: 1.4082
Training Epoch: 14 [19968/50048]	Loss: 1.4634
Training Epoch: 14 [20096/50048]	Loss: 1.5629
Training Epoch: 14 [20224/50048]	Loss: 1.7247
Training Epoch: 14 [20352/50048]	Loss: 1.4718
Training Epoch: 14 [20480/50048]	Loss: 1.5297
Training Epoch: 14 [20608/50048]	Loss: 1.4401
Training Epoch: 14 [20736/50048]	Loss: 1.5415
Training Epoch: 14 [20864/50048]	Loss: 1.5590
Training Epoch: 14 [20992/50048]	Loss: 1.6180
Training Epoch: 14 [21120/50048]	Loss: 1.5215
Training Epoch: 14 [21248/50048]	Loss: 1.5277
Training Epoch: 14 [21376/50048]	Loss: 1.3628
Training Epoch: 14 [21504/50048]	Loss: 1.5379
Training Epoch: 14 [21632/50048]	Loss: 1.4017
Training Epoch: 14 [21760/50048]	Loss: 1.6766
Training Epoch: 14 [21888/50048]	Loss: 1.4831
Training Epoch: 14 [22016/50048]	Loss: 1.3952
Training Epoch: 14 [22144/50048]	Loss: 1.6131
Training Epoch: 14 [22272/50048]	Loss: 1.5252
Training Epoch: 14 [22400/50048]	Loss: 1.6660
Training Epoch: 14 [22528/50048]	Loss: 1.3434
Training Epoch: 14 [22656/50048]	Loss: 1.4588
Training Epoch: 14 [22784/50048]	Loss: 1.6625
Training Epoch: 14 [22912/50048]	Loss: 1.4755
Training Epoch: 14 [23040/50048]	Loss: 1.4813
Training Epoch: 14 [23168/50048]	Loss: 1.6294
Training Epoch: 14 [23296/50048]	Loss: 1.3407
Training Epoch: 14 [23424/50048]	Loss: 1.7232
Training Epoch: 14 [23552/50048]	Loss: 1.5717
Training Epoch: 14 [23680/50048]	Loss: 1.7558
Training Epoch: 14 [23808/50048]	Loss: 1.5674
Training Epoch: 14 [23936/50048]	Loss: 1.6986
Training Epoch: 14 [24064/50048]	Loss: 1.7675
Training Epoch: 14 [24192/50048]	Loss: 1.5180
Training Epoch: 14 [24320/50048]	Loss: 1.2987
Training Epoch: 14 [24448/50048]	Loss: 1.6197
Training Epoch: 14 [24576/50048]	Loss: 1.6570
Training Epoch: 14 [24704/50048]	Loss: 1.3261
Training Epoch: 14 [24832/50048]	Loss: 1.3849
Training Epoch: 14 [24960/50048]	Loss: 1.4124
Training Epoch: 14 [25088/50048]	Loss: 1.5656
Training Epoch: 14 [25216/50048]	Loss: 1.4635
Training Epoch: 14 [25344/50048]	Loss: 1.4767
Training Epoch: 14 [25472/50048]	Loss: 1.6228
Training Epoch: 14 [25600/50048]	Loss: 1.6529
Training Epoch: 14 [25728/50048]	Loss: 1.5387
Training Epoch: 14 [25856/50048]	Loss: 1.4312
Training Epoch: 14 [25984/50048]	Loss: 1.5272
Training Epoch: 14 [26112/50048]	Loss: 1.3929
Training Epoch: 14 [26240/50048]	Loss: 1.5984
Training Epoch: 14 [26368/50048]	Loss: 1.2251
Training Epoch: 14 [26496/50048]	Loss: 1.5883
Training Epoch: 14 [26624/50048]	Loss: 1.4795
Training Epoch: 14 [26752/50048]	Loss: 1.4668
Training Epoch: 14 [26880/50048]	Loss: 1.4694
Training Epoch: 14 [27008/50048]	Loss: 1.3905
Training Epoch: 14 [27136/50048]	Loss: 1.7051
Training Epoch: 14 [27264/50048]	Loss: 1.4784
Training Epoch: 14 [27392/50048]	Loss: 1.7153
Training Epoch: 14 [27520/50048]	Loss: 1.6971
Training Epoch: 14 [27648/50048]	Loss: 1.5281
Training Epoch: 14 [27776/50048]	Loss: 1.7368
Training Epoch: 14 [27904/50048]	Loss: 1.4794
Training Epoch: 14 [28032/50048]	Loss: 1.2482
Training Epoch: 14 [28160/50048]	Loss: 1.5433
Training Epoch: 14 [28288/50048]	Loss: 1.4268
Training Epoch: 14 [28416/50048]	Loss: 1.4043
Training Epoch: 14 [28544/50048]	Loss: 1.5211
Training Epoch: 14 [28672/50048]	Loss: 1.6025
Training Epoch: 14 [28800/50048]	Loss: 1.5445
Training Epoch: 14 [28928/50048]	Loss: 1.4980
Training Epoch: 14 [29056/50048]	Loss: 1.4778
Training Epoch: 14 [29184/50048]	Loss: 1.5479
Training Epoch: 14 [29312/50048]	Loss: 1.5281
Training Epoch: 14 [29440/50048]	Loss: 1.5791
Training Epoch: 14 [29568/50048]	Loss: 1.5909
Training Epoch: 14 [29696/50048]	Loss: 1.6042
Training Epoch: 14 [29824/50048]	Loss: 1.3911
Training Epoch: 14 [29952/50048]	Loss: 1.5529
Training Epoch: 14 [30080/50048]	Loss: 1.2773
Training Epoch: 14 [30208/50048]	Loss: 1.4966
Training Epoch: 14 [30336/50048]	Loss: 1.4281
Training Epoch: 14 [30464/50048]	Loss: 1.4218
Training Epoch: 14 [30592/50048]	Loss: 1.4278
Training Epoch: 14 [30720/50048]	Loss: 1.7799
Training Epoch: 14 [30848/50048]	Loss: 1.4226
Training Epoch: 14 [30976/50048]	Loss: 1.3312
Training Epoch: 14 [31104/50048]	Loss: 1.3952
Training Epoch: 14 [31232/50048]	Loss: 1.3157
Training Epoch: 14 [31360/50048]	Loss: 1.5007
Training Epoch: 14 [31488/50048]	Loss: 1.5402
Training Epoch: 14 [31616/50048]	Loss: 1.5698
Training Epoch: 14 [31744/50048]	Loss: 1.5418
Training Epoch: 14 [31872/50048]	Loss: 1.4816
Training Epoch: 14 [32000/50048]	Loss: 1.3940
Training Epoch: 14 [32128/50048]	Loss: 1.7109
Training Epoch: 14 [32256/50048]	Loss: 1.4175
Training Epoch: 14 [32384/50048]	Loss: 1.4348
Training Epoch: 14 [32512/50048]	Loss: 1.7069
Training Epoch: 14 [32640/50048]	Loss: 1.4998
Training Epoch: 14 [32768/50048]	Loss: 1.5824
Training Epoch: 14 [32896/50048]	Loss: 1.6449
Training Epoch: 14 [33024/50048]	Loss: 1.5552
Training Epoch: 14 [33152/50048]	Loss: 1.7583
Training Epoch: 14 [33280/50048]	Loss: 1.3881
Training Epoch: 14 [33408/50048]	Loss: 1.3550
Training Epoch: 14 [33536/50048]	Loss: 1.8281
Training Epoch: 14 [33664/50048]	Loss: 1.3590
Training Epoch: 14 [33792/50048]	Loss: 1.4489
Training Epoch: 14 [33920/50048]	Loss: 1.6547
Training Epoch: 14 [34048/50048]	Loss: 1.9535
Training Epoch: 14 [34176/50048]	Loss: 1.5383
Training Epoch: 14 [34304/50048]	Loss: 1.7214
Training Epoch: 14 [34432/50048]	Loss: 1.4851
Training Epoch: 14 [34560/50048]	Loss: 1.3871
Training Epoch: 14 [34688/50048]	Loss: 1.6458
Training Epoch: 14 [34816/50048]	Loss: 1.4419
Training Epoch: 14 [34944/50048]	Loss: 1.5297
Training Epoch: 14 [35072/50048]	Loss: 1.7242
Training Epoch: 14 [35200/50048]	Loss: 1.4901
Training Epoch: 14 [35328/50048]	Loss: 1.4512
Training Epoch: 14 [35456/50048]	Loss: 1.5584
Training Epoch: 14 [35584/50048]	Loss: 1.4875
Training Epoch: 14 [35712/50048]	Loss: 1.4589
Training Epoch: 14 [35840/50048]	Loss: 1.4407
Training Epoch: 14 [35968/50048]	Loss: 1.5093
Training Epoch: 14 [36096/50048]	Loss: 1.1462
Training Epoch: 14 [36224/50048]	Loss: 1.5356
Training Epoch: 14 [36352/50048]	Loss: 1.1943
Training Epoch: 14 [36480/50048]	Loss: 1.4384
Training Epoch: 14 [36608/50048]	Loss: 1.3779
Training Epoch: 14 [36736/50048]	Loss: 1.3168
Training Epoch: 14 [36864/50048]	Loss: 1.3797
Training Epoch: 14 [36992/50048]	Loss: 1.4432
Training Epoch: 14 [37120/50048]	Loss: 1.4542
Training Epoch: 14 [37248/50048]	Loss: 1.4281
Training Epoch: 14 [37376/50048]	Loss: 1.6493
Training Epoch: 14 [37504/50048]	Loss: 1.4222
Training Epoch: 14 [37632/50048]	Loss: 1.4197
Training Epoch: 14 [37760/50048]	Loss: 1.5740
Training Epoch: 14 [37888/50048]	Loss: 1.4358
Training Epoch: 14 [38016/50048]	Loss: 1.5482
Training Epoch: 14 [38144/50048]	Loss: 1.3887
Training Epoch: 14 [38272/50048]	Loss: 1.3623
Training Epoch: 14 [38400/50048]	Loss: 1.3459
Training Epoch: 14 [38528/50048]	Loss: 1.3372
Training Epoch: 14 [38656/50048]	Loss: 1.6128
Training Epoch: 14 [38784/50048]	Loss: 1.2526
Training Epoch: 14 [38912/50048]	Loss: 1.3768
Training Epoch: 14 [39040/50048]	Loss: 1.6178
Training Epoch: 14 [39168/50048]	Loss: 1.9389
Training Epoch: 14 [39296/50048]	Loss: 1.8326
Training Epoch: 14 [39424/50048]	Loss: 1.4054
Training Epoch: 14 [39552/50048]	Loss: 1.3217
Training Epoch: 14 [39680/50048]	Loss: 1.7260
Training Epoch: 14 [39808/50048]	Loss: 1.4609
Training Epoch: 14 [39936/50048]	Loss: 1.4878
Training Epoch: 14 [40064/50048]	Loss: 1.3033
Training Epoch: 14 [40192/50048]	Loss: 1.3714
Training Epoch: 14 [40320/50048]	Loss: 1.5025
Training Epoch: 14 [40448/50048]	Loss: 1.5032
Training Epoch: 14 [40576/50048]	Loss: 1.6187
Training Epoch: 14 [40704/50048]	Loss: 1.6293
Training Epoch: 14 [40832/50048]	Loss: 1.5491
Training Epoch: 14 [40960/50048]	Loss: 1.3537
Training Epoch: 14 [41088/50048]	Loss: 1.3668
Training Epoch: 14 [41216/50048]	Loss: 1.6729
Training Epoch: 14 [41344/50048]	Loss: 1.5984
Training Epoch: 14 [41472/50048]	Loss: 1.6709
Training Epoch: 14 [41600/50048]	Loss: 1.5408
Training Epoch: 14 [41728/50048]	Loss: 1.5874
Training Epoch: 14 [41856/50048]	Loss: 1.4891
Training Epoch: 14 [41984/50048]	Loss: 1.2646
Training Epoch: 14 [42112/50048]	Loss: 1.4304
Training Epoch: 14 [42240/50048]	Loss: 1.6096
Training Epoch: 14 [42368/50048]	Loss: 1.3400
Training Epoch: 14 [42496/50048]	Loss: 1.3745
Training Epoch: 14 [42624/50048]	Loss: 1.4854
Training Epoch: 14 [42752/50048]	Loss: 1.6654
Training Epoch: 14 [42880/50048]	Loss: 1.4497
Training Epoch: 14 [43008/50048]	Loss: 1.5088
Training Epoch: 14 [43136/50048]	Loss: 1.2433
Training Epoch: 14 [43264/50048]	Loss: 1.3988
Training Epoch: 14 [43392/50048]	Loss: 1.6023
Training Epoch: 14 [43520/50048]	Loss: 1.4215
Training Epoch: 14 [43648/50048]	Loss: 1.6169
Training Epoch: 14 [43776/50048]	Loss: 1.3758
Training Epoch: 14 [43904/50048]	Loss: 1.4494
Training Epoch: 14 [44032/50048]	Loss: 1.6352
Training Epoch: 14 [44160/50048]	Loss: 1.4446
Training Epoch: 14 [44288/50048]	Loss: 1.4208
Training Epoch: 14 [44416/50048]	Loss: 1.3474
Training Epoch: 14 [44544/50048]	Loss: 1.4242
Training Epoch: 14 [44672/50048]	Loss: 1.7229
Training Epoch: 14 [44800/50048]	Loss: 1.6494
Training Epoch: 14 [44928/50048]	Loss: 1.5807
Training Epoch: 14 [45056/50048]	Loss: 1.6601
Training Epoch: 14 [45184/50048]	Loss: 1.5814
Training Epoch: 14 [45312/50048]	Loss: 1.7017
Training Epoch: 14 [45440/50048]	Loss: 1.6729
Training Epoch: 14 [45568/50048]	Loss: 1.0903
Training Epoch: 14 [45696/50048]	Loss: 1.4436
Training Epoch: 14 [45824/50048]	Loss: 1.4582
Training Epoch: 14 [45952/50048]	Loss: 1.3931
Training Epoch: 14 [46080/50048]	Loss: 1.4949
Training Epoch: 14 [46208/50048]	Loss: 1.6788
Training Epoch: 14 [46336/50048]	Loss: 1.5011
Training Epoch: 14 [46464/50048]	Loss: 1.3830
Training Epoch: 14 [46592/50048]	Loss: 1.7037
Training Epoch: 14 [46720/50048]	Loss: 1.6321
Training Epoch: 14 [46848/50048]	Loss: 1.6204
Training Epoch: 14 [46976/50048]	Loss: 1.5329
Training Epoch: 14 [47104/50048]	Loss: 1.3627
Training Epoch: 14 [47232/50048]	Loss: 1.6729
Training Epoch: 14 [47360/50048]	Loss: 1.6185
Training Epoch: 14 [47488/50048]	Loss: 1.4990
Training Epoch: 14 [47616/50048]	Loss: 1.6115
Training Epoch: 14 [47744/50048]	Loss: 1.4920
Training Epoch: 14 [47872/50048]	Loss: 1.5425
Training Epoch: 14 [48000/50048]	Loss: 1.4271
Training Epoch: 14 [48128/50048]	Loss: 1.5861
Training Epoch: 14 [48256/50048]	Loss: 1.5491
Training Epoch: 14 [48384/50048]	Loss: 1.5573
Training Epoch: 14 [48512/50048]	Loss: 1.5170
Training Epoch: 14 [48640/50048]	Loss: 1.5614
Training Epoch: 14 [48768/50048]	Loss: 1.5932
Training Epoch: 14 [48896/50048]	Loss: 1.5741
Training Epoch: 14 [49024/50048]	Loss: 1.4284
Training Epoch: 14 [49152/50048]	Loss: 1.4912
Training Epoch: 14 [49280/50048]	Loss: 1.7196
Training Epoch: 14 [49408/50048]	Loss: 1.2646
Training Epoch: 14 [49536/50048]	Loss: 1.5124
Training Epoch: 14 [49664/50048]	Loss: 1.5884
Training Epoch: 14 [49792/50048]	Loss: 1.4737
Training Epoch: 14 [49920/50048]	Loss: 1.2566
Training Epoch: 14 [50048/50048]	Loss: 1.5677
Validation Epoch: 14, Average loss: 0.0127, Accuracy: 0.5543
Training Epoch: 15 [128/50048]	Loss: 1.3682
Training Epoch: 15 [256/50048]	Loss: 1.2439
Training Epoch: 15 [384/50048]	Loss: 1.5808
Training Epoch: 15 [512/50048]	Loss: 1.3541
Training Epoch: 15 [640/50048]	Loss: 1.4769
Training Epoch: 15 [768/50048]	Loss: 1.3493
Training Epoch: 15 [896/50048]	Loss: 1.2285
Training Epoch: 15 [1024/50048]	Loss: 1.4629
Training Epoch: 15 [1152/50048]	Loss: 1.5237
Training Epoch: 15 [1280/50048]	Loss: 1.5905
Training Epoch: 15 [1408/50048]	Loss: 1.3705
Training Epoch: 15 [1536/50048]	Loss: 1.3077
Training Epoch: 15 [1664/50048]	Loss: 1.4574
Training Epoch: 15 [1792/50048]	Loss: 1.4202
Training Epoch: 15 [1920/50048]	Loss: 1.2165
Training Epoch: 15 [2048/50048]	Loss: 1.6727
Training Epoch: 15 [2176/50048]	Loss: 1.4961
Training Epoch: 15 [2304/50048]	Loss: 1.3965
Training Epoch: 15 [2432/50048]	Loss: 1.4628
Training Epoch: 15 [2560/50048]	Loss: 1.5919
Training Epoch: 15 [2688/50048]	Loss: 1.2357
Training Epoch: 15 [2816/50048]	Loss: 1.3279
Training Epoch: 15 [2944/50048]	Loss: 1.4189
Training Epoch: 15 [3072/50048]	Loss: 1.4713
Training Epoch: 15 [3200/50048]	Loss: 1.1939
Training Epoch: 15 [3328/50048]	Loss: 1.2948
Training Epoch: 15 [3456/50048]	Loss: 1.3313
Training Epoch: 15 [3584/50048]	Loss: 1.2131
Training Epoch: 15 [3712/50048]	Loss: 1.3629
Training Epoch: 15 [3840/50048]	Loss: 1.5376
Training Epoch: 15 [3968/50048]	Loss: 1.7060
Training Epoch: 15 [4096/50048]	Loss: 1.4894
Training Epoch: 15 [4224/50048]	Loss: 1.3517
Training Epoch: 15 [4352/50048]	Loss: 1.4133
Training Epoch: 15 [4480/50048]	Loss: 1.5661
Training Epoch: 15 [4608/50048]	Loss: 1.2817
Training Epoch: 15 [4736/50048]	Loss: 1.2059
Training Epoch: 15 [4864/50048]	Loss: 1.2462
Training Epoch: 15 [4992/50048]	Loss: 1.8974
Training Epoch: 15 [5120/50048]	Loss: 1.4459
Training Epoch: 15 [5248/50048]	Loss: 1.7270
Training Epoch: 15 [5376/50048]	Loss: 1.4289
Training Epoch: 15 [5504/50048]	Loss: 1.3669
Training Epoch: 15 [5632/50048]	Loss: 1.3846
Training Epoch: 15 [5760/50048]	Loss: 1.5287
Training Epoch: 15 [5888/50048]	Loss: 1.5873
Training Epoch: 15 [6016/50048]	Loss: 1.8334
Training Epoch: 15 [6144/50048]	Loss: 1.8307
Training Epoch: 15 [6272/50048]	Loss: 1.3290
Training Epoch: 15 [6400/50048]	Loss: 1.4746
Training Epoch: 15 [6528/50048]	Loss: 1.5896
Training Epoch: 15 [6656/50048]	Loss: 1.5480
Training Epoch: 15 [6784/50048]	Loss: 1.4427
Training Epoch: 15 [6912/50048]	Loss: 1.4781
Training Epoch: 15 [7040/50048]	Loss: 1.3311
Training Epoch: 15 [7168/50048]	Loss: 1.5166
Training Epoch: 15 [7296/50048]	Loss: 1.3331
Training Epoch: 15 [7424/50048]	Loss: 1.3254
Training Epoch: 15 [7552/50048]	Loss: 1.4239
Training Epoch: 15 [7680/50048]	Loss: 1.5598
Training Epoch: 15 [7808/50048]	Loss: 1.5159
Training Epoch: 15 [7936/50048]	Loss: 1.3673
Training Epoch: 15 [8064/50048]	Loss: 1.4081
Training Epoch: 15 [8192/50048]	Loss: 1.0143
Training Epoch: 15 [8320/50048]	Loss: 1.3021
Training Epoch: 15 [8448/50048]	Loss: 1.3958
Training Epoch: 15 [8576/50048]	Loss: 1.5110
Training Epoch: 15 [8704/50048]	Loss: 1.3448
Training Epoch: 15 [8832/50048]	Loss: 1.3372
Training Epoch: 15 [8960/50048]	Loss: 1.5541
Training Epoch: 15 [9088/50048]	Loss: 1.4946
Training Epoch: 15 [9216/50048]	Loss: 1.5684
Training Epoch: 15 [9344/50048]	Loss: 1.5131
Training Epoch: 15 [9472/50048]	Loss: 1.5389
Training Epoch: 15 [9600/50048]	Loss: 1.5731
Training Epoch: 15 [9728/50048]	Loss: 1.5385
Training Epoch: 15 [9856/50048]	Loss: 1.5136
Training Epoch: 15 [9984/50048]	Loss: 1.4635
Training Epoch: 15 [10112/50048]	Loss: 1.5398
Training Epoch: 15 [10240/50048]	Loss: 1.3278
Training Epoch: 15 [10368/50048]	Loss: 1.5395
Training Epoch: 15 [10496/50048]	Loss: 1.3689
Training Epoch: 15 [10624/50048]	Loss: 1.2678
Training Epoch: 15 [10752/50048]	Loss: 1.4493
Training Epoch: 15 [10880/50048]	Loss: 1.4542
Training Epoch: 15 [11008/50048]	Loss: 1.5735
Training Epoch: 15 [11136/50048]	Loss: 1.4730
Training Epoch: 15 [11264/50048]	Loss: 1.4248
Training Epoch: 15 [11392/50048]	Loss: 1.4290
Training Epoch: 15 [11520/50048]	Loss: 1.6232
Training Epoch: 15 [11648/50048]	Loss: 1.2720
Training Epoch: 15 [11776/50048]	Loss: 1.5061
Training Epoch: 15 [11904/50048]	Loss: 1.4260
Training Epoch: 15 [12032/50048]	Loss: 1.4073
Training Epoch: 15 [12160/50048]	Loss: 1.3058
Training Epoch: 15 [12288/50048]	Loss: 1.5257
Training Epoch: 15 [12416/50048]	Loss: 1.6498
Training Epoch: 15 [12544/50048]	Loss: 1.4718
Training Epoch: 15 [12672/50048]	Loss: 1.3562
Training Epoch: 15 [12800/50048]	Loss: 1.6407
Training Epoch: 15 [12928/50048]	Loss: 1.5450
Training Epoch: 15 [13056/50048]	Loss: 1.4791
Training Epoch: 15 [13184/50048]	Loss: 1.3601
Training Epoch: 15 [13312/50048]	Loss: 1.4803
Training Epoch: 15 [13440/50048]	Loss: 1.5724
Training Epoch: 15 [13568/50048]	Loss: 1.3467
Training Epoch: 15 [13696/50048]	Loss: 1.4727
Training Epoch: 15 [13824/50048]	Loss: 1.3698
Training Epoch: 15 [13952/50048]	Loss: 1.6875
Training Epoch: 15 [14080/50048]	Loss: 1.2950
Training Epoch: 15 [14208/50048]	Loss: 1.5064
Training Epoch: 15 [14336/50048]	Loss: 1.3182
Training Epoch: 15 [14464/50048]	Loss: 1.4506
Training Epoch: 15 [14592/50048]	Loss: 1.5034
Training Epoch: 15 [14720/50048]	Loss: 1.4265
Training Epoch: 15 [14848/50048]	Loss: 1.8523
Training Epoch: 15 [14976/50048]	Loss: 1.1961
Training Epoch: 15 [15104/50048]	Loss: 1.5589
Training Epoch: 15 [15232/50048]	Loss: 1.2817
Training Epoch: 15 [15360/50048]	Loss: 1.4486
Training Epoch: 15 [15488/50048]	Loss: 1.5661
Training Epoch: 15 [15616/50048]	Loss: 1.6285
Training Epoch: 15 [15744/50048]	Loss: 1.4768
Training Epoch: 15 [15872/50048]	Loss: 1.3830
Training Epoch: 15 [16000/50048]	Loss: 1.5247
Training Epoch: 15 [16128/50048]	Loss: 1.4232
Training Epoch: 15 [16256/50048]	Loss: 1.4670
Training Epoch: 15 [16384/50048]	Loss: 1.5044
Training Epoch: 15 [16512/50048]	Loss: 1.7952
Training Epoch: 15 [16640/50048]	Loss: 1.6404
Training Epoch: 15 [16768/50048]	Loss: 1.3916
Training Epoch: 15 [16896/50048]	Loss: 1.2121
Training Epoch: 15 [17024/50048]	Loss: 1.4567
Training Epoch: 15 [17152/50048]	Loss: 1.4278
Training Epoch: 15 [17280/50048]	Loss: 1.3200
Training Epoch: 15 [17408/50048]	Loss: 1.4559
Training Epoch: 15 [17536/50048]	Loss: 1.7186
Training Epoch: 15 [17664/50048]	Loss: 1.4734
Training Epoch: 15 [17792/50048]	Loss: 1.4123
Training Epoch: 15 [17920/50048]	Loss: 1.4453
Training Epoch: 15 [18048/50048]	Loss: 1.7129
Training Epoch: 15 [18176/50048]	Loss: 1.5482
Training Epoch: 15 [18304/50048]	Loss: 1.6423
Training Epoch: 15 [18432/50048]	Loss: 1.5179
Training Epoch: 15 [18560/50048]	Loss: 1.5159
Training Epoch: 15 [18688/50048]	Loss: 1.6996
Training Epoch: 15 [18816/50048]	Loss: 1.4848
Training Epoch: 15 [18944/50048]	Loss: 1.4642
Training Epoch: 15 [19072/50048]	Loss: 1.6199
Training Epoch: 15 [19200/50048]	Loss: 1.3839
Training Epoch: 15 [19328/50048]	Loss: 1.4125
Training Epoch: 15 [19456/50048]	Loss: 1.4840
Training Epoch: 15 [19584/50048]	Loss: 1.1427
Training Epoch: 15 [19712/50048]	Loss: 1.2562
Training Epoch: 15 [19840/50048]	Loss: 1.6628
Training Epoch: 15 [19968/50048]	Loss: 1.4389
Training Epoch: 15 [20096/50048]	Loss: 1.3735
Training Epoch: 15 [20224/50048]	Loss: 1.5730
Training Epoch: 15 [20352/50048]	Loss: 1.4259
Training Epoch: 15 [20480/50048]	Loss: 1.4204
Training Epoch: 15 [20608/50048]	Loss: 1.3234
Training Epoch: 15 [20736/50048]	Loss: 1.5098
Training Epoch: 15 [20864/50048]	Loss: 1.4854
Training Epoch: 15 [20992/50048]	Loss: 1.3685
Training Epoch: 15 [21120/50048]	Loss: 1.6157
Training Epoch: 15 [21248/50048]	Loss: 1.5953
Training Epoch: 15 [21376/50048]	Loss: 1.4412
Training Epoch: 15 [21504/50048]	Loss: 1.4958
Training Epoch: 15 [21632/50048]	Loss: 1.4259
Training Epoch: 15 [21760/50048]	Loss: 1.6074
Training Epoch: 15 [21888/50048]	Loss: 1.6323
Training Epoch: 15 [22016/50048]	Loss: 1.4714
Training Epoch: 15 [22144/50048]	Loss: 1.4984
Training Epoch: 15 [22272/50048]	Loss: 1.5367
Training Epoch: 15 [22400/50048]	Loss: 1.2204
Training Epoch: 15 [22528/50048]	Loss: 1.7830
Training Epoch: 15 [22656/50048]	Loss: 1.5385
Training Epoch: 15 [22784/50048]	Loss: 1.3644
Training Epoch: 15 [22912/50048]	Loss: 1.5592
Training Epoch: 15 [23040/50048]	Loss: 1.3274
Training Epoch: 15 [23168/50048]	Loss: 1.2745
Training Epoch: 15 [23296/50048]	Loss: 1.4351
Training Epoch: 15 [23424/50048]	Loss: 1.3825
Training Epoch: 15 [23552/50048]	Loss: 1.3084
Training Epoch: 15 [23680/50048]	Loss: 1.2938
Training Epoch: 15 [23808/50048]	Loss: 1.2382
Training Epoch: 15 [23936/50048]	Loss: 1.4660
Training Epoch: 15 [24064/50048]	Loss: 1.3211
Training Epoch: 15 [24192/50048]	Loss: 1.4632
Training Epoch: 15 [24320/50048]	Loss: 1.7754
Training Epoch: 15 [24448/50048]	Loss: 1.3608
Training Epoch: 15 [24576/50048]	Loss: 1.4789
Training Epoch: 15 [24704/50048]	Loss: 1.4999
Training Epoch: 15 [24832/50048]	Loss: 1.5053
Training Epoch: 15 [24960/50048]	Loss: 1.9055
Training Epoch: 15 [25088/50048]	Loss: 1.3440
Training Epoch: 15 [25216/50048]	Loss: 1.6193
Training Epoch: 15 [25344/50048]	Loss: 1.4619
Training Epoch: 15 [25472/50048]	Loss: 1.3309
Training Epoch: 15 [25600/50048]	Loss: 1.4426
Training Epoch: 15 [25728/50048]	Loss: 1.6018
Training Epoch: 15 [25856/50048]	Loss: 1.6016
Training Epoch: 15 [25984/50048]	Loss: 1.4610
Training Epoch: 15 [26112/50048]	Loss: 1.3737
Training Epoch: 15 [26240/50048]	Loss: 1.5943
Training Epoch: 15 [26368/50048]	Loss: 1.2559
Training Epoch: 15 [26496/50048]	Loss: 1.7986
Training Epoch: 15 [26624/50048]	Loss: 1.3730
Training Epoch: 15 [26752/50048]	Loss: 1.2604
Training Epoch: 15 [26880/50048]	Loss: 1.4561
Training Epoch: 15 [27008/50048]	Loss: 1.5811
Training Epoch: 15 [27136/50048]	Loss: 1.6434
Training Epoch: 15 [27264/50048]	Loss: 1.6028
Training Epoch: 15 [27392/50048]	Loss: 1.2765
Training Epoch: 15 [27520/50048]	Loss: 1.3298
Training Epoch: 15 [27648/50048]	Loss: 1.4840
Training Epoch: 15 [27776/50048]	Loss: 1.5606
Training Epoch: 15 [27904/50048]	Loss: 1.7300
Training Epoch: 15 [28032/50048]	Loss: 1.6172
Training Epoch: 15 [28160/50048]	Loss: 1.1743
Training Epoch: 15 [28288/50048]	Loss: 1.5037
Training Epoch: 15 [28416/50048]	Loss: 1.4466
Training Epoch: 15 [28544/50048]	Loss: 1.6521
Training Epoch: 15 [28672/50048]	Loss: 1.5022
Training Epoch: 15 [28800/50048]	Loss: 1.5155
Training Epoch: 15 [28928/50048]	Loss: 1.6286
Training Epoch: 15 [29056/50048]	Loss: 1.8992
Training Epoch: 15 [29184/50048]	Loss: 1.4228
Training Epoch: 15 [29312/50048]	Loss: 1.4846
Training Epoch: 15 [29440/50048]	Loss: 1.5969
Training Epoch: 15 [29568/50048]	Loss: 1.4345
Training Epoch: 15 [29696/50048]	Loss: 1.3802
Training Epoch: 15 [29824/50048]	Loss: 1.5107
Training Epoch: 15 [29952/50048]	Loss: 1.2744
Training Epoch: 15 [30080/50048]	Loss: 1.6505
Training Epoch: 15 [30208/50048]	Loss: 1.4924
Training Epoch: 15 [30336/50048]	Loss: 1.4423
Training Epoch: 15 [30464/50048]	Loss: 1.4972
Training Epoch: 15 [30592/50048]	Loss: 1.3883
Training Epoch: 15 [30720/50048]	Loss: 1.3447
Training Epoch: 15 [30848/50048]	Loss: 1.4510
Training Epoch: 15 [30976/50048]	Loss: 1.6074
Training Epoch: 15 [31104/50048]	Loss: 1.3391
Training Epoch: 15 [31232/50048]	Loss: 1.6379
Training Epoch: 15 [31360/50048]	Loss: 1.5281
Training Epoch: 15 [31488/50048]	Loss: 1.3421
Training Epoch: 15 [31616/50048]	Loss: 1.6547
Training Epoch: 15 [31744/50048]	Loss: 1.2772
Training Epoch: 15 [31872/50048]	Loss: 1.4136
Training Epoch: 15 [32000/50048]	Loss: 1.5116
Training Epoch: 15 [32128/50048]	Loss: 1.2589
Training Epoch: 15 [32256/50048]	Loss: 1.2976
Training Epoch: 15 [32384/50048]	Loss: 1.4964
Training Epoch: 15 [32512/50048]	Loss: 1.3334
Training Epoch: 15 [32640/50048]	Loss: 1.2362
Training Epoch: 15 [32768/50048]	Loss: 1.5816
Training Epoch: 15 [32896/50048]	Loss: 1.5634
Training Epoch: 15 [33024/50048]	Loss: 1.4035
Training Epoch: 15 [33152/50048]	Loss: 1.3487
Training Epoch: 15 [33280/50048]	Loss: 1.4834
Training Epoch: 15 [33408/50048]	Loss: 1.3995
Training Epoch: 15 [33536/50048]	Loss: 1.3170
Training Epoch: 15 [33664/50048]	Loss: 1.4300
Training Epoch: 15 [33792/50048]	Loss: 1.3218
Training Epoch: 15 [33920/50048]	Loss: 1.5052
Training Epoch: 15 [34048/50048]	Loss: 1.5532
Training Epoch: 15 [34176/50048]	Loss: 1.1644
Training Epoch: 15 [34304/50048]	Loss: 1.4041
Training Epoch: 15 [34432/50048]	Loss: 1.5718
Training Epoch: 15 [34560/50048]	Loss: 1.6155
Training Epoch: 15 [34688/50048]	Loss: 1.2674
Training Epoch: 15 [34816/50048]	Loss: 1.4388
Training Epoch: 15 [34944/50048]	Loss: 1.3349
Training Epoch: 15 [35072/50048]	Loss: 1.6960
Training Epoch: 15 [35200/50048]	Loss: 1.3489
Training Epoch: 15 [35328/50048]	Loss: 1.4896
Training Epoch: 15 [35456/50048]	Loss: 1.4264
Training Epoch: 15 [35584/50048]	Loss: 1.5243
Training Epoch: 15 [35712/50048]	Loss: 1.3962
Training Epoch: 15 [35840/50048]	Loss: 1.4099
Training Epoch: 15 [35968/50048]	Loss: 1.7325
Training Epoch: 15 [36096/50048]	Loss: 1.4031
Training Epoch: 15 [36224/50048]	Loss: 1.3676
Training Epoch: 15 [36352/50048]	Loss: 1.1594
Training Epoch: 15 [36480/50048]	Loss: 1.4806
Training Epoch: 15 [36608/50048]	Loss: 1.3508
Training Epoch: 15 [36736/50048]	Loss: 1.2906
Training Epoch: 15 [36864/50048]	Loss: 1.3486
Training Epoch: 15 [36992/50048]	Loss: 1.4346
Training Epoch: 15 [37120/50048]	Loss: 1.3092
Training Epoch: 15 [37248/50048]	Loss: 1.5223
Training Epoch: 15 [37376/50048]	Loss: 1.3123
Training Epoch: 15 [37504/50048]	Loss: 1.5229
Training Epoch: 15 [37632/50048]	Loss: 1.3948
Training Epoch: 15 [37760/50048]	Loss: 1.6280
Training Epoch: 15 [37888/50048]	Loss: 1.0921
Training Epoch: 15 [38016/50048]	Loss: 1.2849
Training Epoch: 15 [38144/50048]	Loss: 1.5229
Training Epoch: 15 [38272/50048]	Loss: 1.4427
Training Epoch: 15 [38400/50048]	Loss: 1.2891
Training Epoch: 15 [38528/50048]	Loss: 1.5122
Training Epoch: 15 [38656/50048]	Loss: 1.5469
Training Epoch: 15 [38784/50048]	Loss: 1.3073
Training Epoch: 15 [38912/50048]	Loss: 1.4924
Training Epoch: 15 [39040/50048]	Loss: 1.3712
Training Epoch: 15 [39168/50048]	Loss: 1.0909
Training Epoch: 15 [39296/50048]	Loss: 1.7313
Training Epoch: 15 [39424/50048]	Loss: 1.7195
Training Epoch: 15 [39552/50048]	Loss: 1.2130
Training Epoch: 15 [39680/50048]	Loss: 1.2381
Training Epoch: 15 [39808/50048]	Loss: 1.2503
Training Epoch: 15 [39936/50048]	Loss: 1.4465
Training Epoch: 15 [40064/50048]	Loss: 1.2656
Training Epoch: 15 [40192/50048]	Loss: 1.5278
Training Epoch: 15 [40320/50048]	Loss: 1.3202
Training Epoch: 15 [40448/50048]	Loss: 1.5848
Training Epoch: 15 [40576/50048]	Loss: 1.4471
Training Epoch: 15 [40704/50048]	Loss: 1.3738
Training Epoch: 15 [40832/50048]	Loss: 1.3705
Training Epoch: 15 [40960/50048]	Loss: 1.4781
Training Epoch: 15 [41088/50048]	Loss: 1.3735
Training Epoch: 15 [41216/50048]	Loss: 1.6447
Training Epoch: 15 [41344/50048]	Loss: 1.3151
Training Epoch: 15 [41472/50048]	Loss: 1.3944
Training Epoch: 15 [41600/50048]	Loss: 1.6109
Training Epoch: 15 [41728/50048]	Loss: 1.4264
Training Epoch: 15 [41856/50048]	Loss: 1.4374
Training Epoch: 15 [41984/50048]	Loss: 1.5994
Training Epoch: 15 [42112/50048]	Loss: 1.4678
Training Epoch: 15 [42240/50048]	Loss: 1.4448
Training Epoch: 15 [42368/50048]	Loss: 1.5693
Training Epoch: 15 [42496/50048]	Loss: 1.3765
Training Epoch: 15 [42624/50048]	Loss: 1.5196
Training Epoch: 15 [42752/50048]	Loss: 1.4331
Training Epoch: 15 [42880/50048]	Loss: 1.3536
Training Epoch: 15 [43008/50048]	Loss: 1.5866
Training Epoch: 15 [43136/50048]	Loss: 1.6306
Training Epoch: 15 [43264/50048]	Loss: 1.4580
Training Epoch: 15 [43392/50048]	Loss: 1.7380
Training Epoch: 15 [43520/50048]	Loss: 1.2842
Training Epoch: 15 [43648/50048]	Loss: 1.5923
Training Epoch: 15 [43776/50048]	Loss: 1.3908
Training Epoch: 15 [43904/50048]	Loss: 1.4550
Training Epoch: 15 [44032/50048]	Loss: 1.4424
Training Epoch: 15 [44160/50048]	Loss: 1.5171
Training Epoch: 15 [44288/50048]	Loss: 1.1735
Training Epoch: 15 [44416/50048]	Loss: 1.6057
Training Epoch: 15 [44544/50048]	Loss: 1.6502
Training Epoch: 15 [44672/50048]	Loss: 1.5006
Training Epoch: 15 [44800/50048]	Loss: 1.4092
Training Epoch: 15 [44928/50048]	Loss: 1.6198
Training Epoch: 15 [45056/50048]	Loss: 1.5026
Training Epoch: 15 [45184/50048]	Loss: 1.6586
Training Epoch: 15 [45312/50048]	Loss: 1.4829
Training Epoch: 15 [45440/50048]	Loss: 1.5420
Training Epoch: 15 [45568/50048]	Loss: 1.6513
Training Epoch: 15 [45696/50048]	Loss: 1.5406
Training Epoch: 15 [45824/50048]	Loss: 1.4487
Training Epoch: 15 [45952/50048]	Loss: 1.6604
Training Epoch: 15 [46080/50048]	Loss: 1.4602
Training Epoch: 15 [46208/50048]	Loss: 1.3899
Training Epoch: 15 [46336/50048]	Loss: 1.3055
Training Epoch: 15 [46464/50048]	Loss: 1.1577
Training Epoch: 15 [46592/50048]	Loss: 1.2142
Training Epoch: 15 [46720/50048]	Loss: 1.3951
Training Epoch: 15 [46848/50048]	Loss: 1.7031
Training Epoch: 15 [46976/50048]	Loss: 1.5162
Training Epoch: 15 [47104/50048]	Loss: 1.6735
Training Epoch: 15 [47232/50048]	Loss: 1.3651
Training Epoch: 15 [47360/50048]	Loss: 1.2951
Training Epoch: 15 [47488/50048]	Loss: 1.4753
Training Epoch: 15 [47616/50048]	Loss: 1.7478
Training Epoch: 15 [47744/50048]	Loss: 1.4556
Training Epoch: 15 [47872/50048]	Loss: 1.5715
Training Epoch: 15 [48000/50048]	Loss: 1.5487
Training Epoch: 15 [48128/50048]	Loss: 1.7468
Training Epoch: 15 [48256/50048]	Loss: 1.5154
Training Epoch: 15 [48384/50048]	Loss: 1.4184
Training Epoch: 15 [48512/50048]	Loss: 1.2451
Training Epoch: 15 [48640/50048]	Loss: 1.5366
Training Epoch: 15 [48768/50048]	Loss: 1.4770
Training Epoch: 15 [48896/50048]	Loss: 1.4457
Training Epoch: 15 [49024/50048]	Loss: 1.3890
Training Epoch: 15 [49152/50048]	Loss: 1.2538
Training Epoch: 15 [49280/50048]	Loss: 1.5811
Training Epoch: 15 [49408/50048]	Loss: 1.3639
Training Epoch: 15 [49536/50048]	Loss: 1.3565
Training Epoch: 15 [49664/50048]	Loss: 1.5337
Training Epoch: 15 [49792/50048]	Loss: 1.3649
Training Epoch: 15 [49920/50048]	Loss: 1.4954
Training Epoch: 15 [50048/50048]	Loss: 1.4999
Validation Epoch: 15, Average loss: 0.0126, Accuracy: 0.5575
Training Epoch: 16 [128/50048]	Loss: 1.4956
Training Epoch: 16 [256/50048]	Loss: 1.4031
Training Epoch: 16 [384/50048]	Loss: 1.1831
Training Epoch: 16 [512/50048]	Loss: 1.5460
Training Epoch: 16 [640/50048]	Loss: 1.4527
Training Epoch: 16 [768/50048]	Loss: 1.4500
Training Epoch: 16 [896/50048]	Loss: 1.3996
Training Epoch: 16 [1024/50048]	Loss: 1.6173
Training Epoch: 16 [1152/50048]	Loss: 1.2048
Training Epoch: 16 [1280/50048]	Loss: 1.3786
Training Epoch: 16 [1408/50048]	Loss: 1.4702
Training Epoch: 16 [1536/50048]	Loss: 1.3976
Training Epoch: 16 [1664/50048]	Loss: 1.4375
Training Epoch: 16 [1792/50048]	Loss: 1.2838
Training Epoch: 16 [1920/50048]	Loss: 1.5711
Training Epoch: 16 [2048/50048]	Loss: 1.4625
Training Epoch: 16 [2176/50048]	Loss: 1.6888
Training Epoch: 16 [2304/50048]	Loss: 1.5391
Training Epoch: 16 [2432/50048]	Loss: 1.3424
Training Epoch: 16 [2560/50048]	Loss: 1.2820
Training Epoch: 16 [2688/50048]	Loss: 1.1786
Training Epoch: 16 [2816/50048]	Loss: 1.5215
Training Epoch: 16 [2944/50048]	Loss: 1.3149
Training Epoch: 16 [3072/50048]	Loss: 1.0616
Training Epoch: 16 [3200/50048]	Loss: 1.4802
Training Epoch: 16 [3328/50048]	Loss: 1.5041
Training Epoch: 16 [3456/50048]	Loss: 1.5565
Training Epoch: 16 [3584/50048]	Loss: 1.2999
Training Epoch: 16 [3712/50048]	Loss: 1.4323
Training Epoch: 16 [3840/50048]	Loss: 1.7014
Training Epoch: 16 [3968/50048]	Loss: 1.2072
Training Epoch: 16 [4096/50048]	Loss: 1.4678
Training Epoch: 16 [4224/50048]	Loss: 1.5401
Training Epoch: 16 [4352/50048]	Loss: 1.4808
Training Epoch: 16 [4480/50048]	Loss: 1.3946
Training Epoch: 16 [4608/50048]	Loss: 1.3589
Training Epoch: 16 [4736/50048]	Loss: 1.6622
Training Epoch: 16 [4864/50048]	Loss: 1.2440
Training Epoch: 16 [4992/50048]	Loss: 1.4344
Training Epoch: 16 [5120/50048]	Loss: 1.2102
Training Epoch: 16 [5248/50048]	Loss: 1.3615
Training Epoch: 16 [5376/50048]	Loss: 1.2653
Training Epoch: 16 [5504/50048]	Loss: 1.3825
Training Epoch: 16 [5632/50048]	Loss: 1.5553
Training Epoch: 16 [5760/50048]	Loss: 1.3568
Training Epoch: 16 [5888/50048]	Loss: 1.4121
Training Epoch: 16 [6016/50048]	Loss: 1.4670
Training Epoch: 16 [6144/50048]	Loss: 1.3869
Training Epoch: 16 [6272/50048]	Loss: 1.2425
Training Epoch: 16 [6400/50048]	Loss: 1.4450
Training Epoch: 16 [6528/50048]	Loss: 1.3805
Training Epoch: 16 [6656/50048]	Loss: 1.3165
Training Epoch: 16 [6784/50048]	Loss: 1.5340
Training Epoch: 16 [6912/50048]	Loss: 1.3724
Training Epoch: 16 [7040/50048]	Loss: 1.2881
Training Epoch: 16 [7168/50048]	Loss: 1.3731
Training Epoch: 16 [7296/50048]	Loss: 1.4132
Training Epoch: 16 [7424/50048]	Loss: 1.4769
Training Epoch: 16 [7552/50048]	Loss: 1.2824
Training Epoch: 16 [7680/50048]	Loss: 1.4400
Training Epoch: 16 [7808/50048]	Loss: 1.5942
Training Epoch: 16 [7936/50048]	Loss: 1.3867
Training Epoch: 16 [8064/50048]	Loss: 1.6096
Training Epoch: 16 [8192/50048]	Loss: 1.4670
Training Epoch: 16 [8320/50048]	Loss: 1.5173
Training Epoch: 16 [8448/50048]	Loss: 1.3786
Training Epoch: 16 [8576/50048]	Loss: 1.4238
Training Epoch: 16 [8704/50048]	Loss: 1.5454
Training Epoch: 16 [8832/50048]	Loss: 1.6891
Training Epoch: 16 [8960/50048]	Loss: 1.3705
Training Epoch: 16 [9088/50048]	Loss: 1.3544
Training Epoch: 16 [9216/50048]	Loss: 1.4901
Training Epoch: 16 [9344/50048]	Loss: 1.6806
Training Epoch: 16 [9472/50048]	Loss: 1.3061
Training Epoch: 16 [9600/50048]	Loss: 1.3904
Training Epoch: 16 [9728/50048]	Loss: 1.3910
Training Epoch: 16 [9856/50048]	Loss: 1.4448
Training Epoch: 16 [9984/50048]	Loss: 1.3120
Training Epoch: 16 [10112/50048]	Loss: 1.2894
Training Epoch: 16 [10240/50048]	Loss: 1.2304
Training Epoch: 16 [10368/50048]	Loss: 1.4196
Training Epoch: 16 [10496/50048]	Loss: 1.3759
Training Epoch: 16 [10624/50048]	Loss: 1.4215
Training Epoch: 16 [10752/50048]	Loss: 1.5650
Training Epoch: 16 [10880/50048]	Loss: 1.5735
Training Epoch: 16 [11008/50048]	Loss: 1.5041
Training Epoch: 16 [11136/50048]	Loss: 1.3093
Training Epoch: 16 [11264/50048]	Loss: 1.3422
Training Epoch: 16 [11392/50048]	Loss: 1.4200
Training Epoch: 16 [11520/50048]	Loss: 1.3099
Training Epoch: 16 [11648/50048]	Loss: 1.7507
Training Epoch: 16 [11776/50048]	Loss: 1.5080
Training Epoch: 16 [11904/50048]	Loss: 1.0611
Training Epoch: 16 [12032/50048]	Loss: 1.3484
Training Epoch: 16 [12160/50048]	Loss: 1.2501
Training Epoch: 16 [12288/50048]	Loss: 1.3808
Training Epoch: 16 [12416/50048]	Loss: 1.5675
Training Epoch: 16 [12544/50048]	Loss: 1.3324
Training Epoch: 16 [12672/50048]	Loss: 1.4717
Training Epoch: 16 [12800/50048]	Loss: 1.2091
Training Epoch: 16 [12928/50048]	Loss: 1.7561
Training Epoch: 16 [13056/50048]	Loss: 1.2995
Training Epoch: 16 [13184/50048]	Loss: 1.4231
Training Epoch: 16 [13312/50048]	Loss: 1.3863
Training Epoch: 16 [13440/50048]	Loss: 1.4840
Training Epoch: 16 [13568/50048]	Loss: 1.5358
Training Epoch: 16 [13696/50048]	Loss: 1.4484
Training Epoch: 16 [13824/50048]	Loss: 1.7899
Training Epoch: 16 [13952/50048]	Loss: 1.5106
Training Epoch: 16 [14080/50048]	Loss: 1.4806
Training Epoch: 16 [14208/50048]	Loss: 1.3547
Training Epoch: 16 [14336/50048]	Loss: 1.4351
Training Epoch: 16 [14464/50048]	Loss: 1.4676
Training Epoch: 16 [14592/50048]	Loss: 1.3021
Training Epoch: 16 [14720/50048]	Loss: 1.3970
Training Epoch: 16 [14848/50048]	Loss: 1.3708
Training Epoch: 16 [14976/50048]	Loss: 1.3600
Training Epoch: 16 [15104/50048]	Loss: 1.4500
Training Epoch: 16 [15232/50048]	Loss: 1.7047
Training Epoch: 16 [15360/50048]	Loss: 1.3537
Training Epoch: 16 [15488/50048]	Loss: 1.5166
Training Epoch: 16 [15616/50048]	Loss: 1.4188
Training Epoch: 16 [15744/50048]	Loss: 1.4675
Training Epoch: 16 [15872/50048]	Loss: 1.7075
Training Epoch: 16 [16000/50048]	Loss: 1.4271
Training Epoch: 16 [16128/50048]	Loss: 1.2354
Training Epoch: 16 [16256/50048]	Loss: 1.4822
Training Epoch: 16 [16384/50048]	Loss: 1.3081
Training Epoch: 16 [16512/50048]	Loss: 1.5037
Training Epoch: 16 [16640/50048]	Loss: 1.4443
Training Epoch: 16 [16768/50048]	Loss: 1.3008
Training Epoch: 16 [16896/50048]	Loss: 1.5020
Training Epoch: 16 [17024/50048]	Loss: 1.2875
Training Epoch: 16 [17152/50048]	Loss: 1.3052
Training Epoch: 16 [17280/50048]	Loss: 1.0608
Training Epoch: 16 [17408/50048]	Loss: 1.5441
Training Epoch: 16 [17536/50048]	Loss: 1.4498
Training Epoch: 16 [17664/50048]	Loss: 1.4500
Training Epoch: 16 [17792/50048]	Loss: 1.4369
Training Epoch: 16 [17920/50048]	Loss: 1.4193
Training Epoch: 16 [18048/50048]	Loss: 1.6426
Training Epoch: 16 [18176/50048]	Loss: 1.4494
Training Epoch: 16 [18304/50048]	Loss: 1.5019
Training Epoch: 16 [18432/50048]	Loss: 1.3514
Training Epoch: 16 [18560/50048]	Loss: 1.5811
Training Epoch: 16 [18688/50048]	Loss: 1.4091
Training Epoch: 16 [18816/50048]	Loss: 1.4265
Training Epoch: 16 [18944/50048]	Loss: 1.2570
Training Epoch: 16 [19072/50048]	Loss: 1.6495
Training Epoch: 16 [19200/50048]	Loss: 1.1958
Training Epoch: 16 [19328/50048]	Loss: 1.4949
Training Epoch: 16 [19456/50048]	Loss: 1.3624
Training Epoch: 16 [19584/50048]	Loss: 1.4409
Training Epoch: 16 [19712/50048]	Loss: 1.3046
Training Epoch: 16 [19840/50048]	Loss: 1.4317
Training Epoch: 16 [19968/50048]	Loss: 1.4924
Training Epoch: 16 [20096/50048]	Loss: 1.4520
Training Epoch: 16 [20224/50048]	Loss: 1.1567
Training Epoch: 16 [20352/50048]	Loss: 1.3293
Training Epoch: 16 [20480/50048]	Loss: 1.1826
Training Epoch: 16 [20608/50048]	Loss: 1.5263
Training Epoch: 16 [20736/50048]	Loss: 1.5223
Training Epoch: 16 [20864/50048]	Loss: 1.6036
Training Epoch: 16 [20992/50048]	Loss: 1.2819
Training Epoch: 16 [21120/50048]	Loss: 1.5186
Training Epoch: 16 [21248/50048]	Loss: 1.3361
Training Epoch: 16 [21376/50048]	Loss: 1.3322
Training Epoch: 16 [21504/50048]	Loss: 1.2314
Training Epoch: 16 [21632/50048]	Loss: 1.4311
Training Epoch: 16 [21760/50048]	Loss: 1.6988
Training Epoch: 16 [21888/50048]	Loss: 1.3777
Training Epoch: 16 [22016/50048]	Loss: 1.4383
Training Epoch: 16 [22144/50048]	Loss: 1.3080
Training Epoch: 16 [22272/50048]	Loss: 1.2401
Training Epoch: 16 [22400/50048]	Loss: 1.5842
Training Epoch: 16 [22528/50048]	Loss: 1.2568
Training Epoch: 16 [22656/50048]	Loss: 1.2917
Training Epoch: 16 [22784/50048]	Loss: 1.4894
Training Epoch: 16 [22912/50048]	Loss: 1.4281
Training Epoch: 16 [23040/50048]	Loss: 1.4104
Training Epoch: 16 [23168/50048]	Loss: 1.4762
Training Epoch: 16 [23296/50048]	Loss: 1.6584
Training Epoch: 16 [23424/50048]	Loss: 1.1787
Training Epoch: 16 [23552/50048]	Loss: 1.5242
Training Epoch: 16 [23680/50048]	Loss: 1.3550
Training Epoch: 16 [23808/50048]	Loss: 1.4651
Training Epoch: 16 [23936/50048]	Loss: 1.3423
Training Epoch: 16 [24064/50048]	Loss: 1.5693
Training Epoch: 16 [24192/50048]	Loss: 1.4243
Training Epoch: 16 [24320/50048]	Loss: 1.3712
Training Epoch: 16 [24448/50048]	Loss: 1.3433
Training Epoch: 16 [24576/50048]	Loss: 1.1929
Training Epoch: 16 [24704/50048]	Loss: 1.9480
Training Epoch: 16 [24832/50048]	Loss: 1.4187
Training Epoch: 16 [24960/50048]	Loss: 1.5943
Training Epoch: 16 [25088/50048]	Loss: 1.6666
Training Epoch: 16 [25216/50048]	Loss: 1.1826
Training Epoch: 16 [25344/50048]	Loss: 1.5324
Training Epoch: 16 [25472/50048]	Loss: 1.4043
Training Epoch: 16 [25600/50048]	Loss: 1.3111
Training Epoch: 16 [25728/50048]	Loss: 1.2937
Training Epoch: 16 [25856/50048]	Loss: 1.5373
Training Epoch: 16 [25984/50048]	Loss: 1.3503
Training Epoch: 16 [26112/50048]	Loss: 1.2917
Training Epoch: 16 [26240/50048]	Loss: 1.3707
Training Epoch: 16 [26368/50048]	Loss: 1.4599
Training Epoch: 16 [26496/50048]	Loss: 1.3912
Training Epoch: 16 [26624/50048]	Loss: 1.3464
Training Epoch: 16 [26752/50048]	Loss: 1.7340
Training Epoch: 16 [26880/50048]	Loss: 1.4284
Training Epoch: 16 [27008/50048]	Loss: 1.5370
Training Epoch: 16 [27136/50048]	Loss: 1.3806
Training Epoch: 16 [27264/50048]	Loss: 1.4400
Training Epoch: 16 [27392/50048]	Loss: 1.4733
Training Epoch: 16 [27520/50048]	Loss: 1.4039
Training Epoch: 16 [27648/50048]	Loss: 1.4524
Training Epoch: 16 [27776/50048]	Loss: 1.5072
Training Epoch: 16 [27904/50048]	Loss: 1.3842
Training Epoch: 16 [28032/50048]	Loss: 1.3301
Training Epoch: 16 [28160/50048]	Loss: 1.3131
Training Epoch: 16 [28288/50048]	Loss: 1.5344
Training Epoch: 16 [28416/50048]	Loss: 1.4044
Training Epoch: 16 [28544/50048]	Loss: 1.5591
Training Epoch: 16 [28672/50048]	Loss: 1.3867
Training Epoch: 16 [28800/50048]	Loss: 1.4803
Training Epoch: 16 [28928/50048]	Loss: 1.5025
Training Epoch: 16 [29056/50048]	Loss: 1.3437
Training Epoch: 16 [29184/50048]	Loss: 1.5553
Training Epoch: 16 [29312/50048]	Loss: 1.5399
Training Epoch: 16 [29440/50048]	Loss: 1.6951
Training Epoch: 16 [29568/50048]	Loss: 1.3446
Training Epoch: 16 [29696/50048]	Loss: 1.5518
Training Epoch: 16 [29824/50048]	Loss: 1.6368
Training Epoch: 16 [29952/50048]	Loss: 1.6131
Training Epoch: 16 [30080/50048]	Loss: 1.3009
Training Epoch: 16 [30208/50048]	Loss: 1.4912
Training Epoch: 16 [30336/50048]	Loss: 1.5301
Training Epoch: 16 [30464/50048]	Loss: 1.7072
Training Epoch: 16 [30592/50048]	Loss: 1.5127
Training Epoch: 16 [30720/50048]	Loss: 1.5042
Training Epoch: 16 [30848/50048]	Loss: 1.4258
Training Epoch: 16 [30976/50048]	Loss: 1.5202
Training Epoch: 16 [31104/50048]	Loss: 1.3760
Training Epoch: 16 [31232/50048]	Loss: 1.3869
Training Epoch: 16 [31360/50048]	Loss: 1.5483
Training Epoch: 16 [31488/50048]	Loss: 1.3400
Training Epoch: 16 [31616/50048]	Loss: 1.6788
Training Epoch: 16 [31744/50048]	Loss: 1.4245
Training Epoch: 16 [31872/50048]	Loss: 1.5457
Training Epoch: 16 [32000/50048]	Loss: 1.2726
Training Epoch: 16 [32128/50048]	Loss: 1.5484
Training Epoch: 16 [32256/50048]	Loss: 1.2860
Training Epoch: 16 [32384/50048]	Loss: 1.2790
Training Epoch: 16 [32512/50048]	Loss: 1.3983
Training Epoch: 16 [32640/50048]	Loss: 1.3814
Training Epoch: 16 [32768/50048]	Loss: 1.2799
Training Epoch: 16 [32896/50048]	Loss: 1.2537
Training Epoch: 16 [33024/50048]	Loss: 1.5694
Training Epoch: 16 [33152/50048]	Loss: 1.3883
Training Epoch: 16 [33280/50048]	Loss: 1.3564
Training Epoch: 16 [33408/50048]	Loss: 1.3451
Training Epoch: 16 [33536/50048]	Loss: 1.7363
Training Epoch: 16 [33664/50048]	Loss: 1.5179
Training Epoch: 16 [33792/50048]	Loss: 1.3736
Training Epoch: 16 [33920/50048]	Loss: 1.3694
Training Epoch: 16 [34048/50048]	Loss: 1.3803
Training Epoch: 16 [34176/50048]	Loss: 1.4284
Training Epoch: 16 [34304/50048]	Loss: 1.4329
Training Epoch: 16 [34432/50048]	Loss: 1.4487
Training Epoch: 16 [34560/50048]	Loss: 1.5940
Training Epoch: 16 [34688/50048]	Loss: 1.2298
Training Epoch: 16 [34816/50048]	Loss: 1.2031
Training Epoch: 16 [34944/50048]	Loss: 1.3978
Training Epoch: 16 [35072/50048]	Loss: 1.7597
Training Epoch: 16 [35200/50048]	Loss: 1.3457
Training Epoch: 16 [35328/50048]	Loss: 1.4796
Training Epoch: 16 [35456/50048]	Loss: 1.2308
Training Epoch: 16 [35584/50048]	Loss: 1.6294
Training Epoch: 16 [35712/50048]	Loss: 1.4214
Training Epoch: 16 [35840/50048]	Loss: 1.5573
Training Epoch: 16 [35968/50048]	Loss: 1.5499
Training Epoch: 16 [36096/50048]	Loss: 1.5273
Training Epoch: 16 [36224/50048]	Loss: 1.5332
Training Epoch: 16 [36352/50048]	Loss: 1.9328
Training Epoch: 16 [36480/50048]	Loss: 1.4548
Training Epoch: 16 [36608/50048]	Loss: 1.4958
Training Epoch: 16 [36736/50048]	Loss: 1.4649
Training Epoch: 16 [36864/50048]	Loss: 1.5991
Training Epoch: 16 [36992/50048]	Loss: 1.4708
Training Epoch: 16 [37120/50048]	Loss: 1.3140
Training Epoch: 16 [37248/50048]	Loss: 1.3011
Training Epoch: 16 [37376/50048]	Loss: 1.4078
Training Epoch: 16 [37504/50048]	Loss: 1.3776
Training Epoch: 16 [37632/50048]	Loss: 1.2994
Training Epoch: 16 [37760/50048]	Loss: 1.4231
Training Epoch: 16 [37888/50048]	Loss: 1.3820
Training Epoch: 16 [38016/50048]	Loss: 1.3073
Training Epoch: 16 [38144/50048]	Loss: 1.5247
Training Epoch: 16 [38272/50048]	Loss: 1.3786
Training Epoch: 16 [38400/50048]	Loss: 1.3436
Training Epoch: 16 [38528/50048]	Loss: 1.5006
Training Epoch: 16 [38656/50048]	Loss: 1.3731
Training Epoch: 16 [38784/50048]	Loss: 1.4334
Training Epoch: 16 [38912/50048]	Loss: 1.4340
Training Epoch: 16 [39040/50048]	Loss: 1.4690
Training Epoch: 16 [39168/50048]	Loss: 1.4692
Training Epoch: 16 [39296/50048]	Loss: 1.5606
Training Epoch: 16 [39424/50048]	Loss: 1.4221
Training Epoch: 16 [39552/50048]	Loss: 1.3470
Training Epoch: 16 [39680/50048]	Loss: 1.3813
Training Epoch: 16 [39808/50048]	Loss: 1.6408
Training Epoch: 16 [39936/50048]	Loss: 1.5770
Training Epoch: 16 [40064/50048]	Loss: 1.5731
Training Epoch: 16 [40192/50048]	Loss: 1.4827
Training Epoch: 16 [40320/50048]	Loss: 1.4057
Training Epoch: 16 [40448/50048]	Loss: 1.5067
Training Epoch: 16 [40576/50048]	Loss: 1.3791
Training Epoch: 16 [40704/50048]	Loss: 1.3662
Training Epoch: 16 [40832/50048]	Loss: 1.2999
Training Epoch: 16 [40960/50048]	Loss: 1.4021
Training Epoch: 16 [41088/50048]	Loss: 1.4804
Training Epoch: 16 [41216/50048]	Loss: 1.3131
Training Epoch: 16 [41344/50048]	Loss: 1.3816
Training Epoch: 16 [41472/50048]	Loss: 1.4098
Training Epoch: 16 [41600/50048]	Loss: 1.6676
Training Epoch: 16 [41728/50048]	Loss: 1.5152
Training Epoch: 16 [41856/50048]	Loss: 1.3992
Training Epoch: 16 [41984/50048]	Loss: 1.4245
Training Epoch: 16 [42112/50048]	Loss: 1.4010
Training Epoch: 16 [42240/50048]	Loss: 1.5278
Training Epoch: 16 [42368/50048]	Loss: 1.4814
Training Epoch: 16 [42496/50048]	Loss: 1.5166
Training Epoch: 16 [42624/50048]	Loss: 1.4612
Training Epoch: 16 [42752/50048]	Loss: 1.5238
Training Epoch: 16 [42880/50048]	Loss: 1.2895
Training Epoch: 16 [43008/50048]	Loss: 1.3653
Training Epoch: 16 [43136/50048]	Loss: 1.4083
Training Epoch: 16 [43264/50048]	Loss: 1.6691
Training Epoch: 16 [43392/50048]	Loss: 1.2064
Training Epoch: 16 [43520/50048]	Loss: 1.4016
Training Epoch: 16 [43648/50048]	Loss: 1.3850
Training Epoch: 16 [43776/50048]	Loss: 1.5953
Training Epoch: 16 [43904/50048]	Loss: 1.3438
Training Epoch: 16 [44032/50048]	Loss: 1.4423
Training Epoch: 16 [44160/50048]	Loss: 1.3328
Training Epoch: 16 [44288/50048]	Loss: 1.4379
Training Epoch: 16 [44416/50048]	Loss: 1.4955
Training Epoch: 16 [44544/50048]	Loss: 1.2465
Training Epoch: 16 [44672/50048]	Loss: 1.4777
Training Epoch: 16 [44800/50048]	Loss: 1.2493
Training Epoch: 16 [44928/50048]	Loss: 1.4155
Training Epoch: 16 [45056/50048]	Loss: 1.4446
Training Epoch: 16 [45184/50048]	Loss: 1.4052
Training Epoch: 16 [45312/50048]	Loss: 1.3703
Training Epoch: 16 [45440/50048]	Loss: 1.5070
Training Epoch: 16 [45568/50048]	Loss: 1.4809
Training Epoch: 16 [45696/50048]	Loss: 1.2662
Training Epoch: 16 [45824/50048]	Loss: 1.4075
Training Epoch: 16 [45952/50048]	Loss: 1.4635
Training Epoch: 16 [46080/50048]	Loss: 1.3611
Training Epoch: 16 [46208/50048]	Loss: 1.4000
Training Epoch: 16 [46336/50048]	Loss: 1.4341
Training Epoch: 16 [46464/50048]	Loss: 1.3943
Training Epoch: 16 [46592/50048]	Loss: 1.5266
Training Epoch: 16 [46720/50048]	Loss: 1.3036
Training Epoch: 16 [46848/50048]	Loss: 1.1371
Training Epoch: 16 [46976/50048]	Loss: 1.5034
Training Epoch: 16 [47104/50048]	Loss: 1.6572
Training Epoch: 16 [47232/50048]	Loss: 1.5241
Training Epoch: 16 [47360/50048]	Loss: 1.4405
Training Epoch: 16 [47488/50048]	Loss: 1.4176
Training Epoch: 16 [47616/50048]	Loss: 1.3041
Training Epoch: 16 [47744/50048]	Loss: 1.4546
Training Epoch: 16 [47872/50048]	Loss: 1.3559
Training Epoch: 16 [48000/50048]	Loss: 1.2701
Training Epoch: 16 [48128/50048]	Loss: 1.3938
Training Epoch: 16 [48256/50048]	Loss: 1.4589
Training Epoch: 16 [48384/50048]	Loss: 1.2915
Training Epoch: 16 [48512/50048]	Loss: 1.4146
Training Epoch: 16 [48640/50048]	Loss: 1.2433
Training Epoch: 16 [48768/50048]	Loss: 1.3568
Training Epoch: 16 [48896/50048]	Loss: 1.6113
Training Epoch: 16 [49024/50048]	Loss: 1.6040
Training Epoch: 16 [49152/50048]	Loss: 1.5675
Training Epoch: 16 [49280/50048]	Loss: 1.2736
Training Epoch: 16 [49408/50048]	Loss: 1.4095
Training Epoch: 16 [49536/50048]	Loss: 1.4527
Training Epoch: 16 [49664/50048]	Loss: 1.4727
Training Epoch: 16 [49792/50048]	Loss: 1.1473
Training Epoch: 16 [49920/50048]	Loss: 1.3994
Training Epoch: 16 [50048/50048]	Loss: 1.3655
Validation Epoch: 16, Average loss: 0.0126, Accuracy: 0.5600
Training Epoch: 17 [128/50048]	Loss: 1.5313
Training Epoch: 17 [256/50048]	Loss: 1.5349
Training Epoch: 17 [384/50048]	Loss: 1.3582
Training Epoch: 17 [512/50048]	Loss: 1.3238
Training Epoch: 17 [640/50048]	Loss: 1.4660
Training Epoch: 17 [768/50048]	Loss: 1.4706
Training Epoch: 17 [896/50048]	Loss: 1.3071
Training Epoch: 17 [1024/50048]	Loss: 1.3410
Training Epoch: 17 [1152/50048]	Loss: 1.3412
Training Epoch: 17 [1280/50048]	Loss: 1.4115
Training Epoch: 17 [1408/50048]	Loss: 1.4885
Training Epoch: 17 [1536/50048]	Loss: 1.3697
Training Epoch: 17 [1664/50048]	Loss: 1.3096
Training Epoch: 17 [1792/50048]	Loss: 1.6808
Training Epoch: 17 [1920/50048]	Loss: 1.3885
Training Epoch: 17 [2048/50048]	Loss: 1.3280
Training Epoch: 17 [2176/50048]	Loss: 1.5163
Training Epoch: 17 [2304/50048]	Loss: 1.2315
Training Epoch: 17 [2432/50048]	Loss: 1.3518
Training Epoch: 17 [2560/50048]	Loss: 1.4251
Training Epoch: 17 [2688/50048]	Loss: 1.3822
Training Epoch: 17 [2816/50048]	Loss: 1.3845
Training Epoch: 17 [2944/50048]	Loss: 1.4030
Training Epoch: 17 [3072/50048]	Loss: 1.3078
Training Epoch: 17 [3200/50048]	Loss: 1.5458
Training Epoch: 17 [3328/50048]	Loss: 1.4970
Training Epoch: 17 [3456/50048]	Loss: 1.2953
Training Epoch: 17 [3584/50048]	Loss: 1.5983
Training Epoch: 17 [3712/50048]	Loss: 1.4056
Training Epoch: 17 [3840/50048]	Loss: 1.5533
Training Epoch: 17 [3968/50048]	Loss: 1.4663
Training Epoch: 17 [4096/50048]	Loss: 1.6121
Training Epoch: 17 [4224/50048]	Loss: 1.3750
Training Epoch: 17 [4352/50048]	Loss: 1.3018
Training Epoch: 17 [4480/50048]	Loss: 1.4381
Training Epoch: 17 [4608/50048]	Loss: 1.4643
Training Epoch: 17 [4736/50048]	Loss: 1.4002
Training Epoch: 17 [4864/50048]	Loss: 1.4675
Training Epoch: 17 [4992/50048]	Loss: 1.2355
Training Epoch: 17 [5120/50048]	Loss: 1.3420
Training Epoch: 17 [5248/50048]	Loss: 1.4114
Training Epoch: 17 [5376/50048]	Loss: 1.2425
Training Epoch: 17 [5504/50048]	Loss: 1.3397
Training Epoch: 17 [5632/50048]	Loss: 1.4517
Training Epoch: 17 [5760/50048]	Loss: 1.3134
Training Epoch: 17 [5888/50048]	Loss: 1.3521
Training Epoch: 17 [6016/50048]	Loss: 1.4020
Training Epoch: 17 [6144/50048]	Loss: 1.4379
Training Epoch: 17 [6272/50048]	Loss: 1.3134
Training Epoch: 17 [6400/50048]	Loss: 1.5114
Training Epoch: 17 [6528/50048]	Loss: 1.3577
Training Epoch: 17 [6656/50048]	Loss: 1.4867
Training Epoch: 17 [6784/50048]	Loss: 1.6172
Training Epoch: 17 [6912/50048]	Loss: 1.3197
Training Epoch: 17 [7040/50048]	Loss: 1.2867
Training Epoch: 17 [7168/50048]	Loss: 1.4315
Training Epoch: 17 [7296/50048]	Loss: 1.4308
Training Epoch: 17 [7424/50048]	Loss: 1.5550
Training Epoch: 17 [7552/50048]	Loss: 1.3761
Training Epoch: 17 [7680/50048]	Loss: 1.3403
Training Epoch: 17 [7808/50048]	Loss: 1.4342
Training Epoch: 17 [7936/50048]	Loss: 1.3288
Training Epoch: 17 [8064/50048]	Loss: 1.4103
Training Epoch: 17 [8192/50048]	Loss: 1.4643
Training Epoch: 17 [8320/50048]	Loss: 1.2987
Training Epoch: 17 [8448/50048]	Loss: 1.4422
Training Epoch: 17 [8576/50048]	Loss: 1.3686
Training Epoch: 17 [8704/50048]	Loss: 1.3431
Training Epoch: 17 [8832/50048]	Loss: 1.3385
Training Epoch: 17 [8960/50048]	Loss: 1.5260
Training Epoch: 17 [9088/50048]	Loss: 1.4038
Training Epoch: 17 [9216/50048]	Loss: 1.3561
Training Epoch: 17 [9344/50048]	Loss: 1.4134
Training Epoch: 17 [9472/50048]	Loss: 1.4856
Training Epoch: 17 [9600/50048]	Loss: 1.5305
Training Epoch: 17 [9728/50048]	Loss: 1.2208
Training Epoch: 17 [9856/50048]	Loss: 1.3856
Training Epoch: 17 [9984/50048]	Loss: 1.4888
Training Epoch: 17 [10112/50048]	Loss: 1.4441
Training Epoch: 17 [10240/50048]	Loss: 1.5299
Training Epoch: 17 [10368/50048]	Loss: 1.5136
Training Epoch: 17 [10496/50048]	Loss: 1.2268
Training Epoch: 17 [10624/50048]	Loss: 1.3009
Training Epoch: 17 [10752/50048]	Loss: 1.3770
Training Epoch: 17 [10880/50048]	Loss: 1.3876
Training Epoch: 17 [11008/50048]	Loss: 1.5121
Training Epoch: 17 [11136/50048]	Loss: 1.3838
Training Epoch: 17 [11264/50048]	Loss: 1.4073
Training Epoch: 17 [11392/50048]	Loss: 1.4605
Training Epoch: 17 [11520/50048]	Loss: 1.3401
Training Epoch: 17 [11648/50048]	Loss: 1.3016
Training Epoch: 17 [11776/50048]	Loss: 1.4681
Training Epoch: 17 [11904/50048]	Loss: 1.2616
Training Epoch: 17 [12032/50048]	Loss: 1.5265
Training Epoch: 17 [12160/50048]	Loss: 1.7269
Training Epoch: 17 [12288/50048]	Loss: 1.3284
Training Epoch: 17 [12416/50048]	Loss: 1.1999
Training Epoch: 17 [12544/50048]	Loss: 1.4026
Training Epoch: 17 [12672/50048]	Loss: 1.3001
Training Epoch: 17 [12800/50048]	Loss: 1.3076
Training Epoch: 17 [12928/50048]	Loss: 1.3158
Training Epoch: 17 [13056/50048]	Loss: 1.3965
Training Epoch: 17 [13184/50048]	Loss: 1.4432
Training Epoch: 17 [13312/50048]	Loss: 1.2800
Training Epoch: 17 [13440/50048]	Loss: 1.4377
Training Epoch: 17 [13568/50048]	Loss: 1.1918
Training Epoch: 17 [13696/50048]	Loss: 1.4279
Training Epoch: 17 [13824/50048]	Loss: 1.4078
Training Epoch: 17 [13952/50048]	Loss: 1.5395
Training Epoch: 17 [14080/50048]	Loss: 1.3352
Training Epoch: 17 [14208/50048]	Loss: 1.5422
Training Epoch: 17 [14336/50048]	Loss: 1.2632
Training Epoch: 17 [14464/50048]	Loss: 1.5558
Training Epoch: 17 [14592/50048]	Loss: 1.3060
Training Epoch: 17 [14720/50048]	Loss: 1.3368
Training Epoch: 17 [14848/50048]	Loss: 1.4794
Training Epoch: 17 [14976/50048]	Loss: 1.3503
Training Epoch: 17 [15104/50048]	Loss: 1.6850
Training Epoch: 17 [15232/50048]	Loss: 1.1926
Training Epoch: 17 [15360/50048]	Loss: 1.2940
Training Epoch: 17 [15488/50048]	Loss: 1.2149
Training Epoch: 17 [15616/50048]	Loss: 1.2678
Training Epoch: 17 [15744/50048]	Loss: 1.3777
Training Epoch: 17 [15872/50048]	Loss: 1.3162
Training Epoch: 17 [16000/50048]	Loss: 1.3382
Training Epoch: 17 [16128/50048]	Loss: 1.5318
Training Epoch: 17 [16256/50048]	Loss: 1.6715
Training Epoch: 17 [16384/50048]	Loss: 1.4372
Training Epoch: 17 [16512/50048]	Loss: 1.1479
Training Epoch: 17 [16640/50048]	Loss: 1.3134
Training Epoch: 17 [16768/50048]	Loss: 1.4230
Training Epoch: 17 [16896/50048]	Loss: 1.4653
Training Epoch: 17 [17024/50048]	Loss: 1.3474
Training Epoch: 17 [17152/50048]	Loss: 1.2103
Training Epoch: 17 [17280/50048]	Loss: 1.4231
Training Epoch: 17 [17408/50048]	Loss: 1.3042
Training Epoch: 17 [17536/50048]	Loss: 1.2194
Training Epoch: 17 [17664/50048]	Loss: 1.2487
Training Epoch: 17 [17792/50048]	Loss: 1.2913
Training Epoch: 17 [17920/50048]	Loss: 1.3837
Training Epoch: 17 [18048/50048]	Loss: 1.4451
Training Epoch: 17 [18176/50048]	Loss: 1.4657
Training Epoch: 17 [18304/50048]	Loss: 1.2551
Training Epoch: 17 [18432/50048]	Loss: 1.4406
Training Epoch: 17 [18560/50048]	Loss: 1.3070
Training Epoch: 17 [18688/50048]	Loss: 1.6374
Training Epoch: 17 [18816/50048]	Loss: 1.4807
Training Epoch: 17 [18944/50048]	Loss: 1.0503
Training Epoch: 17 [19072/50048]	Loss: 1.1927
Training Epoch: 17 [19200/50048]	Loss: 1.2553
Training Epoch: 17 [19328/50048]	Loss: 0.9776
Training Epoch: 17 [19456/50048]	Loss: 1.2928
Training Epoch: 17 [19584/50048]	Loss: 1.3763
Training Epoch: 17 [19712/50048]	Loss: 1.6295
Training Epoch: 17 [19840/50048]	Loss: 1.6167
Training Epoch: 17 [19968/50048]	Loss: 1.4329
Training Epoch: 17 [20096/50048]	Loss: 1.5458
Training Epoch: 17 [20224/50048]	Loss: 1.3798
Training Epoch: 17 [20352/50048]	Loss: 1.4987
Training Epoch: 17 [20480/50048]	Loss: 1.4189
Training Epoch: 17 [20608/50048]	Loss: 1.2637
Training Epoch: 17 [20736/50048]	Loss: 1.0661
Training Epoch: 17 [20864/50048]	Loss: 1.2849
Training Epoch: 17 [20992/50048]	Loss: 1.4869
Training Epoch: 17 [21120/50048]	Loss: 1.4256
Training Epoch: 17 [21248/50048]	Loss: 1.3555
Training Epoch: 17 [21376/50048]	Loss: 1.4720
Training Epoch: 17 [21504/50048]	Loss: 1.3123
Training Epoch: 17 [21632/50048]	Loss: 1.2811
Training Epoch: 17 [21760/50048]	Loss: 1.3292
Training Epoch: 17 [21888/50048]	Loss: 1.5936
Training Epoch: 17 [22016/50048]	Loss: 1.3825
Training Epoch: 17 [22144/50048]	Loss: 1.3323
Training Epoch: 17 [22272/50048]	Loss: 1.3289
Training Epoch: 17 [22400/50048]	Loss: 1.2597
Training Epoch: 17 [22528/50048]	Loss: 1.2099
Training Epoch: 17 [22656/50048]	Loss: 1.6879
Training Epoch: 17 [22784/50048]	Loss: 1.3921
Training Epoch: 17 [22912/50048]	Loss: 1.2324
Training Epoch: 17 [23040/50048]	Loss: 1.6108
Training Epoch: 17 [23168/50048]	Loss: 1.5045
Training Epoch: 17 [23296/50048]	Loss: 1.4005
Training Epoch: 17 [23424/50048]	Loss: 1.2701
Training Epoch: 17 [23552/50048]	Loss: 1.4470
Training Epoch: 17 [23680/50048]	Loss: 1.5114
Training Epoch: 17 [23808/50048]	Loss: 1.3311
Training Epoch: 17 [23936/50048]	Loss: 1.4172
Training Epoch: 17 [24064/50048]	Loss: 1.4268
Training Epoch: 17 [24192/50048]	Loss: 1.3460
Training Epoch: 17 [24320/50048]	Loss: 1.5585
Training Epoch: 17 [24448/50048]	Loss: 1.5167
Training Epoch: 17 [24576/50048]	Loss: 1.2877
Training Epoch: 17 [24704/50048]	Loss: 1.5542
Training Epoch: 17 [24832/50048]	Loss: 1.4529
Training Epoch: 17 [24960/50048]	Loss: 1.4039
Training Epoch: 17 [25088/50048]	Loss: 1.4332
Training Epoch: 17 [25216/50048]	Loss: 1.2218
Training Epoch: 17 [25344/50048]	Loss: 1.4135
Training Epoch: 17 [25472/50048]	Loss: 1.3659
Training Epoch: 17 [25600/50048]	Loss: 1.2658
Training Epoch: 17 [25728/50048]	Loss: 1.1998
Training Epoch: 17 [25856/50048]	Loss: 1.3892
Training Epoch: 17 [25984/50048]	Loss: 1.5262
Training Epoch: 17 [26112/50048]	Loss: 1.3178
Training Epoch: 17 [26240/50048]	Loss: 1.2848
Training Epoch: 17 [26368/50048]	Loss: 1.2329
Training Epoch: 17 [26496/50048]	Loss: 1.2561
Training Epoch: 17 [26624/50048]	Loss: 1.5330
Training Epoch: 17 [26752/50048]	Loss: 1.3720
Training Epoch: 17 [26880/50048]	Loss: 1.4621
Training Epoch: 17 [27008/50048]	Loss: 1.4618
Training Epoch: 17 [27136/50048]	Loss: 1.6753
Training Epoch: 17 [27264/50048]	Loss: 1.4396
Training Epoch: 17 [27392/50048]	Loss: 1.4136
Training Epoch: 17 [27520/50048]	Loss: 1.5443
Training Epoch: 17 [27648/50048]	Loss: 1.3301
Training Epoch: 17 [27776/50048]	Loss: 1.4633
Training Epoch: 17 [27904/50048]	Loss: 1.1849
Training Epoch: 17 [28032/50048]	Loss: 1.2577
Training Epoch: 17 [28160/50048]	Loss: 1.2428
Training Epoch: 17 [28288/50048]	Loss: 1.2617
Training Epoch: 17 [28416/50048]	Loss: 1.2056
Training Epoch: 17 [28544/50048]	Loss: 1.3936
Training Epoch: 17 [28672/50048]	Loss: 1.4173
Training Epoch: 17 [28800/50048]	Loss: 1.3871
Training Epoch: 17 [28928/50048]	Loss: 1.3523
Training Epoch: 17 [29056/50048]	Loss: 1.5213
Training Epoch: 17 [29184/50048]	Loss: 1.3622
Training Epoch: 17 [29312/50048]	Loss: 1.5982
Training Epoch: 17 [29440/50048]	Loss: 1.4374
Training Epoch: 17 [29568/50048]	Loss: 1.3416
Training Epoch: 17 [29696/50048]	Loss: 1.2772
Training Epoch: 17 [29824/50048]	Loss: 1.1459
Training Epoch: 17 [29952/50048]	Loss: 1.7818
Training Epoch: 17 [30080/50048]	Loss: 1.3455
Training Epoch: 17 [30208/50048]	Loss: 1.3951
Training Epoch: 17 [30336/50048]	Loss: 1.2774
Training Epoch: 17 [30464/50048]	Loss: 1.4377
Training Epoch: 17 [30592/50048]	Loss: 1.4693
Training Epoch: 17 [30720/50048]	Loss: 1.4856
Training Epoch: 17 [30848/50048]	Loss: 1.2704
Training Epoch: 17 [30976/50048]	Loss: 1.2514
Training Epoch: 17 [31104/50048]	Loss: 1.7106
Training Epoch: 17 [31232/50048]	Loss: 1.2969
Training Epoch: 17 [31360/50048]	Loss: 1.5943
Training Epoch: 17 [31488/50048]	Loss: 1.3838
Training Epoch: 17 [31616/50048]	Loss: 1.6298
Training Epoch: 17 [31744/50048]	Loss: 1.5938
Training Epoch: 17 [31872/50048]	Loss: 1.3172
Training Epoch: 17 [32000/50048]	Loss: 1.5253
Training Epoch: 17 [32128/50048]	Loss: 1.4977
Training Epoch: 17 [32256/50048]	Loss: 1.6291
Training Epoch: 17 [32384/50048]	Loss: 1.3264
Training Epoch: 17 [32512/50048]	Loss: 1.3641
Training Epoch: 17 [32640/50048]	Loss: 1.4788
Training Epoch: 17 [32768/50048]	Loss: 1.3884
Training Epoch: 17 [32896/50048]	Loss: 1.3428
Training Epoch: 17 [33024/50048]	Loss: 1.4956
Training Epoch: 17 [33152/50048]	Loss: 1.3439
Training Epoch: 17 [33280/50048]	Loss: 1.1460
Training Epoch: 17 [33408/50048]	Loss: 1.3806
Training Epoch: 17 [33536/50048]	Loss: 1.1851
Training Epoch: 17 [33664/50048]	Loss: 1.3952
Training Epoch: 17 [33792/50048]	Loss: 1.3059
Training Epoch: 17 [33920/50048]	Loss: 1.3976
Training Epoch: 17 [34048/50048]	Loss: 1.2502
Training Epoch: 17 [34176/50048]	Loss: 1.5932
Training Epoch: 17 [34304/50048]	Loss: 1.3238
Training Epoch: 17 [34432/50048]	Loss: 1.3276
Training Epoch: 17 [34560/50048]	Loss: 1.3010
Training Epoch: 17 [34688/50048]	Loss: 1.5832
Training Epoch: 17 [34816/50048]	Loss: 1.3000
Training Epoch: 17 [34944/50048]	Loss: 1.4484
Training Epoch: 17 [35072/50048]	Loss: 1.3412
Training Epoch: 17 [35200/50048]	Loss: 1.2055
Training Epoch: 17 [35328/50048]	Loss: 1.1954
Training Epoch: 17 [35456/50048]	Loss: 1.3675
Training Epoch: 17 [35584/50048]	Loss: 1.4955
Training Epoch: 17 [35712/50048]	Loss: 1.5886
Training Epoch: 17 [35840/50048]	Loss: 1.4489
Training Epoch: 17 [35968/50048]	Loss: 1.4326
Training Epoch: 17 [36096/50048]	Loss: 1.4707
Training Epoch: 17 [36224/50048]	Loss: 1.5255
Training Epoch: 17 [36352/50048]	Loss: 1.3643
Training Epoch: 17 [36480/50048]	Loss: 1.7577
Training Epoch: 17 [36608/50048]	Loss: 1.4125
Training Epoch: 17 [36736/50048]	Loss: 1.4235
Training Epoch: 17 [36864/50048]	Loss: 1.5640
Training Epoch: 17 [36992/50048]	Loss: 1.5850
Training Epoch: 17 [37120/50048]	Loss: 1.3459
Training Epoch: 17 [37248/50048]	Loss: 1.1957
Training Epoch: 17 [37376/50048]	Loss: 1.0755
Training Epoch: 17 [37504/50048]	Loss: 1.3669
Training Epoch: 17 [37632/50048]	Loss: 1.4064
Training Epoch: 17 [37760/50048]	Loss: 1.4414
Training Epoch: 17 [37888/50048]	Loss: 1.3935
Training Epoch: 17 [38016/50048]	Loss: 1.3164
Training Epoch: 17 [38144/50048]	Loss: 1.3486
Training Epoch: 17 [38272/50048]	Loss: 1.4247
Training Epoch: 17 [38400/50048]	Loss: 1.6170
Training Epoch: 17 [38528/50048]	Loss: 1.3834
Training Epoch: 17 [38656/50048]	Loss: 1.0592
Training Epoch: 17 [38784/50048]	Loss: 1.2355
Training Epoch: 17 [38912/50048]	Loss: 1.2957
Training Epoch: 17 [39040/50048]	Loss: 1.4262
Training Epoch: 17 [39168/50048]	Loss: 1.3041
Training Epoch: 17 [39296/50048]	Loss: 1.1130
Training Epoch: 17 [39424/50048]	Loss: 1.5178
Training Epoch: 17 [39552/50048]	Loss: 1.4199
Training Epoch: 17 [39680/50048]	Loss: 1.3918
Training Epoch: 17 [39808/50048]	Loss: 1.5584
Training Epoch: 17 [39936/50048]	Loss: 1.5245
Training Epoch: 17 [40064/50048]	Loss: 1.3433
Training Epoch: 17 [40192/50048]	Loss: 1.2435
Training Epoch: 17 [40320/50048]	Loss: 1.4839
Training Epoch: 17 [40448/50048]	Loss: 1.1417
Training Epoch: 17 [40576/50048]	Loss: 1.2032
Training Epoch: 17 [40704/50048]	Loss: 1.4449
Training Epoch: 17 [40832/50048]	Loss: 1.1733
Training Epoch: 17 [40960/50048]	Loss: 1.4112
Training Epoch: 17 [41088/50048]	Loss: 1.5854
Training Epoch: 17 [41216/50048]	Loss: 1.3003
Training Epoch: 17 [41344/50048]	Loss: 1.4278
Training Epoch: 17 [41472/50048]	Loss: 1.5857
Training Epoch: 17 [41600/50048]	Loss: 1.3116
Training Epoch: 17 [41728/50048]	Loss: 1.5417
Training Epoch: 17 [41856/50048]	Loss: 1.3468
Training Epoch: 17 [41984/50048]	Loss: 1.4097
Training Epoch: 17 [42112/50048]	Loss: 1.3046
Training Epoch: 17 [42240/50048]	Loss: 1.2191
Training Epoch: 17 [42368/50048]	Loss: 1.4063
Training Epoch: 17 [42496/50048]	Loss: 1.3706
Training Epoch: 17 [42624/50048]	Loss: 1.2862
Training Epoch: 17 [42752/50048]	Loss: 1.5759
Training Epoch: 17 [42880/50048]	Loss: 1.2919
Training Epoch: 17 [43008/50048]	Loss: 1.4651
Training Epoch: 17 [43136/50048]	Loss: 1.4062
Training Epoch: 17 [43264/50048]	Loss: 1.3195
Training Epoch: 17 [43392/50048]	Loss: 1.6624
Training Epoch: 17 [43520/50048]	Loss: 1.6687
Training Epoch: 17 [43648/50048]	Loss: 1.4771
Training Epoch: 17 [43776/50048]	Loss: 1.4294
Training Epoch: 17 [43904/50048]	Loss: 1.4555
Training Epoch: 17 [44032/50048]	Loss: 1.3612
Training Epoch: 17 [44160/50048]	Loss: 1.5140
Training Epoch: 17 [44288/50048]	Loss: 1.4482
Training Epoch: 17 [44416/50048]	Loss: 1.6269
Training Epoch: 17 [44544/50048]	Loss: 1.5573
Training Epoch: 17 [44672/50048]	Loss: 1.3340
Training Epoch: 17 [44800/50048]	Loss: 1.4915
Training Epoch: 17 [44928/50048]	Loss: 1.6771
Training Epoch: 17 [45056/50048]	Loss: 1.3032
Training Epoch: 17 [45184/50048]	Loss: 1.3137
Training Epoch: 17 [45312/50048]	Loss: 1.2210
Training Epoch: 17 [45440/50048]	Loss: 1.3358
Training Epoch: 17 [45568/50048]	Loss: 1.1607
Training Epoch: 17 [45696/50048]	Loss: 1.2717
Training Epoch: 17 [45824/50048]	Loss: 1.6799
Training Epoch: 17 [45952/50048]	Loss: 1.3639
Training Epoch: 17 [46080/50048]	Loss: 1.3851
Training Epoch: 17 [46208/50048]	Loss: 1.5171
Training Epoch: 17 [46336/50048]	Loss: 1.1212
Training Epoch: 17 [46464/50048]	Loss: 1.1790
Training Epoch: 17 [46592/50048]	Loss: 1.3386
Training Epoch: 17 [46720/50048]	Loss: 1.5998
Training Epoch: 17 [46848/50048]	Loss: 1.4751
Training Epoch: 17 [46976/50048]	Loss: 1.3524
Training Epoch: 17 [47104/50048]	Loss: 1.3771
Training Epoch: 17 [47232/50048]	Loss: 1.3332
Training Epoch: 17 [47360/50048]	Loss: 1.3295
Training Epoch: 17 [47488/50048]	Loss: 1.4498
Training Epoch: 17 [47616/50048]	Loss: 1.2790
Training Epoch: 17 [47744/50048]	Loss: 1.2532
Training Epoch: 17 [47872/50048]	Loss: 1.2807
Training Epoch: 17 [48000/50048]	Loss: 1.4540
Training Epoch: 17 [48128/50048]	Loss: 1.2122
Training Epoch: 17 [48256/50048]	Loss: 1.5547
Training Epoch: 17 [48384/50048]	Loss: 1.4422
Training Epoch: 17 [48512/50048]	Loss: 1.3193
Training Epoch: 17 [48640/50048]	Loss: 1.2350
Training Epoch: 17 [48768/50048]	Loss: 1.3387
Training Epoch: 17 [48896/50048]	Loss: 1.5137
Training Epoch: 17 [49024/50048]	Loss: 1.1959
Training Epoch: 17 [49152/50048]	Loss: 1.4633
Training Epoch: 17 [49280/50048]	Loss: 1.6324
Training Epoch: 17 [49408/50048]	Loss: 1.3764
Training Epoch: 17 [49536/50048]	Loss: 1.1916
Training Epoch: 17 [49664/50048]	Loss: 1.4589
Training Epoch: 17 [49792/50048]	Loss: 1.2198
Training Epoch: 17 [49920/50048]	Loss: 1.2102
Training Epoch: 17 [50048/50048]	Loss: 1.1056
Validation Epoch: 17, Average loss: 0.0124, Accuracy: 0.5655
Training Epoch: 18 [128/50048]	Loss: 1.4034
Training Epoch: 18 [256/50048]	Loss: 1.2679
Training Epoch: 18 [384/50048]	Loss: 1.3795
Training Epoch: 18 [512/50048]	Loss: 1.4684
Training Epoch: 18 [640/50048]	Loss: 1.3501
Training Epoch: 18 [768/50048]	Loss: 1.2471
Training Epoch: 18 [896/50048]	Loss: 1.1000
Training Epoch: 18 [1024/50048]	Loss: 1.2845
Training Epoch: 18 [1152/50048]	Loss: 1.4172
Training Epoch: 18 [1280/50048]	Loss: 1.2084
Training Epoch: 18 [1408/50048]	Loss: 1.1751
Training Epoch: 18 [1536/50048]	Loss: 1.3659
Training Epoch: 18 [1664/50048]	Loss: 1.0998
Training Epoch: 18 [1792/50048]	Loss: 1.2726
Training Epoch: 18 [1920/50048]	Loss: 1.1062
Training Epoch: 18 [2048/50048]	Loss: 1.2832
Training Epoch: 18 [2176/50048]	Loss: 1.2388
Training Epoch: 18 [2304/50048]	Loss: 1.4791
Training Epoch: 18 [2432/50048]	Loss: 1.6069
Training Epoch: 18 [2560/50048]	Loss: 1.5649
Training Epoch: 18 [2688/50048]	Loss: 1.3278
Training Epoch: 18 [2816/50048]	Loss: 1.2250
Training Epoch: 18 [2944/50048]	Loss: 1.2685
Training Epoch: 18 [3072/50048]	Loss: 1.4119
Training Epoch: 18 [3200/50048]	Loss: 1.5350
Training Epoch: 18 [3328/50048]	Loss: 1.3464
Training Epoch: 18 [3456/50048]	Loss: 1.5568
Training Epoch: 18 [3584/50048]	Loss: 1.6614
Training Epoch: 18 [3712/50048]	Loss: 1.2895
Training Epoch: 18 [3840/50048]	Loss: 1.2378
Training Epoch: 18 [3968/50048]	Loss: 1.4871
Training Epoch: 18 [4096/50048]	Loss: 1.1849
Training Epoch: 18 [4224/50048]	Loss: 1.2020
Training Epoch: 18 [4352/50048]	Loss: 0.9764
Training Epoch: 18 [4480/50048]	Loss: 1.4320
Training Epoch: 18 [4608/50048]	Loss: 1.3910
Training Epoch: 18 [4736/50048]	Loss: 1.3223
Training Epoch: 18 [4864/50048]	Loss: 1.5114
Training Epoch: 18 [4992/50048]	Loss: 1.2130
Training Epoch: 18 [5120/50048]	Loss: 1.1966
Training Epoch: 18 [5248/50048]	Loss: 1.3927
Training Epoch: 18 [5376/50048]	Loss: 1.2681
Training Epoch: 18 [5504/50048]	Loss: 1.4132
Training Epoch: 18 [5632/50048]	Loss: 1.4456
Training Epoch: 18 [5760/50048]	Loss: 1.4129
Training Epoch: 18 [5888/50048]	Loss: 1.4841
Training Epoch: 18 [6016/50048]	Loss: 1.3604
Training Epoch: 18 [6144/50048]	Loss: 1.5266
Training Epoch: 18 [6272/50048]	Loss: 1.2458
Training Epoch: 18 [6400/50048]	Loss: 1.5257
Training Epoch: 18 [6528/50048]	Loss: 1.2317
Training Epoch: 18 [6656/50048]	Loss: 1.2802
Training Epoch: 18 [6784/50048]	Loss: 1.4123
Training Epoch: 18 [6912/50048]	Loss: 1.0933
Training Epoch: 18 [7040/50048]	Loss: 1.3418
Training Epoch: 18 [7168/50048]	Loss: 1.3050
Training Epoch: 18 [7296/50048]	Loss: 1.2531
Training Epoch: 18 [7424/50048]	Loss: 1.3812
Training Epoch: 18 [7552/50048]	Loss: 1.4766
Training Epoch: 18 [7680/50048]	Loss: 1.3008
Training Epoch: 18 [7808/50048]	Loss: 1.2990
Training Epoch: 18 [7936/50048]	Loss: 1.2691
Training Epoch: 18 [8064/50048]	Loss: 1.3089
Training Epoch: 18 [8192/50048]	Loss: 1.5767
Training Epoch: 18 [8320/50048]	Loss: 1.2716
Training Epoch: 18 [8448/50048]	Loss: 1.3223
Training Epoch: 18 [8576/50048]	Loss: 1.2247
Training Epoch: 18 [8704/50048]	Loss: 1.6542
Training Epoch: 18 [8832/50048]	Loss: 1.4123
Training Epoch: 18 [8960/50048]	Loss: 1.4472
Training Epoch: 18 [9088/50048]	Loss: 1.1709
Training Epoch: 18 [9216/50048]	Loss: 1.2380
Training Epoch: 18 [9344/50048]	Loss: 1.2434
Training Epoch: 18 [9472/50048]	Loss: 1.2765
Training Epoch: 18 [9600/50048]	Loss: 1.3676
Training Epoch: 18 [9728/50048]	Loss: 1.2937
Training Epoch: 18 [9856/50048]	Loss: 1.3149
Training Epoch: 18 [9984/50048]	Loss: 1.6195
Training Epoch: 18 [10112/50048]	Loss: 1.1252
Training Epoch: 18 [10240/50048]	Loss: 1.4668
Training Epoch: 18 [10368/50048]	Loss: 1.4188
Training Epoch: 18 [10496/50048]	Loss: 1.3991
Training Epoch: 18 [10624/50048]	Loss: 1.3643
Training Epoch: 18 [10752/50048]	Loss: 1.2096
Training Epoch: 18 [10880/50048]	Loss: 1.2836
Training Epoch: 18 [11008/50048]	Loss: 1.3014
Training Epoch: 18 [11136/50048]	Loss: 1.6069
Training Epoch: 18 [11264/50048]	Loss: 1.1244
Training Epoch: 18 [11392/50048]	Loss: 1.4295
Training Epoch: 18 [11520/50048]	Loss: 1.2453
Training Epoch: 18 [11648/50048]	Loss: 1.4029
Training Epoch: 18 [11776/50048]	Loss: 1.3802
Training Epoch: 18 [11904/50048]	Loss: 1.6268
Training Epoch: 18 [12032/50048]	Loss: 1.1684
Training Epoch: 18 [12160/50048]	Loss: 1.5684
Training Epoch: 18 [12288/50048]	Loss: 1.5854
Training Epoch: 18 [12416/50048]	Loss: 1.4240
Training Epoch: 18 [12544/50048]	Loss: 1.4613
Training Epoch: 18 [12672/50048]	Loss: 1.3914
Training Epoch: 18 [12800/50048]	Loss: 1.5360
Training Epoch: 18 [12928/50048]	Loss: 1.3162
Training Epoch: 18 [13056/50048]	Loss: 1.3379
Training Epoch: 18 [13184/50048]	Loss: 1.3587
Training Epoch: 18 [13312/50048]	Loss: 1.3136
Training Epoch: 18 [13440/50048]	Loss: 1.2355
Training Epoch: 18 [13568/50048]	Loss: 1.1888
Training Epoch: 18 [13696/50048]	Loss: 1.5435
Training Epoch: 18 [13824/50048]	Loss: 1.4848
Training Epoch: 18 [13952/50048]	Loss: 1.3686
Training Epoch: 18 [14080/50048]	Loss: 1.3231
Training Epoch: 18 [14208/50048]	Loss: 1.6262
Training Epoch: 18 [14336/50048]	Loss: 1.2979
Training Epoch: 18 [14464/50048]	Loss: 1.3195
Training Epoch: 18 [14592/50048]	Loss: 1.4882
Training Epoch: 18 [14720/50048]	Loss: 1.5076
Training Epoch: 18 [14848/50048]	Loss: 1.0653
Training Epoch: 18 [14976/50048]	Loss: 1.3972
Training Epoch: 18 [15104/50048]	Loss: 1.4668
Training Epoch: 18 [15232/50048]	Loss: 1.2626
Training Epoch: 18 [15360/50048]	Loss: 1.1522
Training Epoch: 18 [15488/50048]	Loss: 1.2690
Training Epoch: 18 [15616/50048]	Loss: 1.4081
Training Epoch: 18 [15744/50048]	Loss: 1.4402
Training Epoch: 18 [15872/50048]	Loss: 1.4351
Training Epoch: 18 [16000/50048]	Loss: 1.5143
Training Epoch: 18 [16128/50048]	Loss: 1.2776
Training Epoch: 18 [16256/50048]	Loss: 1.2654
Training Epoch: 18 [16384/50048]	Loss: 1.2641
Training Epoch: 18 [16512/50048]	Loss: 1.4857
Training Epoch: 18 [16640/50048]	Loss: 1.4328
Training Epoch: 18 [16768/50048]	Loss: 1.1910
Training Epoch: 18 [16896/50048]	Loss: 1.4154
Training Epoch: 18 [17024/50048]	Loss: 1.4840
Training Epoch: 18 [17152/50048]	Loss: 1.4645
Training Epoch: 18 [17280/50048]	Loss: 1.7514
Training Epoch: 18 [17408/50048]	Loss: 1.2938
Training Epoch: 18 [17536/50048]	Loss: 1.2746
Training Epoch: 18 [17664/50048]	Loss: 1.1352
Training Epoch: 18 [17792/50048]	Loss: 1.2246
Training Epoch: 18 [17920/50048]	Loss: 1.3122
Training Epoch: 18 [18048/50048]	Loss: 1.3300
Training Epoch: 18 [18176/50048]	Loss: 1.1981
Training Epoch: 18 [18304/50048]	Loss: 1.3295
Training Epoch: 18 [18432/50048]	Loss: 1.5491
Training Epoch: 18 [18560/50048]	Loss: 1.5985
Training Epoch: 18 [18688/50048]	Loss: 1.2732
Training Epoch: 18 [18816/50048]	Loss: 1.3258
Training Epoch: 18 [18944/50048]	Loss: 1.2351
Training Epoch: 18 [19072/50048]	Loss: 1.3335
Training Epoch: 18 [19200/50048]	Loss: 1.3128
Training Epoch: 18 [19328/50048]	Loss: 1.3570
Training Epoch: 18 [19456/50048]	Loss: 1.3310
Training Epoch: 18 [19584/50048]	Loss: 1.3925
Training Epoch: 18 [19712/50048]	Loss: 1.1930
Training Epoch: 18 [19840/50048]	Loss: 1.2291
Training Epoch: 18 [19968/50048]	Loss: 1.4204
Training Epoch: 18 [20096/50048]	Loss: 1.3047
Training Epoch: 18 [20224/50048]	Loss: 1.2975
Training Epoch: 18 [20352/50048]	Loss: 1.6366
Training Epoch: 18 [20480/50048]	Loss: 1.4280
Training Epoch: 18 [20608/50048]	Loss: 1.3223
Training Epoch: 18 [20736/50048]	Loss: 1.5697
Training Epoch: 18 [20864/50048]	Loss: 1.4385
Training Epoch: 18 [20992/50048]	Loss: 1.4100
Training Epoch: 18 [21120/50048]	Loss: 1.2464
Training Epoch: 18 [21248/50048]	Loss: 1.7139
Training Epoch: 18 [21376/50048]	Loss: 1.2072
Training Epoch: 18 [21504/50048]	Loss: 1.3464
Training Epoch: 18 [21632/50048]	Loss: 1.3991
Training Epoch: 18 [21760/50048]	Loss: 1.4783
Training Epoch: 18 [21888/50048]	Loss: 1.1876
Training Epoch: 18 [22016/50048]	Loss: 1.2931
Training Epoch: 18 [22144/50048]	Loss: 1.4032
Training Epoch: 18 [22272/50048]	Loss: 1.4058
Training Epoch: 18 [22400/50048]	Loss: 1.5399
Training Epoch: 18 [22528/50048]	Loss: 1.6398
Training Epoch: 18 [22656/50048]	Loss: 1.3909
Training Epoch: 18 [22784/50048]	Loss: 1.5175
Training Epoch: 18 [22912/50048]	Loss: 1.2590
Training Epoch: 18 [23040/50048]	Loss: 1.3828
Training Epoch: 18 [23168/50048]	Loss: 1.2556
Training Epoch: 18 [23296/50048]	Loss: 1.1927
Training Epoch: 18 [23424/50048]	Loss: 1.2992
Training Epoch: 18 [23552/50048]	Loss: 1.2923
Training Epoch: 18 [23680/50048]	Loss: 1.2663
Training Epoch: 18 [23808/50048]	Loss: 1.1994
Training Epoch: 18 [23936/50048]	Loss: 1.2034
Training Epoch: 18 [24064/50048]	Loss: 1.1210
Training Epoch: 18 [24192/50048]	Loss: 1.4444
Training Epoch: 18 [24320/50048]	Loss: 1.3890
Training Epoch: 18 [24448/50048]	Loss: 1.3621
Training Epoch: 18 [24576/50048]	Loss: 1.4448
Training Epoch: 18 [24704/50048]	Loss: 1.1141
Training Epoch: 18 [24832/50048]	Loss: 1.4654
Training Epoch: 18 [24960/50048]	Loss: 1.6099
Training Epoch: 18 [25088/50048]	Loss: 1.2907
Training Epoch: 18 [25216/50048]	Loss: 1.2080
Training Epoch: 18 [25344/50048]	Loss: 1.3191
Training Epoch: 18 [25472/50048]	Loss: 1.4742
Training Epoch: 18 [25600/50048]	Loss: 1.4300
Training Epoch: 18 [25728/50048]	Loss: 1.3472
Training Epoch: 18 [25856/50048]	Loss: 1.4050
Training Epoch: 18 [25984/50048]	Loss: 1.4456
Training Epoch: 18 [26112/50048]	Loss: 1.5059
Training Epoch: 18 [26240/50048]	Loss: 1.0300
Training Epoch: 18 [26368/50048]	Loss: 1.3118
Training Epoch: 18 [26496/50048]	Loss: 1.2823
Training Epoch: 18 [26624/50048]	Loss: 1.2074
Training Epoch: 18 [26752/50048]	Loss: 1.7399
Training Epoch: 18 [26880/50048]	Loss: 1.3096
Training Epoch: 18 [27008/50048]	Loss: 1.4653
Training Epoch: 18 [27136/50048]	Loss: 1.5204
Training Epoch: 18 [27264/50048]	Loss: 1.6950
Training Epoch: 18 [27392/50048]	Loss: 1.1250
Training Epoch: 18 [27520/50048]	Loss: 1.3240
Training Epoch: 18 [27648/50048]	Loss: 1.2449
Training Epoch: 18 [27776/50048]	Loss: 1.3964
Training Epoch: 18 [27904/50048]	Loss: 1.3449
Training Epoch: 18 [28032/50048]	Loss: 1.4888
Training Epoch: 18 [28160/50048]	Loss: 1.4362
Training Epoch: 18 [28288/50048]	Loss: 1.3314
Training Epoch: 18 [28416/50048]	Loss: 1.2159
Training Epoch: 18 [28544/50048]	Loss: 1.4088
Training Epoch: 18 [28672/50048]	Loss: 1.2999
Training Epoch: 18 [28800/50048]	Loss: 1.4564
Training Epoch: 18 [28928/50048]	Loss: 1.4475
Training Epoch: 18 [29056/50048]	Loss: 1.3963
Training Epoch: 18 [29184/50048]	Loss: 1.3325
Training Epoch: 18 [29312/50048]	Loss: 1.1371
Training Epoch: 18 [29440/50048]	Loss: 1.6118
Training Epoch: 18 [29568/50048]	Loss: 1.3163
Training Epoch: 18 [29696/50048]	Loss: 1.1311
Training Epoch: 18 [29824/50048]	Loss: 1.3097
Training Epoch: 18 [29952/50048]	Loss: 1.3674
Training Epoch: 18 [30080/50048]	Loss: 1.2817
Training Epoch: 18 [30208/50048]	Loss: 1.0262
Training Epoch: 18 [30336/50048]	Loss: 1.3156
Training Epoch: 18 [30464/50048]	Loss: 1.3474
Training Epoch: 18 [30592/50048]	Loss: 1.3059
Training Epoch: 18 [30720/50048]	Loss: 1.2511
Training Epoch: 18 [30848/50048]	Loss: 1.4748
Training Epoch: 18 [30976/50048]	Loss: 1.4154
Training Epoch: 18 [31104/50048]	Loss: 1.3327
Training Epoch: 18 [31232/50048]	Loss: 1.2482
Training Epoch: 18 [31360/50048]	Loss: 1.2564
Training Epoch: 18 [31488/50048]	Loss: 1.4602
Training Epoch: 18 [31616/50048]	Loss: 1.2478
Training Epoch: 18 [31744/50048]	Loss: 1.4185
Training Epoch: 18 [31872/50048]	Loss: 1.1968
Training Epoch: 18 [32000/50048]	Loss: 1.7812
Training Epoch: 18 [32128/50048]	Loss: 1.3693
Training Epoch: 18 [32256/50048]	Loss: 1.5445
Training Epoch: 18 [32384/50048]	Loss: 1.5955
Training Epoch: 18 [32512/50048]	Loss: 1.4498
Training Epoch: 18 [32640/50048]	Loss: 1.3998
Training Epoch: 18 [32768/50048]	Loss: 1.1718
Training Epoch: 18 [32896/50048]	Loss: 1.2827
Training Epoch: 18 [33024/50048]	Loss: 1.4564
Training Epoch: 18 [33152/50048]	Loss: 1.5096
Training Epoch: 18 [33280/50048]	Loss: 1.3671
Training Epoch: 18 [33408/50048]	Loss: 1.1826
Training Epoch: 18 [33536/50048]	Loss: 1.3679
Training Epoch: 18 [33664/50048]	Loss: 1.2984
Training Epoch: 18 [33792/50048]	Loss: 1.1457
Training Epoch: 18 [33920/50048]	Loss: 1.4393
Training Epoch: 18 [34048/50048]	Loss: 1.4251
Training Epoch: 18 [34176/50048]	Loss: 1.6241
Training Epoch: 18 [34304/50048]	Loss: 1.3972
Training Epoch: 18 [34432/50048]	Loss: 1.3418
Training Epoch: 18 [34560/50048]	Loss: 1.1682
Training Epoch: 18 [34688/50048]	Loss: 1.3997
Training Epoch: 18 [34816/50048]	Loss: 1.5531
Training Epoch: 18 [34944/50048]	Loss: 1.4098
Training Epoch: 18 [35072/50048]	Loss: 1.2229
Training Epoch: 18 [35200/50048]	Loss: 1.3267
Training Epoch: 18 [35328/50048]	Loss: 1.3989
Training Epoch: 18 [35456/50048]	Loss: 1.4434
Training Epoch: 18 [35584/50048]	Loss: 1.4306
Training Epoch: 18 [35712/50048]	Loss: 1.3218
Training Epoch: 18 [35840/50048]	Loss: 1.2129
Training Epoch: 18 [35968/50048]	Loss: 1.4027
Training Epoch: 18 [36096/50048]	Loss: 1.2356
Training Epoch: 18 [36224/50048]	Loss: 1.2298
Training Epoch: 18 [36352/50048]	Loss: 1.4727
Training Epoch: 18 [36480/50048]	Loss: 1.3149
Training Epoch: 18 [36608/50048]	Loss: 1.3742
Training Epoch: 18 [36736/50048]	Loss: 1.5577
Training Epoch: 18 [36864/50048]	Loss: 1.4549
Training Epoch: 18 [36992/50048]	Loss: 1.2586
Training Epoch: 18 [37120/50048]	Loss: 1.4404
Training Epoch: 18 [37248/50048]	Loss: 1.1991
Training Epoch: 18 [37376/50048]	Loss: 1.3406
Training Epoch: 18 [37504/50048]	Loss: 1.5967
Training Epoch: 18 [37632/50048]	Loss: 1.2526
Training Epoch: 18 [37760/50048]	Loss: 1.3689
Training Epoch: 18 [37888/50048]	Loss: 1.3967
Training Epoch: 18 [38016/50048]	Loss: 1.4073
Training Epoch: 18 [38144/50048]	Loss: 1.2932
Training Epoch: 18 [38272/50048]	Loss: 1.3683
Training Epoch: 18 [38400/50048]	Loss: 1.3378
Training Epoch: 18 [38528/50048]	Loss: 1.3268
Training Epoch: 18 [38656/50048]	Loss: 1.4802
Training Epoch: 18 [38784/50048]	Loss: 1.3285
Training Epoch: 18 [38912/50048]	Loss: 1.0086
Training Epoch: 18 [39040/50048]	Loss: 1.5416
Training Epoch: 18 [39168/50048]	Loss: 1.5004
Training Epoch: 18 [39296/50048]	Loss: 1.3437
Training Epoch: 18 [39424/50048]	Loss: 1.3134
Training Epoch: 18 [39552/50048]	Loss: 1.5294
Training Epoch: 18 [39680/50048]	Loss: 1.4000
Training Epoch: 18 [39808/50048]	Loss: 1.2477
Training Epoch: 18 [39936/50048]	Loss: 1.3753
Training Epoch: 18 [40064/50048]	Loss: 1.5909
Training Epoch: 18 [40192/50048]	Loss: 1.4008
Training Epoch: 18 [40320/50048]	Loss: 1.1466
Training Epoch: 18 [40448/50048]	Loss: 1.3059
Training Epoch: 18 [40576/50048]	Loss: 1.3798
Training Epoch: 18 [40704/50048]	Loss: 1.4876
Training Epoch: 18 [40832/50048]	Loss: 1.6627
Training Epoch: 18 [40960/50048]	Loss: 1.3019
Training Epoch: 18 [41088/50048]	Loss: 1.3306
Training Epoch: 18 [41216/50048]	Loss: 1.2316
Training Epoch: 18 [41344/50048]	Loss: 1.4368
Training Epoch: 18 [41472/50048]	Loss: 1.2903
Training Epoch: 18 [41600/50048]	Loss: 1.3032
Training Epoch: 18 [41728/50048]	Loss: 1.5194
Training Epoch: 18 [41856/50048]	Loss: 1.5045
Training Epoch: 18 [41984/50048]	Loss: 1.4821
Training Epoch: 18 [42112/50048]	Loss: 1.4695
Training Epoch: 18 [42240/50048]	Loss: 1.3904
Training Epoch: 18 [42368/50048]	Loss: 1.2485
Training Epoch: 18 [42496/50048]	Loss: 1.4397
Training Epoch: 18 [42624/50048]	Loss: 1.5264
Training Epoch: 18 [42752/50048]	Loss: 1.3761
Training Epoch: 18 [42880/50048]	Loss: 1.1813
Training Epoch: 18 [43008/50048]	Loss: 1.3657
Training Epoch: 18 [43136/50048]	Loss: 1.3509
Training Epoch: 18 [43264/50048]	Loss: 1.3756
Training Epoch: 18 [43392/50048]	Loss: 1.3939
Training Epoch: 18 [43520/50048]	Loss: 1.5704
Training Epoch: 18 [43648/50048]	Loss: 1.2980
Training Epoch: 18 [43776/50048]	Loss: 1.4009
Training Epoch: 18 [43904/50048]	Loss: 1.4584
Training Epoch: 18 [44032/50048]	Loss: 1.2391
Training Epoch: 18 [44160/50048]	Loss: 1.3317
Training Epoch: 18 [44288/50048]	Loss: 1.4914
Training Epoch: 18 [44416/50048]	Loss: 1.4999
Training Epoch: 18 [44544/50048]	Loss: 1.7331
Training Epoch: 18 [44672/50048]	Loss: 1.1763
Training Epoch: 18 [44800/50048]	Loss: 1.3034
Training Epoch: 18 [44928/50048]	Loss: 1.3274
Training Epoch: 18 [45056/50048]	Loss: 1.3439
Training Epoch: 18 [45184/50048]	Loss: 1.4115
Training Epoch: 18 [45312/50048]	Loss: 1.6625
Training Epoch: 18 [45440/50048]	Loss: 1.1477
Training Epoch: 18 [45568/50048]	Loss: 1.3331
Training Epoch: 18 [45696/50048]	Loss: 1.0242
Training Epoch: 18 [45824/50048]	Loss: 1.2823
Training Epoch: 18 [45952/50048]	Loss: 1.0468
Training Epoch: 18 [46080/50048]	Loss: 1.2134
Training Epoch: 18 [46208/50048]	Loss: 1.3211
Training Epoch: 18 [46336/50048]	Loss: 1.2941
Training Epoch: 18 [46464/50048]	Loss: 1.2793
Training Epoch: 18 [46592/50048]	Loss: 1.3027
Training Epoch: 18 [46720/50048]	Loss: 1.3401
Training Epoch: 18 [46848/50048]	Loss: 1.3884
Training Epoch: 18 [46976/50048]	Loss: 1.4936
Training Epoch: 18 [47104/50048]	Loss: 1.5974
Training Epoch: 18 [47232/50048]	Loss: 1.4414
Training Epoch: 18 [47360/50048]	Loss: 1.6113
Training Epoch: 18 [47488/50048]	Loss: 1.1850
Training Epoch: 18 [47616/50048]	Loss: 1.1884
Training Epoch: 18 [47744/50048]	Loss: 1.5658
Training Epoch: 18 [47872/50048]	Loss: 1.2773
Training Epoch: 18 [48000/50048]	Loss: 1.4932
Training Epoch: 18 [48128/50048]	Loss: 1.3027
Training Epoch: 18 [48256/50048]	Loss: 1.2273
Training Epoch: 18 [48384/50048]	Loss: 1.4209
Training Epoch: 18 [48512/50048]	Loss: 1.5933
Training Epoch: 18 [48640/50048]	Loss: 1.4083
Training Epoch: 18 [48768/50048]	Loss: 1.2081
Training Epoch: 18 [48896/50048]	Loss: 1.3730
Training Epoch: 18 [49024/50048]	Loss: 1.1909
Training Epoch: 18 [49152/50048]	Loss: 1.5319
Training Epoch: 18 [49280/50048]	Loss: 1.4279
Training Epoch: 18 [49408/50048]	Loss: 1.5278
Training Epoch: 18 [49536/50048]	Loss: 1.1859
Training Epoch: 18 [49664/50048]	Loss: 1.4977
Training Epoch: 18 [49792/50048]	Loss: 1.4278
Training Epoch: 18 [49920/50048]	Loss: 1.3945
Training Epoch: 18 [50048/50048]	Loss: 0.9862
Validation Epoch: 18, Average loss: 0.0123, Accuracy: 0.5702
Training Epoch: 19 [128/50048]	Loss: 1.4768
Training Epoch: 19 [256/50048]	Loss: 1.2890
Training Epoch: 19 [384/50048]	Loss: 1.4047
Training Epoch: 19 [512/50048]	Loss: 1.0477
Training Epoch: 19 [640/50048]	Loss: 1.3190
Training Epoch: 19 [768/50048]	Loss: 1.4299
Training Epoch: 19 [896/50048]	Loss: 1.4216
Training Epoch: 19 [1024/50048]	Loss: 1.5299
Training Epoch: 19 [1152/50048]	Loss: 1.1834
Training Epoch: 19 [1280/50048]	Loss: 1.1221
Training Epoch: 19 [1408/50048]	Loss: 1.3021
Training Epoch: 19 [1536/50048]	Loss: 1.3538
Training Epoch: 19 [1664/50048]	Loss: 1.3317
Training Epoch: 19 [1792/50048]	Loss: 1.4542
Training Epoch: 19 [1920/50048]	Loss: 1.0319
Training Epoch: 19 [2048/50048]	Loss: 1.2599
Training Epoch: 19 [2176/50048]	Loss: 1.2746
Training Epoch: 19 [2304/50048]	Loss: 1.3784
Training Epoch: 19 [2432/50048]	Loss: 1.4318
Training Epoch: 19 [2560/50048]	Loss: 1.2429
Training Epoch: 19 [2688/50048]	Loss: 1.2418
Training Epoch: 19 [2816/50048]	Loss: 1.2075
Training Epoch: 19 [2944/50048]	Loss: 1.2902
Training Epoch: 19 [3072/50048]	Loss: 1.4965
Training Epoch: 19 [3200/50048]	Loss: 1.2686
Training Epoch: 19 [3328/50048]	Loss: 1.5682
Training Epoch: 19 [3456/50048]	Loss: 1.4482
Training Epoch: 19 [3584/50048]	Loss: 1.1227
Training Epoch: 19 [3712/50048]	Loss: 1.2340
Training Epoch: 19 [3840/50048]	Loss: 1.0416
Training Epoch: 19 [3968/50048]	Loss: 1.3099
Training Epoch: 19 [4096/50048]	Loss: 1.2764
Training Epoch: 19 [4224/50048]	Loss: 1.1151
Training Epoch: 19 [4352/50048]	Loss: 1.1792
Training Epoch: 19 [4480/50048]	Loss: 1.1084
Training Epoch: 19 [4608/50048]	Loss: 1.1241
Training Epoch: 19 [4736/50048]	Loss: 1.3351
Training Epoch: 19 [4864/50048]	Loss: 1.2660
Training Epoch: 19 [4992/50048]	Loss: 1.3979
Training Epoch: 19 [5120/50048]	Loss: 1.2158
Training Epoch: 19 [5248/50048]	Loss: 1.1499
Training Epoch: 19 [5376/50048]	Loss: 1.1601
Training Epoch: 19 [5504/50048]	Loss: 1.2808
Training Epoch: 19 [5632/50048]	Loss: 1.3104
Training Epoch: 19 [5760/50048]	Loss: 1.3581
Training Epoch: 19 [5888/50048]	Loss: 1.3124
Training Epoch: 19 [6016/50048]	Loss: 1.2481
Training Epoch: 19 [6144/50048]	Loss: 1.1492
Training Epoch: 19 [6272/50048]	Loss: 1.3735
Training Epoch: 19 [6400/50048]	Loss: 1.2277
Training Epoch: 19 [6528/50048]	Loss: 1.2347
Training Epoch: 19 [6656/50048]	Loss: 1.3942
Training Epoch: 19 [6784/50048]	Loss: 1.2734
Training Epoch: 19 [6912/50048]	Loss: 1.3162
Training Epoch: 19 [7040/50048]	Loss: 1.2973
Training Epoch: 19 [7168/50048]	Loss: 1.4155
Training Epoch: 19 [7296/50048]	Loss: 1.3708
Training Epoch: 19 [7424/50048]	Loss: 1.3868
Training Epoch: 19 [7552/50048]	Loss: 1.1724
Training Epoch: 19 [7680/50048]	Loss: 1.1991
Training Epoch: 19 [7808/50048]	Loss: 1.2305
Training Epoch: 19 [7936/50048]	Loss: 1.1837
Training Epoch: 19 [8064/50048]	Loss: 1.3526
Training Epoch: 19 [8192/50048]	Loss: 1.4866
Training Epoch: 19 [8320/50048]	Loss: 1.2013
Training Epoch: 19 [8448/50048]	Loss: 1.2252
Training Epoch: 19 [8576/50048]	Loss: 1.1968
Training Epoch: 19 [8704/50048]	Loss: 1.3813
Training Epoch: 19 [8832/50048]	Loss: 1.2742
Training Epoch: 19 [8960/50048]	Loss: 1.3843
Training Epoch: 19 [9088/50048]	Loss: 1.0040
Training Epoch: 19 [9216/50048]	Loss: 1.5127
Training Epoch: 19 [9344/50048]	Loss: 1.3871
Training Epoch: 19 [9472/50048]	Loss: 1.2707
Training Epoch: 19 [9600/50048]	Loss: 1.3679
Training Epoch: 19 [9728/50048]	Loss: 1.3760
Training Epoch: 19 [9856/50048]	Loss: 1.4074
Training Epoch: 19 [9984/50048]	Loss: 1.1672
Training Epoch: 19 [10112/50048]	Loss: 1.4131
Training Epoch: 19 [10240/50048]	Loss: 1.2323
Training Epoch: 19 [10368/50048]	Loss: 1.4084
Training Epoch: 19 [10496/50048]	Loss: 1.0728
Training Epoch: 19 [10624/50048]	Loss: 1.3239
Training Epoch: 19 [10752/50048]	Loss: 1.2195
Training Epoch: 19 [10880/50048]	Loss: 1.3397
Training Epoch: 19 [11008/50048]	Loss: 1.3079
Training Epoch: 19 [11136/50048]	Loss: 1.1949
Training Epoch: 19 [11264/50048]	Loss: 1.2333
Training Epoch: 19 [11392/50048]	Loss: 1.5278
Training Epoch: 19 [11520/50048]	Loss: 1.3634
Training Epoch: 19 [11648/50048]	Loss: 1.3386
Training Epoch: 19 [11776/50048]	Loss: 1.3777
Training Epoch: 19 [11904/50048]	Loss: 1.2950
Training Epoch: 19 [12032/50048]	Loss: 1.5155
Training Epoch: 19 [12160/50048]	Loss: 1.3996
Training Epoch: 19 [12288/50048]	Loss: 1.2700
Training Epoch: 19 [12416/50048]	Loss: 1.3714
Training Epoch: 19 [12544/50048]	Loss: 1.4885
Training Epoch: 19 [12672/50048]	Loss: 1.3686
Training Epoch: 19 [12800/50048]	Loss: 1.3325
Training Epoch: 19 [12928/50048]	Loss: 1.3546
Training Epoch: 19 [13056/50048]	Loss: 1.3091
Training Epoch: 19 [13184/50048]	Loss: 1.2211
Training Epoch: 19 [13312/50048]	Loss: 1.2704
Training Epoch: 19 [13440/50048]	Loss: 1.3049
Training Epoch: 19 [13568/50048]	Loss: 1.4668
Training Epoch: 19 [13696/50048]	Loss: 1.3795
Training Epoch: 19 [13824/50048]	Loss: 1.3037
Training Epoch: 19 [13952/50048]	Loss: 1.5012
Training Epoch: 19 [14080/50048]	Loss: 1.3174
Training Epoch: 19 [14208/50048]	Loss: 1.3329
Training Epoch: 19 [14336/50048]	Loss: 1.2188
Training Epoch: 19 [14464/50048]	Loss: 1.2054
Training Epoch: 19 [14592/50048]	Loss: 1.3114
Training Epoch: 19 [14720/50048]	Loss: 1.2998
Training Epoch: 19 [14848/50048]	Loss: 1.0343
Training Epoch: 19 [14976/50048]	Loss: 1.1494
Training Epoch: 19 [15104/50048]	Loss: 1.6115
Training Epoch: 19 [15232/50048]	Loss: 1.2741
Training Epoch: 19 [15360/50048]	Loss: 1.1653
Training Epoch: 19 [15488/50048]	Loss: 1.2440
Training Epoch: 19 [15616/50048]	Loss: 1.3972
Training Epoch: 19 [15744/50048]	Loss: 1.4822
Training Epoch: 19 [15872/50048]	Loss: 1.5018
Training Epoch: 19 [16000/50048]	Loss: 1.2969
Training Epoch: 19 [16128/50048]	Loss: 1.4267
Training Epoch: 19 [16256/50048]	Loss: 1.4543
Training Epoch: 19 [16384/50048]	Loss: 1.5687
Training Epoch: 19 [16512/50048]	Loss: 1.2933
Training Epoch: 19 [16640/50048]	Loss: 1.2689
Training Epoch: 19 [16768/50048]	Loss: 1.5986
Training Epoch: 19 [16896/50048]	Loss: 1.3589
Training Epoch: 19 [17024/50048]	Loss: 1.3125
Training Epoch: 19 [17152/50048]	Loss: 1.0511
Training Epoch: 19 [17280/50048]	Loss: 1.3212
Training Epoch: 19 [17408/50048]	Loss: 1.2159
Training Epoch: 19 [17536/50048]	Loss: 1.3885
Training Epoch: 19 [17664/50048]	Loss: 1.3031
Training Epoch: 19 [17792/50048]	Loss: 1.4659
Training Epoch: 19 [17920/50048]	Loss: 1.2152
Training Epoch: 19 [18048/50048]	Loss: 1.2413
Training Epoch: 19 [18176/50048]	Loss: 1.2506
Training Epoch: 19 [18304/50048]	Loss: 1.1266
Training Epoch: 19 [18432/50048]	Loss: 1.5027
Training Epoch: 19 [18560/50048]	Loss: 1.1986
Training Epoch: 19 [18688/50048]	Loss: 1.2578
Training Epoch: 19 [18816/50048]	Loss: 1.3322
Training Epoch: 19 [18944/50048]	Loss: 1.4814
Training Epoch: 19 [19072/50048]	Loss: 1.3809
Training Epoch: 19 [19200/50048]	Loss: 1.4383
Training Epoch: 19 [19328/50048]	Loss: 1.5452
Training Epoch: 19 [19456/50048]	Loss: 1.2330
Training Epoch: 19 [19584/50048]	Loss: 1.5916
Training Epoch: 19 [19712/50048]	Loss: 1.3620
Training Epoch: 19 [19840/50048]	Loss: 1.3499
Training Epoch: 19 [19968/50048]	Loss: 1.4828
Training Epoch: 19 [20096/50048]	Loss: 1.4852
Training Epoch: 19 [20224/50048]	Loss: 1.0951
Training Epoch: 19 [20352/50048]	Loss: 1.4779
Training Epoch: 19 [20480/50048]	Loss: 1.2008
Training Epoch: 19 [20608/50048]	Loss: 1.2967
Training Epoch: 19 [20736/50048]	Loss: 1.2544
Training Epoch: 19 [20864/50048]	Loss: 1.2492
Training Epoch: 19 [20992/50048]	Loss: 1.4807
Training Epoch: 19 [21120/50048]	Loss: 1.3120
Training Epoch: 19 [21248/50048]	Loss: 1.4605
Training Epoch: 19 [21376/50048]	Loss: 1.2058
Training Epoch: 19 [21504/50048]	Loss: 1.5687
Training Epoch: 19 [21632/50048]	Loss: 1.3069
Training Epoch: 19 [21760/50048]	Loss: 1.3855
Training Epoch: 19 [21888/50048]	Loss: 1.3662
Training Epoch: 19 [22016/50048]	Loss: 1.4281
Training Epoch: 19 [22144/50048]	Loss: 1.2619
Training Epoch: 19 [22272/50048]	Loss: 1.2846
Training Epoch: 19 [22400/50048]	Loss: 1.2934
Training Epoch: 19 [22528/50048]	Loss: 1.1382
Training Epoch: 19 [22656/50048]	Loss: 1.5573
Training Epoch: 19 [22784/50048]	Loss: 1.2765
Training Epoch: 19 [22912/50048]	Loss: 1.2110
Training Epoch: 19 [23040/50048]	Loss: 1.2677
Training Epoch: 19 [23168/50048]	Loss: 1.2569
Training Epoch: 19 [23296/50048]	Loss: 1.2789
Training Epoch: 19 [23424/50048]	Loss: 1.3906
Training Epoch: 19 [23552/50048]	Loss: 1.3808
Training Epoch: 19 [23680/50048]	Loss: 1.3121
Training Epoch: 19 [23808/50048]	Loss: 1.2540
Training Epoch: 19 [23936/50048]	Loss: 1.4065
Training Epoch: 19 [24064/50048]	Loss: 1.5577
Training Epoch: 19 [24192/50048]	Loss: 1.4887
Training Epoch: 19 [24320/50048]	Loss: 1.4139
Training Epoch: 19 [24448/50048]	Loss: 1.3837
Training Epoch: 19 [24576/50048]	Loss: 1.5386
Training Epoch: 19 [24704/50048]	Loss: 1.5816
Training Epoch: 19 [24832/50048]	Loss: 1.5346
Training Epoch: 19 [24960/50048]	Loss: 1.2391
Training Epoch: 19 [25088/50048]	Loss: 1.2325
Training Epoch: 19 [25216/50048]	Loss: 1.3194
Training Epoch: 19 [25344/50048]	Loss: 1.4944
Training Epoch: 19 [25472/50048]	Loss: 1.1534
Training Epoch: 19 [25600/50048]	Loss: 1.3306
Training Epoch: 19 [25728/50048]	Loss: 1.3020
Training Epoch: 19 [25856/50048]	Loss: 1.1695
Training Epoch: 19 [25984/50048]	Loss: 1.3704
Training Epoch: 19 [26112/50048]	Loss: 1.3065
Training Epoch: 19 [26240/50048]	Loss: 1.4434
Training Epoch: 19 [26368/50048]	Loss: 1.2973
Training Epoch: 19 [26496/50048]	Loss: 1.2053
Training Epoch: 19 [26624/50048]	Loss: 1.4433
Training Epoch: 19 [26752/50048]	Loss: 1.3055
Training Epoch: 19 [26880/50048]	Loss: 1.2642
Training Epoch: 19 [27008/50048]	Loss: 1.2401
Training Epoch: 19 [27136/50048]	Loss: 1.4036
Training Epoch: 19 [27264/50048]	Loss: 1.2002
Training Epoch: 19 [27392/50048]	Loss: 1.6292
Training Epoch: 19 [27520/50048]	Loss: 1.4554
Training Epoch: 19 [27648/50048]	Loss: 1.4757
Training Epoch: 19 [27776/50048]	Loss: 1.5315
Training Epoch: 19 [27904/50048]	Loss: 1.5493
Training Epoch: 19 [28032/50048]	Loss: 1.4349
Training Epoch: 19 [28160/50048]	Loss: 1.0532
Training Epoch: 19 [28288/50048]	Loss: 1.2198
Training Epoch: 19 [28416/50048]	Loss: 1.0641
Training Epoch: 19 [28544/50048]	Loss: 1.1951
Training Epoch: 19 [28672/50048]	Loss: 1.3492
Training Epoch: 19 [28800/50048]	Loss: 1.2050
Training Epoch: 19 [28928/50048]	Loss: 1.3764
Training Epoch: 19 [29056/50048]	Loss: 1.5198
Training Epoch: 19 [29184/50048]	Loss: 1.3258
Training Epoch: 19 [29312/50048]	Loss: 1.3725
Training Epoch: 19 [29440/50048]	Loss: 1.5228
Training Epoch: 19 [29568/50048]	Loss: 1.3948
Training Epoch: 19 [29696/50048]	Loss: 1.3787
Training Epoch: 19 [29824/50048]	Loss: 1.5125
Training Epoch: 19 [29952/50048]	Loss: 1.2015
Training Epoch: 19 [30080/50048]	Loss: 1.3956
Training Epoch: 19 [30208/50048]	Loss: 1.2427
Training Epoch: 19 [30336/50048]	Loss: 1.3609
Training Epoch: 19 [30464/50048]	Loss: 1.4418
Training Epoch: 19 [30592/50048]	Loss: 1.1523
Training Epoch: 19 [30720/50048]	Loss: 1.3916
Training Epoch: 19 [30848/50048]	Loss: 1.2596
Training Epoch: 19 [30976/50048]	Loss: 1.3254
Training Epoch: 19 [31104/50048]	Loss: 1.0948
Training Epoch: 19 [31232/50048]	Loss: 1.3643
Training Epoch: 19 [31360/50048]	Loss: 1.3029
Training Epoch: 19 [31488/50048]	Loss: 1.4614
Training Epoch: 19 [31616/50048]	Loss: 1.3520
Training Epoch: 19 [31744/50048]	Loss: 1.5784
Training Epoch: 19 [31872/50048]	Loss: 1.3907
Training Epoch: 19 [32000/50048]	Loss: 1.3857
Training Epoch: 19 [32128/50048]	Loss: 1.3051
Training Epoch: 19 [32256/50048]	Loss: 1.4343
Training Epoch: 19 [32384/50048]	Loss: 1.1900
Training Epoch: 19 [32512/50048]	Loss: 1.3031
Training Epoch: 19 [32640/50048]	Loss: 1.2209
Training Epoch: 19 [32768/50048]	Loss: 1.1788
Training Epoch: 19 [32896/50048]	Loss: 1.2807
Training Epoch: 19 [33024/50048]	Loss: 1.3280
Training Epoch: 19 [33152/50048]	Loss: 1.2690
Training Epoch: 19 [33280/50048]	Loss: 1.4421
Training Epoch: 19 [33408/50048]	Loss: 1.1861
Training Epoch: 19 [33536/50048]	Loss: 1.2188
Training Epoch: 19 [33664/50048]	Loss: 1.2556
Training Epoch: 19 [33792/50048]	Loss: 1.2990
Training Epoch: 19 [33920/50048]	Loss: 1.2861
Training Epoch: 19 [34048/50048]	Loss: 1.3532
Training Epoch: 19 [34176/50048]	Loss: 1.4377
Training Epoch: 19 [34304/50048]	Loss: 1.4237
Training Epoch: 19 [34432/50048]	Loss: 1.4800
Training Epoch: 19 [34560/50048]	Loss: 1.4113
Training Epoch: 19 [34688/50048]	Loss: 1.2982
Training Epoch: 19 [34816/50048]	Loss: 1.2385
Training Epoch: 19 [34944/50048]	Loss: 1.4230
Training Epoch: 19 [35072/50048]	Loss: 1.1968
Training Epoch: 19 [35200/50048]	Loss: 1.3984
Training Epoch: 19 [35328/50048]	Loss: 1.3566
Training Epoch: 19 [35456/50048]	Loss: 1.2959
Training Epoch: 19 [35584/50048]	Loss: 1.6075
Training Epoch: 19 [35712/50048]	Loss: 1.5065
Training Epoch: 19 [35840/50048]	Loss: 1.2618
Training Epoch: 19 [35968/50048]	Loss: 1.1400
Training Epoch: 19 [36096/50048]	Loss: 1.2176
Training Epoch: 19 [36224/50048]	Loss: 1.3359
Training Epoch: 19 [36352/50048]	Loss: 1.3286
Training Epoch: 19 [36480/50048]	Loss: 1.6085
Training Epoch: 19 [36608/50048]	Loss: 1.2858
Training Epoch: 19 [36736/50048]	Loss: 1.4822
Training Epoch: 19 [36864/50048]	Loss: 1.4679
Training Epoch: 19 [36992/50048]	Loss: 1.4501
Training Epoch: 19 [37120/50048]	Loss: 1.3545
Training Epoch: 19 [37248/50048]	Loss: 1.5252
Training Epoch: 19 [37376/50048]	Loss: 1.4794
Training Epoch: 19 [37504/50048]	Loss: 1.3381
Training Epoch: 19 [37632/50048]	Loss: 1.3777
Training Epoch: 19 [37760/50048]	Loss: 1.2465
Training Epoch: 19 [37888/50048]	Loss: 1.1936
Training Epoch: 19 [38016/50048]	Loss: 1.3696
Training Epoch: 19 [38144/50048]	Loss: 1.3579
Training Epoch: 19 [38272/50048]	Loss: 1.2677
Training Epoch: 19 [38400/50048]	Loss: 1.3155
Training Epoch: 19 [38528/50048]	Loss: 1.3260
Training Epoch: 19 [38656/50048]	Loss: 1.5392
Training Epoch: 19 [38784/50048]	Loss: 1.2805
Training Epoch: 19 [38912/50048]	Loss: 1.3567
Training Epoch: 19 [39040/50048]	Loss: 1.2375
Training Epoch: 19 [39168/50048]	Loss: 1.3239
Training Epoch: 19 [39296/50048]	Loss: 1.3065
Training Epoch: 19 [39424/50048]	Loss: 1.4201
Training Epoch: 19 [39552/50048]	Loss: 1.1594
Training Epoch: 19 [39680/50048]	Loss: 1.2581
Training Epoch: 19 [39808/50048]	Loss: 1.2816
Training Epoch: 19 [39936/50048]	Loss: 1.4671
Training Epoch: 19 [40064/50048]	Loss: 0.9629
Training Epoch: 19 [40192/50048]	Loss: 1.3897
Training Epoch: 19 [40320/50048]	Loss: 1.4048
Training Epoch: 19 [40448/50048]	Loss: 1.1183
Training Epoch: 19 [40576/50048]	Loss: 1.3363
Training Epoch: 19 [40704/50048]	Loss: 1.3394
Training Epoch: 19 [40832/50048]	Loss: 1.2007
Training Epoch: 19 [40960/50048]	Loss: 1.5524
Training Epoch: 19 [41088/50048]	Loss: 1.2996
Training Epoch: 19 [41216/50048]	Loss: 1.3496
Training Epoch: 19 [41344/50048]	Loss: 1.5382
Training Epoch: 19 [41472/50048]	Loss: 1.3608
Training Epoch: 19 [41600/50048]	Loss: 1.4061
Training Epoch: 19 [41728/50048]	Loss: 1.4039
Training Epoch: 19 [41856/50048]	Loss: 1.5287
Training Epoch: 19 [41984/50048]	Loss: 1.3272
Training Epoch: 19 [42112/50048]	Loss: 1.4396
Training Epoch: 19 [42240/50048]	Loss: 1.2121
Training Epoch: 19 [42368/50048]	Loss: 1.1164
Training Epoch: 19 [42496/50048]	Loss: 1.3429
Training Epoch: 19 [42624/50048]	Loss: 1.3179
Training Epoch: 19 [42752/50048]	Loss: 1.4069
Training Epoch: 19 [42880/50048]	Loss: 1.6210
Training Epoch: 19 [43008/50048]	Loss: 1.5879
Training Epoch: 19 [43136/50048]	Loss: 1.2312
Training Epoch: 19 [43264/50048]	Loss: 1.5706
Training Epoch: 19 [43392/50048]	Loss: 1.3537
Training Epoch: 19 [43520/50048]	Loss: 1.2036
Training Epoch: 19 [43648/50048]	Loss: 1.1738
Training Epoch: 19 [43776/50048]	Loss: 1.2713
Training Epoch: 19 [43904/50048]	Loss: 1.2131
Training Epoch: 19 [44032/50048]	Loss: 1.1584
Training Epoch: 19 [44160/50048]	Loss: 1.6022
Training Epoch: 19 [44288/50048]	Loss: 1.1667
Training Epoch: 19 [44416/50048]	Loss: 1.5230
Training Epoch: 19 [44544/50048]	Loss: 1.4510
Training Epoch: 19 [44672/50048]	Loss: 1.2612
Training Epoch: 19 [44800/50048]	Loss: 1.2909
Training Epoch: 19 [44928/50048]	Loss: 1.3886
Training Epoch: 19 [45056/50048]	Loss: 1.4975
Training Epoch: 19 [45184/50048]	Loss: 1.3473
Training Epoch: 19 [45312/50048]	Loss: 1.4195
Training Epoch: 19 [45440/50048]	Loss: 1.3362
Training Epoch: 19 [45568/50048]	Loss: 1.3427
Training Epoch: 19 [45696/50048]	Loss: 1.4380
Training Epoch: 19 [45824/50048]	Loss: 1.2928
Training Epoch: 19 [45952/50048]	Loss: 1.4238
Training Epoch: 19 [46080/50048]	Loss: 1.5502
Training Epoch: 19 [46208/50048]	Loss: 1.2329
Training Epoch: 19 [46336/50048]	Loss: 1.4895
Training Epoch: 19 [46464/50048]	Loss: 1.2911
Training Epoch: 19 [46592/50048]	Loss: 1.5413
Training Epoch: 19 [46720/50048]	Loss: 1.4555
Training Epoch: 19 [46848/50048]	Loss: 1.3384
Training Epoch: 19 [46976/50048]	Loss: 1.2648
Training Epoch: 19 [47104/50048]	Loss: 1.1221
Training Epoch: 19 [47232/50048]	Loss: 1.5196
Training Epoch: 19 [47360/50048]	Loss: 1.2793
Training Epoch: 19 [47488/50048]	Loss: 1.4021
Training Epoch: 19 [47616/50048]	Loss: 1.4940
Training Epoch: 19 [47744/50048]	Loss: 1.3055
Training Epoch: 19 [47872/50048]	Loss: 1.2793
Training Epoch: 19 [48000/50048]	Loss: 1.3728
Training Epoch: 19 [48128/50048]	Loss: 1.2831
Training Epoch: 19 [48256/50048]	Loss: 1.1327
Training Epoch: 19 [48384/50048]	Loss: 1.5263
Training Epoch: 19 [48512/50048]	Loss: 1.2079
Training Epoch: 19 [48640/50048]	Loss: 1.5712
Training Epoch: 19 [48768/50048]	Loss: 1.4182
Training Epoch: 19 [48896/50048]	Loss: 1.1735
Training Epoch: 19 [49024/50048]	Loss: 1.4234
Training Epoch: 19 [49152/50048]	Loss: 1.4876
Training Epoch: 19 [49280/50048]	Loss: 1.3580
Training Epoch: 19 [49408/50048]	Loss: 1.4243
Training Epoch: 19 [49536/50048]	Loss: 1.2044
Training Epoch: 19 [49664/50048]	Loss: 1.3631
Training Epoch: 19 [49792/50048]	Loss: 1.3999
Training Epoch: 19 [49920/50048]	Loss: 1.3116
Training Epoch: 19 [50048/50048]	Loss: 1.3436
Validation Epoch: 19, Average loss: 0.0122, Accuracy: 0.5759
Training Epoch: 20 [128/50048]	Loss: 1.3201
Training Epoch: 20 [256/50048]	Loss: 1.4117
Training Epoch: 20 [384/50048]	Loss: 1.2412
Training Epoch: 20 [512/50048]	Loss: 1.3685
Training Epoch: 20 [640/50048]	Loss: 1.3821
Training Epoch: 20 [768/50048]	Loss: 1.3103
Training Epoch: 20 [896/50048]	Loss: 1.0708
Training Epoch: 20 [1024/50048]	Loss: 1.0953
Training Epoch: 20 [1152/50048]	Loss: 1.0370
Training Epoch: 20 [1280/50048]	Loss: 1.0966
Training Epoch: 20 [1408/50048]	Loss: 1.3422
Training Epoch: 20 [1536/50048]	Loss: 1.3730
Training Epoch: 20 [1664/50048]	Loss: 1.2016
Training Epoch: 20 [1792/50048]	Loss: 1.4488
Training Epoch: 20 [1920/50048]	Loss: 1.0846
Training Epoch: 20 [2048/50048]	Loss: 1.2766
Training Epoch: 20 [2176/50048]	Loss: 1.1608
Training Epoch: 20 [2304/50048]	Loss: 1.3936
Training Epoch: 20 [2432/50048]	Loss: 1.0988
Training Epoch: 20 [2560/50048]	Loss: 1.2248
Training Epoch: 20 [2688/50048]	Loss: 1.1909
Training Epoch: 20 [2816/50048]	Loss: 1.2170
Training Epoch: 20 [2944/50048]	Loss: 1.3175
Training Epoch: 20 [3072/50048]	Loss: 1.1831
Training Epoch: 20 [3200/50048]	Loss: 1.2746
Training Epoch: 20 [3328/50048]	Loss: 1.1238
Training Epoch: 20 [3456/50048]	Loss: 1.3356
Training Epoch: 20 [3584/50048]	Loss: 1.3366
Training Epoch: 20 [3712/50048]	Loss: 1.3365
Training Epoch: 20 [3840/50048]	Loss: 1.0474
Training Epoch: 20 [3968/50048]	Loss: 1.3990
Training Epoch: 20 [4096/50048]	Loss: 1.1502
Training Epoch: 20 [4224/50048]	Loss: 1.4525
Training Epoch: 20 [4352/50048]	Loss: 1.2070
Training Epoch: 20 [4480/50048]	Loss: 0.9678
Training Epoch: 20 [4608/50048]	Loss: 1.2671
Training Epoch: 20 [4736/50048]	Loss: 1.1261
Training Epoch: 20 [4864/50048]	Loss: 1.3002
Training Epoch: 20 [4992/50048]	Loss: 1.4424
Training Epoch: 20 [5120/50048]	Loss: 1.3541
Training Epoch: 20 [5248/50048]	Loss: 1.0591
Training Epoch: 20 [5376/50048]	Loss: 1.2007
Training Epoch: 20 [5504/50048]	Loss: 1.2547
Training Epoch: 20 [5632/50048]	Loss: 1.3436
Training Epoch: 20 [5760/50048]	Loss: 1.1491
Training Epoch: 20 [5888/50048]	Loss: 1.4371
Training Epoch: 20 [6016/50048]	Loss: 1.3065
Training Epoch: 20 [6144/50048]	Loss: 1.2396
Training Epoch: 20 [6272/50048]	Loss: 1.3329
Training Epoch: 20 [6400/50048]	Loss: 1.2324
Training Epoch: 20 [6528/50048]	Loss: 1.2883
Training Epoch: 20 [6656/50048]	Loss: 1.5164
Training Epoch: 20 [6784/50048]	Loss: 1.4713
Training Epoch: 20 [6912/50048]	Loss: 1.1989
Training Epoch: 20 [7040/50048]	Loss: 1.2307
Training Epoch: 20 [7168/50048]	Loss: 1.4198
Training Epoch: 20 [7296/50048]	Loss: 1.1275
Training Epoch: 20 [7424/50048]	Loss: 1.4535
Training Epoch: 20 [7552/50048]	Loss: 1.1929
Training Epoch: 20 [7680/50048]	Loss: 1.3202
Training Epoch: 20 [7808/50048]	Loss: 1.1586
Training Epoch: 20 [7936/50048]	Loss: 1.2501
Training Epoch: 20 [8064/50048]	Loss: 1.6954
Training Epoch: 20 [8192/50048]	Loss: 1.2557
Training Epoch: 20 [8320/50048]	Loss: 1.2509
Training Epoch: 20 [8448/50048]	Loss: 1.3215
Training Epoch: 20 [8576/50048]	Loss: 0.8995
Training Epoch: 20 [8704/50048]	Loss: 1.3898
Training Epoch: 20 [8832/50048]	Loss: 1.2971
Training Epoch: 20 [8960/50048]	Loss: 1.2271
Training Epoch: 20 [9088/50048]	Loss: 1.1990
Training Epoch: 20 [9216/50048]	Loss: 1.1627
Training Epoch: 20 [9344/50048]	Loss: 1.2039
Training Epoch: 20 [9472/50048]	Loss: 1.1916
Training Epoch: 20 [9600/50048]	Loss: 1.4174
Training Epoch: 20 [9728/50048]	Loss: 1.2803
Training Epoch: 20 [9856/50048]	Loss: 1.0261
Training Epoch: 20 [9984/50048]	Loss: 1.4866
Training Epoch: 20 [10112/50048]	Loss: 1.1036
Training Epoch: 20 [10240/50048]	Loss: 1.3777
Training Epoch: 20 [10368/50048]	Loss: 1.3011
Training Epoch: 20 [10496/50048]	Loss: 1.2895
Training Epoch: 20 [10624/50048]	Loss: 1.2297
Training Epoch: 20 [10752/50048]	Loss: 1.1848
Training Epoch: 20 [10880/50048]	Loss: 1.1732
Training Epoch: 20 [11008/50048]	Loss: 1.3253
Training Epoch: 20 [11136/50048]	Loss: 1.2041
Training Epoch: 20 [11264/50048]	Loss: 1.2918
Training Epoch: 20 [11392/50048]	Loss: 1.1712
Training Epoch: 20 [11520/50048]	Loss: 1.5667
Training Epoch: 20 [11648/50048]	Loss: 1.0556
Training Epoch: 20 [11776/50048]	Loss: 1.3360
Training Epoch: 20 [11904/50048]	Loss: 1.3012
Training Epoch: 20 [12032/50048]	Loss: 1.3235
Training Epoch: 20 [12160/50048]	Loss: 1.2757
Training Epoch: 20 [12288/50048]	Loss: 1.3037
Training Epoch: 20 [12416/50048]	Loss: 1.2779
Training Epoch: 20 [12544/50048]	Loss: 1.1824
Training Epoch: 20 [12672/50048]	Loss: 1.2527
Training Epoch: 20 [12800/50048]	Loss: 1.5761
Training Epoch: 20 [12928/50048]	Loss: 1.5447
Training Epoch: 20 [13056/50048]	Loss: 1.4138
Training Epoch: 20 [13184/50048]	Loss: 1.3415
Training Epoch: 20 [13312/50048]	Loss: 1.5191
Training Epoch: 20 [13440/50048]	Loss: 1.1122
Training Epoch: 20 [13568/50048]	Loss: 1.2972
Training Epoch: 20 [13696/50048]	Loss: 1.3518
Training Epoch: 20 [13824/50048]	Loss: 1.0913
Training Epoch: 20 [13952/50048]	Loss: 1.1848
Training Epoch: 20 [14080/50048]	Loss: 1.2252
Training Epoch: 20 [14208/50048]	Loss: 1.2021
Training Epoch: 20 [14336/50048]	Loss: 1.1623
Training Epoch: 20 [14464/50048]	Loss: 1.4396
Training Epoch: 20 [14592/50048]	Loss: 1.2758
Training Epoch: 20 [14720/50048]	Loss: 1.0504
Training Epoch: 20 [14848/50048]	Loss: 1.2154
Training Epoch: 20 [14976/50048]	Loss: 1.3425
Training Epoch: 20 [15104/50048]	Loss: 1.3572
Training Epoch: 20 [15232/50048]	Loss: 1.2263
Training Epoch: 20 [15360/50048]	Loss: 1.2279
Training Epoch: 20 [15488/50048]	Loss: 1.3471
Training Epoch: 20 [15616/50048]	Loss: 1.4126
Training Epoch: 20 [15744/50048]	Loss: 1.1959
Training Epoch: 20 [15872/50048]	Loss: 1.3949
Training Epoch: 20 [16000/50048]	Loss: 1.0775
Training Epoch: 20 [16128/50048]	Loss: 1.1946
Training Epoch: 20 [16256/50048]	Loss: 1.3275
Training Epoch: 20 [16384/50048]	Loss: 1.2572
Training Epoch: 20 [16512/50048]	Loss: 1.2030
Training Epoch: 20 [16640/50048]	Loss: 1.2377
Training Epoch: 20 [16768/50048]	Loss: 1.3337
Training Epoch: 20 [16896/50048]	Loss: 1.1981
Training Epoch: 20 [17024/50048]	Loss: 1.6151
Training Epoch: 20 [17152/50048]	Loss: 1.4109
Training Epoch: 20 [17280/50048]	Loss: 1.4124
Training Epoch: 20 [17408/50048]	Loss: 1.2045
Training Epoch: 20 [17536/50048]	Loss: 1.1514
Training Epoch: 20 [17664/50048]	Loss: 1.0932
Training Epoch: 20 [17792/50048]	Loss: 1.2749
Training Epoch: 20 [17920/50048]	Loss: 1.1672
Training Epoch: 20 [18048/50048]	Loss: 1.2250
Training Epoch: 20 [18176/50048]	Loss: 1.2803
Training Epoch: 20 [18304/50048]	Loss: 1.4375
Training Epoch: 20 [18432/50048]	Loss: 1.4259
Training Epoch: 20 [18560/50048]	Loss: 1.2059
Training Epoch: 20 [18688/50048]	Loss: 1.3135
Training Epoch: 20 [18816/50048]	Loss: 1.4674
Training Epoch: 20 [18944/50048]	Loss: 1.7858
Training Epoch: 20 [19072/50048]	Loss: 1.3185
Training Epoch: 20 [19200/50048]	Loss: 1.2327
Training Epoch: 20 [19328/50048]	Loss: 1.2854
Training Epoch: 20 [19456/50048]	Loss: 1.1751
Training Epoch: 20 [19584/50048]	Loss: 1.3513
Training Epoch: 20 [19712/50048]	Loss: 1.3148
Training Epoch: 20 [19840/50048]	Loss: 1.2066
Training Epoch: 20 [19968/50048]	Loss: 1.2943
Training Epoch: 20 [20096/50048]	Loss: 1.2694
Training Epoch: 20 [20224/50048]	Loss: 1.2724
Training Epoch: 20 [20352/50048]	Loss: 1.4692
Training Epoch: 20 [20480/50048]	Loss: 1.2898
Training Epoch: 20 [20608/50048]	Loss: 1.3619
Training Epoch: 20 [20736/50048]	Loss: 1.0586
Training Epoch: 20 [20864/50048]	Loss: 1.4994
Training Epoch: 20 [20992/50048]	Loss: 1.2689
Training Epoch: 20 [21120/50048]	Loss: 1.3770
Training Epoch: 20 [21248/50048]	Loss: 1.4734
Training Epoch: 20 [21376/50048]	Loss: 1.4299
Training Epoch: 20 [21504/50048]	Loss: 1.3207
Training Epoch: 20 [21632/50048]	Loss: 1.1813
Training Epoch: 20 [21760/50048]	Loss: 1.3528
Training Epoch: 20 [21888/50048]	Loss: 1.2398
Training Epoch: 20 [22016/50048]	Loss: 1.5474
Training Epoch: 20 [22144/50048]	Loss: 1.3569
Training Epoch: 20 [22272/50048]	Loss: 1.4243
Training Epoch: 20 [22400/50048]	Loss: 1.3934
Training Epoch: 20 [22528/50048]	Loss: 1.3742
Training Epoch: 20 [22656/50048]	Loss: 1.5019
Training Epoch: 20 [22784/50048]	Loss: 1.1574
Training Epoch: 20 [22912/50048]	Loss: 1.2370
Training Epoch: 20 [23040/50048]	Loss: 1.2249
Training Epoch: 20 [23168/50048]	Loss: 1.3953
Training Epoch: 20 [23296/50048]	Loss: 1.2442
Training Epoch: 20 [23424/50048]	Loss: 1.1943
Training Epoch: 20 [23552/50048]	Loss: 1.4888
Training Epoch: 20 [23680/50048]	Loss: 1.2682
Training Epoch: 20 [23808/50048]	Loss: 1.3973
Training Epoch: 20 [23936/50048]	Loss: 1.2668
Training Epoch: 20 [24064/50048]	Loss: 1.3547
Training Epoch: 20 [24192/50048]	Loss: 1.5341
Training Epoch: 20 [24320/50048]	Loss: 1.4780
Training Epoch: 20 [24448/50048]	Loss: 1.6159
Training Epoch: 20 [24576/50048]	Loss: 1.3608
Training Epoch: 20 [24704/50048]	Loss: 1.2247
Training Epoch: 20 [24832/50048]	Loss: 1.2458
Training Epoch: 20 [24960/50048]	Loss: 1.3200
Training Epoch: 20 [25088/50048]	Loss: 1.4602
Training Epoch: 20 [25216/50048]	Loss: 1.4570
Training Epoch: 20 [25344/50048]	Loss: 1.1442
Training Epoch: 20 [25472/50048]	Loss: 1.3396
Training Epoch: 20 [25600/50048]	Loss: 1.2755
Training Epoch: 20 [25728/50048]	Loss: 1.3068
Training Epoch: 20 [25856/50048]	Loss: 1.3540
Training Epoch: 20 [25984/50048]	Loss: 1.1750
Training Epoch: 20 [26112/50048]	Loss: 1.4402
Training Epoch: 20 [26240/50048]	Loss: 1.1808
Training Epoch: 20 [26368/50048]	Loss: 1.4816
Training Epoch: 20 [26496/50048]	Loss: 1.3419
Training Epoch: 20 [26624/50048]	Loss: 1.3515
Training Epoch: 20 [26752/50048]	Loss: 1.2882
Training Epoch: 20 [26880/50048]	Loss: 1.2748
Training Epoch: 20 [27008/50048]	Loss: 1.5184
Training Epoch: 20 [27136/50048]	Loss: 1.1273
Training Epoch: 20 [27264/50048]	Loss: 1.3637
Training Epoch: 20 [27392/50048]	Loss: 1.3791
Training Epoch: 20 [27520/50048]	Loss: 1.2138
Training Epoch: 20 [27648/50048]	Loss: 1.2520
Training Epoch: 20 [27776/50048]	Loss: 1.3958
Training Epoch: 20 [27904/50048]	Loss: 1.1902
Training Epoch: 20 [28032/50048]	Loss: 1.3784
Training Epoch: 20 [28160/50048]	Loss: 1.3315
Training Epoch: 20 [28288/50048]	Loss: 1.4894
Training Epoch: 20 [28416/50048]	Loss: 1.1595
Training Epoch: 20 [28544/50048]	Loss: 1.4720
Training Epoch: 20 [28672/50048]	Loss: 1.1592
Training Epoch: 20 [28800/50048]	Loss: 1.1454
Training Epoch: 20 [28928/50048]	Loss: 1.4559
Training Epoch: 20 [29056/50048]	Loss: 1.5962
Training Epoch: 20 [29184/50048]	Loss: 1.2819
Training Epoch: 20 [29312/50048]	Loss: 1.2611
Training Epoch: 20 [29440/50048]	Loss: 1.2403
Training Epoch: 20 [29568/50048]	Loss: 1.1367
Training Epoch: 20 [29696/50048]	Loss: 1.2258
Training Epoch: 20 [29824/50048]	Loss: 1.4774
Training Epoch: 20 [29952/50048]	Loss: 1.3437
Training Epoch: 20 [30080/50048]	Loss: 1.2592
Training Epoch: 20 [30208/50048]	Loss: 1.3318
Training Epoch: 20 [30336/50048]	Loss: 1.2409
Training Epoch: 20 [30464/50048]	Loss: 1.3858
Training Epoch: 20 [30592/50048]	Loss: 1.4707
Training Epoch: 20 [30720/50048]	Loss: 1.0946
Training Epoch: 20 [30848/50048]	Loss: 0.9626
Training Epoch: 20 [30976/50048]	Loss: 1.3998
Training Epoch: 20 [31104/50048]	Loss: 1.1994
Training Epoch: 20 [31232/50048]	Loss: 1.1705
Training Epoch: 20 [31360/50048]	Loss: 1.1902
Training Epoch: 20 [31488/50048]	Loss: 1.3885
Training Epoch: 20 [31616/50048]	Loss: 1.3791
Training Epoch: 20 [31744/50048]	Loss: 1.2198
Training Epoch: 20 [31872/50048]	Loss: 1.4532
Training Epoch: 20 [32000/50048]	Loss: 1.3522
Training Epoch: 20 [32128/50048]	Loss: 1.1878
Training Epoch: 20 [32256/50048]	Loss: 1.4011
Training Epoch: 20 [32384/50048]	Loss: 1.4994
Training Epoch: 20 [32512/50048]	Loss: 1.2079
Training Epoch: 20 [32640/50048]	Loss: 1.4191
Training Epoch: 20 [32768/50048]	Loss: 1.2466
Training Epoch: 20 [32896/50048]	Loss: 1.3989
Training Epoch: 20 [33024/50048]	Loss: 1.2714
Training Epoch: 20 [33152/50048]	Loss: 1.2900
Training Epoch: 20 [33280/50048]	Loss: 1.2445
Training Epoch: 20 [33408/50048]	Loss: 1.4712
Training Epoch: 20 [33536/50048]	Loss: 1.1671
Training Epoch: 20 [33664/50048]	Loss: 1.3314
Training Epoch: 20 [33792/50048]	Loss: 1.2984
Training Epoch: 20 [33920/50048]	Loss: 1.3547
Training Epoch: 20 [34048/50048]	Loss: 1.3909
Training Epoch: 20 [34176/50048]	Loss: 1.1261
Training Epoch: 20 [34304/50048]	Loss: 1.4059
Training Epoch: 20 [34432/50048]	Loss: 1.1388
Training Epoch: 20 [34560/50048]	Loss: 1.3182
Training Epoch: 20 [34688/50048]	Loss: 1.2106
Training Epoch: 20 [34816/50048]	Loss: 1.0403
Training Epoch: 20 [34944/50048]	Loss: 1.1847
Training Epoch: 20 [35072/50048]	Loss: 1.3669
Training Epoch: 20 [35200/50048]	Loss: 1.4843
Training Epoch: 20 [35328/50048]	Loss: 1.3281
Training Epoch: 20 [35456/50048]	Loss: 1.2194
Training Epoch: 20 [35584/50048]	Loss: 1.4316
Training Epoch: 20 [35712/50048]	Loss: 1.2321
Training Epoch: 20 [35840/50048]	Loss: 1.4048
Training Epoch: 20 [35968/50048]	Loss: 1.1702
Training Epoch: 20 [36096/50048]	Loss: 1.2408
Training Epoch: 20 [36224/50048]	Loss: 1.2264
Training Epoch: 20 [36352/50048]	Loss: 1.4351
Training Epoch: 20 [36480/50048]	Loss: 1.3761
Training Epoch: 20 [36608/50048]	Loss: 1.3527
Training Epoch: 20 [36736/50048]	Loss: 1.2418
Training Epoch: 20 [36864/50048]	Loss: 1.5952
Training Epoch: 20 [36992/50048]	Loss: 1.3809
Training Epoch: 20 [37120/50048]	Loss: 1.3832
Training Epoch: 20 [37248/50048]	Loss: 1.3323
Training Epoch: 20 [37376/50048]	Loss: 1.3117
Training Epoch: 20 [37504/50048]	Loss: 1.6587
Training Epoch: 20 [37632/50048]	Loss: 1.2941
Training Epoch: 20 [37760/50048]	Loss: 1.5088
Training Epoch: 20 [37888/50048]	Loss: 1.2749
Training Epoch: 20 [38016/50048]	Loss: 1.5503
Training Epoch: 20 [38144/50048]	Loss: 1.5580
Training Epoch: 20 [38272/50048]	Loss: 1.3289
Training Epoch: 20 [38400/50048]	Loss: 1.2511
Training Epoch: 20 [38528/50048]	Loss: 1.4293
Training Epoch: 20 [38656/50048]	Loss: 1.2531
Training Epoch: 20 [38784/50048]	Loss: 1.2551
Training Epoch: 20 [38912/50048]	Loss: 1.2475
Training Epoch: 20 [39040/50048]	Loss: 1.3909
Training Epoch: 20 [39168/50048]	Loss: 1.5606
Training Epoch: 20 [39296/50048]	Loss: 1.1454
Training Epoch: 20 [39424/50048]	Loss: 1.3800
Training Epoch: 20 [39552/50048]	Loss: 1.0550
Training Epoch: 20 [39680/50048]	Loss: 1.4318
Training Epoch: 20 [39808/50048]	Loss: 1.4082
Training Epoch: 20 [39936/50048]	Loss: 1.0469
Training Epoch: 20 [40064/50048]	Loss: 1.5300
Training Epoch: 20 [40192/50048]	Loss: 1.2404
Training Epoch: 20 [40320/50048]	Loss: 1.3276
Training Epoch: 20 [40448/50048]	Loss: 1.3022
Training Epoch: 20 [40576/50048]	Loss: 1.2985
Training Epoch: 20 [40704/50048]	Loss: 1.3565
Training Epoch: 20 [40832/50048]	Loss: 1.3620
Training Epoch: 20 [40960/50048]	Loss: 1.6440
Training Epoch: 20 [41088/50048]	Loss: 1.1905
Training Epoch: 20 [41216/50048]	Loss: 1.3440
Training Epoch: 20 [41344/50048]	Loss: 1.4767
Training Epoch: 20 [41472/50048]	Loss: 1.1810
Training Epoch: 20 [41600/50048]	Loss: 1.3088
Training Epoch: 20 [41728/50048]	Loss: 1.1242
Training Epoch: 20 [41856/50048]	Loss: 1.2660
Training Epoch: 20 [41984/50048]	Loss: 1.3815
Training Epoch: 20 [42112/50048]	Loss: 1.3343
Training Epoch: 20 [42240/50048]	Loss: 1.0066
Training Epoch: 20 [42368/50048]	Loss: 1.4853
Training Epoch: 20 [42496/50048]	Loss: 1.3955
Training Epoch: 20 [42624/50048]	Loss: 1.1096
Training Epoch: 20 [42752/50048]	Loss: 1.2613
Training Epoch: 20 [42880/50048]	Loss: 1.2272
Training Epoch: 20 [43008/50048]	Loss: 1.3285
Training Epoch: 20 [43136/50048]	Loss: 1.4759
Training Epoch: 20 [43264/50048]	Loss: 1.4778
Training Epoch: 20 [43392/50048]	Loss: 1.2695
Training Epoch: 20 [43520/50048]	Loss: 1.2280
Training Epoch: 20 [43648/50048]	Loss: 1.4297
Training Epoch: 20 [43776/50048]	Loss: 1.2396
Training Epoch: 20 [43904/50048]	Loss: 1.3964
Training Epoch: 20 [44032/50048]	Loss: 1.3477
Training Epoch: 20 [44160/50048]	Loss: 1.3116
Training Epoch: 20 [44288/50048]	Loss: 1.2609
Training Epoch: 20 [44416/50048]	Loss: 1.0294
Training Epoch: 20 [44544/50048]	Loss: 1.0458
Training Epoch: 20 [44672/50048]	Loss: 1.0528
Training Epoch: 20 [44800/50048]	Loss: 1.1923
Training Epoch: 20 [44928/50048]	Loss: 1.1097
Training Epoch: 20 [45056/50048]	Loss: 1.0901
Training Epoch: 20 [45184/50048]	Loss: 1.2834
Training Epoch: 20 [45312/50048]	Loss: 1.4826
Training Epoch: 20 [45440/50048]	Loss: 1.4694
Training Epoch: 20 [45568/50048]	Loss: 1.5213
Training Epoch: 20 [45696/50048]	Loss: 1.3917
Training Epoch: 20 [45824/50048]	Loss: 1.1949
Training Epoch: 20 [45952/50048]	Loss: 1.2011
Training Epoch: 20 [46080/50048]	Loss: 1.4055
Training Epoch: 20 [46208/50048]	Loss: 1.2620
Training Epoch: 20 [46336/50048]	Loss: 1.2606
Training Epoch: 20 [46464/50048]	Loss: 1.3793
Training Epoch: 20 [46592/50048]	Loss: 1.1305
Training Epoch: 20 [46720/50048]	Loss: 1.2865
Training Epoch: 20 [46848/50048]	Loss: 1.3867
Training Epoch: 20 [46976/50048]	Loss: 1.3471
Training Epoch: 20 [47104/50048]	Loss: 1.2725
Training Epoch: 20 [47232/50048]	Loss: 1.3702
Training Epoch: 20 [47360/50048]	Loss: 1.5451
Training Epoch: 20 [47488/50048]	Loss: 1.3543
Training Epoch: 20 [47616/50048]	Loss: 1.3650
Training Epoch: 20 [47744/50048]	Loss: 1.3198
Training Epoch: 20 [47872/50048]	Loss: 1.2632
Training Epoch: 20 [48000/50048]	Loss: 1.2367
Training Epoch: 20 [48128/50048]	Loss: 1.0057
Training Epoch: 20 [48256/50048]	Loss: 1.4390
Training Epoch: 20 [48384/50048]	Loss: 1.2450
Training Epoch: 20 [48512/50048]	Loss: 1.2292
Training Epoch: 20 [48640/50048]	Loss: 1.1824
Training Epoch: 20 [48768/50048]	Loss: 1.1922
Training Epoch: 20 [48896/50048]	Loss: 1.5107
Training Epoch: 20 [49024/50048]	Loss: 1.3718
Training Epoch: 20 [49152/50048]	Loss: 1.2718
Training Epoch: 20 [49280/50048]	Loss: 1.3021
Training Epoch: 20 [49408/50048]	Loss: 1.3124
Training Epoch: 20 [49536/50048]	Loss: 1.2575
Training Epoch: 20 [49664/50048]	Loss: 1.0828
Training Epoch: 20 [49792/50048]	Loss: 1.3508
Training Epoch: 20 [49920/50048]	Loss: 1.3828
Training Epoch: 20 [50048/50048]	Loss: 1.2513
Validation Epoch: 20, Average loss: 0.0121, Accuracy: 0.5791
Training Epoch: 21 [128/50048]	Loss: 1.2014
Training Epoch: 21 [256/50048]	Loss: 1.2286
Training Epoch: 21 [384/50048]	Loss: 1.2507
Training Epoch: 21 [512/50048]	Loss: 1.4552
Training Epoch: 21 [640/50048]	Loss: 1.1365
Training Epoch: 21 [768/50048]	Loss: 1.2695
Training Epoch: 21 [896/50048]	Loss: 1.4012
Training Epoch: 21 [1024/50048]	Loss: 1.1300
Training Epoch: 21 [1152/50048]	Loss: 1.0750
Training Epoch: 21 [1280/50048]	Loss: 1.1681
Training Epoch: 21 [1408/50048]	Loss: 1.4026
Training Epoch: 21 [1536/50048]	Loss: 1.4479
Training Epoch: 21 [1664/50048]	Loss: 1.2522
Training Epoch: 21 [1792/50048]	Loss: 1.1776
Training Epoch: 21 [1920/50048]	Loss: 1.5190
Training Epoch: 21 [2048/50048]	Loss: 1.2467
Training Epoch: 21 [2176/50048]	Loss: 1.3789
Training Epoch: 21 [2304/50048]	Loss: 1.0940
Training Epoch: 21 [2432/50048]	Loss: 1.2922
Training Epoch: 21 [2560/50048]	Loss: 1.1168
Training Epoch: 21 [2688/50048]	Loss: 1.2270
Training Epoch: 21 [2816/50048]	Loss: 1.0830
Training Epoch: 21 [2944/50048]	Loss: 1.2284
Training Epoch: 21 [3072/50048]	Loss: 1.1569
Training Epoch: 21 [3200/50048]	Loss: 1.3011
Training Epoch: 21 [3328/50048]	Loss: 1.4031
Training Epoch: 21 [3456/50048]	Loss: 1.3694
Training Epoch: 21 [3584/50048]	Loss: 1.1338
Training Epoch: 21 [3712/50048]	Loss: 1.3126
Training Epoch: 21 [3840/50048]	Loss: 1.2919
Training Epoch: 21 [3968/50048]	Loss: 1.3285
Training Epoch: 21 [4096/50048]	Loss: 1.0714
Training Epoch: 21 [4224/50048]	Loss: 1.3836
Training Epoch: 21 [4352/50048]	Loss: 1.0195
Training Epoch: 21 [4480/50048]	Loss: 1.4204
Training Epoch: 21 [4608/50048]	Loss: 1.4835
Training Epoch: 21 [4736/50048]	Loss: 0.9944
Training Epoch: 21 [4864/50048]	Loss: 1.5076
Training Epoch: 21 [4992/50048]	Loss: 1.3954
Training Epoch: 21 [5120/50048]	Loss: 1.2476
Training Epoch: 21 [5248/50048]	Loss: 1.2265
Training Epoch: 21 [5376/50048]	Loss: 1.2562
Training Epoch: 21 [5504/50048]	Loss: 1.1314
Training Epoch: 21 [5632/50048]	Loss: 1.3578
Training Epoch: 21 [5760/50048]	Loss: 1.2331
Training Epoch: 21 [5888/50048]	Loss: 1.3392
Training Epoch: 21 [6016/50048]	Loss: 1.1824
Training Epoch: 21 [6144/50048]	Loss: 1.4341
Training Epoch: 21 [6272/50048]	Loss: 1.2199
Training Epoch: 21 [6400/50048]	Loss: 1.2846
Training Epoch: 21 [6528/50048]	Loss: 1.3563
Training Epoch: 21 [6656/50048]	Loss: 1.2749
Training Epoch: 21 [6784/50048]	Loss: 1.2966
Training Epoch: 21 [6912/50048]	Loss: 1.3094
Training Epoch: 21 [7040/50048]	Loss: 1.1341
Training Epoch: 21 [7168/50048]	Loss: 1.4540
Training Epoch: 21 [7296/50048]	Loss: 1.2870
Training Epoch: 21 [7424/50048]	Loss: 1.1026
Training Epoch: 21 [7552/50048]	Loss: 1.1539
Training Epoch: 21 [7680/50048]	Loss: 1.2360
Training Epoch: 21 [7808/50048]	Loss: 1.2008
Training Epoch: 21 [7936/50048]	Loss: 1.1584
Training Epoch: 21 [8064/50048]	Loss: 1.2300
Training Epoch: 21 [8192/50048]	Loss: 1.1172
Training Epoch: 21 [8320/50048]	Loss: 1.3348
Training Epoch: 21 [8448/50048]	Loss: 1.1550
Training Epoch: 21 [8576/50048]	Loss: 1.2608
Training Epoch: 21 [8704/50048]	Loss: 1.1938
Training Epoch: 21 [8832/50048]	Loss: 1.1947
Training Epoch: 21 [8960/50048]	Loss: 1.2541
Training Epoch: 21 [9088/50048]	Loss: 1.2850
Training Epoch: 21 [9216/50048]	Loss: 1.0652
Training Epoch: 21 [9344/50048]	Loss: 1.4595
Training Epoch: 21 [9472/50048]	Loss: 1.1556
Training Epoch: 21 [9600/50048]	Loss: 1.5490
Training Epoch: 21 [9728/50048]	Loss: 1.3326
Training Epoch: 21 [9856/50048]	Loss: 1.2099
Training Epoch: 21 [9984/50048]	Loss: 0.9411
Training Epoch: 21 [10112/50048]	Loss: 1.1869
Training Epoch: 21 [10240/50048]	Loss: 1.1267
Training Epoch: 21 [10368/50048]	Loss: 1.2527
Training Epoch: 21 [10496/50048]	Loss: 1.1526
Training Epoch: 21 [10624/50048]	Loss: 1.4267
Training Epoch: 21 [10752/50048]	Loss: 0.9411
Training Epoch: 21 [10880/50048]	Loss: 1.1681
Training Epoch: 21 [11008/50048]	Loss: 1.3696
Training Epoch: 21 [11136/50048]	Loss: 1.1996
Training Epoch: 21 [11264/50048]	Loss: 1.1252
Training Epoch: 21 [11392/50048]	Loss: 1.3519
Training Epoch: 21 [11520/50048]	Loss: 1.6331
Training Epoch: 21 [11648/50048]	Loss: 1.2349
Training Epoch: 21 [11776/50048]	Loss: 1.1909
Training Epoch: 21 [11904/50048]	Loss: 1.1220
Training Epoch: 21 [12032/50048]	Loss: 1.1640
Training Epoch: 21 [12160/50048]	Loss: 1.0155
Training Epoch: 21 [12288/50048]	Loss: 1.0851
Training Epoch: 21 [12416/50048]	Loss: 1.3635
Training Epoch: 21 [12544/50048]	Loss: 1.3705
Training Epoch: 21 [12672/50048]	Loss: 1.1724
Training Epoch: 21 [12800/50048]	Loss: 1.2250
Training Epoch: 21 [12928/50048]	Loss: 1.3184
Training Epoch: 21 [13056/50048]	Loss: 1.1952
Training Epoch: 21 [13184/50048]	Loss: 1.3498
Training Epoch: 21 [13312/50048]	Loss: 1.3083
Training Epoch: 21 [13440/50048]	Loss: 1.3813
Training Epoch: 21 [13568/50048]	Loss: 1.1161
Training Epoch: 21 [13696/50048]	Loss: 1.2657
Training Epoch: 21 [13824/50048]	Loss: 1.4269
Training Epoch: 21 [13952/50048]	Loss: 1.2327
Training Epoch: 21 [14080/50048]	Loss: 1.1720
Training Epoch: 21 [14208/50048]	Loss: 1.1213
Training Epoch: 21 [14336/50048]	Loss: 1.1033
Training Epoch: 21 [14464/50048]	Loss: 1.3066
Training Epoch: 21 [14592/50048]	Loss: 1.2062
Training Epoch: 21 [14720/50048]	Loss: 1.3740
Training Epoch: 21 [14848/50048]	Loss: 1.2979
Training Epoch: 21 [14976/50048]	Loss: 1.2317
Training Epoch: 21 [15104/50048]	Loss: 1.2329
Training Epoch: 21 [15232/50048]	Loss: 1.3465
Training Epoch: 21 [15360/50048]	Loss: 1.3268
Training Epoch: 21 [15488/50048]	Loss: 1.2739
Training Epoch: 21 [15616/50048]	Loss: 1.0110
Training Epoch: 21 [15744/50048]	Loss: 0.9757
Training Epoch: 21 [15872/50048]	Loss: 1.4261
Training Epoch: 21 [16000/50048]	Loss: 1.2297
Training Epoch: 21 [16128/50048]	Loss: 1.4986
Training Epoch: 21 [16256/50048]	Loss: 1.4839
Training Epoch: 21 [16384/50048]	Loss: 1.4050
Training Epoch: 21 [16512/50048]	Loss: 1.2176
Training Epoch: 21 [16640/50048]	Loss: 1.0010
Training Epoch: 21 [16768/50048]	Loss: 1.2328
Training Epoch: 21 [16896/50048]	Loss: 1.3087
Training Epoch: 21 [17024/50048]	Loss: 1.3302
Training Epoch: 21 [17152/50048]	Loss: 1.1265
Training Epoch: 21 [17280/50048]	Loss: 1.3906
Training Epoch: 21 [17408/50048]	Loss: 1.1840
Training Epoch: 21 [17536/50048]	Loss: 1.1285
Training Epoch: 21 [17664/50048]	Loss: 1.1782
Training Epoch: 21 [17792/50048]	Loss: 1.2597
Training Epoch: 21 [17920/50048]	Loss: 1.3663
Training Epoch: 21 [18048/50048]	Loss: 1.3820
Training Epoch: 21 [18176/50048]	Loss: 1.2968
Training Epoch: 21 [18304/50048]	Loss: 1.1400
Training Epoch: 21 [18432/50048]	Loss: 1.3678
Training Epoch: 21 [18560/50048]	Loss: 1.3517
Training Epoch: 21 [18688/50048]	Loss: 1.1130
Training Epoch: 21 [18816/50048]	Loss: 1.3202
Training Epoch: 21 [18944/50048]	Loss: 1.3838
Training Epoch: 21 [19072/50048]	Loss: 1.3215
Training Epoch: 21 [19200/50048]	Loss: 1.3489
Training Epoch: 21 [19328/50048]	Loss: 1.1298
Training Epoch: 21 [19456/50048]	Loss: 1.1174
Training Epoch: 21 [19584/50048]	Loss: 1.2515
Training Epoch: 21 [19712/50048]	Loss: 1.2101
Training Epoch: 21 [19840/50048]	Loss: 1.3652
Training Epoch: 21 [19968/50048]	Loss: 1.5165
Training Epoch: 21 [20096/50048]	Loss: 1.3608
Training Epoch: 21 [20224/50048]	Loss: 1.2589
Training Epoch: 21 [20352/50048]	Loss: 1.3571
Training Epoch: 21 [20480/50048]	Loss: 1.3911
Training Epoch: 21 [20608/50048]	Loss: 1.4859
Training Epoch: 21 [20736/50048]	Loss: 1.3126
Training Epoch: 21 [20864/50048]	Loss: 1.3552
Training Epoch: 21 [20992/50048]	Loss: 1.2504
Training Epoch: 21 [21120/50048]	Loss: 1.2784
Training Epoch: 21 [21248/50048]	Loss: 1.3258
Training Epoch: 21 [21376/50048]	Loss: 0.8996
Training Epoch: 21 [21504/50048]	Loss: 1.1293
Training Epoch: 21 [21632/50048]	Loss: 1.1790
Training Epoch: 21 [21760/50048]	Loss: 1.0521
Training Epoch: 21 [21888/50048]	Loss: 1.2084
Training Epoch: 21 [22016/50048]	Loss: 1.0500
Training Epoch: 21 [22144/50048]	Loss: 1.4079
Training Epoch: 21 [22272/50048]	Loss: 1.1242
Training Epoch: 21 [22400/50048]	Loss: 1.1884
Training Epoch: 21 [22528/50048]	Loss: 1.3930
Training Epoch: 21 [22656/50048]	Loss: 1.0137
Training Epoch: 21 [22784/50048]	Loss: 1.1853
Training Epoch: 21 [22912/50048]	Loss: 1.2520
Training Epoch: 21 [23040/50048]	Loss: 1.2286
Training Epoch: 21 [23168/50048]	Loss: 1.4076
Training Epoch: 21 [23296/50048]	Loss: 1.5254
Training Epoch: 21 [23424/50048]	Loss: 1.1753
Training Epoch: 21 [23552/50048]	Loss: 1.1004
Training Epoch: 21 [23680/50048]	Loss: 1.1879
Training Epoch: 21 [23808/50048]	Loss: 1.5054
Training Epoch: 21 [23936/50048]	Loss: 1.3219
Training Epoch: 21 [24064/50048]	Loss: 1.3893
Training Epoch: 21 [24192/50048]	Loss: 1.2043
Training Epoch: 21 [24320/50048]	Loss: 1.1110
Training Epoch: 21 [24448/50048]	Loss: 1.1233
Training Epoch: 21 [24576/50048]	Loss: 1.3110
Training Epoch: 21 [24704/50048]	Loss: 1.5120
Training Epoch: 21 [24832/50048]	Loss: 1.3987
Training Epoch: 21 [24960/50048]	Loss: 1.5234
Training Epoch: 21 [25088/50048]	Loss: 1.1949
Training Epoch: 21 [25216/50048]	Loss: 1.5232
Training Epoch: 21 [25344/50048]	Loss: 1.1801
Training Epoch: 21 [25472/50048]	Loss: 1.2991
Training Epoch: 21 [25600/50048]	Loss: 1.2669
Training Epoch: 21 [25728/50048]	Loss: 1.4543
Training Epoch: 21 [25856/50048]	Loss: 1.2991
Training Epoch: 21 [25984/50048]	Loss: 1.1892
Training Epoch: 21 [26112/50048]	Loss: 1.4761
Training Epoch: 21 [26240/50048]	Loss: 1.3450
Training Epoch: 21 [26368/50048]	Loss: 1.2605
Training Epoch: 21 [26496/50048]	Loss: 1.3188
Training Epoch: 21 [26624/50048]	Loss: 1.1842
Training Epoch: 21 [26752/50048]	Loss: 1.3015
Training Epoch: 21 [26880/50048]	Loss: 1.1958
Training Epoch: 21 [27008/50048]	Loss: 1.2729
Training Epoch: 21 [27136/50048]	Loss: 1.3601
Training Epoch: 21 [27264/50048]	Loss: 1.4125
Training Epoch: 21 [27392/50048]	Loss: 1.3254
Training Epoch: 21 [27520/50048]	Loss: 1.1626
Training Epoch: 21 [27648/50048]	Loss: 1.0325
Training Epoch: 21 [27776/50048]	Loss: 1.2292
Training Epoch: 21 [27904/50048]	Loss: 1.2313
Training Epoch: 21 [28032/50048]	Loss: 1.2918
Training Epoch: 21 [28160/50048]	Loss: 1.0652
Training Epoch: 21 [28288/50048]	Loss: 1.1519
Training Epoch: 21 [28416/50048]	Loss: 1.4073
Training Epoch: 21 [28544/50048]	Loss: 1.4019
Training Epoch: 21 [28672/50048]	Loss: 1.5242
Training Epoch: 21 [28800/50048]	Loss: 1.3530
Training Epoch: 21 [28928/50048]	Loss: 1.2785
Training Epoch: 21 [29056/50048]	Loss: 1.3201
Training Epoch: 21 [29184/50048]	Loss: 1.1593
Training Epoch: 21 [29312/50048]	Loss: 1.2215
Training Epoch: 21 [29440/50048]	Loss: 1.4456
Training Epoch: 21 [29568/50048]	Loss: 1.2335
Training Epoch: 21 [29696/50048]	Loss: 1.1130
Training Epoch: 21 [29824/50048]	Loss: 1.3671
Training Epoch: 21 [29952/50048]	Loss: 1.3310
Training Epoch: 21 [30080/50048]	Loss: 1.2251
Training Epoch: 21 [30208/50048]	Loss: 1.4651
Training Epoch: 21 [30336/50048]	Loss: 1.4283
Training Epoch: 21 [30464/50048]	Loss: 1.4012
Training Epoch: 21 [30592/50048]	Loss: 1.3522
Training Epoch: 21 [30720/50048]	Loss: 1.2395
Training Epoch: 21 [30848/50048]	Loss: 1.2162
Training Epoch: 21 [30976/50048]	Loss: 1.2851
Training Epoch: 21 [31104/50048]	Loss: 1.3205
Training Epoch: 21 [31232/50048]	Loss: 1.2617
Training Epoch: 21 [31360/50048]	Loss: 1.1249
Training Epoch: 21 [31488/50048]	Loss: 1.5032
Training Epoch: 21 [31616/50048]	Loss: 1.4812
Training Epoch: 21 [31744/50048]	Loss: 1.3848
Training Epoch: 21 [31872/50048]	Loss: 1.2519
Training Epoch: 21 [32000/50048]	Loss: 1.2564
Training Epoch: 21 [32128/50048]	Loss: 1.1843
Training Epoch: 21 [32256/50048]	Loss: 1.2906
Training Epoch: 21 [32384/50048]	Loss: 1.3274
Training Epoch: 21 [32512/50048]	Loss: 1.3253
Training Epoch: 21 [32640/50048]	Loss: 1.0686
Training Epoch: 21 [32768/50048]	Loss: 1.3616
Training Epoch: 21 [32896/50048]	Loss: 1.4605
Training Epoch: 21 [33024/50048]	Loss: 1.1697
Training Epoch: 21 [33152/50048]	Loss: 1.3504
Training Epoch: 21 [33280/50048]	Loss: 1.2986
Training Epoch: 21 [33408/50048]	Loss: 1.2476
Training Epoch: 21 [33536/50048]	Loss: 1.1099
Training Epoch: 21 [33664/50048]	Loss: 1.0382
Training Epoch: 21 [33792/50048]	Loss: 1.3406
Training Epoch: 21 [33920/50048]	Loss: 1.0964
Training Epoch: 21 [34048/50048]	Loss: 1.3733
Training Epoch: 21 [34176/50048]	Loss: 1.0067
Training Epoch: 21 [34304/50048]	Loss: 1.0797
Training Epoch: 21 [34432/50048]	Loss: 1.2540
Training Epoch: 21 [34560/50048]	Loss: 1.1954
Training Epoch: 21 [34688/50048]	Loss: 1.2886
Training Epoch: 21 [34816/50048]	Loss: 1.2282
Training Epoch: 21 [34944/50048]	Loss: 1.2953
Training Epoch: 21 [35072/50048]	Loss: 1.2617
Training Epoch: 21 [35200/50048]	Loss: 1.3753
Training Epoch: 21 [35328/50048]	Loss: 1.1442
Training Epoch: 21 [35456/50048]	Loss: 1.0577
Training Epoch: 21 [35584/50048]	Loss: 1.4767
Training Epoch: 21 [35712/50048]	Loss: 1.3709
Training Epoch: 21 [35840/50048]	Loss: 1.0925
Training Epoch: 21 [35968/50048]	Loss: 1.4310
Training Epoch: 21 [36096/50048]	Loss: 1.2632
Training Epoch: 21 [36224/50048]	Loss: 1.5292
Training Epoch: 21 [36352/50048]	Loss: 1.1817
Training Epoch: 21 [36480/50048]	Loss: 1.6579
Training Epoch: 21 [36608/50048]	Loss: 1.3283
Training Epoch: 21 [36736/50048]	Loss: 1.3124
Training Epoch: 21 [36864/50048]	Loss: 1.1257
Training Epoch: 21 [36992/50048]	Loss: 1.1127
Training Epoch: 21 [37120/50048]	Loss: 1.1784
Training Epoch: 21 [37248/50048]	Loss: 1.5659
Training Epoch: 21 [37376/50048]	Loss: 1.1617
Training Epoch: 21 [37504/50048]	Loss: 1.1544
Training Epoch: 21 [37632/50048]	Loss: 1.3437
Training Epoch: 21 [37760/50048]	Loss: 1.1983
Training Epoch: 21 [37888/50048]	Loss: 1.1736
Training Epoch: 21 [38016/50048]	Loss: 1.0322
Training Epoch: 21 [38144/50048]	Loss: 1.2118
Training Epoch: 21 [38272/50048]	Loss: 1.2618
Training Epoch: 21 [38400/50048]	Loss: 1.4112
Training Epoch: 21 [38528/50048]	Loss: 1.1591
Training Epoch: 21 [38656/50048]	Loss: 1.1208
Training Epoch: 21 [38784/50048]	Loss: 1.3902
Training Epoch: 21 [38912/50048]	Loss: 1.2515
Training Epoch: 21 [39040/50048]	Loss: 1.2382
Training Epoch: 21 [39168/50048]	Loss: 1.1359
Training Epoch: 21 [39296/50048]	Loss: 1.3126
Training Epoch: 21 [39424/50048]	Loss: 1.3218
Training Epoch: 21 [39552/50048]	Loss: 1.3230
Training Epoch: 21 [39680/50048]	Loss: 1.2231
Training Epoch: 21 [39808/50048]	Loss: 1.0507
Training Epoch: 21 [39936/50048]	Loss: 1.3277
Training Epoch: 21 [40064/50048]	Loss: 1.1934
Training Epoch: 21 [40192/50048]	Loss: 1.2300
Training Epoch: 21 [40320/50048]	Loss: 1.0990
Training Epoch: 21 [40448/50048]	Loss: 1.2199
Training Epoch: 21 [40576/50048]	Loss: 1.2833
Training Epoch: 21 [40704/50048]	Loss: 1.3486
Training Epoch: 21 [40832/50048]	Loss: 1.1799
Training Epoch: 21 [40960/50048]	Loss: 1.2722
Training Epoch: 21 [41088/50048]	Loss: 1.3398
Training Epoch: 21 [41216/50048]	Loss: 1.2492
Training Epoch: 21 [41344/50048]	Loss: 1.1378
Training Epoch: 21 [41472/50048]	Loss: 1.6067
Training Epoch: 21 [41600/50048]	Loss: 1.2145
Training Epoch: 21 [41728/50048]	Loss: 1.4261
Training Epoch: 21 [41856/50048]	Loss: 1.4112
Training Epoch: 21 [41984/50048]	Loss: 1.1311
Training Epoch: 21 [42112/50048]	Loss: 1.2226
Training Epoch: 21 [42240/50048]	Loss: 1.1742
Training Epoch: 21 [42368/50048]	Loss: 1.3068
Training Epoch: 21 [42496/50048]	Loss: 1.3084
Training Epoch: 21 [42624/50048]	Loss: 1.2932
Training Epoch: 21 [42752/50048]	Loss: 1.1500
Training Epoch: 21 [42880/50048]	Loss: 1.3136
Training Epoch: 21 [43008/50048]	Loss: 1.2179
Training Epoch: 21 [43136/50048]	Loss: 1.3970
Training Epoch: 21 [43264/50048]	Loss: 1.2146
Training Epoch: 21 [43392/50048]	Loss: 1.5196
Training Epoch: 21 [43520/50048]	Loss: 1.2563
Training Epoch: 21 [43648/50048]	Loss: 1.4647
Training Epoch: 21 [43776/50048]	Loss: 1.3026
Training Epoch: 21 [43904/50048]	Loss: 1.2488
Training Epoch: 21 [44032/50048]	Loss: 1.3657
Training Epoch: 21 [44160/50048]	Loss: 1.1742
Training Epoch: 21 [44288/50048]	Loss: 1.2508
Training Epoch: 21 [44416/50048]	Loss: 1.0518
Training Epoch: 21 [44544/50048]	Loss: 1.3220
Training Epoch: 21 [44672/50048]	Loss: 1.2952
Training Epoch: 21 [44800/50048]	Loss: 1.0739
Training Epoch: 21 [44928/50048]	Loss: 1.4316
Training Epoch: 21 [45056/50048]	Loss: 1.1085
Training Epoch: 21 [45184/50048]	Loss: 0.9973
Training Epoch: 21 [45312/50048]	Loss: 1.2963
Training Epoch: 21 [45440/50048]	Loss: 1.1107
Training Epoch: 21 [45568/50048]	Loss: 1.2290
Training Epoch: 21 [45696/50048]	Loss: 1.2868
Training Epoch: 21 [45824/50048]	Loss: 1.2413
Training Epoch: 21 [45952/50048]	Loss: 1.3062
Training Epoch: 21 [46080/50048]	Loss: 1.2371
Training Epoch: 21 [46208/50048]	Loss: 1.3337
Training Epoch: 21 [46336/50048]	Loss: 1.7056
Training Epoch: 21 [46464/50048]	Loss: 1.5524
Training Epoch: 21 [46592/50048]	Loss: 1.2827
Training Epoch: 21 [46720/50048]	Loss: 1.4536
Training Epoch: 21 [46848/50048]	Loss: 1.2740
Training Epoch: 21 [46976/50048]	Loss: 1.1126
Training Epoch: 21 [47104/50048]	Loss: 1.2311
Training Epoch: 21 [47232/50048]	Loss: 1.2481
Training Epoch: 21 [47360/50048]	Loss: 1.3419
Training Epoch: 21 [47488/50048]	Loss: 1.6931
Training Epoch: 21 [47616/50048]	Loss: 1.1288
Training Epoch: 21 [47744/50048]	Loss: 1.3104
Training Epoch: 21 [47872/50048]	Loss: 1.1361
Training Epoch: 21 [48000/50048]	Loss: 1.1350
Training Epoch: 21 [48128/50048]	Loss: 1.1965
Training Epoch: 21 [48256/50048]	Loss: 1.4583
Training Epoch: 21 [48384/50048]	Loss: 1.3424
Training Epoch: 21 [48512/50048]	Loss: 1.2695
Training Epoch: 21 [48640/50048]	Loss: 1.2831
Training Epoch: 21 [48768/50048]	Loss: 1.2527
Training Epoch: 21 [48896/50048]	Loss: 1.4650
Training Epoch: 21 [49024/50048]	Loss: 1.4858
Training Epoch: 21 [49152/50048]	Loss: 1.4063
Training Epoch: 21 [49280/50048]	Loss: 0.9278
Training Epoch: 21 [49408/50048]	Loss: 1.2343
Training Epoch: 21 [49536/50048]	Loss: 1.0383
Training Epoch: 21 [49664/50048]	Loss: 1.2366
Training Epoch: 21 [49792/50048]	Loss: 1.2228
Training Epoch: 21 [49920/50048]	Loss: 1.1985
Training Epoch: 21 [50048/50048]	Loss: 1.3077
Validation Epoch: 21, Average loss: 0.0121, Accuracy: 0.5825
Training Epoch: 22 [128/50048]	Loss: 1.3191
Training Epoch: 22 [256/50048]	Loss: 1.1613
Training Epoch: 22 [384/50048]	Loss: 1.2211
Training Epoch: 22 [512/50048]	Loss: 1.1783
Training Epoch: 22 [640/50048]	Loss: 1.2262
Training Epoch: 22 [768/50048]	Loss: 1.2419
Training Epoch: 22 [896/50048]	Loss: 1.4642
Training Epoch: 22 [1024/50048]	Loss: 1.1062
Training Epoch: 22 [1152/50048]	Loss: 1.1614
Training Epoch: 22 [1280/50048]	Loss: 1.2096
Training Epoch: 22 [1408/50048]	Loss: 1.1814
Training Epoch: 22 [1536/50048]	Loss: 1.5202
Training Epoch: 22 [1664/50048]	Loss: 1.3139
Training Epoch: 22 [1792/50048]	Loss: 1.2031
Training Epoch: 22 [1920/50048]	Loss: 1.0713
Training Epoch: 22 [2048/50048]	Loss: 1.2007
Training Epoch: 22 [2176/50048]	Loss: 1.2640
Training Epoch: 22 [2304/50048]	Loss: 1.0726
Training Epoch: 22 [2432/50048]	Loss: 1.1116
Training Epoch: 22 [2560/50048]	Loss: 1.2448
Training Epoch: 22 [2688/50048]	Loss: 1.1492
Training Epoch: 22 [2816/50048]	Loss: 1.1597
Training Epoch: 22 [2944/50048]	Loss: 1.2587
Training Epoch: 22 [3072/50048]	Loss: 1.1414
Training Epoch: 22 [3200/50048]	Loss: 1.2342
Training Epoch: 22 [3328/50048]	Loss: 1.3478
Training Epoch: 22 [3456/50048]	Loss: 1.2425
Training Epoch: 22 [3584/50048]	Loss: 1.2060
Training Epoch: 22 [3712/50048]	Loss: 1.0430
Training Epoch: 22 [3840/50048]	Loss: 1.2251
Training Epoch: 22 [3968/50048]	Loss: 1.2304
Training Epoch: 22 [4096/50048]	Loss: 1.2738
Training Epoch: 22 [4224/50048]	Loss: 1.1518
Training Epoch: 22 [4352/50048]	Loss: 1.1790
Training Epoch: 22 [4480/50048]	Loss: 1.0053
Training Epoch: 22 [4608/50048]	Loss: 1.1776
Training Epoch: 22 [4736/50048]	Loss: 1.1687
Training Epoch: 22 [4864/50048]	Loss: 1.1346
Training Epoch: 22 [4992/50048]	Loss: 1.2830
Training Epoch: 22 [5120/50048]	Loss: 1.0864
Training Epoch: 22 [5248/50048]	Loss: 1.2614
Training Epoch: 22 [5376/50048]	Loss: 1.1207
Training Epoch: 22 [5504/50048]	Loss: 1.3161
Training Epoch: 22 [5632/50048]	Loss: 1.1928
Training Epoch: 22 [5760/50048]	Loss: 1.4235
Training Epoch: 22 [5888/50048]	Loss: 1.2799
Training Epoch: 22 [6016/50048]	Loss: 1.1744
Training Epoch: 22 [6144/50048]	Loss: 1.1757
Training Epoch: 22 [6272/50048]	Loss: 1.4132
Training Epoch: 22 [6400/50048]	Loss: 0.9886
Training Epoch: 22 [6528/50048]	Loss: 1.3631
Training Epoch: 22 [6656/50048]	Loss: 1.2855
Training Epoch: 22 [6784/50048]	Loss: 1.1172
Training Epoch: 22 [6912/50048]	Loss: 1.3385
Training Epoch: 22 [7040/50048]	Loss: 1.3389
Training Epoch: 22 [7168/50048]	Loss: 1.4518
Training Epoch: 22 [7296/50048]	Loss: 1.2719
Training Epoch: 22 [7424/50048]	Loss: 1.2246
Training Epoch: 22 [7552/50048]	Loss: 1.1888
Training Epoch: 22 [7680/50048]	Loss: 1.3541
Training Epoch: 22 [7808/50048]	Loss: 1.1760
Training Epoch: 22 [7936/50048]	Loss: 1.2283
Training Epoch: 22 [8064/50048]	Loss: 1.3312
Training Epoch: 22 [8192/50048]	Loss: 1.1441
Training Epoch: 22 [8320/50048]	Loss: 1.3982
Training Epoch: 22 [8448/50048]	Loss: 1.1682
Training Epoch: 22 [8576/50048]	Loss: 1.5134
Training Epoch: 22 [8704/50048]	Loss: 1.2163
Training Epoch: 22 [8832/50048]	Loss: 1.1489
Training Epoch: 22 [8960/50048]	Loss: 1.1439
Training Epoch: 22 [9088/50048]	Loss: 1.1904
Training Epoch: 22 [9216/50048]	Loss: 1.1296
Training Epoch: 22 [9344/50048]	Loss: 1.3296
Training Epoch: 22 [9472/50048]	Loss: 1.1747
Training Epoch: 22 [9600/50048]	Loss: 1.4225
Training Epoch: 22 [9728/50048]	Loss: 1.1692
Training Epoch: 22 [9856/50048]	Loss: 1.2387
Training Epoch: 22 [9984/50048]	Loss: 1.1393
Training Epoch: 22 [10112/50048]	Loss: 1.0840
Training Epoch: 22 [10240/50048]	Loss: 1.2755
Training Epoch: 22 [10368/50048]	Loss: 1.1188
Training Epoch: 22 [10496/50048]	Loss: 1.1926
Training Epoch: 22 [10624/50048]	Loss: 0.9993
Training Epoch: 22 [10752/50048]	Loss: 1.2036
Training Epoch: 22 [10880/50048]	Loss: 1.3110
Training Epoch: 22 [11008/50048]	Loss: 1.5980
Training Epoch: 22 [11136/50048]	Loss: 1.2322
Training Epoch: 22 [11264/50048]	Loss: 1.3036
Training Epoch: 22 [11392/50048]	Loss: 1.1955
Training Epoch: 22 [11520/50048]	Loss: 1.1674
Training Epoch: 22 [11648/50048]	Loss: 1.2467
Training Epoch: 22 [11776/50048]	Loss: 1.2702
Training Epoch: 22 [11904/50048]	Loss: 1.1563
Training Epoch: 22 [12032/50048]	Loss: 1.0374
Training Epoch: 22 [12160/50048]	Loss: 1.3606
Training Epoch: 22 [12288/50048]	Loss: 1.6478
Training Epoch: 22 [12416/50048]	Loss: 1.1157
Training Epoch: 22 [12544/50048]	Loss: 1.1731
Training Epoch: 22 [12672/50048]	Loss: 1.3582
Training Epoch: 22 [12800/50048]	Loss: 1.0452
Training Epoch: 22 [12928/50048]	Loss: 1.1143
Training Epoch: 22 [13056/50048]	Loss: 1.2035
Training Epoch: 22 [13184/50048]	Loss: 1.3731
Training Epoch: 22 [13312/50048]	Loss: 1.0384
Training Epoch: 22 [13440/50048]	Loss: 1.3396
Training Epoch: 22 [13568/50048]	Loss: 1.0433
Training Epoch: 22 [13696/50048]	Loss: 1.3956
Training Epoch: 22 [13824/50048]	Loss: 1.2917
Training Epoch: 22 [13952/50048]	Loss: 1.1136
Training Epoch: 22 [14080/50048]	Loss: 1.2693
Training Epoch: 22 [14208/50048]	Loss: 1.1134
Training Epoch: 22 [14336/50048]	Loss: 1.1425
Training Epoch: 22 [14464/50048]	Loss: 1.2647
Training Epoch: 22 [14592/50048]	Loss: 1.2782
Training Epoch: 22 [14720/50048]	Loss: 1.2321
Training Epoch: 22 [14848/50048]	Loss: 1.2615
Training Epoch: 22 [14976/50048]	Loss: 1.2181
Training Epoch: 22 [15104/50048]	Loss: 1.1962
Training Epoch: 22 [15232/50048]	Loss: 1.1058
Training Epoch: 22 [15360/50048]	Loss: 1.1201
Training Epoch: 22 [15488/50048]	Loss: 1.2035
Training Epoch: 22 [15616/50048]	Loss: 1.1419
Training Epoch: 22 [15744/50048]	Loss: 1.2944
Training Epoch: 22 [15872/50048]	Loss: 0.8994
Training Epoch: 22 [16000/50048]	Loss: 1.3014
Training Epoch: 22 [16128/50048]	Loss: 1.5099
Training Epoch: 22 [16256/50048]	Loss: 1.1862
Training Epoch: 22 [16384/50048]	Loss: 1.2153
Training Epoch: 22 [16512/50048]	Loss: 1.3835
Training Epoch: 22 [16640/50048]	Loss: 1.4171
Training Epoch: 22 [16768/50048]	Loss: 0.9873
Training Epoch: 22 [16896/50048]	Loss: 1.1325
Training Epoch: 22 [17024/50048]	Loss: 1.2845
Training Epoch: 22 [17152/50048]	Loss: 1.2426
Training Epoch: 22 [17280/50048]	Loss: 1.1943
Training Epoch: 22 [17408/50048]	Loss: 1.3704
Training Epoch: 22 [17536/50048]	Loss: 1.3808
Training Epoch: 22 [17664/50048]	Loss: 1.2133
Training Epoch: 22 [17792/50048]	Loss: 1.2212
Training Epoch: 22 [17920/50048]	Loss: 1.4384
Training Epoch: 22 [18048/50048]	Loss: 0.9949
Training Epoch: 22 [18176/50048]	Loss: 1.4174
Training Epoch: 22 [18304/50048]	Loss: 1.3333
Training Epoch: 22 [18432/50048]	Loss: 1.0925
Training Epoch: 22 [18560/50048]	Loss: 1.2110
Training Epoch: 22 [18688/50048]	Loss: 1.0786
Training Epoch: 22 [18816/50048]	Loss: 1.3327
Training Epoch: 22 [18944/50048]	Loss: 1.2161
Training Epoch: 22 [19072/50048]	Loss: 1.3783
Training Epoch: 22 [19200/50048]	Loss: 1.1844
Training Epoch: 22 [19328/50048]	Loss: 1.0911
Training Epoch: 22 [19456/50048]	Loss: 1.2613
Training Epoch: 22 [19584/50048]	Loss: 1.5281
Training Epoch: 22 [19712/50048]	Loss: 1.5850
Training Epoch: 22 [19840/50048]	Loss: 1.1252
Training Epoch: 22 [19968/50048]	Loss: 1.4767
Training Epoch: 22 [20096/50048]	Loss: 1.3142
Training Epoch: 22 [20224/50048]	Loss: 1.0982
Training Epoch: 22 [20352/50048]	Loss: 1.2839
Training Epoch: 22 [20480/50048]	Loss: 1.1519
Training Epoch: 22 [20608/50048]	Loss: 1.1024
Training Epoch: 22 [20736/50048]	Loss: 1.3756
Training Epoch: 22 [20864/50048]	Loss: 1.1838
Training Epoch: 22 [20992/50048]	Loss: 1.3848
Training Epoch: 22 [21120/50048]	Loss: 1.3499
Training Epoch: 22 [21248/50048]	Loss: 1.1148
Training Epoch: 22 [21376/50048]	Loss: 1.4081
Training Epoch: 22 [21504/50048]	Loss: 1.4520
Training Epoch: 22 [21632/50048]	Loss: 1.0872
Training Epoch: 22 [21760/50048]	Loss: 1.2205
Training Epoch: 22 [21888/50048]	Loss: 1.1022
Training Epoch: 22 [22016/50048]	Loss: 1.3475
Training Epoch: 22 [22144/50048]	Loss: 1.2382
Training Epoch: 22 [22272/50048]	Loss: 1.3035
Training Epoch: 22 [22400/50048]	Loss: 1.1523
Training Epoch: 22 [22528/50048]	Loss: 1.4683
Training Epoch: 22 [22656/50048]	Loss: 1.2380
Training Epoch: 22 [22784/50048]	Loss: 1.2869
Training Epoch: 22 [22912/50048]	Loss: 1.3567
Training Epoch: 22 [23040/50048]	Loss: 1.1162
Training Epoch: 22 [23168/50048]	Loss: 1.1081
Training Epoch: 22 [23296/50048]	Loss: 1.1527
Training Epoch: 22 [23424/50048]	Loss: 1.2103
Training Epoch: 22 [23552/50048]	Loss: 1.4632
Training Epoch: 22 [23680/50048]	Loss: 1.1656
Training Epoch: 22 [23808/50048]	Loss: 1.0086
Training Epoch: 22 [23936/50048]	Loss: 1.2703
Training Epoch: 22 [24064/50048]	Loss: 1.4009
Training Epoch: 22 [24192/50048]	Loss: 1.2243
Training Epoch: 22 [24320/50048]	Loss: 1.1445
Training Epoch: 22 [24448/50048]	Loss: 1.2593
Training Epoch: 22 [24576/50048]	Loss: 1.0322
Training Epoch: 22 [24704/50048]	Loss: 1.4617
Training Epoch: 22 [24832/50048]	Loss: 1.0977
Training Epoch: 22 [24960/50048]	Loss: 1.1766
Training Epoch: 22 [25088/50048]	Loss: 1.2195
Training Epoch: 22 [25216/50048]	Loss: 1.0122
Training Epoch: 22 [25344/50048]	Loss: 1.1326
Training Epoch: 22 [25472/50048]	Loss: 1.0630
Training Epoch: 22 [25600/50048]	Loss: 1.3317
Training Epoch: 22 [25728/50048]	Loss: 1.3513
Training Epoch: 22 [25856/50048]	Loss: 1.3160
Training Epoch: 22 [25984/50048]	Loss: 1.1901
Training Epoch: 22 [26112/50048]	Loss: 1.2012
Training Epoch: 22 [26240/50048]	Loss: 1.1544
Training Epoch: 22 [26368/50048]	Loss: 1.4955
Training Epoch: 22 [26496/50048]	Loss: 1.3794
Training Epoch: 22 [26624/50048]	Loss: 0.9353
Training Epoch: 22 [26752/50048]	Loss: 1.1092
Training Epoch: 22 [26880/50048]	Loss: 0.9800
Training Epoch: 22 [27008/50048]	Loss: 1.0821
Training Epoch: 22 [27136/50048]	Loss: 1.1576
Training Epoch: 22 [27264/50048]	Loss: 1.4432
Training Epoch: 22 [27392/50048]	Loss: 1.1655
Training Epoch: 22 [27520/50048]	Loss: 1.1670
Training Epoch: 22 [27648/50048]	Loss: 1.0320
Training Epoch: 22 [27776/50048]	Loss: 1.1979
Training Epoch: 22 [27904/50048]	Loss: 1.2556
Training Epoch: 22 [28032/50048]	Loss: 1.2537
Training Epoch: 22 [28160/50048]	Loss: 1.3416
Training Epoch: 22 [28288/50048]	Loss: 1.2190
Training Epoch: 22 [28416/50048]	Loss: 0.9651
Training Epoch: 22 [28544/50048]	Loss: 1.0797
Training Epoch: 22 [28672/50048]	Loss: 1.2429
Training Epoch: 22 [28800/50048]	Loss: 1.3479
Training Epoch: 22 [28928/50048]	Loss: 1.2115
Training Epoch: 22 [29056/50048]	Loss: 1.1155
Training Epoch: 22 [29184/50048]	Loss: 1.0652
Training Epoch: 22 [29312/50048]	Loss: 1.1896
Training Epoch: 22 [29440/50048]	Loss: 1.1451
Training Epoch: 22 [29568/50048]	Loss: 1.2211
Training Epoch: 22 [29696/50048]	Loss: 1.3164
Training Epoch: 22 [29824/50048]	Loss: 1.2528
Training Epoch: 22 [29952/50048]	Loss: 1.3104
Training Epoch: 22 [30080/50048]	Loss: 1.0423
Training Epoch: 22 [30208/50048]	Loss: 1.1197
Training Epoch: 22 [30336/50048]	Loss: 1.2644
Training Epoch: 22 [30464/50048]	Loss: 1.2457
Training Epoch: 22 [30592/50048]	Loss: 1.1858
Training Epoch: 22 [30720/50048]	Loss: 1.4506
Training Epoch: 22 [30848/50048]	Loss: 1.2390
Training Epoch: 22 [30976/50048]	Loss: 1.0185
Training Epoch: 22 [31104/50048]	Loss: 1.1450
Training Epoch: 22 [31232/50048]	Loss: 1.3169
Training Epoch: 22 [31360/50048]	Loss: 1.1150
Training Epoch: 22 [31488/50048]	Loss: 1.3765
Training Epoch: 22 [31616/50048]	Loss: 1.2220
Training Epoch: 22 [31744/50048]	Loss: 1.2351
Training Epoch: 22 [31872/50048]	Loss: 1.1468
Training Epoch: 22 [32000/50048]	Loss: 1.1750
Training Epoch: 22 [32128/50048]	Loss: 1.1214
Training Epoch: 22 [32256/50048]	Loss: 1.4287
Training Epoch: 22 [32384/50048]	Loss: 1.1396
Training Epoch: 22 [32512/50048]	Loss: 1.4987
Training Epoch: 22 [32640/50048]	Loss: 1.1426
Training Epoch: 22 [32768/50048]	Loss: 1.2409
Training Epoch: 22 [32896/50048]	Loss: 1.3722
Training Epoch: 22 [33024/50048]	Loss: 1.2775
Training Epoch: 22 [33152/50048]	Loss: 1.4989
Training Epoch: 22 [33280/50048]	Loss: 1.1118
Training Epoch: 22 [33408/50048]	Loss: 1.5822
Training Epoch: 22 [33536/50048]	Loss: 1.2966
Training Epoch: 22 [33664/50048]	Loss: 1.0147
Training Epoch: 22 [33792/50048]	Loss: 1.3328
Training Epoch: 22 [33920/50048]	Loss: 1.3726
Training Epoch: 22 [34048/50048]	Loss: 1.1448
Training Epoch: 22 [34176/50048]	Loss: 1.1537
Training Epoch: 22 [34304/50048]	Loss: 1.3648
Training Epoch: 22 [34432/50048]	Loss: 1.2569
Training Epoch: 22 [34560/50048]	Loss: 1.2073
Training Epoch: 22 [34688/50048]	Loss: 1.0989
Training Epoch: 22 [34816/50048]	Loss: 1.3655
Training Epoch: 22 [34944/50048]	Loss: 1.1593
Training Epoch: 22 [35072/50048]	Loss: 1.2180
Training Epoch: 22 [35200/50048]	Loss: 1.2515
Training Epoch: 22 [35328/50048]	Loss: 1.3807
Training Epoch: 22 [35456/50048]	Loss: 1.5512
Training Epoch: 22 [35584/50048]	Loss: 1.2683
Training Epoch: 22 [35712/50048]	Loss: 1.3597
Training Epoch: 22 [35840/50048]	Loss: 1.1128
Training Epoch: 22 [35968/50048]	Loss: 1.2131
Training Epoch: 22 [36096/50048]	Loss: 1.2628
Training Epoch: 22 [36224/50048]	Loss: 1.1489
Training Epoch: 22 [36352/50048]	Loss: 1.1537
Training Epoch: 22 [36480/50048]	Loss: 1.2634
Training Epoch: 22 [36608/50048]	Loss: 1.2911
Training Epoch: 22 [36736/50048]	Loss: 1.0590
Training Epoch: 22 [36864/50048]	Loss: 1.2398
Training Epoch: 22 [36992/50048]	Loss: 1.0526
Training Epoch: 22 [37120/50048]	Loss: 1.1163
Training Epoch: 22 [37248/50048]	Loss: 1.2186
Training Epoch: 22 [37376/50048]	Loss: 1.0297
Training Epoch: 22 [37504/50048]	Loss: 1.3061
Training Epoch: 22 [37632/50048]	Loss: 1.3229
Training Epoch: 22 [37760/50048]	Loss: 1.1652
Training Epoch: 22 [37888/50048]	Loss: 1.2247
Training Epoch: 22 [38016/50048]	Loss: 1.1578
Training Epoch: 22 [38144/50048]	Loss: 1.1876
Training Epoch: 22 [38272/50048]	Loss: 1.5622
Training Epoch: 22 [38400/50048]	Loss: 1.4533
Training Epoch: 22 [38528/50048]	Loss: 1.2360
Training Epoch: 22 [38656/50048]	Loss: 1.4637
Training Epoch: 22 [38784/50048]	Loss: 1.3264
Training Epoch: 22 [38912/50048]	Loss: 1.2137
Training Epoch: 22 [39040/50048]	Loss: 1.1489
Training Epoch: 22 [39168/50048]	Loss: 1.2271
Training Epoch: 22 [39296/50048]	Loss: 0.9405
Training Epoch: 22 [39424/50048]	Loss: 1.2230
Training Epoch: 22 [39552/50048]	Loss: 1.1559
Training Epoch: 22 [39680/50048]	Loss: 0.9702
Training Epoch: 22 [39808/50048]	Loss: 1.1956
Training Epoch: 22 [39936/50048]	Loss: 1.1652
Training Epoch: 22 [40064/50048]	Loss: 1.2175
Training Epoch: 22 [40192/50048]	Loss: 1.1983
Training Epoch: 22 [40320/50048]	Loss: 1.3468
Training Epoch: 22 [40448/50048]	Loss: 1.4258
Training Epoch: 22 [40576/50048]	Loss: 1.2944
Training Epoch: 22 [40704/50048]	Loss: 0.9955
Training Epoch: 22 [40832/50048]	Loss: 1.2861
Training Epoch: 22 [40960/50048]	Loss: 1.5183
Training Epoch: 22 [41088/50048]	Loss: 1.2451
Training Epoch: 22 [41216/50048]	Loss: 1.2216
Training Epoch: 22 [41344/50048]	Loss: 1.1050
Training Epoch: 22 [41472/50048]	Loss: 1.3501
Training Epoch: 22 [41600/50048]	Loss: 1.2187
Training Epoch: 22 [41728/50048]	Loss: 1.0725
Training Epoch: 22 [41856/50048]	Loss: 1.4562
Training Epoch: 22 [41984/50048]	Loss: 1.4067
Training Epoch: 22 [42112/50048]	Loss: 1.0066
Training Epoch: 22 [42240/50048]	Loss: 1.0871
Training Epoch: 22 [42368/50048]	Loss: 1.4345
Training Epoch: 22 [42496/50048]	Loss: 1.1230
Training Epoch: 22 [42624/50048]	Loss: 1.3682
Training Epoch: 22 [42752/50048]	Loss: 1.2247
Training Epoch: 22 [42880/50048]	Loss: 1.0985
Training Epoch: 22 [43008/50048]	Loss: 1.2895
Training Epoch: 22 [43136/50048]	Loss: 1.2291
Training Epoch: 22 [43264/50048]	Loss: 1.3215
Training Epoch: 22 [43392/50048]	Loss: 1.0408
Training Epoch: 22 [43520/50048]	Loss: 1.1037
Training Epoch: 22 [43648/50048]	Loss: 1.3887
Training Epoch: 22 [43776/50048]	Loss: 1.1309
Training Epoch: 22 [43904/50048]	Loss: 1.1996
Training Epoch: 22 [44032/50048]	Loss: 1.4108
Training Epoch: 22 [44160/50048]	Loss: 1.3416
Training Epoch: 22 [44288/50048]	Loss: 1.3579
Training Epoch: 22 [44416/50048]	Loss: 1.5328
Training Epoch: 22 [44544/50048]	Loss: 1.3108
Training Epoch: 22 [44672/50048]	Loss: 1.0478
Training Epoch: 22 [44800/50048]	Loss: 1.2518
Training Epoch: 22 [44928/50048]	Loss: 1.1861
Training Epoch: 22 [45056/50048]	Loss: 1.1685
Training Epoch: 22 [45184/50048]	Loss: 1.1299
Training Epoch: 22 [45312/50048]	Loss: 1.2166
Training Epoch: 22 [45440/50048]	Loss: 1.3918
Training Epoch: 22 [45568/50048]	Loss: 1.2789
Training Epoch: 22 [45696/50048]	Loss: 1.3096
Training Epoch: 22 [45824/50048]	Loss: 1.0409
Training Epoch: 22 [45952/50048]	Loss: 1.3227
Training Epoch: 22 [46080/50048]	Loss: 1.2457
Training Epoch: 22 [46208/50048]	Loss: 1.3940
Training Epoch: 22 [46336/50048]	Loss: 1.3807
Training Epoch: 22 [46464/50048]	Loss: 1.2939
Training Epoch: 22 [46592/50048]	Loss: 1.2233
Training Epoch: 22 [46720/50048]	Loss: 1.0892
Training Epoch: 22 [46848/50048]	Loss: 1.2925
Training Epoch: 22 [46976/50048]	Loss: 1.0356
Training Epoch: 22 [47104/50048]	Loss: 1.3401
Training Epoch: 22 [47232/50048]	Loss: 1.2747
Training Epoch: 22 [47360/50048]	Loss: 1.2754
Training Epoch: 22 [47488/50048]	Loss: 1.1573
Training Epoch: 22 [47616/50048]	Loss: 1.2517
Training Epoch: 22 [47744/50048]	Loss: 1.2017
Training Epoch: 22 [47872/50048]	Loss: 1.1507
Training Epoch: 22 [48000/50048]	Loss: 1.0696
Training Epoch: 22 [48128/50048]	Loss: 1.2359
Training Epoch: 22 [48256/50048]	Loss: 1.2128
Training Epoch: 22 [48384/50048]	Loss: 1.3075
Training Epoch: 22 [48512/50048]	Loss: 1.2461
Training Epoch: 22 [48640/50048]	Loss: 1.1756
Training Epoch: 22 [48768/50048]	Loss: 1.2753
Training Epoch: 22 [48896/50048]	Loss: 1.3032
Training Epoch: 22 [49024/50048]	Loss: 0.9446
Training Epoch: 22 [49152/50048]	Loss: 1.1979
Training Epoch: 22 [49280/50048]	Loss: 1.3823
Training Epoch: 22 [49408/50048]	Loss: 1.2731
Training Epoch: 22 [49536/50048]	Loss: 1.3182
Training Epoch: 22 [49664/50048]	Loss: 1.3748
Training Epoch: 22 [49792/50048]	Loss: 1.2176
Training Epoch: 22 [49920/50048]	Loss: 1.2594
Training Epoch: 22 [50048/50048]	Loss: 1.0648
Validation Epoch: 22, Average loss: 0.0119, Accuracy: 0.5881
Training Epoch: 23 [128/50048]	Loss: 0.9887
Training Epoch: 23 [256/50048]	Loss: 1.1982
Training Epoch: 23 [384/50048]	Loss: 1.2533
Training Epoch: 23 [512/50048]	Loss: 1.0705
Training Epoch: 23 [640/50048]	Loss: 1.3206
Training Epoch: 23 [768/50048]	Loss: 1.1041
Training Epoch: 23 [896/50048]	Loss: 1.1087
Training Epoch: 23 [1024/50048]	Loss: 1.2394
Training Epoch: 23 [1152/50048]	Loss: 0.9932
Training Epoch: 23 [1280/50048]	Loss: 1.1467
Training Epoch: 23 [1408/50048]	Loss: 1.2039
Training Epoch: 23 [1536/50048]	Loss: 1.2258
Training Epoch: 23 [1664/50048]	Loss: 1.2276
Training Epoch: 23 [1792/50048]	Loss: 1.2220
Training Epoch: 23 [1920/50048]	Loss: 1.2174
Training Epoch: 23 [2048/50048]	Loss: 1.1533
Training Epoch: 23 [2176/50048]	Loss: 1.3386
Training Epoch: 23 [2304/50048]	Loss: 1.0739
Training Epoch: 23 [2432/50048]	Loss: 1.2974
Training Epoch: 23 [2560/50048]	Loss: 1.1771
Training Epoch: 23 [2688/50048]	Loss: 1.1292
Training Epoch: 23 [2816/50048]	Loss: 1.1937
Training Epoch: 23 [2944/50048]	Loss: 1.1859
Training Epoch: 23 [3072/50048]	Loss: 1.0200
Training Epoch: 23 [3200/50048]	Loss: 1.3770
Training Epoch: 23 [3328/50048]	Loss: 1.0977
Training Epoch: 23 [3456/50048]	Loss: 1.0570
Training Epoch: 23 [3584/50048]	Loss: 1.1586
Training Epoch: 23 [3712/50048]	Loss: 1.3773
Training Epoch: 23 [3840/50048]	Loss: 1.0689
Training Epoch: 23 [3968/50048]	Loss: 1.2110
Training Epoch: 23 [4096/50048]	Loss: 1.1187
Training Epoch: 23 [4224/50048]	Loss: 1.1445
Training Epoch: 23 [4352/50048]	Loss: 1.1481
Training Epoch: 23 [4480/50048]	Loss: 1.1804
Training Epoch: 23 [4608/50048]	Loss: 1.1425
Training Epoch: 23 [4736/50048]	Loss: 1.0288
Training Epoch: 23 [4864/50048]	Loss: 1.1817
Training Epoch: 23 [4992/50048]	Loss: 1.2353
Training Epoch: 23 [5120/50048]	Loss: 1.5801
Training Epoch: 23 [5248/50048]	Loss: 1.1911
Training Epoch: 23 [5376/50048]	Loss: 1.3147
Training Epoch: 23 [5504/50048]	Loss: 1.3680
Training Epoch: 23 [5632/50048]	Loss: 1.0739
Training Epoch: 23 [5760/50048]	Loss: 1.0812
Training Epoch: 23 [5888/50048]	Loss: 1.3096
Training Epoch: 23 [6016/50048]	Loss: 1.2422
Training Epoch: 23 [6144/50048]	Loss: 1.1742
Training Epoch: 23 [6272/50048]	Loss: 1.2472
Training Epoch: 23 [6400/50048]	Loss: 1.0654
Training Epoch: 23 [6528/50048]	Loss: 1.0975
Training Epoch: 23 [6656/50048]	Loss: 1.2540
Training Epoch: 23 [6784/50048]	Loss: 1.1555
Training Epoch: 23 [6912/50048]	Loss: 1.1819
Training Epoch: 23 [7040/50048]	Loss: 1.3105
Training Epoch: 23 [7168/50048]	Loss: 1.2056
Training Epoch: 23 [7296/50048]	Loss: 1.2340
Training Epoch: 23 [7424/50048]	Loss: 1.2555
Training Epoch: 23 [7552/50048]	Loss: 1.2160
Training Epoch: 23 [7680/50048]	Loss: 1.3702
Training Epoch: 23 [7808/50048]	Loss: 1.2264
Training Epoch: 23 [7936/50048]	Loss: 1.2087
Training Epoch: 23 [8064/50048]	Loss: 1.1447
Training Epoch: 23 [8192/50048]	Loss: 1.2188
Training Epoch: 23 [8320/50048]	Loss: 1.2725
Training Epoch: 23 [8448/50048]	Loss: 1.1532
Training Epoch: 23 [8576/50048]	Loss: 1.3701
Training Epoch: 23 [8704/50048]	Loss: 1.1826
Training Epoch: 23 [8832/50048]	Loss: 1.1007
Training Epoch: 23 [8960/50048]	Loss: 1.2323
Training Epoch: 23 [9088/50048]	Loss: 1.2578
Training Epoch: 23 [9216/50048]	Loss: 1.0221
Training Epoch: 23 [9344/50048]	Loss: 1.2879
Training Epoch: 23 [9472/50048]	Loss: 0.9983
Training Epoch: 23 [9600/50048]	Loss: 1.2166
Training Epoch: 23 [9728/50048]	Loss: 1.1734
Training Epoch: 23 [9856/50048]	Loss: 1.0827
Training Epoch: 23 [9984/50048]	Loss: 1.1011
Training Epoch: 23 [10112/50048]	Loss: 1.1350
Training Epoch: 23 [10240/50048]	Loss: 1.0407
Training Epoch: 23 [10368/50048]	Loss: 1.0471
Training Epoch: 23 [10496/50048]	Loss: 1.0422
Training Epoch: 23 [10624/50048]	Loss: 1.4001
Training Epoch: 23 [10752/50048]	Loss: 1.3436
Training Epoch: 23 [10880/50048]	Loss: 1.3258
Training Epoch: 23 [11008/50048]	Loss: 1.4700
Training Epoch: 23 [11136/50048]	Loss: 1.1309
Training Epoch: 23 [11264/50048]	Loss: 1.1391
Training Epoch: 23 [11392/50048]	Loss: 1.2155
Training Epoch: 23 [11520/50048]	Loss: 1.4072
Training Epoch: 23 [11648/50048]	Loss: 1.4043
Training Epoch: 23 [11776/50048]	Loss: 0.9997
Training Epoch: 23 [11904/50048]	Loss: 1.2119
Training Epoch: 23 [12032/50048]	Loss: 1.2624
Training Epoch: 23 [12160/50048]	Loss: 1.2056
Training Epoch: 23 [12288/50048]	Loss: 1.1853
Training Epoch: 23 [12416/50048]	Loss: 1.0023
Training Epoch: 23 [12544/50048]	Loss: 1.2479
Training Epoch: 23 [12672/50048]	Loss: 1.3664
Training Epoch: 23 [12800/50048]	Loss: 1.4301
Training Epoch: 23 [12928/50048]	Loss: 1.2696
Training Epoch: 23 [13056/50048]	Loss: 1.2456
Training Epoch: 23 [13184/50048]	Loss: 1.0678
Training Epoch: 23 [13312/50048]	Loss: 1.3294
Training Epoch: 23 [13440/50048]	Loss: 1.2350
Training Epoch: 23 [13568/50048]	Loss: 1.1904
Training Epoch: 23 [13696/50048]	Loss: 1.1088
Training Epoch: 23 [13824/50048]	Loss: 1.1294
Training Epoch: 23 [13952/50048]	Loss: 1.2858
Training Epoch: 23 [14080/50048]	Loss: 1.2094
Training Epoch: 23 [14208/50048]	Loss: 1.2000
Training Epoch: 23 [14336/50048]	Loss: 0.9875
Training Epoch: 23 [14464/50048]	Loss: 1.1645
Training Epoch: 23 [14592/50048]	Loss: 1.0324
Training Epoch: 23 [14720/50048]	Loss: 1.0089
Training Epoch: 23 [14848/50048]	Loss: 1.2472
Training Epoch: 23 [14976/50048]	Loss: 1.2376
Training Epoch: 23 [15104/50048]	Loss: 1.2749
Training Epoch: 23 [15232/50048]	Loss: 1.4100
Training Epoch: 23 [15360/50048]	Loss: 1.3829
Training Epoch: 23 [15488/50048]	Loss: 1.3461
Training Epoch: 23 [15616/50048]	Loss: 1.3314
Training Epoch: 23 [15744/50048]	Loss: 1.1619
Training Epoch: 23 [15872/50048]	Loss: 1.4540
Training Epoch: 23 [16000/50048]	Loss: 1.0621
Training Epoch: 23 [16128/50048]	Loss: 1.3971
Training Epoch: 23 [16256/50048]	Loss: 1.0271
Training Epoch: 23 [16384/50048]	Loss: 1.2153
Training Epoch: 23 [16512/50048]	Loss: 1.4401
Training Epoch: 23 [16640/50048]	Loss: 1.1211
Training Epoch: 23 [16768/50048]	Loss: 1.2895
Training Epoch: 23 [16896/50048]	Loss: 0.8792
Training Epoch: 23 [17024/50048]	Loss: 1.1754
Training Epoch: 23 [17152/50048]	Loss: 1.2106
Training Epoch: 23 [17280/50048]	Loss: 1.1799
Training Epoch: 23 [17408/50048]	Loss: 1.0483
Training Epoch: 23 [17536/50048]	Loss: 1.2255
Training Epoch: 23 [17664/50048]	Loss: 1.2926
Training Epoch: 23 [17792/50048]	Loss: 1.1837
Training Epoch: 23 [17920/50048]	Loss: 1.0553
Training Epoch: 23 [18048/50048]	Loss: 1.2396
Training Epoch: 23 [18176/50048]	Loss: 1.2975
Training Epoch: 23 [18304/50048]	Loss: 1.1637
Training Epoch: 23 [18432/50048]	Loss: 1.1427
Training Epoch: 23 [18560/50048]	Loss: 1.2970
Training Epoch: 23 [18688/50048]	Loss: 1.2037
Training Epoch: 23 [18816/50048]	Loss: 1.2047
Training Epoch: 23 [18944/50048]	Loss: 1.2594
Training Epoch: 23 [19072/50048]	Loss: 1.3259
Training Epoch: 23 [19200/50048]	Loss: 1.0696
Training Epoch: 23 [19328/50048]	Loss: 1.2820
Training Epoch: 23 [19456/50048]	Loss: 1.2779
Training Epoch: 23 [19584/50048]	Loss: 1.1861
Training Epoch: 23 [19712/50048]	Loss: 1.1243
Training Epoch: 23 [19840/50048]	Loss: 1.2912
Training Epoch: 23 [19968/50048]	Loss: 1.0551
Training Epoch: 23 [20096/50048]	Loss: 1.2705
Training Epoch: 23 [20224/50048]	Loss: 1.2385
Training Epoch: 23 [20352/50048]	Loss: 1.1435
Training Epoch: 23 [20480/50048]	Loss: 1.5005
Training Epoch: 23 [20608/50048]	Loss: 1.4196
Training Epoch: 23 [20736/50048]	Loss: 1.0291
Training Epoch: 23 [20864/50048]	Loss: 1.2864
Training Epoch: 23 [20992/50048]	Loss: 1.1284
Training Epoch: 23 [21120/50048]	Loss: 1.2106
Training Epoch: 23 [21248/50048]	Loss: 1.1962
Training Epoch: 23 [21376/50048]	Loss: 1.1109
Training Epoch: 23 [21504/50048]	Loss: 1.1300
Training Epoch: 23 [21632/50048]	Loss: 1.2333
Training Epoch: 23 [21760/50048]	Loss: 1.4259
Training Epoch: 23 [21888/50048]	Loss: 1.1585
Training Epoch: 23 [22016/50048]	Loss: 1.2354
Training Epoch: 23 [22144/50048]	Loss: 1.1408
Training Epoch: 23 [22272/50048]	Loss: 1.2456
Training Epoch: 23 [22400/50048]	Loss: 1.1346
Training Epoch: 23 [22528/50048]	Loss: 1.2493
Training Epoch: 23 [22656/50048]	Loss: 1.1356
Training Epoch: 23 [22784/50048]	Loss: 1.5020
Training Epoch: 23 [22912/50048]	Loss: 1.1108
Training Epoch: 23 [23040/50048]	Loss: 1.3355
Training Epoch: 23 [23168/50048]	Loss: 1.2041
Training Epoch: 23 [23296/50048]	Loss: 1.2203
Training Epoch: 23 [23424/50048]	Loss: 1.3002
Training Epoch: 23 [23552/50048]	Loss: 1.2761
Training Epoch: 23 [23680/50048]	Loss: 1.3832
Training Epoch: 23 [23808/50048]	Loss: 1.0194
Training Epoch: 23 [23936/50048]	Loss: 1.3182
Training Epoch: 23 [24064/50048]	Loss: 1.2968
Training Epoch: 23 [24192/50048]	Loss: 1.2113
Training Epoch: 23 [24320/50048]	Loss: 1.1696
Training Epoch: 23 [24448/50048]	Loss: 1.2848
Training Epoch: 23 [24576/50048]	Loss: 1.4552
Training Epoch: 23 [24704/50048]	Loss: 1.3498
Training Epoch: 23 [24832/50048]	Loss: 1.3427
Training Epoch: 23 [24960/50048]	Loss: 1.0773
Training Epoch: 23 [25088/50048]	Loss: 1.0612
Training Epoch: 23 [25216/50048]	Loss: 1.3968
Training Epoch: 23 [25344/50048]	Loss: 1.1041
Training Epoch: 23 [25472/50048]	Loss: 1.1603
Training Epoch: 23 [25600/50048]	Loss: 1.1731
Training Epoch: 23 [25728/50048]	Loss: 1.0653
Training Epoch: 23 [25856/50048]	Loss: 1.2036
Training Epoch: 23 [25984/50048]	Loss: 1.1953
Training Epoch: 23 [26112/50048]	Loss: 1.1764
Training Epoch: 23 [26240/50048]	Loss: 1.2582
Training Epoch: 23 [26368/50048]	Loss: 1.1892
Training Epoch: 23 [26496/50048]	Loss: 1.5778
Training Epoch: 23 [26624/50048]	Loss: 1.2485
Training Epoch: 23 [26752/50048]	Loss: 1.3948
Training Epoch: 23 [26880/50048]	Loss: 1.1760
Training Epoch: 23 [27008/50048]	Loss: 1.2768
Training Epoch: 23 [27136/50048]	Loss: 1.2898
Training Epoch: 23 [27264/50048]	Loss: 1.3113
Training Epoch: 23 [27392/50048]	Loss: 1.2620
Training Epoch: 23 [27520/50048]	Loss: 1.3718
Training Epoch: 23 [27648/50048]	Loss: 1.1723
Training Epoch: 23 [27776/50048]	Loss: 1.2772
Training Epoch: 23 [27904/50048]	Loss: 1.2681
Training Epoch: 23 [28032/50048]	Loss: 1.1999
Training Epoch: 23 [28160/50048]	Loss: 1.1768
Training Epoch: 23 [28288/50048]	Loss: 1.2654
Training Epoch: 23 [28416/50048]	Loss: 1.2453
Training Epoch: 23 [28544/50048]	Loss: 1.1901
Training Epoch: 23 [28672/50048]	Loss: 1.2160
Training Epoch: 23 [28800/50048]	Loss: 1.1802
Training Epoch: 23 [28928/50048]	Loss: 1.2467
Training Epoch: 23 [29056/50048]	Loss: 1.2209
Training Epoch: 23 [29184/50048]	Loss: 1.1660
Training Epoch: 23 [29312/50048]	Loss: 1.0294
Training Epoch: 23 [29440/50048]	Loss: 1.2666
Training Epoch: 23 [29568/50048]	Loss: 1.1146
Training Epoch: 23 [29696/50048]	Loss: 1.1875
Training Epoch: 23 [29824/50048]	Loss: 1.1182
Training Epoch: 23 [29952/50048]	Loss: 1.3115
Training Epoch: 23 [30080/50048]	Loss: 1.1811
Training Epoch: 23 [30208/50048]	Loss: 1.2152
Training Epoch: 23 [30336/50048]	Loss: 1.0996
Training Epoch: 23 [30464/50048]	Loss: 1.0869
Training Epoch: 23 [30592/50048]	Loss: 1.1579
Training Epoch: 23 [30720/50048]	Loss: 1.3046
Training Epoch: 23 [30848/50048]	Loss: 1.2043
Training Epoch: 23 [30976/50048]	Loss: 1.3276
Training Epoch: 23 [31104/50048]	Loss: 1.1744
Training Epoch: 23 [31232/50048]	Loss: 1.1696
Training Epoch: 23 [31360/50048]	Loss: 1.1115
Training Epoch: 23 [31488/50048]	Loss: 1.3455
Training Epoch: 23 [31616/50048]	Loss: 1.0725
Training Epoch: 23 [31744/50048]	Loss: 1.1967
Training Epoch: 23 [31872/50048]	Loss: 1.1126
Training Epoch: 23 [32000/50048]	Loss: 1.4037
Training Epoch: 23 [32128/50048]	Loss: 1.0373
Training Epoch: 23 [32256/50048]	Loss: 1.1061
Training Epoch: 23 [32384/50048]	Loss: 1.4774
Training Epoch: 23 [32512/50048]	Loss: 1.6114
Training Epoch: 23 [32640/50048]	Loss: 1.6842
Training Epoch: 23 [32768/50048]	Loss: 1.3756
Training Epoch: 23 [32896/50048]	Loss: 1.4347
Training Epoch: 23 [33024/50048]	Loss: 1.0985
Training Epoch: 23 [33152/50048]	Loss: 1.2443
Training Epoch: 23 [33280/50048]	Loss: 1.2166
Training Epoch: 23 [33408/50048]	Loss: 1.1741
Training Epoch: 23 [33536/50048]	Loss: 1.0854
Training Epoch: 23 [33664/50048]	Loss: 1.1536
Training Epoch: 23 [33792/50048]	Loss: 1.2065
Training Epoch: 23 [33920/50048]	Loss: 0.8809
Training Epoch: 23 [34048/50048]	Loss: 1.2401
Training Epoch: 23 [34176/50048]	Loss: 1.2998
Training Epoch: 23 [34304/50048]	Loss: 1.2007
Training Epoch: 23 [34432/50048]	Loss: 1.1213
Training Epoch: 23 [34560/50048]	Loss: 1.5374
Training Epoch: 23 [34688/50048]	Loss: 1.1430
Training Epoch: 23 [34816/50048]	Loss: 1.2316
Training Epoch: 23 [34944/50048]	Loss: 1.1068
Training Epoch: 23 [35072/50048]	Loss: 1.2111
Training Epoch: 23 [35200/50048]	Loss: 1.0360
Training Epoch: 23 [35328/50048]	Loss: 1.1905
Training Epoch: 23 [35456/50048]	Loss: 1.1466
Training Epoch: 23 [35584/50048]	Loss: 1.0935
Training Epoch: 23 [35712/50048]	Loss: 1.2931
Training Epoch: 23 [35840/50048]	Loss: 1.2084
Training Epoch: 23 [35968/50048]	Loss: 1.2447
Training Epoch: 23 [36096/50048]	Loss: 1.0396
Training Epoch: 23 [36224/50048]	Loss: 1.4316
Training Epoch: 23 [36352/50048]	Loss: 1.3120
Training Epoch: 23 [36480/50048]	Loss: 1.1621
Training Epoch: 23 [36608/50048]	Loss: 1.0224
Training Epoch: 23 [36736/50048]	Loss: 1.1808
Training Epoch: 23 [36864/50048]	Loss: 1.1934
Training Epoch: 23 [36992/50048]	Loss: 1.3560
Training Epoch: 23 [37120/50048]	Loss: 1.1880
Training Epoch: 23 [37248/50048]	Loss: 1.0715
Training Epoch: 23 [37376/50048]	Loss: 1.2308
Training Epoch: 23 [37504/50048]	Loss: 1.2434
Training Epoch: 23 [37632/50048]	Loss: 0.9993
Training Epoch: 23 [37760/50048]	Loss: 1.2732
Training Epoch: 23 [37888/50048]	Loss: 1.1442
Training Epoch: 23 [38016/50048]	Loss: 1.3834
Training Epoch: 23 [38144/50048]	Loss: 1.4239
Training Epoch: 23 [38272/50048]	Loss: 1.3458
Training Epoch: 23 [38400/50048]	Loss: 1.3477
Training Epoch: 23 [38528/50048]	Loss: 0.9734
Training Epoch: 23 [38656/50048]	Loss: 1.3043
Training Epoch: 23 [38784/50048]	Loss: 1.2370
Training Epoch: 23 [38912/50048]	Loss: 1.2454
Training Epoch: 23 [39040/50048]	Loss: 1.2832
Training Epoch: 23 [39168/50048]	Loss: 1.1190
Training Epoch: 23 [39296/50048]	Loss: 1.2627
Training Epoch: 23 [39424/50048]	Loss: 1.1308
Training Epoch: 23 [39552/50048]	Loss: 1.0700
Training Epoch: 23 [39680/50048]	Loss: 1.1527
Training Epoch: 23 [39808/50048]	Loss: 0.9764
Training Epoch: 23 [39936/50048]	Loss: 1.1893
Training Epoch: 23 [40064/50048]	Loss: 1.1607
Training Epoch: 23 [40192/50048]	Loss: 1.2460
Training Epoch: 23 [40320/50048]	Loss: 1.2160
Training Epoch: 23 [40448/50048]	Loss: 1.2530
Training Epoch: 23 [40576/50048]	Loss: 1.0320
Training Epoch: 23 [40704/50048]	Loss: 1.2609
Training Epoch: 23 [40832/50048]	Loss: 1.2945
Training Epoch: 23 [40960/50048]	Loss: 1.2000
Training Epoch: 23 [41088/50048]	Loss: 1.1515
Training Epoch: 23 [41216/50048]	Loss: 1.3026
Training Epoch: 23 [41344/50048]	Loss: 1.2282
Training Epoch: 23 [41472/50048]	Loss: 1.3467
Training Epoch: 23 [41600/50048]	Loss: 1.3350
Training Epoch: 23 [41728/50048]	Loss: 1.3214
Training Epoch: 23 [41856/50048]	Loss: 1.2676
Training Epoch: 23 [41984/50048]	Loss: 1.1170
Training Epoch: 23 [42112/50048]	Loss: 1.3227
Training Epoch: 23 [42240/50048]	Loss: 1.2086
Training Epoch: 23 [42368/50048]	Loss: 1.2364
Training Epoch: 23 [42496/50048]	Loss: 1.2949
Training Epoch: 23 [42624/50048]	Loss: 1.0411
Training Epoch: 23 [42752/50048]	Loss: 1.2689
Training Epoch: 23 [42880/50048]	Loss: 1.2666
Training Epoch: 23 [43008/50048]	Loss: 1.2474
Training Epoch: 23 [43136/50048]	Loss: 1.2085
Training Epoch: 23 [43264/50048]	Loss: 1.2502
Training Epoch: 23 [43392/50048]	Loss: 1.2710
Training Epoch: 23 [43520/50048]	Loss: 1.2217
Training Epoch: 23 [43648/50048]	Loss: 1.2137
Training Epoch: 23 [43776/50048]	Loss: 1.2564
Training Epoch: 23 [43904/50048]	Loss: 1.1669
Training Epoch: 23 [44032/50048]	Loss: 1.3500
Training Epoch: 23 [44160/50048]	Loss: 1.3459
Training Epoch: 23 [44288/50048]	Loss: 1.0485
Training Epoch: 23 [44416/50048]	Loss: 1.3437
Training Epoch: 23 [44544/50048]	Loss: 1.2468
Training Epoch: 23 [44672/50048]	Loss: 0.9785
Training Epoch: 23 [44800/50048]	Loss: 1.2542
Training Epoch: 23 [44928/50048]	Loss: 1.0999
Training Epoch: 23 [45056/50048]	Loss: 1.1303
Training Epoch: 23 [45184/50048]	Loss: 1.2329
Training Epoch: 23 [45312/50048]	Loss: 1.0559
Training Epoch: 23 [45440/50048]	Loss: 1.1057
Training Epoch: 23 [45568/50048]	Loss: 1.3528
Training Epoch: 23 [45696/50048]	Loss: 1.1169
Training Epoch: 23 [45824/50048]	Loss: 1.1693
Training Epoch: 23 [45952/50048]	Loss: 1.3662
Training Epoch: 23 [46080/50048]	Loss: 0.9432
Training Epoch: 23 [46208/50048]	Loss: 1.1235
Training Epoch: 23 [46336/50048]	Loss: 1.0277
Training Epoch: 23 [46464/50048]	Loss: 1.1457
Training Epoch: 23 [46592/50048]	Loss: 1.1727
Training Epoch: 23 [46720/50048]	Loss: 1.2196
Training Epoch: 23 [46848/50048]	Loss: 1.3140
Training Epoch: 23 [46976/50048]	Loss: 1.2790
Training Epoch: 23 [47104/50048]	Loss: 1.0904
Training Epoch: 23 [47232/50048]	Loss: 1.2309
Training Epoch: 23 [47360/50048]	Loss: 1.3117
Training Epoch: 23 [47488/50048]	Loss: 1.3445
Training Epoch: 23 [47616/50048]	Loss: 1.0561
Training Epoch: 23 [47744/50048]	Loss: 1.1464
Training Epoch: 23 [47872/50048]	Loss: 1.2717
Training Epoch: 23 [48000/50048]	Loss: 1.3043
Training Epoch: 23 [48128/50048]	Loss: 1.2254
Training Epoch: 23 [48256/50048]	Loss: 1.1463
Training Epoch: 23 [48384/50048]	Loss: 1.2525
Training Epoch: 23 [48512/50048]	Loss: 1.4011
Training Epoch: 23 [48640/50048]	Loss: 0.9473
Training Epoch: 23 [48768/50048]	Loss: 1.2929
Training Epoch: 23 [48896/50048]	Loss: 1.0306
Training Epoch: 23 [49024/50048]	Loss: 1.2472
Training Epoch: 23 [49152/50048]	Loss: 1.0659
Training Epoch: 23 [49280/50048]	Loss: 1.2448
Training Epoch: 23 [49408/50048]	Loss: 1.1627
Training Epoch: 23 [49536/50048]	Loss: 1.2214
Training Epoch: 23 [49664/50048]	Loss: 1.2827
Training Epoch: 23 [49792/50048]	Loss: 0.9843
Training Epoch: 23 [49920/50048]	Loss: 1.1066
Training Epoch: 23 [50048/50048]	Loss: 1.1829
Validation Epoch: 23, Average loss: 0.0120, Accuracy: 0.5854
Training Epoch: 24 [128/50048]	Loss: 1.1309
Training Epoch: 24 [256/50048]	Loss: 0.9766
Training Epoch: 24 [384/50048]	Loss: 1.1631
Training Epoch: 24 [512/50048]	Loss: 1.2382
Training Epoch: 24 [640/50048]	Loss: 1.4673
Training Epoch: 24 [768/50048]	Loss: 0.9992
Training Epoch: 24 [896/50048]	Loss: 1.2097
Training Epoch: 24 [1024/50048]	Loss: 1.0693
Training Epoch: 24 [1152/50048]	Loss: 1.1840
Training Epoch: 24 [1280/50048]	Loss: 1.2603
Training Epoch: 24 [1408/50048]	Loss: 1.0449
Training Epoch: 24 [1536/50048]	Loss: 1.0336
Training Epoch: 24 [1664/50048]	Loss: 1.3600
Training Epoch: 24 [1792/50048]	Loss: 1.1675
Training Epoch: 24 [1920/50048]	Loss: 1.2711
Training Epoch: 24 [2048/50048]	Loss: 1.0543
Training Epoch: 24 [2176/50048]	Loss: 1.3491
Training Epoch: 24 [2304/50048]	Loss: 1.2517
Training Epoch: 24 [2432/50048]	Loss: 1.0724
Training Epoch: 24 [2560/50048]	Loss: 1.2188
Training Epoch: 24 [2688/50048]	Loss: 1.3688
Training Epoch: 24 [2816/50048]	Loss: 1.2263
Training Epoch: 24 [2944/50048]	Loss: 1.3919
Training Epoch: 24 [3072/50048]	Loss: 1.1927
Training Epoch: 24 [3200/50048]	Loss: 1.1934
Training Epoch: 24 [3328/50048]	Loss: 1.1155
Training Epoch: 24 [3456/50048]	Loss: 1.1523
Training Epoch: 24 [3584/50048]	Loss: 1.4179
Training Epoch: 24 [3712/50048]	Loss: 1.2536
Training Epoch: 24 [3840/50048]	Loss: 1.0564
Training Epoch: 24 [3968/50048]	Loss: 1.4036
Training Epoch: 24 [4096/50048]	Loss: 1.1322
Training Epoch: 24 [4224/50048]	Loss: 1.1948
Training Epoch: 24 [4352/50048]	Loss: 0.9365
Training Epoch: 24 [4480/50048]	Loss: 1.0102
Training Epoch: 24 [4608/50048]	Loss: 1.3644
Training Epoch: 24 [4736/50048]	Loss: 1.0554
Training Epoch: 24 [4864/50048]	Loss: 1.3551
Training Epoch: 24 [4992/50048]	Loss: 1.2031
Training Epoch: 24 [5120/50048]	Loss: 1.2332
Training Epoch: 24 [5248/50048]	Loss: 0.9645
Training Epoch: 24 [5376/50048]	Loss: 1.2787
Training Epoch: 24 [5504/50048]	Loss: 1.1594
Training Epoch: 24 [5632/50048]	Loss: 1.1865
Training Epoch: 24 [5760/50048]	Loss: 1.3658
Training Epoch: 24 [5888/50048]	Loss: 0.9384
Training Epoch: 24 [6016/50048]	Loss: 1.1742
Training Epoch: 24 [6144/50048]	Loss: 1.0687
Training Epoch: 24 [6272/50048]	Loss: 1.1640
Training Epoch: 24 [6400/50048]	Loss: 1.2491
Training Epoch: 24 [6528/50048]	Loss: 1.1982
Training Epoch: 24 [6656/50048]	Loss: 1.1085
Training Epoch: 24 [6784/50048]	Loss: 1.2168
Training Epoch: 24 [6912/50048]	Loss: 1.0736
Training Epoch: 24 [7040/50048]	Loss: 1.1981
Training Epoch: 24 [7168/50048]	Loss: 1.2316
Training Epoch: 24 [7296/50048]	Loss: 1.1590
Training Epoch: 24 [7424/50048]	Loss: 1.1878
Training Epoch: 24 [7552/50048]	Loss: 1.2253
Training Epoch: 24 [7680/50048]	Loss: 1.3701
Training Epoch: 24 [7808/50048]	Loss: 1.1732
Training Epoch: 24 [7936/50048]	Loss: 1.1564
Training Epoch: 24 [8064/50048]	Loss: 1.2633
Training Epoch: 24 [8192/50048]	Loss: 1.1269
Training Epoch: 24 [8320/50048]	Loss: 1.0964
Training Epoch: 24 [8448/50048]	Loss: 1.2643
Training Epoch: 24 [8576/50048]	Loss: 1.4374
Training Epoch: 24 [8704/50048]	Loss: 1.2478
Training Epoch: 24 [8832/50048]	Loss: 1.1449
Training Epoch: 24 [8960/50048]	Loss: 1.1144
Training Epoch: 24 [9088/50048]	Loss: 1.2263
Training Epoch: 24 [9216/50048]	Loss: 1.0855
Training Epoch: 24 [9344/50048]	Loss: 1.2296
Training Epoch: 24 [9472/50048]	Loss: 1.0619
Training Epoch: 24 [9600/50048]	Loss: 1.1095
Training Epoch: 24 [9728/50048]	Loss: 1.3280
Training Epoch: 24 [9856/50048]	Loss: 1.1426
Training Epoch: 24 [9984/50048]	Loss: 1.0417
Training Epoch: 24 [10112/50048]	Loss: 0.9897
Training Epoch: 24 [10240/50048]	Loss: 1.0246
Training Epoch: 24 [10368/50048]	Loss: 1.1947
Training Epoch: 24 [10496/50048]	Loss: 1.3566
Training Epoch: 24 [10624/50048]	Loss: 1.3585
Training Epoch: 24 [10752/50048]	Loss: 1.2792
Training Epoch: 24 [10880/50048]	Loss: 1.2383
Training Epoch: 24 [11008/50048]	Loss: 0.9660
Training Epoch: 24 [11136/50048]	Loss: 1.0806
Training Epoch: 24 [11264/50048]	Loss: 1.1068
Training Epoch: 24 [11392/50048]	Loss: 1.2689
Training Epoch: 24 [11520/50048]	Loss: 1.2771
Training Epoch: 24 [11648/50048]	Loss: 1.2074
Training Epoch: 24 [11776/50048]	Loss: 1.0532
Training Epoch: 24 [11904/50048]	Loss: 1.0889
Training Epoch: 24 [12032/50048]	Loss: 1.1459
Training Epoch: 24 [12160/50048]	Loss: 1.3559
Training Epoch: 24 [12288/50048]	Loss: 1.1400
Training Epoch: 24 [12416/50048]	Loss: 1.2824
Training Epoch: 24 [12544/50048]	Loss: 1.1808
Training Epoch: 24 [12672/50048]	Loss: 0.9796
Training Epoch: 24 [12800/50048]	Loss: 1.2863
Training Epoch: 24 [12928/50048]	Loss: 1.0124
Training Epoch: 24 [13056/50048]	Loss: 1.1645
Training Epoch: 24 [13184/50048]	Loss: 1.2009
Training Epoch: 24 [13312/50048]	Loss: 1.1582
Training Epoch: 24 [13440/50048]	Loss: 1.1520
Training Epoch: 24 [13568/50048]	Loss: 1.1155
Training Epoch: 24 [13696/50048]	Loss: 1.3208
Training Epoch: 24 [13824/50048]	Loss: 1.3315
Training Epoch: 24 [13952/50048]	Loss: 1.1767
Training Epoch: 24 [14080/50048]	Loss: 1.0046
Training Epoch: 24 [14208/50048]	Loss: 1.0788
Training Epoch: 24 [14336/50048]	Loss: 1.2039
Training Epoch: 24 [14464/50048]	Loss: 1.4580
Training Epoch: 24 [14592/50048]	Loss: 1.3143
Training Epoch: 24 [14720/50048]	Loss: 0.8928
Training Epoch: 24 [14848/50048]	Loss: 1.0339
Training Epoch: 24 [14976/50048]	Loss: 1.4079
Training Epoch: 24 [15104/50048]	Loss: 1.1192
Training Epoch: 24 [15232/50048]	Loss: 1.0939
Training Epoch: 24 [15360/50048]	Loss: 1.3648
Training Epoch: 24 [15488/50048]	Loss: 1.2295
Training Epoch: 24 [15616/50048]	Loss: 1.1969
Training Epoch: 24 [15744/50048]	Loss: 1.3448
Training Epoch: 24 [15872/50048]	Loss: 0.9919
Training Epoch: 24 [16000/50048]	Loss: 1.1985
Training Epoch: 24 [16128/50048]	Loss: 1.2009
Training Epoch: 24 [16256/50048]	Loss: 1.2534
Training Epoch: 24 [16384/50048]	Loss: 1.2251
Training Epoch: 24 [16512/50048]	Loss: 1.1688
Training Epoch: 24 [16640/50048]	Loss: 1.0741
Training Epoch: 24 [16768/50048]	Loss: 1.2238
Training Epoch: 24 [16896/50048]	Loss: 1.1995
Training Epoch: 24 [17024/50048]	Loss: 1.1606
Training Epoch: 24 [17152/50048]	Loss: 1.2993
Training Epoch: 24 [17280/50048]	Loss: 1.1314
Training Epoch: 24 [17408/50048]	Loss: 1.1641
Training Epoch: 24 [17536/50048]	Loss: 1.2199
Training Epoch: 24 [17664/50048]	Loss: 1.2426
Training Epoch: 24 [17792/50048]	Loss: 1.3778
Training Epoch: 24 [17920/50048]	Loss: 1.7089
Training Epoch: 24 [18048/50048]	Loss: 1.0171
Training Epoch: 24 [18176/50048]	Loss: 1.1724
Training Epoch: 24 [18304/50048]	Loss: 1.1717
Training Epoch: 24 [18432/50048]	Loss: 1.3425
Training Epoch: 24 [18560/50048]	Loss: 1.2001
Training Epoch: 24 [18688/50048]	Loss: 1.0728
Training Epoch: 24 [18816/50048]	Loss: 1.1567
Training Epoch: 24 [18944/50048]	Loss: 1.1595
Training Epoch: 24 [19072/50048]	Loss: 1.2156
Training Epoch: 24 [19200/50048]	Loss: 1.0913
Training Epoch: 24 [19328/50048]	Loss: 1.2175
Training Epoch: 24 [19456/50048]	Loss: 1.1670
Training Epoch: 24 [19584/50048]	Loss: 0.8684
Training Epoch: 24 [19712/50048]	Loss: 1.1816
Training Epoch: 24 [19840/50048]	Loss: 1.1124
Training Epoch: 24 [19968/50048]	Loss: 1.3908
Training Epoch: 24 [20096/50048]	Loss: 1.0125
Training Epoch: 24 [20224/50048]	Loss: 1.1817
Training Epoch: 24 [20352/50048]	Loss: 1.2728
Training Epoch: 24 [20480/50048]	Loss: 1.0598
Training Epoch: 24 [20608/50048]	Loss: 0.9454
Training Epoch: 24 [20736/50048]	Loss: 1.1468
Training Epoch: 24 [20864/50048]	Loss: 1.2323
Training Epoch: 24 [20992/50048]	Loss: 0.9951
Training Epoch: 24 [21120/50048]	Loss: 1.0277
Training Epoch: 24 [21248/50048]	Loss: 0.9833
Training Epoch: 24 [21376/50048]	Loss: 1.0483
Training Epoch: 24 [21504/50048]	Loss: 1.1582
Training Epoch: 24 [21632/50048]	Loss: 1.1304
Training Epoch: 24 [21760/50048]	Loss: 1.0900
Training Epoch: 24 [21888/50048]	Loss: 1.0564
Training Epoch: 24 [22016/50048]	Loss: 1.1085
Training Epoch: 24 [22144/50048]	Loss: 1.0270
Training Epoch: 24 [22272/50048]	Loss: 1.3623
Training Epoch: 24 [22400/50048]	Loss: 1.1434
Training Epoch: 24 [22528/50048]	Loss: 1.1299
Training Epoch: 24 [22656/50048]	Loss: 0.9853
Training Epoch: 24 [22784/50048]	Loss: 1.3828
Training Epoch: 24 [22912/50048]	Loss: 1.1537
Training Epoch: 24 [23040/50048]	Loss: 1.3731
Training Epoch: 24 [23168/50048]	Loss: 1.0985
Training Epoch: 24 [23296/50048]	Loss: 1.1944
Training Epoch: 24 [23424/50048]	Loss: 1.2922
Training Epoch: 24 [23552/50048]	Loss: 1.0120
Training Epoch: 24 [23680/50048]	Loss: 1.2425
Training Epoch: 24 [23808/50048]	Loss: 1.5178
Training Epoch: 24 [23936/50048]	Loss: 1.0211
Training Epoch: 24 [24064/50048]	Loss: 1.1031
Training Epoch: 24 [24192/50048]	Loss: 1.3346
Training Epoch: 24 [24320/50048]	Loss: 1.1531
Training Epoch: 24 [24448/50048]	Loss: 1.0005
Training Epoch: 24 [24576/50048]	Loss: 1.1112
Training Epoch: 24 [24704/50048]	Loss: 1.2018
Training Epoch: 24 [24832/50048]	Loss: 0.9562
Training Epoch: 24 [24960/50048]	Loss: 1.3114
Training Epoch: 24 [25088/50048]	Loss: 1.2193
Training Epoch: 24 [25216/50048]	Loss: 1.1025
Training Epoch: 24 [25344/50048]	Loss: 1.0913
Training Epoch: 24 [25472/50048]	Loss: 1.1470
Training Epoch: 24 [25600/50048]	Loss: 1.2243
Training Epoch: 24 [25728/50048]	Loss: 1.2813
Training Epoch: 24 [25856/50048]	Loss: 1.1982
Training Epoch: 24 [25984/50048]	Loss: 1.1543
Training Epoch: 24 [26112/50048]	Loss: 1.2206
Training Epoch: 24 [26240/50048]	Loss: 1.1276
Training Epoch: 24 [26368/50048]	Loss: 0.9802
Training Epoch: 24 [26496/50048]	Loss: 0.9902
Training Epoch: 24 [26624/50048]	Loss: 1.0544
Training Epoch: 24 [26752/50048]	Loss: 0.9924
Training Epoch: 24 [26880/50048]	Loss: 1.1665
Training Epoch: 24 [27008/50048]	Loss: 1.2789
Training Epoch: 24 [27136/50048]	Loss: 1.2041
Training Epoch: 24 [27264/50048]	Loss: 1.2749
Training Epoch: 24 [27392/50048]	Loss: 1.2699
Training Epoch: 24 [27520/50048]	Loss: 1.2936
Training Epoch: 24 [27648/50048]	Loss: 1.2053
Training Epoch: 24 [27776/50048]	Loss: 1.3245
Training Epoch: 24 [27904/50048]	Loss: 1.1630
Training Epoch: 24 [28032/50048]	Loss: 1.1743
Training Epoch: 24 [28160/50048]	Loss: 1.1092
Training Epoch: 24 [28288/50048]	Loss: 1.0524
Training Epoch: 24 [28416/50048]	Loss: 1.0950
Training Epoch: 24 [28544/50048]	Loss: 1.0683
Training Epoch: 24 [28672/50048]	Loss: 0.9201
Training Epoch: 24 [28800/50048]	Loss: 1.2901
Training Epoch: 24 [28928/50048]	Loss: 0.9682
Training Epoch: 24 [29056/50048]	Loss: 1.1441
Training Epoch: 24 [29184/50048]	Loss: 1.3292
Training Epoch: 24 [29312/50048]	Loss: 1.3367
Training Epoch: 24 [29440/50048]	Loss: 1.1815
Training Epoch: 24 [29568/50048]	Loss: 1.0248
Training Epoch: 24 [29696/50048]	Loss: 1.1370
Training Epoch: 24 [29824/50048]	Loss: 1.3697
Training Epoch: 24 [29952/50048]	Loss: 1.0613
Training Epoch: 24 [30080/50048]	Loss: 1.1230
Training Epoch: 24 [30208/50048]	Loss: 1.0771
Training Epoch: 24 [30336/50048]	Loss: 1.0376
Training Epoch: 24 [30464/50048]	Loss: 1.1584
Training Epoch: 24 [30592/50048]	Loss: 1.3201
Training Epoch: 24 [30720/50048]	Loss: 1.2731
Training Epoch: 24 [30848/50048]	Loss: 1.4234
Training Epoch: 24 [30976/50048]	Loss: 1.0357
Training Epoch: 24 [31104/50048]	Loss: 1.2373
Training Epoch: 24 [31232/50048]	Loss: 1.1761
Training Epoch: 24 [31360/50048]	Loss: 0.9941
Training Epoch: 24 [31488/50048]	Loss: 1.3711
Training Epoch: 24 [31616/50048]	Loss: 1.2399
Training Epoch: 24 [31744/50048]	Loss: 1.1239
Training Epoch: 24 [31872/50048]	Loss: 1.3286
Training Epoch: 24 [32000/50048]	Loss: 1.1987
Training Epoch: 24 [32128/50048]	Loss: 0.8613
Training Epoch: 24 [32256/50048]	Loss: 1.1820
Training Epoch: 24 [32384/50048]	Loss: 1.1476
Training Epoch: 24 [32512/50048]	Loss: 1.4264
Training Epoch: 24 [32640/50048]	Loss: 1.0301
Training Epoch: 24 [32768/50048]	Loss: 1.3344
Training Epoch: 24 [32896/50048]	Loss: 1.1746
Training Epoch: 24 [33024/50048]	Loss: 1.2577
Training Epoch: 24 [33152/50048]	Loss: 0.9715
Training Epoch: 24 [33280/50048]	Loss: 1.0140
Training Epoch: 24 [33408/50048]	Loss: 1.0396
Training Epoch: 24 [33536/50048]	Loss: 1.0291
Training Epoch: 24 [33664/50048]	Loss: 1.0920
Training Epoch: 24 [33792/50048]	Loss: 1.1826
Training Epoch: 24 [33920/50048]	Loss: 1.0589
Training Epoch: 24 [34048/50048]	Loss: 1.2870
Training Epoch: 24 [34176/50048]	Loss: 1.2112
Training Epoch: 24 [34304/50048]	Loss: 1.0344
Training Epoch: 24 [34432/50048]	Loss: 1.1967
Training Epoch: 24 [34560/50048]	Loss: 1.3861
Training Epoch: 24 [34688/50048]	Loss: 1.2515
Training Epoch: 24 [34816/50048]	Loss: 1.2660
Training Epoch: 24 [34944/50048]	Loss: 1.1836
Training Epoch: 24 [35072/50048]	Loss: 1.0600
Training Epoch: 24 [35200/50048]	Loss: 1.0604
Training Epoch: 24 [35328/50048]	Loss: 1.1880
Training Epoch: 24 [35456/50048]	Loss: 1.2953
Training Epoch: 24 [35584/50048]	Loss: 1.3441
Training Epoch: 24 [35712/50048]	Loss: 1.1570
Training Epoch: 24 [35840/50048]	Loss: 1.1197
Training Epoch: 24 [35968/50048]	Loss: 1.1330
Training Epoch: 24 [36096/50048]	Loss: 1.3850
Training Epoch: 24 [36224/50048]	Loss: 1.3066
Training Epoch: 24 [36352/50048]	Loss: 1.1256
Training Epoch: 24 [36480/50048]	Loss: 1.1907
Training Epoch: 24 [36608/50048]	Loss: 1.1952
Training Epoch: 24 [36736/50048]	Loss: 1.3040
Training Epoch: 24 [36864/50048]	Loss: 0.8543
Training Epoch: 24 [36992/50048]	Loss: 1.0800
Training Epoch: 24 [37120/50048]	Loss: 1.0776
Training Epoch: 24 [37248/50048]	Loss: 1.3264
Training Epoch: 24 [37376/50048]	Loss: 1.2160
Training Epoch: 24 [37504/50048]	Loss: 1.2681
Training Epoch: 24 [37632/50048]	Loss: 0.8909
Training Epoch: 24 [37760/50048]	Loss: 1.0947
Training Epoch: 24 [37888/50048]	Loss: 1.0678
Training Epoch: 24 [38016/50048]	Loss: 1.0403
Training Epoch: 24 [38144/50048]	Loss: 1.0561
Training Epoch: 24 [38272/50048]	Loss: 1.1694
Training Epoch: 24 [38400/50048]	Loss: 1.1430
Training Epoch: 24 [38528/50048]	Loss: 1.0534
Training Epoch: 24 [38656/50048]	Loss: 1.1107
Training Epoch: 24 [38784/50048]	Loss: 1.1252
Training Epoch: 24 [38912/50048]	Loss: 1.0471
Training Epoch: 24 [39040/50048]	Loss: 1.1579
Training Epoch: 24 [39168/50048]	Loss: 1.0564
Training Epoch: 24 [39296/50048]	Loss: 1.1522
Training Epoch: 24 [39424/50048]	Loss: 1.0261
Training Epoch: 24 [39552/50048]	Loss: 1.4849
Training Epoch: 24 [39680/50048]	Loss: 0.9961
Training Epoch: 24 [39808/50048]	Loss: 1.2573
Training Epoch: 24 [39936/50048]	Loss: 1.2724
Training Epoch: 24 [40064/50048]	Loss: 1.1393
Training Epoch: 24 [40192/50048]	Loss: 1.0267
Training Epoch: 24 [40320/50048]	Loss: 1.1358
Training Epoch: 24 [40448/50048]	Loss: 1.1750
Training Epoch: 24 [40576/50048]	Loss: 1.4430
Training Epoch: 24 [40704/50048]	Loss: 1.2201
Training Epoch: 24 [40832/50048]	Loss: 1.3247
Training Epoch: 24 [40960/50048]	Loss: 1.1112
Training Epoch: 24 [41088/50048]	Loss: 1.1052
Training Epoch: 24 [41216/50048]	Loss: 1.3760
Training Epoch: 24 [41344/50048]	Loss: 1.2115
Training Epoch: 24 [41472/50048]	Loss: 1.2325
Training Epoch: 24 [41600/50048]	Loss: 1.1014
Training Epoch: 24 [41728/50048]	Loss: 1.1865
Training Epoch: 24 [41856/50048]	Loss: 1.1734
Training Epoch: 24 [41984/50048]	Loss: 1.1605
Training Epoch: 24 [42112/50048]	Loss: 1.4193
Training Epoch: 24 [42240/50048]	Loss: 1.2462
Training Epoch: 24 [42368/50048]	Loss: 1.0464
Training Epoch: 24 [42496/50048]	Loss: 1.1006
Training Epoch: 24 [42624/50048]	Loss: 1.0193
Training Epoch: 24 [42752/50048]	Loss: 1.1983
Training Epoch: 24 [42880/50048]	Loss: 1.2313
Training Epoch: 24 [43008/50048]	Loss: 1.1855
Training Epoch: 24 [43136/50048]	Loss: 1.1201
Training Epoch: 24 [43264/50048]	Loss: 1.1718
Training Epoch: 24 [43392/50048]	Loss: 1.3748
Training Epoch: 24 [43520/50048]	Loss: 1.2424
Training Epoch: 24 [43648/50048]	Loss: 1.3791
Training Epoch: 24 [43776/50048]	Loss: 1.0015
Training Epoch: 24 [43904/50048]	Loss: 1.1462
Training Epoch: 24 [44032/50048]	Loss: 1.1874
Training Epoch: 24 [44160/50048]	Loss: 1.0738
Training Epoch: 24 [44288/50048]	Loss: 1.0591
Training Epoch: 24 [44416/50048]	Loss: 1.1293
Training Epoch: 24 [44544/50048]	Loss: 1.0983
Training Epoch: 24 [44672/50048]	Loss: 1.1075
Training Epoch: 24 [44800/50048]	Loss: 1.0847
Training Epoch: 24 [44928/50048]	Loss: 1.5555
Training Epoch: 24 [45056/50048]	Loss: 1.3968
Training Epoch: 24 [45184/50048]	Loss: 1.2101
Training Epoch: 24 [45312/50048]	Loss: 1.2345
Training Epoch: 24 [45440/50048]	Loss: 1.1351
Training Epoch: 24 [45568/50048]	Loss: 1.2021
Training Epoch: 24 [45696/50048]	Loss: 1.3625
Training Epoch: 24 [45824/50048]	Loss: 1.1531
Training Epoch: 24 [45952/50048]	Loss: 1.2663
Training Epoch: 24 [46080/50048]	Loss: 1.4881
Training Epoch: 24 [46208/50048]	Loss: 1.0779
Training Epoch: 24 [46336/50048]	Loss: 1.0382
Training Epoch: 24 [46464/50048]	Loss: 1.3690
Training Epoch: 24 [46592/50048]	Loss: 1.2552
Training Epoch: 24 [46720/50048]	Loss: 1.1418
Training Epoch: 24 [46848/50048]	Loss: 1.1219
Training Epoch: 24 [46976/50048]	Loss: 1.2723
Training Epoch: 24 [47104/50048]	Loss: 1.5226
Training Epoch: 24 [47232/50048]	Loss: 1.4253
Training Epoch: 24 [47360/50048]	Loss: 1.0951
Training Epoch: 24 [47488/50048]	Loss: 0.9342
Training Epoch: 24 [47616/50048]	Loss: 1.0779
Training Epoch: 24 [47744/50048]	Loss: 1.2613
Training Epoch: 24 [47872/50048]	Loss: 1.2484
Training Epoch: 24 [48000/50048]	Loss: 0.9834
Training Epoch: 24 [48128/50048]	Loss: 1.1628
Training Epoch: 24 [48256/50048]	Loss: 1.2182
Training Epoch: 24 [48384/50048]	Loss: 1.3124
Training Epoch: 24 [48512/50048]	Loss: 1.2298
Training Epoch: 24 [48640/50048]	Loss: 1.3623
Training Epoch: 24 [48768/50048]	Loss: 1.2080
Training Epoch: 24 [48896/50048]	Loss: 1.1439
Training Epoch: 24 [49024/50048]	Loss: 1.1125
Training Epoch: 24 [49152/50048]	Loss: 1.3782
Training Epoch: 24 [49280/50048]	Loss: 1.2715
Training Epoch: 24 [49408/50048]	Loss: 1.0506
Training Epoch: 24 [49536/50048]	Loss: 1.3693
Training Epoch: 24 [49664/50048]	Loss: 1.5095
Training Epoch: 24 [49792/50048]	Loss: 1.2408
Training Epoch: 24 [49920/50048]	Loss: 1.2623
Training Epoch: 24 [50048/50048]	Loss: 1.2783
Validation Epoch: 24, Average loss: 0.0118, Accuracy: 0.5919
Training Epoch: 25 [128/50048]	Loss: 1.2077
Training Epoch: 25 [256/50048]	Loss: 1.0291
Training Epoch: 25 [384/50048]	Loss: 1.0476
Training Epoch: 25 [512/50048]	Loss: 1.0315
Training Epoch: 25 [640/50048]	Loss: 1.2206
Training Epoch: 25 [768/50048]	Loss: 1.1592
Training Epoch: 25 [896/50048]	Loss: 1.0384
Training Epoch: 25 [1024/50048]	Loss: 1.0792
Training Epoch: 25 [1152/50048]	Loss: 1.0994
Training Epoch: 25 [1280/50048]	Loss: 1.1903
Training Epoch: 25 [1408/50048]	Loss: 1.3720
Training Epoch: 25 [1536/50048]	Loss: 1.0975
Training Epoch: 25 [1664/50048]	Loss: 1.1927
Training Epoch: 25 [1792/50048]	Loss: 1.0428
Training Epoch: 25 [1920/50048]	Loss: 1.2106
Training Epoch: 25 [2048/50048]	Loss: 1.2920
Training Epoch: 25 [2176/50048]	Loss: 1.2271
Training Epoch: 25 [2304/50048]	Loss: 0.9317
Training Epoch: 25 [2432/50048]	Loss: 1.3909
Training Epoch: 25 [2560/50048]	Loss: 1.1387
Training Epoch: 25 [2688/50048]	Loss: 0.9644
Training Epoch: 25 [2816/50048]	Loss: 1.2116
Training Epoch: 25 [2944/50048]	Loss: 1.0907
Training Epoch: 25 [3072/50048]	Loss: 0.9512
Training Epoch: 25 [3200/50048]	Loss: 0.9738
Training Epoch: 25 [3328/50048]	Loss: 1.2111
Training Epoch: 25 [3456/50048]	Loss: 1.0941
Training Epoch: 25 [3584/50048]	Loss: 1.3435
Training Epoch: 25 [3712/50048]	Loss: 1.2046
Training Epoch: 25 [3840/50048]	Loss: 1.1085
Training Epoch: 25 [3968/50048]	Loss: 1.1293
Training Epoch: 25 [4096/50048]	Loss: 1.1430
Training Epoch: 25 [4224/50048]	Loss: 0.9105
Training Epoch: 25 [4352/50048]	Loss: 1.2166
Training Epoch: 25 [4480/50048]	Loss: 1.2524
Training Epoch: 25 [4608/50048]	Loss: 1.2026
Training Epoch: 25 [4736/50048]	Loss: 1.1732
Training Epoch: 25 [4864/50048]	Loss: 1.0934
Training Epoch: 25 [4992/50048]	Loss: 0.9919
Training Epoch: 25 [5120/50048]	Loss: 0.9175
Training Epoch: 25 [5248/50048]	Loss: 0.9500
Training Epoch: 25 [5376/50048]	Loss: 0.7712
Training Epoch: 25 [5504/50048]	Loss: 0.9792
Training Epoch: 25 [5632/50048]	Loss: 1.2259
Training Epoch: 25 [5760/50048]	Loss: 1.0238
Training Epoch: 25 [5888/50048]	Loss: 1.3048
Training Epoch: 25 [6016/50048]	Loss: 1.0631
Training Epoch: 25 [6144/50048]	Loss: 1.0714
Training Epoch: 25 [6272/50048]	Loss: 1.1282
Training Epoch: 25 [6400/50048]	Loss: 0.9890
Training Epoch: 25 [6528/50048]	Loss: 1.1010
Training Epoch: 25 [6656/50048]	Loss: 0.9778
Training Epoch: 25 [6784/50048]	Loss: 1.1209
Training Epoch: 25 [6912/50048]	Loss: 1.1476
Training Epoch: 25 [7040/50048]	Loss: 1.0377
Training Epoch: 25 [7168/50048]	Loss: 1.3073
Training Epoch: 25 [7296/50048]	Loss: 1.1199
Training Epoch: 25 [7424/50048]	Loss: 1.4395
Training Epoch: 25 [7552/50048]	Loss: 1.1995
Training Epoch: 25 [7680/50048]	Loss: 1.1261
Training Epoch: 25 [7808/50048]	Loss: 1.4350
Training Epoch: 25 [7936/50048]	Loss: 1.1303
Training Epoch: 25 [8064/50048]	Loss: 1.2585
Training Epoch: 25 [8192/50048]	Loss: 1.2326
Training Epoch: 25 [8320/50048]	Loss: 1.1895
Training Epoch: 25 [8448/50048]	Loss: 1.0421
Training Epoch: 25 [8576/50048]	Loss: 1.2880
Training Epoch: 25 [8704/50048]	Loss: 0.8690
Training Epoch: 25 [8832/50048]	Loss: 1.1414
Training Epoch: 25 [8960/50048]	Loss: 1.2996
Training Epoch: 25 [9088/50048]	Loss: 1.2123
Training Epoch: 25 [9216/50048]	Loss: 1.3654
Training Epoch: 25 [9344/50048]	Loss: 1.2048
Training Epoch: 25 [9472/50048]	Loss: 1.2227
Training Epoch: 25 [9600/50048]	Loss: 1.1117
Training Epoch: 25 [9728/50048]	Loss: 1.0768
Training Epoch: 25 [9856/50048]	Loss: 1.0930
Training Epoch: 25 [9984/50048]	Loss: 1.2750
Training Epoch: 25 [10112/50048]	Loss: 1.1422
Training Epoch: 25 [10240/50048]	Loss: 1.2272
Training Epoch: 25 [10368/50048]	Loss: 0.9690
Training Epoch: 25 [10496/50048]	Loss: 1.2752
Training Epoch: 25 [10624/50048]	Loss: 1.2175
Training Epoch: 25 [10752/50048]	Loss: 0.9533
Training Epoch: 25 [10880/50048]	Loss: 1.0162
Training Epoch: 25 [11008/50048]	Loss: 0.9398
Training Epoch: 25 [11136/50048]	Loss: 0.9021
Training Epoch: 25 [11264/50048]	Loss: 1.1195
Training Epoch: 25 [11392/50048]	Loss: 1.1942
Training Epoch: 25 [11520/50048]	Loss: 0.9049
Training Epoch: 25 [11648/50048]	Loss: 1.2361
Training Epoch: 25 [11776/50048]	Loss: 1.1128
Training Epoch: 25 [11904/50048]	Loss: 1.2394
Training Epoch: 25 [12032/50048]	Loss: 1.1671
Training Epoch: 25 [12160/50048]	Loss: 1.2100
Training Epoch: 25 [12288/50048]	Loss: 1.2459
Training Epoch: 25 [12416/50048]	Loss: 1.1470
Training Epoch: 25 [12544/50048]	Loss: 1.1564
Training Epoch: 25 [12672/50048]	Loss: 1.1085
Training Epoch: 25 [12800/50048]	Loss: 1.2326
Training Epoch: 25 [12928/50048]	Loss: 0.8715
Training Epoch: 25 [13056/50048]	Loss: 1.3080
Training Epoch: 25 [13184/50048]	Loss: 1.1929
Training Epoch: 25 [13312/50048]	Loss: 1.1813
Training Epoch: 25 [13440/50048]	Loss: 1.1429
Training Epoch: 25 [13568/50048]	Loss: 0.9685
Training Epoch: 25 [13696/50048]	Loss: 1.0951
Training Epoch: 25 [13824/50048]	Loss: 1.2604
Training Epoch: 25 [13952/50048]	Loss: 1.1526
Training Epoch: 25 [14080/50048]	Loss: 1.2113
Training Epoch: 25 [14208/50048]	Loss: 1.1752
Training Epoch: 25 [14336/50048]	Loss: 1.1006
Training Epoch: 25 [14464/50048]	Loss: 1.0823
Training Epoch: 25 [14592/50048]	Loss: 1.1283
Training Epoch: 25 [14720/50048]	Loss: 1.0580
Training Epoch: 25 [14848/50048]	Loss: 1.4790
Training Epoch: 25 [14976/50048]	Loss: 1.1744
Training Epoch: 25 [15104/50048]	Loss: 0.9840
Training Epoch: 25 [15232/50048]	Loss: 1.0857
Training Epoch: 25 [15360/50048]	Loss: 1.2088
Training Epoch: 25 [15488/50048]	Loss: 1.2743
Training Epoch: 25 [15616/50048]	Loss: 1.1768
Training Epoch: 25 [15744/50048]	Loss: 1.2091
Training Epoch: 25 [15872/50048]	Loss: 1.0921
Training Epoch: 25 [16000/50048]	Loss: 1.0407
Training Epoch: 25 [16128/50048]	Loss: 1.2105
Training Epoch: 25 [16256/50048]	Loss: 1.0887
Training Epoch: 25 [16384/50048]	Loss: 0.9779
Training Epoch: 25 [16512/50048]	Loss: 1.1717
Training Epoch: 25 [16640/50048]	Loss: 1.1668
Training Epoch: 25 [16768/50048]	Loss: 0.9198
Training Epoch: 25 [16896/50048]	Loss: 1.1564
Training Epoch: 25 [17024/50048]	Loss: 1.2948
Training Epoch: 25 [17152/50048]	Loss: 1.3795
Training Epoch: 25 [17280/50048]	Loss: 1.1896
Training Epoch: 25 [17408/50048]	Loss: 1.3109
Training Epoch: 25 [17536/50048]	Loss: 1.0582
Training Epoch: 25 [17664/50048]	Loss: 0.9656
Training Epoch: 25 [17792/50048]	Loss: 1.1622
Training Epoch: 25 [17920/50048]	Loss: 1.2541
Training Epoch: 25 [18048/50048]	Loss: 1.3178
Training Epoch: 25 [18176/50048]	Loss: 1.0728
Training Epoch: 25 [18304/50048]	Loss: 1.2132
Training Epoch: 25 [18432/50048]	Loss: 1.1569
Training Epoch: 25 [18560/50048]	Loss: 0.9451
Training Epoch: 25 [18688/50048]	Loss: 1.1649
Training Epoch: 25 [18816/50048]	Loss: 1.4035
Training Epoch: 25 [18944/50048]	Loss: 1.0975
Training Epoch: 25 [19072/50048]	Loss: 1.0383
Training Epoch: 25 [19200/50048]	Loss: 1.1378
Training Epoch: 25 [19328/50048]	Loss: 1.3106
Training Epoch: 25 [19456/50048]	Loss: 1.0875
Training Epoch: 25 [19584/50048]	Loss: 1.3874
Training Epoch: 25 [19712/50048]	Loss: 0.9603
Training Epoch: 25 [19840/50048]	Loss: 1.4218
Training Epoch: 25 [19968/50048]	Loss: 1.0934
Training Epoch: 25 [20096/50048]	Loss: 1.1401
Training Epoch: 25 [20224/50048]	Loss: 1.3689
Training Epoch: 25 [20352/50048]	Loss: 1.1647
Training Epoch: 25 [20480/50048]	Loss: 1.1718
Training Epoch: 25 [20608/50048]	Loss: 1.0044
Training Epoch: 25 [20736/50048]	Loss: 1.1019
Training Epoch: 25 [20864/50048]	Loss: 1.3522
Training Epoch: 25 [20992/50048]	Loss: 1.3049
Training Epoch: 25 [21120/50048]	Loss: 0.9601
Training Epoch: 25 [21248/50048]	Loss: 1.2317
Training Epoch: 25 [21376/50048]	Loss: 1.2377
Training Epoch: 25 [21504/50048]	Loss: 1.3117
Training Epoch: 25 [21632/50048]	Loss: 1.0530
Training Epoch: 25 [21760/50048]	Loss: 1.2063
Training Epoch: 25 [21888/50048]	Loss: 1.1608
Training Epoch: 25 [22016/50048]	Loss: 1.1374
Training Epoch: 25 [22144/50048]	Loss: 1.1302
Training Epoch: 25 [22272/50048]	Loss: 1.1399
Training Epoch: 25 [22400/50048]	Loss: 1.3275
Training Epoch: 25 [22528/50048]	Loss: 1.2466
Training Epoch: 25 [22656/50048]	Loss: 1.3172
Training Epoch: 25 [22784/50048]	Loss: 1.2629
Training Epoch: 25 [22912/50048]	Loss: 1.1889
Training Epoch: 25 [23040/50048]	Loss: 1.0263
Training Epoch: 25 [23168/50048]	Loss: 0.9696
Training Epoch: 25 [23296/50048]	Loss: 1.0347
Training Epoch: 25 [23424/50048]	Loss: 1.2729
Training Epoch: 25 [23552/50048]	Loss: 1.0686
Training Epoch: 25 [23680/50048]	Loss: 1.3958
Training Epoch: 25 [23808/50048]	Loss: 1.0009
Training Epoch: 25 [23936/50048]	Loss: 1.1979
Training Epoch: 25 [24064/50048]	Loss: 1.1059
Training Epoch: 25 [24192/50048]	Loss: 1.2194
Training Epoch: 25 [24320/50048]	Loss: 1.0206
Training Epoch: 25 [24448/50048]	Loss: 1.1147
Training Epoch: 25 [24576/50048]	Loss: 1.2244
Training Epoch: 25 [24704/50048]	Loss: 1.0722
Training Epoch: 25 [24832/50048]	Loss: 1.0368
Training Epoch: 25 [24960/50048]	Loss: 0.9424
Training Epoch: 25 [25088/50048]	Loss: 1.0308
Training Epoch: 25 [25216/50048]	Loss: 1.1980
Training Epoch: 25 [25344/50048]	Loss: 1.1682
Training Epoch: 25 [25472/50048]	Loss: 1.2618
Training Epoch: 25 [25600/50048]	Loss: 0.9444
Training Epoch: 25 [25728/50048]	Loss: 1.1493
Training Epoch: 25 [25856/50048]	Loss: 1.2108
Training Epoch: 25 [25984/50048]	Loss: 1.0449
Training Epoch: 25 [26112/50048]	Loss: 1.1496
Training Epoch: 25 [26240/50048]	Loss: 1.1357
Training Epoch: 25 [26368/50048]	Loss: 1.2736
Training Epoch: 25 [26496/50048]	Loss: 1.1245
Training Epoch: 25 [26624/50048]	Loss: 1.2897
Training Epoch: 25 [26752/50048]	Loss: 1.2416
Training Epoch: 25 [26880/50048]	Loss: 1.2100
Training Epoch: 25 [27008/50048]	Loss: 1.1828
Training Epoch: 25 [27136/50048]	Loss: 1.3611
Training Epoch: 25 [27264/50048]	Loss: 1.0917
Training Epoch: 25 [27392/50048]	Loss: 1.1546
Training Epoch: 25 [27520/50048]	Loss: 1.2733
Training Epoch: 25 [27648/50048]	Loss: 1.0755
Training Epoch: 25 [27776/50048]	Loss: 1.0985
Training Epoch: 25 [27904/50048]	Loss: 1.1921
Training Epoch: 25 [28032/50048]	Loss: 1.3237
Training Epoch: 25 [28160/50048]	Loss: 1.1915
Training Epoch: 25 [28288/50048]	Loss: 1.1612
Training Epoch: 25 [28416/50048]	Loss: 0.9326
Training Epoch: 25 [28544/50048]	Loss: 1.1395
Training Epoch: 25 [28672/50048]	Loss: 1.3850
Training Epoch: 25 [28800/50048]	Loss: 0.9651
Training Epoch: 25 [28928/50048]	Loss: 1.0717
Training Epoch: 25 [29056/50048]	Loss: 1.1471
Training Epoch: 25 [29184/50048]	Loss: 1.0758
Training Epoch: 25 [29312/50048]	Loss: 1.0604
Training Epoch: 25 [29440/50048]	Loss: 1.1410
Training Epoch: 25 [29568/50048]	Loss: 1.1010
Training Epoch: 25 [29696/50048]	Loss: 1.2806
Training Epoch: 25 [29824/50048]	Loss: 1.0046
Training Epoch: 25 [29952/50048]	Loss: 0.9763
Training Epoch: 25 [30080/50048]	Loss: 1.2563
Training Epoch: 25 [30208/50048]	Loss: 1.1841
Training Epoch: 25 [30336/50048]	Loss: 1.0345
Training Epoch: 25 [30464/50048]	Loss: 1.0400
Training Epoch: 25 [30592/50048]	Loss: 1.1266
Training Epoch: 25 [30720/50048]	Loss: 1.3002
Training Epoch: 25 [30848/50048]	Loss: 1.2328
Training Epoch: 25 [30976/50048]	Loss: 1.1766
Training Epoch: 25 [31104/50048]	Loss: 1.1312
Training Epoch: 25 [31232/50048]	Loss: 1.3844
Training Epoch: 25 [31360/50048]	Loss: 1.2774
Training Epoch: 25 [31488/50048]	Loss: 1.0967
Training Epoch: 25 [31616/50048]	Loss: 0.8287
Training Epoch: 25 [31744/50048]	Loss: 1.5168
Training Epoch: 25 [31872/50048]	Loss: 1.0144
Training Epoch: 25 [32000/50048]	Loss: 1.1851
Training Epoch: 25 [32128/50048]	Loss: 1.3969
Training Epoch: 25 [32256/50048]	Loss: 1.1147
Training Epoch: 25 [32384/50048]	Loss: 1.0279
Training Epoch: 25 [32512/50048]	Loss: 1.0084
Training Epoch: 25 [32640/50048]	Loss: 1.1832
Training Epoch: 25 [32768/50048]	Loss: 1.1097
Training Epoch: 25 [32896/50048]	Loss: 1.3546
Training Epoch: 25 [33024/50048]	Loss: 1.1610
Training Epoch: 25 [33152/50048]	Loss: 1.0494
Training Epoch: 25 [33280/50048]	Loss: 1.1397
Training Epoch: 25 [33408/50048]	Loss: 0.9929
Training Epoch: 25 [33536/50048]	Loss: 1.2739
Training Epoch: 25 [33664/50048]	Loss: 1.1259
Training Epoch: 25 [33792/50048]	Loss: 1.2995
Training Epoch: 25 [33920/50048]	Loss: 1.0710
Training Epoch: 25 [34048/50048]	Loss: 1.1825
Training Epoch: 25 [34176/50048]	Loss: 1.1629
Training Epoch: 25 [34304/50048]	Loss: 1.1643
Training Epoch: 25 [34432/50048]	Loss: 1.2317
Training Epoch: 25 [34560/50048]	Loss: 1.4799
Training Epoch: 25 [34688/50048]	Loss: 1.2570
Training Epoch: 25 [34816/50048]	Loss: 1.2921
Training Epoch: 25 [34944/50048]	Loss: 1.3236
Training Epoch: 25 [35072/50048]	Loss: 1.1118
Training Epoch: 25 [35200/50048]	Loss: 1.1301
Training Epoch: 25 [35328/50048]	Loss: 1.1345
Training Epoch: 25 [35456/50048]	Loss: 1.1947
Training Epoch: 25 [35584/50048]	Loss: 1.2738
Training Epoch: 25 [35712/50048]	Loss: 1.1341
Training Epoch: 25 [35840/50048]	Loss: 1.1350
Training Epoch: 25 [35968/50048]	Loss: 1.3195
Training Epoch: 25 [36096/50048]	Loss: 1.1203
Training Epoch: 25 [36224/50048]	Loss: 1.0711
Training Epoch: 25 [36352/50048]	Loss: 1.1957
Training Epoch: 25 [36480/50048]	Loss: 0.9458
Training Epoch: 25 [36608/50048]	Loss: 1.2273
Training Epoch: 25 [36736/50048]	Loss: 1.3923
Training Epoch: 25 [36864/50048]	Loss: 1.2625
Training Epoch: 25 [36992/50048]	Loss: 1.2207
Training Epoch: 25 [37120/50048]	Loss: 1.2519
Training Epoch: 25 [37248/50048]	Loss: 1.2189
Training Epoch: 25 [37376/50048]	Loss: 1.4322
Training Epoch: 25 [37504/50048]	Loss: 1.1442
Training Epoch: 25 [37632/50048]	Loss: 1.1599
Training Epoch: 25 [37760/50048]	Loss: 1.2698
Training Epoch: 25 [37888/50048]	Loss: 1.1003
Training Epoch: 25 [38016/50048]	Loss: 1.1297
Training Epoch: 25 [38144/50048]	Loss: 1.0244
Training Epoch: 25 [38272/50048]	Loss: 1.1644
Training Epoch: 25 [38400/50048]	Loss: 1.2230
Training Epoch: 25 [38528/50048]	Loss: 1.1493
Training Epoch: 25 [38656/50048]	Loss: 0.9507
Training Epoch: 25 [38784/50048]	Loss: 1.0299
Training Epoch: 25 [38912/50048]	Loss: 1.1311
Training Epoch: 25 [39040/50048]	Loss: 1.0766
Training Epoch: 25 [39168/50048]	Loss: 1.1076
Training Epoch: 25 [39296/50048]	Loss: 1.3722
Training Epoch: 25 [39424/50048]	Loss: 1.1347
Training Epoch: 25 [39552/50048]	Loss: 1.3682
Training Epoch: 25 [39680/50048]	Loss: 0.9406
Training Epoch: 25 [39808/50048]	Loss: 1.3236
Training Epoch: 25 [39936/50048]	Loss: 1.1657
Training Epoch: 25 [40064/50048]	Loss: 1.2663
Training Epoch: 25 [40192/50048]	Loss: 1.1877
Training Epoch: 25 [40320/50048]	Loss: 1.1624
Training Epoch: 25 [40448/50048]	Loss: 1.1365
Training Epoch: 25 [40576/50048]	Loss: 1.3247
Training Epoch: 25 [40704/50048]	Loss: 1.0910
Training Epoch: 25 [40832/50048]	Loss: 1.2881
Training Epoch: 25 [40960/50048]	Loss: 1.0444
Training Epoch: 25 [41088/50048]	Loss: 0.9538
Training Epoch: 25 [41216/50048]	Loss: 1.0495
Training Epoch: 25 [41344/50048]	Loss: 1.3508
Training Epoch: 25 [41472/50048]	Loss: 1.2094
Training Epoch: 25 [41600/50048]	Loss: 1.2245
Training Epoch: 25 [41728/50048]	Loss: 0.9699
Training Epoch: 25 [41856/50048]	Loss: 1.2080
Training Epoch: 25 [41984/50048]	Loss: 0.8761
Training Epoch: 25 [42112/50048]	Loss: 1.0244
Training Epoch: 25 [42240/50048]	Loss: 1.2442
Training Epoch: 25 [42368/50048]	Loss: 1.3630
Training Epoch: 25 [42496/50048]	Loss: 1.0876
Training Epoch: 25 [42624/50048]	Loss: 1.3196
Training Epoch: 25 [42752/50048]	Loss: 1.0236
Training Epoch: 25 [42880/50048]	Loss: 1.2351
Training Epoch: 25 [43008/50048]	Loss: 1.1813
Training Epoch: 25 [43136/50048]	Loss: 1.1763
Training Epoch: 25 [43264/50048]	Loss: 1.0569
Training Epoch: 25 [43392/50048]	Loss: 1.1410
Training Epoch: 25 [43520/50048]	Loss: 1.1148
Training Epoch: 25 [43648/50048]	Loss: 1.0792
Training Epoch: 25 [43776/50048]	Loss: 1.1603
Training Epoch: 25 [43904/50048]	Loss: 1.2540
Training Epoch: 25 [44032/50048]	Loss: 1.1621
Training Epoch: 25 [44160/50048]	Loss: 1.2781
Training Epoch: 25 [44288/50048]	Loss: 1.0724
Training Epoch: 25 [44416/50048]	Loss: 1.0725
Training Epoch: 25 [44544/50048]	Loss: 1.2270
Training Epoch: 25 [44672/50048]	Loss: 0.9313
Training Epoch: 25 [44800/50048]	Loss: 1.0620
Training Epoch: 25 [44928/50048]	Loss: 1.2100
Training Epoch: 25 [45056/50048]	Loss: 1.2910
Training Epoch: 25 [45184/50048]	Loss: 1.1190
Training Epoch: 25 [45312/50048]	Loss: 1.1942
Training Epoch: 25 [45440/50048]	Loss: 1.3874
Training Epoch: 25 [45568/50048]	Loss: 1.0909
Training Epoch: 25 [45696/50048]	Loss: 1.0096
Training Epoch: 25 [45824/50048]	Loss: 1.1641
Training Epoch: 25 [45952/50048]	Loss: 1.1902
Training Epoch: 25 [46080/50048]	Loss: 1.2580
Training Epoch: 25 [46208/50048]	Loss: 1.2478
Training Epoch: 25 [46336/50048]	Loss: 1.1806
Training Epoch: 25 [46464/50048]	Loss: 0.9974
Training Epoch: 25 [46592/50048]	Loss: 1.1724
Training Epoch: 25 [46720/50048]	Loss: 0.9940
Training Epoch: 25 [46848/50048]	Loss: 0.9488
Training Epoch: 25 [46976/50048]	Loss: 0.9621
Training Epoch: 25 [47104/50048]	Loss: 1.1874
Training Epoch: 25 [47232/50048]	Loss: 0.9870
Training Epoch: 25 [47360/50048]	Loss: 1.1295
Training Epoch: 25 [47488/50048]	Loss: 1.1016
Training Epoch: 25 [47616/50048]	Loss: 1.2206
Training Epoch: 25 [47744/50048]	Loss: 1.1453
Training Epoch: 25 [47872/50048]	Loss: 0.9599
Training Epoch: 25 [48000/50048]	Loss: 0.8373
Training Epoch: 25 [48128/50048]	Loss: 1.2874
Training Epoch: 25 [48256/50048]	Loss: 0.9479
Training Epoch: 25 [48384/50048]	Loss: 1.3660
Training Epoch: 25 [48512/50048]	Loss: 1.3119
Training Epoch: 25 [48640/50048]	Loss: 1.0445
Training Epoch: 25 [48768/50048]	Loss: 1.1663
Training Epoch: 25 [48896/50048]	Loss: 1.3488
Training Epoch: 25 [49024/50048]	Loss: 1.1470
Training Epoch: 25 [49152/50048]	Loss: 1.0220
Training Epoch: 25 [49280/50048]	Loss: 1.1030
Training Epoch: 25 [49408/50048]	Loss: 1.3806
Training Epoch: 25 [49536/50048]	Loss: 1.2986
Training Epoch: 25 [49664/50048]	Loss: 1.1951
Training Epoch: 25 [49792/50048]	Loss: 1.5893
Training Epoch: 25 [49920/50048]	Loss: 1.2402
Training Epoch: 25 [50048/50048]	Loss: 0.9349
Validation Epoch: 25, Average loss: 0.0119, Accuracy: 0.5866
Training Epoch: 26 [128/50048]	Loss: 1.2435
Training Epoch: 26 [256/50048]	Loss: 1.1115
Training Epoch: 26 [384/50048]	Loss: 1.0980
Training Epoch: 26 [512/50048]	Loss: 1.1608
Training Epoch: 26 [640/50048]	Loss: 0.9599
Training Epoch: 26 [768/50048]	Loss: 1.2391
Training Epoch: 26 [896/50048]	Loss: 1.2454
Training Epoch: 26 [1024/50048]	Loss: 1.3256
Training Epoch: 26 [1152/50048]	Loss: 1.0841
Training Epoch: 26 [1280/50048]	Loss: 1.2866
Training Epoch: 26 [1408/50048]	Loss: 1.1437
Training Epoch: 26 [1536/50048]	Loss: 1.1999
Training Epoch: 26 [1664/50048]	Loss: 0.9966
Training Epoch: 26 [1792/50048]	Loss: 1.2213
Training Epoch: 26 [1920/50048]	Loss: 1.2238
Training Epoch: 26 [2048/50048]	Loss: 1.1064
Training Epoch: 26 [2176/50048]	Loss: 0.9326
Training Epoch: 26 [2304/50048]	Loss: 0.9353
Training Epoch: 26 [2432/50048]	Loss: 1.1614
Training Epoch: 26 [2560/50048]	Loss: 0.9591
Training Epoch: 26 [2688/50048]	Loss: 1.1524
Training Epoch: 26 [2816/50048]	Loss: 1.2542
Training Epoch: 26 [2944/50048]	Loss: 1.2460
Training Epoch: 26 [3072/50048]	Loss: 0.9492
Training Epoch: 26 [3200/50048]	Loss: 1.3149
Training Epoch: 26 [3328/50048]	Loss: 1.2287
Training Epoch: 26 [3456/50048]	Loss: 1.0442
Training Epoch: 26 [3584/50048]	Loss: 1.1446
Training Epoch: 26 [3712/50048]	Loss: 0.9699
Training Epoch: 26 [3840/50048]	Loss: 1.0911
Training Epoch: 26 [3968/50048]	Loss: 0.8637
Training Epoch: 26 [4096/50048]	Loss: 1.2446
Training Epoch: 26 [4224/50048]	Loss: 1.0524
Training Epoch: 26 [4352/50048]	Loss: 0.9392
Training Epoch: 26 [4480/50048]	Loss: 1.1359
Training Epoch: 26 [4608/50048]	Loss: 1.4214
Training Epoch: 26 [4736/50048]	Loss: 1.0137
Training Epoch: 26 [4864/50048]	Loss: 1.0060
Training Epoch: 26 [4992/50048]	Loss: 0.9963
Training Epoch: 26 [5120/50048]	Loss: 1.1041
Training Epoch: 26 [5248/50048]	Loss: 1.1648
Training Epoch: 26 [5376/50048]	Loss: 1.2908
Training Epoch: 26 [5504/50048]	Loss: 1.1094
Training Epoch: 26 [5632/50048]	Loss: 1.1699
Training Epoch: 26 [5760/50048]	Loss: 1.2122
Training Epoch: 26 [5888/50048]	Loss: 1.0828
Training Epoch: 26 [6016/50048]	Loss: 1.1036
Training Epoch: 26 [6144/50048]	Loss: 1.3544
Training Epoch: 26 [6272/50048]	Loss: 1.0341
Training Epoch: 26 [6400/50048]	Loss: 1.1300
Training Epoch: 26 [6528/50048]	Loss: 1.1735
Training Epoch: 26 [6656/50048]	Loss: 0.8794
Training Epoch: 26 [6784/50048]	Loss: 1.0148
Training Epoch: 26 [6912/50048]	Loss: 1.1476
Training Epoch: 26 [7040/50048]	Loss: 1.0988
Training Epoch: 26 [7168/50048]	Loss: 1.2586
Training Epoch: 26 [7296/50048]	Loss: 1.1871
Training Epoch: 26 [7424/50048]	Loss: 1.1000
Training Epoch: 26 [7552/50048]	Loss: 0.9706
Training Epoch: 26 [7680/50048]	Loss: 1.0337
Training Epoch: 26 [7808/50048]	Loss: 1.2215
Training Epoch: 26 [7936/50048]	Loss: 1.1273
Training Epoch: 26 [8064/50048]	Loss: 0.8294
Training Epoch: 26 [8192/50048]	Loss: 1.4938
Training Epoch: 26 [8320/50048]	Loss: 0.9521
Training Epoch: 26 [8448/50048]	Loss: 1.0052
Training Epoch: 26 [8576/50048]	Loss: 1.0759
Training Epoch: 26 [8704/50048]	Loss: 1.2951
Training Epoch: 26 [8832/50048]	Loss: 1.0713
Training Epoch: 26 [8960/50048]	Loss: 1.0499
Training Epoch: 26 [9088/50048]	Loss: 1.1002
Training Epoch: 26 [9216/50048]	Loss: 1.1100
Training Epoch: 26 [9344/50048]	Loss: 1.2528
Training Epoch: 26 [9472/50048]	Loss: 1.0505
Training Epoch: 26 [9600/50048]	Loss: 1.1312
Training Epoch: 26 [9728/50048]	Loss: 1.0296
Training Epoch: 26 [9856/50048]	Loss: 0.7340
Training Epoch: 26 [9984/50048]	Loss: 1.1016
Training Epoch: 26 [10112/50048]	Loss: 1.0855
Training Epoch: 26 [10240/50048]	Loss: 1.2584
Training Epoch: 26 [10368/50048]	Loss: 1.0577
Training Epoch: 26 [10496/50048]	Loss: 1.0121
Training Epoch: 26 [10624/50048]	Loss: 1.1080
Training Epoch: 26 [10752/50048]	Loss: 1.3180
Training Epoch: 26 [10880/50048]	Loss: 1.0589
Training Epoch: 26 [11008/50048]	Loss: 1.0568
Training Epoch: 26 [11136/50048]	Loss: 1.2058
Training Epoch: 26 [11264/50048]	Loss: 1.2012
Training Epoch: 26 [11392/50048]	Loss: 1.0558
Training Epoch: 26 [11520/50048]	Loss: 1.1211
Training Epoch: 26 [11648/50048]	Loss: 1.1370
Training Epoch: 26 [11776/50048]	Loss: 1.1127
Training Epoch: 26 [11904/50048]	Loss: 1.2957
Training Epoch: 26 [12032/50048]	Loss: 1.2794
Training Epoch: 26 [12160/50048]	Loss: 1.1058
Training Epoch: 26 [12288/50048]	Loss: 1.1967
Training Epoch: 26 [12416/50048]	Loss: 1.1427
Training Epoch: 26 [12544/50048]	Loss: 1.1731
Training Epoch: 26 [12672/50048]	Loss: 1.2328
Training Epoch: 26 [12800/50048]	Loss: 1.0944
Training Epoch: 26 [12928/50048]	Loss: 1.1221
Training Epoch: 26 [13056/50048]	Loss: 1.2002
Training Epoch: 26 [13184/50048]	Loss: 1.2754
Training Epoch: 26 [13312/50048]	Loss: 1.0433
Training Epoch: 26 [13440/50048]	Loss: 1.0040
Training Epoch: 26 [13568/50048]	Loss: 0.8909
Training Epoch: 26 [13696/50048]	Loss: 1.2877
Training Epoch: 26 [13824/50048]	Loss: 1.1599
Training Epoch: 26 [13952/50048]	Loss: 1.2926
Training Epoch: 26 [14080/50048]	Loss: 1.1567
Training Epoch: 26 [14208/50048]	Loss: 1.1292
Training Epoch: 26 [14336/50048]	Loss: 1.0355
Training Epoch: 26 [14464/50048]	Loss: 1.2037
Training Epoch: 26 [14592/50048]	Loss: 1.1487
Training Epoch: 26 [14720/50048]	Loss: 1.0796
Training Epoch: 26 [14848/50048]	Loss: 1.0634
Training Epoch: 26 [14976/50048]	Loss: 0.9851
Training Epoch: 26 [15104/50048]	Loss: 1.0205
Training Epoch: 26 [15232/50048]	Loss: 1.0971
Training Epoch: 26 [15360/50048]	Loss: 1.1886
Training Epoch: 26 [15488/50048]	Loss: 0.8469
Training Epoch: 26 [15616/50048]	Loss: 1.4046
Training Epoch: 26 [15744/50048]	Loss: 0.9968
Training Epoch: 26 [15872/50048]	Loss: 1.2183
Training Epoch: 26 [16000/50048]	Loss: 1.2100
Training Epoch: 26 [16128/50048]	Loss: 1.1366
Training Epoch: 26 [16256/50048]	Loss: 1.0509
Training Epoch: 26 [16384/50048]	Loss: 1.3080
Training Epoch: 26 [16512/50048]	Loss: 1.0658
Training Epoch: 26 [16640/50048]	Loss: 1.0539
Training Epoch: 26 [16768/50048]	Loss: 1.2501
Training Epoch: 26 [16896/50048]	Loss: 1.0498
Training Epoch: 26 [17024/50048]	Loss: 1.2152
Training Epoch: 26 [17152/50048]	Loss: 1.0776
Training Epoch: 26 [17280/50048]	Loss: 1.0124
Training Epoch: 26 [17408/50048]	Loss: 1.0818
Training Epoch: 26 [17536/50048]	Loss: 1.0493
Training Epoch: 26 [17664/50048]	Loss: 1.1358
Training Epoch: 26 [17792/50048]	Loss: 1.1158
Training Epoch: 26 [17920/50048]	Loss: 1.1089
Training Epoch: 26 [18048/50048]	Loss: 1.1001
Training Epoch: 26 [18176/50048]	Loss: 0.9965
Training Epoch: 26 [18304/50048]	Loss: 1.0644
Training Epoch: 26 [18432/50048]	Loss: 1.0844
Training Epoch: 26 [18560/50048]	Loss: 1.0315
Training Epoch: 26 [18688/50048]	Loss: 1.1268
Training Epoch: 26 [18816/50048]	Loss: 1.1026
Training Epoch: 26 [18944/50048]	Loss: 1.1553
Training Epoch: 26 [19072/50048]	Loss: 1.2931
Training Epoch: 26 [19200/50048]	Loss: 1.0763
Training Epoch: 26 [19328/50048]	Loss: 0.9686
Training Epoch: 26 [19456/50048]	Loss: 1.0516
Training Epoch: 26 [19584/50048]	Loss: 1.2466
Training Epoch: 26 [19712/50048]	Loss: 1.2909
Training Epoch: 26 [19840/50048]	Loss: 0.9356
Training Epoch: 26 [19968/50048]	Loss: 1.2174
Training Epoch: 26 [20096/50048]	Loss: 1.2400
Training Epoch: 26 [20224/50048]	Loss: 1.0294
Training Epoch: 26 [20352/50048]	Loss: 1.2857
Training Epoch: 26 [20480/50048]	Loss: 0.9978
Training Epoch: 26 [20608/50048]	Loss: 1.1326
Training Epoch: 26 [20736/50048]	Loss: 1.2932
Training Epoch: 26 [20864/50048]	Loss: 1.1244
Training Epoch: 26 [20992/50048]	Loss: 1.0201
Training Epoch: 26 [21120/50048]	Loss: 0.9959
Training Epoch: 26 [21248/50048]	Loss: 1.0792
Training Epoch: 26 [21376/50048]	Loss: 1.2205
Training Epoch: 26 [21504/50048]	Loss: 0.9521
Training Epoch: 26 [21632/50048]	Loss: 1.1533
Training Epoch: 26 [21760/50048]	Loss: 1.3545
Training Epoch: 26 [21888/50048]	Loss: 0.9977
Training Epoch: 26 [22016/50048]	Loss: 1.2086
Training Epoch: 26 [22144/50048]	Loss: 1.0800
Training Epoch: 26 [22272/50048]	Loss: 1.2297
Training Epoch: 26 [22400/50048]	Loss: 0.9544
Training Epoch: 26 [22528/50048]	Loss: 1.2222
Training Epoch: 26 [22656/50048]	Loss: 1.1409
Training Epoch: 26 [22784/50048]	Loss: 1.2883
Training Epoch: 26 [22912/50048]	Loss: 1.5255
Training Epoch: 26 [23040/50048]	Loss: 1.0481
Training Epoch: 26 [23168/50048]	Loss: 1.2379
Training Epoch: 26 [23296/50048]	Loss: 1.1069
Training Epoch: 26 [23424/50048]	Loss: 0.8910
Training Epoch: 26 [23552/50048]	Loss: 1.0000
Training Epoch: 26 [23680/50048]	Loss: 1.2074
Training Epoch: 26 [23808/50048]	Loss: 1.1305
Training Epoch: 26 [23936/50048]	Loss: 0.9088
Training Epoch: 26 [24064/50048]	Loss: 1.2741
Training Epoch: 26 [24192/50048]	Loss: 1.1587
Training Epoch: 26 [24320/50048]	Loss: 1.1886
Training Epoch: 26 [24448/50048]	Loss: 1.3121
Training Epoch: 26 [24576/50048]	Loss: 1.1985
Training Epoch: 26 [24704/50048]	Loss: 1.0119
Training Epoch: 26 [24832/50048]	Loss: 1.3019
Training Epoch: 26 [24960/50048]	Loss: 0.9035
Training Epoch: 26 [25088/50048]	Loss: 1.1243
Training Epoch: 26 [25216/50048]	Loss: 1.0976
Training Epoch: 26 [25344/50048]	Loss: 1.1662
Training Epoch: 26 [25472/50048]	Loss: 1.0239
Training Epoch: 26 [25600/50048]	Loss: 1.3733
Training Epoch: 26 [25728/50048]	Loss: 1.3274
Training Epoch: 26 [25856/50048]	Loss: 1.1943
Training Epoch: 26 [25984/50048]	Loss: 1.2808
Training Epoch: 26 [26112/50048]	Loss: 1.0983
Training Epoch: 26 [26240/50048]	Loss: 1.0100
Training Epoch: 26 [26368/50048]	Loss: 1.4815
Training Epoch: 26 [26496/50048]	Loss: 1.2056
Training Epoch: 26 [26624/50048]	Loss: 1.1583
Training Epoch: 26 [26752/50048]	Loss: 1.0285
Training Epoch: 26 [26880/50048]	Loss: 1.1839
Training Epoch: 26 [27008/50048]	Loss: 1.0387
Training Epoch: 26 [27136/50048]	Loss: 1.2234
Training Epoch: 26 [27264/50048]	Loss: 1.1775
Training Epoch: 26 [27392/50048]	Loss: 1.0162
Training Epoch: 26 [27520/50048]	Loss: 1.1764
Training Epoch: 26 [27648/50048]	Loss: 1.1459
Training Epoch: 26 [27776/50048]	Loss: 1.2176
Training Epoch: 26 [27904/50048]	Loss: 1.0665
Training Epoch: 26 [28032/50048]	Loss: 1.1273
Training Epoch: 26 [28160/50048]	Loss: 1.1927
Training Epoch: 26 [28288/50048]	Loss: 1.1191
Training Epoch: 26 [28416/50048]	Loss: 1.0904
Training Epoch: 26 [28544/50048]	Loss: 1.1008
Training Epoch: 26 [28672/50048]	Loss: 1.1488
Training Epoch: 26 [28800/50048]	Loss: 1.1829
Training Epoch: 26 [28928/50048]	Loss: 1.0944
Training Epoch: 26 [29056/50048]	Loss: 1.0288
Training Epoch: 26 [29184/50048]	Loss: 1.4121
Training Epoch: 26 [29312/50048]	Loss: 0.9978
Training Epoch: 26 [29440/50048]	Loss: 0.9091
Training Epoch: 26 [29568/50048]	Loss: 1.1284
Training Epoch: 26 [29696/50048]	Loss: 1.0479
Training Epoch: 26 [29824/50048]	Loss: 1.1741
Training Epoch: 26 [29952/50048]	Loss: 1.0405
Training Epoch: 26 [30080/50048]	Loss: 1.2234
Training Epoch: 26 [30208/50048]	Loss: 1.2292
Training Epoch: 26 [30336/50048]	Loss: 1.3668
Training Epoch: 26 [30464/50048]	Loss: 0.8904
Training Epoch: 26 [30592/50048]	Loss: 1.0941
Training Epoch: 26 [30720/50048]	Loss: 0.9393
Training Epoch: 26 [30848/50048]	Loss: 1.1329
Training Epoch: 26 [30976/50048]	Loss: 1.0441
Training Epoch: 26 [31104/50048]	Loss: 1.0265
Training Epoch: 26 [31232/50048]	Loss: 1.1712
Training Epoch: 26 [31360/50048]	Loss: 1.1868
Training Epoch: 26 [31488/50048]	Loss: 1.3152
Training Epoch: 26 [31616/50048]	Loss: 1.1571
Training Epoch: 26 [31744/50048]	Loss: 1.0140
Training Epoch: 26 [31872/50048]	Loss: 1.2273
Training Epoch: 26 [32000/50048]	Loss: 1.0752
Training Epoch: 26 [32128/50048]	Loss: 1.0051
Training Epoch: 26 [32256/50048]	Loss: 1.0213
Training Epoch: 26 [32384/50048]	Loss: 1.1548
Training Epoch: 26 [32512/50048]	Loss: 1.0016
Training Epoch: 26 [32640/50048]	Loss: 1.1308
Training Epoch: 26 [32768/50048]	Loss: 1.1350
Training Epoch: 26 [32896/50048]	Loss: 1.1341
Training Epoch: 26 [33024/50048]	Loss: 1.0013
Training Epoch: 26 [33152/50048]	Loss: 1.0804
Training Epoch: 26 [33280/50048]	Loss: 1.2042
Training Epoch: 26 [33408/50048]	Loss: 1.1009
Training Epoch: 26 [33536/50048]	Loss: 1.2484
Training Epoch: 26 [33664/50048]	Loss: 0.8859
Training Epoch: 26 [33792/50048]	Loss: 1.0611
Training Epoch: 26 [33920/50048]	Loss: 1.1694
Training Epoch: 26 [34048/50048]	Loss: 1.1751
Training Epoch: 26 [34176/50048]	Loss: 1.1341
Training Epoch: 26 [34304/50048]	Loss: 1.3329
Training Epoch: 26 [34432/50048]	Loss: 1.0172
Training Epoch: 26 [34560/50048]	Loss: 1.2174
Training Epoch: 26 [34688/50048]	Loss: 0.9657
Training Epoch: 26 [34816/50048]	Loss: 0.9760
Training Epoch: 26 [34944/50048]	Loss: 1.0910
Training Epoch: 26 [35072/50048]	Loss: 1.4237
Training Epoch: 26 [35200/50048]	Loss: 1.1813
Training Epoch: 26 [35328/50048]	Loss: 0.9237
Training Epoch: 26 [35456/50048]	Loss: 1.0333
Training Epoch: 26 [35584/50048]	Loss: 1.1849
Training Epoch: 26 [35712/50048]	Loss: 1.2493
Training Epoch: 26 [35840/50048]	Loss: 1.2800
Training Epoch: 26 [35968/50048]	Loss: 1.0563
Training Epoch: 26 [36096/50048]	Loss: 1.2617
Training Epoch: 26 [36224/50048]	Loss: 1.0795
Training Epoch: 26 [36352/50048]	Loss: 1.3175
Training Epoch: 26 [36480/50048]	Loss: 1.3710
Training Epoch: 26 [36608/50048]	Loss: 1.0158
Training Epoch: 26 [36736/50048]	Loss: 0.8736
Training Epoch: 26 [36864/50048]	Loss: 1.1091
Training Epoch: 26 [36992/50048]	Loss: 1.1713
Training Epoch: 26 [37120/50048]	Loss: 1.2755
Training Epoch: 26 [37248/50048]	Loss: 0.8794
Training Epoch: 26 [37376/50048]	Loss: 1.2022
Training Epoch: 26 [37504/50048]	Loss: 1.0079
Training Epoch: 26 [37632/50048]	Loss: 1.0491
Training Epoch: 26 [37760/50048]	Loss: 1.0711
Training Epoch: 26 [37888/50048]	Loss: 1.1158
Training Epoch: 26 [38016/50048]	Loss: 1.1534
Training Epoch: 26 [38144/50048]	Loss: 1.0100
Training Epoch: 26 [38272/50048]	Loss: 1.0218
Training Epoch: 26 [38400/50048]	Loss: 1.0791
Training Epoch: 26 [38528/50048]	Loss: 1.1345
Training Epoch: 26 [38656/50048]	Loss: 1.3531
Training Epoch: 26 [38784/50048]	Loss: 1.1345
Training Epoch: 26 [38912/50048]	Loss: 1.2854
Training Epoch: 26 [39040/50048]	Loss: 1.5069
Training Epoch: 26 [39168/50048]	Loss: 1.2972
Training Epoch: 26 [39296/50048]	Loss: 1.0093
Training Epoch: 26 [39424/50048]	Loss: 1.1812
Training Epoch: 26 [39552/50048]	Loss: 1.0974
Training Epoch: 26 [39680/50048]	Loss: 1.0507
Training Epoch: 26 [39808/50048]	Loss: 1.1925
Training Epoch: 26 [39936/50048]	Loss: 1.1871
Training Epoch: 26 [40064/50048]	Loss: 1.2497
Training Epoch: 26 [40192/50048]	Loss: 1.2596
Training Epoch: 26 [40320/50048]	Loss: 1.0153
Training Epoch: 26 [40448/50048]	Loss: 1.1243
Training Epoch: 26 [40576/50048]	Loss: 1.4811
Training Epoch: 26 [40704/50048]	Loss: 0.9461
Training Epoch: 26 [40832/50048]	Loss: 0.9691
Training Epoch: 26 [40960/50048]	Loss: 1.0638
Training Epoch: 26 [41088/50048]	Loss: 1.3771
Training Epoch: 26 [41216/50048]	Loss: 1.1126
Training Epoch: 26 [41344/50048]	Loss: 1.1345
Training Epoch: 26 [41472/50048]	Loss: 1.1185
Training Epoch: 26 [41600/50048]	Loss: 1.2132
Training Epoch: 26 [41728/50048]	Loss: 1.0125
Training Epoch: 26 [41856/50048]	Loss: 1.0996
Training Epoch: 26 [41984/50048]	Loss: 1.1165
Training Epoch: 26 [42112/50048]	Loss: 1.0779
Training Epoch: 26 [42240/50048]	Loss: 1.2007
Training Epoch: 26 [42368/50048]	Loss: 1.2334
Training Epoch: 26 [42496/50048]	Loss: 1.1733
Training Epoch: 26 [42624/50048]	Loss: 1.0549
Training Epoch: 26 [42752/50048]	Loss: 1.2649
Training Epoch: 26 [42880/50048]	Loss: 0.9179
Training Epoch: 26 [43008/50048]	Loss: 0.8911
Training Epoch: 26 [43136/50048]	Loss: 1.1395
Training Epoch: 26 [43264/50048]	Loss: 1.1005
Training Epoch: 26 [43392/50048]	Loss: 1.3446
Training Epoch: 26 [43520/50048]	Loss: 1.0885
Training Epoch: 26 [43648/50048]	Loss: 1.1294
Training Epoch: 26 [43776/50048]	Loss: 1.2607
Training Epoch: 26 [43904/50048]	Loss: 1.2366
Training Epoch: 26 [44032/50048]	Loss: 0.9124
Training Epoch: 26 [44160/50048]	Loss: 0.8305
Training Epoch: 26 [44288/50048]	Loss: 1.3098
Training Epoch: 26 [44416/50048]	Loss: 1.2641
Training Epoch: 26 [44544/50048]	Loss: 1.1008
Training Epoch: 26 [44672/50048]	Loss: 1.0460
Training Epoch: 26 [44800/50048]	Loss: 0.9346
Training Epoch: 26 [44928/50048]	Loss: 1.0628
Training Epoch: 26 [45056/50048]	Loss: 1.1415
Training Epoch: 26 [45184/50048]	Loss: 1.1292
Training Epoch: 26 [45312/50048]	Loss: 1.0046
Training Epoch: 26 [45440/50048]	Loss: 1.1576
Training Epoch: 26 [45568/50048]	Loss: 1.1803
Training Epoch: 26 [45696/50048]	Loss: 1.2089
Training Epoch: 26 [45824/50048]	Loss: 0.8061
Training Epoch: 26 [45952/50048]	Loss: 1.3205
Training Epoch: 26 [46080/50048]	Loss: 0.9973
Training Epoch: 26 [46208/50048]	Loss: 1.3168
Training Epoch: 26 [46336/50048]	Loss: 1.2356
Training Epoch: 26 [46464/50048]	Loss: 1.2005
Training Epoch: 26 [46592/50048]	Loss: 1.2698
Training Epoch: 26 [46720/50048]	Loss: 1.0369
Training Epoch: 26 [46848/50048]	Loss: 0.8605
Training Epoch: 26 [46976/50048]	Loss: 1.1365
Training Epoch: 26 [47104/50048]	Loss: 1.0030
Training Epoch: 26 [47232/50048]	Loss: 1.1685
Training Epoch: 26 [47360/50048]	Loss: 1.1696
Training Epoch: 26 [47488/50048]	Loss: 0.9793
Training Epoch: 26 [47616/50048]	Loss: 1.0604
Training Epoch: 26 [47744/50048]	Loss: 1.2000
Training Epoch: 26 [47872/50048]	Loss: 1.0973
Training Epoch: 26 [48000/50048]	Loss: 1.1557
Training Epoch: 26 [48128/50048]	Loss: 1.2078
Training Epoch: 26 [48256/50048]	Loss: 1.3756
Training Epoch: 26 [48384/50048]	Loss: 1.2378
Training Epoch: 26 [48512/50048]	Loss: 1.0952
Training Epoch: 26 [48640/50048]	Loss: 1.2697
Training Epoch: 26 [48768/50048]	Loss: 1.0910
Training Epoch: 26 [48896/50048]	Loss: 1.2010
Training Epoch: 26 [49024/50048]	Loss: 1.0592
Training Epoch: 26 [49152/50048]	Loss: 1.1377
Training Epoch: 26 [49280/50048]	Loss: 1.0057
Training Epoch: 26 [49408/50048]	Loss: 1.2030
Training Epoch: 26 [49536/50048]	Loss: 1.1254
Training Epoch: 26 [49664/50048]	Loss: 1.3303
Training Epoch: 26 [49792/50048]	Loss: 1.3794
Training Epoch: 26 [49920/50048]	Loss: 1.1962
Training Epoch: 26 [50048/50048]	Loss: 1.1679
Validation Epoch: 26, Average loss: 0.0118, Accuracy: 0.5894
Training Epoch: 27 [128/50048]	Loss: 1.3080
Training Epoch: 27 [256/50048]	Loss: 1.0145
Training Epoch: 27 [384/50048]	Loss: 0.9753
Training Epoch: 27 [512/50048]	Loss: 1.2918
Training Epoch: 27 [640/50048]	Loss: 0.9359
Training Epoch: 27 [768/50048]	Loss: 1.1397
Training Epoch: 27 [896/50048]	Loss: 1.1407
Training Epoch: 27 [1024/50048]	Loss: 1.1961
Training Epoch: 27 [1152/50048]	Loss: 1.1747
Training Epoch: 27 [1280/50048]	Loss: 1.0779
Training Epoch: 27 [1408/50048]	Loss: 1.2139
Training Epoch: 27 [1536/50048]	Loss: 0.9911
Training Epoch: 27 [1664/50048]	Loss: 1.1795
Training Epoch: 27 [1792/50048]	Loss: 1.2174
Training Epoch: 27 [1920/50048]	Loss: 0.7250
Training Epoch: 27 [2048/50048]	Loss: 1.1595
Training Epoch: 27 [2176/50048]	Loss: 1.4747
Training Epoch: 27 [2304/50048]	Loss: 1.0578
Training Epoch: 27 [2432/50048]	Loss: 1.3259
Training Epoch: 27 [2560/50048]	Loss: 1.0311
Training Epoch: 27 [2688/50048]	Loss: 1.0334
Training Epoch: 27 [2816/50048]	Loss: 1.1109
Training Epoch: 27 [2944/50048]	Loss: 1.3284
Training Epoch: 27 [3072/50048]	Loss: 1.0659
Training Epoch: 27 [3200/50048]	Loss: 1.1291
Training Epoch: 27 [3328/50048]	Loss: 1.1656
Training Epoch: 27 [3456/50048]	Loss: 1.0259
Training Epoch: 27 [3584/50048]	Loss: 0.9926
Training Epoch: 27 [3712/50048]	Loss: 1.1422
Training Epoch: 27 [3840/50048]	Loss: 1.2085
Training Epoch: 27 [3968/50048]	Loss: 1.0323
Training Epoch: 27 [4096/50048]	Loss: 1.0081
Training Epoch: 27 [4224/50048]	Loss: 0.9704
Training Epoch: 27 [4352/50048]	Loss: 1.0859
Training Epoch: 27 [4480/50048]	Loss: 1.0978
Training Epoch: 27 [4608/50048]	Loss: 1.2656
Training Epoch: 27 [4736/50048]	Loss: 1.0961
Training Epoch: 27 [4864/50048]	Loss: 0.9740
Training Epoch: 27 [4992/50048]	Loss: 1.3331
Training Epoch: 27 [5120/50048]	Loss: 1.0141
Training Epoch: 27 [5248/50048]	Loss: 0.9809
Training Epoch: 27 [5376/50048]	Loss: 0.9687
Training Epoch: 27 [5504/50048]	Loss: 1.1055
Training Epoch: 27 [5632/50048]	Loss: 0.9821
Training Epoch: 27 [5760/50048]	Loss: 0.9607
Training Epoch: 27 [5888/50048]	Loss: 0.9429
Training Epoch: 27 [6016/50048]	Loss: 1.1347
Training Epoch: 27 [6144/50048]	Loss: 1.1095
Training Epoch: 27 [6272/50048]	Loss: 1.1003
Training Epoch: 27 [6400/50048]	Loss: 1.1777
Training Epoch: 27 [6528/50048]	Loss: 1.2102
Training Epoch: 27 [6656/50048]	Loss: 1.0411
Training Epoch: 27 [6784/50048]	Loss: 1.0501
Training Epoch: 27 [6912/50048]	Loss: 1.0862
Training Epoch: 27 [7040/50048]	Loss: 1.1567
Training Epoch: 27 [7168/50048]	Loss: 1.1020
Training Epoch: 27 [7296/50048]	Loss: 0.8704
Training Epoch: 27 [7424/50048]	Loss: 1.0178
Training Epoch: 27 [7552/50048]	Loss: 1.1919
Training Epoch: 27 [7680/50048]	Loss: 1.1396
Training Epoch: 27 [7808/50048]	Loss: 1.0839
Training Epoch: 27 [7936/50048]	Loss: 1.1703
Training Epoch: 27 [8064/50048]	Loss: 1.0175
Training Epoch: 27 [8192/50048]	Loss: 1.0543
Training Epoch: 27 [8320/50048]	Loss: 0.9441
Training Epoch: 27 [8448/50048]	Loss: 0.9472
Training Epoch: 27 [8576/50048]	Loss: 1.4830
Training Epoch: 27 [8704/50048]	Loss: 1.2227
Training Epoch: 27 [8832/50048]	Loss: 0.9633
Training Epoch: 27 [8960/50048]	Loss: 0.8892
Training Epoch: 27 [9088/50048]	Loss: 1.2431
Training Epoch: 27 [9216/50048]	Loss: 1.0117
Training Epoch: 27 [9344/50048]	Loss: 1.1368
Training Epoch: 27 [9472/50048]	Loss: 1.0377
Training Epoch: 27 [9600/50048]	Loss: 1.0823
Training Epoch: 27 [9728/50048]	Loss: 1.1056
Training Epoch: 27 [9856/50048]	Loss: 1.0788
Training Epoch: 27 [9984/50048]	Loss: 0.9735
Training Epoch: 27 [10112/50048]	Loss: 0.8645
Training Epoch: 27 [10240/50048]	Loss: 1.0535
Training Epoch: 27 [10368/50048]	Loss: 1.1915
Training Epoch: 27 [10496/50048]	Loss: 0.9865
Training Epoch: 27 [10624/50048]	Loss: 0.9463
Training Epoch: 27 [10752/50048]	Loss: 1.0509
Training Epoch: 27 [10880/50048]	Loss: 1.3910
Training Epoch: 27 [11008/50048]	Loss: 0.9872
Training Epoch: 27 [11136/50048]	Loss: 0.9765
Training Epoch: 27 [11264/50048]	Loss: 0.7353
Training Epoch: 27 [11392/50048]	Loss: 1.0346
Training Epoch: 27 [11520/50048]	Loss: 1.1286
Training Epoch: 27 [11648/50048]	Loss: 1.0316
Training Epoch: 27 [11776/50048]	Loss: 0.9341
Training Epoch: 27 [11904/50048]	Loss: 1.0149
Training Epoch: 27 [12032/50048]	Loss: 1.0687
Training Epoch: 27 [12160/50048]	Loss: 1.0372
Training Epoch: 27 [12288/50048]	Loss: 1.3448
Training Epoch: 27 [12416/50048]	Loss: 1.0104
Training Epoch: 27 [12544/50048]	Loss: 1.0505
Training Epoch: 27 [12672/50048]	Loss: 1.2684
Training Epoch: 27 [12800/50048]	Loss: 1.3148
Training Epoch: 27 [12928/50048]	Loss: 1.0507
Training Epoch: 27 [13056/50048]	Loss: 1.1403
Training Epoch: 27 [13184/50048]	Loss: 1.2686
Training Epoch: 27 [13312/50048]	Loss: 0.9385
Training Epoch: 27 [13440/50048]	Loss: 0.9326
Training Epoch: 27 [13568/50048]	Loss: 1.0227
Training Epoch: 27 [13696/50048]	Loss: 1.0892
Training Epoch: 27 [13824/50048]	Loss: 0.9781
Training Epoch: 27 [13952/50048]	Loss: 1.0507
Training Epoch: 27 [14080/50048]	Loss: 1.0401
Training Epoch: 27 [14208/50048]	Loss: 1.2141
Training Epoch: 27 [14336/50048]	Loss: 1.3573
Training Epoch: 27 [14464/50048]	Loss: 0.9652
Training Epoch: 27 [14592/50048]	Loss: 1.0540
Training Epoch: 27 [14720/50048]	Loss: 0.9583
Training Epoch: 27 [14848/50048]	Loss: 0.8676
Training Epoch: 27 [14976/50048]	Loss: 1.1277
Training Epoch: 27 [15104/50048]	Loss: 1.0764
Training Epoch: 27 [15232/50048]	Loss: 1.2344
Training Epoch: 27 [15360/50048]	Loss: 1.0764
Training Epoch: 27 [15488/50048]	Loss: 1.1709
Training Epoch: 27 [15616/50048]	Loss: 0.9751
Training Epoch: 27 [15744/50048]	Loss: 1.0417
Training Epoch: 27 [15872/50048]	Loss: 1.1171
Training Epoch: 27 [16000/50048]	Loss: 1.2079
Training Epoch: 27 [16128/50048]	Loss: 1.0822
Training Epoch: 27 [16256/50048]	Loss: 0.9948
Training Epoch: 27 [16384/50048]	Loss: 1.2450
Training Epoch: 27 [16512/50048]	Loss: 1.0669
Training Epoch: 27 [16640/50048]	Loss: 1.2995
Training Epoch: 27 [16768/50048]	Loss: 1.0607
Training Epoch: 27 [16896/50048]	Loss: 1.1351
Training Epoch: 27 [17024/50048]	Loss: 1.0066
Training Epoch: 27 [17152/50048]	Loss: 1.0652
Training Epoch: 27 [17280/50048]	Loss: 0.9491
Training Epoch: 27 [17408/50048]	Loss: 0.8858
Training Epoch: 27 [17536/50048]	Loss: 1.1444
Training Epoch: 27 [17664/50048]	Loss: 1.0615
Training Epoch: 27 [17792/50048]	Loss: 0.9069
Training Epoch: 27 [17920/50048]	Loss: 0.9852
Training Epoch: 27 [18048/50048]	Loss: 0.9463
Training Epoch: 27 [18176/50048]	Loss: 1.0874
Training Epoch: 27 [18304/50048]	Loss: 1.1508
Training Epoch: 27 [18432/50048]	Loss: 1.1980
Training Epoch: 27 [18560/50048]	Loss: 1.0484
Training Epoch: 27 [18688/50048]	Loss: 1.2847
Training Epoch: 27 [18816/50048]	Loss: 0.9581
Training Epoch: 27 [18944/50048]	Loss: 1.0134
Training Epoch: 27 [19072/50048]	Loss: 1.0266
Training Epoch: 27 [19200/50048]	Loss: 0.9782
Training Epoch: 27 [19328/50048]	Loss: 1.0780
Training Epoch: 27 [19456/50048]	Loss: 0.9699
Training Epoch: 27 [19584/50048]	Loss: 0.9892
Training Epoch: 27 [19712/50048]	Loss: 1.2896
Training Epoch: 27 [19840/50048]	Loss: 0.8720
Training Epoch: 27 [19968/50048]	Loss: 1.2168
Training Epoch: 27 [20096/50048]	Loss: 0.9195
Training Epoch: 27 [20224/50048]	Loss: 0.8935
Training Epoch: 27 [20352/50048]	Loss: 1.2926
Training Epoch: 27 [20480/50048]	Loss: 1.2079
Training Epoch: 27 [20608/50048]	Loss: 1.0841
Training Epoch: 27 [20736/50048]	Loss: 1.1369
Training Epoch: 27 [20864/50048]	Loss: 1.3479
Training Epoch: 27 [20992/50048]	Loss: 1.1155
Training Epoch: 27 [21120/50048]	Loss: 0.9603
Training Epoch: 27 [21248/50048]	Loss: 0.9293
Training Epoch: 27 [21376/50048]	Loss: 1.1750
Training Epoch: 27 [21504/50048]	Loss: 0.9012
Training Epoch: 27 [21632/50048]	Loss: 1.2248
Training Epoch: 27 [21760/50048]	Loss: 1.1750
Training Epoch: 27 [21888/50048]	Loss: 0.9096
Training Epoch: 27 [22016/50048]	Loss: 0.9568
Training Epoch: 27 [22144/50048]	Loss: 1.3477
Training Epoch: 27 [22272/50048]	Loss: 1.0951
Training Epoch: 27 [22400/50048]	Loss: 1.1042
Training Epoch: 27 [22528/50048]	Loss: 1.0793
Training Epoch: 27 [22656/50048]	Loss: 1.1732
Training Epoch: 27 [22784/50048]	Loss: 1.0880
Training Epoch: 27 [22912/50048]	Loss: 1.0876
Training Epoch: 27 [23040/50048]	Loss: 1.0986
Training Epoch: 27 [23168/50048]	Loss: 1.0541
Training Epoch: 27 [23296/50048]	Loss: 1.1823
Training Epoch: 27 [23424/50048]	Loss: 1.4081
Training Epoch: 27 [23552/50048]	Loss: 0.8726
Training Epoch: 27 [23680/50048]	Loss: 1.0407
Training Epoch: 27 [23808/50048]	Loss: 1.1441
Training Epoch: 27 [23936/50048]	Loss: 1.1810
Training Epoch: 27 [24064/50048]	Loss: 1.1020
Training Epoch: 27 [24192/50048]	Loss: 0.9829
Training Epoch: 27 [24320/50048]	Loss: 1.2040
Training Epoch: 27 [24448/50048]	Loss: 1.1411
Training Epoch: 27 [24576/50048]	Loss: 1.1648
Training Epoch: 27 [24704/50048]	Loss: 1.1900
Training Epoch: 27 [24832/50048]	Loss: 1.1528
Training Epoch: 27 [24960/50048]	Loss: 1.1788
Training Epoch: 27 [25088/50048]	Loss: 0.9200
Training Epoch: 27 [25216/50048]	Loss: 1.2800
Training Epoch: 27 [25344/50048]	Loss: 0.9558
Training Epoch: 27 [25472/50048]	Loss: 1.1034
Training Epoch: 27 [25600/50048]	Loss: 1.0590
Training Epoch: 27 [25728/50048]	Loss: 1.0156
Training Epoch: 27 [25856/50048]	Loss: 0.8574
Training Epoch: 27 [25984/50048]	Loss: 1.3624
Training Epoch: 27 [26112/50048]	Loss: 1.1137
Training Epoch: 27 [26240/50048]	Loss: 1.2410
Training Epoch: 27 [26368/50048]	Loss: 1.2829
Training Epoch: 27 [26496/50048]	Loss: 0.8917
Training Epoch: 27 [26624/50048]	Loss: 0.9375
Training Epoch: 27 [26752/50048]	Loss: 1.2505
Training Epoch: 27 [26880/50048]	Loss: 1.1999
Training Epoch: 27 [27008/50048]	Loss: 1.0333
Training Epoch: 27 [27136/50048]	Loss: 1.2012
Training Epoch: 27 [27264/50048]	Loss: 1.0239
Training Epoch: 27 [27392/50048]	Loss: 1.2653
Training Epoch: 27 [27520/50048]	Loss: 0.9372
Training Epoch: 27 [27648/50048]	Loss: 1.0916
Training Epoch: 27 [27776/50048]	Loss: 1.1030
Training Epoch: 27 [27904/50048]	Loss: 1.4093
Training Epoch: 27 [28032/50048]	Loss: 0.9490
Training Epoch: 27 [28160/50048]	Loss: 1.1841
Training Epoch: 27 [28288/50048]	Loss: 1.1065
Training Epoch: 27 [28416/50048]	Loss: 1.1793
Training Epoch: 27 [28544/50048]	Loss: 1.0199
Training Epoch: 27 [28672/50048]	Loss: 1.2294
Training Epoch: 27 [28800/50048]	Loss: 1.1378
Training Epoch: 27 [28928/50048]	Loss: 0.9317
Training Epoch: 27 [29056/50048]	Loss: 1.0750
Training Epoch: 27 [29184/50048]	Loss: 0.8576
Training Epoch: 27 [29312/50048]	Loss: 1.0000
Training Epoch: 27 [29440/50048]	Loss: 1.3516
Training Epoch: 27 [29568/50048]	Loss: 0.9660
Training Epoch: 27 [29696/50048]	Loss: 1.0565
Training Epoch: 27 [29824/50048]	Loss: 1.1662
Training Epoch: 27 [29952/50048]	Loss: 1.0541
Training Epoch: 27 [30080/50048]	Loss: 1.1670
Training Epoch: 27 [30208/50048]	Loss: 1.0959
Training Epoch: 27 [30336/50048]	Loss: 1.0539
Training Epoch: 27 [30464/50048]	Loss: 1.0323
Training Epoch: 27 [30592/50048]	Loss: 0.9847
Training Epoch: 27 [30720/50048]	Loss: 0.9012
Training Epoch: 27 [30848/50048]	Loss: 1.1883
Training Epoch: 27 [30976/50048]	Loss: 1.1034
Training Epoch: 27 [31104/50048]	Loss: 1.0562
Training Epoch: 27 [31232/50048]	Loss: 1.0487
Training Epoch: 27 [31360/50048]	Loss: 1.1312
Training Epoch: 27 [31488/50048]	Loss: 1.4066
Training Epoch: 27 [31616/50048]	Loss: 1.0726
Training Epoch: 27 [31744/50048]	Loss: 1.0066
Training Epoch: 27 [31872/50048]	Loss: 1.1188
Training Epoch: 27 [32000/50048]	Loss: 1.1744
Training Epoch: 27 [32128/50048]	Loss: 1.0571
Training Epoch: 27 [32256/50048]	Loss: 0.9280
Training Epoch: 27 [32384/50048]	Loss: 0.9786
Training Epoch: 27 [32512/50048]	Loss: 1.2302
Training Epoch: 27 [32640/50048]	Loss: 1.0942
Training Epoch: 27 [32768/50048]	Loss: 1.0525
Training Epoch: 27 [32896/50048]	Loss: 1.2403
Training Epoch: 27 [33024/50048]	Loss: 0.9474
Training Epoch: 27 [33152/50048]	Loss: 1.2443
Training Epoch: 27 [33280/50048]	Loss: 1.0300
Training Epoch: 27 [33408/50048]	Loss: 0.9060
Training Epoch: 27 [33536/50048]	Loss: 0.9276
Training Epoch: 27 [33664/50048]	Loss: 1.1566
Training Epoch: 27 [33792/50048]	Loss: 1.0554
Training Epoch: 27 [33920/50048]	Loss: 1.1141
Training Epoch: 27 [34048/50048]	Loss: 1.1847
Training Epoch: 27 [34176/50048]	Loss: 1.0162
Training Epoch: 27 [34304/50048]	Loss: 1.0320
Training Epoch: 27 [34432/50048]	Loss: 1.2700
Training Epoch: 27 [34560/50048]	Loss: 0.9752
Training Epoch: 27 [34688/50048]	Loss: 0.9413
Training Epoch: 27 [34816/50048]	Loss: 1.0180
Training Epoch: 27 [34944/50048]	Loss: 1.0939
Training Epoch: 27 [35072/50048]	Loss: 0.9130
Training Epoch: 27 [35200/50048]	Loss: 1.0526
Training Epoch: 27 [35328/50048]	Loss: 1.3041
Training Epoch: 27 [35456/50048]	Loss: 1.0260
Training Epoch: 27 [35584/50048]	Loss: 1.1019
Training Epoch: 27 [35712/50048]	Loss: 1.1369
Training Epoch: 27 [35840/50048]	Loss: 1.0844
Training Epoch: 27 [35968/50048]	Loss: 1.2294
Training Epoch: 27 [36096/50048]	Loss: 1.1151
Training Epoch: 27 [36224/50048]	Loss: 1.3437
Training Epoch: 27 [36352/50048]	Loss: 1.1531
Training Epoch: 27 [36480/50048]	Loss: 1.1216
Training Epoch: 27 [36608/50048]	Loss: 1.3625
Training Epoch: 27 [36736/50048]	Loss: 1.0119
Training Epoch: 27 [36864/50048]	Loss: 0.9420
Training Epoch: 27 [36992/50048]	Loss: 1.2530
Training Epoch: 27 [37120/50048]	Loss: 1.4342
Training Epoch: 27 [37248/50048]	Loss: 1.2801
Training Epoch: 27 [37376/50048]	Loss: 0.8186
Training Epoch: 27 [37504/50048]	Loss: 1.3868
Training Epoch: 27 [37632/50048]	Loss: 0.9523
Training Epoch: 27 [37760/50048]	Loss: 1.1983
Training Epoch: 27 [37888/50048]	Loss: 1.2546
Training Epoch: 27 [38016/50048]	Loss: 0.8550
Training Epoch: 27 [38144/50048]	Loss: 0.9604
Training Epoch: 27 [38272/50048]	Loss: 1.0355
Training Epoch: 27 [38400/50048]	Loss: 1.0910
Training Epoch: 27 [38528/50048]	Loss: 1.3320
Training Epoch: 27 [38656/50048]	Loss: 0.9899
Training Epoch: 27 [38784/50048]	Loss: 1.3539
Training Epoch: 27 [38912/50048]	Loss: 1.0547
Training Epoch: 27 [39040/50048]	Loss: 1.2550
Training Epoch: 27 [39168/50048]	Loss: 1.2458
Training Epoch: 27 [39296/50048]	Loss: 1.2561
Training Epoch: 27 [39424/50048]	Loss: 1.1473
Training Epoch: 27 [39552/50048]	Loss: 1.1511
Training Epoch: 27 [39680/50048]	Loss: 1.1136
Training Epoch: 27 [39808/50048]	Loss: 0.9671
Training Epoch: 27 [39936/50048]	Loss: 1.2965
Training Epoch: 27 [40064/50048]	Loss: 1.2519
Training Epoch: 27 [40192/50048]	Loss: 1.0295
Training Epoch: 27 [40320/50048]	Loss: 1.0645
Training Epoch: 27 [40448/50048]	Loss: 1.1989
Training Epoch: 27 [40576/50048]	Loss: 1.1744
Training Epoch: 27 [40704/50048]	Loss: 0.8865
Training Epoch: 27 [40832/50048]	Loss: 1.1852
Training Epoch: 27 [40960/50048]	Loss: 1.1489
Training Epoch: 27 [41088/50048]	Loss: 1.1668
Training Epoch: 27 [41216/50048]	Loss: 1.0098
Training Epoch: 27 [41344/50048]	Loss: 1.2112
Training Epoch: 27 [41472/50048]	Loss: 0.9894
Training Epoch: 27 [41600/50048]	Loss: 1.2895
Training Epoch: 27 [41728/50048]	Loss: 1.2427
Training Epoch: 27 [41856/50048]	Loss: 1.3339
Training Epoch: 27 [41984/50048]	Loss: 1.1255
Training Epoch: 27 [42112/50048]	Loss: 1.2767
Training Epoch: 27 [42240/50048]	Loss: 0.9723
Training Epoch: 27 [42368/50048]	Loss: 1.1803
Training Epoch: 27 [42496/50048]	Loss: 1.0277
Training Epoch: 27 [42624/50048]	Loss: 0.8638
Training Epoch: 27 [42752/50048]	Loss: 1.1211
Training Epoch: 27 [42880/50048]	Loss: 1.2358
Training Epoch: 27 [43008/50048]	Loss: 1.3372
Training Epoch: 27 [43136/50048]	Loss: 1.1759
Training Epoch: 27 [43264/50048]	Loss: 1.2741
Training Epoch: 27 [43392/50048]	Loss: 1.0475
Training Epoch: 27 [43520/50048]	Loss: 1.1115
Training Epoch: 27 [43648/50048]	Loss: 1.1455
Training Epoch: 27 [43776/50048]	Loss: 1.0254
Training Epoch: 27 [43904/50048]	Loss: 1.0034
Training Epoch: 27 [44032/50048]	Loss: 1.0424
Training Epoch: 27 [44160/50048]	Loss: 1.1785
Training Epoch: 27 [44288/50048]	Loss: 0.9763
Training Epoch: 27 [44416/50048]	Loss: 1.2421
Training Epoch: 27 [44544/50048]	Loss: 0.9604
Training Epoch: 27 [44672/50048]	Loss: 1.1807
Training Epoch: 27 [44800/50048]	Loss: 1.2026
Training Epoch: 27 [44928/50048]	Loss: 1.2628
Training Epoch: 27 [45056/50048]	Loss: 1.3478
Training Epoch: 27 [45184/50048]	Loss: 1.2474
Training Epoch: 27 [45312/50048]	Loss: 1.0622
Training Epoch: 27 [45440/50048]	Loss: 1.3074
Training Epoch: 27 [45568/50048]	Loss: 0.9880
Training Epoch: 27 [45696/50048]	Loss: 1.2389
Training Epoch: 27 [45824/50048]	Loss: 1.1463
Training Epoch: 27 [45952/50048]	Loss: 1.2708
Training Epoch: 27 [46080/50048]	Loss: 0.9646
Training Epoch: 27 [46208/50048]	Loss: 1.1980
Training Epoch: 27 [46336/50048]	Loss: 1.1080
Training Epoch: 27 [46464/50048]	Loss: 1.1639
Training Epoch: 27 [46592/50048]	Loss: 1.2327
Training Epoch: 27 [46720/50048]	Loss: 1.1827
Training Epoch: 27 [46848/50048]	Loss: 0.9078
Training Epoch: 27 [46976/50048]	Loss: 1.0670
Training Epoch: 27 [47104/50048]	Loss: 1.1924
Training Epoch: 27 [47232/50048]	Loss: 1.3515
Training Epoch: 27 [47360/50048]	Loss: 1.1128
Training Epoch: 27 [47488/50048]	Loss: 1.0600
Training Epoch: 27 [47616/50048]	Loss: 1.1020
Training Epoch: 27 [47744/50048]	Loss: 1.1678
Training Epoch: 27 [47872/50048]	Loss: 1.2833
Training Epoch: 27 [48000/50048]	Loss: 1.0655
Training Epoch: 27 [48128/50048]	Loss: 1.2702
Training Epoch: 27 [48256/50048]	Loss: 1.2764
Training Epoch: 27 [48384/50048]	Loss: 1.0376
Training Epoch: 27 [48512/50048]	Loss: 1.1752
Training Epoch: 27 [48640/50048]	Loss: 0.9337
Training Epoch: 27 [48768/50048]	Loss: 1.2187
Training Epoch: 27 [48896/50048]	Loss: 1.0268
Training Epoch: 27 [49024/50048]	Loss: 1.1044
Training Epoch: 27 [49152/50048]	Loss: 1.2694
Training Epoch: 27 [49280/50048]	Loss: 1.1237
Training Epoch: 27 [49408/50048]	Loss: 1.0025
Training Epoch: 27 [49536/50048]	Loss: 1.2598
Training Epoch: 27 [49664/50048]	Loss: 1.0474
Training Epoch: 27 [49792/50048]	Loss: 1.0233
Training Epoch: 27 [49920/50048]	Loss: 1.1835
Training Epoch: 27 [50048/50048]	Loss: 0.8717
Validation Epoch: 27, Average loss: 0.0119, Accuracy: 0.5892
Training Epoch: 28 [128/50048]	Loss: 1.0754
Training Epoch: 28 [256/50048]	Loss: 0.9751
Training Epoch: 28 [384/50048]	Loss: 0.9295
Training Epoch: 28 [512/50048]	Loss: 0.9788
Training Epoch: 28 [640/50048]	Loss: 1.1896
Training Epoch: 28 [768/50048]	Loss: 0.9280
Training Epoch: 28 [896/50048]	Loss: 1.1054
Training Epoch: 28 [1024/50048]	Loss: 1.1276
Training Epoch: 28 [1152/50048]	Loss: 1.0554
Training Epoch: 28 [1280/50048]	Loss: 1.0928
Training Epoch: 28 [1408/50048]	Loss: 1.3157
Training Epoch: 28 [1536/50048]	Loss: 0.9329
Training Epoch: 28 [1664/50048]	Loss: 1.0279
Training Epoch: 28 [1792/50048]	Loss: 1.1007
Training Epoch: 28 [1920/50048]	Loss: 1.0548
Training Epoch: 28 [2048/50048]	Loss: 1.0893
Training Epoch: 28 [2176/50048]	Loss: 1.1092
Training Epoch: 28 [2304/50048]	Loss: 1.2060
Training Epoch: 28 [2432/50048]	Loss: 0.9939
Training Epoch: 28 [2560/50048]	Loss: 1.0165
Training Epoch: 28 [2688/50048]	Loss: 0.9989
Training Epoch: 28 [2816/50048]	Loss: 1.1599
Training Epoch: 28 [2944/50048]	Loss: 1.2902
Training Epoch: 28 [3072/50048]	Loss: 1.0234
Training Epoch: 28 [3200/50048]	Loss: 1.1375
Training Epoch: 28 [3328/50048]	Loss: 1.0557
Training Epoch: 28 [3456/50048]	Loss: 1.1031
Training Epoch: 28 [3584/50048]	Loss: 1.1473
Training Epoch: 28 [3712/50048]	Loss: 1.1990
Training Epoch: 28 [3840/50048]	Loss: 0.9129
Training Epoch: 28 [3968/50048]	Loss: 0.8908
Training Epoch: 28 [4096/50048]	Loss: 0.9106
Training Epoch: 28 [4224/50048]	Loss: 1.0766
Training Epoch: 28 [4352/50048]	Loss: 1.0150
Training Epoch: 28 [4480/50048]	Loss: 1.0587
Training Epoch: 28 [4608/50048]	Loss: 0.9368
Training Epoch: 28 [4736/50048]	Loss: 1.2411
Training Epoch: 28 [4864/50048]	Loss: 1.2082
Training Epoch: 28 [4992/50048]	Loss: 1.0733
Training Epoch: 28 [5120/50048]	Loss: 0.8721
Training Epoch: 28 [5248/50048]	Loss: 0.9835
Training Epoch: 28 [5376/50048]	Loss: 1.0475
Training Epoch: 28 [5504/50048]	Loss: 1.1824
Training Epoch: 28 [5632/50048]	Loss: 1.2428
Training Epoch: 28 [5760/50048]	Loss: 1.2353
Training Epoch: 28 [5888/50048]	Loss: 1.1208
Training Epoch: 28 [6016/50048]	Loss: 1.2572
Training Epoch: 28 [6144/50048]	Loss: 1.2960
Training Epoch: 28 [6272/50048]	Loss: 1.0699
Training Epoch: 28 [6400/50048]	Loss: 1.0097
Training Epoch: 28 [6528/50048]	Loss: 1.1437
Training Epoch: 28 [6656/50048]	Loss: 1.1364
Training Epoch: 28 [6784/50048]	Loss: 1.1071
Training Epoch: 28 [6912/50048]	Loss: 1.2199
Training Epoch: 28 [7040/50048]	Loss: 0.8252
Training Epoch: 28 [7168/50048]	Loss: 0.9425
Training Epoch: 28 [7296/50048]	Loss: 1.2143
Training Epoch: 28 [7424/50048]	Loss: 0.9693
Training Epoch: 28 [7552/50048]	Loss: 0.9855
Training Epoch: 28 [7680/50048]	Loss: 1.0506
Training Epoch: 28 [7808/50048]	Loss: 1.0328
Training Epoch: 28 [7936/50048]	Loss: 0.9978
Training Epoch: 28 [8064/50048]	Loss: 1.1640
Training Epoch: 28 [8192/50048]	Loss: 0.8792
Training Epoch: 28 [8320/50048]	Loss: 1.1414
Training Epoch: 28 [8448/50048]	Loss: 0.9498
Training Epoch: 28 [8576/50048]	Loss: 1.1511
Training Epoch: 28 [8704/50048]	Loss: 0.9193
Training Epoch: 28 [8832/50048]	Loss: 1.2433
Training Epoch: 28 [8960/50048]	Loss: 1.0876
Training Epoch: 28 [9088/50048]	Loss: 1.1115
Training Epoch: 28 [9216/50048]	Loss: 0.9435
Training Epoch: 28 [9344/50048]	Loss: 0.9165
Training Epoch: 28 [9472/50048]	Loss: 0.9472
Training Epoch: 28 [9600/50048]	Loss: 1.2258
Training Epoch: 28 [9728/50048]	Loss: 1.0420
Training Epoch: 28 [9856/50048]	Loss: 0.9509
Training Epoch: 28 [9984/50048]	Loss: 1.2889
Training Epoch: 28 [10112/50048]	Loss: 1.0346
Training Epoch: 28 [10240/50048]	Loss: 1.0657
Training Epoch: 28 [10368/50048]	Loss: 1.1867
Training Epoch: 28 [10496/50048]	Loss: 0.9338
Training Epoch: 28 [10624/50048]	Loss: 1.1785
Training Epoch: 28 [10752/50048]	Loss: 0.8707
Training Epoch: 28 [10880/50048]	Loss: 0.9291
Training Epoch: 28 [11008/50048]	Loss: 1.0121
Training Epoch: 28 [11136/50048]	Loss: 1.0715
Training Epoch: 28 [11264/50048]	Loss: 1.0987
Training Epoch: 28 [11392/50048]	Loss: 1.0874
Training Epoch: 28 [11520/50048]	Loss: 1.3136
Training Epoch: 28 [11648/50048]	Loss: 0.9825
Training Epoch: 28 [11776/50048]	Loss: 1.0404
Training Epoch: 28 [11904/50048]	Loss: 1.2471
Training Epoch: 28 [12032/50048]	Loss: 0.9857
Training Epoch: 28 [12160/50048]	Loss: 1.2411
Training Epoch: 28 [12288/50048]	Loss: 0.9884
Training Epoch: 28 [12416/50048]	Loss: 1.0635
Training Epoch: 28 [12544/50048]	Loss: 1.2304
Training Epoch: 28 [12672/50048]	Loss: 0.9906
Training Epoch: 28 [12800/50048]	Loss: 1.1715
Training Epoch: 28 [12928/50048]	Loss: 1.0652
Training Epoch: 28 [13056/50048]	Loss: 1.0936
Training Epoch: 28 [13184/50048]	Loss: 1.0857
Training Epoch: 28 [13312/50048]	Loss: 1.2040
Training Epoch: 28 [13440/50048]	Loss: 0.9557
Training Epoch: 28 [13568/50048]	Loss: 1.0265
Training Epoch: 28 [13696/50048]	Loss: 1.1093
Training Epoch: 28 [13824/50048]	Loss: 0.8757
Training Epoch: 28 [13952/50048]	Loss: 1.0753
Training Epoch: 28 [14080/50048]	Loss: 1.1630
Training Epoch: 28 [14208/50048]	Loss: 1.1306
Training Epoch: 28 [14336/50048]	Loss: 1.0736
Training Epoch: 28 [14464/50048]	Loss: 1.0146
Training Epoch: 28 [14592/50048]	Loss: 1.0996
Training Epoch: 28 [14720/50048]	Loss: 1.0579
Training Epoch: 28 [14848/50048]	Loss: 0.7659
Training Epoch: 28 [14976/50048]	Loss: 0.9508
Training Epoch: 28 [15104/50048]	Loss: 1.2052
Training Epoch: 28 [15232/50048]	Loss: 1.2247
Training Epoch: 28 [15360/50048]	Loss: 0.9635
Training Epoch: 28 [15488/50048]	Loss: 0.9157
Training Epoch: 28 [15616/50048]	Loss: 0.7174
Training Epoch: 28 [15744/50048]	Loss: 1.0679
Training Epoch: 28 [15872/50048]	Loss: 0.9909
Training Epoch: 28 [16000/50048]	Loss: 1.0376
Training Epoch: 28 [16128/50048]	Loss: 1.2209
Training Epoch: 28 [16256/50048]	Loss: 1.0841
Training Epoch: 28 [16384/50048]	Loss: 1.1934
Training Epoch: 28 [16512/50048]	Loss: 0.9191
Training Epoch: 28 [16640/50048]	Loss: 1.3674
Training Epoch: 28 [16768/50048]	Loss: 1.0953
Training Epoch: 28 [16896/50048]	Loss: 1.0801
Training Epoch: 28 [17024/50048]	Loss: 1.0995
Training Epoch: 28 [17152/50048]	Loss: 1.0600
Training Epoch: 28 [17280/50048]	Loss: 0.9173
Training Epoch: 28 [17408/50048]	Loss: 0.9919
Training Epoch: 28 [17536/50048]	Loss: 1.0383
Training Epoch: 28 [17664/50048]	Loss: 0.8964
Training Epoch: 28 [17792/50048]	Loss: 1.2882
Training Epoch: 28 [17920/50048]	Loss: 0.9319
Training Epoch: 28 [18048/50048]	Loss: 0.9547
Training Epoch: 28 [18176/50048]	Loss: 1.0588
Training Epoch: 28 [18304/50048]	Loss: 1.4321
Training Epoch: 28 [18432/50048]	Loss: 1.1992
Training Epoch: 28 [18560/50048]	Loss: 1.0507
Training Epoch: 28 [18688/50048]	Loss: 0.9963
Training Epoch: 28 [18816/50048]	Loss: 0.9711
Training Epoch: 28 [18944/50048]	Loss: 0.9093
Training Epoch: 28 [19072/50048]	Loss: 1.0583
Training Epoch: 28 [19200/50048]	Loss: 1.2384
Training Epoch: 28 [19328/50048]	Loss: 1.2150
Training Epoch: 28 [19456/50048]	Loss: 1.1850
Training Epoch: 28 [19584/50048]	Loss: 0.9935
Training Epoch: 28 [19712/50048]	Loss: 1.0999
Training Epoch: 28 [19840/50048]	Loss: 0.8954
Training Epoch: 28 [19968/50048]	Loss: 1.2540
Training Epoch: 28 [20096/50048]	Loss: 1.1425
Training Epoch: 28 [20224/50048]	Loss: 0.8075
Training Epoch: 28 [20352/50048]	Loss: 1.2472
Training Epoch: 28 [20480/50048]	Loss: 1.2579
Training Epoch: 28 [20608/50048]	Loss: 1.1681
Training Epoch: 28 [20736/50048]	Loss: 1.2177
Training Epoch: 28 [20864/50048]	Loss: 0.9716
Training Epoch: 28 [20992/50048]	Loss: 1.0621
Training Epoch: 28 [21120/50048]	Loss: 1.0339
Training Epoch: 28 [21248/50048]	Loss: 0.9423
Training Epoch: 28 [21376/50048]	Loss: 1.0456
Training Epoch: 28 [21504/50048]	Loss: 1.0071
Training Epoch: 28 [21632/50048]	Loss: 1.0952
Training Epoch: 28 [21760/50048]	Loss: 1.3306
Training Epoch: 28 [21888/50048]	Loss: 0.9230
Training Epoch: 28 [22016/50048]	Loss: 1.1152
Training Epoch: 28 [22144/50048]	Loss: 0.9548
Training Epoch: 28 [22272/50048]	Loss: 1.2111
Training Epoch: 28 [22400/50048]	Loss: 1.1081
Training Epoch: 28 [22528/50048]	Loss: 1.1263
Training Epoch: 28 [22656/50048]	Loss: 0.9134
Training Epoch: 28 [22784/50048]	Loss: 0.8779
Training Epoch: 28 [22912/50048]	Loss: 1.0175
Training Epoch: 28 [23040/50048]	Loss: 1.0745
Training Epoch: 28 [23168/50048]	Loss: 0.9733
Training Epoch: 28 [23296/50048]	Loss: 0.7517
Training Epoch: 28 [23424/50048]	Loss: 0.9584
Training Epoch: 28 [23552/50048]	Loss: 0.9878
Training Epoch: 28 [23680/50048]	Loss: 1.0465
Training Epoch: 28 [23808/50048]	Loss: 1.2049
Training Epoch: 28 [23936/50048]	Loss: 1.2249
Training Epoch: 28 [24064/50048]	Loss: 1.2744
Training Epoch: 28 [24192/50048]	Loss: 1.0009
Training Epoch: 28 [24320/50048]	Loss: 1.1103
Training Epoch: 28 [24448/50048]	Loss: 0.9234
Training Epoch: 28 [24576/50048]	Loss: 1.0989
Training Epoch: 28 [24704/50048]	Loss: 1.2896
Training Epoch: 28 [24832/50048]	Loss: 1.0769
Training Epoch: 28 [24960/50048]	Loss: 1.0576
Training Epoch: 28 [25088/50048]	Loss: 1.1231
Training Epoch: 28 [25216/50048]	Loss: 1.2253
Training Epoch: 28 [25344/50048]	Loss: 1.2907
Training Epoch: 28 [25472/50048]	Loss: 0.9910
Training Epoch: 28 [25600/50048]	Loss: 1.2920
Training Epoch: 28 [25728/50048]	Loss: 1.0236
Training Epoch: 28 [25856/50048]	Loss: 0.9630
Training Epoch: 28 [25984/50048]	Loss: 0.9566
Training Epoch: 28 [26112/50048]	Loss: 0.9620
Training Epoch: 28 [26240/50048]	Loss: 1.1190
Training Epoch: 28 [26368/50048]	Loss: 1.0677
Training Epoch: 28 [26496/50048]	Loss: 1.1655
Training Epoch: 28 [26624/50048]	Loss: 1.0805
Training Epoch: 28 [26752/50048]	Loss: 0.9045
Training Epoch: 28 [26880/50048]	Loss: 1.0619
Training Epoch: 28 [27008/50048]	Loss: 1.1177
Training Epoch: 28 [27136/50048]	Loss: 1.0971
Training Epoch: 28 [27264/50048]	Loss: 1.0281
Training Epoch: 28 [27392/50048]	Loss: 1.0717
Training Epoch: 28 [27520/50048]	Loss: 1.0429
Training Epoch: 28 [27648/50048]	Loss: 1.2171
Training Epoch: 28 [27776/50048]	Loss: 1.2260
Training Epoch: 28 [27904/50048]	Loss: 0.9839
Training Epoch: 28 [28032/50048]	Loss: 1.0374
Training Epoch: 28 [28160/50048]	Loss: 1.0185
Training Epoch: 28 [28288/50048]	Loss: 0.8827
Training Epoch: 28 [28416/50048]	Loss: 1.0335
Training Epoch: 28 [28544/50048]	Loss: 1.0467
Training Epoch: 28 [28672/50048]	Loss: 1.1405
Training Epoch: 28 [28800/50048]	Loss: 0.9969
Training Epoch: 28 [28928/50048]	Loss: 1.0859
Training Epoch: 28 [29056/50048]	Loss: 1.0511
Training Epoch: 28 [29184/50048]	Loss: 1.0407
Training Epoch: 28 [29312/50048]	Loss: 1.0300
Training Epoch: 28 [29440/50048]	Loss: 1.1898
Training Epoch: 28 [29568/50048]	Loss: 1.0199
Training Epoch: 28 [29696/50048]	Loss: 1.0922
Training Epoch: 28 [29824/50048]	Loss: 0.9579
Training Epoch: 28 [29952/50048]	Loss: 1.1775
Training Epoch: 28 [30080/50048]	Loss: 1.3380
Training Epoch: 28 [30208/50048]	Loss: 1.0097
Training Epoch: 28 [30336/50048]	Loss: 1.0923
Training Epoch: 28 [30464/50048]	Loss: 1.0925
Training Epoch: 28 [30592/50048]	Loss: 1.0468
Training Epoch: 28 [30720/50048]	Loss: 1.1498
Training Epoch: 28 [30848/50048]	Loss: 1.1835
Training Epoch: 28 [30976/50048]	Loss: 1.2639
Training Epoch: 28 [31104/50048]	Loss: 1.0560
Training Epoch: 28 [31232/50048]	Loss: 1.0456
Training Epoch: 28 [31360/50048]	Loss: 1.0319
Training Epoch: 28 [31488/50048]	Loss: 0.9554
Training Epoch: 28 [31616/50048]	Loss: 0.9845
Training Epoch: 28 [31744/50048]	Loss: 1.0231
Training Epoch: 28 [31872/50048]	Loss: 1.0162
Training Epoch: 28 [32000/50048]	Loss: 1.0835
Training Epoch: 28 [32128/50048]	Loss: 1.0873
Training Epoch: 28 [32256/50048]	Loss: 1.0629
Training Epoch: 28 [32384/50048]	Loss: 1.3890
Training Epoch: 28 [32512/50048]	Loss: 1.0037
Training Epoch: 28 [32640/50048]	Loss: 1.2092
Training Epoch: 28 [32768/50048]	Loss: 0.9289
Training Epoch: 28 [32896/50048]	Loss: 1.1108
Training Epoch: 28 [33024/50048]	Loss: 0.8565
Training Epoch: 28 [33152/50048]	Loss: 1.1142
Training Epoch: 28 [33280/50048]	Loss: 0.9108
Training Epoch: 28 [33408/50048]	Loss: 1.1031
Training Epoch: 28 [33536/50048]	Loss: 1.1657
Training Epoch: 28 [33664/50048]	Loss: 1.1309
Training Epoch: 28 [33792/50048]	Loss: 1.0126
Training Epoch: 28 [33920/50048]	Loss: 1.2317
Training Epoch: 28 [34048/50048]	Loss: 1.1039
Training Epoch: 28 [34176/50048]	Loss: 1.3409
Training Epoch: 28 [34304/50048]	Loss: 1.1504
Training Epoch: 28 [34432/50048]	Loss: 1.1624
Training Epoch: 28 [34560/50048]	Loss: 0.9000
Training Epoch: 28 [34688/50048]	Loss: 1.3710
Training Epoch: 28 [34816/50048]	Loss: 1.1244
Training Epoch: 28 [34944/50048]	Loss: 0.9677
Training Epoch: 28 [35072/50048]	Loss: 0.9406
Training Epoch: 28 [35200/50048]	Loss: 0.9689
Training Epoch: 28 [35328/50048]	Loss: 1.0574
Training Epoch: 28 [35456/50048]	Loss: 0.8399
Training Epoch: 28 [35584/50048]	Loss: 0.9309
Training Epoch: 28 [35712/50048]	Loss: 1.1311
Training Epoch: 28 [35840/50048]	Loss: 1.0892
Training Epoch: 28 [35968/50048]	Loss: 1.2602
Training Epoch: 28 [36096/50048]	Loss: 1.2262
Training Epoch: 28 [36224/50048]	Loss: 0.9732
Training Epoch: 28 [36352/50048]	Loss: 1.1388
Training Epoch: 28 [36480/50048]	Loss: 1.3085
Training Epoch: 28 [36608/50048]	Loss: 1.2426
Training Epoch: 28 [36736/50048]	Loss: 0.9301
Training Epoch: 28 [36864/50048]	Loss: 1.1331
Training Epoch: 28 [36992/50048]	Loss: 0.9900
Training Epoch: 28 [37120/50048]	Loss: 1.0057
Training Epoch: 28 [37248/50048]	Loss: 1.0479
Training Epoch: 28 [37376/50048]	Loss: 1.1209
Training Epoch: 28 [37504/50048]	Loss: 0.8889
Training Epoch: 28 [37632/50048]	Loss: 1.3043
Training Epoch: 28 [37760/50048]	Loss: 1.0775
Training Epoch: 28 [37888/50048]	Loss: 0.9961
Training Epoch: 28 [38016/50048]	Loss: 1.1032
Training Epoch: 28 [38144/50048]	Loss: 1.1218
Training Epoch: 28 [38272/50048]	Loss: 0.9052
Training Epoch: 28 [38400/50048]	Loss: 1.2170
Training Epoch: 28 [38528/50048]	Loss: 1.2338
Training Epoch: 28 [38656/50048]	Loss: 1.3535
Training Epoch: 28 [38784/50048]	Loss: 1.2563
Training Epoch: 28 [38912/50048]	Loss: 1.3419
Training Epoch: 28 [39040/50048]	Loss: 1.0947
Training Epoch: 28 [39168/50048]	Loss: 1.0710
Training Epoch: 28 [39296/50048]	Loss: 0.9936
Training Epoch: 28 [39424/50048]	Loss: 1.0068
Training Epoch: 28 [39552/50048]	Loss: 1.1125
Training Epoch: 28 [39680/50048]	Loss: 1.1944
Training Epoch: 28 [39808/50048]	Loss: 1.2099
Training Epoch: 28 [39936/50048]	Loss: 1.1500
Training Epoch: 28 [40064/50048]	Loss: 1.0575
Training Epoch: 28 [40192/50048]	Loss: 1.1171
Training Epoch: 28 [40320/50048]	Loss: 0.9065
Training Epoch: 28 [40448/50048]	Loss: 0.8682
Training Epoch: 28 [40576/50048]	Loss: 1.1130
Training Epoch: 28 [40704/50048]	Loss: 0.8694
Training Epoch: 28 [40832/50048]	Loss: 1.1477
Training Epoch: 28 [40960/50048]	Loss: 0.8985
Training Epoch: 28 [41088/50048]	Loss: 1.1320
Training Epoch: 28 [41216/50048]	Loss: 1.1578
Training Epoch: 28 [41344/50048]	Loss: 0.9447
Training Epoch: 28 [41472/50048]	Loss: 1.0955
Training Epoch: 28 [41600/50048]	Loss: 0.9992
Training Epoch: 28 [41728/50048]	Loss: 1.0817
Training Epoch: 28 [41856/50048]	Loss: 1.1153
Training Epoch: 28 [41984/50048]	Loss: 1.1160
Training Epoch: 28 [42112/50048]	Loss: 1.0964
Training Epoch: 28 [42240/50048]	Loss: 0.8617
Training Epoch: 28 [42368/50048]	Loss: 0.9916
Training Epoch: 28 [42496/50048]	Loss: 1.0831
Training Epoch: 28 [42624/50048]	Loss: 1.0081
Training Epoch: 28 [42752/50048]	Loss: 1.0643
Training Epoch: 28 [42880/50048]	Loss: 1.1175
Training Epoch: 28 [43008/50048]	Loss: 1.2046
Training Epoch: 28 [43136/50048]	Loss: 0.9800
Training Epoch: 28 [43264/50048]	Loss: 1.0593
Training Epoch: 28 [43392/50048]	Loss: 0.9676
Training Epoch: 28 [43520/50048]	Loss: 1.0198
Training Epoch: 28 [43648/50048]	Loss: 1.0796
Training Epoch: 28 [43776/50048]	Loss: 1.1443
Training Epoch: 28 [43904/50048]	Loss: 1.2352
Training Epoch: 28 [44032/50048]	Loss: 1.0599
Training Epoch: 28 [44160/50048]	Loss: 1.0942
Training Epoch: 28 [44288/50048]	Loss: 0.9682
Training Epoch: 28 [44416/50048]	Loss: 0.9642
Training Epoch: 28 [44544/50048]	Loss: 0.9476
Training Epoch: 28 [44672/50048]	Loss: 1.0984
Training Epoch: 28 [44800/50048]	Loss: 1.3123
Training Epoch: 28 [44928/50048]	Loss: 1.0454
Training Epoch: 28 [45056/50048]	Loss: 1.0677
Training Epoch: 28 [45184/50048]	Loss: 0.9972
Training Epoch: 28 [45312/50048]	Loss: 1.0552
Training Epoch: 28 [45440/50048]	Loss: 1.0109
Training Epoch: 28 [45568/50048]	Loss: 1.0226
Training Epoch: 28 [45696/50048]	Loss: 0.9376
Training Epoch: 28 [45824/50048]	Loss: 0.9756
Training Epoch: 28 [45952/50048]	Loss: 0.9224
Training Epoch: 28 [46080/50048]	Loss: 0.9916
Training Epoch: 28 [46208/50048]	Loss: 0.9618
Training Epoch: 28 [46336/50048]	Loss: 0.9577
Training Epoch: 28 [46464/50048]	Loss: 1.0772
Training Epoch: 28 [46592/50048]	Loss: 1.2186
Training Epoch: 28 [46720/50048]	Loss: 0.9097
Training Epoch: 28 [46848/50048]	Loss: 1.2331
Training Epoch: 28 [46976/50048]	Loss: 1.2807
Training Epoch: 28 [47104/50048]	Loss: 1.1503
Training Epoch: 28 [47232/50048]	Loss: 1.0114
Training Epoch: 28 [47360/50048]	Loss: 0.8938
Training Epoch: 28 [47488/50048]	Loss: 0.9993
Training Epoch: 28 [47616/50048]	Loss: 1.1816
Training Epoch: 28 [47744/50048]	Loss: 1.0214
Training Epoch: 28 [47872/50048]	Loss: 1.1060
Training Epoch: 28 [48000/50048]	Loss: 0.9630
Training Epoch: 28 [48128/50048]	Loss: 1.0123
Training Epoch: 28 [48256/50048]	Loss: 0.8898
Training Epoch: 28 [48384/50048]	Loss: 0.9573
Training Epoch: 28 [48512/50048]	Loss: 0.8139
Training Epoch: 28 [48640/50048]	Loss: 0.9932
Training Epoch: 28 [48768/50048]	Loss: 1.0767
Training Epoch: 28 [48896/50048]	Loss: 1.4179
Training Epoch: 28 [49024/50048]	Loss: 0.9623
Training Epoch: 28 [49152/50048]	Loss: 1.1621
Training Epoch: 28 [49280/50048]	Loss: 1.1547
Training Epoch: 28 [49408/50048]	Loss: 1.3100
Training Epoch: 28 [49536/50048]	Loss: 1.0620
Training Epoch: 28 [49664/50048]	Loss: 1.1005
Training Epoch: 28 [49792/50048]	Loss: 1.0053
Training Epoch: 28 [49920/50048]	Loss: 1.1554
Training Epoch: 28 [50048/50048]	Loss: 1.1125
Validation Epoch: 28, Average loss: 0.0118, Accuracy: 0.5955
Training Epoch: 29 [128/50048]	Loss: 1.1907
Training Epoch: 29 [256/50048]	Loss: 1.0634
Training Epoch: 29 [384/50048]	Loss: 1.2408
Training Epoch: 29 [512/50048]	Loss: 1.2154
Training Epoch: 29 [640/50048]	Loss: 0.9523
Training Epoch: 29 [768/50048]	Loss: 1.1432
Training Epoch: 29 [896/50048]	Loss: 1.2772
Training Epoch: 29 [1024/50048]	Loss: 0.9927
Training Epoch: 29 [1152/50048]	Loss: 1.0852
Training Epoch: 29 [1280/50048]	Loss: 1.2145
Training Epoch: 29 [1408/50048]	Loss: 0.9176
Training Epoch: 29 [1536/50048]	Loss: 0.8693
Training Epoch: 29 [1664/50048]	Loss: 1.0394
Training Epoch: 29 [1792/50048]	Loss: 0.9586
Training Epoch: 29 [1920/50048]	Loss: 0.9380
Training Epoch: 29 [2048/50048]	Loss: 1.0437
Training Epoch: 29 [2176/50048]	Loss: 1.2713
Training Epoch: 29 [2304/50048]	Loss: 0.9053
Training Epoch: 29 [2432/50048]	Loss: 1.0605
Training Epoch: 29 [2560/50048]	Loss: 1.1095
Training Epoch: 29 [2688/50048]	Loss: 0.9498
Training Epoch: 29 [2816/50048]	Loss: 0.8988
Training Epoch: 29 [2944/50048]	Loss: 0.9286
Training Epoch: 29 [3072/50048]	Loss: 0.9823
Training Epoch: 29 [3200/50048]	Loss: 1.1089
Training Epoch: 29 [3328/50048]	Loss: 0.9695
Training Epoch: 29 [3456/50048]	Loss: 1.1094
Training Epoch: 29 [3584/50048]	Loss: 1.0179
Training Epoch: 29 [3712/50048]	Loss: 1.0881
Training Epoch: 29 [3840/50048]	Loss: 0.9621
Training Epoch: 29 [3968/50048]	Loss: 1.0927
Training Epoch: 29 [4096/50048]	Loss: 0.9281
Training Epoch: 29 [4224/50048]	Loss: 0.9292
Training Epoch: 29 [4352/50048]	Loss: 0.9822
Training Epoch: 29 [4480/50048]	Loss: 1.0417
Training Epoch: 29 [4608/50048]	Loss: 1.0094
Training Epoch: 29 [4736/50048]	Loss: 1.1244
Training Epoch: 29 [4864/50048]	Loss: 1.1483
Training Epoch: 29 [4992/50048]	Loss: 0.9950
Training Epoch: 29 [5120/50048]	Loss: 0.9933
Training Epoch: 29 [5248/50048]	Loss: 0.9736
Training Epoch: 29 [5376/50048]	Loss: 1.0440
Training Epoch: 29 [5504/50048]	Loss: 0.9478
Training Epoch: 29 [5632/50048]	Loss: 0.9011
Training Epoch: 29 [5760/50048]	Loss: 0.9053
Training Epoch: 29 [5888/50048]	Loss: 1.0561
Training Epoch: 29 [6016/50048]	Loss: 1.0582
Training Epoch: 29 [6144/50048]	Loss: 1.1441
Training Epoch: 29 [6272/50048]	Loss: 1.0573
Training Epoch: 29 [6400/50048]	Loss: 0.9650
Training Epoch: 29 [6528/50048]	Loss: 1.1750
Training Epoch: 29 [6656/50048]	Loss: 1.1915
Training Epoch: 29 [6784/50048]	Loss: 1.1961
Training Epoch: 29 [6912/50048]	Loss: 0.9360
Training Epoch: 29 [7040/50048]	Loss: 0.8500
Training Epoch: 29 [7168/50048]	Loss: 1.1782
Training Epoch: 29 [7296/50048]	Loss: 1.0437
Training Epoch: 29 [7424/50048]	Loss: 0.9304
Training Epoch: 29 [7552/50048]	Loss: 1.0280
Training Epoch: 29 [7680/50048]	Loss: 1.0829
Training Epoch: 29 [7808/50048]	Loss: 0.9676
Training Epoch: 29 [7936/50048]	Loss: 1.1884
Training Epoch: 29 [8064/50048]	Loss: 1.0950
Training Epoch: 29 [8192/50048]	Loss: 1.2233
Training Epoch: 29 [8320/50048]	Loss: 1.0111
Training Epoch: 29 [8448/50048]	Loss: 0.9510
Training Epoch: 29 [8576/50048]	Loss: 0.7628
Training Epoch: 29 [8704/50048]	Loss: 1.0398
Training Epoch: 29 [8832/50048]	Loss: 0.9297
Training Epoch: 29 [8960/50048]	Loss: 0.9942
Training Epoch: 29 [9088/50048]	Loss: 0.9097
Training Epoch: 29 [9216/50048]	Loss: 1.1266
Training Epoch: 29 [9344/50048]	Loss: 1.0908
Training Epoch: 29 [9472/50048]	Loss: 1.0198
Training Epoch: 29 [9600/50048]	Loss: 0.9687
Training Epoch: 29 [9728/50048]	Loss: 0.9674
Training Epoch: 29 [9856/50048]	Loss: 0.8710
Training Epoch: 29 [9984/50048]	Loss: 1.1932
Training Epoch: 29 [10112/50048]	Loss: 1.0606
Training Epoch: 29 [10240/50048]	Loss: 1.0644
Training Epoch: 29 [10368/50048]	Loss: 1.1615
Training Epoch: 29 [10496/50048]	Loss: 1.0234
Training Epoch: 29 [10624/50048]	Loss: 0.9136
Training Epoch: 29 [10752/50048]	Loss: 1.1253
Training Epoch: 29 [10880/50048]	Loss: 1.0494
Training Epoch: 29 [11008/50048]	Loss: 0.9979
Training Epoch: 29 [11136/50048]	Loss: 1.0625
Training Epoch: 29 [11264/50048]	Loss: 1.1023
Training Epoch: 29 [11392/50048]	Loss: 1.1373
Training Epoch: 29 [11520/50048]	Loss: 1.0634
Training Epoch: 29 [11648/50048]	Loss: 1.0871
Training Epoch: 29 [11776/50048]	Loss: 0.9759
Training Epoch: 29 [11904/50048]	Loss: 1.1259
Training Epoch: 29 [12032/50048]	Loss: 0.9077
Training Epoch: 29 [12160/50048]	Loss: 1.0081
Training Epoch: 29 [12288/50048]	Loss: 1.2571
Training Epoch: 29 [12416/50048]	Loss: 1.2895
Training Epoch: 29 [12544/50048]	Loss: 0.9313
Training Epoch: 29 [12672/50048]	Loss: 1.2056
Training Epoch: 29 [12800/50048]	Loss: 0.9673
Training Epoch: 29 [12928/50048]	Loss: 1.1187
Training Epoch: 29 [13056/50048]	Loss: 0.9983
Training Epoch: 29 [13184/50048]	Loss: 1.0415
Training Epoch: 29 [13312/50048]	Loss: 1.0033
Training Epoch: 29 [13440/50048]	Loss: 0.7492
Training Epoch: 29 [13568/50048]	Loss: 1.0108
Training Epoch: 29 [13696/50048]	Loss: 1.1122
Training Epoch: 29 [13824/50048]	Loss: 1.1774
Training Epoch: 29 [13952/50048]	Loss: 1.1461
Training Epoch: 29 [14080/50048]	Loss: 0.9857
Training Epoch: 29 [14208/50048]	Loss: 0.9148
Training Epoch: 29 [14336/50048]	Loss: 1.1309
Training Epoch: 29 [14464/50048]	Loss: 1.0557
Training Epoch: 29 [14592/50048]	Loss: 0.8489
Training Epoch: 29 [14720/50048]	Loss: 0.9135
Training Epoch: 29 [14848/50048]	Loss: 1.1975
Training Epoch: 29 [14976/50048]	Loss: 0.9910
Training Epoch: 29 [15104/50048]	Loss: 1.2283
Training Epoch: 29 [15232/50048]	Loss: 1.0277
Training Epoch: 29 [15360/50048]	Loss: 0.9780
Training Epoch: 29 [15488/50048]	Loss: 1.1630
Training Epoch: 29 [15616/50048]	Loss: 1.1442
Training Epoch: 29 [15744/50048]	Loss: 1.1595
Training Epoch: 29 [15872/50048]	Loss: 0.9247
Training Epoch: 29 [16000/50048]	Loss: 1.0616
Training Epoch: 29 [16128/50048]	Loss: 0.9894
Training Epoch: 29 [16256/50048]	Loss: 1.0157
Training Epoch: 29 [16384/50048]	Loss: 1.2266
Training Epoch: 29 [16512/50048]	Loss: 1.3193
Training Epoch: 29 [16640/50048]	Loss: 1.0305
Training Epoch: 29 [16768/50048]	Loss: 1.0286
Training Epoch: 29 [16896/50048]	Loss: 1.2335
Training Epoch: 29 [17024/50048]	Loss: 1.1576
Training Epoch: 29 [17152/50048]	Loss: 1.0443
Training Epoch: 29 [17280/50048]	Loss: 1.1635
Training Epoch: 29 [17408/50048]	Loss: 1.1434
Training Epoch: 29 [17536/50048]	Loss: 1.1057
Training Epoch: 29 [17664/50048]	Loss: 1.0324
Training Epoch: 29 [17792/50048]	Loss: 1.1792
Training Epoch: 29 [17920/50048]	Loss: 1.1976
Training Epoch: 29 [18048/50048]	Loss: 1.1559
Training Epoch: 29 [18176/50048]	Loss: 1.0730
Training Epoch: 29 [18304/50048]	Loss: 0.8438
Training Epoch: 29 [18432/50048]	Loss: 0.9544
Training Epoch: 29 [18560/50048]	Loss: 0.9692
Training Epoch: 29 [18688/50048]	Loss: 0.9665
Training Epoch: 29 [18816/50048]	Loss: 1.1438
Training Epoch: 29 [18944/50048]	Loss: 0.9811
Training Epoch: 29 [19072/50048]	Loss: 1.0909
Training Epoch: 29 [19200/50048]	Loss: 1.0153
Training Epoch: 29 [19328/50048]	Loss: 1.1598
Training Epoch: 29 [19456/50048]	Loss: 0.7967
Training Epoch: 29 [19584/50048]	Loss: 1.2732
Training Epoch: 29 [19712/50048]	Loss: 1.0395
Training Epoch: 29 [19840/50048]	Loss: 1.1125
Training Epoch: 29 [19968/50048]	Loss: 0.9804
Training Epoch: 29 [20096/50048]	Loss: 0.9966
Training Epoch: 29 [20224/50048]	Loss: 0.9480
Training Epoch: 29 [20352/50048]	Loss: 1.2498
Training Epoch: 29 [20480/50048]	Loss: 0.9151
Training Epoch: 29 [20608/50048]	Loss: 1.0185
Training Epoch: 29 [20736/50048]	Loss: 1.2177
Training Epoch: 29 [20864/50048]	Loss: 0.9885
Training Epoch: 29 [20992/50048]	Loss: 0.9763
Training Epoch: 29 [21120/50048]	Loss: 0.9173
Training Epoch: 29 [21248/50048]	Loss: 0.9643
Training Epoch: 29 [21376/50048]	Loss: 0.9778
Training Epoch: 29 [21504/50048]	Loss: 1.0980
Training Epoch: 29 [21632/50048]	Loss: 1.0208
Training Epoch: 29 [21760/50048]	Loss: 0.9441
Training Epoch: 29 [21888/50048]	Loss: 1.0916
Training Epoch: 29 [22016/50048]	Loss: 1.1635
Training Epoch: 29 [22144/50048]	Loss: 1.1346
Training Epoch: 29 [22272/50048]	Loss: 1.0447
Training Epoch: 29 [22400/50048]	Loss: 1.0133
Training Epoch: 29 [22528/50048]	Loss: 1.2906
Training Epoch: 29 [22656/50048]	Loss: 1.0241
Training Epoch: 29 [22784/50048]	Loss: 1.3234
Training Epoch: 29 [22912/50048]	Loss: 1.0182
Training Epoch: 29 [23040/50048]	Loss: 0.8787
Training Epoch: 29 [23168/50048]	Loss: 1.0482
Training Epoch: 29 [23296/50048]	Loss: 1.0648
Training Epoch: 29 [23424/50048]	Loss: 0.9740
Training Epoch: 29 [23552/50048]	Loss: 1.0508
Training Epoch: 29 [23680/50048]	Loss: 1.2666
Training Epoch: 29 [23808/50048]	Loss: 1.0768
Training Epoch: 29 [23936/50048]	Loss: 1.2091
Training Epoch: 29 [24064/50048]	Loss: 1.0715
Training Epoch: 29 [24192/50048]	Loss: 0.9997
Training Epoch: 29 [24320/50048]	Loss: 0.9898
Training Epoch: 29 [24448/50048]	Loss: 0.9073
Training Epoch: 29 [24576/50048]	Loss: 1.0904
Training Epoch: 29 [24704/50048]	Loss: 0.8523
Training Epoch: 29 [24832/50048]	Loss: 1.0688
Training Epoch: 29 [24960/50048]	Loss: 1.0985
Training Epoch: 29 [25088/50048]	Loss: 0.9600
Training Epoch: 29 [25216/50048]	Loss: 1.0935
Training Epoch: 29 [25344/50048]	Loss: 1.0610
Training Epoch: 29 [25472/50048]	Loss: 1.0231
Training Epoch: 29 [25600/50048]	Loss: 0.9871
Training Epoch: 29 [25728/50048]	Loss: 1.0050
Training Epoch: 29 [25856/50048]	Loss: 1.0778
Training Epoch: 29 [25984/50048]	Loss: 1.2462
Training Epoch: 29 [26112/50048]	Loss: 0.8809
Training Epoch: 29 [26240/50048]	Loss: 1.0076
Training Epoch: 29 [26368/50048]	Loss: 0.9887
Training Epoch: 29 [26496/50048]	Loss: 0.8120
Training Epoch: 29 [26624/50048]	Loss: 1.1680
Training Epoch: 29 [26752/50048]	Loss: 1.2448
Training Epoch: 29 [26880/50048]	Loss: 0.9332
Training Epoch: 29 [27008/50048]	Loss: 1.1676
Training Epoch: 29 [27136/50048]	Loss: 1.1288
Training Epoch: 29 [27264/50048]	Loss: 0.8512
Training Epoch: 29 [27392/50048]	Loss: 0.8900
Training Epoch: 29 [27520/50048]	Loss: 0.9472
Training Epoch: 29 [27648/50048]	Loss: 1.0527
Training Epoch: 29 [27776/50048]	Loss: 1.0560
Training Epoch: 29 [27904/50048]	Loss: 1.2925
Training Epoch: 29 [28032/50048]	Loss: 1.1046
Training Epoch: 29 [28160/50048]	Loss: 0.9807
Training Epoch: 29 [28288/50048]	Loss: 0.9958
Training Epoch: 29 [28416/50048]	Loss: 1.0846
Training Epoch: 29 [28544/50048]	Loss: 1.1749
Training Epoch: 29 [28672/50048]	Loss: 0.8331
Training Epoch: 29 [28800/50048]	Loss: 1.0910
Training Epoch: 29 [28928/50048]	Loss: 0.9969
Training Epoch: 29 [29056/50048]	Loss: 0.9079
Training Epoch: 29 [29184/50048]	Loss: 1.0232
Training Epoch: 29 [29312/50048]	Loss: 1.0183
Training Epoch: 29 [29440/50048]	Loss: 0.9917
Training Epoch: 29 [29568/50048]	Loss: 1.0384
Training Epoch: 29 [29696/50048]	Loss: 1.1810
Training Epoch: 29 [29824/50048]	Loss: 1.0442
Training Epoch: 29 [29952/50048]	Loss: 0.8552
Training Epoch: 29 [30080/50048]	Loss: 0.9018
Training Epoch: 29 [30208/50048]	Loss: 1.0475
Training Epoch: 29 [30336/50048]	Loss: 1.2131
Training Epoch: 29 [30464/50048]	Loss: 1.0488
Training Epoch: 29 [30592/50048]	Loss: 1.1306
Training Epoch: 29 [30720/50048]	Loss: 1.1888
Training Epoch: 29 [30848/50048]	Loss: 1.1588
Training Epoch: 29 [30976/50048]	Loss: 1.1971
Training Epoch: 29 [31104/50048]	Loss: 0.9668
Training Epoch: 29 [31232/50048]	Loss: 1.0551
Training Epoch: 29 [31360/50048]	Loss: 0.8828
Training Epoch: 29 [31488/50048]	Loss: 1.2285
Training Epoch: 29 [31616/50048]	Loss: 1.0681
Training Epoch: 29 [31744/50048]	Loss: 1.1220
Training Epoch: 29 [31872/50048]	Loss: 1.0444
Training Epoch: 29 [32000/50048]	Loss: 1.0761
Training Epoch: 29 [32128/50048]	Loss: 1.1751
Training Epoch: 29 [32256/50048]	Loss: 1.0334
Training Epoch: 29 [32384/50048]	Loss: 1.2910
Training Epoch: 29 [32512/50048]	Loss: 1.1266
Training Epoch: 29 [32640/50048]	Loss: 0.8475
Training Epoch: 29 [32768/50048]	Loss: 1.0868
Training Epoch: 29 [32896/50048]	Loss: 0.9918
Training Epoch: 29 [33024/50048]	Loss: 1.0920
Training Epoch: 29 [33152/50048]	Loss: 1.1125
Training Epoch: 29 [33280/50048]	Loss: 1.1513
Training Epoch: 29 [33408/50048]	Loss: 1.0668
Training Epoch: 29 [33536/50048]	Loss: 0.9829
Training Epoch: 29 [33664/50048]	Loss: 1.2020
Training Epoch: 29 [33792/50048]	Loss: 0.9957
Training Epoch: 29 [33920/50048]	Loss: 1.2253
Training Epoch: 29 [34048/50048]	Loss: 1.2689
Training Epoch: 29 [34176/50048]	Loss: 1.0219
Training Epoch: 29 [34304/50048]	Loss: 1.0937
Training Epoch: 29 [34432/50048]	Loss: 1.0255
Training Epoch: 29 [34560/50048]	Loss: 1.0479
Training Epoch: 29 [34688/50048]	Loss: 0.8512
Training Epoch: 29 [34816/50048]	Loss: 1.0495
Training Epoch: 29 [34944/50048]	Loss: 1.2654
Training Epoch: 29 [35072/50048]	Loss: 1.0527
Training Epoch: 29 [35200/50048]	Loss: 0.7572
Training Epoch: 29 [35328/50048]	Loss: 1.0263
Training Epoch: 29 [35456/50048]	Loss: 1.0723
Training Epoch: 29 [35584/50048]	Loss: 0.9060
Training Epoch: 29 [35712/50048]	Loss: 1.1167
Training Epoch: 29 [35840/50048]	Loss: 1.2200
Training Epoch: 29 [35968/50048]	Loss: 1.0063
Training Epoch: 29 [36096/50048]	Loss: 0.9938
Training Epoch: 29 [36224/50048]	Loss: 1.1016
Training Epoch: 29 [36352/50048]	Loss: 0.9473
Training Epoch: 29 [36480/50048]	Loss: 0.9409
Training Epoch: 29 [36608/50048]	Loss: 0.9550
Training Epoch: 29 [36736/50048]	Loss: 0.9924
Training Epoch: 29 [36864/50048]	Loss: 1.1188
Training Epoch: 29 [36992/50048]	Loss: 1.0812
Training Epoch: 29 [37120/50048]	Loss: 1.0266
Training Epoch: 29 [37248/50048]	Loss: 1.1367
Training Epoch: 29 [37376/50048]	Loss: 1.2038
Training Epoch: 29 [37504/50048]	Loss: 0.8789
Training Epoch: 29 [37632/50048]	Loss: 1.4680
Training Epoch: 29 [37760/50048]	Loss: 1.1714
Training Epoch: 29 [37888/50048]	Loss: 0.8648
Training Epoch: 29 [38016/50048]	Loss: 1.1210
Training Epoch: 29 [38144/50048]	Loss: 0.9071
Training Epoch: 29 [38272/50048]	Loss: 1.1344
Training Epoch: 29 [38400/50048]	Loss: 0.9803
Training Epoch: 29 [38528/50048]	Loss: 1.1595
Training Epoch: 29 [38656/50048]	Loss: 0.9522
Training Epoch: 29 [38784/50048]	Loss: 1.3228
Training Epoch: 29 [38912/50048]	Loss: 1.0782
Training Epoch: 29 [39040/50048]	Loss: 1.0799
Training Epoch: 29 [39168/50048]	Loss: 0.9685
Training Epoch: 29 [39296/50048]	Loss: 1.1570
Training Epoch: 29 [39424/50048]	Loss: 0.8827
Training Epoch: 29 [39552/50048]	Loss: 0.9771
Training Epoch: 29 [39680/50048]	Loss: 1.0515
Training Epoch: 29 [39808/50048]	Loss: 1.1881
Training Epoch: 29 [39936/50048]	Loss: 0.8480
Training Epoch: 29 [40064/50048]	Loss: 1.1099
Training Epoch: 29 [40192/50048]	Loss: 0.9004
Training Epoch: 29 [40320/50048]	Loss: 1.1478
Training Epoch: 29 [40448/50048]	Loss: 0.9630
Training Epoch: 29 [40576/50048]	Loss: 1.0880
Training Epoch: 29 [40704/50048]	Loss: 1.0163
Training Epoch: 29 [40832/50048]	Loss: 1.1616
Training Epoch: 29 [40960/50048]	Loss: 0.9406
Training Epoch: 29 [41088/50048]	Loss: 1.0480
Training Epoch: 29 [41216/50048]	Loss: 1.2300
Training Epoch: 29 [41344/50048]	Loss: 1.0629
Training Epoch: 29 [41472/50048]	Loss: 1.0876
Training Epoch: 29 [41600/50048]	Loss: 1.0097
Training Epoch: 29 [41728/50048]	Loss: 1.0196
Training Epoch: 29 [41856/50048]	Loss: 1.0229
Training Epoch: 29 [41984/50048]	Loss: 1.0095
Training Epoch: 29 [42112/50048]	Loss: 1.0830
Training Epoch: 29 [42240/50048]	Loss: 1.1459
Training Epoch: 29 [42368/50048]	Loss: 1.2982
Training Epoch: 29 [42496/50048]	Loss: 0.9647
Training Epoch: 29 [42624/50048]	Loss: 0.9757
Training Epoch: 29 [42752/50048]	Loss: 0.9530
Training Epoch: 29 [42880/50048]	Loss: 1.0898
Training Epoch: 29 [43008/50048]	Loss: 0.9403
Training Epoch: 29 [43136/50048]	Loss: 1.3207
Training Epoch: 29 [43264/50048]	Loss: 0.8766
Training Epoch: 29 [43392/50048]	Loss: 0.8446
Training Epoch: 29 [43520/50048]	Loss: 1.0618
Training Epoch: 29 [43648/50048]	Loss: 1.0854
Training Epoch: 29 [43776/50048]	Loss: 1.1917
Training Epoch: 29 [43904/50048]	Loss: 1.2595
Training Epoch: 29 [44032/50048]	Loss: 1.2431
Training Epoch: 29 [44160/50048]	Loss: 0.9759
Training Epoch: 29 [44288/50048]	Loss: 1.0164
Training Epoch: 29 [44416/50048]	Loss: 1.0103
Training Epoch: 29 [44544/50048]	Loss: 1.0626
Training Epoch: 29 [44672/50048]	Loss: 1.1816
Training Epoch: 29 [44800/50048]	Loss: 1.0162
Training Epoch: 29 [44928/50048]	Loss: 1.1756
Training Epoch: 29 [45056/50048]	Loss: 1.2359
Training Epoch: 29 [45184/50048]	Loss: 0.9829
Training Epoch: 29 [45312/50048]	Loss: 0.8470
Training Epoch: 29 [45440/50048]	Loss: 1.2123
Training Epoch: 29 [45568/50048]	Loss: 1.1973
Training Epoch: 29 [45696/50048]	Loss: 0.8648
Training Epoch: 29 [45824/50048]	Loss: 1.0522
Training Epoch: 29 [45952/50048]	Loss: 0.9104
Training Epoch: 29 [46080/50048]	Loss: 1.1926
Training Epoch: 29 [46208/50048]	Loss: 1.2161
Training Epoch: 29 [46336/50048]	Loss: 1.1469
Training Epoch: 29 [46464/50048]	Loss: 1.0417
Training Epoch: 29 [46592/50048]	Loss: 0.8871
Training Epoch: 29 [46720/50048]	Loss: 1.1616
Training Epoch: 29 [46848/50048]	Loss: 0.8181
Training Epoch: 29 [46976/50048]	Loss: 1.2884
Training Epoch: 29 [47104/50048]	Loss: 1.1441
Training Epoch: 29 [47232/50048]	Loss: 1.1333
Training Epoch: 29 [47360/50048]	Loss: 1.2481
Training Epoch: 29 [47488/50048]	Loss: 1.2719
Training Epoch: 29 [47616/50048]	Loss: 1.0237
Training Epoch: 29 [47744/50048]	Loss: 0.9348
Training Epoch: 29 [47872/50048]	Loss: 1.1226
Training Epoch: 29 [48000/50048]	Loss: 1.1182
Training Epoch: 29 [48128/50048]	Loss: 1.1804
Training Epoch: 29 [48256/50048]	Loss: 0.9805
Training Epoch: 29 [48384/50048]	Loss: 1.4343
Training Epoch: 29 [48512/50048]	Loss: 1.0980
Training Epoch: 29 [48640/50048]	Loss: 1.0776
Training Epoch: 29 [48768/50048]	Loss: 1.0744
Training Epoch: 29 [48896/50048]	Loss: 1.1447
Training Epoch: 29 [49024/50048]	Loss: 0.9129
Training Epoch: 29 [49152/50048]	Loss: 0.9267
Training Epoch: 29 [49280/50048]	Loss: 1.0488
Training Epoch: 29 [49408/50048]	Loss: 0.9084
Training Epoch: 29 [49536/50048]	Loss: 0.8694
Training Epoch: 29 [49664/50048]	Loss: 1.0148
Training Epoch: 29 [49792/50048]	Loss: 1.1903
Training Epoch: 29 [49920/50048]	Loss: 1.0284
Training Epoch: 29 [50048/50048]	Loss: 1.1173
Validation Epoch: 29, Average loss: 0.0116, Accuracy: 0.6007
[Training Loop] Target accuracy 0.6 reached!
[Training Loop] Training done
Stopped Zeus monitor 0.
